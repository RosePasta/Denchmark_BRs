<bug id='36958' author='ziofil' open_date='2020-02-21T14:41:24Z' closed_time='2020-04-06T14:58:49Z'>
	<summary>How to optimize a complex phase?</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): yes
OS Platform and Distribution: MacOS 10.15.3
TensorFlow installed from binary: 2.1.0

Describe the current behavior
A real loss function that uses complex numbers internally requires Variables to be complex.
But this leads to complex gradients, even though they should be real.
Describe the expected behavior
In TF1.x I used to circumvent this problem by defining my real variables as (e.g.):
x = tf.complex(tf.Variable(1.0, dtype=tf.float32), tf.constant(0.0, dtype=tf.float32))
TF2.x cannot compute gradients if I use this trick.
Standalone code to reproduce the issue
For example, I want to optimize the real angle x in the complex phase exp(1j*x):
def loss(x):
    return tf.abs(tf.exp(1j*x)-1.0) # minimized by x = 0.0

x = tf.Variable(1.0, dtype=tf.complex64) # forced to have type tf.complex64
LR = 0.01

for n in range(10):
    with tf.GradientTape() as tape:
        L = loss(x)
        grad = tape.gradient(L, x)
        x.assign_sub(LR * grad)

print(x)
# &lt;tf.Variable 'Variable:0' shape=() dtype=complex64, numpy=(0.9122518+0.043542963j)&gt;
	</description>
	<comments>
		<comment id='1' author='ziofil' date='2020-02-25T10:33:24Z'>
		I was able to reproduce the issue with Tf 2.1.
Please take a look at the &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/8aff9ca30ab54aae040b1a2e3628f0bc/untitled402.ipynb&gt;gist&lt;/denchmark-link&gt;
. Thanks
		</comment>
		<comment id='2' author='ziofil' date='2020-03-30T17:24:48Z'>
		As long as you do it inside the GradientTape, the same trick should work fine in 2.x:
&lt;denchmark-code&gt;import tensorflow as tf
v = tf.Variable(1.0, dtype=tf.float32)
with tf.GradientTape() as tape:
  x = tf.complex(v, tf.constant(0.0, dtype=tf.float32)) ** 2.
v_grad = tape.gradient(x, v)
print(v_grad)  # tf.Tensor(2.0, shape=(), dtype=float32)
&lt;/denchmark-code&gt;

In 1.x it probably didn't matter where you put the tf.complex, but with GradientTape it really needs to be inside the scope so the tape can record the operation. But otherwise it'll be treated identically as with tf.gradients.
Does that work for you?
		</comment>
		<comment id='3' author='ziofil' date='2020-04-06T14:58:49Z'>
		It does! Thank you so much (I had forgotten to do this inside the GradientTape context)
		</comment>
		<comment id='4' author='ziofil' date='2020-04-06T14:58:51Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36958&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36958&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>