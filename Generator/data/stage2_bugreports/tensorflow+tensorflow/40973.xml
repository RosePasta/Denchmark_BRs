<bug id='40973' author='yuandaxing' open_date='2020-07-01T08:44:12Z' closed_time='2020-07-14T06:21:44Z'>
	<summary>distribute keras sparse feature training error</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
"16.04.6 LTS (Xenial Xerus)"
TensorFlow installed from
binary
TensorFlow version (use command below):
2.2.0
Python version:
3.7.3
CUDA/cuDNN version:
GPU model and memory:
Tesla P40  22919MiB

Standalone code to reproduce the issue (only part of the code)
import tensorflow as tf
import tensorflow.keras.backend as K
import sys

from datetime import date, timedelta
from tensorflow.keras.layers import *
from tensorflow.keras import Model, Sequential
from meta import ModelMeta
from dataset import train_input_fn
from layers import EmbedLayerForSparse
from deepctr.layers import InnerProductLayer, PredictionLayer
batch_thread_number = 15
batch_size = 8192
dimension = 16

strategy = tf.distribute.experimental.CentralStorageStrategy()
with strategy.scope():
    inputs = {}
    outputs = {}
    for k, v in capacity_map.items():
        inputs[k] = Input(shape=(v,), sparse=True, name=k)
        outputs[k] = EmbedLayerForSparse(vocabulary_size=v, embedding_size=dimension, embed_initializer=tf.keras.initializers.he_normal(), n\
ame='SparseLayer'+k)(inputs[k])
    linear = list(outputs.values())
    inner = InnerProductLayer()([Reshape((1, dimension))(f) for f in outputs.values()])
    inner_reshaped = Reshape((inner.get_shape().as_list()[1],))(inner)
    concat = Concatenate(axis=1)(linear + [inner_reshaped])
    outputs = ReLU()(BatchNormalization()(concat))
    outputs = Dense(512)(outputs)
    outputs = ReLU()(BatchNormalization()(outputs))
    outputs = Dense(128)(outputs)
    outputs = ReLU()(BatchNormalization()(outputs))
    outputs = Dense(1)(outputs)
    outputs = PredictionLayer(use_bias=False)(outputs)

    model = tf.keras.Model(inputs=list(inputs.values()), outputs=outputs)
    model_meta = ModelMeta(model, candidate_meta)
    initial_learning_rate = 0.001
    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
            initial_learning_rate,
            decay_steps=1000,
            decay_rate=0.96,
            staircase=True)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule), loss=tf.keras.losses.BinaryCrossentropy(), metrics=[tf.kera\
s.metrics.AUC(num_thresholds=10000)])
    print('compile complete')
with strategy.scope():
  while start_date &lt; end_date:
    dataset = train_input_fn('/path/to' + start_date.isoformat(), batch_thread_number, batch_size, capacity_map)
    model.fit(dataset, batch_size=batch_size, callbacks=[tf.keras.callbacks.TensorBoard(log_dir='/logs/' + start_date.isoformat(), profile_batch=0)])
    model.save('model.' + start_date.isoformat())
    start_date += timedelta(days=1)
    break
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
work.log:
hon.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
No similar op available at this time.
WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/parsing_config.py:719: sparse_merge (from tensorflow.pyt
hon.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
No similar op available at this time.
2020-07-01 16:16:14.067956: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argumen
t: indices[6689] = [527,217950] is repeated
2020-07-01 16:16:14.123837: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argumen
t: indices[8964] = [527,217950] is repeated
2020-07-01 16:16:14.556197: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argumen
t: indices[6689] = [527,217950] is repeated
2020-07-01 16:16:14.606631: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argumen
t: indices[20296] = [446,1263545] is repeated
2020-07-01 16:16:14.605999: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argumen
t: indices[20296] = [446,1263545] is repeated
2020-07-01 16:16:14.670875: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argumen
t: indices[8964] = [527,217950] is repeated
2020-07-01 16:16:14.806522: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argumen
t: indices[2997] = [104,246925] is repeated
2020-07-01 16:16:15.025014: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argumen
t: indices[27417] = [1409,577748] is repeated
2020-07-01 16:16:15.616607: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argumen
t: indices[121] = [1,827513] is repeated
2020-07-01 16:16:34.125064: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argumen
t: indices[115717] = [4149,1289703] is repeated
2020-07-01 16:16:34.184952: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argumen
2020-07-01 16:16:34.187837: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argumen
t: indices[183021] = [4086,3891] is repeated
Traceback (most recent call last):
File "pnn.py", line 67, in 
model.fit(dataset, batch_size=batch_size, callbacks=[tf.keras.callbacks.TensorBoard(log_dir='/logs/' + start_date.isoformat(), profile_ba
tch=0)])
File "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
return method(self, *args, **kwargs)
File "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py", line 848, in fit
tmp_logs = train_function(iterator)
File "/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 580, in call
result = self._call(*args, **kwds)
File "/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 644, in _call
return self._stateless_fn(*args, **kwds)
File "/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 2420, in call
return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
File "/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 1665, in _filtered_call
self.captured_inputs)
File "/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 1746, in _call_flat
ctx, args, cancellation_manager=cancellation_manager))
File "/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 598, in call
ctx=ctx)
File "/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError:  indices[6689] = [527,217950] is repeated
[[{{node SerializeManySparse_10}}]]
[[MultiDeviceIteratorGetNextFromShard]]
[[RemoteCall]]
[[IteratorGetNext]] [Op:__inference_train_function_25126]
Function call stack:
train_function
ps.log:
2020-07-01 16:16:34.184952: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argumen
t: indices[183021] = [4086,3891] is repeated
2020-07-01 16:16:34.187837: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argumen
t: indices[183021] = [4086,3891] is repeated
Traceback (most recent call last):
File "pnn.py", line 67, in 
model.fit(dataset, batch_size=batch_size, callbacks=[tf.keras.callbacks.TensorBoard(log_dir='/logs/' + start_date.isoformat(), profile_ba
tch=0)])
File "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
return method(self, *args, **kwargs)
File "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py", line 848, in fit
tmp_logs = train_function(iterator)
File "/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 580, in call
result = self._call(*args, **kwds)
File "/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 644, in _call
return self._stateless_fn(*args, **kwds)
File "/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 2420, in call
return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
File "/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 1665, in _filtered_call
self.captured_inputs)
File "/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 1746, in _call_flat
ctx, args, cancellation_manager=cancellation_manager))
File "/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 598, in call
ctx=ctx)
File "/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError:  indices[6689] = [527,217950] is repeated
[[{{node SerializeManySparse_10}}]]
[[MultiDeviceIteratorGetNextFromShard]]
[[RemoteCall]]
[[IteratorGetNext]] [Op:__inference_train_function_25126]
Function call stack:
train_function
	</description>
	<comments>
		<comment id='1' author='yuandaxing' date='2020-07-01T09:10:09Z'>
		&lt;denchmark-link:https://github.com/yuandaxing&gt;@yuandaxing&lt;/denchmark-link&gt;

I have tried in colab with TF -GPU version 2.2 and i am seeing the below error message (ImportError: cannot import name 'ModelMeta').Please, share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!
		</comment>
		<comment id='2' author='yuandaxing' date='2020-07-01T10:53:17Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/yuandaxing&gt;@yuandaxing&lt;/denchmark-link&gt;

I have encountered the same problem, and I put my code on google drive and colab.
Here is the link:
&lt;denchmark-link:https://drive.google.com/drive/folders/1PU7QtZa2_2nuf1sDK_2sNj3w3CtJA9ep&gt;https://drive.google.com/drive/folders/1PU7QtZa2_2nuf1sDK_2sNj3w3CtJA9ep&lt;/denchmark-link&gt;

The script should be run in distributed mode, but I had no idea how to do this in colab.
So I wrote the script 'run-dist.sh', you can reproduce the error by running it.
		</comment>
		<comment id='3' author='yuandaxing' date='2020-07-01T20:12:52Z'>
		Hi &lt;denchmark-link:https://github.com/justsmoke&gt;@justsmoke&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/yuandaxing&gt;@yuandaxing&lt;/denchmark-link&gt;
. Is it possible for one of you to provide a minimal example using data from tf.keras.datasets or tfds so that we don't have to download the data separately to reproduce?
		</comment>
		<comment id='4' author='yuandaxing' date='2020-07-02T00:32:52Z'>
		&lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;

the training data is file p0 which is included in the google drive link:
&lt;denchmark-link:https://drive.google.com/drive/folders/1PU7QtZa2_2nuf1sDK_2sNj3w3CtJA9ep&gt;https://drive.google.com/drive/folders/1PU7QtZa2_2nuf1sDK_2sNj3w3CtJA9ep&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='yuandaxing' date='2020-07-02T01:51:58Z'>
		&lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;

Great thanks for your response, I have blocked with this problem for a long time.
&lt;denchmark-link:https://github.com/justsmoke&gt;@justsmoke&lt;/denchmark-link&gt;
's problem is the same with me. please use code and data he provided above.
		</comment>
		<comment id='6' author='yuandaxing' date='2020-07-02T18:33:58Z'>
		&lt;denchmark-link:https://github.com/justsmoke&gt;@justsmoke&lt;/denchmark-link&gt;
 can you add your stack trace here so I can verify it is the same? I'm seeing something similar, but slightly different to what &lt;denchmark-link:https://github.com/yuandaxing&gt;@yuandaxing&lt;/denchmark-link&gt;
 posted.
		</comment>
		<comment id='7' author='yuandaxing' date='2020-07-02T19:00:28Z'>
		Can either of you also confirm that you still see this same error when your model only contains tf.keras.layers and no deepctr.layers?
		</comment>
		<comment id='8' author='yuandaxing' date='2020-07-03T01:48:52Z'>
		&lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;

I have updated the pnn_keras.py script, and it no longer depends on deepctr.
And the error still occurs, here is my stack trace:
ps.log:
2020-07-03 09:41:50.993744: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-07-03 09:41:52.374363: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2020-07-03 09:41:52.374484: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 6db40ae45fe4
2020-07-03 09:41:52.374520: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 6db40ae45fe4
2020-07-03 09:41:52.374708: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 418.40.4
2020-07-03 09:41:52.374775: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 418.40.4
2020-07-03 09:41:52.374802: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 418.40.4
2020-07-03 09:41:52.375558: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-07-03 09:41:52.393014: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2199890000 Hz
2020-07-03 09:41:52.396294: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fcbf0bafc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-03 09:41:52.396348: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
WARNING:tensorflow:From /opt/conda/envs/deepctr/lib/python3.8/site-packages/tensorflow/python/ops/parsing_config.py:715: sparse_merge (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
No similar op available at this time.
2020-07-03 09:42:06.620853: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[1419] = [73,209681] is repeated
2020-07-03 09:42:06.621060: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[1419] = [73,209681] is repeated
2020-07-03 09:42:06.621303: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[699] = [4,1866171] is repeated
2020-07-03 09:42:06.624197: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[13589] = [542,27051] is repeated
2020-07-03 09:42:06.646976: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[17834] = [790,476866] is repeated
2020-07-03 09:42:06.647197: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[20119] = [426,508352] is repeated
2020-07-03 09:42:06.646990: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[20119] = [426,508352] is repeated
2020-07-03 09:42:06.646995: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[27656] = [933,1537230] is repeated
2020-07-03 09:42:06.647204: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[27656] = [933,1537230] is repeated
2020-07-03 09:42:06.648311: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[64637] = [759,1443134] is repeated
Traceback (most recent call last):
File "pnn.py", line 117, in 
model.fit(dataset, batch_size=batch_size, shuffle=False)
File "/opt/conda/envs/deepctr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
return method(self, *args, **kwargs)
File "/opt/conda/envs/deepctr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 848, in fit
tmp_logs = train_function(iterator)
File "/opt/conda/envs/deepctr/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 580, in 
2020-07-03 09:42:06.685234: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[43729] = [542,355339] is repeated
result = self._call(*args, **kwds)
File "/opt/conda/envs/deepctr/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 644, in _call
return self._stateless_fn(*args, **kwds)
File "/opt/conda/envs/deepctr/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 2420, in 
2020-07-03 09:42:06.734912: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[3872] = [159,1328680] is repeated
2020-07-03 09:42:06.735250: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[31196] = [646,1269494] is repeated
2020-07-03 09:42:06.735724: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[1722] = [19,709261] is repeated
2020-07-03 09:42:06.735998: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[9860] = [410,133009] is repeated
2020-07-03 09:42:06.736760: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[31196] = [646,1269494] is repeated
return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
File "/opt/conda/envs/deepctr/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 1661, in _filtered_call
return self._call_flat(
File "/opt/conda/envs/deepctr/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 1745, in _call_flat
return self._build_call_outputs(self._inference_function.call(
File "/opt/conda/envs/deepctr/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 593, in call
outputs = execute.execute(
File "/opt/conda/envs/deepctr/lib/python3.8/site-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute
tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError:  indices[1419] = [73,209681] is repeated
[[{{node SerializeManySparse_9}}]]
[[MultiDeviceIteratorGetNextFromShard]]
[[RemoteCall]]
[[IteratorGetNext]] [Op:__inference_train_function_10267]
Function call stack:
train_function
worker.log:
2020-07-03 09:42:06.403419: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-03 09:42:07.910154: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[27656] = [933,1537230] is repeated
2020-07-03 09:42:07.910257: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[20119] = [426,508352] is repeated
2020-07-03 09:42:07.918541: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[64637] = [759,1443134] is repeated
2020-07-03 09:42:07.919704: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[20119] = [426,508352] is repeated
2020-07-03 09:42:07.921871: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[17834] = [790,476866] is repeated
2020-07-03 09:42:07.923514: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[27656] = [933,1537230] is repeated
Traceback (most recent call last):
File "pnn.py", line 117, in 
model.fit(dataset, batch_size=batch_size, shuffle=False)
File "/opt/conda/envs/deepctr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
return method(self, *args, **kwargs)
File "/opt/conda/envs/deepctr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 848, in fit
tmp_logs = train_function(iterator)
File "/opt/conda/envs/deepctr/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 580, in call
result = self._call(*args, **kwds)
File "/opt/conda/envs/deepctr/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 644, in _call
return self._stateless_fn(*args, **kwds)
File "/opt/conda/envs/deepctr/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 2420, in call
return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
File "/opt/conda/envs/deepctr/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 1661, in _filtered_call
return self._call_flat(
File "/opt/conda/envs/deepctr/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 1745, in _call_flat
return self._build_call_outputs(self._inference_function.call(
File "/opt/conda/envs/deepctr/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 593, in call
outputs = execute.execute(
File "/opt/conda/envs/deepctr/lib/python3.8/site-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute
tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
(0) Invalid argument:  indices[27656] = [933,1537230] is repeated
[[{{node SerializeManySparse_38}}]]
[[MultiDeviceIteratorGetNextFromShard]]
[[RemoteCall]]
[[IteratorGetNext]]
[[IteratorGetNext/_6]]
(1) Invalid argument:  indices[27656] = [933,1537230] is repeated
[[{{node SerializeManySparse_38}}]]
[[MultiDeviceIteratorGetNextFromShard]]
[[RemoteCall]]
[[IteratorGetNext]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_10267]
Function call stack:
train_function -&gt; train_function
2020-07-03 09:42:07.980333: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[1419] = [73,209681] is repeated
2020-07-03 09:42:07.980589: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[1419] = [73,209681] is repeated
2020-07-03 09:42:07.980987: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[13589] = [542,27051] is repeated
2020-07-03 09:42:07.984101: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[699] = [4,1866171] is repeated
2020-07-03 09:42:08.022799: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[43729] = [542,355339] is repeated
2020-07-03 09:42:08.049527: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[31196] = [646,1269494] is repeated
2020-07-03 09:42:08.049589: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[9860] = [410,133009] is repeated
2020-07-03 09:42:08.050278: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[31196] = [646,1269494] is repeated
2020-07-03 09:42:08.050705: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[3872] = [159,1328680] is repeated
2020-07-03 09:42:08.052225: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at serialize_sparse_op.cc:382 : Invalid argument: indices[1722] = [19,709261] is repeated
		</comment>
		<comment id='9' author='yuandaxing' date='2020-07-07T08:38:54Z'>
		&lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;
 I have found that this error disappears when there is no multi-value features, and once a multi-value feature is added, the error occurs. I don't know why this happens, hope you can help analyze this problem.
		</comment>
		<comment id='10' author='yuandaxing' date='2020-07-07T15:08:32Z'>
		Thanks for helping to debug. Can you give an example of one of these multi-value features? I was able to reproduce the error with the data you provided, so if you can point me to the features in the p0 dataset that you're referring to that would be helpful.
		</comment>
		<comment id='11' author='yuandaxing' date='2020-07-08T02:29:44Z'>
		&lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;
 I create a new file named multiValueFeatures and put it under the shared directory.
		</comment>
		<comment id='12' author='yuandaxing' date='2020-07-08T17:48:54Z'>
		I am running into a "corrupted record" error. Did you change any of your tf record parsing code? Can you also just clarify what you mean by multi value features?
		</comment>
		<comment id='13' author='yuandaxing' date='2020-07-09T03:07:30Z'>
		&lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;

I didn't change the code, can you provide some details about "corrupted record" error?
Multi value features are actually multi-valued categorical features, which is categorical, and may have multiple values in one training sample.
For example, if feature A is a multi-valued categorical feature, and it has three values: v1, v2, v3, then A=v1 and A=v2 can both exist in one training sample.
		</comment>
		<comment id='14' author='yuandaxing' date='2020-07-14T06:13:32Z'>
		&lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/yuandaxing&gt;@yuandaxing&lt;/denchmark-link&gt;

I discovered that there were duplicate feature values in some multi-valued categorical features, and after I remove those duplicates, the error disappears.
Thanks a lot for help debug the problem, and I think that this issue can closed now.
		</comment>
		<comment id='15' author='yuandaxing' date='2020-07-14T06:21:44Z'>
		great job!!!
		</comment>
		<comment id='16' author='yuandaxing' date='2020-07-14T06:21:46Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40973&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40973&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>