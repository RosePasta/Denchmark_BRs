<bug id='31241' author='dawsoneliasen' open_date='2019-08-01T17:34:59Z' closed_time='2019-08-06T16:49:01Z'>
	<summary>Error when changing transformer hyperparameters</summary>
	<description>
&lt;denchmark-h:h2&gt;URL(s) with the issue:&lt;/denchmark-h&gt;

&lt;denchmark-link:https://www.tensorflow.org/beta/tutorials/text/transformer#set_hyperparameters&gt;https://www.tensorflow.org/beta/tutorials/text/transformer#set_hyperparameters&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Description of issue (what needs changing):&lt;/denchmark-h&gt;

I've been working with the 'Transformer model for language understanding' notebook on my own dataset. I got it to work with the default hyperparameters. The tutorial explains that I can create a Transformer XL by adjusting the hyperparameters to those that are used in the paper. I changed them to the suggested values, and I am now getting a ValueError when I try to train.
&lt;denchmark-code&gt;ValueError: Tensor's shape (8220, 128) is not compatible with supplied shape (8220, 512)
&lt;/denchmark-code&gt;

I think that this means that some object is not configured properly (not using the hyper parameter variables), but I can't figure out where it's happening. I tried restarting the runtime and running everything again, but it didn't help.
&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I have made small adjustments to the provided code in the transformer notebook to accommodate my own data. I also changed the values of the hyper parameters of the model.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  I am running the notebook in a Colab GPU runtime; Linux Ubuntu 18.04.2
TensorFlow installed from (source or binary): I'm not exactly sure, I install it using this command, provided with the notebook pip install -q tensorflow-gpu==2.0.0-beta1
TensorFlow version (use command below): tensorflow-gpu==2.0.0-beta1
Python version: 3.6.8
CUDA version: CUDA: 10.0.130
GPU model and memory: I'm not sure how to get this info, but it's a Colab GPU runtime.

&lt;denchmark-h:h2&gt;Code snippets&lt;/denchmark-h&gt;

I changed the output encoder from a SubwordTextEncoder to a TokenTextEncoder:
&lt;denchmark-code&gt;tokenizer_out = tfds.features.text.TokenTextEncoder(
    unique_concepts
)
&lt;/denchmark-code&gt;

I changed the tf_encode function to use a single argument instead of two:
&lt;denchmark-code&gt;def tf_encode(element):
    return tf.py_function(encode, [element[0], element[1]], [tf.int64, tf.int64])
&lt;/denchmark-code&gt;

And I changed the hyperparameter values:
&lt;denchmark-code&gt;num_layers = 6
d_model = 512
dff = 2048
num_heads = 8
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='dawsoneliasen' date='2019-08-02T09:00:17Z'>
		Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?
Make sure you also include the code snippet to reproduce the issue. If you are unclear what to include see the issue template displayed in the Github new issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/new/choose&gt;template&lt;/denchmark-link&gt;
.
We ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!
		</comment>
		<comment id='2' author='dawsoneliasen' date='2019-08-05T15:12:43Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
 Thanks for the response, and sorry for the delay. I updated the issue body with this info. Sorry for not including it originally - I used the documentation issue submission template, which does not say anything about system information.
		</comment>
		<comment id='3' author='dawsoneliasen' date='2019-08-06T11:15:35Z'>
		&lt;denchmark-link:https://github.com/dawsoneliasen&gt;@dawsoneliasen&lt;/denchmark-link&gt;

I tried to reproduce the issue in the doc link &lt;denchmark-link:https://www.tensorflow.org/beta/tutorials/text/transformer#set_hyperparameters&gt;https://www.tensorflow.org/beta/tutorials/text/transformer#set_hyperparameters&lt;/denchmark-link&gt;

by changing SubwordTextEncoder to a TokenTextEncoder:I am getting the below error .Please, help me with the reproducible code. Thanks!
		</comment>
		<comment id='4' author='dawsoneliasen' date='2019-08-06T14:37:43Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;

The  takes a list of tokens to build the vocabulary, instead of building from a corpus. As you can see in the snippet above, I pass a list of tokens to the constructor. You can replicate this by passing any list of unique strings.
		</comment>
		<comment id='5' author='dawsoneliasen' date='2019-08-06T16:49:01Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;

I just restarted and ran again to try to take another stab at identifying the problem, and it worked. I'm not sure what changed but I'm going to close the issue.
		</comment>
		<comment id='6' author='dawsoneliasen' date='2019-08-07T16:46:28Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;

In case you're interested, or someone else runs into this problem:
The issue is caused by the checkpoints that are created when training the model. If you train a model with the default parameters, and then change to the Transformer XL parameters and try to train again, you'll get an error because TF wants to load the checkpoint of the old model.
To resolve, you have to remove the checkpoints before training the Transformer XL. Disconnecting from the runtime doesn't help.
&lt;denchmark-code&gt;!rm -r ./checkpoints
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>