<bug id='13530' author='mellvinbaker' open_date='2017-10-06T16:54:00Z' closed_time='2019-06-11T13:09:58Z'>
	<summary>Pandas_input_fn slow, starving CPU/GPU</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): It is a customized version of the Deep &amp; Wide example code. Fairly close to original code.
**OS Platform and Distribution: Windows Server 2012 R2
**TensorFlow installed from: nightly build WHL through pip (this was tried after numerous other versions, including install through pip)
TensorFlow version (use command below): b'unknown' 1.4.0-dev20170926
Python version: 3.5 and 3.6
Bazel version (if compiling from source): Not compiling
CUDA/cuDNN version: CUDA 8, CUDnn 6.1
GPU model and memory: Tesla M60 GPU 8GB
Exact command to reproduce:  See attached Script.
-For the record, the server vm has 8 xeon physical cores and 240 gb ram allocated. The CPU only machine is a new skylake i7 with 32gb ram.

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

To start, I submitted to stack overflow (&lt;denchmark-link:https://stackoverflow.com/questions/46457476/tensorflow-pandas-input-fn-slow-starving-cpu-gpu&gt;https://stackoverflow.com/questions/46457476/tensorflow-pandas-input-fn-slow-starving-cpu-gpu&lt;/denchmark-link&gt;
) and have not been able to garner assistance after multiple edits to make sure it was framed correctly. I truly believe this is a bug since I am sticking so close to the example code, but if I have made a mistake I am deeply sorry to all of you.
I am working on a wide and deep model following the framework in the Tensorflow Wide and Deep tutorial (&lt;denchmark-link:https://www.tensorflow.org/tutorials/wide_and_deep&gt;https://www.tensorflow.org/tutorials/wide_and_deep&lt;/denchmark-link&gt;
). Model works fine when built the old way (load entire dataset from pandas, convert to tensors, feed in input_fn) which is ok for running on a CPU.
However, to make it work on the GPU the dataset is too large to fit into GPU memory, so batching is necessary. I tried using the pandas_input_fn to batch data to the video card and noticed I get spikes of activity followed by long lulls while the next batch is prepared. The odd thing is, this happens even if I run it on a machine with CPU only. The lulls are almost the exact same length, so it isn't simply the video card crushing through a simple model faster than the proc can deliver it. It seems like it is always waiting to begin loading the next batch until the last one is done training.
(If this function simply cannot be used in this way, can we get an example of Deep and Wide using the dataset API? or a manual build of deep and wide using layers and queues? At the moment, the example code for the dataset api using make_one_shot_iterator for canned estimators doesn't run.)
I increased the complexity of the model to make sure it wasn't too easy to compute and still have the same issue. I have tried increasing the number of threads allocated to pandas_input_fn, I have tried increasing the queue size to far larger than seems reasonable (10x dataset size) which helps a bit, but not much. I am not sure if the slowdown is when it is queueing or de-queueing, but I have been unable to solve the issue after two weeks of troubleshooting. The data I am working with is 117 columns, 400k rows.
I have created a generic script that generates fake values to simulate the problem. However, there are far fewer fake columns than real ones, so the gap between steps is not nearly as long, but still noticeable. Code attached.
--
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

attached
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/1363335/pandas_input_example.txt&gt;pandas_input_example.txt&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='mellvinbaker' date='2017-10-07T01:01:43Z'>
		&lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
, could you make some suggestions?
		</comment>
		<comment id='2' author='mellvinbaker' date='2017-10-12T19:30:32Z'>
		I too have been dealing with extremely low (&lt;10%) GPU usage when using pandas_input_fn with a DNNRegressor and Titan Xp
		</comment>
		<comment id='3' author='mellvinbaker' date='2017-10-16T16:40:29Z'>
		I have not addressed usage itself yet because I assumed it was due to the balance of loading data. I couldn't dial in that balance with the GPU taking such long pauses. That said, my GPU usage was also extremely low, even when I seemed to be filling memory up (growth set to true, etc.) I never got above 12%. Still, I am much more worried about the batching speed issue. This bug, if it is indeed a bug, means we are still stuck loading tabular data via tfrecord/protobuf if we want decent performance. Its a problem because its more than 90% of our ML use cases (image data is fun, but a huge portion of ML is just strait up data). That said, I'm not complaining overall, Tensorflow is an amazing achievement and it gets better every day. Thanks to all the devs volunteering on this project.
		</comment>
		<comment id='4' author='mellvinbaker' date='2017-10-16T17:25:22Z'>
		&lt;denchmark-link:https://github.com/mellvinbaker&gt;@mellvinbaker&lt;/denchmark-link&gt;
 Looking at your example code, I noticed that the  is 200000 (i.e. the full size of the ). Is that what you'd typically use in your application?
Given that the pandas_input_fn() is based on a FeedingQueueRunner, it would have to feed 200000 elements to build such a batch. At ~20us per feed, I'm not surprised that it stalls for a noticeable amount of time.
		</comment>
		<comment id='5' author='mellvinbaker' date='2017-10-16T17:43:00Z'>
		&lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 The dataframe is 400,000 rows, not 200,000 and those 200,000 rows make for about ~.75gb of data (that is with my own dataset, with this generated dataset its closer to .3gb). I am not sure how that is any different than a .3GB array of image values. I would not think that batching .3gb of numbers to a GPU would be outside the scope of pandas_input_fn, but maybe I misunderstand its purpose (or its use, both are very possible). When would you ever need to batch a dataset of say 50mb to a tesla M60? The whole thing would fit in memory, thus avoiding the need for batching at all.
The typical use would be to pull about 2,000,000 rows from SQL, batch them into the most efficient size for the GPU (without compromising training), and run the model. At the moment, I can run the entire 2,000,000 through an estimator if I use CPU only because I have ~128gb of ram on the server. That runs in half the time of batching that same 2,000,000 rows to the GPU, regardless of batch size.
I have tried batch sizes ranging from 20k to 200k and all have the same problem. When one batch is done, the GPU sits idle waiting for more. The smaller the batch though, the lower the GPU utilization, thus causing slowdowns on the other end.
edit: This sounded more argumentative than I meant it too. Rest assured I am trying to be complete and concise, not snarky.
		</comment>
		<comment id='6' author='mellvinbaker' date='2017-10-25T16:28:06Z'>
		FYI, I have rebuilt my input functions to see if TFRecord would work better. While it does do a better job of keeping the processor fed without lag (still a little between steps, but I haven't even optimized, still doing 1 file 1 thread etc.) it isn't a solution for this type of data. It makes sense for pictures, which change very little. But if you are working with a dataset that needs feature engineering it can be a bit iterative.
With 400k rows taking ~8 hours to serialize to TFRecord, we entirely lose any advantage gained by using GPU.
Thanks again guys for looking into issues like this and for all the hard work.
		</comment>
		<comment id='7' author='mellvinbaker' date='2017-10-31T12:53:43Z'>
		Checking in to say that i encountered the same problem.
2kk objects with 300 features, feeded to wideanddeep estimator through TFRecordDataset, cpu is overloaded at 800%(8 cores) while titan gpu is at 10% utilisation. Tried on various batch sizes from 64 to 20k.
Is there something i am doing wrong or is there any updates about this issue?
		</comment>
		<comment id='8' author='mellvinbaker' date='2017-12-05T17:32:21Z'>
		Reassigning this to &lt;denchmark-link:https://github.com/rohan100jain&gt;@rohan100jain&lt;/denchmark-link&gt;
, who has been working on a GPU prefetching optimization for  that should fix the utilization problem.
		</comment>
		<comment id='9' author='mellvinbaker' date='2018-01-21T03:39:58Z'>
		had the same issue, really want to know how to improve the performance.
		</comment>
		<comment id='10' author='mellvinbaker' date='2018-03-07T17:48:55Z'>
		Just wanted to ping here and mention that I'm indeed working on a GPU prefetching optimization that should help parallelize loading of batches much more and hence improve performance. The work is nearly done - I'm just testing and making sure it functions correctly before publishing some instructions as to how to use it. Please stay tuned for a week or so as I solidify this more. Thanks!
		</comment>
		<comment id='11' author='mellvinbaker' date='2018-07-05T05:19:45Z'>
		&lt;denchmark-link:https://github.com/rohan100jain&gt;@rohan100jain&lt;/denchmark-link&gt;
 any update for this performance issue? From the thread above, I assume there is already a solution, wonder when will it be published? Thanks.
		</comment>
		<comment id='12' author='mellvinbaker' date='2018-09-26T19:14:59Z'>
		It is not only pandas, direct reading from csv (TextLineDataset), DNNRegressor still underutilising GPU.
		</comment>
		<comment id='13' author='mellvinbaker' date='2018-10-21T18:58:18Z'>
		Please try out tf.data.experimental.copy_to_device("/gpu:0").prefetch(1) to get prefetching onto the GPU.
		</comment>
		<comment id='14' author='mellvinbaker' date='2018-11-05T19:07:30Z'>
		Sorry, I just saw the response. Where would I put this, is it a runconfig parameter, part of pandas_input_fn, a decorator at the top of the script? Or does this only apply to text_line_dataset?
		</comment>
		<comment id='15' author='mellvinbaker' date='2018-11-14T21:58:40Z'>
		
Sorry, I just saw the response. Where would I put this, is it a runconfig parameter, part of pandas_input_fn, a decorator at the top of the script? Or does this only apply to text_line_dataset?

&lt;denchmark-link:https://github.com/rohan100jain&gt;@rohan100jain&lt;/denchmark-link&gt;
  Could you please answer this ?
		</comment>
		<comment id='16' author='mellvinbaker' date='2019-01-17T02:52:23Z'>
		I have the same problem, the utilization of gpu is too lowï¼Œreally want to know how to improve the performance.
		</comment>
		<comment id='17' author='mellvinbaker' date='2019-02-15T22:49:01Z'>
		Having same issue with CSV input function using TextLineDataset (w/ decodeCsv call). Read speeds are extremely low. I am seeing 2MB/s max even on SSD drive
		</comment>
		<comment id='18' author='mellvinbaker' date='2019-02-20T16:42:23Z'>
		The prefetching currently works only for tf.data, so you'll have to replace your pandas_input_fn with a dataset. I believe you can use SQLDAtaset (&lt;denchmark-link:https://www.tensorflow.org/versions/r1.12/api_docs/python/tf/data/experimental/SqlDataset?hl=en&gt;https://www.tensorflow.org/versions/r1.12/api_docs/python/tf/data/experimental/SqlDataset?hl=en&lt;/denchmark-link&gt;
) to construct one of these.
Once you have a dataset (lets call it sql_ds) with the information, you can do the following
def input_fn():
sql_ds = &lt;create_sql_dataset&gt;
ds = sql_ds.apply(tf.data.experimental.copy_to_device("/gpu:0")).prefetch(1)
return ds
Estimators allow returning datatsets in their input fn and things should flow from there.
For the CSV input fn and other cases, just use different input dataset (CSVDataset for example: &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/data/experimental/CsvDataset?hl=en&gt;https://www.tensorflow.org/api_docs/python/tf/data/experimental/CsvDataset?hl=en&lt;/denchmark-link&gt;
)
		</comment>
		<comment id='19' author='mellvinbaker' date='2019-03-28T17:50:15Z'>
		Same problem with pandas_input_fn. Less than 10% GPU utilization on my brand new RTX 2080 TI... I spent all that money and I can process just as fast on a multi-core AMD CPU??? I don't get it. How are the "big boys" using these cards to get speedup?
		</comment>
		<comment id='20' author='mellvinbaker' date='2019-04-12T13:49:03Z'>
		
It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue?

Well, I've been focusing on learning 2.0 so I haven't focused on this - but still not getting over 15% GPU utilization (with very short bursts to 25%) no matter what I do.  Is there any example I can download to demonstrate full GPU utilization? That would be really helpful.
		</comment>
		<comment id='21' author='mellvinbaker' date='2019-06-11T13:09:59Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=13530&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=13530&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='22' author='mellvinbaker' date='2019-06-11T13:10:28Z'>
		We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!
		</comment>
	</comments>
</bug>