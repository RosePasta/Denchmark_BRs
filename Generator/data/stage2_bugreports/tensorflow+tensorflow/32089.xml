<bug id='32089' author='gaborchris' open_date='2019-08-29T18:36:20Z' closed_time='2019-11-11T20:07:18Z'>
	<summary>TF2.0 RC tf.keras.model.Evaluate has display bug</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No. This is the code from https://www.tensorflow.org/beta/tutorials/quickstart/beginner
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab and Ubuntu 18.04
TensorFlow version (use command below): pip install tensorflow==2.0.0-rc0
Python version: 3.6

Describe the current behavior
When I run model.evaluate with verbose=True, I get too many "=======" signs. I noticed this on Ubuntu 18.04 when I upgraded from TF2.0-beta to TF2.0.0-rc0. This is a mild annoyance when I am running models with soft-warp enabled on the command line.
Describe the expected behavior
There should only be this many equal signs:
[==============================]
Not
[===========================............................................
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
&lt;denchmark-code&gt;
# Install TensorFlow
try:
  # %tensorflow_version only exists in Colab.
  %tensorflow_version 2.x
except Exception:
  pass

import tensorflow as tf

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.fit(x_train, y_train, epochs=1)

model.evaluate(x_test, y_test)
&lt;/denchmark-code&gt;

Other info / logs
This issue may be due to the denominator of the number of evaluated samples completed. It should be 10000/10000, not 10000/1.
Instead of 10000/1 [====
	</description>
	<comments>
		<comment id='1' author='gaborchris' date='2019-08-29T22:14:40Z'>
		Was able to reproduce the issue. Please find the github gist &lt;denchmark-link:https://colab.research.google.com/gist/gowthamkpr/a34035feed7b076e62a8fd51467394a9/untitled116.ipynb&gt;here&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='gaborchris' date='2019-11-11T20:07:18Z'>
		&lt;denchmark-link:https://github.com/gaborchris&gt;@gaborchris&lt;/denchmark-link&gt;
 This was resolved in . Please take a look at the &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/9cf040a428c59f8efbd1e17f70d51a34/untitled116.ipynb&gt;gist&lt;/denchmark-link&gt;
. Thanks!
I am closing this issue as this was resolved. Please feel free to reopen if the issue persists for you. Thanks!
		</comment>
		<comment id='3' author='gaborchris' date='2019-11-11T20:07:20Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32089&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32089&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>