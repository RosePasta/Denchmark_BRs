<bug id='28831' author='MahdiNicoo' open_date='2019-05-18T20:21:28Z' closed_time='2019-07-07T06:39:02Z'>
	<summary>tf-nightly-gpu 2.0 very slow on tape.gradient()</summary>
	<description>
While migrating to tensorflow 2.0 I found a big performance issue.
&lt;denchmark-code&gt;@tf.function
def onTrainStep(self, data, training=True):

    images, labels = data

    with tf.GradientTape() as tape:
        loss, predictions = self.seq2seq(images, labels, training)

    # Calculate the total probability of the output string.
    probability = tf.nn.softmax(predictions)

    ggn = 0
    if training:
        params = self.seq2seq.trainable_variables

        gradients = tape.gradient(loss, params)
        gradients, _ = tf.clip_by_global_norm(gradients, 5.0)
        ggn = tf.linalg.global_norm(gradients)

        # update op - apply gradients
        self.optimizer.apply_gradients(zip(gradients, params))

    return loss, predictions, probability, ggn

&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;tensorflow-gpu 1.13.1:&lt;/denchmark-h&gt;

takes 0.7 seconds
&lt;denchmark-h:h3&gt;tensorflow-gpu 2.0 nightly&lt;/denchmark-h&gt;

takes 2.4 seconds
&lt;denchmark-h:h3&gt;removing gradients = tape.gradient(loss, params) and ggn = tf.linalg.global_norm(gradients):&lt;/denchmark-h&gt;

takes 0.3 seconds
That means gradients = tape.gradient(loss, params) is consuming most of the time
	</description>
	<comments>
		<comment id='1' author='MahdiNicoo' date='2019-05-21T02:16:56Z'>
		I also noticed that tensorflow2.0a is  much faster than the latest nightly build with keras when updating the weights.
		</comment>
		<comment id='2' author='MahdiNicoo' date='2019-05-21T20:30:58Z'>
		Can you add full instructions to reproduce?
This feels like a serious problem but it could be caused by many different things, and so having working code we can debug would be very helpful.
		</comment>
		<comment id='3' author='MahdiNicoo' date='2019-05-21T20:31:42Z'>
		Or if you don't want to share your code if you're willing to run a git bisect and share the results with us that'd also be great.
		</comment>
		<comment id='4' author='MahdiNicoo' date='2019-05-21T23:33:09Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
, my full code is in this notebook:
&lt;denchmark-link:https://github.com/alew3/huia_experience/blob/master/02_train/train_huia_poses_keras_tf2.ipynb&gt;https://github.com/alew3/huia_experience/blob/master/02_train/train_huia_poses_keras_tf2.ipynb&lt;/denchmark-link&gt;

I timed this snippet of training as an example, it is running 3 times slower in the nightly. Visually the slow down seems to happen when it starts updating the weights.
start = datetime.now()
model.fit(train_dataset, epochs=epochs,steps_per_epoch=steps_per_epoch,verbose=1,validation_data=test_dataset,
callbacks=callbacks)
end = datetime.now()
print(f"total time of {end-start} for {epochs} epochs, tensorflow version={tf.version}")
and run twice the code for each version of tensorflow:
TF2 Alpha
total time of 0:01:00.676783 for 5 epochs, tensorflow version=2.0.0-alpha0
total time of 0:01:02.069855 for 5 epochs, tensorflow version=2.0.0-alpha0
TF2 Nightly
total time of 0:03:20.682476 for 5 epochs, tensorflow version=2.0.0-dev20190521
total time of 0:03:21.514782 for 5 epochs, tensorflow version=2.0.0-dev20190521
		</comment>
		<comment id='5' author='MahdiNicoo' date='2019-05-22T17:17:06Z'>
		Can you give me a shorter example to reproduce? I can't quite run your notebook as I don't have the training data or the filesystem setup in the way you do.
		</comment>
		<comment id='6' author='MahdiNicoo' date='2019-05-22T19:49:38Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 , I've cut down the code and included the training data in two different collabs to make things easier to reproduce, with the only change being the version of Tensorflow 2 used. Tensorflow Nightly is 3x slower.

&lt;denchmark-link:https://colab.research.google.com/drive/1AOhKl18zI-W1YZ2BPTHO_e0zezUgw6Jh#scrollTo=OaNzmAVbosq0&gt;https://colab.research.google.com/drive/1AOhKl18zI-W1YZ2BPTHO_e0zezUgw6Jh#scrollTo=OaNzmAVbosq0&lt;/denchmark-link&gt;

run 1. total time of  for 5 epochs, tensorflow version=2.0.0-alpha0
run 2. total time of  for 5 epochs, tensorflow version=2.0.0-alpha0

&lt;denchmark-link:https://colab.research.google.com/drive/1Z9M_ovwlUjHUiswW3a29aS67CsaOHjWo#scrollTo=3cVLAo9Mospi&gt;https://colab.research.google.com/drive/1Z9M_ovwlUjHUiswW3a29aS67CsaOHjWo#scrollTo=3cVLAo9Mospi&lt;/denchmark-link&gt;

run 1. total time of  for 5 epochs, tensorflow version=2.0.0-dev20190522
		</comment>
		<comment id='7' author='MahdiNicoo' date='2019-05-22T23:47:29Z'>
		Enabling device logging during model.fit(), I see that it is mostly running on the GPU. But I'm not sure what Executing op ExperimentalDatasetCardinality in device /job:localhost/replica:0/task:0/device:CPU:0 or
Executing op __inference_keras_scratch_graph_264441 in device unspecified do, but they happen when things start to slow down.
Executing op CloseSummaryWriter in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0
Epoch 1/20
Executing op ExpandDims in device /job:localhost/replica:0/task:0/device:GPU:0
W0522 20:40:51.144862 140192919144256 deprecation.py:323] From /home/ale/anaconda3/envs/tensor20/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support..wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op __inference_keras_scratch_graph_13159 in device 
68/70 [============================&gt;.] - ETA: 0s - loss: 0.8524 - accuracy: 0.7154Executing op ExperimentalDatasetCardinality in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op __inference_keras_scratch_graph_14812 in device unspecified
Executing op WriteScalarSummary in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op WriteScalarSummary in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op WriteHistogramSummary in device /job:localhost/replica:0/task:0/device:CPU:0
70/70 [==============================] - 68s 973ms/step - loss: 0.8384 - accuracy: 0.7196 - val_loss: 1.9080 - val_accuracy: 0.3720
		</comment>
		<comment id='8' author='MahdiNicoo' date='2019-05-23T16:09:29Z'>
		&lt;denchmark-link:https://github.com/iganichev&gt;@iganichev&lt;/denchmark-link&gt;
 could this be related to placer changes?
		</comment>
		<comment id='9' author='MahdiNicoo' date='2019-05-23T19:38:49Z'>
		Unlikely. The new placer should have the same placement decisions (safe for some rare corner cases) for graphs without functions outputting resources on multiple devices. It is unlikely that keras produces such graphs. Also, Dataset ops have not been integrated into the recursive placement framework.
		</comment>
		<comment id='10' author='MahdiNicoo' date='2019-06-08T13:12:09Z'>
		
It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue?

The issue persists. Very slow updating gradients on the nightly and beta compared to the alpha.
		</comment>
		<comment id='11' author='MahdiNicoo' date='2019-06-08T22:32:48Z'>
		The issue persists.
		</comment>
		<comment id='12' author='MahdiNicoo' date='2019-06-20T20:11:05Z'>
		&lt;denchmark-link:https://github.com/robieta&gt;@robieta&lt;/denchmark-link&gt;
 , the notebook which is slow seems to be using model.fit. Do you know what part of it changed between alpha and nightly that could cause this?
&lt;denchmark-link:https://github.com/alew3&gt;@alew3&lt;/denchmark-link&gt;
 this is still a very large example to reproduce, and the notebook you shared doesn't overlap at all in terms of code with the thing starting this post. It's quite hard to understand what the actual problem is.
		</comment>
		<comment id='13' author='MahdiNicoo' date='2019-06-20T20:59:23Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 sorry, I tried to cut down the code to the essential. It is just a simple image classification training via transfer learning using mobilenet with keras and tf.data.dataset as recommended by Tensorflow guides. But the slow down is very noticeable on the newer versions of TF2 when it updates the weights. It is so slow, that makes it  impracticable for real work. I can't guarantee it is the same reason as the original poster's problem.
		</comment>
		<comment id='14' author='MahdiNicoo' date='2019-06-20T22:32:50Z'>
		For example, can you reproduce the slowdown with just a constant size numpy array? Can you reproduce with a smaller keras model?
If you can reproduce with a numpy array with random numbers instead of a real dataset this gets much easier for me to reproduce. Similarly, the smallest the model that is slow the easier it is for me to know what is going on.
		</comment>
		<comment id='15' author='MahdiNicoo' date='2019-06-21T06:07:19Z'>
		I would echo &lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
's point about synthetic data. It's often a source of much complexity in a model, and if an issue reproduces with a synthetic pipeline it drastically narrows down the possible causes. (And a similar line of reasoning goes for models. Complex topologies are hard to debug.) Other than that, however, I thought it was a very high quality repro and much appreciated. I've tweaked it to use a synthetic pipeline: &lt;denchmark-link:https://colab.research.google.com/gist/robieta/0c8a007cf1ec2181872511b1367809e7/tf_issue_28831_repro.ipynb&gt;https://colab.research.google.com/gist/robieta/0c8a007cf1ec2181872511b1367809e7/tf_issue_28831_repro.ipynb&lt;/denchmark-link&gt;

The issue is this bit here:
&lt;denchmark-code&gt;# skip 20% validation images
test_dataset = ds.take(838) 
train_dataset = ds.skip(838)
&lt;/denchmark-code&gt;

There are 4191 examples, hence 20% * 4191 = 838. However at this time the dataset is already batched, so at a batch size of 48 that actually becomes 838 batches = 48 * 838 examples = 9.6 epochs. So you are actually telling the dataset to process nearly 10 epochs before starting validation. (The results of skip are not used by test_dataset; they are simply discarded) This of course also has the unfortunate effect that it does not preserve the train / test split. Generally I would recommend making the split in the dataset as early as possible, as it tends to be both more performant and less error prone.
The reason that this is showing up now is that there have been some changes to how Model.fit interacts with datasets, so the dataset is making a new iterator each time. (As opposed to the alpha code where the inefficiency was only in the first epoch.) &lt;denchmark-link:https://github.com/tomerk&gt;@tomerk&lt;/denchmark-link&gt;
 Is iterator reuse across epochs via the  argument on your radar?
That said, I think the original issue reported by @nicoosokhan seems to be different from the behavior observed by &lt;denchmark-link:https://github.com/alew3&gt;@alew3&lt;/denchmark-link&gt;
. @nicoosokhan if you can provide a minimal repro colab we would be very interested to see what's going on with GradientTape.
		</comment>
		<comment id='16' author='MahdiNicoo' date='2019-06-21T18:14:44Z'>
		&lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
 For the  iterator reuse behavior as well.
		</comment>
		<comment id='17' author='MahdiNicoo' date='2019-06-21T22:21:05Z'>
		&lt;denchmark-link:https://github.com/robieta&gt;@robieta&lt;/denchmark-link&gt;
 thanks for the feedback, I also tried running my code on multiple gpus with mirror distribution strategy and was running out of data, so that's probably related!
		</comment>
		<comment id='18' author='MahdiNicoo' date='2019-07-07T06:39:02Z'>
		I'm going to go ahead and close this out since it seems to be resolved. Feel free to reopen if I missed something.
		</comment>
		<comment id='19' author='MahdiNicoo' date='2019-07-07T06:39:03Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=28831&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=28831&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>