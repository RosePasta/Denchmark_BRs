<bug id='35682' author='bersbersbers' open_date='2020-01-08T22:48:42Z' closed_time='2020-01-24T23:18:19Z'>
	<summary>tf.data.Dataset.map repeats random numbers in each epoch  of a Keras training loop when a graph-level seed is set</summary>
	<description>
Thanks for staying with me after this long title. There are many issues on non-determinism of Dataset.map - this one is contrary. With random data augmentation, I would expect Dataset.map to behave differently between epochs, but it does not if a graph-level seed is set.
System information

Have I written custom code: yes, below.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 &amp; Linux
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): 2.1.0rc2, also 2.0.0 and tf-nightly
Python version: 3.7.6
CUDA/cuDNN version: 10.1/7.x
GPU model and memory: various

Describe the current behavior
The code below outputs the same random numbers in each epoch:
&lt;denchmark-code&gt;Train for 3 steps
Epoch 1/3
WARNING:tensorflow:The list of trainable weights is empty. Make sure that you are not setting model.trainable to False before compiling the model.
0.926393032
0.0866344
0.783794165
3/3 [==============================] - 1s 186ms/step - loss: 0.0000e+00
Epoch 2/3
0.926393032
0.0866344
0.783794165
3/3 [==============================] - 0s 7ms/step - loss: 0.0000e+00
Epoch 3/3
0.926393032
0.0866344
0.783794165
3/3 [==============================] - 0s 6ms/step - loss: 0.0000e+00

&lt;/denchmark-code&gt;

Describe the expected behavior
It should output different random numbers in each epoch.
Code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf

tf.random.set_seed(0)


def do_something_random(*args):
    """It does the same things in each epoch. That's not random!"""
    tf.print(tf.random.uniform((), 0, 1))
    return args


data = [[1], [2], [3]]
data = tf.data.Dataset.from_tensor_slices((data, data)).batch(1)
data = data.map(do_something_random)

layer = tf.keras.layers.Input(shape=(1,))
model = tf.keras.models.Model(inputs=layer, outputs=layer)
model.compile(optimizer="adam", loss="mse")
model.fit(x=data, epochs=3)
&lt;/denchmark-code&gt;

Other info / logs
Initially, I thought, well, the Dataset.map command is maybe compiled once and run identically in each epoch. But

I am using tf.random.uniform explicitly,
the problem disappears when commenting out tf.random.set_seed(0), implying that the Dataset.map command is able to yield different results in each epoch:

&lt;denchmark-code&gt;Train for 3 steps
Epoch 1/3
WARNING:tensorflow:The list of trainable weights is empty. Make sure that you are not setting model.trainable to False before compiling the model.
0.632945657
0.70087862
0.662360072
3/3 [==============================] - 1s 186ms/step - loss: 0.0000e+00
Epoch 2/3
0.536124825
0.0930280685
0.26403141
3/3 [==============================] - 0s 7ms/step - loss: 0.0000e+00
Epoch 3/3
0.323968768
0.376766324
0.693181396
3/3 [==============================] - 0s 6ms/step - loss: 0.0000e+00
&lt;/denchmark-code&gt;

The issue is why the graph-level seed has an influence on this effect. Are the Dataset.map calls in each epoch maybe run in separate graphs, each of which receives the same graph-level seed?
	</description>
	<comments>
		<comment id='1' author='bersbersbers' date='2020-01-09T10:13:53Z'>
		Issue is replicating on colab with tf 2.1.
Please find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/f5f0f14eae16deceb24d70aa08a6e2ec/untitled339.ipynb?authuser=1&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='2' author='bersbersbers' date='2020-01-10T14:05:18Z'>
		Even worse: tf.data.Dataset.shuffle suffers from a similar problem with disable_eager_execution(): shuffles are identical from one epoch to the next.
(This is based on my assumption that Dataset.shuffle should reshuffle between epochs. If that is not the case, then the issue is why it does reshuffle when eager execution is on.)
&lt;denchmark-code&gt;import tensorflow as tf

tf.compat.v1.disable_eager_execution()
tf.random.set_seed(0)


def show_args(*args):
    """It's the same order of samples in each epoch. That's not shuffling!"""
    tf.print(args)
    return args


data = [[1], [2], [3], [4], [5]]
data = tf.data.Dataset.from_tensor_slices((data, data)).batch(1)
data = data.shuffle(buffer_size=1024).map(show_args)

layer = tf.keras.layers.Input(shape=(1,))
model = tf.keras.models.Model(inputs=layer, outputs=layer)
model.compile(optimizer="adam", loss="mse")
model.fit(x=data, epochs=3, verbose=0)
&lt;/denchmark-code&gt;

outputs
&lt;denchmark-code&gt;[[5]] [[5]]
[[1]] [[1]]
[[2]] [[2]]
[[4]] [[4]]
[[3]] [[3]]
[[5]] [[5]]
[[1]] [[1]]
[[2]] [[2]]
[[4]] [[4]]
[[3]] [[3]]
[[5]] [[5]]
[[1]] [[1]]
[[2]] [[2]]
[[4]] [[4]]
[[3]] [[3]]
&lt;/denchmark-code&gt;

(note 5-1-2-4-3 in each epoch).
		</comment>
		<comment id='3' author='bersbersbers' date='2020-01-10T22:12:49Z'>
		The dataset.map() will execute one function on every element of the Dataset separately and returns one transformed element and also as you set the random seed, this is the reason why you see same random numbers in each epoch.
		</comment>
		<comment id='4' author='bersbersbers' date='2020-01-10T23:26:54Z'>
		&lt;denchmark-link:https://github.com/gowthamkpr&gt;@gowthamkpr&lt;/denchmark-link&gt;
 assuming that your explanation is correct, why do I see  random numbers in  in each epoch without a graph-level seed? And why are s  across epochs, even with a graph-level seed set?
Whatever the expected output is, it is inconsistent that the behavior of

dataset.map() is the same across epochs with a seed, but different without;
dataset.shuffle() is the same across epochs with disable_eager_execution, but different without.

		</comment>
		<comment id='5' author='bersbersbers' date='2020-01-10T23:36:20Z'>
		Also, &lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 explains  quite differently in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/22000#issuecomment-444679792&gt;#22000 (comment)&lt;/denchmark-link&gt;
:
&lt;denchmark-code&gt;dataset = tf.data.Dataset.range(100)
dataset = dataset.shuffle(buffer_size=100)
feature = dataset.map(lambda x: x)
label = dataset.map(lambda x: x)
&lt;/denchmark-code&gt;


that is not how tf.data works. The definition in your example will in fact create two independent instances of the range and shuffle datasets. Informally, you can think of the dataset construction as having "by value" semantics.

This makes the feature and label datasets shuffle() differently in the above example. I would expect this principle to hold for map() as well, and also for re-use of shuffle()d datasets in different epochs of a Keras training loop.
		</comment>
		<comment id='6' author='bersbersbers' date='2020-01-10T23:58:52Z'>
		&lt;denchmark-link:https://github.com/bersbersbers&gt;@bersbersbers&lt;/denchmark-link&gt;
 the TLDR is that legacy graph-mode (i.e. ) does not support controlling shuffling behavior across different iterations of the same dataset.
If you would like to control the shuffle order and run in graph-mode with TF2, you should rely on tf.function (as oppposed to the legacy graph-mode). The following snippet illustrates idiomatic mechanism for controlling shuffling behavior in TF 2:
&lt;denchmark-code&gt;from __future__ import print_function

import tensorflow.compat.v2 as tf

tf.enable_v2_behavior()

def eager_execution(reshuffle_each_iteration):
  ds = tf.data.Dataset.range(5).shuffle(5, reshuffle_each_iteration=reshuffle_each_iteration).batch(5)

  def print_dataset(dataset):
    for elem in dataset:
      tf.print(elem)

  print_dataset(ds)
  print_dataset(ds)

print("Eager mode with reshuffling:")
eager_execution(True)
print("Eager mode without reshuffling:")
eager_execution(False)

@tf.function
def graph_execution(reshuffle_each_iteration):
  return eager_execution(reshuffle_each_iteration)

print("Graph mode with reshuffling:")
graph_execution(True)

print("Graph mode without reshuffling:")
graph_execution(False)
&lt;/denchmark-code&gt;

produces:
&lt;denchmark-code&gt;Eager mode with reshuffling:
[4 1 0 2 3]
[2 1 3 4 0]
Eager mode without reshuffling:
[2 0 3 1 4]
[2 0 3 1 4]
Graph mode with reshuffling:
[2 3 4 0 1]
[2 4 1 3 0]
Graph mode without reshuffling:
[4 2 3 0 1]
[4 2 3 0 1]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='bersbersbers' date='2020-01-11T08:11:17Z'>
		Thanks for the explanation, &lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
. I must admit that I was not aware that  existed, but seeing that it defaults to  means that at least my intuition was correct. It is unfortunate that  breaks this control, but your confirmation is valuable and I'll try and see if I can make the workaround work with my example of the Keras training loop (which needs to run non-eagerly since I rely on the  session kwarg.) I'll open a separate issue about this if I encounter other issues [I finished this, confirming indeed  indeed works in my application in eager mode], so let's ignore  in this issue for now and re-focus on the behavior of  again.
So back the the main issue: Should I expect dataset.map() to run once in each Keras epoch (different random numbers), or only once (identical random numbers)?

If so, why are random numbers different across epochs without a seed?
If not, why are random numbers identical across epochs with a seed?

I must admit due to the order of shuffle().map(), since shuffling works, in each epoch, the same operations in map() are applied to different data, so in terms of maximum data augmentation it's not a disaster. Still, I am worried that at least some non-randomness each epoch may impact learning.
		</comment>
		<comment id='8' author='bersbersbers' date='2020-01-13T16:44:36Z'>
		
The dataset.map() will execute one function on every element of the Dataset separately and returns one transformed element and also as you set the random seed, this is the reason why you see same random numbers in each epoch.

Having thought about this for a while, I don't think this is a valid explanation. The fact that the map_fun is called separately for each element together with the random see would explain, if anything, why the random operation returned the same random numbers for each sample, not only for each epoch of samples. But this is not the case. It does not explain why some samples receive different random numbers until a new epoch starts.
Also, if caching would be involved, this would raise the question what the purpose of the Dataset.cache() function should be.
		</comment>
		<comment id='9' author='bersbersbers' date='2020-01-13T17:29:03Z'>
		I am not sure I understand your question regarding map because map does not reorder elements. It simply applies the user-defined transformation to each element. Are you talking about the situation when there is a randomized op inside of the user-defined function? The answer to how randomness behaves in that case depends on the implementation of the op and is orthogonal to map.
In other words, if your input pipeline does not contain shuffle and your map's user-defined function does not contain random ops, then your input pipeline will produce the same sequence every time it is executed.
		</comment>
		<comment id='10' author='bersbersbers' date='2020-01-13T20:18:39Z'>
		
I am not sure I understand your question regarding map because map does not reorder elements. [...] Are you talking about the situation when there is a randomized op inside of the user-defined function?

Yes, exactly!

The answer to how randomness behaves in that case depends on the implementation of the op and is orthogonal to map.

This is what I am interested in: my examples in the very first post DO contain calls to tf.random.uniform, which I would have expected will be re-applied with new random numbers every time I call the sequence. And this is the case in my example when disabling the graph-level seed; it is not the case when a graph-level seed is applied. So how can I make this "depend on the implementation" - what should I do differently if I want TF to behave deterministically (I need to set a graph-level seed) but want different epochs to behave differently?
		</comment>
		<comment id='11' author='bersbersbers' date='2020-01-14T00:00:33Z'>
		My recommendation would be to avoid reliance on graph-level / default seeds and switch to using tf.random.stateless_uniform which takes the seed argument so that you can control the sequence of seeds.
Here is a fully reproducible example that deterministically reshuffles between epochs:
&lt;denchmark-code&gt;from __future__ import print_function

import tensorflow.compat.v2 as tf

tf.enable_v2_behavior()

def map_fn(seed):
  return tf.random.stateless_uniform([], [seed, seed])

ds = tf.data.Dataset.range(5)
ds = ds.map(map_fn).shuffle(5, seed=42, reshuffle_each_iteration=True).batch(5)

for elem in ds:
  print(elem.numpy())
for elem in ds:
  print(elem.numpy())
&lt;/denchmark-code&gt;

The above (consistently) produces the following output:
&lt;denchmark-code&gt;[0.21101546 0.61040807 0.09827709 0.7736759  0.9589814 ]
[0.7736759  0.61040807 0.09827709 0.21101546 0.9589814 ]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='12' author='bersbersbers' date='2020-01-14T09:51:52Z'>
		&lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 sorry for introducing the side-issue of shuffling, consider this solved. Regarding mapping, I fear we're still talking about different aims.
So regarding mapping using a random function, you are proposing to skip the graph-level seed and use the element itself for seeding a stateless RNG, did I get this correct? That is, assuming a deterministic order of fixed input elements, this will yield a deterministic sequence of random numbers for each epoch - I get as much. But this is not what I need (and frankly, not what I expect TF to produce).
I'm rather looking for different random numbers across epochs, and I want these to be identical across runs. And my finding a solution to this issue is hampered by the fact that I don't get none of the the two solutions below provides that. I'll try to summarize my lack of understanding in one piece of code:
&lt;denchmark-code&gt;import tensorflow as tf

def testing():
    rand = lambda elem: tf.random.uniform([])
    data = tf.data.Dataset.range(5).map(rand).batch(5)
    print("epoch 1: ", end="")
    [print(elem.numpy()) for elem in data]
    print("epoch 2: ", end="")
    [print(elem.numpy()) for elem in data]

print("w/o graph-level seed: different random numbers across epochs and runs")
testing()
print("w/ graph-level seed: same random numbers across epochs and runs")
tf.random.set_seed(0)
testing()
&lt;/denchmark-code&gt;

run 1:
&lt;denchmark-code&gt;w/o graph-level seed: different random numbers across epochs and runs
epoch 1: [0.52723086 0.07255614 0.9757483  0.8239068  0.90070856]
epoch 2: [0.6677898  0.4100479  0.21574163 0.144176   0.04567683]
w/ graph-level seed: same random numbers across epochs and runs
epoch 1: [0.2773143  0.92967796 0.11471713 0.51984704 0.6447146 ]
epoch 2: [0.2773143  0.92967796 0.11471713 0.51984704 0.6447146 ]
&lt;/denchmark-code&gt;

run 2 (with annotations):
&lt;denchmark-code&gt;w/o graph-level seed: different random numbers across epochs and runs
epoch 1: [0.5421027 0.3198986 0.8723525 0.5778849 0.7087102] &lt;-- diff. from run 1. expected!
epoch 2: [0.8138256  0.2707013  0.32449722 0.34031594 0.30259633] &lt;-- diff. from epoch 0 (?)
w/ graph-level seed: same random numbers across epochs and runs
epoch 1: [0.2773143  0.92967796 0.11471713 0.51984704 0.6447146 ] &lt;-- same as run 1. expected!
epoch 2: [0.2773143  0.92967796 0.11471713 0.51984704 0.6447146 ] &lt;-- same as epoch 0 (?)
&lt;/denchmark-code&gt;

I focus on the two lines with (?). Is this (both!) intended behavior? If so, why is the first number of epoch 2 the same as in epoch 1 (0.2773143) when a graph-level seed is set, but not without the graph-level seed (0.8138256 != 0.5421027 in run 2 and 0.6677898 != 0.5421027 in run 1), if the whole rand mapping function is only called once? This simply does not make sense to me.
		</comment>
		<comment id='13' author='bersbersbers' date='2020-01-24T22:56:46Z'>
		tf.data does not maintain any state across iterations (with the exception of shuffle and cache). In other words, if your input pipeline is deterministic, does not contain shuffle and unseeded random operations, and does not depend on any external state, it will always produce the same sequence of numbers.
If you would like your input pipeline to produce deterministic behavior, that at the same time is different between different epochs, then you either need to introduce external state (e.g. an epoch counter that is used to seed the randomness) or piggyback on shuffle.
Here is an example of how you can create an input pipeline for which different iterations produce different but deterministic sequence of numbers:
&lt;denchmark-code&gt;seed = tf.Variable(0, dtype=tf.int64)

def get_seed(_):
  seed.assign_add(1)
  return seed

seeds = tf.data.Dataset.range(1).map(get_seed)
seeds = seeds.flat_map(lambda seed: tf.data.experimental.RandomDataset(seed=seed))

ds = tf.data.Dataset.zip((seeds.batch(2), tf.data.Dataset.range(5)))
ds = ds.map(lambda seed, _: tf.random.stateless_uniform([], seed=seed)).batch(5)

print("epoch 1:")
for elem in ds:
  print(elem.numpy())
print("epoch 2:")
for elem in ds:
  print(elem.numpy())  
&lt;/denchmark-code&gt;

In my colab, the above determinstically produces:
&lt;denchmark-code&gt;epoch 1:
[0.17277443 0.01138496 0.5387242  0.14688337 0.98976684]
epoch 2:
[0.12150574 0.7640343  0.28948808 0.09558952 0.08135116]
&lt;/denchmark-code&gt;

Last but not least, to answer your question about (?): both is intended behavior. For the case when the graph-level seed is not specified, the random ops in the input pipeline graph will be unseeded, which means non-deterministic behavior. For the case when the graph-level seed is specified, then the input pipeline will be deterministic and since it does not depend on any state outside of the input pipeline graph, each execution of the input pipeline will be identical.
		</comment>
		<comment id='14' author='bersbersbers' date='2020-01-24T23:18:19Z'>
		Thanks for the explanations!
		</comment>
		<comment id='15' author='bersbersbers' date='2020-01-24T23:18:21Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35682&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35682&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>