<bug id='27807' author='RoyIronGrey' open_date='2019-04-13T05:52:01Z' closed_time='2019-05-09T21:23:44Z'>
	<summary>NNAPI doesn't support tensors with rank 0 (index 3 name PadV2/constant_values)</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

OS Platform and Distribution : Windows 10 and Android 9.0
Mobile device : Xiaomi 8
TensorFlow version : org.tensorflow:tensorflow-lite:0.0.0-nightly (using 1.11.0 has the same problem)
Model: resnet18 convert from Pytorch

logs from Android 9.0
E/tflite: NNAPI doesn't support tensors with rank 0 (index 3 name PadV2/constant_values)
E/tflite: Returning error since TFLite returned failure nnapi_delegate.cc:736.
Failed to build graph for NNAPI
other
if I use resnet50 from tensorflow.contrib.slim.python.slim.nets, it also has similar problem like:
E/tflite: NNAPI doesn't support tensors with rank 0 (index 7 name inference_model/Mean/reduction_indices)
Returning error since TFLite returned failure nnapi_delegate.cc:736.
Failed to build graph for NNAPI
	</description>
	<comments>
		<comment id='1' author='RoyIronGrey' date='2019-04-16T10:47:26Z'>
		&lt;denchmark-link:https://github.com/RoyIronGrey&gt;@RoyIronGrey&lt;/denchmark-link&gt;
 In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!
		</comment>
		<comment id='2' author='RoyIronGrey' date='2019-04-25T02:28:18Z'>
		
@RoyIronGrey In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!

protected Interpreter tflite;
private Interpreter.Options tfliteOptions = new Interpreter.Options();

tfliteModel = loadModelFile(activity);

/** This is for tensorflow-lite:0.0.0-nightly **/
/** If I activate this option, the error like the title will occur**/
tfliteOptions.setUseNNAPI(true);

tflite = new Interpreter(tfliteModel, tfliteOptions);

protected String getModelPath() {
      return "resnet18_mv.tflite";
} 
Below is Python Code and Script to describe how to get the model
'''
  Package Version
    torch:       0.4.0
    torchvision: 0.2.1
    tensorflow:  1.12.0
    mmdnn:       0.2.4
'''

'''
  Part 1: Pytorch resnet18 saving
'''

# Resnet18 model from torchvision
base_model = getattr(torchvision.models, 'resnet18')(pretrained=True)

# Load weights
weights    = 'resnet18_model_checkpoint.pth.tar'
checkpoint = torch.load(weights)
base_dict  = {'.'.join(k.split('.')[1:]): v for k,v in list(checkpoint['state_dict'].items())}
base_model.load_state_dict(base_dict)
base_model.eval()

# Save model
model_path = 'resnet18_model.pth'
torch.save(base_model, model_path)
&lt;denchmark-code&gt;'''
  Part 2: Resnet18 from Pytorch (.pth) to Tensorflow (.pb) with MMdnn
          See https://github.com/Microsoft/MMdnn/blob/master/mmdnn/conversion/pytorch/README.md#convert-pytorch-pre-trained-models-to-ir
'''

mmtoir -f pytorch -d resnet18 --inputShape 3,224,224 -n resnet18_model.pth
mmtocode -f tensorflow --IRModelPath resnet18.pb --IRWeightPath resnet18.npy --dstModelPath tf_resnet18.py
mmtomodel -f tensorflow -in tf_resnet18.py -iw resnet18.npy -o tf_resnet18 --dump_tag SERVING
&lt;/denchmark-code&gt;

'''
  Part 3: Tensorflow (.pb) to Tensorflow Lite (.lite) with TFLiteConverter.from_session
'''

with tf.Session() as sess:

    mmdnn_pb_path = 'tf_resnet18/'

    # Load pb model file (MMdnn, see https://github.com/Microsoft/MMdnn/blob/master/mmdnn/conversion/tensorflow/README.md#reuse-the-converted-tensorflow-model)
    tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], mmdnn_pb_path)

    input_node_name  = 'input'
    output_node_name = 'dense/BiasAdd'
    input_tensor     = sess.graph.get_tensor_by_name(input_node_name  + ':0')   # Input tensor
    output_tensor    = sess.graph.get_tensor_by_name(output_node_name + ':0')   # Logits tensor

    # Convert
    converter        = tf.contrib.lite.TFLiteConverter.from_session(sess, [input_tensor], [output_tensor])
    tflite_model     = converter.convert()

    open('tf_resnet18.tflite', "wb").write(tflite_model)
		</comment>
		<comment id='3' author='RoyIronGrey' date='2019-04-25T02:32:57Z'>
		
@RoyIronGrey In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!

And Thanks for your attention!
		</comment>
		<comment id='4' author='RoyIronGrey' date='2019-05-02T19:13:34Z'>
		&lt;denchmark-link:https://github.com/miaowang14&gt;@miaowang14&lt;/denchmark-link&gt;
 can you provide an update on scalar support?
		</comment>
		<comment id='5' author='RoyIronGrey' date='2019-05-02T21:38:40Z'>
		I have a pending fix internally, and will further test / improve it. Looking forward to get it ready for the next couple of days.
		</comment>
		<comment id='6' author='RoyIronGrey' date='2019-05-07T22:26:16Z'>
		&lt;denchmark-link:https://github.com/RoyIronGrey&gt;@RoyIronGrey&lt;/denchmark-link&gt;
 is the issue still reproducible on top of tree?
		</comment>
		<comment id='7' author='RoyIronGrey' date='2019-05-08T06:35:44Z'>
		
@RoyIronGrey is the issue still reproducible on top of tree?

Thanks for your reply. Yes, If I use the same edition of TF lite API. Is there anything new or any new edition for this problem?
		</comment>
		<comment id='8' author='RoyIronGrey' date='2019-05-08T09:08:06Z'>
		
@RoyIronGrey is the issue still reproducible on top of tree?

Or I may misunderstand what you mean
		</comment>
		<comment id='9' author='RoyIronGrey' date='2019-05-08T22:52:39Z'>
		&lt;denchmark-link:https://github.com/RoyIronGrey&gt;@RoyIronGrey&lt;/denchmark-link&gt;
 We just submitted a couple of fixes related to this issue. I don't think tensorflow-lite:0.0.0-nightly or 1.11.0 build include them.
You might be able to build TFLite binaries from source and try it out: &lt;denchmark-link:https://www.tensorflow.org/lite/guide/build_arm64&gt;https://www.tensorflow.org/lite/guide/build_arm64&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='RoyIronGrey' date='2019-05-08T23:25:58Z'>
		The nightly should have the fixes, but you might need to &lt;denchmark-link:https://stackoverflow.com/questions/23025433/how-to-clear-gradle-cache&gt;clear your gradle cache&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='11' author='RoyIronGrey' date='2019-05-09T03:35:13Z'>
		
The nightly should have the fixes, but you might need to clear your gradle cache.
Yes, It works. But why does it cost more time and unsteady?(Maybe I made some mistakes)
Below it's the difference

     //If set useNNAPI true
     tfliteOptions.setUseNNAPI(true);
     //time from Log
     infer mv/frame: 375 ms
     infer mv/frame: 175 ms
     infer mv/frame: 516 ms
     //If set useNNAPI false
     tfliteOptions.setUseNNAPI(false);
     //time from Log
     infer mv/frame: 100 ms
     infer mv/frame: 87 ms
		</comment>
		<comment id='12' author='RoyIronGrey' date='2019-05-09T21:22:27Z'>
		I guess the performance issues could be related two possible causes:

There are still operations in the Model not supported by NNAPI, so they fallback to TFLite CPU implementation. The overhead and sync cost between multiple subgraphs  could be high.
The model itself is a floating point model, while Mi8 may just have an hardware accelerator (Hexagon DSP) that can only accelerate quantized 8bit models.

		</comment>
		<comment id='13' author='RoyIronGrey' date='2019-05-09T21:23:43Z'>
		I will mark this specific issue as fixed. But feel free to keep comment on it or create new ones if you have questions about the performance and NNAPI delegate.
		</comment>
		<comment id='14' author='RoyIronGrey' date='2019-05-09T21:23:45Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27807&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27807&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='RoyIronGrey' date='2019-05-10T01:13:27Z'>
		
I will mark this specific issue as fixed. But feel free to keep comment on it or create new ones if you have questions about the performance and NNAPI delegate.

Thanks a lot! I'll stay focus on the processing of this project as it really supports my research
		</comment>
	</comments>
</bug>