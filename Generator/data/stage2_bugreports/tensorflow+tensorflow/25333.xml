<bug id='25333' author='peidaqi' open_date='2019-01-30T19:34:20Z' closed_time='2019-01-31T17:34:33Z'>
	<summary>Eager execution - GradientTape.gradient() only returns gradient for last variable w.r.t. y?</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
TensorFlow version (use command below):
Python version:
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

You can collect some of this information using our environment capture &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
Describe the current behavior
GradientTape.gradient() returns gradient for only the last variable in var_list, which is a strange behavior different from tf.gradient()
Describe the expected behavior
GradientTape.gradient() returns gradients for every variable in the var_list, same as tf.gradient()
Code to reproduce the issue
With eager execution (wrong output with only gradient for last variable - x2 calculated):
x1 = tf.constant(3.0)
x2 = x1 * x1
with tf.GradientTape() as g:
g.watch([x1, x2])
y = x2 + 1
g.gradient([x1, x2]).numpy()
[OUTPUT] 1.0
Without eager execution (the expected output):
x1 = tf.constant(3.0)
x2 = x1 * x1
y = x2 + 1
op = tf.gradient(y, [x1, x2])
with tf.Session() as sess:
print(sess.run(op))
[OUTPUT] [6.0, 1.0]
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='peidaqi' date='2019-01-31T16:58:26Z'>
		I'm confused. Did you mean to write g.gradient([x1, x2]).numpy()? I'd have assumed you meant g.gradient(y, [x1, x2]).numpy()?
		</comment>
		<comment id='2' author='peidaqi' date='2019-01-31T17:23:22Z'>
		Your are right. That was a typo.
		</comment>
		<comment id='3' author='peidaqi' date='2019-01-31T17:34:33Z'>
		Then I cannot reproduce. When I run the code as you wrote it I get the error about the missing sources argument. When I run with the version I wrote it complains (as it should) that you're calling .numpy() on a list. If I don't do that I get the result which is working as intended, [None, 1.0]. The gradient wrt x1 is None because you defined x1 outside the tape; if you want a non-None gradient do something like:
&lt;denchmark-code&gt;x1 = tf.constant(3.0)
with tf.GradientTape() as g:
  g.watch(x1)
  x2 = x1 * x1
  y = x2 + 1
g.gradient(y, [x1, x2])
&lt;/denchmark-code&gt;

which returns the right values (6 and 1).
		</comment>
		<comment id='4' author='peidaqi' date='2019-02-01T15:42:14Z'>
		I guess by "defined x1 outside the tape" you mean the operation w.r.t. x1 rather than the x1 variable itself?
I double checked the document and indeed it says:

"Operations are recorded if they are executed within this context manager and at least one of their inputs is being "watched"."

		</comment>
		<comment id='5' author='peidaqi' date='2019-02-01T16:55:27Z'>
		I meant that x2 was defined before you created the tape, so the tape did
not know how x1 and x2 were related.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Fri, Feb 1, 2019 at 7:45 AM peidaqi ***@***.***&gt; wrote:
 I guess by "defined x1 outside the tape" you mean the operation w.r.t. x1
 rather than the x1 variable itself?

 I double checked the document and indeed it says:

 "Operations are recorded if they are executed within this context manager
 and at least one of their inputs is being "watched"."

 —
 You are receiving this because you modified the open/close state.
 Reply to this email directly, view it on GitHub
 &lt;#25333 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAATxVuUlhsnJzgQeX0S9AKi1bLJnQMYks5vJGEogaJpZM4aa6Lr&gt;
 .


-- 
 - Alex

		</comment>
	</comments>
</bug>