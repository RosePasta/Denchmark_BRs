<bug id='18266' author='lenlen' open_date='2018-04-05T16:01:18Z' closed_time='2019-12-16T20:02:38Z'>
	<summary>Cache lockfile already exists</summary>
	<description>
Using the train_and_evaluate method from estimator API and cache policy on filesystem, I get an error because the evaluation starts before that all cache is written on the filesystem. So when the train runs the second time, after the evaluation, I get the error because it finds the lock file.
If the cache is written before the first evaluation I don't get the error.
Here a snippet runnable on colab
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np
from tensorflow import keras as ks
import itertools


def batch_reshape(a, b):
    a.set_shape([None, None, 3])
    b.set_shape([None, None, 1])
    return (a, b)


def gen(n):
    a = np.array(np.random.rand(5, 5, 3).astype(np.float32))
    b = np.array(np.random.rand(5, 5, 1).astype(np.float32))
    return (a, b)


def my_input_fn_train():
    ds = tf.data.Dataset.range(100000)
    ds = ds.map(
        lambda x: tf.py_func(func=gen, inp=[x], Tout=[tf.float32, tf.float32]))
    ds = ds.map(batch_reshape)
    ds = ds.cache("/tmp/mycache_train")

    ds = ds.repeat(3)

    value = ds.make_one_shot_iterator().get_next()

    return {"input_rgb": value[0]}, {"softmax": value[1]}


def my_input_fn_eval():
    ds = tf.data.Dataset.range(100000)
    ds = ds.map(
        lambda x: tf.py_func(func=gen, inp=[x], Tout=[tf.float32, tf.float32]))
    ds = ds.map(batch_reshape)
    ds = ds.cache("/tmp/mycache_eval")

    ds = ds.repeat(3)

    value = ds.make_one_shot_iterator().get_next()

    return {"input_rgb": value[0]}, {"softmax": value[1]}


def main():
    tf.logging.set_verbosity(tf.logging.INFO)
    input_rgb = ks.layers.Input(shape=(1, 5, 5, 3), name="input_rgb")
    x = ks.layers.Dense(1, activation='relu', name="Dense_1")(input_rgb)
    x = ks.layers.Dense(1, activation='softmax', name="softmax")(x)
    model = ks.models.Model(inputs=[input_rgb], outputs=[x])
    model.compile(
        loss={'softmax': 'binary_crossentropy'},
        optimizer=tf.keras.optimizers.Adam())

    estimator = ks.estimator.model_to_estimator(keras_model=model)

    train_spec = tf.estimator.TrainSpec(input_fn=my_input_fn_train)

    eval_spec = tf.estimator.EvalSpec(
        input_fn=my_input_fn_eval, steps=5, throttle_secs=2)

    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)


if __name__ == "__main__":
    main()

&lt;/denchmark-code&gt;

and here the output
&lt;denchmark-code&gt;INFO:tensorflow:Using the Keras model from memory.
INFO:tensorflow:Using default config.
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpd1r865ry
INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpd1r865ry', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x7fcb48f9ecf8&gt;, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
INFO:tensorflow:Running training and evaluation locally (non-distributed).
INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 2 secs (eval_spec.throttle_secs) or training is finished.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from /tmp/tmpd1r865ry/keras_model.ckpt
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpd1r865ry/model.ckpt.
INFO:tensorflow:loss = 9.25762, step = 1
INFO:tensorflow:global_step/sec: 612.893
INFO:tensorflow:loss = 9.348354, step = 101 (0.166 sec)
INFO:tensorflow:global_step/sec: 952.69
INFO:tensorflow:loss = 8.319871, step = 201 (0.108 sec)
INFO:tensorflow:global_step/sec: 838.883
INFO:tensorflow:loss = 7.7207212, step = 301 (0.119 sec)
INFO:tensorflow:global_step/sec: 917.55
INFO:tensorflow:loss = 8.657611, step = 401 (0.110 sec)
INFO:tensorflow:global_step/sec: 866.062
INFO:tensorflow:loss = 8.567427, step = 501 (0.118 sec)
INFO:tensorflow:global_step/sec: 862.789
INFO:tensorflow:loss = 7.4227247, step = 601 (0.110 sec)
INFO:tensorflow:global_step/sec: 847.577
INFO:tensorflow:loss = 6.676866, step = 701 (0.118 sec)
INFO:tensorflow:global_step/sec: 911.827
INFO:tensorflow:loss = 7.4915752, step = 801 (0.112 sec)
INFO:tensorflow:global_step/sec: 837.15
INFO:tensorflow:loss = 7.7332606, step = 901 (0.120 sec)
INFO:tensorflow:global_step/sec: 857.915
INFO:tensorflow:loss = 7.283838, step = 1001 (0.116 sec)
INFO:tensorflow:global_step/sec: 786.144
INFO:tensorflow:loss = 8.61713, step = 1101 (0.127 sec)
INFO:tensorflow:Saving checkpoints for 1150 into /tmp/tmpd1r865ry/model.ckpt.
INFO:tensorflow:Loss for final step: 8.417924.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2018-04-05-15:36:08
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from /tmp/tmpd1r865ry/model.ckpt-1150
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Evaluation [1/5]
INFO:tensorflow:Evaluation [2/5]
INFO:tensorflow:Evaluation [3/5]
INFO:tensorflow:Evaluation [4/5]
INFO:tensorflow:Evaluation [5/5]
INFO:tensorflow:Finished evaluation at 2018-04-05-15:36:08

INFO:tensorflow:Saving dict for global step 1150: global_step = 1150, loss = 8.261229
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from /tmp/tmpd1r865ry/model.ckpt-1150
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.

---------------------------------------------------------------------------
AlreadyExistsError                        Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1360     try:
-&gt; 1361       return fn(*args)
   1362     except errors.OpError as e:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1339           return tf_session.TF_Run(session, options, feed_dict, fetch_list,
-&gt; 1340                                    target_list, status, run_metadata)
   1341 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    515             compat.as_text(c_api.TF_Message(self.status.status)),
--&gt; 516             c_api.TF_GetCode(self.status.status))
    517     # Delete the underlying status object from memory otherwise it stays alive

AlreadyExistsError: There appears to be a concurrent caching iterator running - cache lockfile already exists ('/tmp/mycache_train.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1522942566
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,3], [?,?,1]], output_types=[DT_FLOAT, DT_FLOAT], _device="/job:localhost/replica:0/task:0/device:CPU:0"](OneShotIterator)]]

During handling of the above exception, another exception occurred:

AlreadyExistsError                        Traceback (most recent call last)
&lt;ipython-input-5-9d8332f50ddd&gt; in &lt;module&gt;()
     66 
     67 if __name__ == "__main__":
---&gt; 68     main()

&lt;ipython-input-5-9d8332f50ddd&gt; in main()
     62         input_fn=my_input_fn_eval, steps=5, throttle_secs=2)
     63 
---&gt; 64     tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
     65 
     66 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py in train_and_evaluate(estimator, train_spec, eval_spec)
    419         '(with task id 0).  Given task id {}'.format(config.task_id))
    420 
--&gt; 421   executor.run()
    422 
    423 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py in run(self)
    492         config.task_type != run_config_lib.TaskType.EVALUATOR):
    493       logging.info('Running training and evaluation locally (non-distributed).')
--&gt; 494       self.run_local()
    495       return
    496 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py in run_local(self)
    624           input_fn=self._train_spec.input_fn,
    625           max_steps=self._train_spec.max_steps,
--&gt; 626           hooks=train_hooks)
    627 
    628       # Final export signal: For any eval result with global_step &gt;= train

/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
    350 
    351     saving_listeners = _check_listeners_type(saving_listeners)
--&gt; 352     loss = self._train_model(input_fn, hooks, saving_listeners)
    353     logging.info('Loss for final step: %s.', loss)
    354     return self

/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)
    889         loss = None
    890         while not mon_sess.should_stop():
--&gt; 891           _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
    892       return loss
    893 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)
    544                           feed_dict=feed_dict,
    545                           options=options,
--&gt; 546                           run_metadata=run_metadata)
    547 
    548   def run_step_fn(self, step_fn):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)
   1020                               feed_dict=feed_dict,
   1021                               options=options,
-&gt; 1022                               run_metadata=run_metadata)
   1023       except _PREEMPTION_ERRORS as e:
   1024         logging.info('An error was raised. This may be due to a preemption in '

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)
   1111         raise six.reraise(*original_exc_info)
   1112       else:
-&gt; 1113         raise six.reraise(*original_exc_info)
   1114 
   1115 

/usr/local/lib/python3.6/dist-packages/six.py in reraise(tp, value, tb)
    691             if value.__traceback__ is not tb:
    692                 raise value.with_traceback(tb)
--&gt; 693             raise value
    694         finally:
    695             value = None

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)
   1096   def run(self, *args, **kwargs):
   1097     try:
-&gt; 1098       return self._sess.run(*args, **kwargs)
   1099     except _PREEMPTION_ERRORS:
   1100       raise

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)
   1168                                   feed_dict=feed_dict,
   1169                                   options=options,
-&gt; 1170                                   run_metadata=run_metadata)
   1171 
   1172     for hook in self._hooks:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)
    948 
    949   def run(self, *args, **kwargs):
--&gt; 950     return self._sess.run(*args, **kwargs)
    951 
    952   def run_step_fn(self, step_fn, raw_session, run_with_hooks):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    903     try:
    904       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 905                          run_metadata_ptr)
    906       if run_metadata:
    907         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1135     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1136       results = self._do_run(handle, final_targets, final_fetches,
-&gt; 1137                              feed_dict_tensor, options, run_metadata)
   1138     else:
   1139       results = []

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1353     if handle is None:
   1354       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-&gt; 1355                            options, run_metadata)
   1356     else:
   1357       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1372         except KeyError:
   1373           pass
-&gt; 1374       raise type(e)(node_def, op, message)
   1375 
   1376   def _extend_graph(self):

AlreadyExistsError: There appears to be a concurrent caching iterator running - cache lockfile already exists ('/tmp/mycache_train.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1522942566
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,3], [?,?,1]], output_types=[DT_FLOAT, DT_FLOAT], _device="/job:localhost/replica:0/task:0/device:CPU:0"](OneShotIterator)]]

Caused by op 'IteratorGetNext', defined at:
  File "/usr/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py", line 16, in &lt;module&gt;
    app.launch_new_instance()
  File "/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py", line 658, in launch_instance
    app.start()
  File "/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py", line 477, in start
    ioloop.IOLoop.instance().start()
  File "/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py", line 177, in start
    super(ZMQIOLoop, self).start()
  File "/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py", line 888, in start
    handler_func(fd_obj, events)
  File "/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py", line 440, in _handle_events
    self._handle_recv()
  File "/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File "/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py", line 414, in _run_callback
    callback(*args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File "/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File "/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py", line 399, in execute_request
    user_expressions, allow_stdin)
  File "/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File "/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py", line 2718, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py", line 2822, in run_ast_nodes
    if self.run_code(code, result):
  File "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py", line 2882, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "&lt;ipython-input-5-9d8332f50ddd&gt;", line 68, in &lt;module&gt;
    main()
  File "&lt;ipython-input-5-9d8332f50ddd&gt;", line 64, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py", line 421, in train_and_evaluate
    executor.run()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py", line 494, in run
    self.run_local()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py", line 626, in run_local
    hooks=train_hooks)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py", line 352, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py", line 809, in _train_model
    input_fn, model_fn_lib.ModeKeys.TRAIN))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py", line 668, in _get_features_and_labels_from_input_fn
    result = self._call_input_fn(input_fn, mode)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py", line 760, in _call_input_fn
    return input_fn(**kwargs)
  File "&lt;ipython-input-5-9d8332f50ddd&gt;", line 28, in my_input_fn_train
    value = ds.make_one_shot_iterator().get_next()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py", line 330, in get_next
    name=name)), self._output_types,
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py", line 866, in iterator_get_next
    output_shapes=output_shapes, name=name)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py", line 3271, in create_op
    op_def=op_def)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py", line 1650, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

AlreadyExistsError (see above for traceback): There appears to be a concurrent caching iterator running - cache lockfile already exists ('/tmp/mycache_train.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1522942566
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,3], [?,?,1]], output_types=[DT_FLOAT, DT_FLOAT], _device="/job:localhost/replica:0/task:0/device:CPU:0"](OneShotIterator)]]

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='lenlen' date='2018-04-05T16:09:35Z'>
		&lt;denchmark-link:https://github.com/saeta&gt;@saeta&lt;/denchmark-link&gt;
 Could this be an instance of the same bug as the  one that you're looking at?
		</comment>
		<comment id='2' author='lenlen' date='2018-04-09T10:53:54Z'>
		&lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 I think that there is a conceptual problem with .
Mainly you control train/eval phases with  (with a default throttle_secs=600) and/or check point in .
If the dataset is quite big you cannot evaluate until the cache is done cause a new TrainSpec call will rebuild the input/dataset pipeline and you will find a lock on the cache dir.
There could be a not so trivial workaround with   to block throttle until you will not have covered the epoch  but in the distributed setup I suppose that the dataset cache is working locally per node so is it still plausible?
Do you see any other solution (that I suppose still require documentation)?
		</comment>
		<comment id='3' author='lenlen' date='2018-04-10T17:32:19Z'>
		/cc &lt;denchmark-link:https://github.com/isaprykin&gt;@isaprykin&lt;/denchmark-link&gt;
 for suggestions about how to use .
		</comment>
		<comment id='4' author='lenlen' date='2018-04-11T03:14:07Z'>
		If I understand the problem correctly, user defines the dataset with cache(filename). For this input_fn, as the input_fn is invoked multiple times, train_and_evalaute will error out. Is this the issue?
Unfortunately, train_and_evaluate does not specify the contract how many times train input_fn and eval input_fn will be invoked today. It will limit the implementation space we have.
&lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 can user change the code into following way?
&lt;denchmark-code&gt;ds = tf.data.Dataset.range(100000)
ds = ds.map(
    lambda x: tf.py_func(func=gen, inp=[x], Tout=[tf.float32, tf.float32]))
ds = ds.map(batch_reshape)
ds = ds.cache("/tmp/mycache_train")
ds = ds.repeat(3)

def my_input_fn_train():
    value = ds.make_one_shot_iterator().get_next()
   return {"input_rgb": value[0]}, {"softmax": value[1]}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='lenlen' date='2018-04-11T04:35:54Z'>
		&lt;denchmark-link:https://github.com/xiejw&gt;@xiejw&lt;/denchmark-link&gt;
 Unfortunately that won't work because  will be bound to a graph that is different from the estimator's graph.
I'm not sure why train_and_evaluate() is calling the input function multiple times—it would be more efficient to build a single pipeline and split the elements across the device(s) in the process—but the right solution would seem to be allowing concurrent iterators to build a cache with the same eventual filename.
		</comment>
		<comment id='6' author='lenlen' date='2018-04-11T06:23:11Z'>
		[slightly offtopic] too many issues with the high level APIs. I really hope that there is a  concrete plan with &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/16182#issuecomment-372508885&gt;#16182 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='lenlen' date='2018-04-11T22:07:11Z'>
		What do you think about &lt;denchmark-link:https://github.com/ispirmustafa&gt;@ispirmustafa&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/18016#issuecomment-380603186&gt;comment&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='8' author='lenlen' date='2018-04-11T22:29:43Z'>
		current local run implementation is something like:
for ...:
estimator.train(...)
estimator.evaluate(...)
but making train_and_evaluate local mode to create this graphs only once is a good feature request.
for now as a workaround you can do following:
&lt;denchmark-code&gt;cnt = 0
def input_fn():
  cnt += 1
  cache_file_name = ... + cnt
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='lenlen' date='2018-04-11T22:46:11Z'>
		&lt;denchmark-link:https://github.com/ispirmustafa&gt;@ispirmustafa&lt;/denchmark-link&gt;
 The workaround It will be useless cause will skip the lock but you will never match the cache. Also generally what about cache matching if the pipeline is rebuild and shuffled at every Train phase?
		</comment>
		<comment id='10' author='lenlen' date='2018-04-12T15:58:13Z'>
		&lt;denchmark-link:https://github.com/bhack&gt;@bhack&lt;/denchmark-link&gt;
 could you please explain more? The error message is about using same file as cache. this workaround resolves that error. what do you mean by "skip the lock and never match"?
		</comment>
		<comment id='11' author='lenlen' date='2018-04-12T17:16:28Z'>
		&lt;denchmark-link:https://github.com/ispirmustafa&gt;@ispirmustafa&lt;/denchmark-link&gt;
  I.e. You have a large dataset over a network file system that needs to be cached over multiple train/evaluate rounds. With this you will workaround the lock but you will never match the cache after the first evaluation.
		</comment>
		<comment id='12' author='lenlen' date='2018-04-18T13:52:25Z'>
		Also any temporary workaround is quite invalidated by limits/bugs in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/17650&gt;#17650&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='lenlen' date='2018-06-26T23:12:25Z'>
		Some news at &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/19062#issuecomment-400129963&gt;#19062 (comment)&lt;/denchmark-link&gt;
. But I think it will not solve this.
		</comment>
		<comment id='14' author='lenlen' date='2018-07-02T10:02:20Z'>
		Do you need to fix  &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/19062#issuecomment-401736720&gt;#19062 (comment)&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='15' author='lenlen' date='2019-05-09T02:38:39Z'>
		tensorflow 2.0.0-alpha0
cache lockfile already exists.  When going for the 2nd epoch, of evaluation, the previous lock file isn't removed i can see it on the file system.  Doesnt happen with training data hence it fires this error as it cant get hold of ds.cache(file) it seems:
Code:
&lt;denchmark-code&gt;def get_ds_tfdata_cache(self, X_train, y_train, batch_size, load_preprocess_image, filename):
    import tensorflow as tf
    from Source.tfdata.PathConstants import Constants
    cachedir = Constants().get_tfcache_dir_path()
    ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))
    ds = ds.map(load_preprocess_image)
    ds = ds.cache(filename=cachedir + "/" + filename)
    ds = ds.apply(
        tf.data.experimental.shuffle_and_repeat(buffer_size=batch_size))
    ds = ds.batch(batch_size=batch_size)
    ds = ds.prefetch(buffer_size=batch_size)
    return ds
&lt;/denchmark-code&gt;

image_ds_train = data.get_ds_tfdata_cache(X_train=X_train, y_train=y_train, batch_size=params_list.get_batch_size(),
load_preprocess_image=helpermethods.load_and_preprocess_image,
filename=params_list.get_tf_cache_train_filename())
image_ds_validate = data.get_ds_tfdata_cache(X_train=X_train, y_train=y_train,
batch_size=params_list.get_batch_size(),
load_preprocess_image=helpermethods.load_and_preprocess_image,
filename=params_list.get_tf_cache_test_filename())
early_stop_patience = 5
training_epochs = 2
history = model.fit(x=image_ds_train, validation_data=image_ds_validate, epochs=training_epochs,
steps_per_epoch=steps_per_epoch,
validation_steps=validation_steps, callbacks=[early_stop, tensorboard])
Error:
2019-05-09 11:12:53.961797: W tensorflow/core/framework/op_kernel.cc:1431] OP_REQUIRES failed at iterator_ops.cc:988 : Already exists: There appears to be a concurrent caching iterator running - cache lockfile already exists ('./tfdata_dir/tfcache_dir/cache_validate.tf-data_0.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1557364206
Traceback (most recent call last):
File "", line 53, in 
File "C:\ML_VirtualEnv\tf2_alpha\lib\site-packages\tensorflow\python\keras\engine\training.py", line 791, in fit
initial_epoch=initial_epoch)
File "C:\ML_VirtualEnv\tf2_alpha\lib\site-packages\tensorflow\python\keras\engine\training.py", line 1515, in fit_generator
steps_name='steps_per_epoch')
File "C:\ML_VirtualEnv\tf2_alpha\lib\site-packages\tensorflow\python\keras\engine\training_generator.py", line 315, in model_iteration
steps_name='validation_steps')
File "C:\ML_VirtualEnv\tf2_alpha\lib\site-packages\tensorflow\python\keras\engine\training_generator.py", line 213, in model_iteration
batch_data = _get_next_batch(generator, mode)
File "C:\ML_VirtualEnv\tf2_alpha\lib\site-packages\tensorflow\python\keras\engine\training_generator.py", line 355, in _get_next_batch
generator_output = next(generator)
File "C:\ML_VirtualEnv\tf2_alpha\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py", line 556, in next
return self.next()
File "C:\ML_VirtualEnv\tf2_alpha\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py", line 585, in next
return self._next_internal()
File "C:\ML_VirtualEnv\tf2_alpha\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py", line 577, in _next_internal
output_shapes=self._flat_output_shapes)
File "C:\ML_VirtualEnv\tf2_alpha\lib\site-packages\tensorflow\python\ops\gen_dataset_ops.py", line 1984, in iterator_get_next_sync
_six.raise_from(_core._status_to_exception(e.code, message), None)
File "", line 3, in raise_from
tensorflow.python.framework.errors_impl.AlreadyExistsError: There appears to be a concurrent caching iterator running - cache lockfile already exists ('./tfdata_dir/tfcache_dir/cache_validate.tf-data_0.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1557364206 [Op:IteratorGetNextSync]
		</comment>
		<comment id='16' author='lenlen' date='2019-05-18T22:45:32Z'>
		+1 this is a bug in the design.  cache files should support concurrent usage since one can easily get there via tf.data.experimental.parallel_interleave().
		</comment>
		<comment id='17' author='lenlen' date='2019-08-22T09:20:57Z'>
		I wrote a little utility function to get rid of directories containing lock files
&lt;denchmark-code&gt;import os
import shutil
from pathlib import Path

def delete_trailing_lock_files(cache_root):
  lock_file_dirs = {}
  for filename in Path(cache_root).glob('**/*.lock*'):
    lock_file_dirs[os.path.split(filename)[0] + '/'] = ''
  for dir in lock_file_dirs.keys():
    try:
      shutil.rmtree(dir)
    except:
      print('failed removing dir', filename)
&lt;/denchmark-code&gt;

Note that what you want to do is to create a single dataset iterator and keep using it:
&lt;denchmark-code&gt;val_dataset = tf.data.Dataset.range(10)
val_dataset = val_dataset.cache('/tmp/cacheset/')
val_dataset = val_dataset.repeat()

val_iterator = iterator(val_dataset) #Never do this more than once!
&lt;/denchmark-code&gt;

		</comment>
		<comment id='18' author='lenlen' date='2019-08-25T07:33:49Z'>
		I also find the .cache method of tf.data.Dataset a bit odd, with the docs stating:

filename: A tf.string scalar tf.Tensor, representing the name of a directory on the filesystem to use for caching tensors in this Dataset. If a filename is not provided, the dataset will be cached in memory.

e.g. asking for filename when filename actually represents the name of a directory.
Part of the cache method problem is that the cached file is not always auto-cleaned up.
I think adding a positional argument overwrite solves this issue by allowing users to cache to a known location like:
ds_train = ds_train.cache(filename=os.path.join(cache_dir, 'train'), overwrite=True)
ds_valid = ds_valid.cache(filename=os.path.join(cache_dir, 'valid'), overwrite=True)
		</comment>
		<comment id='19' author='lenlen' date='2019-09-16T06:37:51Z'>
		&lt;denchmark-link:https://github.com/lenlen&gt;@lenlen&lt;/denchmark-link&gt;
, did you try the &lt;denchmark-link:https://github.com/SumNeuron&gt;@SumNeuron&lt;/denchmark-link&gt;
's workaround. Please let us know if that solves your problem. Thanks!
		</comment>
		<comment id='20' author='lenlen' date='2019-09-16T10:05:02Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 the  kwarg does not exist, it's just a proposal: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/data/ops/dataset_ops.py#L1738-L1740&gt;https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/data/ops/dataset_ops.py#L1738-L1740&lt;/denchmark-link&gt;

overwrite would be helpful but wouldn't solve the concurrency problem.  The op needs a do-over.
		</comment>
		<comment id='21' author='lenlen' date='2019-12-06T00:08:14Z'>
		This is fixed with TF version 1.15.0 Thanks!
		</comment>
		<comment id='22' author='lenlen' date='2019-12-16T20:02:40Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/18266&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/18266&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>