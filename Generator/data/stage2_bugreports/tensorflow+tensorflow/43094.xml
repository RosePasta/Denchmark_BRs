<bug id='43094' author='rohin-dasari' open_date='2020-09-10T04:52:41Z' closed_time='2020-09-10T19:59:34Z'>
	<summary>batch training with model.fit not working for all batch_sizes</summary>
	<description>
System information


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes


OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows


TensorFlow installed from (source or binary):
binary via anaconda


TensorFlow version (use command below):
2.1.0


Python version: 3.7.9


Describe the current behavior
When the number of samples in the training set is not equal to a factor of the batch size, model.fit will throw an error. On 2.1.0, I get an error saying that the shapes of two operators are incompatible. Not sure if this is the expected behavior or not.
&lt;denchmark-code&gt;tensorflow.python.framework.errors_impl.InvalidArgumentError:  Incompatible shapes: [32,1] vs. [4,1]
         [[node mul_5 (defined at .\classify.py:31) ]] [Op:__inference_distributed_function_435]

Function call stack:
distributed_function
&lt;/denchmark-code&gt;

It seems like when the function is creating the last batch in the epoch, it doesn't have enough samples remaining to create a full batch, as a result, it builds what it can then errors out. The size of my dataset is 1763, not exactly a neat number. The only other method I have to implement minibatch training is to split the dataset into batches myself and train manually, without model.fit. Like I said, if this is the expected behavior, ignore this probably, but it seems like a hassle. Especially if the size of someone's dataset was a prime number, in which case they would be restricted to training either with a batch size of 1 or the size of their full dataset, which seems inconvenient.
Describe the expected behavior
The expected behavior for me would be for model.fit to split the data into batches without running out of room on the last batch.
Standalone code to reproduce the issue
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf
from tensorflow.keras import Sequential, optimizers, losses
from tensorflow.keras.layers import Input, Dense, Flatten



def make_model(input_shape, output_shape, batch_size=32):
    model = Sequential()
    model.add(Input(shape=input_shape, batch_size=batch_size, name='input'))
    
    model.add(Flatten())
    model.add(Dense(output_shape))
    model.build((batch_size, *input_shape))
    return model
    

if __name__ == '__main__':
    batch_size = 32
    input_size = (5, 5)
    output_size = 1
    num_samples = 100
    model = make_model(input_size, output_size, batch_size=batch_size)
    model.compile(optimizer=optimizers.Adam(), loss=losses.SparseCategoricalCrossentropy(from_logits=True))       
    X = np.ones((num_samples, *input_size))
    y = np.zeros(num_samples)
    model.fit(X, y, batch_size=batch_size, epochs=10)

&lt;/denchmark-code&gt;

Here's a link to a Colab notebook as well. It seems like in more recent version of tensorflow, the error is still there, but the exact error that comes up is slightly different.
&lt;denchmark-link:https://colab.research.google.com/drive/1RSOIKMYFAlg2jfPaljkosBpQyhaFbQNy?usp=sharing&gt;https://colab.research.google.com/drive/1RSOIKMYFAlg2jfPaljkosBpQyhaFbQNy?usp=sharing&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='rohin-dasari' date='2020-09-10T10:06:30Z'>
		Hi,
My guess is that the issue comes from your specifying a fixed batch size in the Input layer.
Could you please change the line model.add(Input(shape=input_shape, batch_size=batch_size, name='input')) to model.add(Input(shape=input_shape, batch_size=None, name='input')) and run the script?
		</comment>
		<comment id='2' author='rohin-dasari' date='2020-09-10T19:59:34Z'>
		Your fix works fantastically. Thank you. Although, in my opinion, this behavior is not immediately intuitive, it gets the job done.
		</comment>
		<comment id='3' author='rohin-dasari' date='2020-09-10T19:59:36Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43094&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43094&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='rohin-dasari' date='2020-09-11T07:11:31Z'>
		Great!
To explain a bit: when you pass batch_size=32 to the Input layer, the computational graph is built to support this, an only this, input batch size, which can result in some optimizations as compared with accepting a dynamic input size. If you wanted, you could use an option in tf.data.Dataset.batch to discard remaining samples that do not form a complete batch - another option could be to zero-pad the final batch, but then you have to use the masking system to ensure padding entries are not used when computing model updates.
On the other hand, passing batch_size=None to the Input layer triggers the support of any input batch size in your graph - but that means some operations may have to be recompiled on the fly to match the various input configurations. It is actually quite the same as when you use None to leave some other input dimensions unspecified; e.g. to build RNNs that receive variable-size sequences.
		</comment>
	</comments>
</bug>