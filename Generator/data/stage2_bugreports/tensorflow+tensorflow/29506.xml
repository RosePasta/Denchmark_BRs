<bug id='29506' author='dbuades' open_date='2019-06-06T17:49:06Z' closed_time='2019-06-14T18:00:41Z'>
	<summary>[TF2.0-nightly] GRU/LSTM layers don't use cuDNN properly</summary>
	<description>
System information

Have I written custom code : Yes
OS Platform and Distribution :

Ubuntu 16.04 + Docker 18.09.6-ce
Arch Linux 5.1.5


TensorFlow installed from : pip install tf-nightly-gpu-2.0-preview
TensorFlow version : 2.0.0-dev20190606, but every nightly since 2.0.0-dev20190319 presents the same behaviour.
Python version: 3.6.8
CUDA/cuDNN version: CUDA V10.0.130 / cuDNN 7.5.0.56
GPU model and memory:

Nvidia GTX 980Ti (6GB)
Nivida GTX 1070 (8GB)



Describe the current behavior
GRU/LSTM layers don't use the cuDNN implementation properly, resulting in much worse performance.  Let's take for example this toy network :
&lt;denchmark-code&gt;# Imports
import numpy as np
import tensorflow as tf
tf.executing_eagerly()
print('TensorFlow version: ' + str(tf.__version__))

# Print checks
from tensorflow.python.eager import context
print('Executing eagerly? : ' + str(context.executing_eagerly()))
print('Number of GPUs: ' + str(context.num_gpus()))

# Generate random data
X = np.random.rand(6720,700,3)
y = X[:,1,1]
print('Shapes: ', X.shape, y.shape)

# Define toy network
input_shape = X.shape[2]
rnn_state_size = 1
timesteps = X.shape[1]

inputs = tf.keras.layers.Input(shape=[timesteps, input_shape], dtype=np.float32)
output = tf.keras.layers.LSTM(rnn_state_size)(inputs)
model = tf.keras.Model(inputs, output)
model.compile('rmsprop', 'mse')
print(model.summary())

# Fit
model.fit(X,y)
&lt;/denchmark-code&gt;

With the last nightly this is what we obtain:
&lt;denchmark-code&gt;TensorFlow version: 2.0.0-dev20190606
Executing eagerly? : True
2019-06-06 12:52:23.635654: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-06-06 12:52:23.660930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1658] Found device 0 with properties: 
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.7845
pciBusID: 0000:42:00.0
2019-06-06 12:52:23.661142: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-06 12:52:23.661983: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-06-06 12:52:23.662749: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-06-06 12:52:23.662937: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-06-06 12:52:23.663896: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-06-06 12:52:23.664621: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-06-06 12:52:23.667023: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-06 12:52:23.667936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1781] Adding visible gpu devices: 0
2019-06-06 12:52:23.668222: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-06-06 12:52:23.756255: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b18abf73d0 executing computations on platform CUDA. Devices:
2019-06-06 12:52:23.756289: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1070, Compute Capability 6.1
2019-06-06 12:52:23.758641: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3494060000 Hz
2019-06-06 12:52:23.759820: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b18aff2990 executing computations on platform Host. Devices:
2019-06-06 12:52:23.759845: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
2019-06-06 12:52:23.760484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1658] Found device 0 with properties: 
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.7845
pciBusID: 0000:42:00.0
2019-06-06 12:52:23.760515: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-06 12:52:23.760527: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-06-06 12:52:23.760537: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-06-06 12:52:23.760547: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-06-06 12:52:23.760557: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-06-06 12:52:23.760567: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-06-06 12:52:23.760577: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-06-06 12:52:23.761521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1781] Adding visible gpu devices: 0
2019-06-06 12:52:23.761549: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-06 12:52:23.762256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-06-06 12:52:23.762272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1205]      0 
2019-06-06 12:52:23.762280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1218] 0:   N 
2019-06-06 12:52:23.763253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6407 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:42:00.0, compute capability: 6.1)
Number of GPUs: 1
Shapes:  (6720, 700, 3) (6720,)
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 700, 3)]          0         
_________________________________________________________________
lstm (LSTM)                  (None, 1)                 20        
=================================================================
Total params: 20
Trainable params: 20
Non-trainable params: 0
_________________________________________________________________
None
Train on 6720 samples
2019-06-06 12:52:26.219667: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
6720/6720 [==============================] - 114s 17ms/sample - loss: 0.1441
&lt;/denchmark-code&gt;

Which is much slower than what we obtained with version 2.0.0-dev20190319 and previous (including version 2.0-alpha) :
&lt;denchmark-code&gt;TensorFlow version: 2.0.0-dev20190319
Executing eagerly? : True
2019-06-06 13:23:14.360714: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-06-06 13:23:14.379231: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-06-06 13:23:14.500580: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558a01550ac0 executing computations on platform CUDA. Devices:
2019-06-06 13:23:14.500637: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1070, Compute Capability 6.1
2019-06-06 13:23:14.525050: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3494060000 Hz
2019-06-06 13:23:14.526497: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558a01662bb0 executing computations on platform Host. Devices:
2019-06-06 13:23:14.526541: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
2019-06-06 13:23:14.526816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1551] Found device 0 with properties: 
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.7845
pciBusID: 0000:42:00.0
totalMemory: 7.92GiB freeMemory: 6.59GiB
2019-06-06 13:23:14.526860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1674] Adding visible gpu devices: 0
2019-06-06 13:23:14.526931: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-06 13:23:14.527880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1082] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-06-06 13:23:14.527903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1088]      0 
2019-06-06 13:23:14.527925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1101] 0:   N 
2019-06-06 13:23:14.528098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1222] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6407 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:42:00.0, compute capability: 6.1)
Number of GPUs: 1
Shapes:  (6720, 700, 3) (6720,)
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 700, 3)]          0         
_________________________________________________________________
lstm (LSTM)                  (None, 1)                 20        
=================================================================
Total params: 20
Trainable params: 20
Non-trainable params: 0
_________________________________________________________________
None
2019-06-06 13:23:16.864613: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
6720/6720 [==============================] - 6s 884us/sample - loss: 0.1065
&lt;/denchmark-code&gt;

Other info / logs
I have tried in different computers and I am able to reproduce the issue.
With the modifications from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/29424&gt;this pull request&lt;/denchmark-link&gt;
, I obtain the same performance in the last nightly as in 2.0.0-dev20190319, but with the advantage of being able to use cuDNN with masking, which was added by &lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
 in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/ac04087f7fb9d535d33b800d6e2bfb82c7df7077#diff-a9f256601f2626075300a37eeb4cea5f&gt;this commit&lt;/denchmark-link&gt;
.
I am willing to contribute to solve this issue in a better way if you would like me to.
Thanks!
	</description>
	<comments>
		<comment id='1' author='dbuades' date='2019-06-06T18:44:54Z'>
		Thanks for reporting the issue. Let me check the details and and see if the kernel is actaully landed on GPU or not. Will reply when I have more findings.
		</comment>
		<comment id='2' author='dbuades' date='2019-06-06T21:56:56Z'>
		I think it does land on GPU, but there is some performance regression introduced by &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/ac04087f7fb9d535d33b800d6e2bfb82c7df7077#diff-a9f256601f2626075300a37eeb4cea5f&gt;ac04087#diff-a9f256601f2626075300a37eeb4cea5f&lt;/denchmark-link&gt;
. Let me dig deep into the root cause.
		</comment>
		<comment id='3' author='dbuades' date='2019-06-06T22:54:21Z'>
		I think I found the issue, submitting the fix now.
		</comment>
		<comment id='4' author='dbuades' date='2019-06-06T23:05:34Z'>
		Nice! I'll test the fix once you submit it.
		</comment>
		<comment id='5' author='dbuades' date='2019-06-06T23:34:02Z'>
		Should be fixed by &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/e691be7814bcd7065950ec940456a4c9d8991645&gt;e691be7&lt;/denchmark-link&gt;
. Will be available in the next nightly build.
		</comment>
		<comment id='6' author='dbuades' date='2019-06-07T15:17:44Z'>
		
Should be fixed by e691be7. Will be available in the next nightly build.

Thanks for the fix!
It is not yet implemented in today's nightly, so I modified recurrent_v2.py manually to test it.
The network is learning correctly and it just takes 4 seconds per epoch now. However, I'm getting this error:
W tensorflow/core/grappler/optimizers/implementation_selector.cc:196] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_357_535' and '__inference___backward_cudnn_lstm_357_535_specialized_for_RMSprop_gradients_lstm_StatefulPartitionedCall_grad_StatefulPartitionedCall_at___inference_keras_scratch_graph_1515' both implement 'lstm_54689970-be31-4336-9a58-a64ddb74d552' but their signatures do not match.
		</comment>
		<comment id='7' author='dbuades' date='2019-06-07T19:53:58Z'>
		ah, that warning was expected during model rewrite. Previously the function inlining was revert the model rewrite to properly kickin, and now it should properly swap the cpu kernel with gpu kernel.
Having said that all, the change I make yesterday was rollback, since it somehow breaks the correctness on GPU when the LSTM is stateful. I need to dig deep and see why that's case. If you are not building a stateful LSTM network, you can leave ur temp fix as is, which will correctly trigger do the GPU kernel, otherwise, the LSTM might predict some incorrect number after layer.reset_states().
		</comment>
		<comment id='8' author='dbuades' date='2019-06-12T08:17:42Z'>
		With the Beta I am having a very similar issue, no GPU use but the warning is slightly different.

[W tensorflow/core/grappler/optimizers/implementation_selector.cc:199] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_1437_1939_specialized_for_training_Adam_gradients_gradients_lstm_StatefulPartitionedCall_grad_StatefulPartitionedCall_at___inference_keras_scratch_graph_2941' and '__inference___backward_cudnn_lstm_425_601' both implement 'lstm_3cf5bdec-2f80-424a-9b99-a4158f3beb59' but their signatures do not match.

		</comment>
		<comment id='9' author='dbuades' date='2019-06-12T16:57:36Z'>
		&lt;denchmark-link:https://github.com/mr-ubik&gt;@mr-ubik&lt;/denchmark-link&gt;
 , the beta release didn't contain my latest fix yet, sorry for the breakage. We are cherrypicking my fix into beta1, which will probably released within this week. If you need to access the latest fix now, you can use tf-nightly-gpu-2.0-preview
		</comment>
		<comment id='10' author='dbuades' date='2019-06-12T16:59:52Z'>
		Btw, there are two fixes for this bug.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/b2dfb9bff5b8f603c19e0a8a3f280b7724fb89c1&gt;b2dfb9b&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/d8379699d3cf5e951e03e70fcc5335726955f260&gt;d837969&lt;/denchmark-link&gt;
. You will need both of them if you would like to patch your forked tensorflow repo.
		</comment>
		<comment id='11' author='dbuades' date='2019-06-13T08:17:32Z'>
		&lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
 I am running a freshly installed version  but the problem is still there when trying the following &lt;denchmark-link:https://github.com/tensorflow/tfjs-examples/blob/master/translation/python/translation.py&gt;example&lt;/denchmark-link&gt;
.

2019-06-13 10:11:22.562132: W tensorflow/core/grappler/optimizers/implementation_selector.cc:199] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_1317_1817_specialized_for_training_RMSprop_gradients_gradients_lstm_1_StatefulPartitionedCall_grad_StatefulPartitionedCall_at___inference_keras_scratch_graph_3180' and '__inference___backward_cudnn_lstm_843_1019' both implement 'lstm_08afbb59-9254-44a3-9bdb-4d8ae621adc2' but their signatures do not match.

		</comment>
		<comment id='12' author='dbuades' date='2019-06-13T11:34:55Z'>
		I made some tests here too.
With tf-nightly-gpu-2.0-preview and I got 90s per epoch (better 25min), but in alpha the epochs take 80s, even with the CuDNNLSTM warnings.
Is this normal?
		</comment>
		<comment id='13' author='dbuades' date='2019-06-13T14:51:58Z'>
		&lt;denchmark-link:https://github.com/mr-ubik&gt;@mr-ubik&lt;/denchmark-link&gt;
, That warning message is just a red herring. I will try to suppress it and only show it when needed.
The nightly version should have performance on-par with alpha release.
		</comment>
		<comment id='14' author='dbuades' date='2019-06-14T18:00:43Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29506&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29506&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='dbuades' date='2019-07-04T21:44:24Z'>
		Hello &lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
 , any timeline for finishing the implementation of  for &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/ba730a4f6de09ab8635091517933462dc70e4443/tensorflow/python/keras/layers/recurrent_v2.py#L406&gt;GRU&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/ba730a4f6de09ab8635091517933462dc70e4443/tensorflow/python/keras/layers/recurrent_v2.py#L927&gt;LSTM&lt;/denchmark-link&gt;
 ? Let me know if I can be of help. Thanks!
		</comment>
		<comment id='16' author='dbuades' date='2019-07-07T16:12:27Z'>
		Hi &lt;denchmark-link:https://github.com/dbuades&gt;@dbuades&lt;/denchmark-link&gt;
, currently the change was blocked by another issue of tf.cond for device placement. It is been actively working on, and we should address it before the formal release.
		</comment>
		<comment id='17' author='dbuades' date='2019-08-02T19:38:20Z'>
		Masking support for LSTM and GRU with CuDNN has now been implemented &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/c33f1d1a6186ab0f4a9ca3b9af3a7affc85f251d&gt;here&lt;/denchmark-link&gt;
 by &lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
 and it is working properly for me. Thanks!
		</comment>
		<comment id='18' author='dbuades' date='2020-08-04T14:10:16Z'>
		I got same problem today, what should i do with keras
		</comment>
	</comments>
</bug>