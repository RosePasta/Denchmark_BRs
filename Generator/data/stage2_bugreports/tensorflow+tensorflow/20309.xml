<bug id='20309' author='moboehle' open_date='2018-06-26T16:19:09Z' closed_time='2018-11-13T23:57:29Z'>
	<summary>Very high discrepancy in memory usage and computation time in convolution operation between tf versions</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;



Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
I attach a small code example that should explain the issue,  but in principle it is nothing but multiple 1x1 convolutions with a conv1D layer.


OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
VERSION="16.04.3 LTS (Xenial Xerus)"
VERSION_ID="16.04"
VERSION_CODENAME=xenial


TensorFlow installed from (source or binary):
pip install tensorflow-gpu==1.3.0
and
pip install tensorflow-gpu==1.8.0


TensorFlow version (use command below):
The following tf versions were used to recreate the issue
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.VERSION = 1.8.0
and
tf.GIT_VERSION = b'unknown'
tf.VERSION = 1.3.0


Python version:
Python 3.5.5 :: Anaconda, Inc.


CUDA/cuDNN version:
For tf 1.3.0:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2016 NVIDIA Corporation
Built on Tue_Jan_10_13:22:03_CST_2017
Cuda compilation tools, release 8.0, V8.0.61
For tf 1.8.0
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2017 NVIDIA Corporation
Built on Fri_Sep__1_21:08:03_CDT_2017
Cuda compilation tools, release 9.0, V9.0.176


GPU model and memory:
GeForce GTX 1080 Ti
total memory shown as 10.91GiB


Exact command to reproduce:
python 'script shown below'


&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

When trying to update from tensorflow 1.3.0 to 1.8.0, we noticed that memory consumption and computation time increased significantly for our networks (both &gt;10%).
We tried to find out what was causing this issue and realized that there is a large discrepancy for memory and computation time in our 1D convolutional layers, both increasing roughly by a factor of two when going from tf 1.3 to tf 1.8.
Something similar happens in the tf 1.3 version when changing the data format from NWC to NCW, but tf 1.8 is slow regardless of the data format.
I attach a small code snippet, which creates a network of 30 1x1 1D convolutinal layers with some arbitrary parameters for batch size, width, and number of filters.
The network output is evaluated 10,000 times and the time spent for this is written to stdout, for different data formats and two different libraries for the 1D convolution (nn and layers).
The following output is generated for the different tf versions.
&lt;denchmark-code&gt;tf version 1.3.0 	 order NWC 	 library layers	 time 5.840178s
tf version 1.3.0 	 order NWC 	 library nn	 time 5.809642s
tf version 1.3.0 	 order NCW 	 library layers	 time 10.344764s
tf version 1.3.0 	 order NCW 	 library nn	 time 9.630965s

tf version 1.8.0 	 order NWC 	 library layers	 time 13.982041s
tf version 1.8.0 	 order NWC 	 library nn	 time 13.616668s
tf version 1.8.0 	 order NCW 	 library layers	 time 11.336001s
tf version 1.8.0 	 order NCW 	 library nn	 time 10.919789s

&lt;/denchmark-code&gt;

Tracking memory consumption for this network with nvidia-smi showed that tf 1.8 allocates around 467MiB for all different configurations, whereas tf 1.3 allocates 223MiB for NWC data format and up to 449MiB for the NCW format.
Moreover, we compared the outputs of the two different data formats. They appear to be the same.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;#############################################
## Created by Moritz Boehle moritz@audatic.ai
#############################################
import time
import tensorflow as tf
import numpy as np
import os

# Exclude tf logs in output for better overview
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# Some arbitrary parameters for the network
num_filters = 16
num_layers = 30
width = 100
batch_size = 20
steps = 10000

# Allow GPU growth
config = tf.ConfigProto()
config.gpu_options.allow_growth = True

# Below, we will compare performance of the two different data formats.
formats = ["NWC", "NCW"]

# Tested with 1.3.0 and 1.8.0
tf_v = tf.__version__

# Different libs for performing 1D convolution.
libs = ["layers", "nn"]

# Options for the two different data formats.
ncw_opts = {"shape": [batch_size, num_filters, width],
            "data_format_layers": "channels_first",
            "data_format_nn": "NCW" if tf_v == "1.8.0" else "NCHW"}
nwc_opts = {"shape": [batch_size, width, num_filters],
            "data_format_layers": "channels_last",
            "data_format_nn": "NWC" if tf_v == "1.8.0" else "NHWC"}
data_format_opts = {"NCW": ncw_opts, "NWC": nwc_opts}

for data_format in formats:
    opts = data_format_opts[data_format]
    for lib in libs:
        with tf.Session(config=config) as sess:
            with tf.device("/gpu:0"):
                shape = opts["shape"]
                format_str = opts["data_format_"+lib]

                # Create arbitrary input.
                layer = tf.constant(np.random.random(shape), dtype=tf.float32)

                # Create num_layers of 1D convolutions.
                for _ in range(num_layers):
                    if lib == "layers":
                        layer = tf.layers.conv1d(layer, filters=num_filters,
                                                 kernel_size=[1], strides=[1],
                                                 data_format=format_str)
                    elif lib == "nn":
                        filters = tf.Variable(tf.random_normal(
                            [1, num_filters, num_filters]))
                        bias = tf.Variable(tf.random_normal(shape))
                        layer = tf.nn.conv1d(layer, filters, 1, "VALID",
                                             data_format=format_str)
                        layer += bias
                sess.run(tf.global_variables_initializer())

                # Measure time to run 'steps' steps.
                start = time.time()
                for i in range(steps):
                    sess.run(layer)
                total = time.time() - start
                print("tf version {tf} \t order {order} \t library {lib}"
                      "\t time {total:f}s".format(tf=tf_v, total=total,
                                                  order=data_format, lib=lib))


&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='moboehle' date='2018-06-27T00:53:43Z'>
		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
Bazel version
		</comment>
		<comment id='2' author='moboehle' date='2018-06-27T01:56:45Z'>
		Thanks for the very detailed report &lt;denchmark-link:https://github.com/moboehle&gt;@moboehle&lt;/denchmark-link&gt;
 . Simply running your program on various versions (and looking at the timings, I didn't focus on memory yet), it seems this happened between versions 1.6 and 1.7:
&lt;denchmark-code&gt;tf version 1.3.0         order NWC       library layers  time 8.185683s
tf version 1.3.0         order NWC       library nn      time 7.606029s
tf version 1.3.0         order NCW       library layers  time 15.392093s
tf version 1.3.0         order NCW       library nn      time 13.677395s

tf version 1.4.0         order NWC       library layers  time 8.245580s
tf version 1.4.0         order NWC       library nn      time 7.654847s
tf version 1.4.0         order NCW       library layers  time 15.371269s
tf version 1.4.0         order NCW       library nn      time 14.107784s

tf version 1.5.0         order NWC       library layers  time 8.871874s
tf version 1.5.0         order NWC       library nn      time 8.447964s
tf version 1.5.0         order NCW       library layers  time 16.345905s
tf version 1.5.0         order NCW       library nn      time 14.314268s

tf version 1.6.0         order NWC       library layers  time 8.817178s
tf version 1.6.0         order NWC       library nn      time 7.883244s
tf version 1.6.0         order NCW       library layers  time 16.071247s
tf version 1.6.0         order NCW       library nn      time 14.459396s

tf version 1.7.0         order NWC       library layers  time 20.263543s
tf version 1.7.0         order NWC       library nn      time 19.896915s
tf version 1.7.0         order NCW       library layers  time 15.326541s
tf version 1.7.0         order NCW       library nn      time 16.310148s

tf version 1.8.0         order NWC       library layers  time 20.150042s
tf version 1.8.0         order NWC       library nn      time 20.640730s
tf version 1.8.0         order NCW       library layers  time 16.102693s
tf version 1.8.0         order NCW       library nn      time 15.144002s
&lt;/denchmark-code&gt;

(I was using the release docker images).
I believe one of the changes between 1.6 and 1.7 was that the layout optimizer was turned on by default
(i.e., it had to be &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/080d59b76ca27b184f0fce605db7f5339ea5a8cf/tensorflow/core/grappler/optimizers/meta_optimizer.cc#L100&gt;turned on explicitly in 1.6&lt;/denchmark-link&gt;
 and starting &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/core/grappler/optimizers/meta_optimizer.cc#L117&gt;1.7 had to be turned explicitly off&lt;/denchmark-link&gt;
). Since  is actually implemented as &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/23c218785eac5bfe737eec4f8081fd0ef8e0684d/tensorflow/python/ops/nn_ops.py#L2458&gt;a wrapper over the conv2d operation&lt;/denchmark-link&gt;
, this "optimization" kicks in but is probably not actually helpful :)
&lt;denchmark-link:https://github.com/rmlarsen&gt;@rmlarsen&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/benoitsteiner&gt;@benoitsteiner&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/zhangyaobit&gt;@zhangyaobit&lt;/denchmark-link&gt;
 - Please do take a look. Perhaps  needs to  be made aware of 1D convolutions and handle them differently?
&lt;denchmark-link:https://github.com/moboehle&gt;@moboehle&lt;/denchmark-link&gt;
 - In the mean time, you could work around this problem by explicitly turning off the layout optimizer using something like:
from google.protobuf import text_format
config = from google.protobuf import text_format
config = text_format.Parse("""
graph_options {
  rewrite_options {
    layout_optimizer: OFF
  }
}
""", tf.ConfigProto())
If I do that, I see the following numbers in 1.7 and 1.8:
&lt;denchmark-code&gt;tf version 1.7.0         order NWC       library layers  time 8.370198s
tf version 1.7.0         order NWC       library nn      time 7.873047s                      
tf version 1.7.0         order NCW       library layers  time 15.633171s
tf version 1.7.0         order NCW       library nn      time 13.691546s 

tf version 1.8.0         order NWC       library layers  time 9.071059s
tf version 1.8.0         order NWC       library nn      time 8.561103s
tf version 1.8.0         order NCW       library layers  time 16.833199s
tf version 1.8.0         order NCW       library nn      time 14.826180s
&lt;/denchmark-code&gt;

Hopefully this workaround tides you over for the short term and we'll look into an appropriate fix for how the layout optimizer handles 1D convolutions. I suspect the memory increase is also explained by the added transposition operation required for the layout change.
Thanks!
		</comment>
		<comment id='3' author='moboehle' date='2018-06-27T09:04:15Z'>
		Thanks &lt;denchmark-link:https://github.com/asimshankar&gt;@asimshankar&lt;/denchmark-link&gt;
 for your quick response. I tried your fix and unfortunately, it seems to have messed up my current installation. I now get the following error
&lt;denchmark-code&gt;2018-06-27 10:56:57.354254: E tensorflow/stream_executor/cuda/cuda_dnn.cc:393] possibly insufficient driver version: 384.130.0
2018-06-27 10:56:57.354261: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM
2018-06-27 10:56:57.354266: F tensorflow/core/kernels/conv_ops.cc:717] Check failed: stream-&gt;parent()-&gt;GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo&lt;T&gt;(), &amp;algorithms) 
Aborted (core dumped)
&lt;/denchmark-code&gt;

The only thing that happened since my post yesterday was including the lines of code you provided. Removing the lines or resetting the layout optimizer does not fix this error, CUDA seems to be messed up somewhat now.
Do you have an idea what could be causing this?
Furthermore, if I am not mistaken, this problem is not only related to the 1D convolution, the same computation time and memory increase occur in 2D convolutions, too. However, I cannot properly test this right now, since my installation is now faulty, but I remember seeing the same issue yesterday with 2D convolutions.
Let me know if you need any extra information, thank you for your help!
		</comment>
		<comment id='4' author='moboehle' date='2018-06-27T12:03:47Z'>
		Thank you very much, my CuDNN must have crashed for an unrelated reason, I reinstalled it now and your fix works as promised :)
The memory consumption issue goes away, too.
And contrary to what I said before, this only affects conv1D!
Best regards and thank you
		</comment>
		<comment id='5' author='moboehle' date='2018-06-27T12:41:37Z'>
		Sorry for going back and forth, but my previous test of the conv2D was insufficiently thorough as it seems. For conv2D with a large height, the layout optimizer indeed boosts performance, as seen in the following output
&lt;denchmark-code&gt;tf version 1.3.0 	 order NWC 	 library layers	 time 18.740896s
tf version 1.5.0 	 order NWC 	 library layers	 time 17.115397s
tf version 1.8.0 	 order NWC 	 library layers	 time 17.450639s
&lt;/denchmark-code&gt;

Here, I only tested the data format NWC for an input of height 20 for the different tensorflow versions, and I explicitly turned on the optimizer also for tf 1.5.
The issue with the optimizer arises, once the height becomes too small, for example with height of 2 I get the following results:
&lt;denchmark-code&gt;tf version 1.3.0 	 order NWC 	 library layers	 time 4.977753s
tf version 1.5.0 	 order NWC 	 library layers	 time 9.492385s
tf version 1.8.0 	 order NWC 	 library layers	 time 9.792400s
&lt;/denchmark-code&gt;

which is what I remembered seeing when first testing this.
Moreover, while for large heights the computation time is indeed decreased, it should be noted that the memory consumption suffers heavily from this optimization, which could be more costly than computation time for some users. E.g. for an input with width and height of 200, memory consumption almost triples(!) from 479MiB to 1359MiB in order to achieve a time speed-up of about 18% in the following output
&lt;denchmark-code&gt;tf version 1.3.0 	 order NWC 	 library layers	 time 31.275929s
tf version 1.5.0 	 order NWC 	 library layers	 time 25.768378s
tf version 1.8.0 	 order NWC 	 library layers	 time 26.140614s
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='moboehle' date='2018-06-27T16:51:00Z'>
		Thanks for the analysis &lt;denchmark-link:https://github.com/moboehle&gt;@moboehle&lt;/denchmark-link&gt;
.
I am under the impression that disabling the optimizer in certain cases is a sufficient workaround for now. Do let me know if that is incorrect.
And we'll look into making the optimizer more robust.
Sound fair?
Once again, thanks for the report and analysis, it is very helpful.
		</comment>
		<comment id='7' author='moboehle' date='2018-06-27T17:36:19Z'>
		Yes, thanks again for the workaround! Just wanted to share my findings to help you fix the issue.
Cheers
		</comment>
		<comment id='8' author='moboehle' date='2018-06-27T18:30:38Z'>
		Thanks &lt;denchmark-link:https://github.com/moboehle&gt;@moboehle&lt;/denchmark-link&gt;
 for the detailed report and &lt;denchmark-link:https://github.com/asimshankar&gt;@asimshankar&lt;/denchmark-link&gt;
 helping on debugging the issue.
With 1x1 filter in NWC format, the underlying Conv implementation is GEMM-based, which could be faster than the non-GEMM-based Conv implementation in NCW format, as you have already observed. NCW is in general faster than NWC, but 1x1 filter is an exception (which we also observed previously).
We plan to add auto-tuning support in future for layout optimizer, which will address this issue (&lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 can provide more details); currently it would use NCW layout for the whole model in a blanket manner. This is less ideal but ok for most models because (1) there are typically fewer conv layers with 1x1 filter than the conv layers with non-1x1 filters (2) the conv layers with 1x1 filter are also much cheaper computationally. But I think your model might be an exception, which extensively uses 1x1 filters.
Before the auto-tuning support, you have a few options to work around this: (1) turn off layout optimizer, and manually specify the data format for each conv, this will achieve the optimal result; it is just that it takes manual effort to implement, (2) either turn on or off layout optimizer considering the overall model-level speed and memory metrics; this produces less than optimal results, but at least give you some control.
		</comment>
		<comment id='9' author='moboehle' date='2018-10-15T12:54:18Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/zhangyaobit&gt;@zhangyaobit&lt;/denchmark-link&gt;
: It has been 109 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='10' author='moboehle' date='2018-10-26T13:24:04Z'>
		&lt;denchmark-link:https://github.com/moboehle&gt;@moboehle&lt;/denchmark-link&gt;
 Auto-tuning support has been added for layout optimizer, could you try the latest TensorFlow version and post any errors here.
		</comment>
		<comment id='11' author='moboehle' date='2018-11-13T23:57:29Z'>
		Closing, feel free to reopen if problem persists
		</comment>
		<comment id='12' author='moboehle' date='2018-11-14T15:38:51Z'>
		Hi &lt;denchmark-link:https://github.com/wt-huang&gt;@wt-huang&lt;/denchmark-link&gt;
, can you link to the commit that added auto-tuning? According to my experiments, manually disabling the layout optimizer is still required in TensorFlow 1.12 to get good performance in the scenario described in this issue.
		</comment>
	</comments>
</bug>