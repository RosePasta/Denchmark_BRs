<bug id='31184' author='hgffly' open_date='2019-07-31T05:38:34Z' closed_time='2020-01-08T22:44:18Z'>
	<summary>Transfer learning trained by custom TF 2.0 training loop performs worse than keras fit</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): 2.0.0-beta1
Python version: 3.6.7
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): 7.4.0
CUDA/cuDNN version: 7.6.0
GPU model and memory: GTX1660Ti, 6 GB

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
I performed transfer learning on pretrained model with TF custom training loop and keras fit.
Both of the settings are the same but TF custom training loop performs worse than the keras fit . I have no idea what's the problem.
I have asked the questions on StackOverflow but not got the answer what I want
&lt;denchmark-link:https://stackoverflow.com/questions/57268705/transfer-learning-with-pretrained-model-by-tf-gradienttape-cant-converge&gt;https://stackoverflow.com/questions/57268705/transfer-learning-with-pretrained-model-by-tf-gradienttape-cant-converge&lt;/denchmark-link&gt;

Describe the expected behavior
The loss and accuracy of model trained by tf.GradientTape should be similar to the one trained by keras fit with the same settings
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

physical_devices = tf.config.experimental.list_physical_devices('GPU')
try:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
except:
    pass

cifar10 = tf.keras.datasets.cifar10
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

def process_data(img, lbl):
    img = tf.image.resize(img, (96, 96))
    img = (img-128) / 128
    return img, lbl
train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(50000).batch(128)
test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(128)
train_data = train_data.map(process_data)
test_data = test_data.map(process_data)
train_data, test_data

# load the pretrained model
base_model = keras.applications.MobileNetV2(input_shape=(96, 96, 3), include_top=False, pooling='avg')
x = base_model.outputs[0]
outputs = layers.Dense(10, activation=tf.nn.softmax)(x)
model = keras.Model(inputs=base_model.inputs, outputs=outputs)

# Trained with keras fit
model.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])
history = model.fit(train_data, epochs=1)

# The results are: loss: 0.4345 - accuracy: 0.8585

# Trained with tf.GradientTape
optimizer = keras.optimizers.Adam()
train_loss = keras.metrics.Mean()
train_acc = keras.metrics.SparseCategoricalAccuracy()
def train_step(data, labels):    
    with tf.GradientTape() as gt:
        pred = model(data)
        loss = keras.losses.SparseCategoricalCrossentropy()(labels, pred)

    grads = gt.gradient(loss, model.trainable_variables)

    optimizer.apply_gradients(zip(grads, model.trainable_variables))

    train_loss(loss)
    train_acc(labels, pred)

model = keras.Model(inputs=base_model.inputs, outputs=outputs)
for xs, ys in train_data:
    train_step(xs, ys)

print('train_loss = {:.3f}, train_acc = {:.3f}'.format(train_loss.result(), train_acc.result()))

# The results are:  train_loss = 12.832, train_acc = 0.099
&lt;/denchmark-code&gt;

Other info / logs
If the model trained by tf.GradientTape with smaller learning rate 0.0001 (the default is 0.001), it works well, train_loss = 0.275, train_acc = 0.915 , but that's not the real solution what I expected, it's just a workaround.
	</description>
	<comments>
		<comment id='1' author='hgffly' date='2019-08-01T06:35:45Z'>
		I have tried on colab with TF version 2.0 beta1 and was able to reproduce the issue.Please, find the &lt;denchmark-link:https://colab.research.google.com/drive/1MnjoWQpt1grgjQAE1gDZaKu6AB7fgWBq&gt;gist&lt;/denchmark-link&gt;
 here.Thanks!
		</comment>
		<comment id='2' author='hgffly' date='2019-08-21T16:14:30Z'>
		When will the next version of TF with these bugs fixed be released ? Thanks
		</comment>
		<comment id='3' author='hgffly' date='2019-08-27T12:10:39Z'>
		I have tried TensorFlow 2.0 RC but it still doesn't work. Is it that the bug is not fix yet? or there is something wrong with the codes ? How should I apply transfer learning with tf.GradientTape successfully ? Thanks
		</comment>
		<comment id='4' author='hgffly' date='2020-01-08T03:45:46Z'>
		Hey, I got the same problem.
Simply add a training=True argument in the model like
pred = model(input, training=True)
Hopefully, you could solve it
		</comment>
		<comment id='5' author='hgffly' date='2020-01-08T22:44:18Z'>
		@JiayuanSternLi Thanks for the support in resolving the issue. &lt;denchmark-link:https://github.com/hgffly&gt;@hgffly&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/ddea44f8549605a6ea9ff069981794ef/untitled737.ipynb&gt;Here&lt;/denchmark-link&gt;
 is the gist for your reference. With , custom training is producing similar results as keras fit. The loss and accuracy are as follows
&lt;denchmark-code&gt;Train for 391 steps
391/391 [==============================] - 2116s 5s/step - loss: 0.4347 - accuracy: 0.8597

&lt;/denchmark-code&gt;

I think this is resolved. I am closing this issue. Please feel free to reopen the issue if it persists again. Thanks!
		</comment>
		<comment id='6' author='hgffly' date='2020-01-08T22:44:20Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31184&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31184&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='hgffly' date='2020-01-09T06:21:45Z'>
		
Hey, I got the same problem.
Simply add a training=True argument in the model like
pred = model(input, training=True)
Hopefully, you could solve it
@JiayuanSternLi  , thanks for help, it did work.
But I don't see document mentioning this, or maybe I miss it and I'll be so glad if you could tell me how to know it.
So I need to add training=True argument to the model whenever I trained with GradientTape or just the example of transfer learning I listed. Because I have tried training other model, it can converge without assigning training=True.
I just wonder it's a bug or it's because I don't know the behavior of the model training in TF2 well.
Thanks

		</comment>
		<comment id='8' author='hgffly' date='2020-01-14T22:45:48Z'>
		FYI regarding setting training=True: I made this &lt;denchmark-link:https://github.com/tensorflow/docs/commit/7ad5e9598e66efeef8b3ed9fa53fa5ed023ddbed&gt;commit&lt;/denchmark-link&gt;
 that makes its need more obvious in tensorflow.org tutorials going forward.
		</comment>
		<comment id='9' author='hgffly' date='2020-01-19T06:18:55Z'>
		

Hey, I got the same problem.
Simply add a training=True argument in the model like
pred = model(input, training=True)
Hopefully, you could solve it
@JiayuanSternLi  , thanks for help, it did work.
But I don't see document mentioning this, or maybe I miss it and I'll be so glad if you could tell me how to know it.
So I need to add training=True argument to the model whenever I trained with GradientTape or just the example of transfer learning I listed. Because I have tried training other model, it can converge without assigning training=True.
I just wonder it's a bug or it's because I don't know the behavior of the model training in TF2 well.
Thanks


"Such an issue differs from models you trained". It's true.
It is caused by the batch norm layers. Those models with BN layers requires the argument traning=True, but others don't
		</comment>
		<comment id='10' author='hgffly' date='2020-01-26T19:41:53Z'>
		
@JiayuanSternLi Thanks for the support in resolving the issue. @hgffly Here is the gist for your reference. With training = True, custom training is producing similar results as keras fit. The loss and accuracy are as follows
Train for 391 steps
391/391 [==============================] - 2116s 5s/step - loss: 0.4347 - accuracy: 0.8597

I think this is resolved. I am closing this issue. Please feel free to reopen the issue if it persists again. Thanks!

&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 . I also run into the same problem when training a tf.keras model even though I add "train=True". The loss reduced sharply when calling model.fit, but the performance is far worse when I use tf.GradientTape.
I created a &lt;denchmark-link:https://colab.research.google.com/drive/1aIRqi_x-YGAFtZCWL4gkOPkW_8sSlZ_N&gt;reproducible example&lt;/denchmark-link&gt;
 on Colab. Could you have a look? It would take 2 minutes to reproduce the problem.
		</comment>
		<comment id='11' author='hgffly' date='2020-01-27T22:55:46Z'>
		&lt;denchmark-link:https://github.com/wmmxk&gt;@wmmxk&lt;/denchmark-link&gt;
 Can you please open a new issue with more details on your issue, error trace and provide the colab (in the above post). It will help others who are facing similar issue like you and uses transformer model. Thanks!
		</comment>
	</comments>
</bug>