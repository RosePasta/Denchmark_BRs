<bug id='39497' author='BorisPolonsky' open_date='2020-05-13T10:01:58Z' closed_time='2020-05-18T04:24:58Z'>
	<summary>tf.keras.layers.RNN object within tf.keras.Model not automatically built after calling when `go_backward` not set to `True`</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): Docker (tensorflow/tensorflow:2.2.0-gpu)
TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0
Python version: 3
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: as shipped with official docker image (tensorflow/tensorflow:2.2.0-gpu)
GPU model and memory: Nvidia GTX 1050, 4GB

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"

Describe the current behavior
tf.keras.layers.RNN objects defined within tf.keras.Model are not automatically built after calling the model with input tensors if argument go_backward is not set to True.
Describe the expected behavior
the built flag should be true for tf.keras.layers.RNN objects within tf.keras.Model object should be flaged as built after calling the model object with tensor.
Standalone code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf
# from crf import CRF

class CrfModel(tf.keras.Model):
    def __init__(self, *args, **kwargs):
        super(CrfModel, self).__init__(*args, **kwargs)


class BiRNNCrf(CrfModel):
    def __init__(self, vocab_size, embedding_dim, cell_creator, num_tags, *args, **kwargs):
        super().__init__(self, *args, **kwargs)
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)
        self.forward_cell = cell_creator()
        self.forward_rnn_layer = tf.keras.layers.RNN(self.forward_cell, return_sequences=True, return_state=False)
        self.backward_cell = cell_creator()
        self.backward_rnn_layer = tf.keras.layers.RNN(self.backward_cell, return_sequences=True, return_state=False, go_backwards=True)
        self.bi_rnn_layer = tf.keras.layers.Bidirectional(self.forward_rnn_layer, backward_layer=self.backward_rnn_layer)
        self.fc = tf.keras.layers.Dense(num_tags)
        # self.crf = CRF(num_tags)

    def lookup(self, inputs):
        out = self.embedding(inputs)
        return out
    
    def birnn(self, inputs, sequence_mask):
        out = self.bi_rnn_layer(inputs, mask=sequence_mask)
        return out

    # @tf.function(input_signature=[tf.TensorSpec([None, None, None], dtype=tf.float32, name="input_ids"), tf.TensorSpec([None], dtype=tf.int32, name="sequence_length")])
    def call(self, inputs, sequence_length):
        out = self.lookup(inputs)
        mask = tf.sequence_mask(sequence_length, maxlen=tf.shape(inputs)[1])
        out = self.birnn(out, mask)
        out = self.fc(out)
        # out = self.crf(out, sequence_length)
        return out

def cell_creator():
    return tf.keras.layers.LSTMCell(300)

"""
def build(self, input_shape, *args, **kwargs):
    super(self, tf.keras.Model).build(input_shape, *args, **kwargs)
    self.crf.build(input_shape)
"""

if __name__ == "__main__":
    nn = BiRNNCrf(1000, 300, cell_creator, 3)
    assert isinstance(nn, tf.keras.Model)
    # nn.build(input_shape=[tf.TensorShape([None,None,3]), tf.TensorShape([None])]) # Not allowed by tensorflow
    nn(inputs=tf.constant([[1, 0, 0], [1, 1, 1]]), sequence_length=tf.constant([1,3]))
    print(nn.forward_rnn_layer.built) # False
    print(nn.backward_rnn_layer.built) # True

    nn.summary() # Fails

&lt;/denchmark-code&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
Log from Tensorflow indicating that the forward_rnn_layer is not automatically built
&lt;denchmark-code&gt;ValueError: You tried to call `count_params` on lstm_cell, but the layer isn't built. You can build it manually via: `lstm_cell.build(batch_input_shape)`.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='BorisPolonsky' date='2020-05-13T14:28:22Z'>
		Was able to reproduce the issue with &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/2cc1f01bc42df21f3ef2cb3e76529354/39497.ipynb&gt;TF v2.2&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/941caaa1c2abd3f6f4e356a396cda012/39497-tf-nightly.ipynb&gt;TF-nightly&lt;/denchmark-link&gt;
. Please find the attached gist. Thanks!
		</comment>
		<comment id='2' author='BorisPolonsky' date='2020-05-18T04:24:58Z'>
		Thanks for reporting the issue.
I think the root cause is that we clone for the forward layer and keep it in the bidirectional wrapper as a standalone instance. This is why the forward_layer.built is not propagate to the self. forward_rnn_layer. Note that model.summary() might print out duplicated weights information since weights will be double reported from self. bi_rnn_layer and forward/backward layers and cells.
The fix for this issue to not assigning the forward/backward layer/cell as the attribute for the model.
&lt;denchmark-code&gt;  def __init__(self, vocab_size, embedding_dim, cell_creator, num_tags, *args, **kwargs):
    super().__init__(self, *args, **kwargs)
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)
    forward_rnn_layer = tf.keras.layers.RNN(cell_creator(), return_sequences=True, return_state=False)
    backward_rnn_layer = tf.keras.layers.RNN(cell_creator(), return_sequences=True, return_state=False,
                                                  go_backwards=True)
    self.bi_rnn_layer = tf.keras.layers.Bidirectional(forward_rnn_layer, backward_layer=backward_rnn_layer)
    self.fc = tf.keras.layers.Dense(num_tags)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='BorisPolonsky' date='2020-05-18T04:24:59Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39497&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39497&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='BorisPolonsky' date='2020-05-19T00:51:16Z'>
		
Thanks for reporting the issue.
I think the root cause is that we clone for the forward layer and keep it in the bidirectional wrapper as a standalone instance. This is why the forward_layer.built is not propagate to the self. forward_rnn_layer. Note that model.summary() might print out duplicated weights information since weights will be double reported from self. bi_rnn_layer and forward/backward layers and cells.
The fix for this issue to not assigning the forward/backward layer/cell as the attribute for the model.
  def __init__(self, vocab_size, embedding_dim, cell_creator, num_tags, *args, **kwargs):
    super().__init__(self, *args, **kwargs)
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)
    forward_rnn_layer = tf.keras.layers.RNN(cell_creator(), return_sequences=True, return_state=False)
    backward_rnn_layer = tf.keras.layers.RNN(cell_creator(), return_sequences=True, return_state=False,
                                                  go_backwards=True)
    self.bi_rnn_layer = tf.keras.layers.Bidirectional(forward_rnn_layer, backward_layer=backward_rnn_layer)
    self.fc = tf.keras.layers.Dense(num_tags)


It works with this solution. The "side effect" here is that unlike Tensorflow 2.1 where the original implementation works, number of weights in forward/backward rnn layers that summary() prints out will be aggregated and represented as a single "Bidirectional" layer in 2.2. Didn't know if this is the desired behavior here.
		</comment>
	</comments>
</bug>