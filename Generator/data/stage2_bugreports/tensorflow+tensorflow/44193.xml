<bug id='44193' author='alanpurple' open_date='2020-10-21T05:02:01Z' closed_time='2020-11-04T07:34:05Z'>
	<summary>exit error when using tftrt in jupyterlab</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu18.04.5
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 2.4.0 master
Python version: 3.8.5
Bazel version (if compiling from source): 3.7.0
GCC/Compiler version (if compiling from source): 7.5.0
CUDA/cuDNN version: 11.1/8.0.4
GPU model and memory: tesla p100 X 2

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"

Describe the current behavior
using jupyterlab
import tensorrt
from tensorflow import saved_model
then import saved tensorrtengine ( built from tftrt )
Describe the expected behavior
exit and no errors
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
using tensorflow, from any saved model, import tensorflow tensorrt, convert, build and save
and load from jupyterlab, and then exit jupyterlab
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
&lt;denchmark-code&gt;[C 13:57:47.406 LabApp] Shutdown confirmed
[I 13:57:47.407 LabApp] Shutting down 2 kernels
[I 13:57:47.608 LabApp] Kernel shutdown: ef64bc65-6695-40bf-ab9b-e2c7ff62e1bd
--- Logging error ---
Traceback (most recent call last):
  File "/home/alan/anaconda3/lib/python3.8/logging/__init__.py", line 1084, in emit
    stream.write(msg + self.terminator)
  File "/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py", line 414, in write
    self._schedule_flush()
  File "/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py", line 335, in _schedule_flush
    self.pub_thread.schedule(_schedule_in_thread)
  File "/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py", line 207, in schedule
    f()
  File "/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py", line 334, in _schedule_in_thread
    self._io_loop.call_later(self.flush_interval, self._flush)
  File "/home/alan/anaconda3/lib/python3.8/site-packages/tornado/ioloop.py", line 602, in call_later
    return self.call_at(self.time() + delay, callback, *args, **kwargs)
  File "/home/alan/anaconda3/lib/python3.8/site-packages/tornado/platform/asyncio.py", line 162, in call_at
    return self.asyncio_loop.call_later(
  File "/home/alan/anaconda3/lib/python3.8/asyncio/base_events.py", line 687, in call_later
    timer = self.call_at(self.time() + delay, callback, *args,
  File "/home/alan/anaconda3/lib/python3.8/asyncio/base_events.py", line 698, in call_at
    self._check_closed()
  File "/home/alan/anaconda3/lib/python3.8/asyncio/base_events.py", line 508, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
Call stack:
  File "/home/alan/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/util.py", line 160, in __del__
    log_fn("Unresolved object in checkpoint: {}"
  File "/home/alan/anaconda3/lib/python3.8/site-packages/tensorflow/python/platform/tf_logging.py", line 178, in warning
    get_logger().warning(msg, *args, **kwargs)
Message: 'Unresolved object in checkpoint: (root).trt_engine_resources.TRTEngineOp_0_0._serialized_trt_resource_filename'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/home/alan/anaconda3/lib/python3.8/logging/__init__.py", line 1085, in emit
    self.flush()
  File "/home/alan/anaconda3/lib/python3.8/logging/__init__.py", line 1065, in flush
    self.stream.flush()
  File "/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py", line 357, in flush
    self._flush()
  File "/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py", line 383, in _flush
    self.session.send(self.pub_thread, u'stream', content=content,
  File "/home/alan/anaconda3/lib/python3.8/site-packages/jupyter_client/session.py", line 751, in send
    stream.send_multipart(to_send, copy=copy)
  File "/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py", line 214, in send_multipart
    self.schedule(lambda : self._really_send(*args, **kwargs))
  File "/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py", line 207, in schedule
    f()
  File "/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py", line 214, in &lt;lambda&gt;
    self.schedule(lambda : self._really_send(*args, **kwargs))
  File "/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py", line 222, in _really_send
    self.socket.send_multipart(msg, *args, **kwargs)
AttributeError: 'NoneType' object has no attribute 'send_multipart'
Call stack:
  File "/home/alan/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/util.py", line 168, in __del__
    log_fn(
  File "/home/alan/anaconda3/lib/python3.8/site-packages/tensorflow/python/platform/tf_logging.py", line 178, in warning
    get_logger().warning(msg, *args, **kwargs)
Message: 'A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.'
Arguments: ()
[I 13:57:48.411 LabApp] Kernel shutdown: fd4e7ab1-7f88-485f-9270-296fbbaca6c4
[I 13:57:48.411 LabApp] Shutting down 0 terminals
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='alanpurple' date='2020-10-21T06:14:03Z'>
		&lt;denchmark-link:https://github.com/alanpurple&gt;@alanpurple&lt;/denchmark-link&gt;

Can you please share code snippet to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!
		</comment>
		<comment id='2' author='alanpurple' date='2020-10-28T06:49:20Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='3' author='alanpurple' date='2020-11-04T07:34:04Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='4' author='alanpurple' date='2020-11-04T07:34:07Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44193&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44193&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>