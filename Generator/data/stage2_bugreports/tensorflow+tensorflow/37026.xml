<bug id='37026' author='sam-ulrich1' open_date='2020-02-24T17:49:07Z' closed_time='2020-03-05T18:46:56Z'>
	<summary>Linearly increasing memory with use_multiprocessing and Keras Sequence</summary>
	<description>
System information
OS: Ubuntu 19.04
CPU: Ryzen 2700X
GPUs: 2 X GTX 1080ti
RAM: 32GB


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes


OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 19.04


Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A


TensorFlow installed from (source or binary):
binary


TensorFlow version (use command below):
Current Install: tf-nightly 2.2.0.dev20200218
Occurs On: &gt;= tf 2.0


Python version:
3.6.8


Bazel version (if compiling from source):
N/A


GCC/Compiler version (if compiling from source):
N/A


CUDA/cuDNN version:
CUDA: 10.1
cuDNN: 7.6.5


GPU model and memory:
2 X GTX 1080ti


Describe the current behavior
When using tf.keras.Sequence base class with 'use_multiprocessing' argument, the memory usage increases linearly every epoch until the program fails with a resource allocation error. I initially noticed this when upgrading to TF2.0. I have identified that the KerasSequenceAdapter has the should_recreate_iterator function returning True. If you set this function to return False the memory still increases over epochs but at a drastically slower rate. I plan to continue to debug this behavior and identify the root cause of the problem so that the memory does not increase at every epoch.
Observed Behavior:

With should_recreate_iterator returning True:

Workers are not killed after each epoch leaving the total workers at the end of training worker_count**epoch_count
Memory usage doubles each epoch at the re-initialization of the KerasSequenceAdapter


With should_recreate_iterator returning False:

Workers are reused correctly for each epoch
Memory still increases every epoch but at a much lower rate (5-10%) of total memory allocation



Describe the expected behavior
X workers should be instantiated to fill a queue of size Y. Once an epoch is complete, those workers should either be re-used for the next epoch or killed and replaced with new workers. The memory should reach its' maximum fill (worker_count * queue_size * batch_size * data_size * 4) and remain at this allocation for the duration of training.
Standalone code to reproduce the issue
I will create a minimal reproducible example in the near future
Other info / logs
I have no logs at the moment but am happy to produce them in the near future if it is desired
Notes

I am aware that the expected action for TF.Keras users is to switch to the tf.Dataset API for parallel data processing. I intend to do this at a later date, however, my current data pipeline took a while to get set up and I am not ready to completely re-write it. I am hoping to find a solution for multiprocessing and tf.keras.Sequence in the near term to bridge the gap for people in situations similar to me.
I chose to open this issue in the TF repo because I am using the TF version of Keras and have no idea if this happens on pure Keras

My Questions

Is there a reason KerasSequenceAdapter's should_recreate_iterator function returns True?

	</description>
	<comments>
		<comment id='1' author='sam-ulrich1' date='2020-02-24T18:02:42Z'>
		I have found the following unresolved issues on this topic in the original Keras repo:

keras-team/keras#8668
keras-team/keras#5835

		</comment>
		<comment id='2' author='sam-ulrich1' date='2020-02-26T08:52:26Z'>
		&lt;denchmark-link:https://github.com/sam-ulrich1&gt;@sam-ulrich1&lt;/denchmark-link&gt;
, Could you post the sample standalone code to replicate the reported issue. Thanks!
		</comment>
		<comment id='3' author='sam-ulrich1' date='2020-02-26T14:08:16Z'>
		import numpy as np
np.random.seed(1)
import tensorflow as tf
tf.random.set_seed(2)

from tensorflow.keras.models import Model
from tensorflow.keras.layers import *
from tensorflow.keras.preprocessing.image import ImageDataGenerator


train_dir = "data/train"
test_dir = "data/test"
total_classes = 200


# Define simple image model
model_input = Input((32, 32, 3))

x = Conv2D(128, (7, 7))(model_input)
x = Conv2D(64, (3, 3))(x)
x = Conv2D(32, (3, 3))(x)
x = Flatten()(x)
x = Dense(400, activation="relu")(x)
x = Dense(total_classes, activation="softmax")(x)

model = Model(inputs=model_input, outputs=x)
model.compile(optimizer="adam", loss="categorical_crossentropy")

# Define data prep
train_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    color_mode='rgb',
    target_size=(32, 32),
    batch_size=128,
    class_mode='categorical')

test_generator = test_datagen.flow_from_directory(
    test_dir,
    color_mode='rgb',
    target_size=(32, 32),
    batch_size=128,
    class_mode='categorical')

model.fit(
    train_generator,
    validation_data=test_generator,
    validation_freq=1,
    epochs=10,
    workers=6,
    use_multiprocessing=True
)
With this example, the memory increase is less pronounced but still plainly apparent. The memory allocation on my system starts at ~1.8GB at epoch 1 and increases to ~2.15GB by epoch 10. If you watch the child processes being spawned you can see that each epoch 6 more processes are spawned and the pre-existing processes remain alive.
If you set should_recreate_iterator to return False for KerasSequenceAdapter, the memory starts at ~1.8GB at epoch 1 and increases to ~1.89GB by epoch 10 (much more reasonable). If you watch the child processes being spawned you can see that each epoch the prior 6 processes are killed and a new 6 processes are spawned leaving only 6 processes alive at any given time.
P.S.
Is there a reason that KerasSequenceAdapter is set to be re-created each epoch by default?
I am noticing no detrimental effects by setting  to False
&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='sam-ulrich1' date='2020-03-05T18:46:56Z'>
		In 2.x, we would recommend using tf.data.Datasets, which can handle multiprocessing of input pipelines. See &lt;denchmark-link:https://www.tensorflow.org/guide/data&gt;https://www.tensorflow.org/guide/data&lt;/denchmark-link&gt;
 for more information.
		</comment>
		<comment id='5' author='sam-ulrich1' date='2020-03-05T18:46:58Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37026&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37026&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='sam-ulrich1' date='2020-03-05T20:38:21Z'>
		&lt;denchmark-link:https://github.com/karmel&gt;@karmel&lt;/denchmark-link&gt;
 As I mentioned earlier, I have created a quite complex data pipeline and I'm not able to completely re-write my pipeline at the moment. Considering there are a number of people in a similar position as me and the fix seems very simple, I believe this warrants more consideration than simply closing the issue.
I think it would be reasonable to explore such a simple solution in pursuit of backwards compatibility with TF1.X. Please consider further reviewing the material I presented in the interest of the many people experiencing similar issues.
		</comment>
		<comment id='7' author='sam-ulrich1' date='2020-03-26T08:32:00Z'>
		
```python
import numpy as np
np.random.seed(1)
import tensorflow as tf
tf.random.set_seed(2)

from tensorflow.keras.models import Model
from tensorflow.keras.layers import *
from tensorflow.keras.preprocessing.image import ImageDataGenerator


train_dir = "data/train"
test_dir = "data/test"
total_classes = 200


# Define simple image model
model_input = Input((32, 32, 3))

x = Conv2D(128, (7, 7))(model_input)
x = Conv2D(64, (3, 3))(x)
x = Conv2D(32, (3, 3))(x)
x = Flatten()(x)
x = Dense(400, activation="relu")(x)
x = Dense(total_classes, activation="softmax")(x)

model = Model(inputs=model_input, outputs=x)
model.compile(optimizer="adam", loss="categorical_crossentropy")

# Define data prep
train_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    color_mode='rgb',
    target_size=(32, 32),
    batch_size=128,
    class_mode='categorical')

test_generator = test_datagen.flow_from_directory(
    test_dir,
    color_mode='rgb',
    target_size=(32, 32),
    batch_size=128,
    class_mode='categorical')

model.fit(
    train_generator,
    validation_data=test_generator,
    validation_freq=1,
    epochs=10,
    workers=6,
    use_multiprocessing=True
)
With this example, the memory increase is less pronounced but still plainly apparent. The memory allocation on my system starts at ~1.8GB at epoch 1 and increases to ~2.15GB by epoch 10. If you watch the child processes being spawned you can see that each epoch 6 more processes are spawned and the pre-existing processes remain alive.
If you set should_recreate_iterator to return False for KerasSequenceAdapter, the memory starts at ~1.8GB at epoch 1 and increases to ~1.89GB by epoch 10 (much more reasonable). If you watch the child processes being spawned you can see that each epoch the prior 6 processes are killed and a new 6 processes are spawned leaving only 6 processes alive at any given time.
P.S.
Is there a reason that KerasSequenceAdapter is set to be re-created each epoch by default?
I am noticing no detrimental effects by setting should_recreate_iterator to False
@gadagashwini


Could you please tell how / where are you setting should_recreate_iterator  ? Im having the same issue. Thank you
		</comment>
		<comment id='8' author='sam-ulrich1' date='2020-03-26T12:46:25Z'>
		@stringcode86 In the KerasSequenceAdapter class there is a function named should_recreate_iterator that simply returns True. Change the return value to False
You can find the function here in the current master: 


tensorflow/tensorflow/python/keras/engine/data_adapter.py


         Line 954
      in
      56f4edb






 def should_recreate_iterator(self): 





		</comment>
		<comment id='9' author='sam-ulrich1' date='2020-04-21T09:42:27Z'>
		Found another walkaround:
You can wrap your Sequence inside an OrderedEnqueuer. I dug into the source code an see actually keras use OrderedEnqueuer as a wrapped for the sequence. The OrderedEnqueuer itself doesn't leak so I guess the leak is hidden in the adapter level which is related to tf. So I came up a way below
&lt;denchmark-code&gt;seq = MySequence(...)
enq = OrderedEnqueuer(self, use_multiprocessing=True)
enq.start(workers=10, max_queue_size=20)
gen = enq.get()

model.fit(
                gen,
                epochs=300,
                steps_per_epoch=steps_per_epoch,
                shuffle=False,
                callbacks=callbacks,
                validation_steps=valid_steps_per_epoch,
                validation_data=valid_dataset,
                initial_epoch=0,
                use_multiprocessing=False,
                max_queue_size=10,
                workers=1,
            )
&lt;/denchmark-code&gt;

Make sure you set the use_multiprocessing in fit to False and workers to 1. Tested with tf-2.1.0 and I didn't see memory leak comparing to directly using sequence.
		</comment>
		<comment id='10' author='sam-ulrich1' date='2020-07-24T10:28:38Z'>
		
@stringcode86 In the KerasSequenceAdapter class there is a function named should_recreate_iterator that simply returns True. Change the return value to False
You can find the function here in the current master:



tensorflow/tensorflow/python/keras/engine/data_adapter.py


         Line 954
      in
      56f4edb






 def should_recreate_iterator(self): 






Just a heads up for anyone tempted to use this workaround:
&lt;denchmark-code&gt;import tensorflow.python.keras.engine
tensorflow.python.keras.engine.data_adapter.KerasSequenceAdapter.should_recreate_iterator = lambda _: False
&lt;/denchmark-code&gt;

This did not adequately address the memory growth issue for me (training + validation = 820GB), and it did have an effect on the model.  In particular, the trajectory of the validation loss function over epochs differed by 3 orders of magnitude from anything I'd ever seen before.
		</comment>
	</comments>
</bug>