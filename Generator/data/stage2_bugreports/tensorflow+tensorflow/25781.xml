<bug id='25781' author='wangwenjing' open_date='2019-02-15T12:48:44Z' closed_time='2019-06-06T21:12:05Z'>
	<summary>How can i build static framework for tf-lite under 300kb for iOS?</summary>
	<description>
Please make sure that this is a documentation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template
System information

TensorFlow version: master
Doc Link:https://www.tensorflow.org/lite/overview

Describe the documentation issue
Dear supporter,
I found "Smaller in size: TensorFlow Lite is smaller than 300KB when all supported operators are linked and less than 200KB when using only the operators needed for supporting InceptionV3 and Mobilenet." under TensorFlow Lite highlights.
I got libtensorflow-lite.a for each archtecture afte using build_ios_universal_lib.sh script. Binary size is far bigger, around 7.5MB. How can I get one smaller than 300KB?  Do I need to do any optimization on compiling and how?
	</description>
	<comments>
		<comment id='1' author='wangwenjing' date='2019-02-19T12:28:44Z'>
		I have the same problem，has been looking for a solution. I only need text forecast function, so that can reduce the size of the package?Or is there any way to reduce the size of the package ？
		</comment>
		<comment id='2' author='wangwenjing' date='2019-02-22T03:51:53Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
  Do we got any progress on this issue?  I still had no clue on this problem.
		</comment>
		<comment id='3' author='wangwenjing' date='2019-02-27T08:04:00Z'>
		&lt;denchmark-link:https://github.com/aselle&gt;@aselle&lt;/denchmark-link&gt;
  ?
		</comment>
		<comment id='4' author='wangwenjing' date='2019-03-12T17:40:15Z'>
		Have you looked at the tensorflow lite micro code?  tensorflow/lite/experimental/micro  Targeted at very small footprint implementations, but very new (and incomplete) at this point.  I'm not sure how to build a library of the size indicated in the docs.  Maybe &lt;denchmark-link:https://github.com/jdduke&gt;@jdduke&lt;/denchmark-link&gt;
 can comment?
		</comment>
		<comment id='5' author='wangwenjing' date='2019-03-12T17:48:55Z'>
		When you say "binary size" is bigger, are you measuring the size impact on the actual iOS app? Or just the static library? Looking at just the static library size is misleading, as it ignores all the size reduction that will occur from op stripping (selective registration) as well as symbol stripping (which is controlled by your build options).
See &lt;denchmark-link:https://www.tensorflow.org/lite/guide/inference#customizing_the_kernel_library&gt;this guide&lt;/denchmark-link&gt;
 for information on how to trim some of the builtin operators. If you know exactly which ops are used by your model, you can create a custom  and register only the ops you need. When your app is actually built/linked, it should trim away unused code.
		</comment>
		<comment id='6' author='wangwenjing' date='2019-03-15T02:48:06Z'>
		&lt;denchmark-link:https://github.com/jdduke&gt;@jdduke&lt;/denchmark-link&gt;
  hi, I mean static library.  there is no size impact on actual iOS app if I only drag the libtensorflow-lite.a into my Xcode project without using it. How can I quick test the size impact as all iOS examples using downloaded .framework ?   when the "selective registration" happens , who trigger it and how does it work?  which option you mean when you say "controlled by your build options", tf-lite build option? Xcode build option?
		</comment>
		<comment id='7' author='wangwenjing' date='2019-05-29T23:49:14Z'>
		The guide is pretty vague on how to trim the built-in operators; is there more specific documentation particularly for iOS/android? The size impact on the actual iOS app is about 1.2 MB just for the framework, so trimming it down would be very useful.
		</comment>
		<comment id='8' author='wangwenjing' date='2019-05-30T23:15:21Z'>
		Agreed that the documentation could be more explicit. You can start by creating a , and then adding only the kernels used by your model. There's an example of doing this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/models/smartreply/predictor.cc#L76&gt;here&lt;/denchmark-link&gt;
, which takes advantage of a bazel build rule for generating only the ops used by a specific model. If you can't use the bazel build rule to generate that code, you'll have to manually add each op yourself (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/register.cc&gt;this code&lt;/denchmark-link&gt;
 demonstrates how this is done for the ).
		</comment>
		<comment id='9' author='wangwenjing' date='2019-05-30T23:26:09Z'>
		Thanks &lt;denchmark-link:https://github.com/jdduke&gt;@jdduke&lt;/denchmark-link&gt;
 I'll try that out
		</comment>
		<comment id='10' author='wangwenjing' date='2019-05-31T20:45:51Z'>
		Worked great! Thanks so much for your help &lt;denchmark-link:https://github.com/jdduke&gt;@jdduke&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='wangwenjing' date='2019-06-06T21:12:05Z'>
		Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!
		</comment>
		<comment id='12' author='wangwenjing' date='2019-06-06T21:12:06Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=25781&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=25781&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='wangwenjing' date='2019-08-23T14:04:56Z'>
		&lt;denchmark-link:https://github.com/jdduke&gt;@jdduke&lt;/denchmark-link&gt;
 fellowed by your example，got “l17_ops_registration.cc” which contains only 12 operators .  but it seems not reduce library size when i include it . How to use it properly? thanks!
		</comment>
		<comment id='14' author='wangwenjing' date='2019-08-23T15:23:52Z'>
		So, using the MutableOpResolver approach won't change the size of the static library, but it should shrink the size of the actual binary that gets produced in an app, as the unneeded code/symbols can be stripped.
		</comment>
		<comment id='15' author='wangwenjing' date='2019-08-25T09:41:46Z'>
		
So, using the MutableOpResolver approach won't change the size of the static library, but it should shrink the size of the actual binary that gets produced in an app, as the unneeded code/symbols can be stripped.

Oh，is there another way to reduce static library?
here is my BUILD file.  Only a "cc_binary + static" .so can be loaded ok in unity(c#).

cc_binary(
name = "Prophet.so",
srcs = ["minimal.h", "Prophet.cc",   ],
linkopts = tflite_linkopts() + select({
"//tensorflow:android": [
"-pie",  # Android 5.0 and later supports only PIE
"-lm",  # some builtin ops, e.g., tanh, need -lm
],
"//conditions:default": [],
}),
deps = [
"//tensorflow/lite:framework",
"//tensorflow/lite/c:c_api_internal",
"//tensorflow/lite/kernels:builtin_ops",
"//tensorflow/lite:builtin_op_data",
"//tensorflow/lite/schema:schema_fbs",
],
linkshared = True,
)

		</comment>
		<comment id='16' author='wangwenjing' date='2019-08-26T16:39:26Z'>
		To reduce the static build you would either need to create your own cc_library target with just the kernel dependencies you need (see this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/cc35420e4a615fa635a5302e476ba2a6118daa62/tensorflow/lite/kernels/BUILD#L376&gt;build rule&lt;/denchmark-link&gt;
), or modify that  target directly for just the kernels you need, and also update the BuiltinOpResolver in the  target.
		</comment>
		<comment id='17' author='wangwenjing' date='2019-09-06T09:16:17Z'>
		fellow your guide, i modified"tensorflow/tensorflow/lite/kernels/register.cc ",and BUILD target "builtin_op_kernels". operators decrease from 88 to 77，lib size reduces from 3.7m to 3.4m。
Is there a powerful  way to reduce size?
it seems like whole dependicies are compiled. Is there efficient way to strip unused symbols automately？ What does below mean?

def tflite_linkopts_unstripped():
"""Defines linker flags to reduce size of TFLite binary.

&lt;denchmark-code&gt;   These are useful when trying to investigate the relative size of the
   symbols in TFLite.

Returns:
   a select object with proper linkopts
"""

# In case you wonder why there's no --icf is because the gains were
# negligible, and created potential compatibility problems.
return select({
    "//tensorflow:android": [
        "-Wl,--no-export-dynamic",  # Only inc syms referenced by dynamic obj.
        "-Wl,--gc-sections",  # Eliminate unused code and data.
        "-Wl,--as-needed",  # Don't link unused libs.
    ],
    "//conditions:default": [],
})
&lt;/denchmark-code&gt;


To reduce the static build you would either need to create your own cc_library target with just the kernel dependencies you need (see this build rule), or modify that builtin_op_kernels target directly for just the kernels you need, and also update the BuiltinOpResolver in the builtin_ops target.

		</comment>
		<comment id='18' author='wangwenjing' date='2019-09-07T11:03:01Z'>
		
Agreed that the documentation could be more explicit. You can start by creating a MutableOpResolver, and then adding only the kernels used by your model. There's an example of doing this here, which takes advantage of a bazel build rule for generating only the ops used by a specific model. If you can't use the bazel build rule to generate that code, you'll have to manually add each op yourself (this code demonstrates how this is done for the BuiltinOpResolver).

why cann't recently(8gb memory) on fresh source?


bazel build tensorflow/python/tools:print_selective_registration_header &amp;&amp; \
bazel-bin/tensorflow/python/tools/print_selective_registration_header \
--graphs=models_arm64/1.pb &gt; ops_to_register.h

&lt;long int, 1&gt;; int packet_size = 4; bool inner_dim_contiguous = false; bool inner_dim_reordered = true; int Alignment = 0]':
./tensorflow/core/kernels/eigen_spatial_convolutions.h:308:11: warning: 'orig_r' may be used uninitialized in this function [-Wmaybe-uninitialized]
./tensorflow/core/kernels/eigen_spatial_convolutions.h:308:11: warning: 'orig_r' may be used uninitialized in this function [-Wmaybe-uninitialized]
gcc: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See file:///usr/share/doc/gcc-7/README.Bugs for instructions.
Target //tensorflow/python/tools:print_selective_registration_header failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 724.181s, Critical Path: 123.60s
INFO: 2191 processes: 2191 local.
FAILED: Build did NOT complete successfully

		</comment>
	</comments>
</bug>