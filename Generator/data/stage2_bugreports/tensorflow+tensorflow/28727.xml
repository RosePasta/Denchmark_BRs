<bug id='28727' author='NEU-Gou' open_date='2019-05-15T06:36:02Z' closed_time='2019-06-06T04:59:15Z'>
	<summary>Converting error for quantize aware trained tf.keras.applications.MobileNetV2</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): r1.13
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.0
GPU model and memory: 2080Ti

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
During the converting the quantize aware trained mobilentnetv2 model from tf.keras, it will raise the follow error massage
&lt;denchmark-code&gt;F tensorflow/lite/toco/tooling_util.cc:1702] Array expanded_conv_project_BN/FusedBatchNorm, which is an input to the Conv operator producing the output array block_1_expand_relu/Relu6, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.
Aborted (core dumped)
&lt;/denchmark-code&gt;

Describe the expected behavior
It works for model built with tf.contrib.slim. I feels like the tf.contrib.quantize.create_eval_graph() doesn't support BN layer from tf.keras.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
&lt;denchmark-code&gt;import tensorflow as tf
working_path='/tmp/tflite'
tf.keras.backend.set_learning_phase(1)
inputs = tf.keras.Input(shape=(224, 224, 3), name='input')
y_true = tf.keras.Input(shape=[1000], name='label')
y_pred = keras_model.output

loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)

# quant aware training
graph = tf.get_default_graph()
tf.contrib.quantize.create_training_graph(input_graph=graph, quant_delay=0)

train_step = tf.train.GradientDescentOptimizer(learning_rate=0.00625).minimize(loss)
saver = tf.train.Saver()
with tf.Session() as sess:
    _input = np.random.rand(10, 224, 224, 3)
    _label = np.zeros([10, 1000])
    _label[:, 2] = np.ones(10)
    sess.run(tf.global_variables_initializer())
    for i in range(3):
        _, _loss = sess.run([train_step, loss], feed_dict={inputs: _input,
                                                           y_true: _label})
        print(_loss)
    # save
    saver.save(sess, os.path.join(working_path, 'checkpoints/model.ckpt'))

'''load and convert to TFLite'''
tf.reset_default_graph()
tf.keras.backend.set_learning_phase(0)
inputs = tf.keras.Input(shape=(224, 224, 3), name='input')
keras_model = tf.keras.applications.MobileNetV2(input_tensor=inputs, alpha=1.0, weights=None, include_top=True)
output = keras_model.output

# insert fake quant nodes
graph = tf.get_default_graph()
tf.contrib.quantize.create_eval_graph(graph)
saver = tf.train.Saver()

with tf.Session(graph=graph) as sess:
    sess.run(tf.global_variables_initializer())
    saver.restore(sess, tf.train.latest_checkpoint(os.path.join(working_path, 'checkpoints/')))

    # freeze graph
    graph_def = graph.as_graph_def()
    froze_graph = tf.graph_util.convert_variables_to_constants(sess, graph_def, [output.op.name])
    tf.io.write_graph(froze_graph, working_path, 'freeze_graph.pb')

# convert to TFLite
graph_def_file = os.path.join(working_path, 'freeze_graph.pb')
input_array = ["input"]
converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_array, [output.op.name],
                                                      input_shapes={"input": [1, 224, 224, 3]})

converter.inference_type = tf.lite.constants.QUANTIZED_UINT8
converter.inference_input_type = tf.lite.constants.QUANTIZED_UINT8
converter.quantized_input_stats = {"input": (0., 255.)}
tfmodel = converter.convert()
open(os.path.join(working_path, "converted_model.tflite"), "wb").write(tfmodel)
&lt;/denchmark-code&gt;

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/home/mgou/projects/tf-lightweight-yolov3/backbone_pretrain/train_recog_quant.py", line 91, in &lt;module&gt;
    tfmodel = converter.convert()
  File "/home/mgou/virtualenv/tf113-py36/lib/python3.6/site-packages/tensorflow/lite/python/lite.py", line 455, in convert
    **converter_kwargs)
  File "/home/mgou/virtualenv/tf113-py36/lib/python3.6/site-packages/tensorflow/lite/python/convert.py", line 442, in toco_convert_impl
    input_data.SerializeToString())
  File "/home/mgou/virtualenv/tf113-py36/lib/python3.6/site-packages/tensorflow/lite/python/convert.py", line 205, in toco_convert_protos
    "TOCO failed. See console for info.\n%s\n%s\n" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2019-05-15 02:27:33.413703: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1161 operators, 1728 arrays (0 quantized)
2019-05-15 02:27:33.440573: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1161 operators, 1728 arrays (0 quantized)
2019-05-15 02:27:33.608574: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 137 operators, 260 arrays (1 quantized)
2019-05-15 02:27:33.610239: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 137 operators, 260 arrays (1 quantized)
2019-05-15 02:27:33.610999: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 76 operators, 199 arrays (1 quantized)
2019-05-15 02:27:33.611690: F tensorflow/lite/toco/tooling_util.cc:1702] Array expanded_conv_project_BN/FusedBatchNorm, which is an input to the Conv operator producing the output array block_1_expand_relu/Relu6, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.
Aborted (core dumped)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='NEU-Gou' date='2019-06-05T18:05:12Z'>
		Hi Mengran,
Yes the contrib.quantize tool may not be able to handle models built by Keras. In the meantime, we are building a replacement that is tightly integrated with tf.keras, as shown on our roadmap (&lt;denchmark-link:https://www.tensorflow.org/model_optimization/guide/roadmap&gt;https://www.tensorflow.org/model_optimization/guide/roadmap&lt;/denchmark-link&gt;
). This is targeting Q3 and the tf.keras.applications.MobilenetV2 is one of the models we'll ensure support for
		</comment>
		<comment id='2' author='NEU-Gou' date='2019-06-06T04:16:45Z'>
		Thank you for your answer! Looking forward for the new quantization toolbox. It's will be very helpful!
		</comment>
		<comment id='3' author='NEU-Gou' date='2019-06-06T04:59:16Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=28727&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=28727&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>