<bug id='31871' author='KichangKim' open_date='2019-08-21T23:05:37Z' closed_time='2019-11-12T22:21:50Z'>
	<summary>TF 2.0 dev20190820 consumes more memory than dev20190504 when training keras model using train_on_batch()</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64 1903
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): 2.0.0-dev20190820
Python version: 3.6.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: CUDA 10, cuDNN 7.6.2.24
GPU model and memory: Geforce RTX 2080 ti 11GB

Describe the current behavior
Program crashes due to insufficient memory issue. But with same code, dev20190504 can train the model without any problem.
Describe the expected behavior
Can train large model using dev20190820 without memory issue, like dev20190504.
Code to reproduce the issue
&lt;denchmark-code&gt;import os
import tensorflow as tf
import numpy as np

os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'


def conv(x, filters, kernel_size, strides=(1, 1), padding='same', initializer='he_normal'):
    c = tf.keras.layers.Conv2D(
        filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_initializer=initializer, use_bias=False)(x)

    return c


def conv_bn(x, filters, kernel_size, strides=(1, 1), padding='same', initializer='he_normal', bn_gamma_initializer='ones'):
    c = conv(x, filters=filters, kernel_size=kernel_size,
             strides=strides, padding=padding, initializer=initializer)

    c_bn = tf.keras.layers.BatchNormalization(
        gamma_initializer=bn_gamma_initializer)(c)

    return c_bn


def conv_bn_relu(x, filters, kernel_size, strides=(1, 1), padding='same', initializer='he_normal', bn_gamma_initializer='ones'):
    c_bn = conv_bn(x, filters=filters, kernel_size=kernel_size, strides=strides, padding=padding,
                   initializer=initializer, bn_gamma_initializer=bn_gamma_initializer)

    return tf.keras.layers.Activation('relu')(c_bn)


def conv_gap(x, output_filters, kernel_size=(1, 1)):
    x = conv(x, filters=output_filters, kernel_size=kernel_size)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)

    return x


def my_block(x, output_filters, inter_filters):
    c1 = conv_bn_relu(x, inter_filters, (1, 1))
    c2 = conv_bn_relu(c1, inter_filters, (3, 3))
    c3 = conv_bn(
        c2, output_filters, (1, 1), bn_gamma_initializer='zeros')

    p = tf.keras.layers.add([c3, x])

    return tf.keras.layers.Activation('relu')(p)


def my_block_inc(x, output_filters, inter_filters, strides1x1=(1, 1), strides2x2=(2, 2)):
    c1 = conv_bn_relu(
        x, inter_filters, (1, 1), strides=strides1x1)
    c2 = conv_bn_relu(
        c1, inter_filters, (3, 3), strides=strides2x2)
    c3 = conv_bn(
        c2, output_filters, (1, 1), bn_gamma_initializer='zeros')

    strides = np.multiply(strides1x1, strides2x2)
    s = conv_bn(
        x, output_filters, (1, 1), strides=strides)  # shortcut

    p = tf.keras.layers.add([c3, s])

    return tf.keras.layers.Activation('relu')(p)


def repeat_blocks(x, block_delegate, count, **kwargs):
    assert count &gt;= 0

    for _ in range(count):
        x = block_delegate(x, **kwargs)
    return x


# This line makes trick!
tf.keras.backend.set_learning_phase(1)
shape = (299, 299, 3)

inputs = tf.keras.Input(shape, dtype='float32')
outputs = conv_bn_relu(inputs, 256 // 4, (7, 7), strides=(2, 2))
outputs = tf.keras.layers.MaxPool2D(
    (3, 3), strides=(2, 2), padding='same')(outputs)
outputs = my_block_inc(outputs, 256, 256 // 4, strides2x2=(1, 1))
outputs = repeat_blocks(outputs, block_delegate=my_block,
                        count=2,
                        output_filters=256,
                        inter_filters=256 // 4)

outputs = my_block_inc(outputs, 512, 512 // 4, strides2x2=(2, 2))
outputs = repeat_blocks(outputs, block_delegate=my_block,
                        count=7,
                        output_filters=512,
                        inter_filters=512 // 4)

outputs = my_block_inc(outputs, 1024, 1024 // 4, strides2x2=(2, 2))
outputs = repeat_blocks(outputs, block_delegate=my_block,
                        count=40,
                        output_filters=1024,
                        inter_filters=1024 // 4)

outputs = my_block_inc(outputs, 1024, 1024 // 4, strides2x2=(2, 2))
outputs = repeat_blocks(outputs, block_delegate=my_block,
                        count=16,
                        output_filters=1024,
                        inter_filters=1024 // 4)

outputs = my_block_inc(outputs, 1024, 1024 // 4, strides2x2=(2, 2))
outputs = repeat_blocks(outputs, block_delegate=my_block,
                        count=16,
                        output_filters=1024,
                        inter_filters=1024 // 4)

outputs = my_block_inc(outputs, 2048, 2048 // 4, strides2x2=(2, 2))
outputs = repeat_blocks(outputs, block_delegate=my_block,
                        count=6,
                        output_filters=2048,
                        inter_filters=2048 // 4)

outputs = conv_gap(outputs, 1024)
outputs = tf.keras.layers.Activation('sigmoid')(outputs)
model = tf.keras.models.Model(
    inputs=inputs, outputs=outputs, name='resnet_custom_v2')

optimizer = tf.optimizers.Adam(0.001)
model.compile(optimizer=optimizer, loss='binary_crossentropy',
              metrics=['mean_absolute_error'])


def make_batch(_):
    x = np.ones((shape[0], shape[1], shape[2]), dtype='float32')
    y = np.ones((1024), dtype='float32')

    return (x, y)


dataset = tf.data.Dataset.from_tensor_slices(
    ['0'] * 40)
dataset = dataset.map(lambda x: tf.py_function(
    make_batch, (x,), (tf.float32, tf.float32)))
dataset = dataset.batch(20)

for x, y in dataset:
    step_results = model.train_on_batch(x=x, y=y)
    print(f'loss={step_results[0]}')

print('press input key')
input()

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='KichangKim' date='2019-08-22T09:16:04Z'>
		I have tried on Colab with TF version 2.0.0-dev20190820  and was able to reproduce the issue.I am able to train the model using 2.0.0-dev20190504 without any problem.Please, find the &lt;denchmark-link:https://colab.research.google.com/drive/1y4Z77hmagOig2fqVN1tnp2vOnXHdhuZg&gt;gist &lt;/denchmark-link&gt;
here.Thanks!
		</comment>
		<comment id='2' author='KichangKim' date='2019-09-12T23:02:48Z'>
		tensorflow-gpu==2.0rc1 still has this issue.
		</comment>
		<comment id='3' author='KichangKim' date='2019-09-18T12:36:59Z'>
		&lt;denchmark-link:https://github.com/robieta&gt;@robieta&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 Is there any news for this issue? I can't test latest RC release due to this problem.
		</comment>
		<comment id='4' author='KichangKim' date='2019-09-29T01:07:49Z'>
		tensorflow-gpu==2.0rc2 still has this issue.
		</comment>
		<comment id='5' author='KichangKim' date='2019-09-30T23:09:50Z'>
		tensorflow-gpu==2.0.0 still suffers this issue.
		</comment>
		<comment id='6' author='KichangKim' date='2019-09-30T23:20:45Z'>
		&lt;denchmark-link:https://github.com/robieta&gt;@robieta&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 TF 2.0.0 is released today, and this issue sill is there. Any news? or is it not a bug, but TF2.0's intended behaviour?
		</comment>
		<comment id='7' author='KichangKim' date='2019-11-12T20:30:18Z'>
		This should be fixed by &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/28d07261622a589b7f427756677c2178ccaca7fb&gt;28d0726&lt;/denchmark-link&gt;
. Can you check whether it resolves your issue?
		</comment>
		<comment id='8' author='KichangKim' date='2019-11-12T22:07:49Z'>
		&lt;denchmark-link:https://github.com/robieta&gt;@robieta&lt;/denchmark-link&gt;
 I can confirm that when I ran &lt;denchmark-link:https://github.com/KichangKim&gt;@KichangKim&lt;/denchmark-link&gt;
 code with , code runs without any issue. &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/5e8af6cf47aee62ec1596328d8fd84ba/untitled134.ipynb&gt;Here&lt;/denchmark-link&gt;
 is the gist. Thanks!
		</comment>
		<comment id='9' author='KichangKim' date='2019-11-12T22:21:50Z'>
		I checked tf-nightly-gpu==2.1.0.dev20191112 and it worked correctly. Thanks.
		</comment>
		<comment id='10' author='KichangKim' date='2019-11-12T22:21:52Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31871&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31871&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>