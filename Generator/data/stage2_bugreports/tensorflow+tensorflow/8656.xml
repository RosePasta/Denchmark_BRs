<bug id='8656' author='matthiasreisser' open_date='2017-03-23T12:34:40Z' closed_time='2017-03-25T16:39:36Z'>
	<summary>Inconsistent behaviour between CPU and GPU gradient step operation</summary>
	<description>
&lt;denchmark-h:h3&gt;What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?&lt;/denchmark-h&gt;

None
&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System: Linux Mint 17.2 Rafaela
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
ls -l /usr/local/cuda/lib64/libcud*
/usr/local/cuda/lib64/libcudadevrt.a
/usr/local/cuda/lib64/libcudart.so -&gt; libcudart.so.8.0
/usr/local/cuda/lib64/libcudart.so.8.0 -&gt; libcudart.so.8.0.61
/usr/local/cuda/lib64/libcudart.so.8.0.61
/usr/local/cuda/lib64/libcudart_static.a
/usr/local/cuda/lib64/libcudnn.so
/usr/local/cuda/lib64/libcudnn.so.5
/usr/local/cuda/lib64/libcudnn.so.5.1.10
/usr/local/cuda/lib64/libcudnn_static.a

The output from python -c "import tensorflow; print(tensorflow.__version__)".
1.1.0-rc0
This bug appeared also on the current TF 1.0 Release when installed via pip install tensorflow-gpu

If installed from source, provide

The commit hash (git rev-parse HEAD)
git rev-parse HEAD: 49380d6
The output of bazel version
Build label: 0.4.5

&lt;denchmark-h:h3&gt;If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import os
os.environ["CUDA_VISIBLE_DEVICES"] = ""

import tensorflow as tf
a = tf.Variable(1.0)
loss = (a-2.0)**2
optimizer = tf.train.GradientDescentOptimizer(1.0)
train_op = optimizer.minimize(loss)

sess = tf.Session()
sess.run(tf.global_variables_initializer())
print(sess.run([train_op,a]))
print(sess.run(a))
&lt;/denchmark-code&gt;

The two print statements evaluate to
&lt;denchmark-code&gt;[None, 3.0]
3.0
&lt;/denchmark-code&gt;

When allowing GPU computation to happen by commenting out the second line above, the two print statements evaluate to:
&lt;denchmark-code&gt;[None, 1.0]
3.0
&lt;/denchmark-code&gt;

So apparently when using the GPU, the Variable a is evaluated before the gradient op is executed - and the other way around on CPU. I am not entirely sure what the desired behaviour is supposed to be, but I'm pretty sure they should not be inconsistent.
&lt;denchmark-h:h3&gt;What other attempted solutions have you tried?&lt;/denchmark-h&gt;

A few things I have observed:
&lt;denchmark-code&gt;import os
# os.environ["CUDA_VISIBLE_DEVICES"] = ""

import tensorflow as tf

with tf.device("/cpu:0"):
    a = tf.Variable(1.0)
loss = (a-2.0)**2
optimizer = tf.train.GradientDescentOptimizer(1.0)
train_op = optimizer.minimize(loss)
init_op = tf.global_variables_initializer()

sess = tf.Session()
sess.run(init_op)
print(sess.run([train_op,a]))
print(sess.run(a))
&lt;/denchmark-code&gt;

evaluates to
&lt;denchmark-code&gt;[None, 3.0]
3.0
&lt;/denchmark-code&gt;

The following code
&lt;denchmark-code&gt;import os
# os.environ["CUDA_VISIBLE_DEVICES"] = ""

import tensorflow as tf

a = tf.Variable(1.0)
loss = (a-2.0)**2
optimizer = tf.train.GradientDescentOptimizer(1.0)
train_op = optimizer.minimize(loss)
init_op = tf.global_variables_initializer()
with tf.control_dependencies([train_op]):
    a = tf.identity(a)
sess = tf.Session()
sess.run(init_op)
print(sess.run(a))
&lt;/denchmark-code&gt;

evaluates to
3.0. This seems to "enforce" the behaviour of CPU-only computation when using the GPU.
Also the initial example has been run on three different machines, all with the same TitanX GPU Model.
What am I missing? Any help would be greatly appreciated.
Matthias
	</description>
	<comments>
		<comment id='1' author='matthiasreisser' date='2017-03-24T18:28:46Z'>
		That's not good. I might take a look later.
&lt;denchmark-link:https://github.com/rmlarsen&gt;@rmlarsen&lt;/denchmark-link&gt;
 FYI
&lt;denchmark-link:https://github.com/gunan&gt;@gunan&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='matthiasreisser' date='2017-03-25T04:55:09Z'>
		I think this is not a bug:  Consider what the data flow graph looks like, and the fact that fetching [train_op, a] says nothing about which value of 'a' should be fetched: the one before train_op complete or after it completes (fetching [train_op, a] is the same as [a, train_op]).  Because there is no contract, there is no inconsistency.  Due to the timing of the execution of the graphs, my guess is you're likely to get one answer more than another in a given configuration, but that's not a guarantee.
The control dependency enforces the order, so I'd expect that to be consistent no matter what devices ops are on.
At least, that's what I am interpreting from the code.
		</comment>
		<comment id='3' author='matthiasreisser' date='2017-03-25T16:39:36Z'>
		&lt;denchmark-link:https://github.com/vrv&gt;@vrv&lt;/denchmark-link&gt;
 is right, the code is not the same! You need to have the same . Closing. Feel free to reopen if the error persists.
		</comment>
		<comment id='4' author='matthiasreisser' date='2017-03-26T11:29:19Z'>
		I see. By not exactly specifying the computation graph, there is ambiguity.
Still I wonder if this ambiguity should be resolved the same way in both configurations.
		</comment>
	</comments>
</bug>