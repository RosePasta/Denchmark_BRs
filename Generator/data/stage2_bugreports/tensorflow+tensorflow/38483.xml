<bug id='38483' author='zzj0402' open_date='2020-04-13T06:24:54Z' closed_time='2020-06-11T23:26:36Z'>
	<summary>How to store tf dataset object to file?</summary>
	<description>
&lt;denchmark-h:h2&gt;URL(s) with the issue:&lt;/denchmark-h&gt;

&lt;denchmark-link:https://www.tensorflow.org/guide/data&gt;https://www.tensorflow.org/guide/data&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Description of issue (what needs changing):&lt;/denchmark-h&gt;

How to store tf.dataset object to file?
For instance,
&lt;denchmark-code&gt;dataset1 = tf.data.Dataset.from_tensor_slices(
    tf.random.uniform([4, 10], minval=1, maxval=10, dtype=tf.int32))
dataset1
&lt;/denchmark-code&gt;

How to store the dataset1 to file?
&lt;denchmark-h:h3&gt;Clear description&lt;/denchmark-h&gt;

For me, a saved copy of tokenized dataset saves lot of training time.
from transformers import AlbertTokenizer
import tensorflow as tf
import DataReader
import Tokenizer


def encode(type, dataPath='./qgdata/nq-train-sample.json'):
    entries = DataReader.read(dataPath)
    encoding = []
    for entry in entries:
        if type == 'context':
            context = Tokenizer.encode(
                entry['passage'], entry['answer'], entry['question'], True)
            encoding.append(context)
        else:
            question = Tokenizer.encode(
                entry['passage'], entry['answer'], entry['question'], False)
            encoding.append(question)
    data = tf.data.Dataset.from_generator(
        lambda: encoding, tf.int64, output_shapes=512)
    return data


def make_dataset(dataPath='./qgdata/nq-train-sample.json', batch_size=1):
    contextData = encode('context', dataPath)
    questionData = encode('question', dataPath)
    dataset = tf.data.Dataset.zip((contextData, questionData))
    return dataset.batch(batch_size)
Instead of running this batching script before each training, it would be very efficient to store the tokenzied dataset object to file and avoid retokenizing.
&lt;denchmark-h:h3&gt;Usage example&lt;/denchmark-h&gt;

Maybe like:
dataset1 = tf.data.Dataset.from_tensor_slices(
    tf.random.uniform([4, 10], minval=1, maxval=10, dtype=tf.int32))
dataset1.save_dataset(path_to_store)
	</description>
	<comments>
		<comment id='1' author='zzj0402' date='2020-04-13T13:24:50Z'>
		&lt;denchmark-link:https://github.com/zzj0402&gt;@zzj0402&lt;/denchmark-link&gt;
, please refer to &lt;denchmark-link:https://www.tensorflow.org/tutorials/load_data/tfrecord#tfrecord_files_using_tfdata&gt;this doc&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='zzj0402' date='2020-04-13T19:16:28Z'>
		The answer from &lt;denchmark-link:https://github.com/ashutosh1919&gt;@ashutosh1919&lt;/denchmark-link&gt;
 is the idiomatic solution for storing a dataset in a file.
		</comment>
		<comment id='3' author='zzj0402' date='2020-04-16T00:09:32Z'>
		
this doc

&lt;denchmark-code&gt;filename = 'test.tfrecord'
writer = tf.data.experimental.TFRecordWriter(filename)
writer.write(serialized_features_dataset)
&lt;/denchmark-code&gt;

Would it work on any kind of tf.dataset? I think that tutorial is very poorly organized. It's separated from the dataset doc and majority of that doc is talking about making a tf.dataset.
		</comment>
		<comment id='4' author='zzj0402' date='2020-04-16T04:59:40Z'>
		I find this doc relevant: &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/data/experimental/TFRecordWriter&gt;https://www.tensorflow.org/api_docs/python/tf/data/experimental/TFRecordWriter&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='zzj0402' date='2020-04-16T23:10:46Z'>
		&lt;denchmark-h:h2&gt;Save/Load functions&lt;/denchmark-h&gt;

def save(dataset, location='data/tf-records/'):
    dataset = dataset.map(tf.io.serialize_tensor)
    writer = tf.data.experimental.TFRecordWriter(location)
    writer.write(dataset)
    return location


def load(tf_record='data/tf-records/'):
    dataset = tf.data.TFRecordDataset(tf_record)
    dataset = dataset.map(lambda x: tf.io.parse_tensor(x, tf.int64))
    return dataset
&lt;denchmark-h:h2&gt;Dataset&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;&lt;BatchDataset shapes: ({input_ids: (None, 512), attention_mask: (None, 512), token_type_ids: (None, 512), position_ids: (None, 512)}, (None, 512)), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32, position_ids: tf.int32}, tf.int32)&gt;
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Reproduction Code&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;        dataset = DataPatcher.make_dataset()
        model = Training.initial_model()
        path=DataPatcher.save(dataset)
        dataset_loaded = DataPatcher.load()
        Training.fit_model(dataset_loaded, model, 1, 'sample')
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Error&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/Volumes/SAMSUNG_T5/natural-question-generation/test/test_dataset.py", line 71, in test_loading
    DataPatcher.save(dataset)
  File "/Volumes/SAMSUNG_T5/natural-question-generation/DataPatcher.py", line 96, in save
    dataset = dataset.map(tf.io.serialize_tensor)
  File "/Users/zijingzhang/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/data/ops/dataset_ops.py", line 1588, in map
    return MapDataset(self, map_func, preserve_cardinality=True)
  File "/Users/zijingzhang/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/data/ops/dataset_ops.py", line 3888, in __init__
    use_legacy_function=use_legacy_function)
  File "/Users/zijingzhang/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/data/ops/dataset_ops.py", line 3147, in __init__
    self._function = wrapper_fn._get_concrete_function_internal()
  File "/Users/zijingzhang/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/eager/function.py", line 2395, in _get_concrete_function_internal
    *args, **kwargs)
  File "/Users/zijingzhang/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/eager/function.py", line 2389, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File "/Users/zijingzhang/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/eager/function.py", line 2703, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/Users/zijingzhang/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/eager/function.py", line 2593, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File "/Users/zijingzhang/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/framework/func_graph.py", line 978, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/Users/zijingzhang/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/data/ops/dataset_ops.py", line 3140, in wrapper_fn
    ret = _wrapper_helper(*args)
  File "/Users/zijingzhang/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/data/ops/dataset_ops.py", line 3082, in _wrapper_helper
    ret = autograph.tf_convert(func, ag_ctx)(*nested_args)
  File "/Users/zijingzhang/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/autograph/impl/api.py", line 237, in wrapper
    raise e.ag_error_metadata.to_exception(e)
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: in converted code:

    /Users/zijingzhang/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/ops/gen_parsing_ops.py:2236 serialize_tensor
        "SerializeTensor", tensor=tensor, name=name)
    /Users/zijingzhang/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/framework/op_def_library.py:351 _apply_op_helper
        with g.as_default(), ops.name_scope(name) as scope:
    /Users/zijingzhang/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/framework/ops.py:6249 __enter__
        return self._name_scope.__enter__()
    /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/contextlib.py:81 __enter__
        return next(self.gen)
    /Users/zijingzhang/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/framework/ops.py:4010 name_scope
        if name:
    /Users/zijingzhang/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/framework/ops.py:757 __bool__
        self._disallow_bool_casting()
    /Users/zijingzhang/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/framework/ops.py:520 _disallow_bool_casting
        "using a `tf.Tensor` as a Python `bool`")
    /Users/zijingzhang/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/framework/ops.py:505 _disallow_when_autograph_disabled
        " Try decorating it directly with @tf.function.".format(task))

    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph is disabled in this function. Try decorating it directly with @tf.function.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='zzj0402' date='2020-05-04T23:20:36Z'>
		Honestly, I didn't understand the documentation at all. I'm trying to find a way to save the modified dataset as a tfrecord file as well. I will be doing lots of filtering on the data, and it seems to slow down any training operation incredibly. I just want to do it once and save it like that.
datasets = tfds.load("nsynth", data_dir="data")
train_dataset, test_dataset, valid_dataset = datasets["train"], datasets["test"], datasets["valid"]
train_dataset = train_dataset.filter(lambda x: x['instrument']['family'] == 3)
tf_writer = tf.data.experimental.TFRecordWriter("filtered/onlyGuitar.tfrecord")
tf_writer.write(train_dataset)
		</comment>
		<comment id='7' author='zzj0402' date='2020-05-04T23:27:35Z'>
		
Honestly, I didn't understand the documentation at all. I'm trying to find a way to save the modified dataset as a tfrecord file as well. I will be doing lots of filtering on the data, and it seems to slow down any training operation incredibly. I just want to do it once and save it like that.
datasets = tfds.load("nsynth", data_dir="data")
train_dataset, test_dataset, valid_dataset = datasets["train"], datasets["test"], datasets["valid"]
train_dataset = train_dataset.filter(lambda x: x['instrument']['family'] == 3)
tf_writer = tf.data.experimental.TFRecordWriter("filtered/onlyGuitar.tfrecord")
tf_writer.write(train_dataset)

I end up switching to Torch.
		</comment>
		<comment id='8' author='zzj0402' date='2020-06-04T00:50:36Z'>
		I'm having similar error as &lt;denchmark-link:https://github.com/zzj0402&gt;@zzj0402&lt;/denchmark-link&gt;
 :
&lt;denchmark-code&gt;OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph is disabled in this function.
&lt;/denchmark-code&gt;

It seems like we can't save a Dataset to file in Tensorflow...
		</comment>
		<comment id='9' author='zzj0402' date='2020-06-04T07:20:36Z'>
		I could finally do it :
You have to iterate your Dataset and define Example, one by one, by yourself.
&lt;denchmark-code&gt;def create_int_feature(value):
  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))

with tf.io.TFRecordWriter("gs://my_bucket/data.tfrecord") as writer:
    for sample in dataset:
        feature = {
            'input_ids': create_int_feature(sample['input_ids']),
            'attention_mask': create_int_feature(sample['attention_mask']),
            'decoder_input_ids': create_int_feature(sample['decoder_input_ids']),
            'lm_labels': create_int_feature(sample['lm_labels'])
        }

        example = tf.train.Example(features=tf.train.Features(feature=feature))
        writer.write(example.SerializeToString())
&lt;/denchmark-code&gt;

To read your TFRecord files :
&lt;denchmark-code&gt;feature_description = {
    'input_ids': tf.io.FixedLenFeature([tokenizer.model_max_length], tf.int64),
    'attention_mask': tf.io.FixedLenFeature([tokenizer.model_max_length], tf.int64),
    'decoder_input_ids': tf.io.FixedLenFeature([tokenizer.model_max_length // 4], tf.int64),
    'lm_labels': tf.io.FixedLenFeature([tokenizer.model_max_length // 4], tf.int64)
}

def deserialize(example_proto):
    return tf.io.parse_single_example(example_proto, feature_description)

dataset = tf.data.TFRecordDataset("gs://my_bucket/data.tfrecord")
dataset = dataset.map(deserialize)
&lt;/denchmark-code&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

That was a very painful and excruciating process. &lt;denchmark-link:https://www.tensorflow.org/tutorials/load_data/tfrecord#tfrecord_files_using_tfdata&gt;Documentation&lt;/denchmark-link&gt;
 is actually complete, but very difficult to follow.
That seems insane that there is no helper function for writing a TFRecord from an existing dataset.
The process is the same no matter if you have raw data or complete tf.Dataset (?!)
		</comment>
		<comment id='10' author='zzj0402' date='2020-06-04T08:17:05Z'>
		Torch solves in one line of code. Guess TF needs some catch-up
		</comment>
		<comment id='11' author='zzj0402' date='2020-06-08T23:46:26Z'>
		&lt;denchmark-link:https://github.com/zzj0402&gt;@zzj0402&lt;/denchmark-link&gt;
 How does torch solve this? As far as I can tell a torch  is just a python iterator, but if you serialize the object you don't actually store the computation up to that point, torch doesn't even have multiple stage you can checkpoint. Am I missing something?
		</comment>
		<comment id='12' author='zzj0402' date='2020-06-08T23:58:02Z'>
		&lt;denchmark-link:https://github.com/cgarciae&gt;@cgarciae&lt;/denchmark-link&gt;
 I don't know any details about both Tensorflow and Pytorch, but as a user, in Pytorch you can do :
torch.save(dataset, file_path) and load it with torch.load(file_path), and it works.
Unlike TF, you don't have to mind any of the internal details of your dataset.
		</comment>
		<comment id='13' author='zzj0402' date='2020-06-08T23:59:08Z'>
		I am working on providing support for save and load and expect it to be available later this month (and certainly for TF 2.3).
		</comment>
		<comment id='14' author='zzj0402' date='2020-06-09T00:11:46Z'>
		&lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 Will it work also for datasets that don't not fit in memory? So like "a converter" to tf_record for an optimized format when you need to train from a remote/cloud storage.
		</comment>
		<comment id='15' author='zzj0402' date='2020-06-09T00:26:42Z'>
		Yes, it will be a streaming API. You will be able to do the following:
&lt;denchmark-code&gt; # Save a dataset
  dataset = tf.data.Dataset.range(10).
  tf.data.experimental.save(dataset, "/path/to/data")

  # Load a previously saved dataset
  new_dataset = tf.data.experimental("/path/to/data",
       element_spec=tf.TensorSpec(shape=(), dtype=tf.int64))
&lt;/denchmark-code&gt;

The load API will require you to specify the type signature for the elements to load, which is required so that shape inference does not have to perform I/O.
		</comment>
		<comment id='16' author='zzj0402' date='2020-06-09T00:48:07Z'>
		I think to fully solve this you need something like &lt;denchmark-link:https://github.com/tensorflow/community/blob/master/rfcs/20200107-tf-data-snapshot.md&gt;tf.data Snapshots&lt;/denchmark-link&gt;
 to be implemented. Its a bit hacky but you can also use  so save the computation but its really meant to be temporal, mostly used to reuse the data of the first epoch.
		</comment>
		<comment id='17' author='zzj0402' date='2020-06-09T00:50:55Z'>
		Indeed. tf.data snapshot will be released in TF 2.3 as well and the aforementioned save and load API will share its implementation.
		</comment>
		<comment id='18' author='zzj0402' date='2020-06-09T00:55:12Z'>
		&lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 I'am thinking also about refreshing a little bit on the original data in the case you have some time-margin to sparsely execute realtime augmentions of some sparse samples.
But I don't if we could handle in some way this metadata (original vs transformed save data).
		</comment>
		<comment id='19' author='zzj0402' date='2020-06-09T00:56:37Z'>
		I do not follow your use case. The loaded dataset will be a tf.data.Dataset so you could apply further transformations to it.
		</comment>
		<comment id='20' author='zzj0402' date='2020-06-09T01:09:02Z'>
		&lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 Yes of course. But suppose that you use it as a converter you will do all the augmentation that cannot apply in realtime and you save it. When loading back you can have a new margin over the training loop ans so you could do some sparse sample augmentation on the original data to refresh some samples.
So I don't know if it could be easy in the streaming context instantiating two different datasets object and interleave these data or we need to handle some special metadata in the API.
		</comment>
		<comment id='21' author='zzj0402' date='2020-06-09T10:38:12Z'>
		&lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 Probably that case could be covered by the experimetal . I think that It could work with your load and save api as It consume dataset objects right?
		</comment>
		<comment id='22' author='zzj0402' date='2020-06-11T23:26:36Z'>
		My changes were submitted as &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/4d58a67a9f19ab8d0cfbb2d8e461ebb73ce06db6&gt;4d58a67&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>