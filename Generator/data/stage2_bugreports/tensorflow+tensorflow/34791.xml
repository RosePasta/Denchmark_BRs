<bug id='34791' author='yanghoonkim' open_date='2019-12-03T11:07:51Z' closed_time='2020-06-16T02:09:19Z'>
	<summary>variable scope is changed by force when reopened (with regard to use Adam) (with new example)</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
TensorFlow version (use command below): 1.12
Python version: 2.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory: 1080ti

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

The same issue with &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/30813&gt;#30813&lt;/denchmark-link&gt;

Describe the expected behavior
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Please compare the results of following codes
code 1:
&lt;denchmark-code&gt;import tensorflow as tf
class generator:
    def __init__(self):
        with tf.variable_scope('myscope'):
            self.a = tf.get_variable('a', [4,3,2], tf.float32)
            self.b = tf.get_variable('b', [4,3,2], tf.float32)
            self.c = tf.reduce_sum(self.a + self.b)
            self.d = tf.train.AdamOptimizer(name = 'adamopt')
            self.e = self.d.compute_gradients(self.c, tf.trainable_variables())
            self.f = self.d.apply_gradients(self.e)
        for item in tf.global_variables():
            if 'beta' in item.name:
                print item
    
    def compute(self):
        with tf.variable_scope('myscope'):
            self.e1 = self.d.compute_gradients(self.c, tf.trainable_variables())
            self.f1 = self.d.apply_gradients(self.e1)
        print '------------------------'
        for item in tf.global_variables():
            if 'beta' in item.name:
                print item
    
    def compute1(self):
        with tf.variable_scope('myscope'):
            self.e2 = self.d.compute_gradients(self.c, tf.trainable_variables())
            self.f2 = self.d.apply_gradients(self.e2)
        print '---------------------------'
        for item in tf.global_variables():
            if 'beta' in item.name:
                print item

g = generator()
g.compute()
g.compute1()
&lt;/denchmark-code&gt;

The printed result will be:
&lt;denchmark-code&gt;&lt;tf.Variable 'myscope/beta1_power:0' shape=() dtype=float32_ref&gt;
&lt;tf.Variable 'myscope/beta2_power:0' shape=() dtype=float32_ref&gt;
------------------------
&lt;tf.Variable 'myscope/beta1_power:0' shape=() dtype=float32_ref&gt;
&lt;tf.Variable 'myscope/beta2_power:0' shape=() dtype=float32_ref&gt;
---------------------------
&lt;tf.Variable 'myscope/beta1_power:0' shape=() dtype=float32_ref&gt;
&lt;tf.Variable 'myscope/beta2_power:0' shape=() dtype=float32_ref&gt;
&lt;/denchmark-code&gt;

However
code 2:
&lt;denchmark-code&gt;import tensorflow as tf
class generator:
    def __init__(self):
        with tf.variable_scope('myscope'):
            self.a = tf.get_variable('a', [4,3,2], tf.float32)
            self.b = tf.get_variable('b', [4,3,2], tf.float32)
            self.c = tf.reduce_sum(self.a + self.b)
            self.d = tf.train.AdamOptimizer(name = 'adamopt')
    
    def compute(self):
        with tf.variable_scope('myscope'):
            self.e1 = self.d.compute_gradients(self.c, tf.trainable_variables())
            self.f1 = self.d.apply_gradients(self.e1)
        print '------------------------'
        for item in tf.global_variables():
            if 'beta' in item.name:
                print item
    
    def compute1(self):
        with tf.variable_scope('myscope'):
            self.e2 = self.d.compute_gradients(self.c, tf.trainable_variables())
            self.f2 = self.d.apply_gradients(self.e2)
        print '---------------------------'
        for item in tf.global_variables():
            if 'beta' in item.name:
                print item

g = generator()
g.compute()
g.compute1()
&lt;/denchmark-code&gt;

results in:
&lt;denchmark-code&gt;------------------------
&lt;tf.Variable 'myscope_1/beta1_power:0' shape=() dtype=float32_ref&gt;
&lt;tf.Variable 'myscope_1/beta2_power:0' shape=() dtype=float32_ref&gt;
---------------------------
&lt;tf.Variable 'myscope_1/beta1_power:0' shape=() dtype=float32_ref&gt;
&lt;tf.Variable 'myscope_1/beta2_power:0' shape=() dtype=float32_ref&gt;
&lt;/denchmark-code&gt;

The only difference between two code is that the first code block doesn't operate 'apply_gradients' operation within the first opening of the variable scope. And I found that this results from the fact that the beta is made with variable_scope.variable operation. Is this a bug? or an intended results?
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='yanghoonkim' date='2019-12-04T07:57:36Z'>
		I have tried on colab with TF version 1.12,1.15 and was able to reproduce the issue.Please, find the gists of &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/8da27df4cb68029c2dae23a95fb01d34/untitled439.ipynb&gt;code1&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/2c0fffd2fae199f489869cf3bf8ae6f6/untitled440.ipynb&gt;code2.&lt;/denchmark-link&gt;
 Thanks!
		</comment>
		<comment id='2' author='yanghoonkim' date='2020-06-02T00:44:24Z'>
		&lt;denchmark-link:https://github.com/yanghoonkim&gt;@yanghoonkim&lt;/denchmark-link&gt;
 Is this still an issue for you? Can you please try latest versions and let us know how it progresses. Thanks!
Please close the issue if this was already resolved for you. Thanks!
		</comment>
		<comment id='3' author='yanghoonkim' date='2020-06-09T01:34:33Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='4' author='yanghoonkim' date='2020-06-16T02:09:17Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='5' author='yanghoonkim' date='2020-06-16T02:09:20Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34791&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34791&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>