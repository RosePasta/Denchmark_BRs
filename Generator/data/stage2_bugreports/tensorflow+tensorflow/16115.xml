<bug id='16115' author='mingu6' open_date='2018-01-14T23:08:53Z' closed_time='2019-08-23T23:43:55Z'>
	<summary>Gradients w.r.t. eigenvalues/eigenvectors</summary>
	<description>
Please go to Stack Overflow for help and support:
&lt;denchmark-link:https://stackoverflow.com/questions/tagged/tensorflow&gt;https://stackoverflow.com/questions/tagged/tensorflow&lt;/denchmark-link&gt;

If you open a GitHub issue, here is our policy:

It must be a bug or a feature request.
The form below must be filled out.
It shouldn't be a TensorBoard issue. Those go here.

Here's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra 10.13.1
TensorFlow installed from (source or binary): pip install tensorflow
TensorFlow version (use command below): v1.2.0-5-g435cdfc 1.2.1
Python version: 3.5.2
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: see code

You can collect some of this information using our environment capture script:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&lt;/denchmark-link&gt;

You can obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Hi,
I'm having issues with evaluating the gradients of eigenvalues/eigenvectors with respect to the underlying matrix. Tensorflow evaluates the gradients, but these are not correct when compared to the true gradients.
We are using tf.self_adjoint_eig to evaluate the spectral decomposition for the tensorflow variable input. We have initialised this with a symmetric matrix to satisfy the self adjoint operator property. We wish to take the derivative of individual eigenvalues or eigenvector with respect to the original matrix (note for eigenvectors we take one element from one eigenvector, e.g. element 2 in eigenvector 1 to avoid the issue of gradient aggregation for now).
The methodology for evaluating true gradients of eigenvalues and vectors of a symmetric real matrix can be found in the paper "On differentiating Eigenvalues and Eigenvectors" by Magnus (1985), Theorem 1 eqn (6) + (7). We evaluated using our input matrix the gradients under this paper and compared it to tensorflow evaluated gradients and the gradients from finite difference approximation. For the eigenvalues, the gradients are similar (identical on diagonal entries, off by a factor of 2 on off-diagonal elements), however the eigenvectors are off by quite a bit outside the diagonal entries. To start, define a matrix A as (excuse the matrix output formatting from Python)
A=[[-3 -2  4]
[-2  1  1]
[ 4  1  5]]
using np.linalg.eig and tf.self_adjoint_eig on A (I simply initialised a variable with A and computed the gradient for the tf implementation)
Tensorflow eigenvalues: [-5.43071561  1.76904987  6.66166575]
Python eigenvalues: [-5.43071561  6.66166575  1.76904987]
I now wish to evaluate the gradient of eigenvalue 1 (-5.43071561) w.r.t. A.
Analytical gradient:
[[ 0.75896178  0.28555906 -0.31842553]
[ 0.28555906  0.10744148 -0.11980748]
[-0.31842553 -0.11980748  0.13359674]]
Tensorflow gradient:
[[[ 0.75896178,  0.        ,  0.        ],
[ 0.57111812,  0.10744148,  0.        ],
[-0.63685107, -0.23961495,  0.13359674]]]
The diagonal entries are the same but the off-diagonal entries are clearly off by a factor of 2.  Now we try and evaluate gradients for the eigenvectors.
Tensorflow eigenvectors:
[[-0.87118413 -0.31452619 -0.37697678]
[-0.32778267  0.94426587 -0.0303395 ]
[ 0.36550888  0.09713517 -0.92572567]]
Python eigenvectors:
[[ 0.87118413  0.37697678 -0.31452619]
[ 0.32778267  0.0303395   0.94426587]
[-0.36550888  0.92572567  0.09713517]]
We try and find the gradient of the eigenvector 1 element 2 (+/-0.32778267). We expect the tensorflow gradient to the equivalent to the analytical gradient (after taking into account the sign difference).
Analytical gradient:
[[ 0.03511309 -0.10795607 -0.01312188]
[ 0.01321128 -0.04061843 -0.0049371 ]
[-0.01473184  0.04529341  0.00550534]]
Tensorflow gradient:
[[[-0.03511309,  0.        ,  0.        ],
[ 0.09474478,  0.04061843,  0.        ],
[ 0.02785372, -0.04035631, -0.00550534]]]
Besides the entries being different between the tensorflow evaluated and real gradients, one issue is that tensorflow only returns the lower triangle of the gradient. Despite A being symmetric, the gradient matrix is not symmetric (as per the true gradient) and so the upper right triangle shouldn't be zeros. I believe this to be a bug, however I understand there may be reasons for the differences. Thank you for reading!
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
Here is a script that reproduces the above.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/1546631/eigen_decomp_examplev2.py.zip&gt;eigen_decomp_examplev2.py.zip&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='mingu6' date='2018-01-19T22:35:01Z'>
		&lt;denchmark-link:https://github.com/rmlarsen&gt;@rmlarsen&lt;/denchmark-link&gt;
, have you experience with any of this code?
		</comment>
		<comment id='2' author='mingu6' date='2018-04-26T10:50:15Z'>
		If the input matrix A is symmetric, changing A_ij, will also change A_ji, so we need to multiple the off-diagonal elements by 2. For more details including results on the generalized eigenvalue problem see this &lt;denchmark-link:https://en.wikipedia.org/wiki/Eigenvalue_perturbation&gt;wiki article on Eigenvalue perturbation&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='3' author='mingu6' date='2019-08-23T23:43:55Z'>
		Closing this issue due to staleness. Please check with the latest version of TensorFlow. Feel free to reopen if issue still persists. Thanks!
		</comment>
		<comment id='4' author='mingu6' date='2019-08-23T23:43:56Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=16115&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=16115&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='mingu6' date='2019-10-22T10:32:34Z'>
		The described problem still exists in TensorFlow 2.0. For example, the first result posted by &lt;denchmark-link:https://github.com/mingu6&gt;@mingu6&lt;/denchmark-link&gt;
 can be reproduced with TensorFlow 2.0 with the below code.
Is there any new insight into the problem?
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np
from numpy.linalg import eig
from numpy.linalg import pinv

#create upper triangular matrix
A_vec = np.array([-3,-2,4,-2,1,1,4,1,5])
A = np.reshape(A_vec,[3,3])
e = eig(A)
#extract eigenvalues and eigenvectors
e_val = e[0]
e_vec = e[1]

#tensorflow variables
A_tf = tf.Variable(tf.constant(A,dtype=tf.float64))
e_tf = tf.linalg.eigh(A_tf)
#extract eigenvalues and eigenvectors
e_val_tf = e_tf[0]
e_vec_tf = e_tf[1]

#print A
print("A:")
print(A)

#show eigenvalues for Tensorflow and R
print("Tensorflow eigenvalues:",tf.keras.backend.eval(e_val_tf))
print("Python eigenvalues:",e_val)

############# Eigenvector element gradient #############

A_tf = tf.Variable(tf.constant(A,dtype=tf.float64))
with tf.GradientTape() as g:
    g.watch(A_tf)

    e_val_tf, e_vec_tf = tf.linalg.eigh(A_tf)
    #extract eigenvalues and eigenvectors
    e_val_0 = e_val_tf[0]

eval_grad_tf = g.gradient(e_val_0, A_tf) #Tensorflow gradient of first eigenvalue w.r.t. A

#analytical gradient
eigenvec = e_vec[:,0]

#analytical deriv from analytical gradient of eigenvalue from Magnus (1985), "On differentiating
#Eigenvalues and Eigenvectors", Theorem 1 eqn (6)
eval_grad_analytical = np.outer(eigenvec,eigenvec)

print("Analytical gradient:")
print(eval_grad_analytical)
print("Tensorflow gradient:")
print(tf.keras.backend.eval(eval_grad_tf))
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>