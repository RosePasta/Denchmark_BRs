<bug id='37289' author='ahnjaewoo' open_date='2020-03-04T10:05:22Z' closed_time='2020-03-05T10:48:41Z'>
	<summary>Unable to remove model and release GPU memory</summary>
	<description>
System information

Custom code
Ubuntu 16.04
TensorFlow installed from source (with pip)
TensorFlow version v2.0.0
Python version: 3.6
CUDA 10.0
TITAN XP

At first, I tried to load the pre-trained transformer-base model (A) and then add additional word embeddings (dynamic vocabulary) to it. However, it doesn't work (i.e., can't add values to tf.Variable)
Instead, I created another model (B) with the size of dynamic vocabulary and set its weights to existing transformer model (A).
Then, I do not need initial model (A), so I want to remove it and clear GPU memory.
I tried such commands but they didn't work as well.
&lt;denchmark-code&gt;# delete not model (B) but only model (A)
tf.keras.backend.clear_session()
del model
gc.collect()
&lt;/denchmark-code&gt;

According to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/36465&gt;#36465&lt;/denchmark-link&gt;
, I can either use multiprocessing functions... but then I have to remove both model (A) and model (B). So I just want more clear solution.
Thanks.
	</description>
	<comments>
		<comment id='1' author='ahnjaewoo' date='2020-03-05T10:39:47Z'>
		I recently looked at this issue of making tensorflow free GPU memory, from what I understood I does not seem like it's gonna be available anytime soon. So for now when tf allocates some GPU memory, it will not free it until the cuda session is closed, regardless of it being used to store objects or not.
You can close the whole cuda session using numba (e.g. &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/19731#issuecomment-423426372&gt;#19731 (comment)&lt;/denchmark-link&gt;
) but this is very hacky: it releases all the memory of the current process but no longer allows you to use the GPU with tf.
The features you're looking for, i.e. garbage collection + ram freezing, is not available on tensorflow and I'm not sure you can do it in any other way. I don't have any "side" in the tf/pytorch war, I use both. If this feature is crutial to you and if it's possible to consider switching in your context, I just wanted to let you know that &lt;denchmark-link:https://pytorch.org/docs/stable/cuda.html#torch.cuda.empty_cache&gt;this feature of accessing cuda garbage collection is available in pytorch&lt;/denchmark-link&gt;
 so it might be an option.
		</comment>
		<comment id='2' author='ahnjaewoo' date='2020-03-05T10:48:41Z'>
		Thanks for suggestion. (especially numba)
I decided to execute it as a sub-process with another python code.
I hope this problem to be resolved in TF society someday :)
Thanks üëç
		</comment>
		<comment id='3' author='ahnjaewoo' date='2020-03-05T10:48:43Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37289&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37289&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>