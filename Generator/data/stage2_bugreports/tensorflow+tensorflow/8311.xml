<bug id='8311' author='eddiepierce' open_date='2017-03-11T21:59:07Z' closed_time='2017-06-16T20:52:02Z'>
	<summary>Different output using CudnnGRU vs GRUCell</summary>
	<description>
Operating System: Arch Linux
Installed version of CUDA and cuDNN:  libcudart.so.8.0.44, libcudnn.so.5.1.5

The commit hash (git rev-parse HEAD): 57e40363eb40a692f7c5dfea3f53031a52024321
The output of bazel version: 0.4.4

I set x = 1, previous_h = 0, all weights and biases = 0, output should be 0. Traditional tensorflow GRU returns 0, but CudnnGRU returns 0.20482421
NOTE: need to use GPU for CudnnGRU to work properly
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops

# GRU with x=1, h_t_minus_1=0, all weights and biases = 0; output (h) should equal 0
# u = sig(Wx + Rh + b)
# r = sig(Wx + Rh + b)
# c = tanh(Wx + r(Rh + b) + b)
# h = (1-u)c + uh


# u = sig(0*1 + 0*0 + 0)
# u = .5
# r = sig(0*1 + 0*0 + 0)
# r = .5
# c = tanh(0*1 + .5(0*0 + 0) + 0)
# c = 0
# h = (1-.5)*0 + .5*0
# h = 0



batch_size = 1
n_time = 1
x_depth = 1
n_cell = 1

x = tf.ones([batch_size, n_time, x_depth])

y_tf, _ = tf.nn.dynamic_rnn(tf.contrib.rnn.GRUCell(n_cell), x, dtype=tf.float32)
param_tf = [v.assign(tf.zeros(tf.shape(v))) for v in tf.trainable_variables()] # y_tf uses these zeroed out params

n_layer = 1
rnn_cudnn = cudnn_rnn_ops.CudnnGRU(n_layer, n_cell, x_depth, 'skip_input')
param_cudnn = tf.Variable(tf.zeros([rnn_cudnn.params_size()]), validate_shape=False)
y_cudnn, state_cudnn = rnn_cudnn(tf.transpose(x, [1,0,2]), tf.zeros([n_layer, batch_size, n_cell]), param_cudnn)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    # NOTE: cudnn has more params because they use twice as many biases
    print('y_tf: {}\ny_cudnn: {}\nparam_tf\n{}\n\nparam_cudnn\n{}\n\n'.format(*sess.run([y_tf, y_cudnn, param_tf, param_cudnn])))



# Output:
# y_tf: [[[ 0.]]]
# y_cudnn: [[[ 0.20482421]]]
# param_tf
# [array([[ 0.,  0.],
#        [ 0.,  0.]], dtype=float32), array([ 0.,  0.], dtype=float32), array([[ 0.],
#        [ 0.]], dtype=float32), array([ 0.], dtype=float32)]

# param_cudnn
# [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='eddiepierce' date='2017-03-12T05:03:09Z'>
		&lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 This seems like something is wrong?
		</comment>
		<comment id='2' author='eddiepierce' date='2017-03-17T22:22:11Z'>
		Try putting linear_input instead of skip_input?
		</comment>
		<comment id='3' author='eddiepierce' date='2017-03-18T12:54:54Z'>
		 does correctly output 0. I think there's still a bug with  though because in my example input_size == num_units, so it shouldn't require a linear projection before the GRU layer. When I try , it returns 0.20482421. Here's the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py#L377&gt;argument definition&lt;/denchmark-link&gt;
.

input_mode: indicate whether there is a linear projection between the
input and The actual computation before the first layer. It could be
'skip_input', 'linear_input' or 'auto_select'.
'skip_input' is only allowed when input_size == num_units;
'auto_select' implies 'skip_input' when input_size == num_units;
otherwise, it implies 'linear_input'.

		</comment>
		<comment id='4' author='eddiepierce' date='2017-03-18T19:44:57Z'>
		skip_input, as far as I can tell, skips the Wx linear transformation of the input and performs roughly half the computation as the other approach. This is probably in case you want to do batch norm through depth and so need flexibility to modify the outputs of Wx.  It does NOT appear to apply an extra linear transform "outside" of the GRU.
As you're expecting to apply Wx, you should enable linear_input.
As a side note, there still appears to be bug in skip_input, since for GRUs if you skip the initial linear transformation you need to pass in an input matrix that has dimensionality 3x that of the hidden dimension (each gate in the GRU takes a separate linear transformation). CuDNN still checks for equality in dimensionality, however, and appears to just copy the input to all three gates. This is likely incorrect and I can't make sense of it. Using linear_input will make the GRU behave as expected.
		</comment>
		<comment id='5' author='eddiepierce' date='2017-06-16T20:52:02Z'>
		&lt;denchmark-link:https://github.com/eddiepierce&gt;@eddiepierce&lt;/denchmark-link&gt;
 is your issue resolved? I'm gonna close this, but please let me know if it should be reopened. Thanks.
		</comment>
	</comments>
</bug>