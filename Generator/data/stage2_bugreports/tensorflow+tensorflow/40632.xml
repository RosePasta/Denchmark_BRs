<bug id='40632' author='rakeshmothukuru1' open_date='2020-06-20T11:41:40Z' closed_time='2020-06-25T17:44:32Z'>
	<summary>Concatenating weights in Keras custom layer using `add_weight` fails while computing gradients</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A. It can be reproduced in Google Colab
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): N/A
TensorFlow version (use command below): 2.2.0 and tf-nightly
Python version: 3.6.7 (Google Colab)
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A

Describe the current behavior: While writing a custom layer, I need to have multiple weight matrices concatenated together. If I do this in the build function using tf.concatenate, I get the error ValueError: No gradients provided for any variable:..., however, if I make a list of weights in build and concatenate them in call it works.
Describe the expected behavior: It should work with tf.concatenate as well.
: This is the &lt;denchmark-link:https://colab.research.google.com/gist/rakeshmothukuru1/90bcb59075b6b5e60d00ffb060270a29/so_62425584.ipynb&gt;Colab link&lt;/denchmark-link&gt;
 to reproduce the issue.
Providing the code below as well.
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np
from tensorflow.keras.optimizers import Adam

print(tf.__version__)

class MultiInputLinear(tf.keras.layers.Layer):
    def __init__(self, output_dim=32, n_inputs=2):
        super(MultiInputLinear, self).__init__()
        self.output_dim = output_dim
        self.n_inputs = n_inputs


    def build(self, input_shapes):
        self.input_dim = input_shapes[0][1]

        self.W = tf.concat(
            [
                self.add_weight(
                    name=f'W_{i}',
                    shape=(self.input_dim, self.output_dim),
                    initializer='random_normal',
                    trainable=True
                ) for i in range(self.n_inputs)
            ], axis=0
        )

    def call(self, inputs):  
        supports = tf.concat(inputs, axis=-1)        
        return tf.matmul(supports, self.W)

N = 100
A = [np.random.normal(size=(N, N)) for _ in range(2)]
y = np.random.binomial(1, .1, size=(N, 32))

A_in = [tf.keras.layers.Input(batch_size=N, shape=(N, )) for _ in range(2)]
Y = MultiInputLinear(y.shape[1], 2)(A_in)

model = tf.keras.models.Model(inputs=A_in, outputs=Y)
model.compile(loss='categorical_crossentropy', optimizer=Adam())

model.fit(A, y, batch_size=N)
&lt;/denchmark-code&gt;

Working Code :
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np
from tensorflow.keras.optimizers import Adam

class MultiInputLinear(tf.keras.layers.Layer):
    def __init__(self, output_dim=32, n_inputs=2):
        super(MultiInputLinear, self).__init__()
        self.output_dim = output_dim
        self.n_inputs = n_inputs


    def build(self, input_shapes):
        self.input_dim = input_shapes[0][1]

        self.W_list = [
                self.add_weight(
                    name=f'W_{i}',
                    shape=(self.input_dim, self.output_dim),
                    initializer='random_normal',
                    trainable=True
                ) for i in range(self.n_inputs)
            ]

    def call(self, inputs):  
        supports = tf.concat(inputs, axis=-1)
        W = tf.concat(self.W_list, axis=0)

        return tf.matmul(supports, W)

N = 100
A = [np.random.normal(size=(N, N)) for _ in range(2)]
y = np.random.binomial(1, .1, size=(N, 32))

A_in = [tf.keras.layers.Input(batch_size=N, shape=(N, )) for _ in range(2)]
Y = MultiInputLinear(y.shape[1], 2)(A_in)

model = tf.keras.models.Model(inputs=A_in, outputs=Y)
model.compile(loss='categorical_crossentropy', optimizer=Adam())

model.fit(A, y, batch_size=N)
&lt;/denchmark-code&gt;

Other info / logs :     ValueError: No gradients provided for any variable: ['multi_input_linear_4/W_0:0', 'multi_input_linear_4/W_1:0'].
	</description>
	<comments>
		<comment id='1' author='rakeshmothukuru1' date='2020-06-22T06:52:57Z'>
		I am able to replicate this issue, please find the &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/31bf133f06118129d0da64a8456ccdcd/untitled238.ipynb&gt;gist here&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='rakeshmothukuru1' date='2020-06-25T17:44:32Z'>
		In this case, the tf.concat in the first build produces an eager tensor that is not tracked when assigned to the .W attribute, as build always runs in an eager context. The correct way to do this is to concat in the call(), as shown in the second example. In general, avoid computations that produce tensors in the build() method (though computations that produce Variables are okay).
We recognize that is a fine line, and hard to reason about as a user. We will add some documentation to the build method to try to explain this more completely.
		</comment>
		<comment id='3' author='rakeshmothukuru1' date='2020-06-25T17:44:34Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40632&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40632&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>