<bug id='41807' author='DeepakG19' open_date='2020-07-28T09:25:48Z' closed_time='2020-08-23T20:34:48Z'>
	<summary>TFLITE Relocate Tensor Fail</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No
TensorFlow installed from (source or binary): pip install tensorflow-gpu
TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
Python version: 3.5.2
Bazel version (if compiling from source): No
GCC/Compiler version (if compiling from source): No
CUDA/cuDNN version:
GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"

Describe the current behavior
Steps to generate .tflite-

Train a ckpt
Create saved_model.pb using None,None input parameter
Generate .TFLITE saved .pb model by specifying default input because None,None doesnt works

Generated .tflite Input Details.
{'shape': array([  1, 256, 256,   3], dtype=int32), 'quantization': (0.0, 0), 'dtype': &lt;class 'numpy.float32'&gt;, 'index': 0, 'name': 'image'}
{'shape': array([  1, 256, 256,   1], dtype=int32), 'quantization': (0.0, 0), 'dtype': &lt;class 'numpy.float32'&gt;, 'index': 1, 'name': 'mask'}
{'shape': array([ 1, 64, 64,  1], dtype=int32), 'quantization': (0.0, 0), 'dtype': &lt;class 'numpy.float32'&gt;, 'index': 2, 'name': 'mask2'}
{'shape': array([  1, 128, 128,   1], dtype=int32), 'quantization': (0.0, 0), 'dtype': &lt;class 'numpy.float32'&gt;, 'index': 3, 'name': 'mask4'}
Actual Input Detail
(1, 432, 492, 3) (1, 432, 492, 1) (1, 216, 246, 1) (1, 108, 123, 1)
Allocating Tensors based on Actual Input Values
interpreter.resize_tensor_input(input_details[0]['index'], (1,h,w,3))
interpreter.resize_tensor_input(input_details[1]['index'], (1,h,w,1))
interpreter.resize_tensor_input(input_details[2]['index'], (1,int(h/4),int(w/4),1))
interpreter.resize_tensor_input(input_details[3]['index'], (1,int(h/2),int(w/2),1))
interpreter.allocate_tensors()
ERROR -
File "testtliteNone.py", line 141, in 
interpreter.allocate_tensors()
File "/homelib/python3.5/site-packages/tensorflow/lite/python/interpreter.py", line 95, in allocate_tensors
return self._interpreter.AllocateTensors()
File "/home/lib/python3.5/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py", line 106, in AllocateTensors
return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
RuntimeError: tensorflow/lite/kernels/kernel_util.cc:233 d1 == d2 || d1 == 1 || d2 == 1 was not true.Node number 4 (MUL) failed to prepare.
Describe the expected behavior
Allocation Should Be Done
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='DeepakG19' date='2020-07-28T11:13:32Z'>
		&lt;denchmark-link:https://github.com/DeepakG19&gt;@DeepakG19&lt;/denchmark-link&gt;

Will it be possible to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!
		</comment>
		<comment id='2' author='DeepakG19' date='2020-07-28T11:17:50Z'>
		
@DeepakG19
Will it be possible to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!

sorry Ravi. can not share the model file. can share commands that i used to generate the model files. Thanks for understanding.
		</comment>
		<comment id='3' author='DeepakG19' date='2020-07-30T23:20:26Z'>
		&lt;denchmark-link:https://github.com/DeepakG19&gt;@DeepakG19&lt;/denchmark-link&gt;
 Can you please try against latest TF (2.3 or nightly) ? Thanks!
		</comment>
		<comment id='4' author='DeepakG19' date='2020-07-31T01:43:15Z'>
		It seems the allocation failed because of the resize input.
Can you try not resize input?
thanks
		</comment>
		<comment id='5' author='DeepakG19' date='2020-07-31T06:21:13Z'>
		
@DeepakG19 Can you please try against latest TF (2.3 or nightly) ? Thanks!

Tried with tf-nightly. Same issue.
		</comment>
		<comment id='6' author='DeepakG19' date='2020-07-31T06:22:14Z'>
		
It seems the allocation failed because of the resize input.
Can you try not resize input?
thanks

How does one make a tflite model independent of input size for inference?
		</comment>
		<comment id='7' author='DeepakG19' date='2020-07-31T06:58:42Z'>
		Not sure about what model you're using, but
&lt;denchmark-code&gt;Actual Input Detail
(1, 432, 492, 3) (1, 432, 492, 1) (1, 216, 246, 1) (1, 108, 123, 1)

Allocating Tensors based on Actual Input Values
interpreter.resize_tensor_input(input_details[0]['index'], (1,h,w,3))
interpreter.resize_tensor_input(input_details[1]['index'], (1,h,w,1))
interpreter.resize_tensor_input(input_details[2]['index'], (1,int(h/4),int(w/4),1))
interpreter.resize_tensor_input(input_details[3]['index'], (1,int(h/2),int(w/2),1))
interpreter.allocate_tensors()

&lt;/denchmark-code&gt;

seems like the resize here may have some inconsistent shape.
		</comment>
		<comment id='8' author='DeepakG19' date='2020-07-31T07:17:42Z'>
		
Not sure about what model you're using, but
Actual Input Detail
(1, 432, 492, 3) (1, 432, 492, 1) (1, 216, 246, 1) (1, 108, 123, 1)

Allocating Tensors based on Actual Input Values
interpreter.resize_tensor_input(input_details[0]['index'], (1,h,w,3))
interpreter.resize_tensor_input(input_details[1]['index'], (1,h,w,1))
interpreter.resize_tensor_input(input_details[2]['index'], (1,int(h/4),int(w/4),1))
interpreter.resize_tensor_input(input_details[3]['index'], (1,int(h/2),int(w/2),1))
interpreter.allocate_tensors()

seems like the resize here may have some inconsistent shape.

&lt;denchmark-link:https://user-images.githubusercontent.com/3595901/89010305-f07d0100-d32b-11ea-932d-391cfcc7552a.jpg&gt;&lt;/denchmark-link&gt;

The respective size ratio match, resize should work?
		</comment>
		<comment id='9' author='DeepakG19' date='2020-07-31T07:25:50Z'>
		sorry, I mean this:
&lt;denchmark-code&gt;Generated .tflite Input Details.
{'shape': array([ 1, 256, 256, 3], dtype=int32), 'quantization': (0.0, 0), 'dtype': &lt;class 'numpy.float32'&gt;, 'index': 0, 'name': 'image'}
{'shape': array([ 1, 256, 256, 1], dtype=int32), 'quantization': (0.0, 0), 'dtype': &lt;class 'numpy.float32'&gt;, 'index': 1, 'name': 'mask'}
{'shape': array([ 1, 64, 64, 1], dtype=int32), 'quantization': (0.0, 0), 'dtype': &lt;class 'numpy.float32'&gt;, 'index': 2, 'name': 'mask2'}
{'shape': array([ 1, 128, 128, 1], dtype=int32), 'quantization': (0.0, 0), 'dtype': &lt;class 'numpy.float32'&gt;, 'index': 3, 'name': 'mask4'}

Actual Input Detail
(1, 432, 492, 3) (1, 432, 492, 1) (1, 216, 246, 1) (1, 108, 123, 1)
&lt;/denchmark-code&gt;

Also, it's possible your model has some fixed shape in the middle which can break the shape propagation.
You will need to build the model with dynamic shape in the first place.
Can you try if not using resize input. and see if it works?
		</comment>
		<comment id='10' author='DeepakG19' date='2020-07-31T07:37:04Z'>
		
sorry, I mean this:
Generated .tflite Input Details.
{'shape': array([ 1, 256, 256, 3], dtype=int32), 'quantization': (0.0, 0), 'dtype': &lt;class 'numpy.float32'&gt;, 'index': 0, 'name': 'image'}
{'shape': array([ 1, 256, 256, 1], dtype=int32), 'quantization': (0.0, 0), 'dtype': &lt;class 'numpy.float32'&gt;, 'index': 1, 'name': 'mask'}
{'shape': array([ 1, 64, 64, 1], dtype=int32), 'quantization': (0.0, 0), 'dtype': &lt;class 'numpy.float32'&gt;, 'index': 2, 'name': 'mask2'}
{'shape': array([ 1, 128, 128, 1], dtype=int32), 'quantization': (0.0, 0), 'dtype': &lt;class 'numpy.float32'&gt;, 'index': 3, 'name': 'mask4'}

Actual Input Detail
(1, 432, 492, 3) (1, 432, 492, 1) (1, 216, 246, 1) (1, 108, 123, 1)

Also, it's possible your model has some fixed shape in the middle which can break the shape propagation.
You will need to build the model with dynamic shape in the first place.
Can you try if not using resize input. and see if it works?

That printed order is different, values are passed correctly, checked.
There were RESIZE ops, since downsampling is not supported, i am passing additional inputs. For upscaling I am using tf.keras.backend.resize_images, which has a scaling parameter, and hence i doubt there is any fixed size.
		</comment>
		<comment id='11' author='DeepakG19' date='2020-08-04T02:07:19Z'>
		Hi Karim, can you help take a look?
Thanks!
		</comment>
		<comment id='12' author='DeepakG19' date='2020-08-04T18:25:23Z'>
		Passing explicit shape during conversion doesn't guarantee you can resize input during inference, and you shouldn't try resizing the input.
You will need to pass None in the input for the dimensions that are dynamic. If your TF model is constructed already with None in shape, then converting the saved model will result in TFLite model with same shape.
If you had problems during conversion, then please reply with more details on what you tried and what was the error you got.
Thanks
		</comment>
		<comment id='13' author='DeepakG19' date='2020-08-04T19:03:30Z'>
		
Passing explicit shape during conversion doesn't guarantee you can resize input during inference, and you shouldn't try resizing the input.
You will need to pass None in the input for the dimensions that are dynamic. If your TF model is constructed already with None in shape, then converting the saved model will result in TFLite model with same shape.
If you had problems during conversion, then please reply with more details on what you tried and what was the error you got.
Thanks


Ckpt is created from training using input 256x256
saved_model.pb is generated, passing input params as None,None for all 4 inputs
tflite_converter CLI is used to convert to tflite. Input shapes provided as None or -1 doesnt work.

Dynamic size input exists at inference time. The error occurs while using INTERPRETER.RELOCATE_TENSOR() after changing input tensor size.
		</comment>
		<comment id='14' author='DeepakG19' date='2020-08-04T19:06:00Z'>
		

Passing explicit shape during conversion doesn't guarantee you can resize input during inference, and you shouldn't try resizing the input.
You will need to pass None in the input for the dimensions that are dynamic. If your TF model is constructed already with None in shape, then converting the saved model will result in TFLite model with same shape.
If you had problems during conversion, then please reply with more details on what you tried and what was the error you got.
Thanks


Ckpt is created from training using input 256x256
saved_model.pb is generated, passing input params as None,None for all 4 inputs
tflite_converter CLI is used to convert to tflite. Input shapes provided as None or -1 doesnt work.

Dynamic size input exists at inference time. The error occurs while using INTERPRETER.RELOCATE_TENSOR() after changing input tensor size.

Also, tflite created for passing any size ie 256x256 or 512x512 etc in tflite_converter works for input of that size but will fail when we try to relocate_tensor
		</comment>
		<comment id='15' author='DeepakG19' date='2020-08-04T19:12:06Z'>
		The failure during allocate_tensors is because a MUL op has operands that are not broadcastable. So either you're setting inputs that are invalid, or some other issue.
Can you run the TF model with same shape ?
Can you share the exact commands you used ?
Please try to share reproduce steps or better a code snippet with sample model that has issue.
Thanks
		</comment>
		<comment id='16' author='DeepakG19' date='2020-08-12T08:22:32Z'>
		
The failure during allocate_tensors is because a MUL op has operands that are not broadcastable. So either you're setting inputs that are invalid, or some other issue.
Can you run the TF model with same shape ?
Can you share the exact commands you used ?
Please try to share reproduce steps or better a code snippet with sample model that has issue.
Thanks

Hi Karim,
The saved_model in pb [saved_model.pb and \variables] runs without any issue for variable sized inputs. This was created after passing None,None in input placeholder dimension.
The above model is further used to create tflite, and since None or -1 or 1 is not a valid input, we specify a dummy size, for instance 512x512.
tflite is created passing 512x512x3:512x512x1:128x128x1:256x256x1 as input for respective placeholders. Any input of the same size (512x512) executes without any error. But when the input size is changed, the error in allocate_tensor appears, as mentioned earlier.
The code used to generate tflite from saved_model is
tflite_convert --saved_model_dir="/None_dir/" --output_file="out.tflite" --input_shapes=1,512,512,3:1,512,512,1;1,128,128,1:1,256,256,1 --input_arrays=image,mask,mask2,mask4 --output_arrays=out --enable_v1_converter
Have also tried -
tflite_convert --saved_model_dir="/None_dir/" --output_file="out.tflite" --input_shapes=1,512,512,3:1,512,512,1;1,128,128,1:1,256,256,1 --input_arrays=image,mask,mask2,mask4 --output_arrays=out --enable_v1_converter --experimental_new_converter=true
		</comment>
		<comment id='17' author='DeepakG19' date='2020-08-12T23:37:28Z'>
		Hi &lt;denchmark-link:https://github.com/DeepakG19&gt;@DeepakG19&lt;/denchmark-link&gt;

Passing dummy values and resize the model during inference to different size is not how you should specify dynamic shape, and not working is expected.
The issue is this part "and since None or -1 or 1 is not a valid input," You should be able to use None as input.
If the model was saved with inputs as None, then please use our Python API to do the conversion by specifying only the saved_model path.
Example
&lt;denchmark-h:h1&gt;Convert the model.&lt;/denchmark-h&gt;

converter = tf.lite.TFLiteConverter.from_saved_model("saved_model_dir_path")
tflite_model = converter.convert()
&lt;denchmark-h:h1&gt;Save the TF Lite model.&lt;/denchmark-h&gt;

with tf.io.gfile.GFile('model.tflite', 'wb') as f:
f.write(tflite_model)
After doing the above, please

Paste any errors you got during the conversion script above.
If conversion passed and inference has issue, then paste code for inference that you tried and what was the error.
Please share the model you are trying to convert.

Thanks
		</comment>
		<comment id='18' author='DeepakG19' date='2020-08-23T20:34:48Z'>
		
Hi @DeepakG19
Passing dummy values and resize the model during inference to different size is not how you should specify dynamic shape, and not working is expected.
The issue is this part "and since None or -1 or 1 is not a valid input," You should be able to use None as input.
If the model was saved with inputs as None, then please use our Python API to do the conversion by specifying only the saved_model path.
Example
Convert the model.
converter = tf.lite.TFLiteConverter.from_saved_model("saved_model_dir_path")
tflite_model = converter.convert()
Save the TF Lite model.
with tf.io.gfile.GFile('model.tflite', 'wb') as f:
f.write(tflite_model)
After doing the above, please

Paste any errors you got during the conversion script above.
If conversion passed and inference has issue, then paste code for inference that you tried and what was the error.
Please share the model you are trying to convert.

Thanks

Hi Karim,
Thanks for quick response. The solution you provided, works at the moment.The problem was using a dummy value (as you said) to save model as tflite, which I got from &lt;denchmark-link:https://stackoverflow.com/a/55732431&gt;https://stackoverflow.com/a/55732431&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='19' author='DeepakG19' date='2020-08-23T20:34:50Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41807&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41807&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>