<bug id='25516' author='mvoelk' open_date='2019-02-05T10:24:23Z' closed_time='2019-03-12T17:45:21Z'>
	<summary>Defining metrics within loss functions was possible in Keras!</summary>
	<description>
System information

TensorFlow version: 1.12.0
Are you willing to contribute it: (Yes/No)

In Keras, it was possible to define metrics inside loss functions using closures. This means that something like the following
&lt;denchmark-code&gt;class Loss():
    def __init__(self):
        self.metrics = []
    
    def loss(self, y_true, y_pred):
        
        # the following can be some complicated intermediate stuff that
        # would otherwise require redundant code
        t1 = tf.reduce_mean(y_true-y_pred)
        m2 = tf.reduce_max(y_true-y_pred)
        m3 = tf.reduce_min(y_true-y_pred)
        
        def m1(y_true, y_pred):
            return t1
        self.metrics.append(m1)
        
        def make_fcn(tensor, name):
            f = lambda y_true, y_pred: tensor
            f.__name__ = name
            return f
        for m in ['m2', 'm3']:
            self.metrics.append(make_fcn(eval(m),m))
        
        return tf.reduce_sum((y_true-y_pred)**2)

output2_loss = Loss()

model.compile(
    optimizer=keras.optimizers.RMSprop(lr=0.001, decay=1e-6), 
    loss={'pred': 'categorical_crossentropy', 'output2': output2_loss.loss},
    metrics={'pred': ['accuracy'], 'output2': output2_loss.metrics}
)
&lt;/denchmark-code&gt;

works perfectly fine in normal Keras but does not work in TensorFlow Keras.
I assume that there is no clear specification when the loss functions are evaluated and it is also clear to me that the approach may not work with eager execution.
Is there any elegant mechanism or workaround to achieve a similar behaviour?
	</description>
	<comments>
		<comment id='1' author='mvoelk' date='2019-02-12T10:09:40Z'>
		The following works, but it is still more a hack than a solution...
&lt;denchmark-code&gt;class Loss():
    def __init__(self):
        self.metric_names = ['m1', 'm2', 'm3']
        
        def make_fcn(name):
            f = lambda y_true, y_pred: self._metric_functions[name](y_true, y_pred)
            f.__name__ = name
            return f
        self._metric_functions = {}
        self.metrics = [make_fcn(name) for name in self.metric_names]
    
    def loss(self, y_true, y_pred):
        
        # the following can be some complicated intermediate stuff that otherwise requires redundant code
        m1 = tf.reduce_mean(y_true-y_pred)
        m2 = tf.reduce_max(y_true-y_pred)
        m3 = tf.reduce_min(y_true-y_pred)
        
        def make_fcn(tensor):
            return lambda y_true, y_pred: tensor
        for name in self.metric_names:
            self._metric_functions[name] = make_fcn(eval(name))
        
        return tf.reduce_sum((y_true-y_pred)**2)

output2_loss = Loss()

model.compile(
    optimizer=keras.optimizers.RMSprop(lr=0.001, decay=1e-6), 
    loss={'pred': 'categorical_crossentropy', 'output2': output2_loss.loss},
    metrics={'pred': ['accuracy'], 'output2': output2_loss.metrics}
)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='mvoelk' date='2019-03-12T17:45:21Z'>
		Hello &lt;denchmark-link:https://github.com/mvoelk&gt;@mvoelk&lt;/denchmark-link&gt;
, definition of metric functions should not be dependent of when a loss function is called. This happens to work in keras-team/keras but it is definitely not an input that is supported.
Loss or metric inputs can be

strings which are names of inbuilt functions
inbuilt functions
instances of inbuilt loss or metric classes
custom functions with standard (y_true, y_pred, ..) signature
instances of custom classes where the call function has the standard signature.

		</comment>
	</comments>
</bug>