<bug id='42082' author='MeghnaNatraj' open_date='2020-08-06T00:02:00Z' closed_time='2020-12-15T20:17:13Z'>
	<summary>QAT conversion RuntimeError: Quantization not yet supported for op: 'DEQUANTIZE' issue with tf-nightly</summary>
	<description>
UPDATE
You can now fully quantize QAT models trained in any TF 2.x version. However, this feature is only available from TF version 2.4.0-rc0 onwards (and will be available in the final TF 2.4 release as well).
You will not require any workaround, i.e, you don't have to use TF 1.x
To verify that your TF version supports this, run the following code and check if runs successfully:
&lt;denchmark-code&gt;import tensorflow as tf
assert tf.__version__[:3] == "2.4", 'Your TF version ({}), does not support full quantization of QAT models. Upgrade to a TF 2.4 version (2.4.0-rc0, 2.4.0-rc1...2.4) or above'.format(tf.__version__)
&lt;/denchmark-code&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

ISSUE
System information
TensorFlow version (use command below): 2.4.0-dev20200728
Describe the current behavior
Error converting quantize aware trained tensorflow model to a fully integer quantized tflite model - error: RuntimeError: Quantization not yet supported for op: 'DEQUANTIZE'
Describe the expected behavior
Convert successfully

Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
&lt;denchmark-link:https://colab.research.google.com/gist/sayakpaul/8c8a1d7c94beca26d93b67d92a90d3f0/qat-bad-accuracy.ipynb&gt;https://colab.research.google.com/gist/sayakpaul/8c8a1d7c94beca26d93b67d92a90d3f0/qat-bad-accuracy.ipynb&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='MeghnaNatraj' date='2020-08-06T01:25:51Z'>
		&lt;denchmark-link:https://github.com/sayakpaul&gt;@sayakpaul&lt;/denchmark-link&gt;

I can reproduce the issue and will get back to you when I resolve this.
&lt;denchmark-link:https://colab.research.google.com/gist/MeghnaNatraj/8458ad508f5355769a980400d4d9d194/qat-bad-accuracy.ipynb&gt;https://colab.research.google.com/gist/MeghnaNatraj/8458ad508f5355769a980400d4d9d194/qat-bad-accuracy.ipynb&lt;/denchmark-link&gt;

Possible issue:
If you remove the TFLITE_BUILTINS_INT8 (don't enforce INT8) -- it works fine. The issue is that the model has 2 consecutive quantize at the beginning and 2 consecutive dequantize at the end (not sure why) -- probably because of the way tf.keras..mobilenetv2 is structured.
Couple of things to note (especially as you are involved in creating awesome tutorials! üëç ):
(The colab gist above has all the final code with the following suggested changes. NOTE: it also has some TODOs where i have simplified the code for faster execution)

Ensure you use the latest tensorflow_model_optimization and tensorflow-datasets and uninstall tensorflow when you install tf-nightly
Code readability: A) Try to group many similar code sections into one. Sections can be: all imports and initial settings code, all data processing related code, all training related code, all conversion code, etc. B) If your model is for a basic tutorial and it's small, use full paths to keras APIs -- tf.keras.layers..... instead of from tf.keras.layers import *.
For data generation: Have 3 parts 1) train_raw (loaded from tfds) - data has 3 dimensions 2) train_preprocessed (with all preprocessing steps) - data has 3 dimensions 3) train_data (the final dataset prepared for training, this would have batching shuffle and prefetch function.) - data has 4 dimensions Note: Repeat all 3 for validation data BUT do not shuffle the data for validation (or test dataset.)
Representative dataset - should only have 4 dimensions for images. You initially used the batched training data  with shape=(32, 244, 244, 3) and we further add a batch size  in the representative dataset (tf.expand_dims(train_data_image, 0)) - as a result the shape increases to 5! (1, 32, 244, 244, 5) This causes some errors which is quite hard to debug  (eg: PAD op dimensions exceeded &gt;=4). You instead want (1, 244, 244, 5) hence we use the train_preprocessed data (check the 3rd point above) where the images don't yet have a batch dimension shape (244, 244, 3) for the representative_dataset function.
Representative dataset - do not use next(iter(train_ds..)). This will make the image and label as a sequential list of items and cause failures. Instead use for image, _ in train_ds_preprocessed:

		</comment>
		<comment id='2' author='MeghnaNatraj' date='2020-08-06T02:25:37Z'>
		Thanks! First of all, the notebook that I had provided to you was meant for reproducing the issue I was facing. Before releasing it publicly, I sure would have modified it a bit.
A couple of things:

uninstall tensorflow when you install tf-nightly

Not sure about this since when I install pip install tf-nightly at the beginning of a Colab session (before doing anything) I have the nightly version gets reflected always. Is there anything specific for which you'd do this?

Sections can be: all imports and initial settings code

I respectfully disagree. I won't put together the pip installs inside the same code block where I am importing dependencies. I try to break longer code blocks some times which you might have seen in my notebook as well. This is my personal preference. If "all training code" seems a bit unreadable to me I'd break it into multiple cells and the same applies for "all conversion code".

If your model is for a basic tutorial and it's small, use full paths to keras APIs -- tf.keras.layers..... instead of from tf.keras.layers import *

Okay, will keep in mind. But for a bit more complex tutorials/notebooks (in general), I don't think I'd follow it.

For data generation

In the &lt;denchmark-link:https://colab.research.google.com/gist/sayakpaul/8c8a1d7c94beca26d93b67d92a90d3f0/qat-bad-accuracy.ipynb&gt;original notebook&lt;/denchmark-link&gt;
, I first loaded the dataset from tfds, visualized it (which I think is a good practice), mapped the resizing step, then mapped the scaling step and batching-shuffling (). The only thing I'd change is merging the resizing step and scaling step inside a utility and map them.
&lt;denchmark-link:https://user-images.githubusercontent.com/22957388/89483019-bf338380-d7b8-11ea-9f40-48a36dfff12d.png&gt;&lt;/denchmark-link&gt;

If you emphasized on the data generation point because I separated the steps into different cells, yes, I won't generally do that.

Representative dataset

Agreed on the point. You might have mistakenly mentioned 5 channels (244, 244, 5) but note that in the flowers' dataset the images come in 3 channels. I also see the problem in the representative_dataset_gen utility I used:
representative_images, _ = next(iter(train_ds))

def representative_dataset_gen():
    for image in representative_images:
        yield [tf.expand_dims(image, 0)]
If I'd have changed it to something like the following I think it should be good.
representative_images, _ = next(iter(train_ds))

def representative_dataset_gen():
    for image in representative_images:
        yield [image]
I can confirm that in this way image would have a shape of (1, 224, 224, 3).
You might also consider adding these instructions in the documentation.

Representative dataset - do not use next(iter(train_ds..)). This will make the image and label as a sequential list of items and cause failures. Instead use for image, _ in train_ds_preprocessed:

Okay. But what if I'd want to restrict the number of instances in the representative dataset? Because for bigger datasets it's very difficult to have the entire training dataset streamed as the representative dataset. Would you suggest something like the following?
train_ds_unbatched = train_ds.unbatch() # train_ds already batched and preprocessed

def representative_dataset_gen():
	for i, (image, _) in enumerate(train_ds_unbatched):
		if i==0: # let's say I want 100 samples only
			break
		yield[image]
		</comment>
		<comment id='3' author='MeghnaNatraj' date='2020-08-06T04:49:14Z'>
		Great points! Yes, you can choose what you think works best --- eg: I learnt many new things from your tutorial (loading TFDS datasets with the [:85%]..method! who knew! :))
For the representative dataset -- Would  work? &lt;denchmark-link:https://www.tensorflow.org/datasets/overview#as_numpy&gt;https://www.tensorflow.org/datasets/overview#as_numpy&lt;/denchmark-link&gt;
 (ignore the  part... just wanted to show an example usage)
		</comment>
		<comment id='4' author='MeghnaNatraj' date='2020-08-06T05:00:38Z'>
		Yes,  should work as well. Having a note in the documentation on handling large datasets while creating the representative dataset would help. The representative dataset generation can get non-trivial at times and here's &lt;denchmark-link:https://github.com/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/Magenta_arbitrary_style_transfer_model_conversion.ipynb&gt;an example&lt;/denchmark-link&gt;
 (which I am sure you are already aware of).
		</comment>
		<comment id='5' author='MeghnaNatraj' date='2020-08-17T12:19:23Z'>
		Do you have any plan solving this? I just encounterd this issue... Here is my minimal reproduing code

Train

import tensorflow as tf
from tensorflow import keras
import tensorflow_model_optimization as tfmot

# Load MNIST dataset
(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()
train_images, test_images = train_images / 255.0, test_images / 255.0

# Define the model architecture.
model = keras.Sequential([
    keras.layers.InputLayer(input_shape=(28, 28)),
    keras.layers.Flatten(),
    keras.layers.Dense(10)
])

# Train the digit classification model
model.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
model.fit(train_images, train_labels, epochs=1, validation_data=(test_images, test_labels))
# 1875/1875 [==============================] - 2s 946us/step - loss: 0.7303 - accuracy: 0.8100 - val_loss: 0.3097 - val_accuracy: 0.9117

# Train the quantization aware model
q_aware_model = tfmot.quantization.keras.quantize_model(model)
q_aware_model.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
q_aware_model.fit(train_images, train_labels, epochs=1, validation_data=(test_images, test_labels))
# 1875/1875 [==============================] - 2s 1ms/step - loss: 0.3107 - accuracy: 0.9136 - val_loss: 0.2824 - val_accuracy: 0.9225

Convert

# Define the representative data.
def representative_data_gen():
    for input_value in tf.data.Dataset.from_tensor_slices(train_images.astype("float32")).batch(1).take(100):
        yield [input_value]

# Successful converting from model
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen
tflite_model = converter.convert()

# Successful converting from model to uint8
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
tflite_model_quant = converter.convert()

# Successful converting from q_aware_model
q_converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)
q_converter.optimizations = [tf.lite.Optimize.DEFAULT]
q_converter.representative_dataset = representative_data_gen
q_tflite_model = q_converter.convert()

# Fail converting from q_aware_model to uint8
q_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
q_converter.inference_input_type = tf.uint8
q_converter.inference_output_type = tf.uint8
q_tflite_model_quant = q_converter.convert()
Throws error
RuntimeError: Quantization not yet supported for op: 'DEQUANTIZE'.

Another test without tf.lite.OpsSet.TFLITE_BUILTINS_INT8

# Successful converting from model to uint8
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
tflite_model_quant = converter.convert()

# Fail converting from q_aware_model to uint8
q_converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)
q_converter.optimizations = [tf.lite.Optimize.DEFAULT]
q_converter.representative_dataset = representative_data_gen
q_converter.inference_input_type = tf.uint8
q_converter.inference_output_type = tf.uint8
q_tflite_model_quant = q_converter.convert()
Throws error
RuntimeError: Unsupported output type UINT8 for output tensor 'Identity' of type FLOAT32.
		</comment>
		<comment id='6' author='MeghnaNatraj' date='2020-08-19T03:03:54Z'>
		You can resolve the second error by using removing all the lines q_converter.inference_output_type = tf.uint8. We're currently working on fixing this -- will post an update when it's done.
		</comment>
		<comment id='7' author='MeghnaNatraj' date='2020-08-19T08:39:05Z'>
		Thanks for your update. Ya, removing q_converter.inference_output_type = tf.uint8 will make it successful, but will leave output as float32.
interpreter = tf.lite.Interpreter(model_content=q_tflite_model_quant)
print('input: ', interpreter.get_input_details()[0]['dtype'])
# input:  &lt;class 'numpy.uint8'&gt;
print('output: ', interpreter.get_output_details()[0]['dtype'])
# output:  &lt;class 'numpy.float32'&gt;
		</comment>
		<comment id='10' author='MeghnaNatraj' date='2020-09-17T18:02:17Z'>
		Is there any update on this issue? I am facing the same error while trying to convert a QAT model to INT8.
		</comment>
		<comment id='11' author='MeghnaNatraj' date='2020-09-17T18:29:55Z'>
		
Is there any update on this issue? I am facing the same error while trying to convert a QAT model to INT8.

As far as my understanding, tf2.0 quantization is not supported Yet for full integer inference. Try QAT for tf1.x and everything is smoothly done
		</comment>
		<comment id='12' author='MeghnaNatraj' date='2020-09-17T19:30:48Z'>
		Thanks &lt;denchmark-link:https://github.com/dtlam26&gt;@dtlam26&lt;/denchmark-link&gt;
 for the response. Strangely, it turns out that for MobileNetV1 the conversion is working fine. For MobileNetV2, as &lt;denchmark-link:https://github.com/MeghnaNatraj&gt;@MeghnaNatraj&lt;/denchmark-link&gt;
 pointed out in earlier comment, there are two consecutive QUANTIZE and DEQUANTIZE nodes for inputs and outputs respectively which might be causing the issue. Attaching the screenshots.
INPUT:
&lt;denchmark-link:https://user-images.githubusercontent.com/71460728/93518336-84595b00-f8e1-11ea-9234-ad0d5e0119df.png&gt;&lt;/denchmark-link&gt;

OUTPUT:
&lt;denchmark-link:https://user-images.githubusercontent.com/71460728/93518371-920ee080-f8e1-11ea-8fe7-5e66a4aaa2d4.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='MeghnaNatraj' date='2020-09-17T19:41:06Z'>
		
Thanks @dtlam26 for the response. Strangely, it turns out that for MobileNetV1 the conversion is working fine. For MobileNetV2, as @MeghnaNatraj pointed out in earlier comment, there are two consecutive QUANTIZE and DEQUANTIZE nodes for inputs and outputs respectively which might be causing the issue. Attaching the screenshots.
INPUT:

OUTPUT:


Yes, you can still quantize the model for other type of quantization except int8 (I mean Builtin_int8)
Furthermore, You can check out this link for the detail bypass QAT on tf2: &lt;denchmark-link:https://github.com/tensorflow/model-optimization/issues/377#issuecomment-625093345&gt;tensorflow/model-optimization#377 (comment)&lt;/denchmark-link&gt;

From this, by declaring double input to the model, that is why the QAT for mobilenetv2 got double quantize and dequantize
		</comment>
		<comment id='14' author='MeghnaNatraj' date='2020-09-20T10:17:24Z'>
		
You can resolve the second error by using removing all the lines q_converter.inference_output_type = tf.uint8. We're currently working on fixing this -- will post an update when it's done.

Any news ??
		</comment>
		<comment id='15' author='MeghnaNatraj' date='2020-09-23T23:25:52Z'>
		

Is there any update on this issue? I am facing the same error while trying to convert a QAT model to INT8.

As far as my understanding, tf2.0 quantization is not supported Yet for full integer inference. Try QAT for tf1.x and everything is smoothly done

&lt;denchmark-link:https://github.com/dtlam26&gt;@dtlam26&lt;/denchmark-link&gt;
 can you point to some resources for QAT for tf1.x and then quanitzation. I am trying the following code (without QAT) but getting some error on TF 1.15:
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
num_classes = 20
model = Sequential([
layers.Conv2D(16, 3, padding='same', activation='relu', input_shape=(256, 256, 3)),
layers.MaxPooling2D(),
layers.Conv2D(32, 3, padding='same', activation='relu'),
layers.MaxPooling2D(),
layers.Conv2D(64, 3, padding='same', activation='relu'),
layers.MaxPooling2D(),
layers.Flatten(),
layers.Dense(128, activation='relu'),
layers.Dense(num_classes)
])
model.compile(optimizer='adam',
loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
metrics=['accuracy'])
model.fit(train_generator, epochs=1, steps_per_epoch=100)
model.save('/tmp/temp.h5')
converter = tf.lite.TFLiteConverter.from_keras_model_file("/tmp/temp.h5")
converter.inference_type = tf.lite.constants.QUANTIZED_UINT8
input_arrays = converter.get_input_arrays()
converter.quantized_input_stats = {input_arrays[0] : (0., 1.)}
tflite_model = converter.convert()
I am getting below error:
ConverterError: See console for info.
2020-09-23 22:55:27.815650: F tensorflow/lite/toco/tooling_util.cc:1734] Array conv2d_3/Relu, which is an input to the MaxPool operator producing the output array max_pooling2d_3/MaxPool, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.
Fatal Python error: Aborted
		</comment>
		<comment id='16' author='MeghnaNatraj' date='2020-09-25T20:56:43Z'>
		&lt;denchmark-link:https://github.com/dtlam26&gt;@dtlam26&lt;/denchmark-link&gt;
  As you are not using QAT and instead using post-training quantization, you need to provide a   in order to quantize the model.
Modify the steps in your code as:
&lt;denchmark-code&gt;.
.

model.save('/tmp/temp.h5')

converter = tf.lite.TFLiteConverter.from_keras_model_file("/tmp/temp.h5")
converter.optimizations = [tf.lite.Optimize.DEFAULT]
def representative_dataset_gen():
  for _ in range(num_calibration_steps):
    # Get sample input data as a numpy array in a method of your choosing.
    yield [input]
converter.representative_dataset = representative_dataset_gen
tflite_model = converter.convert()
&lt;/denchmark-code&gt;

Refer to &lt;denchmark-link:https://www.tensorflow.org/lite/performance/post_training_quantization&gt;post-training quantization&lt;/denchmark-link&gt;
 for more information.
		</comment>
		<comment id='17' author='MeghnaNatraj' date='2020-09-26T19:19:46Z'>
		
@dtlam26 As you are not using QAT and instead using post-training quantization, you need to provide a representative_dataset in order to quantize the model.
Modify the steps in your code as:
.
.

model.save('/tmp/temp.h5')

converter = tf.lite.TFLiteConverter.from_keras_model_file("/tmp/temp.h5")
converter.optimizations = [tf.lite.Optimize.DEFAULT]
def representative_dataset_gen():
  for _ in range(num_calibration_steps):
    # Get sample input data as a numpy array in a method of your choosing.
    yield [input]
converter.representative_dataset = representative_dataset_gen
tflite_model = converter.convert()

Refer to post-training quantization for more information.

Yes, I know this for post quantize, but my model is QAT, and it can't inference to int8 on tf2.x. For tf1 it is ok
		</comment>
		<comment id='18' author='MeghnaNatraj' date='2020-09-26T19:22:34Z'>
		&lt;denchmark-link:https://github.com/dtlam26&gt;@dtlam26&lt;/denchmark-link&gt;


Yes, I know this for post quantize, but my model is QAT, and it can't inference to int8 on tf2.x. For tf1 it is ok

Please can you tell me how you are able to perform QAT in tf1 ??
		</comment>
		<comment id='19' author='MeghnaNatraj' date='2020-09-26T19:27:05Z'>
		


Is there any update on this issue? I am facing the same error while trying to convert a QAT model to INT8.

As far as my understanding, tf2.0 quantization is not supported Yet for full integer inference. Try QAT for tf1.x and everything is smoothly done

@dtlam26 can you point to some resources for QAT for tf1.x and then quanitzation. I am trying the following code (without QAT) but getting some error on TF 1.15:
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
num_classes = 20
model = Sequential([
layers.Conv2D(16, 3, padding='same', activation='relu', input_shape=(256, 256, 3)),
layers.MaxPooling2D(),
layers.Conv2D(32, 3, padding='same', activation='relu'),
layers.MaxPooling2D(),
layers.Conv2D(64, 3, padding='same', activation='relu'),
layers.MaxPooling2D(),
layers.Flatten(),
layers.Dense(128, activation='relu'),
layers.Dense(num_classes)
])
model.compile(optimizer='adam',
loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
metrics=['accuracy'])
model.fit(train_generator, epochs=1, steps_per_epoch=100)
model.save('/tmp/temp.h5')
converter = tf.lite.TFLiteConverter.from_keras_model_file("/tmp/temp.h5")
converter.inference_type = tf.lite.constants.QUANTIZED_UINT8
input_arrays = converter.get_input_arrays()
converter.quantized_input_stats = {input_arrays[0] : (0., 1.)}
tflite_model = converter.convert()
I am getting below error:
ConverterError: See console for info.
2020-09-23 22:55:27.815650: F tensorflow/lite/toco/tooling_util.cc:1734] Array conv2d_3/Relu, which is an input to the MaxPool operator producing the output array max_pooling2d_3/MaxPool, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.
Fatal Python error: Aborted

You can check out this medium and try to create a training graph and eval graph to QAT 1.x
&lt;denchmark-link:https://medium.com/analytics-vidhya/mobile-inference-b943dc99e29b&gt;https://medium.com/analytics-vidhya/mobile-inference-b943dc99e29b&lt;/denchmark-link&gt;

This GitHub is also a good example &lt;denchmark-link:https://github.com/lusinlu/tensorflow_lite_guide&gt;https://github.com/lusinlu/tensorflow_lite_guide&lt;/denchmark-link&gt;

		</comment>
		<comment id='20' author='MeghnaNatraj' date='2020-09-26T19:45:23Z'>
		
@dtlam26

Yes, I know this for post quantize, but my model is QAT, and it can't inference to int8 on tf2.x. For tf1 it is ok

Please can you tell me how you are able to perform QAT in tf1 ??

I have attached the source for example. However, create eval graph will forget the last layer of your model from the graph. You have to add to the graph a dummy part. Example, tf.maximum(output,1e-27) for regression problems
		</comment>
		<comment id='21' author='MeghnaNatraj' date='2020-09-28T01:04:16Z'>
		&lt;denchmark-link:https://github.com/dtlam26&gt;@dtlam26&lt;/denchmark-link&gt;
 Thanks for the resources. &lt;denchmark-link:https://github.com/Mattrix00&gt;@Mattrix00&lt;/denchmark-link&gt;
 I also found this notebook that is working for me
&lt;denchmark-link:https://colab.research.google.com/drive/15itdlIyLmXISK6SDAzAFGUgjatfVr0Yq&gt;https://colab.research.google.com/drive/15itdlIyLmXISK6SDAzAFGUgjatfVr0Yq&lt;/denchmark-link&gt;

		</comment>
		<comment id='22' author='MeghnaNatraj' date='2020-09-28T07:58:28Z'>
		&lt;denchmark-link:https://github.com/hangrymoon01&gt;@hangrymoon01&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/dtlam26&gt;@dtlam26&lt;/denchmark-link&gt;
 thank you so much for your help !
		</comment>
		<comment id='23' author='MeghnaNatraj' date='2020-10-22T18:41:54Z'>
		Making as resolved due to inactivity. Feel free to reopen it if the issue persists.
		</comment>
		<comment id='24' author='MeghnaNatraj' date='2020-11-24T08:04:52Z'>
		&lt;denchmark-link:https://github.com/msokoloff1&gt;@msokoloff1&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/MeghnaNatraj&gt;@MeghnaNatraj&lt;/denchmark-link&gt;
  I faced the similar isuue in tf 2.4.0-rc0. I even tried latest source for tf and tfmot ; but the issue persists. QAT with tk.keras produces quantize and dequantize layers and we are unable to convert them to full integer quantization models, even after using post training quantization on top of it?
Is there any other workarounds?
		</comment>
		<comment id='25' author='MeghnaNatraj' date='2020-11-24T18:24:26Z'>
		&lt;denchmark-link:https://github.com/anilsathyan7&gt;@anilsathyan7&lt;/denchmark-link&gt;
 could you also post a colab just as &lt;denchmark-link:https://github.com/msokoloff1&gt;@msokoloff1&lt;/denchmark-link&gt;
 did before? It will help us debug this better.
There are no workarounds yet, we will get back to you as soon as we find one.
		</comment>
		<comment id='26' author='MeghnaNatraj' date='2020-11-25T08:55:29Z'>
		Here is a modified version of the post training quantization tf example with QAT: &lt;denchmark-link:https://colab.research.google.com/drive/1aqK5Sd1hy1o55Y1t1MUYQLMgiBwmo2VD?usp=sharing&gt;https://colab.research.google.com/drive/1aqK5Sd1hy1o55Y1t1MUYQLMgiBwmo2VD?usp=sharing&lt;/denchmark-link&gt;

		</comment>
		<comment id='27' author='MeghnaNatraj' date='2020-12-01T10:21:37Z'>
		I used latest tf-nightly and the error was fixed.
&lt;denchmark-code&gt;RuntimeError: Quantization not yet supported for op: 'DEQUANTIZE'.
&lt;/denchmark-code&gt;

TensorFlow version: tf-nightly (2.5.0-dev20201130)
My &lt;denchmark-link:https://colab.research.google.com/drive/1qojOz88mQn3Iv85lbZOS_t_62qrzcX8o?usp=sharing&gt;gist&lt;/denchmark-link&gt;

		</comment>
		<comment id='28' author='MeghnaNatraj' date='2020-12-01T10:56:58Z'>
		Hey &lt;denchmark-link:https://github.com/car1hsiung&gt;@car1hsiung&lt;/denchmark-link&gt;
 , i think your resulting model is 'quantization aware' but not quantized (e.g. the weights are float32 instead of int8). Your input and outputs are still in float32 format. Can you check with the  other two links ??
		</comment>
		<comment id='29' author='MeghnaNatraj' date='2020-12-02T08:26:30Z'>
		
Your input and outputs are still in float32 form

Hi &lt;denchmark-link:https://github.com/anilsathyan7&gt;@anilsathyan7&lt;/denchmark-link&gt;
 , I test the normal and quantized models. 
Please check the updated &lt;denchmark-link:https://colab.research.google.com/drive/1qojOz88mQn3Iv85lbZOS_t_62qrzcX8o?usp=sharing&gt;gist&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;RuntimeError: tensorflow/lite/kernels/quantize.cc:113 affine_quantization-&gt;scale-&gt;size == 1 was not true.Node number 0 (QUANTIZE) failed to prepare.
&lt;/denchmark-code&gt;

BTW, the converter.inference_input_type and converter.inference_output_type can be tf.float32 for quantized model.
For example, using float input/output, you will get:
&lt;denchmark-code&gt;&gt;&gt;&gt; print(interpreter.get_input_details())
[{
	'name': 'conv2d_input',
	'index': 0,
	'shape': array([1, 64, 64, 3], dtype = int32),
	'shape_signature': array([-1, 64, 64, 3], dtype = int32),
	'dtype': &lt; class 'numpy.float32' &gt; ,
	'quantization': (0.0, 0),
	'quantization_parameters': {
		'scales': array([], dtype = float32),
		'zero_points': array([], dtype = int32),
		'quantized_dimension': 0
	},
	'sparsity_parameters': {}
}]

&gt;&gt;&gt; print(interpreter.get_output_details())
[{
	'name': 'Identity',
	'index': 13,
	'shape': array([1, 12, 12, 64], dtype = int32),
	'shape_signature': array([-1, 12, 12, 64], dtype = int32),
	'dtype': &lt; class 'numpy.float32' &gt; ,
	'quantization': (0.0, 0),
	'quantization_parameters': {
		'scales': array([], dtype = float32),
		'zero_points': array([], dtype = int32),
		'quantized_dimension': 0
	},
	'sparsity_parameters': {}
}]
&lt;/denchmark-code&gt;

Using uint8/int8 you will get:
&lt;denchmark-code&gt;&gt;&gt;&gt; print(interpreter.get_input_details())
[{
	'name': 'conv2d_input',
	'index': 0,
	'shape': array([1, 64, 64, 3], dtype = int32),
	'shape_signature': array([-1, 64, 64, 3], dtype = int32),
	'dtype': &lt; class 'numpy.uint8' &gt; ,
	'quantization': (3.921568847431445e-09, 127),
	'quantization_parameters': {
		'scales': array([3.921569e-09], dtype = float32),
		'zero_points': array([127], dtype = int32),
		'quantized_dimension': 0
	},
	'sparsity_parameters': {}
}]

&gt;&gt;&gt; print(interpreter.get_output_details())
[{
	'name': 'Identity',
	'index': 13,
	'shape': array([1, 12, 12, 64], dtype = int32),
	'shape_signature': array([-1, 12, 12, 64], dtype = int32),
	'dtype': &lt; class 'numpy.uint8' &gt; ,
	'quantization': (0.0470588244497776, 128),
	'quantization_parameters': {
		'scales': array([0.04705882], dtype = float32),
		'zero_points': array([128], dtype = int32),
		'quantized_dimension': 0
	},
	'sparsity_parameters': {}
}]
&lt;/denchmark-code&gt;

In other words, you need to quantize input and dequantize output with scales and zero points by yourself if using uint8/int8 input and output.
I can successfully convert quantize uint8 tflite with 2.4.0-rc1 converter._experimental_new_quantizer = True
Please check the &lt;denchmark-link:https://colab.research.google.com/drive/1hPP9SpPB7hVCkT4uxtBF13PABrDWT2V9?usp=sharing&gt;tf-2.4-rc1 gist&lt;/denchmark-link&gt;

		</comment>
		<comment id='30' author='MeghnaNatraj' date='2020-12-02T10:27:12Z'>
		Converting and saving a quantized tflite with QAT using float inputs and outputs was not an issue even in tf 2.3 anyway ...
The issue was with full integer quantization in QAT, so that we can use them with hardware acclerators(int).
Also if you have real train data you need to run &lt;denchmark-link:https://www.tensorflow.org/model_optimization/guide/quantization/training_example#train_and_evaluate_the_model_against_baseline&gt;fit/finetune&lt;/denchmark-link&gt;
 on the qaware model before conversion to get proper quantized model.
Setting converter._experimental_new_quantizer = True, seems to be key here...
Thanks, it worked with tf-nightly version also !!!
		</comment>
		<comment id='31' author='MeghnaNatraj' date='2020-12-15T20:17:12Z'>
		For QAT models, you don't need a representative dataset. Also, full integer quantization support for QAT models (full integer with (default float32)/uint8/int8 input/output) is available from TF 2.4 as shown below:
&lt;denchmark-link:https://colab.research.google.com/gist/MeghnaNatraj/d8742112d2cabf2a4e67764d255cf0dd/normal-vs-qat-models.ipynb&gt;Gist&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;!pip uninstall -q -y tensorflow tensorflow-gpu
!pip install tensorflow==2.4
!pip install -q tensorflow-model-optimization

import tensorflow as tf
print(tf.__version__)

import numpy as np
import tensorflow as tf
import tensorflow_model_optimization as tfmot

def get_model(is_qat=False):
  (train_x, train_y) , (_, _) = tf.keras.datasets.mnist.load_data()
  train_x = train_x.astype('float32') / 255
  model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128,activation='relu'),
    tf.keras.layers.Dense(10)
  ])
  if is_qat:
    model = tfmot.quantization.keras.quantize_model(model)
  model.compile(
      optimizer=tf.keras.optimizers.Adam(0.001),
      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]
  )
  model.fit(train_x, train_y, batch_size=64, epochs=2, verbose=1)
  return model

## 1. Normal TF Model
model = get_model()

# 1a. Convert normal TF model to INT8 quantized TFLite model (default float32 input/output)
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
def representative_dataset_gen():
    for i in range(10):
        yield [np.random.uniform(low=0.0, high=1.0, size=(1, 28, 28)).astype(np.float32)]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.representative_dataset = representative_dataset_gen
normal_tf_model_quantized_tflite_model = converter.convert()

# 1b. Convert normal TF model to INT8 quantized TFLite model (uint8 input/output)
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
def representative_dataset_gen():
    for i in range(10):
        yield [np.random.uniform(low=0.0, high=1.0, size=(1, 28, 28)).astype(np.float32)]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.representative_dataset = representative_dataset_gen
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
normal_tf_model_quantized_with_uint8_io_tflite_model = converter.convert()

## 2. QAT (Quantize Aware Trained) TF model
qat_model = get_model(is_qat=True)

# 2a. Convert QAT TF model to INT8 quantized TFLite model (default float32 input/output)
converter = tf.lite.TFLiteConverter.from_keras_model(qat_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
qat_tf_model_quantized_tflite_model = converter.convert()

# 2b. Convert QAT TF model to INT8 quantized TFLite model (uint8 input/output)
converter = tf.lite.TFLiteConverter.from_keras_model(qat_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
qat_tf_model_quantized_with_uint8_io_tflite_model = converter.convert()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='32' author='MeghnaNatraj' date='2020-12-15T20:17:14Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42082&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42082&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='33' author='MeghnaNatraj' date='2020-12-16T06:02:24Z'>
		&lt;denchmark-link:https://github.com/MeghnaNatraj&gt;@MeghnaNatraj&lt;/denchmark-link&gt;
 In a real training example, should'nt we run a  with a train_images_subset between steps 2 and 2.a/2.b in order to maintain accuracy as mentioned in &lt;denchmark-link:https://www.tensorflow.org/model_optimization/guide/quantization/training_example#train_and_evaluate_the_model_against_baseline&gt;tf  doc&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='34' author='MeghnaNatraj' date='2020-12-16T07:21:08Z'>
		&lt;denchmark-link:https://github.com/anilsathyan7&gt;@anilsathyan7&lt;/denchmark-link&gt;
 yes, you would want to train the model actually so that it can adjust to compensate for the information loss (induced for precision loss).
		</comment>
		<comment id='35' author='MeghnaNatraj' date='2020-12-17T10:12:35Z'>
		&lt;denchmark-link:https://github.com/anilsathyan7&gt;@anilsathyan7&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/sayakpaul&gt;@sayakpaul&lt;/denchmark-link&gt;
 yes! Thanks for pointing that out. I've updated the example to also include model training :)
		</comment>
		<comment id='36' author='MeghnaNatraj' date='2021-01-08T04:34:49Z'>
		I have a little question with the 2.4 quantization, why do the upsampling2d is not supported in tf2 while in tf1 it is acceptable?
RuntimeError: Layer up_sampling2d_10:&lt;class 'tensorflow.python.keras.layers.convolutional.UpSampling2D'&gt; is not supported. 
		</comment>
		<comment id='37' author='MeghnaNatraj' date='2021-01-08T05:44:31Z'>
		&lt;denchmark-link:https://github.com/dtlam26&gt;@dtlam26&lt;/denchmark-link&gt;
  I found that 8 bit qunatization of Upsampling2d is supported with latest tf-optimization &lt;denchmark-link:https://github.com/tensorflow/model-optimization/blob/80ccd9a5945ebca22069962edf828caebe213ccf/tensorflow_model_optimization/python/core/quantization/keras/default_8bit/default_8bit_quantize_registry.py#L114&gt;source&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='38' author='MeghnaNatraj' date='2021-01-08T07:24:27Z'>
		
@dtlam26 I found that 8 bit qunatization of Upsampling2d is supported with latest tf-optimization source.

According to your source, It seems like they skip the quantization at the upsampling2d layer and only quantize the output as I suppose. This can be created if I custom quantize as well. It is just a surprise when they use to provide quantization on this
		</comment>
		<comment id='39' author='MeghnaNatraj' date='2021-01-08T07:57:07Z'>
		&lt;denchmark-link:https://github.com/dtlam26&gt;@dtlam26&lt;/denchmark-link&gt;
 I'am not sure about the internal implementation; but i was able to get past that error and convert the model to tflite with QAT(Upsample2D Resize Bilinear) and tf-nighlty. The results(accuracy) seems to be fine when i test them on sample images.
Anyway they also mention this:-

There are gaps between ResizeBilinear with FakeQuant and
# TFLite quantized ResizeBilinear op. It has a bit more quantization
# error than other ops in this test now.

&lt;denchmark-link:https://user-images.githubusercontent.com/1130185/103989690-18637100-51b6-11eb-88b7-ffaadd646cbe.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='40' author='MeghnaNatraj' date='2021-01-08T08:19:17Z'>
		
@dtlam26 I'am not sure about the internal implementation; but i was able to get past that error and convert the model to tflite with QAT(Upsample2D Resize Bilinear) and tf-nighlty. The results(accuracy) seems to be fine when i test them on sample images.
Anyway they also mention this:-

There are gaps between ResizeBilinear with FakeQuant and
TFLite quantized ResizeBilinear op. It has a bit more quantization
error than other ops in this test now.



Yes, I can bypass that if I self configure the quantization with no quantization in weights and activations as well. No need for the nightly
		</comment>
		<comment id='41' author='MeghnaNatraj' date='2021-01-08T08:23:49Z'>
		&lt;denchmark-link:https://github.com/dtlam26&gt;@dtlam26&lt;/denchmark-link&gt;
 Can you share demo code/model?
		</comment>
		<comment id='42' author='MeghnaNatraj' date='2021-01-08T11:07:56Z'>
		
@dtlam26 Can you share demo code/model?

It is nothing much as you just need to follow the spec in the file you give me in those &lt;denchmark-link:https://github.com/tensorflow/model-optimization/blob/80ccd9a5945ebca22069962edf828caebe213ccf/tensorflow_model_optimization/python/core/quantization/keras/default_8bit/default_8bit_quantize_registry.py#L289-L343&gt;lines&lt;/denchmark-link&gt;
 as only quantize the output, not the weights and activation. I follow tf guide for custom quantization. You can look into &lt;denchmark-link:https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/quantization/keras/QuantizeConfig&gt;here&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;class UpSamplingQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):

    def get_weights_and_quantizers(self, layer):
        return []

    def get_activations_and_quantizers(self, layer):
        return []

    def set_quantize_weights(self, layer, quantize_weights):
        return


    def set_quantize_activations(self, layer, quantize_activations):
        return


    def get_output_quantizers(self, layer):
        return [tfmot.quantization.keras.quantizers.MovingAverageQuantizer(
        num_bits=8, per_axis=False, symmetric=False, narrow_range=False)]

    def get_config(self):
        return {}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='43' author='MeghnaNatraj' date='2021-01-08T11:53:38Z'>
		&lt;denchmark-link:https://github.com/dtlam26&gt;@dtlam26&lt;/denchmark-link&gt;
  Can you share your minimum working example for upsample layers? I just wanted to know how the 'weights and activations' are supposed to get quantized for Upsample/ResizeBilinear Layers.
		</comment>
		<comment id='44' author='MeghnaNatraj' date='2021-01-08T12:44:02Z'>
		
@dtlam26 Can you share your minimum working example for upsample layers? I just wanted to know how the 'weights and activations' are supposed to get quantized for Upsample/ResizeBilinear Layers.

Currently, If you print out the upsampling layer, There is no weight and activation in this layer. Therefore, you have to skip it.
Same thing I used to do with &lt;denchmark-link:https://stackoverflow.com/questions/60883928/quantization-aware-training-in-tensorflow-version-2-and-batchnorm-folding/63559933#63559933&gt;BatchNormalize&lt;/denchmark-link&gt;
 when I answered a while ago
The way of upsampling a layer is depended on the interpolation and researchers when applying quantization consider this as output quantization. The reason behind you can see in a lot of recent papers that the activation and outputs are heavily related to bias term. Bias gives quantization a hard way to maximize accuracy.
Therefore, they usually perform the quantization into 2 main streams: weight quantization for kernel/filter and activation quantization for everything that belongs to bias and post bias.
In tf1.x You can see this protocol quite clear these protocol in the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/contrib/quantize/python/quantize.py&gt;contrib&lt;/denchmark-link&gt;
 and their method of inferencing model. They freeze everything in frozen graph and reduce to 8bits, while in tf2.x they use the representation dataset to scale the certainty for the bias term
		</comment>
		<comment id='45' author='MeghnaNatraj' date='2021-01-08T14:01:40Z'>
		I think both in tf 1.x and tf 2.x the basic approach was to train a non-quantized model until convergence and then fine-tune the trained float model with quantization aware training with training data(or subset). for a few more epochs(may be smaller learning rate). Anyway, i got almost (exactly)same accuracy with Upsample2D(ResizeBilinear) in tf2 with QAT, when compared to plain float model(segmentation).Maybe we should train and compare a fixed model with tf1.x and tf2.x and see if there is difference in accuracy.
		</comment>
		<comment id='46' author='MeghnaNatraj' date='2021-01-08T14:07:12Z'>
		
I think both in tf 1.x and tf 2.x the basic approach was to train a non-quantized model until convergence and then fine-tune the trained float model with quantization aware training with training data(or subset). for a few more epochs(may be smaller learning rate). Anyway, i got almost (exactly)same accuracy with Upsample2D(ResizeBilinear) in tf2 with QAT, when compared to plain float model(segmentation).Maybe we should train and compare a fixed model with tf1.x and tf2.x and see if there is difference in accuracy.

You are almost right with the QAT as its protocol is train a non-quantized model until convergence and then fine-tune the trained float model with quantization aware training. This process will let the quantization bit fit better with the model weights and values. But tensorflow apply the representation dataset with only one purpose of fitting the activation quantization better. This if you use with post quantize can be considered as dynamic quantization. I believe that the tf2,x surely is better as the protocol of training is clear defined and of course, it let the representation dataset adjust for the bias, rather using only MovingAverage
		</comment>
		<comment id='47' author='MeghnaNatraj' date='2021-01-08T14:38:05Z'>
		&lt;denchmark-link:https://github.com/dtlam26&gt;@dtlam26&lt;/denchmark-link&gt;
 Yes, it was from the official tf documentation only... The original issue was with QAT, so i was referring to that version of quantization anyway.  Also, post training quantization generally gives lesser accuray compared to QAT.
		</comment>
		<comment id='48' author='MeghnaNatraj' date='2021-01-08T14:44:42Z'>
		yes QAT is the best to give out the closest accuracy, but a little time consuming when a lot operator are not yet supported or designed in QAT manner. Depending on the structure of the model, sometimes making a post quantization is quicker and saving a lot of time. Im looking forward for future version of tensorflow quantization as they are making those protocol easier to implement
		</comment>
		<comment id='49' author='MeghnaNatraj' date='2021-01-08T14:57:28Z'>
		In my case, i  was looking for a model(full int) with high accuracy while dealing with segmentation, where the model accuracy is critical. Training time was not an issue(atleast a few epochs). Yes, i also hope that more operators will be supported in the future with keras QAT.
		</comment>
	</comments>
</bug>