<bug id='36845' author='YINWIZHANG' open_date='2020-02-18T03:11:12Z' closed_time='2020-03-08T01:08:05Z'>
	<summary>tf.keras.losses.SparseCategoricalCrossentropy within the multi-GPU strategy.scope() generates NAN value</summary>
	<description>

No description provided.

	</description>
	<comments>
		<comment id='1' author='YINWIZHANG' date='2020-02-18T03:15:56Z'>
		In the single-gpu case, I have
&lt;denchmark-code&gt;epoch:0, batch:0, loss:1.0988747737
epoch:0, batch:1, loss:0.9993420934
epoch:0, batch:2, loss:0.9823420342
&lt;/denchmark-code&gt;

In the multi-gpu case, I have the result
&lt;denchmark-code&gt;epoch:0, batch:0, loss:11783.943359375
epoch:0, batch:1, loss:inf
epoch:0, batch:2, loss:inf
epoch:0, batch:3, loss:inf
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='YINWIZHANG' date='2020-02-18T09:23:01Z'>
		&lt;denchmark-link:https://github.com/YINWIZHANG&gt;@YINWIZHANG&lt;/denchmark-link&gt;

Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.
Request you to fill the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/new/choose&gt;the Github new issue template&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='3' author='YINWIZHANG' date='2020-02-18T13:32:24Z'>
		
@YINWIZHANG
Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.
Request you to fill the the Github new issue template.Thanks!

I am using ubuntu 18.04 and tensorflow 2.0.0. My PC is a Lambda stack which has 4 RTX8000 GPUs.
		</comment>
		<comment id='4' author='YINWIZHANG' date='2020-02-22T01:50:32Z'>
		&lt;denchmark-link:https://github.com/YINWIZHANG&gt;@YINWIZHANG&lt;/denchmark-link&gt;
 In order to help us investigate this issue, can you provide the following extra bits of information:

Details of your resnet_fcn1
Details of your training dataset, train_ds.

If there can be actual code that forms a minimal reproduction case it'll be great.
		</comment>
		<comment id='5' author='YINWIZHANG' date='2020-02-26T13:41:09Z'>
		&lt;denchmark-link:https://github.com/YINWIZHANG&gt;@YINWIZHANG&lt;/denchmark-link&gt;
 I experienced a similar problem. At first the loss exploded, after some day the loss stalled and at some point, everything just started crashing. In the end is was a defective GPU.
It doesn't mean you have the same problem, but maybe those tipps will help you narrow it down :


Does every GPU work fine? Did you try using tf.device('device:GPU:1'): or 2,3. ?


Test with gpu-burn to check, if all GPUs work properly.


		</comment>
		<comment id='6' author='YINWIZHANG' date='2020-02-26T18:56:17Z'>
		
@YINWIZHANG I experienced a similar problem. At first the loss exploded, after some day the loss stalled and at some point, everything just started crashing. In the end is was a defective GPU.
It doesn't mean you have the same problem, but maybe those tipps will help you narrow it down :
* Does every GPU work fine? Did you try using `tf.device('device:GPU:1'):` or 2,3. ?

* Test with [gpu-burn](https://github.com/wilicc/gpu-burn) to check, if all GPUs work properly.


Thanks for your comments! It turns out that I have all my GPUs working fine. A really strange thing is that, even when I use above multiGPU code in a machine with single GPU, the loss value explodes.
		</comment>
		<comment id='7' author='YINWIZHANG' date='2020-03-07T18:27:04Z'>
		&lt;denchmark-link:https://github.com/YINWIZHANG&gt;@YINWIZHANG&lt;/denchmark-link&gt;

Can you try modifying your loss function under the MirroredStrategy as follows:
def compute_loss(labels, predictions):
  per_example_loss = loss_object(labels, predictions)
  per_example_loss /= tf.cast(
      tf.shape(labels)[1] * tf.shape(labels)[2], tf.float32)  # &lt;--- See if this fixes the NaNs.
  return tf.nn.compute_average_loss(
      per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)
		</comment>
		<comment id='8' author='YINWIZHANG' date='2020-03-08T01:08:01Z'>
		
@YINWIZHANG
Can you try modifying your loss function under the MirroredStrategy as follows:
def compute_loss(labels, predictions):
  per_example_loss = loss_object(labels, predictions)
  per_example_loss /= tf.cast(
      tf.shape(labels)[1] * tf.shape(labels)[2], tf.float32)  # &lt;--- See if this fixes the NaNs.
  return tf.nn.compute_average_loss(
      per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)

Thanks! It solves the problem!
		</comment>
		<comment id='9' author='YINWIZHANG' date='2020-03-08T01:08:06Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36845&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36845&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>