<bug id='29675' author='s4sarath' open_date='2019-06-12T02:11:24Z' closed_time='2019-06-21T01:06:35Z'>
	<summary>Adam/Adagrad gives different derivatives when initializing globaly and localy</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
TensorFlow installed from (source or binary):
TensorFlow version (2.0):
Python version:(3.6)

Describe the current behavior
Adam and Adagrad, both gives different losses, when initialized once (globally) and initialized every time train function is called.
&lt;denchmark-code&gt;#### Locally
def train2(model, inputs, outputs, learning_rate):
    trainable_variables = [model.W, model.b]
    with tf.GradientTape() as t:
        current_loss = loss(model(inputs), outputs)
    optimizer = tf.keras.optimizers.Adagrad(
    learning_rate=learning_rate,
    initial_accumulator_value=0.1,
    epsilon=1e-07,
)
    gradients = t.gradient(current_loss,  trainable_variables)    
    optimizer.apply_gradients(zip(gradients, trainable_variables))
#----------------------------------------------------------------------------------------------------
#### Globally

optimizer_global = tf.keras.optimizers.Adagrad(
    learning_rate=0.1,
    initial_accumulator_value=0.1,
    epsilon=1e-07,
)

def train(model, inputs, outputs, learning_rate):
    trainable_variables = [model.W, model.b]
    with tf.GradientTape() as t:
        current_loss = loss(model(inputs), outputs)
    
    gradients = t.gradient(current_loss,  trainable_variables)    
    optimizer_global.apply_gradients(zip(gradients, trainable_variables))
&lt;/denchmark-code&gt;

Describe the expected behavior
We expect same loss from both train and train2. ( Is it because, some internal parameters are changing inside the optimizer, because SGD optimizer works fine in both the cases).

The plot of derivatives are provided in the image . Full code to reproduce is in the .ipynb file.
&lt;denchmark-link:https://user-images.githubusercontent.com/10637096/59318400-9151cc80-8ce4-11e9-9b65-1712a86d2218.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;#!/usr/bin/env python
# coding: utf-8



import tensorflow as tf
tf.compat.v1.set_random_seed(1)
import numpy as np
tf.__version__



class Model(object):
  def __init__(self):
    # Initialize variable to (5.0, 0.0)
    # In practice, these should be initialized to random values.
    self.W = tf.Variable(5.0)
    self.b = tf.Variable(0.0)
    
  def __call__(self, x):
    return self.W * x + self.b
  
model = Model()
def loss(predicted_y, desired_y):
    return tf.reduce_mean(tf.square(predicted_y - desired_y))

TRUE_W = 3.0
TRUE_b = 2.0
NUM_EXAMPLES = 1000

inputs  = tf.random.normal(shape=[NUM_EXAMPLES])
noise   = tf.random.normal(shape=[NUM_EXAMPLES])
outputs = inputs * TRUE_W + TRUE_b + noise # 3 x + 2 + noise()

import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')
plt.scatter(inputs, outputs, c='b')
plt.scatter(inputs, model(inputs), c='r')
plt.show()

def train2(model, inputs, outputs, learning_rate):
    trainable_variables = [model.W, model.b]
    with tf.GradientTape() as t:
        current_loss = loss(model(inputs), outputs)
    optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, 
                                         epsilon=1e-9)
    gradients = t.gradient(current_loss,  trainable_variables)    
    optimizer.apply_gradients(zip(gradients, trainable_variables))
    
#-----------------------------------------------------------------------------------------------

optimizer_global = tf.keras.optimizers.Adam(learning_rate=0.1, beta_1=0.9, beta_2=0.98, 
                                         epsilon=1e-9)
def train(model, inputs, outputs, learning_rate):
    trainable_variables = [model.W, model.b]
    with tf.GradientTape() as t:
        current_loss = loss(model(inputs), outputs)
    
    gradients = t.gradient(current_loss,  trainable_variables)    
    optimizer_global.apply_gradients(zip(gradients, trainable_variables))

model = Model()

# Collect the history of W-values and b-values to plot later
Ws, bs = [], []
epochs = range(100)
for epoch in epochs:
  Ws.append(model.W.numpy())
  bs.append(model.b.numpy())
  current_loss = loss(model(inputs), outputs)

  train(model, inputs, outputs, learning_rate=0.1)
  print('Epoch %2d: W=%1.2f b=%1.2f, loss=%2.5f' %
        (epoch, Ws[-1], bs[-1], current_loss))

# Let's plot it all
plt.plot(epochs, Ws, 'r',
         epochs, bs, 'b')
plt.plot([TRUE_W] * len(epochs), 'r--',
         [TRUE_b] * len(epochs), 'b--')
plt.legend(['W', 'b', 'true W', 'true_b'])
plt.show()
  

#---------------------------------------------------------

model = Model()

# Collect the history of W-values and b-values to plot later
Ws, bs = [], []
epochs = range(100)
for epoch in epochs:
  Ws.append(model.W.numpy())
  bs.append(model.b.numpy())
  current_loss = loss(model(inputs), outputs)

  train2(model, inputs, outputs, learning_rate=0.1)
  print('Epoch %2d: W=%1.2f b=%1.2f, loss=%2.5f' %
        (epoch, Ws[-1], bs[-1], current_loss))

# Let's plot it all
plt.plot(epochs, Ws, 'r',
         epochs, bs, 'b')
plt.plot([TRUE_W] * len(epochs), 'r--',
         [TRUE_b] * len(epochs), 'b--')
plt.legend(['W', 'b', 'true W', 'true_b'])
plt.show()
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='s4sarath' date='2019-06-19T10:48:37Z'>
		Was able to reproduce the reported issue on Colab with Tensorflow 2.0.0.beta0. Thanks!
		</comment>
		<comment id='2' author='s4sarath' date='2019-06-21T01:06:35Z'>
		Adam is stateful, if you create it everytime calling train2, its behavior is meant to be different than if you create a global optimizer.
		</comment>
		<comment id='3' author='s4sarath' date='2019-06-21T01:06:36Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29675&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29675&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='s4sarath' date='2019-06-22T02:47:56Z'>
		&lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
 - Thanks. Which is the preferred method ? Initializing globally or at each time train function is called.
		</comment>
		<comment id='5' author='s4sarath' date='2019-06-22T02:55:32Z'>
		Initializing globally
		</comment>
		<comment id='6' author='s4sarath' date='2019-08-22T14:22:07Z'>
		&lt;denchmark-link:https://github.com/s4sarath&gt;@s4sarath&lt;/denchmark-link&gt;
  Hi, I have a question about other script ï¼ŒWhen run ...'tf.compat.v1.set_random_seed(1)', it will raise error'module 'tensorflow.compat' has no attribute 'v1'', Can you help me? Waiting your reply!
		</comment>
		<comment id='7' author='s4sarath' date='2019-08-22T15:12:56Z'>
		Depending on your tf version. compat.v1 is introduced only after 1.14. So if you're in 1.14 you shouldn't see the error. Otherwise it would pop up.
		</comment>
	</comments>
</bug>