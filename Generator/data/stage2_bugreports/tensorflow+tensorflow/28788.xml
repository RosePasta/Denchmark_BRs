<bug id='28788' author='xiaohu2015' open_date='2019-05-17T04:54:05Z' closed_time='2019-11-08T18:25:00Z'>
	<summary>Bug in tf,keras.layers.Conv2D when use dilation_rate</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
import tensorflow as tf # TF2

class Conv2D_BN_ReLU(tf.keras.Model):
    """Conv2D + BN + ReLU"""
    def __init__(self,
            filters,
            kernel_size,
            strides=1,
            padding="SAME",
            dilation_rate=1,
            use_bias=False,
            kernel_initializer="he_normal",
            kernel_regularizer=None,
            **bn_params):
        super(Conv2D_BN_ReLU, self).__init__()

        self.conv = tf.keras.layers.Conv2D(filters, kernel_size, strides=strides,
                padding=padding, dilation_rate=dilation_rate, use_bias=use_bias,
                kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer)
        self.bn = tf.keras.layers.BatchNormalization(**bn_params)

    def call(self, x, training=None):
        x = self.conv(x)
        x = tf.nn.relu(self.bn(x, training=training))
        return x

if __name__ == "__main__":
    import os
    os.environ["CUDA_VISIBLE_DEVICES"] = "6"

    net = Conv2D_BN_ReLU(64, 1, 1, dilation_rate=6)
    x = tf.ones((1, 32, 32, 64))
    y = net(x)

    x = tf.ones((1, 64, 64, 64))
    y = net(x)
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
TensorFlow version (use command below): 2.0
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
tensorflow.python.framework.errors_impl.InvalidArgumentError: padded_shape[0]=68 is not divisible by block_shape[0]=6 [Op:SpaceToBatchND]
Describe the expected behavior
I think the second evaluation should also work, because I use padding="SAME"
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='xiaohu2015' date='2019-05-17T07:56:22Z'>
		&lt;denchmark-link:https://github.com/xiaohu2015&gt;@xiaohu2015&lt;/denchmark-link&gt;
 Able to reproduce the issue with the provided code.
InvalidArgumentError: padded_shape[0]=68 is not divisible by block_shape[0]=6 [Op:SpaceToBatchND]
		</comment>
		<comment id='2' author='xiaohu2015' date='2019-05-17T09:09:38Z'>
		&lt;denchmark-link:https://github.com/muddham&gt;@muddham&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 I think the padding="SAME" not work with expectation, it is really a big trouble. The reason behind this, I think it is "deferring weight creation until the shape of the inputs is known" :
class Linear(layers.Layer):

  def __init__(self, units=32):
    super(Linear, self).__init__()
    self.units = units

  def build(self, input_shape):
    self.w = self.add_weight(shape=(input_shape[-1], self.units),
                             initializer='random_normal',
                             trainable=True)
    self.b = self.add_weight(shape=(self.units,),
                             initializer='random_normal',
                             trainable=True)

  def call(self, inputs):
    return tf.matmul(inputs, self.w) + self.b
The call method of your layer will automatically run build the first time it is called. You now have a layer that's lazy and easy to use.
but for different input shape, maybe the configuration of the first call is improper
		</comment>
		<comment id='3' author='xiaohu2015' date='2019-05-19T05:13:50Z'>
		Looks like the same bug I reported 2 months ago: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/26797&gt;#26797&lt;/denchmark-link&gt;

I've already switched to my own implementation of dilated conv &lt;denchmark-link:https://github.com/tensorpack/tensorpack/commit/353cd04fe81ea8920bc67f702bf492174b000768&gt;tensorpack/tensorpack@353cd04&lt;/denchmark-link&gt;
 since they are slow at fixing bugs.
		</comment>
		<comment id='4' author='xiaohu2015' date='2019-06-04T07:20:08Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 something update???
		</comment>
		<comment id='5' author='xiaohu2015' date='2019-10-16T23:19:09Z'>
		having the same issue for tf.keras.layers.conv1D, can't train with data that have various lengths...
		</comment>
		<comment id='6' author='xiaohu2015' date='2019-10-25T21:22:22Z'>
		Thanks for reporting the issue! We have a fix for the issue. Can you run your code with tf-nightly to verify it? Let us know if the issue still exists.
		</comment>
		<comment id='7' author='xiaohu2015' date='2019-11-08T18:25:00Z'>
		Close the issue for now as the fix has been submitted.
		</comment>
		<comment id='8' author='xiaohu2015' date='2019-11-08T18:25:01Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28788&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28788&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='xiaohu2015' date='2019-11-08T23:45:58Z'>
		Hi &lt;denchmark-link:https://github.com/yhliang2018&gt;@yhliang2018&lt;/denchmark-link&gt;
 , thank you for helping with this. We have a question about how to install the tf-nightly you mentioned above. We tried both  and  but still get that error.
We dig a little bit about why we can't it fails and found that this function &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/space_to_batch_nd&gt;tf.space_to_batch_nd&lt;/denchmark-link&gt;
 will be called when using dilation conv, which requires the length of the input should be divisible by the dilation rate. TF will calculate the number of padding needed to make it divisible using  just once based on the first input of the model. However since that calculation only happens once, for the second input with different lengths, TF will still do the same number of padding, which with high chance the length of the input is still not divisible by the dilation rate and throw that error.
		</comment>
		<comment id='10' author='xiaohu2015' date='2019-11-09T00:16:12Z'>
		Hi &lt;denchmark-link:https://github.com/yuanzhedong&gt;@yuanzhedong&lt;/denchmark-link&gt;
 Yes, you are right. What you descripted was indeed the root cause, and &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/7a07f274d65bed12b91b6c6dbe4987a6e1649429/tensorflow/python/keras/layers/convolutional.py#L196&gt;our fix&lt;/denchmark-link&gt;
 to recreate conv_op when a different input is provided is to resolve this issue. I have verified the code example &lt;denchmark-link:https://github.com/xiaohu2015&gt;@xiaohu2015&lt;/denchmark-link&gt;
 provided works correctly with tf-nightly &lt;denchmark-link:https://colab.research.google.com/gist/yhliang2018/ba449f1299ed1476c3ad06560e00f403/verify-github-issue-28788.ipynb&gt;here&lt;/denchmark-link&gt;
. What error do you get for your use case? Could you share a minimal code to reproduce that? Thanks!
		</comment>
		<comment id='11' author='xiaohu2015' date='2019-11-09T22:58:46Z'>
		Thank you for sharing the colab! I find  installed an older version on my own laptop. Retried on ec2 instance and it correctly installs the same version as yours. The code shared by &lt;denchmark-link:https://github.com/xiaohu2015&gt;@xiaohu2015&lt;/denchmark-link&gt;
 works now!!
But when I run my training code (which was working with tf version 2.0.0 if I pad all the inputs to the same length) now fails after two epochs with another error pops up:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "models/ms_tcn/tensorflow/model.py", line 245, in &lt;module&gt;
    app.run(train_ms_tcn)
  File "/home/ubuntu/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/absl/app.py", line 299, in run
    _run_main(main, args)
  File "/home/ubuntu/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "models/ms_tcn/tensorflow/model.py", line 237, in train_ms_tcn
    tf.TensorSpec(shape=[], dtype=tf.bool, name="training"),
  File "/home/ubuntu/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py", line 909, in save
    meta_graph_def, saveable_view, signatures, options.namespace_whitelist)
  File "/home/ubuntu/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py", line 553, in _fill_meta_graph_\
def
    object_map, resource_map, asset_info = saveable_view.map_resources()
  File "/home/ubuntu/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py", line 280, in map_resources
    capture_constant_value = tensor_util.constant_value(capture)
  File "/home/ubuntu/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_util.py", line 827, in constant_val\
ue
    ret = _ConstantValue(tensor, partial)
  File "/home/ubuntu/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_util.py", line 673, in _ConstantVal\
ue
    if tensor.op.type == "Const":
  File "/home/ubuntu/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 2220, in type
    return c_api.TF_OperationOpType(self._c_op)
AttributeError: 'Operation' object has no attribute '_c_op'

&lt;/denchmark-code&gt;

Seems not related with this issue anymore? We will keep investigating.
By the way, do you know when this fix will be officially released?
		</comment>
		<comment id='12' author='xiaohu2015' date='2019-11-11T17:35:02Z'>
		The fix should be included in TF 2.1.0.
Looking at the error message, and looks like it means some non-Eager tensors are accessed outside of tf.function. Not sure if it's caused by this fix. Could you share your code to reproduce this error by opening another issue? I can dig more into it. Thanks!
		</comment>
		<comment id='13' author='xiaohu2015' date='2019-11-11T18:31:06Z'>
		Hi &lt;denchmark-link:https://github.com/yhliang2018&gt;@yhliang2018&lt;/denchmark-link&gt;
, somehow when I use , the error still exists,
&lt;denchmark-code&gt;tensorflow.python.framework.errors_impl.InvalidArgumentError: padded_shape[0]=26 is not divisible by block_shape[0]=4 [Op:SpaceToBatchND]
&lt;/denchmark-code&gt;

the code below  should be able to reproduce that error.
&lt;denchmark-code&gt;from absl import app
from absl import flags
import numpy as np
import os


import tensorflow as tf
from tensorflow import keras

from tensorflow.keras import optimizers
from tensorflow.keras import losses
from tensorflow.keras.layers import Layer
from tensorflow.keras.layers import Activation, Lambda, add, Softmax, ReLU, Concatenate
from tensorflow.keras.layers import Conv1D, Dropout, Dense, MaxPooling1D, GlobalAvgPool1D

print(tf.__version__) #2.1.0-dev20191111

def get_gen():
    x_train = [np.ones((1, 16, 1024)),
               np.ones((1, 130, 1024))]
    y_train = np.ones((2, 1))
    while True:
        for i, x in enumerate(x_train):
            print(x.shape)
            yield x, y_train[i]

train_gen = get_gen()

model = tf.keras.models.Sequential()
model.add(Conv1D(32, 3,
                 activation='relu',
                 #input_shape = (None, 1024),                                                                                                          
                 padding = "same",
))

model.add(Conv1D(32, 3,
                 activation='relu',
                 #input_shape = (None, 1024),                                                                                                          
                 padding = "same",
                 dilation_rate=2
))

model.add(Conv1D(2, 1, padding="same"))
model.add(GlobalAvgPool1D())

optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)

train_epoch_loss = []
for features, labels in train_gen:
    with tf.GradientTape() as tape:
        predictions = model(features, training=True)
        loss = tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)

    grads = tape.gradient(loss, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))

    train_epoch_loss.append(loss.numpy())

    print("Train_Loss_EPOCH:", sum(train_epoch_loss) / len(train_epoch_loss))
&lt;/denchmark-code&gt;

		</comment>
		<comment id='14' author='xiaohu2015' date='2019-11-12T22:21:47Z'>
		&lt;denchmark-link:https://github.com/yuanzhedong&gt;@yuanzhedong&lt;/denchmark-link&gt;
 The issue should be fixed for all conv layers, and I cannot reproduce the error you mentioned by &lt;denchmark-link:https://colab.research.google.com/gist/yhliang2018/7eabaa3a107a063741e552dea3ac234c/verify-github-issue-28788-conv1d.ipynb&gt;running your code&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='15' author='xiaohu2015' date='2019-11-13T01:50:20Z'>
		
@yuanzhedong The issue should be fixed for all conv layers, and I cannot reproduce the error you mentioned by running your code.

Interesting...I tried both my local env(ubuntu18.04 + rtx 2080) and ec2 instance(ubuntu16.04 + k80) and the error is still there, I use conda virtual env, not sure if that's the issue
&lt;denchmark-code&gt;conda create -n=tf-nightly python=3.6
conda activate tf-nightly
pip install tf-nightly
# check the tf version is correct but still crash
&lt;/denchmark-code&gt;

		</comment>
		<comment id='16' author='xiaohu2015' date='2019-11-13T02:36:55Z'>
		Not quite related, but do you know how to set GPU env for codelab? I changed the runtime to be GPU but seems the code is running on CPU?
&lt;denchmark-link:https://user-images.githubusercontent.com/5069709/68728134-43830e00-057b-11ea-802a-b474c826a502.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='17' author='xiaohu2015' date='2019-11-13T17:39:40Z'>
		&lt;denchmark-link:https://github.com/yuanzhedong&gt;@yuanzhedong&lt;/denchmark-link&gt;
 can you open another thread for your second question? This issue has been closed for a while. We try to resolve one issue in one thread, and this will help other people with similar problems. Thanks!
		</comment>
		<comment id='18' author='xiaohu2015' date='2019-11-13T19:48:56Z'>
		Sure, thank you for your help!
		</comment>
		<comment id='19' author='xiaohu2015' date='2020-05-01T14:17:13Z'>
		Hi, the issue is still there on tf 2.1.0, is there any proper way to change the code while still making code upgradable to next version (maintainability is preferred...)?
Typical error is
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py", line 917, in run self.main_result = self.main_fn(*self.main_args, **self.main_kwargs) File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py", line 292, in wrapper return func(*args, **kwargs) File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", line 433, in train_on_batch output_loss_metrics=model._output_loss_metrics) File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_eager.py", line 312, in train_on_batch output_loss_metrics=output_loss_metrics)) File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_eager.py", line 253, in _process_single_batch training=training)) File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_eager.py", line 127, in _model_loss outs = model(inputs, **kwargs) File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py", line 778, in __call__ outputs = call_fn(cast_inputs, *args, **kwargs) File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py", line 717, in call convert_kwargs_to_constants=base_layer_utils.call_context().saving) File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py", line 891, in _run_internal_graph output_tensors = layer(computed_tensors, **kwargs) File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py", line 778, in __call__ outputs = call_fn(cast_inputs, *args, **kwargs) File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/convolutional.py", line 209, in call outputs = self._convolution_op(inputs, self.kernel) File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py", line 1135, in __call__ return self.conv_op(inp, filter) File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py", line 640, in __call__ return self.call(inp, filter) File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py", line 616, in _with_space_to_batch_call block_shape=self.dilation_rate) File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py", line 3303, in required_space_to_batch_paddings const_block_shape = tensor_util.constant_value(block_shape) File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_util.py", line 827, in constant_value ret = _ConstantValue(tensor, partial) File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_util.py", line 673, in _ConstantValue if tensor.op.type == "Const": File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 2220, in type return c_api.TF_OperationOpType(self._c_op) AttributeError: 'Operation' object has no attribute '_c_op'
		</comment>
		<comment id='20' author='xiaohu2015' date='2020-06-06T08:34:18Z'>
		&lt;denchmark-link:https://github.com/yhliang2018&gt;@yhliang2018&lt;/denchmark-link&gt;
 in tf 2.2 the error is still there.
This is the output i get when calling a conv2d with dilation=16. Where the input is in parentheses
&lt;denchmark-code&gt;(6, 12, 16, 256)
Step : 1 - Loss is : 3.9648172855377197
(6, 8, 16, 256)
Step : 2 - Loss is : 3.227323293685913
(6, 20, 16, 256)
Step : 3 - Loss is : 2.098637819290161
(6, 12, 16, 256)
2020-06-06 10:18:51.440173: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at spacetobatch_op.cc:219 : Invalid argument: padded_shape[0]=56 is not divisible by block_shape[0]=16

&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>