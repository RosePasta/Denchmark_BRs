<bug id='29791' author='ssable' open_date='2019-06-14T10:24:10Z' closed_time='2020-01-10T17:59:20Z'>
	<summary>AttributeError: 'KerasTPUModel' object has no attribute '_ckpt_saved_epoch'</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.14.0-rc1
Python version: 3.6.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Code is running in Colab and was working fine with tensorflow 1.13.1.
Colab was updated to tensorflow 1.14.0-rc1 today apparently which is causing the regression.
Running tpu_model.fit for a TPU model that was generated with tf.contrib.tpu.keras_to_tpu_model, I get:
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

AttributeError                            Traceback (most recent call last)
 in ()
14
15 print(datetime.now())
---&gt; 16 history = tpu_model.fit(train_dataset_fn, epochs=nb_epochs, steps_per_epoch=total_samples // batch_size, validation_data=val_dataset_fn, validation_steps=batch_size) # , verbose=2)
17 print(datetime.now())
3 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in configure_callbacks(callbacks, model, do_validation, batch_size, epochs, steps_per_epoch, samples, verbose, count_mode, mode)
118   callback_list.model.stop_training = False
119   # pylint: disable=protected-access
--&gt; 120   if callback_list.model._ckpt_saved_epoch is not None:
121     # The attribute _ckpt_saved_epoch is supposed to be None at the start of
122     # training (it should be made None at the end of successful multi-worker
AttributeError: 'KerasTPUModel' object has no attribute '_ckpt_saved_epoch'
	</description>
	<comments>
		<comment id='1' author='ssable' date='2019-06-17T10:23:29Z'>
		&lt;denchmark-link:https://github.com/ssable&gt;@ssable&lt;/denchmark-link&gt;
 In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!
		</comment>
		<comment id='2' author='ssable' date='2019-06-17T12:02:39Z'>
		I tried to reproduce the issue with a minimal code snippet, but while doing that I found another bug &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29798&gt;#29798&lt;/denchmark-link&gt;
.
Once that other bug is resolved, the code provided there should raise this issue or I should be able to come up with a simple example how to reproduce.
		</comment>
		<comment id='3' author='ssable' date='2019-06-17T12:56:46Z'>
		I have the same issue. This code should raise the error. Instead, in version 1.13.1 should work fine. I tried it in colab (1.14.0-rc1).
&lt;denchmark-code&gt;#!pip install -q tensorflow==1.13.1
import tensorflow as tf
import numpy as np
import os

print(tf.__version__)

tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']

randomImages = np.random.rand(128,50,50,3)
randomLabels = np.random.randint(2, size=128)

#model
inputs = tf.keras.layers.Input(shape=(50, 50, 3))
outputLayer = inputs
for i in range(0,3):
  outputLayer = tf.keras.layers.Conv2D(filters=64, kernel_size = 3, padding = 'valid')(outputLayer)
  outputLayer = tf.keras.layers.BatchNormalization()(outputLayer)
  outputLayer = tf.keras.layers.ReLU()(outputLayer)
outputLayer = tf.keras.layers.Flatten()(outputLayer)
outputLayer = tf.keras.layers.Dense(2)(outputLayer)

model = tf.keras.Model(inputs = inputs, outputs = outputLayer)

#tpu_model 
tpu_model = tf.contrib.tpu.keras_to_tpu_model(
    model,
    strategy=tf.contrib.tpu.TPUDistributionStrategy(
    tf.contrib.cluster_resolver.TPUClusterResolver(tpu_address))
  )

tpu_model.compile(
    optimizer=tf.train.AdamOptimizer(learning_rate = 0.0001),
    loss=tf.keras.losses.sparse_categorical_crossentropy,
    metrics=['sparse_categorical_accuracy']
  )


tpu_model.fit(
              x = randomImages,
              y = randomLabels,
              batch_size = 64,
              epochs = 10
            )
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='ssable' date='2019-06-18T11:19:10Z'>
		&lt;denchmark-link:https://github.com/giovannibaratta&gt;@giovannibaratta&lt;/denchmark-link&gt;
 I tried reproducing the issue on colab with Tf 1.14.1 but I am getting different error, ValueError: Variable tpu_140309960546512//kernel/0 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope?.
Is this same error what you are getting?. Thanks!
		</comment>
		<comment id='5' author='ssable' date='2019-06-18T11:40:02Z'>
		
@giovannibaratta I tried reproducing the issue on colab with Tf 1.14.1 but I am getting different error, ValueError: Variable tpu_140309960546512//kernel/0 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope?.
Is this same error what you are getting?. Thanks!

No, I get this error :
&lt;denchmark-code&gt;---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-1-54cc81aef34f&gt; in &lt;module&gt;()
     40               y = randomLabels,
     41               batch_size = 64,
---&gt; 42               epochs = 10
     43             )



3 frames

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in configure_callbacks(callbacks, model, do_validation, batch_size, epochs, steps_per_epoch, samples, verbose, count_mode, mode)
    118   callback_list.model.stop_training = False
    119   # pylint: disable=protected-access
--&gt; 120   if callback_list.model._ckpt_saved_epoch is not None:
    121     # The attribute `_ckpt_saved_epoch` is supposed to be None at the start of
    122     # training (it should be made None at the end of successful multi-worker

AttributeError: 'KerasTPUModel' object has no attribute '_ckpt_saved_epoch'
&lt;/denchmark-code&gt;

There are also some warnings ("Keras support is now deprecated in support of TPU Strategy") but it's not clear to me how I should migrate to the new distribution strategy.
		</comment>
		<comment id='6' author='ssable' date='2019-06-19T10:54:52Z'>
		I get exactly the same error
%%time  history=tpu_model.fit_generator( train_generator(), steps_per_epoch = 150, validation_data = test_generator(), validation_steps = 30, epochs=15, callbacks=[cp_callback] )
`---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
 in ()
----&gt; 1 get_ipython().run_cell_magic('time', '', 'history=tpu_model.fit_generator(\n                           train_generator(),\n                           steps_per_epoch = 150,\n                           validation_data = test_generator(),\n                           validation_steps = 30,\n                           epochs=15,\n                           callbacks=[cp_callback]\n)')
5 frames
&lt;/usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60&gt; in time(self, line, cell, local_ns)
 in ()
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in configure_callbacks(callbacks, model, do_validation, batch_size, epochs, steps_per_epoch, samples, verbose, count_mode, mode)
118   callback_list.model.stop_training = False
119   # pylint: disable=protected-access
--&gt; 120   if callback_list.model._ckpt_saved_epoch is not None:
121     # The attribute _ckpt_saved_epoch is supposed to be None at the start of
122     # training (it should be made None at the end of successful multi-worker
AttributeError: 'KerasTPUModel' object has no attribute '_ckpt_saved_epoch'`
		</comment>
		<comment id='7' author='ssable' date='2019-06-20T06:03:31Z'>
		I could able to reproduce the issue on Colab with tensorflow 1.14.0-rc1. Thanks!
		</comment>
		<comment id='8' author='ssable' date='2019-06-20T09:28:34Z'>
		Following the MNIST example (&lt;denchmark-link:https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/fashion_mnist.ipynb#scrollTo=xLeZATVaNAnE&gt;https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/fashion_mnist.ipynb#scrollTo=xLeZATVaNAnE&lt;/denchmark-link&gt;
) I was able to run the code in the new version.
&lt;denchmark-code&gt;resolver = tf.contrib.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])
tf.contrib.distribute.initialize_tpu_system(resolver)
strategy = tf.contrib.distribute.TPUStrategy(resolver)

with strategy.scope():
  #define model
  #compile model
  #fit model
&lt;/denchmark-code&gt;

Now, in my code (not the code posted above), I have an other issue related to tf.keras.layers.concatenate that seems to be not implemented, or at least this is what the error says.
		</comment>
		<comment id='9' author='ssable' date='2019-07-03T16:58:10Z'>
		If I rollback to tensorflow 1.13.1 (!pip install tensorflow==1.13.1) my code (above) works in colaboratory, however it becomes very slow. In fact it is quicker to run on GPU
		</comment>
		<comment id='10' author='ssable' date='2019-07-17T16:16:18Z'>
		&lt;denchmark-link:https://github.com/javicivit&gt;@javicivit&lt;/denchmark-link&gt;
 I am not completely sure since I never run code on TPU, but perhaps you should install tensorflow-gpu  (!pip install tensorflow-gpu==1.13.1)?
		</comment>
		<comment id='11' author='ssable' date='2019-08-02T14:43:08Z'>
		
@javicivit I am not completely sure since I never run code on TPU, but perhaps you should install tensorflow-gpu (!pip install tensorflow-gpu==1.13.1)?

Not if we are running on TPU, which is when this error arise. TPU uses tensorflow and not tensorflow-gpu
		</comment>
		<comment id='12' author='ssable' date='2020-01-10T17:59:20Z'>
		By default the training loop for distribution strategy is running on the Colab host, not the TPU host. For larger models this isn't a problem, but for small models like MNIST, the latency between the 2 hosts can dominate the model execution time.
I believe DistributionStrategy performance has improved in the upcoming TF 2.1 release as well. I'm closing this for now as I believe the original question is answered, feel free to re-open if I missed something.
		</comment>
		<comment id='13' author='ssable' date='2020-01-10T17:59:22Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29791&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29791&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>