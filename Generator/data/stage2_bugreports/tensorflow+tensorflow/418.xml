<bug id='418' author='hugman' open_date='2015-12-05T17:25:45Z' closed_time='2016-06-06T18:25:05Z'>
	<summary>Tensor slice or indexing with tensor i, j</summary>
	<description>
What I want to do is retrieving(or slicing 1-element) from tensor with tensor indices.
for example,
&lt;denchmark-code&gt;data = tf.constant( [ [1,2,3], [4,5,6] ] )
i = tf.constant(2)
j = tf.constant(1)

k = data[i,j]    # error
l = tf.gather( tf.gather(data, i) , j) # ok. but generate errors when gradient optimization process..
&lt;/denchmark-code&gt;

sess.run(k, ...) generates bad slice errors.
&lt;denchmark-code&gt; data[i,j]
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py", line 129, in _SliceHelper
    raise TypeError("Bad slice index %s of type %s" % (s, type(s)))
&lt;/denchmark-code&gt;

tf.gather is ok to slice element, but it yields errors when training time (tf.train.GradientDescentOptimizer.minimize)
Any tips to work-around this problem?
	</description>
	<comments>
		<comment id='1' author='hugman' date='2015-12-05T22:05:33Z'>
		We're tracking our current lack of indexing fanciness here: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/206&gt;#206&lt;/denchmark-link&gt;
.
However, I'm less clear on the second part.  What errors are you seeing?   can't yet take multiple index arguments (that's also part of &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/206&gt;#206&lt;/denchmark-link&gt;
); is that what you mean?
		</comment>
		<comment id='2' author='hugman' date='2015-12-07T01:23:53Z'>
		Here I put some toy codes to re-generate errors.
( The loss calculation doesn't make sense, It's just for re-generating errors)
There are two problems.

case I : using method_1() for slicing ( direct element slicing with tensor)

error when graph construction time


case II : using method_2() for slicing (gather approach)

ok when graph construction, but can't back-propagate
also using gpu:0 and cpu:0 options show different error messages.


case III : using mehthod_3() for slicing ( using python INT for indexing)

OK.



Errors can be re-generated by switching
scores = method_1(), scores = method_2(), scores = method_3()
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np
import math

with tf.Graph().as_default():
  with tf.device("/cpu:0"):

    # NxN matrix for weights
    size = 3
    w_init = tf.truncated_normal_initializer(stddev=1.0/ math.sqrt(float(size)) )
    mat    = tf.get_variable("weight_matrix", [size, size], initializer=w_init )

    # Indices to retrieve weight
    length = 3
    x = tf.placeholder(tf.int32, shape=(length) ) 
    y = tf.placeholder(tf.int32, shape=(length) ) 

    # score
    def method_1():   # &lt;-- graph construction error!
      scores = []
      for i in xrange(length):
        v = mat[x[i], y[i]] # slice error 
        scores.append( v ) 
      return scores

    def method_2():  # &lt;-- graph is ok. error when training time
      scores = []
      for i in xrange(length):
        v = tf.gather( tf.gather(mat, x[i]), y[i] ) # ok. but can't minimize
        scores.append( v ) 
      return scores

    def method_3(): # &lt;-- graph is ok. train ok. 
      scores = []
      scores.append( mat[0,0] )
      scores.append( mat[1,1] )
      scores.append( mat[2,2] )
      return scores

    # loss to minimize
    # switch method_1, method_2 and method 3 to generate errors
    scores = method_1()
    loss = tf.reduce_sum( tf.pack(scores) )

    # optmizer
    optimizer   = tf.train.GradientDescentOptimizer(0.01)
    global_step = tf.Variable(0, name='global_step', trainable=False)
    train_op    = optimizer.minimize(loss, global_step=global_step)

  with tf.Session() as sess:
    init  = tf.initialize_all_variables()
    sess.run(init)

    feed_data = {}
    feed_data[x] = np.array([2,1,0])
    feed_data[y] = np.array([0,1,1])

    l = sess.run(loss, feed_dict=feed_data)
    print(l)

    print("!!! Training Time !!!")
    _ = sess.run(train_op, feed_dict=feed_data) # &lt;-- error!
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='hugman' date='2015-12-07T02:34:55Z'>
		Method 1 won't work until we fix &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/206&gt;#206&lt;/denchmark-link&gt;
.  I don't know why method 2 doesn't work, since you didn't include the error message.
		</comment>
		<comment id='4' author='hugman' date='2015-12-07T03:35:38Z'>
		method 2 - errors (with /cpu:0 option)
&lt;denchmark-code&gt;...
I tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 24
-0.196204
!!! Training Time !!!
W tensorflow/core/common_runtime/executor.cc:1027] 0x3ae60b0 Compute status: Invalid argument: ConcatOp : Expected concatenating dimensions in the range [0, 0), but got 0
         [[Node: gradients/concat_7 = Concat[N=3, T=DT_INT32, _device="/job:localhost/replica:0/task:0/cpu:0"](gradients/concat_7/concat_dim, Squeeze, Squeeze_2, Squeeze_4)]]
Traceback (most recent call last):
  File "tf_gather_test.py", line 76, in &lt;module&gt;
    _ = sess.run(train_op, feed_dict=feed_data) # &lt;-- error!
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 345, in run
    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 419, in _do_run
    e.code)
tensorflow.python.framework.errors.InvalidArgumentError: ConcatOp : Expected concatenating dimensions in the range [0, 0), but got 0
         [[Node: gradients/concat_7 = Concat[N=3, T=DT_INT32, _device="/job:localhost/replica:0/task:0/cpu:0"](gradients/concat_7/concat_dim, Squeeze, Squeeze_2, Squeeze_4)]]
Caused by op u'gradients/concat_7', defined at:
  File "tf_gather_test.py", line 62, in &lt;module&gt;
    train_op    = optimizer.minimize(loss, global_step=global_step)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py", line 165, in minimize
    gate_gradients=gate_gradients)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py", line 205, in compute_gradients
    loss, var_list, gate_gradients=(gate_gradients == Optimizer.GATE_OP))
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py", line 385, in gradients
    aggregation_method)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py", line 605, in _AggregatedGrads
    array_ops.concat(0, [x.indices for x in out_grad]),
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py", line 290, in concat
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py", line 70, in _concat
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py", line 633, in apply_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1710, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 988, in __init__
    self._traceback = _extract_stack()
&lt;/denchmark-code&gt;

method 2 errors - with /gpu:0 option
&lt;denchmark-code&gt;I tensorflow/core/common_runtime/gpu/gpu_device.cc:643] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla K80, pci bus id: 0000:85:00.0)
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:47] Setting region size to 284540928
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:47] Setting region size to 11286332212
I tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 24
Traceback (most recent call last):
  File "tf_gather_test.py", line 66, in &lt;module&gt;
    sess.run(init)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 345, in run
    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 419, in _do_run
    e.code)
tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'global_step': Could not satisfy explicit device specification '/gpu:0'
         [[Node: global_step = Variable[container="", dtype=DT_INT32, shape=[], shared_name="", _device="/gpu:0"]()]]
Caused by op u'global_step', defined at:
  File "tf_gather_test.py", line 61, in &lt;module&gt;
    global_step = tf.Variable(0, name='global_step', trainable=False)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py", line 187, in __init__
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/state_ops.py", line 96, in variable_op
    container=container, shared_name=shared_name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py", line 334, in _variable
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py", line 633, in apply_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1710, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 988, in __init__
    self._traceback = _extract_stack()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='hugman' date='2015-12-07T22:11:10Z'>
		Thanks!  The cpu case is a bug in the gradient of tf.gather, which wasn't correctly updated when I made gather handle arbitrary rank indices (including scalars).  I'll fix that.
I'm not sure about the  case, but it's possible that we don't yet allow scalars in some places on the GPU for Eigen-related reasons.  &lt;denchmark-link:https://github.com/benoitsteiner&gt;@benoitsteiner&lt;/denchmark-link&gt;
: Is that possible?
		</comment>
		<comment id='6' author='hugman' date='2015-12-08T01:30:45Z'>
		Upon further investigation, fixing (2) in an efficient way seems tricky.  The problem is that an optimizer step for two nested calls to tf.gather can't be efficient unless tf.scatter handles multiple index arguments.  Specifically, we'd need to make IndexedSlices accept multiple index arguments so that the gradient of tf.gather could build one IndexedSlices and then build a deeper one for the next tf.gather call.
I'm going to leave this open as a bug and reference it from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/206&gt;#206&lt;/denchmark-link&gt;
.  Apologies that I won't have a fix for you soon.
		</comment>
		<comment id='7' author='hugman' date='2015-12-08T01:31:46Z'>
		Here is a small failing test case for when someone gets to this.
&lt;denchmark-code&gt;def testNestedGather(self):
  """Catch https://github.com/tensorflow/tensorflow/issues/418."""
  with self.test_session() as sess:
    init = np.arange(6).reshape(2, 3).astype(np.float32)
    var = tf.Variable(init)
    i = tf.constant(1)
    j = tf.constant(2)
    loss = tf.gather(tf.gather(var, i), j)
    sess.run(tf.initialize_all_variables())
    self.assertEqual(loss.eval(), init[1, 2])
    # Run one step of optimization
    optimizer = tf.train.GradientDescentOptimizer(0.01)
    train = optimizer.minimize(loss)
    sess.run(train)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='hugman' date='2016-01-14T05:12:15Z'>
		&lt;denchmark-link:https://github.com/girving&gt;@girving&lt;/denchmark-link&gt;
 Any progress or plan on this?
I'm working on sequence wise custom loss function which requires confusion matrix indexing.
like  matrix[ i , j]  where matrix = tensor [num_classes x num_classes]
		</comment>
		<comment id='9' author='hugman' date='2016-01-14T15:36:49Z'>
		Unfortunately no.  As a workaround until it gets fixed, does this work for you?
&lt;denchmark-code&gt;tf.gather(tf.reshape(matrix, [-1]), i * tf.shape(matrix)[1] + j)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='hugman' date='2016-01-18T08:22:50Z'>
		&lt;denchmark-link:https://github.com/girving&gt;@girving&lt;/denchmark-link&gt;
  Thanks. It works for indexing, and gradient descent updates too.
Maybe we need temporary smart walkaround-wrappers.
Also following tips are really very helpful to implement numpy like indexing.
The trick is using sparse tensor multiplication for slicing.
&lt;denchmark-link:http://stackoverflow.com/questions/34685947/adjust-single-value-within-tensor-tensorflow/34686952#34686952&gt;http://stackoverflow.com/questions/34685947/adjust-single-value-within-tensor-tensorflow/34686952#34686952&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='hugman' date='2016-02-10T19:49:57Z'>
		Will try to work on an efficient multi-index gather this week.  No promises on timelines though.
		</comment>
		<comment id='12' author='hugman' date='2016-03-15T15:08:48Z'>
		I made a  . From a multi-index, it produces an tensor suitable for indexing a flattened tensor. See the &lt;denchmark-link:http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.ravel_multi_index.html&gt;numpy&lt;/denchmark-link&gt;
 documentation for instance.
import tensorflow as tf
import numpy as np

def __cumprod(l):
    # Get the length and make a copy
    ll = len(l)
    l = [v for v in l]

    # Reverse cumulative product
    for i in range(ll-1):
        l[ll-i-2] *= l[ll-i-1]

    return l

def ravel_multi_index(tensor, multi_idx):
    """
    Returns a tensor suitable for use as the index
    on a gather operation on argument tensor.
    """

    if not isinstance(tensor, (tf.Variable, tf.Tensor)):
        raise TypeError('tensor should be a tf.Variable')

    if not isinstance(multi_idx, list):
        multi_idx = [multi_idx]

    # Shape of the tensor in ints
    shape = [i.value for i in tensor.get_shape()]

    if len(shape) != len(multi_idx):
        raise ValueError("Tensor rank is different "
                        "from the multi_idx length.")

    # Work out the shape of each tensor in the multi_idx
    idx_shape = [tuple(j.value for j in i.get_shape()) for i in multi_idx]
    # Ensure that each multi_idx tensor is length 1
    assert all(len(i) == 1 for i in idx_shape)

    # Create a list of reshaped indices. New shape will be
    # [1, 1, dim[0], 1] for the 3rd index in multi_idx
    # for example.
    reshaped_idx = [tf.reshape(idx, [1 if i !=j else dim[0]
                    for j in range(len(shape))])
                for i, (idx, dim)
                in enumerate(zip(multi_idx, idx_shape))]

    # Figure out the base indices for each dimension
    base = __cumprod(shape)

    # Now multiply base indices by each reshaped index
    # to produce the flat index
    return (sum(b*s for b, s in zip(base[1:], reshaped_idx[:-1]))
        + reshaped_idx[-1])

# Shape and slice starts and sizes
shape = (Z, Y, X) = 4, 5, 6
Z0, Y0, X0 = 1, 1, 1
ZS, YS, XS = 3, 3, 4

# Numpy matrix and index
M = np.random.random(size=shape)
idx = [
    np.arange(Z0, Z0+ZS).reshape(ZS,1,1),
    np.arange(Y0, Y0+YS).reshape(1,YS,1),
    np.arange(X0, X0+XS).reshape(1,1,XS),
]

# Tensorflow matrix and indices
TM = tf.Variable(M)
TF_flat_idx = ravel_multi_index(TM, [
    tf.range(Z0, Z0+ZS),
    tf.range(Y0, Y0+YS),
    tf.range(X0, X0+XS)])
TF_data = tf.gather(tf.reshape(TM,[-1]), TF_flat_idx)

with tf.Session() as S:
    S.run(tf.initialize_all_variables())

    # Obtain data via flat indexing
    data = S.run(TF_data)

    # Check that it agrees with data obtained
    # by numpy smart indexing
    assert np.all(data == M[idx])
		</comment>
		<comment id='13' author='hugman' date='2016-03-20T06:09:43Z'>
		&lt;denchmark-link:https://github.com/girving&gt;@girving&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/sjperkins&gt;@sjperkins&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/hugman&gt;@hugman&lt;/denchmark-link&gt;

I need to slice a tensor this way:
 where  is a  tensor and  and  are int32 vectors of sizes  such that k is less than m and n respectively. I want to slice and obtain a length  vector eventually i.e. vectorized operation of 
What would be an optimized way of doing this?  takes only a vector  and can return all the rows corresponding to the vector 
		</comment>
		<comment id='14' author='hugman' date='2016-03-20T08:52:50Z'>
		&lt;denchmark-link:https://github.com/tejaskhot&gt;@tejaskhot&lt;/denchmark-link&gt;
  above should do it
matrix = tf.random_normal([M,N])
row_indices = tf.Variable([1,5,8,9])
col_indices = tf.Variable([5,3,8,2])
flat_index = ravel_multi_index(matrix, [row_indices, col_indices])
data = rf.gather(tf.reshape(matrix, [-1]), flat_index)
Note that  only works with the static tensor shape inferred prior to running the expression tree. If your tensor shape is dynamic,  will need to be updated to use  during expression tree execution. &lt;denchmark-link:http://stackoverflow.com/questions/36035092/multi-dimensional-gather-in-tensorflow&gt;This&lt;/denchmark-link&gt;
 StackOverflow question would be a good place to answer that. However, the indices ( and ) are currently evaluated at run-time.
		</comment>
		<comment id='15' author='hugman' date='2016-03-24T07:14:19Z'>
		&lt;denchmark-link:https://github.com/sjperkins&gt;@sjperkins&lt;/denchmark-link&gt;

It doesn't seem to work with . An example to reproduce the issue:
&lt;denchmark-code&gt;def train():
    mat1 = np.ones((75000,50)).astype(np.float32)
    idx = np.arange(100).astype(np.int32)
    wts = np.ones((50,50)).astype(np.float32)

    mat_1 = tf.Variable(mat1, name="mat_1")
    indices = tf.Variable(idx, name="indices")
    dot_1 = tf.gather(mat_1, indices)

    W = tf.Variable(wts, name="W")
    dot_2 = tf.matmul(dot_1, W)

    activation = tf.nn.softmax(dot_2)
    output = -tf.log(activation)

    row_indices = tf.placeholder("int32", name="row_indices")
    col_indices = tf.placeholder("int32", name="col_indices")
    flat_index = ravel_multi_index(output, [row_indices, col_indices])
    data = tf.gather(tf.reshape(output, [-1]), flat_index)
    return data


row_idx = np.arange(10).astype(np.int32)
col_idx = np.arange(15).astype(np.int32)

sess = tf.Session()
data = train()
sess.run(tf.initialize_all_variables())
result = sess.run(data, feed_dict={row_indices:row_idx, col_indices:col_idx})
print type(result), result.shape

&lt;/denchmark-code&gt;

Any ideas on how to adapt this if the indices are placeholders and not variables?
It gets stuck on the call to ravel_multi_index
		</comment>
		<comment id='16' author='hugman' date='2016-03-24T09:36:23Z'>
		&lt;denchmark-link:https://github.com/tejaskhot&gt;@tejaskhot&lt;/denchmark-link&gt;
 Ah rats, the following line:
# Work out the shape of each tensor in the multi_idx
idx_shape = [tuple(j.value for j in i.get_shape()) for i in multi_idx]
is dealing with the static (get_shape()) rather than the  dynamic ('tf.shape(...)`) shape. I don't ahve time to improve it now, but it gives you the clue to continue the investigation. Update the stackoverflow question if you figure it out. ;-)
		</comment>
		<comment id='17' author='hugman' date='2016-03-29T05:31:15Z'>
		Please see the new op gather_nd in array ops at HEAD/ in the nightly build.
		</comment>
		<comment id='18' author='hugman' date='2016-03-29T06:05:39Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 Shiny :-)
I gather it'll be used to implement &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/206&gt;#206&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='19' author='hugman' date='2016-06-06T18:25:05Z'>
		Closing as a duplicate of part of &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/206&gt;#206&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='20' author='hugman' date='2016-08-16T15:38:04Z'>
		I like add some functions in deep MNIST but I get the error as tensorflow.python.pywrap_tensorflow.StatusNotOK: Invalid argument: Node 'multiplelayer error/initial_value': Node name contains invalid characters
[[Node: multiplelayer error = Variable&lt;denchmark-link:&gt;container="", dtype=DT_INT32, shape=[], shared_name=""&lt;/denchmark-link&gt;
]]?
		</comment>
		<comment id='21' author='hugman' date='2019-01-11T19:08:37Z'>
		I have matrices of different shape, I want to use them as an input in Tensorflow or Keras, any suggestions how to proceed on this?
		</comment>
	</comments>
</bug>