<bug id='10222' author='hunsteve' open_date='2017-05-26T15:41:33Z' closed_time='2017-06-16T18:47:00Z'>
	<summary>tf.nn.moments with tf.concat numerical ambiguity</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code, see below
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 LTE
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')
Bazel version (if compiling from source): n/a
CUDA/cuDNN version: CUDA: 8.0 / cuDNN: 5.1
GPU model and memory: GTX960m 4GB and GTX1080 8GB
Exact command to reproduce: run the code below

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

tf.nn.moments GPU version produces different mean and variance values for numerically same input tensors. The only difference between the inputs is that they are the outputs of one and two tf.concat operations (see the code below). CPU version works well.
The bad output is:
&lt;denchmark-code&gt;input diff:0.0
mean diff:2.98023223877e-08
var diff:7.45058059692e-09
&lt;/denchmark-code&gt;

The correct output should be:
&lt;denchmark-code&gt;input diff:0.0
mean diff:0.0
var diff:0.0
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf

with tf.device("/gpu:0"):
    input = tf.placeholder(shape=[16, 4, 4, 8], dtype=tf.float32)
    input1 = tf.concat([input], axis=0)
    input2 = tf.concat([tf.concat([input], axis=0)], axis=0)

    mean1, var1 = tf.nn.moments(input1, axes=[0,1,2])
    mean2, var2 = tf.nn.moments(input2, axes=[0,1,2])

    input_diff_max = tf.reduce_max(tf.abs(input1 - input2))
    mean_diff_max = tf.reduce_max(tf.abs(mean1 - mean2))
    var_diff_max = tf.reduce_max(tf.abs(var1 - var2))

    with tf.Session() as sess:
        i_v, m_v, v_v = sess.run([input_diff_max, mean_diff_max, var_diff_max], feed_dict={input: np.random.rand(16, 4, 4, 8)})
        print("input diff:{}".format(i_v))
        print("mean diff:{}".format(m_v))
        print("var diff:{}".format(v_v))
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='hunsteve' date='2017-05-30T19:31:02Z'>
		&lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ekelsen&gt;@ekelsen&lt;/denchmark-link&gt;
 : Mind taking a look?
		</comment>
		<comment id='2' author='hunsteve' date='2017-05-30T20:21:11Z'>
		The results are not deterministic on the GPU, sometimes I get:
input diff:0.0 mean diff:0.0 var diff:1.49011611938e-08 
Likely this is due to non-deterministic reductions inside nn.moments.
We are considering adding determinism options - out of curiosity, is this an actual problem, or just unexpected?
		</comment>
		<comment id='3' author='hunsteve' date='2017-05-30T20:38:14Z'>
		Determinism option sounds great. Until then a notice in the docs would be nice, saying that the reduction ops might be non-deterministic.
Currently this behavior does not cause serious problems to us. The way it came up was a test, where we tried to numerically compare the outputs and gradients of our implementation of a method with the reference implementation of that method in tensorflow. We got suprised when the reference implementation was not consistent even with itself in our test.
Shortly after submitting this issue, we found this article: &lt;denchmark-link:https://www.twosigma.com/insights/a-workaround-for-non-determinism-in-tensorflow&gt;https://www.twosigma.com/insights/a-workaround-for-non-determinism-in-tensorflow&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>