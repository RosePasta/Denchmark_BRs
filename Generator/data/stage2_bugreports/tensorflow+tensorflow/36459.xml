<bug id='36459' author='yetanotheryeti' open_date='2020-02-04T10:13:18Z' closed_time='2020-02-25T05:18:27Z'>
	<summary>High RAM Usage for TF Runtime?</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4 LTS
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): tensorflow-gpu==2.0.0 or tensorflow==2.1.0
Python version: 3.6.8
CUDA/cuDNN version: CUDA Toolkit 10.1.243 / cuDNN 7.6.5.32
GPU model and memory: 8 x GeForce RTX 2080 Ti, 11019MiB
System memory: 32GB

Describe the current behavior
Even the smallest 'computation' leads to very high RAM usages of the system memory (not GPU memory). As shown in the following, a simple single-float-Variable initialization leads to more than 2GB RAM increase. The fewer graphics cards are visible for tensorflow, the less RAM is used after all.
Describe the expected behavior
The expected behavior is way less memory usage and also a constant amount, independently of the number of GPUs visible to tensorflow. More than 2GB per instance is not viable for my (/our) situation, since multiple users are working on the machine.
Code to reproduce the issue
&lt;denchmark-code&gt;import os, psutil
p = psutil.Process(os.getpid())

print("Memory Usage (before import):", p.memory_info().rss/1024/1024, "MB")

import tensorflow as tf
num_visible_gpus = 8 # OPTIONAL
gpu_devs = tf.config.experimental.list_physical_devices("GPU") # OPTIONAL
tf.config.experimental.set_visible_devices(gpu_devs[:num_visible_gpus], "GPU") # OPTIONAL

print("Memory Usage (after  import):", p.memory_info().rss/1024/1024, "MB")

tf.Variable(42.0) # Do some pseudowork...

print("Memory Usage (after var-def):", p.memory_info().rss/1024/1024, "MB")
&lt;/denchmark-code&gt;

Output:

Memory Usage (before import): 47.62109375 MB
Memory Usage (after  import): 340.15625 MB
Memory Usage (after var-def): 2485.1796875 MB

Lesser GPUs result in less memory usage (after var-def):

num_visible_gpus=8 =&gt; 2626.039063 MB
num_visible_gpus=7 =&gt; 2488.765626 MB
num_visible_gpus=6 =&gt; 2267.289063 MB
num_visible_gpus=5 =&gt; 2173.917968 MB
num_visible_gpus=4 =&gt; 2011.917968 MB
num_visible_gpus=3 =&gt; 1809.957031 MB
num_visible_gpus=2 =&gt; 1681.316406 MB
num_visible_gpus=1 =&gt; 1539.554688 MB
num_visible_gpus=0 =&gt; 1201.152343 MB

These results were obtained in a jupyter-notebook environment. When run as a py-file, the usages are slightly (by ~200MB) lower (e.g. num_visible_gpus=8 =&gt; 2347.7148437 MB).
	</description>
	<comments>
		<comment id='1' author='yetanotheryeti' date='2020-02-05T08:48:42Z'>
		I tried on colab, it took around 1GB of RAM with Tf 2.1 and 2.0.
Please take a look at the gist &lt;denchmark-link:https://colab.research.google.com/gist/gadagashwini/c0fb1823be9b1feb7bcd78705a20b5ba/untitled375.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='yetanotheryeti' date='2020-02-05T12:29:42Z'>
		This is because Colab does not have the same setup as I have. Colab only utilizes a single GPU:
tf.config.experimental.list_physical_devices("GPU")
outputs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
This aggrees somehow with my observations when I turn only one of the eight GPUs visible (num_visible_gpus=1 =&gt; 1539.554688 MB) or zero (num_visible_gpus=0 =&gt; 1201.152343 MB).
So does this mean, that Tensorflow simply aquires this much of memory in such a minimal scenario?
Also: Is it not possible to lower the usage while more GPUs are visible?
		</comment>
		<comment id='3' author='yetanotheryeti' date='2020-02-05T20:51:01Z'>
		&lt;denchmark-link:https://github.com/yetanotheryeti&gt;@yetanotheryeti&lt;/denchmark-link&gt;
 Please go through this &lt;denchmark-link:https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth&gt;doc &lt;/denchmark-link&gt;
 and let me know if it helps. Thanks!
		</comment>
		<comment id='4' author='yetanotheryeti' date='2020-02-06T12:18:58Z'>
		Sorry, this does not help, since it is not the GPU-memory, but the system memory, which is the bottleneck for us.
Nevertheless, I tried setting the memory growth to true and it surprisingly lowers the usage a little bit  (about 200 MB). Still, ~2.4 GB usage is too much for our usecase.
What I'm interested in is if this is reproducable for other systems with more than one GPU.
If the answer to this problem is simply: "Well tensorflow simply needs to allocate this much memory, because ..., so you can not do anything about it." then I'd be happy.
However, I am still unsure if the high usage is a "bug" or a "mishandling" on my side or simply how it is.
		</comment>
		<comment id='5' author='yetanotheryeti' date='2020-02-06T20:40:48Z'>
		&lt;denchmark-link:https://github.com/sanjoy&gt;@sanjoy&lt;/denchmark-link&gt;
 Can you PTAL? Thanks!
		</comment>
		<comment id='6' author='yetanotheryeti' date='2020-02-13T09:34:39Z'>
		&lt;denchmark-link:https://github.com/sanjoy&gt;@sanjoy&lt;/denchmark-link&gt;

Hello, are there any updates? This is a somewhat blocking issue for our team.
Thank you!
		</comment>
		<comment id='7' author='yetanotheryeti' date='2020-02-16T06:05:55Z'>
		I believe RSS includes all of the CUDA libraries TensorFlow loads which could explain the large memory footprint you're seeing.  Given that, I would expect multiple TF gpu processes to share the actual physical memory for these loaded shared objects.
		</comment>
		<comment id='8' author='yetanotheryeti' date='2020-02-24T05:54:11Z'>
		please update if possible, facing the same issue. too much CPU memory used for pinned memory
		</comment>
		<comment id='9' author='yetanotheryeti' date='2020-02-25T05:18:27Z'>
		I think this is working as intended (so I'm closing the issue).  The CUDA libraries are provided by NVIDIA and we can't do much to reduce their size.  On the other hand, I would expect RSS to be shared with other processes so the motivating concern

More than 2GB per instance is not viable for my (/our) situation, since multiple users are working on the machine.

does not apply.
&lt;denchmark-link:https://github.com/yetanotheryeti&gt;@yetanotheryeti&lt;/denchmark-link&gt;
, please reopen the issue if you disagree.

too much CPU memory used for pinned memory

&lt;denchmark-link:https://github.com/billythegod&gt;@billythegod&lt;/denchmark-link&gt;
 I believe you have a different issue, this issue is not about pinned host memory.  Please open a separate issue with a reproducer.
		</comment>
		<comment id='10' author='yetanotheryeti' date='2020-02-25T05:18:29Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36459&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36459&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='yetanotheryeti' date='2020-02-25T14:40:48Z'>
		
On the other hand, I would expect RSS to be shared with other processes

To clarify: each process has the described RSS-footprint (such as ~2GB), so it is not shared, even among processes of the same user. Sorry, I did not clearly state this before.

The CUDA libraries are provided by NVIDIA and we can't do much to reduce their size.

Ok, this is what I fairly expected (similar to as I stated before: "Well tensorflow (or CUDA) simply needs to allocate this much memory, because ..., so you can not do anything about it.").
This leaves us with either figuring out if and how the CUDA libraries can be shared between processes, or finding another strategy for our team.
		</comment>
		<comment id='12' author='yetanotheryeti' date='2020-02-25T20:12:38Z'>
		
To clarify: each process has the described RSS-footprint (such as ~2GB), so it is not shared, even among processes of the same user. Sorry, I did not clearly state this before.

Are you sure this ~2GB number is not misleading?  I believe that a page loaded in memory will be accounted in the RSS count for all processes that have that page in their page table.  So the sum of RSS footprints for all running processes can exceed the total amount of RAM in the system.
		</comment>
		<comment id='13' author='yetanotheryeti' date='2020-02-26T10:10:36Z'>
		
Are you sure this ~2GB number is not misleading?

I am quite sure. We had multiple OOMs, leading to process kills by the OOM killer or even system crashes. Also, according to (h)top the total system memory consumption is rising per process as well:
Four python instances with tensorflow loaded. Each instance has its own GPU assigned via 'set_visible_devices':
&lt;denchmark-link:https://user-images.githubusercontent.com/5852533/75334278-d4aa9600-5887-11ea-931d-1750e4f6b722.png&gt;&lt;/denchmark-link&gt;

After initializing 'tf.Variable(42.)' for each instance:
&lt;denchmark-link:https://user-images.githubusercontent.com/5852533/75334299-df652b00-5887-11ea-85ba-6aa928819b45.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='yetanotheryeti' date='2020-03-25T17:15:24Z'>
		I have experienced the same observation and have been wondering why the rss is higher when using tensorflow working with the GPU vs CPU only. For a same running program, I have observed these rss percentages (on a 16GB ram computer):

CPU only : 6.7%
GPU : 13,7%
GPU (and allow_growth set to True) : 11.6%

		</comment>
		<comment id='15' author='yetanotheryeti' date='2020-09-13T00:58:02Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/31312#issuecomment-691583440&gt;#31312 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='16' author='yetanotheryeti' date='2020-11-05T10:19:46Z'>
		run 6 small models(.pb about 44MB ), inference with 6 multiprocessing in python3.7 and tensorflow1.13.1.
In windows, use 4G memory, but in Ubuntu use 12G memory, don't know why.
		</comment>
		<comment id='17' author='yetanotheryeti' date='2020-12-10T13:01:51Z'>
		
run 6 small models(.pb about 44MB ), inference with 6 multiprocessing in python3.7 and tensorflow1.13.1.
In windows, use 4G memory, but in Ubuntu use 12G memory, don't know why.

I noticed the same behavior in linux vs windows
		</comment>
		<comment id='18' author='yetanotheryeti' date='2020-12-28T16:39:20Z'>
		It seems like whatever computation we're doing, the RAM (and CPU) overhead are used for maintaining the GPUs to run, so it seems like for small models we need to deliberately turn off some GPU to prevent excess memory usage. If that's not the case, I also noticed in my case that using tf.Dataset API .cache() for large dataset will just cache all the data and run out of memory eventually so I disable .cache() and everything works fine for me.
		</comment>
	</comments>
</bug>