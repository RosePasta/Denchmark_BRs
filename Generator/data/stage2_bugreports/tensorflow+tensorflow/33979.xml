<bug id='33979' author='leonardoaraujosantos' open_date='2019-11-04T17:01:52Z' closed_time='2019-12-19T22:57:54Z'>
	<summary>Crash with MultiWorkerMirroredStrategy Keras Example from docs</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution: Ubuntu 18.04
TensorFlow installed from (source or binary): pip install tensorflow-gpu
TensorFlow version (use command below): 2.0
Python version: 3.7.3
CUDA/cuDNN version: 10/7.6.4.38
GPU model and memory: Tesla P100 16GB


I can't make run the tutorial described here:
&lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras&gt;https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras&lt;/denchmark-link&gt;

The example is crashing with the following TF_CONFIG
os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        'worker': ["server1:23531", "server2:41660"]
    },
    'task': {'type': 'worker', 'index': 0}
})
In the other machine
os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        'worker': ["server1:23531", "server2:41660"]
    },
    'task': {'type': 'worker', 'index': 1}
})
When the script start processing the first epoch it crashes, I tested on a single machine and it worked,
Describe the expected behavior
Don't crash...
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
#!/usr/bin/env python
# coding: utf-8

# # Multiple Worker Training
import tensorflow as tf
# Use tensorflow datasets
import tensorflow_datasets as tfds
tfds.disable_progress_bar()
import os
import json

BUFFER_SIZE = 10000
BATCH_SIZE = 64

NUM_WORKERS = 2
# Here the batch size scales up by number of workers since 
# `tf.data.Dataset.batch` expects the global batch size. Previously we used 64, 
# and now this becomes 128.
GLOBAL_BATCH_SIZE = BATCH_SIZE * NUM_WORKERS

# Define TF_CONVIG environemnt variable
# This step create a environment variable that gives the location and ports available on the server to perform training.
os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        'worker': ["server1:23531", "server2:41660"]
    },
    'task': {'type': 'worker', 'index': 1}
})

# This need to be called at the program startup
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()


def make_datasets_unbatched():
    # Scaling MNIST data from (0, 255] to (0., 1.]
    def scale(image, label):
        image = tf.cast(image, tf.float32)
        image /= 255
        return image, label

    datasets, info = tfds.load(name='mnist',with_info=True,as_supervised=True)

    #return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)
    return datasets['train'].map(scale).shuffle(BUFFER_SIZE)


def build_and_compile_cnn_model():
    model = tf.keras.Sequential([
          tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
          tf.keras.layers.MaxPooling2D(),
          tf.keras.layers.Flatten(),
          tf.keras.layers.Dense(64, activation='relu'),
          tf.keras.layers.Dense(10, activation='softmax')
    ])
    model.compile(
          loss=tf.keras.losses.sparse_categorical_crossentropy,
          optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
          metrics=['accuracy'])
    return model

# #### Define a Strategy and distribute training
with strategy.scope():
    # Creation of dataset, and model building/compiling need to be within 
    # `strategy.scope()`.
    train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)
    multi_worker_model = build_and_compile_cnn_model()
    

multi_worker_model.fit(x=train_datasets, epochs=3)
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
2019-11-04 16:59:37.654216: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-11-04 16:59:38.488142: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-11-04 16:59:38.488853: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Collective Op  has group_size 8 and group_key2 but that group has size 4
Additional GRPC error information:
{"created":"@1572886778.488763529","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Collective Op  has group_size 8 and group_key2 but that group has size 4","grpc_status":13}
         [[{{node allreduce_1/CollectiveReduce_1}}]]
         [[replica_2/strided_slice/_7]]
2019-11-04 16:59:38.488860: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Collective Op  has group_size 8 and group_key2 but that group has size 4
Additional GRPC error information:
{"created":"@1572886778.488763529","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Collective Op  has group_size 8 and group_key2 but that group has size 4","grpc_status":13}
         [[{{node allreduce_1/CollectiveReduce_1}}]]
         [[GroupCrossDeviceControlEdges_0/metrics/accuracy/div_no_nan/_85]]
2019-11-04 16:59:38.489032: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Collective Op  has group_size 8 and group_key2 but that group has size 4
Additional GRPC error information:
{"created":"@1572886778.488763529","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Collective Op  has group_size 8 and group_key2 but that group has size 4","grpc_status":13}
         [[{{node allreduce_1/CollectiveReduce_1}}]]
         [[SGD/SGD/group_deps/_165]]
2019-11-04 16:59:38.489335: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Collective Op  has group_size 8 and group_key2 but that group has size 4
Additional GRPC error information:
{"created":"@1572886778.488763529","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Collective Op  has group_size 8 and group_key2 but that group has size 4","grpc_status":13}
         [[{{node allreduce_1/CollectiveReduce_1}}]]
2019-11-04 16:59:39.358800: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Collective Op  has group_size 8 and group_key2 but that group has size 4
Additional GRPC error information:
{"created":"@1572886778.488763529","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Collective Op  has group_size 8 and group_key2 but that group has size 4","grpc_status":13}
         [[{{node allreduce_1/CollectiveReduce_1}}]]
         [[GroupCrossDeviceControlEdges_2/SGD/SGD/update_0/Const/_117]]
2019-11-04 16:59:40.201240: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-11-04 16:59:40.210692: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-11-04 16:59:40.276529: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-11-04 16:59:40.294888: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-11-04 16:59:40.356048: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-11-04 16:59:40.360051: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
      1/Unknown - 7s 7s/stepTraceback (most recent call last):
  File "multi_worker_train.py", line 83, in &lt;module&gt;
    multi_worker_model.fit(x=train_datasets, epochs=3)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py", line 728, in fit
    use_multiprocessing=use_multiprocessing)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 789, in fit
    *args, **kwargs)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 776, in wrapper
    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py", line 853, in run_distribute_coordinator
    task_id, session_config, rpc_layer)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py", line 360, in _run_single_worker
    return worker_fn(strategy)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 771, in _worker_fn
    return method(model, **kwargs)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 324, in fit
    total_epochs=epochs)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 123, in run_one_epoch
    batch_outs = execution_function(iterator)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", line 86, in execution_function
    distributed_function(input_fn))
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 457, in __call__
    result = self._call(*args, **kwds)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 520, in _call
    return self._stateless_fn(*args, **kwds)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1823, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1141, in _filtered_call
    self.captured_inputs)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1224, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 511, in call
    ctx=ctx)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "&lt;string&gt;", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: 3 root error(s) found.
  (0) Internal:  Collective Op  has group_size 8 and group_key2 but that group has size 4
Additional GRPC error information:
{"created":"@1572886778.488763529","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Collective Op  has group_size 8 and group_key2 but that group has size 4","grpc_status":13}
         [[node allreduce_1/CollectiveReduce_1 (defined at /mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]
         [[GroupCrossDeviceControlEdges_0/metrics/accuracy/div_no_nan/_85]]
  (1) Internal:  Collective Op  has group_size 8 and group_key2 but that group has size 4
Additional GRPC error information:
{"created":"@1572886778.488763529","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Collective Op  has group_size 8 and group_key2 but that group has size 4","grpc_status":13}
         [[node allreduce_1/CollectiveReduce_1 (defined at /mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]
  (2) Internal:  Collective Op  has group_size 8 and group_key2 but that group has size 4
Additional GRPC error information:
{"created":"@1572886778.488763529","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Collective Op  has group_size 8 and group_key2 but that group has size 4","grpc_status":13}
         [[node allreduce_1/CollectiveReduce_1 (defined at /mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]
         [[replica_2/strided_slice/_7]]
0 successful operations.
2 derived errors ignored. [Op:__inference_distributed_function_2500]

Function call stack:
distributed_function -&gt; distributed_function -&gt; distributed_function

2019-11-04 16:59:40.527776: W tensorflow/core/kernels/data/generator_dataset_op.cc:102] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
2019-11-04 16:59:40.884570: W tensorflow/core/common_runtime/eager/context.cc:290] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.
	</description>
	<comments>
		<comment id='1' author='leonardoaraujosantos' date='2019-11-27T03:33:22Z'>
		
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information
* Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

* OS Platform and Distribution: Ubuntu 18.04

* TensorFlow installed from (source or binary): pip install tensorflow-gpu

* TensorFlow version (use command below): 2.0

* Python version: 3.7.3

* CUDA/cuDNN version: 10/7.6.4.38

* GPU model and memory: Tesla P100 16GB

Describe the current behavior
I can't make run the tutorial described here:
https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras
The example is crashing with the following TF_CONFIG
os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        'worker': ["server1:23531", "server2:41660"]
    },
    'task': {'type': 'worker', 'index': 0}
})
In the other machine
os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        'worker': ["server1:23531", "server2:41660"]
    },
    'task': {'type': 'worker', 'index': 1}
})
When the script start processing the first epoch it crashes, I tested on a single machine and it worked,
Describe the expected behavior
Don't crash...
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
#!/usr/bin/env python
# coding: utf-8

# # Multiple Worker Training
import tensorflow as tf
# Use tensorflow datasets
import tensorflow_datasets as tfds
tfds.disable_progress_bar()
import os
import json

BUFFER_SIZE = 10000
BATCH_SIZE = 64

NUM_WORKERS = 2
# Here the batch size scales up by number of workers since 
# `tf.data.Dataset.batch` expects the global batch size. Previously we used 64, 
# and now this becomes 128.
GLOBAL_BATCH_SIZE = BATCH_SIZE * NUM_WORKERS

# Define TF_CONVIG environemnt variable
# This step create a environment variable that gives the location and ports available on the server to perform training.
os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        'worker': ["server1:23531", "server2:41660"]
    },
    'task': {'type': 'worker', 'index': 1}
})

# This need to be called at the program startup
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()


def make_datasets_unbatched():
    # Scaling MNIST data from (0, 255] to (0., 1.]
    def scale(image, label):
        image = tf.cast(image, tf.float32)
        image /= 255
        return image, label

    datasets, info = tfds.load(name='mnist',with_info=True,as_supervised=True)

    #return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)
    return datasets['train'].map(scale).shuffle(BUFFER_SIZE)


def build_and_compile_cnn_model():
    model = tf.keras.Sequential([
          tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
          tf.keras.layers.MaxPooling2D(),
          tf.keras.layers.Flatten(),
          tf.keras.layers.Dense(64, activation='relu'),
          tf.keras.layers.Dense(10, activation='softmax')
    ])
    model.compile(
          loss=tf.keras.losses.sparse_categorical_crossentropy,
          optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
          metrics=['accuracy'])
    return model

# #### Define a Strategy and distribute training
with strategy.scope():
    # Creation of dataset, and model building/compiling need to be within 
    # `strategy.scope()`.
    train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)
    multi_worker_model = build_and_compile_cnn_model()
    

multi_worker_model.fit(x=train_datasets, epochs=3)
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
2019-11-04 16:59:37.654216: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-11-04 16:59:38.488142: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-11-04 16:59:38.488853: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Collective Op  has group_size 8 and group_key2 but that group has size 4
Additional GRPC error information:
{"created":"@1572886778.488763529","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Collective Op  has group_size 8 and group_key2 but that group has size 4","grpc_status":13}
         [[{{node allreduce_1/CollectiveReduce_1}}]]
         [[replica_2/strided_slice/_7]]
2019-11-04 16:59:38.488860: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Collective Op  has group_size 8 and group_key2 but that group has size 4
Additional GRPC error information:
{"created":"@1572886778.488763529","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Collective Op  has group_size 8 and group_key2 but that group has size 4","grpc_status":13}
         [[{{node allreduce_1/CollectiveReduce_1}}]]
         [[GroupCrossDeviceControlEdges_0/metrics/accuracy/div_no_nan/_85]]
2019-11-04 16:59:38.489032: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Collective Op  has group_size 8 and group_key2 but that group has size 4
Additional GRPC error information:
{"created":"@1572886778.488763529","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Collective Op  has group_size 8 and group_key2 but that group has size 4","grpc_status":13}
         [[{{node allreduce_1/CollectiveReduce_1}}]]
         [[SGD/SGD/group_deps/_165]]
2019-11-04 16:59:38.489335: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Collective Op  has group_size 8 and group_key2 but that group has size 4
Additional GRPC error information:
{"created":"@1572886778.488763529","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Collective Op  has group_size 8 and group_key2 but that group has size 4","grpc_status":13}
         [[{{node allreduce_1/CollectiveReduce_1}}]]
2019-11-04 16:59:39.358800: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Collective Op  has group_size 8 and group_key2 but that group has size 4
Additional GRPC error information:
{"created":"@1572886778.488763529","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Collective Op  has group_size 8 and group_key2 but that group has size 4","grpc_status":13}
         [[{{node allreduce_1/CollectiveReduce_1}}]]
         [[GroupCrossDeviceControlEdges_2/SGD/SGD/update_0/Const/_117]]
2019-11-04 16:59:40.201240: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-11-04 16:59:40.210692: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-11-04 16:59:40.276529: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-11-04 16:59:40.294888: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-11-04 16:59:40.356048: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-11-04 16:59:40.360051: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
      1/Unknown - 7s 7s/stepTraceback (most recent call last):
  File "multi_worker_train.py", line 83, in &lt;module&gt;
    multi_worker_model.fit(x=train_datasets, epochs=3)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py", line 728, in fit
    use_multiprocessing=use_multiprocessing)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 789, in fit
    *args, **kwargs)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 776, in wrapper
    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py", line 853, in run_distribute_coordinator
    task_id, session_config, rpc_layer)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py", line 360, in _run_single_worker
    return worker_fn(strategy)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 771, in _worker_fn
    return method(model, **kwargs)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 324, in fit
    total_epochs=epochs)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 123, in run_one_epoch
    batch_outs = execution_function(iterator)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", line 86, in execution_function
    distributed_function(input_fn))
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 457, in __call__
    result = self._call(*args, **kwds)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 520, in _call
    return self._stateless_fn(*args, **kwds)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1823, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1141, in _filtered_call
    self.captured_inputs)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1224, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 511, in call
    ctx=ctx)
  File "/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "&lt;string&gt;", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: 3 root error(s) found.
  (0) Internal:  Collective Op  has group_size 8 and group_key2 but that group has size 4
Additional GRPC error information:
{"created":"@1572886778.488763529","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Collective Op  has group_size 8 and group_key2 but that group has size 4","grpc_status":13}
         [[node allreduce_1/CollectiveReduce_1 (defined at /mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]
         [[GroupCrossDeviceControlEdges_0/metrics/accuracy/div_no_nan/_85]]
  (1) Internal:  Collective Op  has group_size 8 and group_key2 but that group has size 4
Additional GRPC error information:
{"created":"@1572886778.488763529","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Collective Op  has group_size 8 and group_key2 but that group has size 4","grpc_status":13}
         [[node allreduce_1/CollectiveReduce_1 (defined at /mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]
  (2) Internal:  Collective Op  has group_size 8 and group_key2 but that group has size 4
Additional GRPC error information:
{"created":"@1572886778.488763529","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Collective Op  has group_size 8 and group_key2 but that group has size 4","grpc_status":13}
         [[node allreduce_1/CollectiveReduce_1 (defined at /mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]
         [[replica_2/strided_slice/_7]]
0 successful operations.
2 derived errors ignored. [Op:__inference_distributed_function_2500]

Function call stack:
distributed_function -&gt; distributed_function -&gt; distributed_function

2019-11-04 16:59:40.527776: W tensorflow/core/kernels/data/generator_dataset_op.cc:102] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
2019-11-04 16:59:40.884570: W tensorflow/core/common_runtime/eager/context.cc:290] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.

I get the same issue,
multi_worker_model.fit(x=train_datasets, epochs=3)
change to
train_datasets = train_datasets.repeat()
model.fit(train_datasets, steps_per_epoch=total_train_num/ // batch_size, epochs=3)
it works.
		</comment>
		<comment id='2' author='leonardoaraujosantos' date='2019-11-27T18:33:47Z'>
		I realized as well, that quite often tensorflow 2.0 becomes more stable when we define specifically the tf.data dataset size (I've also found similar issue on multiple GPUs). Anyway this highlight a bug or at least something that need to be changed on the tutorial right?
Anyway thanks &lt;denchmark-link:https://github.com/zwqjoy&gt;@zwqjoy&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='leonardoaraujosantos' date='2019-11-28T03:22:28Z'>
		
I realized as well, that quite often tensorflow 2.0 becomes more stable when we define specifically the tf.data dataset size (I've also found similar issue on multiple GPUs). Anyway this highlight a bug or at least something that need to be changed on the tutorial right?
Anyway thanks @zwqjoy

export NCCL_DEBUG=INFO
export NCCL_SOCKET_IFNAME=eth0
worker 0:
&lt;denchmark-link:https://user-images.githubusercontent.com/12653212/69774767-da81d400-11d1-11ea-8563-48366b80c27f.png&gt;&lt;/denchmark-link&gt;

worker 1:
&lt;denchmark-link:https://user-images.githubusercontent.com/12653212/69774776-e077b500-11d1-11ea-97bd-22215aac2279.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='leonardoaraujosantos' date='2019-12-13T01:10:55Z'>
		Thanks for the report - we'll update the tutorial to mention that steps_per_epoch needs to be provided to ensure successful training.
		</comment>
		<comment id='5' author='leonardoaraujosantos' date='2019-12-19T22:57:54Z'>
		&lt;denchmark-link:https://github.com/leonardoaraujosantos&gt;@leonardoaraujosantos&lt;/denchmark-link&gt;
 I think this issue is resolved by &lt;denchmark-link:https://github.com/rcho&gt;@rcho&lt;/denchmark-link&gt;
 who updated the &lt;denchmark-link:https://github.com/tensorflow/docs/commit/ce303cb9045562e3af14be7f642b08b9f0c7a9e2#diff-d4562f95b1b572ef2da36dbba776cf3c&gt;source&lt;/denchmark-link&gt;
. You could also check the updated tutorial &lt;denchmark-link:https://github.com/tensorflow/docs/blob/master/site/en/tutorials/distribute/multi_worker_with_keras.ipynb&gt;here&lt;/denchmark-link&gt;
.
I am closing this issue as it was resolved. feel free to open the issue if it persists again. Thanks!
		</comment>
		<comment id='6' author='leonardoaraujosantos' date='2019-12-19T23:03:04Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33979&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33979&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>