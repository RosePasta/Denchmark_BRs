<bug id='28065' author='wookayin' open_date='2019-04-23T06:48:13Z' closed_time='2019-05-06T20:26:31Z'>
	<summary>scatter_update of cache-enabled variable gives wrong output when sliced</summary>
	<description>
Well, this looks a very weird bug to me. But this might be a serious one as it can give a "wrong" output.
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
TensorFlow installed from (source or binary): binary, from pypi
TensorFlow version (use command below): tensorflow-gpu 1.13.1
Python version: 3.6
Bazel version (if compiling from source): -
GCC/Compiler version (if compiling from source): -
CUDA/cuDNN version: CUDA 10.0, cudnn 7.4 (but it happens to CPU only as well)
GPU model and memory: -

&lt;denchmark-h:h3&gt;Code to reproduce the issue&lt;/denchmark-h&gt;

Please take a look at the following code:
import tensorflow as tf
tf.enable_eager_execution()

K = tf.get_variable("K", shape=[100, 32], dtype=tf.float32, trainable=False,
                    initializer=tf.random_uniform_initializer(-0.0, 0.0),
                    caching_device='/cpu:0')

# update the variable (the first row of K, i.e. K[0])
tf.scatter_update(K, [0], tf.ones([1, 32]))

# These two lines should give the same answer
print(K[0, :].numpy() [:10])
print(K.numpy()[0, :] [:10])
The output is:
&lt;denchmark-code&gt;[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
&lt;/denchmark-code&gt;

The expected behavior:
&lt;denchmark-code&gt;[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Other information&lt;/denchmark-h&gt;



A variable could be placed either on GPU or on CPU, giving the same behavior.


Other tensor manipulations such as tf.reduce_sum(K) will give the correct answer, so definitely the content of the tensor is well updated.


If you comment out caching_device, it works as expected. This means that this has something to do with caching mechanism. Tensor slice operations seemingly are reading from invalidated caches.


When it comes to the static graph mode (i.e. not eager), it works as normal and expected:


import tensorflow as tf
K = tf.get_variable("K", shape=[100, 32], dtype=tf.float32, trainable=False,
                    initializer=tf.random_uniform_initializer(-0.0, 0.0),
                    caching_device='/cpu:0'
                   )
sess = tf.InteractiveSession()
sess.run(K.initializer)

update_op = tf.scatter_update(K, [0], tf.ones([1, 32]))
sess.run(update_op)

# gives [1. 1. ..... 1.]
print( sess.run(K[0, :]) )
	</description>
	<comments>
		<comment id='1' author='wookayin' date='2019-05-06T20:22:44Z'>
		&lt;denchmark-link:https://github.com/wookayin&gt;@wookayin&lt;/denchmark-link&gt;
 I could reproduce the issue with TF1.13.1. Thanks!
		</comment>
		<comment id='2' author='wookayin' date='2019-05-06T20:26:31Z'>
		Yes, caching_device is dangerous, specially with eager execution.
		</comment>
		<comment id='3' author='wookayin' date='2019-05-06T20:27:12Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=28065&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=28065&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='wookayin' date='2019-06-07T18:53:11Z'>
		Shouldn't we at least make a documentation or warning against it?
		</comment>
		<comment id='5' author='wookayin' date='2019-06-07T20:26:07Z'>
		Yes, I agree. Want to send a PR?
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Fri, Jun 7, 2019 at 12:00 PM Jongwook Choi ***@***.***&gt; wrote:
 Shouldn't we at least make a documentation or warning against it?

 —
 You are receiving this because you modified the open/close state.
 Reply to this email directly, view it on GitHub
 &lt;#28065?email_source=notifications&amp;email_token=AAABHRJA3L4F5RCC3DV34QDPZKV3TA5CNFSM4HHVBVCKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXGWFXA#issuecomment-499999452&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAABHRNQ3EJYUF6ML2K4KKLPZKV3TANCNFSM4HHVBVCA&gt;
 .


-- 
 - Alex

		</comment>
	</comments>
</bug>