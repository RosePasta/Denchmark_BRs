<bug id='45536' author='WindQAQ' open_date='2020-12-09T05:48:49Z' closed_time='2020-12-11T02:48:10Z'>
	<summary>RMSprop fails if using mixed precision training and eager functions</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.4.0rc4
Python version: any
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A

v2.4.0-rc3-20-g97c3fef64ba 2.4.0-rc4
Describe the current behavior
RMSprop failes if using mixed precision training and eager functions
Describe the expected behavior
succeed without errors
Standalone code to reproduce the issue
&lt;denchmark-link:https://colab.research.google.com/drive/1-ZrJyKtaWmOMtga44Efc8DOu1fnRo1wN?usp=sharing&gt;https://colab.research.google.com/drive/1-ZrJyKtaWmOMtga44Efc8DOu1fnRo1wN?usp=sharing&lt;/denchmark-link&gt;

Originated from addons &lt;denchmark-link:https://github.com/tensorflow/addons/pull/2250&gt;tensorflow/addons#2250&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='WindQAQ' date='2020-12-09T19:03:53Z'>
		&lt;denchmark-link:https://github.com/WindQAQ&gt;@WindQAQ&lt;/denchmark-link&gt;
 Thanks for creating this issue. I can reproduce the issue with  and  but works as expected with . We will look into it. Thanks!
The following gists are for our reference.
&lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/980c1c17909d390528b4117a976b14ed/untitled115.ipynb&gt;Here&lt;/denchmark-link&gt;
 is a gist with .
&lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/fbac121c81ac4b9e04d8b13552aebdd1/untitled115.ipynb&gt;Here&lt;/denchmark-link&gt;
 is a gist .
		</comment>
		<comment id='2' author='WindQAQ' date='2020-12-11T02:11:53Z'>
		Thank you for filing this issue. The error occurs when mixed precision is enabled and RMSprop.apply_gradients or Nadam.apply_gradients is called in Eager mode (outside any tf.functions). This occurs in Model.fit if you pass run_eagerly=False to Model.compile. The error that occurs is:

Tensor.op is meaningless when eager execution is enabled

Unfortunately, TF 2.4 is about to be released, so this cannot be fixed for 2.4. It will be fixed in 2.5.
As a workaround, you can paste the following snippet of code into your program right after import tensorflow as tf. This snippet monkey patches the fix directly into Keras to avoid the error:
if tf.__version__.startswith('2.4.'):
  from tensorflow.python.keras.mixed_precision import autocast_variable

  # Monkey patch AutoCastVariable.op to not raise AttributeError
  @property
  def op(self):
    if self._op is not None:
      return self._op
    return getattr(self._variable, 'op', None)

  autocast_variable.AutoCastVariable.op = op
Admittedly, this monkey patching is hacky, but the version check for TF 2.4 means it is safe and won't break when run with TF 2.5 or other versions. Alternatively, you can ensure you do not call apply_gradients in Eager mode when using an RMSprop or Nadam optimizer. This can be done by not passing run_eagerly=True when using Model.compile and Model.fit, and by calling apply_gradients under a tf.function when using a custom training loop.
		</comment>
		<comment id='3' author='WindQAQ' date='2020-12-11T02:24:46Z'>
		&lt;denchmark-link:https://github.com/reedwm&gt;@reedwm&lt;/denchmark-link&gt;
 Thanks for clarification! Addons uses it in unittests only so it doesn't matter if we change the optimizer. Just out of curiosity. Do we really need to return  of the variables in ? Seems that returning the variables directly can work as expected.



tensorflow/tensorflow/python/keras/optimizer_v2/rmsprop.py


         Line 220
      in
      579ce3a






 return state_ops.assign(var, var_t, use_locking=self._use_locking).op 





		</comment>
		<comment id='4' author='WindQAQ' date='2020-12-11T02:43:09Z'>
		When TF2 behavior is disabled with , I think removing the  might make  return a variable instead of an op. This probably won't break anyone, but &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer?version=nightly#minimize&gt;the documentation&lt;/denchmark-link&gt;
 does state  returns an op. (The documentation should also be updated to state it returns None in Eager mode).
A better implementation of that .op line would probably be to use var.assign instead of state_ops.assign, then pass in read_value=False to directly get the op instead of returning the variable:
return var.assign(var_t, use_locking=self._use_locking, read_value=False)
In any case, I plan on fixing this in AutoCastVariable regardless, since there may be other cases where the .op attribute is accessed.
		</comment>
		<comment id='5' author='WindQAQ' date='2020-12-11T02:48:10Z'>
		Thanks for the help. Actually, all optimizers in addons use the way you suggested. It's nice to see that we do not need to change anything. Thanks again for the clarification!
		</comment>
		<comment id='6' author='WindQAQ' date='2020-12-11T02:48:12Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45536&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45536&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='WindQAQ' date='2020-12-15T05:40:17Z'>
		I've got a lot if custom layers. All my tests use tensorflow.python.keras.testing_utils.layer_test which internally use RMSProp as optimizer. So now i can't test my layers for mixed precision compatibility... :(
		</comment>
	</comments>
</bug>