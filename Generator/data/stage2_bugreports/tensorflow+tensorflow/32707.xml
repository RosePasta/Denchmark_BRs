<bug id='32707' author='tocab' open_date='2019-09-21T10:12:26Z' closed_time='2019-10-07T23:24:28Z'>
	<summary>Out of memory error during training</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 / Google Colab
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary (Win10)
TensorFlow version (use command below): 2.0.0rc1
Python version: 3.6.8 (Colab) / 3.6.9 (Win10)
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: Cuda 10 CuDNN 7.6.3.30 (Win10)
GPU model and memory: Geforce 1080 Ti, 12 GB (Win10)

Describe the current behavior
I am getting an out of memory error during the training of a encoder-decoder model using the subclassing api of tensorflow 2.
Describe the expected behavior
Once the graph is built and the batch size is constant, the memory usage of the model should stay the same so that there is no out of memory error upcoming during the training process.

I have created a minimum example out of my model in google colab:
&lt;denchmark-link:https://colab.research.google.com/drive/1Xbzcj0ZlALhLhJhv-JaKSVUu3rfB3Urh&gt;https://colab.research.google.com/drive/1Xbzcj0ZlALhLhJhv-JaKSVUu3rfB3Urh&lt;/denchmark-link&gt;

Other info / logs
The error occurs at about example index (current_index) 25000 of 80000, so there must be an issue that something is written on the gpu memory which leads to the following oom error:
&lt;denchmark-code&gt;&lt;ipython-input-2-fe0e8431350e&gt; in train_model(self, data, batch_size)
     27 
     28             with tf.GradientTape() as tape:
---&gt; 29                 batch_output = self.model([input_sequences_batch, output_sequences_batch])
     30                 loss = self.loss_fn(data[1, current_index:current_index + batch_size, :], batch_output)
     31 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    889           with base_layer_utils.autocast_context_manager(
    890               self._compute_dtype):
--&gt; 891             outputs = self.call(cast_inputs, *args, **kwargs)
    892           self._handle_activity_regularization(inputs, outputs)
    893           self._set_mask_metadata(inputs, outputs, input_masks)

&lt;ipython-input-2-fe0e8431350e&gt; in call(self, inputs)
     72 
     73         decoder_output = self.sequence_decoder(self.embeddings(output_sequence[:, :-1]), initial_state=[state_h, state_c])
---&gt; 74         output = self.time_distributed_dense(decoder_output)
     75         return output

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    889           with base_layer_utils.autocast_context_manager(
    890               self._compute_dtype):
--&gt; 891             outputs = self.call(cast_inputs, *args, **kwargs)
    892           self._handle_activity_regularization(inputs, outputs)
    893           self._set_mask_metadata(inputs, outputs, input_masks)

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/wrappers.py in call(self, inputs, training, mask)
    252         inner_mask_shape = self._get_shape_tuple((-1,), mask, 2)
    253         kwargs['mask'] = K.reshape(mask, inner_mask_shape)
--&gt; 254       y = self.layer(inputs, **kwargs)
    255       # Shape: (num_samples, timesteps, ...)
    256       output_shape = self.compute_output_shape(input_shape).as_list()

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    889           with base_layer_utils.autocast_context_manager(
    890               self._compute_dtype):
--&gt; 891             outputs = self.call(cast_inputs, *args, **kwargs)
    892           self._handle_activity_regularization(inputs, outputs)
    893           self._set_mask_metadata(inputs, outputs, input_masks)

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/core.py in call(self, inputs)
   1056         outputs = gen_math_ops.mat_mul(inputs, self.kernel)
   1057     if self.use_bias:
-&gt; 1058       outputs = nn.bias_add(outputs, self.bias)
   1059     if self.activation is not None:
   1060       return self.activation(outputs)  # pylint: disable=not-callable

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py in bias_add(value, bias, data_format, name)
   2716       value = ops.convert_to_tensor(value, name="input")
   2717       bias = ops.convert_to_tensor(bias, dtype=value.dtype, name="bias")
-&gt; 2718     return gen_nn_ops.bias_add(value, bias, data_format=data_format, name=name)
   2719 
   2720 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py in bias_add(value, bias, data_format, name)
    753       else:
    754         message = e.message
--&gt; 755       _six.raise_from(_core._status_to_exception(e.code, message), None)
    756   # Add nodes to the TensorFlow graph.
    757   if data_format is None:

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

ResourceExhaustedError: OOM when allocating tensor with shape[3136,50000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:BiasAdd] name: model/time_distributed/dense/BiasAdd/
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='tocab' date='2019-09-23T06:39:07Z'>
		I could reproduce the issue with Tensorflow 2.0.0.rc1.
Please take a look at colab &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/43f6f6ca0b5c59ebe84bb8118489db99/untitled161.ipynb&gt;gist&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='tocab' date='2019-09-23T23:12:43Z'>
		&lt;denchmark-link:https://github.com/tocab&gt;@tocab&lt;/denchmark-link&gt;
 Looks like shape of tensor is very big and consuming more memory. Can you try to optimize some parameters so that tensor size and memory requirements works well with your GPU? Also, can you try 2.0.0rc2? Thanks!
		</comment>
		<comment id='3' author='tocab' date='2019-09-24T13:44:38Z'>
		I get the same error also with 2.0.0rc2. How can I optimize tensor size and memory requirements? With not using the keras api in tensorflow 1, the training never got an oom after it startet and the batch size stayed the same.
Another question: Is it possible to finalize the graph with the keras api? Then I could test if more parameters are added during the training that leads to the oom.
		</comment>
		<comment id='4' author='tocab' date='2019-09-30T20:40:38Z'>
		I tested it with the final release 2.0.0 and got the same error
		</comment>
		<comment id='5' author='tocab' date='2019-10-04T19:21:00Z'>
		Hi,
Any news about this problem, because I have same issue. With 4x 2080Ti I have problem with simple unet model with input size 320x480
		</comment>
		<comment id='6' author='tocab' date='2019-10-07T23:24:28Z'>
		This is expected, and stems from the fact that you're running eagerly.
In graph mode, there are a number of optimizations to reuse of aggressively free memory based on an analysis of the overall dataflow graph. A good example to illustrate this is buffer forwarding:
&lt;denchmark-code&gt;k = 100000  # Just some large number
x = tf.ones((k, k))
y = x + 1.
z = y ** 2
&lt;/denchmark-code&gt;

In graph mode, the runtime can observe that  is the only consumer of , and  is the only consumer of . So it just allocates one chunk of memory and does all of the operations in place because it can prove that that is safe. On the other hand, if you run that code eagerly then the Python handles for , , and  would be alive at the same time, and you could subsequently use any or all of them; the runtime has no way of knowing what you'll do next. So it has to keep 3x as much data in memory as the graph versions since it can't prove that in place mutations are safe. (And there are a number of similar optimizations. See &lt;denchmark-link:https://web.stanford.edu/class/cs245/slides/TFGraphOptimizationsStanford.pdf&gt;this presentation&lt;/denchmark-link&gt;
 for a more thorough summary.)
So, back to your example. I doubled the batch size just to force an OOM in the first step (Note that you need to pass BATCH_SIZE to model_trainer.train_model in your colab to see the effect), and if instead of:
&lt;denchmark-code&gt;            with tf.GradientTape() as tape:
                batch_output = self.model([input_sequences_batch, output_sequences_batch])
                loss = self.loss_fn(data[1, current_index:current_index + batch_size, :], batch_output)

            gradients = tape.gradient(loss, self.model.trainable_variables)
            self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))
&lt;/denchmark-code&gt;

you create a tf.function outside of the loop:
&lt;denchmark-code&gt;        @tf.function
        def train_step(features, labels):
          with tf.GradientTape() as tape:
                batch_output = self.model([input_sequences_batch, output_sequences_batch])
                loss = self.loss_fn(data[1, current_index:current_index + batch_size, :], batch_output)

          gradients = tape.gradient(loss, self.model.trainable_variables)
          self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))
          return loss
&lt;/denchmark-code&gt;

And then call it inside the training loop:
&lt;denchmark-code&gt;    loss = train_step([input_sequences_batch, output_sequences_batch],
                               data[1, current_index:current_index + batch_size, :])
&lt;/denchmark-code&gt;

The model no longer OOMs. (Because the tf.function can apply all of the same tricks that TF 1.x uses to conserve memory.)
So why doesn't the model OOM immediately? EagerTensors behave like python objects, including matching garbage collection semantics. So what happens is that after most loops python's gc is clearing out Tensors in time for the memory to ready for the next batch, but it's not guaranteed. So if gc is ever late, the last batch worth of Tensors are still waiting to be collected and the GPU OOMs.
Hope this helps.
		</comment>
		<comment id='7' author='tocab' date='2019-10-07T23:24:29Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32707&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32707&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>