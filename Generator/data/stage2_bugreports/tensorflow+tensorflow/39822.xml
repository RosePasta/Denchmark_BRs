<bug id='39822' author='lucaoyuan' open_date='2020-05-23T19:27:52Z' closed_time='2020-06-09T15:34:39Z'>
	<summary>Translation from frozen graph to lite model is incorrect when quantization is enabled.</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
TensorFlow installed from (source or binary): binary
TensorFlow version (or github SHA if from source): 2.1.0

Command used to run the converter or code if youâ€™re using the Python API
If possible, please share a link to Colab/Jupyter/any notebook.
From a correct frozen graph, I try to use quantized tflite conversion which fails to give a correct prediction while unquantized version DOES give correct prediction. I attach all the code to generate these conversion as follows.
&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow.compat.v1 as tf1

def tf2lite(model_file, input_img_size, quantized=False):
    # We only run this function with CPU
    my_devices = tf.config.list_physical_devices(device_type='CPU')
    tf.config.set_visible_devices(devices=my_devices, device_type='CPU')

    assert os.path.isfile(model_file), 'File {} does not exist'.format(model_file)
    assert os.path.splitext(model_file)[1] == '.pb', 'File {} is not TF model'.format(model_file)

    converter  = tf1.lite.TFLiteConverter.from_frozen_graph(
                    model_file,
                    input_arrays=['input_images'],
                    output_arrays=['classification_result'],
                    input_shapes={"input_images": [1, input_img_size, input_img_size, 3]})
    if quantized: # enable quantization
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        rep = Representative_data(input_img_size)
        converter.representative_dataset = rep.data_gen
    tflite_net = converter.convert()
    return tflite_net

def tf2lite_save(model_file, input_img_size, save_file, quantized=False):
    net = tf2lite(model_file, input_img_size, quantized)
    with open(save_file, 'wb') as f:
        f.write(net)

frozen_file = './frozen_imgsize_{}.pb'.format(224)
tflite_file = './converted_imgsize_{}.tflite'.format(224)
tf2lite_save(frozen_file, 224, tflite_file, quantized=True)

# Verify TFLite model vs TF model
lite_model = tf.lite.Interpreter(model_path=tflite_file)
lite_model.allocate_tensors()
input_details = lite_model.get_input_details()
output_details = lite_model.get_output_details()
lite_model.set_tensor(input_details[0]['index'], tf_input)
lite_model.invoke()
lite_classify = lite_model.get_tensor(output_details[0]['index'])
print('TFLite: prediction={}'.format(np.argmax(lite_classify)))
print(lite_classify)
assert np.argmax(lite_classify) == 20
&lt;/denchmark-code&gt;

I miss out two things:

tf_input is No.1000 picture in ImageNet validation dataset whose correct classification index should be 20. I did the following picture transformation.


resize picture size to 256
center crop 224
normalize with   mean=[0.485, 0.456, 0.406],
std=[0.229, 0.224, 0.225]


Representative_data class has too much code to upload. It is a ImageNet data loader which read val data and give 100 samples randomly for quantization to use. the image preprocessing is the same as above.

If we modify the above code to
tf2lite_save(frozen_file, 224, tflite_file, quantized=False)
It will pass with correct result (see converted_imgsize_224_unquantized.tflite and log.txt). Otherwise, it will produce almost all 0 with incorrect classification (see converted_imgsize_224_quantized.tflite and log.txt).
Please help me to figure out what I did wrong. Many thanks.
The output from the converter invocation
&lt;denchmark-code&gt;[frozen_imgsize_224.pb.txt](https://github.com/tensorflow/tensorflow/files/4672384/frozen_imgsize_224.pb.txt)
[converted_imgsize_224_unquantized.tflite.txt](https://github.com/tensorflow/tensorflow/files/4672386/converted_imgsize_224_unquantized.tflite.txt)
[converted_imgsize_224_quantized.tflite.txt](https://github.com/tensorflow/tensorflow/files/4672387/converted_imgsize_224_quantized.tflite.txt)
[log.txt](https://github.com/tensorflow/tensorflow/files/4672396/log.txt)


&lt;/denchmark-code&gt;

Also, please include a link to the saved model or GraphDef
&lt;denchmark-code&gt;# Put link here or attach to the issue.
&lt;/denchmark-code&gt;

Failure details
If the conversion is successful, but the generated model is wrong,
state what is wrong:

Producing wrong results and/or decrease in accuracy
Producing correct results, but the model is slower than expected (model generated from old converter)

RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.
Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='lucaoyuan' date='2020-05-23T19:32:09Z'>
		oops! I just notice that the frozen graph that I upload is incorrect. Reload:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4672412/frozen_imgsize_224.pb.txt&gt;frozen_imgsize_224.pb.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='lucaoyuan' date='2020-05-26T14:54:11Z'>
		&lt;denchmark-link:https://github.com/lucaoyuan&gt;@lucaoyuan&lt;/denchmark-link&gt;

The text shared is not in readable format, you can share a colab gist for us to analyse the issue.
		</comment>
		<comment id='3' author='lucaoyuan' date='2020-06-02T15:10:54Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='4' author='lucaoyuan' date='2020-06-09T15:34:36Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
	</comments>
</bug>