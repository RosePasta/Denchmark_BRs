<bug id='45181' author='szcf-weiya' open_date='2020-11-25T11:14:51Z' closed_time='2020-12-22T16:22:34Z'>
	<summary>Possible problematic example in the guide "Automatic Differentiation"</summary>
	<description>
&lt;denchmark-h:h2&gt;URL(s) with the issue:&lt;/denchmark-h&gt;

&lt;denchmark-link:https://www.tensorflow.org/guide/autodiff&gt;https://www.tensorflow.org/guide/autodiff&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Description of issue (what needs changing):&lt;/denchmark-h&gt;

In the calculation of gradient by passing a dictionary of variables,
my_vars = {
    'w': tf.Variable(tf.random.normal((3, 2)), name='w'),
    'b': tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')
}

grad = tape.gradient(loss, my_vars)
grad['b']
It seems inappropriate since my_vars creates two new variables, but the pre-defined loss is not calculated from these two variables, and it turns out that no output from grab['b'].
I think there might be two possible modifications,

use the pre-define w and b in the dictionary

my_vars = {
    'w': w,
    'b': b
}
grad = tape.gradient(loss, my_vars)
grad['b']

define a new loss using my_vars

with tf.GradientTape(persistent=True) as tape:
    y = x @ my_vars['w'] + my_vars['b']
    loss = tf.reduce_mean(y**2)
grad = tape.gradient(loss, my_vars)
grad['b']
Note that persistent=True is particularly specified, so I guess the first choice might more agree with the author's initial purpose.
	</description>
	<comments>
		<comment id='1' author='szcf-weiya' date='2020-12-22T16:15:22Z'>
		&lt;denchmark-link:https://github.com/szcf-weiya&gt;@szcf-weiya&lt;/denchmark-link&gt;
,
Can we close this issue as the linked PR is merged? Thanks!
		</comment>
		<comment id='2' author='szcf-weiya' date='2020-12-22T16:22:34Z'>
		&lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;
 OK, thanks!
		</comment>
	</comments>
</bug>