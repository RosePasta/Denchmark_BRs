<bug id='39569' author='ethanyanjiali' open_date='2020-05-15T07:02:21Z' closed_time='2020-08-18T23:27:36Z'>
	<summary>SyncBatchNormalization would cause gradient explosion occasionally</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
TensorFlow version (use command below): 2.2.0
Python version: 3.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.1
GPU model and memory: 8x V100 16G

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"

Describe the current behavior
When using SyncBN in my network, sometimes i would see gradient explosion for no reason. However, there's no such problem when using regular BN.
i'm not 100% sure if this is a bug of SyncBN, but want to leave an issue here to see if anyone else has run into similar problem.
i printed gradient and loss for two mini batches, the first one is a normal batch, and gradient exploded in the second batch.
&lt;denchmark-code&gt;# normal batch
max gradient:  0.326069444 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  0.340717554 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  0.231284797 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  0.460334092 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  1.16038787 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  0.489672422 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  0.276092589 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  0.48520276 max grad var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  82.8172226 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  82.8172226 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  82.8172226 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  82.8172226 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  82.8172226 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  82.8172226 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  82.8172226 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  82.8172226 max weights var:  detector_scale_large_final_conv2d/bias:0
Trained batch: 1051 batch loss: 120.549194 batch l2 loss 5413.74561

# exploded batch
max gradient:  6.35045412e+17 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  1.04548427e+19 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  8.70996347e+16 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  2.92331335e+18 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  1.48902926e+17 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  7.38057557e+17 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  3.23904746e+18 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  4.20917e+17 max grad var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  4.87674142e+14 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  4.87674142e+14 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  4.87674142e+14 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  4.87674142e+14 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  4.87674142e+14 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  4.87674142e+14 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  4.87674142e+14 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  4.87674142e+14 max weights var:  detector_scale_large_final_conv2d/bias:0
Trained batch: 1052 batch loss: 120.594421 batch l2 loss 5413.69141
&lt;/denchmark-code&gt;

Describe the expected behavior
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='ethanyanjiali' date='2020-05-15T09:29:50Z'>
		&lt;denchmark-link:https://github.com/ethanyanjiali&gt;@ethanyanjiali&lt;/denchmark-link&gt;

Request you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!
		</comment>
		<comment id='2' author='ethanyanjiali' date='2020-05-15T16:36:28Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
 due to the nature of this bug, this is very hard to make a minimum reproducible example, two reasons:

SyncBN is used in multi-gpu training, i'm not sure if you can do that in colab?
from my experience, this is not a deterministic bug. i'm not sure if it will occur with a smaller network. it probably requires some special combination of data, loss function, batch size and network arch to reproduce.

anyhow, i'll see if i can make a simple example and share it here
		</comment>
		<comment id='3' author='ethanyanjiali' date='2020-08-03T20:21:18Z'>
		Could you try:
&lt;denchmark-code&gt;tf.distribute.experimental.MultiWorkerMirroredStrategy(communication=
    tf.distribute.experimental.CollectiveCommunication.NCCL)
&lt;/denchmark-code&gt;

instead of MirroredStrategy to see if the issue still occurs. We have a suspect in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/41980&gt;#41980&lt;/denchmark-link&gt;
, and it would be to verify whether this is the same issue. You can use MultiWorkerMirroredStrategy with one host without additional setup.
		</comment>
		<comment id='4' author='ethanyanjiali' date='2020-08-11T23:21:06Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='5' author='ethanyanjiali' date='2020-08-18T23:27:35Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='6' author='ethanyanjiali' date='2020-08-18T23:27:37Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39569&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39569&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>