<bug id='27288' author='irvingzhang0512' open_date='2019-03-29T13:05:49Z' closed_time='2019-05-27T06:58:32Z'>
	<summary>GPU Memory Leak for eager mode, tf.scatter_nd_update.</summary>
	<description>

Have I written custom code : yes
OS Platform and Distribution : ubuntu 16.04
TensorFlow installed from: binary
TensorFlow version: v1.12.0
Python version: 3.6.8
CUDA/cuDNN version: 9.2/7.2.1
GPU model and memory: Tesla P40, 24GB

when i use tf.scatter_nd_update, if ref is tf.int32 Variable, everything is fine. if ref is tf.float32 Variable, then there is a memory leak.
i hope the folloing code will reproduce the bug.
import os
import tensorflow as tf
from tensorflow.contrib.memory_stats import BytesInUse

os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "2"
config = tf.ConfigProto(allow_soft_placement=True)
config.gpu_options.allow_growth = True
tf.enable_eager_execution(config=config)

for i in range(10):
    ref = tf.zeros((128, 21, 4), dtype=tf.int32)
    indices = tf.zeros([128, 2], dtype=tf.int32)
    updates = tf.ones([128, 4], dtype=tf.int32)
    update = tf.scatter_nd_update(tf.Variable(ref), indices, updates)
    tf.set_random_seed(1)
    print(BytesInUse().numpy())

print('------------------------------------')

for i in range(10):
    ref = tf.zeros((128, 21, 4), dtype=tf.float32)
    indices = tf.zeros([128, 2], dtype=tf.int32)
    updates = tf.ones([128, 4], dtype=tf.float32)
    update = tf.scatter_nd_update(tf.Variable(ref), indices, updates)
    tf.set_random_seed(1)
    print(BytesInUse().numpy())
when i run the code, get the following results
&lt;denchmark-code&gt;1280
1280
1280
1280
1280
1280
1280
1280
1280
1280
------------------------------------
89344
132608
175616
261376
303616
347648
389632
433664
475648
519680
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='irvingzhang0512' date='2019-04-02T22:49:16Z'>
		Can you remove allow_growth and add a "import gc; gc.collect()" before printing the bytes in use to confirm it's not an issue just of the python gc taking a while to run?
		</comment>
		<comment id='2' author='irvingzhang0512' date='2019-04-03T01:14:45Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 hi, thanks for your reply. i add  &amp; remove session config and get the same result.
codes
import os
import tensorflow as tf
from tensorflow.contrib.memory_stats import BytesInUse

os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "2"
tf.enable_eager_execution()

for i in range(10):
    ref = tf.zeros((128, 21, 4), dtype=tf.int32)
    indices = tf.zeros([128, 2], dtype=tf.int32)
    updates = tf.ones([128, 4], dtype=tf.int32)
    update = tf.scatter_nd_update(tf.Variable(ref), indices, updates)
    tf.set_random_seed(1)
    import gc; gc.collect()
    print(BytesInUse().numpy())

print('------------------------------------')

for i in range(10):
    ref = tf.zeros((128, 21, 4), dtype=tf.float32)
    indices = tf.zeros([128, 2], dtype=tf.int32)
    updates = tf.ones([128, 4], dtype=tf.float32)
    update = tf.scatter_nd_update(tf.Variable(ref), indices, updates)
    tf.set_random_seed(1)
    import gc; gc.collect()
    print(BytesInUse().numpy())
results
&lt;denchmark-code&gt;1280
1280
1280
1280
1280
1280
1280
1280
1280
1280
------------------------------------
89344
132608
175616
261376
303616
347648
389632
433664
475648
519680
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='irvingzhang0512' date='2019-04-03T16:11:04Z'>
		&lt;denchmark-link:https://github.com/jaingaurav&gt;@jaingaurav&lt;/denchmark-link&gt;
 can you look at this?
		</comment>
		<comment id='4' author='irvingzhang0512' date='2019-04-03T16:12:59Z'>
		Also does this also reproduce on tf nightly? 1.12 is a little old and it had some known memory leaks.
		</comment>
		<comment id='5' author='irvingzhang0512' date='2019-04-04T01:13:29Z'>
		I do know there are some known memory leaks for v1.12.0
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/19671&gt;#19671&lt;/denchmark-link&gt;
, this issue mentioned that  can fix the known issue.
As far as i know, tf nightly gpu need cuda 10 instead of cuda 9. It's a bit difficult for me to run the code on tf nightly right now. I may have a try in two days.
		</comment>
		<comment id='6' author='irvingzhang0512' date='2019-04-05T08:57:28Z'>
		tf nightly gpu(1.14.1-dev20190404) can reproduce the bug.
		</comment>
		<comment id='7' author='irvingzhang0512' date='2019-04-10T11:23:26Z'>
		&lt;denchmark-link:https://github.com/jaingaurav&gt;@jaingaurav&lt;/denchmark-link&gt;
 could you please look at this?
		</comment>
		<comment id='8' author='irvingzhang0512' date='2019-04-11T16:27:30Z'>
		&lt;denchmark-link:https://github.com/irvingzhang0512&gt;@irvingzhang0512&lt;/denchmark-link&gt;
: A little swamped at the moment, but I plan on starting work on this bug next week.
		</comment>
		<comment id='9' author='irvingzhang0512' date='2019-04-24T06:43:16Z'>
		same issue here, any updates? &lt;denchmark-link:https://github.com/irvingzhang0512&gt;@irvingzhang0512&lt;/denchmark-link&gt;
  btw, tf.set_random_seed(x) workaround could assign any value for x, right?
		</comment>
		<comment id='10' author='irvingzhang0512' date='2019-04-24T09:48:26Z'>
		&lt;denchmark-link:https://github.com/XuesongYang&gt;@XuesongYang&lt;/denchmark-link&gt;
 yes, i think  clears eager mode caches by calling . so any value works.
		</comment>
		<comment id='11' author='irvingzhang0512' date='2019-05-16T05:22:45Z'>
		at what point do you add tf.set_random_seed(1). i placed the line before model.compile() but i'm still having problems. I have to comment out tf.enable_eager_execution() otherwise my RAM gets depleted and computer freezes.
		</comment>
		<comment id='12' author='irvingzhang0512' date='2019-05-17T18:37:56Z'>
		&lt;denchmark-link:https://github.com/jaingaurav&gt;@jaingaurav&lt;/denchmark-link&gt;
 this is serious enough we should at least triage it before the 2.0 RC
		</comment>
		<comment id='13' author='irvingzhang0512' date='2019-05-23T22:16:33Z'>
		I believe I'm able to reproduce this in a unit test now. Working on finding the source of the leak.
		</comment>
		<comment id='14' author='irvingzhang0512' date='2019-05-27T06:58:33Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27288&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27288&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>