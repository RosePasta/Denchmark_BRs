<bug id='5287' author='elibixby' open_date='2016-10-30T18:23:56Z' closed_time='2016-10-30T19:17:47Z'>
	<summary>[tf.learn] MetricSpec doesn't work with streaming_mean</summary>
	<description>
MetricSpecs always pass both values and labels to metrics, meaning metrics which don't use labels like streaming_mean are broken.
	</description>
	<comments>
		<comment id='1' author='elibixby' date='2016-10-30T19:01:55Z'>
		I wouldn't say that streaming_mean is broken. Rather, it just doesn't fit the mold of the other metrics. We should consider moving it and other similar "metrics" like streaming_concat into their own library of primitives for streaming computation.
		</comment>
		<comment id='2' author='elibixby' date='2016-10-30T19:13:22Z'>
		Maybe.. To me both the Metric and Monitor patterns feel too restrictive to me, for what they are (basically run this thing in the training loop somewhere). They feel similar but different enough to be frustrating. I don't have any concrete suggestions yet but I'm thinking about it (along with a lot of other tf.learn thoughts).
EDIT: less abstract: I guess the docstring is fairly clear. The problem here is "metric" being a dramatically overloaded term in TF. Feel free to open a new bug to track this if you want.
		</comment>
	</comments>
</bug>