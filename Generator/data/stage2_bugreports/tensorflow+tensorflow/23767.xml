<bug id='23767' author='skiler07' open_date='2018-11-15T10:45:36Z' closed_time='2019-08-28T18:20:50Z'>
	<summary>Tensorflow calculates incorrect loss for `tf.keras` models when using Weights.</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (No):
OS Platform and Distribution (e.g., MacOS + Google Cloud VMs):
TensorFlow installed from (source):
TensorFlow version (1.10 and 1.12):
Python version: 3.5


The loss calculation is not correct when working with . After building the model, &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/models/Model&gt;tf.keras.fit_generator&lt;/denchmark-link&gt;
 should accept  as inputs. However, if I multiply the  by 10000,  the loss doesn't change.
The bug seems to appear from 1.10 version of Tensorflow onwards e.g. (1.11, 1.12)
Describe the expected behavior
If I increase the weighting by a certain factor, the overall loss of the model should increase by the same factor. Given the model is doing random guessing.
Code to reproduce the issue
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf

WEIGHT_VARIABLE = 1

no_of_features = 10
timesteps = 3
batch_size = 32

def data_gen():

    while True:
        numerical = np.random.randint(5, size=(batch_size, timesteps, no_of_features))
        y = np.random.randint(2, size=batch_size)
        w = np.ones(batch_size) * WEIGHT_VARIABLE # or np.where() for imbalanced datasets

        yield {'numeric_input': numerical}, y, w


def build_model():
    numerical_input = tf.keras.layers.Input(shape=(timesteps, no_of_features), name='numeric_input')
    rnn_out = tf.keras.layers.GRU(32, return_sequences=False)(numerical_input)
    dense = tf.keras.layers.Dense(1, activation='sigmoid', name='main_output')(rnn_out)

    model = tf.keras.models.Model(numerical_input, dense)

    params = {
        'loss': 'binary_crossentropy',
        'optimizer': tf.keras.optimizers.Adam(),
        'metrics': [tf.keras.metrics.binary_crossentropy, tf.keras.metrics.binary_accuracy]
    }
    model.compile(**params)

    return model


def train_model():
    gen1 = data_gen()
    model = build_model()

    model.fit_generator(gen1, epochs=30, steps_per_epoch=10)


if __name__ == '__main__':
    train_model()
&lt;/denchmark-code&gt;

In the above code, you simply need to change the WEIGHT_VARIABLE = 1 From 1 to 100000 and rerun the file.
Other info / logs
&lt;denchmark-h:h3&gt;v1.10&lt;/denchmark-h&gt;

WEIGHT_VARIABLE = 1
10/10 [==============================] - 1s 128ms/step - loss: 0.7407 - binary_crossentropy: 0.7407 - binary_accuracy: 0.5031
Epoch 2/5
10/10 [==============================] - 0s 4ms/step - loss: 0.7043 - binary_crossentropy: 0.7043 - binary_accuracy: 0.5125
Epoch 3/5
10/10 [==============================] - 0s 4ms/step - loss: 0.7055 - binary_crossentropy: 0.7055 - binary_accuracy: 0.5219
Epoch 4/5
10/10 [==============================] - 0s 4ms/step - loss: 0.7002 - binary_crossentropy: 0.7002 - binary_accuracy: 0.5250
Epoch 5/5
10/10 [==============================] - 0s 4ms/step - loss: 0.6944 - binary_crossentropy: 0.6944 - binary_accuracy: 0.5375
WEIGHT_VARIABLE = 10000
10/10 [==============================] - 1s 131ms/step - loss: 7235.5976 - binary_crossentropy: 0.7236 - binary_accuracy: 0.4562
Epoch 2/5
10/10 [==============================] - 0s 4ms/step - loss: 7271.9184 - binary_crossentropy: 0.7272 - binary_accuracy: 0.4844
Epoch 3/5
10/10 [==============================] - 0s 4ms/step - loss: 7276.9147 - binary_crossentropy: 0.7277 - binary_accuracy: 0.4500
Epoch 4/5
10/10 [==============================] - 0s 4ms/step - loss: 7052.0121 - binary_crossentropy: 0.7052 - binary_accuracy: 0.4625
Epoch 5/5
10/10 [==============================] - 0s 4ms/step - loss: 7187.0285 - binary_crossentropy: 0.7187 - binary_accuracy: 0.4969
&lt;denchmark-h:h3&gt;v1.12&lt;/denchmark-h&gt;

WEIGHT_VARIABLE = 1
10/10 [==============================] - 1s 68ms/step - loss: 0.7188 - binary_crossentropy: 0.7188 - binary_accuracy: 0.5312
Epoch 2/5
10/10 [==============================] - 0s 4ms/step - loss: 0.7044 - binary_crossentropy: 0.7044 - binary_accuracy: 0.4969
Epoch 3/5
10/10 [==============================] - 0s 4ms/step - loss: 0.7086 - binary_crossentropy: 0.7086 - binary_accuracy: 0.4844
Epoch 4/5
10/10 [==============================] - 0s 4ms/step - loss: 0.7075 - binary_crossentropy: 0.7075 - binary_accuracy: 0.4500
Epoch 5/5
10/10 [==============================] - 0s 4ms/step - loss: 0.6950 - binary_crossentropy: 0.6950 - binary_accuracy: 0.5187
WEIGHT_VARIABLE = 10000
10/10 [==============================] - 1s 74ms/step - loss: 0.9084 - binary_crossentropy: 0.9084 - binary_accuracy: 0.4719
Epoch 2/5
10/10 [==============================] - 0s 4ms/step - loss: 0.7120 - binary_crossentropy: 0.7120 - binary_accuracy: 0.5062
Epoch 3/5
10/10 [==============================] - 0s 4ms/step - loss: 0.7024 - binary_crossentropy: 0.7024 - binary_accuracy: 0.5344
Epoch 4/5
10/10 [==============================] - 0s 4ms/step - loss: 0.7257 - binary_crossentropy: 0.7257 - binary_accuracy: 0.4500
Epoch 5/5
10/10 [==============================] - 0s 4ms/step - loss: 0.7013 - binary_crossentropy: 0.7013 - binary_accuracy: 0.4844
	</description>
	<comments>
		<comment id='1' author='skiler07' date='2018-11-15T17:09:12Z'>
		I can also confirm this issue, looks like it was introduced in Tensorflow==1.11.0
The same issue effects class weights too:
import numpy as np
import tensorflow as tf
from tensorflow.python.keras.layers import Input, Dense
from tensorflow.python.keras.models import Model

print (tf.__version__)
batch_size = 10
num_input = 5
num_output = 2
positive_weight = 10000


from tensorflow.python.keras import backend as K


def weighted_cross_entropy(y_true, y_pred, class_weights=[1, positive_weight]):
    """ Sample weights/class weights are broken in TensorFlow 1.12.

    See issue: https://github.com/tensorflow/tensorflow/issues/23767
    """
    y_pred /= K.sum(y_pred, axis=-1, keepdims=True)
    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())
    return -K.sum(y_true * K.log(y_pred) * K.variable(class_weights), -1)

def _to_one_hot(y, num_classes):
    """ Compute one-hot encoding of an array. """
    y_one_hot = np.zeros((len(y), num_classes)).astype(np.float32)
    y_one_hot[np.arange(len(y)), y.squeeze()] = 1

    return y_one_hot


class DummyGenerator(object):

    def class_weights(self):
        np.random.seed(7)
        while True:
            X = np.random.rand(batch_size, num_input)
            y = np.random.randint(num_output, size=batch_size)
            y = _to_one_hot(y, 2)
            yield X, y

    def sample_weights(self):
        np.random.seed(7)
        while True:
            X = np.random.rand(batch_size, num_input)
            y = np.random.randint(num_output, size=batch_size)
            w = y * (positive_weight - 1) + 1
            y = _to_one_hot(y, 2)
            yield X, y, w


def dummy_model():
    inputs = Input(shape=(num_input,))
    outputs = Dense(2, activation="softmax")(inputs)
    model = Model(inputs=inputs, outputs=outputs)
    return model


model = dummy_model()
model.save_weights("~/fixed.hdf5")
gen = DummyGenerator()

print("Training with sample_weights")
model.load_weights("~/fixed.hdf5")
model.compile(loss="categorical_crossentropy", optimizer="adam")
model.fit_generator(gen.sample_weights(), steps_per_epoch=1000, epochs=1)

print("Training with class_weights")
model.load_weights("~/fixed.hdf5")
model.compile(loss="categorical_crossentropy", optimizer="adam")
model.fit_generator(gen.class_weights(), steps_per_epoch=1000, epochs=1, class_weight={0:1, 1:positive_weight})

print("Training with loss_weights")
model.compile(loss=weighted_cross_entropy, optimizer="adam")
model.load_weights("~/fixed.hdf5")
model.fit_generator(gen.class_weights(), steps_per_epoch=1000, epochs=1)
		</comment>
		<comment id='2' author='skiler07' date='2018-11-15T17:53:37Z'>
		Confirmed that we ran into the same issue. üëç
		</comment>
		<comment id='3' author='skiler07' date='2018-11-15T18:43:09Z'>
		Same issue here üëç
		</comment>
		<comment id='4' author='skiler07' date='2018-11-27T17:01:16Z'>
		Can we get an acknowledgement of this issue? I know &lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 is very busy, maybe &lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;
 could give it a quick look?
		</comment>
		<comment id='5' author='skiler07' date='2019-07-11T04:11:20Z'>
		I do not think it has been fixed. I can still see it happens in 1.14 and even tf2.0 beta. I think it was fixed in 1.13 but re-introduced afterwards.
		</comment>
		<comment id='6' author='skiler07' date='2019-08-28T18:20:50Z'>
		This was resolved already. I cannot reproduce the issue when I use tf-nightly. Please check the &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/2d385f53d15b4e024d5d56a02892cab6/tf_23767_keras_weights.ipynb&gt;gist with TF1.14.0&lt;/denchmark-link&gt;
 and [gist with &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/75d60f494371d04184a98ab217c07f69/tfnightly_22207_keras_datasets.ipynb&gt;tf-nightly&lt;/denchmark-link&gt;
. Thanks!
&lt;denchmark-h:h2&gt;output with WEIGHT_VARIABLE = 1&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;Epoch 1/5
10/10 [==============================] - 2s 199ms/step - loss: 0.7167 - binary_crossentropy: 0.7167 - binary_accuracy: 0.4750
Epoch 2/5
10/10 [==============================] - 0s 6ms/step - loss: 0.6973 - binary_crossentropy: 0.6973 - binary_accuracy: 0.5031
Epoch 3/5
10/10 [==============================] - 0s 6ms/step - loss: 0.6901 - binary_crossentropy: 0.6901 - binary_accuracy: 0.5437
Epoch 4/5
10/10 [==============================] - 0s 7ms/step - loss: 0.6977 - binary_crossentropy: 0.6977 - binary_accuracy: 0.4781
Epoch 5/5
10/10 [==============================] - 0s 6ms/step - loss: 0.7034 - binary_crossentropy: 0.7034 - binary_accuracy: 0.4938
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;output with WEIGHT_VARIABLE = 1000&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;Epoch 1/5
10/10 [==============================] - 2s 166ms/step - loss: 722.5937 - binary_crossentropy: 0.7226 - binary_accuracy: 0.5188
Epoch 2/5
10/10 [==============================] - 0s 6ms/step - loss: 704.5393 - binary_crossentropy: 0.7045 - binary_accuracy: 0.4812
Epoch 3/5
10/10 [==============================] - 0s 6ms/step - loss: 718.4684 - binary_crossentropy: 0.7185 - binary_accuracy: 0.4812
Epoch 4/5
10/10 [==============================] - 0s 7ms/step - loss: 710.2846 - binary_crossentropy: 0.7103 - binary_accuracy: 0.4719
Epoch 5/5
10/10 [==============================] - 0s 6ms/step - loss: 698.0638 - binary_crossentropy: 0.6981 - binary_accuracy: 0.5281
&lt;/denchmark-code&gt;

I am closing this issue as it was resolved. Please feel free to open it if the issue persists again. Thanks!
		</comment>
	</comments>
</bug>