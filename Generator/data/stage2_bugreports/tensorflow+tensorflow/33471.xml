<bug id='33471' author='Nephalen' open_date='2019-10-17T15:01:50Z' closed_time='2020-02-15T06:06:15Z'>
	<summary>TF 2.0 Keras add_loss() function seems to make trainable weight not in model but added in add_loss() also being updated during training</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04.2 LTS
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
pip
TensorFlow version (use command below):
v2.0.0-rc2-26-g64c3d38 2.0.0
v1.14.0-rc1-22-gaf24dc9 1.14.0
Python version:
3.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

Describe the current behavior
In TF 2.0, if output of another model with trainable weight is included in add_loss() function, all weight from that model will be updated during training.
I also couldn't find any documentation regarding this change. So I assume it is unintended.
Describe the expected behavior
It should be like in TF 1.14. The gradient of additional model in add_loss() should be back propagated because the model to be trained may be used as input for additional model. But additional model should not be updated.
Code to reproduce the issue
&lt;denchmark-code&gt;import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_probability as tfp
import sys
import time

from tensorflow.keras.layers import Dense, Input, Add, Lambda
from tensorflow.keras import Model
from tensorflow.keras.models import load_model, clone_model
from tensorflow.keras import optimizers

from scipy.io import loadmat
from scipy.spatial.distance import cdist
tf.compat.v1.disable_eager_execution()

x = np.random.rand(100).reshape(100, 1).astype(float)
y = np.array(x&gt;0.5).reshape(100, 1).astype(float)

input1 = Input(shape=(1, ), name='input1')
d1 = Dense(12, name='d11')(input1)
d1 = Dense(12, name='d12')(d1)
d1 = Dense(1, name='output1')(d1)

input2 = Input(shape=(1, ), name='input2')
d2 = Dense(12, name = 'd21')(input2)
d2 = Dense(12, name = 'd22')(d2)
d2 = Dense(1, name = 'output2')(d2)

label = Input(shape = (1, ), name = 'label')

model1 = Model(inputs = [input1, label], outputs = d1)
model2 = Model(inputs = input2, outputs = d2)
model2_cl = clone_model(model2)
model1_cl = clone_model(model1)

pre_result1 = model1.predict({'input1': x, 'label': y})
pre_result1_cl = model1_cl.predict({'input1': x, 'label': y})
pre_result2 = model2.predict(x)

def cust_loss(xi, yi, yp):
    loss = tf.reduce_mean((yi - yp)**2) + tf.reduce_mean((model2(xi) - yi)**2)
    
    return loss

model1.add_loss(cust_loss(input1, label, d1))
#optimizer = optimizers.Adam(lr = 3e-4)
optimizer = tf.compat.v1.train.AdamOptimizer(3e-4)
model1.compile(optimizer, loss = None)

model1.fit({'input1': x, 'label': y}, None, epochs=100)
post_result1 = model1.predict({'input1': x, 'label': y})
post_result1_cl = model1_cl.predict({'input1': x, 'label': y})
post_result2 = model2.predict(x)

print(sum(pre_result2 != post_result2))
&lt;/denchmark-code&gt;

Other info / logs
If you run the code above in TF 1.14, it will show 0 because model2 remains and should remain the same after training of model1.
But if you run it in TF 2.0, it will show 100 as model2 is updated as well for some reason during model1 training.
	</description>
	<comments>
		<comment id='1' author='Nephalen' date='2019-10-17T17:25:25Z'>
		Yes in TF 1.14 it shows 0 but in TF 1.15rc3 and TF 2.0 its showing 100. I think this is clearly a bug
Here are my github gists &lt;denchmark-link:https://colab.sandbox.google.com/gist/gowthamkpr/1fd2d4e745b093af63325bfbd7dbefb7/untitled198.ipynb&gt;Tf 1.14&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://colab.sandbox.google.com/gist/gowthamkpr/edf243d59c16cbe5709754d8d346636e/untitled199.ipynb&gt;TF 1.15rc3&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://colab.sandbox.google.com/gist/gowthamkpr/0fefab16eefe321e944f7fe528376765/untitled197.ipynb&gt;TF2.0&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='Nephalen' date='2020-02-07T06:11:36Z'>
		As a followup, it seems add_loss() function incorrectly add any trainable variables in the model linked within custom loss function into the main model being trained. You should be able to see the difference by printing out model1's trainable variable in TF 1.14 and TF 1.15.
		</comment>
		<comment id='3' author='Nephalen' date='2020-02-15T06:06:15Z'>
		&lt;denchmark-link:https://github.com/matterport/Mask_RCNN/issues/1889#issuecomment-566217231&gt;matterport/Mask_RCNN#1889 (comment)&lt;/denchmark-link&gt;

As mentioned in the post above. The issue can be resolved by converting passed function into a lambda function without input parameters.
		</comment>
		<comment id='4' author='Nephalen' date='2020-02-15T06:06:17Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33471&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33471&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>