<bug id='7065' author='SeguinBe' open_date='2017-01-25T18:40:58Z' closed_time='2017-05-10T03:03:31Z'>
	<summary>pytorch 2.5x faster on VGG16</summary>
	<description>
&lt;denchmark-h:h3&gt;What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?&lt;/denchmark-h&gt;

Started on SO, and was told to post here (&lt;denchmark-link:http://stackoverflow.com/questions/41832779/tensorflow-2-5x-slower-than-pytorch-on-vgg16-architecture?noredirect=1#comment70901342_41832779&gt;SO post&lt;/denchmark-link&gt;
)
&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System:
Ubuntu 14.04 + Maxwell Titan X
Installed version of CUDA and cuDNN:
CUDA 8.0, cuDNN 5.1
:~$ ls -l /usr/local/cuda/lib64/libcud*
-rw-r--r-- 1 root root    558720 Jan 25 08:23 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root        16 Jan 25 08:23 /usr/local/cuda/lib64/libcudart.so -&gt; libcudart.so.8.0
lrwxrwxrwx 1 root root        19 Jan 25 08:23 /usr/local/cuda/lib64/libcudart.so.8.0 -&gt; libcudart.so.8.0.44
-rwxr-xr-x 1 root root    415432 Jan 25 08:23 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root    775162 Jan 25 08:23 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 1000 users       13 Jul 27 07:55 /usr/local/cuda/lib64/libcudnn.so -&gt; libcudnn.so.5
lrwxrwxrwx 1 1000 users       17 Jul 27 07:55 /usr/local/cuda/lib64/libcudnn.so.5 -&gt; libcudnn.so.5.1.5
-rwxrwxr-x 1 1000 users 79337624 Jul 27 07:53 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-rw-r-- 1 1000 users 69756172 Jul 27 07:53 /usr/local/cuda/lib64/libcudnn_static.a
Installed from binary pip package :

https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-linux_x86_64.whl with an Anaconda distribution
The output from python -c "import tensorflow; print(tensorflow.__version__)":

&lt;denchmark-code&gt;I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
0.12.1
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)&lt;/denchmark-h&gt;

Using the following code to do a forward pass on a pretrained VGG16 :
import tensorflow as tf
from tensorflow.contrib import slim
from tensorflow.contrib.slim import nets

tf.reset_default_graph()
# Use RNG to avoid the feed_dict argument
input_images = tf.random_uniform((16, 224, 224, 3), maxval=255)  
preds = nets.vgg.vgg_16(input_images, is_training=False)[0]
saver = tf.train.Saver()

config = tf.ConfigProto(log_device_placement=True)
sess = tf.InteractiveSession(config=config)
saver.restore(sess, './vgg_16.ckpt')

# With jupyter notebook magic
%timeit sess.run(preds)
Compared to the pytorch version on the same machine :
import numpy as np
import torch
import torchvision.models as models
from torch.autograd import Variable
torch.backends.cudnn.benchmark = True

net = models.vgg16()
net.cuda()

_in = Variable(torch.from_numpy(np.random.randn(16, 3, 224, 224).astype(np.float32)).cuda())

# With jupyter notebook magic
%timeit net(_in)
I get the following results by comparing the frameworks. Surprisingly, there is a small difference with the more complicated resnet-50 while I get a huge gap for the VGG16 architecture which (almost) just uses 3x3 convolutions.



Model
TF
pytorch




VGG16
160ms
65ms


resnet-50
58ms
48ms



	</description>
	<comments>
		<comment id='1' author='SeguinBe' date='2017-01-25T18:42:42Z'>
		&lt;denchmark-link:https://github.com/sguada&gt;@sguada&lt;/denchmark-link&gt;
 do you see anything that got added recently that could address performance gap? (maybe some new fused ops?)
		</comment>
		<comment id='2' author='SeguinBe' date='2017-01-25T18:50:11Z'>
		cc: &lt;denchmark-link:https://github.com/vincentvanhoucke&gt;@vincentvanhoucke&lt;/denchmark-link&gt;
 in case he knows others working on VGG-like models
		</comment>
		<comment id='3' author='SeguinBe' date='2017-01-25T21:18:26Z'>
		Hi &lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
.  I am going to reproduce the result and get back to you.  Wanted to let you know we are looking at this issue.
		</comment>
		<comment id='4' author='SeguinBe' date='2017-01-25T21:19:32Z'>
		thanks
ps: &lt;denchmark-link:https://github.com/SeguinBe&gt;@SeguinBe&lt;/denchmark-link&gt;
 is the affected party here
		</comment>
		<comment id='5' author='SeguinBe' date='2017-01-25T22:36:18Z'>
		One drive-by observation: the setup with
&lt;denchmark-code&gt;input_images = tf.random_uniform(16, 224, 224, 3), maxval=255)
&lt;/denchmark-code&gt;

...might be slow because it's invoking the random number generator for every batch. In the PyTorch program, you run the RNG once, outside the timing loop (in the call to np.random.randn()), and reuse its results several times.
The following might be a fairer comparison:
&lt;denchmark-code&gt;input_images = tf.Variable(tf.random_uniform((16, 224, 224, 3), maxval=255))
preds = nets.vgg.vgg_16(input_images, is_training=False)[0]
sess.run(tf.global_variable_initializer())
# ...
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='SeguinBe' date='2017-01-25T22:58:26Z'>
		&lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 tried it already, it does not change the timing at all.
import tensorflow as tf
from tensorflow.contrib import slim
from tensorflow.contrib.slim import nets

tf.reset_default_graph()
input_images = tf.Variable(tf.random_uniform((16, 224, 224, 3), maxval=255))
preds = nets.vgg.vgg_16(input_images, is_training=False)[0]
saver = tf.train.Saver(var_list=[v for v in tf.global_variables() if 'vgg_16' in v.name])
init_op = tf.variables_initializer([input_images])

config = tf.ConfigProto(log_device_placement=False)
sess = tf.InteractiveSession(config=config)
saver.restore(sess, './vgg_16.ckpt')
sess.run(init_op)
On a side note, the resnet-50 timing in the end is more like 50ms so basically the same as pytorch.
		</comment>
		<comment id='7' author='SeguinBe' date='2017-01-26T20:37:08Z'>
		The tf version is using the slower NHWC data format. Changing it to NCHW will most likely speed things up.
		</comment>
		<comment id='8' author='SeguinBe' date='2017-01-27T01:41:58Z'>
		Can you try?
with slim.arg_scope([slim.conv2d], data_format='NCHW'):
preds, _ = nets.vgg.vgg_16(input_images, is_training=False)
		</comment>
		<comment id='9' author='SeguinBe' date='2017-01-27T09:51:57Z'>
		input_images = tf.Variable(tf.random_uniform((16, 3, 224, 224), maxval=255))
with slim.arg_scope([slim.conv2d, slim.max_pool2d], data_format='NCHW'):
    preds, _ = nets.vgg.vgg_16(input_images, is_training=False, spatial_squeeze=False)
init_op = tf.global_variables_initializer()

config = tf.ConfigProto(log_device_placement=True)
sess = tf.InteractiveSession(config=config)
sess.run(init_op)
Using data_format='NHWC' and size [X, 224, 224, 3] I still get 160ms and with the data_format='NCHW' it is slightly better at 150ms...
I have to note that the fc6-fc7 are implemented as convolution in the TF version, will try if modifying them to pure matrix-multiplication changes anything
		</comment>
		<comment id='10' author='SeguinBe' date='2017-01-27T10:16:59Z'>
		So I think that was mainly the solution, the tensorflow definition of the network was using a convolution instead of the fully connected linear matrix multiplication for fc6 fc7 fc8 (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/vgg.py#L116&gt;here&lt;/denchmark-link&gt;
). Did not think originally it would be a big problem but to recapitulate :



Model
Timing




TF-slim default
160ms


TF-slim + NCHW
150ms


fc layers instead of conv
94ms


fc layers instead of conv + NCHW
82ms


pytorch
65ms



There is still a gap but it is definitely more acceptable, should we consider this as resolved?
		</comment>
		<comment id='11' author='SeguinBe' date='2017-01-27T17:38:50Z'>
		&lt;denchmark-link:https://github.com/SeguinBe&gt;@SeguinBe&lt;/denchmark-link&gt;
 are the models identical? One way of telling is initializing with same weights and running the computation through. Since they both rely on CuDNN they should get the same timing, 30% slower seems off.
BTW, you can print out layers and their flops like this, this can sometimes help spot the difference
&lt;denchmark-code&gt;  tf.contrib.tfprof.model_analyzer.print_model_analysis(
      tf.get_default_graph(),
      tfprof_options=tf.contrib.tfprof.model_analyzer.FLOAT_OPS_OPTIONS)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='12' author='SeguinBe' date='2017-01-27T20:35:03Z'>
		Looped in by &lt;denchmark-link:https://github.com/sguada&gt;@sguada&lt;/denchmark-link&gt;
.
The discrepancy between using convolutional vs. fully connected layers should be fixed.
The convolution kernel correctly calls CuBlas gemm for 1x1 convolutions and NHWC format (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops.cc#L439&gt;see here&lt;/denchmark-link&gt;
).
Using  vs.  should not make a difference for 'fc7' or 'fc8' -- &lt;denchmark-link:https://github.com/SeguinBe&gt;@SeguinBe&lt;/denchmark-link&gt;
 can you verify that please?
However, the convolution kernel does not currently call CuBlas gemm for the 7x7 convolution in 'fc5'. I can add one more branch and also call gemm when
convolution_type is 'VALID' and kernel_height==height and kernel_width==width and format==NHWC
		</comment>
		<comment id='13' author='SeguinBe' date='2017-01-28T00:49:51Z'>
		&lt;denchmark-link:https://github.com/gpapan&gt;@gpapan&lt;/denchmark-link&gt;
 yeah it would be great if it works when the input size and the kernel size are the same and padding is 'VALID'.
Also, it seems that the optimization won't work with data_format='NCWH', but it should also work, isn't it?
		</comment>
		<comment id='14' author='SeguinBe' date='2017-01-28T18:09:18Z'>
		&lt;denchmark-link:https://github.com/gpapan&gt;@gpapan&lt;/denchmark-link&gt;
 Yes, setting fc7 or/and fc8 as 1x1 conv does not change the timing (both in NHWC and NCHW btw)
&lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 It'd take a bit more time to properly transfer the weights from one framework to another. Though I think that I find the same proportional gap even in the 3x3 convolutional layers. I'll investigate more in the coming days.
		</comment>
		<comment id='15' author='SeguinBe' date='2017-01-29T22:33:00Z'>
		&lt;denchmark-link:https://github.com/SeguinBe&gt;@SeguinBe&lt;/denchmark-link&gt;
 Thanks, that's very informative, I will go ahead and submit a change to optimize the branch in which the filter and input activations have the same size.
&lt;denchmark-link:https://github.com/sguada&gt;@sguada&lt;/denchmark-link&gt;
 I think that cudnn natively supports the NCHW format and handles this special case internally. The last experiment by &lt;denchmark-link:https://github.com/SeguinBe&gt;@SeguinBe&lt;/denchmark-link&gt;
 also hints in the same direction.
		</comment>
		<comment id='16' author='SeguinBe' date='2017-01-31T20:00:27Z'>
		Can someone try it out and get timings with the latest head?
		</comment>
		<comment id='17' author='SeguinBe' date='2017-01-31T21:21:53Z'>
		&lt;denchmark-link:https://github.com/SeguinBe&gt;@SeguinBe&lt;/denchmark-link&gt;
 can you please verify that the fix just pushed solves the conv2d vs. fully connected discrepancy issue?
		</comment>
		<comment id='18' author='SeguinBe' date='2017-02-01T12:43:52Z'>
		Recent update of &lt;denchmark-link:https://arxiv.org/abs/1608.07249&gt;Benchmarking State-of-the-Art Deep Learning Software Tools&lt;/denchmark-link&gt;
 shows some performance issues. For example, (see table 7)  is significantly (~ 10 times) slower in TF than in other frameworks, an it's even slower at GTX 980 than at GTX 1080. Also, ResNet-50 is ~5.5 times faster in MXNet. Those are most significant differences.
In addition, LSTM is around 3 times faster in CNTK, and ResNet-56 is twice faster in MXNet.
Version used was TensorFlow 0.11 (commit &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/47dd089db3cd16d76595791b2e8483e2fd0b0a25&gt;47dd089&lt;/denchmark-link&gt;
) with CUDA 8.0 and cuDNN 5.1
		</comment>
		<comment id='19' author='SeguinBe' date='2017-02-01T14:18:28Z'>
		&lt;denchmark-link:https://github.com/Randl&gt;@Randl&lt;/denchmark-link&gt;
 -- thanks, &lt;denchmark-link:https://github.com/annarev&gt;@annarev&lt;/denchmark-link&gt;
 has been looking at those differences (ps, additional details should probably go to a different github issue since the problems there are different from vgg difference which is almost solved)
		</comment>
		<comment id='20' author='SeguinBe' date='2017-05-10T03:03:29Z'>
		Closing this item.  Comments can still be made.  While it does not always make a difference, you can also try adding .  This seems to make a bigger difference on K80 and it is model dependent.  For Pascal I saw a 9% improvement when testing a Wide ResNet implementation.  Our &lt;denchmark-link:https://www.tensorflow.org/performance/performance_models&gt;benchmark scripts&lt;/denchmark-link&gt;
 also include a VGG16 model, and I asked the team to check that implementation of VGG16 against the findings in this thread.
		</comment>
		<comment id='21' author='SeguinBe' date='2018-09-07T14:44:19Z'>
		So I'm coming back to some old issues and tried again this one:
&lt;denchmark-h:h2&gt;Setup&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;conda create -n deepl python=3.6
conda activate deepl
conda install tensorflow-gpu=1.9 jupyter
conda install pytorch torchvision -c pytorch
    
wget http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz
tar -xvf vgg_16_2016_08_28.tar.gz
&lt;/denchmark-code&gt;

Ubuntu 18.04 + Titan X Maxwell
TF 1.9, Cuda 9.0, cuDNN 7.1.2 (have to update the drivers to go to 1.10, 9.2, and 7.2)
&lt;denchmark-h:h2&gt;Commands&lt;/denchmark-h&gt;

&lt;denchmark-h:h4&gt;Tensorflow&lt;/denchmark-h&gt;

import os
## Does not change anything
#os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'
import tensorflow as tf
from tensorflow.contrib import slim
from tensorflow.contrib.slim import nets
import numpy as np

tf.reset_default_graph()
if False:  # 'NHWC' format
    # Use RNG to avoid the feed_dict argument
    input_images = tf.constant(np.random.randn(16, 224, 224, 3).astype(np.float32))
    net = nets.vgg.vgg_16(input_images, is_training=False, spatial_squeeze=False)
    conv_layer = net[1]['vgg_16/pool5']
    preds = net[0]
else:  # 'NCHW' format
    input_images = tf.constant(np.random.randn(16, 3, 224, 224).astype(np.float32))
    with slim.arg_scope([slim.conv2d, slim.max_pool2d], data_format='NCHW'):
        net = nets.vgg.vgg_16(input_images, is_training=False, spatial_squeeze=False)
        conv_layer = net[1]['vgg_16/pool5']
        preds = net[0]
saver = tf.train.Saver()

config = tf.ConfigProto(log_device_placement=True)
sess = tf.InteractiveSession(config=config)
saver.restore(sess, './vgg_16.ckpt')

# With jupyter notebook magic
%timeit sess.run(conv_layer)
%timeit sess.run(preds)
&lt;denchmark-h:h4&gt;pyTorch&lt;/denchmark-h&gt;

import numpy as np
import torch
import torchvision.models as models
torch.backends.cudnn.benchmark = True

net = models.vgg16()
net.cuda()

_in = torch.from_numpy(np.random.randn(16, 3, 224, 224).astype(np.float32)).cuda()

# With jupyter notebook magic
%timeit net.features(_in).data.cpu().numpy()
%timeit net(_in).data.cpu().numpy()
&lt;denchmark-h:h2&gt;Results&lt;/denchmark-h&gt;




Framework
TF-NHWC
TF-NCHW
pyTorch




pool5 output
72.5
60.1
59.1


fc8 output
73.6
131.0
60.8



&lt;denchmark-h:h3&gt;Conclusion&lt;/denchmark-h&gt;

Performance is the same as long as the same data format is used.
However, as we stated before the TF version is implementing the FC layers as convolutions instead of actual FC layers. For the NHWC case, an optimization makes this difference transparent but it is not the case for the NCHW layout, which explains the difference here. However, this is more about the networks being defined differently than actual performance issues.
		</comment>
	</comments>
</bug>