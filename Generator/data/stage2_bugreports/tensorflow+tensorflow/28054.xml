<bug id='28054' author='vishalsubbiah' open_date='2019-04-22T23:10:12Z' closed_time='2019-04-29T18:42:38Z'>
	<summary>misalignment in BahdanauAttention documentation</summary>
	<description>
System information

TensorFlow version: 1.13
Doc Link: https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BahdanauAttention

Under the description for the init function, in the Args, the memory_sequence_length should be in its own point and not part of memory.
We welcome contributions by users. Will you be able to update submit a PR (use the doc style guide) to fix the doc Issue? Yes, I can submit a PR for this.
	</description>
	<comments>
		<comment id='1' author='vishalsubbiah' date='2019-04-29T18:29:40Z'>
		Added PR &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/28267&gt;#28267&lt;/denchmark-link&gt;
 for the fix.
		</comment>
	</comments>
</bug>