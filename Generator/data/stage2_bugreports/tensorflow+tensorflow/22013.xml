<bug id='22013' author='mpekalski' open_date='2018-09-02T21:09:33Z' closed_time='2018-10-12T20:17:50Z'>
	<summary>tf.scatter_nd_update - Segmentation fault (core dumped)</summary>
	<description>
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
source
TensorFlow version (use command below):

TF checkpoint I have built
&lt;denchmark-code&gt;/tmp/tensorflow# git log   
commit 09792df012c22622324f085f46edde33006c7355
Author: A. Unique TensorFlower &lt;gardener@tensorflow.org&gt;
Date:   Sun Aug 26 02:07:11 2018 -0700

    compat: Update forward compatibility horizon to 2018-08-26
    
    PiperOrigin-RevId: 210266798
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;== cat /etc/issue ===============================================
Linux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION="16.04.5 LTS (Xenial Xerus)"
VERSION_ID="16.04"
VERSION_CODENAME=xenial

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.14.5)
protobuf (3.6.1)
tensorflow (1.10.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.10.0
tf.GIT_VERSION = b'unknown'
tf.COMPILER_VERSION = b'unknown'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/lib/amd64/server/:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/opt/boost/lib:/opt/conda/lib/:/usr/local/cuda/lib64/:/opt/conda/lib/R/lib/:/usr/local/nvidia/lib64/:/usr/local/nvidia/lib:/lib/x86_64-linux-gnu:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu:/opt/opencv/lib
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Wed Aug 29 19:57:14 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 396.26                 Driver Version: 396.26                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |
|  0%   41C    P0    76W / 250W |    708MiB / 11177MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.148
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a

&lt;/denchmark-code&gt;

Bazel version
&lt;denchmark-code&gt;$ bazel version
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command "bazel shutdown".
Build label: 0.16.0
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Jul 31 17:01:24 2018 (1533056484)
Build timestamp: 1533056484
Build timestamp as int: 1533056484
&lt;/denchmark-code&gt;

CUDNN version:
&lt;denchmark-code&gt;$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2018 NVIDIA Corporation
Built on Tue_Jun_12_23:07:04_CDT_2018
Cuda compilation tools, release 9.2, V9.2.148
&lt;/denchmark-code&gt;

GPU: GEFORCE GTX 1080Ti, 11GB, GIGABYTE AORUS

Exact command to reproduce:

&lt;denchmark-code&gt;from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf

def scope_1():
    print("DS1 SCOPE =============")
    with tf.variable_scope("scope_1", reuse=tf.AUTO_REUSE):
        x = tf.get_variable("x", initializer=lambda: tf.zeros(shape=[], dtype=tf.float32), dtype=tf.float32
                               , trainable=False, use_resource=True)             
        print("graph: {}".format(x.graph))
        print("scope: {}".format(tf.get_variable_scope().name))
        print(" name: {}".format(x.name))
        print("  var: {}".format(str(x)))
        current_scope = tf.get_variable_scope()       
        assign_one = tf.assign(x, 1.0, name="x_is_one")
    
    def scope_2(inputs, label):        
        print("initial scope: {}".format(tf.get_variable_scope().name))
        print("DS1 SCOPE =============")
        #with tf.variable_scope("scope_1", reuse=tf.AUTO_REUSE):
        with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):
            y = tf.get_variable("x", initializer=lambda: tf.zeros(shape=[], dtype=tf.float32), dtype=tf.float32
                                   , trainable=False, use_resource=True)         
            print("graph: {}".format(y.graph))
            print("scope: {}".format(tf.get_variable_scope().name))
            print(" name: {}".format(y.name))
            print("  var: {}".format(str(y)))
            print("=============")
            print(y)
            print(inputs)
            #assign_two = tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32),1.0), name="inputs_plus_1")
            assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), 1.0)))
            with tf.control_dependencies([assign_two]):
                with tf.control_dependencies([tf.scatter_nd_update(y, [[0]], [0.22])]):
                    return y.read_value(), label
            #return x,label
    
    # test that original x is mutable
    with tf.control_dependencies([assign_one]):
        dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [-1,-2,-3,-4,-5]))
                    .map(scope_2)
                    .batch(1)
                    .repeat(1)        
                    )
    return dataset
    
                
with tf.variable_scope("scope_0"):
        dataset_fn = scope_1()

with tf.variable_scope("iterator"):
    # Define iterator from_string_handle. In general it is useful to have
    # this kind of iterator if one wants to switch between train and validation
    # within the training loop.        
    iterator_t = dataset_fn.make_initializable_iterator()
    iterator_handle = tf.placeholder(tf.string, shape=[], name="iterator_handle")
    iterator = tf.data.Iterator.from_string_handle(iterator_handle, 
                                                iterator_t.output_types,
                                                iterator_t.output_shapes)
    
    def get_next_item():
        next_elem = iterator.get_next(name="next_element")
        x, y = tf.cast(next_elem[0], tf.float32), next_elem[1]# tf.cast(next_elem[1], tf.int32)
        return x, y    
with tf.Session() as sess:

    sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])
    handle_t = sess.run(iterator_t.string_handle())
    # Run data iterator initialisation
    sess.run(iterator_t.initializer)
    print(sess.graph.get_operations()) 
    while True:
        try:
            print(sess.run(get_next_item(), feed_dict={iterator_handle:handle_t}))
        except tf.errors.OutOfRangeError:
                        print("End of training dataset.")
                        break        
    print()
    print("global vars: {}".format(tf.global_variables()))
    print("local vars: {}".format(tf.local_variables()))
    print(tf.get_default_graph().get_name_scope())

&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

I am trying to create a function that would modify a Tensor within a pipeline of Dataset API.
The scoping may seem weird, but that is minimal example that shows the problem that I created from my project. After adding  I started to get segmentation fault.
A minimal example with  instead of  worked, see &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/22009&gt;#22009&lt;/denchmark-link&gt;

I tried disabling GPU with config=tf.ConfigProto(device_count={'GPU': 0}) and CUDA_VISIBLE_DEVICES="" but the result was the same.
I will recompile TF overnight (from the current master) with --copt=-g and try to provide a stacktrace
&lt;denchmark-code&gt;TF_BUILD_INFO = {container_type: "gpu", command: "bazel build --config=opt --config=cuda --copt=-march=native --copt=-mfpmath=both --copt=-mtune=native --copt=-g --verbose_failures --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --jobs=8 --config=mkl --action_env=LD_LIBRARY_PATH=/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/lib/amd64/server/:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/opt/boost/lib:/opt/conda/lib/:/usr/local/cuda/lib64/:/opt/conda/lib/R/lib/:/usr/local/nvidia/lib64/:/usr/local/nvidia/lib:/lib/x86_64-linux-gnu:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu:/opt/opencv/lib tensorflow/tools/pip_package:build_pip_package", source_HEAD: "201be3d514d7239aa19496dba4dd0c85303b03f1", source_remote_origin: "https://github.com/tensorflow/tensorflow", OS: "Linux", kernel: "4.13.0-38-generic", architecture: "x86_64", processor: "Intel(R) Core(TM) i7-4770K CPU @ 3.50GHz", processor_count: "8", memory_total: "32877820 kB", swap_total: "69444596 kB", Bazel_version: "Build label: 0.16.0", Java_version: "1.8.0_181", Python_version: "3.6.2", gpp_version: "g++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609", swig_version: "", NVIDIA_driver_version: "396.26", CUDA_device_count: "1", CUDA_device_names: "GeForce GTX 1080 Ti   (*PrimaryCard),", CUDA_toolkit_version: "V9.2.148"}
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;DS1 SCOPE =============
graph: &lt;tensorflow.python.framework.ops.Graph object at 0x7f0940f582b0&gt;
scope: scope_0/scope_1
 name: scope_0/scope_1/x:0
  var: &lt;tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32&gt;
initial scope: 
DS1 SCOPE =============
graph: &lt;tensorflow.python.framework.ops.Graph object at 0x7f0940f582b0&gt;
scope: scope_0/scope_1
 name: scope_0/scope_1/x_1:0
  var: &lt;tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32&gt;
=============
&lt;tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32&gt;
Tensor("arg0:0", shape=(), dtype=int32)
2018-09-02 20:52:39.456276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-02 20:52:39.456710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721
pciBusID: 0000:01:00.0
totalMemory: 10.92GiB freeMemory: 10.23GiB
2018-09-02 20:52:39.456728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0
2018-09-02 20:52:39.665323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-02 20:52:39.665362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 
2018-09-02 20:52:39.665369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N 
2018-09-02 20:52:39.665731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9887 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-09-02 20:52:39.756397: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Segmentation fault (core dumped)
&lt;/denchmark-code&gt;

log of run on CPU
&lt;denchmark-code&gt;CUDA_VISIBLE_DEVICES="" python3 bug.py 
DS1 SCOPE =============
graph: &lt;tensorflow.python.framework.ops.Graph object at 0x7ffaa61de390&gt;
scope: scope_0/scope_1
 name: scope_0/scope_1/x:0
  var: &lt;tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32&gt;
initial scope: 
DS1 SCOPE =============
graph: &lt;tensorflow.python.framework.ops.Graph object at 0x7ffaa61de390&gt;
scope: scope_0/scope_1
 name: scope_0/scope_1/x_1:0
  var: &lt;tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32&gt;
=============
&lt;tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32&gt;
Tensor("arg0:0", shape=(), dtype=int32)
2018-09-02 20:54:41.271567: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2018-09-02 20:54:41.271604: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: 3bed2f328777
2018-09-02 20:54:41.271614: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: 3bed2f328777
2018-09-02 20:54:41.271655: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: 396.26.0
2018-09-02 20:54:41.271686: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 396.26.0
2018-09-02 20:54:41.271694: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:305] kernel version seems to match DSO: 396.26.0
2018-09-02 20:54:41.271962: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Segmentation fault (core dumped)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='mpekalski' date='2018-09-03T06:35:19Z'>
		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
CUDA/cuDNN version
GPU model and memory
		</comment>
		<comment id='2' author='mpekalski' date='2018-09-03T07:44:02Z'>
		When executing line by line everything works fine until it starts the session.
I've tried running it on different environment, on kaggle.com, and the it also fails.
		</comment>
		<comment id='3' author='mpekalski' date='2018-09-03T08:14:44Z'>
		I tried it on one more environment. Where I have just installed TF from &lt;denchmark-link:https://files.pythonhosted.org/packages/04/7e/a484776c73b1431f2b077e13801531e966113492552194fe721e6ef88d5d/tensorflow-1.10.1-cp36-cp36m-manylinux1_x86_64.whl&gt;https://files.pythonhosted.org/packages/04/7e/a484776c73b1431f2b077e13801531e966113492552194fe721e6ef88d5d/tensorflow-1.10.1-cp36-cp36m-manylinux1_x86_64.whl&lt;/denchmark-link&gt;

I had to modify a bit lambda initializer in get_variable and specify the shape, but otherwise I got the same segmentation fault. This env does not have GPU, so it was run on a CPU.
&lt;denchmark-code&gt;# python3 tf_bug.py 
/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
DS1 SCOPE =============
graph: &lt;tensorflow.python.framework.ops.Graph object at 0x7fa7eb105a20&gt;
scope: scope_0/scope_1
 name: scope_0/scope_1/x:0
  var: &lt;tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32&gt;
initial scope: 
DS1 SCOPE =============
graph: &lt;tensorflow.python.framework.ops.Graph object at 0x7fa7eb105a20&gt;
scope: scope_0/scope_1
 name: scope_0/scope_1/x_1:0
  var: &lt;tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32&gt;
=============
&lt;tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32&gt;
Tensor("arg0:0", shape=(), dtype=int32)
2018-09-03 08:12:16.809170: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Segmentation fault (core dumped)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='mpekalski' date='2018-09-03T09:04:06Z'>
		I tried running the script in gdb but it says no stack. Any hints how to get it work?
		</comment>
		<comment id='5' author='mpekalski' date='2018-09-04T08:33:16Z'>
		I've managed to get gdb working, looks like the issue is with inferring shape in scatter_nd_update
&lt;denchmark-code&gt;0x00007fffcd5f912f in tensorflow::shape_inference::ScatterNdUpdateShape(tensorflow::shape_inference::InferenceContext*) ()
   from /opt/conda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so
&lt;/denchmark-code&gt;

Full log
&lt;denchmark-code&gt;# gdb python3
GNU gdb (Ubuntu 7.11.1-0ubuntu1~16.5) 7.11.1                                              Copyright (C) 2016 Free Software Foundation, Inc.                                         License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt; py
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type "show copying"gdb py
and "show warranty" for details.
This GDB was configured as "x86_64-linux-gnu".
Type "show configuration" for configuration details.
For bug reporting instructions, please see:
&lt;http://www.gnu.org/software/gdb/bugs/&gt;.
Find the GDB manual and other documentation resources online at:
&lt;http://www.gnu.org/software/gdb/documentation/&gt;.
For help, type "help".
Type "apropos word" to search for commands related to "word"...
Reading symbols from python3...done.
(gdb) run bug.py
Starting program: /opt/conda/bin/python3 bug.py
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
[New Thread 0x7ffff31fa700 (LWP 606)]
[New Thread 0x7ffff09f9700 (LWP 607)]
[New Thread 0x7fffee1f8700 (LWP 608)]
[New Thread 0x7fffeb9f7700 (LWP 609)]
[New Thread 0x7fffe91f6700 (LWP 610)]
[New Thread 0x7fffe69f5700 (LWP 611)]
[New Thread 0x7fffe41f4700 (LWP 612)]
[Thread 0x7fffe41f4700 (LWP 612) exited]
[Thread 0x7fffe69f5700 (LWP 611) exited]
[Thread 0x7fffe91f6700 (LWP 610) exited]
[Thread 0x7fffeb9f7700 (LWP 609) exited]
[Thread 0x7fffee1f8700 (LWP 608) exited]
[Thread 0x7ffff09f9700 (LWP 607) exited]
[Thread 0x7ffff31fa700 (LWP 606) exited]
/opt/conda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/opt/conda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
[New Thread 0x7fffe41f4700 (LWP 617)]
DS1 SCOPE =============
graph: &lt;tensorflow.python.framework.ops.Graph object at 0x7ffff6789160&gt;
scope: scope_0/scope_1
 name: scope_0/scope_1/x:0
  var: &lt;tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32&gt;
[New Thread 0x7fffe69f5700 (LWP 618)]
initial scope:
DS1 SCOPE =============
graph: &lt;tensorflow.python.framework.ops.Graph object at 0x7ffff6789160&gt;
scope: scope_0/scope_1
 name: scope_0/scope_1/x_1:0
  var: &lt;tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32&gt;
=============
&lt;tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32&gt;
Tensor("arg0:0", shape=(), dtype=int32)
[New Thread 0x7fffe91f6700 (LWP 619)]
[New Thread 0x7fffeb9f7700 (LWP 620)]
[New Thread 0x7fff871ff700 (LWP 621)]
[New Thread 0x7fff869fe700 (LWP 622)]
[New Thread 0x7fff861fd700 (LWP 623)]
[New Thread 0x7fff859fc700 (LWP 624)]
[New Thread 0x7fff851fb700 (LWP 625)]
[New Thread 0x7fff849fa700 (LWP 626)]
[New Thread 0x7fff5fdff700 (LWP 627)]
[New Thread 0x7fff5f5fe700 (LWP 628)]
[New Thread 0x7fff5edfd700 (LWP 629)]
[New Thread 0x7fff5e5fc700 (LWP 630)]
2018-09-04 08:28:02.088839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-04 08:28:02.089267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721
pciBusID: 0000:01:00.0
totalMemory: 10.92GiB freeMemory: 10.20GiB
2018-09-04 08:28:02.089287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-09-04 08:28:02.089297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-04 08:28:02.089303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0
2018-09-04 08:28:02.089307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N
2018-09-04 08:28:02.089464: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
[New Thread 0x7fff5ddfb700 (LWP 631)]
[New Thread 0x7fff5d5fa700 (LWP 632)]
[New Thread 0x7fff5cdf9700 (LWP 633)]
[Thread 0x7fff5cdf9700 (LWP 633) exited]
[New Thread 0x7fff5cdf9700 (LWP 634)]
[Thread 0x7fff5cdf9700 (LWP 634) exited]
[New Thread 0x7fff5cdf9700 (LWP 635)]
[Thread 0x7fff5cdf9700 (LWP 635) exited]
[New Thread 0x7fff5cdf9700 (LWP 636)]
[Thread 0x7fff5cdf9700 (LWP 636) exited]
[New Thread 0x7fff5cdf9700 (LWP 637)]
[Thread 0x7fff5cdf9700 (LWP 637) exited]
[New Thread 0x7fff5cdf9700 (LWP 638)]
[Thread 0x7fff5cdf9700 (LWP 638) exited]
[New Thread 0x7fff5cdf9700 (LWP 639)]
[Thread 0x7fff5cdf9700 (LWP 639) exited]
[New Thread 0x7fff5cdf9700 (LWP 640)]
[Thread 0x7fff5cdf9700 (LWP 640) exited]

Thread 1 "python3" received signal SIGSEGV, Segmentation fault.
0x00007fffcd5f912f in tensorflow::shape_inference::ScatterNdUpdateShape(tensorflow::shape_inference::InferenceContext*) ()
   from /opt/conda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so
(gdb)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='mpekalski' date='2018-09-04T09:10:55Z'>
		I've got the code running by explicitly specifying shape in tf.get_variable(), and then making sure that the corresponding tensors in assign ops had the same shape (so not assigning 1.0 but [1.0]). Still, I could not make it work with dynamic shapes, and segmentation fault should not take place but throw some error message.
Here is the working code
&lt;denchmark-code&gt;from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf

def scope_1():
    print("DS1 SCOPE =============")
    with tf.variable_scope("scope_1", reuse=tf.AUTO_REUSE):
        x = tf.get_variable("x", initializer=lambda shape, dtype, partition_info: tf.zeros(shape=shape, dtype=dtype), dtype=tf.float32
                               , trainable=False, use_resource=True, shape=[1])             
        print("graph: {}".format(x.graph))
        print("scope: {}".format(tf.get_variable_scope().name))
        print(" name: {}".format(x.name))
        print("  var: {}".format(str(x)))
        current_scope = tf.get_variable_scope()       
        assign_one = tf.assign(x, [1.0], name="x_is_one")
    
    def scope_2(inputs, label):        
        print("initial scope: {}".format(tf.get_variable_scope().name))
        print("DS1 SCOPE =============")
        #with tf.variable_scope("scope_1", reuse=tf.AUTO_REUSE):
        with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):
            y = tf.get_variable("x", initializer=lambda shape, dtype, partition_info: tf.zeros(shape=shape, dtype=dtype), dtype=tf.float32
                                   , trainable=False, use_resource=True, shape=[1])         
            print("graph: {}".format(y.graph))
            print("scope: {}".format(tf.get_variable_scope().name))
            print(" name: {}".format(y.name))
            print("  var: {}".format(str(y)))
            print("=============")
            print(y)
            print(inputs)
            #assign_two = tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32),1.0), name="inputs_plus_1")
            assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), [1.0])))
            with tf.control_dependencies([assign_two]):
                with tf.control_dependencies([tf.scatter_nd_update(y, [[0]], [0.22])]):
                    return y.read_value(), label

            #return x,label
    
    # test that original x is mutable
    with tf.control_dependencies([assign_one]):
        dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [-1,-2,-3,-4,-5]))
                    .map(scope_2)
                    .batch(1)
                    .repeat(1)        
                    )
    return dataset
    
                
with tf.variable_scope("scope_0"):
        dataset_fn = scope_1()

with tf.variable_scope("iterator"):
    # Define iterator from_string_handle. In general it is useful to have
    # this kind of iterator if one wants to switch between train and validation
    # within the training loop.        
    iterator_t = dataset_fn.make_initializable_iterator()
    iterator_handle = tf.placeholder(tf.string, shape=[], name="iterator_handle")
    iterator = tf.data.Iterator.from_string_handle(iterator_handle, 
                                                iterator_t.output_types,
                                                iterator_t.output_shapes)
    
    def get_next_item():
        next_elem = iterator.get_next(name="next_element")
        x, y = tf.cast(next_elem[0], tf.float32), next_elem[1]# tf.cast(next_elem[1], tf.int32)
        return x, y    
        
with tf.Session() as sess:

    sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])
    handle_t = sess.run(iterator_t.string_handle())
    # Run data iterator initialisation
    sess.run(iterator_t.initializer)
    print(sess.graph.get_operations()) 
    while True:
        try:
            print(sess.run(get_next_item(), feed_dict={iterator_handle:handle_t}))
        except tf.errors.OutOfRangeError:
                        print("End of training dataset.")
                        break        
    print()
    print("global vars: {}".format(tf.global_variables()))
    print("local vars: {}".format(tf.local_variables()))
    print(tf.get_default_graph().get_name_scope())
&lt;/denchmark-code&gt;

and the output
&lt;denchmark-code&gt;DS1 SCOPE =============
graph: &lt;tensorflow.python.framework.ops.Graph object at 0x7f67cfcce940&gt;
scope: scope_0/scope_1
 name: scope_0/scope_1/x:0
  var: &lt;tf.Variable 'scope_0/scope_1/x:0' shape=(1,) dtype=float32&gt;
initial scope: 
DS1 SCOPE =============
graph: &lt;tensorflow.python.framework.ops.Graph object at 0x7f67cfcce940&gt;
scope: scope_0/scope_1
 name: scope_0/scope_1/x_1:0
  var: &lt;tf.Variable 'scope_0/scope_1/x_1:0' shape=(1,) dtype=float32&gt;
=============
&lt;tf.Variable 'scope_0/scope_1/x_1:0' shape=(1,) dtype=float32&gt;
Tensor("arg0:0", shape=(), dtype=int32)
[&lt;tf.Operation 'Placeholder' type=Placeholder&gt;, &lt;tf.Operation 'scope_0/scope_1/Placeholder' type=Placeholder&gt;, &lt;tf.Operation 'scope_0_1/scope_1/Placeholder' type=Placeholder&gt;, &lt;tf.Operation 'scope_0/scope_1/x/Initializer/zeros/shape_as_tensor' type=Const&gt;, &lt;tf.Operation 'scope_0/scope_1/x/Initializer/zeros/Const' type=Const&gt;, &lt;tf.Operation 'scope_0/scope_1/x/Initializer/zeros' type=Fill&gt;, &lt;tf.Operation 'scope_0/scope_1/x' type=VarHandleOp&gt;, &lt;tf.Operation 'scope_0/scope_1/x/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp&gt;, &lt;tf.Operation 'scope_0/scope_1/x/Assign' type=AssignVariableOp&gt;, &lt;tf.Operation 'scope_0/scope_1/x/Read/ReadVariableOp' type=ReadVariableOp&gt;, &lt;tf.Operation 'scope_0_1/scope_1/Const' type=Const&gt;, &lt;tf.Operation 'scope_0_1/scope_1/x_is_one' type=AssignVariableOp&gt;, &lt;tf.Operation 'scope_0_1/scope_1/ReadVariableOp' type=ReadVariableOp&gt;, &lt;tf.Operation 'scope_0_1/tensors/component_0' type=Const&gt;, &lt;tf.Operation 'scope_0_1/tensors/component_1' type=Const&gt;, &lt;tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros/shape_as_tensor' type=Const&gt;, &lt;tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros/Const' type=Const&gt;, &lt;tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros' type=Fill&gt;, &lt;tf.Operation 'scope_0/scope_1/x_1' type=VarHandleOp&gt;, &lt;tf.Operation 'scope_0/scope_1/x_1/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp&gt;, &lt;tf.Operation 'scope_0/scope_1/x_1/Assign' type=AssignVariableOp&gt;, &lt;tf.Operation 'scope_0/scope_1/x_1/Read/ReadVariableOp' type=ReadVariableOp&gt;, &lt;tf.Operation 'scope_0_1/batch_size' type=Const&gt;, &lt;tf.Operation 'scope_0_1/count' type=Const&gt;, &lt;tf.Operation 'iterator/Iterator' type=Iterator&gt;, &lt;tf.Operation 'iterator/TensorSliceDataset' type=TensorSliceDataset&gt;, &lt;tf.Operation 'iterator/MapDataset' type=MapDataset&gt;, &lt;tf.Operation 'iterator/BatchDataset' type=BatchDataset&gt;, &lt;tf.Operation 'iterator/RepeatDataset' type=RepeatDataset&gt;, &lt;tf.Operation 'iterator/MakeIterator' type=MakeIterator&gt;, &lt;tf.Operation 'iterator/IteratorToStringHandle' type=IteratorToStringHandle&gt;, &lt;tf.Operation 'iterator/iterator_handle' type=Placeholder&gt;, &lt;tf.Operation 'iterator/IteratorFromStringHandle' type=IteratorFromStringHandle&gt;, &lt;tf.Operation 'iterator/IteratorToStringHandle_1' type=IteratorToStringHandle&gt;, &lt;tf.Operation 'init' type=NoOp&gt;, &lt;tf.Operation 'init_1' type=NoOp&gt;]
(array([[0.22]], dtype=float32), array([-1], dtype=int32))
(array([[0.22]], dtype=float32), array([-2], dtype=int32))
(array([[0.22]], dtype=float32), array([-3], dtype=int32))
(array([[0.22]], dtype=float32), array([-4], dtype=int32))
(array([[0.22]], dtype=float32), array([-5], dtype=int32))
End of training dataset.

global vars: [&lt;tf.Variable 'scope_0/scope_1/x:0' shape=(1,) dtype=float32&gt;, &lt;tf.Variable 'scope_0/scope_1/x_1:0' shape=(1,) dtype=float32&gt;]
local vars: []
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='mpekalski' date='2018-09-04T09:14:25Z'>
		&lt;denchmark-link:https://github.com/rohan100jain&gt;@rohan100jain&lt;/denchmark-link&gt;
 I have identified the reason for the segmentation fault. Please have a look at my comments above.
		</comment>
		<comment id='8' author='mpekalski' date='2018-10-04T19:06:58Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/rohan100jain&gt;@rohan100jain&lt;/denchmark-link&gt;
: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='9' author='mpekalski' date='2018-10-11T00:46:16Z'>
		Looking at the documentation for scatter_nd_update (&lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/scatter_nd_update&gt;https://www.tensorflow.org/api_docs/python/tf/scatter_nd_update&lt;/denchmark-link&gt;
) seems like indexing into a zero rank tensor is sort of invalid? Adding Eugene for more info.
		</comment>
		<comment id='10' author='mpekalski' date='2018-10-11T19:35:54Z'>
		Thanks for reporting.  I found the cause of the problem and will send a fix.
		</comment>
	</comments>
</bug>