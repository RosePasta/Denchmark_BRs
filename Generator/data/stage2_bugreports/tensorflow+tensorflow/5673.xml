<bug id='5673' author='shuvrobiswas' open_date='2016-11-17T19:12:40Z' closed_time='2016-11-18T22:42:53Z'>
	<summary>Possible typo in docstring of embedding_rnn_seq2seq and embedding_attention_seq2seq?</summary>
	<description>
So in the docstring of embedding_rnn_seq2seq:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L296&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L296&lt;/denchmark-link&gt;

It says:
&lt;denchmark-code&gt;  This model first embeds encoder_inputs by a newly created embedding (of shape
  [num_encoder_symbols x input_size]). Then it runs an RNN to encode
&lt;/denchmark-code&gt;

Should that instead read:
&lt;denchmark-code&gt;  [embedding_size x input_size]). Then it runs an RNN to encode
&lt;/denchmark-code&gt;

Since if you are creating an embedding it is to do dimensionality reduction (and so you usually want your embedding size to be smaller than the number of encoder symbols). Also in the code:
&lt;denchmark-code&gt;    # Encoder.
    encoder_cell = rnn_cell.EmbeddingWrapper(
        cell, embedding_classes=num_encoder_symbols,
        embedding_size=embedding_size)
    _, encoder_state = rnn.rnn(encoder_cell, encoder_inputs, dtype=dtype)
&lt;/denchmark-code&gt;

It looks like the encoder inputs are mapped to vectors of size embedding_size.
Looking forward to hearing the community thoughts (this is also the case in the docstring of embedding_attention_seq2seq).
	</description>
	<comments>
		<comment id='1' author='shuvrobiswas' date='2016-11-18T16:14:21Z'>
		Yep, looks like this should be fixed.
		</comment>
		<comment id='2' author='shuvrobiswas' date='2016-11-18T22:42:53Z'>
		shuvrobiswas@: you ask if it the shape of the embedding is [embedding_size x input_size], right?
The answer is no. It's [num_encoder_symbols x embedding_size] -- because every symbol (e.g., word) needs to be embedded into a dense vector of size embedding_size. It's dimensionality reduction only in a certain sense -- most importantly, it's going from a sparse space of size num_encoder_symbols to  dense vectors of embedding_size floats. And "embedding" here means the embedding matrix, every single symbol will be embedded into a vector of size [embedding_size]. Hope that helps!
		</comment>
		<comment id='3' author='shuvrobiswas' date='2016-11-18T23:09:03Z'>
		&lt;denchmark-link:https://github.com/lukaszkaiser&gt;@lukaszkaiser&lt;/denchmark-link&gt;
 Thanks for the response, I agree the shape of the embedding is of course [num_encoder_symbols x embedding_size]
In the docstring it says "model first embeds encoder_inputs by a newly created embedding (of shape
[num_encoder_symbols x input_size])". I interpreted that as talking about taking encoder_inputs and then getting the corresponding dense vector representation of those encoder_inputs which would be of dimension [input_size x embedding_size] or the transpose [embedding_size x input_size]?
		</comment>
	</comments>
</bug>