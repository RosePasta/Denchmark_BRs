<bug id='32281' author='doublexxking' open_date='2019-09-06T06:45:52Z' closed_time='2019-12-20T18:54:31Z'>
	<summary>The TF function for the TRT segment could not be empty</summary>
	<description>
--
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary):  when I convert pb to tensorrt, I use tf-nightly-gpu-1.15 from pip install. When I do infer, I use tf-1.14.0 from source.
TensorFlow version (use command below): tf-1.14.0
Python version: 3.5
Bazel version (if compiling from source): 0.24.1
GCC/Compiler version (if compiling from source): 5.4
CUDA/cuDNN version: cuda 10 cudnn 17
GPU model and memory: 1060Ti

Describe the current problem
Hi,
I successfully used trt_convert to my pb model to tensorRT plan
and I want to use c++ to infer my model.
So I use bazel complie tf with tensorrt together.
In my code, pb model can run successfully.
I use the same code, and TensorRT model can load successfully but as session running, tf give me the following error:
2019-09-06 06:56:37.455586: E tensorflow/core/common_runtime/executor.cc:642] Executor failed to create kernel. Invalid argument: The TF function for the TRT segment could not be empty
[[{{node fa_layer4_c0/TRTEngineOp_108}}]]
When I convert pb to tensorrt, it is shown as following:
2019-09-06 06:25:59.150320: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-09-06 06:25:59.183609: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:831] TensorRT node fa_layer4/TRTEngineOp_107 added for segment 107 consisting of 3 nodes succeeded.
2019-09-06 06:25:59.189007: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:831] TensorRT node fa_layer4_c0/TRTEngineOp_108 added for segment 108 consisting of 2 nodes succeeded.
2019-09-06 06:25:59.189378: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-09-06 06:25:59.189403: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:41] DefaultLogger Network must have at least one output
2019-09-06 06:25:59.189426: W tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:834] TensorRT node fa_layer4_c0/conv0/bn/cond_1/TRTEngineOp_109 added for segment 109 consisting of 4 nodes failed: Internal: Failed to build TensorRT engine. Fallback to TF...
Additionally, I can use the tensorrt model in python code to infer. (python installed from pip)
Anyone could provide some ideas how to solve it?
	</description>
	<comments>
		<comment id='1' author='doublexxking' date='2019-10-30T02:04:00Z'>
		I met this problem too, have you solved this problem?
		</comment>
		<comment id='2' author='doublexxking' date='2019-10-30T07:56:35Z'>
		I tried convert savedmodel to trt_savedmodel  successfully with below command line:
&lt;denchmark-code&gt;saved_model_cli convert \
--dir "/home/yilrr/tf-serving/faster-rcnn/saved_model/versions/1" \
--output_dir "/home/yilrr/tf-serving/trt-frcnn" \
--tag_set serve \
tensorrt --precision_mode FP32 --max_batch_size 32 --is_dynamic_op True
&lt;/denchmark-code&gt;

And deploy the trt_savedmodel with tf-serving, no error found. But when send grpc request to tf-serving. error occurs below :
&lt;denchmark-code&gt;&lt;_Rendezvous of RPC that terminated with:
 status = StatusCode.INVALID_ARGUMENT
 details = "The TF function for the TRT segment could not be empty
  [[{{node TRTEngineOp_0}}]]"
 debug_error_string = "{"created":"@1572421829.964622904","description":"Error received from peer ipv4:192.168.23.17:8500","file":"src/core/lib/surface/call.cc","file_line":1055,"grpc_message":"The TF function for the TRT segment could not be empty\n\t [[{{node TRTEngineOp_0}}]]","grpc_status":3}"
&lt;/denchmark-code&gt;

anybody who can explain this? thanks in advance!
		</comment>
		<comment id='3' author='doublexxking' date='2019-11-09T00:27:36Z'>
		@double344931987 &lt;denchmark-link:https://github.com/superhg2012&gt;@superhg2012&lt;/denchmark-link&gt;
 could you provide a detailed repro? E.g. which model you're using and how you deploy/run the model with TF serving?
Thanks.
		</comment>
		<comment id='4' author='doublexxking' date='2019-11-11T01:47:03Z'>
		&lt;denchmark-link:https://github.com/aaroey&gt;@aaroey&lt;/denchmark-link&gt;
 I am ok now. I updated tensorflow version to 1.14.0 and tensorRT from 5.0.26 to 5.1.5.0.
		</comment>
		<comment id='5' author='doublexxking' date='2019-12-20T18:54:30Z'>
		Thanks &lt;denchmark-link:https://github.com/superhg2012&gt;@superhg2012&lt;/denchmark-link&gt;
.
@double344931987 I'm closing this, feel free to reopen with a repro (model+script) if you're still experiencing this issue. Thanks.
		</comment>
		<comment id='6' author='doublexxking' date='2019-12-20T18:54:33Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32281&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32281&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>