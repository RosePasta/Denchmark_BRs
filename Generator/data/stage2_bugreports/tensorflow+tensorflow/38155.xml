<bug id='38155' author='luke-mao' open_date='2020-04-02T10:06:27Z' closed_time='2020-04-03T08:04:26Z'>
	<summary>tf.keras.Model subclassing: when check inputs size inside the model, the size display "batch number" as None</summary>
	<description>
Hello. I am new to deep learning with the tf2.0 . And I am currently working on the cifar 10 dataset.
Recently I encounter a problem with the model subclassing. The sample code is given as

assign inputs from the processing, the batch size is [500, 32, 32, 3] with channel_last format. So each batch has 500 images.
define a CNN model

&lt;denchmark-code&gt;class CNN(tf.keras.Model)
        def __init__(self):
            super(CNN, self).__init__()
            self.conv1 = layers.conv2d(......)
            # and other layers definition
         def call(self, inputs):
             # here i try to test the size of inputs, using tf.shape(inputs) and inputs.shape()
             # however, tf.shape returns (4,) and inputs.shape returns [None, 32, 32, 3]
             #    I don't know why. The inputs should be 500 images. 
              #   And it looks like it only contains one image. 
&lt;/denchmark-code&gt;


I call the model, such as

&lt;denchmark-code&gt;model = CNN()
y = model.predict(inputs)
&lt;/denchmark-code&gt;

The sample codes are stated above.
The problem is i cannot input 500 images all in once to the model subclassing. I do not quite sure what is wrong with it. I tried the "def build(self, input_shapes)" as listed on the doc to define the input_shapes as [500, 32, 32, 3] but it still fails. I found a similar issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/36991&gt;#36991&lt;/denchmark-link&gt;
. In one discussion, someone says "then use batch number = 1". But i would like to know if there is anyway to fix my code so that i can feed mini batch into the model.
The full code is below. When you run this code, there will be errors. But I simply would like to address the issue with the batch number dimension. Since if the "inputs" inside the model does not have that dimension, i cannot do the 500 images flatten process.
Thank you ~~
&lt;denchmark-code&gt;# for github issue use

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.datasets import cifar10


if __name__ == '__main__':
    (x_train, y_train), (x_label, y_label) = cifar10.load_data()
    x_train = x_train.astype(np.float32)

    class CNN(tf.keras.Model):
        def __init__(self):
            super(CNN, self).__init__()

            self.conv1 = layers.Conv2D(
                filters=32,
                kernel_size=[5, 5],
                strides=(1, 1),
                padding='same',
                activation=tf.nn.relu,
                kernel_initializer=tf.random_normal_initializer(
                    mean=0.0, stddev=0.01
                ),
                bias_initializer=tf.zeros_initializer(),
                data_format='channels_last'
            )
            # the above output is [n, 32, 32, 32]
            # from the book, the channel number 3 disappears after process
            # 池化输出大小=[（输入大小-卷积核（过滤器）大小）／步长]+1

            self.pool1 = layers.MaxPool2D(
                pool_size=[3, 3], strides=2,
                data_format='channels_last',
                padding='VALID'
            )
            # the output above is [n, 15, 15, 64]

            # self.lrn1 = LRNLayer(
            #     depth_radius=5,
            #     bias=1,
            #     alpha=1,
            #     beta=0.5
            # )
            # during class call, add the lrn layer, output size not change

            self.conv2 = layers.Conv2D(
                filters=64,
                kernel_size=[5, 5],
                strides=(1,1),
                kernel_initializer=tf.random_normal_initializer(
                    mean=0, stddev=0.01
                ),
                bias_initializer=tf.zeros_initializer(),
                padding='same',
                activation=tf.nn.relu,
                data_format='channels_last'
            )
            # padding = same, so output is [n, 15, 15, 64]
            # self.lrn2 = LRNLayer(
            #     depth_radius=5,
            #     bias=1,
            #     alpha=1,
            #     beta=0.5
            # )

            self.pool2 = layers.MaxPool2D(
                pool_size=[3, 3], strides=2,
                data_format='channels_last',
                padding="VALID"
            )
            # output size = [N, 7, 7, 64]

            # self.flatten = layers.Reshape(target_shape=(-1, 7*7*64))

            self.dense1 = layers.Dense(
                units=784,
                activation=tf.nn.relu,
                kernel_initializer=tf.random_normal_initializer(
                    mean=0.0, stddev=0.05
                ),
                bias_initializer=tf.zeros_initializer()
            )
            # output is [n, 784]
            self.batchNorm = layers.BatchNormalization()

            self.dense2 = layers.Dense(
                units=10,
                activation=tf.nn.relu,
                kernel_initializer=tf.random_normal_initializer(
                    mean=0.0, stddev=0.05
                ),
                bias_initializer=tf.zeros_initializer()
            )
            # output is 10

        # def build(self, input_shape):
        #         #     super(CNN, self).build(input_shape)

        def call(self, inputs):
            # print(inputs[0, ::])
            print("here inside the model")
            print("inputs.shape = {}".format(inputs.shape)) # (None, 32, 32, 3)
            print("tf.shape = {}".format(tf.shape(inputs)))
            print("type(inputs) = {}".format(type(inputs)))


            x = self.conv1(inputs) # (None, 32, 32, 32)
            print("x.shape = {}".format(x.shape))
            print("tf.shape = {}".format(tf.shape(x)))

            # not useful for this issue.
            # x = self.pool1(x)
            # # x = self.lrn1(x)
            # x = self.conv2(x)
            # # x = self.lrn2(x)
            # x = self.pool2(x)
            # x = tf.reshape(tensor=x, shape=(x.shape[0], -1))
            # x = self.dense1(x)
            # x = self.batchNorm(x)
            # x = self.dense2(x)
            # y = tf.nn.softmax(x)

            return y


    iteration = 500
    batch_size = 500
    learning_rate = 0.001

    model = CNN()
    # model.build(input_shape=(None, 32, 32, 3))
    # # print(model.conv1.input_spec())

    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

    print("Here try to predict")
    y = model.predict(x_train[:500, ::])

    # for i in range(iteration + 1):
    #     batch = batch(train, batch_size)
    #
    #     print(batch.data.shape, batch.label.shape) # (500, 32, 32, 3) (500,)
    #     print(type(batch.data))
    #
    #    # get some accuracy to examine
    #     if i % 100 == 0:
    #         batch_predict = model.predict(batch.data)
    #         test_predict = model.predict(test.data)
    #
    #         acc_train = accuracy(
    #             prediction=batch_predict, label=batch.label
    #         )
    #
    #         acc_test = accuracy(
    #             prediction=test_predict, label=test.label
    #         )
    #
    #         train_acc_list.append(acc_train)
    #         test_acc_list.append(acc_test)
    #
    #         print("Iter {} of {}: train_acc={}, test_acc={}".format(i, iteration, acc_train, acc_test))
    #
    #     # now do the training
    #     with tf.GradientTape() as tape:
    #         batch_predict = model(batch.data)
    #         loss = tf.keras.losses.sparse_categorical_crossentropy(
    #             y_pred=batch_predict, y_true=batch.label
    #         )
    #         loss = tf.reduce_mean(loss)
    #
    #     train_loss_list.append(loss)
    #
    #     grads = tape.gradient(loss, model.variables)
    #     optimizer.apply_gradients(
    #         grads_and_vars=zip(grads, model.variables)
    #     )
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='luke-mao' date='2020-04-02T10:08:28Z'>
		In the last part of my statement, i do not mean "I cannot do the 500 images flatten process". I mean i would like to input 500 images at once so that to achieve a mini batch process. Thank you.
		</comment>
		<comment id='2' author='luke-mao' date='2020-04-02T10:09:50Z'>
		Although the model is not trained before i call the "predict", i would like to address the batch dimension appears as None in this issue. Thank you ~~
		</comment>
		<comment id='3' author='luke-mao' date='2020-04-02T10:40:13Z'>
		Hello. I would like to present a simpler code for this issue.
&lt;denchmark-code&gt;# for github issue use

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers

if __name__ == '__main__':

    class CNN2(tf.keras.Model):
        def __init__(self):
            super(CNN2, self).__init__()
            print("initialized successfully")

        def build(self, input_shape):
            super(CNN2, self).build(input_shape)

        def call(self, inputs):
            # print(inputs[0, ::])
            print("here inside the model")
            print("inputs.shape = {}".format(inputs.shape)) # (None, 32, 32, 3)
            print("tf.shape = {}".format(tf.shape(inputs)))
            print("type(inputs) = {}".format(type(inputs)))

            # just return something
            return inputs



    model = CNN2()
    model.build(input_shape=(500, 32, 32, 3))
    # # print(model.conv1.input_spec())

    print("Here try to predict")
    var = tf.zeros(shape=[500, 32, 32, 3])
    print(var[0, 0, 0, :])
    y = model.predict(var)
&lt;/denchmark-code&gt;

So the problem is when the model is called, it prints that
&lt;denchmark-code&gt;inputs.shape = (None, 32, 32, 3)
&lt;/denchmark-code&gt;

which is very strange as i actually put a 500 in the first dim.
Thank you. ~~
		</comment>
		<comment id='4' author='luke-mao' date='2020-04-02T11:15:43Z'>
		Hello. I found that, if i use
&lt;denchmark-code&gt;y = model(x) 
&lt;/denchmark-code&gt;

to train the model, everything works fine. The shape of "inputs" inside the model is [500, 32, 32, 3].
But if i use
&lt;denchmark-code&gt;y = model.predict(x)
&lt;/denchmark-code&gt;

x does not support for batch input. If i input a whole batch of 500 images, inside the model it only has shape [None, 32, 32, 3]. How to fix for batch input? Do i have to input all images one by one for prediction?
Thank you and looking forward for your help~~
		</comment>
		<comment id='5' author='luke-mao' date='2020-04-03T07:57:01Z'>
		&lt;denchmark-link:https://github.com/luke-mao&gt;@luke-mao&lt;/denchmark-link&gt;
, Keras functional model allows to input in batch size. Set the batch size accordingly. Please refer the Tensorflow &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly#predict&gt;documentation&lt;/denchmark-link&gt;
 for more. Thanks!
		</comment>
		<comment id='6' author='luke-mao' date='2020-04-03T08:04:26Z'>
		That's wonderful. Thank you a lot. Now my model is up and running!! Thank you for the help.
		</comment>
		<comment id='7' author='luke-mao' date='2020-04-03T08:04:28Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38155&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38155&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>