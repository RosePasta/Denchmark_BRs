<bug id='1234' author='rdipietro' open_date='2016-02-22T10:09:32Z' closed_time='2016-03-23T17:14:57Z'>
	<summary>Update documentation for softmax-with-cross-entropy loss functions</summary>
	<description>
softmax_cross_entropy_with_logits and sparse_softmax_cross_entropy_with_logits both have useful  behavior that conflicts with current documentation.
In softmax_cross_entropy_with_logits: "All that is required is that each row of labels is
a valid probability distribution." In sparse_softmax_cross_entropy_with_logits: "labels: Each entry labels[i] must be an index in [0, num_classes)."
Neither of these statements is true, and using all 0s in the case of softmax_cross_entropy_with_logits or -1s in the case of sparse_softmax_cross_entropy_with_logits is useful when signals include entries for which we don't want to compute loss.
softmax_cross_entropy_with_logits example:
&lt;denchmark-code&gt;logits_list = [tf.Variable([[0.0, 0.0, 0.0]]), tf.Variable([[99.0, 0.0, 0.0]])]
labels = tf.Variable([[0.0, 0.0, 0.0]])
loss_list = [tf.nn.softmax_cross_entropy_with_logits(logits, labels)
             for logits in logits_list]

with tf.Session() as sess:
    sess.run(tf.initialize_all_variables())
    print(sess.run(loss_list))

# [array([ 0.], dtype=float32), array([ 0.], dtype=float32)]
&lt;/denchmark-code&gt;

sparse_softmax_cross_entropy_with_logits example:
&lt;denchmark-code&gt;logits_list = [tf.Variable([[0.0, 0.0, 0.0]]), tf.Variable([[99.0, 0.0, 0.0]])]
labels = tf.cast(tf.Variable([-1]), tf.int64)
loss_list = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels)
             for logits in logits_list]

with tf.Session() as sess:
    sess.run(tf.initialize_all_variables())
    print(sess.run(loss_list))

# [array([ 0.], dtype=float32), array([ 0.], dtype=float32)]
&lt;/denchmark-code&gt;

We could filter invalid entries out before computing logits/loss, but in my use cases this would add a lot of boilerplate code and make only a negligible difference performance wise. (RNNs with long sequences, a fairly small number of time steps, multiple predictions per time step, and batch entries with varying sequence_lengths. Using this trick lets us avoid having to break up logits/targets etc. into valid chunks vs. invalid chunks.
Edit: I ended up applying a boolean mask anyway since I wanted to view the loss over time (not just take gradients and minimize). In any case I like the current behavior so that I can apply masks after I obtain loss values over all targets, both valid and invalid.
	</description>
	<comments>
		<comment id='1' author='rdipietro' date='2016-03-08T01:07:33Z'>
		This seems like reasonable behavior to preserve, since we're unlikely to ever add any kind of checking to these ops (doing so is problematic due to numerical error) and we're unlikely to normalize due to speed.  Documentation contributions welcome!
		</comment>
		<comment id='2' author='rdipietro' date='2016-09-11T18:48:27Z'>
		The described convenient behavior has changed somewhere between versions 0.9 and 0.10.
The snippet with sparse_softmax_cross_entropy_with_logits now results in:
&lt;denchmark-code&gt;[array([ nan], dtype=float32), array([ nan], dtype=float32)]
&lt;/denchmark-code&gt;

Cross entropy for padding logits became nan instead of 0.0. Looks like a breaking change: in my case forward pass is still okay (with masking), but gradients are now stuffed with nans . It took some time to realize that new behavior is version dependent.
Versions with "0" behavior:
&lt;denchmark-link:https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl&gt;linux/gpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl&lt;/denchmark-link&gt;

Versions with "nan" behavior:
&lt;denchmark-link:https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl&gt;linux/gpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl&lt;/denchmark-link&gt;

&lt;denchmark-link:https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl&gt;linux/gpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl&lt;/denchmark-link&gt;

Should we open a new issue or is this new behavior by design?
		</comment>
		<comment id='3' author='rdipietro' date='2016-09-12T05:39:04Z'>
		Can you open a new bug for this, explaining what behavior you think is correct?
		</comment>
		<comment id='4' author='rdipietro' date='2016-09-16T08:36:08Z'>
		I let ivan create the bug issue but I confirm that the new behavior can cause some troubles. I am using sparse_softmax_cross_entropy_with_logits and, in my opinion, the best behavior would be to return a loss of 0.0 and null gradients when the label is not in the range [0, num_classes); never return nan.
		</comment>
		<comment id='5' author='rdipietro' date='2016-09-22T00:45:06Z'>
		This is only going to get more strict.  Passing labels that are out of range is a sign of a hidden bug; one that may have model builders searching for a long time before figuring out the true problem.  Starting in a few days, this op will raise exceptions when passed invalid labels on CPU.  On GPU it will continue to return NaNs because we can't easily pass error flags back from the GPU.
		</comment>
	</comments>
</bug>