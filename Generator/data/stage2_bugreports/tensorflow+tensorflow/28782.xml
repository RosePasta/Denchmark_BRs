<bug id='28782' author='lgeiger' open_date='2019-05-16T22:59:05Z' closed_time='2020-04-02T19:50:27Z'>
	<summary>[TF2.0] Can't use tf.data.Dataset::cache with distribution strategy</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
TensorFlow installed from (source or binary): pip tf-nightly-gpu-2.0-preview
TensorFlow version (use command below): 2.0.0.dev20190514
Python version: 3.7.3
CUDA/cuDNN version: 10 / 7.4.2.24
GPU model and memory: V100

Describe the current behavior
Using tf.data.Dataset::cache and tf.distribute.MirroredStrategy() will fail with Cache should only be read after it has been completed. [Op:MultiDeviceIteratorInit] in TF 2.0 nightly.
Describe the expected behavior
Using tf-nightly-gpu==1.14.1.dev20190514 graph mode tf.data caching works using distribution strategy and doesn't throw an error.
Code to reproduce the issue
import tensorflow as tf
import tensorflow_datasets as tfds

mnist_train, mnist_test = tfds.load(name='mnist', split=[tfds.Split.TRAIN, tfds.Split.TEST], as_supervised=True, data_dir="gs://plumerai/data")

strategy = tf.distribute.MirroredStrategy()

def scale(image, label):
  image = tf.cast(image, tf.float32)
  image /= 255

  return image, label

train_dataset = mnist_train.cache().repeat().map(scale).shuffle(1000).batch(256)
test_dataset = mnist_test.cache().repeat().map(scale).batch(256)

with strategy.scope():
  model = tf.keras.Sequential([
      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(10, activation='softmax')
  ])

  model.compile(loss='sparse_categorical_crossentropy',
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['accuracy'])

model.fit(train_dataset, validation_data=test_dataset, steps_per_epoch=100, validation_steps=10, epochs=10)
Other info / logs
File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py", line 646, in fit validation_freq=validation_freq)
File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_distributed.py", line 143, in fit_distributed steps_name='steps_per_epoch')
File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_arrays.py", line 194, in model_iteration val_iterator = _get_iterator(val_inputs, model._distribution_strategy)
File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_arrays.py", line 512, in _get_iterator inputs, distribution_strategy)
File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/distribute/distributed_training_utils.py", line 529, in get_iterator initialize_iterator(iterator, distribution_strategy)
File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/distribute/distributed_training_utils.py", line 535, in initialize_iterator init_op = control_flow_ops.group(iterator.initialize())
File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py", line 274, in initialize return super(DistributedIteratorV1, self)._initializer
File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py", line 259, in _initializer init_ops.extend(it.initialize())
File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py", line 660, in initialize self._iterator._eager_reset() # pylint: disable=protected-access
File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py", line 333, in _eager_reset max_buffer_size=self._max_buffer_size)
File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py", line 3118, in multi_device_iterator_init _six.raise_from(_core._status_to_exception(e.code, message), None)
File "&lt;string&gt;", line 3, in raise_from tensorflow.python.framework.errors_impl.InternalError: Cache should only be read after it has been completed. [Op:MultiDeviceIteratorInit]
	</description>
	<comments>
		<comment id='1' author='lgeiger' date='2019-05-17T07:34:31Z'>
		Will it be possible to provide full minimal code snippet that can reproduce the issue on version 2.0.0.dev20190514. That would really help us to debug faster. Thanks!
		</comment>
		<comment id='2' author='lgeiger' date='2019-05-17T10:10:45Z'>
		&lt;denchmark-link:https://github.com/achandraa&gt;@achandraa&lt;/denchmark-link&gt;
 Sorry for the incomplete code sample. This was the result of some late night debugging ;)
I added a complete code sample above.
		</comment>
		<comment id='3' author='lgeiger' date='2019-07-23T23:40:35Z'>
		Hi! Any update in this?
		</comment>
		<comment id='4' author='lgeiger' date='2019-10-12T08:04:43Z'>
		I meet the same problem today with newest released tf2.0.
		</comment>
		<comment id='5' author='lgeiger' date='2019-10-16T19:18:02Z'>
		Same problem with me. If I cache on the validation data and use the validation_step in the fit() call, I always get the same error.
However, if I do not include the validation_step, and remove the repeat() from the validation dataset, it works nicely, as the validation runs until the validation_data dataset is exhausted (see the documentation of fit(). As it is not necessary to shuffle the validation data, the repeat is not really necessary here. So this way the problem is solved.
So I guess I did not use the correct validation_steps value after all, but it is not really needed.
		</comment>
		<comment id='6' author='lgeiger' date='2019-10-18T02:28:43Z'>
		
Same problem with me. If I cache on the validation data and use the validation_step in the fit() call, I always get the same error.
However, if I do not include the validation_step, and remove the repeat() from the validation dataset, it works nicely, as the validation runs until the validation_data dataset is exhausted (see the documentation of fit(). As it is not necessary to shuffle the validation data, the repeat is not really necessary here. So this way the problem is solved.
So I guess I did not use the correct validation_steps value after all, but it is not really needed.

I just found my problem come from that I accidently used same tf dataset for both train and valid stage. In the same time, the number of steps I set do not exhausted all data in the cache.
After I use different tf.data.dataset and set a larger number of steps, the problem solved.
		</comment>
		<comment id='7' author='lgeiger' date='2019-10-23T02:05:55Z'>
		Thanks for &lt;denchmark-link:https://github.com/darcula1993&gt;@darcula1993&lt;/denchmark-link&gt;
 . Using  after  or  solved my problem.
		</comment>
		<comment id='8' author='lgeiger' date='2019-11-05T02:10:24Z'>
		I'm experiencing the same issue.
both suggestions by darcula1993 and sbl1996 dont help
A minimal reproduction code:
&lt;denchmark-code&gt;
%tensorflow_version 2.x
#must use tf 1.x to use TPU properly
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import tensorflow_datasets as tfds
import os

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.experimental.TPUStrategy(resolver)

#constants
IMG_HEIGHT=IMG_WIDTH=200
BATCH_SIZE=128

def create_model():
  model = tf.keras.models.Sequential()
  model.add(tf.keras.layers.BatchNormalization(input_shape=(IMG_HEIGHT,IMG_WIDTH,1)))
  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu'))
  model.add(tf.keras.layers.MaxPooling2D(2))
  model.add(tf.keras.layers.Dropout(0.25))

  model.add(tf.keras.layers.BatchNormalization())
  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu'))
  model.add(tf.keras.layers.MaxPooling2D(2))
  model.add(tf.keras.layers.Dropout(0.25))

  model.add(tf.keras.layers.BatchNormalization())
  model.add(tf.keras.layers.Conv2D(64, (5, 5), activation='elu'))
  model.add(tf.keras.layers.MaxPooling2D(2))
  model.add(tf.keras.layers.Dropout(0.25))

  model.add(tf.keras.layers.BatchNormalization())
  model.add(tf.keras.layers.Conv2D(128, (3, 3),  activation='elu'))
  model.add(tf.keras.layers.MaxPooling2D(2))
  model.add(tf.keras.layers.Dropout(0.25))

  model.add(tf.keras.layers.BatchNormalization())
  model.add(tf.keras.layers.Conv2D(256, (3, 3),  activation='elu'))
  #model.add(tf.keras.layers.MaxPooling2D(2))
  model.add(tf.keras.layers.Dropout(0.25))

  model.add(tf.keras.layers.BatchNormalization())
  model.add(tf.keras.layers.Conv2D(512, (3, 3),  activation='elu'))
  model.add(tf.keras.layers.MaxPooling2D(2))
  model.add(tf.keras.layers.Dropout(0.25))

  model.add(tf.keras.layers.Flatten())
  model.add(tf.keras.layers.Dense(256))
  model.add(tf.keras.layers.Activation('elu'))
  model.add(tf.keras.layers.Dropout(0.5))
  model.add(tf.keras.layers.Dense(10))
  model.add(tf.keras.layers.Activation('softmax'))
  return model

@tf.function
def convert(image,label):
  return (tf.image.convert_image_dtype(image, tf.float32),tf.expand_dims(tf.cast(label,tf.float32),0))
@tf.function
def resize(image,label):
  return (tf.image.resize(image,(IMG_HEIGHT,IMG_WIDTH)),label)

test_split, valid_split, train_split = tfds.Split.TRAIN.subsplit([10, 10, 80])
train_ds = (tfds.load("mnist", split=train_split, as_supervised=True, try_gcs=True) 
  .map(convert)
  .map(resize)
  .batch(BATCH_SIZE).take(144).cache().repeat().prefetch(50))
#if the .cache() is removed the issue disappears

with strategy.scope():
  model=create_model()
  model.compile(
        optimizer=tf.keras.optimizers.Adam(),
        loss="sparse_categorical_crossentropy",
        metrics=["sparse_categorical_accuracy"])

model.fit(train_ds,epochs=15,steps_per_epoch=200)
&lt;/denchmark-code&gt;

Produces this error
&lt;denchmark-code&gt;InternalError: Cache should only be read after it has been completed.
Additional GRPC error information:
{"created":"@1572919336.377239490","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Cache should only be read after it has been completed.","grpc_status":13} [Op:MultiDeviceIteratorInit]
&lt;/denchmark-code&gt;

The environment is a Colab notebook with python 3.6 and TPU accelerator.
TF version:v2.0.0-rc2-26-g64c3d38 2.0.0
The problem disappears if the code is switched to TF 1.x and training runs well.
		</comment>
		<comment id='9' author='lgeiger' date='2019-11-08T02:05:07Z'>
		
I'm experiencing the same issue.
both suggestions by darcula1993 and sbl1996 dont help
A minimal reproduction code:

%tensorflow_version 2.x
#must use tf 1.x to use TPU properly
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import tensorflow_datasets as tfds
import os

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.experimental.TPUStrategy(resolver)

#constants
IMG_HEIGHT=IMG_WIDTH=200
BATCH_SIZE=128

def create_model():
  model = tf.keras.models.Sequential()
  model.add(tf.keras.layers.BatchNormalization(input_shape=(IMG_HEIGHT,IMG_WIDTH,1)))
  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu'))
  model.add(tf.keras.layers.MaxPooling2D(2))
  model.add(tf.keras.layers.Dropout(0.25))

  model.add(tf.keras.layers.BatchNormalization())
  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu'))
  model.add(tf.keras.layers.MaxPooling2D(2))
  model.add(tf.keras.layers.Dropout(0.25))

  model.add(tf.keras.layers.BatchNormalization())
  model.add(tf.keras.layers.Conv2D(64, (5, 5), activation='elu'))
  model.add(tf.keras.layers.MaxPooling2D(2))
  model.add(tf.keras.layers.Dropout(0.25))

  model.add(tf.keras.layers.BatchNormalization())
  model.add(tf.keras.layers.Conv2D(128, (3, 3),  activation='elu'))
  model.add(tf.keras.layers.MaxPooling2D(2))
  model.add(tf.keras.layers.Dropout(0.25))

  model.add(tf.keras.layers.BatchNormalization())
  model.add(tf.keras.layers.Conv2D(256, (3, 3),  activation='elu'))
  #model.add(tf.keras.layers.MaxPooling2D(2))
  model.add(tf.keras.layers.Dropout(0.25))

  model.add(tf.keras.layers.BatchNormalization())
  model.add(tf.keras.layers.Conv2D(512, (3, 3),  activation='elu'))
  model.add(tf.keras.layers.MaxPooling2D(2))
  model.add(tf.keras.layers.Dropout(0.25))

  model.add(tf.keras.layers.Flatten())
  model.add(tf.keras.layers.Dense(256))
  model.add(tf.keras.layers.Activation('elu'))
  model.add(tf.keras.layers.Dropout(0.5))
  model.add(tf.keras.layers.Dense(10))
  model.add(tf.keras.layers.Activation('softmax'))
  return model

@tf.function
def convert(image,label):
  return (tf.image.convert_image_dtype(image, tf.float32),tf.expand_dims(tf.cast(label,tf.float32),0))
@tf.function
def resize(image,label):
  return (tf.image.resize(image,(IMG_HEIGHT,IMG_WIDTH)),label)

test_split, valid_split, train_split = tfds.Split.TRAIN.subsplit([10, 10, 80])
train_ds = (tfds.load("mnist", split=train_split, as_supervised=True, try_gcs=True) 
  .map(convert)
  .map(resize)
  .batch(BATCH_SIZE).take(144).cache().repeat().prefetch(50))
#if the .cache() is removed the issue disappears

with strategy.scope():
  model=create_model()
  model.compile(
        optimizer=tf.keras.optimizers.Adam(),
        loss="sparse_categorical_crossentropy",
        metrics=["sparse_categorical_accuracy"])

model.fit(train_ds,epochs=15,steps_per_epoch=200)

Produces this error
InternalError: Cache should only be read after it has been completed.
Additional GRPC error information:
{"created":"@1572919336.377239490","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Cache should only be read after it has been completed.","grpc_status":13} [Op:MultiDeviceIteratorInit]

The environment is a Colab notebook with python 3.6 and TPU accelerator.
TF version:v2.0.0-rc2-26-g64c3d38 2.0.0
The problem disappears if the code is switched to TF 1.x and training runs well.

try catche first then map?
		</comment>
		<comment id='10' author='lgeiger' date='2020-01-13T21:20:10Z'>
		Can you try with TF 2.1? I believe this issue should have been fixed by &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/08f41c6216d177933ba8eb48cd171a1e004e6ca2&gt;08f41c6&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='lgeiger' date='2020-04-02T19:50:27Z'>
		Closing this issue since it's resolved. Feel free to reopen if still have problems. Thanks!
		</comment>
		<comment id='12' author='lgeiger' date='2020-04-02T19:50:34Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28782&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28782&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>