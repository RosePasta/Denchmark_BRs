<bug id='31309' author='ingo-m' open_date='2019-08-03T20:08:00Z' closed_time='2019-12-17T20:33:31Z'>
	<summary>Custom loss function fails with sample_weight and batch_size &amp;gt; 1</summary>
	<description>
System information

Have I written custom code: Yes
OS Platform and Distribution: Debian 9.9
TensorFlow installed from: conda (-c anaconda)
TensorFlow version: 1.14.0
Python version: 3.7.3
GPU model and memory: n/a - tested in CPU mode

Describe the current behavior
An error occurs when training an LSTM with a custom loss function, using sample_weight and batch_size &gt; 1. The error does not occur if batch_size = 1, or if sample_weight = None.
Describe the expected behavior
I would expect custom loss functions to work irrespective of batch size and sample weights.
Code to reproduce the issue
Hereâ€™s a minimal example:
import numpy as np
import tensorflow as tf

batch_size = 32  # no problem if this is 1
sequence_len = 1
embedding_size = 100

x_train = np.random.randn(batch_size, sequence_len, embedding_size)
y_train = np.random.randn(batch_size, embedding_size)
sample_weight = np.random.randn(batch_size)  # no problem if this is None

train_input = tf.keras.Input(shape=(sequence_len, embedding_size),
                             batch_size=batch_size)

lstm_layer = tf.keras.layers.LSTM(200,
                                  return_sequences=False,
                                  )(train_input)

dense_layer = tf.keras.layers.Dense(embedding_size,
                                    )(lstm_layer)

model = tf.keras.models.Model(inputs=train_input, outputs=dense_layer)

model.summary()

# Custom loss function. This function could of course be replaced with
# tf.keras.losses.mean_squared_error, but I have a use case where I need a
# custom loss function.
class customLoss(tf.keras.losses.Loss):
    def call(self, y_true, y_pred):
        return tf.reduce_mean(tf.math.squared_difference(y_true, y_pred))

model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),
              loss=customLoss())

loss = model.train_on_batch(x_train,
                            y=y_train,
                            sample_weight=sample_weight)
Other info / logs
In &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29026&gt;#29026&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;
 has pointed out that loss functions from  do not work with keras, and suggested to use loss functions from  instead (thanks again!). Consequently, I thought that defining a custom loss function using the  base class should be possible. (Please note that in my actual use case I have a more complex custom loss function for which I need some math operations from .)
Traceback:
Traceback (most recent call last):
  File "/home/john/PhD/GitLab/literary_lstm/bug_minimal_example_03.py", line 38, in &lt;module&gt;
    sample_weight=sample_weight)
  File "/home/john/miniconda3/envs/py_tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py", line 1175, in train_on_batch
    outputs = self.train_function(ins)  # pylint: disable=not-callable
  File "/home/john/miniconda3/envs/py_tf/lib/python3.7/site-packages/tensorflow/python/keras/backend.py", line 3292, in __call__
    run_metadata=self.run_metadata)
  File "/home/john/miniconda3/envs/py_tf/lib/python3.7/site-packages/tensorflow/python/client/session.py", line 1458, in __call__
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Can not squeeze dim[0], expected a dimension of 1, got 32
	 [[{{node loss_1/dense_loss/weighted_loss/Squeeze}}]]
	</description>
	<comments>
		<comment id='1' author='ingo-m' date='2019-12-17T16:40:47Z'>
		I have exactly the same issue.
I use tf.keras.loss but I still get this error, I don't know hot to fix that.
		</comment>
		<comment id='2' author='ingo-m' date='2019-12-17T20:33:27Z'>
		Thank for reporting this &lt;denchmark-link:https://github.com/ingo-m&gt;@ingo-m&lt;/denchmark-link&gt;
 . The issue is because of how the custom loss is expected to be implemented. The  in  class is expected to return per-sample loss values.
Please take a look at the documentation for , ,  here: &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/losses/Loss?version=nightly#__call&gt;https://www.tensorflow.org/api_docs/python/tf/keras/losses/Loss?version=nightly#__call&lt;/denchmark-link&gt;
__
call basically reduces y_true and y_pred with shape [batch_size, d0, .. dN] to loss values of shape [batch_size, d0, .. dN-1]. The reduce_mean is along the last dimension. To this result sample_weight that is broadcastable to shape [batch_size, d0, .. dN-1] is applied.
In your code updating the loss function like:
return tf.reduce_mean(tf.math.squared_difference(y_true, y_pred), axis=-1) will fix the issue.
Hope this helps :) Closing out the issue, please feel free to add comments if you have more questions.
		</comment>
		<comment id='3' author='ingo-m' date='2019-12-17T20:33:32Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31309&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31309&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='ingo-m' date='2020-07-29T15:26:20Z'>
		I'm currently trying to extend the example here with sample weights: &lt;denchmark-link:https://www.tensorflow.org/guide/keras/custom_layers_and_models&gt;https://www.tensorflow.org/guide/keras/custom_layers_and_models&lt;/denchmark-link&gt;

Until now I had no luck :(.
&lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;
 Could you please explain how the example can be modified to use sample weights?  (or class_weights)
		</comment>
		<comment id='5' author='ingo-m' date='2020-08-17T23:07:24Z'>
		&lt;denchmark-link:https://github.com/oholimoli&gt;@oholimoli&lt;/denchmark-link&gt;
  You will have to provide the weights when calling the loss function in the custom training loop. here -&gt; &lt;denchmark-link:https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example&gt;https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>