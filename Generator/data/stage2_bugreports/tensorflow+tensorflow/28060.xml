<bug id='28060' author='xykong1958' open_date='2019-04-23T00:49:43Z' closed_time='2019-05-06T18:20:35Z'>
	<summary>TensorFlow/XLA reports an internal error "Duplicate variable passed tp XLA cluster" when build and run BERT</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):


OS Platform and Distribution (e.g., Linux Ubuntu 16.04):


Linux Ubuntu 16.04.3

Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):

source

TensorFlow version (use command below):

ToT

Python version:

3.6

Bazel version (if compiling from source):

19.04

GCC/Compiler version (if compiling from source):

5.4.0

CUDA/cuDNN version:

10.0/7.4

GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
When XLA is off,  the BERT compiles fine.
Describe the expected behavior
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
The following code pattern in the TF graph will expose the problem,
op0:  Node "VarHandleOp"
op1:  Node "Switch" op0, pred
op2:  Node "ReadVariableOp" op0
op3:  Node "ReadVariableOp" op1
Whem op2 and op3 are clustered together in XLA,  the inputs op0 and op1 are
considered resource arguments, and they happen to map the the same resource
op0, and lead to the error  "Duplicate variable passed to XLA cluster" reported at
line 135, xla_launch_util.cc.
I am worarounding the problem by placing constraint on clustable nodes in
mark_for_compilation_pass.cc, so that nodes with resource input from a "Switch"
node be rejected for clustering.  Some people suggest that it may be better to fix
the problem where the problem is reported.
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='xykong1958' date='2019-04-25T14:24:25Z'>
		This is a current limitation of XLA (not allowing the same resource variable to flow into a cluster twice). There is a change in flight that will make XLA fall back to normal tensorflow execution but that probably is not what you are looking for.
The change you are proposing might fix the immediate issue at hands but does not really solve the problem. There are other ways that a resource variable might end up aliasing. But as a workaround for your problem it is possibly fine.
I have to check up on XLA semantics around resource variables to see whether this is a limitation of the runtime or rather a conceptual mismatch with XLA semantics.
		</comment>
		<comment id='2' author='xykong1958' date='2019-04-25T17:18:59Z'>
		Thanks for your quick response.  I would think fall back to TF-native is not a good option for the case,
though it is a way to avoid the problem going forward, but it may cause potential performance
issue, too.
I thought "Switch" may be a special case causing the problem, not sure if you could provide
examples of other cases with the same problem (duplicated resources passing to the same cluster),
On the workaround, do you think I should file a PR to get it into the ToT ?
		</comment>
		<comment id='3' author='xykong1958' date='2019-04-29T22:26:29Z'>
		&lt;denchmark-link:https://github.com/jlebar&gt;@jlebar&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='xykong1958' date='2019-04-29T22:38:08Z'>
		&lt;denchmark-link:https://github.com/sherhut&gt;@sherhut&lt;/denchmark-link&gt;
 works on XLA and appears to be on it?
		</comment>
		<comment id='5' author='xykong1958' date='2019-04-29T22:38:13Z'>
		
The following code pattern in the TF graph will expose the problem,
op0: Node "VarHandleOp"
op1: Node "Switch" op0, pred
op2: Node "ReadVariableOp" op0
op3: Node "ReadVariableOp" op1

This seems suspicious to me -- op2 and op3 should not be in the same cluster because they have different deadness.
		</comment>
		<comment id='6' author='xykong1958' date='2019-05-06T18:20:35Z'>
		Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
		<comment id='7' author='xykong1958' date='2019-05-06T18:20:36Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=28060&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=28060&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>