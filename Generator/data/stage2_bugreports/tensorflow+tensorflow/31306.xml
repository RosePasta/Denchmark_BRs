<bug id='31306' author='erikchwang' open_date='2019-08-03T15:34:47Z' closed_time='2019-09-06T17:35:34Z'>
	<summary>A correct way to use tf.contrib.opt.AdamWOptimizer</summary>
	<description>
tf.contrib.opt.AdamWOptimizer requires two arguments: weight_decay and learning_rate. Since learning_rate is usually decayed along with the training, should weight_decay also be decayed with the same schedule? Would you please provide an example?
	</description>
	<comments>
		<comment id='1' author='erikchwang' date='2019-08-16T16:24:13Z'>
		When using warm up, do we need to warm up weight_decay together with learning_rate?
		</comment>
		<comment id='2' author='erikchwang' date='2019-08-23T13:15:06Z'>
		I can imagine implementing it either way.
&lt;denchmark-link:https://github.com/mingxingtan&gt;@mingxingtan&lt;/denchmark-link&gt;
 appears to be the author, maybe they can help.
		</comment>
		<comment id='3' author='erikchwang' date='2019-09-06T17:13:40Z'>
		Sorry, I just merged an external PR. The original PR that adds this optimizer should be this: &lt;denchmark-link:https://github.com/tensorflow/addons/pull/164&gt;tensorflow/addons#164&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='erikchwang' date='2019-09-06T17:35:34Z'>
		Cool.
So since tf.contrib is being deleted, and this lives in tensorflow/addons now Lets close this bug, and anyone who's interested can continue the conversation in tensorflow/addons/issues.
Right?
		</comment>
		<comment id='5' author='erikchwang' date='2019-09-06T17:35:36Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=31306&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=31306&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>