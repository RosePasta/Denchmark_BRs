<bug id='35535' author='apple11361' open_date='2020-01-02T07:36:31Z' closed_time='2020-01-13T23:34:55Z'>
	<summary>lite/micro: Incorrectly copy quantization information from serialized data</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): source
TensorFlow version (use command below):
Python version:
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source): gcc/c++ 5.4.0
CUDA/cuDNN version:
GPU model and memory:

Describe the current behavior
When micro allocator copies quantization information from serialized data in InitializeRuntimeTensor(), it copies 8 bytes for 'zero_point'. However, the size of 'zero_point' is 4 bytes, so the next member data('allocation_type') will be modified accidentally.
Describe the expected behavior
I think it should just copy 4 bytes for 'zero_point'.
Code to reproduce the issue
Any test case in tensorflow/lite/micro/examples/ can reproduce the issue.
Other info / logs
This is the fragment of bug code
&lt;denchmark-code&gt;  // This magic handles issues with little-endianness.
  for (unsigned int b = 0; b &lt; sizeof(int64_t); ++b)
    *(reinterpret_cast&lt;char*&gt;(&amp;result-&gt;params.zero_point) + b) =
        *(reinterpret_cast&lt;const char*&gt;(
          src_quantization-&gt;zero_point()-&gt;Data()) +
            b);
    result-&gt;params.zero_point =
      flatbuffers::EndianScalar(result-&gt;params.zero_point);
&lt;/denchmark-code&gt;

Screenshot below is a tensor before the code
&lt;denchmark-link:https://user-images.githubusercontent.com/11289375/71656049-b5562780-2d74-11ea-84ee-48cbe5b424b0.png&gt;&lt;/denchmark-link&gt;

Screenshot below is a tensor after the code
&lt;denchmark-link:https://user-images.githubusercontent.com/11289375/71656203-652b9500-2d75-11ea-8e35-8ff7810da4d1.png&gt;&lt;/denchmark-link&gt;

The quantization information should not modify the allocation_type. I think sizeof(int64_t) should be changed into sizeof(result-&gt;params.zero_point)
	</description>
	<comments>
		<comment id='1' author='apple11361' date='2020-01-13T23:34:55Z'>
		Thanks for reporting this issue.  I have created a patch to fix this internally, and it should merge in shortly.
		</comment>
		<comment id='2' author='apple11361' date='2020-01-13T23:34:56Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35535&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35535&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='apple11361' date='2020-01-15T03:32:39Z'>
		Hello, &lt;denchmark-link:https://github.com/njeffrie&gt;@njeffrie&lt;/denchmark-link&gt;
 :
Thank you for your patch. However, I don't think the patch can resolve the issue. I want to know why the patch is that change  into . I think it doesn't make sense because the return type of  is  and the type of  is . In some machines,  is 8, that will overwrite memory allocation type. Why are there two .
		</comment>
		<comment id='4' author='apple11361' date='2020-01-15T18:58:31Z'>
		Oops, good catch!  Unintentionally added an additional sizeof there. Submitting patch to fix this.
		</comment>
		<comment id='5' author='apple11361' date='2020-01-16T02:17:16Z'>
		Thank you very much.
		</comment>
	</comments>
</bug>