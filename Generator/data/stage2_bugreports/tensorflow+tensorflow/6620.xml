<bug id='6620' author='boche' open_date='2017-01-03T20:54:17Z' closed_time='2017-11-30T05:26:23Z'>
	<summary>CudnnLSTM doesn't work with AdamOptimizer</summary>
	<description>
I am testing how to use CudnnLSTM, there is not a lot of documentation on this. In my own experiment, I found when use AdamOptimizer with CudnnLSTM, it always raises the following Exception.
I also found another repository using CudnnLSTM, and uploaded it here: &lt;denchmark-link:https://github.com/boche/LM-PTB-CUDNNLSTM&gt;https://github.com/boche/LM-PTB-CUDNNLSTM&lt;/denchmark-link&gt;
. It also raises the same exception.

File "ptb_word_lm.py", line 465, in main
m = PTBModel(is_training=True, config=config, debug=FLAGS.debug)
File "ptb_word_lm.py", line 254, in init
self._train_op = optimizer.apply_gradients(zip(allgrads, allvars))
File "/data/ASR1/ramons/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py", line 409, in apply_gradients
self._create_slots(var_list)
File "/data/ASR1/ramons/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/adam.py", line 119, in _create_slots
self._zeros_slot(v, "m", self._name)
File "/data/ASR1/ramons/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py", line 609, in _zeros_slot
named_slots[var] = slot_creator.create_zeros_slot(var, op_name)
File "/data/ASR1/ramons/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py", line 121, in create_zeros_slot
val = array_ops.zeros(primary.get_shape().as_list(), dtype=dtype)
File "/data/ASR1/ramons/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py", line 782, in as_list
raise ValueError("as_list() is not defined on an unknown TensorShape.")
ValueError: as_list() is not defined on an unknown TensorShape.

For now, using GradientDescentOptimizer is okay, but it seems all other fancy optimizers all have similar problems. I have looked up this problem on stackoverflow, and found a related thread: &lt;denchmark-link:http://stackoverflow.com/questions/40698821/tensorflow-adamoptimizer-throws-error-when-variable-has-validate-shape-false&gt;http://stackoverflow.com/questions/40698821/tensorflow-adamoptimizer-throws-error-when-variable-has-validate-shape-false&lt;/denchmark-link&gt;
 .
It seems the problem is that parameter buffer used in CudnnLSTM doesn't have fixed shape (validate_shape=False, defined in line 145 of &lt;denchmark-link:https://github.com/boche/LM-PTB-CUDNNLSTM/blob/master/ptb_word_lm.py&gt;https://github.com/boche/LM-PTB-CUDNNLSTM/blob/master/ptb_word_lm.py&lt;/denchmark-link&gt;
 ) , which is required by AdamOptimizer.
Cudnn seems to be faster (less time per epoch), but if we can't use better learning algorithms with it (meaning more epochs), then the total running time may not be improved so much.
	</description>
	<comments>
		<comment id='1' author='boche' date='2017-01-05T02:41:11Z'>
		I also met the problem. Now I am using a method like this: Compute the params size first , and build the cudnn_lstm model with fixed shape. It can work, but looks ugly. Hope to get a better solution.
		</comment>
		<comment id='2' author='boche' date='2017-01-12T21:20:54Z'>
		&lt;denchmark-link:https://github.com/boche&gt;@boche&lt;/denchmark-link&gt;
, yeah, you are right that CudnnLSTM doesn't have a shape known at static time, which is required by AdamOptimizer. As CuDNN manages the internal storage structure for weight and bias parameters, this shape is hardware-and-CuDNN dependent and only available at runtime. I think the solution by &lt;denchmark-link:https://github.com/robotnc&gt;@robotnc&lt;/denchmark-link&gt;
 is a right, natural thing to do, given the this requirement of a known shape by AdamOptimizer, and the fact that it could only be provided at runtime by CuDNN.
		</comment>
		<comment id='3' author='boche' date='2017-04-25T18:52:59Z'>
		Looks like a serious issue; &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/zhangyaobit&gt;@zhangyaobit&lt;/denchmark-link&gt;
 - can we add static shape info to the parameters created by cudnn-rnn?  Perhaps by exposing the cudnn library helper functions via swig?  Is there a document describing how the shape varies as a fn of hardware and version?
Alternatively can we modify the Adam optimizer to not require static shapes?
		</comment>
		<comment id='4' author='boche' date='2017-04-25T18:54:33Z'>
		Following up: looking at the slot_creator code, looks like we no longer require static shapes.  Can you check to see if the error still exists in the TF nightlies?
		</comment>
		<comment id='5' author='boche' date='2017-04-25T20:47:01Z'>
		I just tried it with the latest nightly build and the problem is still there.
Note that what's needed is not really the shape, but the size of the parameter buffer, since what's being passed to CudnnLSTM is just a flat 1D vector anyway. And the size should not vary from HW iteration to another (otherwise there cannot be a 'canonical' version and the whole premise of RNNParamsSaveable would be moot). Given that, it seems like a very simple solution would be to just calculate the size and return that for model.params_size(). For an LSTM it's literally just:
&lt;denchmark-code&gt;params_size = ((input_size * layer_size * 4) + (layer_size * layer_size * 4) + (layer_size * 2 * 4)) * dir_count
&lt;/denchmark-code&gt;

for one layer. Multiple layers would need to modify the input_size for layers &gt; 1 to be the size of the previous layer.
		</comment>
		<comment id='6' author='boche' date='2017-05-11T03:05:33Z'>
		Any updates on this?
		</comment>
		<comment id='7' author='boche' date='2017-07-05T08:12:43Z'>
		A related issue is &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/5972&gt;#5972&lt;/denchmark-link&gt;
, to be able to optimize variables with dynamic/unknown shape.
		</comment>
		<comment id='8' author='boche' date='2017-08-24T22:04:28Z'>
		&lt;denchmark-link:https://github.com/robotnc&gt;@robotnc&lt;/denchmark-link&gt;
 Could you elaborate more on your workaround? Thanks!
		</comment>
		<comment id='9' author='boche' date='2017-08-25T08:53:14Z'>
		&lt;denchmark-link:https://github.com/dchaws&gt;@dchaws&lt;/denchmark-link&gt;
: When you create the variable, use the static size  like &lt;denchmark-link:https://github.com/alquraishi&gt;@alquraishi&lt;/denchmark-link&gt;
 describes. That's all.
		</comment>
		<comment id='10' author='boche' date='2017-09-18T11:16:50Z'>
		could somebody give a code example for the workaround, please?
		</comment>
		<comment id='11' author='boche' date='2017-11-02T04:15:45Z'>
		any updates on this or a workaround ?
		</comment>
		<comment id='12' author='boche' date='2017-11-03T04:06:40Z'>
		AdamOptimizer should work with the new Cudnn layer API in cudnn_rnn.py
We have a CL to switch tf.contrib.cudnn_rnn.CudnnLSTM to point to classes in cudnn_rnn.py instead of cudnn_rnn_ops.py, it's running a series of tests, should be submitted soon.
The following should work with the new layer API.
  num_layers = 4
  num_units = 2
  batch_size = 8
  dir_count = 1

  inputs = tf.random_uniform([
      num_layers * dir_count, batch_size, num_units], dtype=tf.float32)

  lstm = cudnn_rnn.CudnnLSTM(num_layers, num_units,
                             name="awesome_lstm")

  outputs, _ = lstm(inputs)
  loss = tf.reduce_sum(outputs)
  var = lstm.trainable_variables[0]

  grad = tf.gradients(loss, var)[0]
  print('grad.shape: %s' % grad.shape)
  print('var.shape: %s' % var.shape)

  opt = tf.train.AdamOptimizer()
  train_op = opt.apply_gradients([(grad, lstm.trainable_variables[0])])

  with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(outputs))
    sess.run(train_op)
    print(sess.run(outputs))
		</comment>
		<comment id='13' author='boche' date='2017-11-03T08:31:17Z'>
		&lt;denchmark-link:https://github.com/protoget&gt;@protoget&lt;/denchmark-link&gt;
 what does "CL" stand for?
		</comment>
		<comment id='14' author='boche' date='2017-11-03T15:54:46Z'>
		It means "PR"
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Fri, Nov 3, 2017 at 1:31 AM, stefbraun ***@***.***&gt; wrote:
 @protoget &lt;https://github.com/protoget&gt; what does "CL" stand for?

 —
 You are receiving this because you modified the open/close state.
 Reply to this email directly, view it on GitHub
 &lt;#6620 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtimyHlSvu5hKFY6pkM6qc961PBCKmMks5sys9xgaJpZM4LaA_V&gt;
 .



		</comment>
		<comment id='15' author='boche' date='2017-11-03T21:22:55Z'>
		TypeError: call() takes at least 5 arguments (2 given)
Also , I am having difficulty figuring out the param sizes for bidirectional Cudnns.  So If my expected output for a single layer bi-directional cudnnLSTM is 2*dim  and input is of size [batch_size,seq_len,dim] how should i set up the h,c matrices . Is the following method correct in terms of usage. (Basically i cannot figure out the link between num_units,input_size in CudnnLSTM and in h and c matrix.

def createCudnnLayer(batch_size, input, input_length, dim, dropout_rate,scope_name, reuse, is_training) :
with tf.variable_scope(scope_name, reuse=reuse):
    inputs = tf.transpose(input, [1, 0, 2])
    cell = tf.contrib.cudnn_rnn.CudnnLSTM(direction='bidirectional',
                                          num_layers=1,
                                          num_units=dim*2,
                                          input_size=dim*2,
                                          dropout=dropout_rate if is_training else 0)
    params_size_t = cell.params_size()
    rnn_params = tf.get_variable(
        "cudnn_params",
        initializer=tf.random_uniform(
            [params_size_t], -0.1, 0.1),
        validate_shape=False)
    c = tf.zeros([2, batch_size, dim], tf.float32)
    h = tf.zeros([2, batch_size, dim],tf.float32)
    outputs, h, c = cell(input_data=inputs, input_h=h, input_c=c, params=rnn_params, is_training=is_training)
    outputs = tf.transpose(outputs, [1, 0, 2])
    f_rep = outputs[:, :, 0:dim]
    b_rep = outputs[:, :, dim:2*dim]
    return (f_rep,b_rep) 


		</comment>
		<comment id='16' author='boche' date='2017-11-04T04:05:37Z'>
		You might be using the old cudnn_rnn "ops" API instead of the "layers" API.
The new nightly build (tmr) should now point tf.contrib.cudnn_rnn.CuDNNLSTM to the layers API.
With the layer API, you don't need to figure out the size yourself, the layer creates and owns the variables.
		</comment>
		<comment id='17' author='boche' date='2017-11-05T20:15:45Z'>
		excellent !! Thanks it works now.
		</comment>
		<comment id='18' author='boche' date='2017-11-30T05:14:05Z'>
		will this change be included in tf-1.5? any examples for multi gpu usage of the new CudnnLSTM?
		</comment>
		<comment id='19' author='boche' date='2017-11-30T05:26:17Z'>
		yes it'd be included in 1.5.
You can check cudnn_rnn_test.py.
ptb_word_lm.py is updated but not released to the public yet.
		</comment>
		<comment id='20' author='boche' date='2017-11-30T06:51:29Z'>
		Thanks again. Could you please advise on how to use CudnnLSTM trained models on cpu's at test time ?  i,e how to convert Bidirectional CudnnLSTM to say Bidirectional BasicLSTM that can be used on cpus.
		</comment>
		<comment id='21' author='boche' date='2018-01-15T19:06:18Z'>
		I'm no longer seeing problems with Adam, but both RMSprop and Adagrad don't appear to work with cudnnLSTM. This is the error I'm getting with RMSprop:
Shape of a new variable (geomnet/geomnet/layer0/fw/cudnn_lstm/opaque_kernel/RMSProp/) must be fully defined, but instead was &lt;unknown&gt;.
And Adagrad:
ValueError: If initializer is a constant, do not specify shape.
I tried steepest, momentum, adam, and adadelta and they all work.
		</comment>
		<comment id='22' author='boche' date='2018-01-23T19:14:18Z'>
		Re &lt;denchmark-link:https://github.com/alquraishi&gt;@alquraishi&lt;/denchmark-link&gt;
 I have a fix pending review and hopefully submit in 2 dasy.
		</comment>
		<comment id='23' author='boche' date='2018-01-24T06:49:13Z'>
		&lt;denchmark-link:https://github.com/protoget&gt;@protoget&lt;/denchmark-link&gt;
  It does need to give state_h and state_c variable for CudnnLSTM in TF 1.5?
		</comment>
	</comments>
</bug>