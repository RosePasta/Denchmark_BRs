<bug id='42263' author='Jordy-VL' open_date='2020-08-12T12:41:26Z' closed_time='2020-09-02T18:22:18Z'>
	<summary>Custom TF metric: "Graph" tensor leakage</summary>
	<description>
System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v2.2
Python version: 3.6.9
CUDA/cuDNN version: v10.2
GPU model and memory: GeForce GTX 1070 - 8117MiB
Describe the current behavior:
My custom metric works on sample inputs; yet after model compilation breaks down due to an error related to eager and graph tensors. I did not find anything specific which helps me resolve the issue.
Issue:
&lt;denchmark-code&gt;a "Graph" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: Max:0
&lt;/denchmark-code&gt;

Describe the expected behavior:
Custom metric is usable as a metric in a TF2 model, logging to tensorboard as well.
Standalone code to reproduce the issue
&lt;denchmark-link:https://colab.research.google.com/drive/1FPGz3dK2zSI2wjuTYrywaUG0UlHnej6c?usp=sharing&gt;https://colab.research.google.com/drive/1FPGz3dK2zSI2wjuTYrywaUG0UlHnej6c?usp=sharing&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='Jordy-VL' date='2020-08-13T11:05:19Z'>
		I am able to replicate this issue, please find the &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/8c8a0b0da94d5de7210ab07d077fba22/untitled369.ipynb&gt;gist here&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='Jordy-VL' date='2020-08-14T13:08:16Z'>
		Any updates &lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='3' author='Jordy-VL' date='2020-08-14T21:36:16Z'>
		Hi &lt;denchmark-link:https://github.com/Jordy-VL&gt;@Jordy-VL&lt;/denchmark-link&gt;
, I've seen this error reported a few times and as I understand it it's caused when an outer tf.function is retraced where there's an inner tf.function that has accessed a tensor created in the outer tf.function. Here is a simple example which results in the same error message. Removing (1) or (2) should make the error go away.
&lt;denchmark-code&gt;class Model:
  def __init__(self):
    self.v = None
    self.t = None

  @tf.function  # (1)
  def bar(self):
    print('tracing bar_g')
    return self.t  # (2)

  def foo(self):
    y = self.bar()
    self.v.assign_sub(1.0)
    
  @tf.function
  def loop(self):
    print('tracing loop')
    if self.t is None:
      self.t = tf.constant(1.0)
    if self.v is None:
      self.v = tf.Variable(1.0)
    self.foo()

m = Model()
m.loop()
&lt;/denchmark-code&gt;

I think that explains why you aren't seeing the error in test_simple() and you are seeing the error with fit. If you set run_eagerly=True in compile, it seems to run okay as well.
It's not immediately obvious to me from your code what is causing the problem. Can you take a look at this similar issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/32889&gt;#32889&lt;/denchmark-link&gt;
 and let me know if that helps? I'll do some more investigation into whether this is a bug or intended behavior.
		</comment>
		<comment id='4' author='Jordy-VL' date='2020-08-17T15:24:16Z'>
		
Hi @Jordy-VL, I've seen this error reported a few times and as I understand it it's caused when an outer tf.function is retraced where there's an inner tf.function that has accessed a tensor created in the outer tf.function. Here is a simple example which results in the same error function. Removing (1) or (2) should make the error go away.
class Model:
  def __init__(self):
    self.v = None
    self.t = None

  @tf.function  # (1)
  def bar(self):
    print('tracing bar_g')
    return self.t  # (2)

  def foo(self):
    y = self.bar()
    self.v.assign_sub(1.0)
    
  @tf.function
  def loop(self):
    print('tracing loop')
    if self.t is None:
      self.t = tf.constant(1.0)
    if self.v is None:
      self.v = tf.Variable(1.0)
    self.foo()

m = Model()
m.loop()

I think that explains why you aren't seeing the error in test_simple() and you are seeing the error with fit. If you set run_eagerly=True in compile, it seems to run okay as well.
It's not immediately obvious to me from your code what is causing the problem. Can you take a look at this similar issue #32889 and let me know if that helps? I'll do some more investigation into whether this is a bug or intended behavior.

Thank you for the explanation.
Essentially, I believe my error is caused by having my self.confidences and self.correctness as np.arrays or lists to which I concatenate new entries. Since it is not a TF.variable or constant, it loses the trace.
So my follow-up question is, how can I keep a variable-size list to which I append more entries?
The metric does not work when only saving float/int information like "True_Positives" AND the number of training/validation samples is not known upfront.
		</comment>
		<comment id='5' author='Jordy-VL' date='2020-08-17T17:24:30Z'>
		For a variable-size list have you tried using a &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/TensorArray&gt;tf.TensorArray&lt;/denchmark-link&gt;
? The &lt;denchmark-link:https://www.tensorflow.org/guide/function#loops&gt;Accumulating values in a loop&lt;/denchmark-link&gt;
 example in the tf.function guide might be helpful.
		</comment>
		<comment id='6' author='Jordy-VL' date='2020-08-19T12:46:20Z'>
		I tried it with Tf.TensorArrray.
&lt;denchmark-code&gt;            self.correctness = tf.TensorArray(tf.int32, dynamic_size=True, infer_shape=True)
            self.confidences = tf.TensorArray(tf.float32, dynamic_size=True, infer_shape=True)
&lt;/denchmark-code&gt;

I got the following error:
ValueError: Size must be declared for TensorArrays when eager execution is enabled.
		</comment>
		<comment id='7' author='Jordy-VL' date='2020-08-19T17:21:45Z'>
		You can set the  argument to your TensorArray. With  then the array can grow past its original size. You can see that noted under the &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/TensorArray&gt;Args section of the TensorArray docs&lt;/denchmark-link&gt;
 for 
		</comment>
		<comment id='8' author='Jordy-VL' date='2020-08-26T17:25:41Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='9' author='Jordy-VL' date='2020-09-02T18:22:17Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='10' author='Jordy-VL' date='2020-09-02T18:22:20Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42263&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42263&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>