<bug id='41116' author='amaiya' open_date='2020-07-06T02:57:41Z' closed_time='2020-07-13T21:09:20Z'>
	<summary>save_weights and load_weights do not work as expected when used to restore original initialized weights</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0
Python version: 3.6.9

Describe the current behavior
The save_weights and load_weights functions do not work as expected when used to save and restore the original, initialized weights of a model. For instance, in the following example, we create a model, save the original weights, corrupt the weights, and restore the original, uncorrupted weights.  The training results in a very high loss when the corrupt_weights_but_restore function is invoked, but a low loss when the corrupt_weights_but_restore is commented out.
import tempfile
from tensorflow.keras import backend as K
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.datasets import boston_housing
(x_train, y_train), (x_test, y_test) = boston_housing.load_data()
def corrupt_weights_but_restore(model, X, Y):

   # save original uncorrupted weights
    new_file, weightfile = tempfile.mkstemp()
    model.save_weights(weightfile)       

   # corrupt/damage weights with very high learning rate
    K.set_value(model.optimizer.lr, 100) 
    model.fit(X, Y, epochs=5)

   # load original uncorrupted weights
    model.load_weights(weightfile) 
    return model

# build model
model = Sequential()
model.add(Dense(1, input_shape=(x_train.shape[1],), activation='linear'))
model.compile(optimizer='adam', loss='mse', metrics=['mse', 'mae'])

# uncommenting the line below produces high loss despite restoring original weights
model = corrupt_weights_but_restore(model, x_train, y_train)

# train
K.set_value(model.optimizer.lr, 0.05)
model.fit(x_train, y_train,
          batch_size=32,
          epochs=100)
~                       
Describe the expected behavior
Loss should be the same (and be low) regardless of whether or not corrupt_weights_but_restore is run, since the original, uncorrupted weights are restored in corrupt_weights_but_restore.
	</description>
	<comments>
		<comment id='1' author='amaiya' date='2020-07-06T06:56:25Z'>
		&lt;denchmark-link:https://github.com/amaiya&gt;@amaiya&lt;/denchmark-link&gt;

I have tried in colab with TF version 2.2.Please, find the gist here. You are also seeing the same behavior?.Thanks!
		</comment>
		<comment id='2' author='amaiya' date='2020-07-06T10:29:08Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
 Sorry - where exactly is the gist?  I don't see a link i your post.
In TensorFlow 2.2.0, when the corrupt_weights_but_restore method is commented out, the training loss is around 30.  When it is uncommented and invoked, the loss is several hundred.  I've reproduced this across several different models and datasets.
		</comment>
		<comment id='3' author='amaiya' date='2020-07-06T11:10:14Z'>
		&lt;denchmark-link:https://github.com/amaiya&gt;@amaiya&lt;/denchmark-link&gt;

Sorry, my bad. Forgot to attach &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/5ec20121376d550db67784ae6aeca738/untitled84.ipynb&gt;gist&lt;/denchmark-link&gt;
.You are also seeing the same behavior?Thanks!
		</comment>
		<comment id='4' author='amaiya' date='2020-07-06T12:33:13Z'>
		Thanks - no worries.   Yes that's the behavior I'm seeing.  Now, if you do either the following, the problem disappears and the loss is much lower (between 30 and 40 versus the loss of 702 shown in your gist):

comment out the line that calls corrupt_weights_but_restore
replace save_weights and load_weights with model.save and load_model, respectively

This suggests that save_weights and load_weights are not doing the right thing, which is problematic.
See &lt;denchmark-link:https://colab.research.google.com/drive/19K5PxnxOxV00AdBQWLwB7Rc3-KWu_R0b?usp=sharing&gt;this Google Colab&lt;/denchmark-link&gt;
 that shows this.
		</comment>
		<comment id='5' author='amaiya' date='2020-07-07T06:47:10Z'>
		I have tried in colab with nightly version() and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/85dd12dbec8001a129e888d7b1392197/untitled87.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='6' author='amaiya' date='2020-07-13T20:15:23Z'>
		Just a note -- h5 doesn't restore optimizer weights, so if the loss after training is extremely high, that could mean that the adam optimizer weights are corrupted from calling model.fit() on the high learning rate. I would suggest using the TF format, or recompiling the model after loading the weights.
		</comment>
		<comment id='7' author='amaiya' date='2020-07-13T21:09:19Z'>
		&lt;denchmark-link:https://github.com/k-w-w&gt;@k-w-w&lt;/denchmark-link&gt;
 :  Thank you - it looks like you're exactly right about what's happening.  Thanks for the suggestions.  I am closing this issue
		</comment>
		<comment id='8' author='amaiya' date='2020-07-13T21:09:21Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41116&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41116&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>