<bug id='32785' author='TimCapes' open_date='2019-09-24T17:05:02Z' closed_time='2019-09-30T16:08:59Z'>
	<summary>Converting tf fashion mnist model with Supported Operations to TFLite breaks due to Operand Shape Mismatch</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.12.6
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A
TensorFlow installed from (source or binary): binary (pip)
TensorFlow version (use command below):2.0.0-rc1
Python version: 3.7.1
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
When converting model to tflite a supported operation is attempted to be fused and a dimension error occurs.
Describe the expected behavior
When converting model to tflite the supported operation fuses correctly.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
import tensorflow.compat.v1 as tf
import numpy as np
tf.disable_v2_behavior()
fashion_mnist = tf.keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
train_images = train_images/255.0
test_images = test_images/255.0
input = tf.placeholder(dtype=tf.float32, shape=(None,28,28), name="input")
reshape = tf.reshape(input, [tf.shape(input)[0],784])
w1 = tf.Variable(tf.random_normal([128,784], dtype=tf.float32), name="w1")
b1 = tf.Variable(tf.random_normal([128], dtype=tf.float32), name="b1")
layer_one_unbiased = tf.matmul(w1,tf.transpose(reshape))
print(layer_one_unbiased, b1)
layer_one_biased = tf.add(tf.transpose(layer_one_unbiased),b1)
print(layer_one_biased)
activated_layer_one = tf.nn.relu(layer_one_biased)
w2 = tf.Variable(tf.random_normal([10,128]), name="w2")
b2 = tf.Variable(tf.random_normal([10], name="b2"))
print(activated_layer_one)
layer_two_unbiased = tf.matmul(w2,tf.transpose(activated_layer_one))
print(tf.transpose(layer_two_unbiased), b2)
print(layer_two_unbiased, b2)
layer_two_biased = tf.add(tf.transpose(layer_two_unbiased), b2)
print(layer_two_biased)
predictions = tf.nn.softmax(layer_two_biased, name="final")
labels= tf.placeholder(dtype=tf.int32, shape=(None))
loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=layer_two_biased)
optimizer = tf.train.AdamOptimizer()
train = optimizer.minimize(loss)
label_acc = tf.one_hot(labels,10)
accuracy = 1 - tf.norm(tf.subtract(predictions,label_acc), axis=1)/2
accuracy_averaged = tf.math.reduce_mean(accuracy)
sess = tf.Session()
sess.run(tf.global_variables_initializer())
feed_dict = {input: train_images, labels: train_labels}
sess.run(train,feed_dict=feed_dict)
feed_dict2 = {input: test_images, labels: test_labels}
print("Test accuracy", sess.run(accuracy_averaged, feed_dict = feed_dict2))
print("Training accuracy",sess.run(accuracy_averaged, feed_dict = feed_dict))
saver =tf.train.Saver()
save_path = saver.save(sess, "model.ckpt")
tf.io.write_graph(sess.graph, "", 'train.pbtxt')
#converter = tf.lite.TFLiteConverter.from_session(sess, [input], [predictions])
#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
#tflite_model = converter.convert()
#open('converted_model.tflite', "wb").write(tflite_model)
sess.close()
&lt;denchmark-h:h1&gt;Above code is the code used to generate the model&lt;/denchmark-h&gt;

freeze_graph.py is then run on the output of the above code.
&lt;denchmark-h:h1&gt;The below code is used to convert&lt;/denchmark-h&gt;

import tensorflow.compat.v1 as tf
import numpy as np
tf.disable_v2_behavior()
graph_def_file = "freeze_graph.pbtxt"
input_arrays = ["input"]
output_arrays = ["final"]
converter = tf.lite.TFLiteConverter.from_frozen_graph(
graph_def_file, input_arrays, output_arrays)
tflite_model = converter.convert()
open("converted_model.tflite", "wb").write(tflite_model)
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
WARNING: Logging before flag parsing goes to stderr.
W0924 12:49:42.307250 140736348050368 deprecation.py:323] From /Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2019-09-24 12:49:42.308630: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-09-24 12:49:42.324658: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fb5c10b8740 executing computations on platform Host. Devices:
2019-09-24 12:49:42.324684: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-09-24 12:49:42.336278: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count &gt;= 8, compute capability &gt;= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2019-09-24 12:49:42.336378: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2019-09-24 12:49:42.344845: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize
2019-09-24 12:49:42.344864: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 27 nodes (-4), 27 edges (-4), time = 3.984ms.
2019-09-24 12:49:42.344871: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 27 nodes (0), 27 edges (0), time = 0.999ms.
Traceback (most recent call last):
File "conversion.py", line 11, in 
tflite_model = converter.convert()
File "/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py", line 983, in convert
**converter_kwargs)
File "/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py", line 449, in toco_convert_impl
enable_mlir_converter=enable_mlir_converter)
File "/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py", line 200, in toco_convert_protos
raise ConverterError("See console for info.\n%s\n%s\n" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2019-09-24 12:49:44.232035: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 14 operators, 27 arrays (0 quantized)
2019-09-24 12:49:44.232297: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 14 operators, 27 arrays (0 quantized)
2019-09-24 12:49:44.232628: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 13 operators, 26 arrays (0 quantized)
2019-09-24 12:49:44.232720: F tensorflow/lite/toco/graph_transformations/fuse_binary_into_preceding_affine.cc:62] Operand shape mismatch.
Fatal Python error: Aborted
Current thread 0x00007fffbc0853c0 (most recent call first):
File "/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py", line 52 in execute
File "/Users/t.capes/miniconda3/lib/python3.7/site-packages/absl/app.py", line 251 in _run_main
File "/Users/t.capes/miniconda3/lib/python3.7/site-packages/absl/app.py", line 300 in run
File "/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py", line 40 in run
File "/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py", line 89 in main
File "/Users/t.capes/miniconda3/bin/toco_from_protos", line 10 in 
	</description>
	<comments>
		<comment id='1' author='TimCapes' date='2019-09-24T17:05:56Z'>
		I am aware there is a bug in the model training which results in poor accuracy. I still want to be able to convert this model and it appears to use only supported operations once freeze_graph is used to replace tf.Variable with tf.constant.
		</comment>
		<comment id='2' author='TimCapes' date='2019-09-25T16:55:47Z'>
		It is worth noting that this model will successfully convert if I exclude the TFLITE_BUILTINS and use only SELECT_TF_OPS but according to the documentation this should work with TFLITE_BUILTINS as every op is compatible.  This also breaks if I use both TFLITE_BUILTINS and SELECT_TF_OPS in that order.
		</comment>
		<comment id='3' author='TimCapes' date='2019-09-26T23:12:43Z'>
		Hi, thanks for posting your run script and conversion  script. Could you attach the model you get from freeze_graph.py to this issue? I'm having trouble reproducing your exact error. Thanks!
		</comment>
		<comment id='4' author='TimCapes' date='2019-09-27T21:04:34Z'>
		Changed name from .pbtxt to .txt because of unsupported file format. Uploaded.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3664520/freeze_graph.txt&gt;freeze_graph.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='TimCapes' date='2019-09-28T00:27:38Z'>
		Hi I believe the problem is that your bias vectors were one dimension row vectors instead of true "column" vectors that have to be specified as two dimensional vectors of size (128, 1) for b1 and (10, 1) for b2. I made this change and re-ran your script, converted with freeze_graph, and then the model was able to successfully convert with TF Lite. Here is the code:
&lt;denchmark-link:https://gist.github.com/talumbau/042ee5b278b4f73229333f6baf427bbe&gt;https://gist.github.com/talumbau/042ee5b278b4f73229333f6baf427bbe&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='TimCapes' date='2019-09-30T16:08:59Z'>
		Looks good to me. Closing issue. Thanks.
		</comment>
		<comment id='7' author='TimCapes' date='2019-09-30T16:09:01Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=32785&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=32785&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>