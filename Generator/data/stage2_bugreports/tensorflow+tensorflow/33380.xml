<bug id='33380' author='fhausmann' open_date='2019-10-15T14:42:53Z' closed_time='2019-10-17T04:34:53Z'>
	<summary>get_config missing from AdditiveAttention</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): 2.0.0
Python version:
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

Describe the current behavior
Model containing AdditiveAttention cannot be saved due to missing get_config
Describe the expected behavior
AdditiveAttention has get_config and can be saved
Code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf
max_tokens=6
dimension = 2

# Variable-length int sequences.
query_input = tf.keras.Input(shape=(None,), dtype='int32')
value_input = tf.keras.Input(shape=(None,), dtype='int32')

# Embedding lookup.
token_embedding = tf.keras.layers.Embedding(max_tokens, dimension)
# Query embeddings of shape [batch_size, Tq, dimension].
query_embeddings = token_embedding(query_input)
# Value embeddings of shape [batch_size, Tv, dimension].
value_embeddings = token_embedding(query_input)

# CNN layer.
cnn_layer = tf.keras.layers.Conv1D(
    filters=100,
    kernel_size=4,
    # Use 'same' padding so outputs have the same shape as inputs.
    padding='same')
# Query encoding of shape [batch_size, Tq, filters].
query_seq_encoding = cnn_layer(query_embeddings)
# Value encoding of shape [batch_size, Tv, filters].
value_seq_encoding = cnn_layer(value_embeddings)

# Query-value attention of shape [batch_size, Tq, filters].
query_value_attention_seq = tf.keras.layers.AdditiveAttention()(
    [query_seq_encoding, value_seq_encoding])

# Reduce over the sequence axis to produce encodings of shape
# [batch_size, filters].
query_encoding = tf.keras.layers.GlobalAveragePooling1D()(
    query_seq_encoding)
query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(
    query_value_attention_seq)

# Concatenate query and document encodings to produce a DNN input layer.
input_layer = tf.keras.layers.Concatenate()(
    [query_encoding, query_value_attention])


model = tf.keras.Model(inputs=(query_input,value_input),outputs=input_layer)
model.save('test.h5')
#NotImplementedError: Layers with arguments in `__init__` must override `get_config`.
&lt;/denchmark-code&gt;


Code based on &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention&gt;https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='fhausmann' date='2019-10-16T08:51:09Z'>
		I could reproduce the issue with tf 2.0.0. Please take a look at colab &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/6ab8b86252382e26b668b41815b19305/untitled197.ipynb&gt;gist&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='fhausmann' date='2019-10-16T23:26:44Z'>
		I'm having a similar issue with tf.keras.layers.Attention with TensorFlow 2.0.0:
import tensorflow as tf

attention = tf.keras.layers.Attention()
attention.get_config()
Error:
&lt;denchmark-code&gt;NotImplementedError: Layers with arguments in `__init__` must override `get_config`.
&lt;/denchmark-code&gt;

I didn't want to start a new issue, but this is also preventing me from saving models with the HDF5 format.
My temporary workaround is something like the following:
import tensorflow as tf

attention_kwargs = dict(use_scale=True, name='attention')
attention = tf.keras.layers.Attention(**attention_kwargs)
attention.get_config = lambda: attention_kwargs  # manually make a get_config()

# Build and save the model
query = tf.keras.layers.Input((None, 10))
value = tf.keras.layers.Input((None, 10))
outputs = attention([query, value])
model = tf.keras.Model(inputs=[query, value], outputs=outputs)
model.save('model.h5', save_format='h5')

# Restore the model
restored_model = tf.keras.models.load_model(
    filepath='model.h5',
    custom_objects={'Attention': tf.keras.layers.Attention},
)
Note that if I omit custom_objects when loading the model, I get the error
&lt;denchmark-code&gt;ValueError: Unknown layer: Attention
&lt;/denchmark-code&gt;

Related: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/32662&gt;#32662&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='fhausmann' date='2019-10-17T04:34:55Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33380&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33380&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>