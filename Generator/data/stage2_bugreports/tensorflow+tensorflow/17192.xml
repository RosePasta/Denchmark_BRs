<bug id='17192' author='keotic' open_date='2018-02-22T09:25:46Z' closed_time='2018-07-03T00:55:37Z'>
	<summary>TrainingHelper &amp; ScheduledEmbeddingTrainingHelper  GPU Error - works on CPU</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


**Have I written custom code **: Yes
OS Platform and Distribution:16.04.3 LTS (Xenial Xerus)
TensorFlow installed from (source or binary):source
TensorFlow version (use command below):1.4.1
Python version: Python 3.5.2
Bazel version (if compiling from source):NA
GCC/Compiler version (if compiling from source):c++ (Ubuntu 5.4.0-6ubuntu1~16.04.6) 5.4.0 20160609
CUDA/cuDNN version:CUDA 8.0 / 6.0
GPU model and memory:GeForce GTX 1080, 8G RAM, NVIDIA DRIVER: 390.25
Exact command to reproduce:NA

&lt;denchmark-h:h3&gt;problem&lt;/denchmark-h&gt;

Having trouble running an RNN dynamic decoder on GPU when using the following  decoding helpers:
tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper
tf.contrib.seq2seq.TrainingHelper
It seems that there is a problem with the sequence length stopping condition, as the problem
is not present when using  tf.contrib.seq2seq.GreedyEmbeddingHelper as a decoder helper.
On CPU all functions are working correctly, the problem arise on GPUs only.
Tried on TF-1.4 compiled from source, TF-1.4 installed from pip, and TF-1.5 installed from source.
They all fail
&lt;denchmark-h:h3&gt;Error log:&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;NotFoundError (see above for traceback): No registered 'Switch' OpKernel for GPU devices compatible with node Decoders/StackedDecoder_text_rnn_0/decoder/TrainingHelperInitialize/cond/TensorArrayReadV3/Switch = Switch[T=DT_RESOURCE, _class=["loc:@Decoders/StackedDecoder_text_rnn_0/ScheduledEmbeddingSamplingWrapper/TrainingHelper/TensorArray"], _device="/job:localhost/replica:0/task:0/device:GPU:0"](Decoders/StackedDecoder_text_rnn_0/ScheduledEmbeddingSamplingWrapper/TrainingHelper/TensorArray/_281, Decoders/StackedDecoder_text_rnn_0/decoder/TrainingHelperInitialize/All)
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; T in [DT_STRING]
  device='GPU'; T in [DT_BOOL]
  device='GPU'; T in [DT_INT32]
  device='GPU'; T in [DT_COMPLEX128]
  device='GPU'; T in [DT_COMPLEX64]
  device='GPU'; T in [DT_INT8]
  device='GPU'; T in [DT_UINT8]
  device='GPU'; T in [DT_INT16]
  device='GPU'; T in [DT_UINT16]
  device='GPU'; T in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_HALF]
  device='CPU'; T in [DT_QINT32]
  device='CPU'; T in [DT_QUINT8]
  device='CPU'; T in [DT_QINT8]
  device='CPU'; T in [DT_RESOURCE]
  device='CPU'; T in [DT_STRING]
  device='CPU'; T in [DT_BOOL]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_INT8]
  device='CPU'; T in [DT_UINT8]
  device='CPU'; T in [DT_INT16]
  device='CPU'; T in [DT_UINT16]
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_INT64]
	 [[Node: Decoders/StackedDecoder_text_rnn_0/decoder/TrainingHelperInitialize/cond/TensorArrayReadV3/Switch = Switch[T=DT_RESOURCE, _class=["loc:@Decoders/StackedDecoder_text_rnn_0/ScheduledEmbeddingSamplingWrapper/TrainingHelper/TensorArray"], _device="/job:localhost/replica:0/task:0/device:GPU:0"](Decoders/StackedDecoder_text_rnn_0/ScheduledEmbeddingSamplingWrapper/TrainingHelper/TensorArray/_281, Decoders/StackedDecoder_text_rnn_0/decoder/TrainingHelperInitialize/All)]]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='keotic' date='2018-02-22T19:34:17Z'>
		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
Have I written custom code
OS Platform and Distribution
TensorFlow installed from
TensorFlow version
Bazel version
Exact command to reproduce
		</comment>
		<comment id='2' author='keotic' date='2018-03-09T13:34:46Z'>
		Nagging Awaiting Response: It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='3' author='keotic' date='2018-03-10T08:47:51Z'>
		Any resolution to this issue?
		</comment>
		<comment id='4' author='keotic' date='2018-04-03T18:02:39Z'>
		&lt;denchmark-link:https://github.com/keotic&gt;@keotic&lt;/denchmark-link&gt;
 Can you provide some code to help replicate this?
&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 Can you comment on this?
		</comment>
		<comment id='5' author='keotic' date='2018-04-19T18:59:37Z'>
		It has been 15 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='6' author='keotic' date='2018-04-19T23:33:13Z'>
		Sounds like we need a host-only switch for GPU.
Have you tried running with soft placement on?
		</comment>
		<comment id='7' author='keotic' date='2018-04-20T16:57:21Z'>
		Yes soft placement is on.
		</comment>
		<comment id='8' author='keotic' date='2018-04-24T23:38:26Z'>
		&lt;denchmark-link:https://github.com/keotic&gt;@keotic&lt;/denchmark-link&gt;
 are you able to provide a script to reproduce this error?
		</comment>
		<comment id='9' author='keotic' date='2018-05-10T01:00:50Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/skye&gt;@skye&lt;/denchmark-link&gt;
: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='10' author='keotic' date='2018-06-02T07:19:57Z'>
		It has been 16 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='11' author='keotic' date='2018-06-17T18:35:37Z'>
		It has been 31 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='12' author='keotic' date='2018-07-03T00:58:10Z'>
		We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!
		</comment>
	</comments>
</bug>