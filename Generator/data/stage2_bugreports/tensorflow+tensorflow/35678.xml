<bug id='35678' author='AlexVaith' open_date='2020-01-08T15:41:15Z' closed_time='2020-01-23T23:21:02Z'>
	<summary>TensorFlowLite_LSTM_Keras_Tutorial.ipynb update</summary>
	<description>
Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.
The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: &lt;denchmark-link:https://www.tensorflow.org/community/contribute/docs&gt;https://www.tensorflow.org/community/contribute/docs&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;URL(s) with the issue:&lt;/denchmark-h&gt;

Please provide a link to the documentation entry, for example:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/examples/lstm/TensorFlowLite_LSTM_Keras_Tutorial.ipynb&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/examples/lstm/TensorFlowLite_LSTM_Keras_Tutorial.ipynb&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Description of issue (what needs changing):&lt;/denchmark-h&gt;

The example script on how to make lstm layers ready for tf lite is outdated and not working anymore, because the requested tf-nightly package causes issues.
I would like to get an updated tutorial or a better alternative. To use the TFLite converter with the experimental_flag set to True works with lstm layers, but does not allow post training quantization. As a general question I would like to know, if this would be possible with the the model that is build in the example script?
	</description>
	<comments>
		<comment id='1' author='AlexVaith' date='2020-01-09T06:16:29Z'>
		&lt;denchmark-link:https://github.com/AlexVaith&gt;@AlexVaith&lt;/denchmark-link&gt;
 Are you looking for the tutorial with the new converter? &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/keras_lstm.ipynb&gt;Here&lt;/denchmark-link&gt;
 is the tutorial with the new converter. Couple of days back there was an update, so if you plan to use , then you don't need to specify  which is default in the latest . Thanks!
		</comment>
		<comment id='2' author='AlexVaith' date='2020-01-09T09:22:22Z'>
		
@AlexVaith Are you looking for the tutorial with the new converter? Here is the tutorial with the new converter. Couple of days back there was an update, so if you plan to use tf-nightly, then you don't need to specify converter.experimental_new_converter = True which is default in the latest tf-nightly. Thanks!

Thanks for the quick reply  &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 . I did not know that this tutorial existed, but I found the same solution on stackoverflow. I just thpught that the official documentation should be up to date, which it is apparently.
However, I thought that with the old implementation via the TFLiteLSTMCell and tf.lite.experimental.nn.dynamic_rnn, post training quantization could be possible, which is not with the converter.experimental_new_converter = True. Is this something that is added to tf lite in the future or are lstm or rnn structures in general not compatible with the requirements needed for post training quantization?
		</comment>
		<comment id='3' author='AlexVaith' date='2020-01-09T18:55:51Z'>
		&lt;denchmark-link:https://github.com/AlexVaith&gt;@AlexVaith&lt;/denchmark-link&gt;
 Can you please check this page on &lt;denchmark-link:https://www.tensorflow.org/lite/performance/post_training_quantization&gt;Post-training quantization&lt;/denchmark-link&gt;
 and also use the experimental flag  and let us know what you think? Thanks!
		</comment>
		<comment id='4' author='AlexVaith' date='2020-01-10T08:34:28Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 I tried this link with an lstm model looking like the following:
&lt;denchmark-code&gt;def build_model_tf(lr, window_size, features, classes):
    # input layer
    inp = tf.keras.layers.Input(shape=[window_size, features], batch_size=42)
    # batch norm layer to normalize data
    x = tf.keras.layers.BatchNormalization()(inp)

    # two convolutional layers
    x = tf.keras.layers.Conv1D(filters=16, kernel_size=3)(x)
    x = tf.keras.layers.Activation('relu')(x)
    x = tf.keras.layers.BatchNormalization()(x)

    x = tf.keras.layers.Conv1D(filters=32, kernel_size=6)(x)
    x = tf.keras.layers.Activation('relu')(x)
    x = tf.keras.layers.BatchNormalization()(x)

    # 3 lstm layers
    x = tf.keras.layers.LSTM(32, return_sequences=True, recurrent_regularizer=tf.keras.regularizers.L1L2(0.5, 0.1))(x)
    x = tf.keras.layers.LSTM(24, return_sequences=True, recurrent_regularizer=tf.keras.regularizers.L1L2(0.5, 0.1))(x)
    x = tf.keras.layers.LSTM(16, return_sequences=False, recurrent_regularizer=tf.keras.regularizers.L1L2(0.5, 0.1))(x)

    # dense layer
    x = tf.keras.layers.Dense(8)(x)
    x = tf.keras.layers.Activation('relu')(x)
    x = tf.keras.layers.Dropout(0.35)(x)
    #
    # output layer
    out = tf.keras.layers.Dense(classes, activation='softmax')(x)

    # create functional model object
    model = tf.keras.models.Model(inputs=inp, outputs=out)

    model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr, decay=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])

    return model
&lt;/denchmark-code&gt;

it was than converted like this:
&lt;denchmark-code&gt;# convert keras model to concrete functions
tf.saved_model.save(tf_keras_model, join(ROOT, 'trained_models', model_name + '_conv'))
model = tf.saved_model.load(join(ROOT, 'trained_models', model_name + '_conv'))
concrete_func = model.signatures[
  tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]
# initialize the tf_light converter
converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
# add experimental flag, because rnn structures are not covered by the default converter
converter.experimental_new_converter = True
# convert model to tf_light
tf_lite_model = converter.convert()
&lt;/denchmark-code&gt;

Without the following line it works:
&lt;denchmark-code&gt;converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
&lt;/denchmark-code&gt;

But with this line the same issue like in the following post occurs:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/35194&gt;#35194&lt;/denchmark-link&gt;

I am using tensorflow 2.0.0 and tf nightly.
		</comment>
		<comment id='5' author='AlexVaith' date='2020-01-10T22:56:52Z'>
		&lt;denchmark-link:https://github.com/AlexVaith&gt;@AlexVaith&lt;/denchmark-link&gt;
 Can you please create a standalone code (a colab or jupyter notebook) and share it. That will be fastest way to resolve issues. Thanks!
		</comment>
		<comment id='6' author='AlexVaith' date='2020-01-13T23:42:50Z'>
		&lt;denchmark-link:https://github.com/AlexVaith&gt;@AlexVaith&lt;/denchmark-link&gt;
 The line in your code , will only trigger weight-only (hybrid) quantization (hybrid quantization). TFLite doesn't have hybrid LSTM kernels. Please switch to  and specify the sample data for a fully quantization. Thanks!
		</comment>
		<comment id='7' author='AlexVaith' date='2020-01-23T23:21:02Z'>
		I am closing the issue as it was resolved. Please feel free to open it if the issue persists again. Thanks!
		</comment>
	</comments>
</bug>