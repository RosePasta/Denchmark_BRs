<bug id='40717' author='nikhil1008' open_date='2020-06-23T13:58:24Z' closed_time='2020-07-16T17:34:42Z'>
	<summary>Writing custom py_function inside custom layer , gradient backpropagation</summary>
	<description>
Hi ,
I'm writing a custom py_function inside a custom layer. But I get error during gradient backpropagation. I'm using tensorflow 1.1.4 with eager execution enabled. While debugging I reduced the complexity of code, and now 'm just trying 1-d DCT of input. I still get the error.
Here is my code :
from scipy.special import expit
from scipy.fftpack import dct, idct
import time
def fdct (a):
return dct(a.T, norm='ortho').T
def fdct_top (inp):
return tf.convert_to_tensor(np.array([fdct(in_) for in_ in list(inp.numpy())]))
def fidct (a):
return idct(a.T, norm='ortho').T
def fidct_top (inp):
return tf.convert_to_tensor(np.array([fidct(in_) for in_ in list(inp.numpy())]))
class MyDenseLayer(tf.keras.layers.Layer):
def init(self, y_mat):
super(MyDenseLayer, self).init()
self.w_mat = y_mat
def build(self, input_shape):
self.w = tf.Variable (initial_value = self.w_mat, trainable = True)
def call(self, input):
dct_i = tf.py_function (fdct_top, (input,), 'float32')
dct_i.set_shape(tf.TensorShape((None, 10)))
&lt;denchmark-code&gt;dcts_q = tf.multiply(dct_i, 1/self.w)    
print(dcts_q)
z_floored_NOT_differentiable = tf.floor(dcts_q)
z_floored_differentiable = (dcts_q - (tf.stop_gradient(dcts_q) - z_floored_NOT_differentiable))
print(z_floored_differentiable)
dcts_deg = tf.multiply(z_floored_differentiable, tf.cast(self.w, 'float32'))

dcts_deg = tf.py_function (fidct_top, (dcts_deg,), 'float32')
dcts_deg.set_shape(tf.TensorShape((None, 10)))

return dcts_deg
&lt;/denchmark-code&gt;

img_inputs1 = keras.Input(shape=(10,))
q_mat = 5*np.ones((10)).astype(np.float32)
mul_layer = MyDenseLayer (q_mat)
outputs = mul_layer (img_inputs1)
model = keras.Model(inputs=[img_inputs1], outputs=outputs)
model.summary()
x = 10*np.random.rand(100,10).astype(np.float32)
y = x
optimizer = tf.keras.optimizers.RMSprop(0.1)
model.compile(loss=tf.keras.losses.MSE,
optimizer=optimizer,
)
history = model.fit(x, y, epochs = 50 )
InvalidArgumentError: Input to reshape is a tensor with 10 values, but the requested shape has 320
[[{{node training_1/RMSprop/gradients/Mul_3_grad/Reshape}}]] [Op:StatefulPartitionedCall]
	</description>
	<comments>
		<comment id='1' author='nikhil1008' date='2020-06-23T15:01:53Z'>
		&lt;denchmark-link:https://github.com/nikhil1008&gt;@nikhil1008&lt;/denchmark-link&gt;

I ran your code on tf 1.14 and 2.2 and do not face any error as reported, please find the &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/a80dfe73ebd37b6c1cf63e7ae7f43164/untitled238.ipynb&gt;gist here&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='nikhil1008' date='2020-06-23T15:16:21Z'>
		Hi,
I think the Indentation in your code was not same as mine , I corrected it in your notebook.
Here is my full code :
&lt;denchmark-link:https://colab.research.google.com/drive/1ZEc9cGJ6uNT6ZBxwxIX08J4-RR9_rEzU#scrollTo=lMYfv3ChvZRE&gt;https://colab.research.google.com/drive/1ZEc9cGJ6uNT6ZBxwxIX08J4-RR9_rEzU#scrollTo=lMYfv3ChvZRE&lt;/denchmark-link&gt;

pasting here as well :
import tensorflow as tf
import numpy as np
from tensorflow import keras
from tensorflow.keras import layers
from PIL import Image
CUDA_VISIBLE_DEVICES=0
tf.compat.v1.enable_eager_execution()
tf.compat.v1.enable_v2_behavior
from scipy.special import expit
from scipy.fftpack import dct, idct
import time
def fdct (a):
return dct(a.T, norm='ortho').T
def fdct_top (inp):
return tf.convert_to_tensor(np.array([fdct(in_) for in_ in list(inp.numpy())]))
def fidct (a):
return idct(a.T, norm='ortho').T
def fidct_top (inp):
return tf.convert_to_tensor(np.array([fidct(in_) for in_ in list(inp.numpy())]))
class MyDenseLayer(tf.keras.layers.Layer):
def init(self, y_mat):
super(MyDenseLayer, self).init()
self.w_mat = y_mat
def build(self, input_shape):
self.w = tf.Variable (initial_value = self.w_mat, trainable = True)
def call(self, input):
dct_i = tf.py_function (fdct_top, (input,), 'float32')
dct_i.set_shape(tf.TensorShape((None, 10)))
&lt;denchmark-code&gt;dcts_q = tf.multiply(dct_i, 1/self.w)    
print(dcts_q)
z_floored_NOT_differentiable = tf.floor(dcts_q)
z_floored_differentiable = (dcts_q - (tf.stop_gradient(dcts_q) - z_floored_NOT_differentiable))
print(z_floored_differentiable)

dcts_deg = tf.multiply(z_floored_differentiable, tf.cast(self.w, 'float32'))

dcts_deg = tf.py_function (fidct_top, (dcts_deg,), 'float32')
dcts_deg.set_shape(tf.TensorShape((None, 10)))


return dcts_deg
&lt;/denchmark-code&gt;

img_inputs1 = keras.Input(shape=(10,))
q_mat = 5*np.ones((10)).astype(np.float32)
mul_layer = MyDenseLayer (q_mat)
outputs = mul_layer (img_inputs1)
model = keras.Model(inputs=[img_inputs1], outputs=outputs)
model.summary()
x = 10*np.random.rand(100,10).astype(np.float32)
y = x
optimizer = tf.keras.optimizers.RMSprop(0.1)
model.compile(loss=tf.keras.losses.MSE,
optimizer=optimizer,
)
history = model.fit(x, y, epochs = 50, batch_size  = 1 )
		</comment>
		<comment id='3' author='nikhil1008' date='2020-06-23T15:35:33Z'>
		&lt;denchmark-link:https://github.com/nikhil1008&gt;@nikhil1008&lt;/denchmark-link&gt;

I had to correct the indentation in your code else i am facing indentation error, please find the &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/68a3926dc13fcd6f4f29daeedcc24117/untitled238.ipynb&gt;gist here&lt;/denchmark-link&gt;
.
Please share a  colab gist with your code where you face an error. the above link is not accessible.
Is there any particular reason for using older version on tf when later versions are available, you could try using later versions and let us know if that helps.
		</comment>
		<comment id='4' author='nikhil1008' date='2020-06-23T15:58:46Z'>
		[&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
]
I edited your notebook, Please take a look. The issue is reproducible now.
&lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/68a3926dc13fcd6f4f29daeedcc24117/untitled238.ipynb#scrollTo=bR0mK9eG0CHF&gt;https://colab.research.google.com/gist/Saduf2019/68a3926dc13fcd6f4f29daeedcc24117/untitled238.ipynb#scrollTo=bR0mK9eG0CHF&lt;/denchmark-link&gt;

I have dependency over cuda version , therefore I need to use TF=1.14. Btw, I tried with 2.2.0 as well , I get same error.
&lt;denchmark-link:https://user-images.githubusercontent.com/66033489/85427535-ba7c7e00-b530-11ea-8a62-97c654b83d31.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='nikhil1008' date='2020-06-23T16:14:06Z'>
		Please use following link in case you can't see changes I made to your notebook.
&lt;denchmark-link:https://colab.research.google.com/drive/1s9QPYIclIMcrcFPkrhcOhOHemB-LSuKl?usp=sharing&gt;https://colab.research.google.com/drive/1s9QPYIclIMcrcFPkrhcOhOHemB-LSuKl?usp=sharing&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='nikhil1008' date='2020-07-16T17:34:42Z'>
		There is a lot that has changed between 1.14 and 2.x, especially for eager execution. Can you retry this with a pure 2.x install, and open a new issue if you run into trouble?
		</comment>
		<comment id='7' author='nikhil1008' date='2020-07-16T17:34:47Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40717&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40717&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>