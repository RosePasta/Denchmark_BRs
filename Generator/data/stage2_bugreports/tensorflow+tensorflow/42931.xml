<bug id='42931' author='roussel-ryan' open_date='2020-09-03T17:09:22Z' closed_time='2020-12-11T19:43:53Z'>
	<summary>Multiprocessing TypeError: can't pickle _thread.RLock objects, using tf.keras.model</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Enterprise, cygwin64 2.905
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): 2.1.0
Python version: 3.7.6
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: V10.1.105
GPU model and memory: NVIDIA GeForce RTX 2070 Super 4.0 GB

Describe the current behavior
When I run the standalone code (see below) that loads a tf.keras.model from a .h5 file, see attached and tries to use the model in a python multiprocessing pool I receive the following error
&lt;denchmark-code&gt;  File "simple.py", line 18, in &lt;module&gt;
    pool.starmap(load_model,[[m,x],[m,x],[m,x]])
  File "C:\ProgramData\Anaconda3\lib\multiprocessing\pool.py", line 276, in starmap
    return self._map_async(func, iterable, starmapstar, chunksize).get()
  File "C:\ProgramData\Anaconda3\lib\multiprocessing\pool.py", line 657, in get
    raise self._value
  File "C:\ProgramData\Anaconda3\lib\multiprocessing\pool.py", line 431, in _handle_tasks
    put(task)
  File "C:\ProgramData\Anaconda3\lib\multiprocessing\connection.py", line 206, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\ProgramData\Anaconda3\lib\multiprocessing\reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
TypeError: can't pickle _thread.RLock objects
&lt;/denchmark-code&gt;

Describe the expected behavior
I expect the code to provide the output of model.predict() using the loaded model using 2 CPUs
Standalone code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf
import multiprocessing
import numpy as np

def load_model(model,pt):
    return model.predict(pt)

if __name__=='__main__':
    x = np.zeros(6)
    m = tf.keras.models.load_model('test.h5')

    with multiprocessing.Pool(2) as pool:
        pool.starmap(load_model,[[m,x],[m,x],[m,x]])
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/5170124/test.zip&gt;test.zip&lt;/denchmark-link&gt;

Note: This happens if only the CPU is used as well by adding
&lt;denchmark-code&gt;import os
os.environ["CUDA_VISIBLE_DEVICES"] = '-1'
&lt;/denchmark-code&gt;

There are a number of github posts referencing this error, but I have not found any that came to a helpful conclusion.
	</description>
	<comments>
		<comment id='1' author='roussel-ryan' date='2020-09-04T11:48:43Z'>
		I have tried in colab with TF version 2.3, nightly version() and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/08ad44fe425558fc8f2ac965bbb0cc3e/untitled309.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='roussel-ryan' date='2020-10-09T16:00:22Z'>
		Hi, has there been any progress on this?
		</comment>
		<comment id='3' author='roussel-ryan' date='2020-12-05T14:43:58Z'>
		I am also facing the same issue @scawsome
I did a workaround for this. The workaround is - Instead of passing the model in pool.starmap, I passed the model path, and it worked.
test_def.py
&lt;denchmark-code&gt;import tensorflow as tf

def load_model(model_path,pt):    
    model=tf.keras.models.load_model(model_path)    
    return model.predict(pt)
&lt;/denchmark-code&gt;

Tensorflow_Multiprocessing.py
&lt;denchmark-code&gt;
from test_def import load_model
import multiprocessing
import numpy as np

if __name__=='__main__':  
    x = np.zeros(6)
    m='test.h5'
    with multiprocessing.Pool(2) as pool:
        pool.starmap(load_model,[[m,x],[m,x],[m,x]])
&lt;/denchmark-code&gt;

Hope this helps!
		</comment>
		<comment id='4' author='roussel-ryan' date='2020-12-07T21:09:46Z'>
		Very helpful, thanks!
		</comment>
		<comment id='5' author='roussel-ryan' date='2020-12-11T19:43:53Z'>
		Closing the issue as it has been resolved. Thanks!
		</comment>
		<comment id='6' author='roussel-ryan' date='2020-12-11T19:43:55Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42931&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42931&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='roussel-ryan' date='2021-01-11T12:47:02Z'>
		I am facing the same issue. However, I want to fetch the model from a cloud bucket. The fetching takes some time. Therefore, I would prefer to do it only once. Do I understand it correctly that if I want to use this solution, I would need to save the model locally and then load the locally stored model for every process?
		</comment>
		<comment id='8' author='roussel-ryan' date='2021-01-12T04:30:32Z'>
		
I am facing the same issue. However, I want to fetch the model from a cloud bucket. The fetching takes some time. Therefore, I would prefer to do it only once. Do I understand it correctly that if I want to use this solution, I would need to save the model locally and then load the locally stored model for every process?

Yes, your understanding is correct.
		</comment>
	</comments>
</bug>