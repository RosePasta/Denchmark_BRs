<bug id='43931' author='MarkoSagadin' open_date='2020-10-10T19:10:33Z' closed_time='2020-10-11T19:13:18Z'>
	<summary>Slow inference with libopencm3 when compared to Mbed</summary>
	<description>
@tensorflow/micro
System information

Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.10
TensorFlow installed from: source
Tensorflow version: 2.3.0, commit b36436b
Target platform: Building for Nucleo-F767ZI dev board, comparing performance between Mbed and libopencm3

Describe the problem
Hello,
I have been having this problem for a while now, but only recently made enough progress so I can ask for help.
I'm trying to make TensorFlow Lite for Microcontrollers (TFMicro) library to work with libopencm3, which is a open-source firmware library for various ARM Cortex-M microcontrollers.
When this is done it should be quick to run TensorFlow on any micro that libopencm3 supports.
Right now I'm extensively testing this on Nucleo-F767ZI dev board which has STM32F767ZI micro, with 2MB of flash and 1M of SRAM, 216 MHz.
TFMicro port is working as it should, I tested it with several different models, everthing compiles and I also get same outputs as compared to TFLite python interpreter.
However, on device inference is very slow. It takes about 1465 ms for one inference, with -O3 flag
I managed to get the same setup working with a generated Mbed project. With -O3 flag I get inference time of 486 ms, almost a second faster.
Mbed has a option to export a makefile of the project, that you usually need to compile with mbed command line tool.
I exported the makefile and with some small changes the project compiled and inference time was again 486 ms.
I hoped that with a exported mbed makefile and my makefile I could compare the differences and come down to the core fo the problem, but so far I have not managed to so.
So far I can tell that the problem is not in the tensorflow code or in the model, both setups use the same things. I also copied exported mbed makefile into my project and started changing out pieces. I managed get to a setup where I am using all compile flags from mbed makefile and a libopencm archive file, linker file and startup file, but inference is stilll around 1465 ms.
Both setups set the clock frequency of a micro to 216MHz, I am timing inference in exactly the same way, with a DWT counter which increments for every clock cycle, to get to milliseconds I do a bit of calculation that is the same in both cases.
Both setups also use cmsis-nn kernel implementations, i had to make manually sure of that.
For now I can not tell what exactly is the problem, but the things that are different are:

linker script file
linker flags
startup routine

I can see that libopencm needs linker with -nostartfiles, -specs=nano.specs, -specs=nosys.specs, while mbed makefile just passes many -Wl,--wrap flags. Judging by the look of it it uses crt0.s for some startup work, but this is way over my head.
What can make microcontroller run slower, even though the clock is the same in both examples? Can a incorrect linker script slow down a micro, or are there some settings that make loading a model from flash slow?
Any help or a suggested path how to continue wiht this problem would be extremly appreciated.
Please provide the exact sequence of commands/steps when you ran into the problem
Both setups are available on my github.

&lt;denchmark-link:https://github.com/MarkoSagadin/tensorflow_mbed_test&gt;https://github.com/MarkoSagadin/tensorflow_mbed_test&lt;/denchmark-link&gt;

To load the dependencies this code requires, run:


To compile this for any platform supported by mbed:
mbed compile -m auto -t GCC_ARM -f --profile my_profile.json -f
To compile this for Nucleo_F767ZI dev board you can run makefile:
make flash -j4
or with mbed:
mbed compile -m NUCLEO_F767ZI -t GCC_ARM -f --profile my_profile.json -f
Inspect serial output with minicom -b 9600
Location of main file:
&lt;denchmark-link:https://github.com/MarkoSagadin/tensorflow_mbed_test/tree/master/tensorflow/lite/micro/examples/hello_world&gt;https://github.com/MarkoSagadin/tensorflow_mbed_test/tree/master/tensorflow/lite/micro/examples/hello_world&lt;/denchmark-link&gt;

Linker file that is used can be found here:
&lt;denchmark-link:https://github.com/ARMmbed/mbed-os/blob/master/targets/TARGET_STM/TARGET_STM32F7/TARGET_STM32F767xI/TOOLCHAIN_GCC_ARM/STM32F767xI.ld&gt;https://github.com/ARMmbed/mbed-os/blob/master/targets/TARGET_STM/TARGET_STM32F7/TARGET_STM32F767xI/TOOLCHAIN_GCC_ARM/STM32F767xI.ld&lt;/denchmark-link&gt;

Startup file that is used can be found here:
&lt;denchmark-link:https://github.com/ARMmbed/mbed-os/blob/master/targets/TARGET_STM/TARGET_STM32F7/TARGET_STM32F767xI/TOOLCHAIN_GCC_ARM/startup_stm32f767xx.S&gt;https://github.com/ARMmbed/mbed-os/blob/master/targets/TARGET_STM/TARGET_STM32F7/TARGET_STM32F767xI/TOOLCHAIN_GCC_ARM/startup_stm32f767xx.S&lt;/denchmark-link&gt;


&lt;denchmark-link:https://github.com/MarkoSagadin/MicroML/tree/mbed_makefile&gt;https://github.com/MarkoSagadin/MicroML/tree/mbed_makefile&lt;/denchmark-link&gt;

The setup for my project takes more time and more commands to setup, as im using tensorflow and libopencm as submodules. Please note that im using branch mbed_makefile to showcase my problem. To get everthing setup you should just copy commands below.
&lt;denchmark-code&gt;git clone --recurse-submodules https://github.com/SkobecSlo/MicroML.git
git checkout mbed_makefile
cd tensorflow
sudo make -f tensorflow/lite/micro/tools/make/Makefile hello_world
cd ..
make -C libopencm3
mbed config root .
mbed deploy
&lt;/denchmark-code&gt;

To compile and flash to Nucelo board:
make flash -j4
Inspect serial output with minicom -b 115200
Location of main file and linker script that is used:
&lt;denchmark-link:https://github.com/MarkoSagadin/MicroML/tree/mbed_makefile/projects/mbed_test&gt;https://github.com/MarkoSagadin/MicroML/tree/mbed_makefile/projects/mbed_test&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='MarkoSagadin' date='2020-10-11T19:13:17Z'>
		I managed to find the issue: mbed-os by default, before program enter main configures few important settings:

It enables I-chache
It enabled D-chache
It enables Flash ART accelator
It enables Flash prefetch

Although libopencm3 does not support these functions,or even registers, it was trivial to copy and implement them in my project.
With this settings combined my inference dropped down to 357 ms, which makes me extremly happy. I hope that this issue might help someone in the future.
		</comment>
	</comments>
</bug>