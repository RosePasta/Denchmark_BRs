<bug id='37968' author='lamba92' open_date='2020-03-27T11:00:38Z' closed_time='2020-05-19T16:19:19Z'>
	<summary>tf.keras.models.Model.fit strange behaviour after upgrading from 2.1 to 2.2-rc1</summary>
	<description>
I was experimenting with tf.keras.applications.inception_v3.InceptionV3 for classifying skin cancer lesions. It was going smooth since when Colaboratory decided to upgrade its VM's TF version from 2.1 to 2.2-rc1.
Now when loading the model from disk it says:
&lt;denchmark-code&gt;WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.
&lt;/denchmark-code&gt;

But most importantly the Model.fit is not able anymore to properly process a keras.utils.Sequence! Indeed during training the steps per epoch are no more inferred from the Sequence object showing 1/Unknown, also it does not actually terminate the epoch!
Snippet to reproduce the issue:
Colab code: &lt;denchmark-link:https://colab.research.google.com/drive/1wdlWES83ibvLCwzpHrhJsQBNep-Aycqj&gt;https://colab.research.google.com/drive/1wdlWES83ibvLCwzpHrhJsQBNep-Aycqj&lt;/denchmark-link&gt;

HAM1000 dataset: &lt;denchmark-link:https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000&gt;https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000&lt;/denchmark-link&gt;

ISIC dataset: &lt;denchmark-link:https://www.isic-archive.com/#!/topWithHeader/wideContentTop/main&gt;https://www.isic-archive.com/#!/topWithHeader/wideContentTop/main&lt;/denchmark-link&gt;

The code stopped working overnight after Colab updated their VM images.
In the code the HAM dataset is automatically downloaded and rearranged provided you have  file with the API Token. The ISIC dataset needs to be downlaoded manually from their official website or from my GDrive &lt;denchmark-link:https://drive.google.com/drive/folders/1E_xIwuA3HkovP4F_cDDzD1K-nEl0TNUC?usp=sharing&gt;here&lt;/denchmark-link&gt;

NB: Also even if I create a new network and start training it with a Sequence the steps per epochs are not inferred as well.
	</description>
	<comments>
		<comment id='1' author='lamba92' date='2020-03-28T15:33:01Z'>
		I have same issue
		</comment>
		<comment id='2' author='lamba92' date='2020-03-28T22:22:25Z'>
		Can you test whether rc2 still has the same issue? We have done a few more cherry-picks and want to check if these also fix this
		</comment>
		<comment id='3' author='lamba92' date='2020-03-29T08:29:00Z'>
		Just tried 2.2.0-rc2, error stays the same.
		</comment>
		<comment id='4' author='lamba92' date='2020-03-29T21:56:08Z'>
		Am having exactly the same issue. I tried changing from h5 to tf file storage, but that only succeeded in removing the warning message. The optimizer still can't be loaded, but now fails silently.
Running model.optimizer.get_weights() after reloading the model gives me [].
I should get something more like [643, array([[[[-5.82830864e-04,  5.68370800e-04, -2.78998603e-04, 1.33891811e-03, -2.89490534e-04,  3.54266516e-03, 5.05483190e-11,  3.68256005e-03, -3.21522425e-03, 9.00787301e-04, -2.92667857e-04,  7.44584668e-03, -8.97338055e-03,  9.71068395e-04, -1.64545365e-02, 1.86355431e-02, -3.00804037e-03,  3.82911152e-04, -4.87643149e-04,  1.59692019e-03, -2.45196698e-03, -1.29825426e-02, -2.17458670e-04, -9.59509239e-03, -3.59311025e-03, -2.34026265e-05,  5.58328955e-03, 8.98914295e-04,  1.44025474e-03,  1.85646396e-02, -3.81471426e-03, -9.47254710e-03], [ 3.06007161e-04, -6.80638070e-04,  2.04466240e-04, 4.62170545e-04, -1.00154630e-05,  2.06052428e-04, 7.26481453e-09,  8.34458694e-03, -1.16562960e-03, 2.11097838e-04, -1.14672934e-04,  7.56402314e-03, -9.89045482e-03, -1.20888278e-03, -1.09558310e-02, 1.33366007e-02,  3.07402760e-03, -7.10781242e-05, -3.85403604e-04,  4.09532833e-04,  3.30612151e-04, -8.95670522e-03, -3.33556207e-04, -9.61863622e-03, -5.05439332e-03, -1.13033391e-04,  6.13513310e-03, 1.88108839e-04,  1.44792779e-04,  1.49300322e-02, -3.55098210e-03, -9.76313185e-03], [ 2.15022286e-04, -6.72350463e-04,  5.12044353e-05, 6.01820939e-04,  6.96969568e-04, -4.09249496e-03, 9.14621179e-09,  5.68558183e-03, -3.26973875e-03, -1.09577522e-04, -9.90386688e-05,  7.76778394e-03, -5.82592143e-03,  1.29449691e-04, -3.73055600e-03, 1.32836401e-03, -6.25009136e-03, -2.75031198e-04, 5.25884447e-04, -7.52125517e-04,  6.06745540e-04, -8.11741501e-03, -5.46381052e-04, -8.79659690e-03, -4.84210020e-03, -4.62682547e-05,  6.59690751e-03, -5.15048916e-04, -5.88135117e-05,  9.80930589e-03, -3.64231272e-03, -9.75961983e-03]],
		</comment>
		<comment id='5' author='lamba92' date='2020-04-01T16:48:27Z'>
		&lt;denchmark-link:https://github.com/lamba92&gt;@lamba92&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/DanielMorton&gt;@DanielMorton&lt;/denchmark-link&gt;
 thanks for reporting the issue, we are looking into it.
		</comment>
		<comment id='6' author='lamba92' date='2020-04-14T21:01:55Z'>
		This issue has been fixed with &lt;denchmark-link:https://pypi.org/project/tensorflow/2.2.0rc3/#files&gt;2.2.0-rc3&lt;/denchmark-link&gt;
.
&lt;denchmark-link:https://github.com/lamba92&gt;@lamba92&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/DanielMorton&gt;@DanielMorton&lt;/denchmark-link&gt;
 Can you please check  and if so close this issue ?
		</comment>
		<comment id='7' author='lamba92' date='2020-04-20T17:25:23Z'>
		The loading of the model seems fixed, the step per epoch definitely not!
&lt;denchmark-code&gt;Epoch 1/100
      1/Unknown - 0s 31us/step - loss: 0.6810 - accuracy: 0.7969
&lt;/denchmark-code&gt;

Tested on the same notebook above!
		</comment>
		<comment id='8' author='lamba92' date='2020-04-20T20:03:37Z'>
		&lt;denchmark-link:https://github.com/lamba92&gt;@lamba92&lt;/denchmark-link&gt;
 Actually, an iterator generates data dynamically. So the length of a dataset iterator is unknown until you iterate through it at least once. You could pass steps_per_epoch argument to the  as shown below. Then, it prints steps as you are expecting.
&lt;denchmark-code&gt;training_history = model.fit(
    train_generator, steps_per_epoch=len(train_generator),
    validation_data=validation_generator,
    validation_steps=len(validation_generator),
    epochs=100,
    callbacks=[
        ModelCheckpoint(model_checkpoint_path, save_best_only=True),
        EarlyStopping(patience=10, restore_best_weights=True)
    ],
    workers=10
)
&lt;/denchmark-code&gt;

Can you please verify above and close the issue if this was resolved for you. Thanks!
		</comment>
		<comment id='9' author='lamba92' date='2020-04-20T23:54:46Z'>
		I am passing a keras.utils.Sequence not a Python generator.
Sequence has __len__() to know the actual number of batches.
When using &lt;denchmark-link:https://keras.io/models/model/#fit_generator&gt;keras.model.Model.fit_generator()&lt;/denchmark-link&gt;
  it is stated that when using a  the  are taken from the  itself.
But when using  a warning is emitted stating that  will instead be used and that  is deprecated and will be removed.
Therefore I assumed that fit() as well was able to infer the steps_per_epochs from a Sequence and indeed until TF v2.1 it did!
		</comment>
		<comment id='10' author='lamba92' date='2020-05-19T16:19:19Z'>
		Thanks for the issue! This should be fixed in 2.2
		</comment>
		<comment id='11' author='lamba92' date='2020-05-19T16:19:21Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37968&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37968&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>