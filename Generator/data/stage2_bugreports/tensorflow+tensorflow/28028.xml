<bug id='28028' author='KichangKim' open_date='2019-04-22T01:36:50Z' closed_time='2020-02-27T08:56:11Z'>
	<summary>[TF2.0] Custom training loop for keras example should provide information about tf.keras.backend.set_learning_phase()</summary>
	<description>
System information

TensorFlow version: tensorflow-gpu 2.0.0-alpha0
Doc Link: https://www.tensorflow.org/alpha/guide/keras/training_and_evaluation#using_the_gradienttape_a_first_end-to-end_example


That samples did not use tf.keras.backend.set_learning_phase() for manual training loop. So if the model has BatchNormalization() or other layers which depend on training state, it will be not trained correctly. There is no description about tf.keras.backend.set_learning_phase(), even in the page of BatchNormalization(). (&lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/BatchNormalization&gt;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/BatchNormalization&lt;/denchmark-link&gt;
)
	</description>
	<comments>
		<comment id='1' author='KichangKim' date='2019-05-02T11:38:04Z'>
		Are there any examples on a custom training loop with BatchNormalization?
I've managed to set training_phase, although the model doesn't train the same way as via keras default training loop.
		</comment>
		<comment id='2' author='KichangKim' date='2019-05-02T12:26:28Z'>
		
Keras has different side operations. It depends on whether train, test or predict phase is executed.


https://github.com/keras-team/keras/blob/master/keras/engine/training.py#L497
https://github.com/keras-team/keras/blob/master/keras/engine/training.py#L524
https://github.com/keras-team/keras/blob/master/keras/engine/training.py#L542


In general a layer has unconditional and conditioned on input updates and losses. Updates can be that of running mean average in batch normalization.
https://github.com/keras-team/keras/blob/master/keras/layers/normalization.py#L197
Layer losses may include regularization.
https://github.com/keras-team/keras/blob/master/keras/engine/base_layer.py#L246
Upon first call to K.learning_phase() a placeholder for the phase state is created with default value False. To train the model feed dict should contain {K.learning_phase(): True}.
https://github.com/keras-team/keras/blob/master/keras/backend/tensorflow_backend.py#L123
https://github.com/keras-team/keras/blob/master/keras/backend/tensorflow_backend.py#L3345
https://github.com/keras-team/keras/blob/master/keras/layers/normalization.py#L206

		</comment>
		<comment id='3' author='KichangKim' date='2019-05-07T01:06:19Z'>
		I simply called set training_phase before creating the model, and train the model with same method in TF 2.0 tutorial. Without calling it, the model does not trained well.
		</comment>
		<comment id='4' author='KichangKim' date='2019-05-07T21:40:31Z'>
		I found that other document describes "training=True" flag for manual training loop:
&lt;denchmark-link:https://www.tensorflow.org/alpha/guide/migration_guide#customize_the_training_step&gt;https://www.tensorflow.org/alpha/guide/migration_guide#customize_the_training_step&lt;/denchmark-link&gt;

But this does not:
&lt;denchmark-link:https://www.tensorflow.org/alpha/guide/keras/training_and_evaluation#using_the_gradienttape_a_first_end-to-end_example&gt;https://www.tensorflow.org/alpha/guide/keras/training_and_evaluation#using_the_gradienttape_a_first_end-to-end_example&lt;/denchmark-link&gt;

Also, second document does not use tf.function for its training step, so extremely slow than first sample. Additionally, GradientTape version manual trainig loop requires more memory than train_on_batch version. I can't figure out whether this issue is intended behaviour or bug. (Maybe related to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/28110&gt;#28110&lt;/denchmark-link&gt;
)
		</comment>
		<comment id='5' author='KichangKim' date='2019-05-08T03:57:20Z'>
		With respect to the initial issue, I don't thing a discussion of the learning phase is appropriate, because the example doesn't include any layers that depend on it. Writing a custom loop that uses batch norm is something of an advanced use case both because of the change in behavior between training and inference and the fact that its updates are not gradient updates. All of that complexity would distract from the goal of a simple first example.
With respect to the actual use of the learning phase, I would recommend directly using the  argument of the model rather than . Passing what you want directly into a function (a model is a function, after all) is more idiomatic TF 2.0 than setting a global persistent state. (Or  can split the difference) I do, however, agree that the learning phase is not always a straightforward concept and it can be easy to get tripped up on layers whose behavior depend on it. &lt;denchmark-link:https://github.com/dynamicwebpaige&gt;@dynamicwebpaige&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 Thoughts on adding a learning phase example to the documentation queue?
Lastly, with respect to performance what you describe is expected. Eager performance is expected to be slower than graph in most cases, and higher memory consumption is also expected. In graph mode the runtime is free to free memory during backprop, but in eager it must simultaneously hold all of the activations and all of the gradients in memory. As for acceleration on with tf.function: there are definitely guides on that, but it has some quirks (function tracing and only once execution, trace caching, etc.) that again makes me think it probably shouldn't be in the very first run loop example.
		</comment>
		<comment id='6' author='KichangKim' date='2019-05-09T18:03:00Z'>
		I agree that it would be nice to have an example with batch normalization.
I also stumbled about this when I converted my code to TF 2: In graph mode you had to run the collected update ops from your model. In eager mode all updates were done in place and the call to updates() just did nothing. But once I wrapped my training step in a tf.function I ended up getting an error and it was really not clear to me if I had to run the updates() inside a tf.function or not. I think it might be helpful to clarify that.
		</comment>
		<comment id='7' author='KichangKim' date='2019-05-09T22:34:29Z'>
		
Lastly, with respect to performance what you describe is expected. Eager performance is expected to be slower than graph in most cases, and higher memory consumption is also expected. In graph mode the runtime is free to free memory during backprop, but in eager it must simultaneously hold all of the activations and all of the gradients in memory. As for acceleration on with tf.function: there are definitely guides on that, but it has some quirks (function tracing and only once execution, trace caching, etc.) that again makes me think it probably shouldn't be in the very first run loop example.

&lt;denchmark-link:https://github.com/robieta&gt;@robieta&lt;/denchmark-link&gt;
 Thanks for clarification. I think that this also should be clearly described in documents for new users. Current training sample (Eager without tf.function) is extremely slow as you described, so it makes user misunderstand that TF 2.0 is slow (like me).
I know that TF 2.0 is in the early stage, but these issue makes me that hard to investigate whether an issue is intended behaviour or bug without documentations.
		</comment>
		<comment id='8' author='KichangKim' date='2019-05-27T07:13:02Z'>
		With regard to the tf.keras.BatchNormalization layer, the documentation specifies that the second layer input is a python boolean indicating whether the model is used for training or not.
How does this relate to the training parameter of Model? If one left the layer flag unspecified, would the Model parameter be used instead?
In the examples, only the layer input is applied to BatchNormalization.
What if one wants to alternate a train and test step, is it sufficient to edit the model variable or should it be rebuilt?
I think that the documentation is lacking this kind of information, which is quite crucial for successfully using recent models.
		</comment>
		<comment id='9' author='KichangKim' date='2019-07-15T02:57:35Z'>
		Additionally, when I using Model.train_on_batch(), there is no way to pass Training=True flag, so, does TF 2.0 automatically set True when calling train_on_batch() or should I set set_learning_phase() manually?
Current documentation is too useless for that situation.
		</comment>
		<comment id='10' author='KichangKim' date='2019-07-15T03:45:58Z'>
		I think this is a hidden thing in tf.keras now.
With keras model.fit() api, all these are hidden. &lt;denchmark-link:https://github.com/robieta&gt;@robieta&lt;/denchmark-link&gt;
 documentation is necessary.
However, if you want to directly use keras layers and models, there is a training field inside keras model. For evaluation, you shall set it as false. &lt;denchmark-link:https://github.com/tensorflow/models/blob/master/official/bert/model_training_utils.py#L249&gt;https://github.com/tensorflow/models/blob/master/official/bert/model_training_utils.py#L249&lt;/denchmark-link&gt;

Correct me if I am wrong....
Thx
		</comment>
		<comment id='11' author='KichangKim' date='2019-07-30T18:51:57Z'>
		The migration guide actually have it for TF2.0: &lt;denchmark-link:https://www.tensorflow.org/beta/guide/migration_guide&gt;https://www.tensorflow.org/beta/guide/migration_guide&lt;/denchmark-link&gt;
. But it wasn't mentioned in keyword. So it's sort of hidden here.
		</comment>
		<comment id='12' author='KichangKim' date='2019-07-30T18:54:52Z'>
		
System information

TensorFlow version: tensorflow-gpu 2.0.0-alpha0
Doc Link: https://www.tensorflow.org/alpha/guide/keras/training_and_evaluation#using_the_gradienttape_a_first_end-to-end_example

Describe the documentation issue
That samples did not use tf.keras.backend.set_learning_phase() for manual training loop. So if the model has BatchNormalization() or other layers which depend on training state, it will be not trained correctly. There is no description about tf.keras.backend.set_learning_phase(), even in the page of BatchNormalization(). (https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/BatchNormalization)

Additionally, using training=False or training=True doesn't set learning phase in keras. So when I print keras.backend.learning_phase(), it will also say 0 (test phase) even if I use the training argument. This is confusing.
		</comment>
		<comment id='13' author='KichangKim' date='2019-07-31T06:04:10Z'>
		
Additionally, using training=False or training=True doesn't set learning phase in keras. So when I print keras.backend.learning_phase(), it will also say 0 (test phase) even if I use the training argument. This is confusing.

It doesn't because  variable directly affects which computing branch is being returned at graph construction time inside a python interpreter.
&lt;denchmark-link:https://github.com/keras-team/keras/blob/master/keras/backend/tensorflow_backend.py#L3363&gt;https://github.com/keras-team/keras/blob/master/keras/backend/tensorflow_backend.py#L3363&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='KichangKim' date='2019-08-08T15:27:54Z'>
		
Are there any examples on a custom training loop with BatchNormalization?
I've managed to set training_phase, although the model doesn't train the same way as via keras
default training loop.

You can find an example of the training variable usage for batch norm layers using the functional API and custom training loops in the Tensorflow resnet50 implementation.
&lt;denchmark-link:https://github.com/tensorflow/models/blob/e38d570ef03c16c26d037b23fc2f35ecd4f87032/official/resnet/ctl/ctl_imagenet_main.py#L184&gt;Here&lt;/denchmark-link&gt;
 is the model call with the  in the custom training loop.
Note that the model definition is using the functional API and does  specify the batch norm layers with the  parameter. &lt;denchmark-link:https://github.com/tensorflow/models/blob/e38d570ef03c16c26d037b23fc2f35ecd4f87032/official/resnet/keras/resnet_model.py#L202&gt;see here&lt;/denchmark-link&gt;

When the custom training loop calls the model with the  parameter, the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/7c2a7d4684b55f6f869bd814d67f2d2d4629fd55/tensorflow/python/keras/engine/network.py#L675&gt;call method in tensorflow/python/keras/engine/network.py&lt;/denchmark-link&gt;
 calls  which resets the  parameter for each layer that has  in its argspec. &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/7c2a7d4684b55f6f869bd814d67f2d2d4629fd55/tensorflow/python/keras/engine/network.py#L828-L830&gt;see here&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='KichangKim' date='2019-09-10T11:55:18Z'>
		What about models from keras.applications?
I would to like to use keras.Model from keras.applications in my custom training loop.
It is not clear, because these models create like tf.keras.Model(inputs, outputs) and there is not any information about call function parameters.
		</comment>
		<comment id='16' author='KichangKim' date='2019-10-04T03:35:54Z'>
		&lt;denchmark-link:https://github.com/rwl93&gt;@rwl93&lt;/denchmark-link&gt;
 , I cannot find any special handling of batch norm for custom training loops in the code you linked
&lt;denchmark-link:https://github.com/robieta&gt;@robieta&lt;/denchmark-link&gt;
 , you mention that custom training loops with batch norm are an advanced topic
So which is it ? When I try, my model is not training to the same accuracy as with model.fit(). But then I might be hitting another issue.
Is there something special to do when using tf.keras.layers.BatchNormalization with a custom training loop ?
		</comment>
		<comment id='17' author='KichangKim' date='2019-10-04T06:14:30Z'>
		I put together a quick example of batch norm and CTL &lt;denchmark-link:https://colab.sandbox.google.com/gist/robieta/044ec2031399e1dae5e42fc7c8bd48d3/batch_norm_in_ctl.ipynb&gt;gist&lt;/denchmark-link&gt;
. The main thing is to pass the training arg when calling the model. (I was lazy and just used model.evaluate to assess the CTL) Interestingly that also seems to apply the updates to the moving statistics. For whatever reason I thought those had to be handled separately (handling them correctly was a major stumbling block of 1.x CTL's), but it seems that either the layer or automatic control dependencies are picking them up automagically. So I guess no so advanced a use case as I initially thought.
		</comment>
		<comment id='18' author='KichangKim' date='2019-10-04T06:24:31Z'>
		I also noticed the problem months ago. I remember none of our examples pass
training arg. However, we should pass it explicitly in CTL and all official
models have that now.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Oct 3, 2019, 11:17 PM Taylor Robie ***@***.***&gt; wrote:
 I put together a quick example of batch norm and CTL gist
 &lt;https://colab.sandbox.google.com/gist/robieta/044ec2031399e1dae5e42fc7c8bd48d3/batch_norm_in_ctl.ipynb&gt;.
 The main thing is to pass the training arg when calling the model. (I was
 lazy and just used model.evaluate to assess the CTL) Interestingly that
 also seems to apply the updates to the moving statistics. For whatever
 reason I thought those had to be handled separately (handling them
 correctly was a major stumbling block of 1.x CTL's), but it seems that
 either the layer or automatic control dependencies are picking them up
 automagically. So I guess no so advanced a use case as I initially thought.

 —
 You are receiving this because you commented.
 Reply to this email directly, view it on GitHub
 &lt;#28028?email_source=notifications&amp;email_token=ABFFXZPC3L7LOUBIK7VVPI3QM3NXVA5CNFSM4HHM2SS2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEAKR4WQ#issuecomment-538254938&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABFFXZJXQNEHSWV4HWQ6O6LQM3NXVANCNFSM4HHM2SSQ&gt;
 .



		</comment>
		<comment id='19' author='KichangKim' date='2019-10-04T06:30:52Z'>
		Thank you for putting this example together. It is really nice. You say the main thing is to call the model with model(data, training=True) during training. After reading your code, it seems to me that's the only thing you are doing differently. Correct ?
		</comment>
		<comment id='20' author='KichangKim' date='2019-10-04T15:25:05Z'>
		That is correct.
		</comment>
		<comment id='21' author='KichangKim' date='2019-10-09T10:09:19Z'>
		Recommend to look at this method



tensorflow/tensorflow/python/keras/engine/base_layer.py


         Line 627
      in
      99dda8e






 def __call__(self, inputs, *args, **kwargs): 





it calls before every specific layer calls
firstly training flag is searched in kwargs, and if not training= backend.learning_phase
		</comment>
		<comment id='22' author='KichangKim' date='2019-11-13T01:30:23Z'>
		
I put together a quick example of batch norm and CTL gist. The main thing is to pass the training arg when calling the model. (I was lazy and just used model.evaluate to assess the CTL) Interestingly that also seems to apply the updates to the moving statistics. For whatever reason I thought those had to be handled separately (handling them correctly was a major stumbling block of 1.x CTL's), but it seems that either the layer or automatic control dependencies are picking them up automagically. So I guess no so advanced a use case as I initially thought.

&lt;denchmark-link:https://github.com/robieta&gt;@robieta&lt;/denchmark-link&gt;
  Thanks for sample code. It clarifies how to train BatchNormalization layer by using GradientTape. But how about train_on_batch() and predict_on_batch() API? it doesn't have training parameter. In my understand, keras API (train, predict, evaluate and so on) handles training status internally (so it will be True while I running train_xx method) and I should not call set_learning_phase() manually. Is this correct?
		</comment>
		<comment id='23' author='KichangKim' date='2019-11-13T17:16:19Z'>
		That is correct: the Model.*_on_batch methods set the learning phase for you.
		</comment>
		<comment id='24' author='KichangKim' date='2020-01-14T22:29:24Z'>
		With regards to the original issue, I made &lt;denchmark-link:https://github.com/tensorflow/docs/commit/7ad5e9598e66efeef8b3ed9fa53fa5ed023ddbed&gt;this commit&lt;/denchmark-link&gt;
 that gives users a heads up about the training=True/False for custom training loops across the tutorials on tensorflow.org where possible (some examples have single layers that don't have or need a training=True/False and trying to pass the training param would throw an error).
		</comment>
	</comments>
</bug>