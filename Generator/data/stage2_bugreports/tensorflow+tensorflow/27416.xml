<bug id='27416' author='bingyuanlee' open_date='2019-04-02T08:49:30Z' closed_time='2020-10-12T15:20:01Z'>
	<summary>Unable to use FeatureColumn with Keras Functional API</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below):  2.0.0-alpha0
Python version: 3.7
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A


I followed the guide: &lt;denchmark-link:https://www.tensorflow.org/alpha/tutorials/keras/feature_columns&gt;Classify structured data&lt;/denchmark-link&gt;
. The author used TF Feature Column and TF Data alongside a Sequential model, which worked out just fine.
Then, I've tried implementing the same using Keras Functional API, but was greeted by the error message below:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-19-9647c70de900&gt; in &lt;module&gt;
----&gt; 1 inputs = layers.Input(tensor=feature_layer, name='features')
      2 x = layers.Dense(128, activation='relu')(inputs)
      3 x = layers.Dense(64, activation='relu')(x)
      4 
      5 baggage_pred = layers.Dense(1, activation='sigmoid', name='baggage')(x)

~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_layer.py in Input(shape, batch_size, name, dtype, sparse, tensor, **kwargs)
    231       dtype=dtype,
    232       sparse=sparse,
--&gt; 233       input_tensor=tensor)
    234   # Return tensor including `_keras_history`.
    235   # Note that in this case train_output and test_output are the same pointer.

~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_layer.py in __init__(self, input_shape, batch_size, dtype, input_tensor, sparse, name, **kwargs)
     77         dtype = backend.floatx()
     78       else:
---&gt; 79         dtype = backend.dtype(input_tensor)
     80     elif input_tensor is not None and input_tensor.dtype != dtype:
     81       raise ValueError('`input_tensor.dtype` differs from `dtype`: %s vs. %s' %

~/.local/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in dtype(x)
   1025   ```
   1026   """
-&gt; 1027   return x.dtype.base_dtype.name
   1028 
   1029 

AttributeError: 'str' object has no attribute 'base_dtype'
&lt;/denchmark-code&gt;

Describe the expected behavior
If I understood correctly, TF Keras is supposed to be interoperable with Feature Column. And the way to achieve that is to wrap a list of feature columns with tf.keras.layers.DenseFeatures() and parse it to the input layer as a tensor like so:
feature_layer = tf.keras.layers.DenseFeatures(feature_columns)
inputs = layers.Input(tensor=feature_layer, name='features')
Code to reproduce the issue
Here's the code to reproduce the error:
&lt;denchmark-code&gt;from __future__ import absolute_import, division, print_function

import numpy as np
import pandas as pd

#!pip install tensorflow==2.0.0-alpha0
import tensorflow as tf

from tensorflow import feature_column
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split

URL = 'https://storage.googleapis.com/applied-dl/heart.csv'
dataframe = pd.read_csv(URL)
dataframe.head()

train, test = train_test_split(dataframe, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)
print(len(train), 'train examples')
print(len(val), 'validation examples')
print(len(test), 'test examples')

# A utility method to create a tf.data dataset from a Pandas Dataframe
def df_to_dataset(dataframe, shuffle=True, batch_size=32):
  dataframe = dataframe.copy()
  labels = dataframe.pop('target')
  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
  if shuffle:
    ds = ds.shuffle(buffer_size=len(dataframe))
  ds = ds.batch(batch_size)
  return ds

batch_size = 5 # A small batch sized is used for demonstration purposes
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

age = feature_column.numeric_column("age")
age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])
thal = feature_column.categorical_column_with_vocabulary_list(
      'thal', ['fixed', 'normal', 'reversible'])
thal_one_hot = feature_column.indicator_column(thal)
thal_embedding = feature_column.embedding_column(thal, dimension=8)
thal_hashed = feature_column.categorical_column_with_hash_bucket(
      'thal', hash_bucket_size=1000)
crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)

feature_columns = []

# numeric cols
for header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:
  feature_columns.append(feature_column.numeric_column(header))

# bucketized cols
age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])
feature_columns.append(age_buckets)

# indicator cols
thal = feature_column.categorical_column_with_vocabulary_list(
      'thal', ['fixed', 'normal', 'reversible'])
thal_one_hot = feature_column.indicator_column(thal)
feature_columns.append(thal_one_hot)

# embedding cols
thal_embedding = feature_column.embedding_column(thal, dimension=8)
feature_columns.append(thal_embedding)

# crossed cols
crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)
crossed_feature = feature_column.indicator_column(crossed_feature)
feature_columns.append(crossed_feature)

batch_size = 32
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

feature_layer = tf.keras.layers.DenseFeatures(feature_columns)

inputs = layers.Input(tensor=feature_layer, name='features')
x = layers.Dense(128, activation='relu')(inputs)
x = layers.Dense(64, activation='relu')(x)

baggage_pred = layers.Dense(1, activation='sigmoid')(x)

model = keras.Model(inputs=inputs,outputs=baggage_pred)

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
&lt;/denchmark-code&gt;

Other info / logs
N/A
	</description>
	<comments>
		<comment id='1' author='bingyuanlee' date='2019-04-03T14:38:17Z'>
		i've been struggling with this as well.
I found 2 workarounds but hope there is easier way..
First method is you subclass tf.keras.models.Model  there, you have inputs parameter in def call method, and you feed it to DenseFeatures
But subclassing keras model has shortcommings in that it can't do model.save()
Second method is you define input tensors for all your feature (Input(name="feature_name")) when you use functional api to build keras model. and build feature_inputs = {"feature_name" : input_tensor} dictionary, and feed the dictionary to tf.keras.layers.DenseFeatures(feature_columns)(feature_inputs) .
So you have to give input tensors to feature_layer ...  not the other way around
It would be much easier if one could create input tensors from feature_columns
		</comment>
		<comment id='2' author='bingyuanlee' date='2019-04-04T01:21:09Z'>
		&lt;denchmark-link:https://github.com/littlehome-eugene&gt;@littlehome-eugene&lt;/denchmark-link&gt;
 : Taking on the second method you've suggested, I've referred to the &lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/DenseFeatures&gt;documentation of DenseFeatures&lt;/denchmark-link&gt;
 and came across the example:
&lt;denchmark-code&gt;price = numeric_column('price')
keywords_embedded = embedding_column(
    categorical_column_with_hash_bucket("keywords", 10K), dimensions=16)
columns = [price, keywords_embedded, ...]
feature_layer = DenseFeatures(columns)

features = tf.parse_example(..., features=make_parse_example_spec(columns))
dense_tensor = feature_layer(features)
for units in [128, 64, 32]:
  dense_tensor = tf.layers.dense(dense_tensor, units, tf.nn.relu)
prediction = tf.layers.dense(dense_tensor, 1).
&lt;/denchmark-code&gt;

It seems features, in this regard, refers to the input tensor. It is fed to the feature layer in dense_tensor = feature_layer(features), and finally to the model dense_tensor = tf.layers.dense(dense_tensor, units, tf.nn.relu).
I have tried the same by defining:
&lt;denchmark-code&gt;...
feature_layer = tf.keras.layers.DenseFeatures(get_cols()) 
# get_cols() returns a list of feature columns

features = tf.io.parse_example(features=tf.feature_column.make_parse_example_spec(get_cols()))
dense_tensor = feature_layer(features)

inputs = keras.layers.Input(tensor=dense_tensor, name='features')
x = layers.Dense(128, activation='relu')(inputs)
x = layers.Dense(64, activation='relu')(x)
...
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-12-60a3dbfaf487&gt; in &lt;module&gt;
      1 feature_layer = tf.keras.layers.DenseFeatures(get_cols())
      2 
----&gt; 3 features = tf.io.parse_example(features=tf.feature_column.make_parse_example_spec(get_cols()))
      4 dense_tensor = feature_layer(features)
      5 

TypeError: parse_example_v2() missing 1 required positional argument: 'serialized'
&lt;/denchmark-code&gt;

I'm having trouble defining serialized in tf.io.parse_example. Any pointers?
		</comment>
		<comment id='3' author='bingyuanlee' date='2019-04-04T01:31:15Z'>
		oh, I was just saying I'm trying to solve the same problem, I do have partial solutions as I outlined.
So you could try what I did before tensorflow has the definitive answer to this.
With my limited knowledge of tensorflow (have worked with only a few weeks),
but I don't think parse_example would help here, the serialized arguments is the actual data like training data, so you can't use it to define your functional api-based model.  (I had hard time trying to find what it is due to their lack of explanation in the api guide myself :()
		</comment>
		<comment id='4' author='bingyuanlee' date='2019-04-04T01:48:34Z'>
		&lt;denchmark-link:https://github.com/littlehome-eugene&gt;@littlehome-eugene&lt;/denchmark-link&gt;
: No issue. :)
Okay, so  was only meant for demo purposes.
Digging deeper:
features = tf.feature_column.make_parse_example_spec(get_cols())
print(features)
returns me a schema/placeholder:
&lt;denchmark-code&gt;{'aaa': VarLenFeature(dtype=tf.string),
 'bbb': VarLenFeature(dtype=tf.string),
 'count': VarLenFeature(dtype=tf.int64),
...
 'prob_aaa': FixedLenFeature(shape=(1,), dtype=tf.float32, default_value=None)}
&lt;/denchmark-code&gt;

I was thinking if it is possible to congregate these placeholders and use it as the input to Input(shape=...) instead...
ps. I'm planning to use Dataset in batched reading fashion for training. So it wouldn't make sense to parse a tensor directly to the model..
		</comment>
		<comment id='5' author='bingyuanlee' date='2019-04-04T03:32:42Z'>
		I think I found another way..
it's hacky.. but you can try
I extracted symbolic tensor ( I think it's tensor definition not associated with a data)
and gave them to keras.Input() as a parameter
here's a sample
&lt;denchmark-code&gt;
 import copy
 import tensorflow as tf
 import pandas as pd
 from tensorflow.keras.layers import Input, Embedding, concatenate, Dense, Flatten
 from tensorflow import feature_column
 from tensorflow.python.keras.engine import training_utils

 # tf.compat.v1.disable_eager_execution()

 def df_to_dataset(dataframe, shuffle=True, batch_size=32):
   dataframe = dataframe.copy()
   labels = dataframe.pop('target')
   d = dict(dataframe)

   ds = tf.data.Dataset.from_tensor_slices((d, labels))
   if shuffle:
     ds = ds.shuffle(buffer_size=len(dataframe))
   ds = ds.batch(batch_size)
   return ds


 # import pdb; pdb.set_trace()
 # feature_layer = tf.keras.layers.DenseFeatures(feature_columns)


 class FeatureModel(tf.keras.Model):

   def __init__(self, *args, **kwargs):
     feature_columns = kwargs.pop('feature_columns', None)
     super().__init__(*args, **kwargs)

     self.feature_columns = feature_columns
     self.feature_layer = tf.keras.layers.DenseFeatures(self.feature_columns)
     self.dense1 = Dense(1, activation='sigmoid')

   def call(self, inputs, training=False):
       a_inputs = {k:v for k, v in inputs.items() if k in ['child_month_young', 'child_gender_young']}
       model_inputs = training_utils.ModelInputs(a_inputs)
       feature_inputs = model_inputs.get_symbolic_inputs()

       self.feature_inputs = feature_inputs

       features = self.feature_layer(inputs)

       o = self.dense1(features)
       return o


 feature_columns = []
 child_month_young = feature_column.numeric_column("child_month_young")
 kid_age_youngest_buckets = feature_column.bucketized_column(child_month_young, boundaries=[0, 12, 24, 36, 72, 96])
 feature_columns.append(kid_age_youngest_buckets)

 child_gender_young = feature_column.categorical_column_with_vocabulary_list(
   'child_gender_young', ['boy', 'girl', ''])
 child_gender_young_one_hot = feature_column.indicator_column(child_gender_young)
 feature_columns.append(child_gender_young_one_hot)

 class FModel(object):
     def get_model(self, feature_inputs):
         first_input = Input(shape = (1,), name='first_input')
         second_input = Input(shape = (1,), name='second_input' )

         feature_inputs_d = {}
         for name, tensor in feature_inputs.items():
             input_t = Input(shape=tensor.shape, name=name, tensor=tensor)
             setattr(self, name, input_t)

             feature_inputs_d[name] = input_t

         embedding_layer = Embedding(input_dim=10, output_dim=3, input_length=1)

         first_input_encoded = embedding_layer(first_input)
         first_input_encoded = tf.keras.layers.Reshape((3,))(first_input_encoded)
         second_input_encoded = embedding_layer(second_input)
         second_input_encoded = tf.keras.layers.Reshape((3,))(second_input_encoded)

         feature_layer = tf.keras.layers.DenseFeatures(feature_columns)

         features = feature_layer(feature_inputs_d)
         o = concatenate([first_input_encoded, second_input_encoded, features])
         o = Dense(1)(o)

         inputs = [first_input, second_input] + list(feature_inputs_d.values())
         model = tf.keras.models.Model(inputs=inputs, outputs=o)
         return model


 df = pd.DataFrame(
     [[3,4,5, 'boy', 0],
     [2,6,7, 'girl', 1]], columns=['first_input', 'second_input', 'child_month_young', 'child_gender_young', 'target'])


 def get_feature_inputs(train_ds):

     feature_model = FeatureModel(feature_columns=feature_columns)
     feature_model.compile(optimizer='adam',
                           loss='binary_crossentropy',
                           metrics=['accuracy'])


     feature_model.fit(train_ds, epochs=1)


     return dict(feature_model.feature_inputs)



 train_ds = df_to_dataset(df)
 val_ds = df_to_dataset(df)

 # feature_inputs, _, _ = training_utils.extract_tensors_from_dataset(train_ds)
 # feature_inputs, _, _ = training_utils.unpack_iterator_input(train_ds)

 feature_inputs = get_feature_inputs(train_ds)
 f = FModel()
 model = f.get_model(feature_inputs)

 model.compile(optimizer='adam',
               loss='binary_crossentropy',
               metrics=['accuracy'])


 model.fit(
     train_ds,
     validation_data=val_ds,
     epochs=1
 )

&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='bingyuanlee' date='2019-04-08T06:36:44Z'>
		DenseFeatures.dtype is None, so it can't be passed into Input in existing code.
nice workaround &lt;denchmark-link:https://github.com/littlehome-eugene&gt;@littlehome-eugene&lt;/denchmark-link&gt;
 !
&lt;denchmark-code&gt;         feature_layer = tf.keras.layers.DenseFeatures(feature_columns)
         features = feature_layer(feature_inputs_d)
&lt;/denchmark-code&gt;

any syntax sugar to create map[string]Input feature_inputs_d per column or from feature_column directly?
then it doesn't need to fit FeatureModel in this way
		</comment>
		<comment id='7' author='bingyuanlee' date='2019-04-22T12:32:45Z'>
		&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;

I am having the same issue. Is there any plan to make feature columns work with Keras functional API? Because Keras functional API preserves the static nature of a computation graph, it is more efficient performance-wise.
		</comment>
		<comment id='8' author='bingyuanlee' date='2019-06-07T21:52:46Z'>
		Hello. Has there been any progress on this issue? I wanted to use the Feature Columns with the Functional API as well. Is there a good workaround to this issue?
		</comment>
		<comment id='9' author='bingyuanlee' date='2019-06-13T08:10:40Z'>
		I also wanted to use the Feature Columns with the Functional API. Waiting for a good workaround to this issue.
		</comment>
		<comment id='10' author='bingyuanlee' date='2019-06-13T14:24:11Z'>
		Apologies for the delay. Will take a look today.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Jun 13, 2019 at 1:18 AM admu ***@***.***&gt; wrote:
 I also wanted to use the Feature Columns with the Functional API. Waiting
 for a good workaround to this issue.

 —
 You are receiving this because you were assigned.
 Reply to this email directly, view it on GitHub
 &lt;#27416?email_source=notifications&amp;email_token=AKEVL2AFAV4GS6SNAN2E5R3P2H7EJA5CNFSM4HC55GZKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXS4PTI#issuecomment-501598157&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AKEVL2HA25XU7SY6EYU2QETP2H7EJANCNFSM4HC55GZA&gt;
 .



		</comment>
		<comment id='11' author='bingyuanlee' date='2019-06-14T09:26:15Z'>
		&lt;denchmark-link:https://towardsdatascience.com/how-to-build-a-wide-and-deep-model-using-keras-in-tensorflow-2-0-2f7a236b5a4b&gt;How to build a wide-and-deep model using Keras in TensorFlow 2.0 (Using Feature columns with the Keras Functional API)&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='bingyuanlee' date='2019-06-14T16:51:50Z'>
		I am also trying to make feature column work with Keras function API. waiting for a reasonable solution.
		</comment>
		<comment id='13' author='bingyuanlee' date='2019-06-14T18:35:29Z'>
		As unfortunate as this is, for now you will have to make it work by assigning tf.keras.Input to each original feature column that you have, i.e., those numeric feature column and categorical feature columns. Here's the code snippet that will make it work:
from __future__ import absolute_import, division, print_function

import numpy as np
import pandas as pd

#!pip install tensorflow==2.0.0-alpha0
import tensorflow as tf

from tensorflow import feature_column
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split

URL = 'https://storage.googleapis.com/applied-dl/heart.csv'
dataframe = pd.read_csv(URL)
dataframe.head()

train, test = train_test_split(dataframe, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)
print(len(train), 'train examples')
print(len(val), 'validation examples')
print(len(test), 'test examples')

# A utility method to create a tf.data dataset from a Pandas Dataframe
def df_to_dataset(dataframe, shuffle=True, batch_size=32):
  dataframe = dataframe.copy()
  labels = dataframe.pop('target')
  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
  if shuffle:
    ds = ds.shuffle(buffer_size=len(dataframe))
  ds = ds.batch(batch_size)
  return ds

batch_size = 5 # A small batch sized is used for demonstration purposes
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

age = feature_column.numeric_column("age")

feature_columns = []
feature_layer_inputs = {}

# numeric cols
for header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:
  feature_columns.append(feature_column.numeric_column(header))
  feature_layer_inputs[header] = tf.keras.Input(shape=(1,), name=header)

# bucketized cols
age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])
feature_columns.append(age_buckets)

# indicator cols
thal = feature_column.categorical_column_with_vocabulary_list(
      'thal', ['fixed', 'normal', 'reversible'])
thal_one_hot = feature_column.indicator_column(thal)
feature_columns.append(thal_one_hot)
feature_layer_inputs['thal'] = tf.keras.Input(shape=(1,), name='thal', dtype=tf.string)

# embedding cols
thal_embedding = feature_column.embedding_column(thal, dimension=8)
feature_columns.append(thal_embedding)

# crossed cols
crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)
crossed_feature = feature_column.indicator_column(crossed_feature)
feature_columns.append(crossed_feature)

batch_size = 32
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

feature_layer = tf.keras.layers.DenseFeatures(feature_columns)
feature_layer_outputs = feature_layer(feature_layer_inputs)

x = layers.Dense(128, activation='relu')(feature_layer_outputs)
x = layers.Dense(64, activation='relu')(x)

baggage_pred = layers.Dense(1, activation='sigmoid')(x)

model = keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=baggage_pred)

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.fit(train_ds)
		</comment>
		<comment id='14' author='bingyuanlee' date='2019-06-14T20:21:07Z'>
		&lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;

Thank you so much for the code example. Appreciate it!
I noticed that inside of the code, embedding columns and crossed columns are not added to feature_layer_inputs, is that intentional?
if feature_layer_inputs doesn't include the two left-out columns, the respective data won't flow through the network, right?
feature_layer_outputs = feature_layer(feature_layer_inputs)
		</comment>
		<comment id='15' author='bingyuanlee' date='2019-06-14T20:38:36Z'>
		Right, that's why I mentioned it needs to be "originating columns", i.e. those which is actually your input that you will feed in. embedding is derived from you categorical column, and your input (from your data pipeline or dataframe in this case) doesn't contain that, so no need to create input for it.
		</comment>
		<comment id='16' author='bingyuanlee' date='2019-06-16T02:59:44Z'>
		
@tanzhenyu
Thank you so much for the code example. Appreciate it!
I noticed that inside of the code, embedding columns and crossed columns are not added to feature_layer_inputs, is that intentional?
if feature_layer_inputs doesn't include the two left-out columns, the respective data won't flow through the network, right?
feature_layer_outputs = feature_layer(feature_layer_inputs)

To add on to &lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
's response, note the below line:
feature_layer = tf.keras.layers.DenseFeatures(feature_columns)
This handles the feature column transformation required during model definition. Meanwhile, during model compilation:
keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=baggage_pred)
feature_layer_inputs passes in the actual input (original features) to the model.
Hope it helps ;)
Thank you &lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
 for the codes! I shall try this out when I have the chance to. Kudos!
		</comment>
		<comment id='17' author='bingyuanlee' date='2019-06-25T16:38:27Z'>
		Thanks, this solved the table initialization problem in non-eager mode:
with tf.compat.v1.Session() as sess:
    sess.run(tf.compat.v1.initialize_all_tables())
    model.fit(train_ds)
		</comment>
		<comment id='18' author='bingyuanlee' date='2019-07-12T10:07:30Z'>
		Unable to use FeatureColumn with Keras Functional API«
Have a better solution
？？？
		</comment>
		<comment id='19' author='bingyuanlee' date='2019-08-13T08:22:29Z'>
		Any news about the saving issue?
		</comment>
		<comment id='20' author='bingyuanlee' date='2019-08-14T20:14:17Z'>
		What issue is remaining regarding this?
		</comment>
		<comment id='21' author='bingyuanlee' date='2019-08-16T13:30:12Z'>
		&lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
  when I try to save this model:
&lt;denchmark-code&gt;       input = [v for v in feature_layer_inputs.values()]
        x = Dense(2048, activation='relu')(feature_layer)
        x = Dropout(0.5)(x)
        x = Dense(1024, activation='relu')(x)
        x = Dropout(0.5)(x)
        x = Dense(512, activation='relu')(x)
        x = Dropout(0.5)(x)
        x = Dense(256, activation='relu')(x)
        x = Dropout(0.5)(x)
        out = Dense(1, activation='sigmoid')(x)
        self.model = Model(inputs=[input], outputs=out)
        self.model.summary()
        self.model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001), metrics=['binary_accuracy', 'AUC'])
&lt;/denchmark-code&gt;

I obtain the following error:
&lt;denchmark-code&gt;  File "classifier.py", line 165, in &lt;module&gt;
    main()
  File "classifier.py", line 148, in main
    c.save_model(filename, stats)
  File "classifier.py", line 108, in save_model
    self.model.save('{}.model'.format(filename))
  File "/home/sartiano/projects/suspicious-network-event-recognition-2019/models.py", line 65, in save
    self.model.save(file_model)
  File "/home/sartiano/projects/suspicious-network-event-recognition-2019/.env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py", line 1157, in save
    saving.save_model(self, filepath, overwrite, include_optimizer, save_format)
  File "/home/sartiano/projects/suspicious-network-event-recognition-2019/.env/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py", line 107, in save_model
    saved_model_save.save(model, filepath, overwrite, include_optimizer)
  File "/home/sartiano/projects/suspicious-network-event-recognition-2019/.env/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py", line 85, in save
    save_lib.save(model, filepath)
  File "/home/sartiano/projects/suspicious-network-event-recognition-2019/.env/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py", line 855, in save
    meta_graph_def, saveable_view, signatures)
  File "/home/sartiano/projects/suspicious-network-event-recognition-2019/.env/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py", line 585, in _fill_meta_graph_def
    signatures = _generate_signatures(signature_functions, resource_map)
  File "/home/sartiano/projects/suspicious-network-event-recognition-2019/.env/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py", line 459, in _generate_signatures
    function, mapped_inputs, resource_map)
  File "/home/sartiano/projects/suspicious-network-event-recognition-2019/.env/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py", line 411, in _call_function_with_mapped_captures
    function.graph.captures, resource_map)
  File "/home/sartiano/projects/suspicious-network-event-recognition-2019/.env/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py", line 333, in _map_captures_to_created_tensors
    .format(interior))
AssertionError: Tried to export a function which references untracked object Tensor("StatefulPartitionedCall/args_106:0", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='22' author='bingyuanlee' date='2019-08-16T14:53:41Z'>
		That is not related to the original issue. Can you file another github issue for this?
		</comment>
		<comment id='23' author='bingyuanlee' date='2019-08-20T08:31:20Z'>
		For anyone landing here using tf 2.0 ... DO NOT START with the keras functional API if you are doing anything other than vanilla regression. Subclass API is first class now and much simpler. Start there.
I'm looking for tf.feature_columns examples in that context. Landed here from google search. Post refs for the next person if you have any.
		</comment>
		<comment id='24' author='bingyuanlee' date='2019-10-10T09:36:22Z'>
		
i've been struggling with this as well.
I found 2 workarounds but hope there is easier way..
First method is you subclass tf.keras.models.Model there, you have inputs parameter in def call method, and you feed it to DenseFeatures
But subclassing keras model has shortcommings in that it can't do model.save()
Second method is you define input tensors for all your feature (Input(name="feature_name")) when you use functional api to build keras model. and build feature_inputs = {"feature_name" : input_tensor} dictionary, and feed the dictionary to tf.keras.layers.DenseFeatures(feature_columns)(feature_inputs) .
So you have to give input tensors to feature_layer ... not the other way around
It would be much easier if one could create input tensors from feature_columns

This is quit the only two ways I found....since there has been 6 months since you post this comment, is there any other solution?
		</comment>
		<comment id='25' author='bingyuanlee' date='2019-10-25T04:11:32Z'>
		&lt;denchmark-link:https://github.com/littlebeandog&gt;@littlebeandog&lt;/denchmark-link&gt;
 I was able to find a workaround to get past the above error (but I'm still figuring out how to initialize the table in TF2.0) I think this is a bit cleaner, but maybe still suboptimal.
&lt;denchmark-code&gt;    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)
    features,labels = tf.data.experimental.get_single_element(training_dataset.take(1))
    
    dense_inputs = feature_layer(features)
&lt;/denchmark-code&gt;

The inputs to the keras model would then be dense_inputs.
		</comment>
		<comment id='26' author='bingyuanlee' date='2019-10-29T01:47:45Z'>
		The workarounds above don't cover the case where the FeatureColumns contain trainable parameters which require the gradients during model training; e.g. embedding columns. In the case of an embedding, gradients are needed to refine the embedding during training, and precomputing via a DenseFeatures layer is not a workable solution for this use -- if I am understanding correctly, the above examples would just generate randomly-initialized embeddings that are never actually learning anything during the training iterations.
Are there any solutions which don't require placing the DenseFeatures outside the Model?
		</comment>
		<comment id='27' author='bingyuanlee' date='2019-10-29T03:30:13Z'>
		&lt;denchmark-link:https://github.com/jpgard&gt;@jpgard&lt;/denchmark-link&gt;
 I believe that the above workarounds actually include the DenseFeatures params in the model despite the funky calling. Running model.summary() on tanzhenyu's workaround gives me the output shown below. Note that the DenseFeatures layers are in the middle and have trainable params as desired.
&lt;denchmark-code&gt;Model: "model_2"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
age (InputLayer)                [(None, 1)]          0                                            
__________________________________________________________________________________________________
ca (InputLayer)                 [(None, 1)]          0                                            
__________________________________________________________________________________________________
chol (InputLayer)               [(None, 1)]          0                                            
__________________________________________________________________________________________________
oldpeak (InputLayer)            [(None, 1)]          0                                            
__________________________________________________________________________________________________
slope (InputLayer)              [(None, 1)]          0                                            
__________________________________________________________________________________________________
thal (InputLayer)               [(None, 1)]          0                                            
__________________________________________________________________________________________________
thalach (InputLayer)            [(None, 1)]          0                                            
__________________________________________________________________________________________________
trestbps (InputLayer)           [(None, 1)]          0                                            
__________________________________________________________________________________________________
dense_features_2 (DenseFeatures (None, 1029)         24          age[0][0]                        
                                                                 ca[0][0]                         
                                                                 chol[0][0]                       
                                                                 oldpeak[0][0]                    
                                                                 slope[0][0]                      
                                                                 thal[0][0]                       
                                                                 thalach[0][0]                    
                                                                 trestbps[0][0]                   
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 128)          131840      dense_features_2[0][0]           
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 64)           8256        dense_6[0][0]                    
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 1)            65          dense_7[0][0]                    
==================================================================================================
Total params: 140,185
Trainable params: 140,185
Non-trainable params: 0
&lt;/denchmark-code&gt;

		</comment>
		<comment id='28' author='bingyuanlee' date='2019-10-29T23:24:53Z'>
		Ah, I see, thanks &lt;denchmark-link:https://github.com/JoshEZiegler&gt;@JoshEZiegler&lt;/denchmark-link&gt;

		</comment>
		<comment id='29' author='bingyuanlee' date='2019-11-12T07:15:29Z'>
		
As unfortunate as this is, for now you will have to make it work by assigning tf.keras.Input to each original feature column that you have, i.e., those numeric feature column and categorical feature columns. Here's the code snippet that will make it work:
from __future__ import absolute_import, division, print_function

import numpy as np
import pandas as pd

#!pip install tensorflow==2.0.0-alpha0
import tensorflow as tf

from tensorflow import feature_column
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split

URL = 'https://storage.googleapis.com/applied-dl/heart.csv'
dataframe = pd.read_csv(URL)
dataframe.head()

train, test = train_test_split(dataframe, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)
print(len(train), 'train examples')
print(len(val), 'validation examples')
print(len(test), 'test examples')

# A utility method to create a tf.data dataset from a Pandas Dataframe
def df_to_dataset(dataframe, shuffle=True, batch_size=32):
  dataframe = dataframe.copy()
  labels = dataframe.pop('target')
  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
  if shuffle:
    ds = ds.shuffle(buffer_size=len(dataframe))
  ds = ds.batch(batch_size)
  return ds

batch_size = 5 # A small batch sized is used for demonstration purposes
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

age = feature_column.numeric_column("age")

feature_columns = []
feature_layer_inputs = {}

# numeric cols
for header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:
  feature_columns.append(feature_column.numeric_column(header))
  feature_layer_inputs[header] = tf.keras.Input(shape=(1,), name=header)

# bucketized cols
age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])
feature_columns.append(age_buckets)

# indicator cols
thal = feature_column.categorical_column_with_vocabulary_list(
      'thal', ['fixed', 'normal', 'reversible'])
thal_one_hot = feature_column.indicator_column(thal)
feature_columns.append(thal_one_hot)
feature_layer_inputs['thal'] = tf.keras.Input(shape=(1,), name='thal', dtype=tf.string)

# embedding cols
thal_embedding = feature_column.embedding_column(thal, dimension=8)
feature_columns.append(thal_embedding)

# crossed cols
crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)
crossed_feature = feature_column.indicator_column(crossed_feature)
feature_columns.append(crossed_feature)

batch_size = 32
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

feature_layer = tf.keras.layers.DenseFeatures(feature_columns)
feature_layer_outputs = feature_layer(feature_layer_inputs)

x = layers.Dense(128, activation='relu')(feature_layer_outputs)
x = layers.Dense(64, activation='relu')(x)

baggage_pred = layers.Dense(1, activation='sigmoid')(x)

model = keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=baggage_pred)

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.fit(train_ds)

thank you so much. I also want to ask you a question，When I deploy the model in this way on tf-serving，I find that every request takes hundreds of milliseconds，I don't think it's normal. Do you have any other methods other than your one? For example, use a layer of example to accept all feature columns？
		</comment>
		<comment id='30' author='bingyuanlee' date='2019-12-13T00:59:59Z'>
		To all:
The interaction between feature columns and Keras is painful. To address this issue, we have proposed Keras preprocessing layers. If you're interested, please put your comment under this &lt;denchmark-link:https://github.com/tensorflow/community/pull/188&gt;RFC&lt;/denchmark-link&gt;

		</comment>
		<comment id='31' author='bingyuanlee' date='2020-01-27T14:49:12Z'>
		
As unfortunate as this is, for now you will have to make it work by assigning tf.keras.Input to each original feature column that you have, i.e., those numeric feature column and categorical feature columns. Here's the code snippet that will make it work:
from __future__ import absolute_import, division, print_function

import numpy as np
import pandas as pd

#!pip install tensorflow==2.0.0-alpha0
import tensorflow as tf

from tensorflow import feature_column
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split

URL = 'https://storage.googleapis.com/applied-dl/heart.csv'
dataframe = pd.read_csv(URL)
dataframe.head()

train, test = train_test_split(dataframe, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)
print(len(train), 'train examples')
print(len(val), 'validation examples')
print(len(test), 'test examples')

# A utility method to create a tf.data dataset from a Pandas Dataframe
def df_to_dataset(dataframe, shuffle=True, batch_size=32):
  dataframe = dataframe.copy()
  labels = dataframe.pop('target')
  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
  if shuffle:
    ds = ds.shuffle(buffer_size=len(dataframe))
  ds = ds.batch(batch_size)
  return ds

batch_size = 5 # A small batch sized is used for demonstration purposes
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

age = feature_column.numeric_column("age")

feature_columns = []
feature_layer_inputs = {}

# numeric cols
for header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:
  feature_columns.append(feature_column.numeric_column(header))
  feature_layer_inputs[header] = tf.keras.Input(shape=(1,), name=header)

# bucketized cols
age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])
feature_columns.append(age_buckets)

# indicator cols
thal = feature_column.categorical_column_with_vocabulary_list(
      'thal', ['fixed', 'normal', 'reversible'])
thal_one_hot = feature_column.indicator_column(thal)
feature_columns.append(thal_one_hot)
feature_layer_inputs['thal'] = tf.keras.Input(shape=(1,), name='thal', dtype=tf.string)

# embedding cols
thal_embedding = feature_column.embedding_column(thal, dimension=8)
feature_columns.append(thal_embedding)

# crossed cols
crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)
crossed_feature = feature_column.indicator_column(crossed_feature)
feature_columns.append(crossed_feature)

batch_size = 32
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

feature_layer = tf.keras.layers.DenseFeatures(feature_columns)
feature_layer_outputs = feature_layer(feature_layer_inputs)

x = layers.Dense(128, activation='relu')(feature_layer_outputs)
x = layers.Dense(64, activation='relu')(x)

baggage_pred = layers.Dense(1, activation='sigmoid')(x)

model = keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=baggage_pred)

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.fit(train_ds)

I noticed that in this post when you create your model you get the values of feature_layer_inputs with .values() function. I think this could be dangerous because you cannot gaurantee that the values are in the correct order. I think you should probably use an OrderedDict no?
		</comment>
		<comment id='32' author='bingyuanlee' date='2020-02-18T02:47:34Z'>
		I don't believe the above workaround is effective if using this inside an estimator
&lt;denchmark-code&gt;def mlp_fn(features, labels, mode, params):
    
    feature_layer_inputs = {}
    feature_layer_inputs['dense_input'] = tf.keras.Input(shape=(4,), name='dense_input', dtype=tf.string)

    
    feature_columns = [
        tf.feature_column.numeric_column('dense_input')
    ]
    
    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)
    feature_layer_outputs = feature_layer(feature_layer_inputs)

    
    # --- create NN ---
    
    # input
    input_layer = tf.keras.layers.DenseFeatures(feature_layer_outputs)
    
    # output
    logits = tf.keras.layers.Dense(units=3, activation=tf.nn.relu)(input_layer)
    output = tf.keras.layers.Softmax(logits)

    
    model = keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=output)
&lt;/denchmark-code&gt;

In this case the error
OperatorNotAllowedInGraphError: iterating over tf.Tensor is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.
is thrown
		</comment>
		<comment id='33' author='bingyuanlee' date='2020-05-05T11:51:50Z'>
		Any update on this thread?
		</comment>
		<comment id='34' author='bingyuanlee' date='2020-05-09T19:31:45Z'>
		&lt;denchmark-link:https://github.com/sivaharsh&gt;@sivaharsh&lt;/denchmark-link&gt;

Please review:
&lt;denchmark-link:https://github.com/keras-team/governance/blob/master/rfcs/20190502-preprocessing-layers.md&gt;https://github.com/keras-team/governance/blob/master/rfcs/20190502-preprocessing-layers.md&lt;/denchmark-link&gt;

and
&lt;denchmark-link:https://github.com/tensorflow/community/blob/master/rfcs/20191212-keras-categorical-inputs.md&gt;https://github.com/tensorflow/community/blob/master/rfcs/20191212-keras-categorical-inputs.md&lt;/denchmark-link&gt;

The work product of which would 'replace feature columns and tf.keras.layers.DenseFeatures with proposed layers'.
See &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/&gt;https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/&lt;/denchmark-link&gt;

		</comment>
		<comment id='35' author='bingyuanlee' date='2020-10-12T07:15:27Z'>
		any update then?
		</comment>
		<comment id='36' author='bingyuanlee' date='2020-10-12T15:20:01Z'>
		Closing this issue. For anyone working on Keras + feature columns, please note that we have now replaced it with Keras Preprocessing Layers (links above)
		</comment>
		<comment id='37' author='bingyuanlee' date='2020-10-12T15:20:03Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27416&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27416&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>