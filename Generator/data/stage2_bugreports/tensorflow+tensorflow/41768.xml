<bug id='41768' author='DekraN' open_date='2020-07-27T11:28:13Z' closed_time='2020-07-28T15:21:27Z'>
	<summary>[TF 2.2.0] Error on creating optimizer slot variable under multi GPU training with tf.distribute.MirroredStrategy</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution: Red Hat Enterprise Linux Server release 7.6 (Maipo)
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): tensorflow/2.2.0--cuda--10.1
Python version: 3.8.2
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source): [GCC 4.8.5 20150623 (Red Hat 4.8.5-36)]
CUDA/cuDNN version: 10.1
GPU model and memory: 4x Tesla V100-SXM2-16GB

Describe the current behavior
I am using Tensorflow 2.2.0 GPU with Tensorflow's Keras API for training a GAN model with multi GPU using Distributed Tensorflow MirroredStrategy. Under the mirrored strategy scope, I've:

instantiated the model;
defined the optimizer (tf.keras.optimizer.RMSProp);
compiled the model with that optimizer.

In the train_step function I've defined the computational graph in Pythonic-way. I'm using Lazy mode (as opposed to the default TF2.0 eager mode and Keras graph mode) because TF warned me that running MirroredStrategy with eager execution still has overhead problems.
When the execution arrives to the apply_gradients call on the optimizer of my model, TF crashes reporting me this error:
   ValueError: Trying to create optimizer slot variable under the scope for tf.distribute.Strategy (&lt;tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x20079d15d4f0&gt;), which is different from the scope used for the original variable (MirroredVariable:{ 0: &lt;tf.Variable 'conv2d_20/kernel:0' shape=(3, 3, 2, 2) dtype=float32&gt;, 1: &lt;tf.Variable 'conv2d_20/kernel/replica_1:0' shape=(3, 3, 2, 2) dtype=float32&gt;, 2: &lt;tf.Variable 'conv2d_20/kernel/replica_2:0' shape=(3, 3, 2, 2) dtype=float32&gt;, 3: &lt;tf.Variable 'conv2d_20/kernel/replica_3:0' shape=(3, 3, 2, 2) dtype=float32&gt; }). Make sure the slot variables are created under the same strategy scope. This may happen if you're restoring from a checkpoint outside the scope

The apply gradient should execute fine, because I've correctly defined the model and the optimizer under the MirroredStrategy object, as reported in: &lt;denchmark-link:https://www.tensorflow.org/guide/distributed_training&gt;https://www.tensorflow.org/guide/distributed_training&lt;/denchmark-link&gt;


&lt;denchmark-link:https://pastebin.com/MyT6p1T8&gt;https://pastebin.com/MyT6p1T8&lt;/denchmark-link&gt;

Other info / logs
	</description>
	<comments>
		<comment id='1' author='DekraN' date='2020-07-27T17:36:27Z'>
		Hi &lt;denchmark-link:https://github.com/DekraN&gt;@DekraN&lt;/denchmark-link&gt;
, I've tried to run the code you've shared but am getting syntax errors. If I fix the syntax errors, I get the error  Can you please double check the code runs and provide it in Colab?
Additionally, although I am not able to run the code yet, there are a few things that stand out. Firstly the use of tf.compat.v1.disable_eager_execution() as we don't support custom losses in legacy graph mode (which is what you have if you disable eager mode). It is true that running MirroredStrategy with eager execution still has overhead problems, but that just means you need to wrap your distributed training step in a @tf.function (which it looks like you have done), and not disable eager execution all together.
Lastly, it doesn't look like you've distributed your dataset. As explained &lt;denchmark-link:https://www.tensorflow.org/guide/distributed_training#using_tfdistributestrategy_with_custom_training_loops&gt;here&lt;/denchmark-link&gt;
, you'll need  when using MirroredStrategy with a custom training loop.
Happy to provide further support if you can share reproducible code.
		</comment>
		<comment id='2' author='DekraN' date='2020-07-28T15:21:26Z'>
		Greatly thanks for the answer! I was looking for a confirm regarding the execution modality to use in TF2.2.0. In fact, in my code I use custom losses, therefore I should use normal eager execution instead of legacy graph mode.
Now, after some other issues, I've been successful in executing with multi-GPU my code in eager execution
Sorry for the not reproducible code.
		</comment>
		<comment id='3' author='DekraN' date='2020-07-28T15:21:29Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41768&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41768&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='DekraN' date='2020-10-01T01:03:41Z'>
		&lt;denchmark-link:https://github.com/DekraN&gt;@DekraN&lt;/denchmark-link&gt;
 Could you please tell me how exactly do you fix the code? I am encountering the same issues.
		</comment>
	</comments>
</bug>