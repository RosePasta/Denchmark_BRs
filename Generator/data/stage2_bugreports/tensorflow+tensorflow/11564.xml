<bug id='11564' author='nurlonn' open_date='2017-07-18T00:42:31Z' closed_time='2018-02-22T00:58:31Z'>
	<summary>XLA bugs on training accuracy</summary>
	<description>
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): r1.2.1
Python version: 2.7.12
Bazel version (if compiling from source):
CUDA/cuDNN version: 8.0 / 6.0
GPU model and memory: NVIDIA TITAN Xp 12GB
Exact command to reproduce:

(At the tensorflow/model/inception directory)
bazel-bin/inception/imagenet_train --num_gpus=1 --batch_size=32 --train_dir=/tmp/imagenet_train --data_dir=/tmp/imagenet_data
bazel-bin/inception/imagenet_eval --checkpoint_dir=/tmp/imagenet_train --eval_dir=/tmp/imagenet_eval
== cat /etc/issue ===============================================
Linux Ares 4.8.0-58-generic &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/63&gt;#63&lt;/denchmark-link&gt;
~16.04.1-Ubuntu SMP Mon Jun 26 18:08:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION="16.04.2 LTS (Xenial Xerus)"
VERSION_ID="16.04"
VERSION_CODENAME=xenial
== are we in docker =============================================
No
== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
== uname -a =====================================================
Linux Ares 4.8.0-58-generic &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/63&gt;#63&lt;/denchmark-link&gt;
~16.04.1-Ubuntu SMP Mon Jun 26 18:08:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
== check pips ===================================================
numpy (1.13.1)
protobuf (3.3.0)
tensorflow (1.2.1)
tensorflow-tensorboard (0.1.2)
== check for virtualenv =========================================
False
== tensorflow import ============================================
tf.VERSION = 1.2.1
tf.GIT_VERSION = v1.2.1-2-gc996c7b
tf.COMPILER_VERSION = v1.2.1-2-gc996c7b
Sanity check: array([1], dtype=int32)
== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda-8.0/lib64:/usr/local/cuda-8.0/extras/CUPTI/lib64:/usr/local/cuda-8.0/lib64:/usr/local/cuda/extras/CUPTI/lib64
DYLD_LIBRARY_PATH is unset
== nvidia-smi ===================================================
Tue Jul 18 09:26:11 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 381.09                 Driver Version: 381.09                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  TITAN Xp            Off  | 0000:01:00.0     Off |                  N/A |
| 48%   75C    P2   287W / 250W |  11771MiB / 12189MiB |     54%      Default |
+-------------------------------+----------------------+----------------------+
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1005    G   /usr/lib/xorg/Xorg                              18MiB |
|    0     30938    C   /usr/bin/python                              11737MiB |
+-----------------------------------------------------------------------------+
== cuda libs  ===================================================
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

I activated XLA and trained the Inception model.
When I verify the training results, the accuracy is always 0.001.
(It is considered that the training is not performed normally.)
When XLA is disabled, normal accuracy is achieved.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

I added some codes of model/inception/inception/inception_train.py to enable XLA.
I attached the file.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/1154355/inception_train.zip&gt;inception_train.zip&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='nurlonn' date='2017-07-25T17:18:06Z'>
		Can you reproduce this on a simpler example than full inception training? I.e. does a simple example work for you with XLA?
		</comment>
		<comment id='2' author='nurlonn' date='2017-07-26T04:11:22Z'>
		I ran the simple example. (tensorflow/examples/tutorials/mnist/mnist_softmax_xla.py)
This example seems to work correctly even with XLA.
To ensure that xla optimization is enabled, the --xla_generate_hlo_graph option is enabled.
&lt;denchmark-code&gt;nurlonn@Ares:~/workspace/mnist$ ls
mnist_softmax_xla.py
nurlonn@Ares:~/workspace/mnist$ TF_XLA_FLAGS=--xla_generate_hlo_graph=.* python mnist_softmax_xla.py --xla=''
Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz
2017-07-26 13:04:01.942110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-07-26 13:04:01.942576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: TITAN Xp
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:01:00.0
Total memory: 11.90GiB
Free memory: 11.72GiB
2017-07-26 13:04:01.942595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2017-07-26 13:04:01.942601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2017-07-26 13:04:01.942609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0)
2017-07-26 13:04:01.989717: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-07-26 13:04:01.989757: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 4 visible devices
2017-07-26 13:04:01.990281: I tensorflow/compiler/xla/service/service.cc:198] XLA service 0x3f5d970 executing computations on platform Host. Devices:
2017-07-26 13:04:01.990308: I tensorflow/compiler/xla/service/service.cc:206]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
2017-07-26 13:04:01.990549: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-07-26 13:04:01.990571: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 4 visible devices
2017-07-26 13:04:01.990868: I tensorflow/compiler/xla/service/service.cc:198] XLA service 0x3f6bd50 executing computations on platform CUDA. Devices:
2017-07-26 13:04:01.990884: I tensorflow/compiler/xla/service/service.cc:206]   StreamExecutor device (0): TITAN Xp, Compute Capability 6.1
2017-07-26 13:04:03.225400: I tensorflow/stream_executor/dso_loader.cc:139] successfully opened CUDA library libcupti.so.8.0 locally
0.9192
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;nurlonn@Ares:~/workspace/mnist$ TF_XLA_FLAGS=--xla_generate_hlo_graph=.* python mnist_softmax_xla.py
Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz
2017-07-26 13:10:04.847645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-07-26 13:10:04.848130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: TITAN Xp
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:01:00.0
Total memory: 11.90GiB
Free memory: 11.72GiB
2017-07-26 13:10:04.848148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2017-07-26 13:10:04.848154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2017-07-26 13:10:04.848163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0)
2017-07-26 13:10:04.895322: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-07-26 13:10:04.895369: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 4 visible devices
2017-07-26 13:10:04.895798: I tensorflow/compiler/xla/service/service.cc:198] XLA service 0x55020d0 executing computations on platform Host. Devices:
2017-07-26 13:10:04.895819: I tensorflow/compiler/xla/service/service.cc:206]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
2017-07-26 13:10:04.895980: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-07-26 13:10:04.895992: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 4 visible devices
2017-07-26 13:10:04.896399: I tensorflow/compiler/xla/service/service.cc:198] XLA service 0x55384d0 executing computations on platform CUDA. Devices:
2017-07-26 13:10:04.896415: I tensorflow/compiler/xla/service/service.cc:206]   StreamExecutor device (0): TITAN Xp, Compute Capability 6.1
2017-07-26 13:10:05.202342: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:573] computation cluster_0[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v82 [optimization: pipeline start, before simplification]: /tmp/hlo_graph_0.DUlZcS.dot
2017-07-26 13:10:05.202706: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:573] computation cluster_0[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v82 [simplification: pipeline start, before algsimp]: /tmp/hlo_graph_1.7hTRMk.dot
2017-07-26 13:10:05.203085: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:573] computation cluster_0[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v82 [simplification: after algsimp, before reshape-motion]: /tmp/hlo_graph_2.46FPmN.dot
2017-07-26 13:10:05.203403: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:573] computation cluster_0[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v82 [simplification: after reshape-motion, before constant_folding]: /tmp/hlo_graph_3.jbcSWf.dot

...
&lt;omitted&gt;
...

2017-07-26 13:10:06.055764: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:573] computation cluster_1[_XlaCompiledKernel=true,_XlaNumConstantArgs=0,_XlaNumResourceArgs=0].v11 [GPU-ir-emit-prepare: after dce, before flatten-call-graph]: /tmp/hlo_graph_91.569aPU.dot
2017-07-26 13:10:06.055924: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:573] computation cluster_1[_XlaCompiledKernel=true,_XlaNumConstantArgs=0,_XlaNumResourceArgs=0].v11 [GPU-ir-emit-prepare: after flatten-call-graph, pipeline end]: /tmp/hlo_graph_92.jcgozq.dot
0.9212
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='nurlonn' date='2017-07-26T07:29:47Z'>
		FYI, I attach execution logs of mnist and inception with "--tf_xla_parallel_checking=true --parallel_check_failfast=false".
MNIST examples shows correct results as I mentioned before.
But inception shows that optimized subgraphs generate different output from original subgraphs.
Check attached log for more information.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/1175763/inception_parallel_check_logs.txt&gt;inception_parallel_check_logs.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/1175762/mnist_parallel_check_logs.txt&gt;mnist_parallel_check_logs.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='nurlonn' date='2017-07-26T17:42:45Z'>
		&lt;denchmark-link:https://github.com/tatatodd&gt;@tatatodd&lt;/denchmark-link&gt;
, are you aware if anyone has successfully used the models/inception training on imagenet with xla? I don't really know how to start debugging this.
		</comment>
		<comment id='5' author='nurlonn' date='2017-08-03T05:07:33Z'>
		I tried adjusting some HLO optimization options to find the problems.
I found that reshape-motion in HLO optimizer optimization is a problem.
(tensorflow/compiler/xla/service/reshape_mover.cc)
(I cannot be sure this optimization is always a problem. In some cases this seems to work well.)
I used the parallel check option to compare the results of turning the reshape-motion optimization option on and off.
Default - All HLO optimization is enabled
Command
TF_XLA_FLAGS="--tf_xla_parallel_checking=true --parallel_check_failfast=false" bazel-bin/inception/imagenet_train --num_gpus=1 --batch_size=32 --train_dir=${TRAIN_DIR} --data_dir=${DATA_DIR} --max_steps=40040
The results are recorded in the attached file.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/1196029/inception_enable_all.txt&gt;inception_enable_all.txt&lt;/denchmark-link&gt;

Turn off reshape-motion optimization
Command
TF_XLA_FLAGS="--tf_xla_parallel_checking=true --parallel_check_failfast=false --xla_disable_hlo_passes=reshape-motion" bazel-bin/inception/imagenet_train --num_gpus=1 --batch_size=32 --train_dir=${TRAIN_DIR} --data_dir=${DATA_DIR} --max_steps=40040
The results are also recorded in the attached file.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/1196028/inception_disable_reshape.txt&gt;inception_disable_reshape.txt&lt;/denchmark-link&gt;

You can see that the result is much closer to the original value.
I checked the inception for 1epoch using the imagenet input, and confirmed that normal accuracy was obtained.
		</comment>
		<comment id='6' author='nurlonn' date='2017-12-20T19:08:42Z'>
		It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='7' author='nurlonn' date='2017-12-20T19:14:19Z'>
		Ping &lt;denchmark-link:https://github.com/tatatodd&gt;@tatatodd&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='8' author='nurlonn' date='2018-01-04T19:08:40Z'>
		It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='9' author='nurlonn' date='2018-01-04T22:32:09Z'>
		&lt;denchmark-link:https://github.com/jlebar&gt;@jlebar&lt;/denchmark-link&gt;
 knows more about XLA GPU related issues, and might have some thoughts here.
		</comment>
		<comment id='10' author='nurlonn' date='2018-01-08T17:07:28Z'>
		If this still reproduces, this is quite bad.  &lt;denchmark-link:https://github.com/hpucha&gt;@hpucha&lt;/denchmark-link&gt;
, do you have cycles to take a look?
		</comment>
		<comment id='11' author='nurlonn' date='2018-01-09T00:15:34Z'>
		Will take a look.
		</comment>
		<comment id='12' author='nurlonn' date='2018-01-23T23:19:15Z'>
		A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.
		</comment>
		<comment id='13' author='nurlonn' date='2018-02-03T01:29:32Z'>
		&lt;denchmark-link:https://github.com/hpucha&gt;@hpucha&lt;/denchmark-link&gt;
 any luck?
		</comment>
		<comment id='14' author='nurlonn' date='2018-02-17T13:31:21Z'>
		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='15' author='nurlonn' date='2018-02-22T00:58:31Z'>
		Hi, I know this has sat for way too long; I'm sorry about that.
I just tried this again, and it seems to work.  I'm using the latest version of inception from tensorflow/models, and I hacked it like you did to allow enabling XLA.  (I'll try to push a change upstream to add this flag.)  I ran locally with a P100 and CUDA 8.
Without XLA:
step 2950, loss = 12.21 (122.6 examples/sec; 0.261 sec/batch)
With XLA:
step 2950, loss = 12.51 (122.7 examples/sec; 0.261 sec/batch)
I think the 12.21 vs 12.51 is noise; I let the XLA one run a bit longer and by step 3360 the loss was down to 11.96.
I'm going to close this because...it seems to be working?  But please don't hesitate to reopen this if it's not working for you.  We have more folks working on this, so I think / hope we'll be able to have a turnaround time significantly shorter than 6 months.  :)
Thanks again for your patience, and sorry this dragged on so long.
		</comment>
		<comment id='16' author='nurlonn' date='2018-02-22T01:21:33Z'>
		Awesome, thanks for checking!
		</comment>
		<comment id='17' author='nurlonn' date='2018-02-22T01:32:42Z'>
		Sorry, forgot to mention, I had to patch in our fix for &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/17090&gt;#17090&lt;/denchmark-link&gt;
, which should be upstream soon.
Just to be sure I'm going to leave this XLA one going overnight.
		</comment>
		<comment id='18' author='nurlonn' date='2018-02-22T17:40:04Z'>
		The XLA run got down to
step 125000, loss = 7.53 (113.8 examples/sec; 0.281 sec/batch)
before my hard drive ran out of space for storing checkpoints.  :)
		</comment>
	</comments>
</bug>