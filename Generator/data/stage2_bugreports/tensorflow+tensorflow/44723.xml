<bug id='44723' author='maxkaustav' open_date='2020-11-10T08:14:09Z' closed_time='2020-11-18T16:41:26Z'>
	<summary>When using opt.apply_gradient(zip(grads,model.trainable variables))</summary>
	<description>
ValueError: No gradients provided for any variable: ['dense_flipout_1/kernel_posterior_loc:0', 'dense_flipout_1/kernel_posterior_untransformed_scale:0', 'dense_flipout_1/bias_posterior_loc:0', 'dense_flipout_2/kernel_posterior_loc:0', 'dense_flipout_2/kernel_posterior_untransformed_scale:0', 'dense_flipout_2/bias_posterior_loc:0'].
	</description>
	<comments>
		<comment id='1' author='maxkaustav' date='2020-11-11T09:02:26Z'>
		&lt;denchmark-link:https://github.com/maxkaustav&gt;@maxkaustav&lt;/denchmark-link&gt;

We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]
		</comment>
		<comment id='2' author='maxkaustav' date='2020-11-14T08:17:13Z'>
		tf.version=2.3
&lt;denchmark-code&gt;                              tfp.layers.Convolution1DFlipout(64,1,padding="same",kernel_divergence_fn=kl_divergence_function,kernel_posterior_fn=tfp.layers.default_mean_field_normal_fn(),activation=tf.nn.relu),
                              tfp.layers.DenseFlipout(32,kernel_divergence_fn=kl_divergence_function,kernel_posterior_fn=tfp.layers.default_mean_field_normal_fn(),activation=tf.nn.relu),
                              tf.keras.layers.Dropout(0.4),
                              tfp.layers.DenseFlipout(8,kernel_divergence_fn=kl_divergence_function,kernel_posterior_fn=tfp.layers.default_mean_field_normal_fn(),activation=tf.nn.relu),
                              tf.keras.layers.Dropout(0.4),
                              tfp.layers.DenseFlipout(2,kernel_divergence_fn=kl_divergence_function,kernel_posterior_fn=tfp.layers.default_mean_field_normal_fn())
                              ])
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;def train_loop(x,y):
  with tf.GradientTape() as tape:
    logits=model33(x,training=True)
    l=loss(y,logits)
  grad=tape.gradient(l,model3.trainable_variables)
  optimizer.apply_gradients(zip(grad,model3.trainable_variables))
  return l,logits
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='maxkaustav' date='2020-11-17T08:33:13Z'>
		&lt;denchmark-link:https://github.com/maxkaustav&gt;@maxkaustav&lt;/denchmark-link&gt;

I ran the code shared and it has indentation issues, please share a colab gist with the issue reported.
		</comment>
		<comment id='4' author='maxkaustav' date='2020-11-18T16:41:25Z'>
		Thanks but this issue is solved in another issue in tensorflow probability
		</comment>
		<comment id='5' author='maxkaustav' date='2020-11-18T16:41:27Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44723&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44723&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>