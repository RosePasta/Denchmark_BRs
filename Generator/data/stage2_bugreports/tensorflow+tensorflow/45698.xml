<bug id='45698' author='Leslie-Fang' open_date='2020-12-15T14:28:55Z' closed_time='2020-12-22T16:27:48Z'>
	<summary>Failed to run Benchmark</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7.6
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): Latest Master 203f925
Python version:
Bazel version (if compiling from source): 3.1
GCC/Compiler version (if compiling from source): 10.2
CUDA/cuDNN version:
GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"

Describe the current behavior
There is no output of benchmark
&lt;denchmark-code&gt;bazel --output_user_root=$build_dir run --copt=-O3 //tensorflow/core/kernels/image:non_max_suppression_op_benchmark_test -- --benchmarks=../
&lt;/denchmark-code&gt;

Describe the expected behavior
It should output the performance data of unit test.
Isolate to this Commit: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/29bb0deb26db7179eefc87e750fd8755b2c9def3&gt;29bb0de&lt;/denchmark-link&gt;
 which enables the new benchmark feature.
	</description>
	<comments>
		<comment id='1' author='Leslie-Fang' date='2020-12-15T14:33:31Z'>
		Is there any document about this new benchmark? Looking into the source code, it confuse me. Since the 


tensorflow/tensorflow/core/platform/default/test_benchmark.cc


        Lines 55 to 62
      in
      c70e89e






 Benchmark::Benchmark(const char* name, void (*fn)(::testing::benchmark::State&amp;)) 



     : name_(name), 



 // -1 because the number of parameters is not part of the benchmark 



 // routine signature. 



 num_args_(-1), 



       fn_state_(fn) { 



 Register(); 



 } 




 will set no args. But the benchmark do needs args to trigger the testing 


tensorflow/tensorflow/core/platform/default/test_benchmark.cc


        Lines 159 to 228
      in
      c70e89e






 for (auto arg : b-&gt;args_) { 



     name.resize(b-&gt;name_.size()); 



 if (arg.first &gt;= 0) { 



 strings::StrAppend(&amp;name, "/", arg.first); 



 if (arg.second &gt;= 0) { 



 strings::StrAppend(&amp;name, "/", arg.second); 



       } 



     } 



 



 // TODO(vrv): Check against 'pattern' using a regex before 



 // computing the width, if we start allowing clients to pass in 



 // a custom pattern. 



     width = std::max&lt;int&gt;(width, name.size()); 



   } 



 } 



 



 printf("%-*s %10s %10s\n", width, "Benchmark", "Time(ns)", "Iterations"); 



 printf("%s\n", string(width + 22, '-').c_str()); 



 for (auto b : *all_benchmarks) { 



   name = b-&gt;name_; 



 for (auto arg : b-&gt;args_) { 



     name.resize(b-&gt;name_.size()); 



 if (arg.first &gt;= 0) { 



 strings::StrAppend(&amp;name, "/", arg.first); 



 if (arg.second &gt;= 0) { 



 strings::StrAppend(&amp;name, "/", arg.second); 



       } 



     } 



 



 // TODO(vrv): Match 'name' against 'pattern' using a regex 



 // before continuing, if we start allowing clients to pass in a 



 // custom pattern. 



 



 int iters; 



 double seconds; 



     b-&gt;Run(arg.first, arg.second, &amp;iters, &amp;seconds); 



 



 char buf[100]; 



     std::string full_label = label; 



 if (bytes_processed &gt; 0) { 



 snprintf(buf, sizeof(buf), " %.1fMB/s", 



                (bytes_processed * 1e-6) / seconds); 



       full_label += buf; 



     } 



 if (items_processed &gt; 0) { 



 snprintf(buf, sizeof(buf), " %.1fM items/s", 



                (items_processed * 1e-6) / seconds); 



       full_label += buf; 



     } 



 printf("%-*s %10.0f %10d\t%s\n", width, name.c_str(), 



            seconds * 1e9 / iters, iters, full_label.c_str()); 



 



     TestReporter reporter(name); 



     Status s = reporter.Initialize(); 



 if (!s.ok()) { 



 LOG(ERROR) &lt;&lt; s.ToString(); 



 exit(EXIT_FAILURE); 



     } 



     s = reporter.Benchmark(iters, 0.0, seconds, 



                            items_processed * 1e-6 / seconds); 



 if (!s.ok()) { 



 LOG(ERROR) &lt;&lt; s.ToString(); 



 exit(EXIT_FAILURE); 



     } 



     s = reporter.Close(); 



 if (!s.ok()) { 



 LOG(ERROR) &lt;&lt; s.ToString(); 



 exit(EXIT_FAILURE); 



     } 



   } 




.
I think that's why there is no output of the benchmark results.
		</comment>
		<comment id='2' author='Leslie-Fang' date='2020-12-22T16:27:48Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/bba12d0401800fbf873ea35f34517a8c47a54272&gt;bba12d0&lt;/denchmark-link&gt;
 should have fixed the issue
		</comment>
		<comment id='3' author='Leslie-Fang' date='2020-12-22T16:27:50Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45698&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45698&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>