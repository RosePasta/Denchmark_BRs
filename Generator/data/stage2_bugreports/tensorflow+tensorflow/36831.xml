<bug id='36831' author='Riccorl' open_date='2020-02-17T18:24:22Z' closed_time='2020-03-03T16:11:56Z'>
	<summary>"ValueError: No gradients provided for any variable" in TF2.1 custom models</summary>
	<description>
System information

OS Platform and Distribution: Arch Linux
TensorFlow version: v2.1.0-rc2-17-ge5bf8de 2.1.0

I'm trying to train a custom model with tf.GradientTape, but it gives me this error:
&lt;denchmark-code&gt;ValueError: No gradients provided for any variable: ['MyModel/conv2d/kernel:0', 'MyModel/conv2d/bias:0'].
&lt;/denchmark-code&gt;

I don't see anything wrong in my code, can you point out where is the problem?
train_dir = config.TRAIN_DIR
train_ds = tf.data.Dataset.list_files(str(train_dir / "*"))
train_ds = (
    train_ds.map(load_frames, num_parallel_calls=12)
    .batch(batch_size)
    .prefetch(buffer_size=batch_size)
)

model = MyModel()
# Keep results for plotting
train_loss_results = []
optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)
mse_loss_fn = tf.keras.losses.MeanSquaredError()
for epoch in epochs:
    epoch_loss_avg = tf.keras.metrics.Mean()
    for inputs in train_ds:
        with tf.GradientTape() as tape:
            input_1, input_2, input_3 = inputs
            predictions, warping_output = model(inputs, training=True)
            rec_loss = mse_loss_fn(input_3, predictions)
            
        grads = tape.gradient(rec_loss, model.trainable_weights)
        optimizer.apply_gradients(zip(grads, model.trainable_weights))
        epoch_loss_avg(grads)  # Add current batch loss

    train_loss_results.append(epoch_loss_avg.result())
    if epoch % 50 == 0:
        print("Epoch {:03d}: Loss: {:.3f}".format(epoch, epoch_loss_avg.result()))
where MyModel is for example
class MyModel(tf.keras.Model):
    def __init__(self, name="MyModel", **kwargs):
        super(MyModel, self).__init__(name=name, **kwargs)
        self.conv = tf.keras.layers.Conv2D(
            filters=32, kernel_size=7, strides=1, padding="same"
        )

    def call(self, inputs, training=True, **kwargs):
        input_1, input_2, input_3 = inputs
        out = self.conv(input_1)
        return out
	</description>
	<comments>
		<comment id='1' author='Riccorl' date='2020-02-18T11:59:10Z'>
		&lt;denchmark-link:https://github.com/Riccorl&gt;@Riccorl&lt;/denchmark-link&gt;
, Please provide the complete standalone code to replicate the reported issue. Thanks!
		</comment>
		<comment id='2' author='Riccorl' date='2020-02-18T19:35:48Z'>
		If you use tf.compat.v1.train.AdamOptimizer the problem would most likely go away.
Could you please check and respond ?
		</comment>
		<comment id='3' author='Riccorl' date='2020-02-19T17:00:24Z'>
		I'll provide a working code as soon as possible, sorry for the delay.
		</comment>
		<comment id='4' author='Riccorl' date='2020-02-25T01:07:59Z'>
		Hi, Did you find the solution of this problem?
		</comment>
		<comment id='5' author='Riccorl' date='2020-02-25T14:06:50Z'>
		&lt;denchmark-link:https://github.com/ksrath0re&gt;@ksrath0re&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Riccorl&gt;@Riccorl&lt;/denchmark-link&gt;
  I believe this problem is very general and has two components


There is some sort of mismatch or incompatibility between the APIs of tf.compat.v1....  and tf.keras.....  What is the exact nature of this incompatibility is not clear to me. I have been trying to implement some object detection techniques in TensorFlow and have been badly eroded by this problem.  I am quite sure that with tf.compat.v1.train.AdamOptimizer the above code will work i.e at least start running. But the OP will have to respond on this one.


If you load a SavedModel and then try to fine-tune it, you are again likely to face this problem with quite the same error. This is often the case when you want to add some extra layers on top of a pretrained model and then do end-to-end training. In that case also using tf.compat.v1.... in place of tf.keras... API will make the code running but you are likely to see the issue that the saved model's layers are not getting updated.


The fact that these GitHub issues (this and &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/34211&gt;#34211&lt;/denchmark-link&gt;
 ) have not been addressed, ( except providing a CoLab GIST ) does speak something terribly unfortunate.
		</comment>
		<comment id='6' author='Riccorl' date='2020-02-25T18:33:13Z'>
		
@ksrath0re @Riccorl I believe this problem is very general and has two components
1. There is some sort of mismatch or incompatibility between the APIs of `tf.compat.v1....`  and `tf.keras....`.  What is the exact nature of this incompatibility is not clear to me. I have been trying to implement some object detection techniques in TensorFlow and have been badly eroded by this problem.  I am quite sure that with tf.compat.v1.train.AdamOptimizer the above code will work i.e at least start running. But the OP will have to respond on this one.

2. If you load a SavedModel and then try to fine-tune it, you are again likely to face this problem with quite the same error. This is often the case when you want to add some extra layers on top of a pretrained model and then do end-to-end training. In that case also using `tf.compat.v1....` in place of `tf.keras...` API will make the code running but you are likely to see the issue that the saved model's layers are not getting updated.

The fact that these GitHub issues (this and #34211 ) have not been addressed, ( except providing a CoLab GIST ) does speak something terribly unfortunate.

Si I tried the approach you mentioned of using  tf.compat.v1.train.AdamOptimizer and it works and execution starts. The problem is that the gradient values are None. I am using Tensorflow-gpu 2.0.0 version.

length of gradients :  268
gradients # 0  :  None
gradients # 1  :  None
gradients # 2  :  None
gradients # 3  :  None
gradients # 4  :  None
gradients # 5  :  None

I am setting those values to Zero and continuing. How does that affect the computation?
		</comment>
		<comment id='7' author='Riccorl' date='2020-02-25T21:20:33Z'>
		I wrote a &lt;denchmark-link:https://colab.research.google.com/drive/1OLGnpj7gzbjaI-G0eOzmURhFxHESiIwH&gt;gist&lt;/denchmark-link&gt;
 on Colab but, there, I cannot reproduce the error anymore (neither with tensorflow 2.0 nor tensorflow 2.1).
The code here is essentially the same as the one provided in the OP (and identical to the original one at the time I had this error). Strange thing is that the same problem appeared on 3 different PC with different configuration and hardware.
&lt;denchmark-link:https://github.com/ksrath0re&gt;@ksrath0re&lt;/denchmark-link&gt;
 The problem disappeared after changing everything from preprocessing to the train code, but honestly, since the same code now works in Colab, I don't know. We tried many different things (like putting  inside the  context) and eventually the train started correctly.
		</comment>
		<comment id='8' author='Riccorl' date='2020-02-26T09:22:36Z'>
		

@ksrath0re @Riccorl I believe this problem is very general and has two components
1. There is some sort of mismatch or incompatibility between the APIs of `tf.compat.v1....`  and `tf.keras....`.  What is the exact nature of this incompatibility is not clear to me. I have been trying to implement some object detection techniques in TensorFlow and have been badly eroded by this problem.  I am quite sure that with tf.compat.v1.train.AdamOptimizer the above code will work i.e at least start running. But the OP will have to respond on this one.

2. If you load a SavedModel and then try to fine-tune it, you are again likely to face this problem with quite the same error. This is often the case when you want to add some extra layers on top of a pretrained model and then do end-to-end training. In that case also using `tf.compat.v1....` in place of `tf.keras...` API will make the code running but you are likely to see the issue that the saved model's layers are not getting updated.

The fact that these GitHub issues (this and #34211 ) have not been addressed, ( except providing a CoLab GIST ) does speak something terribly unfortunate.

Si I tried the approach you mentioned of using tf.compat.v1.train.AdamOptimizer and it works and execution starts. The problem is that the gradient values are None. I am using Tensorflow-gpu 2.0.0 version.

length of gradients :  268
gradients # 0  :  None
gradients # 1  :  None
gradients # 2  :  None
gradients # 3  :  None
gradients # 4  :  None
gradients # 5  :  None

I am setting those values to Zero and continuing. How does that affect the computation?

I am having the same issue. The gradients are continuously None. This kind of problem was reported in another issue ( I do not remember the issue nunber !! ) where the OP pointed out that the problem appears primarily due to a mixing of tf.keras... and tf.math operations. So, this further reinforces the belief that there is some gross compatibility issues in there.
		</comment>
		<comment id='9' author='Riccorl' date='2020-03-03T11:17:39Z'>
		&lt;denchmark-link:https://github.com/Riccorl&gt;@Riccorl&lt;/denchmark-link&gt;
, Is this still an issue?
		</comment>
		<comment id='10' author='Riccorl' date='2020-03-03T16:11:51Z'>
		
@Riccorl, Is this still an issue?

At the moment it's not an issue, I'll close it. I'll open it if I get this error again. Thank you.
		</comment>
		<comment id='11' author='Riccorl' date='2020-03-03T16:11:58Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36831&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36831&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>