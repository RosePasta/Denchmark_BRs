<bug id='30021' author='LukeBolly' open_date='2019-06-21T09:01:54Z' closed_time='2019-06-26T16:03:13Z'>
	<summary>tf.keras.layers.BatchNormalization API does not fully support masking</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): TF 1.13.1
Python version: 3.6.7
CUDA/cuDNN version:
GPU model and memory:

Describe the current behavior
Trying to apply a mask when calling a BatchNormalization layer fails with:
TypeError: call() got an unexpected keyword argument 'mask'
Describe the expected behavior
Masking should be supported:



tensorflow/tensorflow/python/keras/layers/normalization.py


         Line 188
      in
      93dd14d






 self.supports_masking = True 





If a layer supports masking, it should be a kwarg to call. Until all layers support masking, it's not practical to only support masking via an upstream layer. A Masking Layer can't always be inserted into a model because downstream convolution layers don't support it at all. Feeding the mask into the call of a recurrent layer is the only workaround to combine Convolution, LSTM and masking, and it would be great if it worked the same way for BatchNorm:
encoded = keras.layers.LSTM(10)(input, mask=input_mask).
Code to reproduce the issue
&lt;denchmark-code&gt;from tensorflow import keras

def Works():
    input = keras.layers.Input(shape=(None, 1))
    masked_input = keras.layers.Masking(-1)(input)
    normalized = keras.layers.BatchNormalization(epsilon=1.1e-5)(masked_input)

def DoesntWork():
    input = keras.layers.Input(shape=(None, 1))
    mask = keras.layers.Masking(-1).compute_mask(input)
    normalized = keras.layers.BatchNormalization(epsilon=1.1e-5)(input, mask=mask)

if __name__ == '__main__':
    Works()
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='LukeBolly' date='2019-06-24T09:28:50Z'>
		In order to expedite the trouble-shooting process, please provide a full code snippet to reproduce the issue reported here. Thanks!
		</comment>
		<comment id='2' author='LukeBolly' date='2019-06-24T10:15:08Z'>
		If you run this, you will see the error:
&lt;denchmark-code&gt;from tensorflow import keras

def DoesntWork():
    input = keras.layers.Input(shape=(None, 1))
    mask = keras.layers.Masking(-1).compute_mask(input)
    normalized = keras.layers.BatchNormalization(epsilon=1.1e-5)(input, mask=mask)

if __name__ == '__main__':
    DoesntWork()
&lt;/denchmark-code&gt;

TypeError: call() got an unexpected keyword argument 'mask'
		</comment>
		<comment id='3' author='LukeBolly' date='2019-06-25T06:41:14Z'>
		I have tried on Colab with TF version 1.13.1 and was able to reproduce issue.
		</comment>
		<comment id='4' author='LukeBolly' date='2019-06-26T02:07:21Z'>
		Is the Mask actually used when calculating the mean/variance? The padded values shouldn't be included in the calculations or they are going to bias the mean and lower the variance.
		</comment>
		<comment id='5' author='LukeBolly' date='2019-06-26T05:05:56Z'>
		This is probably part of a bigger issue: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/30161&gt;#30161&lt;/denchmark-link&gt;

If the layer incorrectly uses the Mask (ie. ignores it) and causes a model to silently fail, then it shouldn't allow input from a masked layer. The current behavior is even worse than getting an error because it implies that it will operate as expected, when it is most likely impacting performance.
		</comment>
		<comment id='6' author='LukeBolly' date='2019-06-26T16:03:13Z'>
		The way masking works is that we categorize all layers into three categories:

producer, that has compute_mask
consumer, that takes mask inside call()
some kind of passenger, that simply pass through the masking.

for 3), masking is "silently" supported by self.support_masking=True and you don't have to pass mask into call(), because we treat batch norm as passing through the mask, not consuming the mask. On the other hand, recurrent layers are consumer of mask, i.e., passing mask into the call makes sure the output reflects it. recurrent layers will also produce mask as well. Mask layer is merely a producer.
That said, in terms of masking, batch norm is just like dense or conv or activation layers, they all fall into 3).
		</comment>
		<comment id='7' author='LukeBolly' date='2019-06-26T16:03:15Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30021&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30021&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='LukeBolly' date='2019-06-27T00:18:05Z'>
		In the case of Convolutional layers, that's not true.
&lt;denchmark-code&gt;from tensorflow import keras

if __name__ == '__main__':
    input = keras.layers.Input(shape=(None, None, 1))
    masked_input = keras.layers.Masking(-1)(input)
    conv = keras.layers.Conv2D(1,3)(masked_input)
    normalized = keras.layers.BatchNormalization(epsilon=1.1e-5)(conv)
&lt;/denchmark-code&gt;

TypeError: Layer conv2d does not support masking, but was passed an input_mask: Tensor("masking/Any_1:0", shape=(?, ?, ?), dtype=bool)
I'd argue that BatchNorm (and probably Dense layers too if they have bias nodes) also don't support masking because they don't work as expected with it. Conv2d could pass the mask through too, but instead the user is notified that it won't work which is much better behavior in my opinion. At least in my case, I knew I needed to work the mask around my conv layers. I did not know the same thing for BatchNormalization.
Is the logic that:

If a layer has self.support_masking = True, then it passes it through.
If a layer also has a mask argument in call, then it consumes the mask.
If a layer has neither, it will throw an error because it cannot pass the mask and cannot consume it.

To be fair I should have looked further than self.support_masking=True, but to me it seems that there is either an issue with consistency or with documentation. This is the first time I've seen those 3 categories of Masking you've listed above. Apart from looking at the source, there is no way to know if a mask is passed through or if it is consumed.
As a side note, if you stack an RNN layer on a Masking layer, do you still need to pass the mask value in with call to the RNN in the Functional API? I assumed that the Masking layer handled that.
		</comment>
	</comments>
</bug>