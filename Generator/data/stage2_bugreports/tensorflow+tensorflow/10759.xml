<bug id='10759' author='nicaogr' open_date='2017-06-16T10:59:45Z' closed_time='2017-06-20T01:06:56Z'>
	<summary>GPU-&amp;gt;CPU Memcpy failed Error or InternalError c2c fft failed Error when using FFT2D</summary>
	<description>
&lt;denchmark-h:h3&gt;The problem&lt;/denchmark-h&gt;

I wrote a python script that used FFT2D from tensorflow you can find the Python script attached : TestFFT2D.py using GPU.
In this script, I first create a convolution apply to a tensor and than compute a loss between a reference input tensor and a variable one with ftt2d and ifft2d operations.
When I launch my script, I get a GPU-&gt;CPU Memcpy failed Error or an InternalError c2c fft failed Error. I don't know why the error change when I run again my script.
I test my code on two differents machine with Ubuntu 16.04 one with a GeForce GTX 680 GPU and one with a GeForce GTX 1080. In both case, I can randomly get both of the error messages.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/1080533/Abandon.txt&gt;Abandon.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/1080532/InternalError.txt&gt;InternalError.txt&lt;/denchmark-link&gt;

Sometimes, my machine just crashes and I need to reboot it.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

You can find the terminal messages errors attached.
This is the TestFFT2D.py code : &lt;denchmark-link:https://github.com/nicaogr/silver-octo-fortnight/blob/master/TestFFT2D.py&gt;TestFFT2D.py &lt;/denchmark-link&gt;

`
import tensorflow as tf
import numpy as np
def get_loss(sess,net,img_ref,layer):
&lt;denchmark-code&gt;total_loss  = 0.

sess.run(net['input'].assign(img_ref))  
x = net[layer]
a = sess.run(net[layer])
x = tf.transpose(x, [0,3,1,2])
a = tf.transpose(a, [0,3,1,2])
_,N,_,_ = a.shape
F_x = tf.fft2d(tf.complex(x,0.))
F_x_conj = tf.conj(F_x)
F_a = tf.fft2d(tf.complex(a,0.))
F_a_conj = tf.conj(F_a)

for i in range(N):
    inter_corr_x = tf.multiply(F_x,F_x_conj)
    inter_corr_a = tf.multiply(F_a,F_a_conj)
    
    ifft2_corr_x = tf.ifft2d(inter_corr_x)
    ifft2_corr_a = tf.ifft2d(inter_corr_a)
    
    R_x = tf.real(ifft2_corr_x)
    R_a = tf.real(ifft2_corr_a)
    
    style_loss = tf.nn.l2_loss(tf.subtract(R_x,R_a))  
    total_loss += style_loss
    
    # Shift the tensor from on 1 unit on the dimension 1
    F_x = tf.concat([tf.expand_dims(F_x[:,-1,:,:],0), F_x[:,:-1,:,:]], axis=1)
    F_a = tf.concat([tf.expand_dims(F_a[:,-1,:,:],0), F_a[:,:-1,:,:]], axis=1)
        
return(total_loss)
&lt;/denchmark-code&gt;

def main(args):
&lt;denchmark-code&gt;# Definition of the first operations :
height, width, numberChannels = 400,300,3
net = {}
current = tf.Variable(np.zeros((1, height, width, numberChannels), dtype=np.float32))
net['input'] = current
kernel = tf.constant(np.random.uniform(low=-1,high=1,size=(400,300,3,64)),dtype=np.float32)
conv = tf.nn.conv2d(current, kernel, strides=(1, 1, 1, 1),padding='SAME',name='conv')
bias = tf.constant(np.random.uniform(low=-1,high=1,size=(64)),dtype=np.float32)
conv_add_bias = tf.nn.bias_add(conv, bias)
net['conv1_1'] = conv_add_bias

img_ref = np.random.uniform(low=-128,high=128,size=(1, height, width, numberChannels))
init_img = np.random.uniform(low=-128,high=128,size=(1, height, width, numberChannels))

sess = tf.Session()

sess.run(net['input'].assign(img_ref))  
# Definition of the loss 
loss = get_loss(sess,net,img_ref,'conv1_1')
    
# Preparation of the assignation operation
placeholder = tf.placeholder(tf.float32, shape=init_img.shape)
assign_op = net['input'].assign(placeholder)
    
sess.run(tf.global_variables_initializer())
sess.run(assign_op, {placeholder: init_img})
print("Before loss evaluation")
loss_evaluation = sess.run(loss)
print("loss_evaluation",loss_evaluation)

return(0)
&lt;/denchmark-code&gt;

if name == 'main':
import sys
sys.exit(main(sys.argv))
`
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


**Have I written custom code **: TestFFT2D.py
OS Platform and Distribution: Linux Ubuntu 16.04
TensorFlow installed from: Source
TensorFlow version: v1.0.0-65-g4763edf-dirty 1.0.1
CUDA/cuDNN version:
Cuda compilation tools, release 7.5, V7.5.17 and  cuDNN  5 with the GeForce GTX 680
Cuda compilation tools, release 8.0, V8.0.61and cuDNN 5 with the GeForce GTX 1080
GPU model and memory: Tested on GeForce GTX 680 with 2Go and on GeForce GTX 1080 with 12Go
** Python version**: Python 3.6.1 |Anaconda 4.4.0 (64-bit)|
Exact command to reproduce: python TestFFT2D.py : copy-paste the code above and run it with python 3

	</description>
	<comments>
		<comment id='1' author='nicaogr' date='2017-06-16T19:31:21Z'>
		It looks like you are compiling off the 1.0 branch.  Could you try reproducing this with master (or at least 1.2)?
		</comment>
		<comment id='2' author='nicaogr' date='2017-06-19T12:04:52Z'>
		I have updated my Tensorflow version 1.0.1 to the version 1.2 and I can't reproduce those error with the former script : &lt;denchmark-link:https://github.com/nicaogr/silver-octo-fortnight/blob/master/TestFFT2D.py&gt;TestFFT2D.py&lt;/denchmark-link&gt;

But when I increase the size of the tensor I used (from 400 * 300 to 800 * 600), I got an Internal Error message again :
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/1084781/InternalError2.txt&gt;InternalError2.txt&lt;/denchmark-link&gt;

You can test with this script : &lt;denchmark-link:https://github.com/nicaogr/silver-octo-fortnight/blob/master/TestFFT2D_1.py&gt;TestFFT2D_1.py&lt;/denchmark-link&gt;
 (I also reduce the size of the kernel filters).
I can get the same type of error when I increase the number of filters I used.
Do you advice me to present my problem on StackOverflow instead of Github ?
		</comment>
		<comment id='3' author='nicaogr' date='2017-06-19T18:19:10Z'>
		I think that may be an out-of-memory error, as seen here: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/8710&gt;#8710&lt;/denchmark-link&gt;
; same
return code.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, Jun 19, 2017 at 5:05 AM, Gonthier Nicolas ***@***.***&gt; wrote:
 I have updated my Tensorflow version 1.0.1 to the version 1.2 and I can't
 reproduce those error with the former script : TestFFT2D.py
 &lt;https://github.com/nicaogr/silver-octo-fortnight/blob/master/TestFFT2D.py&gt;

 But when I increase the size of the tensor I used (from 400 * 300 to 800 *
 600), I got an Internal Error message again :
 InternalError2.txt
 &lt;https://github.com/tensorflow/tensorflow/files/1084781/InternalError2.txt&gt;
 You can test with this script : TestFFT2D_1.py
 &lt;https://github.com/nicaogr/silver-octo-fortnight/blob/master/TestFFT2D_1.py&gt;
 (I also reduce the size of the kernel filters).

 I can get the same type of error when I increase the number of filters I
 used.

 Do you advice me to present my problem on StackOverflow instead of Github ?

 —
 You are receiving this because you commented.
 Reply to this email directly, view it on GitHub
 &lt;#10759 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AA1YDsLdIEIspSIkb-soRG7HC8PBjjGrks5sFmP5gaJpZM4N8Upp&gt;
 .



		</comment>
		<comment id='4' author='nicaogr' date='2017-06-20T01:06:55Z'>
		Yup, as &lt;denchmark-link:https://github.com/wolffg&gt;@wolffg&lt;/denchmark-link&gt;
 points out, this looks like the same out-of-memory error as &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/8710&gt;#8710&lt;/denchmark-link&gt;
.  Closing this out, since it doesn't look like this is a bug in the core framework.
		</comment>
	</comments>
</bug>