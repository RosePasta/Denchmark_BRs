<bug id='37501' author='talhaanwarch' open_date='2020-03-11T06:27:11Z' closed_time='2020-06-19T06:32:46Z'>
	<summary>WARNING:tensorflow:Gradients do not exist for variables</summary>
	<description>
While training BERT on TPU i am getting these warnings and my precision and recall is zero while accuracy is 100
&lt;denchmark-code&gt;Train for 45205 steps, validate for 206 steps
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.

&lt;/denchmark-code&gt;

Here is google colab file i am using
&lt;denchmark-link:https://colab.research.google.com/drive/1l0Eoram6vJRK5xQBmnCx-eU1e46higlo&gt;https://colab.research.google.com/drive/1l0Eoram6vJRK5xQBmnCx-eU1e46higlo&lt;/denchmark-link&gt;

updated colab file
&lt;denchmark-link:https://colab.research.google.com/drive/1D-eKgddRHROG39R2iyqE3qR7w0N0OHAB&gt;https://colab.research.google.com/drive/1D-eKgddRHROG39R2iyqE3qR7w0N0OHAB&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='talhaanwarch' date='2020-03-11T14:29:41Z'>
		&lt;denchmark-link:https://github.com/talhaanwarch&gt;@talhaanwarch&lt;/denchmark-link&gt;
,
I was unable to reproduce the issue as the file  is missing. Please find the gist of it &lt;denchmark-link:https://colab.sandbox.google.com/gist/amahendrakar/0768ebd3095d4a8410ba8407f45e595e/37501.ipynb&gt;here&lt;/denchmark-link&gt;
.
Could you please share all the supporting files required to run the given code? Thanks!
		</comment>
		<comment id='2' author='talhaanwarch' date='2020-03-12T10:19:55Z'>
		&lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;
  please check this file, i have updated.
&lt;denchmark-link:https://colab.research.google.com/drive/1D-eKgddRHROG39R2iyqE3qR7w0N0OHAB&gt;https://colab.research.google.com/drive/1D-eKgddRHROG39R2iyqE3qR7w0N0OHAB&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='talhaanwarch' date='2020-03-13T07:53:07Z'>
		&lt;denchmark-link:https://github.com/talhaanwarch&gt;@talhaanwarch&lt;/denchmark-link&gt;
,
On running the updated colab gist, I got the metrics . Please find the gist of it &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/5c2225faee2306e9fce0550bfc55aec1/37501-v2.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='4' author='talhaanwarch' date='2020-03-13T10:10:06Z'>
		but the warning is still there. WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.  I am thinking it should be an error instead of warning. becuase if gradient donot exist, how it will be updated?
		</comment>
		<comment id='5' author='talhaanwarch' date='2020-03-19T21:11:37Z'>
		&lt;denchmark-link:https://github.com/talhaanwarch&gt;@talhaanwarch&lt;/denchmark-link&gt;
 Please take a look at this &lt;denchmark-link:https://github.com/huggingface/transformers/issues/2256&gt;issue&lt;/denchmark-link&gt;
 which is very similar to this issue.
		</comment>
		<comment id='6' author='talhaanwarch' date='2020-03-30T17:20:44Z'>
		&lt;denchmark-link:https://github.com/talhaanwarch&gt;@talhaanwarch&lt;/denchmark-link&gt;
 Can yo please respond to the above comment so that we can take the discussion further. Thanks!
		</comment>
		<comment id='7' author='talhaanwarch' date='2020-03-31T01:09:53Z'>
		i requested at &lt;denchmark-link:https://github.com/huggingface/transformers/issues/2256&gt;issue &lt;/denchmark-link&gt;
 if he can update his colab notebook with solution, but for me i am unable to figure out the solution
		</comment>
		<comment id='8' author='talhaanwarch' date='2020-04-22T13:07:18Z'>
		I'm getting same error. No solution found so far.
		</comment>
		<comment id='9' author='talhaanwarch' date='2020-06-03T08:33:36Z'>
		same error founded.
		</comment>
		<comment id='10' author='talhaanwarch' date='2020-06-03T21:58:27Z'>
		same error for me
		</comment>
		<comment id='11' author='talhaanwarch' date='2020-06-04T19:15:40Z'>
		&lt;denchmark-link:https://github.com/talhaanwarch&gt;@talhaanwarch&lt;/denchmark-link&gt;
 Please post your issue &lt;denchmark-link:https://github.com/google-research/bert/issues&gt;here&lt;/denchmark-link&gt;
. Is this happening only on TPUs ? or is it happening on GPUs too?
		</comment>
		<comment id='12' author='talhaanwarch' date='2020-06-10T08:31:41Z'>
		I have a similar warning message using TF2.2 on Colab GPU:
WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss.
I figured though that the Variables are still being updated.
Is this warning ignorable?
		</comment>
		<comment id='13' author='talhaanwarch' date='2020-06-12T01:42:45Z'>
		&lt;denchmark-link:https://github.com/gowthamkpr&gt;@gowthamkpr&lt;/denchmark-link&gt;
  i am getting this error on TPU only
		</comment>
		<comment id='14' author='talhaanwarch' date='2020-06-17T17:35:50Z'>
		&lt;denchmark-link:https://github.com/talhaanwarch&gt;@talhaanwarch&lt;/denchmark-link&gt;
 The warning occurs on GPU too. Its not specific to TPU. Please find the the gist &lt;denchmark-link:https://colab.research.google.com/gist/gowthamkpr/88af1c86ace8bdcaf82e5c7228d83b31/37501-v2.ipynb&gt;here&lt;/denchmark-link&gt;
. As mentioned in this &lt;denchmark-link:https://github.com/huggingface/transformers/issues/1727#issuecomment-568058704&gt;issue comment&lt;/denchmark-link&gt;
, this is not an bug. It means that these variables are not updated during training.
If you have more questions, please post the issue &lt;denchmark-link:https://github.com/google-research/bert/issues&gt;here&lt;/denchmark-link&gt;
 or in stackoverflow. Thanks!
		</comment>
		<comment id='15' author='talhaanwarch' date='2020-06-19T06:32:48Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37501&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37501&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='16' author='talhaanwarch' date='2020-08-11T06:40:53Z'>
		I'm also getting this warning ! my embedding layers and attention layers are not updating and remains fixed as initialized. if anyone have solution please tell me.
		</comment>
		<comment id='17' author='talhaanwarch' date='2020-08-11T06:46:34Z'>
		&lt;denchmark-code&gt;
embd=Embedding(input_dim=len(vocab),output_dim=100,name="embd")
lstm1=Bidirectional(LSTM(units=100,return_sequences=True,name="lstm1"),name="bd1")
lstm2=Bidirectional(LSTM(units=100,return_sequences=True,name="lstm2"),name="bd2")
attention_layer=Attention_Model(21,200)
dense1=Dense(units=80,name="dense1",kernel_regularizer="l2")
dropout1=Dropout(0.5)
act1=Activation('sigmoid')

dense2=Dense(units=50,name="dense2",kernel_regularizer="l2")
dropout2=Dropout(0.4)
act2=Activation('sigmoid')

dense3=Dense(units=30,name="dense3",kernel_regularizer="l2")
dropout3=Dropout(0.3)
act3=Activation('sigmoid')

dense4=Dense(units=len(classes),name="dense4")
dropout4=Dropout(0.2)
output=Activation('softmax')

&lt;/denchmark-code&gt;

Forward Pass :
&lt;denchmark-code&gt;def forward_pass(X):
  t=embd(X)
 
  t=lstm1(t)
  
  t=lstm2(t)
  

 
  t=attention_layer(t)
  
  
  t=dense1(t)
  t=dropout1(t)
  t=act1(t)

  t=dense2(t)
  t=dropout2(t)
  t=act2(t)

  t=dense3(t)
  t=dropout3(t)
  t=act3(t)
  
  t=dense4(t)
  t=dropout4(t)
  t=output(t)

  return t


&lt;/denchmark-code&gt;

Attention Model :
&lt;denchmark-code&gt;
class Attention_Model():
  def __init__(self,seq_length,units):
    self.seq_length=seq_length
    self.units=units
    self.lstm=LSTM(units=units,return_sequences=True,return_state=True)
    

  def get_lstm_s(self,seq_no):
    input_lstm=tf.expand_dims(tf.reduce_sum(self.X*(self.alphas[:,:,seq_no:seq_no+1]),axis=1),axis=1)
    a,b,c=self.lstm(input_lstm)
    self.output[:,seq_no,:]=a[:,0,:]

    return b

  def __call__(self,X):
    self.X=X
    self.output=np.zeros(shape=(self.X.shape[0],self.seq_length,self.units))
    self.dense=Dense(units=self.seq_length)
    self.softmax=Softmax(axis=1)
    

    for i in range(self.seq_length+1):
      if i==0 :
        s=np.zeros(shape=(self.X.shape[0],self.units))
      else :
        s=self.get_lstm_s(i-1)
      if(i==self.seq_length):
        break 
      
      s=RepeatVector(self.X.shape[1])(s)
      concate_X=np.concatenate([self.X,s],axis=-1)
      
      self.alphas=self.softmax(self.dense(concate_X))

    return self.output
      
&lt;/denchmark-code&gt;

is anything wrong with implementation or something else ?
		</comment>
		<comment id='18' author='talhaanwarch' date='2020-08-16T17:04:05Z'>
		&lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;

how to remove these WARNING from output Info ???
		</comment>
		<comment id='19' author='talhaanwarch' date='2020-08-18T17:36:08Z'>
		&lt;denchmark-link:https://github.com/SmileTM&gt;@SmileTM&lt;/denchmark-link&gt;
,
To disable the warning logs, try changing the log level using the below code.
&lt;denchmark-code&gt;import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' 
import tensorflow as tf
&lt;/denchmark-code&gt;

If you need further help, please submit a new issue from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/new/choose&gt;this link&lt;/denchmark-link&gt;
 and fill in the template, so that we can track the issue there. Thanks!
		</comment>
	</comments>
</bug>