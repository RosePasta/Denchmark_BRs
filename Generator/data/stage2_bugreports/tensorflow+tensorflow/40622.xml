<bug id='40622' author='swghosh' open_date='2020-06-19T19:01:58Z' closed_time='2020-08-17T21:04:35Z'>
	<summary>tf.tpu.experimental.initialize_tpu_system fails to work on nightly builds</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colaboratory (Linux)
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): tf-nightly_v2.3.0.dev20200619
Python version: v3.6.9
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
TPU: Google Colab runtime with TPU accelerator

Describe the current behavior
tf.tpu.experimental.initialize_tpu_system(tpu_cluster_resolver) raises an unexpected error. The stack trace containing this error is provided underneath.
The sample notebook that was being used to train an  model with  containing the error message is provided &lt;denchmark-link:https://colab.research.google.com/drive/1twKOkGxWO8NpYIE_SZ2qroVNEyGCxQrS?usp=sharing&gt;here&lt;/denchmark-link&gt;
.

A ResNet50 model (as EfficientNetB0 is only present in TF-nightly) with similar code is able to run successfully with TPUStrategy and there are no such errors reported while calling .  A notebook with the corresponding training code for TFv2.2 can be found &lt;denchmark-link:https://colab.research.google.com/drive/1BPsM3Gh1d6-nXY0urp0AFuSm2WKlYorv?usp=sharing&gt;here&lt;/denchmark-link&gt;
.
Standalone code to reproduce the issue
Just calling tf.tpu.experimental.initialize_tpu_system using the standard mechanism on a Colab runtime with TPU should suffice.
tpu_url = 'grpc://' + os.environ['COLAB_TPU_ADDR']
tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_url)

tf.config.experimental_connect_to_cluster(tpu_cluster_resolver)
tf.tpu.experimental.initialize_tpu_system(tpu_cluster_resolver)

strategy = tf.distribute.experimental.TPUStrategy(tpu_cluster_resolver)
Other info / logs
Running on TPU  ['10.57.138.26:8470']
INFO:tensorflow:Initializing the TPU system: grpc://10.57.138.26:8470

INFO:tensorflow:Initializing the TPU system: grpc://10.57.138.26:8470

INFO:tensorflow:Clearing out eager caches

INFO:tensorflow:Clearing out eager caches

---------------------------------------------------------------------------

InvalidArgumentError                      Traceback (most recent call last)

&lt;ipython-input-4-a42f01f7e70e&gt; in &lt;module&gt;()
      6 
      7 tf.config.experimental_connect_to_cluster(tpu)
----&gt; 8 tf.tpu.experimental.initialize_tpu_system(tpu)
      9 tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)

3 frames

/usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu_strategy_util.py in initialize_tpu_system(cluster_resolver)
    101     context.context()._clear_caches()  # pylint: disable=protected-access
    102 
--&gt; 103     serialized_topology = output.numpy()
    104 
    105     # TODO(b/134094971): Remove this when lazy tensor copy in multi-device

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in numpy(self)
   1061     """
   1062     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.
-&gt; 1063     maybe_arr = self._numpy()  # pylint: disable=protected-access
   1064     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr
   1065 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _numpy(self)
   1029       return self._numpy_internal()
   1030     except core._NotOkStatusException as e:  # pylint: disable=protected-access
-&gt; 1031       six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access
   1032 
   1033   @property

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: NodeDef expected inputs 'string' do not match 0 inputs specified; Op&lt;name=_Send; signature=tensor:T -&gt; ; attr=T:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true&gt;; NodeDef: {{node _Send}}
Typically, this bug is prevalent only on nightly builds (master branch) and not on TF v2.2 release.
/cc: &lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='swghosh' date='2020-06-21T07:49:20Z'>
		I have seen this issue on TF 2.2.0. I think there is something lingering on the TPU cores that initialize_tpu_system is unable to clear out.
		</comment>
		<comment id='2' author='swghosh' date='2020-06-22T07:10:33Z'>
		I have tried in colab with TF version 2.2 and i am not seeing any issue.However i am seeing this issue with nightly versions(`2.3.0-dev20200621').Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/f1c56122bf30a1bdadb7c72e7700e632/untitled54.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='3' author='swghosh' date='2020-06-22T07:31:40Z'>
		Yes &lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
, thanks for acknowledging.
		</comment>
		<comment id='4' author='swghosh' date='2020-07-05T22:18:52Z'>
		It is due to a version mismatch between your client and TPU worker. Did you use nightly on both instances?
		</comment>
		<comment id='5' author='swghosh' date='2020-07-06T15:04:46Z'>
		Hi &lt;denchmark-link:https://github.com/rxsang&gt;@rxsang&lt;/denchmark-link&gt;
, not sure if it's an error regarding the TPU version mismatch between the worker and the client. I had encountered the error on Colab. (Colab TPU runtimes do not provide for choice of versions for the TPU workers) I might need to try the same on Compute Engine Cloud TPUs for checking operability amongst different TPU worker versions.
AFAIK client and worker versions are interoperable as previously I had used TPU workers running TFv1.15 with TFv2.x, so I think.
Thanks.
		</comment>
		<comment id='6' author='swghosh' date='2020-07-06T22:05:46Z'>
		&lt;denchmark-link:https://github.com/swghosh&gt;@swghosh&lt;/denchmark-link&gt;
 Yeah, Colab always defaults to the latest stable version of TPUs because of technical limitations. The current version of Colab TPU that runs is version 2.2, so Colab TPUs are only supported with TensorFlow v2.2 by default.
In general, we only support TPUs when the same version is used on both the user TF and on the TPUs. In practice, there is some scope for different versions working together if there are no changes in terms of protocol or op definitions (e.g. no new ops, ops didn't change signature, etc...) but it is not supported.
		</comment>
		<comment id='7' author='swghosh' date='2020-07-06T22:11:03Z'>
		&lt;denchmark-link:https://github.com/frankchn&gt;@frankchn&lt;/denchmark-link&gt;
 Thanks for the clarification. Much help.
		</comment>
		<comment id='8' author='swghosh' date='2020-07-31T09:38:28Z'>
		
@swghosh Yeah, Colab always defaults to the latest stable version of TPUs because of technical limitations. The current version of Colab TPU that runs is version 2.2, so Colab TPUs are only supported with TensorFlow v2.2 by default.
In general, we only support TPUs when the same version is used on both the user TF and on the TPUs. In practice, there is some scope for different versions working together if there are no changes in terms of protocol or op definitions (e.g. no new ops, ops didn't change signature, etc...) but it is not supported.

&lt;denchmark-link:https://github.com/frankchn&gt;@frankchn&lt;/denchmark-link&gt;

I found that tf 2.3 has been released, is there an update plan for the tf version on colab?
		</comment>
		<comment id='9' author='swghosh' date='2020-07-31T17:26:07Z'>
		The Colab roll out process lags the TensorFlow one by a few days, but it is being rolled out right now and TF 2.3 will be usable by the end of this week. When that happens, TF 2.2 will not be usable any more because of the same incompatibility between versions.
		</comment>
		<comment id='10' author='swghosh' date='2020-08-01T04:19:04Z'>
		Ok, thanks frank.
		</comment>
		<comment id='11' author='swghosh' date='2020-08-06T02:28:35Z'>
		still got following error with tf 2.3.0  tpu on colab.
&lt;denchmark-code&gt;tensorflow.python.framework.errors_impl.InternalError: RET_CHECK failure (learning/brain/google/xla/distributed_tpu_rewrite_pass.cc:1413) arg_shape.handle_type != DT_INVALID 
2020-08-06 01:47:30.383712: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 36311, Output num: 0
Additional GRPC error information from remote target /job:worker/replica:0/task:0:
:{"created":"@1596678450.383612323","description":"Error received from peer ipv4:10.81.58.138:8470","file":"external/com_github_grpc_grpc/src/core/lib/surface/call.cc","file_line":1056,"grpc_message":"Unable to find the relevant tensor remote_handle: Op ID: 36311, Output num: 0","grpc_status":3}
&lt;/denchmark-code&gt;

Please, find the &lt;denchmark-link:https://colab.research.google.com/gist/feiwofeifeixiaowo/a7ef19b4cb13517a18eec4c6786ac6e0/tpu-train-delf.ipynb#scrollTo=jmU5sTUR9Gxw&gt;gist &lt;/denchmark-link&gt;
here.Thanks!
		</comment>
		<comment id='12' author='swghosh' date='2020-08-06T02:37:36Z'>
		From the trace, it seems you enabled tf summary inside TPU computation. Can you try calling tf.config.set_soft_device_placement(True) at the beginning, so outside compilation can be enabled?
		</comment>
		<comment id='13' author='swghosh' date='2020-08-17T21:04:36Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40622&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40622&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='swghosh' date='2020-11-18T08:41:28Z'>
		Now, it seems to initialize well with 2.3.0 and nightly with Colab. However, whenever I run gcloud ai-platform training scripts, it always produces the same error whatever version of tf I use.. It is really frustrating. If it is all about the version mismatch, I can't find any single clue for solution anywhere.
		</comment>
		<comment id='15' author='swghosh' date='2020-11-19T21:44:07Z'>
		I'm not familiar with the  scripts, which TF and TPU version do that script use? &lt;denchmark-link:https://github.com/gagika&gt;@gagika&lt;/denchmark-link&gt;
 Gigik, anyone from Cloud TPU team could help with that?
		</comment>
		<comment id='16' author='swghosh' date='2020-11-19T22:31:33Z'>
		For choosing matching Tensorflow version on TPU in colab and kaggle you can use the code below (see &lt;denchmark-link:https://stackoverflow.com/questions/63180202/invalidargumenterror-while-initializing-ttpu/63271669#63271669&gt;here&lt;/denchmark-link&gt;
)
&lt;denchmark-code&gt;!pip install cloud-tpu-client

import tensorflow as tf
from cloud_tpu_client import Client
print(tf.__version__)

Client().configure_tpu_version(tf.__version__, restart_type='ifNeeded')
&lt;/denchmark-code&gt;

For using training on TPUs on ai platform you can follow the steps &lt;denchmark-link:https://cloud.google.com/ai-platform/training/docs/using-tpus&gt;here&lt;/denchmark-link&gt;
. At the moment AI supports TF versions .
		</comment>
	</comments>
</bug>