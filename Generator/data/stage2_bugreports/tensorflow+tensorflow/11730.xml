<bug id='11730' author='villanuevab' open_date='2017-07-25T01:05:27Z' closed_time='2017-10-30T01:10:45Z'>
	<summary>Using XLA JIT Compilation results in bad_alloc error</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): v1.0.1-0-ge895d5c-dirty 1.0.1
Python version: Python 3.5.2
Bazel version (if compiling from source): Build label: 0.4.5- (@non-git)
CUDA/cuDNN version: 8.0/5.1
GPU model and memory: NVIDIA TX2
Exact command to reproduce: python3 script.py

&lt;denchmark-code&gt;# Config to turn on XLA JIT compilation
config = tf.ConfigProto()
config.graph_options.optimizer_options.global_jit_level = \
    tf.OptimizerOptions.ON_1
with tf.Session(config=config) as sess:
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Bug in XLA JIT compilation. Script works as expected without assigning config with JIT optimizer option.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

Changing the following code:
&lt;denchmark-code&gt;with tf.Session() as sess:
&lt;/denchmark-code&gt;

To:
&lt;denchmark-code&gt;# Config to turn on XLA JIT compilation
config = tf.ConfigProto()
config.graph_options.optimizer_options.global_jit_level = \
    tf.OptimizerOptions.ON_1
with tf.Session(config=config) as sess:
&lt;/denchmark-code&gt;

Results in the following error at runtime:
&lt;denchmark-code&gt;nvidia@tegra-ubuntu:~/dev$ python3 script.py
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:873] ARM has no NUMA node, hardcoding to return zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GP10B
major: 6 minor: 2 memoryClockRate (GHz) 1.3005
pciBusID 0000:00:00.0
Total memory: 7.67GiB
Free memory: 4.32GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GP10B, pci bus id: 0000:00:00.0)
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 4.03G (4323303424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 6 visible devices
I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:
I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 6 visible devices
I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform CUDA. Devices:
I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): GP10B, Compute Capability 6.2
terminate called after throwing an instance of 'std::bad_alloc'
  what():  std::bad_alloc
Aborted (core dumped)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='villanuevab' date='2017-07-25T04:17:28Z'>
		Can you try different values for per_process_gpu_memory_fraction in the following code and try to reproduce your code?
The following code will allocate GPU memory with   Total available GPU memory * per_process_gpu_memory_fraction
&lt;denchmark-code&gt;gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)
sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='villanuevab' date='2017-07-25T18:56:51Z'>
		&lt;denchmark-link:https://github.com/printdhruv&gt;@printdhruv&lt;/denchmark-link&gt;
, doing so results in no CUDA OOM errors, but the process still mysteriously quits.
&lt;denchmark-code&gt;# explicitly limit per_process_gpu_memory_fraction
gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3)
# config to turn on XLA JIT compilation
config = tf.ConfigProto(gpu_options=gpu_options)
config.graph_options.optimizer_options.global_jit_level = \
    tf.OptimizerOptions.ON_1
&lt;/denchmark-code&gt;

See the error log below:
&lt;denchmark-code&gt;I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:873] ARM has no NUMA node, hardcoding to return zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GP10B
major: 6 minor: 2 memoryClockRate (GHz) 1.3005
pciBusID 0000:00:00.0
Total memory: 7.67GiB
Free memory: 4.04GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GP10B, pci bus id: 0000:00:00.0)
I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 6 visible devices
I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:
I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 6 visible devices
I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform CUDA. Devices:
I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): GP10B, Compute Capability 6.2
Killed
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='villanuevab' date='2017-07-31T05:55:35Z'>
		&lt;denchmark-link:https://github.com/tatatodd&gt;@tatatodd&lt;/denchmark-link&gt;
, any ideas?
		</comment>
		<comment id='4' author='villanuevab' date='2017-10-30T01:10:53Z'>
		This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!
		</comment>
	</comments>
</bug>