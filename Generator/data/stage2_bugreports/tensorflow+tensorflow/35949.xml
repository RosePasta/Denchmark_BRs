<bug id='35949' author='nbro' open_date='2020-01-16T16:32:58Z' closed_time='2020-06-22T18:37:50Z'>
	<summary>AttributeError: 'Tensor' object has no attribute '_datatype_enum'</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X Catalina (10.15.2)
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0
Python version: 3.7.5
GPU model and memory: Intel Iris Pro 1536 MB

Describe the current behavior
I get the errors

tensorflow.python.eager.core._FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.

then

AttributeError: 'Tensor' object has no attribute '_datatype_enum'

and then

AttributeError: 'ProgbarLogger' object has no attribute 'log_values'

when I add the following callback to the list of callbacks of my_model.fit
&lt;denchmark-code&gt;my_callback = tf.keras.callbacks.LambdaCallback(on_batch_begin=lambda batch, logs: tf.print(my_model.losses))
&lt;/denchmark-code&gt;

Describe the expected behavior
No error.
Code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf


def get_model():
    inp = tf.keras.layers.Input(shape=(1,))
    x = tf.keras.layers.Dense(8, activity_regularizer=tf.keras.regularizers.l1(0.01))(inp)
    x = tf.keras.layers.Dense(16, activity_regularizer=tf.keras.regularizers.l1(0.01))(x)
    out = tf.keras.layers.Dense(1)(x)
    model = tf.keras.Model(inputs=inp, outputs=out)
    return model


def train():
    my_model = get_model()
    my_model.compile(optimizer="adam", loss="mse")
    my_callback = tf.keras.callbacks.LambdaCallback(on_batch_begin=lambda batch, logs: tf.print(my_model.losses))
    my_model.fit([1, 2, 3, 4], [0.1, 0.2, 0.4, 0.2], callbacks=[my_callback])


if __name__ == '__main__':
    train()
&lt;/denchmark-code&gt;

This issue may be related to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/28924&gt;#28924&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29931&gt;#29931&lt;/denchmark-link&gt;
. Note that, if I don't use any regulariser,  prints an empty list and no error occurs.
	</description>
	<comments>
		<comment id='1' author='nbro' date='2020-01-17T09:07:49Z'>
		Issue replicating for 2.0 and &lt;denchmark-link:https://colab.sandbox.google.com/gist/oanush/f48bb5fa2028ece28e5fcee88551e852/35949.ipynb&gt;tf-nightly&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='2' author='nbro' date='2020-01-17T19:24:23Z'>
		You may use print instead of tf.print in LambdaCallback like,
my_callback = tf.keras.callbacks.LambdaCallback(on_batch_begin=lambda batch, logs: print(my_model.losses))
Output:
Train on 4 samples
[&lt;tf.Tensor 'dense_6/ActivityRegularizer/truediv:0' shape=() dtype=float32&gt;, &lt;tf.Tensor 'dense_7/ActivityRegularizer/truediv:0' shape=() dtype=float32&gt;]
4/4 [==============================] - 2s 432ms/sample - loss: 1.7403
		</comment>
		<comment id='3' author='nbro' date='2020-01-17T19:32:18Z'>
		&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
 In the example above, I am trying to print the  property, but, actually, I wanted to print the output of a layer.
Here's a complete working example to reproduce the same error when attempting to print out the values of the outputs of a layer.
&lt;denchmark-code&gt;import tensorflow as tf


def get_model():
    inp = tf.keras.layers.Input(shape=(1,))
    x = tf.keras.layers.Dense(8, activity_regularizer=tf.keras.regularizers.l1(0.01))(inp)
    x = tf.keras.layers.Dense(16, activity_regularizer=tf.keras.regularizers.l1(0.01))(x)
    out = tf.keras.layers.Dense(1)(x)
    model = tf.keras.Model(inputs=inp, outputs=out)
    model.summary()
    return model


def train():
    my_model = get_model()
    my_model.compile(optimizer="adam", loss="mse")
    my_callback = tf.keras.callbacks.LambdaCallback(
        on_batch_begin=lambda batch, logs: tf.print(my_model.get_layer("dense").output))

    my_model.fit([1, 2, 3, 4], [0.1, 0.2, 0.4, 0.2], callbacks=[my_callback])


if __name__ == '__main__':
    train()
&lt;/denchmark-code&gt;

Note that I want to print out the values of the tensors and not their symbolic representation.
		</comment>
		<comment id='4' author='nbro' date='2020-01-18T02:04:09Z'>
		&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
 The documentation of  says

A TensorFlow operator that prints the specified inputs to a desired output stream or logging level. The inputs may be dense or sparse Tensors, primitive python objects, data structures that contain tensors, and printable Python objects. Printed tensors will recursively show the first and last elements of each dimension to summarize.

So are outputs of layers considered either sparse tensors or dense tensors? They should not be considered primitive Python objects, data structures that contain tensors or printable Python objects.
Anyway, my main goal is to be able to print out the activations for each layer of a model at the end of the mini-batch (thus while training the model). See also &lt;denchmark-link:https://stackoverflow.com/q/59789526/3924118&gt;https://stackoverflow.com/q/59789526/3924118&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='5' author='nbro' date='2020-03-01T11:54:25Z'>
		same issue with minimal example:
ar0 = Input(shape=(2,8,8,),name='ar0')
tf.print(ar0)
		</comment>
		<comment id='6' author='nbro' date='2020-04-14T13:34:04Z'>
		&lt;denchmark-link:https://github.com/Apsylem&gt;@Apsylem&lt;/denchmark-link&gt;

I had probably the same issue as you.
The reason IMHO is that the statement "tf.print(ar0)" is being executed. in the declaration phase of the tensorflow graph and not when the graph itself is being executed. In the declaration phase the tensors are not completely defined, they  have no real content.
I came over this by creating a small Print-Layer and attaching it to the graph.
You could do something like this:
&lt;denchmark-code&gt;
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Layer, Input

#The Printer-Layer class
class Print(Layer):
    verbose=True#False

    def __init__(self,message="", fn=lambda arg: arg,**kwargs):
        super(Print, self).__init__(**kwargs)
        self.message=message
        self.fn=fn
        
    def get_config(self):
        return super(Print, self).get_config()

    def call(self, inputs, *args, **kwargs):
        if Print.verbose:
          tf.print(self.message,self.fn(inputs))
        return super(Print, self).call(inputs, *args, **kwargs)
      
    @classmethod
    def print(cls,*args, **kwargs):    
        if Print.verbose:
          tf.print(*args, **kwargs)

#somewhere where you define your model :
def defineModel():
  ar0 = Input(shape=(2,8,8,),name='ar0')
  ar0Intermediate = Print("shape of ar0 =",fn=lambda x: tf.shape(x) )(ar0) #show the shape of ar0
  ar0Intermediate = Print("Content of ar0 = " )(ar0Intermediate) #show the content of ar0
  out = tf.keras.layers.Dense(1)(ar0Intermediate)
  model = tf.keras.Model(inputs=ar0, outputs=out)
  return model
  
def train():
    trainData=np.random.uniform(low=0,high=15.0,size=(4,2,8,8))
    testData=np.random.uniform(low=0,high=10.0,size=(4,2,8,8))

    my_model = defineModel()
    my_model.compile(optimizer="adam", loss="mse")
    my_model.fit(trainData, testData)

if __name__ == '__main__':
   Print.verbose=True #False
  train()
&lt;/denchmark-code&gt;

ATTENTION!
In the function defineModel() I did  ar0Intermediate = Print(...  by purpose!
If I had written  ar0 = Print(...  the tensorflow graph would have been broken because in
 model = tf.keras.Model(inputs=ar0, outputs=out) ar0 is used as input tensor.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4475613/GitHubIssue35950_Apsylem.txt&gt;GitHubIssue35950_Apsylem.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='nbro' date='2020-06-06T18:05:39Z'>
		Is there a simple solution to this issue?
Same problem here in tensorflow 2.2.0. I'm not able to print the values of a tensor in my loss function used within a keras model.
		</comment>
		<comment id='8' author='nbro' date='2020-06-09T07:58:15Z'>
		&lt;denchmark-link:https://github.com/omalleyt12&gt;@omalleyt12&lt;/denchmark-link&gt;
 We might expect tf.print to be wrapped by TensorFlowOpLayer?
		</comment>
		<comment id='9' author='nbro' date='2020-06-16T06:38:33Z'>
		I have tried in colab with TF version 2.2, nightly version and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/cf46faed2315a686d2c7aa5789b4626c/untitled26.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='10' author='nbro' date='2020-06-16T07:16:03Z'>
		&lt;denchmark-link:https://github.com/tomerk&gt;@tomerk&lt;/denchmark-link&gt;
 this is likely something s fixes
		</comment>
		<comment id='11' author='nbro' date='2020-06-22T18:37:50Z'>
		So, some context on what's happening here:
tf.print is meant to mimic python printing, and does not return values.
If you do x = print(...) x holds no value. x = tf.print(...) acts the same way. If there was meant to be a return value it would be very ambiguous whether the returned value should be the inputs, or the printed string.
So, if you do:
&lt;denchmark-code&gt;a = tf.keras.Input(...)
b = layer(a)
c = tf.print(b)
&lt;/denchmark-code&gt;

 will not hold any value. So, putting  in the middle of your functional api construction won't automatically turn it into a "printing layer". We don't plan to change this because printing is not an identity operation, and thinking of printing as an identity is an anti-pattern. More context on the design of  can be found here: &lt;denchmark-link:https://github.com/tensorflow/community/blob/master/rfcs/20180824-tf-print-v2.md&gt;https://github.com/tensorflow/community/blob/master/rfcs/20180824-tf-print-v2.md&lt;/denchmark-link&gt;

During functional model construction tf.print will act like print and try to print the python object that symbolically represents that intermediate output.
If you want to observe intermediate values during the model call, and you're using functional model construction, then you have a few options:

Use a keras lambda layer to run a function that tf.prints the inputs and then returns them
Write a custom Printer layer that prints the inputs
include the intermediate outputs you want to inspect in the outputs of your functional model. You can then directly inspect them programmatically after calling your model

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

I'm closing the issue because the discussion has veered off from the initial AttributeError report. and it won't be easy to find for people who are looking for info about printing in Keras.
But, if the above three options don't meet your needs we encourage you to open a feature request specifically around extending the mechanisms for printingdebugging intermediate activations in Keras layers.
Some extra context about your needs when you open the feature request would also be useful, so we can understand why the correct solutions don't meet them.
E.g.
Are you looking to interactively debug the activations?
Would you like the model to always print?
Do you want printing/debugging to happen during actual fitting, or just when interacting with a model object batch by batch?
Do you need sporadic logging of intermediate activations that don't happen at each batch?
		</comment>
		<comment id='12' author='nbro' date='2020-06-22T18:37:54Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35949&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35949&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>