<bug id='33960' author='kierenj' open_date='2019-11-04T08:41:15Z' closed_time='2019-12-05T22:53:41Z'>
	<summary>Docker/Kubernetes memory limits not respected? OOMKilled when deployed to GCP</summary>
	<description>
&lt;denchmark-code&gt;== check python ===================================================
python version: 3.5.6
python branch: 
python build version: ('default', 'Aug 26 2018 21:41:56')
python compiler version: GCC 7.3.0
python implementation: CPython


== check os platform ===============================================
os: Linux
os kernel version: #1 SMP Tue Jul 2 22:58:16 UTC 2019
os release version: 4.9.184-linuxkit
os platform: Linux-4.9.184-linuxkit-x86_64-with-debian-buster-sid
linux distribution: ('debian', 'buster/sid', '')
linux os distribution: ('debian', 'buster/sid', '')
mac version: ('', ('', '', ''), '')
uname: uname_result(system='Linux', node='413655a3a81f', release='4.9.184-linuxkit', version='#1 SMP Tue Jul 2 22:58:16 UTC 2019', machine='x86_64', processor='x86_64')
architecture: ('64bit', '')
machine: x86_64


== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== check pips ===================================================
numpy                              1.15.2           
numpydoc                           0.9.1            
protobuf                           3.9.1            
tensorflow                         1.14.0           
tensorflow-estimator               1.14.0           
tensorflow-hub                     0.6.0            
tensorflow-serving-api             1.14.0           

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.version.VERSION = 1.14.0
tf.version.GIT_VERSION = v1.14.0-0-g87989f6
tf.version.COMPILER_VERSION = 4.8.4
Sanity check: array([1], dtype=int32)
        65:	find library=libpthread.so.0 [0]; searching
   (hundreds of lines follow here)
== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh.1: line 147: nvidia-smi: command not found

== cuda libs  ===================================================

== tensorflow installed from info ==================
Name: tensorflow
Version: 1.14.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /root/miniconda3/lib/python3.5/site-packages
Required-by: witwidget, tensorflow-serving-api, seldon-core

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 5, 6, 'final', 0)

== bazel version  ===============================================
Build label: 0.19.0
Build time: Mon Oct 29 14:35:30 2018 (1540823730)
Build timestamp: 1540823730
Build timestamp as int: 1540823730
&lt;/denchmark-code&gt;

Describe the current behavior
I'm using Kubeflow to deploy a simple Keras/TF model training job (two LSTM layers, 28000x150x27 input from CSV). The Dockerfile it produces is:
&lt;denchmark-code&gt;FROM gcr.io/deeplearning-platform-release/tf-cpu.1-14
WORKDIR /python_env
COPY requirements.txt .
RUN python3 -m pip install -r requirements.txt
COPY . .
&lt;/denchmark-code&gt;

...and the output at the top of this issue is from within the resulting container.
The container has a CPU limit of 7 cores and a memory limit of 26Gi (the host node has 8 cores and 30Gi).
By the 3rd of 20 epochs, after about 50 minutes, the container is killed by Kubernetes with OOMKilled. Looking at a graph of memory use, it increases linearly over the 50 minutes, and evidently ignores the limits.
I have previously trained this model on my laptop (8 cores / 16GB RAM), 20 epochs, without issue, so this looks to be related to the Docker or Kubernetes environment.
Describe the expected behavior
Memory limits are respected.
Code to reproduce the issue
The above Dockerfile, plus:
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM
from urllib.request import urlopen
import numpy as np
import importlib
import os

# here's how data is loaded
dataset = np.loadtxt(raw_data, delimiter=",")

# build model
model = Sequential()
model.add(LSTM(32, return_sequences=True))
model.add(LSTM(32))
model.add(Dense(32, activation='softmax'))
opt = tf.keras.optimizers.Adam(lr=1e-3, decay=1e-5)
model.compile(loss='sparse_categorical_crossentropy',
              optimizer=opt, metrics=['accuracy'])

# training (data has been loaded via np.loadtxt)
model.fit(
    training_dataset,
    training_labels,
    epochs=20,
    shuffle=True,
    validation_data=(validation_dataset, validation_labels)
)
	</description>
	<comments>
		<comment id='1' author='kierenj' date='2019-11-04T21:54:39Z'>
		Additionally, running the same image locally on my MBP (default Docker setup: 2GiB RAM allocated), the process exited with a Killed message.  Via dmesg after that:
&lt;denchmark-code&gt;[ 2045.760028] Out of memory: Kill process 2439 (python3) score 888 or sacrifice child
[ 2045.761277] Killed process 2439 (python3) total-vm:5014356kB, anon-rss:1879228kB, file-rss:0kB, shmem-rss:0kB
[ 2046.137806] oom_reaper: reaped process 2439 (python3), now anon-rss:0kB, file-rss:0kB, shmem-rss:0kB
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='kierenj' date='2019-11-05T18:38:02Z'>
		A quick note, when I use tensorflow/tensorflow:latest-py3, I do not experience the bug. Let me know if there's somewhere else I should raise this (it's gcr.io/deeplearning-platform-release/tf-cpu.1-14, GCP's purpose-built-for-GCP image, which doesn't work).
		</comment>
		<comment id='3' author='kierenj' date='2019-11-06T21:57:17Z'>
		&lt;denchmark-link:https://github.com/kierenj&gt;@kierenj&lt;/denchmark-link&gt;
 What file are you using i.e., 'raw_data' in your code?
		</comment>
		<comment id='4' author='kierenj' date='2019-11-21T12:32:07Z'>
		It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='5' author='kierenj' date='2019-11-22T08:43:42Z'>
		I'm very sorry, my time for this whole project was very limited, and I've worked around using tensorflow/tensorflow:latest-py3. I haven't managed to find even an hour to share more detail, though I know that is not helpful.
		</comment>
		<comment id='6' author='kierenj' date='2019-12-05T22:53:41Z'>
		I am gonna close this issue as it has been resolved. Please add additional comments and we can open this issue again. Thanks!
		</comment>
		<comment id='7' author='kierenj' date='2019-12-05T22:53:43Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33960&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33960&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>