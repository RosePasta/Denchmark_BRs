<bug id='40357' author='franksacco' open_date='2020-06-10T15:32:46Z' closed_time='2020-07-07T16:32:13Z'>
	<summary>[TF Lite] TfLiteGpuDelegate Init: CONV_2D: Unsupported data type for float32 tensor</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
TensorFlow installed from (source or binary): pip install tensorflow
TensorFlow version (or github SHA if from source): 2.2.0
TensorFlow Lite version: org.tensorflow:tensorflow-lite:2.2.0, org.tensorflow:tensorflow-lite-gpu:2.2.0


The code used to run the converter can be found in this notebook: &lt;denchmark-link:https://github.com/franksacco/where-is-wally/blob/master/converter.ipynb&gt;https://github.com/franksacco/where-is-wally/blob/master/converter.ipynb&lt;/denchmark-link&gt;
.
In particular, the part of interest is:
from tensorflow import lite

converter = lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [lite.Optimize.DEFAULT]
tflite_model = converter.convert()

with open('models/unet_v2.f1lo-b14-e60-lr0.001.44.optimized.tflite', 'wb') as f:
    f.write(tflite_model)
The output from the converter invocation
The optimized TFLite model is successfully generated.
Also, please include a link to the saved model or GraphDef

Trained model: https://github.com/franksacco/where-is-wally/raw/master/models/unet_v2.f1lo-b14-e60-lr0.001.44.hdf5
Optimized TFLite Model: https://github.com/franksacco/where-is-wally/raw/master/models/unet_v2.f1lo-b14-e60-lr0.001.44.optimized.tflite

The model uses &lt;denchmark-link:https://www.depends-on-the-definition.com/unet-keras-segmenting-images/&gt;this implementation&lt;/denchmark-link&gt;
 of the &lt;denchmark-link:https://arxiv.org/pdf/1505.04597.pdf&gt;U-Net&lt;/denchmark-link&gt;
 and takes a 256x256x3 image and outputs a 256x256x1 grayscale mask.
Failure details
The model without optimizations works perfectly.
The model with optimizations works correctly when executed on the CPU.
However, when I try to initialize the TFLite interpreter with the GPU delegate using the following code, I get the error java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: TfLiteGpuDelegate Init: CONV_2D: Unsupported data type for float32 tensor.
MappedByteBuffer model = FileUtil.loadMappedFile(
    activity, "unet_v2.f1lo-b14-e60-lr0.001.44.optimized.tflite"
);

Interpreter.Options options = new Interpreter.Options();
options.addDelegate(new GpuDelegate());

Interpreter interpreter = new Interpreter(model, options);
Any other info / logs
Traceback of the exception:
&lt;denchmark-code&gt;I/tflite: Created TensorFlow Lite delegate for GPU.
I/tflite: Initialized TensorFlow Lite runtime.
W/System.err: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: TfLiteGpuDelegate Init: CONV_2D: Unsupported data type for float32 tensor
W/System.err: TfLiteGpuDelegate Prepare: delegate is not initialized
    Node number 36 (TfLiteGpuDelegateV2) failed to prepare.
    Restored previous execution plan after delegate application failure.
W/System.err:     at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegates(NativeInterpreterWrapper.java:318)
W/System.err:     at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:82)
        at org.tensorflow.lite.NativeInterpreterWrapper.&lt;init&gt;(NativeInterpreterWrapper.java:63)
        at org.tensorflow.lite.Interpreter.&lt;init&gt;(Interpreter.java:237)
        at it.unipr.advmobdev.whereiswally.ModelExecutor.loadInterpreter(ModelExecutor.java:245)
        at it.unipr.advmobdev.whereiswally.ModelExecutor.run(ModelExecutor.java:160)
&lt;/denchmark-code&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

I noticed that in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/062cf92d066771ab3cf2910f125b0209c305eb2b&gt;this commit&lt;/denchmark-link&gt;
 was added the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/java/src/main/java/org/tensorflow/lite/gpu/GpuDelegate.java#L66-L76&gt;setQuantizedModelsAllowed()&lt;/denchmark-link&gt;
 method in the  class. This method is not available in my current version of the library.
Is it possible that the optimized / quantized model cannot be run with the GPU delegate? I was unable to find this information in the documentation.
	</description>
	<comments>
		<comment id='1' author='franksacco' date='2020-06-11T10:01:58Z'>
		Could you try nightly? It seems that your model works well with GPU on master.
&lt;denchmark-code&gt;dependencies {
    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'
    implementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'
}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='franksacco' date='2020-06-11T17:02:37Z'>
		I changed the version of TensorFlow Lite dependencies, now they are:

org.tensorflow:tensorflow-lite:0.0.0-nightly
org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly

With this update, the model works properly with the GPU but performance is worse: latency with GPU is remained pretty much the same but the time required on the CPU is more than twenty times greater than before.
I report you the latency measured for the execution of the model on a 256x256 image.



Model
TFLite version
CPU
GPU




Normal (8.6 MB)
2.2.0
1.790 s
0.404 s


Optimized (3.4 MB)
2.2.0
1.023 s
Not working


Normal
0.0.0-nightly
37.828 s
0.412 s


Optimized
0.0.0-nightly
27.643 s
0.423 s



Why isn't the latency of the optimized model running on GPU better than that of the normal model? And most importantly, why is CPU latency with the 0.0.0-nightly version worse?
		</comment>
		<comment id='3' author='franksacco' date='2020-06-15T07:30:04Z'>
		You can use benchmark tool with "--enable_op_profiling=true" to check how it's executed.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark&gt;https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark&lt;/denchmark-link&gt;

It seems that there is a regression on nightly. (I didn't see it when I tested)
What you said optimized model should be integer quantization.
&lt;denchmark-link:https://www.tensorflow.org/lite/performance/post_training_quantization&gt;https://www.tensorflow.org/lite/performance/post_training_quantization&lt;/denchmark-link&gt;

Integer quantization is mainly for CPU since CPU calculates integer better than float number. When you run it on GPU, it converts integer to float internally. So there is no much benefit except the size of model.
		</comment>
		<comment id='4' author='franksacco' date='2020-07-07T16:32:12Z'>
		Thank you for your answer and sorry for the late reply.

Integer quantization is mainly for CPU since CPU calculates integer better than float number. When you run it on GPU, it converts integer to float internally. So there is no much benefit except the size of model.

I didn't know this detail. From now, I won't execute an optimized model on a GPU.
Since I think the problem is solved, I close the issue.
PS: I think I will use the normal model with the 2.2.0 version of TFLite in my application because I obtained better results with it.
		</comment>
		<comment id='5' author='franksacco' date='2020-07-07T16:32:15Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40357&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40357&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>