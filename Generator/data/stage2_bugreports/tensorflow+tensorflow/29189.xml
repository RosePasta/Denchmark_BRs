<bug id='29189' author='mckinziebrandon' open_date='2019-05-30T22:00:04Z' closed_time='2019-07-24T04:23:23Z'>
	<summary>Keras LSTM does not work with tf.distribute [2.0]</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Minor tweak to tutorial code.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.
TensorFlow version (use command below): tf-nightly-gpu-2.0-preview
Python version: 3.6
CUDA/cuDNN version: 10.0/7.4

I copy-pasted &lt;denchmark-link:https://www.tensorflow.org/alpha/tutorials/distribute/multi_worker&gt;this tutorial code&lt;/denchmark-link&gt;
 (MNIST distributed training with TF2.0) but used tf.distribute.MirroredStrategy() (instead of MultiWorker). It worked. Then I changed the model architecture to a simple Embedding -&gt; LSTM -&gt; Dense architecture. It broke with the following errror:
&lt;denchmark-code&gt;Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:worker/replica:0/task:0/device:GPU:0 vs /job:worker/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal
	 [[node sequential/lstm/StatefulPartitionedCall (defined at tf2_multiworker_tutorial/main.py:109) ]]
&lt;/denchmark-code&gt;

This was executed on a remote cluster single-machine with 2 GPUs. Note that I've been seeing this error ever since the initial 2.0 alpha release. The code is as follows:
import tensorflow as tf

BUFFER_SIZE = 10000
BATCH_SIZE = 64
LEARNING_RATE = 1e-4


def input_fn(mode, input_context=None):
    max_seq_len = 3
    rnn_dataset = tf.data.Dataset\
        .range(10)\
        .repeat(10 * BUFFER_SIZE) \
        .map(lambda x: (
        tf.ones(shape=(max_seq_len,), dtype=tf.int64),
        tf.ones(shape=(max_seq_len,), dtype=tf.int64)))
    if input_context:
        rnn_dataset = rnn_dataset.shard(
            input_context.num_input_pipelines,
            input_context.input_pipeline_id)
    return rnn_dataset.batch(BATCH_SIZE)


def model_fn(features, labels, mode):
    vocab_size = 100
    embed_size = 16
    state_size = 7
    model = tf.keras.Sequential([
        tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size),
        tf.keras.layers.LSTM(units=state_size, return_sequences=True),
        tf.keras.layers.Dense(10, activation='softmax')])
    logits = model(features, training=False)

    if mode == tf.estimator.ModeKeys.PREDICT:
        return tf.estimator.EstimatorSpec(
            tf.estimator.ModeKeys.PREDICT,
            predictions={'logits': logits})

    optimizer = tf.compat.v1.train.GradientDescentOptimizer(
        learning_rate=LEARNING_RATE)
    loss = tf.keras.losses.SparseCategoricalCrossentropy(
        from_logits=True,
        reduction=tf.keras.losses.Reduction.NONE)
    loss = tf.reduce_sum(loss(labels, logits)) * (1. / BATCH_SIZE)
    if mode == tf.estimator.ModeKeys.EVAL:
        return tf.estimator.EstimatorSpec(mode, loss=loss)

    return tf.estimator.EstimatorSpec(
        mode=mode, loss=loss,
        train_op=optimizer.minimize(
            loss, tf.compat.v1.train.get_or_create_global_step()))


def main():
    strategy = tf.distribute.MirroredStrategy()
    config = tf.estimator.RunConfig(
        train_distribute=strategy,
        log_step_count_steps=1)

    classifier = tf.estimator.Estimator(
        model_fn=model_fn, model_dir='/tmp/multiworker', config=config)
    tf.estimator.train_and_evaluate(
        classifier,
        train_spec=tf.estimator.TrainSpec(input_fn=input_fn, max_steps=10),
        eval_spec=tf.estimator.EvalSpec(input_fn=input_fn))


if __name__ == '__main__':
    main()
Again, the common theme I've observed is that if tf.keras.LSTM is part of my model and I'm using tf.distribute, it breaks with this error. Otherwise, it works just fine.
	</description>
	<comments>
		<comment id='1' author='mckinziebrandon' date='2019-05-31T12:28:05Z'>
		&lt;denchmark-link:https://github.com/mckinziebrandon&gt;@mckinziebrandon&lt;/denchmark-link&gt;
 Ran the code in colab using GPU got the below error.
AttributeError: module 'tensorflow._api.v1.keras.losses' has no attribute 'SparseCategoricalCrossentropy'.
		</comment>
		<comment id='2' author='mckinziebrandon' date='2019-05-31T15:27:48Z'>
		&lt;denchmark-link:https://github.com/muddham&gt;@muddham&lt;/denchmark-link&gt;
 using TF 2.0? That's an error I get if I use TF 1.13. For example, &lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy&gt;here is the link&lt;/denchmark-link&gt;
 for its documentation.
Also, &lt;denchmark-link:https://github.com/muddham&gt;@muddham&lt;/denchmark-link&gt;
 are you able to run this on a multi-GPU setup? The above code works as expected for tensorflow 2.0 CPU, but breaks when run with tf-nightly-gpu-2.0-preview (or tensorflow-gpu=2.0.0-alpha0) on 2 GPUS.
		</comment>
		<comment id='3' author='mckinziebrandon' date='2019-06-06T15:56:47Z'>
		&lt;denchmark-link:https://github.com/muddham&gt;@muddham&lt;/denchmark-link&gt;
 Any update on this?
		</comment>
		<comment id='4' author='mckinziebrandon' date='2019-06-11T05:21:47Z'>
		&lt;denchmark-link:https://github.com/mckinziebrandon&gt;@mckinziebrandon&lt;/denchmark-link&gt;
 Is this still an issue?
		</comment>
		<comment id='5' author='mckinziebrandon' date='2019-06-13T20:34:05Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 Just confirmed that yes, this is still an issue on the current TF gpu-nightly (2.0).
		</comment>
		<comment id='6' author='mckinziebrandon' date='2019-06-20T17:42:25Z'>
		Can you use Distribution Strategies directly with the Keras model, instead of passing through Estimator? There are likely bad effects here resulting from the mixing of many APIs (including v1 optimizers), and if you can swap to the fully v2 version (Dist Strat + keras optimizers + keras model + keras LSTM), this should work.
		</comment>
		<comment id='7' author='mckinziebrandon' date='2019-06-22T20:24:59Z'>
		&lt;denchmark-link:https://github.com/karmel&gt;@karmel&lt;/denchmark-link&gt;
 I can try that, sure. However, if that's the case (that people should not mix them), then perhaps TensorFlow should not suggest doing so in their own tutorials (recall that this post is a fairly trivial modification to an official tutorial). It does look like a lot has been updated regarding the tutorials on distributed training, so I'll check those out too. Will update here after I try your suggestion.
		</comment>
		<comment id='8' author='mckinziebrandon' date='2019-06-24T23:41:28Z'>
		Ah, that tutorial has been replaced with this one: &lt;denchmark-link:https://www.tensorflow.org/beta/tutorials/distribute/multi_worker_with_keras&gt;https://www.tensorflow.org/beta/tutorials/distribute/multi_worker_with_keras&lt;/denchmark-link&gt;
 , and &lt;denchmark-link:https://github.com/rchao&gt;@rchao&lt;/denchmark-link&gt;
 is removing references to the old. Can you start from the new one?
		</comment>
		<comment id='9' author='mckinziebrandon' date='2019-07-09T08:47:59Z'>
		Hi,
I am trying to run mnist example using distributed training tf 2.0.I am getting below similar error. I request to help me with workaround for this error.
&lt;denchmark-code&gt;tf.distribute.experimental.CollectiveCommunication.NCCL)
&lt;/denchmark-code&gt;

File "/home/administrator/venv/lib/python3.5/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py", line 80, in init
communication=communication))
File "/home/administrator/venv/lib/python3.5/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py", line 110, in init
self._initialize_strategy(cluster_resolver)
File "/home/administrator/venv/lib/python3.5/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py", line 116, in _initialize_strategy
self._initialize_multi_worker(cluster_resolver)
File "/home/administrator/venv/lib/python3.5/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py", line 208, in _initialize_multi_worker
device_filters=("/job:%s/task:%d" % (task_type, task_id),))
File "/home/administrator/venv/lib/python3.5/site-packages/tensorflow/python/eager/context.py", line 541, in configure_collective_ops
raise RuntimeError("Collective ops must be configured at program startup")
RuntimeError: Collective ops must be configured at program startup
		</comment>
		<comment id='10' author='mckinziebrandon' date='2019-07-09T14:33:31Z'>
		&lt;denchmark-link:https://github.com/mvp2301&gt;@mvp2301&lt;/denchmark-link&gt;
 you error seems to be totally different issue, please file a separate bug with more details.
		</comment>
		<comment id='11' author='mckinziebrandon' date='2019-07-09T16:49:56Z'>
		&lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
 Hi, running into the same issue. I was running on tf.keras.layers.RNN(cell=tf.keras.layers.LSTMCell(...)) until recently, but decided to switch to tf.keras.layers.LSTM for the CuDNN implmentation. Is the LSTM layers the only access point for the CuDNN LSTM ? In fact, I also opened a feature request on it but that's a different matter (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/30535&gt;#30535&lt;/denchmark-link&gt;
)
		</comment>
		<comment id='12' author='mckinziebrandon' date='2019-07-09T17:22:01Z'>
		I would also like to point out that I am getting this error without even being on a distributed strategy. Using LSTM, Dataset, TF 2.0...
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

InvalidArgumentError                      Traceback (most recent call last)
 in 
7         optimizer=optimizer,
8         batch=(seq_x, len_x, len_y),
----&gt; 9         global_batch_size=240
10     )
11     t1 = time.time()
~/anaconda3/envs/seq3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in call(self, *args, **kwds)
426         # Lifting succeeded, so variables are initialized and we can run the
427         # stateless function.
--&gt; 428         return self._stateless_fn(*args, **kwds)
429     else:
430       canon_args, canon_kwds = \
~/anaconda3/envs/seq3/lib/python3.6/site-packages/tensorflow/python/eager/function.py in (self, *args, **kwargs)
1333     """Calls a graph function specialized to the inputs."""
1334     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-&gt; 1335     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
1336
1337   &lt;denchmark-link:https://github.com/Property&gt;@Property&lt;/denchmark-link&gt;

~/anaconda3/envs/seq3/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs)
587     """
588     return self._call_flat(
--&gt; 589         (t for t in nest.flatten((args, kwargs), expand_composites=True)
590          if isinstance(t, (ops.Tensor,
591                            resource_variable_ops.ResourceVariable))))
~/anaconda3/envs/seq3/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args)
669     # Only need to override the gradient in graph mode and when we have outputs.
670     if context.executing_eagerly() or not self.outputs:
--&gt; 671       outputs = self._inference_function.call(ctx, args)
672     else:
673       self._register_gradient()
~/anaconda3/envs/seq3/lib/python3.6/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args)
443             attrs=("executor_type", executor_type,
444                    "config_proto", config),
--&gt; 445             ctx=ctx)
446       # Replace empty list with None
447       outputs = outputs or None
~/anaconda3/envs/seq3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
65     else:
66       message = e.message
---&gt; 67     six.raise_from(core._status_to_exception(e.code, message), None)
68   except TypeError as e:
69     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):
~/anaconda3/envs/seq3/lib/python3.6/site-packages/six.py in raise_from(value, from_value)
InvalidArgumentError: 2 root error(s) found.
(0) Invalid argument:  Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:0 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_20/exit/_94 , and the dst node is while_0_RetVal
[[{{node se_q3/seq_encoder/while/body/_195/decoder_c/lstm_2/StatefulPartitionedCall}}]]
[[se_q3/seq_decoder/encoder/bidirectional/cond_1/then/_958/while/exit/_6952/_2016]]
(1) Invalid argument:  Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:0 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_20/exit/_94 , and the dst node is while_0_RetVal
[[{{node se_q3/seq_encoder/while/body/_195/decoder_c/lstm_2/StatefulPartitionedCall}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_step_54702]
Function call stack:
train_step -&gt; train_step
		</comment>
		<comment id='13' author='mckinziebrandon' date='2019-07-09T18:42:29Z'>
		&lt;denchmark-link:https://github.com/jkamalu&gt;@jkamalu&lt;/denchmark-link&gt;
, can u provide the full code snippet to reproduce your issue?
		</comment>
		<comment id='14' author='mckinziebrandon' date='2019-07-10T05:03:02Z'>
		
@mvp2301 you error seems to be totally different issue, please file a separate bug with more details.

Thanks for reply.created a new bug issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/30551&gt;#30551&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='mckinziebrandon' date='2019-07-10T14:18:06Z'>
		&lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
 I will when I find the time and am able to produce it on a smaller example. In the meantime, check out this issue which may be related (the logs were the same for me when I got the cannot place graph error): &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29525&gt;#29525&lt;/denchmark-link&gt;

		</comment>
		<comment id='16' author='mckinziebrandon' date='2019-07-11T12:51:25Z'>
		Hi &lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
, I finally got around to finding a minimal example that shows that the LSTM implementation breaks in some cases without the distributed strategy. Here you go: &lt;denchmark-link:https://github.com/jkamalu/tensorflow_bugs/blob/master/LSTMGraphPlacement.py&gt;https://github.com/jkamalu/tensorflow_bugs/blob/master/LSTMGraphPlacement.py&lt;/denchmark-link&gt;

It has to do the the inclusion of the loop. Without the loop, it works fine. My real use case of the while loop is to perform dynamic decoding, but I get the same error as you will run into here.
		</comment>
		<comment id='17' author='mckinziebrandon' date='2019-07-23T16:11:00Z'>
		&lt;denchmark-link:https://github.com/mckinziebrandon&gt;@mckinziebrandon&lt;/denchmark-link&gt;
 I am currently able to run a model containing the tf.keras.layers.LSTM layer with the MirroredStrategy on 3 GPUs in parallel as of the July 23 2.0 gpu nightly. See this comment by &lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
 on my now closed issue pointing to two recent commits that may have resolved a myriad of problems: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/30639#issuecomment-513599641&gt;#30639 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='18' author='mckinziebrandon' date='2019-07-23T16:59:19Z'>
		Thanks for reporting the issue, the issue should be solved by &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/ca7acecce5066f518b775da339b6258fafd3db23&gt;ca7acec&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/f0fd2bed4d5358ebdb8bae5243c1a86fd7967f27&gt;f0fd2be&lt;/denchmark-link&gt;
. Could u try the code again with the tf-2.0-nightly builds?
		</comment>
		<comment id='19' author='mckinziebrandon' date='2019-07-24T04:23:24Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29189&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29189&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='20' author='mckinziebrandon' date='2019-08-02T13:14:13Z'>
		does this already fixed?
		</comment>
		<comment id='21' author='mckinziebrandon' date='2019-08-02T15:12:26Z'>
		Yes.
		</comment>
		<comment id='22' author='mckinziebrandon' date='2019-10-10T11:52:05Z'>
		Hi, I still encountered a similar error with the official 2.0 version and tf-nightly.
Here is the code snippet: &lt;denchmark-link:https://gist.github.com/matthew-z/e5848d545b60792dd84bfb9470ea541f&gt;https://gist.github.com/matthew-z/e5848d545b60792dd84bfb9470ea541f&lt;/denchmark-link&gt;

model.fit works well, but model.evaluate raises this error:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
&lt;ipython-input-20-8e7870f41c98&gt; in &lt;module&gt;
----&gt; 1 model.evaluate([left, right], labels)

~/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in evaluate(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)
    831         max_queue_size=max_queue_size,
    832         workers=workers,
--&gt; 833         use_multiprocessing=use_multiprocessing)
    834 
    835   def predict(self,

~/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in evaluate(self, model, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)
    454     return self._model_iteration(
    455         model, ModeKeys.TEST, x=x, y=y, batch_size=batch_size, verbose=verbose,
--&gt; 456         sample_weight=sample_weight, steps=steps, callbacks=callbacks, **kwargs)
    457 
    458   def predict(self, model, x, batch_size=None, verbose=0, steps=None,

~/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in _model_iteration(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)
    442               mode=mode,
    443               training_context=training_context,
--&gt; 444               total_epochs=1)
    445           cbks.make_logs(model, epoch_logs, result, mode)
    446 

~/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    121         step=step, mode=mode, size=current_batch_size) as batch_logs:
    122       try:
--&gt; 123         batch_outs = execution_function(iterator)
    124       except (StopIteration, errors.OutOfRangeError):
    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?

~/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)
     84     # `numpy` translates Tensors to values in Eager mode.
     85     return nest.map_structure(_non_none_constant_value,
---&gt; 86                               distributed_function(input_fn))
     87 
     88   return execution_function

~/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    455 
    456     tracing_count = self._get_tracing_count()
--&gt; 457     result = self._call(*args, **kwds)
    458     if tracing_count == self._get_tracing_count():
    459       self._call_counter.called_without_tracing()

~/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)
    524               *args, **kwds)
    525       # If we did not create any variables the trace we have is good enough.
--&gt; 526       return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
    527 
    528     def fn_with_cond(*inner_args, **inner_kwds):

~/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)
   1139          if isinstance(t, (ops.Tensor,
   1140                            resource_variable_ops.BaseResourceVariable))),
-&gt; 1141         self.captured_inputs)
   1142 
   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

~/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1222     if executing_eagerly:
   1223       flat_outputs = forward_function.call(
-&gt; 1224           ctx, args, cancellation_manager=cancellation_manager)
   1225     else:
   1226       gradient_name = self._delayed_rewrite_functions.register()

~/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    509               inputs=args,
    510               attrs=("executor_type", executor_type, "config_proto", config),
--&gt; 511               ctx=ctx)
    512         else:
    513           outputs = execute.execute_with_cancellation(

~/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     65     else:
     66       message = e.message
---&gt; 67     six.raise_from(core._status_to_exception(e.code, message), None)
     68   except TypeError as e:
     69     keras_symbolic_tensors = [

~/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/six.py in raise_from(value, from_value)

NotFoundError: 2 root error(s) found.
  (0) Not found:  Resource AnonymousIterator/AnonymousIterator8/N10tensorflow4data16IteratorResourceE does not exist.
	 [[node IteratorGetNext (defined at /home/zhaohao/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]
	 [[IteratorGetNext/_10]]
  (1) Not found:  Resource AnonymousIterator/AnonymousIterator8/N10tensorflow4data16IteratorResourceE does not exist.
	 [[node IteratorGetNext (defined at /home/zhaohao/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]
0 successful operations.
0 derived errors ignored. [Op:__inference_distributed_function_45417]

Function call stack:
distributed_function -&gt; distributed_function
&lt;/denchmark-code&gt;

		</comment>
		<comment id='23' author='mckinziebrandon' date='2019-10-10T16:40:26Z'>
		&lt;denchmark-link:https://github.com/matthew-z&gt;@matthew-z&lt;/denchmark-link&gt;
, thanks for reporting the issue, I can't reproduce it. Seems that to be a different issue since the your code does not use distribution strategy. Do u mind create new a ticket for that, with the details about your local environment?
Thanks.
		</comment>
		<comment id='24' author='mckinziebrandon' date='2019-10-11T18:20:22Z'>
		&lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
 Thank you for the reply. I have filed a new issue:  &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33258&gt;#33258&lt;/denchmark-link&gt;
 with detailed environment info (tested on GCE machine and image).
		</comment>
		<comment id='25' author='mckinziebrandon' date='2019-11-14T10:50:39Z'>
		&lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
  I have this issue when using TimeDistributed LSTM with mask_zero=True
this is my model :

and this is the error :
`C:\Users\jalil\PycharmProjects\untitled1\venv\Scripts\python.exe C:/Users/jalil/PycharmProjects/untitled1/main_file.py
2019-11-14 14:11:36.983144: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2019-11-14 14:11:45.638679: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2019-11-14 14:11:46.216495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: GeForce GTX 970M major: 5 minor: 2 memoryClockRate(GHz): 1.038
pciBusID: 0000:01:00.0
2019-11-14 14:11:46.216676: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-11-14 14:11:46.217282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-11-14 14:11:50.885396: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-11-14 14:11:51.214275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: GeForce GTX 970M major: 5 minor: 2 memoryClockRate(GHz): 1.038
pciBusID: 0000:01:00.0
2019-11-14 14:11:51.214484: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-11-14 14:11:51.218182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-11-14 14:11:51.905201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-14 14:11:51.905307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0
2019-11-14 14:11:51.905366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N
2019-11-14 14:11:51.906228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4757 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0, compute capability: 5.2)
WARNING:tensorflow:From C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\keras\backend.py:3983: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Train on 35000 samples, validate on 6447 samples
Epoch 1/1000
2019-11-14 14:12:33.178251: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_671418_672877' and '__inference___backward_cudnn_lstm_with_fallback_671418_672877_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_675292' both implement 'lstm_81cdaa4a-fa6f-4675-abbb-02fb4cd0189b' but their signatures do not match.
2019-11-14 14:12:33.544669: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2019-11-14 14:12:34.397677: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
32/35000 [..............................] - ETA: 2:03:422019-11-14 14:12:35.151804: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at cudnn_rnn_ops.cc:1498 : Unknown: CUDNN_STATUS_BAD_PARAM
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&amp;padding_fill)'
2019-11-14 14:12:35.152137: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: CUDNN_STATUS_BAD_PARAM
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&amp;padding_fill)'
[[{{node cond_64/then/_0/CudnnRNNV3}}]]
2019-11-14 14:12:35.152541: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: {{function_node __forward_cudnn_lstm_with_fallback_672876_specialized_for_sequential_time_distributed_1_lstm_StatefulPartitionedCall_at___inference_distributed_function_675292}} {{function_node __forward_cudnn_lstm_with_fallback_672876_specialized_for_sequential_time_distributed_1_lstm_StatefulPartitionedCall_at___inference_distributed_function_675292}} CUDNN_STATUS_BAD_PARAM
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&amp;padding_fill)'
[[{{node cond_64/then/_0/CudnnRNNV3}}]]
[[sequential/time_distributed_1/lstm/StatefulPartitionedCall]]
32/35000 [..............................] - ETA: 2:25:41Traceback (most recent call last):
File "C:/Users/jalil/PycharmProjects/untitled1/main_file.py", line 102, in 
main_model_instance.train_model(train_batch_data,train_batch_labels,test_batch_data,test_batch_labels)
File "C:\Users\jalil\PycharmProjects\untitled1\main_model.py", line 103, in train_model
history = self.model.fit(x=np.array(train_batch_data),y=np.array(train_batch_labels),validation_data=(np.array(test_batch_data),np.array(test_batch_labels)),epochs=1000,callbacks=[tensorboard_callback])
File "C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\keras\engine\training.py", line 734, in fit
use_multiprocessing=use_multiprocessing)
File "C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py", line 324, in fit
total_epochs=epochs)
File "C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py", line 123, in run_one_epoch
batch_outs = execution_function(iterator)
File "C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py", line 86, in execution_function
distributed_function(input_fn))
File "C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\eager\def_function.py", line 439, in call
return self._stateless_fn(*args, *kwds)
File "C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\eager\function.py", line 1822, in call
return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
File "C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\eager\function.py", line 1141, in _filtered_call
self.captured_inputs)
File "C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\eager\function.py", line 1224, in _call_flat
ctx, args, cancellation_manager=cancellation_manager)
File "C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\eager\function.py", line 511, in call
ctx=ctx)
File "C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\eager\execute.py", line 67, in quick_execute
six.raise_from(core._status_to_exception(e.code, message), None)
File "", line 3, in raise_from
tensorflow.python.framework.errors_impl.UnknownError:  [Derived]  CUDNN_STATUS_BAD_PARAM
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void)&amp;padding_fill)'
[[{{node cond_64/then/_0/CudnnRNNV3}}]]
[[sequential/time_distributed_1/lstm/StatefulPartitionedCall]] [Op:__inference_distributed_function_675292]
Function call stack:
distributed_function -&gt; distributed_function -&gt; distributed_function
`
do you have any suggestions, please?
		</comment>
		<comment id='26' author='mckinziebrandon' date='2019-11-14T10:55:20Z'>
		by the way, on CPU it works fine, the problem appears when using the GPU version. I also tried every GPU version of TensorFlow and the problem is still there.
		</comment>
		<comment id='27' author='mckinziebrandon' date='2019-11-14T17:12:15Z'>
		&lt;denchmark-link:https://github.com/jalilasadi&gt;@jalilasadi&lt;/denchmark-link&gt;
, do u mind opening another ticket with more details about your issue? We don't want to track multiple issue in the same ticket if they are different.
		</comment>
		<comment id='28' author='mckinziebrandon' date='2019-11-14T18:08:16Z'>
		&lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
  thanks for your answer, actually there is an issue on this problem  &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33148&gt;#33148&lt;/denchmark-link&gt;
   but no one helped us there.  I will be happy to see you there.
		</comment>
		<comment id='29' author='mckinziebrandon' date='2019-11-14T18:24:37Z'>
		Ok, we will take a look for that issue.
		</comment>
		<comment id='30' author='mckinziebrandon' date='2019-11-15T08:31:15Z'>
		&lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
 we are waiting for you please.
		</comment>
		<comment id='31' author='mckinziebrandon' date='2020-07-28T10:10:56Z'>
		I'm still getting this in TF 2.2.0 when using MirroredStrategy
&lt;denchmark-code&gt;tensorflow.python.framework.errors_impl.UnknownError:  [_Derived_]  CUDNN_STATUS_BAD_PARAM
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1459): 'cudnnSetTensorNdDescriptor( tensor_desc.get(), data_type, sizeof(dims) / sizeof(dims[0]), dims, strides)'
	 [[{{node CudnnRNN}}]]
	 [[StatefulPartitionedCall/StatefulPartitionedCall/replica_2/transducer/encoder_network/lstm/StatefulPartitionedCall]]
	 [[StatefulPartitionedCall/StatefulPartitionedCall/Identity_1/_230]] [Op:__inference__test_function_85050]
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>