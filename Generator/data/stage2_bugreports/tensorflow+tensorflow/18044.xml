<bug id='18044' author='aforslow' open_date='2018-03-28T09:35:19Z' closed_time='2018-10-26T21:07:46Z'>
	<summary>FailedPreconditionError (see above for traceback): Failed to rename: &amp;lt;file_name&amp;gt; to: &amp;lt;file_name&amp;gt; : The process cannot access the file because it is being used by another process. ; Broken pipe</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10 Pro 64-bit (10.0, Build 16299)
TensorFlow installed from (source or binary):
Binary
TensorFlow version (use command below):
1.6.0
Python version:
b'unknown' 1.6.0
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:

You can collect some of this information using our environment capture script:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&lt;/denchmark-link&gt;

You can obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

I try to run a model using tf.train.MonitoredTrainingSession(), but get an error when the CheckPointSaverHook() tries to save the model. Having loked at the target saving directory, the saver seems to create a temporary directory from shards (something I assume is done because of the model being quite big) for later use. When the saver later on tries to use the sharded files in this directory, it seems to get blocked by another process accessing those files, resulting in a broken pipe error.
I assume the problem has to do with the sharding mechanism, as this is the first time I've seen the saver having to save the checkpoints in shards. I am not sure though, so if you're sure something else is causing this error, you're probably right.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

Below is the error message I get when running my code.
&lt;denchmark-code&gt;Caused by op 'save/SaveV2', defined at:
  File "em_routing_train.py", line 218, in &lt;module&gt;
    tf.app.run()
  File "C:\Python36\lib\site-packages\tensorflow\python\platform\app.py", line 126, in run
    _sys.exit(main(argv))
  File "em_routing_train.py", line 214, in main
    train()
  File "em_routing_train.py", line 135, in train
    log_device_placement=FLAGS.log_device_placement)) as mon_sess:
  File "C:\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py", line 384, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File "C:\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py", line 795, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File "C:\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py", line 518, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File "C:\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py", line 981, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File "C:\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py", line 986, in _create_session
    return self._sess_creator.create_session()
  File "C:\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py", line 675, in create_session
    self.tf_sess = self._session_creator.create_session()
  File "C:\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py", line 437, in create_session
    self._scaffold.finalize()
  File "C:\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py", line 212, in finalize
    self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access
  File "C:\Python36\lib\site-packages\tensorflow\python\training\saver.py", line 871, in _get_saver_or_default
    saver = Saver(sharded=True, allow_empty=True)
  File "C:\Python36\lib\site-packages\tensorflow\python\training\saver.py", line 1293, in __init__
    self.build()
  File "C:\Python36\lib\site-packages\tensorflow\python\training\saver.py", line 1302, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File "C:\Python36\lib\site-packages\tensorflow\python\training\saver.py", line 1339, in _build
    build_save=build_save, build_restore=build_restore)
  File "C:\Python36\lib\site-packages\tensorflow\python\training\saver.py", line 787, in _build_internal
    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)
  File "C:\Python36\lib\site-packages\tensorflow\python\training\saver.py", line 411, in _AddShardedSaveOps
    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)
  File "C:\Python36\lib\site-packages\tensorflow\python\training\saver.py", line 385, in _AddShardedSaveOpsForV2
    sharded_saves.append(self._AddSaveOps(sharded_filename, saveables))
  File "C:\Python36\lib\site-packages\tensorflow\python\training\saver.py", line 326, in _AddSaveOps
    save = self.save_op(filename_tensor, saveables)
  File "C:\Python36\lib\site-packages\tensorflow\python\training\saver.py", line 241, in save_op
    tensors)
  File "C:\Python36\lib\site-packages\tensorflow\python\ops\gen_io_ops.py", line 1286, in save_v2
    shape_and_slices=shape_and_slices, tensors=tensors, name=name)
  File "C:\Python36\lib\site-packages\tensorflow\python\framework\op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "C:\Python36\lib\site-packages\tensorflow\python\framework\ops.py", line 3271, in create_op
    op_def=op_def)
  File "C:\Python36\lib\site-packages\tensorflow\python\framework\ops.py", line 1650, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

FailedPreconditionError (see above for traceback): Failed to rename: 
./tmp/em_routing_train3\model.ckpt-1_temp_79c748505e6941cfa43f30080736273a/part-00000-of-
00001.index.tempstate15074727511180888636 to: ./tmp/em_routing_train3\model.ckpt-
1_temp_79c748505e6941cfa43f30080736273a/part-00000-of-00001.index : The process cannot access 
the file because it is being used by another process.
; Broken pipe
         [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., 
DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], 
_device="/job:localhost/replica:0/task:0/device:CPU:0"](save/ShardedFilename, 
save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, Class_caps/capsule_cl_tr/beta_u/beta_u, 
Class_caps/capsule_cl_tr/beta_u/beta_u/Adam, Class_caps/capsule_cl_tr/beta_u/beta_u/Adam_1, 
Class_caps/capsule_cl_tr/weights/Class_caps/capsule_cl_tr/weights, 
Class_caps/capsule_cl_tr/weights/Class_caps/capsule_cl_tr/weights/Adam, 
Class_caps/capsule_cl_tr/weights/Class_caps/capsule_cl_tr/weights/Adam_1, ReLu_Conv1/conv2d/bias, 
ReLu_Conv1/conv2d/bias/Adam, ReLu_Conv1/conv2d/bias/Adam_1, ReLu_Conv1/conv2d/kernel, 
ReLu_Conv1/conv2d/kernel/Adam, ReLu_Conv1/conv2d/kernel/Adam_1, beta1_power, beta2_power, 
beta_a/beta_a, beta_a/beta_a/Adam, beta_a/beta_a/Adam_1, convCaps1/beta_u/beta_u, 
convCaps1/beta_u/beta_u/Adam, convCaps1/beta_u/beta_u/Adam_1, convCaps1/weights/weights, 
convCaps1/weights/weights/Adam, convCaps1/weights/weights/Adam_1, convCaps2/beta_u/beta_u, 
convCaps2/beta_u/beta_u/Adam, convCaps2/beta_u/beta_u/Adam_1, convCaps2/weights/weights, 
convCaps2/weights/weights/Adam, convCaps2/weights/weights/Adam_1, global_step, 
primaryCaps/weights1, primaryCaps/weights1/Adam, primaryCaps/weights1/Adam_1, 
primaryCaps/weights2, primaryCaps/weights2/Adam, primaryCaps/weights2/Adam_1)]]
&lt;/denchmark-code&gt;

The important part of this error message is, as I see it:
&lt;denchmark-code&gt;FailedPreconditionError (see above for traceback): Failed to rename: 
./tmp/em_routing_train3\model.ckpt-1_temp_79c748505e6941cfa43f30080736273a/part-00000-of-
00001.index.tempstate15074727511180888636 to: ./tmp/em_routing_train3\model.ckpt-
1_temp_79c748505e6941cfa43f30080736273a/part-00000-of-00001.index : The process cannot access 
the file because it is being used by another process.
; Broken pipe
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='aforslow' date='2018-09-07T08:39:15Z'>
		i m also having same problem while training a model. Can anyone please help me to solve this error
		</comment>
		<comment id='2' author='aforslow' date='2018-09-07T18:29:05Z'>
		Hi &lt;denchmark-link:https://github.com/prasilla487&gt;@prasilla487&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/aforslow&gt;@aforslow&lt;/denchmark-link&gt;
 ,
could one of you provide us a small self sufficient script which reproduces this issue?
thanks
		</comment>
		<comment id='3' author='aforslow' date='2018-10-11T08:52:35Z'>
		Same here ! I'm working on an Azure virtual machine, which may cause delays. The only way I see to move around the problem is to space out the saver calls... but it's not fail-safe !
		</comment>
		<comment id='4' author='aforslow' date='2018-10-26T12:46:54Z'>
		It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='5' author='aforslow' date='2018-10-26T21:07:46Z'>
		Closing due to low activity
		</comment>
		<comment id='6' author='aforslow' date='2020-11-09T00:09:21Z'>
		Hello, I have an issue while training the data.
&lt;denchmark-link:https://user-images.githubusercontent.com/72600760/98488193-125f1b80-226b-11eb-9a8f-8ac46b331779.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/72600760/98488213-3589cb00-226b-11eb-8ca8-cc441c95a0c9.png&gt;&lt;/denchmark-link&gt;

Can anyone help me with this?
		</comment>
	</comments>
</bug>