<bug id='33603' author='steph-en-m' open_date='2019-10-22T10:56:47Z' closed_time='2019-10-22T16:58:45Z'>
	<summary>tf.pow() overflow??</summary>
	<description>
Env:

Colab (with gpu on)
TF 2.0
Python3

Running tf.pow() with integer powers greater than 5 returns wrong results and to me it looks like an overflow.
a = tf.constant(50)
b = tf.constant(6)
tf.pow(a,b)
The above returns:
&lt;denchmark-code&gt;tf.Tensor(-1554869184, shape=(), dtype=int32)
&lt;/denchmark-code&gt;

Which is mathematically wrong. Here is the result (the correct one) when using python's math library.
import math
math.pow(50,6)
The above returns 15625000000.0
Using python's math library gives correct results for higher powers which I'd expect tf.pow() to do for integer inputs.
Any explanation for this discrepancy!!
	</description>
	<comments>
		<comment id='1' author='steph-en-m' date='2019-10-22T11:12:30Z'>
		Issue replicating in TF-2.0, kindly find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/oanush/bb4b5fe54b25afa435b0055243f63d14/33603.ipynb&gt;gist&lt;/denchmark-link&gt;
 of colab.Thanks!
		</comment>
		<comment id='2' author='steph-en-m' date='2019-10-22T11:33:05Z'>
		The range for int32 is from -2,147,483,648 to 2,147,483,647 while the result you want exceeds the limit. Therefore, the solution is
&lt;denchmark-code&gt;a = tf.constant(50, tf.int64)
b = tf.constant(6, tf.int64)
tf.pow(a,b)
&lt;/denchmark-code&gt;

which returns
&lt;tf.Tensor: id=2, shape=(), dtype=int64, numpy=15625000000&gt; 
		</comment>
		<comment id='3' author='steph-en-m' date='2019-10-22T13:36:20Z'>
		Thanks &lt;denchmark-link:https://github.com/oanush&gt;@oanush&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/blairhan&gt;@blairhan&lt;/denchmark-link&gt;
. I thought the types were applied implicitly.
		</comment>
		<comment id='4' author='steph-en-m' date='2019-10-22T16:58:45Z'>
		&lt;denchmark-link:https://github.com/steph-en-m&gt;@steph-en-m&lt;/denchmark-link&gt;
 Closing this issue as it has been resolved. Please add additional comments and we can open the issue again
		</comment>
		<comment id='5' author='steph-en-m' date='2019-10-22T16:58:47Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33603&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33603&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='steph-en-m' date='2019-10-23T07:52:14Z'>
		
Thanks @oanush and @blairhan. I thought the types were applied implicitly.

Actually am more interested in knowing why the tensorflow team chose to implement it this way (where one explicitly sets dtypes) rather than the python math.pow() style implementation
		</comment>
		<comment id='7' author='steph-en-m' date='2019-10-23T16:08:15Z'>
		Because these types need to be propagated to the C++ core layer. Dynamic typing is not a panacea
		</comment>
	</comments>
</bug>