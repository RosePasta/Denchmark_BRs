<bug id='31335' author='laket' open_date='2019-08-05T16:46:01Z' closed_time='2020-06-23T16:35:43Z'>
	<summary>A custom operator get Segmentation Fault in tf.function</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04
TensorFlow installed from (source or binary):
binary
TensorFlow version (use command below):
v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
Python version:
3.6.8
GCC/Compiler version (if compiling from source):
g++ 7.4.0

Describe the current behavior
When calling a custom op from a python function with tf.function, I got a segmentation fault.
The op run normally without tf.function.

I implemented a custom op with &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/adding_an_op/zero_out_op_kernel_1.cc&gt;zero_out_op_kernel_1.cc&lt;/denchmark-link&gt;
 in tensorflow repository.
I called the operator from the following python code.
import tensorflow as tf

_zero_out_module = tf.load_op_library('custom_ops.so')
zero_out = _zero_out_module.zero_out

@tf.function
def make_zero(x):
    return zero_out(x)

c = tf.constant([4,2,8,9])
res  = make_zero(c)

I confirmed that InferenceContext is NULL.
This bug is similar to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/30494&gt;#30494&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='laket' date='2019-08-06T20:44:07Z'>
		&lt;denchmark-link:https://github.com/laket&gt;@laket&lt;/denchmark-link&gt;
 Can you share a standalone code to reproduce the issue? Currently the code throws the following error. So need 
NotFoundError: ./custom_ops.so: cannot open shared object file: No such file or directory.
Thanks!
		</comment>
		<comment id='2' author='laket' date='2019-08-06T21:56:04Z'>
		I attach a C++ code and Makefile to build it.
zero_out_op.cc
#include "tensorflow/core/framework/op.h"
#include "tensorflow/core/framework/op_kernel.h"
#include "tensorflow/core/framework/shape_inference.h"
#include &lt;iostream&gt;
using namespace tensorflow;  // NOLINT(build/namespaces)

REGISTER_OP("ZeroOut")
    .Input("to_zero: int32")
    .Output("zeroed: int32")
    .SetShapeFn([](shape_inference::InferenceContext* c) {
    using namespace std;
cout &lt;&lt; c &lt;&lt; endl;
      c-&gt;set_output(0, c-&gt;input(0));
      return Status::OK();
    })
    .Doc(R"doc(
Zeros out all but the first value of a Tensor.

zeroed: A Tensor whose first value is identical to `to_zero`, and 0
  otherwise.
)doc");

class ZeroOutOp : public OpKernel {
 public:
  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}

  void Compute(OpKernelContext* context) override {
    // Grab the input tensor
    const Tensor&amp; input_tensor = context-&gt;input(0);
    auto input = input_tensor.flat&lt;int32&gt;();

    // Create an output tensor
    Tensor* output_tensor = nullptr;
    OP_REQUIRES_OK(context, context-&gt;allocate_output(0, input_tensor.shape(),
                                                     &amp;output_tensor));
    auto output = output_tensor-&gt;template flat&lt;int32&gt;();

    // Set all but the first element of the output tensor to 0.
    const int N = input.size();
    for (int i = 1; i &lt; N; i++) {
      output(i) = 0;
    }

    // Preserve the first input value.
    if (N &gt; 0) output(0) = input(0);
  }
};

REGISTER_KERNEL_BUILDER(Name("ZeroOut").Device(DEVICE_CPU), ZeroOutOp);

Makefile
&lt;denchmark-code&gt;DIR := .

TARGET_NAME := $(DIR)/custom_ops.so

TF_CFLAGS=$(shell python3 -c 'import tensorflow as tf; print(" ".join(tf.sysconfig.get_compile_flags()))')
TF_LIB= $(shell python3 -c 'import tensorflow as tf; print(" ".join(tf.sysconfig.get_link_flags()))')
CXX := g++

SOURCES := zero_out_op.cc

all: $(TARGET_NAME)

$(TARGET_NAME):
	$(CXX) -std=c++11 -shared  $(SOURCES) -o $@ -fPIC ${TF_CFLAGS} ${TF_LIB}

clean:
	rm -rf $(TARGET_NAME)

remake: clean all

&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='laket' date='2019-08-07T16:14:17Z'>
		I built tensorflow from source to investigate this bug.
The above code run normally with the tensorflow whether or not tf.function is attached. So I suspect that the difference of compile environments such as g++ version causes this problem.
The following is the detail of the tensorflow from source,

TensorFlow version (use command below):
v2.0.0-beta1-0-g8e423e3d56 2.0.0-beta1
compile command
bazel build --cxxopt="-D_GLIBCXX_USE_CXX11_ABI=0" --config=v2 --config=opt  //tensorflow/tools/pip_package:build_pip_package
-gcc version
gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0

		</comment>
		<comment id='4' author='laket' date='2019-08-08T16:20:33Z'>
		The issue here seems to be with incompatible compiler versions leading to the shape inference code being called improperly. &lt;denchmark-link:https://github.com/gunan&gt;@gunan&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/yifeif&gt;@yifeif&lt;/denchmark-link&gt;
 any idea what our recommendation is here?
		</comment>
		<comment id='5' author='laket' date='2019-08-08T16:45:51Z'>
		The pip package and the custom op have to be built with the exact same compiler.
If you are using our prebuilt pip packages, you should follow the instructions at github.com/tensorflow/custom-op to build your custom ops.
		</comment>
		<comment id='6' author='laket' date='2019-08-09T00:29:20Z'>
		I agree that using the exact same compiler is a solution for this problem. But it is not consistent with &lt;denchmark-link:https://www.tensorflow.org/guide/extend/op#compile_the_op_using_your_system_compiler_tensorflow_binary_installation&gt;the Adding a New Op document&lt;/denchmark-link&gt;
. If you encourage the solution, you should encourage readers to use the same compiler in the document.
I found that this problem comes from recently changes. I confirmed that the above code runs normally with tensorflow 1.13.2 (in graph mode), and tensorflow 1.14.0 cause a segmentation fault.
I think this is the same problem in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/30494&gt;#30494&lt;/denchmark-link&gt;
 .
		</comment>
		<comment id='7' author='laket' date='2019-08-09T04:56:57Z'>
		&lt;denchmark-link:https://github.com/lamberta&gt;@lamberta&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/yifeif&gt;@yifeif&lt;/denchmark-link&gt;
 about the documentation.
It is certainly possible to see the behaviour you described. Subtle changes can cause changes in the ABI, and we certainly have seen similar issues where seemingly irrelevant changes have started causing Segmentation faults.
		</comment>
		<comment id='8' author='laket' date='2019-08-11T01:46:53Z'>
		The docker container provided by github.com/tensorflow/custom-op is a practical solution.
So I am concerned about documentations only. My environment is very popular (Ubuntu 18.04) and InferenceContext is a fundamental component. The solution should be mentioned as much as the warning about "-D_GLIBCXX_USE_CXX11_ABI=0".
		</comment>
		<comment id='9' author='laket' date='2019-08-26T22:34:02Z'>
		
The pip package and the custom op have to be built with the exact same compiler.

If, by "pip package", do you mean "the TF binary installed via pip"? I hope not! How would I even find what compiler was used to build it? That would make the prebuilt binaries close to useless on Linux.
If compiling TF from source is (effectively) necessary in order to write custom ops, that should be documented as such in big bright red caps :-)
		</comment>
		<comment id='10' author='laket' date='2019-08-26T22:44:16Z'>
		It is effectively necessary; otherwise you can use our docker images, which
let you reproduce exactly the build environment for our releases. This is a
downside of using C++ for TF, which has no stable ABI. We're working on a C
API for custom ops which will not have this restriction.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, Aug 26, 2019 at 3:40 PM Steven Johnson ***@***.***&gt; wrote:
 The pip package and the custom op have to be built with the exact same
 compiler.

 If, by "pip package", do you mean "the TF binary installed via pip"? I
 hope not! How would I even find what compiler was used to build it? That
 would make the prebuilt binaries close to useless on Linux.

 If compiling TF from source is (effectively) necessary in order to write
 custom ops, that should be documented as such in big bright red caps :-)

 —
 You are receiving this because you were assigned.
 Reply to this email directly, view it on GitHub
 &lt;#31335?email_source=notifications&amp;email_token=AAABHRLSIAXTKTBMHHPJY43QGRLXRA5CNFSM4IJNAS62YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5F4JHI#issuecomment-525059229&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAABHRNWPXLHZRXRS2XA74LQGRLXRANCNFSM4IJNAS6Q&gt;
 .


-- 
 - Alex

		</comment>
		<comment id='11' author='laket' date='2019-08-26T22:49:58Z'>
		
It is effectively necessary; otherwise you can use our docker images, which let you reproduce exactly the build environment for our releases.

Understood. May I strenuously suggest that you make this clear in the custom-op documentation? Knowing this ahead of time would have saved me several hours of frustration.
		</comment>
		<comment id='12' author='laket' date='2019-08-27T18:13:42Z'>
		&lt;denchmark-link:https://github.com/MarkDaoust&gt;@MarkDaoust&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/lamberta&gt;@lamberta&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/yifeif&gt;@yifeif&lt;/denchmark-link&gt;

While we have &lt;denchmark-link:https://github.com/tensorflow/custom-op&gt;https://github.com/tensorflow/custom-op&lt;/denchmark-link&gt;

The website still does not point to that guide:
&lt;denchmark-link:https://www.tensorflow.org/guide/extend/op#build_the_op_library&gt;https://www.tensorflow.org/guide/extend/op#build_the_op_library&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='laket' date='2019-08-27T18:31:31Z'>
		Also, the guide as &lt;denchmark-link:https://www.tensorflow.org/guide/extend/op&gt;https://www.tensorflow.org/guide/extend/op&lt;/denchmark-link&gt;
 also uses TF1.x syntax; it would be thoughtful to provide TF2-syntax code there as well.
		</comment>
		<comment id='14' author='laket' date='2020-06-23T07:58:48Z'>
		&lt;denchmark-link:https://github.com/laket&gt;@laket&lt;/denchmark-link&gt;

Is this still an issue.
		</comment>
		<comment id='15' author='laket' date='2020-06-23T16:35:45Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31335&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31335&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>