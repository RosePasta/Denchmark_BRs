<bug id='36449' author='artemmavrin' open_date='2020-02-03T22:38:35Z' closed_time='2020-03-06T13:43:32Z'>
	<summary>Unclear documentation for implementing custom TensorFlow Keras optimizers</summary>
	<description>
&lt;denchmark-h:h2&gt;URL(s) with the issue:&lt;/denchmark-h&gt;

&lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer&gt;tf.keras.optimizers.Optimizer&lt;/denchmark-link&gt;
, specifically the section &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer#write_a_customized_optimizer_2&gt;Write a customized optimizer&lt;/denchmark-link&gt;
.
&lt;denchmark-h:h2&gt;Description of issue (what needs changing):&lt;/denchmark-h&gt;

The instructions for creating a custom optimizer seem to be inconsistent with how tf.keras.optimizers.Optimizer subclasses are defined in TensorFlow and other projects.
&lt;denchmark-h:h3&gt;Clear description&lt;/denchmark-h&gt;

This originated as a &lt;denchmark-link:https://stackoverflow.com/q/58772846/1917160&gt;question on Stack Overflow&lt;/denchmark-link&gt;
, which is reproduced below.
Suppose I want to write a custom optimizer class that conforms to the tf.keras API (using TensorFlow version&gt;=2.0). I am confused about the documented way to do this versus what's done in implementations.
The documentation for tf.keras.optimizers.Optimizer states,



tensorflow/tensorflow/python/keras/optimizer_v2/optimizer_v2.py


        Lines 217 to 224
      in
      7b1283e






   ### Write a customized optimizer. 



   If you intend to create your own optimization algorithm, simply inherit from 



   this class and override the following methods: 



  



     - resource_apply_dense (update variable given gradient tensor is dense) 



     - resource_apply_sparse (update variable given gradient tensor is sparse) 



     - create_slots (if your optimizer algorithm requires additional variables) 



     - get_config (serialization of the optimizer, include all hyper parameters) 





However, the current  implementation does not define a  method, but it  define a private-looking &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/7b1283ecf14e5f057b1a5c321a46db907ea713fc/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L916-L928&gt;_resource_apply_dense method stub&lt;/denchmark-link&gt;
. Similarly, there are no  or  methods, but there are a &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/7b1283ecf14e5f057b1a5c321a46db907ea713fc/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L958-L977&gt;_resource_apply_sparse method stub&lt;/denchmark-link&gt;
 and a &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/7b1283ecf14e5f057b1a5c321a46db907ea713fc/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L434&gt;_create_slots method call&lt;/denchmark-link&gt;
.
In official  subclasses (using  as an example), there are &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/7b1283ecf14e5f057b1a5c321a46db907ea713fc/tensorflow/python/keras/optimizer_v2/adam.py#L192-L227&gt;_resource_apply_dense&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/7b1283ecf14e5f057b1a5c321a46db907ea713fc/tensorflow/python/keras/optimizer_v2/adam.py#L229-L267&gt;_resource_apply_sparse&lt;/denchmark-link&gt;
, and &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/7b1283ecf14e5f057b1a5c321a46db907ea713fc/tensorflow/python/keras/optimizer_v2/adam.py#L150-L159&gt;_create_slots&lt;/denchmark-link&gt;
 methods, and there are no such methods without the leading underscore.
There are similar leading-underscore methods in slightly-less-official  subclasses (e.g.,  from TensorFlow Addons: &lt;denchmark-link:https://github.com/tensorflow/addons/blob/999aebc0961ccddb8174cc5331cc23a7291a2255/tensorflow_addons/optimizers/average_wrapper.py#L73-L76&gt;_resource_apply_dense&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/tensorflow/addons/blob/999aebc0961ccddb8174cc5331cc23a7291a2255/tensorflow_addons/optimizers/average_wrapper.py#L78-L82&gt;_resource_apply_sparse&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/tensorflow/addons/blob/999aebc0961ccddb8174cc5331cc23a7291a2255/tensorflow_addons/optimizers/moving_average.py#L92-L95&gt;_create_slots&lt;/denchmark-link&gt;
).
Another confounding point for me is that some of the TensorFlow Addons optimizers  override the  method (e.g., &lt;denchmark-link:https://github.com/tensorflow/addons/blob/999aebc0961ccddb8174cc5331cc23a7291a2255/tensorflow_addons/optimizers/average_wrapper.py#L55-L57&gt;tfa.optimizers.MovingAverage&lt;/denchmark-link&gt;
), whereas the  optimizers do not.
Moreover, I noticed that the apply_gradients method of tf.keras.optimizers.Optimizer method calls _create_slots, but the base tf.keras.optimizers.Optimizer class does not have a _create_slots method. So, it seems that a _create_slots method must be defined in an optimizer subclass if that subclass does not override apply_gradients.
&lt;denchmark-h:h4&gt;Questions&lt;/denchmark-h&gt;

What is the correct way to subclass a tf.keras.optimizers.Optimizer? Specifically,

Does the tf.keras.optimizers.Optimizer documentation listed at the top simply mean to override the leading-underscore versions of the methods they mention (e.g., _resource_apply_dense instead of resource_apply_dense)? If so, are there any API guarantees about these private-looking methods not changing their behavior in future versions of TensorFlow? What are the signatures of these methods?
When would one override apply_gradients in addition to the _apply_resource_[dense|sparse] methods?

	</description>
	<comments>
		<comment id='1' author='artemmavrin' date='2020-03-06T13:43:31Z'>
		&lt;denchmark-link:https://github.com/artemmavrin&gt;@artemmavrin&lt;/denchmark-link&gt;
 Thanks for the report! This has been fixed nightly &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/9f7ded2d44a8bf8c3c47cc2a475a5ca6bab6b765&gt;here&lt;/denchmark-link&gt;

Let us know if you have other questions.
		</comment>
	</comments>
</bug>