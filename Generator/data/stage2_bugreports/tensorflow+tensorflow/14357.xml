<bug id='14357' author='naktinis' open_date='2017-11-08T10:32:36Z' closed_time='2018-10-06T20:19:40Z'>
	<summary>Following instructions in batch_normalization docs produces an exception</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 17.10
TensorFlow installed from (source or binary): Binary (pip install tensorflow)
TensorFlow version (use command below): v1.4.0-rc1-11-g130a514 1.4.0
Python version: 3.6.3
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: (see the "Source code" section)

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

I was following the instructions for updating  and  by using the code snippet provided in &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization&gt;batch_normalization documentation&lt;/denchmark-link&gt;
 and it resulted in an "tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value" exception.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

Full instructions to reproduce:
&lt;denchmark-code&gt;git clone -b control-dependencies-exc https://github.com/naktinis/language-id.git ctrl-dep-exc
cd ctrl-dep-exc/
python3 -m venv venv
. venv/bin/activate
pip install tensorflow==1.4.0 Pillow
python3 main.py --image-dir test_data/ --label-file test_data/labels.csv --model rnn
&lt;/denchmark-code&gt;

The specific code change that was enough to produce the exception (seems to match the snippet in the official &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization&gt;documentation&lt;/denchmark-link&gt;
):
&lt;denchmark-link:https://github.com/naktinis/language-id/commit/50740f&gt;naktinis/language-id@50740f&lt;/denchmark-link&gt;

Full exception:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "main.py", line 173, in &lt;module&gt;
    tf.app.run(main=run_experiment)
  File "/home/naktinis/code/ctrl-dep-exc/venv/lib/python3.6/site-packages/tensorflow/python/platform/app.py", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File "main.py", line 165, in run_experiment
    hparams=params,
  File "/home/naktinis/code/ctrl-dep-exc/venv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py", line 218, in run
    return _execute_schedule(experiment, schedule)
  File "/home/naktinis/code/ctrl-dep-exc/venv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py", line 46, in _execute_schedule
    return task()
  File "/home/naktinis/code/ctrl-dep-exc/venv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py", line 625, in train_and_evaluate
    self.train(delay_secs=0)
  File "/home/naktinis/code/ctrl-dep-exc/venv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py", line 367, in train
    hooks=self._train_monitors + extra_hooks)
  File "/home/naktinis/code/ctrl-dep-exc/venv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py", line 807, in _call_train
    hooks=hooks)
  File "/home/naktinis/code/ctrl-dep-exc/venv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py", line 302, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File "/home/naktinis/code/ctrl-dep-exc/venv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py", line 783, in _train_model
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File "/home/naktinis/code/ctrl-dep-exc/venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py", line 521, in run
    run_metadata=run_metadata)
  File "/home/naktinis/code/ctrl-dep-exc/venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py", line 892, in run
    run_metadata=run_metadata)
  File "/home/naktinis/code/ctrl-dep-exc/venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py", line 967, in run
    raise six.reraise(*original_exc_info)
  File "/home/naktinis/code/ctrl-dep-exc/venv/lib/python3.6/site-packages/six.py", line 693, in reraise
    raise value
  File "/home/naktinis/code/ctrl-dep-exc/venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py", line 952, in run
    return self._sess.run(*args, **kwargs)
  File "/home/naktinis/code/ctrl-dep-exc/venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py", line 1024, in run
    run_metadata=run_metadata)
  File "/home/naktinis/code/ctrl-dep-exc/venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py", line 827, in run
    return self._sess.run(*args, **kwargs)
  File "/home/naktinis/code/ctrl-dep-exc/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 889, in run
    run_metadata_ptr)
  File "/home/naktinis/code/ctrl-dep-exc/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/naktinis/code/ctrl-dep-exc/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1317, in _do_run
    options, run_metadata)
  File "/home/naktinis/code/ctrl-dep-exc/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='naktinis' date='2017-11-08T21:04:10Z'>
		It's hard to say if this is a local issue (for which &lt;denchmark-link:http://stackoverflow.com/questions/tagged/tensorflow&gt;Stack Overflow&lt;/denchmark-link&gt;
 may be more helpful) or a documentation bug. &lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
, can you take a look?
		</comment>
		<comment id='2' author='naktinis' date='2017-12-20T21:27:03Z'>
		&lt;denchmark-link:https://github.com/naktinis&gt;@naktinis&lt;/denchmark-link&gt;
 can you provide standalone code snippet to reproduce your issue? Something that could be copy/pasted and run, that would result in the exception you're seeing.
		</comment>
		<comment id='3' author='naktinis' date='2017-12-21T19:48:01Z'>
		&lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 this seems to be enough to reproduce it &lt;denchmark-link:https://gist.github.com/naktinis/4200f955087bb07223f1fc3bb6255528&gt;https://gist.github.com/naktinis/4200f955087bb07223f1fc3bb6255528&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='naktinis' date='2018-01-30T02:52:26Z'>
		A simply modification of the reproduction script no longer produces the error (removing the GRU):
import numpy as np
import tensorflow as tf

class_count = 5
width = 100

# Placeholders
ph = tf.placeholder(shape=[None, width], dtype=tf.float32)
labels_ph = tf.placeholder(shape=[None], dtype=tf.int32)

norm_output_gru = tf.layers.batch_normalization(ph, training=True, axis=1)

# The prediction layer
logits = tf.layers.dense(inputs=norm_output_gru, units=class_count)
onehot_labels = tf.one_hot(labels_ph, depth=class_count)

# Optimizer
loss = tf.losses.softmax_cross_entropy(onehot_labels, logits)
optimizer = tf.train.MomentumOptimizer(learning_rate=0.1, momentum=0.9)

update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    train_op = optimizer.minimize(loss)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    sess.run([train_op, loss],
             feed_dict={ph: np.zeros([1, width]),
                        labels_ph: np.zeros([1])})
This indicates that the problem lies in the combination of GRU and batchnorm updates.
Looking at the stack trace, the error appears in many contexts, but seems most commonly associated with TF RNNs and while loops.
		</comment>
		<comment id='5' author='naktinis' date='2018-01-30T21:05:43Z'>
		/CC &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 can you take a look, since this looks to be an RNN issue?
		</comment>
		<comment id='6' author='naktinis' date='2018-02-14T16:36:45Z'>
		Is this still an issue in TF nightlies?
		</comment>
		<comment id='7' author='naktinis' date='2018-02-15T10:01:43Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 Yes, I just ran the &lt;denchmark-link:https://gist.github.com/naktinis/4200f955087bb07223f1fc3bb6255528&gt;example&lt;/denchmark-link&gt;
 with a fresh  and it seems to still produce the same .
		</comment>
		<comment id='8' author='naktinis' date='2018-02-22T23:10:19Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 the issue still exists in tf nightly. Can you please take another look?
		</comment>
		<comment id='9' author='naktinis' date='2018-02-22T23:18:36Z'>
		&lt;denchmark-link:https://github.com/skye&gt;@skye&lt;/denchmark-link&gt;
 we should be giving better rror messages here.  i believe the update_op is created inside a while loop context; but we're allowing it to be added as a control dependency on an op outside the context.
&lt;denchmark-link:https://github.com/sguada&gt;@sguada&lt;/denchmark-link&gt;
 i remember we had a variant of batch_normalization that didn't require update op collections.  do you recall how?
		</comment>
		<comment id='10' author='naktinis' date='2018-02-23T01:53:47Z'>
		&lt;denchmark-link:https://github.com/jpienaar&gt;@jpienaar&lt;/denchmark-link&gt;
 had some plans to raise a better error when an op inside a while loop is used as a control input to an op outside the loop. I'm not sure what the status or timeline of that is at the moment.
		</comment>
		<comment id='11' author='naktinis' date='2018-04-02T08:10:24Z'>
		I got this error too,  Retval[0] does not have value
I used batch norm after a dynamic_rnn made up of GRUCell, before a dense layer. As I happened to notice, using batch norm WITHOUT incorporating the dynamic_rnn seem to eliminate the issue.
I've no idea if batch norm and dynamic rnn working together could yield better results.
		</comment>
		<comment id='12' author='naktinis' date='2018-04-04T20:05:13Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 it is part of tf.contrib.layers.batch_norm you need to use update_collection=None for that


		</comment>
		<comment id='13' author='naktinis' date='2018-05-07T08:37:19Z'>
		I met the same problem
&lt;denchmark-code&gt;inputs = ....
inputs = RNN op on inputs
inputs = tf.layers.batch_normalization(inputs, training=self.is_train, trainable=True)
.....
update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    self.optimizer = ....
&lt;/denchmark-code&gt;

When removing RNN ops or replacing tf.layers.batch_normalization with tf.contrib.layers.batch_norm(inputs=inputs, updates_collections=None), all things go well. Otherwise,  Retval[0] does not have value occur.
		</comment>
		<comment id='14' author='naktinis' date='2018-05-21T05:38:47Z'>
		I met the same problem using TensorFlow 1,8.0
		</comment>
		<comment id='15' author='naktinis' date='2018-05-31T19:03:58Z'>
		I met the same problem using:

TF 1.5.1 w: CUDA 9.0: Deadlocks the computation instead of throwing an error (you have to kill the process, not even interrupt works)
TF 1.6 CPU: Throws error described above

I've also managed to find a workaround. Saving the update_ops operation and forcing its evaluation explicitly through in session.run() call instead of using with tf.control_dependencies(update_ops)  seems to mitigate the issue.
		</comment>
		<comment id='16' author='naktinis' date='2018-06-21T23:11:49Z'>
		I ran into this issue as well.
I noticed that similar to &lt;denchmark-link:https://github.com/petrroll&gt;@petrroll&lt;/denchmark-link&gt;
's issues with deadlocks on GPU, if you use a MonitoredTrainingSession in TF 1.5.1 it deadlocks instead of throwing an error even on CPU.
		</comment>
		<comment id='17' author='naktinis' date='2018-08-07T19:48:53Z'>
		Have you tried using batch norm with:


tf.contrib.layers.batch_norm you need to use update_collection=None
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Aug 7, 2018, 12:00 PM Alfred Sorten Wolf ***@***.***&gt; wrote:
 Nagging Assignee @ebrevdo &lt;https://github.com/ebrevdo&gt;: It has been 46
 days with no activity and this issue has an assignee. Please update the
 label and/or status accordingly.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#14357 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtim4GSKmqHEznhv2ZnzjtbiDAbAnbGks5uOePlgaJpZM4QWKkb&gt;
 .



		</comment>
		<comment id='18' author='naktinis' date='2018-10-06T18:38:34Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
: It has been 19 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='19' author='naktinis' date='2018-10-06T20:19:40Z'>
		I'm assuming the solutions above suffice.  Please reopen if you continue to have issues with the TF nightlies.
		</comment>
		<comment id='20' author='naktinis' date='2018-11-23T08:02:05Z'>
		

How is the workaround with tf.contrib.layers.batch_norm using updates_collection=None supposed to work exactly? Does it update the batch norm stats with the input iff is_training is True? If it updates the batch norm stats on every input, even when is_training is False, then it's a poor workaround. I can't tell the exact behavior from reading the docs, which just says "one can set updates_collections=None to force the updates in place".


I have a similar problem using a modified version of the @naktinis's gist using Keras layers. I was able to make the problem go away by applying the tf.contrib.layers.batch_norm workaround. So, is the official prescription to never use a recurrent layer followed by a batch norm layers other than a tf.contrib.layers.batch_norm with updates_collections=None? Is it OK to mix Keras layers with non-Keras layers like those from tf.contrib.layers? However, isn't tf.contrib going away with TF 2.0? How should the current problem be addressed then?


		</comment>
		<comment id='21' author='naktinis' date='2018-11-23T16:32:05Z'>
		+Francois Chollet &lt;fchollet@google.com&gt; the keras batchnorm layer needs an
option to work via control dependencies.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Fri, Nov 23, 2018, 12:02 AM Joshua Chia ***@***.*** wrote:

    1.

    How is the workaround with tf.contrib.layers.batch_norm using
    updates_collection=None supposed to work exactly? Does it update the
    batch norm stats with the input iff is_training is True? If it updates
    the batch norm stats on every input, even when is_training is False,
    then it's a poor workaround. I can't tell the exact behavior from reading
    the docs, with just says "one can set updates_collections=None to force the
    updates in place".
    2.

    I have a similar problem using a modified version
    &lt;https://gist.github.com/jchia/4c30e4f6a901d117193c4833680f8be4&gt; of
    the @naktinis &lt;https://github.com/naktinis&gt;'s gist
    &lt;https://gist.github.com/naktinis/4200f955087bb07223f1fc3bb6255528&gt;
    using Keras layers. I was able to make the problem go away by applying the
    tf.contrib.layers.batch_norm workaround. So, is the official
    prescription to never use a recurrent layer followed by a batch norm layers
    other than a tf.contrib.layers.batch_norm with updates_collections=None?
    Is it OK to mix Keras layers with non-Keras layers like those from
    tf.contrib.layers? However, isn't tf.contrib going away with TF 2.0? How
    should the current problem be addressed then?

 —
 You are receiving this because you modified the open/close state.
 Reply to this email directly, view it on GitHub
 &lt;#14357 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtim8vrBaoFIjuk2SvWF10cr6a7zPOhks5ux6ukgaJpZM4QWKkb&gt;
 .



		</comment>
		<comment id='22' author='naktinis' date='2018-11-23T17:18:36Z'>
		&lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;

		</comment>
		<comment id='23' author='naktinis' date='2019-01-25T20:36:53Z'>
		After setting updates_collections=None in tf.contrib.layers.batch_norm, do we also need to do this:
&lt;denchmark-code&gt;update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    self.optimizer = ....
&lt;/denchmark-code&gt;

		</comment>
		<comment id='24' author='naktinis' date='2019-01-26T20:57:27Z'>
		Not necessary, since update_ops would be an empty list.
&lt;denchmark-code&gt;update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
print(update_ops)  # Should print an empty list []
&lt;/denchmark-code&gt;

		</comment>
		<comment id='25' author='naktinis' date='2019-05-09T05:39:12Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;

Does the tf.contrib.layers.batch_norm workaround work if I'm not doing the usual optimizer.minimize() training, but using optimizer.compute_gradients() on 2 small batches, adding up the gradients, and then calling optimizer.apply_gradients() on the summed gradients? Will the BN moving mean &amp; variance get update once every small batch, every other small batch, every apply_gradients() or never?
This is important for cases where the batch size is too big for GPU memory and each batch needs to have gradients calculated separately for smaller sub-batches.
I don't think I have authorization to re-open this issue.
		</comment>
	</comments>
</bug>