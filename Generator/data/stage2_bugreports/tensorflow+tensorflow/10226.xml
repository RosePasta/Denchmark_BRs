<bug id='10226' author='jthestness' open_date='2017-05-26T17:33:03Z' closed_time='2017-05-31T17:30:44Z'>
	<summary>Inconsistent Tensor Initialization on Multiple GPUs</summary>
	<description>
&lt;denchmark-h:h3&gt;The problem (bug?):&lt;/denchmark-h&gt;

I'm having trouble getting consistent initialization of variables across multiple GPUs, and it appears to be a bug. Below is a test that replicates the bug.
Basically, the test just sets up a tensor on each GPU and an initializer for each. After running initialization and grabbing the initialized tensors, they do not match despite the same initializer configurations. The problem exists across multiple platforms, any number of GPUs &gt; 1, and multiple TF versions.
Gory details: The inconsistency is non-deterministic, occurring in about 50% of runs with 2 GPUs. Roughly 0.0002% of matrix values do not match. The matrix indices that do not match have significantly different values (i.e. greater than FP rounding errors). Assuming row-major tensor storage, the incorrect indices are in contiguous groups of 4 floats (16B), and the distance - in memory addresses - between these groups is consistent but platform dependent (e.g. 512kB stride between groups on GTX980 vs. 480kB between groups on K40m)

I have written custom code:

&lt;denchmark-code&gt;import numpy as np
import os
import tensorflow as tf
from tensorflow.python.platform import test


class AllreduceTest(test.TestCase):
    def dumpFailure(self, my_rank, num_ranks, first_output, second_output):
        out_dims = first_output.shape
        assert(len(out_dims) == 2)
        for i in range(out_dims[0]):
            for j in range(out_dims[1]):
                if first_output[i][j] != second_output[i][j]:
                    print("{}: [{}][{}]: {} {}"
                          .format(my_rank, i, j, first_output[i][j],
                                  second_output[i][j]),
                          flush=True)

    def test_mpi_allreduce(self):
        num_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))
        gpu_indices = [index for index in range(num_gpus)]

        mat_dim = 3072

        outputs = []
        for index in gpu_indices:
            with tf.device("/gpu:{}".format(index)):
                initer = tf.random_uniform_initializer(-0.1, 0.1, seed=1234,
                                                       dtype=tf.float32)
                outputs.append(tf.get_variable("outputs-{}".format(index),
                                               shape=(mat_dim, mat_dim),
                                               dtype=tf.float32,
                                               initializer=initer))

        # Session to test initialization across multiple GPUs
        gpu_options = tf.GPUOptions(
            visible_device_list=','.join(str(idx) for idx in gpu_indices))
        config = tf.ConfigProto(gpu_options=gpu_options)
        with tf.Session(config=config) as sess:
            sess.run(tf.global_variables_initializer())
            output_result = sess.run(outputs)
            for index in gpu_indices:
                if not np.allclose(output_result[0], output_result[index]):
                    print("CRAP: Init outputs 0 and {} do not match"
                          .format(index), flush=True)
                    self.dumpFailure(index, num_gpus, output_result[0],
                                     output_result[index])
                    assert(np.allclose(output_result[0],
                                       output_result[index]))

if __name__ == '__main__':
    test.main()
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution: Linux Ubuntu 14.04.2
TensorFlow installed from: source
TensorFlow version: 1.0.1 (tf.GIT_VERSION = b'v1.0.1-0-ge895d5c', tf.COMPILER_VERSION = b'v1.0.1-0-ge895d5c', protobuf = 3.1.0) and 1.1.0-rc2 (tf.GIT_VERSION = b'v1.1.0-rc2-1164-g1d993dd', tf.COMPILER_VERSION = b'v1.1.0-rc2-1164-g1d993dd', protobuf = 3.3.0)
Bazel version: 0.45
Numpy version: 1.12.1
CUDA/cuDNN version: cuda-8.0, cudnn-6
GPU model and memory: GeForce GTX 980, TITAN X Maxwell, Tesla K40m, Tesla M40 24GB

	</description>
	<comments>
		<comment id='1' author='jthestness' date='2017-05-30T15:46:01Z'>
		I have traced this bug back to a race condition in the PhiloxRandom GPU code. Taking a stab at a fix, and will post a PR after testing.
		</comment>
		<comment id='2' author='jthestness' date='2017-05-30T18:13:35Z'>
		Thanks for looking into it, looking forward to a PR.
FYI &lt;denchmark-link:https://github.com/ekelsen&gt;@ekelsen&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='jthestness' date='2017-05-30T19:01:00Z'>
		Created PR here that fixes the race condition: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/10298&gt;#10298&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='jthestness' date='2017-05-31T17:30:37Z'>
		Thanks for creating the fix &lt;denchmark-link:https://github.com/jthestness&gt;@jthestness&lt;/denchmark-link&gt;
!
		</comment>
	</comments>
</bug>