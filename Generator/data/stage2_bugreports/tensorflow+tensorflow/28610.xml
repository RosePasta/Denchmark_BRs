<bug id='28610' author='kenryd' open_date='2019-05-10T20:28:30Z' closed_time='2019-05-20T21:11:44Z'>
	<summary>tf.cast() throws an error using DEVICE_PLACEMENT_EXPLICIT that contradicts the argument's .device</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubunut 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v1.13.1-2-g09e3b09e69 1.13.1
Python version: 2.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: Cuda 10
GPU model and memory: GeForce GTX 1080

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
After getting tf.shape() on a tensor that's on the GPU, the resulting tensor says it's on the GPU. Calling tf.cast() has an error saying it's on the CPU. Explicitly calling .gpu() makes tf.cast() work, even though .device tells me it's on the gpu.
Describe the expected behavior
Either tf.cast() gives me a tensor that tells me it's on the CPU, or else tf.cast() should work correctly. Either the tensor's .device is wrong, or tf.cast() is wrong.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
&lt;denchmark-code&gt;from __future__ import print_function
import tensorflow as tf
tf.enable_eager_execution(device_policy=tf.contrib.eager.DEVICE_PLACEMENT_EXPLICIT)
import numpy as np

# Create an arbitrary tensor on the GPU.
a = tf.convert_to_tensor(np.zeros((5, 5)), dtype=tf.float32).gpu()
print(a.device)  # says it's on the GPU, correctly

b = tf.shape(a)
print(b.device)  # says it's on the GPU

c = tf.cast(b, tf.float32) # error, says first argument is on the CPU
c = tf.cast(b.gpu(), tf.float32)  # no error

print(b.device == b.gpu().device)  # True
&lt;/denchmark-code&gt;

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='kenryd' date='2019-05-13T12:57:34Z'>
		&lt;denchmark-link:https://github.com/kenryd&gt;@kenryd&lt;/denchmark-link&gt;
 Able to reproduce the issue with the provided code.
/job:localhost/replica:0/task:0/device:GPU:0
/job:localhost/replica:0/task:0/device:GPU:0
---&gt; 14 c = tf.cast(b, tf.float32) # error, says first argument is on the CPU
InvalidArgumentError: Tensors on conflicting devices: cannot compute Cast as input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:CPU:0) Tensors can be copied explicitly using .gpu() or .cpu() methods, or transparently copied by using tf.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT). Copying tensors between devices may slow down your model [Op:Cast] name: Cast/
		</comment>
		<comment id='2' author='kenryd' date='2019-05-20T21:11:44Z'>
		You're being bit by the fact that tensorflow doesn't actually store most int32 tensors on the GPU, they're on the GPU device (technically, for graph execution purposes) but stored in host memory.
Drawbacks like this make it quite hard to effectively use explicit device placement.
		</comment>
		<comment id='3' author='kenryd' date='2019-05-20T21:11:45Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=28610&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=28610&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='kenryd' date='2019-05-20T22:08:34Z'>
		
they're on the GPU device... but stored in host memory.

What does it mean to be on the GPU device if it's not in GPU memory &lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 ? In any case, this is still a bug and this does not resolve the issue.
		</comment>
		<comment id='5' author='kenryd' date='2019-05-20T22:22:48Z'>
		I've closed the bug because it's not fixable. TF has a weird definition of
what being on the gpu device is, because of the graph executor. Something
is on the GPU device if the op kernels run in the GPU executor, not if it's
in the GPU memory.

A way conceptualize why this is necessary is the kernel for zeros. It has
to run on the GPU device because it has a GPU output, but its input is a
shape tensor that has to be in the CPU memory because we need it to
determine how much memory to allocate for the output and the allocator runs
on the CPU. So the input to zeros is a host memory tensor in the GPU device
(since it's an input to a GPU kernel). This is incidentally why most int32
operations have inputs and outputs in host memory, as if you need to
slightly preprocess a shape before passing it to zeros you don't want it to
bounce back and forth between the GPU and the host CPU.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, May 20, 2019 at 3:15 PM kenryd ***@***.***&gt; wrote:
 GPU device (technically, for graph execution purposes) but stored in host
 memory.

 What does it mean to be on the GPU device if it's not in GPU memory
 @alextp &lt;https://github.com/alextp&gt; ? In any case, this is still a bug
 and this does not resolve the issue.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#28610?email_source=notifications&amp;email_token=AAABHRPMQ6NOVSQGYMRPYC3PWMPHJA5CNFSM4HMGAKOKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV2GSSA#issuecomment-494168392&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAABHROWNZWP2VUW2RGMLADPWMPHJANCNFSM4HMGAKOA&gt;
 .


-- 
 - Alex

		</comment>
		<comment id='6' author='kenryd' date='2020-11-13T17:06:50Z'>
		
A way conceptualize why this is necessary is the kernel for zeros. It has to run on the GPU device because it has a GPU output, but its input is a shape tensor that has to be in the CPU memory because we need it to determine how much memory to allocate for the output and the allocator runs on the CPU. So the input to zeros is a host memory tensor in the GPU device (since it's an input to a GPU kernel). This is incidentally why most int32 operations have inputs and outputs in host memory, as if you need to slightly preprocess a shape before passing it to zeros you don't want it to bounce back and forth between the GPU and the host CPU.

In this case, is the input to zeros from host memory, then be automatically transferred to GPU memory?
		</comment>
	</comments>
</bug>