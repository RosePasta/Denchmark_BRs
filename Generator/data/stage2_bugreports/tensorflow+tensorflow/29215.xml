<bug id='29215' author='llan-ml' open_date='2019-05-31T13:09:17Z' closed_time='2020-12-08T07:50:03Z'>
	<summary>[TF 2.0] Inconvenient graph visualization</summary>
	<description>
System information

TensorFlow version (use command below): 2.0.0-alpha0
Python version: 3.6.5

When visualizing the graph with tensorboard, there are many nodes, which maybe generated by auotgraph, to make it quite inconvenient to debug with the graph.
To reproduce:
import numpy as np
import tensorflow as tf


class Model(tf.keras.Model):
    def __init__(self):
        super(Model, self).__init__()
        self.dense1 = tf.keras.layers.Dense(100)

    def call(self, inputs):
        outputs = self.dense1(inputs)
        return outputs


model = Model()
optimizer = tf.keras.optimizers.Adam()
writer = tf.summary.create_file_writer("./logdir")


@tf.function
def train(data):
    with tf.name_scope("xxx"):
        with tf.GradientTape() as tape:
            y = model(data)
            loss = tf.reduce_mean(tf.square(y))
        grads = tape.gradient(loss, model.trainable_weights)
        optimizer.apply_gradients(zip(grads, model.trainable_weights))


x = np.random.rand(10, 100).astype(np.float32)
# y = model(x)
# train.python_function(x)
tf.summary.trace_on()
train(x)
with writer.as_default():
    tf.summary.trace_export("graph", step=0)
    tf.summary.trace_off()
    writer.flush()

The graph visualization contains many  and  outside, as follows:
&lt;denchmark-link:https://user-images.githubusercontent.com/22030149/59081332-196c5680-8920-11e9-9b76-bfa482ccf2b2.png&gt;&lt;/denchmark-link&gt;

If we unocmment , the number of outside ops decrease, as follows:
&lt;denchmark-link:https://user-images.githubusercontent.com/22030149/59081425-9bf51600-8920-11e9-8dfb-4c3b813f9f5c.png&gt;&lt;/denchmark-link&gt;

If we uncomment , all  and  decrease:
&lt;denchmark-link:https://user-images.githubusercontent.com/22030149/59081460-c6df6a00-8920-11e9-8448-6e368ef5d573.png&gt;&lt;/denchmark-link&gt;

I think this is quite related to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29442&gt;#29442&lt;/denchmark-link&gt;
 . The keras layers add variables during the first call. If the first call is inside , these variables are created in a graph and some unexpected behaviors happen as reported in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29442&gt;#29442&lt;/denchmark-link&gt;
 .
Uncommenting y = model(x) makes sure that the model variables are created in eager mode, and uncommenting train.python_function(x) makes sure that optimizer-related variables are also created in eager mode.
	</description>
	<comments>
		<comment id='1' author='llan-ml' date='2019-06-03T08:04:32Z'>
		&lt;denchmark-link:https://github.com/llan-ml&gt;@llan-ml&lt;/denchmark-link&gt;
 Code snippet looks incomplete could you provide the complete code to reproduce the issue. Thanks!
		</comment>
		<comment id='2' author='llan-ml' date='2019-06-03T08:47:12Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 I updated the code snippet above.
		</comment>
		<comment id='3' author='llan-ml' date='2019-06-06T20:54:05Z'>
		&lt;denchmark-link:https://github.com/llan-ml&gt;@llan-ml&lt;/denchmark-link&gt;
 Autograph doesn't affect name scopes in the default configuration, so it should not affect the graph display. Can you re-run the snippet setting  to confirm?
&lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 Any thoughts about which ops are created outside the  inside ?
		</comment>
		<comment id='4' author='llan-ml' date='2019-06-07T04:07:13Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 Aftering setting , there is no difference.
As shown in the above picture, the ReadVariableOp and AssignVariableOp are outside.
		</comment>
		<comment id='5' author='llan-ml' date='2019-06-07T04:53:50Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 I updated the issue and it shows more findings now.
		</comment>
		<comment id='6' author='llan-ml' date='2019-06-07T13:18:21Z'>
		Thanks for double-checking. I'm not sure what creates the ops outside the name scope - I suspect it's either the optimizer or the model tracer.
		</comment>
		<comment id='7' author='llan-ml' date='2019-06-25T18:55:22Z'>
		how do you add a keras model to tensorflow graph when you're using GradientTape? No reference available on this ... docs only describe the callbacks with model.fit, seems impossible to add a keras model to tensorboard if using custom training loop
		</comment>
		<comment id='8' author='llan-ml' date='2019-06-26T02:30:21Z'>
		&lt;denchmark-link:https://github.com/bionicles&gt;@bionicles&lt;/denchmark-link&gt;
 The decorator  will automatically transform the code in a function into the graph-style code, and  will trace graph execution.
		</comment>
		<comment id='9' author='llan-ml' date='2019-07-03T15:21:59Z'>
		&lt;denchmark-link:https://github.com/llan-ml&gt;@llan-ml&lt;/denchmark-link&gt;
 thank you
		</comment>
		<comment id='10' author='llan-ml' date='2019-08-16T17:57:08Z'>
		It's not clear to me that this is a Keras issue as opposed to a more general issue with how Tensorboard visualizes variable assignment operators when variables are captured from tf.functions vs. created in a run of the tf.function.
That said, we've made a lot of changes to Keras's execution path recently, so if you try the new nightly you might get a less noisy visualization. Meanwhile, re-assigning the more-general question of how to visualize assignment operators to the tensorboard team for triaging.
		</comment>
		<comment id='11' author='llan-ml' date='2019-08-23T17:09:05Z'>
		Reassigning to davidsoergel to prioritize for the Graph plugin.
		</comment>
		<comment id='12' author='llan-ml' date='2019-10-01T20:37:48Z'>
		I ran &lt;denchmark-link:https://github.com/llan-ml&gt;@llan-ml&lt;/denchmark-link&gt;
 code in recently released  and see simpler graph than the one with . &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/129a7c6950a0c001da57319aa0adaad1/tf_29215.ipynb&gt;Here&lt;/denchmark-link&gt;
 is the gist with . Thanks!
		</comment>
		<comment id='13' author='llan-ml' date='2019-10-02T04:54:38Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 Thanks for your effort!
For now, there are still some extra internal ops existing like
&lt;denchmark-link:https://user-images.githubusercontent.com/22030149/66018682-9d230200-e512-11e9-9c3e-25f2415bde6f.png&gt;&lt;/denchmark-link&gt;

It would be better if either we can batch these extra ops (except for data) within a predefined or customized scope or they are removed from the main graph by default.
		</comment>
		<comment id='14' author='llan-ml' date='2020-03-10T19:53:24Z'>
		
@bionicles The decorator tf.function will automatically transform the code in a function into the graph-style code, and tf.summary.trace_on will trace graph execution.

When I do this and open tensorboard using tensorboard --logdir logdirlocation only a scalars tab opens up. Do you happen to know why the graphs tab does not appear? I have tf.summary.trace_on defined after I instantiated my models.
		</comment>
		<comment id='15' author='llan-ml' date='2020-04-11T01:48:50Z'>
		I have the same problem with graph visualization.
I used to rely on TB graph visualization in TF 1.X for sanity checks and it was extremely useful.
Now, it's frustrating to normally keep track of the I/O flow of each block.
There are plenty of nodes with this naming pattern: XXXX_cast_readvariableop_resource
Putting a custom model in Keras also doesn't work for visualizing a conceptual graph.
		</comment>
		<comment id='16' author='llan-ml' date='2020-12-07T14:29:18Z'>
		For those who still suffer from this issue, I found that we can first convert the custom model into a functional model (i.e., initialize the model &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/25fba035f3e453d94490932096282c7b0624bbb3/tensorflow/python/keras/engine/network.py#L173&gt;in a graph manner&lt;/denchmark-link&gt;
) and then visualize the graph with , which usually results in a much better graph visualization than that generated as in this &lt;denchmark-link:https://www.tensorflow.org/tensorboard/graphs#graphs_of_tffunctions&gt;link&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='17' author='llan-ml' date='2020-12-08T07:50:02Z'>
		I believe that this issue has been resolved with the new public summary operation &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/summary/graph&gt;tf.summary.graph&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='18' author='llan-ml' date='2020-12-08T07:50:04Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29215&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29215&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>