<bug id='32842' author='Efaq' open_date='2019-09-26T14:58:34Z' closed_time='2019-09-30T16:48:28Z'>
	<summary>Using Dataset.map with python dicts as dataset elements passes Tensors without numpy values to the mapping function</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
TensorFlow installed from (source or binary): Pycharm, pip-install
TensorFlow version (use command below): v2.0.0-rc1-51-g2646d23074 2.0.0-rc2
Python version: 3.6
Bazel version (if compiling from source): n/a
GCC/Compiler version (if compiling from source): n/a
CUDA/cuDNN version: n/a
GPU model and memory: n/a

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
I'm doing a simple pipeline where I load a dataset from a csv file, convert the dataset to a dict structure (where one element of the dataset is a dict with strings as keys and tf.Tensor as values) and then try to use this resulting dataset. The issue is that, when I create the dict-based dataset, I can see that the numpy values are there in the Tensors, BUT when I use Dataset.map to apply a function to the dict-based dataset, I can't access the numpy values inside this function. The numpy values ""disappear"".
Describe the expected behavior
I should be able to see and use the numpy values.
Code to reproduce the issue
IMPORTANT: replace the TODO in the beginning of the code with the path that the csv I provided after the code, below, will have in your system.
&lt;denchmark-code&gt;import os
import tensorflow as tf

#TODO REPLACE THE LINE BELOW BY ONE THAT DESCRIBES THE PATH OF THE CSV IN YOUR SYSTEM
data_source = os.path.join(".", "Data", "mini.csv")

#Help function to convert tuples of Tensors to dicts, so I have the column names from the csv as keys
def _convert_to_dict(*el):
    base_list = ["var1", "var2", "var3"] #as in the csv header
    dicto = dict()
    for i in range(len(base_list)):
        dicto[base_list[i]] = el[i]
    return dicto

#Simple function which will try to investigate and use the numpy values inside the Tensors in the datapoints
def _use_dict(dicto, key):
    """
    dicto: dict with string keys and Tensors as values
    :param key: a specific string key
    :return:
    """
    print("Printing inspections from _use_dict function")
    print(dicto) #Here we print the dataset element passed to the function, which is a dictionary, and see that no numpy value is associated with the Tensors
    print(dicto[key]) #We look closer, no numpy values
    print(dicto[key].numpy()) #No numpy value, so we get an exception here
    return None

def load_and_preprocess_dataset(): #Main function which will load the dataset from csv and transform to a dict-like structure
    #Load the dataset from CSV
    dataset = tf.data.experimental.CsvDataset(
        header=True,
        filenames=data_source,
        record_defaults=[
            tf.int64,
            tf.string,
            tf.float64,
        ]
    )
    #Convert each element from the dataset from a tuple of Tensors to a dict with keys being the csv header values, and values being the corresponding Tensors
    dataset = dataset.map(map_func=_convert_to_dict)
    #Here we inspect the values of the dataset, and observe that the Tensors contain numpy values
    print("Printing dataset elements after transforming to dict-like structure")
    for a in dataset:
        print(a)
    #Here we try to apply a function which accepts a dict-like structured dataset element as input
    dataset = dataset.map(map_func=lambda x: _use_dict(dicto=x, key="var1"))
    return dataset

if __name__ == '__main__':
    dataset = load_and_preprocess_dataset()
&lt;/denchmark-code&gt;

Csv file content:
&lt;denchmark-code&gt;var1,var2,var3
2,"foo",1.3
3,"bar",1.5
4,"barfoo",1.8
&lt;/denchmark-code&gt;

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
&lt;denchmark-code&gt;Stack trace:

2019-09-26 16:49:14.948332: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
Printing dataset elements after transforming to dict-like structure
{'var1': &lt;tf.Tensor: id=27, shape=(), dtype=int64, numpy=2&gt;, 'var2': &lt;tf.Tensor: id=28, shape=(), dtype=string, numpy=b'foo'&gt;, 'var3': &lt;tf.Tensor: id=29, shape=(), dtype=float64, numpy=1.3&gt;}
{'var1': &lt;tf.Tensor: id=30, shape=(), dtype=int64, numpy=3&gt;, 'var2': &lt;tf.Tensor: id=31, shape=(), dtype=string, numpy=b'bar'&gt;, 'var3': &lt;tf.Tensor: id=32, shape=(), dtype=float64, numpy=1.5&gt;}
{'var1': &lt;tf.Tensor: id=33, shape=(), dtype=int64, numpy=4&gt;, 'var2': &lt;tf.Tensor: id=34, shape=(), dtype=string, numpy=b'barfoo'&gt;, 'var3': &lt;tf.Tensor: id=35, shape=(), dtype=float64, numpy=1.8&gt;}
Printing inspections from _use_dict function
{'var1': &lt;tf.Tensor 'args_0:0' shape=() dtype=int64&gt;, 'var2': &lt;tf.Tensor 'args_1:0' shape=() dtype=string&gt;, 'var3': &lt;tf.Tensor 'args_2:0' shape=() dtype=float64&gt;}
Tensor("args_0:0", shape=(), dtype=int64)
Traceback (most recent call last):
  File "...temp.py", line 50, in &lt;module&gt;
    dataset = load_and_preprocess_dataset()
  File "...temp.py", line 46, in load_and_preprocess_dataset
    dataset = dataset.map(map_func=lambda x: _use_dict(dicto=x, key="var1"))
  File "...site-packages\tensorflow_core\python\data\ops\dataset_ops.py", line 1211, in map
    return MapDataset(self, map_func, preserve_cardinality=True)
  File "...site-packages\tensorflow_core\python\data\ops\dataset_ops.py", line 3416, in __init__
    use_legacy_function=use_legacy_function)
  File "...site-packages\tensorflow_core\python\data\ops\dataset_ops.py", line 2695, in __init__
    self._function = wrapper_fn._get_concrete_function_internal()
  File "...site-packages\tensorflow_core\python\eager\function.py", line 1854, in _get_concrete_function_internal
    *args, **kwargs)
  File "...site-packages\tensorflow_core\python\eager\function.py", line 1848, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File "...site-packages\tensorflow_core\python\eager\function.py", line 2150, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "...site-packages\tensorflow_core\python\eager\function.py", line 2041, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File "...site-packages\tensorflow_core\python\framework\func_graph.py", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "...site-packages\tensorflow_core\python\data\ops\dataset_ops.py", line 2689, in wrapper_fn
    ret = _wrapper_helper(*args)
  File "...site-packages\tensorflow_core\python\data\ops\dataset_ops.py", line 2634, in _wrapper_helper
    ret = autograph.tf_convert(func, ag_ctx)(*nested_args)
  File "...site-packages\tensorflow_core\python\autograph\impl\api.py", line 237, in wrapper
    raise e.ag_error_metadata.to_exception(e)
AttributeError: in converted code:
    relative to ...:

    temp.py:46 None  *
        dataset = dataset.map(map_func=lambda x: _use_dict(dicto=x, key="var1"))
    temp.py:25 _use_dict  *
        print(dicto[key].numpy()) #No numpy value, so we get an exception here

    AttributeError: 'Tensor' object has no attribute 'numpy'


Process finished with exit code 1

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Efaq' date='2019-09-26T14:59:33Z'>
		I wonder if it is related to the closed (but maybe not solved?) issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/27519&gt;#27519&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='Efaq' date='2019-09-27T04:45:02Z'>
		Try this on top-
tf.enable_eager_execution()
		</comment>
		<comment id='3' author='Efaq' date='2019-09-27T05:02:17Z'>
		&lt;denchmark-link:https://github.com/Efaq&gt;@Efaq&lt;/denchmark-link&gt;
 ,
Can you try &lt;denchmark-link:https://github.com/AkashNagaraj&gt;@AkashNagaraj&lt;/denchmark-link&gt;
 suggestion? Thanks!
		</comment>
		<comment id='4' author='Efaq' date='2019-09-27T08:29:01Z'>
		&lt;denchmark-link:https://github.com/AkashNagaraj&gt;@AkashNagaraj&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/oanush&gt;@oanush&lt;/denchmark-link&gt;
 Tried it now, and I'm getting:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "...", line 3, in &lt;module&gt;
    tf.enable_eager_execution()
AttributeError: module 'tensorflow' has no attribute 'enable_eager_execution'

Process finished with exit code 1
&lt;/denchmark-code&gt;

I'm in Tensorflow 2.0 rc2, so I supposed the eager execution would be enabled by default. Shouldn't it be the case?
Also, tf.executing_eagerly() returns True, what would make me believe that I'm already in eager execution.
Thank you for the help!
		</comment>
		<comment id='5' author='Efaq' date='2019-09-27T08:30:22Z'>
		&lt;denchmark-link:https://github.com/oanush&gt;@oanush&lt;/denchmark-link&gt;
 just noticed that you labeled this issue as , while I have the version 
		</comment>
		<comment id='6' author='Efaq' date='2019-09-30T10:09:28Z'>
		I was able to replicate the issue for TF-2.0rc2 and TF-nightly-2.0-preview, kindly find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/oanush/7cb7982ed907c4ed4f534fb029a904e3/32842.ipynb&gt;gist&lt;/denchmark-link&gt;
 of colab.Thanks!
		</comment>
		<comment id='7' author='Efaq' date='2019-09-30T10:11:19Z'>
		
@oanush just noticed that you labeled this issue as rc0, while I have the version v2.0.0-rc1-51-g2646d23074 2.0.0-rc2

&lt;denchmark-link:https://github.com/Efaq&gt;@Efaq&lt;/denchmark-link&gt;
 ,
We just have 2.0rc0 label available now as the latest version in Github.
		</comment>
		<comment id='8' author='Efaq' date='2019-09-30T13:42:57Z'>
		&lt;denchmark-link:https://github.com/oanush&gt;@oanush&lt;/denchmark-link&gt;
 alright!
		</comment>
		<comment id='9' author='Efaq' date='2019-09-30T16:48:28Z'>
		This is expected behavior. tf.data traces all of its user-defined functions and data objects observable by user-defined functions will be s and not s (and s do not have ). If you would like to execute (a subset of) tf.data user-defined function in eager mode, use &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/py_function&gt;py_function&lt;/denchmark-link&gt;
. Note that the use of  has limitations, including no parallel execution (because of GIL) and precluding TensorFlow from applying static optimizations to your program.
		</comment>
		<comment id='10' author='Efaq' date='2019-09-30T16:48:29Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=32842&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=32842&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='Efaq' date='2019-10-01T07:16:33Z'>
		&lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 Alright, thanks! Could you please exemplify how to change the code provided in order to correctly run eagerly?
I was really expecting that TF 2.0, when saying it has eager execution by default, would really execute the whole code eagerly, and not only parts of it...
		</comment>
		<comment id='12' author='Efaq' date='2019-10-23T06:53:09Z'>
		&lt;denchmark-link:https://github.com/Efaq&gt;@Efaq&lt;/denchmark-link&gt;
 How did you resolve this issue?
		</comment>
		<comment id='13' author='Efaq' date='2019-10-23T08:21:37Z'>
		&lt;denchmark-link:https://github.com/AlbinDavid&gt;@AlbinDavid&lt;/denchmark-link&gt;
 I haven't. Asked some extra help from &lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 above but haven't gotten an example.
More specifically, after 2 years dealing with Tensorflow and more recently with Tensorflow Extended, I changed my attention to scikit-learn and pure Keras and I have never been happier. So if you are not tied to very high performance, I would recommend considering the same.
Cheers
		</comment>
		<comment id='14' author='Efaq' date='2019-10-23T08:43:42Z'>
		&lt;denchmark-link:https://github.com/Efaq&gt;@Efaq&lt;/denchmark-link&gt;
 thanks for your response
		</comment>
		<comment id='15' author='Efaq' date='2020-01-23T14:56:18Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/30653&gt;#30653&lt;/denchmark-link&gt;

		</comment>
		<comment id='16' author='Efaq' date='2020-09-15T07:58:37Z'>
		Following code worked:
&lt;denchmark-code&gt;def parse_str(str_tensor):
    raw_string = str_tensor.numpy().decode("utf-8") 

    # play with raw string
    raw_string = 'AAA'+raw_string     
    return raw_string
&lt;/denchmark-code&gt;

Call parse function:
&lt;denchmark-code&gt;def tf_pre_processing(row):
  return tf.py_function(parse_str, [row['context']], [tf.string])


train = t.map(tf_pre_processing).batch(1).take(1)

list(train)
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>