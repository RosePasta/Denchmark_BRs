<bug id='13187' author='balaji-in-git' open_date='2017-09-20T15:20:07Z' closed_time='2018-04-10T22:40:51Z'>
	<summary>parse_example is awfully slow</summary>
	<description>
&lt;denchmark-link:https://github.com/skearnes&gt;@skearnes&lt;/denchmark-link&gt;

you have indicated in this post of yours &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/390&gt;#390&lt;/denchmark-link&gt;
 way back in 2015 that parse_example is about 30 times faster than parse_single_example.
I have tried different options to modify my simple training script which only prints about 100000 tfrecords batched in 1000 and just does a print of feature and label after session.run(feature, label). Feature is a sparseTensor BTW.
Can you please put a test sample which proved that parse_example was that fast. parse_single_example was taking ~320 secs, now parse_example takes ~240 secs.
@Admin, please do not close this issue and refer to stackoverflow, as I don't think this is something to do with API usage or wrong parameters.
This is to do with the performance of queues (enqueue, dequeue) &amp; threads
	</description>
	<comments>
		<comment id='1' author='balaji-in-git' date='2017-09-21T16:07:38Z'>
		It's possible there was performance regression in parse_example
For this to get resolved there needs to be at minimum a reproducible example, and ideally, some investigation of the cause of the slowness


It takes 240 secs, how much do you expect it to take? Is it possible that it's as as fast as it can be? Similar discussion with parse_csv slow
#11713


gperftools profile to show where time is spent is useful, like here
#3009 (comment)


		</comment>
		<comment id='2' author='balaji-in-git' date='2017-09-22T12:02:44Z'>
		just to add more information to debug, here is my lscpu
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                4
On-line CPU(s) list:   0-3
Thread(s) per core:    1
Core(s) per socket:    2
Socket(s):             2
NUMA node(s):          1
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 79
Model name:            Intel(R) Xeon(R) CPU E5-2683 v4 @ 2.10GHz
Stepping:              1
CPU MHz:               2095.148
BogoMIPS:              4190.29
Hypervisor vendor:     VMware
Virtualization type:   full
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              40960K
NUMA node0 CPU(s):     0-3
		</comment>
		<comment id='3' author='balaji-in-git' date='2017-09-22T12:10:54Z'>
		Each TFRecord is a sparse structure of feature shape  1000 222215 viz. basically I am making batches of 1000 features. TFRecordWriter works faster as compared to burning a 1000 examples of (1x222215)  . My labels are on one-hot with 58565 labels and 1000 rows again to match 1000 features.
Using SparseFeature and parse_example is faster compared to using parse_single_example and/or VarLenFeature. But not significantly fast. (I didn't do decode_csv, it would have been worse anyway.)
With this structure, I am able to train 400000 examples trained in batches of 1000 (each TFRecord) and 5 epochs in 861-900 secs. Is this the best I can get. I am ONLY printing the tensors. No matmuls, no optimizers.
I use 4 producer queues (string_input_producer), to read 8 TFRecord files (2 each) and shuffle_batch with 1-2 threads. If I use 4 threads, as is the case of shuffle_batch_join which creates as many number of threads to enqueue as the tensors in the list viz., 4 in my case, I get Timedout error waiting for notification. So I believe producers don't get enough time if I do that.
Now the question back, is this the best I can get on the said hardware?
		</comment>
		<comment id='4' author='balaji-in-git' date='2017-09-22T13:18:08Z'>
		That was the story with 1.2.1. I thought I should upgrade to 1.3 and now I did a bazel build hoping for better. Unfortunately, it's got worse. Same run above, now takes 1184 secs.
Does Tensorflow require some minimum hardware to show it's muscles.
		</comment>
		<comment id='5' author='balaji-in-git' date='2017-09-22T14:12:01Z'>
		I think the input pipeline is getting slower. I am sure if I do away with TFRecords and queues and feed the sparse matrix directly, processing using scipy or some other python library, it's been running faster earlier, though it was single threaded and may not be using all the logical cores concurrently.
		</comment>
		<comment id='6' author='balaji-in-git' date='2017-09-22T15:03:43Z'>
		There are many ways to get slow performance that is not a bug in TensorFlow (ie, wrong choice of threads, wrong data format, wrong design of pipeline etc). Stackoverflow can help with that with tag "tensorflow". It could be a bug in parse_example op. If you think it's a bug in parse_example, it would help with extra details I mentioned above (reproducible example + performance profile)
		</comment>
		<comment id='7' author='balaji-in-git' date='2017-09-27T14:27:06Z'>
		I reduced the number of labels in the training examples from about 70000, which is the super set to about 1500 which is the number labels that's in the training set. Now the dry run, (WITHOUT matmuls, cost optimizers etc.) runs very fast. About 0.5 mn training examples pipelined and get enqueued/dequeued in about 17 secs and 5 epochs. That's about 1 million training examples in about 8-9 secs. I am not sure how &amp; why this correlates internally, the features are SparseFeature and label is FixedLenFeature.
Now the big question, if I have to train with all the 70000 labels, will the performance again take a beating.
		</comment>
		<comment id='8' author='balaji-in-git' date='2017-10-13T11:25:01Z'>
		Update: I think the slowness has nothing to do with the number of labels. I removed one-hot dense vector and made it one-hot sparse and it's as fast as otherwise viz., its just a fixedLenFeature with the batch of numbers.
What helped me is to write a batch of say 1000 rows into tfrecords and read it as a single_example (though I know its a sparse-batch example) and treat it accordingly. Writing a batch of examples into tfrecords is definitely faster than writing each feature row/matrix, vector.
But parse_example is nowhere faster than parse_single_example as claimed here &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/390&gt;#390&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='9' author='balaji-in-git' date='2017-11-28T13:56:07Z'>
		&lt;denchmark-link:https://github.com/balaji-in-git&gt;@balaji-in-git&lt;/denchmark-link&gt;
 ,I think parse_example is much faster than parse_single_example. in my case , speed change from 25000 sample/second to 32000 sample/second
		</comment>
		<comment id='10' author='balaji-in-git' date='2017-12-11T23:07:26Z'>
		&lt;denchmark-link:https://github.com/balaji-in-git&gt;@balaji-in-git&lt;/denchmark-link&gt;
, can you provide code to verify this regression? Something that used to be fast but is now slow?
		</comment>
		<comment id='11' author='balaji-in-git' date='2018-01-03T07:42:58Z'>
		It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue? Please update the label and/or status accordingly.
		</comment>
		<comment id='12' author='balaji-in-git' date='2018-01-17T19:15:33Z'>
		Nagging Awaiting Response: It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='13' author='balaji-in-git' date='2018-02-06T07:47:58Z'>
		Nagging Awaiting Response: It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='14' author='balaji-in-git' date='2018-02-20T19:40:26Z'>
		Nagging Awaiting Response: It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='15' author='balaji-in-git' date='2018-03-07T13:18:17Z'>
		Nagging Awaiting Response: It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='16' author='balaji-in-git' date='2018-03-25T00:55:08Z'>
		It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='17' author='balaji-in-git' date='2018-04-10T22:40:51Z'>
		Closing due to lack of activity but please reopen if there's new information.
		</comment>
	</comments>
</bug>