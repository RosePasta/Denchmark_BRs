<bug id='32043' author='nsantavas' open_date='2019-08-28T12:55:17Z' closed_time='2019-09-27T17:43:17Z'>
	<summary>[TF 2.0.0rc0] Cannot connect to TPU device</summary>
	<description>
Created VM and v3-8 TPU with ctpu up command and updated TF version to TF2.0.0rc0 via pip3.
When i try to connect to tpu device returns error:
&lt;denchmark-code&gt;InvalidArgumentError: Unable to find a context_id matching the specified one (5613663074031560004). Perhaps the worker was restarted, or the context was GC'd?
Additional GRPC error information:
{"created":"@1566994715.938381293","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Unable to find a context_id matching the specified one (5613663074031560004). Perhaps the worker was restarted, or the context was GC'd?","grpc_status":3}
2019-08-28 12:18:36.196440: E tensorflow/core/distributed_runtime/rpc/eager/grpc_eager_client.cc:72] Remote EagerContext with id 5613663074031560004 does not seem to exist.
&lt;/denchmark-code&gt;

I also tried the same in Colab, with rc0 version and i have the same error. The code i used is the one given in documentation:
&lt;denchmark-code&gt;tpu='test'
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu)
tf.config.experimental_connect_to_host(resolver.master())
tf.tpu.experimental.initialize_tpu_system(resolver)
tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='nsantavas' date='2019-08-28T18:21:12Z'>
		Same situation here. I am trying to train a NN in a TPU in eager mode. Following the advice given in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/30129&gt;#30129&lt;/denchmark-link&gt;
 by &lt;denchmark-link:https://github.com/capilano&gt;@capilano&lt;/denchmark-link&gt;
, I switched to the nightly build: 2.0.0-dev20190731, and I found this error.
		</comment>
		<comment id='2' author='nsantavas' date='2019-08-29T06:44:50Z'>
		&lt;denchmark-link:https://github.com/jmgc&gt;@jmgc&lt;/denchmark-link&gt;
 TPUs do not support eager execution. Try training without enabling it. For more info look at the question 11 &lt;denchmark-link:https://cloud.google.com/tpu/docs/faq&gt;here&lt;/denchmark-link&gt;
.
&lt;denchmark-link:https://github.com/nsantavas&gt;@nsantavas&lt;/denchmark-link&gt;
 maybe you are also using eager execution?
		</comment>
		<comment id='3' author='nsantavas' date='2019-08-29T07:04:55Z'>
		&lt;denchmark-link:https://github.com/rishabhsahrawat&gt;@rishabhsahrawat&lt;/denchmark-link&gt;
 no
		</comment>
		<comment id='4' author='nsantavas' date='2019-08-29T07:19:04Z'>
		&lt;denchmark-link:https://github.com/nsantavas&gt;@nsantavas&lt;/denchmark-link&gt;
 You have to pass the TPU_address in the first line,in colab you can leave it blank, but the TPU would default to using 1.14 in colab,even if you change the tensorflow version to 2.0 in the colab VM.
If you are using TF2.0 , tf.function will automatically handle graphs, so the input pipeline can be executed in eager mode.
Edit:  Here is an example that uses TF 2.0, which uses eager by default
&lt;denchmark-link:https://github.com/tensorflow/tpu/blob/master/models/experimental/resnet50_keras/resnet50_tf2.py&gt;https://github.com/tensorflow/tpu/blob/master/models/experimental/resnet50_keras/resnet50_tf2.py&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='nsantavas' date='2019-08-29T10:59:46Z'>
		I am currently facing the same problem.
I also use v3-8 TPU and tf2.0 rc0.
&lt;denchmark-link:https://github.com/capilano&gt;@capilano&lt;/denchmark-link&gt;

Which version of tensorflow can run &lt;denchmark-link:https://github.com/tensorflow/tpu/blob/master/models/experimental/resnet50_keras/resnet50_tf2.py&gt;that link code&lt;/denchmark-link&gt;
?
I think this is not tf2.0 code because  and  are called in the source code.
&lt;denchmark-link:https://github.com/tensorflow/tpu/blob/aada81f730478e66ed736aea5db33fc9895341a4/models/experimental/resnet50_keras/resnet50_tf2.py#L34&gt;https://github.com/tensorflow/tpu/blob/aada81f730478e66ed736aea5db33fc9895341a4/models/experimental/resnet50_keras/resnet50_tf2.py#L34&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tpu/blob/aada81f730478e66ed736aea5db33fc9895341a4/models/experimental/resnet50_keras/resnet50_tf2.py#L170&gt;https://github.com/tensorflow/tpu/blob/aada81f730478e66ed736aea5db33fc9895341a4/models/experimental/resnet50_keras/resnet50_tf2.py#L170&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/goldiegadde&gt;@goldiegadde&lt;/denchmark-link&gt;

I want to know the status of TPU support on tensorflow 2.0rc0.
I have no other information than release notes and tensorflow guide.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/releases&gt;https://github.com/tensorflow/tensorflow/releases&lt;/denchmark-link&gt;

&lt;denchmark-link:https://www.tensorflow.org/beta/guide/distribute_strategy&gt;https://www.tensorflow.org/beta/guide/distribute_strategy&lt;/denchmark-link&gt;

In the current situation I don't know if this is a bug to report or just not supported.
		</comment>
		<comment id='6' author='nsantavas' date='2019-08-29T12:08:24Z'>
		&lt;denchmark-link:https://github.com/pshiko&gt;@pshiko&lt;/denchmark-link&gt;
 I am not really sure, I think you can call that in 2.0 and 1.0. I have seen people post TF2.0 and TPU issues(training issues) , so there is support, maybe there is a bug. Or possibly you could try TF 2.0-nightly.
Personally, I do not have access to  a TPU that has tensorflow 2.0, so all my code is on 1.14 and I have eager disabled. Apparently later versions of 1.0 also support eager training similar to 2.0 if you call those lines.
		</comment>
		<comment id='7' author='nsantavas' date='2019-08-29T18:24:08Z'>
		Hello! I don't think there is an official TPU release for 2.0 (i.e. you cannot specify --version=2.0 when creating a TPU with ctpu or gcloud) so it is currently unsupported.
		</comment>
		<comment id='8' author='nsantavas' date='2019-08-29T18:27:00Z'>
		In general, we anticipate that TF 2.1 (coming later this year) will have much better support for TPUs than the initial 2.0 release.
		</comment>
		<comment id='9' author='nsantavas' date='2019-08-29T18:34:34Z'>
		Yes, but if you update VM host via pip to TF-nightly-2.0.0rc0 ?
		</comment>
		<comment id='10' author='nsantavas' date='2019-08-29T19:18:09Z'>
		No, the TPU itself must have the same version as the VM host, or the configuration is not supported.
		</comment>
		<comment id='11' author='nsantavas' date='2019-08-30T11:20:16Z'>
		&lt;denchmark-link:https://github.com/frankchn&gt;@frankchn&lt;/denchmark-link&gt;

Thank you for your kind reply !!
I understood the current status of TPU support.
According to &lt;denchmark-link:https://www.tensorflow.org/beta/guide/distribute_strategy&gt;https://www.tensorflow.org/beta/guide/distribute_strategy&lt;/denchmark-link&gt;
 ,  with  support is planned in 2.0RC.
But the situation seems to have change.
I will consider using TF1.x for a while.
Thank you!!
		</comment>
		<comment id='12' author='nsantavas' date='2019-08-30T17:16:17Z'>
		&lt;denchmark-link:https://github.com/pshiko&gt;@pshiko&lt;/denchmark-link&gt;
 Yeah, we have been working on that, but only a small subset of customers have access to the internal TF2.0 image for the TPU itself, so it is not broadly available yet.
Otherwise, running a TF2.0 VM host with a TF1.14 TPU is not compatible because the 1.14 TPU (having been released months prior to any 2.0 release) would lack code that supports the new features in 2.0.
		</comment>
		<comment id='13' author='nsantavas' date='2019-09-27T17:43:18Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=32043&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=32043&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>