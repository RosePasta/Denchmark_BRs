<bug id='39336' author='hroetsc' open_date='2020-05-09T07:11:31Z' closed_time='2020-05-17T12:40:58Z'>
	<summary>Distributed Training on multiple nodes - process gets stuck after initializing grpc channel</summary>
	<description>
System information

TF v.2.1
Linux-based HPC
Snakemake workflow manager
Slurm as scheduler
TensorFlow installed from (source or binary): virtual conda environment
Python version: 3.7.6
CUDA/cuDNN version: 10.1
GPU model and memory: GeForce GTX 980 computeCapability: 5.2,  coreClock: 1.2405GHz coreCount: 16 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 208.91GiB/s

Describe the current behavior
Hi there,
My aim is to train a neural net on multiple nodes and GPUs on a HPC. Therefore, I am using TF's MultiWorkerMirroredStrategy and the SlurmClusterResolver to get the configuration of my nodes and to set the TF_CONFIG variable.
However, when trying to connect to the cluster using:
&lt;denchmark-code&gt;tf.config.experimental_connect_to_cluster(resolver,
                                            job_name = 'worker',
                                            task_index = cfg['task']['index'],
                                            protocol = 'grpc')
&lt;/denchmark-code&gt;

the process gets stuck (please see the log below).
When I ssh into the nodes I see my processes there but they are all sleeping. GPUs aren't used either.
Has anyone experience with this kind of setup and can provide help?
If I don't use the experimental_connect_to_cluster every node is executing the job independently, instead of working together.
I know that TF_CONFIG should be set before calling the MultiWorkerMirroredStrategy but this caused a RunTimeError.
I also played around with the position of GPU initialization in the code - this doesn't seem to have an effect.
Note that I am using the most recent version of the SlurmClusterResolver, I basically copied the script from the GitHub and call it using slurm_cluster_resolver.SlurmClusterResolver
Standalone code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf

tf.keras.backend.clear_session()
# print all tensor allocations
tf.debugging.set_log_device_placement(True)
tf.random.set_seed(42)

# instantiate strategy at program startup to prevent RuntimeError
print("-------------------------------------------------------------------------")

print("DEFINE DISTRIBUTED TRAINING STRATEGY")
# only RING communication uses grpc protocols

multiworker_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(communication=tf.distribute.experimental.CollectiveCommunication.NCCL)


import os
import sys
import json
import gc
import numpy as np
import pandas as pd



print("-------------------------------------------------------------------------")

print('CLUSTER CONFIGURATION')

def set_tf_config(resolver, environment=None):
    """Set the TF_CONFIG env variable from the given cluster resolver"""
    cfg = {
        'cluster': resolver.cluster_spec().as_dict(),
        'task': {
            'type': resolver.get_task_info()[0],
            'index': resolver.get_task_info()[1],
        },
        'rpc_layer': resolver.rpc_layer,
    }
    if environment:
        cfg['environment'] = environment
    os.environ['TF_CONFIG'] = json.dumps(cfg)

    return cfg


# there must be one GPU for every task
resolver = slurm_cluster_resolver.SlurmClusterResolver(port_base = 11214, gpus_per_task=1, tasks_per_node=2)

print("-------------------------------------------------------------------------")

cfg = set_tf_config(resolver)
tf.print(cfg)
print(cfg)


print('CONNECT TO CLUSTER')

tf.config.experimental_connect_to_cluster(resolver,
                                            job_name = 'worker',
                                            task_index = cfg['task']['index'],
                                            protocol = 'grpc')

print("-------------------------------------------------------------------------")

# doesn't matter if at the beginning or not
print("GPU CONFIGURATION")
# allow memory growth

gpus = tf.config.experimental.list_physical_devices('GPU')

print('INITIALIZE GPUs')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

if gpus:
    tf.config.experimental.set_visible_devices(gpus, 'GPU')
    print(gpus)

print("-------------------------------------------------------------------------")



&lt;/denchmark-code&gt;

Other info / logs
&lt;denchmark-code&gt;2020-05-09 08:40:42.220277: I tensorflow/core/common_runtime/eager/execute.cc:573] Executing op StringFormat in device /job:localhost/replica:0/task:0/device:CPU:0
2020-05-09 08:40:42.220277: I tensorflow/core/common_runtime/eager/execute.cc:573] Executing op StringFormat in device /job:localhost/replica:0/task:0/device:CPU:0
2020-05-09 08:40:42.220607: I tensorflow/core/common_runtime/eager/execute.cc:573] Executing op PrintV2 in device /job:localhost/replica:0/task:0/device:CPU:0
2020-05-09 08:40:42.220639: I tensorflow/core/common_runtime/eager/execute.cc:573] Executing op PrintV2 in device /job:localhost/replica:0/task:0/device:CPU:0
{'cluster': {'worker': ['dge10:11214',
                'dge10:11215',
                'dge12:11214',
                'dge12:11215',
                'dge13:11214',
                'dge13:11215',
                'dge14:11214',
                'dge14:11215',
                'dge15:11214',
                'dge15:11215',
                'dge9:11214',
                'dge9:11215']},
'rpc_layer': 'grpc',
'task': {'index': 5, 'type': 'worker'}}
{'cluster': {'worker': ['dge10:11214',
                'dge10:11215',
                'dge12:11214',
                'dge12:11215',
                'dge13:11214',
                'dge13:11215',
                'dge14:11214',
                'dge14:11215',
                'dge15:11214',
                'dge15:11215',
                'dge9:11214',
                'dge9:11215']},
'rpc_layer': 'grpc',
'task': {'index': 4, 'type': 'worker'}}
2020-05-09 08:40:42.223059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-05-09 08:40:42.223095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-05-09 08:40:42.223113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]
2020-05-09 08:40:42.223157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]
2020-05-09 08:40:42.226758: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; dge10:11214, 1 -&gt; dge10:11215, 2 -&gt; dge12:11214, 3 -&gt; dge12:11215, 4 -&gt; localhost:11214, 5 -&gt; dge13:11215, 6 -&gt; dge14:11214, 7 -&gt; dge14:11215, 8 -&gt; dge15:11214, 9 -&gt; dge15:11215, 10 -&gt; dge9:11214, 11 -&gt; dge9:11215}
2020-05-09 08:40:42.226828: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; dge10:11214, 1 -&gt; dge10:11215, 2 -&gt; dge12:11214, 3 -&gt; dge12:11215, 4 -&gt; dge13:11214, 5 -&gt; localhost:11215, 6 -&gt; dge14:11214, 7 -&gt; dge14:11215, 8 -&gt; dge15:11214, 9 -&gt; dge15:11215, 10 -&gt; dge9:11214, 11 -&gt; dge9:11215}
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='hroetsc' date='2020-05-11T10:41:48Z'>
		&lt;denchmark-link:https://github.com/hroetsc&gt;@hroetsc&lt;/denchmark-link&gt;

I ran the code shared above but face a different error, please refer to this &lt;denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/2ec7fc4324cb2118d6aa2c4be1f92e3d/untitled176.ipynb&gt;gist here&lt;/denchmark-link&gt;
 for the same.
		</comment>
		<comment id='2' author='hroetsc' date='2020-05-11T10:50:48Z'>
		instead of slurm_cluster_resolver.SlurmClusterResolver() you could just run tf.distribute.cluster_resolver.SlurmClusterResolver() or take the most recent version from the GitHub, copy it into you directory and name it slurm_cluster_resolver.py
		</comment>
		<comment id='3' author='hroetsc' date='2020-05-12T19:41:58Z'>
		experimental_connect_to_cluster  is not necessary for MultiWorkerMirroredStrategy. What are the issues to set TF_CONFIG?
		</comment>
		<comment id='4' author='hroetsc' date='2020-05-14T06:04:38Z'>
		manually set TF_CONFIG variables were not maintained when i logged out of the node... Since the cluster is very large it is also very inconvenient to set them manually
		</comment>
		<comment id='5' author='hroetsc' date='2020-05-15T06:48:41Z'>
		MultiWorkerMirroredStrategy also accepts ClusterResolver as input. You can pass your resolver into it instead of using experimental_connect_to_cluster. Note the ClusterResolver passed in must have task_type and task_index set.
		</comment>
		<comment id='6' author='hroetsc' date='2020-05-15T08:30:00Z'>
		i ended up getting a RunTimeError when calling the MultiWorkerMirroredStrategy after the ClusterResolver
		</comment>
		<comment id='7' author='hroetsc' date='2020-05-15T19:26:41Z'>
		Did you do something with your cluster_resolver? Maybe you triggered something that initialized some global state. Note you should never call experimental_connect_to_cluster for MultiWorkerMirroredStrategy.
Could you show us the code how you construct your cluster_resolver and how you pass it to MultiWorkerMirroredStrategy?
		</comment>
		<comment id='8' author='hroetsc' date='2020-05-17T12:40:42Z'>
		i solved the problem - using Horovod with Tensorflow instead of Tensorflow's dirstributed training. But thank you for your help!
		</comment>
		<comment id='9' author='hroetsc' date='2020-05-17T12:40:59Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39336&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39336&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>