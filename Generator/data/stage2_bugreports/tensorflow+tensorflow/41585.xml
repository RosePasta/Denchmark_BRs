<bug id='41585' author='smallworld-network-wupeng' open_date='2020-07-21T08:42:07Z' closed_time='2020-07-27T14:18:33Z'>
	<summary>When use MirroredStrategy for multi-gpu and fit with multi-workers there is a error --"task_done() called too many times"</summary>
	<description>
System information
OS: Ubuntu 18.04
Tensorflow 2.20 from pip install
Python version: 3.7.7
CUDA Version: 10.2
cuDNN Version: release 10.2, V10.2.89
GPU : 2070 x 2
Describe the current behavior
code:
gpus = tf.config.experimental.list_physical_devices('GPU')
for g in gpus:
    tf.config.experimental.set_virtual_device_configuration(g, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7000)])

mirrored_strategy = tf.distribute.MirroredStrategy(devices=["/gpu:0", "/gpu:1"])

train_image_generator = ImageDataGenerator(rescale=1./255, horizontal_flip=True, zoom_range=0.1, rotation_range=45) # Generator for our training data
validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data

train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,
                                                           directory=train_dir,
                                                           shuffle=True,
                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                           class_mode='binary')

val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,
                                                              directory=validation_dir,
                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                              class_mode='binary')

sample_training_images, _ = next(train_data_gen)


with mirrored_strategy.scope():
    tinydarknet = keras.Sequential([
        keras.layers.Conv2D(16, (3, 3), strides=[1, 1], padding="same", input_shape=(IMG_HEIGHT,IMG_WIDTH, 3)),
        keras.layers.BatchNormalization(),
        keras.layers.LeakyReLU(alpha=0.1),
        keras.layers.MaxPooling2D((2, 2), strides=(2, 2)),
        ..........
        keras.layers.BatchNormalization(),
        keras.layers.AveragePooling2D(),
        keras.layers.Flatten(),
        keras.layers.Dense(1)
    ])


    # In[8]:


    tinydarknet.compile(optimizer="adam",
                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                 metrics=["accuracy"])


# In[9]:

history = tinydarknet.fit(#_generator(
    train_data_gen,
    steps_per_epoch=total_train // batch_size,
    epochs=epochs,
    validation_data=val_data_gen,
    validation_steps=total_val // batch_size,
    workers=NUM_WORKERS
)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss=history.history['loss']
val_loss=history.history['val_loss']

epochs_range = range(epochs)

tinydarknet.save("keras_model")
Describe the expected behavior
When I set workers=1 in fit(), it is normal work, but when I workers more than one , it is got a error:
tensorflow/core/framework/op_kernel.cc:1741] Invalid argument: ValueError: task_done() called too many times
Traceback (most recent call last):
File "/data_ssd/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py", line 243, in call
ret = func(*args)
File "/data_ssd/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py", line 309, in wrapper
return func(*args, **kwargs)
File "/data_ssd/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py", line 785, in generator_py_func
values = next(generator_state.get_iterator(iterator_id))
File "/data_ssd/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py", line 801, in wrapped_generator
for data in generator_fn():
File "/data_ssd/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/utils/data_utils.py", line 880, in get
six.reraise(*sys.exc_info())
File "/data_ssd/anaconda3/envs/tf2/lib/python3.7/site-packages/six.py", line 703, in reraise
raise value
File "/data_ssd/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/utils/data_utils.py", line 875, in get
self.queue.task_done()
File "/data_ssd/anaconda3/envs/tf2/lib/python3.7/queue.py", line 74, in task_done
raise ValueError('task_done() called too many times')
ValueError: task_done() called too many times
	</description>
	<comments>
		<comment id='1' author='smallworld-network-wupeng' date='2020-07-21T14:47:27Z'>
		&lt;denchmark-link:https://github.com/smallworld-network-wupeng&gt;@smallworld-network-wupeng&lt;/denchmark-link&gt;

I ran the code shared and its unindented, please share simple stand alone indented code such that we can replicate the error faced or if possible please share a colab gist with the error.
		</comment>
		<comment id='2' author='smallworld-network-wupeng' date='2020-07-22T05:15:23Z'>
		
@smallworld-network-wupeng
I ran the code shared and its unindented, please share simple stand alone indented code such that we can replicate the error faced or if possible please share a colab gist with the error.

fixed
		</comment>
		<comment id='3' author='smallworld-network-wupeng' date='2020-07-22T06:32:28Z'>
		&lt;denchmark-link:https://github.com/smallworld-network-wupeng&gt;@smallworld-network-wupeng&lt;/denchmark-link&gt;

Kindly provide all &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/00626c9969693c68618cec46623dd509/untitled291.ipynb&gt;details to replicate the issue&lt;/denchmark-link&gt;
 or share a colab gist with the error faced.
		</comment>
		<comment id='4' author='smallworld-network-wupeng' date='2020-07-22T07:05:20Z'>
		
@smallworld-network-wupeng
Kindly provide all details to replicate the issue or share a colab gist with the error faced.

I put all code to this colab. &lt;denchmark-link:https://colab.research.google.com/drive/18zlBNCw-hNRiW2An2dC_zHzK8uyhJz9l&gt;https://colab.research.google.com/drive/18zlBNCw-hNRiW2An2dC_zHzK8uyhJz9l&lt;/denchmark-link&gt;
  But how to upload the train images ?
		</comment>
		<comment id='5' author='smallworld-network-wupeng' date='2020-07-22T16:03:25Z'>
		&lt;denchmark-link:https://github.com/smallworld-network-wupeng&gt;@smallworld-network-wupeng&lt;/denchmark-link&gt;

I do not have access to the link shared, please attach the images in a  zipped folder.
		</comment>
		<comment id='6' author='smallworld-network-wupeng' date='2020-07-22T19:56:52Z'>
		I have the same error with my code, I can't share it but could try to replicate it with the code and images of &lt;denchmark-link:https://github.com/smallworld-network-wupeng&gt;@smallworld-network-wupeng&lt;/denchmark-link&gt;
 once fully provided.
&lt;denchmark-code&gt;OS: Ubuntu 20.04
Tensorflow 2.2.0 from pip install
Python version: 3.8.2
CUDA Version: 10.1, V10.1.243 (nvcc --version)
GPU: Titan RTX x 2
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/39933&gt;#39933&lt;/denchmark-link&gt;
 might fix the issue, is it included in the current release candidates for 2.3.0?
		</comment>
		<comment id='7' author='smallworld-network-wupeng' date='2020-07-23T01:55:24Z'>
		
I have the same error with my code, I can't share it but could try to replicate it with the code and images of @smallworld-network-wupeng once fully provided.
OS: Ubuntu 20.04
Tensorflow 2.2.0 from pip install
Python version: 3.8.2
CUDA Version: 10.1, V10.1.243 (nvcc --version)
GPU: Titan RTX x 2

#39933 might fix the issue, is it included in the current release candidates for 2.3.0?

Yes , it is same bug , I fixed it.
But I don't understand it slow than one GPU
2GPU:
37/37 [==============================] - 36s 969ms/step - accuracy: 0.6183 - loss: 1.2452 - val_accuracy: 0.4935 - val_loss: 0.6968
1GPU:
37/37 [==============================] - 29s 794ms/step - loss: 1.1153 - accuracy: 0.6241 - val_loss: 0.6943 - val_accuracy: 0.4518
		</comment>
		<comment id='8' author='smallworld-network-wupeng' date='2020-07-23T15:59:46Z'>
		Hi &lt;denchmark-link:https://github.com/smallworld-network-wupeng&gt;@smallworld-network-wupeng&lt;/denchmark-link&gt;
, glad to hear you were able to fix the issue and get your model training.
This slowdown can happen if your program is input bound and your GPUs aren't being used as efficiently as possible (there are other possibilities too, but this one is common). I would suggest taking a look at this tutorial for the &lt;denchmark-link:https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras&gt;Tensorboard Profiler&lt;/denchmark-link&gt;
 which can help to discover what inefficiencies you might have. If you can share fully reproducible code, I'd be happy to help diagnose further.
Also since &lt;denchmark-link:https://github.com/quassy&gt;@quassy&lt;/denchmark-link&gt;
 had the same original issue, do you might explaining how you fixed it? Just in case someone else runs into the same problem in the future.
		</comment>
		<comment id='9' author='smallworld-network-wupeng' date='2020-07-27T13:46:52Z'>
		&lt;denchmark-link:https://github.com/smallworld-network-wupeng&gt;@smallworld-network-wupeng&lt;/denchmark-link&gt;
 Could you share what you have changed?
I personally tested my code with tensorflow==2.3.0rc2 just now and it seems to run without this error now. Will report back if the error comes up again.
		</comment>
		<comment id='10' author='smallworld-network-wupeng' date='2020-07-27T14:12:58Z'>
		
@smallworld-network-wupeng Could you share what you have changed?
I personally tested my code with tensorflow==2.3.0rc2 just now and it seems to run without this error now. Will report back if the error comes up again.

yes, it is looks fixed in 2.3.0rc2 . You can fixed it as &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/39933&gt;#39933 &lt;/denchmark-link&gt;
 if you use 2.2.0
		</comment>
		<comment id='11' author='smallworld-network-wupeng' date='2020-07-27T14:18:35Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41585&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41585&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>