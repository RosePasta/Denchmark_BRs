<bug id='37610' author='foxik' open_date='2020-03-15T11:28:14Z' closed_time='2020-05-01T05:58:45Z'>
	<summary>Generated docs for TF 2.1 and TF 2.2 are missing information present in the source docs</summary>
	<description>
&lt;denchmark-h:h2&gt;URL(s) with the issue:&lt;/denchmark-h&gt;

The issue affects many pages, here is one example:

TF 2.2: https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay
TF 2.1: https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay
TF 2.0: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay

&lt;denchmark-h:h2&gt;Description of issue (what needs changing):&lt;/denchmark-h&gt;

The generated docs for TF 2.1 and TF 2.2 is missing important information. Notably, the whole documentation of __init__ method containing information like detailed learning rate computation
def decayed_learning_rate(step):
  return initial_learning_rate * decay_rate ^ (step / decay_steps)
is missing in TF 2.1 and TF 2.2. However, the information is still present in the source, see



tensorflow/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py


        Lines 64 to 134
      in
      3c1e8c0






 class ExponentialDecay(LearningRateSchedule): 



 """A LearningRateSchedule that uses an exponential decay schedule.""" 



 



 def __init__( 



 self, 



 initial_learning_rate, 



 decay_steps, 



 decay_rate, 



 staircase=False, 



 name=None): 



 """Applies exponential decay to the learning rate. 



  



     When training a model, it is often recommended to lower the learning rate as 



     the training progresses. This schedule applies an exponential decay function 



     to an optimizer step, given a provided initial learning rate. 



  



     The schedule a 1-arg callable that produces a decayed learning 



     rate when passed the current optimizer step. This can be useful for changing 



     the learning rate value across different invocations of optimizer functions. 



     It is computed as: 



  



     ```python 



     def decayed_learning_rate(step): 



       return initial_learning_rate * decay_rate ^ (step / decay_steps) 



     ``` 



  



     If the argument `staircase` is `True`, then `step / decay_steps` is 



     an integer division and the decayed learning rate follows a 



     staircase function. 



  



     You can pass this schedule directly into a `tf.keras.optimizers.Optimizer` 



     as the learning rate. 



     Example: When fitting a Keras model, decay every 100000 steps with a base 



     of 0.96: 



  



     ```python 



     initial_learning_rate = 0.1 



     lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay( 



         initial_learning_rate, 



         decay_steps=100000, 



         decay_rate=0.96, 



         staircase=True) 



  



     model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule), 



                   loss='sparse_categorical_crossentropy', 



                   metrics=['accuracy']) 



  



     model.fit(data, labels, epochs=5) 



     ``` 



  



     The learning rate schedule is also serializable and deserializable using 



     `tf.keras.optimizers.schedules.serialize` and 



     `tf.keras.optimizers.schedules.deserialize`. 



  



     Args: 



       initial_learning_rate: A scalar `float32` or `float64` `Tensor` or a 



         Python number.  The initial learning rate. 



       decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number. 



         Must be positive.  See the decay computation above. 



       decay_rate: A scalar `float32` or `float64` `Tensor` or a 



         Python number.  The decay rate. 



       staircase: Boolean.  If `True` decay the learning rate at discrete 



         intervals 



       name: String.  Optional name of the operation.  Defaults to 



         'ExponentialDecay'. 



  



     Returns: 



       A 1-arg callable learning rate schedule that takes the current optimizer 



       step and outputs the decayed learning rate, a scalar `Tensor` of the same 



       type as `initial_learning_rate`. 



     """ 





&lt;denchmark-h:h2&gt;Further examples&lt;/denchmark-h&gt;

For example Adam optimizer is also affected, see

TF 2.2: https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/optimizers/Adam
TF 2.0: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers/Adam
The TF 2.0 version contains a lot of math describing how Adam works, which is not present in TF 2.2 docs.

	</description>
	<comments>
		<comment id='1' author='foxik' date='2020-03-16T16:09:49Z'>
		Thanks for reporting. It will be fixed in the nightly version of the API docs tomorrow.
		</comment>
		<comment id='2' author='foxik' date='2020-05-01T05:58:44Z'>
		It is indeed fixed in the nightly, thanks!
		</comment>
	</comments>
</bug>