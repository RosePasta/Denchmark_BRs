<bug id='34821' author='372046933' open_date='2019-12-04T11:00:59Z' closed_time='2020-03-19T07:31:23Z'>
	<summary>MultiWorkerMirroredStategy cannot run</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (Linux Ubuntu 18.04.3 LTS):
TensorFlow installed from (binary):
TensorFlow version (2.0.0):
Python version: 3.6.8
CUDA/cuDNN version: 10.1
GPU model and memory: Tesla P100


When I run the tutorial code in &lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras&gt;Multi worker training with Keras&lt;/denchmark-link&gt;
. Train fails after the first epoch. I guess that the  fails.  was  generated by kubeflow, and I can assure that it is correct.
Describe the expected behavior
Should run normally.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
&lt;denchmark-code&gt;import tensorflow_datasets as tfds
import tensorflow as tf
from absl import app, flags
import os
import json

tfds.disable_progress_bar()
FLAGS = flags.FLAGS

flags.DEFINE_string('input_data_path', default='hdfs://30.78.5.52:9000/data/public/dataset/tensorflow_datasets', help='HDFS Input data path')
flags.DEFINE_string('checkpoint_dir', default=None, help='HDFS checkpoint dir')

BUFFER_SIZE = 10000
BATCH_SIZE = 64

def make_datasets_unbatched():
  # Scaling MNIST data from (0, 255] to (0., 1.]
  def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255
    return image, label

  datasets, info = tfds.load(name='mnist',
                            with_info=True,
                            as_supervised=True,
                            download=False,
                            data_dir=FLAGS.input_data_path)

  return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)


def build_and_compile_cnn_model():
  model = tf.keras.Sequential([
      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(10, activation='softmax')
  ])
  model.compile(
      loss=tf.keras.losses.sparse_categorical_crossentropy,
      optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
      metrics=['accuracy'])
  return model

def main(argv):
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

  NUM_WORKERS = 2
  # Here the batch size scales up by number of workers since
  # `tf.data.Dataset.batch` expects the global batch size. Previously we used 64,
  # and now this becomes 128.
  GLOBAL_BATCH_SIZE = 64 * NUM_WORKERS
  # Replace the `filepath` argument with a path in the file system
  # accessible by all workers.
  callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=FLAGS.checkpoint_dir)]
  with strategy.scope():
    # Creation of dataset, and model building/compiling need to be within
    # `strategy.scope()`.
    train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)
    multi_worker_model = build_and_compile_cnn_model()
  multi_worker_model.fit(x=train_datasets, epochs=2, callbacks=callbacks)



if __name__ == '__main__':
  app.run(main)

&lt;/denchmark-code&gt;

Other info / logs
After the first epoch, the following error occurred.
&lt;denchmark-code&gt;    469/Unknown - 10s 20ms/step - loss: 2.1859 - accuracy: 0.31432019-12-04 18:43:39.653487: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
         [[GroupCrossDeviceControlEdges_0/Identity_4/_19]]
2019-12-04 18:43:39.653688: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
2019-12-04 18:43:40.073912: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Out of range: [_Derived_]End of sequence
         [[{{node IteratorGetNext}}]]
         [[GroupCrossDeviceControlEdges_0/Identity_4/_19]]
2019-12-04 18:43:40.074084: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence
         [[{{node IteratorGetNext}}]]
         [[GroupCrossDeviceControlEdges_0/Identity_4/_19]]
2019-12-04 18:43:40.074306: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Out of range: [_Derived_]End of sequence
         [[{{node IteratorGetNext}}]]
         [[GroupCrossDeviceControlEdges_0/Identity_4/_19]]
2019-12-04 18:43:40.074408: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence
         [[{{node IteratorGetNext}}]]
         [[GroupCrossDeviceControlEdges_0/Identity_4/_19]]
         [[CollectiveReduce]]
         [[CollectiveReduce/_2]]
2019-12-04 18:43:40.074591: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence
         [[{{node IteratorGetNext}}]]
         [[GroupCrossDeviceControlEdges_0/Identity_4/_19]]
         [[CollectiveReduce]]
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py", line 668, in on_start
    yield
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py", line 372, in fit
    prefix='val_')
  File "/usr/lib/python3.6/contextlib.py", line 88, in __exit__
    next(self.gen)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py", line 685, in on_epoch
    self.callbacks.on_epoch_end(epoch, epoch_logs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py", line 298, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py", line 963, in on_epoch_end
    self._save_model(epoch=epoch, logs=logs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py", line 1012, in _save_model
    self.model.save(filepath, overwrite=True)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py", line 975, in save
    signatures, options)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/save.py", line 115, in save_model
    signatures, options)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/save.py", line 74, in save
    save_lib.save(model, filepath, signatures, options)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/save.py", line 883, in save
    _ = _SaveableView(checkpoint_graph_view)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/save.py", line 164, in __init__
    self.checkpoint_view.objects_ids_and_slot_variables())
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/graph_view.py", line 425, in objects_ids_and_slot_variables
    object_names=object_names)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/graph_view.py", line 96, in _serialize_slot_variables
    or hasattr(trackable, "_create_or_restore_slot_variable")):
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py", line 389, in __getattr__
    return getattr(self.get(), name)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py", line 322, in get
    return self._get_cross_replica()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py", line 1237, in _get_cross_replica
    self, axis=None)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py", line 805, in reduce
    return self._extended._reduce(reduce_op, value)  # pylint: disable=protected-access
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py", line 1436, in _reduce
    device_util.current() or "/device:CPU:0"))[0]
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/collective_all_reduce_strategy.py", line 490, in _reduce_to
    reduce_op, value, destinations=destinations)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 282, in reduce
    destinations)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 1025, in reduce_implementation
    all_reduced = self._batch_all_reduce(reduce_op, [per_replica_value])[0]
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 1091, in _batch_all_reduce
    dense_results = self._do_batch_all_reduce_dense(reduce_op, dense_values)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 1120, in _do_batch_all_reduce_dense
    "Id")
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/cross_device_utils.py", line 365, in build_collective_reduce
    return collective_all_reduce()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py", line 457, in __call__
    result = self._call(*args, **kwds)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py", line 526, in _call
    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py", line 1141, in _filtered_call
    self.captured_inputs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py", line 1224, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py", line 511, in call
    ctx=ctx)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "&lt;string&gt;", line 3, in raise_from
tensorflow.python.framework.errors_impl.OutOfRangeError:  [_Derived_]End of sequence
         [[{{node IteratorGetNext}}]]
         [[GroupCrossDeviceControlEdges_0/Identity_4/_19]]
         [[CollectiveReduce]]
         [[CollectiveReduce/_2]] [Op:__inference_collective_all_reduce_2649]

Function call stack:
collective_all_reduce


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "multi_worker_mirrored_strategy_keras_mnist.py", line 81, in &lt;module&gt;
    app.run(main)
  File "/usr/local/lib/python3.6/dist-packages/absl/app.py", line 299, in run
    _run_main(main, args)
  File "/usr/local/lib/python3.6/dist-packages/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "multi_worker_mirrored_strategy_keras_mnist.py", line 76, in main
    multi_worker_model.fit(x=train_datasets, epochs=2, callbacks=callbacks)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py", line 728, in fit
    use_multiprocessing=use_multiprocessing)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 789, in fit
    *args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 776, in wrapper
    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_coordinator.py", line 853, in run_distribute_coordinator
    task_id, session_config, rpc_layer)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_coordinator.py", line 360, in _run_single_worker
    return worker_fn(strategy)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 771, in _worker_fn
    return method(model, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py", line 372, in fit
    prefix='val_')
  File "/usr/lib/python3.6/contextlib.py", line 99, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py", line 671, in on_start
    self.callbacks._call_end_hook(mode)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py", line 258, in _call_end_hook
    self.on_train_end()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py", line 375, in on_train_end
    callback.on_train_end(logs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py", line 940, in on_train_end
    self._training_state.delete_backup()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/distribute/multi_worker_training_state.py", line 161, in delete_backup
    tracking.AutoTrackable.__delattr__(self._model, CKPT_SAVED_EPOCH)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/tracking.py", line 94, in __delattr__
    super(AutoTrackable, self).__delattr__(name)
AttributeError: _ckpt_saved_epoch
2019-12-04 18:43:40.494701: W tensorflow/core/common_runtime/eager/context.cc:290] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='372046933' date='2020-02-06T15:15:53Z'>
		I ran into the same issue. The problem is with multi_worker_model.fit(x=train_datasets, epochs=2, callbacks=callbacks)
&lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#train_the_model_with_multiworkermirroredstrategy&gt;https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#train_the_model_with_multiworkermirroredstrategy&lt;/denchmark-link&gt;


Since MultiWorkerMirroredStrategy does not support last partial batch handling, pass the steps_per_epoch argument to model.fit() when dataset is imbalanced on multiple workers.

--&gt; You need to pass in steps_per_epoch
		</comment>
		<comment id='2' author='372046933' date='2020-03-19T07:31:24Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34821&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34821&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>