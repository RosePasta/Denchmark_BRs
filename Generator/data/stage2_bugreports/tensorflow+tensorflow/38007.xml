<bug id='38007' author='mlexpert' open_date='2020-03-28T21:36:49Z' closed_time='2020-05-06T15:40:27Z'>
	<summary>TF 2 ignores one of 2 GPUs</summary>
	<description>
Dear community,
I have a problem regarding tensorflow calculation on 2 GPUs connected via SLI technology: only one of them is working and second one is not, although both GPUs are recognized by TF.
Setup:

Ubuntu 18.04
Python 3
Tensorflow 2.1
Cuda 10.1
Nvidia drivers (officials) 440.64
AMD Ryzen 2700
Asus x470 prime
Two GPUs of GTX 1070 connected via SLI techno.

I have already tested many things that I had found in internet. Concretely:


I started with Tensorflow 2.0, it did not work, so I updated it to TF 2.1. The problem remains


Purged and reinstalled the Nvidia drivers 430.50. Updated them to 440.64. The problem remains


I verified each of my GPUs separately. I removed physically one of them, and launched code on the remaining. It worked and it seems that the GPUs are OK.


I verified each of the GPU's ports on my motherboard separately. It worked and it means that each of the ports are fine.


I inserted two GPUs with and without hardware SLI connection and launched the following code:


&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.applications import Xception
import numpy as np

num_samples = 100
height = 224
width = 224
num_classes = 50

strategy = tf.distribute.MirroredStrategy(devices=['/GPU:0', '/GPU:1'])
with strategy.scope():
    parallel_model = Xception(weights=None,
                              input_shape=(height, width, 3),
                              classes=num_classes)
    parallel_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

### Works only for the first GPU of the
# parallel_model = Xception(weights=None,
#                           input_shape=(height, width, 3),
#                           classes=num_classes)
# parallel_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))

# Generate dummy data.
x = np.random.random((num_samples, height, width, 3))
y = np.random.random((num_samples, num_classes))

parallel_model.summary()
# This `fit` call will be distributed on 8 GPUs.
# Since the batch size is 256, each GPU will process 32 samples.
parallel_model.fit(x, y, epochs=20, batch_size=16)
&lt;/denchmark-code&gt;

As a result, when strategy = tf.distribute.MirroredStrategy(devices=['/GPU:0']), the code is running fine.
However, when devices=['/GPU:1'] or devices=['/GPU:0', '/GPU:1'], the nvidia-smi shows some process on the 2nd GPU, but the code execution is stacked at line
&lt;denchmark-code&gt;2020-03-28 21:51:14.891325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7162 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:08:00.0, compute capability: 6.1)
2020-03-28 21:51:14.891805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-28 21:51:14.892399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7624 MB memory) -&gt; physical GPU (device: 1, name: GeForce GTX 1070, pci bus id: 0000:09:00.0, compute capability: 6.1),
&lt;/denchmark-code&gt;

so I have to reboot the computer, because it s dead.

Initially, my X11 configuration (xorg.conf) was not configured for SLI:

&lt;denchmark-code&gt;Section "Device"
    Identifier     "Device0"
    Driver         "nvidia"
    VendorName     "NVIDIA Corporation"
EndSection

Section "Device"
    Identifier     "Device1"
    Driver         "nvidia"
    VendorName     "NVIDIA Corporation"
EndSection

Section "Screen"
    Identifier     "Screen0"
    Device         "Device0"
    Monitor        "Monitor0"
    DefaultDepth    24
    SubSection     "Display"
        Depth       24
    EndSubSection
EndSection
&lt;/denchmark-code&gt;

After google search, I played with sudo nvidia-xconfig -sli=on; sudo nvidia-xconfig -sli=auto, etc
As a result, after reboot, I obtain a bootloop with 2 lines:
&lt;denchmark-code&gt;recovering journal
/dev/nume0n1p2: clean, XXX/XXX files, XXX/XXX blocks
&lt;/denchmark-code&gt;

Every ~3 sec the screen becomes black and then these 2 lines show again.
Impossible to access to TTY, because it is in bootloop as well. I looked everything that I could find on this subject, nothing worked. So, I kept the previous X11 config without SLI
If you experienced such type of problem, do not hesitate to share it. Any advice would help.
Thanks!
	</description>
	<comments>
		<comment id='1' author='mlexpert' date='2020-04-06T02:46:03Z'>
		
so I have to reboot the computer, because it s dead.

So once execution reaches that point you cannot just kill the TF process, you have to reboot?  If so, this seems like it could be a problem with the NVIDIA driver.
&lt;denchmark-link:https://github.com/chsigg&gt;@chsigg&lt;/denchmark-link&gt;
 Do you know of any extra logging we can enable on the CUDA driver to help triage this further?
		</comment>
		<comment id='2' author='mlexpert' date='2020-04-06T08:48:12Z'>
		CUDA_VISIBLE_DEVICES might make debugging simpler without having to remove one of the cards. But the fact that you're now stuck in a bootloop seems to indicate that there is a more fundamental problem with your system. You could also try to write a little test program, along these lines:
&lt;denchmark-code&gt;size_t buffer_size = 1024;
cudaSetDevice(0);
void* dev0_buffer;
cudaMalloc(&amp;dev0_buffer, buffer_size);
void* dev1_buffer;
cudaSetDevice(1);
cudaMalloc(&amp;dev1_buffer, buffer_size);
cudaMemcpy(dev1_buffer, dev0_buffer, buffer_size, cudaMemcpyDefault);
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='mlexpert' date='2020-04-06T13:56:22Z'>
		Thanks for your reply.
&lt;denchmark-link:https://github.com/sanjoy&gt;@sanjoy&lt;/denchmark-link&gt;


So once execution reaches that point you cannot just kill the TF process, you have to reboot? If so, this seems like it could be a problem with the NVIDIA driver.
Yes, it is impossible to kill the process, so I have to reboot the PC. I thought that it was the problem of the drivers as well. I reinstalled it, but the problem remains.

&lt;denchmark-link:https://github.com/chsigg&gt;@chsigg&lt;/denchmark-link&gt;

I run this code, but obtain the following errors:
&lt;denchmark-code&gt;hello_world.cpp:2:14: error: expected constructor, destructor, or type conversion before ‘(’ token
 cudaSetDevice(0);
              ^
hello_world.cpp:4:11: error: expected constructor, destructor, or type conversion before ‘(’ token
 cudaMalloc(&amp;dev0_buffer, buffer_size);
           ^
hello_world.cpp:6:14: error: expected constructor, destructor, or type conversion before ‘(’ token
 cudaSetDevice(1);
              ^
hello_world.cpp:7:11: error: expected constructor, destructor, or type conversion before ‘(’ token
 cudaMalloc(&amp;dev1_buffer, buffer_size);
           ^
hello_world.cpp:8:11: error: expected constructor, destructor, or type conversion before ‘(’ token
 cudaMemcpy(dev1_buffer, dev0_buffer, buffer_size, cudaMemcpyDefault);
           ^
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='mlexpert' date='2020-04-07T10:33:19Z'>
		Small update:
The both gpu's are functional when I run the following test:
&lt;denchmark-code&gt;import tensorflow as tf

n = 12345
dtype = tf.float32
print(2 * n*n*32/8 / 1.e9)
with tf.device("/gpu:1"): # /gpu:0
    for i in range(100):
        matrix1 = tf.Variable(tf.random.uniform((n, n), dtype=dtype))
        matrix2 = tf.Variable(tf.random.uniform((n, n), dtype=dtype))
        product = tf.norm(tf.matmul(matrix1, matrix2))
        print(i, product)
&lt;/denchmark-code&gt;

It means that when gpu=/gpu:0, I obtain a good execution of code with
&lt;denchmark-code&gt;+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.64       Driver Version: 440.64       CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1070    Off  | 00000000:08:00.0  On |                  N/A |
|  0%   43C    P8    13W / 180W |    7989MiB /  8116MiB |      2%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 1070    Off  | 00000000:09:00.0 Off |                  N/A |
|  0%   43C    P8     6W / 180W |      2MiB /  8119MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
&lt;/denchmark-code&gt;

When gpu=/gpu:1, I obtain
&lt;denchmark-code&gt;+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.64       Driver Version: 440.64       CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1070    Off  | 00000000:08:00.0  On |                  N/A |
|  0%   39C    P2    37W / 180W |    552MiB /  8116MiB |      1%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 1070    Off  | 00000000:09:00.0 Off |                  N/A |
|  0%   42C    P2   175W / 180W |   7855MiB /  8119MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='mlexpert' date='2020-04-07T12:12:06Z'>
		Another test provides very interesting results. Here is the code:
&lt;denchmark-code&gt;import tensorflow as tf

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

# strategy = tf.distribute.MirroredStrategy()
# with strategy.scope():
with tf.device("/gpu:1"):
    model = tf.keras.models.Sequential([
      tf.keras.layers.Flatten(input_shape=(28, 28)),
      tf.keras.layers.Dense(128, activation='relu'),
      tf.keras.layers.Dense(10)
    ])

    model.compile(optimizer='adam',
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                  metrics=['accuracy'])

    model.fit(x_train, y_train, epochs=5)
&lt;/denchmark-code&gt;

This code works (it means that it arrives at the end), but yields a numerically-strange result:
&lt;denchmark-code&gt;2020-04-07 14:00:39.080936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-07 14:00:39.081553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-07 14:00:39.082173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-07 14:00:39.082777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7259 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:08:00.0, compute capability: 6.1)
2020-04-07 14:00:39.083251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-07 14:00:39.084005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7624 MB memory) -&gt; physical GPU (device: 1, name: GeForce GTX 1070, pci bus id: 0000:09:00.0, compute capability: 6.1)
Train on 60000 samples
Epoch 1/5
2020-04-07 14:00:40.645991: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
60000/60000 [==============================] - 4s 68us/sample - loss: 2.3016 - accuracy: 0.1116
Epoch 2/5
60000/60000 [==============================] - 3s 56us/sample - loss: 2.3013 - accuracy: 0.1124
Epoch 3/5
60000/60000 [==============================] - 3s 54us/sample - loss: 2.3014 - accuracy: 0.1124
Epoch 4/5
60000/60000 [==============================] - 3s 53us/sample - loss: 2.3013 - accuracy: 0.1124
Epoch 5/5
60000/60000 [==============================] - 3s 53us/sample - loss: 2.3013 - accuracy: 0.1124

Process finished with exit code 0
&lt;/denchmark-code&gt;

As you can see, the accuracy goes never up to ~0.11 (I run this test multiple times).
Another interesting thing is that even if I precise that GPU=GPU:/1, it seems that TF calls both of the GPUs:
&lt;denchmark-code&gt;+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.64       Driver Version: 440.64       CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1070    Off  | 00000000:08:00.0  On |                  N/A |
|  0%   50C    P2    46W / 180W |   7830MiB /  8116MiB |      1%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 1070    Off  | 00000000:09:00.0 Off |                  N/A |
|  0%   49C    P2    36W / 180W |   7855MiB /  8119MiB |     13%      Default |
+-------------------------------+----------------------+----------------------+
&lt;/denchmark-code&gt;

I removed SMI hardware connection, but result repeated.
And the 3rd interesting behavior is that when you put line model.fit(x_train, y_train, epochs=5) out of the with tf.device("/gpu:1"): section, it seems that gpu is not used at all. One iteration takes a lot of time and loss is nan (or strangely big number).
At the end, I add that when you put with tf.device("/gpu:0"):, everything that I ve tested finishes up to a good result:
&lt;denchmark-code&gt;2020-04-07 14:09:28.088388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-07 14:09:28.089664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-07 14:09:28.090964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-07 14:09:28.091713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7263 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:08:00.0, compute capability: 6.1)
2020-04-07 14:09:28.092022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-07 14:09:28.092459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7624 MB memory) -&gt; physical GPU (device: 1, name: GeForce GTX 1070, pci bus id: 0000:09:00.0, compute capability: 6.1)
Train on 60000 samples
Epoch 1/5
2020-04-07 14:09:28.751855: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
60000/60000 [==============================] - 3s 56us/sample - loss: 0.2543 - accuracy: 0.9271
Epoch 2/5
60000/60000 [==============================] - 3s 51us/sample - loss: 0.1128 - accuracy: 0.9670
Epoch 3/5
60000/60000 [==============================] - 3s 52us/sample - loss: 0.0763 - accuracy: 0.9772
Epoch 4/5
60000/60000 [==============================] - 3s 52us/sample - loss: 0.0573 - accuracy: 0.9822
Epoch 5/5
60000/60000 [==============================] - 3s 51us/sample - loss: 0.0451 - accuracy: 0.9858

Process finished with exit code 0
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='mlexpert' date='2020-04-11T06:25:28Z'>
		
Another interesting thing is that even if I precise that GPU=GPU:/1, it seems that TF calls both of the GPUs:

That may not have sufficient to completely disable access to gpu:0.  Can you try using &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/config/set_visible_devices&gt;set_visible_devices&lt;/denchmark-link&gt;
 instead?
		</comment>
		<comment id='7' author='mlexpert' date='2020-04-11T20:49:52Z'>
		
That may not have sufficient to completely disable access to gpu:0. Can you try using set_visible_devices instead?

&lt;denchmark-link:https://github.com/sanjoy&gt;@sanjoy&lt;/denchmark-link&gt;
 Thanks for the advice, you were right. I integrated  into the code and managed to disable the gpu:0.
The code is the following one:
&lt;denchmark-code&gt;import tensorflow as tf

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

physical_devices = tf.config.list_physical_devices('GPU')
print(physical_devices)
# tf.config.set_visible_devices(physical_devices[0], 'GPU')
tf.config.set_visible_devices(physical_devices[1], 'GPU')

model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10)
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)
&lt;/denchmark-code&gt;

To sum up: I manage to train MNIST on each of the GPUs using set_visible_devices and nvidia-smi shows that only one of the GPUs is working.
However, the initial problem remains: it is still not possible to train on multiple GPUs. One of the codes that I used is the following one:
&lt;denchmark-code&gt;import tensorflow as tf

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

physical_devices = tf.config.list_physical_devices('GPU')
print(physical_devices)
tf.config.set_visible_devices(physical_devices, 'GPU')

strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    model = tf.keras.models.Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28)),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(10)])
    model.compile(optimizer='adam',
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                  metrics=['accuracy'])
    model.fit(x_train, y_train, epochs=5)
&lt;/denchmark-code&gt;

Verbose that I get:
&lt;denchmark-code&gt;2020-04-11 22:45:54.156999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7245 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:08:00.0, compute capability: 6.1)
2020-04-11 22:45:54.157457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-11 22:45:54.158053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7624 MB memory) -&gt; physical GPU (device: 1, name: GeForce GTX 1070, pci bus id: 0000:09:00.0, compute capability: 6.1)
Train on 60000 samples
Epoch 1/5
2020-04-11 22:45:59.774139: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
&lt;/denchmark-code&gt;

And from this moment nothing happens, so that I have to kill the process / reboot PC.
Do you have any ideas?
Thanks!
		</comment>
		<comment id='8' author='mlexpert' date='2020-04-11T22:54:17Z'>
		
And from this moment nothing happens, so that I have to kill the process / reboot PC.

Do you have to kill the process or reboot the PC?  As I stated above, if you have to reboot the PC then it is likely a driver or a hardware issue.
		</comment>
		<comment id='9' author='mlexpert' date='2020-04-12T14:46:47Z'>
		
Do you have to kill the process or reboot the PC? As I stated above, if you have to reboot the PC then it is likely a driver or a hardware issue.

If I am fast enough (I did not time it, but i am talking about first 5-10 sec of code execution), I can kill it. When I stop, I get the following message:
&lt;denchmark-code&gt;Train on 60000 samples
Epoch 1/5
   32/60000 [..............................] - ETA: 1:33:29Traceback (most recent call last):
  File "/home/ivan/Work/Code/diagnolly/datascience/gpu_testing.py", line 55, in &lt;module&gt;
    model.fit(x_train, y_train, epochs=5)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 819, in fit
    use_multiprocessing=use_multiprocessing)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 342, in fit
    total_epochs=epochs)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 128, in run_one_epoch
    batch_outs = execution_function(iterator)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", line 98, in execution_function
    distributed_function(input_fn))
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 568, in __call__
    result = self._call(*args, **kwds)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 632, in _call
    return self._stateless_fn(*args, **kwds)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 2362, in __call__
    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 2703, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 2593, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py", line 978, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 439, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", line 85, in distributed_function
    per_replica_function, args=args)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py", line 763, in experimental_run_v2
    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py", line 1819, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py", line 694, in _call_for_each_replica
    fn, args, kwargs)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py", line 201, in _call_for_each_replica
    coord.join(threads)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/training/coordinator.py", line 363, in join
    while any(t.is_alive() for t in threads) and not self.wait_for_stop(1.0):
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/training/coordinator.py", line 311, in wait_for_stop
    return self._stop_event.wait(timeout)
  File "/usr/lib/python3.6/threading.py", line 551, in wait
    signaled = self._cond.wait(timeout)
  File "/usr/lib/python3.6/threading.py", line 299, in wait
    gotit = waiter.acquire(True, timeout)
KeyboardInterrupt

Process finished with exit code 137 (interrupted by signal 9: SIGKILL)
&lt;/denchmark-code&gt;

When I wait too much time, the process completely and I have to reboot PC
		</comment>
		<comment id='10' author='mlexpert' date='2020-04-12T14:56:16Z'>
		Small update:
I just reinstalled everything (including Ubuntu), but the problem remains...
To sum up:

Each of the GPUs working well using tf.config.set_visible_devices and ables to train correctly MNIST
When try to use both of the GPUs simultaneously using tf.distribute.MirroredStrategy()

&lt;denchmark-code&gt;strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    model.compile(...)
    model.fit(...)
&lt;/denchmark-code&gt;

The execution stops at
&lt;denchmark-code&gt;2020-04-11 22:45:54.156999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7245 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:08:00.0, compute capability: 6.1)
2020-04-11 22:45:54.157457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-11 22:45:54.158053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7624 MB memory) -&gt; physical GPU (device: 1, name: GeForce GTX 1070, pci bus id: 0000:09:00.0, compute capability: 6.1)
Train on 60000 samples
Epoch 1/5
2020-04-11 22:45:59.774139: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
&lt;/denchmark-code&gt;

And the process gets stacked. If we try to stop this process, we obtain:
&lt;denchmark-code&gt;Train on 60000 samples
Epoch 1/5
   32/60000 [..............................] - ETA: 1:33:29Traceback (most recent call last):
  File "/home/ivan/Work/Code/diagnolly/datascience/gpu_testing.py", line 55, in &lt;module&gt;
    model.fit(x_train, y_train, epochs=5)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 819, in fit
    use_multiprocessing=use_multiprocessing)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 342, in fit
    total_epochs=epochs)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 128, in run_one_epoch
    batch_outs = execution_function(iterator)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", line 98, in execution_function
    distributed_function(input_fn))
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 568, in __call__
    result = self._call(*args, **kwds)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 632, in _call
    return self._stateless_fn(*args, **kwds)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 2362, in __call__
    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 2703, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 2593, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py", line 978, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 439, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", line 85, in distributed_function
    per_replica_function, args=args)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py", line 763, in experimental_run_v2
    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py", line 1819, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py", line 694, in _call_for_each_replica
    fn, args, kwargs)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py", line 201, in _call_for_each_replica
    coord.join(threads)
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/training/coordinator.py", line 363, in join
    while any(t.is_alive() for t in threads) and not self.wait_for_stop(1.0):
  File "/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/training/coordinator.py", line 311, in wait_for_stop
    return self._stop_event.wait(timeout)
  File "/usr/lib/python3.6/threading.py", line 551, in wait
    signaled = self._cond.wait(timeout)
  File "/usr/lib/python3.6/threading.py", line 299, in wait
    gotit = waiter.acquire(True, timeout)
KeyboardInterrupt

Process finished with exit code 137 (interrupted by signal 9: SIGKILL)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='mlexpert' date='2020-04-20T12:43:27Z'>
		I have a similar issue.. though i did not go that deep as &lt;denchmark-link:https://github.com/mlexpert&gt;@mlexpert&lt;/denchmark-link&gt;
 .. in short I am not able to train using MirroredStrategy .
My Config is roughly similar to &lt;denchmark-link:https://github.com/mlexpert&gt;@mlexpert&lt;/denchmark-link&gt;

Things I tried :


Run sample Mnist code for distributed training in a docker instance of tensorflow nightly and latest gpu containers.


It stalls precisely at this line and no progress is seen further.
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',). INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',). INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',). INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',). INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',). INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',). INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',). INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).


My configuration : Amd Ryzen 3900x 32gb Ram, Ubuntu 20.04 , Gtx 1070 X 2 connected via SLI
FYI : Single GPU training is working fine.
		</comment>
		<comment id='12' author='mlexpert' date='2020-04-23T06:41:12Z'>
		Ok i managed to resolve this by turning off IOMMU in my system bios.
Apparently this has to do something with pcie acs.
		</comment>
		<comment id='13' author='mlexpert' date='2020-04-24T09:07:14Z'>
		Hi &lt;denchmark-link:https://github.com/jogiji&gt;@jogiji&lt;/denchmark-link&gt;
,
Thanks for your input. In fact, I am not sure that I have an option of enable/disable of IOMMU for my motherboard.
So, for the moment the problem ramains...
Any advice would help,
Thanks.
		</comment>
		<comment id='14' author='mlexpert' date='2020-05-02T01:07:19Z'>
		&lt;denchmark-link:https://github.com/mlexpert&gt;@mlexpert&lt;/denchmark-link&gt;
 atleast i had that option.. if not then try searching about turning acs off via command line immediatley after system reboot.
For more info &lt;denchmark-link:https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/troubleshooting.html&gt;https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/troubleshooting.html&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='mlexpert' date='2020-05-06T15:40:27Z'>
		Dear &lt;denchmark-link:https://github.com/jogiji&gt;@jogiji&lt;/denchmark-link&gt;
,
I updated my BIOS and then found and turned off IOMMU.
And it worked!
Some more interesting behavior that could be helpful for someone in a similar situation:
I installed NCCL with tests and tried to launch them. On 1 GPU ./build/all_reduce_perf -b 8 -e 128M -f 2 -g 1 everything worked goog, however, on 2 GPUs
./build/all_reduce_perf -b 8 -e 128M -f 2 -g 2 the tests got stacked.
The same story about CUDA native tests in 1_Utilities. They did not work.
So, anyone who has a problem with multi-gpu (TF, CUDA, or NCCL) try to turn off the IOMMU.
Thnaks a lot, &lt;denchmark-link:https://github.com/jogiji&gt;@jogiji&lt;/denchmark-link&gt;

Have a nice day all!
BR
P.S. problem may be treated as solved
		</comment>
		<comment id='16' author='mlexpert' date='2020-05-06T15:40:29Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38007&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38007&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>