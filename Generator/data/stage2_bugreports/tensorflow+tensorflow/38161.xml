<bug id='38161' author='orangesomethingorange' open_date='2020-04-02T11:23:33Z' closed_time='2020-04-11T10:44:25Z'>
	<summary>tflite model crashes before loading .tflite file, with no error or exception</summary>
	<description>
System information

Have I written custom code:

Yes I have:
In Activity.java:
=======================================================================
&lt;denchmark-code&gt;gpuDelegate = new GpuDelegate();
Interpreter.Options tfliteOptions = (new Interpreter.Options()
          .addDelegate(gpuDelegate));
tfliteModel
        = FileUtil.loadMappedFile(myContext,
        "my_model.tflite");
tfliteInterpreter = new Interpreter(tfliteModel, tfliteOptions);
&lt;/denchmark-code&gt;

...
&lt;denchmark-code&gt;imageProcessor =
        new ImageProcessor.Builder()
                .add(new NormalizeOp(MY_MEAN, MY_STDDEV))
                .build();

inputImageBuffer.load(bitmap);
imageProcessor.process(inputImageBuffer);
&lt;/denchmark-code&gt;

...
tfliteInterpreter.run(inputImageBuffer.getBuffer(), outputBuffer.getBuffer().rewind());
=======================================================================
In build.gradle:
=======================================================================
&lt;denchmark-code&gt;implementation 'org.tensorflow:tensorflow-lite:+'
implementation 'org.tensorflow:tensorflow-lite-gpu:+'
implementation 'org.tensorflow:tensorflow-lite-support:+'
&lt;/denchmark-code&gt;

=======================================================================

OS Platform and Distribution:

Linux Ubuntu 16.04

Mobile device:

Samsung Galaxy S10

TensorFlow installed:

From binary (pip)

TensorFlow version:

TensorFlow 2.0

Python version:

Python 3.6

GCC/Compiler version:

Wasn't compiled from source, but gcc 7.4.0

CUDA/cuDNN version:

CUDA 10.0

GPU model and memory:

GeForce RTX 2080 Ti
11GB
Describe the current behavior
Model input is a 1MP image.
I'm trying to run the model via CPU (tfliteOptions are default), GPU (GPU delegate) and NNAPI (NNAPI delegate).
1MP runs with the GPU delegate only - the other two options crash.
When crashing - the debug process is detached from Android.
There are no exceptions, errors or hints regarding what happened.
After that, I take the same exact model, and I only change the input size, from 1MP to 10MP (in python it's just changing the HxW of the input image). From the 10MP model I create a new .tflite file.
When trying to run the 10MP model - it doesn't even load.
Is it a memory issue? and if so - how can I check that? What can I do to reduce the impact?
Describe the expected behavior
Have some exceptions, error messages or any other hint as to what's going on.
Let's say it's a memory issue - how can I handle that? Which part fails exactly?
Other info / logs

Debugging the 1MP model on CPU (the issue is that the app crashes):

Last debugger frames:
getRuntime:166, VMRuntime (dalvik.system)
:69, DirectByteBuffer$MemoryRef (java.nio)
allocateDirect:258, ByteBuffer (java.nio)
allocateMemory:352, TensorBuffer (org.tensorflow.lite.support.tensorbuffer)
:304, TensorBuffer (org.tensorflow.lite.support.tensorbuffer)
:38, TensorBufferFloat (org.tensorflow.lite.support.tensorbuffer)
createDynamic:96, TensorBuffer (org.tensorflow.lite.support.tensorbuffer)
getTensorBuffer:297, TensorImage$ImageContainer (org.tensorflow.lite.support.image)
getTensorBuffer:213, TensorImage (org.tensorflow.lite.support.image)
apply:52, TensorOperatorWrapper (org.tensorflow.lite.support.image.ops)
Process is detached when calling ByteBuffer.allocateDirect (in allocateMemory function):
&lt;denchmark-code&gt;    private void allocateMemory(int[] shape) {
        SupportPreconditions.checkNotNull(shape, "TensorBuffer shape cannot be null.");
        SupportPreconditions.checkArgument(isShapeValid(shape), "Values in TensorBuffer shape should be non-negative.");
        int newFlatSize = computeFlatSize(shape);
        if (this.flatSize != newFlatSize) {
            this.flatSize = newFlatSize;
            this.shape = (int[])shape.clone();
            this.buffer = ByteBuffer.allocateDirect(this.flatSize * this.getTypeSize()); &lt;&lt;&lt;&lt; HERE &lt;&lt;&lt;&lt;
            this.buffer.order(ByteOrder.nativeOrder());
        }
    }
&lt;/denchmark-code&gt;


Debugging the 1MP model on NNAPI (the issue is that the app crashes):

Process is detached when calling applyDelegate (in applyDelegates function, in NativeInterpreterWapper.class):
&lt;denchmark-code&gt;while(var6.hasNext()) {
                Delegate delegate = (Delegate)var6.next();
                applyDelegate(this.interpreterHandle, this.errorHandle, delegate.getNativeHandle()); &lt;&lt;&lt;&lt; HERE &lt;&lt;&lt;&lt;
                this.delegates.add(delegate);
            }
&lt;/denchmark-code&gt;


Debugging the 10MP model on GPU (the issue is that the .tflite model doesn't even gets loaded):

I see that the Asset Manager gets closed.
Last debugger frames:
get:145, ClosedGuard (dalvik.system)
:104, ParcelFileDescriptor (android.os)
:184, ParcelFileDescriptor (android.os)
nativeOpenAssetFd:-1, AssetManager (android.content.res)
openFd:848, AssetManager (android.content.res)
loadMappedFile:154, FileUtil (org.tensorflow.lite.support.common)
	</description>
	<comments>
		<comment id='1' author='orangesomethingorange' date='2020-04-03T17:40:52Z'>
		&lt;denchmark-link:https://github.com/orangesomethingorange&gt;@orangesomethingorange&lt;/denchmark-link&gt;
 Can you please share the code to create model and converting it to tflite model? It is difficult to find root-cause of the issue without model. I will check the model and resolve the issue. Thanks!
		</comment>
		<comment id='2' author='orangesomethingorange' date='2020-04-11T10:44:19Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 Thanks! I already changed the whole design since I couldn't get pass the memory issue. Now the Interpreter instance is running, but I have different issues regarding batch image processing and converting byte, float and int values. I'll open a new issue if I'll find a bug or post on stackoverflow :)
		</comment>
		<comment id='3' author='orangesomethingorange' date='2020-04-11T10:44:27Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38161&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38161&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='orangesomethingorange' date='2020-07-24T06:45:12Z'>
		
@jvishnuvardhan Thanks! I already changed the whole design since I couldn't get pass the memory issue. Now the Interpreter instance is running, but I have different issues regarding batch image processing and converting byte, float and int values. I'll open a new issue if I'll find a bug or post on stackoverflow :)

hello,How did you solve your problemï¼ŸI ran into the same problem
		</comment>
		<comment id='5' author='orangesomethingorange' date='2020-07-24T06:46:10Z'>
		I would be very grateful if you could explain briefly
		</comment>
		<comment id='6' author='orangesomethingorange' date='2020-07-24T17:31:05Z'>
		&lt;denchmark-link:https://github.com/WindowsDriver&gt;@WindowsDriver&lt;/denchmark-link&gt;
 Please create a new issue with a standalone code to reproduce the error. Thanks!
		</comment>
	</comments>
</bug>