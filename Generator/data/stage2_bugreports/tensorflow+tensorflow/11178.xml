<bug id='11178' author='elbaro' open_date='2017-06-30T15:24:56Z' closed_time='2017-12-29T22:46:16Z'>
	<summary>Inconsistent losses for Keras</summary>
	<description>
TF 1.2.0, Python 3.5.
&lt;denchmark-h:h2&gt;TF Official Keras&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;tf.contrib.keras.backend.sparse_categorical_crossentropy (output, target)
tf.contrib.keras.losses.sparse_categorical_crossentropy  (target, output)
tf.contrib.keras.metrics.sparse_categorical_crossentropy (target, output)
tf.losses.sparse_softmax_cross_entropy (target, output)  - no sentinel
tf.nn.sparse_softmax_cross_entropy_with_logits (target, output)
'sparse_softmax_cross_entropy'
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Pure Keras&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;keras.losses.sparse_categorical_crossentropy(target, output)
&lt;/denchmark-code&gt;

There are many sparse_softmax_cross_entropy but they all have different APIs. Some are compatible with tf.contrib.keras.models.Model.compile() and  the others are not because of _sentinel and different order of output and target.
Interestingly both the original TF and Keras use (target, output) and only tf.contrib.keras.backend version uses (output, target). tf-keras needs to use (output, target) otherwise the model is not trained.
Can we unify the interface and document when to use each version?
	</description>
	<comments>
		<comment id='1' author='elbaro' date='2017-06-30T20:11:04Z'>
		Thanks for reaching out!
cc: &lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='elbaro' date='2017-06-30T20:42:07Z'>
		Yes. We can remove the sentinel. We put it in as a safety since the those losses were not symmetric and we switched the argument order.
Hopefully, then, all losses will use (target, output), and can all be used with compile.
		</comment>
		<comment id='3' author='elbaro' date='2017-06-30T21:03:18Z'>
		
tf.contrib.keras.backend version uses (output, target)

That was modeled after the initial TF version. It was an oversight and we can change it. backend.sparse_categorical_crossentropy is meant as a cross-backend layer, it's not for public consumption; the official API is in keras.losses.
		</comment>
		<comment id='4' author='elbaro' date='2017-09-20T21:58:15Z'>
		I also found that the input arguments order of keras.backend.categorical_crossentropy() and keras.backend.tensorflow_backend.categorical_crossentropy() are not consistent with keras.losses.categorical_crossentropy(). I found this when I try to use the loss function from keras to check the numpy implementation of cross entropy loss function:
&lt;denchmark-code&gt;import numpy as np
import keras
import keras.backend as K

np.random.seed(666)
def categorical_crossentropy_np(y_true, y_pred):
    _EPSILON = K.epsilon()
    y_pred = y_pred / np.expand_dims(np.sum(y_pred, axis=-1), axis=-1)
    y_pred = np.clip(y_pred, _EPSILON, 1.0-_EPSILON)
    return -np.sum(y_true * np.log(y_pred), axis=-1)

def check_loss():
    shape = (6, 8)

    y_a = np.random.random(shape).astype(np.float32)
    y_b = np.random.random(shape).astype(np.float32)

    # case 1
    out1 = K.eval(K.categorical_crossentropy(K.variable(y_a), K.variable(y_b)))
    # case 2
    # out1 = K.eval(K.categorical_crossentropy(K.variable(y_b), K.variable(y_a))) 
    # case 3
    # out1 = K.eval(keras.backend.tensorflow_backend.categorical_crossentropy(K.variable(y_a), K.variable(y_b))) 
    # case 4
    # out1 = K.eval(keras.backend.tensorflow_backend.categorical_crossentropy(K.variable(y_b), K.variable(y_a))) 
    # case 5
    # out1 = K.eval(keras.losses.categorical_crossentropy(K.variable(y_a), K.variable(y_b))) 
    # case 6
    out2 = categorical_crossentropy_np(y_a, y_b)

    assert out1.shape == out2.shape
    assert out1.shape == shape[:-1]

    print out1
    print out2

if __name__ == '__main__':
    check_loss()

&lt;/denchmark-code&gt;

The results are:
(case 2 = case 4 = case 5 = case 6) ≠ (case 1 = case 3)
		</comment>
		<comment id='5' author='elbaro' date='2017-09-25T16:08:26Z'>
		The backend functions are internal only and fixable. PRs welcome.
		</comment>
		<comment id='6' author='elbaro' date='2017-09-25T17:47:52Z'>
		This specific issue has been fixed in Keras 2.0.7. The version included in
contrib as of TF 1.3 is Keras 2.0.6, though.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On 25 September 2017 at 09:12, Martin Wicke ***@***.***&gt; wrote:
 The backend functions are internal only and fixable. PRs welcome.

 —
 You are receiving this because you were assigned.
 Reply to this email directly, view it on GitHub
 &lt;#11178 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AArWb3stv4yHDeqTMwyVzdRIPQCYN4ufks5sl9EJgaJpZM4OKuWG&gt;
 .



		</comment>
		<comment id='7' author='elbaro' date='2017-09-25T20:37:28Z'>
		Got it. Thanks for your reply!
		</comment>
		<comment id='8' author='elbaro' date='2017-12-20T19:06:20Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='9' author='elbaro' date='2017-12-29T22:46:16Z'>
		We updated past 2.0.9, so this should be fixed.
		</comment>
	</comments>
</bug>