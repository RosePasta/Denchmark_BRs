<bug id='41101' author='houseofai' open_date='2020-07-05T13:11:37Z' closed_time='2020-07-31T12:44:49Z'>
	<summary>MultiWorkerMirroredStrategy: Multi-workers don't train once synced</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No. I used Keras tutorial: Multi-worker training with Keras
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Chief: Ubuntu 20.04
Worker: Ubuntu 20.04
TensorFlow installed from (source or binary):From a Docker image:  tensorflow/tensorflow:latest-gpu
TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0
Python version:  3.6.9
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: V10.1.243
GPU model and memory:
Chief: Titan RTX 24 Go
Worker: 4x GTX 1060 6Go
You can collect some of this information using our environment capture: tf_env.txt

Describe the current behavior
I'm running the multi_worker_with_keras tutorial on 2 computers: 1 chief and 1 worker, both equipped with different GPUs.
I run the tutorial into a tensorflow-gpu docker image on each computer. Each computer has been configured to communicate with an ssh key, without a password needed.


The single_worker_model part of the tutorial works fine on both computers: each computer go through the epochs individually


Regarding the MultiWorkerMirroredStrategy part, If I omit to set TF_CONFIG as an environment variable, both nodes train correctly on their side. Note: On the worker, the strategy distributes the work across the 4 local GPUs as expected.


When I run the final script - TF_CONFIG + MultiWorkerMirroredStrategy - by starting the chief first, then the worker, the chief wait for the worker, then they sync. They finally both start epoch 1 and hold.


Describe the expected behavior

Have both workers to train after logging Epoch 1/60
Have both workers to sync

Standalone code to reproduce the issue
On the chief:
&lt;denchmark-code&gt;docker run --gpus all -it -p 12345:12345 --rm tensorflow/tensorflow:latest-gpu
export TF_CONFIG='{"cluster": {"worker": ["192.168.1.31:12345", "192.168.1.46:12345"]}, "task": {"index": 0, "type": "worker"}}'
[copy the script to the docker container]
python multi_worker_with_keras.py
&lt;/denchmark-code&gt;

On the worker:
&lt;denchmark-code&gt;docker run --gpus all -it -p 12345:12345 --rm tensorflow/tensorflow:latest-gpu
export TF_CONFIG='{"cluster": {"worker": ["192.168.1.31:12345", "192.168.1.46:12345"]}, "task": {"index": 1, "type": "worker"}}'
[copy the script to the docker container]
python multi_worker_with_keras.py
&lt;/denchmark-code&gt;

Note: Only the index changes
The script:
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np

def mnist_dataset(batch_size):
  (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()
  x_train = x_train / np.float32(255)
  y_train = y_train.astype(np.int64)
  train_dataset = tf.data.Dataset.from_tensor_slices(
      (x_train, y_train)).shuffle(60000).repeat().batch(batch_size)
  return train_dataset

def build_and_compile_cnn_model():
  model = tf.keras.Sequential([
      tf.keras.Input(shape=(28, 28)),
      tf.keras.layers.Reshape(target_shape=(28, 28, 1)),
      tf.keras.layers.Conv2D(32, 3, activation='relu'),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(128, activation='relu'),
      tf.keras.layers.Dense(10)
  ])
  model.compile(
      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
      optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
      metrics=['accuracy'])
  return model


strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

per_worker_batch_size = 512
num_workers = 2

global_batch_size = per_worker_batch_size * num_workers
multi_worker_dataset = mnist_dataset(global_batch_size)

with strategy.scope():
  multi_worker_model = build_and_compile_cnn_model()

multi_worker_model.fit(multi_worker_dataset, epochs=60, steps_per_epoch=60)

&lt;/denchmark-code&gt;

Note: I have raised the number of epochs, steps_per_epoch, and per_worker_batch_size. I have changed the number of num_workers from 4 to 2.
Note: I haven't done the "ModelCheckpoint callback" part as I wanted to validate the training sync first.
Note: I have tested with auto sharding policy OFF (code from the tutorial), and I don't get the "Found an unshardable source dataset" error. But the workers don't train either.
&lt;denchmark-code&gt;options = tf.data.Options()
options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF
dataset_no_auto_shard = multi_worker_dataset.with_options(options)
&lt;/denchmark-code&gt;



&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4874928/chief.log&gt;chief.log&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;2020-07-05 12:52:14.867922: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-05 12:52:14.867929: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-05 12:52:14.867935: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-05 12:52:14.867941: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-05 12:52:14.867948: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-05 12:52:14.867954: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-05 12:52:14.867960: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-05 12:52:14.868007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-05 12:52:14.868671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-05 12:52:14.869292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-07-05 12:52:14.869303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-05 12:52:14.869307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2020-07-05 12:52:14.869311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2020-07-05 12:52:14.869374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-05 12:52:14.870033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-07-05 12:52:14.870662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 21960 MB memory) -&gt; physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:09:00.0, compute capability: 7.5)
2020-07-05 12:52:14.873278: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:12345, 1 -&gt; 192.168.1.46:12345}
2020-07-05 12:52:14.873849: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:12345
WARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an "evaluator" task exists in the cluster.
WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
2020-07-05 12:52:22.818158: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:434] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Found an unshardable source dataset: name: "TensorSliceDataset/_2"
op: "TensorSliceDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 28
        }
        dim {
          size: 28
        }
      }
      shape {
      }
    }
  }
}

Epoch 1/60
2020-07-05 12:52:25.479341: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-05 12:52:25.731883: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7

&lt;/denchmark-code&gt;


&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4874930/worker.log&gt;worker.log&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;2020-07-05 12:52:20.891307: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-07-05 12:52:20.891332: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-05 12:52:20.891358: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-07-05 12:52:20.891383: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-07-05 12:52:20.891407: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-07-05 12:52:20.891431: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-07-05 12:52:20.891455: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-05 12:52:20.898699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1, 2, 3
2020-07-05 12:52:20.898900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-05 12:52:20.898916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1 2 3 
2020-07-05 12:52:20.898928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N Y Y Y 
2020-07-05 12:52:20.898938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   Y N Y Y 
2020-07-05 12:52:20.898948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 2:   Y Y N Y 
2020-07-05 12:52:20.898958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 3:   Y Y Y N 
2020-07-05 12:52:20.904332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:0 with 5644 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:19:00.0, compute capability: 6.1)
2020-07-05 12:52:20.905569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:1 with 5644 MB memory) -&gt; physical GPU (device: 1, name: GeForce GTX 1060 6GB, pci bus id: 0000:1a:00.0, compute capability: 6.1)
2020-07-05 12:52:20.906769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:2 with 5644 MB memory) -&gt; physical GPU (device: 2, name: GeForce GTX 1060 6GB, pci bus id: 0000:67:00.0, compute capability: 6.1)
2020-07-05 12:52:20.907967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:3 with 5631 MB memory) -&gt; physical GPU (device: 3, name: GeForce GTX 1060 6GB, pci bus id: 0000:68:00.0, compute capability: 6.1)
2020-07-05 12:52:20.915563: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; 192.168.1.31:12345, 1 -&gt; localhost:12345}
2020-07-05 12:52:20.917163: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:12345
WARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an "evaluator" task exists in the cluster.
WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
2020-07-05 12:52:22.833446: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:434] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Found an unshardable source dataset: name: "TensorSliceDataset/_2"
op: "TensorSliceDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 28
        }
        dim {
          size: 28
        }
      }
      shape {
      }
    }
  }
}

Epoch 1/60
2020-07-05 12:52:25.979992: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-07-05 12:52:26.242036: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='houseofai' date='2020-07-15T20:06:52Z'>
		Hi @OdysseeT,  to clarify you are trying to run multiworker training with two machines, following the tutorial in the docs. When you run your script with TF CONFIG set on each machine, the program hangs. Is that correct?
I don't know what the exact issue is here, but as a first step can you take a look at these similar issues and let me know if that helps?  &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/37108&gt;#37108&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/35878&gt;#35878&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/36846&gt;#36846&lt;/denchmark-link&gt;

Additionally, can you run on tf nightly and let me know if the issue persists?
		</comment>
		<comment id='2' author='houseofai' date='2020-07-18T07:01:04Z'>
		Hi &lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;
, thanks for your answer. Indeed, it looks similar to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/35878&gt;#35878&lt;/denchmark-link&gt;
. I would require a couple of days to set up the environment again.
Is there a way to add more verbosity to the logs?
		</comment>
		<comment id='3' author='houseofai' date='2020-07-30T15:20:07Z'>
		Adding this for reference:
export GRPC_VERBOSITY=DEBUG
It provides more log.
I tried to reproduce the issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/35878&gt;#35878&lt;/denchmark-link&gt;
 with working code on two aws ec2 instances without any success.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/5002142/tf-distributed-ec2.log&gt;tf-distributed-ec2.log&lt;/denchmark-link&gt;

The two instances are syncing, they can connect without any password through SSH using a public key.
Without any way to print more logs, I can't provide more input
		</comment>
		<comment id='4' author='houseofai' date='2020-07-30T22:26:40Z'>
		I think I'm a little confused now. Can you clarify what you mean when you say you tried to reproduce the issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/35878&gt;#35878&lt;/denchmark-link&gt;
? You ran the code from that issue and worked or didn't work? Also what code did you run?
		</comment>
		<comment id='5' author='houseofai' date='2020-07-31T09:25:58Z'>
		I ran two codes separately:

My code from the current issue. Taken from the online Keras tutorial mentioned above.
The working code of the issue #35878 (from this comment)

I couldn't get any of them working and they hang at the same step.
		</comment>
		<comment id='6' author='houseofai' date='2020-07-31T12:29:49Z'>
		Oh gosh... got it working ðŸ˜„ it was painful ðŸ¤•
Here are a couple of environment variables to set which could help to debug:
&lt;denchmark-code&gt;GRPC_TRACE=all
GRPC_VERBOSITY=DEBUG
GRPC_GO_LOG_SEVERITY_LEVEL=info
GRPC_GO_LOG_VERBOSITY_LEVEL=2
CGO_ENABLED=1
NCCL_DEBUG=INFO
&lt;/denchmark-code&gt;

At least there are some logs displayed on both machines
&lt;denchmark-code&gt;2020-07-31 12:10:26.242083: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:405] Started server with target: grpc://172.31.15.58:12345
ip-172-31-15-58:6249:6375 [0] NCCL INFO Bootstrap : Using [0]ens5:172.31.15.58&lt;0&gt;
ip-172-31-15-58:6249:6375 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
ip-172-31-15-58:6249:6375 [0] NCCL INFO NET/IB : No device found.
ip-172-31-15-58:6249:6375 [0] NCCL INFO NET/Socket : Using [0]ens5:172.31.15.58&lt;0&gt;
ip-172-31-15-58:6249:6375 [0] NCCL INFO Using network Socket
D0731 12:10:33.011138327    6360 dns_resolver.cc:261]        Start resolving.
I0731 12:10:33.011426331    6293 subchannel.cc:1055]         New connected subchannel at 0x7fe070010690 for subchannel 0x7fe084018f00
WARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an "evaluator" task exists in the cluster.
WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
Epoch 1/235
WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Iterator.get_next_as_optional()` instead.
2020-07-31 12:10:36.476241: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-07-31 12:10:36.688205: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
235/235 [==============================] - 8s 33ms/step - loss: 2.2482 - accuracy: 0.3359
Epoch 2/235
235/235 [==============================] - 8s 33ms/step - loss: 2.0450 - accuracy: 0.6375
[...]
&lt;/denchmark-code&gt;

But I have no idea if there is any sync happening between the epochs as nothing shows up...
		</comment>
		<comment id='7' author='houseofai' date='2020-07-31T12:44:50Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41101&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41101&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='houseofai' date='2020-12-21T17:22:25Z'>
		
Oh gosh... got it working ðŸ˜„ it was painful ðŸ¤•
Here are a couple of environment variables to set which could help to debug:
GRPC_TRACE=all
GRPC_VERBOSITY=DEBUG
GRPC_GO_LOG_SEVERITY_LEVEL=info
GRPC_GO_LOG_VERBOSITY_LEVEL=2
CGO_ENABLED=1
NCCL_DEBUG=INFO

At least there are some logs displayed on both machines
2020-07-31 12:10:26.242083: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:405] Started server with target: grpc://172.31.15.58:12345
ip-172-31-15-58:6249:6375 [0] NCCL INFO Bootstrap : Using [0]ens5:172.31.15.58&lt;0&gt;
ip-172-31-15-58:6249:6375 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
ip-172-31-15-58:6249:6375 [0] NCCL INFO NET/IB : No device found.
ip-172-31-15-58:6249:6375 [0] NCCL INFO NET/Socket : Using [0]ens5:172.31.15.58&lt;0&gt;
ip-172-31-15-58:6249:6375 [0] NCCL INFO Using network Socket
D0731 12:10:33.011138327    6360 dns_resolver.cc:261]        Start resolving.
I0731 12:10:33.011426331    6293 subchannel.cc:1055]         New connected subchannel at 0x7fe070010690 for subchannel 0x7fe084018f00
WARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an "evaluator" task exists in the cluster.
WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
Epoch 1/235
WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Iterator.get_next_as_optional()` instead.
2020-07-31 12:10:36.476241: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-07-31 12:10:36.688205: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
235/235 [==============================] - 8s 33ms/step - loss: 2.2482 - accuracy: 0.3359
Epoch 2/235
235/235 [==============================] - 8s 33ms/step - loss: 2.0450 - accuracy: 0.6375
[...]

But I have no idea if there is any sync happening between the epochs as nothing shows up...

&lt;denchmark-link:https://github.com/houseofai&gt;@houseofai&lt;/denchmark-link&gt;
 How did you manage to make it work?
What have you changed/added in your code?
		</comment>
		<comment id='9' author='houseofai' date='2020-12-23T08:41:46Z'>
		&lt;denchmark-link:https://github.com/diman82&gt;@diman82&lt;/denchmark-link&gt;
 As far as I remember, I haven't change my code. The environement variables mentionned above helped to understand the different steps of synchronization. I mainly tried to switch the first to start: worker or chief. I gueess there is also a "moment" when to start the right one. But this is still obscure to me.
		</comment>
	</comments>
</bug>