<bug id='19283' author='tkurmann' open_date='2018-05-15T02:37:04Z' closed_time='2019-03-07T00:15:49Z'>
	<summary>Tensor with GPUBFCAllocator generate Segfaults on tensorflow::ops</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): r1.8
Python version:  2.7
Bazel version (if compiling from source): 0.18
GCC/Compiler version (if compiling from source): 5.4
CUDA/cuDNN version: 9.0 / 7.1
GPU model and memory: nvidia p6000
Exact command to reproduce:

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Using the C++ api, when a tensorflow::Tensor is allocated using GPUBFCAllocator, all further operations (such as tensorflow::op::Identity) generate a segfault. Memory is properly allocated (tested with cudaMemcpy). If the allocator is removed, or switched to tensorflow::cpu_allocator(), the error does not occur, but the memory is not allocated on the gpu (memcpy fails). My final goal is to obtain the cuda pointer of the tensor to fill the tensor with data manually.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

Reproducible code:
&lt;denchmark-code&gt;  auto root = tensorflow::Scope::NewRootScope().WithDevice("/gpu:0");
  tensorflow::ClientSession* session = new tensorflow::ClientSession(root);
  
  tensorflow::VisitableAllocator* gpu_allocator;
  gpu_allocator = new tensorflow::GPUBFCAllocator(tensorflow::CudaGpuId(0), sizeof(float) * 512 * 640 * 3, "bfc_gpu_0");
  gpu_allocator = new tensorflow::GPUcudaMallocAllocator(gpu_allocator, tensorflow::CudaGpuId(0));

  void* myp = gpu_allocator-&gt;AllocateRaw(1,512*640*3*4);
  tensorflow::Tensor *myTensor = new tensorflow::Tensor(gpu_allocator,tensorflow::DT_FLOAT, tensorflow::TensorShape({1, 512, 640, 3}));

  checkCudaErrors(cudaMemcpy(tensorflow::DMAHelper::base(myTensor), myp, sizeof(float) * 512 * 640 *3, cudaMemcpyDeviceToDevice));

  auto identity = tensorflow::ops::Identity(root.WithOpName("init"), *myTensor);
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='tkurmann' date='2018-05-15T17:35:17Z'>
		&lt;denchmark-link:https://github.com/ekelsen&gt;@ekelsen&lt;/denchmark-link&gt;
 can you comment?
		</comment>
		<comment id='2' author='tkurmann' date='2018-08-15T23:57:52Z'>
		My 2 cents: currently all CUDA device memory are allocated through the CUDA Stream API, which does not guarantee to synchronize with user CUDA code, i.e. it only enqueues the memory allocation ops and returns immediately instead of waiting it to finish.
You could either do manual synchronization or use TensorFlowâ€™s wrapped CUDA StreamExecutor for async cuMemcpy (higher performance in general).
		</comment>
		<comment id='3' author='tkurmann' date='2018-11-21T02:33:30Z'>
		thank you for your share, Now I can use GPU memory Directly feed into Tensor gragh
		</comment>
		<comment id='4' author='tkurmann' date='2019-02-22T22:04:59Z'>
		Could I get unassigned from this please?
		</comment>
		<comment id='5' author='tkurmann' date='2019-03-03T17:03:37Z'>
		&lt;denchmark-link:https://github.com/tkurmann&gt;@tkurmann&lt;/denchmark-link&gt;
 Could you follow &lt;denchmark-link:https://github.com/byronyi&gt;@byronyi&lt;/denchmark-link&gt;
 suggestion or install new version of TF and check whether the bug persists. If the issue was resolved, please close the issue. Thanks!
		</comment>
		<comment id='6' author='tkurmann' date='2019-03-07T00:15:48Z'>
		I think it was resolved. Please open a new ticket if I am wrong. Please also check this bug with the latest TF. Thanks!
		</comment>
	</comments>
</bug>