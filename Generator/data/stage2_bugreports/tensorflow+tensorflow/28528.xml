<bug id='28528' author='ajarthurs' open_date='2019-05-08T19:20:22Z' closed_time='2019-05-15T13:17:49Z'>
	<summary>TFLite calibration-and-quantization I/O min/max mismatch on some operators</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes


OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Arch Linux 5.0.10


Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A


TensorFlow installed from (source or binary):
source


TensorFlow version (use command below):
Running: https://github.com/ajarthurs/tensorflow/tree/q_leakyrelu
Base commit: 3ea8756
Related PR: #27028
Related issue: #28268


Python version:
3.7.3


Bazel version (if compiling from source):
0.25.0


GCC/Compiler version (if compiling from source):
8.3.0


CUDA/cuDNN version:
N/A


GPU model and memory:
N/A


You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
During calibration-and-quantization (see tensorflow/lite/tools/optimize/calibration), the tensor feeding a RELU operator shows that its minimum value is 0 instead of a negative number. This may lead to a quantized model that produces erroneous results; however, I currently cannot confirm if the said behavior is causing erroneous quantization.
Describe the expected behavior
I expect RELU's input minimum value (for quantization) to match the input tensor's actual minimum value (a negative real number).
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
An MSCOCO-trained floating-point (32bit) model that I am attempting to calibrate-and-quantize is available at &lt;denchmark-link:https://www.dropbox.com/s/hnkmqzcasb8lu8n/retinanet-float32.tflite?dl=0&gt;https://www.dropbox.com/s/hnkmqzcasb8lu8n/retinanet-float32.tflite?dl=0&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3154074/calibrate_and_quantize.py.zip&gt;calibrate_and_quantize.py&lt;/denchmark-link&gt;
 is an example script that runs TF Lite's calibration-and-quantization tool. It can be executed as such:
&lt;denchmark-code&gt;# Adjust paths as needed.
$ PYTHONPATH=path/to/tensorflow/tpu/models/official/retinanet:$PYTHONPATH python calibrate_and_quantize.py --input_tflite_file=retinanet-float32.tflite --output_tflite_file=retinanet-int8.tflite --train_file_pattern=path/to/mscoco-tfrecords/train*
&lt;/denchmark-code&gt;

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
While running the attached Python script, it reports the following warnings:
&lt;denchmark-code&gt;Note the output min/max is different from the input min/max for op MAX_POOL_2D at index 2 in subgraph 0. This is legal but should happens rarely.
Note the output min/max is different from the input min/max for op RESIZE_BILINEAR at index 77 in subgraph 0. This is legal but should happens rarely.
Note the output min/max is different from the input min/max for op RESIZE_BILINEAR at index 80 in subgraph 0. This is legal but should happens rarely.
&lt;/denchmark-code&gt;

And during one my gdb sessions, I check the I/O min/max values of a MAX_POOL_2D operator highlighted by one of the warnings above. The output minimum value of the MAX_POOL_2D operator is 0 instead of -19.3, which would throw the said warning.
&lt;denchmark-code&gt;Thread 1 "calibrate_and_q" hit Breakpoint 2, tflite::optimize::(anonymous namespace)::QuantizeOpOutput (
    model=0x55822760c890, subgraph_idx=0, op_idx=2, property=..., output_idx=0, error_reporter=0x558223944030)
    at tensorflow/lite/tools/optimize/quantize_model.cc:504
504             printf(
(gdb) p min
$5 = -19.2778091
(gdb) p max
$6 = 22.6024609
...
(gdb) p output_tensor-&gt;quantization-&gt;min[0]
$10 = 0
(gdb) p op_code
$11 = tflite::BuiltinOperator_MAX_POOL_2D
(gdb) p output_tensor-&gt;quantization-&gt;max[0]
$12 = 22.6024609
(gdb) bt
#0  tflite::optimize::(anonymous namespace)::QuantizeOpOutput (model=0x55822760c890, subgraph_idx=0, op_idx=2,
    property=..., output_idx=0, error_reporter=0x558223944030)
    at tensorflow/lite/tools/optimize/quantize_model.cc:504
#1  0x00007f78cc28497c in tflite::optimize::(anonymous namespace)::QuantizeWeightsInputOutput (
    builder=0x7fff304b4860, model=0x55822760c890, allow_float=false, error_reporter=0x558223944030)
    at tensorflow/lite/tools/optimize/quantize_model.cc:570
#2  0x00007f78cc284faf in tflite::optimize::QuantizeModel (builder=0x7fff304b4860, model=0x55822760c890,
    input_type=@0x7fff304b4844: tflite::TensorType_INT8, output_type=@0x7fff304b4848: tflite::TensorType_INT8,
    allow_float=false, error_reporter=0x558223944030) at tensorflow/lite/tools/optimize/quantize_model.cc:638
#3  0x00007f78cc26a8c1 in tflite::calibration_wrapper::CalibrationWrapper::QuantizeModel (this=0x5582274c7790,
    input_py_type=1, output_py_type=1, allow_float=false)
    at tensorflow/lite/python/optimize/calibration_wrapper.cc:201
#4  0x00007f78cc268f88 in _wrap_CalibrationWrapper_QuantizeModel (args=0x7f78cc697458)
    at bazel-out/k8-dbg/bin/tensorflow/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.cc:3418
#5  0x00007f795c552e68 in _PyMethodDef_RawFastCallKeywords () from /usr/lib/libpython3.7m.so.1.0
#6  0x00007f795c553101 in _PyCFunction_FastCallKeywords () from /usr/lib/libpython3.7m.so.1.0
#7  0x00007f795c5c3d19 in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.7m.so.1.0
#8  0x00007f795c5526db in _PyFunction_FastCallKeywords () from /usr/lib/libpython3.7m.so.1.0
#9  0x00007f795c5c36ea in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.7m.so.1.0
#10 0x00007f795c50bd09 in _PyEval_EvalCodeWithName () from /usr/lib/libpython3.7m.so.1.0
#11 0x00007f795c552882 in _PyFunction_FastCallKeywords () from /usr/lib/libpython3.7m.so.1.0
#12 0x00007f795c5bff9c in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.7m.so.1.0
#13 0x00007f795c50bd09 in _PyEval_EvalCodeWithName () from /usr/lib/libpython3.7m.so.1.0
#14 0x00007f795c552882 in _PyFunction_FastCallKeywords () from /usr/lib/libpython3.7m.so.1.0
#15 0x00007f795c5bf22d in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.7m.so.1.0
#16 0x00007f795c5526db in _PyFunction_FastCallKeywords () from /usr/lib/libpython3.7m.so.1.0
#17 0x00007f795c5bf22d in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.7m.so.1.0
#18 0x00007f795c50bd09 in _PyEval_EvalCodeWithName () from /usr/lib/libpython3.7m.so.1.0
#19 0x00007f795c552882 in _PyFunction_FastCallKeywords () from /usr/lib/libpython3.7m.so.1.0
#20 0x00007f795c5c36ea in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.7m.so.1.0
#21 0x00007f795c50bd09 in _PyEval_EvalCodeWithName () from /usr/lib/libpython3.7m.so.1.0
--Type &lt;RET&gt; for more, q to quit, c to continue without paging--q
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='ajarthurs' date='2019-05-14T05:53:46Z'>
		&lt;denchmark-link:https://github.com/ajarthurs&gt;@ajarthurs&lt;/denchmark-link&gt;
 , is this problem solved for you ? My understanding is that you are getting min max output as different from input in MAX2Dpool, as toco graph transform does the hardcoding for the Min and max values.
Regards
Amit
		</comment>
		<comment id='2' author='ajarthurs' date='2019-05-14T14:50:12Z'>
		&lt;denchmark-link:https://github.com/amitsrivastava78&gt;@amitsrivastava78&lt;/denchmark-link&gt;
 The calibration-and-quantization tool is reporting different min/max values around  as a warning. The source of the warning is at &lt;denchmark-link:https://github.com/ajarthurs/tensorflow/blob/q_leakyrelu/tensorflow/lite/tools/optimize/quantize_model.cc#L504-L508&gt;tensorflow/lite/tools/optimize/quantize_model.cc&lt;/denchmark-link&gt;
. But following that, &lt;denchmark-link:https://github.com/ajarthurs/tensorflow/blob/q_leakyrelu/tensorflow/lite/tools/optimize/quantize_model.cc#L512-L518&gt;the node's output tensor's quant-params are replaced with the input tensor's quant-params&lt;/denchmark-link&gt;
. Also confirmed with  that 's output minimum is correctly set to a negative value with  as its activation function.
So, this looks to be a non-issue save for the warnings. This issue can be closed unless you think the warnings need to be looked into.
Att: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3178371/qint8.html.txt&gt;qint8.html.txt&lt;/denchmark-link&gt;
 -- Quantized int8 model HTML from 
		</comment>
		<comment id='3' author='ajarthurs' date='2019-05-15T04:03:23Z'>
		&lt;denchmark-link:https://github.com/ajarthurs&gt;@ajarthurs&lt;/denchmark-link&gt;
 thanks for the confirmation what i meant in my response was that source of the warning is the toco graph transformation which hardcodes the min and max of the maxpool2D, you can refer to HardcodeMinMaxForAverageOrMaxPool function in lite/toco/graph_transformations/hardcode_min_max.cc
Yes this looks like not an issue to me as well. We can close this one.
Regards
Amit
		</comment>
		<comment id='4' author='ajarthurs' date='2019-05-15T13:17:50Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=28528&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=28528&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='ajarthurs' date='2019-05-15T13:21:32Z'>
		&lt;denchmark-link:https://github.com/amitsrivastava78&gt;@amitsrivastava78&lt;/denchmark-link&gt;
 OK. Thanks, Amit.
		</comment>
	</comments>
</bug>