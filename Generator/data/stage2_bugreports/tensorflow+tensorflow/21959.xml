<bug id='21959' author='htcai' open_date='2018-08-29T20:55:57Z' closed_time='2019-02-11T19:24:13Z'>
	<summary>tf.contrib.training.batch_sequences_with_states treats batching as duplicating</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): N/A
TensorFlow version (use command below): 1.10.0
Python version: 3.6.3
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

When train.batch_sequences_with_states extracts batches from a sequence of samples, what it actually does is duplicating the same segment for batch_size times.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

The code is revised based on &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/contrib/training/python/training/batch_sequences_with_states_test.py&gt;batch_sequences_with_states_test.py&lt;/denchmark-link&gt;

from pprint import pprint

import numpy as np
import tensorflow as tf
from tensorflow.contrib.training.python.training import sequence_queueing_state_saver as sqss

batch_size = 3
num_unroll = 2
lstm_size = 4
value_length = 8
input_key = tf.as_string(tf.cast(10000 * tf.random_uniform(()), tf.int32))
input_sequences = {'input': np.random.rand(value_length, 3)}
input_context = {'context_key': [1]}
initial_states = {"lstm_state": np.random.rand(10, lstm_size)}

with tf.Session() as sess:
    batch = sqss.batch_sequences_with_states(
        input_key=input_key,
        input_sequences=input_sequences,
        input_context=input_context,
        input_length=value_length,
        initial_states=initial_states,
        num_unroll=num_unroll,
        batch_size=batch_size)
    state = batch.state('lstm_state')
    update_state = batch.save_state('lstm_state', state + 1)
    coord = tf.train.Coordinator()
    tf.train.start_queue_runners(sess=sess, coord=coord)
    input_batch_val = sess.run([
        batch.key, batch.next_key, batch.sequences['input'],
        batch.context['context_key'], state, batch.length, update_state][2])
    pprint(input_batch_val)
Output:
&lt;denchmark-code&gt;array([[[0.33537843, 0.76504494, 0.368679  ],
        [0.47943187, 0.58871135, 0.06263617]],

       [[0.33537843, 0.76504494, 0.368679  ],
        [0.47943187, 0.58871135, 0.06263617]],

       [[0.33537843, 0.76504494, 0.368679  ],
        [0.47943187, 0.58871135, 0.06263617]]])
&lt;/denchmark-code&gt;

What further justifies my suspect is the expected values of sequences in unit tests &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/contrib/training/python/training/batch_sequences_with_states_test.py&gt;batch_sequences_with_states_test.py&lt;/denchmark-link&gt;
, which is duplicating the first segment of .
def _testBasicPadding(self, pad, key=None, make_keys_unique=False):
    num_unroll = 2  # Divisor of value_length - so no padding necessary.
    expected_seq1_batch1 = np.tile(
        self.sequences["seq1"][np.newaxis, 0:num_unroll, :],
(self.batch_size, 1, 1))
	</description>
	<comments>
		<comment id='1' author='htcai' date='2019-01-10T22:28:04Z'>
		Is this still useful? It looks like this is using queues. Is there functionality in tf.data that you can use instead?
		</comment>
		<comment id='2' author='htcai' date='2019-02-09T18:46:20Z'>
		It has been 29 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='3' author='htcai' date='2019-02-11T19:24:13Z'>
		Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
		<comment id='4' author='htcai' date='2019-02-11T19:41:45Z'>
		Sorry that I didn't notice your comment posted in January.
After extensive exploration of the source code of tensorflow and the &lt;denchmark-link:https://github.com/tensorflow/models/tree/master/research/adversarial_text&gt;code&lt;/denchmark-link&gt;
 written by Andrew Dai and Neal Wu, I found out that the argument  requires thread-and-queue-based implementation of the input sequences. Nonetheless, this crucial point is absent from the documentation. Later, I made a &lt;denchmark-link:https://github.com/htcai/Companion-to-TensorFlow-Documentation/blob/master/tf_contrib_training_batch_sequences_with_states.ipynb&gt;notebook&lt;/denchmark-link&gt;
 illustrating the usage of .
Thanks for your suggestion, I have started to learn tf.data.
		</comment>
		<comment id='5' author='htcai' date='2019-02-11T21:57:06Z'>
		&lt;denchmark-link:https://github.com/htcai&gt;@htcai&lt;/denchmark-link&gt;
 Thanks for the nice writeup and the resource. Thanks!
		</comment>
	</comments>
</bug>