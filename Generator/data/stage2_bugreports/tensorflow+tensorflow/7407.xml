<bug id='7407' author='bxshi' open_date='2017-02-10T05:53:27Z' closed_time='2017-10-10T12:49:11Z'>
	<summary>Estimator does not catch OutOfRange error</summary>
	<description>
I created a [4, 1, 5] tensor and use
&lt;denchmark-code&gt;feature_a = tf.train.limit_epochs(testing_data_a, num_epochs=1, name="feature_a_limit")
tf.train.batch([feature_a], batch_size=2, enqueue_many=True,
                                                                allow_smaller_final_batch=True, name="feature_a_batch")
&lt;/denchmark-code&gt;

to create a batch queue that produces the tensor only once, and I then use this in the input_fn function of model.evaluate with steps=2. Because the batch size is 2 and the steps is 2, I expect this could use up the batch queue without an OutOfRange error (or the Estimator will catch this error and use it as an indicator to stop the evaluation), however the Estimator does not catch it and the program ends.
Is this an expected behavior? If I remember correctly Estimator used to catch such error. Another question is, why the FIFO queue still throw the OutOfRange even if I just request the exact number of elements in the queue?
&lt;denchmark-h:h3&gt;What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?&lt;/denchmark-h&gt;

&lt;denchmark-link:http://stackoverflow.com/questions/42127447/tensorflows-estimator-can-only-get-n-1-batches-from-tf-train-limit-epochs&gt;http://stackoverflow.com/questions/42127447/tensorflows-estimator-can-only-get-n-1-batches-from-tf-train-limit-epochs&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System:
&lt;denchmark-code&gt;Linux DOMAIN 3.13.0-107-generic #154-Ubuntu SMP Tue Dec 20 09:57:27 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
&lt;/denchmark-code&gt;


The commit hash (git rev-parse HEAD)

&lt;denchmark-code&gt;1536a84f32f1fe77efd3fee6e5933a1dfe4e10bb
&lt;/denchmark-code&gt;


The output of bazel version

&lt;denchmark-code&gt;Build label: 0.4.3
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Dec 22 12:31:25 2016 (1482409885)
Build timestamp: 1482409885
Build timestamp as int: 1482409885
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)&lt;/denchmark-h&gt;

import tensorflow as tf
from tensorflow.contrib.layers.python.layers.optimizers import optimize_loss
from tensorflow.contrib.learn.python.learn.estimators import model_fn
from tensorflow.contrib.learn.python.learn.estimators.estimator import Estimator
from tensorflow.python import debug as tf_debug
from tensorflow.python.framework import ops


def main(_):
    hooks = [tf_debug.LocalCLIDebugHook()]

    def func(features, targets, mode, params):
        idx = tf.concat([features['a'], features['b']], axis=1)

        embedding = tf.get_variable("embed", [10, 20], dtype=tf.float32)

        pred = tf.reduce_sum(tf.nn.embedding_lookup(embedding, idx))

        train_op = optimize_loss(loss=pred,
                                 global_step=tf.train.get_global_step(),
                                 learning_rate=0.001,
                                 optimizer='Adam',
                                 variables=tf.trainable_variables(),
                                 name="training_loss_optimizer")

        eval_metric_dict = dict()
        eval_metric_dict['metric'] = pred

        return model_fn.ModelFnOps(mode=mode,
                                   predictions=pred,
                                   loss=pred,
                                   train_op=train_op,
                                   eval_metric_ops=eval_metric_dict)

    model = Estimator(func, params={})

    model.fit(
        input_fn=lambda: (
            {'a': ops.convert_to_tensor([[1, 2, 3, 4, 5]]), 'b': ops.convert_to_tensor([[2, 3, 4, 3, 5]])},
            None), max_steps=10)

    testing_data_a = [[1, 2, 3, 4, 5], [1, 2, 3, 4, 5] , [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]
    testing_data_b = [[2, 3, 4, 3, 5], [2, 3, 4, 3, 5] , [2, 3, 4, 3, 5], [2, 3, 4, 3, 5]]

    def test_input_fn():
        print("test_input_fn entered")
        feature_a = tf.train.limit_epochs(testing_data_a, num_epochs=1, name="feature_a_limit")
        feature_b = tf.train.limit_epochs(testing_data_b, num_epochs=1, name="feature_b_limit")

        feature_a_producer, feature_b_producer = tf.train.batch([feature_a, feature_b], batch_size=2, enqueue_many=True,
                                                                allow_smaller_final_batch=True, name="feature_a_batch")

        print("test_input_fn exit")
        return {'a': feature_a_producer, 'b': feature_b_producer}, None

    for i in range(10):
        print(model.evaluate(input_fn=test_input_fn, steps=2))
        print("one iteration done")


if __name__ == "__main__":
    tf.app.run()
&lt;denchmark-h:h3&gt;What other attempted solutions have you tried?&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Logs or other output that would be helpful&lt;/denchmark-h&gt;

(If logs are large, please upload as attachment or provide link).
&lt;denchmark-code&gt;ssh://bshi@dsg1.crc.nd.edu:22/data/bshi/py3env/bin/python -u /data/bshi/ProjC/estimator_test.py --debug
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpxtkow_oq
test_input_fn entered
test_input_fn exit
W tensorflow/core/framework/op_kernel.cc:993] Out of range: FIFOQueue '_0_feature_a_batch/fifo_queue' is closed and has insufficient elements (requested 2, current size 0)
	 [[Node: feature_a_batch = QueueDequeueUpToV2[component_types=[DT_INT32, DT_INT32], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](feature_a_batch/fifo_queue, feature_a_batch/n)]]
Traceback (most recent call last):
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py", line 1022, in _do_call
    return fn(*args)
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py", line 1004, in _run_fn
    status, run_metadata)
  File "/usr/lib/python3.4/contextlib.py", line 66, in __exit__
    next(self.gen)
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/framework/errors_impl.py", line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_0_feature_a_batch/fifo_queue' is closed and has insufficient elements (requested 2, current size 0)
	 [[Node: feature_a_batch = QueueDequeueUpToV2[component_types=[DT_INT32, DT_INT32], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](feature_a_batch/fifo_queue, feature_a_batch/n)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/data/bshi/ProjC/estimator_test.py", line 62, in &lt;module&gt;
    tf.app.run()
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/platform/app.py", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File "/data/bshi/ProjC/estimator_test.py", line 57, in main
    print(model.evaluate(input_fn=test_input_fn, steps=2))
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/util/deprecation.py", line 280, in new_func
    return func(*args, **kwargs)
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py", line 514, in evaluate
    log_progress=log_progress)
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py", line 836, in _evaluate_model
    hooks=hooks)
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/contrib/training/python/training/evaluation.py", line 430, in evaluate_once
    session.run(eval_ops, feed_dict)
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py", line 478, in __exit__
    self._close_internal(exception_type)
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py", line 508, in _close_internal
    h.end(self._coordinated_creator.tf_sess)
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/training/basic_session_run_hooks.py", line 647, in end
    feed_dict=self._final_ops_feed_dict)
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py", line 767, in run
    run_metadata_ptr)
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py", line 965, in _run
    feed_dict_string, options, run_metadata)
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py", line 1015, in _do_run
    target_list, options, run_metadata)
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_0_feature_a_batch/fifo_queue' is closed and has insufficient elements (requested 2, current size 0)
	 [[Node: feature_a_batch = QueueDequeueUpToV2[component_types=[DT_INT32, DT_INT32], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](feature_a_batch/fifo_queue, feature_a_batch/n)]]

Caused by op 'feature_a_batch', defined at:
  File "/data/bshi/ProjC/estimator_test.py", line 62, in &lt;module&gt;
    tf.app.run()
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/platform/app.py", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File "/data/bshi/ProjC/estimator_test.py", line 57, in main
    print(model.evaluate(input_fn=test_input_fn, steps=2))
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/util/deprecation.py", line 280, in new_func
    return func(*args, **kwargs)
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py", line 514, in evaluate
    log_progress=log_progress)
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py", line 800, in _evaluate_model
    features, labels = input_fn()
  File "/data/bshi/ProjC/estimator_test.py", line 51, in test_input_fn
    allow_smaller_final_batch=True, name="feature_a_batch")
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/training/input.py", line 872, in batch
    name=name)
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/training/input.py", line 665, in _batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/ops/data_flow_ops.py", line 510, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 1402, in _queue_dequeue_up_to_v2
    timeout_ms=timeout_ms, name=name)
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py", line 763, in apply_op
    op_def=op_def)
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/framework/ops.py", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File "/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/framework/ops.py", line 1264, in __init__
    self._traceback = _extract_stack()

OutOfRangeError (see above for traceback): FIFOQueue '_0_feature_a_batch/fifo_queue' is closed and has insufficient elements (requested 2, current size 0)
	 [[Node: feature_a_batch = QueueDequeueUpToV2[component_types=[DT_INT32, DT_INT32], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](feature_a_batch/fifo_queue, feature_a_batch/n)]]


Process finished with exit code 1
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='bxshi' date='2017-02-10T16:55:55Z'>
		&lt;denchmark-link:https://github.com/ilblackdragon&gt;@ilblackdragon&lt;/denchmark-link&gt;
 Should learn catch this?
		</comment>
		<comment id='2' author='bxshi' date='2017-02-12T20:32:02Z'>
		Is there an alternative way to feed batches into model.evaluate?
		</comment>
		<comment id='3' author='bxshi' date='2017-03-09T13:45:27Z'>
		I have the same issue on Windows 10. I saw that when I use the batched inputs for the fit operation and setting steps=None it works and terminates the training after the epoch is finished. But when I use the same input queue for evaluate and also provide steps=None as parameter, it crashes with the OutOfRangeError.
		</comment>
		<comment id='4' author='bxshi' date='2017-04-21T13:37:06Z'>
		Current workaround is setting eval_steps parameter to a concrete integer value but not None.
I think it should catch every OutOfRangeError because developers usually want to use whole validation/test dataset to get the best distribution.
		</comment>
		<comment id='5' author='bxshi' date='2017-04-25T21:06:10Z'>
		Ouch. I'm currently experiencing the same problem with the same parameters to tf.train.batch() and with different batch_sizes. Using TensorFlow 1.0.1 and I'm parsing TFRecords.
		</comment>
		<comment id='6' author='bxshi' date='2017-05-21T17:11:39Z'>
		Same problem here on TensorFlow 1.1 (Windows build).
		</comment>
		<comment id='7' author='bxshi' date='2017-05-21T17:20:14Z'>
		Seems to be solved on TensorFlow 1.1. (at least on Linux).
&lt;denchmark-link:https://github.com/sunsided&gt;@sunsided&lt;/denchmark-link&gt;
 It's rather odd because Estimators are written in Python. Did you make sure that you are using 1.1 release and not 1.1rcX?
		</comment>
		<comment id='8' author='bxshi' date='2017-05-22T22:11:49Z'>
		&lt;denchmark-link:https://github.com/pkubik&gt;@pkubik&lt;/denchmark-link&gt;
 Yep, 1.1.0 as per . (Further, calling the input function in a validation monitor (despite deprecated) stops my training when the "validation" input queue reaches its end. That might be a different issue, but it definitely is not smooth.)
		</comment>
		<comment id='9' author='bxshi' date='2017-05-24T18:55:56Z'>
		&lt;denchmark-link:https://github.com/pkubik&gt;@pkubik&lt;/denchmark-link&gt;
 Just tried on Linux (built from  branch) and have the same problem with "contrib"   and the . Looking at the code of the latter, there also seems to be no  at all ... am I missing something obvious?
&lt;denchmark-code&gt;2017-05-24 20:52:38.532029: W tensorflow/core/framework/op_kernel.cc:1152] Out of range: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]
2017-05-24 20:52:38.532066: W tensorflow/core/framework/op_kernel.cc:1152] Out of range: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]
2017-05-24 20:52:38.532262: W tensorflow/core/framework/op_kernel.cc:1152] Out of range: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]
2017-05-24 20:52:38.694023: W tensorflow/core/framework/op_kernel.cc:1152] Out of range: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]
2017-05-24 20:52:38.694249: W tensorflow/core/framework/op_kernel.cc:1152] Out of range: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]
2017-05-24 20:52:38.695189: W tensorflow/core/framework/op_kernel.cc:1152] Out of range: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]
2017-05-24 20:52:38.695218: W tensorflow/core/framework/op_kernel.cc:1152] Out of range: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]
2017-05-24 20:52:38.695568: W tensorflow/core/framework/op_kernel.cc:1152] Out of range: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]
2017-05-24 20:52:38.695924: W tensorflow/core/framework/op_kernel.cc:1152] Out of range: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]
Traceback (most recent call last):
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1039, in _do_call
    return fn(*args)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1021, in _run_fn
    status, run_metadata)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/contextlib.py", line 89, in __exit__
    next(self.gen)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/evaluation.py", line 182, in _evaluate_once
    session.run(eval_ops, feed_dict)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py", line 484, in run
    run_metadata=run_metadata)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py", line 820, in run
    run_metadata=run_metadata)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py", line 776, in run
    return self._sess.run(*args, **kwargs)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py", line 930, in run
    run_metadata=run_metadata)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py", line 776, in run
    return self._sess.run(*args, **kwargs)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 778, in run
    run_metadata_ptr)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 982, in _run
    feed_dict_string, options, run_metadata)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1032, in _do_run
    target_list, options, run_metadata)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1052, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op 'shuffle_batch', defined at:
  File "/opt/pycharm-2017.1.2/helpers/pydev/pydevd.py", line 1585, in &lt;module&gt;
    globals = debugger.run(setup['file'], None, None, is_module)
  File "/opt/pycharm-2017.1.2/helpers/pydev/pydevd.py", line 1015, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File "/opt/pycharm-2017.1.2/helpers/pydev/_pydev_imps/_pydev_execfile.py", line 18, in execfile
    exec(compile(contents+"\n", file, 'exec'), glob, loc)
  File "/home/mmayer/dev/foobar.py", line 206, in &lt;module&gt;
    main()
  File "/home/mmayer/dev/foobar.py", line 40, in main
    run_training(nn, args, model_params)
  File "/home/mmayer/dev/foobar.py", line 58, in run_training
    ev = run_evaluation(nn, args, model_params, best_f=None, use_best_checkpoint=True)
  File "/home/mmayer/dev/foobar.py", line 82, in run_evaluation
    ev = nn.evaluate(input_fn=lambda: input_fn(args.eval_dir, params=model_params, epochs=1), steps=None)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py", line 268, in evaluate
    name=name)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py", line 604, in _evaluate_model
    features, labels = input_fn()
  File "/home/mmayer/dev/foobar.py", line 82, in &lt;lambda&gt;
    ev = nn.evaluate(input_fn=lambda: input_fn(args.eval_dir, params=model_params, epochs=1), steps=None)
  File "/home/mmayer/dev/input_pipeline/input_fn.py", line 31, in input_fn
    allow_smaller_final_batch=True)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/input.py", line 1214, in shuffle_batch
    name=name)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/input.py", line 782, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/ops/data_flow_ops.py", line 499, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 1420, in _queue_dequeue_up_to_v2
    timeout_ms=timeout_ms, name=name)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", line 768, in apply_op
    op_def=op_def)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 2336, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1228, in __init__
    self._traceback = _extract_stack()

OutOfRangeError (see above for traceback): RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1039, in _do_call
    return fn(*args)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1021, in _run_fn
    status, run_metadata)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/contextlib.py", line 89, in __exit__
    next(self.gen)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/pycharm-2017.1.2/helpers/pydev/pydevd.py", line 1585, in &lt;module&gt;
    globals = debugger.run(setup['file'], None, None, is_module)
  File "/opt/pycharm-2017.1.2/helpers/pydev/pydevd.py", line 1015, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File "/opt/pycharm-2017.1.2/helpers/pydev/_pydev_imps/_pydev_execfile.py", line 18, in execfile
    exec(compile(contents+"\n", file, 'exec'), glob, loc)
  File "/home/mmayer/dev/foobar.py", line 206, in &lt;module&gt;
    main()
  File "/home/mmayer/dev/foobar.py", line 40, in main
    run_training(nn, args, model_params)
  File "/home/mmayer/dev/foobar.py", line 58, in run_training
    ev = run_evaluation(nn, args, model_params, best_f=None, use_best_checkpoint=True)
  File "/home/mmayer/dev/foobar.py", line 82, in run_evaluation
    ev = nn.evaluate(input_fn=lambda: input_fn(args.eval_dir, params=model_params, epochs=1), steps=None)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py", line 268, in evaluate
    name=name)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py", line 632, in _evaluate_model
    config=config_pb2.ConfigProto(allow_soft_placement=True))
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/evaluation.py", line 182, in _evaluate_once
    session.run(eval_ops, feed_dict)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py", line 500, in __exit__
    self._close_internal(exception_type)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py", line 532, in _close_internal
    h.end(self._coordinated_creator.tf_sess)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py", line 663, in end
    feed_dict=self._final_ops_feed_dict)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 778, in run
    run_metadata_ptr)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 982, in _run
    feed_dict_string, options, run_metadata)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1032, in _do_run
    target_list, options, run_metadata)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1052, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]

Caused by op 'shuffle_batch', defined at:
  File "/opt/pycharm-2017.1.2/helpers/pydev/pydevd.py", line 1585, in &lt;module&gt;
    globals = debugger.run(setup['file'], None, None, is_module)
  File "/opt/pycharm-2017.1.2/helpers/pydev/pydevd.py", line 1015, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File "/opt/pycharm-2017.1.2/helpers/pydev/_pydev_imps/_pydev_execfile.py", line 18, in execfile
    exec(compile(contents+"\n", file, 'exec'), glob, loc)
  File "/home/mmayer/dev/foobar.py", line 206, in &lt;module&gt;
    main()
  File "/home/mmayer/dev/foobar.py", line 40, in main
    run_training(nn, args, model_params)
  File "/home/mmayer/dev/foobar.py", line 58, in run_training
    ev = run_evaluation(nn, args, model_params, best_f=None, use_best_checkpoint=True)
  File "/home/mmayer/dev/foobar.py", line 82, in run_evaluation
    ev = nn.evaluate(input_fn=lambda: input_fn(args.eval_dir, params=model_params, epochs=1), steps=None)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py", line 268, in evaluate
    name=name)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py", line 604, in _evaluate_model
    features, labels = input_fn()
  File "/home/mmayer/dev/foobar.py", line 82, in &lt;lambda&gt;
    ev = nn.evaluate(input_fn=lambda: input_fn(args.eval_dir, params=model_params, epochs=1), steps=None)
  File "/home/mmayer/dev/input_pipeline/input_fn.py", line 31, in input_fn
    allow_smaller_final_batch=True)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/input.py", line 1214, in shuffle_batch
    name=name)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/input.py", line 782, in _shuffle_batch
    dequeued = queue.dequeue_up_to(batch_size, name=name)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/ops/data_flow_ops.py", line 499, in dequeue_up_to
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 1420, in _queue_dequeue_up_to_v2
    timeout_ms=timeout_ms, name=name)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", line 768, in apply_op
    op_def=op_def)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 2336, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File "/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1228, in __init__
    self._traceback = _extract_stack()

OutOfRangeError (see above for traceback): RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)
	 [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]
&lt;/denchmark-code&gt;

The line in question in my code is a
image_batch = tf.train.shuffle_batch([file_names, image_bytes],
                                                 batch_size=params['batch_size'],
                                                 capacity=1000, min_after_dequeue=100,
                                                 num_threads=params['input_threads'],
                                                 enqueue_many=True,
                                                 allow_smaller_final_batch=True)
		</comment>
		<comment id='10' author='bxshi' date='2017-05-25T08:59:54Z'>
		I found that my code using Estimator works well when it is passed no eval_metric_ops. Does someone experience the same?
		</comment>
		<comment id='11' author='bxshi' date='2017-05-26T10:37:26Z'>
		&lt;denchmark-link:https://github.com/raviqqe&gt;@raviqqe&lt;/denchmark-link&gt;
 I have partially reproduced your error. It occurs after passing an invalid metric to the . Each metric should be a tuple of . It worked for me until I made a quick hack (don't ask why) and passed a  to the .
I'm not sure how is it related to the queues though. Anyway, please make sure that your metric ops are correct.
Edit:
I should add that you'd probably want to use tf.metrics.*.
		</comment>
		<comment id='12' author='bxshi' date='2017-06-16T22:49:34Z'>
		&lt;denchmark-link:https://github.com/bxshi&gt;@bxshi&lt;/denchmark-link&gt;
, did &lt;denchmark-link:https://github.com/pkubik&gt;@pkubik&lt;/denchmark-link&gt;
's suggestion to fix your metrix solve your problem?
		</comment>
		<comment id='13' author='bxshi' date='2017-07-10T14:42:56Z'>
		&lt;denchmark-link:https://github.com/pkubik&gt;@pkubik&lt;/denchmark-link&gt;
's suggestion fixed the problem for me.
After replacing my homemade metric
tf.reduce_mean(tf.cast(tf.equal(predicted_classes, labels), tf.float32))
with
tf.metrics.accuracy(labels, predicted_classes)
it runs smoothly.
Maybe something could throw a more useful exception that indicates how to solve the problem, when people try to use improper metrics?
		</comment>
		<comment id='14' author='bxshi' date='2017-10-10T12:49:31Z'>
		This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!
		</comment>
		<comment id='15' author='bxshi' date='2018-02-14T06:52:24Z'>
		I have a workaround that makes use of a homemade metric. I just wrapped around the homemade metric with tf.metrics.mean like so:
tf.metrics.mean(tf.reduce_mean(tf.cast(tf.equal(predicted_classes, labels), tf.float32)))
It works but I'm not sure if it outputs the same thing as using tf.metrics.accuracy. If it does output the same thing, it would be possible to use homemade metrics.
		</comment>
	</comments>
</bug>