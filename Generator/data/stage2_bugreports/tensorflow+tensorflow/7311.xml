<bug id='7311' author='loliverhennigh' open_date='2017-02-07T02:34:28Z' closed_time='2017-04-24T22:00:49Z'>
	<summary>tfrecords giving parsing error when saved examples are too big</summary>
	<description>
When I try to save a example to a tfrecord that is too large a parsing error occurs. I need to save a large sequence of float valued images (states of a fluid flow simulation) and when the states and sequences get too large a parsing error occurs.
&lt;denchmark-h:h3&gt;What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?&lt;/denchmark-h&gt;

I think that this might be the same error and problem that other people are getting when saving long sequences to tfrecords like seen here &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/5234&gt;#5234&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/OliviaMG/xiaomeng/issues/1&gt;OliviaMG/xiaomeng#1&lt;/denchmark-link&gt;
. While both of these threads found solutions the underlining problem does not seem to be found.
&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System:
Ubuntu 16.04
CUDA 8.0
CUDNN 5.1
TensorFlow 0.12.1
&lt;denchmark-h:h3&gt;If possible, provide a minimal reproducible example&lt;/denchmark-h&gt;

Running this minimal script will cause the error. Lowering the SIZE_RECORD value will cause the script to run with out error.
&lt;denchmark-code&gt;from tqdm import tqdm
import numpy as np
import tensorflow as tf

# this will kill it
SIZE_RECORD=20000000
# this will run just fine
# SIZE_RECORD=2000000

writer = tf.python_io.TFRecordWriter("test.tfrecords")
# iterate over 10 times
# wrap with tqdm for a progress bar
for example_idx in tqdm(xrange(2)):
    features = np.zeros((SIZE_RECORD))

    # construct the Example proto boject
    example = tf.train.Example(
        # Example contains a Features proto object
        features=tf.train.Features(
          # Features contains a map of string to Feature proto objects
          feature={
            # A Feature contains one of either a int64_list,
            # float_list, or bytes_list
            'image': tf.train.Feature(
                float_list=tf.train.FloatList(value=features.astype("float"))),
    }))
    # use the proto object to serialize the example to a string
    serialized = example.SerializeToString()
    # write the serialized object to disk
    writer.write(serialized)
writer.close()

def read_and_decode_single_example(filename):
    # first construct a queue containing a list of filenames.
    # this lets a user split up there dataset in multiple files to keep
    # size down
    filename_queue = tf.train.string_input_producer([filename],
                                                    num_epochs=None)
    # Unlike the TFRecordWriter, the TFRecordReader is symbolic
    reader = tf.TFRecordReader()
    # One can read a single serialized example from a filename
    # serialized_example is a Tensor of type string.
    _, serialized_example = reader.read(filename_queue)
    # The serialized example is converted back to actual values.
    # One needs to describe the format of the objects to be returned
    features = tf.parse_single_example(
        serialized_example,
        features={
            # We know the length of both fields. If not the
            # tf.VarLenFeature could be used
            'image': tf.FixedLenFeature([SIZE_RECORD], tf.float32)
        })
    # now return the converted data
    image = features['image']
    return image

# get single examples
image = read_and_decode_single_example("test.tfrecords")
# groups examples into batches randomly
images_batch = tf.train.shuffle_batch(
    [image], batch_size=1,
    capacity=3,
    min_after_dequeue=2)

# simple model
w = tf.get_variable("w1", [SIZE_RECORD, 1])
y_pred = tf.matmul(images_batch, w)
loss = tf.nn.l2_loss(y_pred - 1.0)

# for monitoring
loss_mean = tf.reduce_mean(loss)

train_op = tf.train.AdamOptimizer().minimize(loss)

sess = tf.Session()
init = tf.initialize_all_variables()
sess.run(init)
tf.train.start_queue_runners(sess=sess)

_, loss_val = sess.run([train_op, loss_mean])
print loss_val
print("worked!!!")
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;What other attempted solutions have you tried?&lt;/denchmark-h&gt;

I have tried saving the example in a variety of different ways including converting it to a string and breaking up the vector into multiple features.
&lt;denchmark-h:h3&gt;Logs or other output that would be helpful&lt;/denchmark-h&gt;

This is the output when the tfrecord is too big
W tensorflow/core/framework/op_kernel.cc:975] Invalid argument: Could not parse example input, value: '
���&amp;
���&amp;
image����&amp;����&amp;
���&amp;
ERROR:tensorflow:Exception in QueueRunner: 'utf8' codec can't decode byte 0x9b in position 40: invalid start byte
Exception in thread Thread-3:
Traceback (most recent call last):
File "/usr/lib/python2.7/threading.py", line 801, in __bootstrap_inner
self.run()
File "/usr/lib/python2.7/threading.py", line 754, in run
self.__target(*self.__args, **self.__kwargs)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py", line 234, in _run
sess.run(enqueue_op)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 766, in run
run_metadata_ptr)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 964, in _run
feed_dict_string, options, run_metadata)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1014, in _do_run
target_list, options, run_metadata)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1021, in _do_call
return fn(*args)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1003, in _run_fn
status, run_metadata)
File "/usr/lib/python2.7/contextlib.py", line 24, in exit
self.gen.next()
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py", line 468, in raise_exception_on_not_ok_status
compat.as_text(pywrap_tensorflow.TF_Message(status)),
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/compat.py", line 84, in as_text
return bytes_or_text.decode(encoding)
File "/usr/lib/python2.7/encodings/utf_8.py", line 16, in decode
return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: 'utf8' codec can't decode byte 0x9b in position 40: invalid start byte
Thank!!!
	</description>
	<comments>
		<comment id='1' author='loliverhennigh' date='2017-02-07T15:59:32Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 I'm suspicious that this is a TFRecord problem, since the sizes don't approach INT_MAX.  The next likely candidate is the example parsing code.  Thoughts?
		</comment>
		<comment id='2' author='loliverhennigh' date='2017-02-07T16:11:29Z'>
		I have the same issue when writing large float lists to the tfrecords file. 64 MB seems to be the limit.
		</comment>
		<comment id='3' author='loliverhennigh' date='2017-02-07T16:15:15Z'>
		&lt;denchmark-link:https://github.com/jonasrauber&gt;@jonasrauber&lt;/denchmark-link&gt;
 Ah, right.  Protobufs are limited to 64 MB outside of Google for some unfathomable reason.  &lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
 Was there a hack to increase this?
		</comment>
		<comment id='4' author='loliverhennigh' date='2017-02-07T16:27:32Z'>
		Using the other protobuf library as mentioned here doesn't help: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/5676&gt;#5676&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='loliverhennigh' date='2017-02-11T23:14:01Z'>
		I am having a similar problem when trying to read my Example protocol bufffer in batches. I have also tried the other protobuf library, but that did not help.
		</comment>
		<comment id='6' author='loliverhennigh' date='2017-02-13T22:01:27Z'>
		Same issue for me. Increasing size of protobufs would be great :)
		</comment>
		<comment id='7' author='loliverhennigh' date='2017-04-03T03:18:22Z'>
		Facing the same issue working with high-resolution images. Anyone find a hack around this?
		</comment>
		<comment id='8' author='loliverhennigh' date='2017-04-03T04:32:51Z'>
		Looks like installing Tensorflow 1.1.0-rc0 solved the issue for me.
		</comment>
		<comment id='9' author='loliverhennigh' date='2017-04-24T21:31:54Z'>
		I can also confirm that using Tensorflow 1.1.0 solved the problem for us.
		</comment>
		<comment id='10' author='loliverhennigh' date='2017-04-24T21:55:40Z'>
		Same here :D
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Apr 24, 2017 4:33 PM, "Elmer Garduno" ***@***.***&gt; wrote:
 I can also confirm that using Tensorflow 1.1.0 solved the problem for us.

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 &lt;#7311 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AEv26DKUPuzl_kTfBrjuYtZKn7gvPlCwks5rzRU1gaJpZM4L4-up&gt;
 .



		</comment>
		<comment id='11' author='loliverhennigh' date='2017-04-24T22:00:49Z'>
		Thank you for checking back. I will close this issue as resolved in 1.1.
		</comment>
	</comments>
</bug>