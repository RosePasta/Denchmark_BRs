<bug id='38416' author='TMaysGGS' open_date='2020-04-10T03:07:49Z' closed_time='2020-04-30T17:41:34Z'>
	<summary>tf.keras: Model parameters suddenly updated to 'nan' during back propagation when training</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.1.0
Python version: 3.7.4
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.1/7.6.5
GPU model and memory: Nvidia 1080Ti

Describe the current behavior
I was trying to train a small net similar to PNet of MTCNN and I wrote a custom loss to test if it is working. During the training, after several epochs the model weights and loss became 'nan'.
I test the training procedure one epoch by one and found that the last epoch that gives a normal loss (0.0814), also outputs a model with all parameters of 'nan'. Thus I think when giving a normal loss, the backward propagation has something wrong and gives the model a 'nan' update.
What I have done to rule out some other possibilities:
(1) Check &amp; clean the data:
My data set is:
X: images of shape (12, 12, 3);
Y: label, box regression coords &amp; 6-landmark regression coords concatenated together of shape (17, ).
For the label, it could be 1, -1, 0, -2 where only labels 1 and 0 will participate in calculating the custom loss I wrote myself.
For the roi &amp; landmark coords, they all belong to [-1, 1].
For the image data, it will be processed as: (x - 127.5) / 128. before being sent into the training stream.
I tried both the TFRecords dataflow &amp; numpy array as the input for training.
(2) Add BatchNormalization layer, add L2-Norm to the weights, use Xavier initialization and pick a smaller learning rate (from 0.001 to 0.0001) to avoid problems like gradient exploding.
(3) Replace the custom loss I wrote myself with 'mse'.
All the three changes made did not fix the 'nan' loss thing.
Describe the expected behavior
The training procedure should work well.
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
&lt;denchmark-code&gt;def pnet_train1(train_with_landmark = False):
    
    X = Input(shape = (12, 12, 3), name = 'Pnet_input')
    
    M = Conv2D(10, 3, strides = 1, padding = 'valid', kernel_initializer = glorot_normal, kernel_regularizer = l2(0.00001), name = 'Pnet_conv1')(X)
    M = PReLU(shared_axes = [1, 2], name = 'Pnet_prelu1')(M)
    M = MaxPooling2D(pool_size = 2, name = 'Pnet_maxpool1')(M) # default 'pool_size' is 2!!! 
    
    M = Conv2D(16, 3, strides = 1, padding = 'valid', kernel_initializer = glorot_normal, kernel_regularizer = l2(0.00001), name = 'Pnet_conv2')(M)
    M = PReLU(shared_axes= [1, 2], name = 'Pnet_prelu2')(M)
    
    M = Conv2D(32, 3, strides = 1, padding = 'valid', kernel_initializer = glorot_normal, kernel_regularizer = l2(0.00001), name = 'Pnet_conv3')(M)
    M = PReLU(shared_axes= [1, 2], name = 'Pnet_prelu3')(M)
    
    Classifier_conv = Conv2D(1, 1, activation = 'sigmoid', name = 'Pnet_classifier_conv', kernel_initializer = glorot_normal)(M)
    Bbox_regressor_conv = Conv2D(4, 1, name = 'Pnet_bbox_regressor_conv', kernel_initializer = glorot_normal)(M)
    Landmark_regressor_conv = Conv2D(12, 1, name = 'Pnet_landmark_regressor_conv', kernel_initializer = glorot_normal)(M)
    
    Classifier = Reshape((1, ), name = 'Pnet_classifier')(Classifier_conv)
    Bbox_regressor = Reshape((4, ), name = 'Pnet_bbox_regressor')(Bbox_regressor_conv) 
    if train_with_landmark: 
        Landmark_regressor = Reshape((12, ), name = 'Pnet_landmark_regressor')(Landmark_regressor_conv)
        Pnet_output = Concatenate()([Classifier, Bbox_regressor, Landmark_regressor]) 
        model = Model(X, Pnet_output) 
    else:
        Pnet_output = Concatenate()([Classifier, Bbox_regressor])
        model = Model(X, Pnet_output)
    
    return model

def pnet_train2(train_with_landmark = False):

    X = Input(shape = (12, 12, 3), name = 'Pnet_input')

    M = Conv2D(10, 3, strides = 1, padding = 'valid', use_bias = False, kernel_initializer = glorot_normal, kernel_regularizer = l2(0.00001), name = 'Pnet_conv1')(X)
    M = BatchNormalization(axis = -1, name = 'Pnet_bn1')(M)
    M = PReLU(shared_axes = [1, 2], name = 'Pnet_prelu1')(M)
    M = MaxPooling2D(pool_size = 2, name = 'Pnet_maxpool1')(M) # default 'pool_size' is 2!!! 

    M = Conv2D(16, 3, strides = 1, padding = 'valid', use_bias = False, kernel_initializer = glorot_normal, kernel_regularizer = l2(0.00001), name = 'Pnet_conv2')(M)
    M = BatchNormalization(axis = -1, name = 'Pnet_bn2')(M)
    M = PReLU(shared_axes= [1, 2], name = 'Pnet_prelu2')(M)

    M = Conv2D(32, 3, strides = 1, padding = 'valid', use_bias = False, kernel_initializer = glorot_normal, kernel_regularizer = l2(0.00001), name = 'Pnet_conv3')(M)
    M = BatchNormalization(axis = -1, name = 'Pnet_bn3')(M)
    M = PReLU(shared_axes= [1, 2], name = 'Pnet_prelu3')(M)

    Classifier_conv = Conv2D(1, 1, activation = 'sigmoid', name = 'Pnet_classifier_conv', kernel_initializer = glorot_normal)(M)
    Bbox_regressor_conv = Conv2D(4, 1, name = 'Pnet_bbox_regressor_conv', kernel_initializer = glorot_normal)(M)
    Landmark_regressor_conv = Conv2D(12, 1, name = 'Pnet_landmark_regressor_conv', kernel_initializer = glorot_normal)(M)

    Classifier = Reshape((1, ), name = 'Pnet_classifier')(Classifier_conv)
    Bbox_regressor = Reshape((4, ), name = 'Pnet_bbox_regressor')(Bbox_regressor_conv) 
    if train_with_landmark: 
        Landmark_regressor = Reshape((12, ), name = 'Pnet_landmark_regressor')(Landmark_regressor_conv)
        Pnet_output = Concatenate()([Classifier, Bbox_regressor, Landmark_regressor]) 
        model = Model(X, Pnet_output) 
    else:
        Pnet_output = Concatenate()([Classifier, Bbox_regressor])
        model = Model(X, Pnet_output)

    return model

# Here just check the the first classify loss. 
def custom_loss(y_true, y_pred):
        
    zero_index = K.zeros_like(y_true[:, 0]) 
    ones_index = K.ones_like(y_true[:, 0]) 
    
    labels = y_true[:, 0] 
    class_preds = y_pred[:, 0] 
    bi_crossentropy_loss = -labels * K.log(class_preds) - (1 - labels) * K.log(1 - class_preds) 
    
    classify_valid_index = tf.where(K.less(y_true[:, 0], 0), zero_index, ones_index) 
    classify_keep_num = K.cast(tf.cast(tf.reduce_sum(classify_valid_index), tf.float32) * 0.7, dtype = tf.int32) 
    
    classify_loss_sum = bi_crossentropy_loss * tf.cast(classify_valid_index, bi_crossentropy_loss.dtype) 
    classify_loss_sum_filtered, _ = tf.nn.top_k(classify_loss_sum, k = classify_keep_num) 
    classify_loss = tf.where(K.equal(classify_keep_num, 0), tf.constant(0, dtype = tf.float32), K.mean(classify_loss_sum_filtered)) 
    
    loss = classify_loss 
    
    return loss
&lt;/denchmark-code&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
&lt;denchmark-link:https://user-images.githubusercontent.com/31966022/78958654-4d557100-7b1b-11ea-8f5b-52406fb76111.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='TMaysGGS' date='2020-04-13T08:16:18Z'>
		&lt;denchmark-link:https://github.com/TMaysGGS&gt;@TMaysGGS&lt;/denchmark-link&gt;
,
In order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here along with the dataset you are using . Thanks!
		</comment>
		<comment id='2' author='TMaysGGS' date='2020-04-13T15:01:11Z'>
		
@TMaysGGS,
In order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here along with the dataset you are using . Thanks!

Hi, &lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;
 ,
Thanks for your reply. The complete version of my codes is the training pnet part of a Keras ver. MTCNN in my repo &lt;denchmark-link:https://github.com/TMaysGGS/MTCNN-Keras&gt;MTCNN-Keras&lt;/denchmark-link&gt;
.
The main codes are below:
&lt;denchmark-code&gt;import os
import sys
import argparse
import tensorflow as tf
import tensorflow.python.keras.backend as K
from tensorflow.python.keras.optimizers import SGD 
from tensorflow.python.keras.optimizer_v2.adam import Adam

sys.path.append(r'../')
from MTCNN_models import pnet_train1, rnet, onet

def main(args):
    
    IMG_SIZE = args.IMG_SIZE
    if args.USE_PRETRAINED_MODEL == 0:
        USE_PRETRAINED_MODEL = False
    else:
        USE_PRETRAINED_MODEL = True
    BATCH_SIZE = 1792
    EPOCHS = 100
    DATA_COMPOSE_RATIO = [1 / 7., 3 / 7., 1 / 7., 2 / 7.]
    SAMPLE_KEEP_RATIO = 0.7
    if args.OPTIMIZER == 'sgd':
        OPTIMIZER = SGD
    else:
        OPTIMIZER = Adam
    if IMG_SIZE == 12:
        print('Training PNet')
        loss_weights = [1., 0.5, 0.5]
        model = pnet_train1(train_with_landmark = not args.TRAIN_WITHOUT_LANDMARK)
        model.summary()
        if USE_PRETRAINED_MODEL:
            model.load_weights('../Models/PNet_train.h5')
    elif IMG_SIZE == 24:
        print('Training RNet')
        loss_weights = [1., 0.5, 0.5]
        model = rnet(training = True, train_with_landmark = not args.TRAIN_WITHOUT_LANDMARK)
        if USE_PRETRAINED_MODEL:
            model.load_weights('../Models/RNet_train.h5')
    elif IMG_SIZE == 48:
        print('Training ONet')
        loss_weights = [1., 0.5, 1.]
        model = onet(training = True, train_with_landmark = not args.TRAIN_WITHOUT_LANDMARK)
        if USE_PRETRAINED_MODEL:
            model.load_weights('../Models/ONet_train.h5')
    else:
        raise Exception("IMG_SIZE must be one of 12, 24 and 48. ")
    
    TFRECORDS_DIR = os.path.join(r'../Data', str(IMG_SIZE))
    POS_TFRECORDS_PATH_LIST = []
    NEG_TFRECORDS_PATH_LIST = []
    PART_TFRECORDS_PATH_LIST = []
    LANDMARK_TFRECORDS_PATH_LIST = []
    for file_name in os.listdir(TFRECORDS_DIR):
        if len(file_name) &gt; 9 and file_name[-9: ] == '.tfrecord':
            if 'pos' in file_name:
                POS_TFRECORDS_PATH_LIST.append(os.path.join(TFRECORDS_DIR, file_name))
            elif 'neg' in file_name:
                NEG_TFRECORDS_PATH_LIST.append(os.path.join(TFRECORDS_DIR, file_name))
            elif 'part' in file_name:
                PART_TFRECORDS_PATH_LIST.append(os.path.join(TFRECORDS_DIR, file_name))
            elif 'landmark' in file_name:
                LANDMARK_TFRECORDS_PATH_LIST.append(os.path.join(TFRECORDS_DIR, file_name))
    
    raw_pos_dataset = tf.data.TFRecordDataset(POS_TFRECORDS_PATH_LIST)
    raw_neg_dataset = tf.data.TFRecordDataset(NEG_TFRECORDS_PATH_LIST)
    raw_part_dataset = tf.data.TFRecordDataset(PART_TFRECORDS_PATH_LIST)
    raw_landmark_dataset = tf.data.TFRecordDataset(LANDMARK_TFRECORDS_PATH_LIST)
    
    image_feature_description = {
        'height': tf.io.FixedLenFeature([], tf.int64),
        'width': tf.io.FixedLenFeature([], tf.int64),
        'depth': tf.io.FixedLenFeature([], tf.int64),
        'info': tf.io.FixedLenFeature([17], tf.float32),
        'image_raw': tf.io.FixedLenFeature([], tf.string),
        }
    
    def _read_tfrecord(serialized_example):
        
        example = tf.io.parse_single_example(serialized_example, image_feature_description)
        
        img = tf.image.decode_jpeg(example['image_raw'], channels = 3) # RGB rather than BGR!!! 
        img = (tf.cast(img, tf.float32) - 127.5) / 128.
        img_shape = [example['height'], example['width'], example['depth']]
        img = tf.reshape(img, img_shape)
        
        info = example['info']
        
        return img, info
    
    parsed_pos_dataset = raw_pos_dataset.map(_read_tfrecord)
    parsed_neg_dataset = raw_neg_dataset.map(_read_tfrecord)
    parsed_part_dataset = raw_part_dataset.map(_read_tfrecord)
    parsed_landmark_dataset = raw_landmark_dataset.map(_read_tfrecord)
    
    ds = tf.data.experimental.sample_from_datasets([parsed_pos_dataset.repeat(), 
                                                    parsed_neg_dataset.repeat(), 
                                                    parsed_part_dataset.repeat(), 
                                                    parsed_landmark_dataset.repeat()], DATA_COMPOSE_RATIO)
    
    ds = ds.batch(BATCH_SIZE)
    ds = ds.prefetch(28672)
    
    '''Building custom loss/cost function'''
    def custom_loss(y_true, y_pred, loss_weights = loss_weights):
        
        zero_index = K.zeros_like(y_true[:, 0]) 
        ones_index = K.ones_like(y_true[:, 0]) 
        
        labels = y_true[:, 0] 
        class_preds = y_pred[:, 0] 
        bi_crossentropy_loss = -labels * K.log(class_preds) - (1 - labels) * K.log(1 - class_preds) 
        
        classify_valid_index = tf.where(K.less(y_true[:, 0], 0), zero_index, ones_index) 
        classify_keep_num = K.cast(tf.cast(tf.reduce_sum(classify_valid_index), tf.float32) * SAMPLE_KEEP_RATIO, dtype = tf.int32) 
        
        classify_loss_sum = bi_crossentropy_loss * tf.cast(classify_valid_index, bi_crossentropy_loss.dtype) 
        classify_loss_sum_filtered, _ = tf.nn.top_k(classify_loss_sum, k = classify_keep_num) 
        classify_loss = tf.where(K.equal(classify_keep_num, 0), tf.constant(0, dtype = tf.float32), K.mean(classify_loss_sum_filtered)) 
        
        rois = y_true[:, 1: 5] 
        roi_preds = y_pred[:, 1: 5] 
        roi_raw_mean_square_error = K.sum(K.square(rois - roi_preds), axis = 1)
        
        roi_valid_index = tf.where(K.equal(K.abs(y_true[:, 0]), 1), ones_index, zero_index) 
        roi_keep_num = K.cast(tf.reduce_sum(roi_valid_index), dtype = tf.int32) 
        
        roi_valid_mean_square_error = roi_raw_mean_square_error * tf.cast(roi_valid_index, roi_raw_mean_square_error.dtype)
        roi_filtered_mean_square_error, _ = tf.nn.top_k(roi_valid_mean_square_error, k = roi_keep_num) 
        roi_loss = tf.where(K.equal(roi_keep_num, 0), tf.constant(0, dtype = tf.float32), K.mean(roi_filtered_mean_square_error)) 
        
        pts = y_true[:, 5: 17] 
        pt_preds = y_pred[:, 5: 17] 
        pts_raw_mean_square_error  = K.sum(K.square(pts - pt_preds), axis = 1) # mse 
        
        pts_valid_index = tf.where(K.equal(y_true[:, 0], -2), ones_index, zero_index) 
        pts_keep_num = K.cast(tf.reduce_sum(pts_valid_index), dtype = tf.int32) 
        
        pts_valid_mean_square_error = pts_raw_mean_square_error * tf.cast(pts_valid_index, tf.float32) 
        pts_filtered_mean_square_error, _ = tf.nn.top_k(pts_valid_mean_square_error, k = pts_keep_num) 
        pts_loss = tf.where(K.equal(pts_keep_num, 0), tf.constant(0, dtype = tf.float32), K.mean(pts_filtered_mean_square_error)) 
        
        loss = classify_loss * loss_weights[0] + roi_loss * loss_weights[1] + pts_loss * loss_weights[2]
        
        return loss 
        
    '''Training'''
    lr = args.LEARNING_RATE
    model.compile(optimizer = OPTIMIZER(lr = lr), loss = custom_loss)
    model.fit(ds, steps_per_epoch = 1636, epochs = EPOCHS, validation_data = ds, validation_steps = 1636)
    model.save(r'../Models/PNet_trained_without_lm.h5')

def parse_arguments(argv):
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--IMG_SIZE', type = int, help = 'The input image size', default = 0)
    parser.add_argument('--USE_PRETRAINED_MODEL', type = int, help = 'Use pretrained model or not', default = 0)
    parser.add_argument('--TRAIN_WITHOUT_LANDMARK', action = 'store_false', help = 'Train model with landmark regression or not')
    parser.add_argument('--OPTIMIZER', type = str, help = 'The training optimizer', default = 'adam')
    parser.add_argument('--LEARNING_RATE', type = float, help = 'The initial learning rate', default = 0.001)
    
    return parser.parse_args(argv)

if __name__ == '__main__':
    
    main(parse_arguments(sys.argv[1:]))
&lt;/denchmark-code&gt;

If you need the complete steps, the steps are:

Use Data_Augmentation/00.1_WIDER_FACE_annotation_intergration.py or Data_Augmentation/00.1_wider_face_add_lm_annotation_integration.py to generate training data annotation for face detection; use Data_Augmentation/00.1_CelebA_data_marking_chin.py to generate training data annotation for landmark regression.
Use Data_Augmentation/00.2_CelebA_annotation_integration.py to integrate the extra chin point annotation for landmark regression.
Use Data_Augmentation/01_WIDER_FACE_Pnet_data_aug.py or Data_Augmentation/01_WIDER_FACE_add_lm_annotation_Pnet_data_aug.py to generate augmented images for face detection.
Use Data_Augmentation/01_CelebA_landmark_data_gen.py to generate augmented images for landmark regression.
Use Data_Augmentation/02_TFRecords_generation.py to generate TFRecords for training.
Use Training/Training_with_TFRecords_v2.py to train PNet and the error occurs during the training.

If you need extra information for this, please inform me and I will reply ASAP. Thanks.
		</comment>
		<comment id='3' author='TMaysGGS' date='2020-04-21T17:02:52Z'>
		&lt;denchmark-link:https://github.com/TMaysGGS&gt;@TMaysGGS&lt;/denchmark-link&gt;
,
Sorry for the delayed response. Is it possible for you to provide a minimal code sample to replicate the issue reported here? Thanks!
		</comment>
		<comment id='4' author='TMaysGGS' date='2020-04-22T05:41:35Z'>
		&lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;
 ,
Below is the minimal code to reproduce the issue:
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf
import tensorflow.python.keras.backend as K
from tensorflow.python.keras.optimizer_v2.adam import Adam
from tensorflow.keras.layers import Input, Conv2D, PReLU, MaxPooling2D, Reshape, Concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.initializers import glorot_normal
from tensorflow.keras.regularizers import l2

SAMPLE_KEEP_RATIO = 0.7
LOSS_WEIGHTS = [1., 0.5, 0.5]

# Create model
def pnet(train_with_landmark = False):
    
    X = Input(shape = (12, 12, 3), name = 'Pnet_input')
    
    M = Conv2D(10, 3, strides = 1, padding = 'valid', kernel_initializer = glorot_normal, kernel_regularizer = l2(0.00001), name = 'Pnet_conv1')(X)
    M = PReLU(shared_axes = [1, 2], name = 'Pnet_prelu1')(M)
    M = MaxPooling2D(pool_size = 2, name = 'Pnet_maxpool1')(M) 
    
    M = Conv2D(16, 3, strides = 1, padding = 'valid', kernel_initializer = glorot_normal, kernel_regularizer = l2(0.00001), name = 'Pnet_conv2')(M)
    M = PReLU(shared_axes= [1, 2], name = 'Pnet_prelu2')(M)
    
    M = Conv2D(32, 3, strides = 1, padding = 'valid', kernel_initializer = glorot_normal, kernel_regularizer = l2(0.00001), name = 'Pnet_conv3')(M)
    M = PReLU(shared_axes= [1, 2], name = 'Pnet_prelu3')(M)
    
    Classifier_conv = Conv2D(1, 1, activation = 'sigmoid', name = 'Pnet_classifier_conv', kernel_initializer = glorot_normal)(M)
    Bbox_regressor_conv = Conv2D(4, 1, name = 'Pnet_bbox_regressor_conv', kernel_initializer = glorot_normal)(M)
    Landmark_regressor_conv = Conv2D(12, 1, name = 'Pnet_landmark_regressor_conv', kernel_initializer = glorot_normal)(M)
    
    Classifier = Reshape((1, ), name = 'Pnet_classifier')(Classifier_conv)
    Bbox_regressor = Reshape((4, ), name = 'Pnet_bbox_regressor')(Bbox_regressor_conv) 
    if train_with_landmark: 
        Landmark_regressor = Reshape((12, ), name = 'Pnet_landmark_regressor')(Landmark_regressor_conv)
        Pnet_output = Concatenate()([Classifier, Bbox_regressor, Landmark_regressor]) 
        model = Model(X, Pnet_output) 
    else:
        Pnet_output = Concatenate()([Classifier, Bbox_regressor])
        model = Model(X, Pnet_output)
    
    return model

model = pnet(train_with_landmark = True)

# Create data
x1 = np.random.rand(1792, 12, 12, 3)
y1 = np.ones((1792, 17), dtype = np.float32) * (-1)
y1[: 256, 0] = 1
y1[256: 1024, 0] = 0
y1[1024: 1280, 0] = -1
y1[1280: , 0] = -2
for i in range(1792):
    if y1[i][0] == 1 or y1[i][0] == -1:
        y1[i][1: 5] = np.random.rand(4, )
    elif y1[i][0] == -2:
        y1[i][5: ] = np.random.rand(12, )

# Create loss
def custom_loss(y_true, y_pred, loss_weights = LOSS_WEIGHTS):
    
    zero_index = K.zeros_like(y_true[:, 0]) 
    ones_index = K.ones_like(y_true[:, 0]) 
    
    # Classifier
    labels = y_true[:, 0] 
    class_preds = y_pred[:, 0] 
    bi_crossentropy_loss = -labels * K.log(class_preds) - (1 - labels) * K.log(1 - class_preds) 
    
    classify_valid_index = tf.where(K.less(y_true[:, 0], 0), zero_index, ones_index) 
    classify_keep_num = K.cast(tf.cast(tf.reduce_sum(classify_valid_index), tf.float32) * SAMPLE_KEEP_RATIO, dtype = tf.int32) 
    # For classification problem, only pick 70% of the valid samples. 
    
    classify_loss_sum = bi_crossentropy_loss * tf.cast(classify_valid_index, bi_crossentropy_loss.dtype) 
    classify_loss_sum_filtered, _ = tf.nn.top_k(classify_loss_sum, k = classify_keep_num) 
    classify_loss = tf.where(K.equal(classify_keep_num, 0), tf.constant(0, dtype = tf.float32), K.mean(classify_loss_sum_filtered)) 

    loss = classify_loss * loss_weights[0] # + roi_loss * loss_weights[1] + pts_loss * loss_weights[2]
    
    return loss 
    
# Train
model.compile(optimizer = Adam(lr = 0.001), loss = custom_loss)
model.fit(x1, y1, batch_size = 896, epochs = 200, shuffle = True)
model.save(r'../Models/PNet_train.h5')
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='TMaysGGS' date='2020-04-24T08:03:49Z'>
		&lt;denchmark-link:https://github.com/TMaysGGS&gt;@TMaysGGS&lt;/denchmark-link&gt;
,
I did not observe any nan values while running the code with &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/9cf66344fe1c93248508ec3f011bf455/38416-2-1.ipynb&gt;TF v2.1&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/2254b7122f8a08ca71aa6f898c0b8eca/38416-2-2.ipynb&gt;TF v2.2.0-rc3&lt;/denchmark-link&gt;
.
However, the  line throws an error stating . Please find the attached gist. Thanks!
		</comment>
		<comment id='6' author='TMaysGGS' date='2020-04-24T08:18:57Z'>
		
@TMaysGGS,
I did not observe any nan values while running the code with TF v2.1 and TF v2.2.0-rc3.
However, the model.save line throws an error stating TypeError: get_config() missing 1 required positional argument: 'self'. Please find the attached gist. Thanks!

&lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;
 ,
Thanks for your reply. I re-ran my codes and found the 'nan' loss occurred on epoch 345. Please change the line  to  and the 'nan' loss should occur when the loss is reduced to around 0.0178.
And I also slightly modified a little on the initialization of the model so the 'model.save' error will not occur. Please use the following code:
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf
import tensorflow.python.keras.backend as K
from tensorflow.python.keras.optimizer_v2.adam import Adam
from tensorflow.keras.layers import Input, Conv2D, PReLU, MaxPooling2D, Reshape, Concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.initializers import glorot_normal
from tensorflow.keras.regularizers import l2

SAMPLE_KEEP_RATIO = 0.7
LOSS_WEIGHTS = [1., 0.5, 0.5]

# Create model
def pnet(train_with_landmark = False):
    
    X = Input(shape = (12, 12, 3), name = 'Pnet_input')
    
    M = Conv2D(10, 3, strides = 1, padding = 'valid', kernel_initializer = glorot_normal(), kernel_regularizer = l2(0.00001), name = 'Pnet_conv1')(X)
    M = PReLU(shared_axes = [1, 2], name = 'Pnet_prelu1')(M)
    M = MaxPooling2D(pool_size = 2, name = 'Pnet_maxpool1')(M) 
    
    M = Conv2D(16, 3, strides = 1, padding = 'valid', kernel_initializer = glorot_normal(), kernel_regularizer = l2(0.00001), name = 'Pnet_conv2')(M)
    M = PReLU(shared_axes= [1, 2], name = 'Pnet_prelu2')(M)
    
    M = Conv2D(32, 3, strides = 1, padding = 'valid', kernel_initializer = glorot_normal(), kernel_regularizer = l2(0.00001), name = 'Pnet_conv3')(M)
    M = PReLU(shared_axes= [1, 2], name = 'Pnet_prelu3')(M)
    
    Classifier_conv = Conv2D(1, 1, activation = 'sigmoid', name = 'Pnet_classifier_conv', kernel_initializer = glorot_normal())(M)
    Bbox_regressor_conv = Conv2D(4, 1, name = 'Pnet_bbox_regressor_conv', kernel_initializer = glorot_normal())(M)
    Landmark_regressor_conv = Conv2D(12, 1, name = 'Pnet_landmark_regressor_conv', kernel_initializer = glorot_normal())(M)
    
    Classifier = Reshape((1, ), name = 'Pnet_classifier')(Classifier_conv)
    Bbox_regressor = Reshape((4, ), name = 'Pnet_bbox_regressor')(Bbox_regressor_conv) 
    if train_with_landmark: 
        Landmark_regressor = Reshape((12, ), name = 'Pnet_landmark_regressor')(Landmark_regressor_conv)
        Pnet_output = Concatenate()([Classifier, Bbox_regressor, Landmark_regressor]) 
        model = Model(X, Pnet_output) 
    else:
        Pnet_output = Concatenate()([Classifier, Bbox_regressor])
        model = Model(X, Pnet_output)
    
    return model

model = pnet(train_with_landmark = True)

# Create data
x1 = np.random.rand(1792, 12, 12, 3)
y1 = np.ones((1792, 17), dtype = np.float32) * (-1)
y1[: 256, 0] = 1
y1[256: 1024, 0] = 0
y1[1024: 1280, 0] = -1
y1[1280: , 0] = -2
for i in range(1792):
    if y1[i][0] == 1 or y1[i][0] == -1:
        y1[i][1: 5] = np.random.rand(4, )
    elif y1[i][0] == -2:
        y1[i][5: ] = np.random.rand(12, )

# Create loss
def custom_loss(y_true, y_pred, loss_weights = LOSS_WEIGHTS):
    
    zero_index = K.zeros_like(y_true[:, 0]) 
    ones_index = K.ones_like(y_true[:, 0]) 
    
    # Classifier
    labels = y_true[:, 0] 
    class_preds = y_pred[:, 0] 
    bi_crossentropy_loss = -labels * K.log(class_preds) - (1 - labels) * K.log(1 - class_preds) 
    
    classify_valid_index = tf.where(K.less(y_true[:, 0], 0), zero_index, ones_index) 
    classify_keep_num = K.cast(tf.cast(tf.reduce_sum(classify_valid_index), tf.float32) * SAMPLE_KEEP_RATIO, dtype = tf.int32) 
    # For classification problem, only pick 70% of the valid samples. 
    
    classify_loss_sum = bi_crossentropy_loss * tf.cast(classify_valid_index, bi_crossentropy_loss.dtype) 
    classify_loss_sum_filtered, _ = tf.nn.top_k(classify_loss_sum, k = classify_keep_num) 
    classify_loss = tf.where(K.equal(classify_keep_num, 0), tf.constant(0, dtype = tf.float32), K.mean(classify_loss_sum_filtered)) 

    loss = classify_loss * loss_weights[0] # + roi_loss * loss_weights[1] + pts_loss * loss_weights[2]
    
    return loss 
    
# Train
model.compile(optimizer = Adam(lr = 0.001), loss = custom_loss)
model.fit(x1, y1, batch_size = 896, epochs = 400, shuffle = True)
model.save(r'PNet_train.h5')
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='TMaysGGS' date='2020-04-24T10:36:21Z'>
		&lt;denchmark-link:https://github.com/TMaysGGS&gt;@TMaysGGS&lt;/denchmark-link&gt;
,
I was able to reproduce the issue with &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/80bcebe4d22acdf01bb8c5ae07da630d/38416.ipynb&gt;TF v2.2.0-rc3&lt;/denchmark-link&gt;
.
The issue seems to be fixed with the latest &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/df7727f839692cf57c2d3a153d10abdd/38416-tf-nightly.ipynb&gt;TF-nightly&lt;/denchmark-link&gt;
, as I was able to run the code without any issues. Please find the attached gist. Thanks!
		</comment>
		<comment id='8' author='TMaysGGS' date='2020-04-24T18:11:16Z'>
		
@TMaysGGS,
I was able to reproduce the issue with TF v2.2.0-rc3.
The issue seems to be fixed with the latest TF-nightly, as I was able to run the code without any issues. Please find the attached gist. Thanks!

&lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;
 ,
I just tried the latest TF-nightly installed by using the command 'pip install tf-nightly'. This issue still happens and this time it happens randomly in some epoch. I tried training several times and please increase the total epochs to 1200 so that this problem would always happen.
		</comment>
		<comment id='9' author='TMaysGGS' date='2020-04-28T07:45:42Z'>
		Was able to reproduce the issue with latest TF-nightly too i.e. TF v2.2.0-dev20200427. Please find the gist &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/b2b3b5450d0005a16662cf5afa609e6e/38416-tf-nightly.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='10' author='TMaysGGS' date='2020-04-30T17:41:34Z'>
		This is likely a numerical stability issue in the way the model is set up. At this stage, this is better suited for Stack Overflow, as there is a larger community that can help debug there.
You can try using run_eagerly=True in your model to walk through step by step and layer by layer to see which layer is causing the issue.
If you can create a smaller example that narrows down the issue to a bug, please reopen an issue here.
		</comment>
		<comment id='11' author='TMaysGGS' date='2020-04-30T17:41:36Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38416&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38416&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>