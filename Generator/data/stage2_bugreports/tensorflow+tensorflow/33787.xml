<bug id='33787' author='hgaiser' open_date='2019-10-28T15:49:17Z' closed_time='2020-02-27T23:48:23Z'>
	<summary>Using tf.keras.backend.zeros in while loops.</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): 2.0.0
Python version: 3.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: N/A
GPU model and memory: N/A

Describe the current behavior
The effect of tf.zeros and tf.keras.backend.zeros is not the same and results in some inconsistent behaviour. This also holds for other functions such as tf.keras.backend.ones and others like it.
Specifically, using tf.keras.backend.zeros in a tf.map_fn function (or something similar) breaks because tf.keras.backend.zeros has a tf.init_scope which causes it to be created out of the context of the while loop.
Describe the expected behavior
Expected behaviour would be that tf.zeros and tf.keras.backend.zeros are identical and that they follow the usage of tf.zeros; meaning not changing the scope in which they are created.
Code to reproduce the issue
import tensorflow as tf

tf.compat.v1.disable_v2_behavior()

# Works because we don't change the scope.
def works(inputs):
	return tf.zeros((tf.keras.backend.shape(inputs[0])[0],))

# Works because both tf.zeros and its inputs are in the same scope.
## This function helps explain the root of the issue.
def works2(inputs):
	with tf.init_scope():
		return tf.zeros((tf.keras.backend.shape(inputs[0])[0],))

# Breaks because tf.keras.backend.zeros is being created in a different scope from tf.keras.backend.shape.
def breaks(inputs):
	return tf.keras.backend.zeros((tf.keras.backend.shape(inputs[0])[0],))

# Breaks because the shape is created outside of the context of the tf.zeros.
## This function helps explain the root of the issue.
## This is an extract of how tf.keras.backend.zeros is implemented.
def breaks2(inputs):
	shape = (tf.keras.backend.shape(inputs[0])[0],)
	with tf.init_scope():
		return tf.zeros(shape)

inputs = [tf.keras.layers.Input(shape=(5, 5))]

# This works when using tf.zeros because it doesn't change the scope.
tf.map_fn(works, elems=inputs, dtype=tf.keras.backend.floatx())

# This works when tf.zeros and its inputs are in the same scope (using tf.init_scope).
tf.map_fn(works2, elems=inputs, dtype=tf.keras.backend.floatx())

# This breaks when using tf.keras.backend.zeros because it creates the zeros in a new scope.
try:
	tf.map_fn(breaks, elems=inputs, dtype=tf.keras.backend.floatx())
except ValueError as e:
	print("Caught error: {}".format(e))

# This breaks when using tf.zeros when its inputs are defined in a different scope.
try:
	tf.map_fn(breaks2, elems=inputs, dtype=tf.keras.backend.floatx())
except ValueError as e:
	print("Caught error: {}".format(e))

This change was introduced in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/1d91f3532ee4df36749dda1a39b8a2a78232dd74&gt;1d91f35&lt;/denchmark-link&gt;
 by &lt;denchmark-link:https://github.com/rjpower&gt;@rjpower&lt;/denchmark-link&gt;
 . It would be great if I could get some feedback on why this  got added there and if it should be changed.
	</description>
	<comments>
		<comment id='1' author='hgaiser' date='2019-10-30T21:49:54Z'>
		Indeed, this was my change, but I've long forgotten the reasoning. I looked up a bit more context. Summarizing the conversation at the time:
Variables initialized inside of a loop (as from a training loop) need to be in the init_scope to avoid an error.  For layers, this is handled by the make_variable helper, but optimizers can end up creating temporary variables and triggering this issue.
I'm not sure what the contract is for tf.keras.backend (is it intended as public or not). But I support fixing this. We should remmove the scoping from backend.{zeros, ones} and explicitly add the init_scope when initializing optimizer variables. I'll defer to the keras-team for how to properly stage the fix.
		</comment>
		<comment id='2' author='hgaiser' date='2019-10-31T08:06:32Z'>
		Thank you for the feedback &lt;denchmark-link:https://github.com/rjpower&gt;@rjpower&lt;/denchmark-link&gt;
 . I'm fairly certain  is intended to be public, as it provides an easy migration from  to .

I'll defer to the keras-team for how to properly stage the fix.

Is there a way I can follow this progress? Or will this issue be updated when the keras-team handles it?
		</comment>
		<comment id='3' author='hgaiser' date='2019-11-12T23:15:17Z'>
		Sorry for the delay -- yes -- I'll ask someone on the Keras/DS team to look into this and update the status on this issue.
		</comment>
		<comment id='4' author='hgaiser' date='2019-12-12T09:32:32Z'>
		Hey &lt;denchmark-link:https://github.com/rjpower&gt;@rjpower&lt;/denchmark-link&gt;
 , it has been a while, is there an update on this issue?
		</comment>
		<comment id='5' author='hgaiser' date='2020-02-06T09:13:55Z'>
		&lt;denchmark-link:https://github.com/rjpower&gt;@rjpower&lt;/denchmark-link&gt;
 any update on this?
		</comment>
		<comment id='6' author='hgaiser' date='2020-02-10T20:22:52Z'>
		&lt;denchmark-link:https://github.com/hgaiser&gt;@hgaiser&lt;/denchmark-link&gt;
 apologies for the delay.
So: tf.keras.backend should be considered private to Keras. Unfortunately it looks like fixing the behavior will be somewhat challenging as it affects variable initialization. There's some long term work planned to try to make this more consistent, but I'm afraid it won't land soon.
		</comment>
		<comment id='7' author='hgaiser' date='2020-02-27T21:20:45Z'>
		Note that tf.zeros and tf.keras.backend.zeros are different.
tf.zeros is expected to return a constant tensor, whereas tf.keras.backend.zeros is expected to return a variable that filled with zeros. The keras one is more like a shortcut to create variable with a zero initializer.
Closing this bug since the expectation of those two API are different.
		</comment>
		<comment id='8' author='hgaiser' date='2020-02-27T21:20:51Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33787&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33787&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='hgaiser' date='2020-02-27T21:49:12Z'>
		&lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
 I understand that  creates a constant tensor and  creates a variable, but shouldn't they both be useable in a while loop?
Currently, tf.keras.backend.zeros can't be used in while loops, while if you created a tf.variable containing zeros you can use it in while loops. Surely that's a bug right?
		</comment>
		<comment id='10' author='hgaiser' date='2020-02-27T22:18:16Z'>
		Thanks for bring this up. I agree that forcing init_scope under the hood is not ideal, and its not clearly documented in the API docstring.
I think the issue that &lt;denchmark-link:https://github.com/rjpower&gt;@rjpower&lt;/denchmark-link&gt;
 trying to walkaround is for the optimizer, which use backend.zeros to create varibles. I think we should remove the init_scope in backend.zeros, and add init_scope explicitly to the optimizer.
		</comment>
		<comment id='11' author='hgaiser' date='2020-02-27T23:48:20Z'>
		Checked with &lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 offline for this issue. He think it is fine to create variables with zeros/ones under init scope, and we will keep this behavior. If you need non-variable zeros under while loop, you can use tf.zeros, otherwise, you can use tf.Variable(), but the initiali value of it need to be under init_scope as well.
		</comment>
		<comment id='12' author='hgaiser' date='2020-02-27T23:48:25Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33787&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33787&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='hgaiser' date='2020-02-28T09:06:53Z'>
		I respectfully disagree with you &lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
 . What is the motivation for &lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 to say it is fine? I can't think of any reason for a function as basic as a  or  to not work in a different scope (like a loop). Specially since it used to work before &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/1d91f3532ee4df36749dda1a39b8a2a78232dd74&gt;1d91f35&lt;/denchmark-link&gt;
 .
I don't know what the motivation is for zeros and ones to be in init_scope for the optimizer, but it makes more sense to perform the scope change there instead of forcing it everywhere when it isn't necessary. This simply breaks expected (and previously working) functionality. What is the benefit, outside of the optimizer, to use tf.init_scope?
You suggest to use tf.zeros instead, which is a possible (yet in my opinion illogical) solution, but this same issue exists in Keras. In the case of Keras it makes no sense at all to use tf.zeros since the whole idea is to be backend independent. I was expecting this to be fixed in tf.keras so that it can possibly be fixed in Keras as well.
		</comment>
	</comments>
</bug>