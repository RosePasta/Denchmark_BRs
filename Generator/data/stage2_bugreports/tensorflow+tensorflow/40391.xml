<bug id='40391' author='krzysztofrusek' open_date='2020-06-11T18:54:40Z' closed_time='2020-06-12T13:12:10Z'>
	<summary>Custom train loop inconsistent with Keras `fit` for vector variables</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab, MacOS 10.15.5
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): Binary pip
TensorFlow version (use command below):2.2.0
Python version:3.7.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:None
GPU model and memory:None

Describe the current behavior
Training a linear model using a custom train loop and tf.GradientTape is inconsistent with tf.keras.Model.fit and the variables are updated in the wrong way. Scalar variables are updated correctly, vector variables get the wrong gradient.
Describe the expected behavior
The final results should be close in both cases.
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
import tensorflow as tf
import numpy as np

nn=40
_x= np.random.normal(size=(nn,3)).astype(np.float32)
_y= .4*_x[:,0] + 0.2*_x[:,1] -0.3*_x[:,2]+1.5 + np.random.normal(size=(nn), scale=.1).astype(np.float32)

_x = tf.convert_to_tensor(_x)
_y = tf.convert_to_tensor(_y)

modelx=tf.keras.Sequential([
    tf.keras.layers.Dense(1)
])

modelx(_x)

opt = tf.keras.optimizers.Adam(learning_rate=0.1)


for i in range(400):
    with tf.GradientTape() as tape:

        yhat = modelx(_x)
        loss = tf.reduce_sum(tf.math.squared_difference(yhat,_y ))
    grads = tape.gradient(loss, modelx.trainable_weights)

    opt.apply_gradients(zip(grads, modelx.trainable_weights))


modelx.variables
Colab to reproduce the bug:
&lt;denchmark-link:https://colab.research.google.com/drive/12YlqtjsekXqhEZiTnlTnjxz2s0RJCV0r?usp=sharing&gt;https://colab.research.google.com/drive/12YlqtjsekXqhEZiTnlTnjxz2s0RJCV0r?usp=sharing&lt;/denchmark-link&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='krzysztofrusek' date='2020-06-12T04:21:21Z'>
		I have tried in colab with TF version 2.2, nightly version() and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/241892acf963a7ca7d2a195eb92662ff/untitled10.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='2' author='krzysztofrusek' date='2020-06-12T11:12:54Z'>
		That is just a mistake in the your custom training loop. The problem is that yhat has shape [batch_size, 1], while _y has shape [batch_size]. So subtracting them gives a result of [batch_size, batch_size], which is definitely not what you wanted.
If you for example used
loss = tf.reduce_sum(tf.math.squared_difference(yhat[:, 0],_y ))
which converts yhat first to vector of shape [batch_size], it would work fine.
Also, the object losses perform this conversion automatically, so
loss = tf.keras.losses.MeanSquaredError()(yhat,_y )
also works (and this is what the .fit method does, I believe).
		</comment>
		<comment id='3' author='krzysztofrusek' date='2020-06-12T13:12:03Z'>
		Yes, indeed, now it works as expected.
Thank you!
		</comment>
		<comment id='4' author='krzysztofrusek' date='2020-06-12T13:12:11Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40391&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40391&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>