<bug id='38056' author='lutzgruber' open_date='2020-03-30T19:15:33Z' closed_time='2020-08-23T06:21:55Z'>
	<summary>Error with tensorflow 2: Inputs to eager execution function cannot be Keras symbolic tensors</summary>
	<description>
Hi everyone,
The tensorflow 2 release notes request that an issue be filed when experiencing problems with the new single path execution code.
I regularly work with custom loss functions that require additional information other than the predictors and observed outcomes. A simple example is estimating a general binomial regression model, where the number of trials are part of the likelihood/loss function, but they are not part of the predictors or observed outcome. Sample R code is provided in &lt;denchmark-link:https://github.com/rstudio/keras/issues/1008&gt;rstudio/keras#1008&lt;/denchmark-link&gt;

Would you please make available a permanent option in tensorflow 2 to pass additional inputs layers into custom loss functions so that we can keep using tensorflow to estimate such models? I guess that setting experimental_run_tf_function=False is only a temporary fix.
Thank you
	</description>
	<comments>
		<comment id='1' author='lutzgruber' date='2020-03-31T08:52:36Z'>
		&lt;denchmark-link:https://github.com/lutzgruber&gt;@lutzgruber&lt;/denchmark-link&gt;

Request you to share the colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!
		</comment>
		<comment id='2' author='lutzgruber' date='2020-04-02T19:15:42Z'>
		Below is Python code to reproduce the issue:

With tensorflow 1.15.0, the code works as posted.
With tensorflow 2.1.0, the call to model.fit fails unless the option experimental_run_tf_function=False is set in model.compile.

The error is
&lt;denchmark-code&gt;Train on 16 samples, validate on 4 samples
Epoch 1/1000
16/16 [==============================] - 0s 14ms/sample
Traceback (most recent call last):
  File "/home/xxx/anaconda3/envs/tf2python/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py", line 61, in quick_execute
    num_outputs)
TypeError: An op outside of the function building code is being passed
a "Graph" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: logNchooseK:0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "&lt;stdin&gt;", line 11, in &lt;module&gt;
  File "/home/xxx/anaconda3/envs/tf2python/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py", line 819, in fit
    use_multiprocessing=use_multiprocessing)
  File "/home/xxx/anaconda3/envs/tf2python/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 342, in fit
    total_epochs=epochs)
  File "/home/xxx/anaconda3/envs/tf2python/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 128, in run_one_epoch
    batch_outs = execution_function(iterator)
  File "/home/xxx/anaconda3/envs/tf2python/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", line 98, in execution_function
    distributed_function(input_fn))
  File "/home/xxx/anaconda3/envs/tf2python/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 568, in __call__
    result = self._call(*args, **kwds)
  File "/home/xxx/anaconda3/envs/tf2python/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 632, in _call
    return self._stateless_fn(*args, **kwds)
  File "/home/xxx/anaconda3/envs/tf2python/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 2363, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/home/xxx/anaconda3/envs/tf2python/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1611, in _filtered_call
    self.captured_inputs)
  File "/home/xxx/anaconda3/envs/tf2python/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1692, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/xxx/anaconda3/envs/tf2python/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 545, in call
    ctx=ctx)
  File "/home/xxx/anaconda3/envs/tf2python/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py", line 75, in quick_execute
    "tensors, but found {}".format(keras_symbolic_tensors))
tensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [&lt;tf.Tensor 'logNchooseK:0' shape=(None, 1) dtype=float32&gt;, &lt;tf.Tensor 'N:0' shape=(None, 1) dtype=float32&gt;]
&lt;/denchmark-code&gt;

This is a demo case to estimate a model with a binomial likelihood/loss function, which requires that information on the number of trials is passed through to the custom loss function; this information is neither part of the predictors nor of the observed outcome. I do this by defining this outside information as input layers, and referencing them in my loss function.
While setting experimental_run_tf_function=False serves as an intermediate fix by reverting to the old execution path, my reading of the release notes is that this option was only intended as temporary.
Is there a new way of supplying outside information to custom loss functions in tensorflow 2? If not, I would ask that such an option be made available again.
&lt;denchmark-code&gt;import tensorflow.keras as keras
import numpy as np

predictors_layer = keras.layers.Input(
    shape = (2, ),
    name = "predictors"
)

N_layer = keras.layers.Input(
  shape = (1, ),
  name = "N"
)

logNchooseK_layer = keras.layers.Input(
  shape = (1, ),
  name = "logNchooseK"
)

output_layer = keras.layers.Dense(
  units = 1,
  activation = "linear",
  name = "output"
)(predictors_layer)

model = keras.models.Model(
    inputs = (predictors_layer, N_layer, logNchooseK_layer),
    outputs = output_layer
)

def binomial_loss(y_true, y_pred):
    predicted_prob = keras.backend.exp(y_pred) / (1 + keras.backend.exp(y_pred))

    neg_loglik = -(logNchooseK_layer + y_true * keras.backend.log(predicted_prob) + (N_layer - y_true) * keras.backend.log(1 - predicted_prob))

    return neg_loglik

model.compile(
    loss = binomial_loss,
    optimizer = "adam"#,
    #experimental_run_tf_function = False
)


# train model ----------------------------------------------------------------------------------------------

# the columns are: predictor1, predictor2, N: number of trials, true success probability as function of predictors 1+2, k: realized number of successes, log(N choose k)
sample_data = np.matrix(
    [
        [0.586	,0.78	,88	,0.706	,65	,48.214931],
        [0.709	,1.46	,52	,0.809	,41	,24.824317],
        [-0.109	,-0.644	,1	,0.369	,1	,0],
        [-0.453	,-1.55	,2	,0.199	,0	,0],
        [0.606	,-1.6	,15	,0.29	,4	,7.218910],
        [-1.82	,1.81	,31	,0.609	,15	,19.521092],
        [0.63	,-0.482	,83	,0.488	,44	,54.944102],
        [-0.276	,0.62	,51	,0.581	,29	,32.681372],
        [-0.284	,0.612	,81	,0.579	,45	,53.223945],
        [-0.919	,-0.162	,7	,0.359	,2	,3.044522],
        [-0.116	,0.812	,93	,0.634	,61	,57.420880],
        [1.82	,2.2	,81	,0.928	,75	,19.597920],
        [0.371	,2.05	,8	,0.848	,7	,2.079442],
        [0.52	,1.63	,61	,0.815	,54	,19.893774],
        [-0.751	,0.254	,72	,0.454	,34	,47.429363],
        [0.817	,0.491	,52	,0.685	,33	,31.966485],
        [-0.886	,-0.324	,73	,0.335	,17	,37.410978],
        [-0.332	,-1.66	,75	,0.196	,16	,36.684713],
        [1.12	,1.77	,10	,0.868	,8	,3.806662],
        [0.299	,0.0258	,40	,0.542	,20	,25.649407]
    ]
)

model.fit(
  x = {
    'predictors': sample_data[:,(0,1)],
    'N': sample_data[:,2],
    'logNchooseK': sample_data[:,5]
  },
  y = {
    'output': sample_data[:,4]
  },
  validation_split = 0.2,
  epochs = 1000
)

# evaluate model

predicted_logodds = model.predict(
    x = {
        'predictors': sample_data[:,(0,1)],
        'N': sample_data[:,2],
        'logNchooseK': sample_data[:,5]
    }
)

predicted_probs = np.exp(predicted_logodds) / (1 + np.exp(predicted_logodds))

print(np.transpose(np.reshape([sample_data[:, 3], predicted_probs], [2, 20])))
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='lutzgruber' date='2020-04-03T10:47:29Z'>
		I have tried on colab with TF version 2.0 , 2.1,'2.2.0-rc2' and was able to reproduce the issue.PLease, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/d2390bb0ba9663172de91109057bcfec/untitled763.ipynb#scrollTo=1PWWBxaUnbtf&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='4' author='lutzgruber' date='2020-05-14T18:49:27Z'>
		Thanks &lt;denchmark-link:https://github.com/lutzgruber&gt;@lutzgruber&lt;/denchmark-link&gt;
 . It sounds like you might be looking for the add_loss API, which allows you to add losses to the model:
&lt;denchmark-link:https://keras.io/api/losses#the-addloss-api&gt;https://keras.io/api/losses#the-addloss-api&lt;/denchmark-link&gt;

&lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#add_loss&gt;https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#add_loss&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='lutzgruber' date='2020-08-23T06:21:55Z'>
		&lt;denchmark-link:https://github.com/lutzgruber&gt;@lutzgruber&lt;/denchmark-link&gt;
 Closing due to lack of recent activity. Please feel free to  reopen the issue if you have any concern. Thanks!
		</comment>
		<comment id='6' author='lutzgruber' date='2020-08-23T06:21:57Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38056&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38056&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>