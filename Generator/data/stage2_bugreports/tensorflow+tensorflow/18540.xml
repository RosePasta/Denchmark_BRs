<bug id='18540' author='Tal-Golan' open_date='2018-04-16T00:06:46Z' closed_time='2018-05-14T17:41:23Z'>
	<summary>Tensorflow leaks 1280 bytes with each session opened and closed? (Python API)</summary>
	<description>

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 17.10
TensorFlow installed from (source or binary):
Binary
TensorFlow version (use command below):
1.6 (also tested on 1.7)
Python version:
3.6
Bazel version (if compiling from source):
N/A
GCC/Compiler version (if compiling from source):
N/A
CUDA/cuDNN version:
9.0/7.0
GPU model and memory:
Titan XP, 12GB
Exact command to reproduce:
To reproduce, save the following python script as memory_test.py:

&lt;denchmark-code&gt;    import tensorflow as tf
    import sys
    n_Iterations=int(sys.argv[1])
    def open_and_close_session():
       with tf.Session() as sess:
          pass
    for _ in range(n_Iterations):
       open_and_close_session()
    with tf.Session() as sess:
       print("bytes used=",sess.run(tf.contrib.memory_stats.BytesInUse()))
&lt;/denchmark-code&gt;

Then run it from the command line using different number of iterations:
python memory_test.py 0, python memory_test.py 1, python memory_test.py 10, and so on.
&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

It seems that each Tensorflow session I open and close consumes 1280 bytes from the GPU memory, which are not released until the python kernel is terminated.
Running the script given above, which simply opens and closes sessions without any further operation, yields these results:

python memory_test.py 0 yields bytes used= 1280
python memory_test.py 1 yields bytes used= 2560.
python memory_test.py 10 yields bytes used= 14080.
python memory_test.py 100 yields bytes used= 129280.
python memory_test.py 1000 yields bytes used= 1281280.

The math is easy - each session opened and closed leaks 1280 bytes. I tested this script on two different ubuntu 17.10 workstations with tensorflow-gpu 1.6 and 1.7 and different NVIDIA GPUs.
(here's a related &lt;denchmark-link:https://stackoverflow.com/q/49735217/1500585&gt;stackoverflow question&lt;/denchmark-link&gt;
, at least one user was able to reproduce).
	</description>
	<comments>
		<comment id='1' author='Tal-Golan' date='2018-04-16T12:30:24Z'>
		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
Bazel version
Exact command to reproduce
		</comment>
		<comment id='2' author='Tal-Golan' date='2018-04-17T00:38:09Z'>
		Thanks for the bug report.  I was able to reproduce and diagnose.  The missing memory belongs to scratch space allocated here:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L292&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L292&lt;/denchmark-link&gt;

I'll start working through a change to deallocate this memory in ~BaseGPUDevice, but I'm not yet 100% sure it's safe to do so.
		</comment>
		<comment id='3' author='Tal-Golan' date='2018-05-01T18:39:06Z'>
		It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='4' author='Tal-Golan' date='2018-05-03T16:01:52Z'>
		The bug is still there, trying the most recent nightly build.
		</comment>
		<comment id='5' author='Tal-Golan' date='2018-05-03T16:36:39Z'>
		A fix went into the internal version on April 26.
I see it in the github repository source here:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L269&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L269&lt;/denchmark-link&gt;

So, I think it should be fixed, at least for a build from source.
		</comment>
	</comments>
</bug>