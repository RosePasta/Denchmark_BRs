<bug id='36718' author='MichaelKarpe' open_date='2020-02-13T03:09:29Z' closed_time='2020-08-20T20:27:39Z'>
	<summary>[TPU Colab] [TF2.1] Many issues only on TPU due to data types, batch sizes and memory (gist provided with explanations)</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see gists with code on Colab not working on TPU and working on GPU.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
TensorFlow installed from (source or binary): %tensorflow_version 2.x command
TensorFlow version (use command below): 2.1.0

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

All the issues are shown and described in the &lt;denchmark-link:https://colab.research.google.com/drive/1yk4Emyzxju85gqflEii9FVuMD6VF5vta&gt;gist provided&lt;/denchmark-link&gt;
. Here is the list of the issues:

Issue 1: Models fitted on TPU does not deal number of samples not divisible by batch size.
Issue 1bis: With argument validation_split on TPU, issue 1 may occur with number of samples not divisible by batch size.
Issue 2: List of supported dtypes on TPU is restrictive.
Issue 2bis: Predict on TPU has the same dtype constraints than fit on TPU.
Issue 3: Number of samples may cause error for prediction on TPU
Issue 4: Batch size may cause error for prediction on TPU
Issue 4bis: Warnings when predicting on TPU
Issue 5: Trying to fit or predict on TPU a model on a supported dtype after having tried on a not supported dtype may raise an error.
Issue 6: UnavailableError: Socket closed on TPU and related errors

&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

&lt;denchmark-link:https://colab.research.google.com/drive/1yk4Emyzxju85gqflEii9FVuMD6VF5vta&gt;See Gist provided.&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='MichaelKarpe' date='2020-02-13T21:32:48Z'>
		Thanks for reporting the issues.
The &lt;denchmark-link:https://cloud.google.com/tpu/docs/troubleshooting#batch-too-small&gt;batch size&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://cloud.google.com/tpu/docs/troubleshooting#unsupported_data_type&gt;dtype&lt;/denchmark-link&gt;
 limitations are known.
Practically speaking, the TPU systems have significant amounts of host memory, so converting INT8 to INT32 data types manually should not make it OOM unless the batch and data sizes are truly enormous.
Assigning the rest of to the current TPU oncall &lt;denchmark-link:https://github.com/hongjunChoi&gt;@hongjunChoi&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='MichaelKarpe' date='2020-02-14T22:51:59Z'>
		Hello Michael,
Thank you for reporting. As for errors regarding sample size not being divisible by batch size, could you try converting numpy arrays to tf.data.Dataset?
Currently, partial batch size for final batch is supported for dataset inputs.
		</comment>
		<comment id='3' author='MichaelKarpe' date='2020-03-09T06:51:50Z'>
		Hi &lt;denchmark-link:https://github.com/hongjunChoi&gt;@hongjunChoi&lt;/denchmark-link&gt;
,
Sorry for the late reply, I have very little time these days to work on this. I tried with tf.data.Dataset, it looks like it's approximately working...
For example I can find a workaround for the validation_split parameter by using validation_data instead, however it seems the model.predict returns different predictions depending on the batch size (using .batch() on tf.data.Dataset).
I hadn't time to investigate but I'm quite surprised predictions may be the same for different samples of random generated data (however it also may be explained by the fact that the model is not really able to train...)
Please find a new gist &lt;denchmark-link:https://colab.research.google.com/drive/1IM3t4BF3iypclJ2fO9VCGg-6TdgORd-R&gt;here&lt;/denchmark-link&gt;
 where I tried , I hope it'll help you to investigate.
		</comment>
		<comment id='4' author='MichaelKarpe' date='2020-03-23T20:33:33Z'>
		Hello Michael,
Apologies for the late update. We are currently looking into this issue (more specifically on different predict result values) and will update you once the bug is fixed.
Thank you!
		</comment>
		<comment id='5' author='MichaelKarpe' date='2020-08-06T19:38:04Z'>
		Updating this thread as the bug of different predict results has been fixed.
		</comment>
		<comment id='6' author='MichaelKarpe' date='2020-08-13T20:27:23Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='7' author='MichaelKarpe' date='2020-08-20T20:27:38Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='8' author='MichaelKarpe' date='2020-08-20T20:27:41Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36718&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36718&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>