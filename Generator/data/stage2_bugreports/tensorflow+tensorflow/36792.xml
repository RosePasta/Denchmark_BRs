<bug id='36792' author='oeyvindds' open_date='2020-02-16T10:12:27Z' closed_time='2020-03-27T18:16:32Z'>
	<summary>Cannot convert a symbolic Tensor (Neg_1:0) to a numpy array</summary>
	<description>
Hello.
I am trying to derive with Tensorflow, and get an error telling me to report the behavior to the TensorFlow team. Hence this post.
My code:
from math import e

def cal_sigma(y):
    return 1 / (1 + np.power(e, -y) )

@tf.function
def get_derivative(x, y):
    with tf.GradientTape(persistent=True) as tape:
        tape.watch(x)
        tape.watch(y)
        y_sigma = cal_sigma(-y)
        f = (x + y_sigma) / np.power((x -y), 2)
    df_dy = tape.gradient(f, y) 
    return df_dy

x = tf.constant (1.)
y = tf.constant (-3.)
print(get_derivative(x, y))

If I derive with respect to x (df_dy = tape.gradient(f, x) in second to last line; it works. However, with y, I get the following error:

WARNING:tensorflow:AutoGraph could not transform &lt;method-wrapper '__call__' of numpy.ufunc object at 0x110a96950&gt; and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Cannot convert a symbolic Tensor (Neg_1:0) to a numpy array.
WARNING: AutoGraph could not transform &lt;method-wrapper '__call__' of numpy.ufunc object at 0x110a96950&gt; and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Cannot convert a symbolic Tensor (Neg_1:0) to a numpy array.
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
&lt;ipython-input-11-a4cdc628e4ad&gt; in &lt;module&gt;
      2 y = tf.constant (-3.)
      3 #print(get_derivative(x, y))
----&gt; 4 print(get_derivative2(x, y))

/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    566         xla_context.Exit()
    567     else:
--&gt; 568       result = self._call(*args, **kwds)
    569 
    570     if tracing_count == self._get_tracing_count():

/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)
    604       # In this case we have not created variables on the first call. So we can
    605       # run the first trace but we should fail if variables are created.
--&gt; 606       results = self._stateful_fn(*args, **kwds)
    607       if self._created_variables:
    608         raise ValueError("Creating variables on a non-first call to a function"

/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)
   2360     """Calls a graph function specialized to the inputs."""
   2361     with self._lock:
-&gt; 2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
   2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   2364 

/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2701 
   2702       self._function_cache.missed.add(call_context_key)
-&gt; 2703       graph_function = self._create_graph_function(args, kwargs)
   2704       self._function_cache.primary[cache_key] = graph_function
   2705       return graph_function, args, kwargs

/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2591             arg_names=arg_names,
   2592             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 2593             capture_by_value=self._capture_by_value),
   2594         self._function_attributes,
   2595         # Tell the ConcreteFunction to clean up its graph once it goes out of

/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    976                                           converted_func)
    977 
--&gt; 978       func_outputs = python_func(*func_args, **func_kwargs)
    979 
    980       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    437         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    438         # the function a weak reference to itself to avoid a reference cycle.
--&gt; 439         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    440     weak_wrapped_fn = weakref.ref(wrapped_fn)
    441 

/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in wrapper(*args, **kwargs)
    966           except Exception as e:  # pylint:disable=broad-except
    967             if hasattr(e, "ag_error_metadata"):
--&gt; 968               raise e.ag_error_metadata.to_exception(e)
    969             else:
    970               raise

NotImplementedError: in converted code:

    &lt;ipython-input-9-af8fccc461fb&gt;:20 get_derivative2  *
        y_sigma = cal_sigma(-y)
    &lt;ipython-input-9-af8fccc461fb&gt;:4 cal_sigma  *
        return 1 / (1 + np.power(e, -y) )
    /opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:728 __array__
        " array.".format(self.name))

    NotImplementedError: Cannot convert a symbolic Tensor (Neg_1:0) to a numpy array.
	</description>
	<comments>
		<comment id='1' author='oeyvindds' date='2020-02-16T10:18:15Z'>
		I did manage to get around it by not relying on numpy. The new code is:
def cal_sigma(y):
return 1 / (1 + e**-y)
@tf.function
def get_derivative2(x, y):
with tf.GradientTape(persistent=True) as tape:
tape.watch(y)
y_sigma = cal_sigma(-y)
f = (x + y_sigma) / ((x -y)**2)
df_dy = tape.gradient(f, y)
return df_dy
With these changes, it does work also for y. However, it gives an unpredictable answer. See &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/36793&gt;#36793&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='oeyvindds' date='2020-02-17T12:34:47Z'>
		&lt;denchmark-link:https://github.com/oeyvindds&gt;@oeyvindds&lt;/denchmark-link&gt;
  Could you please confirm to close this as it looks like a duplicate issue  You can track the progress in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/36793&gt;#36793&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='3' author='oeyvindds' date='2020-02-17T15:22:24Z'>
		No. This is not the same issue. This seems to be a bug related to TensorFlow not being able to use Numpy. Here it cannot convert the symbolic Tensor to a numpy array. Hence, when using Numpy it comes to a complete stop.
The workaround for this issue turned into issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/36793&gt;#36793&lt;/denchmark-link&gt;
. But I believe both issues should be researched and hopefully fixed.
As issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/36793&gt;#36793&lt;/denchmark-link&gt;
 is a different issue from my point of view. There it works fine with numpy. However, the two seemingly identical ways of doing a task give two different outputs.
It might be related to numpy as well. However, here TensorFlow comes to a complete stop and gives an error-message, while in the other issue it gives a result that most likely is not correct.
		</comment>
		<comment id='4' author='oeyvindds' date='2020-02-18T13:13:10Z'>
		Was able to reproduce the issue in colab with TF 2.1. With out using Numpy ,I am not facing any issue. Please find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/saikumarchalla/aa5e9834f9cca84bf3ab1e6b65d2b01f/untitled8.ipynb&gt;gist&lt;/denchmark-link&gt;
 here. Thanks!
		</comment>
		<comment id='5' author='oeyvindds' date='2020-03-16T04:00:06Z'>
		I also am getting the error NotImplementedError: Cannot convert a symbolic Tensor (truediv_3:0) to a numpy array.
My code is:
&lt;denchmark-code&gt;batch_size = 250
latent_space_depth = 128

def sample_z(args):
    z_mean, z_log_var = args
    eps = K.random_normal(shape=(batch_size, latent_space_depth), mean=0., stddev=1.)
    return z_mean + K.exp(z_log_var / 2) * eps
def VariationalAutoEncoder(num_pixels):
    
    input_img = Input(shape=(32, 32, 1))

    channels = 4
    x = input_img
    for i in range(5):
        left = Conv2D(channels, (3, 3), activation='relu', padding='same')(x)
        right = Conv2D(channels, (2, 2), activation='relu', padding='same')(x)
        conc = Concatenate()([left, right])
        x = MaxPooling2D((2, 2), padding='same')(conc)
        channels *= 2

    x = Dense(channels)(x)
    encoder_hidden = Flatten()(x)

    z_mean = Dense(latent_space_depth, activation='linear')(encoder_hidden)
    z_log_var = Dense(latent_space_depth, activation='linear')(encoder_hidden)
    
    def KL_loss(y_true, y_pred):
        return 0.0001 * K.sum(K.exp(z_log_var) + K.square(z_mean) - 1 - z_log_var, axis=1)

    def reconstruction_loss(y_true, y_pred):
        y_true = K.batch_flatten(y_true)
        y_pred = K.batch_flatten(y_pred)
        return binary_crossentropy(y_true, y_pred)

    def total_loss(y_true, y_pred):
        return reconstruction_loss(y_true, y_pred) + KL_loss(y_true, y_pred)

    z = Lambda(sample_z, output_shape=(latent_space_depth, ))([z_mean, z_log_var])
    decoder_in = Input(shape=(latent_space_depth,))

    d_x = Reshape((1, 1, latent_space_depth))(decoder_in)
    e_x = Reshape((1, 1, latent_space_depth))(z)
    for i in range(5):
        conv = Conv2D(channels, (3, 3), activation='relu', padding='same')
        upsampling = UpSampling2D((2, 2))
        d_x = conv(d_x)
        d_x = upsampling(d_x)
        e_x = conv(e_x)
        e_x = upsampling(e_x)
        channels //= 2

    final_conv = Conv2D(1, (3, 3), activation='sigmoid', padding='same')
    auto_decoded = final_conv(e_x)
    decoder_out = final_conv(d_x)
    
    decoder = Model(decoder_in, decoder_out)    
    
    auto_encoder = Model(input_img, auto_decoded)

    auto_encoder.compile(optimizer=Adam(lr=0.001), 
                         loss=total_loss,
                         metrics=[KL_loss, reconstruction_loss])
    
    return auto_encoder, decoder

var_auto_encoder, decoder = VariationalAutoEncoder(32)
var_auto_encoder.summary()

def truncate_to_batch(x):
    l = x.shape[0]
    return x[:l - l % batch_size, :, :, :]

x_train_trunc = truncate_to_batch(x_train)
x_test_trunc = truncate_to_batch(x_test)
x_train_trunc.shape, x_test_trunc.shape

import tensorflow as tf
tf.config.experimental_run_functions_eagerly(True)
var_auto_encoder.fit(x_train_trunc, x_train_trunc, verbose=1, 
                 batch_size=batch_size, epochs=100,
                 validation_data=(x_test_trunc, x_test_trunc))
&lt;/denchmark-code&gt;

My calculations of loss use K, not numpy
The actual trace is:
&lt;denchmark-code&gt;Train on 6500 samples, validate on 1000 samples
Epoch 1/100
 250/6500 [&gt;.............................]

---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
~/anaconda3/envs/TF/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in on_epoch(self, epoch, mode)
    766     try:
--&gt; 767       yield epoch_logs
    768     finally:

~/anaconda3/envs/TF/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    341                 training_context=training_context,
--&gt; 342                 total_epochs=epochs)
    343             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)

~/anaconda3/envs/TF/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    180       cbks.make_logs(model, batch_logs, batch_outs, mode)
--&gt; 181       step += 1
    182 

~/anaconda3/envs/TF/lib/python3.7/contextlib.py in __exit__(self, type, value, traceback)
    118             try:
--&gt; 119                 next(self.gen)
    120             except StopIteration:

~/anaconda3/envs/TF/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in on_batch(self, step, mode, size)
    788               mode, 'end', step, batch_logs)
--&gt; 789           self.progbar.on_batch_end(step, batch_logs)

~/anaconda3/envs/TF/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py in on_batch_end(self, batch, logs)
    780     if self.verbose and (self.target is None or self.seen &lt; self.target):
--&gt; 781       self.progbar.update(self.seen, self.log_values)
    782 

~/anaconda3/envs/TF/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/generic_utils.py in update(self, current, values)
    558         if isinstance(self._values[k], list):
--&gt; 559           avg = np.mean(self._values[k][0] / max(1, self._values[k][1]))
    560           if abs(avg) &gt; 1e-3:

&lt;__array_function__ internals&gt; in mean(*args, **kwargs)

~/anaconda3/envs/TF/lib/python3.7/site-packages/numpy/core/fromnumeric.py in mean(a, axis, dtype, out, keepdims)
   3334     return _methods._mean(a, axis=axis, dtype=dtype,
-&gt; 3335                           out=out, **kwargs)
   3336 

~/anaconda3/envs/TF/lib/python3.7/site-packages/numpy/core/_methods.py in _mean(a, axis, dtype, out, keepdims)
    134 def _mean(a, axis=None, dtype=None, out=None, keepdims=False):
--&gt; 135     arr = asanyarray(a)
    136 

~/anaconda3/envs/TF/lib/python3.7/site-packages/numpy/core/_asarray.py in asanyarray(a, dtype, order)
    137     """
--&gt; 138     return array(a, dtype, copy=False, order=order, subok=True)
    139 

~/anaconda3/envs/TF/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in __array__(self)
    727     raise NotImplementedError("Cannot convert a symbolic Tensor ({}) to a numpy"
--&gt; 728                               " array.".format(self.name))
    729 

NotImplementedError: Cannot convert a symbolic Tensor (truediv_1:0) to a numpy array.

During handling of the above exception, another exception occurred:

NotImplementedError                       Traceback (most recent call last)
&lt;ipython-input-28-9bb14bc81d2b&gt; in &lt;module&gt;
      3 var_auto_encoder.fit(x_train_trunc, x_train_trunc, verbose=1, 
      4                  batch_size=batch_size, epochs=100,
----&gt; 5                  validation_data=(x_test_trunc, x_test_trunc))

~/anaconda3/envs/TF/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    817         max_queue_size=max_queue_size,
    818         workers=workers,
--&gt; 819         use_multiprocessing=use_multiprocessing)
    820 
    821   def evaluate(self,

~/anaconda3/envs/TF/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    395                       total_epochs=1)
    396                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,
--&gt; 397                                  prefix='val_')
    398 
    399     return model.history

~/anaconda3/envs/TF/lib/python3.7/contextlib.py in __exit__(self, type, value, traceback)
    128                 value = type()
    129             try:
--&gt; 130                 self.gen.throw(type, value, traceback)
    131             except StopIteration as exc:
    132                 # Suppress StopIteration *unless* it's the same exception that

~/anaconda3/envs/TF/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in on_epoch(self, epoch, mode)
    770         # Epochs only apply to `fit`.
    771         self.callbacks.on_epoch_end(epoch, epoch_logs)
--&gt; 772       self.progbar.on_epoch_end(epoch, epoch_logs)
    773 
    774   @tf_contextlib.contextmanager

~/anaconda3/envs/TF/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py in on_epoch_end(self, epoch, logs)
    787         self.log_values.append((k, logs[k]))
    788     if self.verbose:
--&gt; 789       self.progbar.update(self.seen, self.log_values)
    790 
    791 

~/anaconda3/envs/TF/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/generic_utils.py in update(self, current, values)
    557         info += ' - %s:' % k
    558         if isinstance(self._values[k], list):
--&gt; 559           avg = np.mean(self._values[k][0] / max(1, self._values[k][1]))
    560           if abs(avg) &gt; 1e-3:
    561             info += ' %.4f' % avg

&lt;__array_function__ internals&gt; in mean(*args, **kwargs)

~/anaconda3/envs/TF/lib/python3.7/site-packages/numpy/core/fromnumeric.py in mean(a, axis, dtype, out, keepdims)
   3333 
   3334     return _methods._mean(a, axis=axis, dtype=dtype,
-&gt; 3335                           out=out, **kwargs)
   3336 
   3337 

~/anaconda3/envs/TF/lib/python3.7/site-packages/numpy/core/_methods.py in _mean(a, axis, dtype, out, keepdims)
    133 
    134 def _mean(a, axis=None, dtype=None, out=None, keepdims=False):
--&gt; 135     arr = asanyarray(a)
    136 
    137     is_float16_result = False

~/anaconda3/envs/TF/lib/python3.7/site-packages/numpy/core/_asarray.py in asanyarray(a, dtype, order)
    136 
    137     """
--&gt; 138     return array(a, dtype, copy=False, order=order, subok=True)
    139 
    140 

~/anaconda3/envs/TF/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in __array__(self)
    726   def __array__(self):
    727     raise NotImplementedError("Cannot convert a symbolic Tensor ({}) to a numpy"
--&gt; 728                               " array.".format(self.name))
    729 
    730   def __len__(self):
`
NotImplementedError: Cannot convert a symbolic Tensor (truediv_3:0) to a numpy array.`


&lt;/denchmark-code&gt;

Am I misreading the trace when the error seems to stem from yield epoch_logs
		</comment>
		<comment id='6' author='oeyvindds' date='2020-03-26T23:43:57Z'>
		In general, you can't mix NumPy and TF ops. But the error messages should be more informative, and we need to fix that.
		</comment>
		<comment id='7' author='oeyvindds' date='2020-03-27T18:16:34Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36792&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36792&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='oeyvindds' date='2020-03-27T19:13:34Z'>
		This PR resolves the autograph warning.
There will be a subsequent PR to improve the error message a bit.
		</comment>
		<comment id='9' author='oeyvindds' date='2020-08-11T08:51:21Z'>
		I cant get np.array or list from Tensor, too.
&lt;denchmark-code&gt;def elmo_base_test():
    import tensorflow_hub as hub
    import tensorflow as tf
    from tensorflow.python.framework.ops import disable_eager_execution
    disable_eager_execution()
    elmo = hub.Module("https://tfhub.dev/google/elmo/3", trainable=True)
    embeddings = elmo(["the cat is on the mat", "dogs are in the fog"], signature="default", as_dict=True)["default"]
    #sentences_embeddings = embeddings.numpy.tolist()
    sentences_embeddings = np.asarray(embeddings)
    print(sentences_embeddings, type(sentences_embeddings))
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>