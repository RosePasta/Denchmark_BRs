<bug id='28748' author='lioutasb' open_date='2019-05-15T20:39:02Z' closed_time='2019-05-18T02:44:50Z'>
	<summary>Counting parameters from Keras model doesn't work correctly with list of layers</summary>
	<description>
Below is an example that keras model is not able to count correctly the number of parameters when custom keras layers are inside a list that is defined in an another custom layer.
&lt;denchmark-code&gt;import tensorflow as tf

class TestLayer2(tf.keras.layers.Layer):
    def __init__(self, dim):
        super(TestLayer2, self).__init__()
        self.dense1 = tf.keras.layers.Dense(dim)
        
    def call(self, x):
        x = self.dense1(x)
        return x

class TestLayer1(tf.keras.layers.Layer):
    def __init__(self, dim):
        super(TestLayer1, self).__init__()
        self.list_of_dense = [TestLayer2(dim) for _ in range(2)]
        
    def call(self, x):
        for i in range(len(self.list_of_dense)):
            x = self.list_of_dense[i](x)
        return x
    
class TestModel(tf.keras.Model):
    def __init__(self, dim):
        super(TestModel, self).__init__()
        self.dense1 = tf.keras.layers.Dense(dim)
        self.layer = TestLayer1(dim*2)
        
    def call(self, x):
        x = self.dense1(x)
        x = self.layer(x)
        return x

t_model = TestModel(512)

tmp = tf.random.normal([64,512])

t_model(tmp)

t_model.summary()
&lt;/denchmark-code&gt;

returns
&lt;denchmark-code&gt;Layer (type)                 Output Shape              Param #   
=================================================================
dense_4728 (Dense)           multiple                  262656    
_________________________________________________________________
test_layer1_13 (TestLayer1)  multiple                  0         
=================================================================
Total params: 262,656
Trainable params: 262,656
Non-trainable params: 0
&lt;/denchmark-code&gt;

which is not correct. If the nested layers were outside a list then are counted correctly.
&lt;denchmark-code&gt;import tensorflow as tf

class TestLayer2(tf.keras.layers.Layer):
    def __init__(self, dim):
        super(TestLayer2, self).__init__()
        self.dense1 = tf.keras.layers.Dense(dim)
        
    def call(self, x):
        x = self.dense1(x)
        return x

class TestLayer1(tf.keras.layers.Layer):
    def __init__(self, dim):
        super(TestLayer1, self).__init__()
        self.dense1 = TestLayer2(dim)
        self.dense2 = TestLayer2(dim)
        
    def call(self, x):
        x = self.dense1(x)
        x = self.dense2(x)
        return x

class TestModel(tf.keras.Model):
    def __init__(self, dim):
        super(TestModel, self).__init__()
        self.dense1 = tf.keras.layers.Dense(dim)
        self.layer = TestLayer1(dim*2)
        
    def call(self, x):
        x = self.dense1(x)
        x = self.layer(x)
        return x

t_model = TestModel(512)

tmp = tf.random.normal([64,512])

t_model(tmp)

t_model.summary()
&lt;/denchmark-code&gt;

which gives
&lt;denchmark-code&gt;Layer (type)                 Output Shape              Param #   
=================================================================
dense_4731 (Dense)           multiple                  262656    
_________________________________________________________________
test_layer1_14 (TestLayer1)  multiple                  1574912   
=================================================================
Total params: 1,837,568
Trainable params: 1,837,568
Non-trainable params: 0
&lt;/denchmark-code&gt;

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): 1.13
Python version: 3.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: CUDA 10.0/CuDNN 7.3.1
GPU model and memory: Nvidia Titan V

	</description>
	<comments>
		<comment id='1' author='lioutasb' date='2019-05-16T08:38:29Z'>
		&lt;denchmark-link:https://github.com/lioutasb&gt;@lioutasb&lt;/denchmark-link&gt;
 Able to reproduce the issue as mentioned , tested the count of the number of parameters when custom keras layers are inside a list that is defined in an another custom layer.
Layer (type)                 Output Shape              Param #
dense (Dense)                multiple                  262656
test_layer1 (TestLayer1)     multiple                  0
Total params: 262,656
Trainable params: 262,656
Non-trainable params: 0
Below is the output for the nested layers that were outside a list.
Layer (type)                 Output Shape              Param #
dense_3 (Dense)              multiple                  262656
test_layer1_1 (TestLayer1)   multiple                  1574912
Total params: 1,837,568
Trainable params: 1,837,568
Non-trainable params: 0
		</comment>
		<comment id='2' author='lioutasb' date='2019-05-16T15:52:02Z'>
		&lt;denchmark-link:https://github.com/muddham&gt;@muddham&lt;/denchmark-link&gt;
 is there any workaround to count fast the number of parameters from a very large network? Manually it would be a pain.
		</comment>
		<comment id='3' author='lioutasb' date='2019-05-16T18:47:47Z'>
		&lt;denchmark-link:https://github.com/muddham&gt;@muddham&lt;/denchmark-link&gt;
 This is not mentioned before but this issue poses a more serious problem because this doesn't let training to be executed for the whole model.
&lt;denchmark-code&gt;import tensorflow as tf

class TestLayer2(tf.keras.layers.Layer):
    def __init__(self, dim):
        super(TestLayer2, self).__init__()
        self.dense1 = tf.keras.layers.Dense(dim)
        
    def call(self, x):
        x = self.dense1(x)
        return x

class TestLayer1(tf.keras.layers.Layer):
    def __init__(self, dim):
        super(TestLayer1, self).__init__()
        self.list_of_dense = [TestLayer2(dim) for _ in range(2)]
        
    def call(self, x):
        for i in range(len(self.list_of_dense)):
            x = self.list_of_dense[i](x)
        return x
    
class TestModel(tf.keras.Model):
    def __init__(self, dim):
        super(TestModel, self).__init__()
        self.dense1 = tf.keras.layers.Dense(dim)
        self.layer = TestLayer1(dim*2)
        
    def call(self, x):
        x = self.dense1(x)
        x = self.layer(x)
        return x

t_model = TestModel(512)

tmp = tf.random.normal([64,512])

t_model(tmp)

len(t_model.trainable_variables)
&lt;/denchmark-code&gt;

gives 2 which is wrong since it only sees the weight and bias matrices from the first dense layer.
This means that if you try to train the model using the following code it will only optimize the first dense layer.
&lt;denchmark-code&gt;gradients = tape.gradient(loss, t_model.trainable_variables)
optimizer.apply_gradients(list(zip(gradients, t_model.trainable_variables)))
&lt;/denchmark-code&gt;

I tested the same code on TF2.0 and it works as expected there so it seems only 1.13 has this bug.
		</comment>
		<comment id='4' author='lioutasb' date='2019-05-17T05:15:57Z'>
		Keras actually overloads the setattr to check whenever a layer is added. So a list would not trigger it.
Here is a workaround:
&lt;denchmark-code&gt;class TestLayer1(tf.keras.layers.Layer):
    def __init__(self, dim):
        super(TestLayer1, self).__init__()
       
        self.layer_names = ['layer_' + i for i in range(2)]
        for layer_name in self.layer_names:
             self.__setattr__(layer_name, TestLayer2(dim))
                
    def call(self, x):
        for layer_name in self.layer_names:
            x = self.__getattribute__(layer_name)(x)
        return x
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='lioutasb' date='2019-05-17T13:12:25Z'>
		&lt;denchmark-link:https://github.com/jnd77&gt;@jnd77&lt;/denchmark-link&gt;
 I confirm that this workaround works as expected.
		</comment>
		<comment id='6' author='lioutasb' date='2019-05-18T02:44:50Z'>
		Closing this out since I understand it to be resolved, but please let me know if I'm mistaken. Thanks!
		</comment>
		<comment id='7' author='lioutasb' date='2019-05-18T02:44:51Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=28748&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=28748&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>