<bug id='41657' author='Laggger164' open_date='2020-07-23T10:52:56Z' closed_time='2020-08-07T21:41:25Z'>
	<summary>NaN loss and accuracy when using MirroredStrategy on multiple GPUs (ROCm)</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No, the code is from this example: https://www.tensorflow.org/tutorials/distribute/keras
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
TensorFlow installed from (source or binary):
ROCm 3.5
TensorFlow version (use command below):
v2.2.0-30-g34c3143 2.2.0
Python version:
3.6.9
Bazel version (if compiling from source):
No
GCC/Compiler version (if compiling from source):
No
CUDA/cuDNN version:
ROCm
GPU model and memory:
2x AMD VEGA 10 XT (Vega 64) 8GB

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"

Describe the current behavior
Mirrored strategy causes the following loss and accuracy values:
Epoch 12/12
468/469 [============================&gt;.] - ETA: 0s - loss: nan - accuracy: nan
469/469 [==============================] - 12s 26ms/step - loss: nan - accuracy: nan - lr: 1.0000e-05
79/79 [==============================] - 1s 12ms/step - loss: nan - accuracy: nan
Eval loss: nan, Eval Accuracy: nan
This seems to happen on Nvidia cards too:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/36224&gt;#36224&lt;/denchmark-link&gt;

I have tried several other scripts and all do the same.
Describe the expected behavior
Running on a single card produces an expected output of loss and accuracy.
I would like that to be the case in multiple GPUs too.
Not sure whether it's just the loss and accuracy calculations screwed up, as the training goes about twice as fast on 2 GPUs.
This code seems to work flawlessly on both GPUs:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4965502/mnistmultigpu.txt&gt;mnistmultigpu.txt&lt;/denchmark-link&gt;

However, it only seems to utilize 1 GPU, which is strange because it did utilize both before...
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Unfortunately I cannot guarantee you will be able to reproduce this as this is tied to having multiple GPUs (Nvidia or not)
 The code I used 
# Import TensorFlow and TensorFlow Datasets
import tensorflow_datasets as tfds
import tensorflow as tf
tfds.disable_progress_bar()
import os
datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)
mnist_train, mnist_test = datasets['train'], datasets['test']
strategy = tf.distribute.MirroredStrategy()
print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
You can also do info.splits.total_num_examples to get the total
number of examples in the dataset.
num_train_examples = info.splits['train'].num_examples
num_test_examples = info.splits['test'].num_examples
BUFFER_SIZE = 10000
BATCH_SIZE_PER_REPLICA = 64
BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync
def scale(image, label):
image = tf.cast(image, tf.float32)
image /= 255
return image, label
train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)
with strategy.scope():
model = tf.keras.Sequential([
tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
tf.keras.layers.MaxPooling2D(),
tf.keras.layers.Flatten(),
tf.keras.layers.Dense(64, activation='relu'),
tf.keras.layers.Dense(10)
])
model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
optimizer=tf.keras.optimizers.Adam(),
metrics=['accuracy'])
Define the checkpoint directory to store the checkpoints
checkpoint_dir = './training_checkpoints'
Name of the checkpoint files
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt_{epoch}")
Function for decaying the learning rate.
You can define any decay function you need.
def decay(epoch):
if epoch &lt; 3:
return 1e-3
elif epoch &gt;= 3 and epoch &lt; 7:
return 1e-4
else:
return 1e-5
Callback for printing the LR at the end of each epoch.
class PrintLR(tf.keras.callbacks.Callback):
def on_epoch_end(self, epoch, logs=None):
print('\nLearning rate for epoch {} is {}'.format(epoch + 1,
model.optimizer.lr.numpy()))
callbacks = [
tf.keras.callbacks.TensorBoard(log_dir='./logs'),
tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,
save_weights_only=True),
tf.keras.callbacks.LearningRateScheduler(decay),
PrintLR()
]
model.fit(train_dataset, epochs=12, callbacks=callbacks)
model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))
eval_loss, eval_acc = model.evaluate(eval_dataset)
print('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4965388/runlog.txt&gt;runlog.txt&lt;/denchmark-link&gt;

My code in a text file:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4965443/mirroredstrategy.txt&gt;mirroredstrategy.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/36224&gt;#36224&lt;/denchmark-link&gt;

Any and all help would be appreciated, thank you!
	</description>
	<comments>
		<comment id='1' author='Laggger164' date='2020-07-23T11:54:34Z'>
		
This code seems to work flawlessly on both GPUs:
mnistmultigpu.txt
However, it only seems to utilize 1 GPU, which is strange because it did utilize both before...

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/30321&gt;#30321&lt;/denchmark-link&gt;

It seems that it may have worked properly when I experimented with an older version of TensorFlow.
So basically even that one doesn't work.
		</comment>
		<comment id='2' author='Laggger164' date='2020-07-23T19:58:18Z'>
		Hi &lt;denchmark-link:https://github.com/Laggger164&gt;@Laggger164&lt;/denchmark-link&gt;
, just to make sure I'm clear on this, you are running the basic Distributed training with Keras MNIST example. When you have just one GPU with MirroredStrategy, everything works. However, when you add a second GPU you are seeing NaN loss and accuracy. Is that a correct summary?
I'm not able to reproduce this error. So I'm wondering if maybe there's an issue with one of your GPUs? If you have more than one GPU in your system, the GPU with the lowest ID will be selected by default. Can you try and train with one GPU but enforce that it's GPU:1 instead of GPU:0? You can do that with tf.config.set_visible_devices to specify which devices are visible to the runtime.
		</comment>
		<comment id='3' author='Laggger164' date='2020-07-24T07:50:23Z'>
		
Hi @Laggger164, just to make sure I'm clear on this, you are running the basic Distributed training with Keras MNIST example. When you have just one GPU with MirroredStrategy, everything works. However, when you add a second GPU you are seeing NaN loss and accuracy. Is that a correct summary?
Pretty much, yes. Thing is it goes about twice as fast with 2 GPUs so it is possible that it works correctly, just the loss and accuracy metrics are calculated incorrectly.


I'm not able to reproduce this error. So I'm wondering if maybe there's an issue with one of your GPUs? If you have more than one GPU in your system, the GPU with the lowest ID will be selected by default. Can you try and train with one GPU but enforce that it's GPU:1 instead of GPU:0? You can do that with tf.config.set_visible_devices to specify which devices are visible to the runtime.

I tried with both GPUs, both seem to work identically well.
Here are the logs:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4970727/mirroredstrategysinglegpu1.txt&gt;mirroredstrategysinglegpu1.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4970728/mirroredstrategysinglegpu2.txt&gt;mirroredstrategysinglegpu2.txt&lt;/denchmark-link&gt;

And I tried modifying the code to remove the mirrored strategy:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4970733/strategynomirror.txt&gt;strategynomirror.txt&lt;/denchmark-link&gt;

and the logs from that:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4970735/strategysinglegpu1.txt&gt;strategysinglegpu1.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4970736/strategysinglegpu2.txt&gt;strategysinglegpu2.txt&lt;/denchmark-link&gt;

Seemed to be just a little faster (9 seconds instead of 11) with just one GPU than the one with mirrored strategy.
I used HID_VISIBLE_DEVICES=0 and 1, verifying the usage with rocm-smi on watch.
So I believe there is a problem in how either the code or the system is handling mirrored strategy.
I could try reinstalling the OS with a different version of everything, then upgrading to the newer ones, however that seems like a shot in the dark. I have no idea what I am doing basically...
Also, you mentioned you aren't able to reproduce this, meaning you don't have the hardware or system, or did you try and it is working on your end?
		</comment>
		<comment id='4' author='Laggger164' date='2020-07-24T13:42:15Z'>
		It seems that I managed to solve this by reinstalling everything.
One thing that I did change was that I didn't install the AMDGPU-PRO drivers before installing ROCm.
Which I later found out was completely useless as ROCm is another type of driver, so it is possible I screwed something up that way.
I still have a problem with the mirrorStrategy as it is only utilizing multiple GPUs to exactly half their power when using 2 and exactly a quarter when using 4.
Pretty sure this could be caused by some setting in Tensorflow that I don't yet know of.
If you do know then I would be thankful if you could tell me.
Not sure if we should close this issue at this point and make another one. You decide üëç
		</comment>
		<comment id='5' author='Laggger164' date='2020-07-24T14:32:53Z'>
		Glad to hear that reinstalling worked!
As for the performance issue, can you explain a bit more about what you mean by using half their power? Are you seeing a slowdown with the more GPUs you add?
		</comment>
		<comment id='6' author='Laggger164' date='2020-07-24T15:16:08Z'>
		
Glad to hear that reinstalling worked!
As for the performance issue, can you explain a bit more about what you mean by using half their power? Are you seeing a slowdown with the more GPUs you add?

Yes, that's exactly what happens.
However it did work better with both of the cards connected to full PCIe 16x slots... which improved the training time by about 2 seconds from 17 seconds.
As opposed to using PCIe 1x risers, which just slow it down.
As for the power I mean GPU utilization and GPU power draw, metric taken from rocm-smi on watch.
i.e. when using a single GPU both are at 100%, but when using 2, both are at around 50%. VRAM being always full.
It's basically telling me that it could work but doesn't right now.
I also noticed the CPU utilization was almost always at 100% with 4 cards running, while being at around 70% with 2 cards.
I am wondering whether there is a bandwidth issue, the CPU and motherboard are pretty cheap so I will try a much stronger setup at some point.
But if you have any ideas, feel free to tell me!
		</comment>
		<comment id='7' author='Laggger164' date='2020-08-01T20:42:28Z'>
		If you're still facing this utilization problem, can you file a new issue? Any Tensorboard Profiler info you can provide as well will be helpful to debug. There are guides &lt;denchmark-link:https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras&gt;here&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://www.tensorflow.org/guide/profiler&gt;here&lt;/denchmark-link&gt;
 on how to use the tool.
		</comment>
		<comment id='8' author='Laggger164' date='2020-08-07T21:41:24Z'>
		Closing this issue now since original problem was solved. If performance issues persist, please file a new issue and I'd be happy to help debug!
		</comment>
		<comment id='9' author='Laggger164' date='2020-08-07T21:41:26Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41657&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41657&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>