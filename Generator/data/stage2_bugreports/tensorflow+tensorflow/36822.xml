<bug id='36822' author='ngc92' open_date='2020-02-17T13:44:06Z' closed_time='2020-06-22T23:44:08Z'>
	<summary>exception in gradient computation for losses when combining Tensor and EagerTensor</summary>
	<description>
System information - Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): yes

OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): - Linux Mint 19
TensorFlow installed from (source or binary): binary (pip)
TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0
Python version: 3.6

Describe the current behavior
The following minimal example works when using the second implementation alternative, but raises an error for the first:
ip = layers.Input(10)
with tf.GradientTape() as tape:
    t = tf.zeros((10, 10))
    tape.watch(t)
    # this line throws an error 
    result = keras.losses.MeanSquaredError()(t, ip)
    # but this one works
    # result = (t - ip)**2 / tf.cast(tf.reduce_prod(tf.shape(t)), tf.float32)
tape.gradient(result, t)
The error is
&lt;denchmark-code&gt;  File "~/wrapper.py", line 101, in &lt;module&gt;
    print(tape.gradient(result, t))
  File "~/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py", line 1029, in gradient
    unconnected_gradients=unconnected_gradients)
  File "~/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py", line 77, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File "~/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py", line 141, in _gradient_function
    return grad_fn(mock_op, *out_grads)
  File "~/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py", line 258, in _MeanGrad
    sum_grad = _SumGrad(op, grad)[0]
  File "~/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py", line 213, in _SumGrad
    op.inputs[1])
  File "~/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py", line 3502, in reduced_shape
    input_shape = input_shape.numpy()
AttributeError: 'Tensor' object has no attribute 'numpy'
&lt;/denchmark-code&gt;

Describe the expected behavior
As the second implementation works, i would also expect the first to work.
I have not done exhaustive testing of different loss functions, but the same problem also occurs when replacing the MeanSquaredError by CategoricalCrossEntropy. The problem seems to be caused by the fact that only one of the arguments to the loss function is an EagerTensor.
	</description>
	<comments>
		<comment id='1' author='ngc92' date='2020-02-18T12:05:36Z'>
		I have tried on colab with TF version 2.1.0, 2.2.0-dev20200218 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/7085b1ad3a03380e559078aa220d186b/untitled646.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='ngc92' date='2020-06-22T23:44:08Z'>
		Thank you &lt;denchmark-link:https://github.com/ngc92&gt;@ngc92&lt;/denchmark-link&gt;
 . The equivalent of  will not be .
It will be the following roughly:
&lt;denchmark-code&gt;      mean_per_sample = math_ops.mean((t - ip)**2 / math_ops.cast(math_ops.reduce_prod(array_ops.shape(t)), dtypes.float32), axis=-1)
      total_loss = math_ops.reduce_sum(mean_per_sample)
      num_present = math_ops.cast(array_ops.size(mean_per_sample), dtype=mean_per_sample.dtype)
      result = math_ops.div_no_nan(total_loss, num_present, name='value')
&lt;/denchmark-code&gt;

Here is a detailed example for writing custom training loop: &lt;denchmark-link:https://keras.io/guides/writing_a_training_loop_from_scratch/&gt;https://keras.io/guides/writing_a_training_loop_from_scratch/&lt;/denchmark-link&gt;
. Please check it out.
		</comment>
		<comment id='3' author='ngc92' date='2020-06-22T23:44:10Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36822&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36822&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='ngc92' date='2020-06-23T22:23:57Z'>
		&lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;

My main point was not that I wanted to find out how to do a custom training loop, but the fact that the function throws an exception when called with one  and one , which I find to be surprising. The example was mostly to show that in other cases, tf is perfectly happy to mix those two.
		</comment>
	</comments>
</bug>