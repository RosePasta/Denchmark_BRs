<bug id='37581' author='JIMonroe' open_date='2020-03-13T17:56:42Z' closed_time='2020-08-31T17:34:38Z'>
	<summary>tf.keras.Model.fit() training fails, custom training loop succeeds for identical model, optimizer, and loss function</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;



Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, but can reproduce with (almost) stock example script


OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MacOS Mojave and CentOS 7


Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A


TensorFlow installed from (source or binary):
binary


TensorFlow version (use command below):
v2.1.0-rc2-17-ge5bf8de 2.1.0


Python version:
3.7.4


Bazel version (if compiling from source):
N/A


GCC/Compiler version (if compiling from source):
N/A


CUDA/cuDNN version:
10.1, but have also validated on CPU


GPU model and memory:
GeForce RTX 2080TI


Exact command to reproduce:
See script below.


&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

When training the same model with the same data, loss function, and optimizer, significantly different performance is obtained when using the built in training loops from tf.keras.Model.fit() versus implementing the training loop manually. This is clear by comparing the output of these two cases for the VAE example code provided at &lt;denchmark-link:https://www.tensorflow.org/guide/keras/custom_layers_and_models&gt;Writing custom layers and models&lt;/denchmark-link&gt;
, which is reproduced below with minimal alterations. The issue becomes more clear when using a BinaryCrossentropy loss rather than MSE loss, and I have also reproduced it with custom loss functions. Obviously, the ability of users to detect the bug will depend on the data, loss function, optimizer, and extent of training. However, it should be possible to obtain the same performance between a simple custom training loop and the fit() method, as advertised in the documentation at the link above. Additionally, it is concerning that the recommended option (i.e. the fit() method) performs more poorly to the alternative, even with many more iterations (I have tried many, many iterations). Perhaps the fit() method is implementing additional loss terms not easily exposed to the user that are modifying the optimization landscape and subsequently the convergence behavior? The results are robust across multiple runs and (presumably) different random number seeds.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

A script to reproduce is provided, making use of the dsprites data set through tensorflow_datasets. This example is likely not minimal, but it consistently reproduces the bug. Image reproductions with the trained VAE models are also included for the two training cases
import tensorflow as tf
from tensorflow.keras import layers
import tensorflow_datasets as tfds
import numpy as np
import matplotlib.pyplot as plt

class Sampling(layers.Layer):
  """Uses (z_mean, z_log_var) to sample z, the vector encoding a digit."""

  def call(self, inputs):
    z_mean, z_log_var = inputs
    batch = tf.shape(z_mean)[0]
    dim = tf.shape(z_mean)[1]
    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
    return z_mean + tf.exp(0.5 * z_log_var) * epsilon


class Encoder(layers.Layer):
  """Maps MNIST digits to a triplet (z_mean, z_log_var, z)."""

  def __init__(self,
               latent_dim=32,
               intermediate_dim=64,
               name='encoder',
               **kwargs):
    super(Encoder, self).__init__(name=name, **kwargs)
    self.dense_proj = layers.Dense(intermediate_dim, activation='relu')
    self.dense_mean = layers.Dense(latent_dim)
    self.dense_log_var = layers.Dense(latent_dim)
    self.sampling = Sampling()

  def call(self, inputs):
    x = self.dense_proj(inputs)
    z_mean = self.dense_mean(x)
    z_log_var = self.dense_log_var(x)
    z = self.sampling((z_mean, z_log_var))
    return z_mean, z_log_var, z


class Decoder(layers.Layer):
  """Converts z, the encoded digit vector, back into a readable digit."""

  def __init__(self,
               original_dim,
               intermediate_dim=64,
               name='decoder',
               **kwargs):
    super(Decoder, self).__init__(name=name, **kwargs)
    self.dense_proj = layers.Dense(intermediate_dim, activation='relu')
    self.dense_output = layers.Dense(original_dim, activation='sigmoid')

  def call(self, inputs):
    x = self.dense_proj(inputs)
    return self.dense_output(x)


class VariationalAutoEncoder(tf.keras.Model):
  """Combines the encoder and decoder into an end-to-end model for training."""

  def __init__(self,
               original_dim,
               intermediate_dim=64,
               latent_dim=32,
               name='autoencoder',
               **kwargs):
    super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)
    self.original_dim = original_dim
    self.encoder = Encoder(latent_dim=latent_dim,
                           intermediate_dim=intermediate_dim)
    self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)

  def call(self, inputs):
    z_mean, z_log_var, z = self.encoder(inputs)
    reconstructed = self.decoder(z)
    # Add KL divergence regularization loss.
    kl_loss = - 0.5 * tf.reduce_mean(
        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)
    self.add_loss(kl_loss)
    return reconstructed


def plotRecons(model, dat, savePlot=None):
  """Plots reconstructions of provided images.
  """
  fig, ax = plt.subplots(len(list(dat)), 2)
  for i, im in enumerate(dat):
    ax[i,0].imshow(im[:,:,0], cmap='gray_r', vmin=0.0, vmax=1.0)
    thisrecon = model(tf.cast(tf.reshape(im, (1,4096)), 'float32'))
    thisrecon = np.reshape(thisrecon, (64,64))
    randomIm = np.random.random(thisrecon.shape)
    #thisrecon = np.array((thisrecon &gt; randomIm), dtype=int)
    ax[i,1].imshow(thisrecon, cmap='gray_r', vmin=0.0, vmax=1.0)
    ax[i,0].tick_params(axis='both', which='both',
                        left=False, right=False, bottom=False, top=False,
                        labelleft=False, labelbottom=False,
                        labelright=False, labeltop=False)
    ax[i,1].tick_params(axis='both', which='both',
                        left=False, right=False, bottom=False, top=False,
                        labelleft=False, labelbottom=False,
                        labelright=False, labeltop=False)

  fig.tight_layout()
  if savePlot is not None:
    fig.savefig(savePlot)
  plt.show()


def trainCustom(train_dataset, epochs=2, original_dim=4096):
  vae = VariationalAutoEncoder(original_dim, 64, 32)
  
  optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
  #loss_fn = tf.keras.losses.MeanSquaredError()
  loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False,
                                               reduction=tf.keras.losses.Reduction.SUM)

  # Iterate over epochs.
  for epoch in range(epochs):
    print('Start of epoch %d' % (epoch,))
  
    # Iterate over the batches of the dataset.
    for step, x_batch_train in enumerate(train_dataset):
      with tf.GradientTape() as tape:
        reconstructed = vae(x_batch_train)
        # Compute reconstruction loss
        loss = loss_fn(x_batch_train, reconstructed)
        loss += sum(vae.losses)  # Add KLD regularization loss
  
      grads = tape.gradient(loss, vae.trainable_weights)
      optimizer.apply_gradients(zip(grads, vae.trainable_weights))
  
      if step % 1000 == 0:
        print('step %s: mean loss = %s' % (step, loss))
  
  return vae


def trainBuiltIn(train_dataset, epochs=2, original_dim=4096):
  vae = VariationalAutoEncoder(original_dim, 64, 32)
  
  optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
  
  vae.compile(optimizer, #loss=tf.keras.losses.MeanSquaredError())
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False,
                                                      reduction=tf.keras.losses.Reduction.SUM))

  zipdata = tf.data.Dataset.zip((train_dataset, train_dataset))

  vae.fit(zipdata, epochs=epochs)

  return vae


def main():

  def flatImFromDict(adict):
    return tf.squeeze(tf.reshape(tf.cast(adict['image'], 'float32'), (-1, 4096)))

  trainDataRaw = tfds.load("dsprites", split="train")
  trainData = trainDataRaw.map(flatImFromDict)
  trainData = trainData.shuffle(buffer_size=64).batch(64, drop_remainder=True)
  plotData = [tf.cast(adict['image'], 'float32') for i, adict in enumerate(trainDataRaw) if i&lt;5]

  #First test custom, well-controlled loop
  vaeCustom = trainCustom(trainData, epochs=2)
  plotRecons(vaeCustom, plotData, savePlot='recons_custom.png')

  #Next look at build in loops with fit function
  vaeBuiltIn = trainBuiltIn(trainData, epochs=2)
  plotRecons(vaeBuiltIn, plotData, savePlot='recons_built-in.png')


if __name__=="__main__":
  main()
&lt;denchmark-link:https://user-images.githubusercontent.com/7297977/76646659-b7d1bc00-6531-11ea-9e4d-8d280531121a.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/7297977/76646661-b86a5280-6531-11ea-916a-4ae20c4b78a8.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='JIMonroe' date='2020-03-16T14:01:20Z'>
		i have replicated this issue, please find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/06c139856a5a70086b953dd7ba1cd558/37581.ipynb&gt;gist here&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='JIMonroe' date='2020-05-19T13:58:36Z'>
		Has there been any progress on this?  Or an explanation?  I would be satisfied with just updating the documentation and discussion at the link I provided in order to convey why differences occur with the different training routines.  If the loss function with the fit() method is by default modified in some way, the user should be notified of this.
		</comment>
		<comment id='3' author='JIMonroe' date='2020-08-31T17:34:34Z'>
		Hi &lt;denchmark-link:https://github.com/JIMonroe&gt;@JIMonroe&lt;/denchmark-link&gt;
,
There's a subtle bug in your code.
&lt;denchmark-code&gt;zipdata = tf.data.Dataset.zip((train_dataset, train_dataset))
&lt;/denchmark-code&gt;

Should be:
&lt;denchmark-code&gt;zipdata = train_dataset.map(lambda x: (x,x))
&lt;/denchmark-code&gt;

In the zip the two copies were getting independent shuffles.
This sort of thing's always easier to spot if you minmize the example.
		</comment>
		<comment id='4' author='JIMonroe' date='2020-08-31T17:34:39Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37581&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37581&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='JIMonroe' date='2020-08-31T20:52:52Z'>
		Great, thanks!  Totally makes sense if you're zipping together two iterators.
		</comment>
	</comments>
</bug>