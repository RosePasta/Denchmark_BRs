<bug id='26559' author='lxl910915' open_date='2019-03-11T03:18:30Z' closed_time='2019-04-09T06:46:42Z'>
	<summary>tensorflow1.12 hangs at LocalMaster::RunStep with tf.train.MonitoredTrainingSession</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.12
Python version: 3.6
Bazel version (if compiling from source): 0.18
GCC/Compiler version (if compiling from source): 5.4
CUDA/cuDNN version:
GPU model and memory:

You can collect some of this information using our environment capture &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
b'v1.12.0-18-gd60b574' 1.12.0
Describe the current behavior
In distributed tensorflow, some workers hang at the last batch of dataset when the dataset epoch is 1 and the step in StopAtStepHook() is very large. What's more, the number of samples in the last batch of dataset may less than the batch_size.
If we use tf.train.StopAtStepHook() to stop training, all workers can exit successfully.
Describe the expected behavior
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
In local_master.cc, WaitForNotification() function is as follows:
&lt;denchmark-code&gt;Status WaitForNotification(CallOptions* call_options,
                           const int64 default_timeout_in_ms, Notification* n) {
  int64 timeout_in_ms = call_options-&gt;GetTimeout();
  if (timeout_in_ms == 0) {
    timeout_in_ms = default_timeout_in_ms;
  }
  if (timeout_in_ms &gt; 0) {
    int64 timeout_in_us = timeout_in_ms * 1000;
    bool notified = WaitForNotificationWithTimeout(n, timeout_in_us);
    if (!notified) {
      call_options-&gt;StartCancel();
      // The call has borrowed pointers to the request and response
      // messages, so we must still wait for the call to complete.
      n-&gt;WaitForNotification();
      return errors::DeadlineExceeded("Operation timed out.");
    }
  } else {
    n-&gt;WaitForNotification();
  }
  return Status::OK();
}
&lt;/denchmark-code&gt;

Workers hang at 'n-&gt;WaitForNotification()'. We confuse that WaitForNotificationWithTimeout(n, timeout_in_us) has been called, why WaitForNotification() is called again ?
bt is as follows:
&lt;denchmark-code&gt;#0  syscall () at ../sysdeps/unix/sysv/linux/x86_64/syscall.S:38
#1  0x00007f475f56e1ac in nsync::futex (uaddr=0x555c3d471368, op=393, val=0, timeout=0x0, uaddr2=0x0, val3=-1) at external/nsync/platform/linux/src/nsync_semaphore_futex.c:21
#2  0x00007f475f56e357 in nsync::nsync_mu_semaphore_p_with_deadline (s=0x555c3d471368, abs_deadline=...) at external/nsync/platform/linux/src/nsync_semaphore_futex.c:108
#3  0x00007f475f56d1f6 in nsync::nsync_sem_wait_with_cancel_ (w=0x555c3d471360, abs_deadline=..., cancel_note=0x0) at external/nsync/internal/sem_wait.c:36
#4  0x00007f475f569277 in nsync::nsync_cv_wait_with_deadline_generic (pcv=0x7fff46c23e20, pmu=0x7fff46c23e10, lock=0x7f475f568fdf &lt;nsync::void_mu_lock(void*)&gt;, 
    unlock=0x7f475f568ffa &lt;nsync::void_mu_unlock(void*)&gt;, abs_deadline=..., cancel_note=0x0) at external/nsync/internal/cv.c:246
#5  0x00007f475f5699f5 in nsync::nsync_cv_wait_with_deadline (pcv=0x7fff46c23e20, pmu=0x7fff46c23e10, abs_deadline=..., cancel_note=0x0) at external/nsync/internal/cv.c:440
#6  0x00007f475f569a32 in nsync::nsync_cv_wait (pcv=0x7fff46c23e20, pmu=0x7fff46c23e10) at external/nsync/internal/cv.c:450
#7  0x00007f474de90bf5 in tensorflow::condition_variable::wait (this=0x7fff46c23e20, lock=...) at tensorflow/core/platform/default/mutex.cc:72
#8  0x00007f4756864a5d in tensorflow::Notification::WaitForNotification (this=0x7fff46c23e10) at ./tensorflow/core/platform/default/notification.h:54
#9  0x00007f4756c3dc34 in tensorflow::(anonymous namespace)::WaitForNotification (call_options=0x7fff46c24090, default_timeout_in_ms=0, n=0x7fff46c23e10)
    at tensorflow/core/distributed_runtime/local_master.cc:39
#10 0x00007f4756c3e439 in tensorflow::LocalMaster::RunStep (this=0x555c3ba54780, call_options=0x7fff46c24090, request=0x555c38dbae60, response=0x555c38dbb110)
    at tensorflow/core/distributed_runtime/local_master.cc:105
#11 0x00007f475686d03d in tensorflow::GrpcSession::RunProto (this=0x555c3b986280, call_options=0x7fff46c24090, req=0x555c38dbae60, resp=0x555c38dbb110)
    at tensorflow/core/distributed_runtime/rpc/grpc_session.cc:289
#12 0x00007f475686c7d6 in tensorflow::GrpcSession::RunHelper (this=0x555c3b986280, run_options=..., inputs=std::vector of length 0, capacity 0, 
    output_tensor_names=std::vector of length 4, capacity 4 = {...}, target_node_names=std::vector of length 1, capacity 1 = {...}, outputs=0x7fff46c242f0, run_metadata=0x7fff46c24350, 
    prun_handle="") at tensorflow/core/distributed_runtime/rpc/grpc_session.cc:221
#13 0x00007f475686ce35 in tensorflow::GrpcSession::Run (this=0x555c3b986280, run_options=..., inputs=std::vector of length 0, capacity 0, 
    output_tensor_names=std::vector of length 4, capacity 4 = {...}, target_node_names=std::vector of length 1, capacity 1 = {...}, outputs=0x7fff46c242f0, run_metadata=0x7fff46c24350)
    at tensorflow/core/distributed_runtime/rpc/grpc_session.cc:270
#14 0x00007f4756832f34 in tensorflow::SessionRef::Run (this=0x555c3ba38880, run_options=..., inputs=std::vector of length 0, capacity 0, 
    output_tensor_names=std::vector of length 4, capacity 4 = {...}, target_node_names=std::vector of length 1, capacity 1 = {...}, outputs=0x7fff46c242f0, run_metadata=0x7fff46c24350)
    at tensorflow/python/client/session_ref.cc:427
#15 0x00007f4756aeeabc in TF_Run_Helper (session=0x555c3ba38880, handle=0x0, run_options=0x555c38437160, input_pairs=std::vector of length 0, capacity 0, 
    output_tensor_names=std::vector of length 4, capacity 4 = {...}, c_outputs=0x7fff46c246d8, target_oper_names=std::vector of length 1, capacity 1 = {...}, run_metadata=0x555c38dd9b70, 
    status=0x555c38ddf6d0) at tensorflow/c/c_api.cc:783
&lt;/denchmark-code&gt;

Using 'thread apply all bt' in gdb, we can see there are two threads wait at cond_var_.wait(l) in BackgroundWorker::WorkerLoop() method.
&lt;denchmark-code&gt;(gdb) thread apply all bt
Thread 204 (Thread 0x7f366b7fe700 (LWP 457)):
#0  syscall () at ../sysdeps/unix/sysv/linux/x86_64/syscall.S:38
#1  0x00007f3a48857cac in nsync::futex (uaddr=0x7f379c001d08, op=393, val=0, timeout=0x0, uaddr2=0x0, val3=-1) at external/nsync/platform/linux/src/nsync_semaphore_futex.c:21
#2  0x00007f3a48857e57 in nsync::nsync_mu_semaphore_p_with_deadline (s=0x7f379c001d08, abs_deadline=...) at external/nsync/platform/linux/src/nsync_semaphore_futex.c:108
#3  0x00007f3a48856cf6 in nsync::nsync_sem_wait_with_cancel_ (w=0x7f379c001d00, abs_deadline=..., cancel_note=0x0) at external/nsync/internal/sem_wait.c:36
#4  0x00007f3a48852d77 in nsync::nsync_cv_wait_with_deadline_generic (pcv=0x558523ec4f98, pmu=0x558523ec4f88, lock=0x7f3a48852adf &lt;nsync::void_mu_lock(void*)&gt;, 
    unlock=0x7f3a48852afa &lt;nsync::void_mu_unlock(void*)&gt;, abs_deadline=..., cancel_note=0x0) at external/nsync/internal/cv.c:246
#5  0x00007f3a488534f5 in nsync::nsync_cv_wait_with_deadline (pcv=0x558523ec4f98, pmu=0x558523ec4f88, abs_deadline=..., cancel_note=0x0) at external/nsync/internal/cv.c:440
#6  0x00007f3a48853532 in nsync::nsync_cv_wait (pcv=0x558523ec4f98, pmu=0x558523ec4f88) at external/nsync/internal/cv.c:450
#7  0x00007f3a3717abf5 in tensorflow::condition_variable::wait (this=0x558523ec4f98, lock=...) at tensorflow/core/platform/default/mutex.cc:72
#8  0x00007f3a36ccb475 in tensorflow::data::BackgroundWorker::WorkerLoop (this=0x558523ec4f80) at tensorflow/core/framework/dataset.cc:318
#9  0x00007f3a36ccb1f5 in tensorflow::data::BackgroundWorker::BackgroundWorker(tensorflow::Env*, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)::{lambda()#1}::operator()() const () at tensorflow/core/framework/dataset.cc:287
#10 0x00007f3a36ccb887 in std::_Function_handler&lt;void(), tensorflow::data::BackgroundWorker::BackgroundWorker(tensorflow::Env*, const string&amp;)::&lt;lambda()&gt; &gt;::_M_invoke(const std::_Any_data &amp;) (__functor=...) at /usr/include/c++/5/functional:1871
#11 0x00007f3a36cab688 in std::function&lt;void ()&gt;::operator()() const (this=0x558522569ff8) at /usr/include/c++/5/functional:2267
#12 0x00007f3a3717d7c8 in std::_Bind_simple&lt;std::function&lt;void ()&gt; ()&gt;::_M_invoke&lt;&gt;(std::_Index_tuple&lt;&gt;) (this=0x558522569ff8) at /usr/include/c++/5/functional:1531
#13 0x00007f3a3717d765 in std::_Bind_simple&lt;std::function&lt;void ()&gt; ()&gt;::operator()() (this=0x558522569ff8) at /usr/include/c++/5/functional:1520
#14 0x00007f3a3717d704 in std::thread::_Impl&lt;std::_Bind_simple&lt;std::function&lt;void ()&gt; ()&gt; &gt;::_M_run() (this=0x558522569fe0) at /usr/include/c++/5/thread:115
#15 0x00007f3a5475f678 in std::execute_native_thread_routine_compat (__p=&lt;optimized out&gt;)
    at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/thread.cc:94
#16 0x00007f3a68b266ba in start_thread (arg=0x7f366b7fe700) at pthread_create.c:333
#17 0x00007f3a6885c41d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109

Thread 203 (Thread 0x7f3805137700 (LWP 456)):
#0  syscall () at ../sysdeps/unix/sysv/linux/x86_64/syscall.S:38
#1  0x00007f3a48857cac in nsync::futex (uaddr=0x7f378d8850a8, op=393, val=0, timeout=0x0, uaddr2=0x0, val3=-1) at external/nsync/platform/linux/src/nsync_semaphore_futex.c:21
#2  0x00007f3a48857e57 in nsync::nsync_mu_semaphore_p_with_deadline (s=0x7f378d8850a8, abs_deadline=...) at external/nsync/platform/linux/src/nsync_semaphore_futex.c:108
#3  0x00007f3a48856cf6 in nsync::nsync_sem_wait_with_cancel_ (w=0x7f378d8850a0, abs_deadline=..., cancel_note=0x0) at external/nsync/internal/sem_wait.c:36
#4  0x00007f3a48852d77 in nsync::nsync_cv_wait_with_deadline_generic (pcv=0x5585225338f0, pmu=0x5585225338e0, lock=0x7f3a48852adf &lt;nsync::void_mu_lock(void*)&gt;, 
    unlock=0x7f3a48852afa &lt;nsync::void_mu_unlock(void*)&gt;, abs_deadline=..., cancel_note=0x0) at external/nsync/internal/cv.c:246
#5  0x00007f3a488534f5 in nsync::nsync_cv_wait_with_deadline (pcv=0x5585225338f0, pmu=0x5585225338e0, abs_deadline=..., cancel_note=0x0) at external/nsync/internal/cv.c:440
#6  0x00007f3a48853532 in nsync::nsync_cv_wait (pcv=0x5585225338f0, pmu=0x5585225338e0) at external/nsync/internal/cv.c:450
#7  0x00007f3a3717abf5 in tensorflow::condition_variable::wait (this=0x5585225338f0, lock=...) at tensorflow/core/platform/default/mutex.cc:72
#8  0x00007f3a36ccb475 in tensorflow::data::BackgroundWorker::WorkerLoop (this=0x5585225338d8) at tensorflow/core/framework/dataset.cc:318
#9  0x00007f3a36ccb1f5 in tensorflow::data::BackgroundWorker::BackgroundWorker(tensorflow::Env*, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)::{lambda()#1}::operator()() const () at tensorflow/core/framework/dataset.cc:287
#10 0x00007f3a36ccb887 in std::_Function_handler&lt;void(), tensorflow::data::BackgroundWorker::BackgroundWorker(tensorflow::Env*, const string&amp;)::&lt;lambda()&gt; &gt;::_M_invoke(const std::_Any_data &amp;) (__functor=...) at /usr/include/c++/5/functional:1871
#11 0x00007f3a36cab688 in std::function&lt;void ()&gt;::operator()() const (this=0x558523c0df78) at /usr/include/c++/5/functional:2267
#12 0x00007f3a3717d7c8 in std::_Bind_simple&lt;std::function&lt;void ()&gt; ()&gt;::_M_invoke&lt;&gt;(std::_Index_tuple&lt;&gt;) (this=0x558523c0df78) at /usr/include/c++/5/functional:1531
#13 0x00007f3a3717d765 in std::_Bind_simple&lt;std::function&lt;void ()&gt; ()&gt;::operator()() (this=0x558523c0df78) at /usr/include/c++/5/functional:1520
#14 0x00007f3a3717d704 in std::thread::_Impl&lt;std::_Bind_simple&lt;std::function&lt;void ()&gt; ()&gt; &gt;::_M_run() (this=0x558523c0df60) at /usr/include/c++/5/thread:115
#15 0x00007f3a5475f678 in std::execute_native_thread_routine_compat (__p=&lt;optimized out&gt;)
    at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/thread.cc:94
#16 0x00007f3a68b266ba in start_thread (arg=0x7f3805137700) at pthread_create.c:333
#17 0x00007f3a6885c41d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='lxl910915' date='2019-03-13T00:31:28Z'>
		Please assign to the owner of this code. I am not the owner.
		</comment>
		<comment id='2' author='lxl910915' date='2019-04-02T00:54:25Z'>
		&lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 -- any guesses here?
		</comment>
		<comment id='3' author='lxl910915' date='2019-04-08T15:58:08Z'>
		From the bug description, all we know is that some Session.run() call is blocked (from the thread blocked in WaitForNotification()) and the session is still active (from the two tf.data background threads).
Without a way to reproduce the problem, there's no way to tell why it's blocked. From previous experience, it's possible that the termination logic in Estimator (with or without StopAtStepHook) has a race condition. One case that I've seen is the past is that there might be an accidental dependency between worker tasks, where when one worker exits cleanly, another worker might start a concurrent Session.run() call that depends on the exited worker, and will block forever waiting for that worker to come back up. However, I would have expected the default worker device filter to prevent it, based on the code here: 


tensorflow/tensorflow/python/estimator/run_config.py


         Line 567
      in
      5b0c525






 device_filters = ['/job:ps', '/job:worker/task:%d' % self._task_id] 





Are you using that device filter? Which tasks (worker or PS) have exited (if any) when you observe the hang?
		</comment>
		<comment id='4' author='lxl910915' date='2019-04-09T06:46:42Z'>
		
From the bug description, all we know is that some Session.run() call is blocked (from the thread blocked in WaitForNotification()) and the session is still active (from the two tf.data background threads).
Without a way to reproduce the problem, there's no way to tell why it's blocked. From previous experience, it's possible that the termination logic in Estimator (with or without StopAtStepHook) has a race condition. One case that I've seen is the past is that there might be an accidental dependency between worker tasks, where when one worker exits cleanly, another worker might start a concurrent Session.run() call that depends on the exited worker, and will block forever waiting for that worker to come back up. However, I would have expected the default worker device filter to prevent it, based on the code here:
tensorflow/tensorflow/python/estimator/run_config.py
Line 567 in 5b0c525
device_filters = ['/job:ps', '/job:worker/task:%d' % self._task_id]
Are you using that device filter? Which tasks (worker or PS) have exited (if any) when you observe the hang?

It workers
		</comment>
		<comment id='5' author='lxl910915' date='2019-04-09T06:46:44Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=26559&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=26559&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>