<bug id='30372' author='novog' open_date='2019-07-03T20:49:47Z' closed_time='2019-07-12T08:21:52Z'>
	<summary>Poor Feature/Example serialization performance</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Enterprise
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): unknown 2.0.0-beta1
Python version: 3.7.2
CUDA/cuDNN version: 10.0/7.5
GPU model and memory: GeForce GTX Titan X, 12GB

Describe the current behavior
The rate of serializing examples is unreasonably slow. In particular, performance is slow for Features that are a sequence of values. I have a dataset with numerous fields, including two that are represented by short, fixed length, 1-dimensional Tensors. When excluding those two features, serialization happens in a slow but manageable amount of time. Adding either of those two features causes it to take many times as long.
Notably, when mapping a Dataset to a function that performs serialization, increasing the number of threads does not significantly impact performance and the CPU remains mostly idle. I do see marked performance improvements and higher CPU usage when parallelizing other mapped functions, so it could be that there is some kind of global bottleneck for this operation.
Describe the expected behavior
Records should be serialized in a reasonable amount of time.
Code to reproduce the issue
The following code serializes 10000 records in about 6s on a particular machine. Note that if I replace the mapped function with one that simply returns a constant, it takes less than 1s to complete, so the serialization is the problem.
&lt;denchmark-code&gt;def make_example_1(*data_list):
    feature_dict = {
        'a':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[0]])),
        'b':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[1]])),
        'c':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[2]])),
        'd':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[3]])),
        'e':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[4]])),
        'f':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[5]])),
        'g':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[6]])),
        'h':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[7]])),
        'i':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[8]])),
        'j':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[9]]))}
    example = tf.train.Example(features=tf.train.Features(feature=feature_dict))
    return example.SerializeToString()

def make_example_1_wrap(*input):
    data = input[0]
    return tf.py_function(
        make_example_1,
        (
            data['a'],data['b'],data['c'],data['d'],data['e'],
            data['f'],data['g'],data['h'],data['i'],data['j']),
        Tout=tf.string)
    
def feature_test_1(num_threads=None):
    source = {
        'a':tf.constant(0),
        'b':tf.constant(1),
        'c':tf.constant(2),
        'd':tf.constant(3),
        'e':tf.constant(4),
        'f':tf.constant(5),
        'g':tf.constant(6),
        'h':tf.constant(7),
        'i':tf.constant(8),
        'j':tf.constant(9)}
    ds = tf.data.Dataset.from_tensors(source).repeat(10000)
    ds = ds.map(make_example_1_wrap, num_threads)
    it = iter(ds)
    for x in it:
        pass
&lt;/denchmark-code&gt;

The following example uses about the same input data size as the previous one, but uses 2 features of length 5 instead of 10 features of length 1. The execution time increases to 17s, highlighting the problem with sequences as Int64Lists.
&lt;denchmark-code&gt;def make_example_2(*data_list):
    feature_dict = {
        'a':tf.train.Feature(int64_list=tf.train.Int64List(value=data_list[0])),
        'b':tf.train.Feature(int64_list=tf.train.Int64List(value=data_list[1]))}
    example = tf.train.Example(features=tf.train.Features(feature=feature_dict))
    return example.SerializeToString()

def make_example_2_wrap(*input):
    data = input[0]
    return tf.py_function(
        make_example_2,
        (data['a'], data['b']),
        Tout=tf.string)

def feature_test_2(num_threads=None):
    x = {
        'a':tf.constant([0,1,2,3,4]),
        'b':tf.constant([5,6,7,8,9])}
    ds = tf.data.Dataset.from_tensors(x).repeat(10000)
    ds = ds.map(make_example_2_wrap, num_threads)
    it = iter(ds)
    for x in it:
        pass
&lt;/denchmark-code&gt;


A possibly related performance issue is mentioned by many users in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/16933&gt;#16933&lt;/denchmark-link&gt;
, although that issue was closed over a year ago due to inactivity.
	</description>
	<comments>
		<comment id='1' author='novog' date='2019-07-04T09:04:33Z'>
		Yes, same issue as mentioned in other issues like &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/16933&gt;#16933&lt;/denchmark-link&gt;
. To write something performant we've used  with  as mentioned briefly eg in the recent &lt;denchmark-link:https://www.tensorflow.org/tutorials/load_data/images&gt;load images&lt;/denchmark-link&gt;
 tutorial. This is quite cumbersome to use compared to Examples and Features.
		</comment>
		<comment id='2' author='novog' date='2019-07-04T12:05:10Z'>
		&lt;denchmark-link:https://github.com/novog&gt;@novog&lt;/denchmark-link&gt;
 ,
Can you please confirm if using the APIs and referring the Tutorial mentioned by &lt;denchmark-link:https://github.com/areeh&gt;@areeh&lt;/denchmark-link&gt;
 has given you expected performance.
		</comment>
		<comment id='3' author='novog' date='2019-07-05T14:30:23Z'>
		OK, thanks. If using Example isn't viable for the moment, then I suppose that will do.
In my case, I have not a single tensor, but a number of tensors corresponding to named features of differing types, so I can't just use tf.io.serialize_tensor. I could serialize them individually, put them in a dictionary, serialize the dictionary (using something like python's json), then pass those strings to TFRecordWriter. Let me know if you have a cleaner solution. Otherwise, I'll try that and post a follow-up on the relative performance.
		</comment>
		<comment id='4' author='novog' date='2019-07-05T17:48:02Z'>
		Yes, it is much faster using tf.io.serialize_tensor manually rather than using Feature/Example. The following code completes in about 3.5s instead of 17s:
&lt;denchmark-code&gt;def make_example_2(*data_list):
    feature_dict = {
        'a':tf.io.serialize_tensor(data_list[0]).numpy(),
        'b':tf.io.serialize_tensor(data_list[1]).numpy()}
    serialized = pickle.dumps(feature_dict)
    return serialized
&lt;/denchmark-code&gt;

Given that mapping to a function that simply returns a constant string takes 2.5s, the time added by serialization decreased from about 14.5s to 1s.
As to the status of this issue, while there is a sort of workaround, I would consider the defect itself to remain: Feature/Example perform unreasonably poorly.
		</comment>
		<comment id='5' author='novog' date='2019-07-10T12:39:36Z'>
		&lt;denchmark-link:https://github.com/novog&gt;@novog&lt;/denchmark-link&gt;
 Yes and I don't know of a way to have behaviour more or less equivalent to / with 
edit: for the record, the use-case I'm thinking of is similar to &lt;denchmark-link:https://github.com/novog&gt;@novog&lt;/denchmark-link&gt;
 with named (potentially heterogeneous) dicts of tensors written, read and processed as a single unit in terms of operations such as shuffling, interleaving reads from shards, etc
		</comment>
		<comment id='6' author='novog' date='2019-07-12T08:21:52Z'>
		I don't think we can meaningfully speed up protocol buffer serialization in TF. I suggest you follow this bug up with one against the maintaners of protocol buffers themselves.
Either that or use, as suggested above, another serialization format.
		</comment>
		<comment id='7' author='novog' date='2019-07-12T08:21:54Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30372&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30372&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='novog' date='2020-06-03T14:35:37Z'>
		I tried &lt;denchmark-link:https://stackoverflow.com/questions/62174662/slow-serialization-of-ndarray-to-tfrecord&gt;both approaches&lt;/denchmark-link&gt;
 and frankly  performed even worse. Ideas?
		</comment>
	</comments>
</bug>