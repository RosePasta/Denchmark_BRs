<bug id='38810' author='Arvindia' open_date='2020-04-22T19:27:36Z' closed_time='2020-07-17T06:25:02Z'>
	<summary>Verbose between epochs in tf version 2.2.0-rc3</summary>
	<description>
Started to receive this today when colab upgraded to 2.2.0-rc3. Yesterday I've trained and tf.version was 2.2.0-rc2.
The model I'm using is created using tf.keras.
Epoch 1/2
2020-04-22 18:49:48.629597: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.
1/2 [==============&gt;...............] - ETA: 0s - loss: 0.8884 - categorical_accuracy: 0.7188

2020-04-22 18:49:48.646518: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1479] CUPTI activity buffer flushed
2020-04-22 18:49:48.646786: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:216]  GpuTracer has collected 133 callback api events and 133 activity events.
2020-04-22 18:49:48.663147: I tensorflow/core/profiler/rpc/client/save_profile.cc:168] Creating directory: /content/drive/My Drive/Colab Notebooks/model_log_dirs_new1/logs_board/train/plugins/profile/2020_04_22_18_49_48
2020-04-22 18:49:48.670211: I tensorflow/core/profiler/rpc/client/save_profile.cc:174] Dumped gzipped tool data for trace.json.gz to /content/drive/My Drive/Colab Notebooks/model_log_dirs_new1/logs_board/train/plugins/profile/2020_04_22_18_49_48/ea98f4633ef6.trace.json.gz
2020-04-22 18:49:48.672098: I tensorflow/core/profiler/utils/event_span.cc:288] Generation of step-events took 0.026 ms
2020-04-22 18:49:48.689016: I tensorflow/python/profiler/internal/profiler_wrapper.cc:87] Creating directory: /content/drive/My Drive/Colab Notebooks/model_log_dirs_new1/logs_board/train/plugins/profile/2020_04_22_18_49_48Dumped tool data for overview_page.pb to /content/drive/My Drive/Colab Notebooks/model_log_dirs_new1/logs_board/train/plugins/profile/2020_04_22_18_49_48/ea98f4633ef6.overview_page.pb
Dumped tool data for input_pipeline.pb to /content/drive/My Drive/Colab Notebooks/model_log_dirs_new1/logs_board/train/plugins/profile/2020_04_22_18_49_48/ea98f4633ef6.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to /content/drive/My Drive/Colab Notebooks/model_log_dirs_new1/logs_board/train/plugins/profile/2020_04_22_18_49_48/ea98f4633ef6.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to /content/drive/My Drive/Colab Notebooks/model_log_dirs_new1/logs_board/train/plugins/profile/2020_04_22_18_49_48/ea98f4633ef6.kernel_stats.pb

2/2 [==============================] - 1s 306ms/step - loss: 0.9273 - categorical_accuracy: 0.6797 - val_loss: 0.7531 - val_categorical_accuracy: 0.7508
Epoch 2/2
2/2 [==============================] - 1s 253ms/step - loss: 0.8796 - categorical_accuracy: 0.7188 - val_loss: 0.6971 - val_categorical_accuracy: 0.7675
Between epochs I'm getting all this mumbo jumbo, previous version never resulted in this.
	</description>
	<comments>
		<comment id='1' author='Arvindia' date='2020-04-23T10:53:18Z'>
		&lt;denchmark-link:https://github.com/Arvindia&gt;@Arvindia&lt;/denchmark-link&gt;

can you please share a simple stand alone code for us to replicate this.
		</comment>
		<comment id='2' author='Arvindia' date='2020-04-23T14:42:41Z'>
		This is my model which worked perfectly in 2.2-rc2.
&lt;denchmark-code&gt;def model_create(self, input_flatten_shape=None, reshape=None):
    input_flatten = input_flatten_shape
    fingerprint = layers.Input(shape=input_flatten_shape,
                               name='fingerprint_input')
    fingerprint_4d = layers.Reshape(reshape)(fingerprint)
    first_conv2d_layer = layers.Conv2D(64,(20,8),
                                      padding='same',
                                      strides=(1,1),
                                      activation='relu')(fingerprint_4d)
    first_dropout = layers.Dropout(self.dropout_rate)(first_conv2d_layer)
    first_pool_layer = layers.MaxPool2D(pool_size=(2,2),
                                       strides=(2,2), 
                                       padding='same')(first_dropout)
    second_conv2d_layer = layers.Conv2D(64, (10,4),
                                       padding='same',
                                       strides=(1,1),
                                       activation='relu')(first_pool_layer)
    second_dropout = layers.Dropout(self.dropout_rate)(second_conv2d_layer)
    second_pool_layer = layers.MaxPool2D(pool_size=(2,2),
                                        strides=(2,2),
                                        padding='same')(second_dropout)
    #flatten = layers.Flatten()(second_pool_layer)
    flatten = layers.Flatten()(second_dropout)
    final_fc = layers.Dense(self.output_units, activation='softmax')(flatten)
    self.model = Model(inputs=fingerprint, outputs=final_fc)
    ConvModel.__model_compile(self, self.model)
    return self.model

def __model_compile(self, model):
    model.compile(optimizer=self.optimizer,
            loss='categorical_crossentropy',
            metrics=['categorical_accuracy'])
    ConvModel.__logs(self)

def __logs(self):
    self.model_ckpt_path = self.logs+"/checkpoints/"
    self.model_weights = self.logs+"/model_weights/weights"
    self.logdir_board = callbacks.TensorBoard(
                                      log_dir=self.logs+"/logs_board", 
                                      histogram_freq=1000, 
                                      update_freq=500,
                                      write_images=False, 
                                      embeddings_freq=0)
    # Save the model after every epoch or some period.
    self.model_ckpt = callbacks.ModelCheckpoint(
                                      self.model_ckpt_path,
                                      monitor='val_loss',
                                      mode='auto',
                                      save_weights_only=False,
                                      save_best_only=False,
                                      save_freq=500) 
def model_train(self, x_train=None, y_train=None, 
                x_val=None, y_val=None,
                epochs=1, batch_size=64):
    #print('model_train:', time.ctime())
    self.model.fit(x=x_train, y=y_train, 
                  validation_data=(x_val, y_val),
                  epochs=epochs, batch_size=batch_size,
                  callbacks=[self.logdir_board, self.model_ckpt],
                  verbose=1)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='Arvindia' date='2020-04-23T15:10:50Z'>
		&lt;denchmark-link:https://github.com/Arvindia&gt;@Arvindia&lt;/denchmark-link&gt;

i ran the code on nightly, please find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/9444377b1b2e33f28c29ef6c80545880/untitled149.ipynb&gt;gist here&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='Arvindia' date='2020-04-23T15:32:03Z'>
		&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
  I'm still getting the same error. Ran using nightly
		</comment>
		<comment id='5' author='Arvindia' date='2020-04-23T17:14:18Z'>
		&lt;denchmark-link:https://github.com/Arvindia&gt;@Arvindia&lt;/denchmark-link&gt;
 the code is not complete if you could provide a fully working standalone colab of the issue it will help faster resolution.
which platform are you running the code on, we fixed profiler to work on Windows in 2.2.0-rc3 which resulted in these messages. These are debug messages to see if the profiling is successful.
		</comment>
		<comment id='6' author='Arvindia' date='2020-04-24T00:10:23Z'>
		&lt;denchmark-link:https://github.com/goldiegadde&gt;@goldiegadde&lt;/denchmark-link&gt;
 Here is the link to my github repository : &lt;denchmark-link:url&gt;https://github.com/Arvindia/Speech-Recognition.git&lt;/denchmark-link&gt;

I'm running in colab using GPU.
Here is all the verbose I'm getting from start:
&lt;denchmark-code&gt;2020-04-23 23:49:16.382747: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
&gt;&gt; Downloading speech_commands_v0.02.tar.gz 100.0%
Successfully downloaded speech_commands_v0.02.tar.gz 2428923189bytes
INFO:tensorflow:Successfully downloaded speech_commands_v0.02.tar.gz (2428923189 bytes)
I0423 23:50:20.806859 140270006437760 download_extract.py:47] Successfully downloaded speech_commands_v0.02.tar.gz (2428923189 bytes)
creating datasets...
no.of wanted words 10
no.of training files:  31228
no.of validation files:  3467
no.of test files:  3851
2020-04-23 23:52:59.078208: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-04-23 23:52:59.140469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-23 23:52:59.141232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2020-04-23 23:52:59.141295: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-04-23 23:52:59.438951: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-04-23 23:52:59.559854: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-04-23 23:52:59.587268: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-04-23 23:52:59.869298: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-04-23 23:52:59.934182: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-04-23 23:53:00.440004: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-04-23 23:53:00.440200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-23 23:53:00.440883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-23 23:53:00.441404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-04-23 23:53:00.485204: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz
2020-04-23 23:53:00.485503: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2f27100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-04-23 23:53:00.485557: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-04-23 23:53:00.657127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-23 23:53:00.658256: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2f26f40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-04-23 23:53:00.658327: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0
2020-04-23 23:53:00.659922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-23 23:53:00.660637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2020-04-23 23:53:00.660700: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-04-23 23:53:00.660751: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-04-23 23:53:00.660773: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-04-23 23:53:00.660792: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-04-23 23:53:00.660818: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-04-23 23:53:00.660844: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-04-23 23:53:00.660863: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-04-23 23:53:00.660940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-23 23:53:00.661877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-23 23:53:00.662412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-04-23 23:53:00.666331: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-04-23 23:53:07.080023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-04-23 23:53:07.080090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2020-04-23 23:53:07.080115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2020-04-23 23:53:07.089191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-23 23:53:07.089919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-04-23 23:53:07.090479: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2020-04-23 23:53:07.090597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -&gt; physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
Creating model ...
2020-04-23 23:53:07.474138: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.
2020-04-23 23:53:07.479072: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1363] Profiler found 1 GPUs
2020-04-23 23:53:07.535285: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcupti.so.10.1
2020-04-23 23:53:07.708516: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1479] CUPTI activity buffer flushed
INFO:tensorflow:Collecting validation data:
I0423 23:53:07.708695 140270006437760 train1.py:152] Collecting validation data:
2020-04-23 23:53:07.892614: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-04-23 23:53:08.811312: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
INFO:tensorflow:Training:
I0423 23:55:10.771466 140270006437760 train1.py:164] Training:
====================================================================================================
run_step-0
====================================================================================================
1
Epoch 1/2
2020-04-23 23:55:15.142494: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-04-23 23:55:19.753216: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.
1/2 [==============&gt;...............] - ETA: 0s - loss: 2.5445 - categorical_accuracy: 0.09382020-04-23 23:55:19.774968: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1479] CUPTI activity buffer flushed
2020-04-23 23:55:19.775444: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:216]  GpuTracer has collected 161 callback api events and 161 activity events.
2020-04-23 23:55:19.802973: I tensorflow/core/profiler/rpc/client/save_profile.cc:168] Creating directory: /content/model_log_dirs/logs_board/train/plugins/profile/2020_04_23_23_55_19
2020-04-23 23:55:19.808442: I tensorflow/core/profiler/rpc/client/save_profile.cc:174] Dumped gzipped tool data for trace.json.gz to /content/model_log_dirs/logs_board/train/plugins/profile/2020_04_23_23_55_19/f34a72ca0adf.trace.json.gz
2020-04-23 23:55:19.816096: I tensorflow/core/profiler/utils/event_span.cc:288] Generation of step-events took 0.031 ms
2020-04-23 23:55:19.817386: I tensorflow/python/profiler/internal/profiler_wrapper.cc:87] Creating directory: /content/model_log_dirs/logs_board/train/plugins/profile/2020_04_23_23_55_19Dumped tool data for overview_page.pb to /content/model_log_dirs/logs_board/train/plugins/profile/2020_04_23_23_55_19/f34a72ca0adf.overview_page.pb
Dumped tool data for input_pipeline.pb to /content/model_log_dirs/logs_board/train/plugins/profile/2020_04_23_23_55_19/f34a72ca0adf.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to /content/model_log_dirs/logs_board/train/plugins/profile/2020_04_23_23_55_19/f34a72ca0adf.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to /content/model_log_dirs/logs_board/train/plugins/profile/2020_04_23_23_55_19/f34a72ca0adf.kernel_stats.pb
2/2 [==============================] - 1s 479ms/step - loss: 5.4786 - categorical_accuracy: 0.0938 - val_loss: 2.5750 - val_categorical_accuracy: 0.1110
Epoch 2/2
2/2 [==============================] - 1s 297ms/step - loss: 2.3521 - categorical_accuracy: 0.1641 - val_loss: 2.3145 - val_categorical_accuracy: 0.1073
2
Epoch 1/2
2020-04-23 23:55:23.755341: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.
1/2 [==============&gt;...............] - ETA: 0s - loss: 2.3077 - categorical_accuracy: 0.15622020-04-23 23:55:23.774772: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1479] CUPTI activity buffer flushed
2020-04-23 23:55:23.774966: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:216]  GpuTracer has collected 131 callback api events and 131 activity events.
2020-04-23 23:55:23.779834: I tensorflow/core/profiler/rpc/client/save_profile.cc:168] Creating directory: /content/model_log_dirs/logs_board/train/plugins/profile/2020_04_23_23_55_23
2020-04-23 23:55:23.783134: I tensorflow/core/profiler/rpc/client/save_profile.cc:174] Dumped gzipped tool data for trace.json.gz to /content/model_log_dirs/logs_board/train/plugins/profile/2020_04_23_23_55_23/f34a72ca0adf.trace.json.gz
2020-04-23 23:55:23.784225: I tensorflow/core/profiler/utils/event_span.cc:288] Generation of step-events took 0.026 ms
2020-04-23 23:55:23.784931: I tensorflow/python/profiler/internal/profiler_wrapper.cc:87] Creating directory: /content/model_log_dirs/logs_board/train/plugins/profile/2020_04_23_23_55_23Dumped tool data for overview_page.pb to /content/model_log_dirs/logs_board/train/plugins/profile/2020_04_23_23_55_23/f34a72ca0adf.overview_page.pb
Dumped tool data for input_pipeline.pb to /content/model_log_dirs/logs_board/train/plugins/profile/2020_04_23_23_55_23/f34a72ca0adf.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to /content/model_log_dirs/logs_board/train/plugins/profile/2020_04_23_23_55_23/f34a72ca0adf.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to /content/model_log_dirs/logs_board/train/plugins/profile/2020_04_23_23_55_23/f34a72ca0adf.kernel_stats.pb
2/2 [==============================] - 1s 323ms/step - loss: 2.3212 - categorical_accuracy: 0.1406 - val_loss: 2.3003 - val_categorical_accuracy: 0.1110
Epoch 2/2
2/2 [==============================] - 1s 265ms/step - loss: 2.2834 - categorical_accuracy: 0.1719 - val_loss: 2.2950 - val_categorical_accuracy: 0.1408
3
Epoch 1/2
2020-04-23 23:55:28.022401: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.
1/2 [==============&gt;...............] - ETA: 0s - loss: 2.2768 - categorical_accuracy: 0.15622020-04-23 23:55:28.040704: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1479] CUPTI activity buffer flushed
2020-04-23 23:55:28.040994: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:216]  GpuTracer has collected 131 callback api events and 131 activity events.
2020-04-23 23:55:28.050350: I tensorflow/core/profiler/rpc/client/save_profile.cc:168] Creating directory: /content/model_log_dirs/logs_board/train/plugins/profile/2020_04_23_23_55_28
2020-04-23 23:55:28.054894: I tensorflow/core/profiler/rpc/client/save_profile.cc:174] Dumped gzipped tool data for trace.json.gz to /content/model_log_dirs/logs_board/train/plugins/profile/2020_04_23_23_55_28/f34a72ca0adf.trace.json.gz
2020-04-23 23:55:28.056706: I tensorflow/core/profiler/utils/event_span.cc:288] Generation of step-events took 0.038 ms
2020-04-23 23:55:28.057694: I tensorflow/python/profiler/internal/profiler_wrapper.cc:87] Creating directory: /content/model_log_dirs/logs_board/train/plugins/profile/2020_04_23_23_55_28Dumped tool data for overview_page.pb to /content/model_log_dirs/logs_board/train/plugins/profile/2020_04_23_23_55_28/f34a72ca0adf.overview_page.pb
Dumped tool data for input_pipeline.pb to /content/model_log_dirs/logs_board/train/plugins/profile/2020_04_23_23_55_28/f34a72ca0adf.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to /content/model_log_dirs/logs_board/train/plugins/profile/2020_04_23_23_55_28/f34a72ca0adf.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to /content/model_log_dirs/logs_board/train/plugins/profile/2020_04_23_23_55_28/f34a72ca0adf.kernel_stats.pb
2/2 [==============================] - 1s 336ms/step - loss: 2.2701 - categorical_accuracy: 0.1484 - val_loss: 2.2907 - val_categorical_accuracy: 0.1315
Epoch 2/2
2/2 [==============================] - 1s 276ms/step - loss: 2.2100 - categorical_accuracy: 0.1875 - val_loss: 2.2971 - val_categorical_accuracy: 0.1292
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='Arvindia' date='2020-04-24T22:21:25Z'>
		Please use ``` around large code blocks so that they look readable (see edit of above comment)
Also, can you please try controlling the logging level? It seems somehow you are printing information logs too, not only warnings and errors.
		</comment>
		<comment id='8' author='Arvindia' date='2020-04-25T06:44:52Z'>
		&lt;denchmark-link:https://github.com/mihaimaruseac&gt;@mihaimaruseac&lt;/denchmark-link&gt;
 I'm not printing any logs. It worked perfectly in tensorflow 2.2.0-rc2 without all those verbose between  and prior to start of the .
It was as shown below
&lt;denchmark-code&gt;2020-04-23 23:49:16.382747: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
&gt;&gt; Downloading speech_commands_v0.02.tar.gz 100.0%
Successfully downloaded speech_commands_v0.02.tar.gz 2428923189bytes
INFO:tensorflow:Successfully downloaded speech_commands_v0.02.tar.gz (2428923189 bytes)
I0423 23:50:20.806859 140270006437760 download_extract.py:47] Successfully downloaded speech_commands_v0.02.tar.gz (2428923189 bytes)
creating datasets...
no.of wanted words 10
no.of training files:  31228
no.of validation files:  3467
no.of test files:  3851
Creating model ...
INFO:tensorflow:Collecting validation data:
I0423 23:53:07.708695 140270006437760 train1.py:152] Collecting validation data:
INFO:tensorflow:Training:
I0423 23:55:10.771466 140270006437760 train1.py:164] Training:
====================================================================================================
run_step-0
====================================================================================================
1
Epoch 1/2
1/2 [==============================] - 1s 479ms/step - loss: 5.4786 - categorical_accuracy: 0.0938 - val_loss: 2.5750 - val_categorical_accuracy: 0.1110
Epoch 2/2
2/2 [==============================] - 1s 297ms/step - loss: 2.3521 - categorical_accuracy: 0.1641 - val_loss: 2.3145 - val_categorical_accuracy: 0.1073
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='Arvindia' date='2020-06-02T23:50:38Z'>
		I am having the same issue. These statements follow profile dumps in the form of new folders in the tensorboard logdir (logdir/run.../train/plugins/profile/).
I tried changing the TensorBoard callback parameter 'profile_batch' to different values (including 0) and it had no effect. Removing the callback altogether fixes the excessive logging and dumps, but this is obviously not a practical solution.
Note that I am using v2.2.0.
		</comment>
		<comment id='10' author='Arvindia' date='2020-06-04T00:08:55Z'>
		Another note is that my models are running in graph mode (not eager). This may not be compatible with the TensorBoard profiler, though I should still be able to disable it with profile_batch=0.
		</comment>
		<comment id='11' author='Arvindia' date='2020-06-18T22:18:29Z'>
		This commit (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/7b43b3ce08035b6c502b1aa4caa23ba59e4710f2&gt;7b43b3c&lt;/denchmark-link&gt;
) should have fixed &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/38810#issue-605005604&gt;#38810 (comment)&lt;/denchmark-link&gt;

In 2.2.0, it should only profile once for a model.fit. So there shouldn't have many verbose logs in between epochs.
&lt;denchmark-link:https://github.com/ghannum&gt;@ghannum&lt;/denchmark-link&gt;
, do you have a replicate for your issue? profile_batch=0 should disable profiling in graph mode.
		</comment>
		<comment id='12' author='Arvindia' date='2020-06-18T22:39:59Z'>
		&lt;denchmark-link:https://github.com/ghannum&gt;@ghannum&lt;/denchmark-link&gt;
 would it also be possible for you to try TF nightly and see if that will fix your issue?
		</comment>
		<comment id='13' author='Arvindia' date='2020-06-29T17:00:20Z'>
		&lt;denchmark-link:https://github.com/qiuminxu&gt;@qiuminxu&lt;/denchmark-link&gt;
 I'd love to, but my GPUs are busy training a long job. &lt;denchmark-link:https://github.com/Arvindia&gt;@Arvindia&lt;/denchmark-link&gt;
 - would you be able to test this out?
		</comment>
		<comment id='14' author='Arvindia' date='2020-06-30T06:19:14Z'>
		
@qiuminxu I'd love to, but my GPUs are busy training a long job. @Arvindia - would you be able to test this out?

Same error is cropping up no change
		</comment>
		<comment id='15' author='Arvindia' date='2020-07-07T14:18:13Z'>
		Setting profile_batch=0 does not work when using graph mode, somehow the callback keeps profiling each batch. You can use the following workaround:
tb_cb = tf.keras.callbacks.TensorBoard(log_dir='logs', profile_batch=0)

def noop():
    pass

tb_cb._enable_trace = noop
		</comment>
		<comment id='16' author='Arvindia' date='2020-07-07T15:52:26Z'>
		&lt;denchmark-link:https://github.com/Arvindia&gt;@Arvindia&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Arvindia/Speech-Recognition.git&gt;https://github.com/Arvindia/Speech-Recognition.git&lt;/denchmark-link&gt;
 is not accessible. Is it possible to share a fully working colab/gist for us to replicate the issue?
		</comment>
		<comment id='17' author='Arvindia' date='2020-07-17T06:25:02Z'>
		&lt;denchmark-link:https://github.com/qiuminxu&gt;@qiuminxu&lt;/denchmark-link&gt;
 setting  solved the problem. I'm running in eager mode.  Thanks everyone.
		</comment>
		<comment id='18' author='Arvindia' date='2020-07-17T06:25:04Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38810&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38810&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='19' author='Arvindia' date='2020-08-12T23:47:49Z'>
		this issue is not resolved. profile_batch=0 does not achieve the desired effect in graph mode.
		</comment>
		<comment id='20' author='Arvindia' date='2020-08-13T15:53:03Z'>
		&lt;denchmark-link:https://github.com/liob&gt;@liob&lt;/denchmark-link&gt;
 We couldn't reproduce your issue. That would be very helpful if you can provide a minimum reproducible example for us to debug the issue.
		</comment>
	</comments>
</bug>