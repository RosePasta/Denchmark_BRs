<bug id='19270' author='gilmoright' open_date='2018-05-14T14:17:44Z' closed_time='2018-11-07T22:52:12Z'>
	<summary>Different dynamic rnn output depending on batch_size</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, own code
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 6.7 and Windows 10
TensorFlow installed from (source or binary):  by pip install
Bazel version:  N/A
TensorFlow installed from (source or binary):  by pip install
TensorFlow version (use command below): ('v1.4.0-rc0-21-g1e25994', '1.4.0-rc1')
on CentOS ; 1.8.0 on Windows
Python version: 2.7 on CentOS ; 3 on Windows
CUDA/cuDNN version: CUDA 8.0/cuDNN 6 on CentOS; CUDA 9.0/cuDNN 7.1 on Windows
GPU model and memory: NVidia Tesla K80 on CentOS ; GeForce GTX 1050 Ti 4GB on Windows
Exact command to reproduce: Example below, tested in jupyter-notebook from anaconda 5.1 (i think so)
ipykernel==4.8.2
ipython==5.6.0
jupyter==1.0.0
jupyter-client==5.2.3
jupyter-console==5.2.0
jupyter-core==4.4.0
jupyter-tensorboard==0.1.6

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Context:  I'm trying to create a syntactic parser with NN classifier which defines state of next parsing step. Batch size is dynamic and equals to parsing steps count during training, but on prediction i feed into NN only 1 state per parsing step, so batch size=1. When i found that i lost some prediction accuracy i started to dig and that's what i found.
Problem: hidden states of dynamic LSTM  are a bit different when batch size is 1 and &gt;1. The difference between them is small, about 0.0000001, but since i have several LSTM in NN, it affects the output of network. And interesting that if batch size is 2, 3 or more, the outputs are equal, but they are not if batch size is 1. And last one, it's ok with input with small dimensions, like [batch_size, 4, 4], but not when i have [batch size, 4, &gt;10]
I wrote some test example to represent this. It worked for me on 2 systems.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf

config = tf.ConfigProto(allow_soft_placement=True)
config.gpu_options.allow_growth = True
sess = tf.InteractiveSession(config=config)

model_config = {}
model_config["n_hidden"]=10
model_config["lstmStacks"]=2
wordsCount = 4
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;gatherOut = tf.placeholder(shape=(None,None,model_config["n_hidden"]), dtype=tf.float32)
inp_l1_length = tf.placeholder(shape=(None, ), dtype=tf.int32,name="inp_l1_length")

cell = tf.contrib.rnn.LSTMCell(model_config["n_hidden"])
lstm_layer, lstm_states = tf.nn.dynamic_rnn(cell, gatherOut, sequence_length=inp_l1_length, dtype=tf.float32)    
listOut = lstm_states[1]

sess.run(tf.global_variables_initializer())
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;inpArr = np.random.uniform(high=1,low=0,size=(1,wordsCount, model_config["n_hidden"]))

res1 = sess.run(listOut, feed_dict= {
    gatherOut: inpArr,
    inp_l1_length: [1]
})

res2 = sess.run(listOut, feed_dict= {
    gatherOut: np.tile(inpArr,(2,1,1)),
    inp_l1_length: [1,1]
})

res3 = sess.run(listOut, feed_dict= {
    gatherOut: np.tile(inpArr,(3,1,1)),
    inp_l1_length: [1,1,1]
})
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;(res1[0]==res2[0]).all()  # False
(res2[:2]==res3[:2]).all()  # True
(res2[-1]==res3[-1]).all()  # True
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='gilmoright' date='2018-05-15T00:59:42Z'>
		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
Bazel version
Exact command to reproduce
		</comment>
		<comment id='2' author='gilmoright' date='2018-05-16T13:15:07Z'>
		UPD
Example code was tested in jupyter-notebook.
Also one of my colleague tried this code, and he got all True using model_config["n_hidden"]=10, but when he tried model_config["n_hidden"]=256, sometimes (res1[0]==res2[0]).all() was True, sometimes False, same with (res2[-1]==res3[-1]).all()
		</comment>
		<comment id='3' author='gilmoright' date='2018-06-18T13:01:37Z'>
		UPD.
Issue still exists
I'm not sure if it's  actually an issue of tensorflow, maybe it's some sort of error in rounding off or something, and I just shouldn't use batches with size=1.
Tried it on windows without jupyter notebook
Enviroment: Python 3.6.0 |Anaconda 4.3.1 (64-bit)| (default, Dec 23 2016, 11:57:41) [MSC v.1900 64 bit (AMD64)] on win32
tf.version'1.8.0'
Have got same results, First and second elements of batches with size 2 and 3 are equal, but first element of batch with size 1 and batches with size &gt;1 are not
res1[0]==res2[0]).all() # False
res2[:2]==res3[:2]).all()  # True
res2[-1]==res3[-1]).all()  # True
		</comment>
		<comment id='4' author='gilmoright' date='2018-10-31T12:43:33Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
: It has been 134 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='5' author='gilmoright' date='2018-11-07T22:52:12Z'>
		When performing floating point operations, you only get guarantees up to machine precision.  You cannot check for equality using "=="; in fact, the machine precision for float32 calculations is:
&lt;denchmark-code&gt;np.finfo(np.float32).eps
1.1920929e-07
&lt;/denchmark-code&gt;

which matches your errors pretty well.
try this:
&lt;denchmark-code&gt;a = tf.Variable(initial_value=np.random.randn(512, 1024).astype(np.float32))
b = tf.Variable(initial_value=np.random.randn(10, 512).astype(np.float32))
ba = tf.matmul(b, a)
b0 = b[0][tf.newaxis, ...]

version1 = tf.matmul(b0, a)
version2 = tf.matmul(b, a)[0]

session = tf.Session()
session.run(tf.global_variables_initializer())
values = session.run((version1, version2))
&lt;/denchmark-code&gt;

in perfect precision, you will get the same values.  in practice values[0] and values[1] are off by about 10 * eps or 1e-6:
&lt;denchmark-code&gt;In [14]: values[0] - values[1]
Out[14]: 
array([[ 9.5367432e-07, -6.6757202e-06,  1.9073486e-05, ...,
        -9.5367432e-06, -9.5367432e-06,  4.7683716e-07]], dtype=float32)
&lt;/denchmark-code&gt;

The underlying reasoning behind this is that cuda's matmul kernels only guarantee accuracy up to machine epsilon.  They use different algorithms (accumulation order) depending on the shape of the inputs, and sometimes even for the same input shapes they'll perform accumulation differently.
See &lt;denchmark-link:https://en.wikipedia.org/wiki/Machine_epsilon&gt;machine epsilon&lt;/denchmark-link&gt;
 for more details.
		</comment>
	</comments>
</bug>