<bug id='42382' author='Otakarlp' open_date='2020-08-15T04:38:36Z' closed_time='2020-08-17T17:07:36Z'>
	<summary>Sparsemax &amp; tf.Cumsum</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
TensorFlow installed from (source or binary): 2.3
TensorFlow version (or github SHA if from source):

Provide the text output from tflite_convert
&lt;denchmark-code&gt;# INFO:tensorflow:Assets written to: C:\Users\The-author\AppData\Local\Temp\tmp1ty7mqo0\assets
INFO:tensorflow:Assets written to: C:\Users\The-author\AppData\Local\Temp\tmp1ty7mqo0\assets
---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
~\AppData\Roaming\Python\Python37\site-packages\tensorflow\lite\python\convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    198                                                  debug_info_str,
--&gt; 199                                                  enable_mlir_converter)
    200       return model_str

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\lite\python\wrap_toco.py in wrapped_toco_convert(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
     37       debug_info_str,
---&gt; 38       enable_mlir_converter)
     39 

Exception: C:\Users\The-author\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\ops\math_ops.py:3736:0: error: 'tf.Cumsum' op is neither a custom op nor a flex op
C:\Users\The-author\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\util\dispatch.py:201:0: note: called from
c:\users\The-author\appdata\local\programs\python\python37\lib\site-packages\tensorflow_addons\activations\sparsemax.py:105:0: note: called from
c:\users\The-author\appdata\local\programs\python\python37\lib\site-packages\tensorflow_addons\activations\sparsemax.py:47:0: note: called from
C:\Users\The-author\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\layers\convolutional.py:269:0: note: called from
C:\Users\The-author\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\base_layer.py:985:0: note: called from
C:\Users\The-author\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\functional.py:508:0: note: called from
C:\Users\The-author\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\functional.py:386:0: note: called from
C:\Users\The-author\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\base_layer.py:985:0: note: called from
C:\Users\The-author\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\saving\saving_utils.py:134:0: note: called from
C:\Users\The-author\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\ops\math_ops.py:3736:0: note: see current operation: %12 = "tf.Cumsum"(%values, %cst_1) {device = "", exclusive = false, reverse = false} : (tensor&lt;?x?xf32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;?x?xf32&gt;
&lt;unknown&gt;:0: error: failed while converting: 'main': Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):
	tf.Cumsum {device = "", exclusive = false, reverse = false}
&lt;unknown&gt;:0: note: see current operation: "func"() ( {
^bb0(%arg0: tensor&lt;?x256x256x3xf32&gt;):  // no predecessors
  %cst = "std.constant"() {value = dense&lt;0.000000e+00&gt; : tensor&lt;f32&gt;} : () -&gt; tensor&lt;f32&gt;
  %cst_0 = "std.constant"() {value = dense&lt;0x7FC00000&gt; : tensor&lt;f32&gt;} : () -&gt; tensor&lt;f32&gt;
  %cst_1 = "std.constant"() {value = dense&lt;-1&gt; : tensor&lt;i32&gt;} : () -&gt; tensor&lt;i32&gt;
  %cst_2 = "std.constant"() {value = dense&lt;0&gt; : tensor&lt;i32&gt;} : () -&gt; tensor&lt;i32&gt;
  %cst_3 = "std.constant"() {value = dense&lt;1&gt; : tensor&lt;i32&gt;} : () -&gt; tensor&lt;i32&gt;
  %cst_4 = "std.constant"() {value = dense&lt;1.000000e+00&gt; : tensor&lt;f32&gt;} : () -&gt; tensor&lt;f32&gt;
  %cst_5 = "std.constant"() {value = dense&lt;"0xB2936CBE88F3B63E70265B3ED3BFC5BEDB44C63E4365F43E241B69BE14CBD0BDE4A013BE81638FBEAECCF8BE1FF1ADBEA9CB8ABE019BBD3E6ED7753DF441F53D01D5143FA511A6BE5F2A97BEE01FCFBEEBC3E23C99C39F3E6C510E3FD29ACD3E5E5F94BE26B3C2BE5BE612BF09CAB6BD97D9A83E0076BFBEE1950C3FEC438ABD41F56EBE33D299BECCFCEEBE70D91A3E3C6FD6BD7440483E123E17BDA734813EB9A9103E98BFB3BC6FA1BFBE00B5BE3E0E84AABE07AE0BBE6385B63D5B9B08BE7506263EDC6FA93ECF488EBC046AAFBE32B567BC9B356CBEFC4C803E12A3973EF17A333ECE1E1C3E4F85C7BDFA8AA6BEF72768BC53141F3B0E1D213EAD543BBE37F004BD2720973E078C2D3EEC62913E2D23863D2BF081BE4B3FC5BD4BEABFBDD257DA3ED93FA2BE20C04BBE3A34583EB16BD43E36CCCBBE318360BE3007823E26DF61BED0CA423E866A8FBE435DAC3DFD61DF3D29CD723E67DECDBE79B99EBD3870183CC82A863E8BB9C83D57CFE1BE4A8D99BE42BD263ECBAA2CBDFBF3EA3E809D4EBE2E7E04BFB6DB2BBE6D670BBE7642B5BE1A9EA0BD7C9C3ABE7074623E51C034BE10F3093E460FB1BE7F2E99BE"&gt; : tensor&lt;4x3x3x3xf32&gt;} : () -&gt; tensor&lt;4x3x3x3xf32&gt;
  %cst_6 = "std.constant"() {value = dense&lt;0.000000e+00&gt; : tensor&lt;4xf32&gt;} : () -&gt; tensor&lt;4xf32&gt;
  %cst_7 = "std.constant"() {value = dense&lt;-1&gt; : tensor&lt;1xi32&gt;} : () -&gt; tensor&lt;1xi32&gt;
  %cst_8 = "std.constant"() {value = dense&lt;0&gt; : tensor&lt;1xi32&gt;} : () -&gt; tensor&lt;1xi32&gt;
  %cst_9 = "std.constant"() {value = dense&lt;1&gt; : tensor&lt;1xi32&gt;} : () -&gt; tensor&lt;1xi32&gt;
  %cst_10 = "std.constant"() {value = dense&lt;[0, -1]&gt; : tensor&lt;2xi32&gt;} : () -&gt; tensor&lt;2xi32&gt;
  %cst_11 = "std.constant"() {value = dense&lt;0&gt; : tensor&lt;2xi32&gt;} : () -&gt; tensor&lt;2xi32&gt;
  %cst_12 = "std.constant"() {value = dense&lt;1&gt; : tensor&lt;2xi32&gt;} : () -&gt; tensor&lt;2xi32&gt;
  %0 = "tfl.conv_2d"(%arg0, %cst_5, %cst_6) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = "NONE", padding = "SAME", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor&lt;?x256x256x3xf32&gt;, tensor&lt;4x3x3x3xf32&gt;, tensor&lt;4xf32&gt;) -&gt; tensor&lt;?x256x256x4xf32&gt;
  %1 = "tfl.shape"(%0) : (tensor&lt;?x256x256x4xf32&gt;) -&gt; tensor&lt;4xi32&gt;
  %2 = "tfl.strided_slice"(%1, %cst_8, %cst_7, %cst_9) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 0 : i32} : (tensor&lt;4xi32&gt;, tensor&lt;1xi32&gt;, tensor&lt;1xi32&gt;, tensor&lt;1xi32&gt;) -&gt; tensor&lt;3xi32&gt;
  %3 = "tfl.reduce_prod"(%2, %cst_8) {keep_dims = false} : (tensor&lt;3xi32&gt;, tensor&lt;1xi32&gt;) -&gt; tensor&lt;i32&gt;
  %4 = "tfl.range"(%cst_2, %3, %cst_3) : (tensor&lt;i32&gt;, tensor&lt;i32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;?xi32&gt;
  %5 = "tfl.strided_slice"(%1, %cst_7, %cst_8, %cst_9) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor&lt;4xi32&gt;, tensor&lt;1xi32&gt;, tensor&lt;1xi32&gt;, tensor&lt;1xi32&gt;) -&gt; tensor&lt;i32&gt;
  %6 = "tfl.cast"(%5) : (tensor&lt;i32&gt;) -&gt; tensor&lt;f32&gt;
  %7 = "tfl.add"(%6, %cst_4) {fused_activation_function = "NONE"} : (tensor&lt;f32&gt;, tensor&lt;f32&gt;) -&gt; tensor&lt;f32&gt;
  %8 = "tfl.range"(%cst_4, %7, %cst_4) : (tensor&lt;f32&gt;, tensor&lt;f32&gt;, tensor&lt;f32&gt;) -&gt; tensor&lt;?xf32&gt;
  %9 = "tfl.pack"(%3, %5) {axis = 0 : i32, values_count = 2 : i32} : (tensor&lt;i32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;2xi32&gt;
  %10 = "tfl.fill"(%9, %cst_0) : (tensor&lt;2xi32&gt;, tensor&lt;f32&gt;) -&gt; tensor&lt;?x?xf32&gt;
  %11 = "tfl.reshape"(%0, %9) : (tensor&lt;?x256x256x4xf32&gt;, tensor&lt;2xi32&gt;) -&gt; tensor&lt;?x?xf32&gt;
  %values, %indices = "tfl.topk_v2"(%11, %5) : (tensor&lt;?x?xf32&gt;, tensor&lt;i32&gt;) -&gt; (tensor&lt;?x?xf32&gt;, tensor&lt;?x?xi32&gt;)
  %12 = "tf.Cumsum"(%values, %cst_1) {device = "", exclusive = false, reverse = false} : (tensor&lt;?x?xf32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;?x?xf32&gt;
  %13 = "tfl.strided_slice"(%12, %cst_10, %cst_11, %cst_12) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor&lt;?x?xf32&gt;, tensor&lt;2xi32&gt;, tensor&lt;2xi32&gt;, tensor&lt;2xi32&gt;) -&gt; tensor&lt;?xf32&gt;
  %14 = "tf.IsNan"(%13) {device = ""} : (tensor&lt;?xf32&gt;) -&gt; tensor&lt;?xi1&gt;
  %15 = "tfl.mul"(%8, %values) {fused_activation_function = "NONE"} : (tensor&lt;?xf32&gt;, tensor&lt;?x?xf32&gt;) -&gt; tensor&lt;?x?xf32&gt;
  %16 = "tfl.add"(%15, %cst_4) {fused_activation_function = "NONE"} : (tensor&lt;?x?xf32&gt;, tensor&lt;f32&gt;) -&gt; tensor&lt;?x?xf32&gt;
  %17 = "tfl.greater"(%16, %12) : (tensor&lt;?x?xf32&gt;, tensor&lt;?x?xf32&gt;) -&gt; tensor&lt;?x?xi1&gt;
  %18 = "tfl.cast"(%17) : (tensor&lt;?x?xi1&gt;) -&gt; tensor&lt;?x?xi32&gt;
  %19 = "tfl.sum"(%18, %cst_1) {keep_dims = false} : (tensor&lt;?x?xi32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;?xi32&gt;
  %20 = "tfl.cast"(%19) : (tensor&lt;?xi32&gt;) -&gt; tensor&lt;?xf32&gt;
  %21 = "tfl.equal"(%19, %cst_2) : (tensor&lt;?xi32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;?xi1&gt;
  %22 = "tfl.logical_or"(%21, %14) : (tensor&lt;?xi1&gt;, tensor&lt;?xi1&gt;) -&gt; tensor&lt;?xi1&gt;
  %23 = "tfl.expand_dims"(%22, %cst_1) : (tensor&lt;?xi1&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;?x1xi1&gt;
  %24 = "tfl.maximum"(%19, %cst_3) : (tensor&lt;?xi32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;?xi32&gt;
  %25 = "tfl.sub"(%24, %cst_3) {fused_activation_function = "NONE"} : (tensor&lt;?xi32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;?xi32&gt;
  %26 = "tfl.reshape"(%25, %cst_7) : (tensor&lt;?xi32&gt;, tensor&lt;1xi32&gt;) -&gt; tensor&lt;?xi32&gt;
  %27 = "tfl.pack"(%4, %26) {axis = 1 : i32, values_count = 2 : i32} : (tensor&lt;?xi32&gt;, tensor&lt;?xi32&gt;) -&gt; tensor&lt;?x2xi32&gt;
  %28 = "tfl.gather_nd"(%12, %27) : (tensor&lt;?x?xf32&gt;, tensor&lt;?x2xi32&gt;) -&gt; tensor&lt;?xf32&gt;
  %29 = "tfl.sub"(%28, %cst_4) {fused_activation_function = "NONE"} : (tensor&lt;?xf32&gt;, tensor&lt;f32&gt;) -&gt; tensor&lt;?xf32&gt;
  %30 = "tfl.div"(%29, %20) {fused_activation_function = "NONE"} : (tensor&lt;?xf32&gt;, tensor&lt;?xf32&gt;) -&gt; tensor&lt;?xf32&gt;
  %31 = "tfl.expand_dims"(%30, %cst_1) : (tensor&lt;?xf32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;?x1xf32&gt;
  %32 = "tfl.sub"(%11, %31) {fused_activation_function = "NONE"} : (tensor&lt;?x?xf32&gt;, tensor&lt;?x1xf32&gt;) -&gt; tensor&lt;?x?xf32&gt;
  %33 = "tfl.maximum"(%32, %cst) : (tensor&lt;?x?xf32&gt;, tensor&lt;f32&gt;) -&gt; tensor&lt;?x?xf32&gt;
  %34 = "tfl.select_v2"(%23, %10, %33) : (tensor&lt;?x1xi1&gt;, tensor&lt;?x?xf32&gt;, tensor&lt;?x?xf32&gt;) -&gt; tensor&lt;?x?xf32&gt;
  %35 = "tfl.reshape"(%34, %1) : (tensor&lt;?x?xf32&gt;, tensor&lt;4xi32&gt;) -&gt; tensor&lt;?x?x?x?xf32&gt;
  "std.return"(%35) : (tensor&lt;?x?x?x?xf32&gt;) -&gt; ()
}) {sym_name = "main", tf.entry_function = {control_outputs = "", inputs = "input_8", outputs = "Identity"}, type = (tensor&lt;?x256x256x3xf32&gt;) -&gt; tensor&lt;?x?x?x?xf32&gt;} : () -&gt; ()


During handling of the above exception, another exception occurred:

ConverterError                            Traceback (most recent call last)
&lt;ipython-input-31-e001f5ecf3f4&gt; in &lt;module&gt;
      4 converter.experimental_new_converter = True
      5 
----&gt; 6 tflite_model = converter.convert()

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\lite\python\lite.py in convert(self)
    829 
    830     return super(TFLiteKerasModelConverterV2,
--&gt; 831                  self).convert(graph_def, input_tensors, output_tensors)
    832 
    833 

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\lite\python\lite.py in convert(self, graph_def, input_tensors, output_tensors)
    631         input_tensors=input_tensors,
    632         output_tensors=output_tensors,
--&gt; 633         **converter_kwargs)
    634 
    635     calibrate_and_quantize, flags = quant_mode.quantizer_flags(

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\lite\python\convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)
    572       input_data.SerializeToString(),
    573       debug_info_str=debug_info_str,
--&gt; 574       enable_mlir_converter=enable_mlir_converter)
    575   return data
    576 

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\lite\python\convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    200       return model_str
    201     except Exception as e:
--&gt; 202       raise ConverterError(str(e))
    203 
    204   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:

ConverterError: C:\Users\The-author\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\ops\math_ops.py:3736:0: error: 'tf.Cumsum' op is neither a custom op nor a flex op
C:\Users\The-author\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\util\dispatch.py:201:0: note: called from
c:\users\The-author\appdata\local\programs\python\python37\lib\site-packages\tensorflow_addons\activations\sparsemax.py:105:0: note: called from
c:\users\The-author\appdata\local\programs\python\python37\lib\site-packages\tensorflow_addons\activations\sparsemax.py:47:0: note: called from
C:\Users\The-author\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\layers\convolutional.py:269:0: note: called from
C:\Users\The-author\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\base_layer.py:985:0: note: called from
C:\Users\The-author\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\functional.py:508:0: note: called from
C:\Users\The-author\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\functional.py:386:0: note: called from
C:\Users\The-author\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\base_layer.py:985:0: note: called from
C:\Users\The-author\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\saving\saving_utils.py:134:0: note: called from
C:\Users\The-author\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\ops\math_ops.py:3736:0: note: see current operation: %12 = "tf.Cumsum"(%values, %cst_1) {device = "", exclusive = false, reverse = false} : (tensor&lt;?x?xf32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;?x?xf32&gt;
&lt;unknown&gt;:0: error: failed while converting: 'main': Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):
	tf.Cumsum {device = "", exclusive = false, reverse = false}
&lt;unknown&gt;:0: note: see current operation: "func"() ( {
^bb0(%arg0: tensor&lt;?x256x256x3xf32&gt;):  // no predecessors
  %cst = "std.constant"() {value = dense&lt;0.000000e+00&gt; : tensor&lt;f32&gt;} : () -&gt; tensor&lt;f32&gt;
  %cst_0 = "std.constant"() {value = dense&lt;0x7FC00000&gt; : tensor&lt;f32&gt;} : () -&gt; tensor&lt;f32&gt;
  %cst_1 = "std.constant"() {value = dense&lt;-1&gt; : tensor&lt;i32&gt;} : () -&gt; tensor&lt;i32&gt;
  %cst_2 = "std.constant"() {value = dense&lt;0&gt; : tensor&lt;i32&gt;} : () -&gt; tensor&lt;i32&gt;
  %cst_3 = "std.constant"() {value = dense&lt;1&gt; : tensor&lt;i32&gt;} : () -&gt; tensor&lt;i32&gt;
  %cst_4 = "std.constant"() {value = dense&lt;1.000000e+00&gt; : tensor&lt;f32&gt;} : () -&gt; tensor&lt;f32&gt;
  %cst_5 = "std.constant"() {value = dense&lt;"0xB2936CBE88F3B63E70265B3ED3BFC5BEDB44C63E4365F43E241B69BE14CBD0BDE4A013BE81638FBEAECCF8BE1FF1ADBEA9CB8ABE019BBD3E6ED7753DF441F53D01D5143FA511A6BE5F2A97BEE01FCFBEEBC3E23C99C39F3E6C510E3FD29ACD3E5E5F94BE26B3C2BE5BE612BF09CAB6BD97D9A83E0076BFBEE1950C3FEC438ABD41F56EBE33D299BECCFCEEBE70D91A3E3C6FD6BD7440483E123E17BDA734813EB9A9103E98BFB3BC6FA1BFBE00B5BE3E0E84AABE07AE0BBE6385B63D5B9B08BE7506263EDC6FA93ECF488EBC046AAFBE32B567BC9B356CBEFC4C803E12A3973EF17A333ECE1E1C3E4F85C7BDFA8AA6BEF72768BC53141F3B0E1D213EAD543BBE37F004BD2720973E078C2D3EEC62913E2D23863D2BF081BE4B3FC5BD4BEABFBDD257DA3ED93FA2BE20C04BBE3A34583EB16BD43E36CCCBBE318360BE3007823E26DF61BED0CA423E866A8FBE435DAC3DFD61DF3D29CD723E67DECDBE79B99EBD3870183CC82A863E8BB9C83D57CFE1BE4A8D99BE42BD263ECBAA2CBDFBF3EA3E809D4EBE2E7E04BFB6DB2BBE6D670BBE7642B5BE1A9EA0BD7C9C3ABE7074623E51C034BE10F3093E460FB1BE7F2E99BE"&gt; : tensor&lt;4x3x3x3xf32&gt;} : () -&gt; tensor&lt;4x3x3x3xf32&gt;
  %cst_6 = "std.constant"() {value = dense&lt;0.000000e+00&gt; : tensor&lt;4xf32&gt;} : () -&gt; tensor&lt;4xf32&gt;
  %cst_7 = "std.constant"() {value = dense&lt;-1&gt; : tensor&lt;1xi32&gt;} : () -&gt; tensor&lt;1xi32&gt;
  %cst_8 = "std.constant"() {value = dense&lt;0&gt; : tensor&lt;1xi32&gt;} : () -&gt; tensor&lt;1xi32&gt;
  %cst_9 = "std.constant"() {value = dense&lt;1&gt; : tensor&lt;1xi32&gt;} : () -&gt; tensor&lt;1xi32&gt;
  %cst_10 = "std.constant"() {value = dense&lt;[0, -1]&gt; : tensor&lt;2xi32&gt;} : () -&gt; tensor&lt;2xi32&gt;
  %cst_11 = "std.constant"() {value = dense&lt;0&gt; : tensor&lt;2xi32&gt;} : () -&gt; tensor&lt;2xi32&gt;
  %cst_12 = "std.constant"() {value = dense&lt;1&gt; : tensor&lt;2xi32&gt;} : () -&gt; tensor&lt;2xi32&gt;
  %0 = "tfl.conv_2d"(%arg0, %cst_5, %cst_6) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = "NONE", padding = "SAME", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor&lt;?x256x256x3xf32&gt;, tensor&lt;4x3x3x3xf32&gt;, tensor&lt;4xf32&gt;) -&gt; tensor&lt;?x256x256x4xf32&gt;
  %1 = "tfl.shape"(%0) : (tensor&lt;?x256x256x4xf32&gt;) -&gt; tensor&lt;4xi32&gt;
  %2 = "tfl.strided_slice"(%1, %cst_8, %cst_7, %cst_9) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 0 : i32} : (tensor&lt;4xi32&gt;, tensor&lt;1xi32&gt;, tensor&lt;1xi32&gt;, tensor&lt;1xi32&gt;) -&gt; tensor&lt;3xi32&gt;
  %3 = "tfl.reduce_prod"(%2, %cst_8) {keep_dims = false} : (tensor&lt;3xi32&gt;, tensor&lt;1xi32&gt;) -&gt; tensor&lt;i32&gt;
  %4 = "tfl.range"(%cst_2, %3, %cst_3) : (tensor&lt;i32&gt;, tensor&lt;i32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;?xi32&gt;
  %5 = "tfl.strided_slice"(%1, %cst_7, %cst_8, %cst_9) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor&lt;4xi32&gt;, tensor&lt;1xi32&gt;, tensor&lt;1xi32&gt;, tensor&lt;1xi32&gt;) -&gt; tensor&lt;i32&gt;
  %6 = "tfl.cast"(%5) : (tensor&lt;i32&gt;) -&gt; tensor&lt;f32&gt;
  %7 = "tfl.add"(%6, %cst_4) {fused_activation_function = "NONE"} : (tensor&lt;f32&gt;, tensor&lt;f32&gt;) -&gt; tensor&lt;f32&gt;
  %8 = "tfl.range"(%cst_4, %7, %cst_4) : (tensor&lt;f32&gt;, tensor&lt;f32&gt;, tensor&lt;f32&gt;) -&gt; tensor&lt;?xf32&gt;
  %9 = "tfl.pack"(%3, %5) {axis = 0 : i32, values_count = 2 : i32} : (tensor&lt;i32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;2xi32&gt;
  %10 = "tfl.fill"(%9, %cst_0) : (tensor&lt;2xi32&gt;, tensor&lt;f32&gt;) -&gt; tensor&lt;?x?xf32&gt;
  %11 = "tfl.reshape"(%0, %9) : (tensor&lt;?x256x256x4xf32&gt;, tensor&lt;2xi32&gt;) -&gt; tensor&lt;?x?xf32&gt;
  %values, %indices = "tfl.topk_v2"(%11, %5) : (tensor&lt;?x?xf32&gt;, tensor&lt;i32&gt;) -&gt; (tensor&lt;?x?xf32&gt;, tensor&lt;?x?xi32&gt;)
  %12 = "tf.Cumsum"(%values, %cst_1) {device = "", exclusive = false, reverse = false} : (tensor&lt;?x?xf32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;?x?xf32&gt;
  %13 = "tfl.strided_slice"(%12, %cst_10, %cst_11, %cst_12) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor&lt;?x?xf32&gt;, tensor&lt;2xi32&gt;, tensor&lt;2xi32&gt;, tensor&lt;2xi32&gt;) -&gt; tensor&lt;?xf32&gt;
  %14 = "tf.IsNan"(%13) {device = ""} : (tensor&lt;?xf32&gt;) -&gt; tensor&lt;?xi1&gt;
  %15 = "tfl.mul"(%8, %values) {fused_activation_function = "NONE"} : (tensor&lt;?xf32&gt;, tensor&lt;?x?xf32&gt;) -&gt; tensor&lt;?x?xf32&gt;
  %16 = "tfl.add"(%15, %cst_4) {fused_activation_function = "NONE"} : (tensor&lt;?x?xf32&gt;, tensor&lt;f32&gt;) -&gt; tensor&lt;?x?xf32&gt;
  %17 = "tfl.greater"(%16, %12) : (tensor&lt;?x?xf32&gt;, tensor&lt;?x?xf32&gt;) -&gt; tensor&lt;?x?xi1&gt;
  %18 = "tfl.cast"(%17) : (tensor&lt;?x?xi1&gt;) -&gt; tensor&lt;?x?xi32&gt;
  %19 = "tfl.sum"(%18, %cst_1) {keep_dims = false} : (tensor&lt;?x?xi32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;?xi32&gt;
  %20 = "tfl.cast"(%19) : (tensor&lt;?xi32&gt;) -&gt; tensor&lt;?xf32&gt;
  %21 = "tfl.equal"(%19, %cst_2) : (tensor&lt;?xi32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;?xi1&gt;
  %22 = "tfl.logical_or"(%21, %14) : (tensor&lt;?xi1&gt;, tensor&lt;?xi1&gt;) -&gt; tensor&lt;?xi1&gt;
  %23 = "tfl.expand_dims"(%22, %cst_1) : (tensor&lt;?xi1&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;?x1xi1&gt;
  %24 = "tfl.maximum"(%19, %cst_3) : (tensor&lt;?xi32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;?xi32&gt;
  %25 = "tfl.sub"(%24, %cst_3) {fused_activation_function = "NONE"} : (tensor&lt;?xi32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;?xi32&gt;
  %26 = "tfl.reshape"(%25, %cst_7) : (tensor&lt;?xi32&gt;, tensor&lt;1xi32&gt;) -&gt; tensor&lt;?xi32&gt;
  %27 = "tfl.pack"(%4, %26) {axis = 1 : i32, values_count = 2 : i32} : (tensor&lt;?xi32&gt;, tensor&lt;?xi32&gt;) -&gt; tensor&lt;?x2xi32&gt;
  %28 = "tfl.gather_nd"(%12, %27) : (tensor&lt;?x?xf32&gt;, tensor&lt;?x2xi32&gt;) -&gt; tensor&lt;?xf32&gt;
  %29 = "tfl.sub"(%28, %cst_4) {fused_activation_function = "NONE"} : (tensor&lt;?xf32&gt;, tensor&lt;f32&gt;) -&gt; tensor&lt;?xf32&gt;
  %30 = "tfl.div"(%29, %20) {fused_activation_function = "NONE"} : (tensor&lt;?xf32&gt;, tensor&lt;?xf32&gt;) -&gt; tensor&lt;?xf32&gt;
  %31 = "tfl.expand_dims"(%30, %cst_1) : (tensor&lt;?xf32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;?x1xf32&gt;
  %32 = "tfl.sub"(%11, %31) {fused_activation_function = "NONE"} : (tensor&lt;?x?xf32&gt;, tensor&lt;?x1xf32&gt;) -&gt; tensor&lt;?x?xf32&gt;
  %33 = "tfl.maximum"(%32, %cst) : (tensor&lt;?x?xf32&gt;, tensor&lt;f32&gt;) -&gt; tensor&lt;?x?xf32&gt;
  %34 = "tfl.select_v2"(%23, %10, %33) : (tensor&lt;?x1xi1&gt;, tensor&lt;?x?xf32&gt;, tensor&lt;?x?xf32&gt;) -&gt; tensor&lt;?x?xf32&gt;
  %35 = "tfl.reshape"(%34, %1) : (tensor&lt;?x?xf32&gt;, tensor&lt;4xi32&gt;) -&gt; tensor&lt;?x?x?x?xf32&gt;
  "std.return"(%35) : (tensor&lt;?x?x?x?xf32&gt;) -&gt; ()
}) {sym_name = "main", tf.entry_function = {control_outputs = "", inputs = "input_8", outputs = "Identity"}, type = (tensor&lt;?x256x256x3xf32&gt;) -&gt; tensor&lt;?x?x?x?xf32&gt;} : () -&gt; ()
&lt;/denchmark-code&gt;

Standalone code to reproduce the issue
inputs = Input((256,256,3))
outputs = tf.keras.layers.Conv2D(activation=tfa.activations.sparsemax,filters=4, kernel_size=(3,3), strides=1, padding="same",kernel_initializer=tf.keras.initializers.HeNormal())(inputs)
model = Model(inputs=[inputs], outputs=[outputs])
model.compile(tf.keras.optimizers.Nadam(name="Nadam"),
loss=tfa.losses.SparsemaxLoss(from_logits=True),
metrics=tf.keras.metrics.MeanIoU(4,name='mean_iou'))
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()
It can be "fixed" ( if we call that fixing ) by switching from the " Sparsemax " to the " Softmax " activation, but of course the softmax doesn't give the same results as the sparsemax.
I also tried the " tfa.layers.Sparsemax " but i guess that they have the same root implementation since i got an equivalent error.
**Thanks for your attention **
	</description>
	<comments>
		<comment id='1' author='Otakarlp' date='2020-08-17T10:03:20Z'>
		&lt;denchmark-link:https://github.com/Otakarlp&gt;@Otakarlp&lt;/denchmark-link&gt;
,
I was able to reproduce the issue with &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/99bafff4461fc35606440ccbc78387df/42382.ipynb#scrollTo=FbW9MLa3zysb&gt;TF v2.3&lt;/denchmark-link&gt;
.
However, the issue seems to be resolved with the latest &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/fff90ece097490efab0938f121a86e4f/42382-tf-nightly.ipynb&gt;TF-nightly&lt;/denchmark-link&gt;
. I was able to convert the model without any issues. Please find the attached gist. Thanks!
		</comment>
		<comment id='2' author='Otakarlp' date='2020-08-17T17:07:33Z'>
		&lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;

I was able to make the tflite model and run inference with the TF-nightly version " 2.4.0-dev20200813 " ( latest version on windows i suppose ) as you suggested it, and that worked perfectly.
Thanks for your help, have a great day !
		</comment>
		<comment id='3' author='Otakarlp' date='2020-08-17T17:07:38Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42382&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42382&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>