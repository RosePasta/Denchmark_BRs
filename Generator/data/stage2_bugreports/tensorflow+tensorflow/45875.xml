<bug id='45875' author='nehasoni3' open_date='2020-12-19T13:36:26Z' closed_time='2021-01-08T17:51:02Z'>
	<summary>How to create Flex-free model in TFLite</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): Installed via PIP
TensorFlow version (or github SHA if from source): Tensorflow 2.3.1

Command used to run the converter or code if youâ€™re using the Python API
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
tflite_model = converter.convert()
open("converted_model.tflite", "wb").write(tflite_model)
NOTE: Model is successfully converted.
Failure details
If the conversion is successful, but the generated model is wrong,
state what is wrong:

TFlite model is working fine at CPU Linux 16.04 and RaspberryPi. But it gives BUS ERROR at edge device. Details.

The original model is in PyTorch. I have used PyTorch-ONNX-Tensorflow-TFLite approach to convert the model in TFLite.
I am sharing graphs of ONNX and TFLite model. &lt;denchmark-link:https://drive.google.com/drive/folders/1jzLZKus-9Dey2vgcMWjspBZfCRJbLCWW?usp=sharing&gt;Link&lt;/denchmark-link&gt;

Geeks says

Using Flex op requires to build TF op kernels which is difficult to support various targets.
If the required Flex op is only FlexMul, I think you might be able to create Flex-free model with some refactoring.

How can do this "create Flex-free "?
	</description>
	<comments>
		<comment id='1' author='nehasoni3' date='2020-12-21T03:08:39Z'>
		Could you share your FlexMul op details? Which type being calculated? Which input shape being calculated?
TFLite Mul can support F32, I32, QI8, QUI8, and QI16 inputs and support up to 4 dims if broadcasting is needed or the same input shapes regardless of dimensions.
If you can modify your model to meet the above Mul op constraints, the FlexMul op requirement will be gone.
		</comment>
		<comment id='2' author='nehasoni3' date='2020-12-22T07:38:54Z'>
		
Could you share your FlexMul op details? Which type being calculated? Which input shape being calculated?
TFLite Mul can support F32, I32, QI8, QUI8, and QI16 inputs and support up to 4 dims if broadcasting is needed or the same input shapes regardless of dimensions.
If you can modify your model to meet the above Mul op constraints, the FlexMul op requirement will be gone.

I have already shared graphs.&lt;denchmark-link:https://drive.google.com/drive/folders/1jzLZKus-9Dey2vgcMWjspBZfCRJbLCWW&gt;Link&lt;/denchmark-link&gt;

Input to model is = [1,500,120]

If you can modify your model to meet the above Mul op constraints, the FlexMul op requirement will be gone.
HOW?

		</comment>
		<comment id='3' author='nehasoni3' date='2020-12-22T09:10:46Z'>
		Could you share your Flex-enabled tflite model file to us instead of the graph image if possible? And it would be helpful to understand why the FlexMul op is introduced if you can share some warning logs like the below example. You can see the following warnings in tf-nightly version when you are converting the model.
&lt;denchmark-code&gt;I1120 16:16:16.002125 2530115 flatbuffer_export.cc:...] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following flex op(s):

Flex ops: tf.MaxPool
Details:
        tf.MaxPool {data_format = "NCHW", device = "", explicit_paddings = [], ksize = [1, 1, 2, 2], padding = "VALID", strides = [1, 1, 2, 2]}
        tf.MaxPool {data_format = "NCHW", device = "", explicit_paddings = [], ksize = [1, 1, 3, 3], padding = "VALID", strides = [1, 1, 1, 1]}
&lt;/denchmark-code&gt;

Based on the above information, you can find which one is the corresponding operator in the source graph for the FlexMul op in the TensorFlow Lite and you can find a way to replace the source operator with the supported operator case by modifying the source graph.
		</comment>
		<comment id='4' author='nehasoni3' date='2020-12-22T12:38:20Z'>
		&lt;denchmark-link:https://github.com/nehasoni3&gt;@nehasoni3&lt;/denchmark-link&gt;
 can you share the screenshot of the tabs that appear when you click Mul op and FlexMul op in the netron app?
And can you share the input tensor type of Mul op and input shape information of Mul op?
		</comment>
		<comment id='5' author='nehasoni3' date='2020-12-22T12:44:33Z'>
		
@nehasoni3 can you share the screenshot of the tabs that appear when you click Mul op and FlexMul op in the netron app?
And can you share the input tensor type of Mul op and input shape information of Mul op?

&lt;denchmark-link:https://user-images.githubusercontent.com/31642462/102889931-894e1d80-4481-11eb-8099-a845c3e75ba0.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/31642462/102890151-fb266700-4481-11eb-85db-f424f667c7ae.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='nehasoni3' date='2020-12-22T12:50:35Z'>
		If you can click the + buttons at the netron, which are located at each input and each output, there is more information regarding input types and shapes. It would be better to get information from both graphs, one for Mul op and one for FlexMul op.
		</comment>
		<comment id='7' author='nehasoni3' date='2020-12-22T13:24:41Z'>
		If I use this only,

This works but every op is of Flex.
&lt;denchmark-link:https://user-images.githubusercontent.com/31642462/102893714-0e3c3580-4488-11eb-9151-f5278bd0bf66.png&gt;&lt;/denchmark-link&gt;

But If I use,
converter.target_spec.supported_ops = [ tf.lite.OpsSet.TFLITE_BUILTINS] tflite_model = converter.convert()
I got this log-
`---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
212                                                  debug_info_str,
--&gt; 213                                                  enable_mlir_converter)
214       return model_str
4 frames
Exception: :0: error: loc(callsite(callsite("Mul@__inference___call___2921" at "PartitionedCall@__inference_signature_wrapper_2928") at "PartitionedCall")): 'tf.Mul' op is neither a custom op nor a flex op
:0: note: loc("PartitionedCall"): called from
:0: error: loc(callsite(callsite("add_2@__inference___call___2921" at "PartitionedCall@__inference_signature_wrapper_2928") at "PartitionedCall")): 'tf.AddV2' op is neither a custom op nor a flex op`
		</comment>
		<comment id='8' author='nehasoni3' date='2020-12-22T13:43:27Z'>
		Yes, this is an expected behavior. The current model input for conversion can not be covered by the TFLite builtin op set. So we need flex ops or custom ops.
		</comment>
		<comment id='9' author='nehasoni3' date='2020-12-23T02:18:33Z'>
		If you can fix the input shapes in the model during conversion, those Flex op requirements might be gone.
		</comment>
		<comment id='10' author='nehasoni3' date='2020-12-23T07:49:09Z'>
		
If you can fix the input shapes in the model during conversion, those Flex op requirements might be gone.

How can I fix the input shapes. Do I need to change its dtype or what?
		</comment>
		<comment id='11' author='nehasoni3' date='2020-12-24T01:46:11Z'>
		I am assuming that the model is using float32 type, which is a typical choice among ML model. If it is true, the Flex requirement will be introduced because of the input shapes. The input shape can be fixed by (1) modifying the original source graph or (2) using TFLite converter V1 API's input_shapes argument &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/compat/v1/lite/TFLiteConverter&gt;https://www.tensorflow.org/api_docs/python/tf/compat/v1/lite/TFLiteConverter&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='nehasoni3' date='2020-12-27T09:57:55Z'>
		&lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
 Now, FlexMUL is removed but FlexAddV2 op is still there. Will you guide me to remove that FlexAddV2 op?
		</comment>
		<comment id='13' author='nehasoni3' date='2020-12-28T04:00:37Z'>
		The same strategy can be applied to FlexAddV2 ops. Fixing the input shape of the FlexAddV2 ops can help replacing them with TFLite Add ops.
		</comment>
		<comment id='14' author='nehasoni3' date='2021-01-08T17:51:04Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45875&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45875&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>