<bug id='33837' author='alexbooth' open_date='2019-10-30T04:01:17Z' closed_time='2020-10-09T20:21:51Z'>
	<summary>Using intermediate layer outputs in custom loss function causes CUDA_ERROR_OUT_OF_MEMORY</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 19.10
TensorFlow installed from (source or binary):
*Binary
TensorFlow version (use command below):
2.1.0-dev20191027 (same results in 2.0)
Python version:
3.7.5rc1
CUDA/cuDNN version:
10.1
GPU model and memory:
2080 TI 12gb + driver 430.50

Describe the current behavior
I'm trying to add a kl divergence regularizer which relies on 2 intermediate layers of the encoder model. This custom loss in Keras model returns a tensor instead of a scalar when using outputs of intermediate layers. This appears to be adding operators to the graph at each iteration until I get a CUDA out of memory error.
&lt;denchmark-code&gt;__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 64, 64, 3)]  0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 32, 32, 32)   1568        input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 16, 16, 32)   16416       conv2d[0][0]                     
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 8, 8, 32)     16416       conv2d_1[0][0]                   
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 4, 4, 32)     16416       conv2d_2[0][0]                   
__________________________________________________________________________________________________
flatten (Flatten)               (None, 512)          0           conv2d_3[0][0]                   
__________________________________________________________________________________________________
dense (Dense)                   (None, 256)          131328      flatten[0][0]                    
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 256)          65792       dense[0][0]                      
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 32)           8224        dense_1[0][0]                    
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 32)           8224        dense_1[0][0]                    
__________________________________________________________________________________________________
reparameterize (Reparameterize) (None, 32)           0           dense_2[0][0]                    
                                                                 dense_3[0][0]                    
__________________________________________________________________________________________________
model_1 (Model)                 (None, 64, 64, 3)    256611      reparameterize[0][0]             
==================================================================================================
Total params: 520,995
Trainable params: 520,995
Non-trainable params: 0
__________________________________________________________________________________________________
2019-10-29 20:54:07.009002: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-10-29 20:54:07.911330: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
 |----------------------------------------| 0.0%  recon: 163.58 kl: 0.0 capacity (nats): 0.0 Epoch: 1/10 Loss: Tensor("add_2:0", shape=(), dtype=float32) SAMPLING FRAME 1
 |----------------------------------------| 0.1%  recon: 183.64 kl: 0.0 capacity (nats): 0.0 Epoch: 1/10 Loss: Tensor("add_6:0", shape=(), dtype=float32) SAMPLING FRAME 2
 |----------------------------------------| 0.3%  recon: 171.7 kl: 0.0 capacity (nats): 0.01 Epoch: 1/10 Loss: Tensor("add_18:0", shape=(), dtype=float32) SAMPLING FRAME 3
 |----------------------------------------| 0.6%  recon: 180.35 kl: 0.0 capacity (nats): 0.02 Epoch: 1/10 Loss: Tensor("add_38:0", shape=(), dtype=float32) SAMPLING FRAME 4
 |----------------------------------------| 1.1%  recon: 179.76 kl: 0.0 capacity (nats): 0.03 Epoch: 1/10 Loss: Tensor("add_66:0", shape=(), dtype=float32) SAMPLING FRAME 5
 |█---------------------------------------| 1.7%  recon: 182.14 kl: 0.0 capacity (nats): 0.04 Epoch: 1/10 Loss: Tensor("add_102:0", shape=(), dtype=float32) SAMPLING FRAME 6
 |█---------------------------------------| 2.4%  recon: 175.29 kl: 0.0 capacity (nats): 0.05 Epoch: 1/10 Loss: Tensor("add_142:0", shape=(), dtype=float32)2019-10-29 20:54:13.957278: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 1.18G (1263714304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
 |█---------------------------------------| 2.5%  recon: 182.22 kl: 0.0 capacity (nats): 0.05 Epoch: 1/10 Loss: Tensor("add_146:0", shape=(), dtype=float32) SAMPLING FRAME 7
 |█---------------------------------------| 2.7%  recon: 168.09 kl: 0.0 capacity (nats): 0.06 Epoch: 1/10 Loss: Tensor("add_158:0", shape=(), dtype=float32)2019-10-29 20:54:14.610518: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 120.52M (126371328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-10-29 20:54:14.610906: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 120.52M (126371328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
&lt;/denchmark-code&gt;

Notice that "loss: " prints something like Tensor("add_146:0", shape=(), dtype=float32) when I call train_on_batch. Previously this was just a scalar.
Describe the expected behavior
I expect kl_divergence(X, X_pred) in my code to return a scalar, and no out of memory errors.
Code to reproduce the issue
git clone https://github.com/alexbooth/Beta-VAE-Tensorflow-2.0.git  
cd Beta-VAE-Tensorflow-2.0  
python3 train.py --batch_size=512
Model + Loss
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as np
import tensorflow as tf

from tensorflow.keras import Model
from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense
from tensorflow.keras.layers import Flatten, Reshape, Input
from tensorflow.keras.optimizers import Adam


def Conv(n_filters, filter_width, strides=2, activation="relu", name=None):
    return Conv2D(n_filters, filter_width, 
                  strides=strides, padding="same", activation=activation, name=name)


def Deconv(n_filters, filter_width, strides=2, activation="relu", name=None):
    return Conv2DTranspose(n_filters, filter_width, 
                  strides=strides, padding="same", activation=activation, name=name)


class Reparameterize(tf.keras.layers.Layer):
    """
    Custom layer.
     
    Reparameterization trick, sample random latent vectors Z from 
    the latent Gaussian distribution which has the following parameters 

    mean = Z_mu
    std = exp(0.5 * Z_logvar)
    """
    def call(self, inputs):
        Z_mu, Z_logvar = inputs
        epsilon = tf.random.normal(tf.shape(Z_mu))
        sigma = tf.math.exp(0.5 * Z_logvar)
        return Z_mu + sigma * epsilon


class BetaVAE:
    def __init__(self, input_shape, latent_dim=32, loss_type="mse", learning_rate=0.0005):
        self.latent_dim = latent_dim
        self.C = 0
        self.gamma = 100

        channels = input_shape[2]

        # create encoder
        encoder_input = Input(shape=input_shape)
        X = Conv(32, 4)(encoder_input)
        X = Conv(32, 4)(X)
        X = Conv(32, 4)(X)
        X = Conv(32, 4)(X)
        X = Flatten()(X)
        X = Dense(256, activation="relu")(X)
        X = Dense(256,  activation="relu")(X)
        Z_mu = Dense(self.latent_dim)(X)
        Z_logvar = Dense(self.latent_dim, activation="relu")(X)
        Z = Reparameterize()([Z_mu, Z_logvar])

        # create decoder
        output_activation = "sigmoid" if channels == 1 else None
        decoder_input = Input(shape=(self.latent_dim,))
        X = Dense(256,  activation="relu")(decoder_input)
        X = Dense(256,  activation="relu")(X)
        X = Dense(512,  activation="relu")(X)
        X = Reshape((4, 4, 32))(X)
        X = Deconv(32, 4)(X)
        X = Deconv(32, 4)(X)
        X = Deconv(32, 4)(X)
        decoder_output = Deconv(channels, 4, activation=output_activation)(X)

        # define vae losses
        def reconstruction_loss(X, X_pred):
            if loss_type == "bce":
                bce = tf.losses.BinaryCrossentropy() 
                return bce(X, X_pred) * np.prod(input_shape)
            elif loss_type == "mse":
                mse = tf.losses.MeanSquaredError()
                return mse(X, X_pred) * np.prod(input_shape)
            else:
                raise ValueError("Unknown reconstruction loss type. Try 'bce' or 'mse'")

        def kl_divergence(X, X_pred):
            self.C += (1/1440) # TODO use correct scalar
            self.C = min(self.C, 35) # TODO make variable
            kl = -0.5 * tf.reduce_mean(1 + Z_logvar - Z_mu**2 - tf.math.exp(Z_logvar))
            return self.gamma * tf.math.abs(kl - self.C)

        def loss(X, X_pred):
            return reconstruction_loss(X, X_pred) + kl_divergence(X, X_pred)

        # create models
        self.encoder = Model(encoder_input, [Z_mu, Z_logvar, Z])
        self.decoder = Model(decoder_input, decoder_output)
        self.vae = Model(encoder_input, self.decoder(Z))
        self.vae.compile(optimizer='adam', loss=loss, metrics=[reconstruction_loss, kl_divergence])

    def predict(self, inputs, mode=None):
        if mode == "encode":
            _, _, self.Z = self.encoder.predict(inputs)
            return self.Z
        if mode == "decode":
            return self.decoder.predict(inputs)
        if mode == None:
            return self.vae.predict(inputs) 
        raise ValueError("Unsupported mode during call to model.") 
	</description>
	<comments>
		<comment id='1' author='alexbooth' date='2019-10-31T06:55:10Z'>
		&lt;denchmark-link:https://github.com/alexbooth&gt;@alexbooth&lt;/denchmark-link&gt;
,
This issue shows when the GPU resources are already used by another resources or process. Please do check if any parallel python process is being used?.
And also, check which programs take memory on your GPU using the  command in a terminal. Let us know how it progresses. Thanks!
		</comment>
		<comment id='2' author='alexbooth' date='2019-11-02T03:02:07Z'>
		Hi &lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 , thanks for the reply. Here is the output of  at two points during execution.
Just after training begins:
&lt;denchmark-code&gt;Fri Nov  1 19:48:56 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 430.50       Driver Version: 430.50       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce RTX 208...  Off  | 00000000:01:00.0  On |                  N/A |
| 14%   55C    P0    73W / 250W |   3834MiB / 10997MiB |     18%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1242      G   /usr/lib/xorg/Xorg                            36MiB |
|    0      1737      G   /usr/lib/xorg/Xorg                           341MiB |
|    0      1946      G   /usr/bin/gnome-shell                         224MiB |
|    0      2344      G   ...quest-channel-token=2209957141405069853   238MiB |
|    0      4386      G   /home/alex/.steam/ubuntu12_32/steam           80MiB |
|    0      4413      G   ./steamwebhelper                              34MiB |
|    0      4452      G   ...quest-channel-token=8933237957317349096    71MiB |
|    0     31754      C   python3                                     2741MiB |
+-----------------------------------------------------------------------------+
&lt;/denchmark-code&gt;

Just before the script crashes:
&lt;denchmark-code&gt;Fri Nov  1 19:49:01 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 430.50       Driver Version: 430.50       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce RTX 208...  Off  | 00000000:01:00.0  On |                  N/A |
| 14%   55C    P0    53W / 250W |   9978MiB / 10997MiB |     15%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1242      G   /usr/lib/xorg/Xorg                            36MiB |
|    0      1737      G   /usr/lib/xorg/Xorg                           341MiB |
|    0      1946      G   /usr/bin/gnome-shell                         224MiB |
|    0      2344      G   ...quest-channel-token=2209957141405069853   238MiB |
|    0      4386      G   /home/alex/.steam/ubuntu12_32/steam           80MiB |
|    0      4413      G   ./steamwebhelper                              34MiB |
|    0      4452      G   ...quest-channel-token=8933237957317349096    71MiB |
|    0     31754      C   python3                                     8885MiB |
+-----------------------------------------------------------------------------+
&lt;/denchmark-code&gt;

So it is indeed an out of memory error. It seems like new ops are being added to the graph until the oom error.
The only parallel process I think could be tf.data.Dataset which I had set to prefetch with an autotuned buffer size and also generate data in a while True loop. But I replaced this with a generic serial batch function and still had the same issue
self.dataset = tf.data.Dataset.from_generator(self.generate, tf.float32, output_shapes=self.train_input_shape)
self.dataset = self.dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
self.dataset = self.dataset.batch(batch_size=self.batch_size)
self.dataset_iterator = iter(self.dataset)
Also wanted to share something interesting. When I compile the model with the flag experimental_run_tf_function=False I get the expected output from my custom loss function (scalar value instead of tensor). But my hyper parameter (self.C) in kl_divergence(X, X_pred) does not update.
Usage of flag inspired from this tfp issue: &lt;denchmark-link:https://github.com/tensorflow/probability/issues/519&gt;tensorflow/probability#519&lt;/denchmark-link&gt;

Output from that:
&lt;denchmark-code&gt;__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 64, 64, 3)]  0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 32, 32, 32)   1568        input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 16, 16, 32)   16416       conv2d[0][0]                     
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 8, 8, 32)     16416       conv2d_1[0][0]                   
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 4, 4, 32)     16416       conv2d_2[0][0]                   
__________________________________________________________________________________________________
flatten (Flatten)               (None, 512)          0           conv2d_3[0][0]                   
__________________________________________________________________________________________________
dense (Dense)                   (None, 256)          131328      flatten[0][0]                    
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 256)          65792       dense[0][0]                      
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 32)           8224        dense_1[0][0]                    
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 32)           8224        dense_1[0][0]                    
__________________________________________________________________________________________________
reparameterize (Reparameterize) (None, 32)           0           dense_2[0][0]                    
                                                                 dense_3[0][0]                    
__________________________________________________________________________________________________
model_1 (Model)                 (None, 64, 64, 3)    256611      reparameterize[0][0]             
==================================================================================================
Total params: 520,995
Trainable params: 520,995
Non-trainable params: 0
__________________________________________________________________________________________________
2019-11-01 19:38:24.141589: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-11-01 19:38:24.298499: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
 |----------------------------------------| 0.0%  recon: 174.61 kl: 0.06 capacity (nats): 0.0 Epoch: 1/10 Loss: 174.74608 SAMPLING FRAME 1
 |----------------------------------------| 0.1%  recon: 172.69 kl: 0.0 capacity (nats): 0.0 Epoch: 1/10 Loss: 172.76025 SAMPLING FRAME 2
 |----------------------------------------| 0.3%  recon: 169.32 kl: 0.03 capacity (nats): 0.0 Epoch: 1/10 Loss: 169.3567 SAMPLING FRAME 3
 |----------------------------------------| 0.6%  recon: 165.3 kl: 0.06 capacity (nats): 0.0 Epoch: 1/10 Loss: 165.30199 SAMPLING FRAME 4
 |----------------------------------------| 1.1%  recon: 163.67 kl: 0.09 capacity (nats): 0.0 Epoch: 1/10 Loss: 163.6917 SAMPLING FRAME 5
 |█---------------------------------------| 1.7%  recon: 153.0 kl: 0.41 capacity (nats): 0.0 Epoch: 1/10 Loss: 153.34195 SAMPLING FRAME 6
 |█---------------------------------------| 2.5%  recon: 157.61 kl: 0.95 capacity (nats): 0.0 Epoch: 1/10 Loss: 158.49252 SAMPLING FRAME 7
 |█---------------------------------------| 3.4%  recon: 142.18 kl: 4.65 capacity (nats): 0.0 Epoch: 1/10 Loss: 146.75726 SAMPLING FRAME 8
 |██--------------------------------------| 4.4%  recon: 120.83 kl: 13.64 capacity (nats): 0.0 Epoch: 1/10 Loss: 134.39842 SAMPLING FRAME 9
 |██--------------------------------------| 5.6%  recon: 106.34 kl: 18.35 capacity (nats): 0.0 Epoch: 1/10 Loss: 124.62601 SAMPLING FRAME 10
 |███-------------------------------------| 6.9%  recon: 100.26 kl: 16.07 capacity (nats): 0.0 Epoch: 1/10 Loss: 116.25942 SAMPLING FRAME 11
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='alexbooth' date='2019-11-04T11:23:18Z'>
		Issue is replicating with Tf 2.0.
Please see the colab gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/1d01d8742c17caaca5de509e82de3263/untitled232.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='4' author='alexbooth' date='2019-11-05T05:19:54Z'>
		Thanks. Please let me know if you need any more info on my end.
		</comment>
		<comment id='5' author='alexbooth' date='2019-12-07T00:00:40Z'>
		Any updates?
		</comment>
		<comment id='6' author='alexbooth' date='2020-07-08T14:57:21Z'>
		Hello! This issue feels like have the same root cause &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/41200&gt;#41200&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='alexbooth' date='2020-09-29T20:20:08Z'>
		&lt;denchmark-link:https://github.com/alexbooth&gt;@alexbooth&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/RomanGirin&gt;@RomanGirin&lt;/denchmark-link&gt;
 I tried to reproduce this issue using latest version of tf-nightly and ran into this issue. You can find the gist &lt;denchmark-link:https://colab.research.google.com/gist/gowthamkpr/278d021807df0870d9d2d6911a90fefc/untitled317.ipynb&gt;here&lt;/denchmark-link&gt;
. Can you please try reproducing it and let us know if the issue  still persists. Thanks!
		</comment>
		<comment id='8' author='alexbooth' date='2020-09-30T08:51:57Z'>
		&lt;denchmark-link:https://github.com/gowthamkpr&gt;@gowthamkpr&lt;/denchmark-link&gt;
 I used simplified example to model the issue. And it seems it's design issue. I described fix for my simple example in this comment &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/41200#issuecomment-695779647&gt;#41200 (comment)&lt;/denchmark-link&gt;

Please, take a look it may apply to this issue also
		</comment>
		<comment id='9' author='alexbooth' date='2020-10-07T21:18:36Z'>
		CUDA_ERROR_OUT_OF_MEMORY error is not seen in the latest 2.3 and the tf-nightly version. Also with reference to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/41200#issuecomment-695779647&gt;#41200 (comment)&lt;/denchmark-link&gt;
 Please let us know if it is ok to close this issue.
		</comment>
		<comment id='10' author='alexbooth' date='2020-10-07T21:18:38Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33837&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33837&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='alexbooth' date='2020-10-09T20:21:51Z'>
		Closing the issue. Please feel free to reopen it..
		</comment>
		<comment id='12' author='alexbooth' date='2020-10-09T20:21:53Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33837&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33837&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>