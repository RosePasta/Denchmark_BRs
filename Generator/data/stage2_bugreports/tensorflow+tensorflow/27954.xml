<bug id='27954' author='KichangKim' open_date='2019-04-18T12:32:07Z' closed_time='2019-04-29T03:56:30Z'>
	<summary>Tensorflow 2.0 has missing features related to checkpointing Dataset</summary>
	<description>
System information

TensorFlow version: 2.0-alpha.0
Doc Link:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/experimental/make_saveable_from_iterator

Describe the documentation issue
With 2.0-alpha.0, tf.data.Dataset does not have any make_xx_iterator() methods, so I can't save/restore iterate state while training by using tf.data.experimental.make_saveable_from_iterator().
How to save/restore internal state of Dataset?
	</description>
	<comments>
		<comment id='1' author='KichangKim' date='2019-04-18T12:51:43Z'>
		It seems that iterator from Dataset.iter() can be serialized with tf.train.Checkpoint(). Even though it is not described in document.
But if I used tf.py_function in Dataset chain, it throws error "Saving stateful functions is not supported yet". It seems that old tf.py_func operator has stateful flag, but tf.py_function doesn't have.
		</comment>
		<comment id='2' author='KichangKim' date='2019-04-26T22:28:55Z'>
		Checkpointing of stateful functions is not supported.
As far as I can tell from looking at the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/script_ops.py&gt;implementation&lt;/denchmark-link&gt;
 of  and .  is unconditionally stateless while  is unconditionally stateful. So the error you are describing does not match the expected behavior of the code.
If you would like further help, please provide a reproducible example.
		</comment>
		<comment id='3' author='KichangKim' date='2019-04-27T00:37:34Z'>
		Here is reproducible sample.
&lt;denchmark-code&gt;import tensorflow as tf

image_paths = ['test.png']


def my_map_py(x):
    return x


dataset = tf.data.Dataset.from_tensor_slices(image_paths)
dataset = dataset.map(lambda x: tf.py_function(my_map_py, [x], [tf.string]))

iterator = dataset.__iter__()

checkpoint = tf.train.Checkpoint(iterator=iterator)
checkpoint.save('checkpoint.tf')
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='KichangKim' date='2019-04-28T19:55:29Z'>
		Thank you for the reproducible example.
I took a closer look at the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/script_ops.py&gt;implementation&lt;/denchmark-link&gt;
 of  and it turns out it is implemented using the  op, which is also stateful. In other words, you will not be able to checkpoint an input pipeline that contains  or py_function`.
		</comment>
		<comment id='5' author='KichangKim' date='2019-04-29T03:56:30Z'>
		Thanks for invedtigation. I divided my dataset into multiple datasets and manually save checkpoint via its offset.
		</comment>
	</comments>
</bug>