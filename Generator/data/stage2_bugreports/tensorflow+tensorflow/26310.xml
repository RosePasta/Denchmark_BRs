<bug id='26310' author='xuefengxiaoyang' open_date='2019-03-04T06:30:04Z' closed_time='2020-01-02T18:15:57Z'>
	<summary>Trained model inference on GPU of nvidia TX2 get poor result  even error result</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:nvidia TX2
TensorFlow installed from (source or binary):binary from https://nvidia.box.com/v/JP33-TF1-11-0-py35-wTRT
TensorFlow version (use command below):1.110
Python version:3.5
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:CUDA9.0 cudnn 7.15
GPU model and memory:8G

You can collect some of this information using our environment capture &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
Describe the current behavior
I trained the model on the server and deployed the same version of tensorflow on TX2, but when I run the trained model with the GPU on TX2, I get a lot worse than on the server, but running the model on the CPU of TX2 does not cause this problem.
Describe the expected behavior

The result of running on the GPU of the server should be the same as the result of the GPU running on TX2. There should not be such a big gap.
The GPU running result on TX2 should be the same as the CPU running result on TX2.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='xuefengxiaoyang' date='2019-05-13T17:56:55Z'>
		&lt;denchmark-link:https://github.com/xuefengxiaoyang&gt;@xuefengxiaoyang&lt;/denchmark-link&gt;
 Can you please update to a more recent version of jetpack and TF version. Also could you please clarify what do you mean by a lot worse?
		</comment>
		<comment id='2' author='xuefengxiaoyang' date='2019-10-12T09:13:54Z'>
		&lt;denchmark-link:https://github.com/xuefengxiaoyang&gt;@xuefengxiaoyang&lt;/denchmark-link&gt;
  hello, did you get the answer of this issue? I am facing the same problem, trained model, the TX2 GPU inference result diff with PC inference result.
		</comment>
		<comment id='3' author='xuefengxiaoyang' date='2019-10-12T10:36:06Z'>
		I got  the answer,  just change the Compute Capability of ARCH to match your edge device. for TX2, its 6.2
		</comment>
		<comment id='4' author='xuefengxiaoyang' date='2020-01-02T18:15:57Z'>
		Closing since it looks like this is resolved as per &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/26310#issuecomment-541312223&gt;#26310 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='xuefengxiaoyang' date='2020-01-02T18:15:59Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26310&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26310&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>