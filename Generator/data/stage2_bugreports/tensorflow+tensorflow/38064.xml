<bug id='38064' author='naripok' open_date='2020-03-31T05:19:16Z' closed_time='2020-10-23T14:18:49Z'>
	<summary>UnboundLocalError: local variable 'logs' referenced before assignment on training with little data</summary>
	<description>
I found an error caused by an attempt of coping training logs from a not yet assigned variable.
The error occurred on my machine (Arch linux, tensorflow v2.2rc2 compiled from source) and I managed to reproduce the error on colab, on a stock environment.
It only happens when the model.fit method is called with very little training / eval data.
The logs variable is assigned inside a for loop that never happens when there is no sufficient data.
The code lives here:



tensorflow/tensorflow/python/keras/engine/training.py


         Line 793
      in
      e6e5d6d






 epoch_logs = copy.copy(logs) 





The notebook gist link for reproducing the bug:
&lt;denchmark-link:https://colab.research.google.com/gist/naripok/8ce09ec9c3e795b3635a6b1ac11ebd4b/tpu_transformer_model.ipynb&gt;https://colab.research.google.com/gist/naripok/8ce09ec9c3e795b3635a6b1ac11ebd4b/tpu_transformer_model.ipynb&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='naripok' date='2020-03-31T08:35:07Z'>
		Was able to reproduce the issue with TF v2.2.0-rc2. Please find the gist &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/88f59bf9ab4c7cd27ef62a49e28162be/38064.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='naripok' date='2020-04-14T20:17:23Z'>
		I have the same issue with TF v2.2.0-rc2
&lt;denchmark-code&gt;Epoch 19/20
18/18 [==============================] - 2s 125ms/step - loss: 0.0035 - accuracy: 1.0000
Epoch 20/20
18/18 [==============================] - 2s 126ms/step - loss: 0.0373 - accuracy: 0.9844

Traceback (most recent call last):
  File "run.py", line 155, in &lt;module&gt;
    main()
  File "run.py", line 120, in main
    accuracy, num_of_classes = train_Full_visible(unique_name)
  File "run.py", line 78, in train_Full_visible
    acc = neuro.train(picdb, train_ids, test_ids, "Full body visible")
  File "/ssd/200410 3rd Try/neuro.py", line 232, in train
    test_loss, test_acc = self.model.evaluate(test_generator, verbose=0)
  File "/home/frank/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/home/frank/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py", line 1028, in evaluate
    logs = tf_utils.to_numpy_or_python_type(logs)
UnboundLocalError: local variable 'logs' referenced before assignment
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='naripok' date='2020-04-16T17:50:33Z'>
		&lt;denchmark-link:https://github.com/Tuxius&gt;@Tuxius&lt;/denchmark-link&gt;
 and others which stuck until this bug is fixed:
I had the same issue  and I found that my validation data set had less samples as the batch_size.  Because I'm working with TFRecords-data-sets which have no meta data about how many records, i.e. samples, are in the data set I check now upfront whether the data set  contains at least batch_size records (samples).
For that I use the following helper functions:
&lt;denchmark-code&gt;def doesDataSetContainsEnoughDataForBatch(dataset, batch_size):
    return len(list(dataset.take(batch_size).as_numpy_iterator())) == batch_size
  
def doesDataSetFileContainsEnoughDataForBatch(sampleFileName="", batch_size=100):
    dataset = tf.data.TFRecordDataset(sampleFileName)
    return doesDataSetContainsEnoughDataForBatch(dataset, batch_size=batch_size)

if __name__ == '__main__':
  dataSetFileName="./Samples/validation_data_123.tfrecord"
  if not doesDataSetFileContainsEnoughDataForBatch(dataSetFileName,batch_size=100):
    raise Exception(f"Data set file {dataSetFileName} doesn't contain enough data")
   
  # Now open the data set a second time knowing you have enough data 
  # and use it ....
  """
  trainDataset = tf.data.TFRecordDataset(dataSetFileName)
  
  model.fit ( trainDataset ... 
  
  """

&lt;/denchmark-code&gt;

Please keep in mind that by using the first helper function directly ,
i.e. doesDataSetContainsEnoughDataForBatch,  you already read batch_size samples from the data set. So you should recreate the data set after the check.
By using the second helper function  you just lose some execution time upfront.
If you don't use TFRecord data sets you might have a similar  issue. But then it is also a good idea to check upfront whether you have enough data for at least one batch.
		</comment>
		<comment id='4' author='naripok' date='2020-04-25T00:16:52Z'>
		
@Tuxius and others which stuck until this bug is fixed:
I had the same issue and I found that my validation data set had less samples as the batch_size. Because I'm working with TFRecords-data-sets which have no meta data about how many records, i.e. samples, are in the data set I check now upfront whether the data set contains at least batch_size records (samples).
For that I use the following helper functions:
def doesDataSetContainsEnoughDataForBatch(dataset, batch_size):
    return len(list(dataset.take(batch_size).as_numpy_iterator())) == batch_size
  
def doesDataSetFileContainsEnoughDataForBatch(sampleFileName="", batch_size=100):
    dataset = tf.data.TFRecordDataset(sampleFileName)
    return doesDataSetContainsEnoughDataForBatch(dataset, batch_size=batch_size)

if __name__ == '__main__':
  dataSetFileName="./Samples/validation_data_123.tfrecord"
  if not doesDataSetFileContainsEnoughDataForBatch(dataSetFileName,batch_size=100):
    raise Exception(f"Data set file {dataSetFileName} doesn't contain enough data")
   
  # Now open the data set a second time knowing you have enough data 
  # and use it ....
  """
  trainDataset = tf.data.TFRecordDataset(dataSetFileName)
  
  model.fit ( trainDataset ... 
  
  """

Please keep in mind that by using the first helper function directly ,
i.e. doesDataSetContainsEnoughDataForBatch, you already read batch_size samples from the data set. So you should recreate the data set after the check.
By using the second helper function you just lose some execution time upfront.
If you don't use TFRecord data sets you might have a similar issue. But then it is also a good idea to check upfront whether you have enough data for at least one batch.

I added more data. It works well!
Thank you!
		</comment>
		<comment id='5' author='naripok' date='2020-04-28T16:56:58Z'>
		Hi there, I change the batch_size to smaller so that it could be more evenly divided during training.
For example, when you have 300 training samples, do not use batch_size=128, try batch_size =10 or batch_size=20 something, it works for me.
		</comment>
		<comment id='6' author='naripok' date='2020-05-07T19:59:17Z'>
		The issue is still reproducible with 2.2.0rc4, will test with 2.2.0 release.
		</comment>
		<comment id='7' author='naripok' date='2020-05-08T07:13:57Z'>
		2.2 is still reproducible for me
		</comment>
		<comment id='8' author='naripok' date='2020-05-11T17:20:36Z'>
		2.2 is still reproducible indeed
		</comment>
		<comment id='9' author='naripok' date='2020-05-26T11:09:28Z'>
		Are there any other workarounds known yet? The batch size one did not work out for me.
		</comment>
		<comment id='10' author='naripok' date='2020-06-02T12:15:18Z'>
		Same problem: Is there way to fix or alternatives ?
		</comment>
		<comment id='11' author='naripok' date='2020-06-10T06:43:47Z'>
		same problem: Is there way ro fix the issue?
		</comment>
		<comment id='12' author='naripok' date='2020-06-10T08:13:57Z'>
		Same problem, the root cause for this issue is that the training cannot perform a single step because your dataset is not large enough to fill one training iteration.
Anyway, there is still a bug here, this is not the expected behavior.
		</comment>
		<comment id='13' author='naripok' date='2020-06-11T06:02:45Z'>
		
Same problem, the root cause for this issue is that the training cannot perform a single step because your dataset is not large enough to fill one training iteration.
Anyway, there is still a bug here, this is not the expected behavior.

Yes. You are right. When i increased training data the error doesnot occure.
		</comment>
		<comment id='14' author='naripok' date='2020-06-12T08:41:38Z'>
		I have also same error arise , even increase the training dataset its not solved. Again i got the same error. kindly share me the solution if anyone knows about this.
Thank you
		</comment>
		<comment id='15' author='naripok' date='2020-06-12T11:37:29Z'>
		
I have also same error arise , even increase the training dataset its not solved. Again i got the same error. kindly share me the solution if anyone knows about this.
Thank you

What is the size of the dataset you are using?
		</comment>
		<comment id='16' author='naripok' date='2020-06-12T11:48:22Z'>
		512, 512 size and 247 images and masks
		</comment>
		<comment id='17' author='naripok' date='2020-06-12T19:39:08Z'>
		I solved my particular issue setting the batch_size very small, in my case batch_size = 1. I confirm that this happens with small datasets.
		</comment>
		<comment id='18' author='naripok' date='2020-06-13T07:21:15Z'>
		yes  sir, i have also reduced batchsize as 1, but i dont know the reason why the same error arise.
		</comment>
		<comment id='19' author='naripok' date='2020-06-20T01:42:23Z'>
		I had the same error with using model.fit it turns out the validation_steps or steps_per_epoch was zero. By making them &gt;=1, the bug disappeared.
		</comment>
		<comment id='20' author='naripok' date='2020-06-20T16:54:18Z'>
		same problem also. is there a workaround?
		</comment>
		<comment id='21' author='naripok' date='2020-06-22T14:04:10Z'>
		same problem during training BiT. Batch size 128, Steps_per_epoch = 100
Edit: I throw out validation_data and it works. but I want valid model
		</comment>
		<comment id='22' author='naripok' date='2020-06-30T18:15:01Z'>
		Working with a small dataset
changed batch_size = 64 - worked
		</comment>
		<comment id='23' author='naripok' date='2020-07-01T03:18:39Z'>
		This error also raise when len(traindata) or len(validdata) is 0.
		</comment>
		<comment id='24' author='naripok' date='2020-07-07T09:18:09Z'>
		I found this error while running the training part of vgg16 model
def vgg16_model( num_classes=None):
&lt;denchmark-code&gt;model = VGG16(weights='/kaggle/input/keras-pretrained-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', include_top=False, input_shape=(224, 224, 3))
x=Flatten()(model.output)
output=Dense(num_classes,activation='softmax')(x)
model=Model(model.input,output)
return model
&lt;/denchmark-code&gt;

vgg_conv=vgg16_model(6)
def kappa_score(y_true, y_pred):
&lt;denchmark-code&gt;y_true=tf.math.argmax(y_true)
y_pred=tf.math.argmax(y_pred)
return tf.compat.v1.py_func(cohen_kappa_score ,(y_true, y_pred),tf.double)
&lt;/denchmark-code&gt;

opt = SGD(lr=0.001)
vgg_conv.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy',kappa_score])
nb_epochs = 3
batch_size = 16
nb_train_steps = train.shape[0]//batch_size
nb_val_steps=validation.shape[0]//batch_size
print("Number of training and validation steps: {} and {}".format(nb_train_steps,nb_val_steps))
check_point = ModelCheckpoint('./model.h5',monitor='val_loss',verbose=True, save_best_only=True, save_weights_only=True)
early_stop = EarlyStopping(monitor='val_loss',patience=25,verbose=True)
callbacks = [check_point,early_stop]
vgg_conv.fit_generator(
train_generator,
verbose=2,
steps_per_epoch=nb_train_steps,
epochs=nb_epochs,
validation_data=validation_generator,
validation_steps=nb_val_steps,
callbacks=callbacks,
use_multiprocessing=True)
Found 16 non-validated image filenames belonging to 6 classes.
Found 4 non-validated image filenames belonging to 2 classes.
Number of training and validation steps: 1 and 0
Epoch 1/3
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

UnboundLocalError                         Traceback (most recent call last)
 in 
63     validation_steps=nb_val_steps,
64     callbacks=callbacks,
---&gt; 65     use_multiprocessing=True)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
322               'in a future version' if date is None else ('after %s' % date),
323               instructions)
--&gt; 324       return func(*args, **kwargs)
325     return tf_decorator.make_decorator(
326         func, new_func, 'deprecated',
/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
1477         use_multiprocessing=use_multiprocessing,
1478         shuffle=shuffle,
-&gt; 1479         initial_epoch=initial_epoch)
1480
1481   @deprecation.deprecated(
/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
64   def _method_wrapper(self, *args, **kwargs):
65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
---&gt; 66       return method(self, *args, **kwargs)
67
68     # Running inside run_distribute_coordinator already.
/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
870               workers=workers,
871               use_multiprocessing=use_multiprocessing,
--&gt; 872               return_dict=True)
873           val_logs = {'val_' + name: val for name, val in val_logs.items()}
874           epoch_logs.update(val_logs)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
64   def _method_wrapper(self, *args, **kwargs):
65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
---&gt; 66       return method(self, *args, **kwargs)
67
68     # Running inside run_distribute_coordinator already.
/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in evaluate(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)
1089       callbacks.on_test_end()
1090
-&gt; 1091       logs = tf_utils.to_numpy_or_python_type(logs)
1092       if return_dict:
1093         return logs
UnboundLocalError: local variable 'logs' referenced before assignment
		</comment>
		<comment id='25' author='naripok' date='2020-07-07T13:35:53Z'>
		Can we add tag 2.3 for this issue?
		</comment>
		<comment id='26' author='naripok' date='2020-07-07T14:25:04Z'>
		
Can we add tag 2.3 for this issue?
I am using tensorflow 2.2.0

		</comment>
		<comment id='27' author='naripok' date='2020-07-24T14:16:04Z'>
		I am surprised there is no self-contained code example reproducing this, so here is one:
&lt;denchmark-code&gt;import numpy as np
from tensorflow import keras
model = keras.models.Sequential([keras.layers.Dense(1, input_shape=(1,))])
model.compile(loss="mse", optimizer="adam")
model.fit(x=np.ones((1, 1)), y=np.ones((1, 1)), validation_split=0.5)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='28' author='naripok' date='2020-07-30T10:29:35Z'>
		This is reproducible in TF 2.3. I am using batch_size of 1 and no validation data.
By the way, this is working when I ran my custom model eager mode run_eagerly=True
		</comment>
		<comment id='29' author='naripok' date='2020-09-03T15:23:42Z'>
		Encountered this error, myself, and thought I'd share my debugging in case others would benefit in some way.
Tracing the calls and returns, it seems that inside fit, a local variable logs is first defined inside a for loop iterating over DataHandler.steps(), which, in many cases, is a number dependent on the cardinality of the Dataset. If the Dataset cardinality is zero, then DataHandler.steps() performs 0 iterations, and logs will never be created, resulting in the UnboundLocalError we're all getting.
The cardinality of my Dataset was being returned as 0 because I had created it from a .take() called on a BatchDataset that had a batch_size &gt; 1 -- this was an artifact of a previous iteration of my code. (Worth noting that when .take() was called on the BatchDataset with batch_size of 1, then my Dataset.cardinality() was nonzero.)
After cleaning the code (in my case, calling take on a Dataset that had not been batched), my Dataset.cardinality() returned a nonzero value, and the error was resolved.
		</comment>
		<comment id='30' author='naripok' date='2020-09-09T09:55:55Z'>
		when turn off validation_data, it works! tf.version == '2.2.0'
&lt;denchmark-code&gt;logistic_regression.fit(train_dense_x, 
          train_label, epochs=100, batch_size=256,
#           validation_data=(val_dense_x, val_label),  
          callbacks=[tbCallBack])
&lt;/denchmark-code&gt;

		</comment>
		<comment id='31' author='naripok' date='2020-09-09T10:00:33Z'>
		check number of your training data. It might equal to zero
		</comment>
		<comment id='32' author='naripok' date='2020-10-02T23:34:24Z'>
		Looks like this happens when size of the training data is &lt;= batch size
		</comment>
		<comment id='33' author='naripok' date='2020-10-09T05:04:06Z'>
		
check number of your training data. It might equal to zero

In my case, I read my tfrecords incorrectly but didn't get any warnings or errors. After reading the record, you can try printing its size as a sanity check.
		</comment>
		<comment id='34' author='naripok' date='2020-10-12T17:34:43Z'>
		&lt;denchmark-link:https://github.com/naripok&gt;@naripok&lt;/denchmark-link&gt;
 the original issue of "UnboundLocalError: local variable 'logs' referenced before assignment" is no longer with the latest tf-nightly. Can you please confirm and if so close the issue ?
		</comment>
		<comment id='35' author='naripok' date='2020-10-23T14:18:49Z'>
		Sorry for the delay,
Confirmed.
Thanks!
		</comment>
		<comment id='36' author='naripok' date='2020-10-23T14:18:51Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38064&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38064&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='37' author='naripok' date='2020-11-13T22:45:25Z'>
		
had less samples as the batch_size

The same situation can occur if the train-set has less number of examples than the batch size. In my case my train-set's directory was wrongly given.
		</comment>
		<comment id='38' author='naripok' date='2020-11-22T03:29:03Z'>
		
check number of your training data. It might equal to zero

Thank you!
		</comment>
	</comments>
</bug>