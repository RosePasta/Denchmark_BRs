<bug id='26218' author='iperov' open_date='2019-02-28T15:49:36Z' closed_time='2020-01-03T22:21:02Z'>
	<summary>tf-gpu==1.13.1  : 35% less batch size before OOM vs tf-gpu==1.11.0</summary>
	<description>
System information

Windows 7
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): 1.11.0 , 1.13.1
Python version: 3.6.5
CUDA/cuDNN version: 9/7.1.4 , 10/7.4.1
GPU model and memory: GTX 1060 6GB

Describe the current behavior
I have standard AE network with pixel shuffler layer.
on tf.1.11.0-cuda 9 maximum batch size for my GTX 1060 6GB is 132
but after upgrade to tf.1.13.1-cuda 10 tf cannot handle same batch size it produces OOM error
and maximum now 90 for my card.
Describe the expected behavior
expected not to downgrade performance when upgrading tensorflow
Code to reproduce the issue
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf
keras = tf.keras
KL = keras.layers
K = keras.backend

bgr_shape = (128, 128, 3)
#batch_size = 132 #max -tf.1.11.0-cuda 9
batch_size = 86 #max -tf.1.13.1-cuda 10
 
class PixelShuffler(keras.layers.Layer):
    def __init__(self, size=(2, 2), data_format=None, **kwargs):
        super(PixelShuffler, self).__init__(**kwargs)
        self.size = size

    def call(self, inputs):

        input_shape = K.int_shape(inputs)
        if len(input_shape) != 4:
            raise ValueError('Inputs should have rank ' +
                             str(4) +
                             '; Received input shape:', str(input_shape))


        batch_size, h, w, c = input_shape
        if batch_size is None:
            batch_size = -1
        rh, rw = self.size
        oh, ow = h * rh, w * rw
        oc = c // (rh * rw)

        out = K.reshape(inputs, (batch_size, h, w, rh, rw, oc))
        out = K.permute_dimensions(out, (0, 1, 3, 2, 4, 5))
        out = K.reshape(out, (batch_size, oh, ow, oc))
        return out

    def compute_output_shape(self, input_shape):

        if len(input_shape) != 4:
            raise ValueError('Inputs should have rank ' +
                             str(4) +
                             '; Received input shape:', str(input_shape))


        height = input_shape[1] * self.size[0] if input_shape[1] is not None else None
        width = input_shape[2] * self.size[1] if input_shape[2] is not None else None
        channels = input_shape[3] // self.size[0] // self.size[1]

        if channels * self.size[0] * self.size[1] != input_shape[3]:
            raise ValueError('channels of input and size are incompatible')

        return (input_shape[0],
                height,
                width,
                channels)

    def get_config(self):
        config = {'size': self.size}
        base_config = super(PixelShuffler, self).get_config()

        return dict(list(base_config.items()) + list(config.items()))
        
def upscale (dim):
    def func(x):
        return PixelShuffler()((KL.Conv2D(dim * 4, kernel_size=3, strides=1, padding='same')(x)))
    return func 
            
inp = KL.Input(bgr_shape)
x = inp
x = KL.Conv2D(128, 5, strides=2, padding='same')(x)
x = KL.Conv2D(256, 5, strides=2, padding='same')(x)
x = KL.Conv2D(512, 5, strides=2, padding='same')(x)
x = KL.Conv2D(1024, 5, strides=2, padding='same')(x)
x = KL.Dense(1024)(KL.Flatten()(x))
x = KL.Dense(8 * 8 * 1024)(x)
x = KL.Reshape((8, 8, 1024))(x)
x = upscale(512)(x)
x = upscale(256)(x)
x = upscale(128)(x)
x = upscale(64)(x)
x = KL.Conv2D(3, 5, strides=1, padding='same')(x)

model = keras.models.Model ([inp], [x])
model.compile(optimizer=keras.optimizers.Adam(lr=5e-5, beta_1=0.5, beta_2=0.999), loss='mae')

training_data = np.zeros ( (batch_size,128,128,3) )
loss = model.train_on_batch( [training_data], [training_data] )
print ("FINE")
&lt;/denchmark-code&gt;

Other info / logs
&lt;denchmark-code&gt;1] 1 Chunks of size 12032 totalling 11.8KiB
2019-02-28 19:45:23.516100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 4 Chunks of size 19200 totalling 75.0KiB
2019-02-28 19:45:23.517100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 4 Chunks of size 38400 totalling 150.0KiB
2019-02-28 19:45:23.517100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 4 Chunks of size 262144 totalling 1.00MiB
2019-02-28 19:45:23.517100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 1 Chunks of size 368640 totalling 360.0KiB
2019-02-28 19:45:23.517100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 4 Chunks of size 1179648 totalling 4.50MiB
2019-02-28 19:45:23.517100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 5 Chunks of size 3276800 totalling 15.63MiB
2019-02-28 19:45:23.517100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 4 Chunks of size 4718592 totalling 18.00MiB
2019-02-28 19:45:23.520100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 3 Chunks of size 13107200 totalling 37.50MiB
2019-02-28 19:45:23.520100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 1 Chunks of size 17028352 totalling 16.24MiB
2019-02-28 19:45:23.521100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 1 Chunks of size 17694720 totalling 16.88MiB
2019-02-28 19:45:23.521100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 1 Chunks of size 17694976 totalling 16.88MiB
2019-02-28 19:45:23.521100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 3 Chunks of size 18874368 totalling 54.00MiB
2019-02-28 19:45:23.521100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 1 Chunks of size 23592960 totalling 22.50MiB
2019-02-28 19:45:23.521100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 5 Chunks of size 52428800 totalling 250.00MiB
2019-02-28 19:45:23.529100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 5 Chunks of size 75497472 totalling 360.00MiB
2019-02-28 19:45:23.529100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 1 Chunks of size 94371840 totalling 90.00MiB
2019-02-28 19:45:23.530100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 1 Chunks of size 100362240 totalling 95.71MiB
2019-02-28 19:45:23.530100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 2 Chunks of size 188743680 totalling 360.00MiB
2019-02-28 19:45:23.530100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 1 Chunks of size 194688000 totalling 185.67MiB
2019-02-28 19:45:23.530100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 12 Chunks of size 268435456 totalling 3.00GiB
2019-02-28 19:45:23.530100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 1 Chunks of size 552317184 totalling 526.73MiB
2019-02-28 19:45:23.530100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
5] Sum Total of in-use chunks: 5.02GiB
2019-02-28 19:45:23.530100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
7] Stats:
Limit:                  5838622720
InUse:                  5393793792
MaxInUse:               5708028928
NumAllocs:                     434
MaxAllocSize:           1363673088

2019-02-28 19:45:23.531100: W tensorflow/core/common_runtime/bfc_allocator.cc:27
1] *****************************************************__**********_***********
**********************x
2019-02-28 19:45:23.531100: W tensorflow/core/framework/op_kernel.cc:1401] OP_RE
QUIRES failed at conv_grad_input_ops.cc:1054 : Resource exhausted: OOM when allo
cating tensor with shape[90,128,64,64] and type float on /job:localhost/replica:
0/task:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File "D:\DeepFaceLab\_internal\bin\DeepFaceLab\test.py", line 87, in &lt;module&gt;
    loss = model.train_on_batch( [training_data], [training_data] )
  File "D:\DeepFaceLab\_internal\bin\lib\site-packages\tensorflow\python\keras\e
ngine\training.py", line 1188, in train_on_batch
    outputs = self.train_function(ins)  # pylint: disable=not-callable
  File "D:\DeepFaceLab\_internal\bin\lib\site-packages\tensorflow\python\keras\b
ackend.py", line 3076, in __call__
    run_metadata=self.run_metadata)
  File "D:\DeepFaceLab\_internal\bin\lib\site-packages\tensorflow\python\client\
session.py", line 1439, in __call__
    run_metadata_ptr)
  File "D:\DeepFaceLab\_internal\bin\lib\site-packages\tensorflow\python\framewo
rk\errors_impl.py", line 528, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocat
ing tensor with shape[90,128,64,64] and type float on /job:localhost/replica:0/t
ask:0/device:GPU:0 by allocator GPU_0_bfc
         [[{{node training/Adam/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropInp
ut}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add repor
t_tensor_allocations_upon_oom to RunOptions for current allocation info.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='iperov' date='2019-04-06T17:36:51Z'>
		still no solution?
I guess it is not fixed in tf 2.0
		</comment>
		<comment id='2' author='iperov' date='2019-04-06T20:03:08Z'>
		&lt;denchmark-link:https://github.com/iperov&gt;@iperov&lt;/denchmark-link&gt;
 Is this still an issue?
		</comment>
		<comment id='3' author='iperov' date='2019-04-06T20:08:06Z'>
		&lt;denchmark-link:https://github.com/tatianashp&gt;@tatianashp&lt;/denchmark-link&gt;
 are you kidding?
reproduced significantly performance downgrade
no fix or official comment about it
		</comment>
		<comment id='4' author='iperov' date='2019-04-07T06:34:48Z'>
		&lt;denchmark-link:https://github.com/smit-hinsu&gt;@smit-hinsu&lt;/denchmark-link&gt;
 Can you look into what's happening here?
		</comment>
		<comment id='5' author='iperov' date='2019-04-07T06:35:29Z'>
		&lt;denchmark-link:https://github.com/iperov&gt;@iperov&lt;/denchmark-link&gt;
 Sorry it fell through the cracks.
		</comment>
		<comment id='6' author='iperov' date='2019-04-15T09:18:44Z'>
		Compared to 1.12, I'm finding that the exact same code uses about 10% extra GPU memory as per tf.profiler. Specifically, I get about 6400MB usage total (i.e., for _TFProfRoot) on 1.12.0 but about 7100MB for 1.13.1. With a smaller version of the same model, the proportional difference is about the same---about 3450MB for 1.13.1 and about 3100MB for 1.12.0.
		</comment>
		<comment id='7' author='iperov' date='2019-04-15T09:42:00Z'>
		&lt;denchmark-link:https://github.com/rightaditya&gt;@rightaditya&lt;/denchmark-link&gt;

thx, you reproduced the bug too.
What are next actions?
		</comment>
		<comment id='8' author='iperov' date='2019-04-25T09:40:22Z'>
		Sorry for the long delay for the update.
This looks like a bug introduced between cuDNN v7.2 and v7.4. We will report this to NVIDIA and update this issue after that.
Thanks for providing with a small example to reproduce the issue.
		</comment>
		<comment id='9' author='iperov' date='2019-05-01T07:38:32Z'>
		The difference seems to be coming from a change in CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING convolution algorithm implementation causing it to use more memory with cuDNN v7.4. I don't see any mention of this in the release notes but this might be an intended change.
I was able to work around this issue by setting TF_CUDNN_WORKSPACE_LIMIT_IN_MB environment variable to "1024".
If for some reason that does not work out for you, then also give it a try setting TF_CUDNN_WORKSPACE_LIMIT_IN_MB to "0" and then TF_CUDNN_USE_AUTOTUNE to "0".
Let us know if either of the above helps resolve the issue.
		</comment>
		<comment id='10' author='iperov' date='2019-05-01T12:15:36Z'>
		&lt;denchmark-link:https://github.com/smit-hinsu&gt;@smit-hinsu&lt;/denchmark-link&gt;
 tried both solutions, but oom still happens
		</comment>
		<comment id='11' author='iperov' date='2019-05-01T12:18:33Z'>
		last cudnn 7.5.1 also fails
		</comment>
		<comment id='12' author='iperov' date='2019-05-01T15:26:25Z'>
		I don't have the exact code that I tested earlier, but it didn't use convolutions at all. It was all LSTMs, the CUDNN ops if I'm remembering correctly.
However, I should point out that some tests I ran yesterday found substantial variation in the memory usage reported by tfprof for the same code and data, so what I saw before may have been within the bounds of that variation. I did try it a few times with each version to get an idea of the distributions though and they didn't seem to overlap.
		</comment>
		<comment id='13' author='iperov' date='2019-05-09T13:39:26Z'>
		We are experiencing the same issue even with tf-2.0. The model we use consumes approx 1GB of  more GPU memory than the pytorch implementation of the same model, though tf model is way faster. Looking forward to the solution.
		</comment>
		<comment id='14' author='iperov' date='2019-05-30T16:18:45Z'>
		
last cudnn 7.5.1 also fails

Did cudnn 7.6.0 solve this issue?
		</comment>
		<comment id='15' author='iperov' date='2019-05-31T09:22:44Z'>
		cudnn 7.6.0 same problem
		</comment>
		<comment id='16' author='iperov' date='2019-08-29T12:06:08Z'>
		Hi, any updates on this issue?
		</comment>
		<comment id='17' author='iperov' date='2020-01-02T18:15:19Z'>
		&lt;denchmark-link:https://github.com/timshen91&gt;@timshen91&lt;/denchmark-link&gt;
 Can you please take a look at this?  I think the next steps are:

Verify that this is a problem still.
If it is still a problem, file a bug with NVIDIA (CC @nluehr )

		</comment>
		<comment id='18' author='iperov' date='2020-01-03T22:21:02Z'>
		With tf2.0.0 and batch=132, I cannot reproduce the OOM with the garbage collector on. With GC off, I can still see the OOM.
I'll close the bug as GC seems to deallocate dead memory. Please re-open it if the problem still remains.
&lt;denchmark-code&gt;2020-01-03 14:18:31.905093: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-01-03 14:18:31.913239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: TITAN V major: 7 minor: 0 memoryClockRate(GHz): 1.455
pciBusID: 0000:d8:00.0
2020-01-03 14:18:31.913417: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-01-03 14:18:31.914810: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-01-03 14:18:31.915919: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-01-03 14:18:31.916171: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-01-03 14:18:31.917729: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-01-03 14:18:31.918760: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-01-03 14:18:31.922199: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-01-03 14:18:31.922733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-01-03 14:18:31.922922: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-01-03 14:18:31.964210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000000000 Hz
2020-01-03 14:18:31.976496: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x52556b0 executing computations on platform Host. Devices:
2020-01-03 14:18:31.976542: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2020-01-03 14:18:32.117365: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x52b8130 executing computations on platform CUDA. Devices:
2020-01-03 14:18:32.117430: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): TITAN V, Compute Capability 7.0
2020-01-03 14:18:32.118844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: TITAN V major: 7 minor: 0 memoryClockRate(GHz): 1.455
pciBusID: 0000:d8:00.0
2020-01-03 14:18:32.118913: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-01-03 14:18:32.118941: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-01-03 14:18:32.118963: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-01-03 14:18:32.118985: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-01-03 14:18:32.119006: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-01-03 14:18:32.119027: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-01-03 14:18:32.119049: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-01-03 14:18:32.120211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-01-03 14:18:32.120271: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-01-03 14:18:32.122202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-03 14:18:32.122230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-01-03 14:18:32.122244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-01-03 14:18:32.124553: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2020-01-03 14:18:32.124591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5537 MB memory) -&gt; physical GPU (device: 0, name: TITAN V, pci bus id: 0000:d8:00.0, compute capability: 7.0)
2020-01-03 14:18:32.911533: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-01-03 14:18:33.885741: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
2020-01-03 14:18:34.006646: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.63GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-01-03 14:18:34.006681: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.63GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-01-03 14:18:34.385341: W tensorflow/core/common_runtime/bfc_allocator.cc:305] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.
2020-01-03 14:18:34.461297: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 3.43G (3678928896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-01-03 14:18:34.557533: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-01-03 14:18:35.221099: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.96GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-01-03 14:18:35.221135: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.96GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-01-03 14:18:35.641610: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.34GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-01-03 14:18:35.641645: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.34GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-01-03 14:18:35.708188: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-01-03 14:18:35.708215: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-01-03 14:18:35.834415: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.38GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-01-03 14:18:35.834441: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.38GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
FINE
&lt;/denchmark-code&gt;

		</comment>
		<comment id='19' author='iperov' date='2020-01-03T22:21:04Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26218&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26218&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>