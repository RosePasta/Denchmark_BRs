<bug id='21631' author='ageron' open_date='2018-08-15T13:35:15Z' closed_time='2019-08-08T22:07:15Z'>
	<summary>Keras model converted to an Estimator does not handle 1D labels correctly</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOSX 10.13.6
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): No
TensorFlow version (use command below): v1.10.0-rc1-19-g656e7a2b34 1.10.0
Python version: 3.6.6
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: see script below (keras model converges, not estimator)

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Keras model converges on MNIST (&gt; 97% accuracy after 5 epochs with batches of 32 instances), using the class IDs as labels (i.e., a 1D array of int32). However, after I convert the model to an Estimator, it does not converge (around 10% accuracy, again after 5 epochs with batches of 32 instances). Once I reshape the labels to a column vector (y_train.reshape(-1, 1)), it converges well. I double-checked with a DNNClassifier, and it accepts the labels both as a 1D array or as a column vector (2D). I believe this is a bug, and people might spend hours searching for the cause (as I did).
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

If you run the code below, it will train three models on MNIST, first using 1D labels, then using 2D labels (i.e., column vectors):

a Keras model
the same Keras model converted to an Estimator
an equivalent DNNClassifier

The final outputs below show that they all converge except for the Keras model converted to an Estimator when the labels are 1D (I removed the "You CPU supports..." and "Using temporary folder..." warnings from the outputs). The behavior is identical in eager mode and in graph mode.
&lt;denchmark-code&gt;==== Training with 1D labels... ====
Keras model:
    Eval: [0.07029167589747813, 0.9792]
Keras model converted to an estimator:
    Eval: ({'accuracy': 0.10245253, 'loss': 0.07640489, 'global_step': 18751}, [])
DNNClassifier:
    Eval: ({'accuracy': 0.1135, 'average_loss': 2.3010197, 'loss': 291.2683, 'global_step': 9375}, [])

==== Training with 2D labels... ====
Keras model:
    Eval: [0.06944960513310507, 0.9779]
Keras model converted to an estimator:
    Eval: ({'accuracy': 0.97171676, 'loss': 0.09046984, 'global_step': 18751}, [])
DNNClassifier:
    Eval: ({'accuracy': 0.976, 'average_loss': 0.08481478, 'loss': 10.736049, 'global_step': 9375}, [])
&lt;/denchmark-code&gt;

Here is the source code:
from __future__ import division, print_function, unicode_literals

import logging
logging.basicConfig(level=logging.WARNING)

import tensorflow as tf
import tensorflow.contrib.eager as tfe
#tf.enable_eager_execution() # same problem in eager mode or graph mode
from tensorflow import keras
import numpy as np

n_epochs = 5
batch_size = 32

def create_keras_model():
    keras.backend.clear_session()
    model = keras.models.Sequential([
        keras.layers.Dense(300, activation="relu", input_shape=[28*28]),
        keras.layers.Dense(100, activation="relu"),
        keras.layers.Dense(10, activation="softmax")
    ])
    optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9)
    model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
                  metrics=["accuracy"])
    return model

def create_keras_estimator():
    model = create_keras_model()
    input_name = model.input_names[0]
    estimator = tf.keras.estimator.model_to_estimator(model)
    return estimator, input_name

def create_dnn_estimator():
    input_name = "pixels"
    pixels = tf.feature_column.numeric_column(input_name, shape=[28 * 28])
    estimator = tf.estimator.DNNClassifier(hidden_units=[300, 100, 10],
                                           n_classes=10,
                                           feature_columns=[pixels])
    return estimator, input_name

def train_and_evaluate_estimator(estimator, input_name,
                                 X_train, y_train, X_test, y_test):
    train_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={input_name: X_train}, y=y_train, num_epochs=5, batch_size=32,
        shuffle=True)
    test_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={input_name: X_test}, y=y_test, shuffle=False)
    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn)
    eval_spec = tf.estimator.EvalSpec(input_fn=test_input_fn)
    return tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)

def load_and_scale_MNIST():
    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()
    X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0
    X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0
    y_train = y_train.astype(np.int32)
    y_test = y_test.astype(np.int32)
    return X_train, y_train, X_test, y_test

if __name__ == '__main__':
    X_train, y_train, X_test, y_test = load_and_scale_MNIST()
    
    for run in range(2):
        if run == 0:
            print("==== Training with 1D labels... ====")
            assert y_train.ndim == 1 and y_test.ndim == 1
        else:
            y_train = y_train.reshape(-1, 1)
            y_test = y_test.reshape(-1, 1)
            print()
            print("==== Training with 2D labels... ====")
            assert y_train.ndim == 2 and y_test.ndim == 2

        print("Keras model:")
        keras_model = create_keras_model()
        keras_model.fit(X_train, y_train, epochs=n_epochs,
                        batch_size=batch_size, verbose=0)
        print("    Eval:", keras_model.evaluate(X_test, y_test, verbose=0))

        print("Keras model converted to an estimator:")
        keras_estimator, input_name = create_keras_estimator()
        print("    Eval:", train_and_evaluate_estimator(
            keras_estimator, input_name, X_train, y_train, X_test, y_test))

        print("DNNClassifier:")
        dnn_estimator, input_name = create_dnn_estimator()
        print("    Eval:", train_and_evaluate_estimator(
            dnn_estimator, input_name, X_train, y_train, X_test, y_test))
	</description>
	<comments>
		<comment id='1' author='ageron' date='2018-08-18T11:58:42Z'>
		Thank you for providing such a clear minimal case. cc &lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='ageron' date='2018-09-03T06:05:03Z'>
		&lt;denchmark-link:https://github.com/yifeif&gt;@yifeif&lt;/denchmark-link&gt;
 Hi, yifei. Would you have any thoughts on this?
		</comment>
		<comment id='3' author='ageron' date='2019-08-08T22:07:15Z'>
		I think this is a stale issue. I cannot reproduce the issue with the tf-nightly and the results are as expected. Please check below for more details.
&lt;denchmark-code&gt;==== Training with 1D labels currently shows expected output... ====
Keras model:
    Eval: [0.06719633173451293, 0.9787]
Keras model converted to an estimator:
    Eval: ({'acc': 0.9802, 'loss': 0.0662133, 'global_step': 9375}, [])
DNNClassifier:
    Eval: ({'accuracy': 0.1135, 'average_loss': 2.3010228, 'loss': 291.26868, 'global_step': 9375}, [])
&lt;/denchmark-code&gt;

I am closing the issue. If the issue still persists in the newer version of TF, please feel free to reopen it. Thanks!
		</comment>
		<comment id='4' author='ageron' date='2019-08-08T22:07:17Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=21631&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=21631&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>