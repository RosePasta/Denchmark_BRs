<bug id='34634' author='HLSS-Hen' open_date='2019-11-27T05:40:05Z' closed_time='2020-01-02T18:21:09Z'>
	<summary>Why model.Summary() shows all layers in sub model.</summary>
	<description>
In old version(I forget which one). Sub model just shows model's name but not all layers.
But now,it shows all layers.
My versoin: TF2.0(GPU)
demo code:
&lt;denchmark-code&gt;class Swish(keras.layers.Layer):
    def __init__(self):
        super(Swish, self).__init__()
        self.weight = self.add_weight(initializer='uniform',trainable=True)

    def __call__(self, inputs):
        return inputs+tf.sigmoid(self.weight*inputs)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='HLSS-Hen' date='2019-11-27T07:19:17Z'>
		the full code :&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/34637&gt;#34637&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='HLSS-Hen' date='2019-11-28T06:07:20Z'>
		&lt;denchmark-link:https://github.com/HLSS-Hen&gt;@HLSS-Hen&lt;/denchmark-link&gt;
, Tried replicating the issue but got error at . Looks like file or class method is missing. Please provide us more information to reproduce the issue. Thanks!
		</comment>
		<comment id='3' author='HLSS-Hen' date='2019-11-28T06:10:24Z'>
		
@HLSS-Hen, Tried replicating the issue but got error at from Models import MCN. Looks like file or class method is missing. Please provide us more information to reproduce the issue. Thanks!

No, there is.
MCN.py is the file you miss.
		</comment>
		<comment id='4' author='HLSS-Hen' date='2019-11-28T09:09:21Z'>
		&lt;denchmark-link:https://github.com/HLSS-Hen&gt;@HLSS-Hen&lt;/denchmark-link&gt;
, MCN.py file doesn't have Models class method.
		</comment>
		<comment id='5' author='HLSS-Hen' date='2019-11-28T09:57:41Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 Models is just a file.
		</comment>
		<comment id='6' author='HLSS-Hen' date='2019-12-05T06:07:34Z'>
		&lt;denchmark-link:https://github.com/HLSS-Hen&gt;@HLSS-Hen&lt;/denchmark-link&gt;
, Just to verify, MCN.py and Models are they same file or different.
		</comment>
		<comment id='7' author='HLSS-Hen' date='2019-12-05T06:13:54Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;

you can use following code(use any database with shape [?,256,256,3]):
&lt;denchmark-code&gt;import os
os.environ['TF_AUTO_MIXED_PRECISION_GRAPH_REWRITE_IGNORE_PERFORMANCE'] = '1'
import math
import tensorflow as tf
from tensorflow import keras

class Swish(keras.layers.Layer):
    def __init__(self):
        super(Swish, self).__init__()
        self.weight = self.add_weight(initializer='uniform',trainable=True)

    def __call__(self, inputs):
        return inputs+tf.sigmoid(self.weight*inputs)


class Conv(keras.Model):
    def __init__(self,filters,kernel_size=1,strides=1,padding='valid'):
        super(Conv, self).__init__()
        self.conv = keras.layers.Conv2D(filters,kernel_size,strides,padding)
        self.bn = keras.layers.BatchNormalization()
        self.ac = Swish()

    def __call__(self,inputs):
        return self.ac(self.bn(self.conv(inputs)))


class SEBlock(keras.Model):
    def __init__(self, filters):
        super(SEBlock, self).__init__()
        self.conv0 = keras.layers.Conv2D(filters//4,1,1)
        self.drop = keras.layers.Dropout(0.25)
        self.conv1 = keras.layers.Conv2D(filters,1,1)
        self.bn = keras.layers.BatchNormalization()
        self.ac = Swish()

    def __call__(self,inputs):
        x = self.conv1(self.drop(self.conv0(tf.reduce_mean(inputs,[1,2],keepdims=True))))
        return self.ac(self.bn(tf.sigmoid(x)*inputs))


class ResBlock(keras.Model):
    def __init__(self, filters):
        super(ResBlock, self).__init__()
        self.conv0 = keras.layers.Conv2D(filters//4,1,1)
        self.drop = keras.layers.Dropout(0.25)
        self.conv1 = keras.layers.Conv2D(filters,3,1,'same')
        self.bn = keras.layers.BatchNormalization()
        self.ac = Swish()

    def __call__(self,inputs):
        x = self.conv1(self.drop(self.conv0(inputs)))
        return self.ac(self.bn(inputs+x))

def mcn_520(width, growth,input_shape=[256,256,3]):
    fs = int(width*growth)
    inputs=keras.layers.Input(input_shape)
    x=keras.layers.Conv2D(fs,8,2)(inputs)
    x=keras.layers.MaxPool2D(2)(x)
    x1=Conv(fs//width)(SEBlock(fs)(x))
    x2=Conv(fs//width)(ResBlock(fs)(x))
    for i, depth in enumerate([2, 3, 5, 4]):
        for _ in range(int(6*depth)):
            fs+=int(math.sqrt(fs*width))
            t=keras.layers.Concatenate()([x,x1,x2])
            t=keras.layers.Dropout(0.25)(t)
            t=Conv(fs//width, 1, 1)(t)
            t=keras.layers.Dropout(0.25)(t)
            x1=SEBlock(fs//width)(t)
            x2=ResBlock(fs//width)(t)
            t=keras.layers.Concatenate()([t,x1,x2])
            t=keras.layers.Dropout(0.25)(t)
            t=Conv(growth,1,1)(t)
            x=keras.layers.Concatenate()([x,t])
        if i != 3:
            fs //= 2
            x=keras.layers.MaxPool2D(2)(Conv(fs)(x))
            x1=keras.layers.MaxPool2D(2)(Conv(fs//width)(x1))
            x2=keras.layers.MaxPool2D(2)(Conv(fs//width)(x2))
    x=keras.layers.GlobalMaxPool2D()(x)
    x=keras.layers.Dropout(0.25)(x)
    outputs=keras.layers.Dense(1000,activation='softmax')(x)
    return keras.Model(inputs=inputs,outputs=outputs,name='MCN520')

for gpu in tf.config.experimental.list_physical_devices('GPU'):
    tf.config.experimental.set_memory_growth(gpu, True)
logical_gpus = tf.config.experimental.list_logical_devices('GPU')

starategy=tf.distribute.MirroredStrategy()
with starategy.scope():
    model=MCN.mcn_520(2,24)
    model.summary()
    model.compile(
        loss=tf.keras.losses.SparseCategoricalCrossentropy(),
        optimizer=tf.keras.optimizers.SGD(),
        metrics=[tf.keras.metrics.TopKCategoricalAccuracy(1,'top1'),tf.keras.metrics.TopKCategoricalAccuracy(5,'top5')]
        )
    fit_ds,val_ds=ds(BATCH_SIZE)
    model.fit(
        fit_ds, # please use your database
        epochs=1000000,
        steps_per_epoch=BATCHS_PER_APLY_GRADIENTS*200,
    )
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='HLSS-Hen' date='2019-12-12T10:39:02Z'>
		&lt;denchmark-link:https://github.com/HLSS-Hen&gt;@HLSS-Hen&lt;/denchmark-link&gt;
, Please find the attached the &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/68712f12f85f31da6a6fce73cde257e4/untitled295.ipynb&gt;gist&lt;/denchmark-link&gt;
 and help us in reproducing the issue. Thanks!
		</comment>
		<comment id='9' author='HLSS-Hen' date='2019-12-16T07:58:54Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
  , sorry I took some time to  visit the page. And my google account have some wrong.
use full code here:
&lt;denchmark-code&gt;import os
os.environ['TF_AUTO_MIXED_PRECISION_GRAPH_REWRITE_IGNORE_PERFORMANCE'] = '1'
import math
import tensorflow as tf
from tensorflow import keras

class Swish(keras.layers.Layer):
    def __init__(self):
        super(Swish, self).__init__()
        self.weight = self.add_weight(initializer='uniform',trainable=True)

    def __call__(self, inputs):
        return inputs+tf.sigmoid(self.weight*inputs)


class Conv(keras.Model):
    def __init__(self,filters,kernel_size=1,strides=1,padding='valid'):
        super(Conv, self).__init__()
        self.conv = keras.layers.Conv2D(filters,kernel_size,strides,padding)
        self.bn = keras.layers.BatchNormalization()
        self.ac = Swish()

    def __call__(self,inputs):
        return self.ac(self.bn(self.conv(inputs)))


class SEBlock(keras.Model):
    def __init__(self, filters):
        super(SEBlock, self).__init__()
        self.conv0 = keras.layers.Conv2D(filters//4,1,1)
        self.drop = keras.layers.Dropout(0.25)
        self.conv1 = keras.layers.Conv2D(filters,1,1)
        self.bn = keras.layers.BatchNormalization()
        self.ac = Swish()

    def __call__(self,inputs):
        x = self.conv1(self.drop(self.conv0(tf.reduce_mean(inputs,[1,2],keepdims=True))))
        return self.ac(self.bn(tf.sigmoid(x)*inputs))


class ResBlock(keras.Model):
    def __init__(self, filters):
        super(ResBlock, self).__init__()
        self.conv0 = keras.layers.Conv2D(filters//4,1,1)
        self.drop = keras.layers.Dropout(0.25)
        self.conv1 = keras.layers.Conv2D(filters,3,1,'same')
        self.bn = keras.layers.BatchNormalization()
        self.ac = Swish()

    def __call__(self,inputs):
        x = self.conv1(self.drop(self.conv0(inputs)))
        return self.ac(self.bn(inputs+x))

def mcn_520(width, growth,input_shape=[256,256,3]):
    fs = int(width*growth)
    inputs=keras.layers.Input(input_shape)
    x=keras.layers.Conv2D(fs,8,2)(inputs)
    x=keras.layers.MaxPool2D(2)(x)
    x1=Conv(fs//width)(SEBlock(fs)(x))
    x2=Conv(fs//width)(ResBlock(fs)(x))
    for i, depth in enumerate([2, 3, 5, 4]):
        for _ in range(int(6*depth)):
            fs+=int(math.sqrt(fs*width))
            t=keras.layers.Concatenate()([x,x1,x2])
            t=keras.layers.Dropout(0.25)(t)
            t=Conv(fs//width, 1, 1)(t)
            t=keras.layers.Dropout(0.25)(t)
            x1=SEBlock(fs//width)(t)
            x2=ResBlock(fs//width)(t)
            t=keras.layers.Concatenate()([t,x1,x2])
            t=keras.layers.Dropout(0.25)(t)
            t=Conv(growth,1,1)(t)
            x=keras.layers.Concatenate()([x,t])
        if i != 3:
            fs //= 2
            x=keras.layers.MaxPool2D(2)(Conv(fs)(x))
            x1=keras.layers.MaxPool2D(2)(Conv(fs//width)(x1))
            x2=keras.layers.MaxPool2D(2)(Conv(fs//width)(x2))
    x=keras.layers.GlobalMaxPool2D()(x)
    x=keras.layers.Dropout(0.25)(x)
    outputs=keras.layers.Dense(1000,activation='softmax')(x)
    return keras.Model(inputs=inputs,outputs=outputs,name='MCN520')

for gpu in tf.config.experimental.list_physical_devices('GPU'):
    tf.config.experimental.set_memory_growth(gpu, True)
logical_gpus = tf.config.experimental.list_logical_devices('GPU')

starategy=tf.distribute.MirroredStrategy()
with starategy.scope():
    model=mcn_520(2,24)
    model.summary()
    model.compile(
        loss=tf.keras.losses.SparseCategoricalCrossentropy(),
        optimizer=tf.keras.optimizers.SGD(),
        metrics=[tf.keras.metrics.TopKCategoricalAccuracy(1,'top1'),tf.keras.metrics.TopKCategoricalAccuracy(5,'top5')]
        )
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='HLSS-Hen' date='2019-12-17T07:40:56Z'>
		I could replicate the issue with Tf 2.0.
Please see the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/955ccf74deb3065fb10a17cd08a0939d/untitled295.ipynb#scrollTo=chYTIWSIrgzX&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='11' author='HLSS-Hen' date='2019-12-20T23:52:52Z'>
		&lt;denchmark-link:https://github.com/HLSS-Hen&gt;@HLSS-Hen&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29472#issue-452812370&gt;This&lt;/denchmark-link&gt;
 model is related to your issue but check the approach ('base_model') used to show pretrained model as one layer. Here is the colab &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/0b278a807fa9b91e9ad4fd5d701c3081/untitled722.ipynb&gt;gist&lt;/denchmark-link&gt;
.  Thanks.
		</comment>
		<comment id='12' author='HLSS-Hen' date='2020-01-02T18:21:09Z'>
		Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
		<comment id='13' author='HLSS-Hen' date='2020-01-02T18:21:11Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34634&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34634&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>