<bug id='26020' author='ghost(ghost)' open_date='2019-02-22T20:18:27Z' closed_time='2019-05-03T00:57:20Z'>
	<summary>MirroredStrategy doesn't use GPUs</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): pip install tensorflow-gpu==1.12
TensorFlow version (use command below): 1.12
Python version: 3.5.2
CUDA/cuDNN version: CUDA 9 / cuDNN 7.3
GPU model and memory: 2 x Nvidia Titan X

Describe the current behavior
I was working on rewriting a script from the queue/threading approach to tf.data.Dataset approach of providing data. I got really nice throughput of data with over &gt;90% util of GPUs.  Now that I have rewritten it, when starting the training with MirroredStrategy the GPUs are not used at all and I get the following output:
&lt;denchmark-code&gt;INFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0
INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:0
INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_CPU:0
INFO:tensorflow:Configured nccl all-reduce.
INFO:tensorflow:Calling model_fn.
...
&lt;/denchmark-code&gt;

At this point I am thinking there is some issue with TF1.12.
Code to reproduce the issue
Here is basically the structure of my code. I tried out different things like train directly with tf.keras.fit with multi_gpu_model() but it didn't work out. Basically I am trying to reproduce the functionality of my RandomShuffleQueue() I had before with multiple threads filling up the queue.
&lt;denchmark-code&gt;model = models.Model(inputs=input, outputs=y_output)
optimizer = tf.train.AdamOptimizer(LEARNING_RATE)
# model = utils.multi_gpu_model(model, gpus=NUM_GPUS, cpu_relocation=True)
model.compile(loss=lossFunc, optimizer=optimizer)

def generator(n):
    while True:
        try:
            imgBatch = []
            
            ...
            
            yield imgBatch
                
        except ValueError:
            pass
        
def get_generator(n):
    return partial(generator, n)

def dataset(n):
    return tf.data.Dataset.from_generator(get_generator(n), output_types=[(tf.float32, tf.float32)], output_shapes=[(tf.TensorShape([None,None,1]),tf.TensorShape([None,None,1]))

def input_fn():
    ds = tf.data.Dataset.range(len(dataSets)).apply(tf.data.experimental.parallel_interleave(dataset, cycle_length=len(dataSets), sloppy=True))
    ds = ds.map(map_func=lambda imgBatch: processImage(img,lbl), num_parallel_calls=12)
    ds = ds.shuffle(SHUFFLE_SIZE)
    ds = ds.batch(BATCH_SIZE)
    ds = ds.prefetch(1)
    return ds
                                                                                                                    
strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=NUM_GPUS)
config = tf.estimator.RunConfig(train_distribute=strategy)
estimator = tf.keras.estimator.model_to_estimator(model,
                                                  config=config)
                                                                                                                    
estimator.train(lambda:input_fn())
&lt;/denchmark-code&gt;

Any help would be greatly appreciated since I'm stuck on it since a while now.
	</description>
	<comments>
		<comment id='1' author='ghost(ghost)' date='2019-02-26T07:16:30Z'>
		That output looks fine and is expected. The devices it should be using are "/device:GPU:0" etc which it doesn't complain about.
Why do you think GPUs are not being used?
		</comment>
		<comment id='2' author='ghost(ghost)' date='2019-02-26T09:00:21Z'>
		I see. The info seemed strange to me. But also I I don't see a lot of util on my GPUs - to be exact it is about 5%/0%, peaking at around 27%/10% sometimes in spikes here and there. In contrast when using the old queue/threaded approach (using a RandomShuffleQueue) instead of tf.data.Dataset, I had a much higher util on the GPUs - about 50% on each GPU without any break.
So I'm thinking there must be something wrong with either my input pipeline or the initialization of the GPUs. Any ideas? Maybe there is some issue on how I use the Dataset API and there is some bottleneck regarding creating the data. I am creating multiple Generators to simulate multi-threaded providing of data for shuffling. Appreciate any help since I'm struggling with that for quite some time now.
Also any ideas how i could debug that issue?
		</comment>
		<comment id='3' author='ghost(ghost)' date='2019-03-12T08:34:32Z'>
		Is there actually a proper way to provide the Dataset Shuffle with data in a multi-threaded way? The solution that I did feels kind of hacky and not proper. Might also be the reason it doesn't work as intended.
		</comment>
		<comment id='4' author='ghost(ghost)' date='2019-03-12T11:00:23Z'>
		I believe it's an input pipeline issue. Could you check out the &lt;denchmark-link:https://www.tensorflow.org/guide/performance/datasets&gt;performance guide&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='5' author='ghost(ghost)' date='2019-05-03T00:57:20Z'>
		You could monitor $ nvidia-smi during your training to see whether GPUs are used as well.
Please re-open if you think it's not an input pipeline issue and still suspect TF not utilizing available GPUs.
		</comment>
		<comment id='6' author='ghost(ghost)' date='2019-05-03T00:57:21Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=26020&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=26020&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>