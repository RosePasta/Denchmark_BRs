<bug id='40652' author='lokesharma-dev' open_date='2020-06-21T22:12:01Z' closed_time='2020-06-22T20:16:56Z'>
	<summary>Gradient with respect to input returns None using GradientTape().</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
TensorFlow version (use command below): 2.2.0

Describe the current behavior
I am trying to calculate gradient for a KL divergence scalar loss value wrt input.
Implementation has been with two approaches and returns [None] in both cases.

Differentiate wrt to a tensor. Output: None
Differentiate wrt to a Variable. Output: None

The following works with TensorFlow 1.x which used tf.gradients() however, GradientTape() is returning None and it very frustrating now. As I have been trying to overcome this for the last 3 weeks. Please provide some support.
The colab link is provided below.
Describe the expected behavior
Gradient matrix should contain values and until that I can't work on building my model.

Updated link:
&lt;denchmark-link:url&gt;https://colab.research.google.com/drive/1hwFX3VMzKtfRjqd5-EG-0WePvtCbmChM?usp=sharing&lt;/denchmark-link&gt;


The below code is taken from this source:
[https://gist.github.com/divamgupta/c778c17459c1f162e789560d5e0b2f0b]
Also followed a similar issue on this link: &lt;denchmark-link:url&gt;https://github.com/tensorflow/tensorflow/issues/36596&lt;/denchmark-link&gt;
 . But it doesn't help me to solve my problem.
Any help is highly appreciated. Thanks for your time
	</description>
	<comments>
		<comment id='1' author='lokesharma-dev' date='2020-06-22T05:15:44Z'>
		&lt;denchmark-link:https://github.com/lokesharma-dev&gt;@lokesharma-dev&lt;/denchmark-link&gt;

The colab gist shared is empty, can you please check and share again or paste the code here.
		</comment>
		<comment id='2' author='lokesharma-dev' date='2020-06-22T06:08:59Z'>
		&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;

I have and the link is working. Please confirm the same.
&lt;denchmark-link:https://colab.research.google.com/drive/1hwFX3VMzKtfRjqd5-EG-0WePvtCbmChM?usp=sharing&gt;https://colab.research.google.com/drive/1hwFX3VMzKtfRjqd5-EG-0WePvtCbmChM?usp=sharing&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='lokesharma-dev' date='2020-06-22T08:51:21Z'>
		I am able to replicate the issue faced, please find the &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/35a6dcdb1ef38079e49606bf44bb3c74/untitled238.ipynb&gt;gist here&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='lokesharma-dev' date='2020-06-22T13:59:58Z'>
		isn't this is a coding error instead of a bug?  You are asking for gradient of kl with respect to r, but kl only depends on p_logit and p_logit_r inside gradienttape. p_logit_r need to interact with r after the tape watches r, cannot interact prior to the watch.
		</comment>
		<comment id='5' author='lokesharma-dev' date='2020-06-22T14:17:48Z'>
		That was my first attempt, however, it leads to a numpy Attribute error and I have cross-checked elsewhere. All variables are Tensor objects. Error attached for the below modification. Thanks.
&lt;denchmark-link:https://user-images.githubusercontent.com/63437596/85297700-624b6a80-b4a3-11ea-9e29-b8d54f3fe121.png&gt;&lt;/denchmark-link&gt;

`model_input = Input((2,))
p_logit = model(model_input)
p = Activation('softmax')(model_input)
r = tf.random.uniform(shape=tf.shape(model_input))
r = make_unit_norm(r)
#p_logit_r = model(model_input + 10*r)
print('Shape of the noise tensor: ', r)
with tf.GradientTape(watch_accessed_variables=False) as tape:
tape.watch(r)
p_logit_r = model(model_input + 10*r)
kl = tf.reduce_mean(compute_kld(p_logit, p_logit_r))
grad_kl = tape.gradient(kl, r)
print("Shape of Gradient Matrix : ", grad_kl)`
Updated link: &lt;denchmark-link:url&gt;https://colab.research.google.com/drive/1hwFX3VMzKtfRjqd5-EG-0WePvtCbmChM?usp=sharing#scrollTo=GJna81gE2OrE&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='lokesharma-dev' date='2020-06-22T15:24:30Z'>
		that is because r is not an eager tensor because of how you created it, shape is not well defined. You cannot use the shape of a symbolic tensor as the shape to create an eager tensor. Moreover, why model_input + 10*r, since model_input is keras symbolic tensor waiting for input but why you put the symbolic input tensor to the input of the model? That does not make sense.
I think you are confusing Tensorflow 1 API with Tensorflow 2 which is eager executed by default.
		</comment>
		<comment id='7' author='lokesharma-dev' date='2020-06-22T15:43:13Z'>
		whatever you are trying to do, slightly modifying your code and this one will run without error.
# Import Libraries
from numpy.random import seed
seed(0)
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
tf.random.set_seed(0)
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input, Activation
from sklearn.metrics import accuracy_score
from sklearn import datasets


# Generate dataset
circles = datasets.make_circles(n_samples=1000, noise=.05, factor=.3, random_state=3)
circles_test = datasets.make_circles(n_samples=1000, noise=0, factor=.3, random_state=1)

n_points = 8
inds = list(np.where(circles[1] == 0)[0][:n_points]) + list(np.where(circles[1] == 1)[0][:n_points])

x_train = circles[0][inds]
y_train = circles[1][inds]
y_train_cat = tf.keras.utils.to_categorical(circles[1][inds])

x_test = circles_test[0][inds]
y_test = circles_test[1][inds]
y_test_cat = tf.keras.utils.to_categorical(circles_test[1][inds])

n_classes = int(np.max(y_train)+1)

# Plot data points
plt.scatter(x_test[:,0], x_test[:,1], c=y_test, s=20, cmap='winter', edgecolors='none', alpha=0.005)
plt.scatter(x_train[:,0], x_train[:,1], c=y_train, s=20, cmap='winter', edgecolors='k')

# VAT Model
model = Sequential()
model.add(Dense(100, activation='relu', input_shape=(2,)))
model.add(Dense(2, activation='softmax'))
model.compile('sgd', 'categorical_crossentropy', metrics=['accuracy'])

def compute_kld(p_logit, q_logit):
  p = tf.nn.softmax(p_logit)
  q = tf.nn.softmax(q_logit)
  kl_score = tf.reduce_sum( p * (tf.math.log(p+1e-16) - tf.math.log(q+1e-16)), axis = 1)
  print(type(kl_score))
  return kl_score # lower kl means closer the distributions are

def make_unit_norm(x):
    return x/(tf.reshape(tf.sqrt(tf.reduce_sum(tf.pow(x, 2.0), axis=1)), [-1, 1]) + 1e-16)


r = tf.random.uniform(shape=(64, 2))
r = make_unit_norm(r)

with tf.GradientTape(watch_accessed_variables=False) as tape:
  tape.watch(r)
  p_logit_r = model(10*r)
  kl = tf.reduce_mean(compute_kld(r, p_logit_r))

grad_kl = tape.gradient(kl, r)

print(grad_kl)
		</comment>
		<comment id='8' author='lokesharma-dev' date='2020-06-22T20:16:56Z'>
		&lt;denchmark-link:https://github.com/henrysky&gt;@henrysky&lt;/denchmark-link&gt;
 Thanks for your suggestion. It helped me to understand how to differentiate with respect to the input.  However, the correct solution to this issue would be the following.
`r = tf.random.uniform(shape=tf.shape(v), minval=0.001, maxval=0.005)
r = tf.add(v,r)
with tf.GradientTape() as tape:
tape.watch(r)
h1 = layers.LSTM(units=128, name='LSTM_layer_r')(r)
p_logit_r = layers.Dense(units=10, name='Dense_layer_r')(h1)
kl = compute_kld(p_logit, p_logit_r)
grad = tape.gradient(kl, r)
print(grad)`
Just in case someone comes across a similar issue. This will help them.
		</comment>
		<comment id='9' author='lokesharma-dev' date='2020-06-22T20:16:57Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40652&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40652&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>