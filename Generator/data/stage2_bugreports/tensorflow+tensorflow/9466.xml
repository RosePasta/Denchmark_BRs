<bug id='9466' author='alquraishi' open_date='2017-04-26T17:02:47Z' closed_time='2018-01-05T19:31:27Z'>
	<summary>Improved documentation for TensorArray</summary>
	<description>
The current documentation for  could use an expanded introduction explaining its utility and main motivation, examples of actual usage, and further explanation of some of its methods. The docs &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/TensorArray&gt;here&lt;/denchmark-link&gt;
 do not have a single usage example. The overall description of the section is just:
"This class is meant to be used with dynamic iteration primitives such as while_loop and map_fn. It supports gradient back-propagation via special "flow" control flow dependencies."
How is meant to be used with dynamic iteration primitives? What does that entail and why do we need a special construct for dynamic iteration? And what is "flow"? It's very cryptic as it stands. Furthermore, some methods are very poorly described, like . It's unclear if it needs to be invoked at the end, or what? &lt;denchmark-link:http://stackoverflow.com/questions/41113004/what-is-the-effect-of-calling-tensorarray-close&gt;Comments on stack overflow&lt;/denchmark-link&gt;
 only add to the confusion. With the increasing importance of dynamic graphs, a basic construct like  should really be well documented, like .
	</description>
	<comments>
		<comment id='1' author='alquraishi' date='2017-04-26T18:03:48Z'>
		Added to the doc fixit.  Assigning to &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 for now.
		</comment>
		<comment id='2' author='alquraishi' date='2017-04-27T04:13:17Z'>
		it does need improved documentation. no promises when that documentation will materialize...
		</comment>
		<comment id='3' author='alquraishi' date='2017-04-27T04:13:48Z'>
		regarding examples, see the unit tests which have a variety of usage examples.
		</comment>
		<comment id='4' author='alquraishi' date='2017-04-27T15:50:19Z'>
		Ok I will look forward to the full docs and will peruse the unit tests for now. In the meantime, would it be possible to at least explain what close is doing and if there's any benefit to calling it in terms of freeing memory or some such?
		</comment>
		<comment id='5' author='alquraishi' date='2017-06-22T19:24:28Z'>
		which unit tests are you referring to?
		</comment>
		<comment id='6' author='alquraishi' date='2017-12-22T07:31:28Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='7' author='alquraishi' date='2018-01-05T19:13:18Z'>
		Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='8' author='alquraishi' date='2018-01-05T19:31:27Z'>
		The unit tests are &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/tensor_array_ops_test.py&gt;here&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='9' author='alquraishi' date='2019-11-04T20:40:15Z'>
		The world is still confused about best practices for calling .close().  Does anybody know?
		</comment>
		<comment id='10' author='alquraishi' date='2019-11-04T20:45:32Z'>
		Sorry; in TF2 does one of two things:

In eager mode, it deletes the underlying values in the tensorarray (essentially releases memory)
In tf.function building mode, it does nothing.  The underlying data is released when there are no more ops referencing it.

This is kinda inconsistent.  In eager mode, reusing the TensorArray after close will lead to errors.  In tf.function mode, it'll work just fine (and in that case close has no effect).  &lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 if we care enough about this inconsistency.
		</comment>
		<comment id='11' author='alquraishi' date='2019-11-04T21:17:21Z'>
		Ok gotcha that is very helpful
		</comment>
	</comments>
</bug>