<bug id='37561' author='FLming' open_date='2020-03-13T06:27:27Z' closed_time='2020-06-18T02:16:25Z'>
	<summary>When using MirroredStrategy, the training process is stuck</summary>
	<description>
System information

OS Platform and Distribution: Ubuntu 18.04
Python version: 3.6.9
CUDA/cuDNN version: 2x1080 Ti
Tensorflow version: 2.2.0

Describe the current behavior
I built a CRNN model for recognize the text, a CNN-RNN-CTC model, it work well without tf.distribute.Strategy, but when I follow the guide to move the creation and compiling of Keras model inside strategy.scope, It does not start distributed training like the guide. here is the output:
&lt;denchmark-code&gt;2020-03-13 06:19:04.839700: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-03-13 06:19:04.874256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s
2020-03-13 06:19:04.875077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: 
pciBusID: 0000:02:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2020-03-13 06:19:04.875273: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-03-13 06:19:04.876811: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-03-13 06:19:04.878155: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-03-13 06:19:04.878383: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-03-13 06:19:04.879998: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-03-13 06:19:04.881014: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-03-13 06:19:04.884664: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-03-13 06:19:04.887439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1
2020-03-13 06:19:04.887722: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-03-13 06:19:04.892905: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3598110000 Hz
2020-03-13 06:19:04.893743: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f1224000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-03-13 06:19:04.893784: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-03-13 06:19:05.147750: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x59b91e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-03-13 06:19:05.147778: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2020-03-13 06:19:05.147788: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce GTX 1080 Ti, Compute Capability 6.1
2020-03-13 06:19:05.150073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s
2020-03-13 06:19:05.150888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: 
pciBusID: 0000:02:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2020-03-13 06:19:05.150926: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-03-13 06:19:05.150943: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-03-13 06:19:05.150958: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-03-13 06:19:05.150972: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-03-13 06:19:05.150986: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-03-13 06:19:05.151000: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-03-13 06:19:05.151015: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-03-13 06:19:05.154118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1
2020-03-13 06:19:05.154160: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-03-13 06:19:05.156174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-03-13 06:19:05.156195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1 
2020-03-13 06:19:05.156206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N Y 
2020-03-13 06:19:05.156214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   Y N 
2020-03-13 06:19:05.158304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9569 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2020-03-13 06:19:05.159295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10369 MB memory) -&gt; physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)
Epoch 1/5
&lt;/denchmark-code&gt;

The training process seems to be interrupted without any output, but the program runs out of graphics card memory and the program does not exit.
Describe the expected behavior
Start training just like without distributed strategy.

The full code can be view &lt;denchmark-link:https://github.com/FLming/CRNN.tf2&gt;here&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='FLming' date='2020-03-26T17:28:13Z'>
		Could you try tf.experimental.output_all_intermediates(True)?
It would be also easier if you can provide a smaller reproduction.
		</comment>
		<comment id='2' author='FLming' date='2020-03-27T01:51:57Z'>
		The code is available in &lt;denchmark-link:https://github.com/FLming/CRNN.tf2&gt;CRNN.tf2&lt;/denchmark-link&gt;
, To reproduce this error, clone it, add MirroredStrategy to train.py, then run it with example data by:
python train.py -ta example/annotation.txt -tf example -va example/annotation.txt -vf example -t example/table.txt
The training process will stuck in first epoch.
&lt;denchmark-code&gt;...
Epoch 1/30
&lt;/denchmark-code&gt;

I tried adding drop_remainder=True to Dataset.batch function, still stuck.
		</comment>
		<comment id='3' author='FLming' date='2020-04-07T02:08:08Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37561&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37561&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='FLming' date='2020-06-16T09:33:23Z'>
		This problem still exists, I can't find a way to solve it. There are no errors, and the GPU is not calculating, but it takes up GPU memory.
		</comment>
		<comment id='5' author='FLming' date='2020-06-16T13:22:21Z'>
		&lt;denchmark-link:https://github.com/FLming&gt;@FLming&lt;/denchmark-link&gt;
 Can you please try adding  and  to  and then test whether the error persists or not? Thanks!
		</comment>
		<comment id='6' author='FLming' date='2020-06-17T01:29:05Z'>
		I set steps_per_epoch to number of training samples // batch size and validation_steps to number of val samples // batch size, the problem still exists.
		</comment>
		<comment id='7' author='FLming' date='2020-06-17T01:41:21Z'>
		One more thing, although it’s a digression. Once I used DepthwiseConv2D to build the model, the same thing happened, there was no progress bar, no error, and the output was only
&lt;denchmark-code&gt;...
Epoch 1/x
&lt;/denchmark-code&gt;

By chance I switched the program from GPU to CPU, the error raised, that said current implementation only supports equal length strides in the row and column dimensions. Follow this tip, the program worked on GPU.
Will tensorflow lose some output prompts when running on the GPU?
		</comment>
		<comment id='8' author='FLming' date='2020-06-17T04:13:18Z'>
		&lt;denchmark-link:https://github.com/FLming&gt;@FLming&lt;/denchmark-link&gt;
 Code that is running on CPU and GPU are similar but they are little different from implementation side so that the GPU accelerators are optimally used.
If this is resolved for you, then please close the issue. Thanks!
		</comment>
		<comment id='9' author='FLming' date='2020-06-17T04:19:39Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 No, the problem still exists. The above is another problem that  is't support different stride setting, and on GPU, the prompt doesn't appear.
		</comment>
		<comment id='10' author='FLming' date='2020-06-17T07:08:08Z'>
		Things have progressed.

I removed custom Metric class in model.fit, the distributed training works normally, but why?
Compile with custom metric class, set validation_data to None, still stuck.

Here is the definition, this class will compare two sparse tensors and calculate how many equal rows
&lt;denchmark-code&gt;class WordAccuracy(keras.metrics.Metric):
    """
    Calculate the word accuracy between y_true and y_pred.
    """
    def __init__(self, name='word_accuracy', **kwargs):
        super().__init__(name=name, **kwargs)
        self.total = self.add_weight(name='total', dtype=tf.int32, 
                                     initializer=tf.zeros_initializer())
        self.count = self.add_weight(name='count', dtype=tf.int32, 
                                     initializer=tf.zeros_initializer())
                
    def update_state(self, y_true, y_pred, sample_weight=None):
        """
        Maybe have more fast implementation.
        """
        b = tf.shape(y_true)[0]
        max_width = tf.maximum(tf.shape(y_true)[1], tf.shape(y_pred)[1])
        logit_length = tf.fill([tf.shape(y_pred)[0]], tf.shape(y_pred)[1])        
        decoded, _ = tf.nn.ctc_greedy_decoder(
            inputs=tf.transpose(y_pred, perm=[1, 0, 2]),
            sequence_length=logit_length)
        y_true = tf.sparse.reset_shape(y_true, [b, max_width])
        y_pred = tf.sparse.reset_shape(decoded[0], [b, max_width])
        y_true = tf.sparse.to_dense(y_true, default_value=-1)
        y_pred = tf.sparse.to_dense(y_pred, default_value=-1)
        y_true = tf.cast(y_true, tf.int32)
        y_pred = tf.cast(y_pred, tf.int32)
        values = tf.math.reduce_any(tf.math.not_equal(y_true, y_pred), axis=1)
        values = tf.cast(values, tf.int32)
        values = tf.reduce_sum(values)
        self.total.assign_add(b)
        self.count.assign_add(b - values)

    def result(self):
        return self.count / self.total

    def reset_states(self):
        self.count.assign(0)
        self.total.assign(0)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='FLming' date='2020-06-18T02:16:25Z'>
		I found the reason, that is because I use dtype=tf.int32 in self.add_weight function.
		</comment>
		<comment id='12' author='FLming' date='2020-06-18T02:16:27Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37561&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37561&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='FLming' date='2020-10-08T09:47:49Z'>
		
I found the reason, that is because I use dtype=tf.int32 in self.add_weight function.
@FLming can you please share your experience how you found the root cause?

		</comment>
		<comment id='14' author='FLming' date='2020-10-09T08:07:40Z'>
		Comment and run, step by step, narrow the scope.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


 在 2020年10月8日，下午5:48，Boggis30 ***@***.***&gt; 写道：

 ﻿
 I found the reason, that is because I use dtype=tf.int32 in self.add_weight function.
 @FLming can you please share your experience how you found the root cause?

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub, or unsubscribe.


		</comment>
	</comments>
</bug>