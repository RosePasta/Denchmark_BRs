<bug id='8566' author='sullivak' open_date='2017-03-20T21:16:22Z' closed_time='2017-06-16T21:45:48Z'>
	<summary>avg_pool() is quietly returning garbage.</summary>
	<description>
I'm seeing an issue with various networks where, depending on image input size, it either works, crashes with a CUDA_ERROR_ILLEGAL_ADDRESS error, or, most worryingly, runs without issue but visually has strange artifacts (see attached). Does not happen if running on CPU.
OS: Ubuntu 16.04. Running docker image based on tensorflow/tensorflow:1.0.1-gpu, with nvidia-docker 17.03.0-ce. GPUs: 2x Titan X (Pascal). Was also able to replicate running native on similar machine with same OS and hardware.
Possibly related github issues:  &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/6509&gt;#6509&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/4425&gt;#4425&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/3422&gt;#3422&lt;/denchmark-link&gt;
 but these seem to be generally fixed by upgrading to same or earlier tensorflow or cudnn versions.
ls -l /usr/local/nvidia/lib64/libcud*
lrwxrwxrwx 1  999  999      17 Nov 21 20:42 /usr/local/nvidia/lib64/libcuda.so -&gt; libcuda.so.370.28
lrwxrwxrwx 1  999  999      17 Nov 21 20:42 /usr/local/nvidia/lib64/libcuda.so.1 -&gt; libcuda.so.370.28
-rw-r--r-- 2 root root 8219624 Sep  2  2016 /usr/local/nvidia/lib64/libcuda.so.370.28
This code can demonstrate the issues. Changing run_type changes the effect, hopefully each option is clear. Problem here starts after avg_pool, but I've seen with other outputs, e.g. activation functions.
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf

run_type = "gpu_artifacts"  # "gpu_works" "gpu_artifacts" "gpu_crashes" "cpu"
if run_type == "gpu_works":
    num_gpus = 1
    num_rows = 900
elif run_type == "gpu_crashes":
    num_gpus = 1
    num_rows = 950
elif run_type == "gpu_artifacts":
    num_gpus = 1
    num_rows = 1000
elif run_type == "cpu":
    num_gpus = 0
    num_rows = 950

num_cols = 1900
xx, yy = np.meshgrid(np.linspace(0, 2 * np.pi, num_cols), np.linspace(0, 2 * np.pi, num_rows))
faux_img = np.zeros((num_rows, num_cols, 3))
faux_img[:, :, 0] = np.sin(xx) + np.cos(yy)
faux_img[:, :, 1] = np.cos(xx) + np.cos(yy)
faux_img[:, :, 2] = np.cos(xx) + np.sin(yy)

plt.figure()
plt.imshow(faux_img)
plt.title("input")

input_shape = faux_img.shape
input_shape = np.append([-1], input_shape)
inputs = np.array([faux_img.copy().ravel()])

x_in_ravel = tf.placeholder(tf.float32, [None, np.prod(input_shape[1::])], name="x_in_ravel")
x_in = tf.reshape(x_in_ravel, input_shape)
filt_vals = np.random.randn(5, 5, 3, 30).astype(np.float32)
W_0 = tf.Variable(tf.constant(filt_vals))
strides = [1, 1, 1, 1]
conv_out = tf.nn.conv2d(x_in, W_0, strides=strides, padding="VALID", name="conv1")
pool_out = tf.nn.avg_pool(conv_out, ksize=[1, 9, 9, 1], strides=strides, padding="VALID", name="pool1")

config = tf.ConfigProto(device_count={'GPU': num_gpus})
sess = tf.Session(config=config)
sess.run(tf.global_variables_initializer())
outs_conv, outs_pool = sess.run([conv_out, pool_out], feed_dict={x_in_ravel: inputs})

filt_out_fig, filt_out_ax = plt.subplots()
pool_out_fig, pool_out_ax = plt.subplots()
filt_out_ax.imshow(outs_conv[0, :, :, 15])
filt_out_ax.set_title("conv2d output, looks normal")
pool_out_ax.imshow(outs_pool[0, :, :, 15])
pool_out_ax.set_title("avg_pool output, striping artifacts")

plt.show()
Output for run_type = "gpu_crashes":
&lt;denchmark-code&gt;I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:02:00.0
Total memory: 11.90GiB
Free memory: 11.75GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x37cb5b0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:01:00.0
Total memory: 11.90GiB
Free memory: 10.97GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: TITAN X (Pascal), pci bus id: 0000:01:00.0)
E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS
F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:198] Unexpected Event status: 1
&lt;/denchmark-code&gt;

Output plots for run_type = "gpu_artifacts":
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/3643255/24121814/e47dd59c-0d76-11e7-9b35-4ee1327c0230.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://cloud.githubusercontent.com/assets/3643255/24121821/e806480c-0d76-11e7-839d-b94da94a3c75.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://cloud.githubusercontent.com/assets/3643255/24121824/e9f23176-0d76-11e7-931f-62bb49bf2abd.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='sullivak' date='2017-03-24T16:22:38Z'>
		&lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
, have you seen anything like this?
		</comment>
		<comment id='2' author='sullivak' date='2017-04-28T00:55:48Z'>
		Assigning to protoget@
		</comment>
		<comment id='3' author='sullivak' date='2017-05-04T23:25:44Z'>
		&lt;denchmark-link:https://github.com/sullivak&gt;@sullivak&lt;/denchmark-link&gt;
 please try setting environment variable TF_AVGPOOL_USE_CUDNN=1.
We use Eigen implementation of this operation, if you use the default data_format setting in tf.nn.avg_pool and leaving TF_AVGPOOL_USE_CUDNN=0.(default)
Our investigation shows there's a potential a bug in Eigen SpatialAvgPooling.
Plus Cudnn impl is way faster than Eigen.
		</comment>
		<comment id='4' author='sullivak' date='2017-05-04T23:31:13Z'>
		Good catch, protoget@!
In the past, we couldn't turn this flag on by default, because Cudnn handles boundary differently. Maybe we can try to enable it back again.
		</comment>
		<comment id='5' author='sullivak' date='2017-06-19T18:33:41Z'>
		Not sure how I missed seeing the suggestion from &lt;denchmark-link:https://github.com/protoget&gt;@protoget&lt;/denchmark-link&gt;
 but that seems to work, thanks!
		</comment>
	</comments>
</bug>