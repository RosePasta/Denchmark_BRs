<bug id='14940' author='gauss-clb' open_date='2017-11-28T13:45:35Z' closed_time='2018-01-24T09:04:05Z'>
	<summary>The keys in end_points of slim are not unified for different layers.</summary>
	<description>
Look at the code:
  with tf.name_scope('xx'):
    end_points_collection = 'dd'
    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],
                      outputs_collections=end_points_collection):
      y = slim.conv2d(np.zeros([1,20,20,3],dtype=np.float32), 10, [2, 2])
      x = slim.max_pool2d(np.zeros([1,20,20,3],dtype=np.float32), [2, 2])
      end_points = slim.utils.convert_collection_to_dict(end_points_collection)
  print(end_points)
It output
&lt;denchmark-code&gt;OrderedDict([('Conv', &lt;tf.Tensor 'xx/Conv/Relu:0' shape=(1, 20, 20, 10) dtype=fl
oat32&gt;), ('xx/MaxPool2D', &lt;tf.Tensor 'xx/MaxPool2D/MaxPool:0' shape=(1, 10, 10,
3) dtype=float32&gt;)])
&lt;/denchmark-code&gt;

For max_pool2d layer, the key has prefix xx, but for conv2d layer, it don't have prefix xx. Because in conv2d it uses variable_scope but in max_pool2d it uses name_scope. So the behavior looks inconsistent, and may cause the code make mistake. Do we need to unify the behavior ?
For example, if we use multi-gpu to train the network, and we have many clones of network, which name_scope's prefix  are clone_1,  clone_2 and so on. If we want to use the key of end_points to get the output of layer on different gpu. We should deal with max_pool2d and conv2d  differently.
	</description>
	<comments>
		<comment id='1' author='gauss-clb' date='2017-11-28T18:14:08Z'>
		&lt;denchmark-link:https://github.com/sguada&gt;@sguada&lt;/denchmark-link&gt;
, could you please comment on this?
		</comment>
		<comment id='2' author='gauss-clb' date='2017-11-28T18:45:47Z'>
		If you replace the name_scope with a variable_scope do both align?
In general when creating layers and variables one should use variable_scopes instead of name_scope.
		</comment>
		<comment id='3' author='gauss-clb' date='2017-11-29T01:25:34Z'>
		But in some case, we just need ops should have different name rather than variable, we want the shared variables on different gpu, so we can't use variable_scope.
		</comment>
		<comment id='4' author='gauss-clb' date='2017-12-20T01:26:50Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='5' author='gauss-clb' date='2018-01-03T19:05:22Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='6' author='gauss-clb' date='2018-01-18T19:11:25Z'>
		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='7' author='gauss-clb' date='2018-01-23T22:59:56Z'>
		A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.
		</comment>
	</comments>
</bug>