<bug id='33929' author='bloc97' open_date='2019-11-02T04:43:08Z' closed_time='2020-08-13T17:15:00Z'>
	<summary>Optimizer clipvalue and clipnorm not working in Tensorflow 2.0</summary>
	<description>
System information

Python 3 Google Compute Engine backend (GPU)

Describe the current behavior
clipvalue and clipnorm in Optimizers does nothing!
Describe the expected behavior
By setting clipvalue=0 or clipnorm=0 no training should occur (gradients should be 0!), but the network still trains, and if using a large learning rate, loss goes to nan.

&lt;denchmark-link:https://user-images.githubusercontent.com/567732/68066124-4d01b000-fd09-11e9-8e41-51ece3684869.png&gt;&lt;/denchmark-link&gt;


&lt;denchmark-link:https://user-images.githubusercontent.com/567732/68066145-7589aa00-fd09-11e9-8c94-1ae4bcf33e45.png&gt;&lt;/denchmark-link&gt;



	</description>
	<comments>
		<comment id='1' author='bloc97' date='2019-11-04T07:49:06Z'>
		Looks like code is incomplete.Please provide a simple standalone code to reproduce the issue in our environment, then it is easy for localizing the issue faster. Thanks!
		</comment>
		<comment id='2' author='bloc97' date='2019-11-12T07:39:21Z'>
		&lt;denchmark-link:https://github.com/bloc97&gt;@bloc97&lt;/denchmark-link&gt;

Any update on this issue please. Thanks!
		</comment>
		<comment id='3' author='bloc97' date='2019-11-12T18:47:28Z'>
		I didn't think that standalone code would be needed as this unexpected behavior should be instantly caught by a unit test. But here you go:
&lt;denchmark-link:https://colab.research.google.com/drive/16gxaqBuY7NRYZaDqeV26yH7caLChTLY6&gt;https://colab.research.google.com/drive/16gxaqBuY7NRYZaDqeV26yH7caLChTLY6&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='bloc97' date='2019-11-12T18:58:26Z'>
		Just for completeness' sake, here's the same code but on Tensorflow 1.x:
&lt;denchmark-link:https://colab.research.google.com/drive/1BylY2e43DLZRBkOdIDGJ_rBPDlbGo-Vm&gt;https://colab.research.google.com/drive/1BylY2e43DLZRBkOdIDGJ_rBPDlbGo-Vm&lt;/denchmark-link&gt;

This runs as expected, no training occurs, suggesting some problems within the Tensorflow 2.0 eager execution code.
		</comment>
		<comment id='5' author='bloc97' date='2019-11-15T10:58:56Z'>
		&lt;denchmark-link:https://github.com/bloc97&gt;@bloc97&lt;/denchmark-link&gt;

I tried in colab with TF 2.0 for learning rate 0 and 100 and i am able to see loss in output. However loss goes to nan if we use very large value . Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/af23720e98b0f655f794773c30cd486c/untitled371.ipynb&gt;here&lt;/denchmark-link&gt;
. However with TF 1.15 version i am able to see loss for learning rate 0, 100 and 1e10 .Thanks!
		</comment>
		<comment id='6' author='bloc97' date='2019-11-15T11:51:16Z'>
		&lt;denchmark-link:https://github.com/bloc97&gt;@bloc97&lt;/denchmark-link&gt;
,
Agree that   is resulting in  but can you please let me know why do you want to use such a Huge Value for Learning Rate. Thanks!
		</comment>
		<comment id='7' author='bloc97' date='2019-11-15T17:06:24Z'>
		Many research papers using high learning rate regimes will diverge if gradient clipping does not work. I simply provided a small example that shows the issue.
For example, in VDSR the authors use a learning rate of 0.1 with gradient clipping of 0.001. Since the network is extremely deep (20+ layers), it will diverge even on learning rates of 0.1 or 0.01 without gradient clipping.
I discovered this issue when trying to run code that worked on TF 1.5, but as we are migrating to TF 2.0, it doesn't work anymore.
		</comment>
		<comment id='8' author='bloc97' date='2020-01-08T18:57:25Z'>
		Any updates on this matter? Or is there any workaround (manually clip the gradients before the update pass)?
		</comment>
		<comment id='9' author='bloc97' date='2020-02-14T23:09:20Z'>
		This should be fixed for 2.2 (If you're not using a distribution strategy). We're working on a longer-term change for optimizers that should fix this more fundamentally.
		</comment>
		<comment id='10' author='bloc97' date='2020-03-17T06:52:13Z'>
		I assume that this is the fix: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/69da929ad4d5ba605507efa1f52b382a55b6a969&gt;69da929&lt;/denchmark-link&gt;

May I ask whether this fix is only relevant for eager execution or also graph execution?
In other words: For graph execution, did clipnorm already work correctly?
For me the situation is that I was able to train my models in TF2.0 with graph execution. Then I upgraded to 2.1 and enabled eager execution (and made some other changes), and since, my models cannot train properly anymore. So knowing whether this only relates to eager execution would allow me to pinpoint the problem.
When upgrading to 2.2 (nightly) the problem seems to be fixed.
		</comment>
		<comment id='11' author='bloc97' date='2020-03-17T17:52:48Z'>
		Yes, norm clipping still worked correctly when eager execution was disabled in 2.0 and 2.1.
		</comment>
		<comment id='12' author='bloc97' date='2020-03-18T08:23:05Z'>
		
Yes, norm clipping still worked correctly when eager execution was disabled in 2.0 and 2.1.

Thanks for clearing that up.
		</comment>
		<comment id='13' author='bloc97' date='2020-03-22T16:35:14Z'>
		Oh man, I've just been debugging my NaN losses for over an hour now. How could this even go unnoticed...
Will this be fixed for 2.1.x as well or is 2.2 the only way to get this working again? I can't disable eager mode because then I lose the cuDNN implementation of LSTMs.
		</comment>
		<comment id='14' author='bloc97' date='2020-03-23T17:55:08Z'>
		Hi Goldie, someone wants to know if a bug fix in 2.2 can be backported to
2.1.x. What's our policy on minor release versions?

Best,
Tomer
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Sun, Mar 22, 2020 at 9:35 AM jlherren ***@***.***&gt; wrote:
 Oh man, I've just been debugging my NaN losses for over an hour now. How
 could this even go unnoticed...

 Will this be fixed for 2.1.x as well or is 2.2 the only way to get this
 working again? I can't disable eager mode because then I lose the cuDNN
 implementation of LSTMs.

 —
 You are receiving this because you were assigned.
 Reply to this email directly, view it on GitHub
 &lt;#33929 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AAFFEBMU4OQKDECN2UCLUQTRIY45FANCNFSM4JICXMIA&gt;
 .



		</comment>
		<comment id='15' author='bloc97' date='2020-04-09T02:30:07Z'>
		&lt;denchmark-link:https://github.com/tomerk&gt;@tomerk&lt;/denchmark-link&gt;
 Our policy about back-porting the code to previous versions is mentioned by  &lt;denchmark-link:https://github.com/mihaimaruseac&gt;@mihaimaruseac&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29624#issuecomment-610601027&gt;Here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='16' author='bloc97' date='2020-05-14T01:59:36Z'>
		
This should be fixed for 2.2 (If you're not using a distribution strategy). We're working on a longer-term change for optimizers that should fix this more fundamentally.

Does it work for distribution strategy in 2.2 now? I get the same error when I use MultiWorkerMirroredStrategy in 2.2.0-dev20200508.
		</comment>
		<comment id='17' author='bloc97' date='2020-05-26T16:35:38Z'>
		&lt;denchmark-link:https://github.com/tomerk&gt;@tomerk&lt;/denchmark-link&gt;
 can you elaborate on why one shouldn't use gradient clipping with a distribution strategy?
		</comment>
		<comment id='18' author='bloc97' date='2020-05-26T17:58:16Z'>
		&lt;denchmark-link:https://github.com/karlhjm&gt;@karlhjm&lt;/denchmark-link&gt;
 no, we still disable it in 2.2 w/ distribution strategies enabled.
&lt;denchmark-link:https://github.com/zaccharieramzi&gt;@zaccharieramzi&lt;/denchmark-link&gt;
 yes, happy to elaborate:
There's two possible places to clip when you have distribution strategies enabled:

before gradients get aggregated (usually wrong)
after gradients get aggregated (usually right &amp; what people expect)

We want it working w/ the second case (clipping after gradients are aggregated).
The issue is the optimizers are written with clipping happening in the code before aggregation does.
We looked into changing this, but it would have required either:

api changes that break existing users of optimizer apply_gradients/other non-minimize methods
changing the signatures of methods optimizer implementers need to implement, breaking existing custom optimizers

So rather than:

quietly doing clipping in the wrong place
increasing churn &amp; breaking existing users or existing custom optimizers just for this individual feature

We instead decided to leave this disabled for now. We'll roll support for this into a larger optimizer refactoring that solves a larger set of issues.
(RFC for that is at &lt;denchmark-link:https://github.com/tensorflow/community/pull/234&gt;tensorflow/community#234&lt;/denchmark-link&gt;
)
		</comment>
		<comment id='19' author='bloc97' date='2020-06-06T16:30:18Z'>
		Short of reducing the learning rate is there any way to workaround this issue?
		</comment>
		<comment id='20' author='bloc97' date='2020-07-06T03:11:49Z'>
		
Short of reducing the learning rate is there any way to workaround this issue?

In Ludwig I have implemented a workaround: A wrapper class that wraps the optimizer and clips the gradients before applying them: &lt;denchmark-link:https://github.com/uber/ludwig/blob/tf2_porting/ludwig/models/modules/optimization_modules.py&gt;https://github.com/uber/ludwig/blob/tf2_porting/ludwig/models/modules/optimization_modules.py&lt;/denchmark-link&gt;

It's still WIP but seems to work fine.
		</comment>
		<comment id='21' author='bloc97' date='2020-08-06T07:04:55Z'>
		&lt;denchmark-link:https://github.com/bloc97&gt;@bloc97&lt;/denchmark-link&gt;

I have tried the code on tf 2.3 and do not get any nan values as output, can you please refer to this gist and confirm if it resolves the issue.
Please find the &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/0cc40cbbc833655d43a7b5567d5f212e/untitled321.ipynb&gt;gist here&lt;/denchmark-link&gt;

		</comment>
		<comment id='22' author='bloc97' date='2020-08-13T07:21:09Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='23' author='bloc97' date='2020-08-13T17:15:00Z'>
		The issue has been fixed for me on Tensorflow v2.3.0 on google colab. Thanks!
		</comment>
		<comment id='24' author='bloc97' date='2020-08-13T17:15:02Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33929&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33929&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='25' author='bloc97' date='2020-08-24T18:37:27Z'>
		Yes, the optimizer RFC I mentioned above is now complete and this should be resolved :)
		</comment>
		<comment id='26' author='bloc97' date='2020-09-19T01:04:21Z'>
		One question seems still open though. TF provides tf.clip_by_norm, tf.clip_by_value and tf.clip_by_global_norm, while the current implementation only supports the first two, but not the global norm one. Should i open a new issue to have it included?
		</comment>
		<comment id='27' author='bloc97' date='2020-09-19T02:13:48Z'>
		
One question seems still open though. TF provides tf.clip_by_norm, tf.clip_by_value and tf.clip_by_global_norm, while the current implementation only supports the first two, but not the global norm one. Should i open a new issue to have it included?

THere is a clip_globalnorm with keras optimizer in nightly
		</comment>
	</comments>
</bug>