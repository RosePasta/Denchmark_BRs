<bug id='34445' author='yashmanuda' open_date='2019-11-20T10:04:13Z' closed_time='2019-12-16T22:33:01Z'>
	<summary>Adapter to allow sparse matrix as target labels in Keras models</summary>
	<description>
Tensorflow version : 2.0
OS : Debian 9
I am training a sequential model as shown below :
&lt;denchmark-code&gt;model = Sequential()
model.add(Embedding(input_dim=170000, output_dim=100, input_length=10))
model.add(GlobalAveragePooling1D())
model.add(Dense(num_target=3811, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001))

model.fit(X_train,
          Y_train
          batch_size=16384,
          epochs=200,
          verbose=1)
&lt;/denchmark-code&gt;

X_train.shape = (12528566, 10)
Y_train.shape = (12528566, 3811)
X_train type : numpy.ndarray
Y_train type : scipy.sparse.csr.csr_matrix
Earlier in tensorflow 1.13 the training worked fine, but in Tensorflow 2.0 it is throwing error :
 Failed to find data adapter that can handle input: &lt;class 'numpy.ndarray'&gt;, &lt;class 'scipy.sparse.csr.csr_matrix'&gt;
I can convert Y_train dense type, by command :
Y_train.toarray()
but it throws error saying,
MemoryError: Unable to allocate array with shape (12528566, 3811) and data type int64
The size of Y_train when dense is, 12528566 * 3811 * 8 = ~381GB
Dense Y_train works fine, but only for smaller data.
So in order to make above thing work, I changed my loss to 'sparse_categorical_crossentropy' &amp; Y_train to np.asarray(Y_train.tocoo().col) which is basically the index of my label vector which maps to the corresponding output label, and model gets trained on Tensorflow 2.0 and predicts as expected.
&lt;denchmark-code&gt;model = Sequential()
model.add(Embedding(input_dim=170000, output_dim=100, input_length=10))
model.add(GlobalAveragePooling1D())
model.add(Dense(num_target=3811, activation='softmax'))
model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001))
&lt;/denchmark-code&gt;

Now I want to train a model with 'binary_crossentropy' loss model with 'sigmoid' activation for which I need to make Y_train as dense matrix, since there is no 'sparse_binary_crossentropy' loss type. If I don't use dense Y_train I get an error :
ValueError: A target array with shape (12528566, 1) was passed for an output of shape (None, 3811) while using as loss binary_crossentropy. This loss expects targets to have the same shape as the output.

Why is the adapter to handle sparse matrix removed from Tensorflow 2.0?
Why is there no 'sparse_binary_crossentropy' loss type?

	</description>
	<comments>
		<comment id='1' author='yashmanuda' date='2019-11-20T12:18:00Z'>
		EDIT : You can't use Sparse tensors either in mode.fit()
Failed to convert object of type &lt;class 'tensorflow.python.framework.sparse_tensor.SparseTensor'&gt; to Tensor. Contents: SparseTensor(indices=Tensor("DeserializeSparse:0", shape=(None, 2), dtype=int64), values=Tensor("DeserializeSparse:1", shape=(None,), dtype=int64), dense_shape=Tensor("DeserializeSparse:2", shape=(2,), dtype=int64)). Consider casting elements to a supported type.
		</comment>
		<comment id='2' author='yashmanuda' date='2019-11-21T04:56:01Z'>
		&lt;denchmark-link:https://github.com/yashmanuda&gt;@yashmanuda&lt;/denchmark-link&gt;

Looks like code is incomplete. Can you help us with minimal stand alone code and supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!
		</comment>
		<comment id='3' author='yashmanuda' date='2019-11-21T08:19:37Z'>
		&lt;denchmark-code&gt;import os

os.environ['CUDA_VISIBLE_DEVICES'] = '2'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'

import tensorflow
import numpy as np
import string
import random

from tensorflow.keras.models import Sequential
from sklearn.preprocessing import LabelBinarizer
from tensorflow.keras.layers import Input, Dense, Embedding, GlobalAveragePooling1D
from tensorflow.keras.optimizers import Adam

training_size = 12528566  # if you change the size to 1000 it will work if to_use_sparse is False or is_convert is False

to_use_sparse = True  # if set True and to_convert is False, throws error that adapter is not found

X_train = np.random.randint(low=0, high=169999, size=(training_size, 10), dtype='int32')
labels = []
for i in range(3811):
    labels.append(''.join(random.choices(string.ascii_uppercase + string.digits, k=6)))
Y = [random.choice(labels) for i in range(training_size)]

if to_use_sparse:
    Y_train = LabelBinarizer(sparse_output=True).fit(labels).transform(Y)  # no adapter is found
else:
    Y_train = LabelBinarizer(sparse_output=True).fit(labels).transform(Y).toarray()  # this thing works but only if training_size is small, say 1000

    
to_convert = True # converting the labels to integers of their index and using sparse_categorical_crossentropy

if to_convert and to_use_sparse: # if true, it works as expected
    model = Sequential()
    model.add(Embedding(input_dim=170000, output_dim=100, input_length=10))
    model.add(GlobalAveragePooling1D())
    model.add(Dense(3811, 'softmax'))
    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001))
    model.fit(X_train,
              np.asarray(Y_train.tocoo().col),
              batch_size=16384,
              epochs=5,
              verbose=1)
else:
    model = Sequential()
    model.add(Embedding(input_dim=170000, output_dim=100, input_length=10))
    model.add(GlobalAveragePooling1D())
    model.add(Dense(3811, 'softmax'))
    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001))
    model.fit(X_train,
              Y_train,
              batch_size=16384,
              epochs=5,
              verbose=1)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='yashmanuda' date='2019-11-21T08:28:17Z'>
		Please let me know if you don't understand anything.
		</comment>
		<comment id='5' author='yashmanuda' date='2019-11-25T11:23:58Z'>
		I have tried on colab with TF version 2.0  with  and i am not seeing any issue. However with  i am seeing error as . Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/1dd62dcfbb45014cc34558bf92118fa3/untitled409.ipynb&gt;here&lt;/denchmark-link&gt;
. Is this the expected behavior?. Thanks!
		</comment>
		<comment id='6' author='yashmanuda' date='2019-11-25T18:14:00Z'>
		Yes, it is the expected output of the above code. Is there any way to fix this?
Also, if I set to_use_sparse to False and training_size to 12528566 with is_convert to False, I get memory errors. This is because Y_train is not sparse anymore and it consumes lot of memory.
		</comment>
		<comment id='7' author='yashmanuda' date='2019-12-03T07:38:16Z'>
		Is there any update on this?
		</comment>
		<comment id='8' author='yashmanuda' date='2019-12-16T22:33:01Z'>
		This is the default behaviour of Tensorflow 2.0 &lt;denchmark-link:https://github.com/yashmanuda&gt;@yashmanuda&lt;/denchmark-link&gt;

Also as this question is not related to bug/performance, build/install, docs related issues, please post it in stackoverflow where there is a bigger community to respond. Thanks!
		</comment>
		<comment id='9' author='yashmanuda' date='2019-12-16T22:33:04Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34445&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34445&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>