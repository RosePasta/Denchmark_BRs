<bug id='26725' author='PangHua' open_date='2019-03-15T02:49:13Z' closed_time='2019-05-20T22:33:14Z'>
	<summary>BaseCollectiveExecutor::StartAbort Invalid argument: Upper bound check fail is reported with CollectiveAllReduceStrategy</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  CentOS Linux release 7.4.1708 (Core)
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): ('v1.13.1-0-g6612da8951', '1.13.1')
Python version: 2.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.0
GPU model and memory: Tesla P100, 16G

You can collect some of this information using our environment capture &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
Describe the current behavior
When running transformer within the tensor2tensor library with CollectiveAllReduceStrategy via little customized change for distributed training in t2t only, it reported below error:
INFO:tensorflow:Graph was finalized.
WARNING:tensorflow:From /home/xh/tf-1.13.1/lib/python2.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
INFO:tensorflow:Restoring parameters from /home/xh/datasets/t2t_train_enzh_multi-gpu-ok/translate_enzh_wmt32k/transformer-transformer_base/model.ckpt-179000
2019-03-14 15:30:36.585213: I tensorflow/core/distributed_runtime/master_session.cc:1192] Start master session 4c2e055e28f41c2f with config: device_filters: "/job:worker/task:0" device_filters: "/job:worker/task:0" gpu_options { per_process_gpu_memory_fraction: 0.95 } allow_soft_placement: true graph_options { optimizer_options { global_jit_level: OFF } rewrite_options { scoped_allocator_optimization: ON scoped_allocator_opts { enable_op: "CollectiveReduce" enable_op: "CollectiveReduce" enable_op: "CollectiveReduce" enable_op: "CollectiveReduce" } } } experimental { collective_group_leader: "/job:worker/replica:0/task:0" }
WARNING:tensorflow:From /home/xh/tf-1.13.1/lib/python2.7/site-packages/tensorflow/python/training/saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file utilities to get mtimes.
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Initialize strategy
INFO:tensorflow:Saving checkpoints for 179000 into /home/xh/datasets/t2t_train_enzh_multi-gpu-ok/translate_enzh_wmt32k/transformer-transformer_base/model.ckpt.
2019-03-14 15:32:18.009875: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
2019-03-14 15:32:28.447121: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:101] Filling up shuffle buffer (this may take a while): 387 of 512
2019-03-14 15:32:31.537519: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:140] Shuffle buffer filled.
2019-03-14 15:32:31.695275: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Invalid argument: Upper bound check fail for input 1 from node training/gradients/transformer/parallel_0_5/transformer/transformer/symbol_modality_32610_512_1/softmax/concat_grad/Slice_7 to node scoped_allocator_concat_232 input bounds = [0x7fb060a84f00, 0x7fb060e7ff00] backing_tensor bounds = [0x7faf8da62700, 0x7faf8fe35700]
[[{{node scoped_allocator_concat_232}}]]
2019-03-14 15:32:31.708208: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Invalid argument: Upper bound check fail for input 1 from node training/gradients/transformer/parallel_0_5/transformer/transformer/symbol_modality_32610_512_1/softmax/concat_grad/Slice_7 to node scoped_allocator_concat_232 input bounds = [0x7fb060a84f00, 0x7fb060e7ff00] backing_tensor bounds = [0x7faf8da62700, 0x7faf8fe35700]
[[{{node scoped_allocator_concat_232}}]]
[[{{node _send_add_1_0}}]]
InvalidArgumentError: InvalidA...ntError()
Describe the expected behavior
What does above error log mean, and is there any way to fix it? thanks.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='PangHua' date='2019-03-19T18:57:36Z'>
		Also seeing this bug when using tf.contrib.distribute.CollectiveAllReduceStrategy, any information on what it means?
		</comment>
		<comment id='2' author='PangHua' date='2019-03-26T06:38:35Z'>
		Any update on this issue? Even I am facing the exact same error. &lt;denchmark-link:https://github.com/isaacnoble&gt;@isaacnoble&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='PangHua' date='2019-03-26T06:41:56Z'>
		Ping &lt;denchmark-link:https://github.com/poxvoculi&gt;@poxvoculi&lt;/denchmark-link&gt;
; looks like a ScopeAllocator issue.
		</comment>
		<comment id='4' author='PangHua' date='2019-03-26T10:47:55Z'>
		&lt;denchmark-link:https://github.com/poxvoculi&gt;@poxvoculi&lt;/denchmark-link&gt;
 by any chance did you get time to look into this issue? &lt;denchmark-link:https://github.com/byronyi&gt;@byronyi&lt;/denchmark-link&gt;
 any workaround you can suggest for this?
		</comment>
		<comment id='5' author='PangHua' date='2019-03-26T12:55:32Z'>
		Pinging &lt;denchmark-link:https://github.com/dubey&gt;@dubey&lt;/denchmark-link&gt;
 too
		</comment>
		<comment id='6' author='PangHua' date='2019-03-27T08:19:03Z'>
		Even I'm facing the same issue. Do update us as soon as you find a solution.
		</comment>
		<comment id='7' author='PangHua' date='2019-03-27T08:21:57Z'>
		Even I'm facing the same with tensorflow version=1.11 Python=3.6
		</comment>
		<comment id='8' author='PangHua' date='2019-03-27T16:43:11Z'>
		Thanks for filing the issue.  &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/589fb0dfdb694d6318784e1523672fc2833b736d&gt;589fb0dfdb&lt;/denchmark-link&gt;
 fixed a similar issue in .  Can you try running with a nightly build and let us know if you still see this bug?  If yes, please also post instructions on how to reproduce.
		</comment>
		<comment id='9' author='PangHua' date='2019-04-10T07:40:37Z'>
		
Thanks for filing the issue. 589fb0dfdb fixed a similar issue in ScopedAllocator. Can you try running with a nightly build and let us know if you still see this bug? If yes, please also post instructions on how to reproduce.

Hi &lt;denchmark-link:https://github.com/dubey&gt;@dubey&lt;/denchmark-link&gt;
 ,
I have a constraint where I need to work on conda tensorflow v1.11
Following are the steps to reproduce the error with tensorflow v1.11


Run the worker1.ipynb on node 1
worker1.zip


Run the worker2.ipynb on node 2
worker2.zip


Note: Do modify the TF_Config variable according to your cluster specs in both the jupyter files
You will get errors &lt;denchmark-link:url&gt;https://gist.github.com/vishald527/5c464addba66188625ea0088853db970&lt;/denchmark-link&gt;
 and &lt;denchmark-link:url&gt;https://gist.github.com/vishald527/4c06f908651cae57b8a9f72996ea834d&lt;/denchmark-link&gt;
 on node 1 and 2 respectively.
After making the changes as suggested in &lt;denchmark-link:url&gt;https://github.com/tensorflow/tensorflow/commit/f10b00558de87020554c9c0512537dab96dba918&lt;/denchmark-link&gt;
 re-run the jupyter files.
Now you will get this &lt;denchmark-link:url&gt;https://gist.github.com/vishald527/616c1dc32f20d8226d712f4552b025fa&lt;/denchmark-link&gt;
 error on node 1 and this &lt;denchmark-link:url&gt;https://gist.github.com/vishald527/d47696e61c3f31f4d3063ff48dffaea7&lt;/denchmark-link&gt;
 log on node 2.
Post making the changes suggested here &lt;denchmark-link:url&gt;https://github.com/tensorflow/tensorflow/commit/e692dda4c8b199555e2fa32132a7784e0893c870&lt;/denchmark-link&gt;
 re-run the jupyter files.
Now you will get this &lt;denchmark-link:url&gt;https://gist.github.com/vishald527/b1a0d7cb436f70902594dbd742701794&lt;/denchmark-link&gt;
 error on node 1 and this &lt;denchmark-link:url&gt;https://gist.github.com/vishald527/6dc8b8719b4a67e0ee6dabd3a61ad779&lt;/denchmark-link&gt;
 on node 2.
We even tried building tensorflow v1.11 after making the changes you had suggested here &lt;denchmark-link:url&gt;https://github.com/tensorflow/tensorflow/commit/589fb0dfdb694d6318784e1523672fc2833b736d&lt;/denchmark-link&gt;
, but we are again getting the same Upper/Lower bound check fail error on both nodes.
Request you to please suggest what fixes are to be done in tensorflow v1.11 to get rid of this error and get CollectiveAllReduceStrategy up and running.
		</comment>
		<comment id='10' author='PangHua' date='2019-04-10T10:53:58Z'>
		&lt;denchmark-link:https://github.com/vishald527&gt;@vishald527&lt;/denchmark-link&gt;
 I am not a member of the TF team but I believe they do not accept bugfix PR on release branches for experimental features, a.k.a. those in the contrib folder.
CollectiveAllReduceStrategy was an experimental feature in r1.11, so you probably need to cherry pick the bugfix patches, and backport them to r1.11. I am not sure how hard it is though.
Please correct me if I am wrong.
		</comment>
		<comment id='11' author='PangHua' date='2019-04-12T06:17:07Z'>
		&lt;denchmark-link:https://github.com/byronyi&gt;@byronyi&lt;/denchmark-link&gt;
 got your point but how to get a patch for this particular error? i am getting this error in v1.11 and the person who raised this issue got the same error in v1.13.
The patch for this error which &lt;denchmark-link:https://github.com/dubey&gt;@dubey&lt;/denchmark-link&gt;
 had mentioned above didn't work.
Also, do we say that currently tensorflow does not support a stable multi node gpu based synchronous distributed training strategy at all?
		</comment>
		<comment id='12' author='PangHua' date='2019-04-12T07:15:27Z'>
		
@byronyi got your point but how to get a patch for this particular error? i am getting this error in v1.11 and the person who raised this issue got the same error in v1.13.
The patch for this error which @dubey had mentioned above didn't work.

I do not know. I would suggest you to upgrade your TF version.

Also, do we say that currently tensorflow does not support a stable multi node gpu based synchronous distributed training strategy at all?

It depends on your definition to "stable". It was an experimental feature in r1.11, it is still an experimental feature right now. Use it at your own risk.
		</comment>
		<comment id='13' author='PangHua' date='2019-04-12T07:42:33Z'>
		
I do not know. I would suggest you to upgrade your TF version.

TF v1.13 already has the same error and TF v2.0 does not have this strategy..so not sure which TF version you are suggesting to upgrade.

It depends on your definition to "stable". It was an experimental feature in r1.11, it is still an experimental feature right now. Use it at your own risk.

By stable I meant a strategy which would at least start synchronous distributed training on a multiple node gpu based system. Currently we are in a situation where we don't have a single strategy which even starts distributed training on multiple nodes synchronously. Please do correct me if I am wrong.
Also is there any method you are aware of by which we can run distributed training on gpu based multiple node system?
		</comment>
		<comment id='14' author='PangHua' date='2019-04-12T08:57:21Z'>
		
TF v1.13 already has the same error and TF v2.0 does not have this strategy..so not sure which TF version you are suggesting to upgrade.

CollectiveAllReduce is exposed as  in TF 2.0. See &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/distribute/collective_all_reduce_strategy.py#L48&gt;here&lt;/denchmark-link&gt;
 for details.

Also is there any method you are aware of by which we can run distributed training on gpu based multiple node system?

Apart from MultiWorkerMirroredStrategy, you could try ParameterServerStrategy as well. They are both in the tf.distribute.experimental subpackage.
		</comment>
		<comment id='15' author='PangHua' date='2019-04-12T09:36:32Z'>
		
CollectiveAllReduce is exposed as tf.distribute.experimental.MultiWorkerMirroredStrategy in TF 2.0. See here for details.

TF 2.0 i guess requires CUDA 10 and I have CUDA 9.2 installed. Upgrading CUDA is not an option for me right now and that is why I need help on fixing this reported issue for TF 1.11/1.13. Any pointers on who might be able to help resolving this issue?

Apart from MultiWorkerMirroredStrategy, you could try ParameterServerStrategy as well. They are both in the tf.distribute.experimental subpackage.

ParameterServerStrategy is an asynchronous technique. I am looking for a synchronous one.
		</comment>
		<comment id='16' author='PangHua' date='2019-04-12T10:01:41Z'>
		
TF 2.0 i guess requires CUDA 10 and I have CUDA 9.2 installed. Upgrading CUDA is not an option for me right now and that is why I need help on fixing this reported issue for TF 1.11/1.13. Any pointers on who might be able to help resolving this issue?

You could recompile your TF using CUDA 9.2.
		</comment>
		<comment id='17' author='PangHua' date='2019-04-12T10:33:25Z'>
		
You could recompile your TF using CUDA 9.2.

Will try that too.
&lt;denchmark-link:https://github.com/dubey&gt;@dubey&lt;/denchmark-link&gt;
 any luck on finding the cause and fix for this issue?
		</comment>
		<comment id='18' author='PangHua' date='2019-04-12T20:49:27Z'>
		Apologies for the delay, this was a bit tricky to debug and fix.  I just submitted &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/a3d2ee6ada53fdfde96a76d635c721629acb9582&gt;a3d2ee6&lt;/denchmark-link&gt;
 which should fix the bounds check failure.  Let us know if it works for you.
		</comment>
		<comment id='19' author='PangHua' date='2019-04-15T02:00:32Z'>
		&lt;denchmark-link:https://github.com/dubey&gt;@dubey&lt;/denchmark-link&gt;
 do I need to make both the code changes you had given and then try or only the latest one?
		</comment>
		<comment id='20' author='PangHua' date='2019-04-16T05:41:01Z'>
		I'd recommend patching both changes.
		</comment>
		<comment id='21' author='PangHua' date='2019-04-23T12:07:44Z'>
		&lt;denchmark-link:https://github.com/dubey&gt;@dubey&lt;/denchmark-link&gt;

I used a recent built(6 days ago)  docker image, this error still exists
		</comment>
		<comment id='22' author='PangHua' date='2019-04-23T16:06:48Z'>
		&lt;denchmark-link:https://github.com/royxue&gt;@royxue&lt;/denchmark-link&gt;
 thanks for the update.  Can you post the error message you get with the recent image, as well as instructions to reproduce the error?
		</comment>
		<comment id='23' author='PangHua' date='2019-04-24T02:43:14Z'>
		I will try run again with the most recent docker image, and get back to you the result
		</comment>
		<comment id='24' author='PangHua' date='2019-04-24T03:11:54Z'>
		&lt;denchmark-link:https://github.com/dubey&gt;@dubey&lt;/denchmark-link&gt;
 Here is the logs.
I slightly modified the TF object detection. And run the collective allreduce strategy with 4 workers (1 gpu per worker) on k8s.
&lt;denchmark-code&gt;2019-04-24 03:09:04.405555: W tensorflow/core/common_runtime/base_collective_executor.cc:215] BaseCollectiveExecutor::StartAbort Invalid argument: Upper bound check fail for input 1 from node gradients/AddN_20 to node scoped_allocator_concat_100_16 input bounds = [0x7fdbfcfb0f00, 0x7fdbfcff8f00] backing_tensor bounds = [0x7fdbea25fd00, 0x7fdbea2f7900]
	 [[{{node scoped_allocator_concat_100_16}}]]
	 [[global_norm/global_norm_G6542]]
2019-04-24 03:09:04.405608: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Invalid argument: Upper bound check fail for input 1 from node gradients/AddN_20 to node scoped_allocator_concat_100_16 input bounds = [0x7fdbfcfb0f00, 0x7fdbfcff8f00] backing_tensor bounds = [0x7fdbea25fd00, 0x7fdbea2f7900]
	 [[{{node scoped_allocator_concat_100_16}}]]
	 [[global_norm/global_norm_G6542]]
2019-04-24 03:09:04.405617: W tensorflow/core/common_runtime/base_collective_executor.cc:215] BaseCollectiveExecutor::StartAbort Invalid argument: Upper bound check fail for input 1 from node gradients/AddN_20 to node scoped_allocator_concat_100_16 input bounds = [0x7fdbfcfb0f00, 0x7fdbfcff8f00] backing_tensor bounds = [0x7fdbea25fd00, 0x7fdbea2f7900]
	 [[{{node scoped_allocator_concat_100_16}}]]
	 [[global_norm/global_norm_G6542]]
2019-04-24 03:09:04.405728: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Invalid argument: Upper bound check fail for input 1 from node gradients/AddN_20 to node scoped_allocator_concat_100_16 input bounds = [0x7fdbfcfb0f00, 0x7fdbfcff8f00] backing_tensor bounds = [0x7fdbea25fd00, 0x7fdbea2f7900]
	 [[{{node scoped_allocator_concat_100_16}}]]
	 [[global_norm/global_norm_G6542]]
2019-04-24 03:09:04.405741: W tensorflow/core/common_runtime/base_collective_executor.cc:215] BaseCollectiveExecutor::StartAbort Invalid argument: Upper bound check fail for input 1 from node gradients/AddN_20 to node scoped_allocator_concat_100_16 input bounds = [0x7fdbfcfb0f00, 0x7fdbfcff8f00] backing_tensor bounds = [0x7fdbea25fd00, 0x7fdbea2f7900]
	 [[{{node scoped_allocator_concat_100_16}}]]
	 [[global_norm/global_norm_G6542]]
2019-04-24 03:09:04.405823: W tensorflow/core/framework/op_kernel.cc:1455] OP_REQUIRES failed at collective_ops.cc:223 : Invalid argument: Upper bound check fail for input 1 from node gradients/AddN_20 to node scoped_allocator_concat_100_16 input bounds = [0x7fdbfcfb0f00, 0x7fdbfcff8f00] backing_tensor bounds = [0x7fdbea25fd00, 0x7fdbea2f7900]
	 [[{{node scoped_allocator_concat_100_16}}]]
	 [[global_norm/global_norm_G6542]]
2019-04-24 03:09:04.405885: W tensorflow/core/framework/op_kernel.cc:1455] OP_REQUIRES failed at collective_ops.cc:223 : Invalid argument: Upper bound check fail for input 1 from node gradients/AddN_20 to node scoped_allocator_concat_100_16 input bounds = [0x7fdbfcfb0f00, 0x7fdbfcff8f00] backing_tensor bounds = [0x7fdbea25fd00, 0x7fdbea2f7900]
	 [[{{node scoped_allocator_concat_100_16}}]]
	 [[global_norm/global_norm_G6542]]
2019-04-24 03:09:04.408029: W tensorflow/core/common_runtime/base_collective_executor.cc:215] BaseCollectiveExecutor::StartAbort Invalid argument: Upper bound check fail for input 1 from node gradients/AddN_20 to node scoped_allocator_concat_100_16 input bounds = [0x7fdbfcfb0f00, 0x7fdbfcff8f00] backing_tensor bounds = [0x7fdbea25fd00, 0x7fdbea2f7900]
	 [[{{node scoped_allocator_concat_100_16}}]]
Traceback (most recent call last):
  File "/models/object_detection/model_main.py", line 127, in &lt;module&gt;
    tf.app.run()
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "/usr/local/lib/python2.7/dist-packages/absl/app.py", line 300, in run
    _run_main(main, args)
  File "/usr/local/lib/python2.7/dist-packages/absl/app.py", line 251, in _run_main
    sys.exit(main(argv))
  File "/models/object_detection/model_main.py", line 123, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File "/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/training.py", line 464, in train_and_evaluate
    estimator, train_spec, eval_spec, _TrainingExecutor)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/estimator_training.py", line 290, in train_and_evaluate
    session_config=run_config.session_config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/distribute_coordinator.py", line 853, in run_distribute_coordinator
    task_id, session_config, rpc_layer)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/distribute_coordinator.py", line 360, in _run_single_worker
    return worker_fn(strategy)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/estimator_training.py", line 252, in _worker_fn
    hooks=hooks)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py", line 362, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py", line 1152, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py", line 1215, in _train_model_distributed
    self._config._train_distribute, input_fn, hooks, saving_listeners)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py", line 1325, in _actual_train_model_distributed
    saving_listeners)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py", line 1402, in _train_with_estimator_spec
    estimator_spec, worker_hooks, saving_listeners)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py", line 1348, in _train_with_estimator_spec_distributed
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py", line 754, in run
    run_metadata=run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py", line 1252, in run
    run_metadata=run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py", line 1353, in run
    raise six.reraise(*original_exc_info)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py", line 1338, in run
    return self._sess.run(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py", line 1411, in run
    run_metadata=run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py", line 1169, in run
    return self._sess.run(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 948, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1171, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1348, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1368, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Upper bound check fail for input 1 from node gradients/AddN_20 to node scoped_allocator_concat_100_16 input bounds = [0x7fdbfcfb0f00, 0x7fdbfcff8f00] backing_tensor bounds = [0x7fdbea25fd00, 0x7fdbea2f7900]
	 [[{{node scoped_allocator_concat_100_16}}]]
	 [[global_norm/global_norm_G6542]]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='25' author='PangHua' date='2019-04-24T18:07:38Z'>
		Can you provide a bare minimum test case that reproduces this error?
For some context, the scoped allocator is a static optimization that converts multiple collective all-reduce ops in to a single all-reduce.  In this optimization, we pre-allocate a buffer that is large enough to hold all the inputs to the all reduce.  We add an attribute to each OpKernel whose output takes part in this all-reduce indicating that the output should be assigned from a slice of this pre-allocated buffer.  The bounds check failure is a runtime error that indicates that the OpKernel somehow circumvented this process and used a different buffer.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/a3d2ee6ada53fdfde96a76d635c721629acb9582&gt;a3d2ee6&lt;/denchmark-link&gt;
 fixed the case when an OpKernel calls  instead of .
In this example, it looks like the input to the all-reduce comes from an AddN op.  I tried a small test case which fuses 2 ops using the scoped allocator whose inputs are both AddN, but my test case passed.
		</comment>
		<comment id='26' author='PangHua' date='2019-04-25T02:02:55Z'>
		&lt;denchmark-link:https://github.com/dubey&gt;@dubey&lt;/denchmark-link&gt;

I was checking the solution yesterdays. If I started training from a checkpoint, this error happened. But is I started training from scratch this error will not happen. However the training process will hang up after the graph is initialized. Im thinking this is may caused by the conflicts between (object detection api used) and . As mentioned, the strategy do not support deprecated  api.
		</comment>
		<comment id='27' author='PangHua' date='2019-05-03T00:51:11Z'>
		Is this resolved or is it still an issue?
		</comment>
		<comment id='28' author='PangHua' date='2019-05-17T13:24:45Z'>
		It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='29' author='PangHua' date='2019-05-20T22:33:14Z'>
		Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!
		</comment>
		<comment id='30' author='PangHua' date='2019-05-20T22:33:16Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=26725&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=26725&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>