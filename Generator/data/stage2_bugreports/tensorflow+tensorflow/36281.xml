<bug id='36281' author='Flamefire' open_date='2020-01-28T09:56:44Z' closed_time='2020-07-06T15:09:49Z'>
	<summary>unsupported operand type(s) for *: 'int' and 'NoneType' in training_v2</summary>
	<description>
System information

TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 2.1.0

Running a simple evaluation of a Lenet Model with MNIST dataset (from TFDS) with a large batch size (e.g. due to using many workers) leads to
&lt;denchmark-code&gt;tensorflow.python.framework.errors_impl.OutOfRangeError: 2 root error(s) found.
  (0) Out of range:  End of sequence
	 [[node IteratorGetNext_2 (defined at git/tensorflow_tests/train_distributed.py:212) ]]
	 [[metrics/accuracy/div_no_nan/allreduce_1/CollectiveReduce/_70]]
  (1) Out of range:  End of sequence
	 [[node IteratorGetNext_2 (defined at git/tensorflow_tests/train_distributed.py:212) ]]
&lt;/denchmark-code&gt;

and
&lt;denchmark-code&gt;File "/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 152, in run_one_epoch
    total_epochs * steps_per_epoch))
TypeError: unsupported operand type(s) for *: 'int' and 'NoneType'
&lt;/denchmark-code&gt;

The bug is pretty obvious in 


tensorflow/tensorflow/python/keras/engine/training_v2.py


         Line 152
      in
      7b0cfad






 total_epochs * steps_per_epoch)) 




 where steps_per_epoch is used although it can be (and is) None, see also 


tensorflow/tensorflow/python/keras/engine/training_v2.py


         Line 135
      in
      7b0cfad






 and steps_per_epoch is None 





	</description>
	<comments>
		<comment id='1' author='Flamefire' date='2020-01-29T06:41:52Z'>
		&lt;denchmark-link:https://github.com/Flamefire&gt;@Flamefire&lt;/denchmark-link&gt;
 Could you please provide us with simple standalone code to reproduce the issue in our environment, Thanks.
		</comment>
		<comment id='2' author='Flamefire' date='2020-01-29T10:37:03Z'>
		I'm having trouble reproducing this with a reduced sample. What I did was using the MultiWorkerMirroredStrategy with a simple MNIST test and 2 nodes. For some reason the dataset was not able to produce a single batch and raised OutOfRangeError which lead to the above linked handling code. As the step is still zero the 2nd branch (see link above) is taken and steps_per_epoch is used without checking it for None (which it still is)
I see if I can come up with some sample code which is small enough but the issue is obvious from reading the code.
		</comment>
		<comment id='3' author='Flamefire' date='2020-03-06T14:53:45Z'>
		FYI I saw this same failure, but in my case the cause was that I was running with a very small dataset (puny laptop testing...), such that the validation dataset (which I was creating as just the last 10% of the overall set) did not fill a full batch (and I was discarding the remainder, so probably feeding an empty validation set). &lt;denchmark-link:https://github.com/Flamefire&gt;@Flamefire&lt;/denchmark-link&gt;
 you mentioned “with a large batch size” — might this be your situation as well?
		</comment>
		<comment id='4' author='Flamefire' date='2020-03-14T14:37:08Z'>
		&lt;denchmark-link:https://github.com/gthb&gt;@gthb&lt;/denchmark-link&gt;
 I guess so, yes. But there still is a bug in the code as a value which can be None is used expecting it to be not None.
		</comment>
		<comment id='5' author='Flamefire' date='2020-04-25T00:19:07Z'>
		I guess you are using TF 2.1
I upgrade to version V2.2.0.rc3. It solved the problem! (This version is not using training_v2 anymore, btw)
src: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/releases&gt;https://github.com/tensorflow/tensorflow/releases&lt;/denchmark-link&gt;

Thanks!
		</comment>
		<comment id='6' author='Flamefire' date='2020-06-22T11:11:38Z'>
		It happened to me while trying to check tf.data.Dataset elements :
for (image, label) in train_ds.take(3):
-----   print(f'{image.numpy().shape} and {label.numpy().shape}')
strangely, I restarted the kernel and the error just disappeared.
		</comment>
		<comment id='7' author='Flamefire' date='2020-07-06T15:09:49Z'>
		Reviewing the code for v2.3.0-rc0 I conclude that this was fixed
		</comment>
		<comment id='8' author='Flamefire' date='2020-07-06T15:09:51Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36281&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36281&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>