<bug id='29391' author='ptigas' open_date='2019-06-04T12:57:43Z' closed_time='2019-06-06T18:29:15Z'>
	<summary>Dropout in GRU/LSTM in Tensorflow 2.0 doesn't reset dropout masks on call</summary>
	<description>
System information
Tensorflow
GIT VERSION: v1.12.1-3283-geff4ae822a
VERSION: 2.0.0-dev20190604
Colab environment
Describe the current behavior
In RNNs, the dropout masks should be reset after every call. However, in training mode (where the dropouts are activated) GRU and LSTM implementation in tensorflow 2.0 seems to be re-using the same dropout masks, leading to deterministic behavior.
Simple RNN seems to be doing the right thing, re-sampling dropout masks after each call.
Describe the expected behavior
The expected behaviour should be the same as SimpleRNN (re-sample dropout masks on each call).
Code to reproduce the issue
The following code produces the correct behaviour in tensorflow 1.13.1:
&lt;denchmark-code&gt;from __future__ import absolute_import, division, print_function

import tensorflow as tf
tf.enable_eager_execution()

import numpy as np

tf.enable_eager_execution()
print(tf.__version__)
data = np.random.normal(0, 1, (1, 10, 2)).astype(np.float32)
rnn = tf.keras.layers.GRU(units=10, dropout=0.5, recurrent_dropout=0.5)
print(set([rnn(data, training=True).numpy()[0, 0] for _ in range(5)]))
&lt;/denchmark-code&gt;

output
&lt;denchmark-code&gt;1.13.1
{0.09432551, -0.07633728, 0.03358479, 0.010588642, 0.0}
&lt;/denchmark-code&gt;

but in tensorflow 2.0 it doesn't
&lt;denchmark-code&gt;from __future__ import absolute_import, division, print_function

import tensorflow as tf
import numpy as np

print(tf.version.GIT_VERSION, tf.version.VERSION)
data = np.random.normal(0, 1, (1, 10, 2)).astype(np.float32)
rnn = tf.keras.layers.GRU(units=10, dropout=0.5, recurrent_dropout=0.5)
print(set([rnn(data, training=True).numpy()[0, 0] for _ in range(5)]))
&lt;/denchmark-code&gt;

output
&lt;denchmark-code&gt;v1.12.1-3283-geff4ae822a 2.0.0-dev20190604
{-0.14212656}
&lt;/denchmark-code&gt;

A quick fix is to call reset_dropout_mask() and reset_recurrent_dropout_mask() between calls however this looks like a breaking change.
&lt;denchmark-code&gt;def fixed_rnn():
  rnn.cell.reset_dropout_mask()
  rnn.cell.reset_recurrent_dropout_mask()
  return rnn(data, training=True).numpy()[0, 0]

print(set([fixed_rnn() for _ in range(5)]))
&lt;/denchmark-code&gt;

output
&lt;denchmark-code&gt;{-0.004232532, 1.1669009, -0.009177759, 3.0901778, 4.5860972}
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='ptigas' date='2019-06-05T08:42:20Z'>
		Have tried on Colab with TF CPU versions 1.13.1 and 2.0.0-dev20190604 and was able to reproduce the issue.
		</comment>
		<comment id='2' author='ptigas' date='2019-06-06T18:00:47Z'>
		Thanks for reporting the issue. There was a similar issue, and has been recently fixed in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/180f28a26660ca2e1ba27477f4f9592db5f9c4e8&gt;180f28a&lt;/denchmark-link&gt;
. Can u try with the latest nightly again?
		</comment>
		<comment id='3' author='ptigas' date='2019-06-06T18:01:21Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29187&gt;#29187&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='ptigas' date='2019-06-06T18:20:19Z'>
		For version v1.12.1-3447-g5a0f1bbfb7 2.0.0-dev20190606  the issue seems resolved.
		</comment>
		<comment id='5' author='ptigas' date='2019-06-06T18:29:15Z'>
		Thanks for verifying this, closing bug now.
		</comment>
		<comment id='6' author='ptigas' date='2019-06-06T18:29:17Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29391&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29391&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='ptigas' date='2019-06-06T21:15:54Z'>
		&lt;denchmark-link:https://github.com/sbagroy986&gt;@sbagroy986&lt;/denchmark-link&gt;
, the CPU and GPU implementation is covered under same code path, which means they should all be covered by the change. Can u report a issue with details if you feel this hasn't be fixed? Also, please make sure you install the latest pip package, which should include the fix.
		</comment>
	</comments>
</bug>