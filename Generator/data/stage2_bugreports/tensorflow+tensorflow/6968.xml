<bug id='6968' author='yaroslavvb' open_date='2017-01-20T02:14:02Z' closed_time='2017-01-24T17:07:12Z'>
	<summary>Error in `python': double free or corruption (!prev)</summary>
	<description>
I'm consistently getting this error when stopping training (CTRL+C) on version built from head on Jan17. On other hand, running on version from Jan5 head does not exhibit this behavior
tf.__git__version = '0.12.1-1934-g27fca7d-dirty'
&lt;denchmark-code&gt;session.run completed in 0.01 sec with .0.500000 acc
session.run completed in 0.02 sec with .0.000000 acc
^CTraceback (most recent call last):
  File "train.py", line 247, in &lt;module&gt;
    a,_ = sess.run([train_acc,optimizer], feed_dict)
  File "/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 767, in run
    run_metadata_ptr)
  File "/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 965, in _run
    feed_dict_string, options, run_metadata)
  File "/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 1015, in _do_run
    target_list, options, run_metadata)
  File "/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 1022, in _do_call
    return fn(*args)
  File "/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 1004, in _run_fn
    status, run_metadata)
KeyboardInterrupt
*** Error in `python': double free or corruption (!prev): 0x00000000016c55d0 ***
Aborted (core dumped)
&lt;/denchmark-code&gt;

Looking at core, it looks like dictionary deletion.
&lt;denchmark-code&gt;#0  0x00007fe9cbf8a01f in _int_free (av=0x7fe9cc2c9760 &lt;main_arena&gt;, p=&lt;optimized out&gt;, have_lock=0) at malloc.c:3996
#1  0x00007fe9cceb500a in dict_dealloc (mp=0x7fe9558073c8) at Objects/dictobject.c:1596
#2  0x00007fe9cced121f in subtype_dealloc (self=0x7fe95580a080) at Objects/typeobject.c:1193
#3  0x00007fe9cceb023f in free_keys_object (keys=0x24f9620) at Objects/dictobject.c:354
#4  0x00007fe9cced3936 in type_clear (type=0x24f9c68) at Objects/typeobject.c:3270
#5  0x00007fe9ccf8a97c in delete_garbage (old=&lt;optimized out&gt;, collectable=&lt;optimized out&gt;) at Modules/gcmodule.c:866
#6  collect (generation=2, n_collected=0x0, n_uncollectable=0x0, nofail=1) at Modules/gcmodule.c:1014
#7  0x00007fe9ccf8aedd in _PyGC_CollectNoFail () at Modules/gcmodule.c:1605
#8  0x00007fe9ccf5e6d5 in PyImport_Cleanup () at Python/import.c:428
#9  0x00007fe9ccf6a90e in Py_Finalize () at Python/pylifecycle.c:576
#10 0x00007fe9ccf891b9 in Py_Main (argc=&lt;optimized out&gt;, argv=&lt;optimized out&gt;) at Modules/main.c:789
#11 0x0000000000400add in main (argc=2, argv=0x7ffde1cf3f98) at ./Programs/python.c:65
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='yaroslavvb' date='2017-01-20T02:18:51Z'>
		I suspect this error is connected to jemalloc since that got added recently. Turning on tcmalloc through export LD_PRELOAD="/usr/lib/libtcmalloc.so.4" gets rid of the error
		</comment>
		<comment id='2' author='yaroslavvb' date='2017-01-20T02:22:23Z'>
		It's possible that the malloc changes are responsible... &lt;denchmark-link:https://github.com/jhseu&gt;@jhseu&lt;/denchmark-link&gt;
 would know best, since he made those changes.
		</comment>
		<comment id='3' author='yaroslavvb' date='2017-01-24T02:39:38Z'>
		I haven't been able to reproduce it yet. Do you happen to have a script that I can use?
It seems unlikely to be related to the jemalloc change. We don't (and technically can't) override Python's malloc/free. My guess is that setting tcmalloc is just hiding the error and that the issue is in the script itself.
If you have time, mind disabling jemalloc in ./configure and trying again?
		</comment>
		<comment id='4' author='yaroslavvb' date='2017-01-24T17:07:12Z'>
		I think I'll close this for now as unreproducible and reopen when someone can provide more info
		</comment>
		<comment id='5' author='yaroslavvb' date='2017-02-07T02:37:30Z'>
		Not sure if this is related but I am unable to build with jemalloc on a cluster with gcc 4.8.2 and an older version of libc.so.6 (old enough that I have to build tensor flow from source). The build fails with a message:
ERROR:~/.cache/bazel/bazel/e924d9c3ba75314415252c6f4f93bb86/external/jemalloc/BUILD:10:1: C++ compilation of rule '@jemalloc//:jemalloc' failed: gcc failed: error executing command /opt/apps/compilers/gcc/4.8.2/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/opt/apps/compilers/gcc/4.8.2/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 38 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
Disabling jemalloc at the configure stage means I can finish the tensor flow build and install. I can then train my models (i.e. model.fit()) on the cluster but when I try to  run model.predict_prob() I am getting this error:
*** glibc detected *** python: double free or corruption (!prev): 0x00000000013fcda0 ***
======= Backtrace: =========
/lib64/libc.so.6[0x3e22e75e66]
/lib64/libc.so.6[0x3e22e789b3]
/lib64/ld-linux-x86-64.so.2(_dl_deallocate_tls+0x67)[0x3e226112f7]
/lib64/libpthread.so.0[0x3e2320675d]
/lib64/libpthread.so.0[0x3e232078ea]
/lib64/libpthread.so.0(pthread_join+0xd4)[0x3e232081f4]
/opt/apps/compilers/gcc/4.8.2/lib64/libstdc++.so.6(_ZNSt6thread4joinEv+0x27)[0x7f657b6263c7]
/users/k1511981/.pyenv/versions/t2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0x2567e60)[0x7f657dde2e60]
/users/k1511981/.pyenv/versions/t2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow6thread10ThreadPool4ImplD0Ev+0xb3)[0x7f657ddbf5b3]
/users/k1511981/.pyenv/versions/t2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow6thread10ThreadPoolD1Ev+0x1a)[0x7f657ddbfc1a]
/users/k1511981/.pyenv/versions/t2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow10FileSystem16GetMatchingPathsERKSsPSt6vectorISsSaISsEE+0x592)[0x7f657dddf8a2]
/users/k1511981/.pyenv/versions/t2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow3Env16GetMatchingPathsERKSsPSt6vectorISsSaISsEE+0x9b)[0x7f657dddbc1b]
/users/k1511981/.pyenv/versions/t2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0xadc66b)[0x7f657c35766b]
/users/k1511981/.pyenv/versions/t2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0xade610)[0x7f657c359610]
So just wondering if there is some conflict with compiler/glibc libraries that is causing these issues?
		</comment>
		<comment id='6' author='yaroslavvb' date='2017-02-08T20:18:00Z'>
		Strangely, I'm getting the exact same stack trace for &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/7338&gt;#7338&lt;/denchmark-link&gt;
 (and it happens even with jemalloc disabled). Pretty hard to track down the root cause...
		</comment>
		<comment id='7' author='yaroslavvb' date='2017-02-10T20:41:04Z'>
		Can confirm this. I ran into this on my CI server and the the following fixed it:
&lt;denchmark-code&gt;sudo apt-get install libtcmalloc-minimal4
export LD_PRELOAD="/usr/lib/libtcmalloc_minimal.so.4"
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='yaroslavvb' date='2017-02-10T20:45:30Z'>
		What Linux distribution and version were you running?
In &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/7338&gt;#7338&lt;/denchmark-link&gt;
, it seemed to be an issue with pypi's numpy on Ubuntu 14.04. Compiling from source fixed it.
		</comment>
		<comment id='9' author='yaroslavvb' date='2017-02-10T20:48:10Z'>
		Using Ubuntu 14.04 - &lt;denchmark-link:https://circleci.com/docs/build-image-trusty/&gt;https://circleci.com/docs/build-image-trusty/&lt;/denchmark-link&gt;
. I was using the tf1.0-rc2 pip package.
		</comment>
		<comment id='10' author='yaroslavvb' date='2017-02-11T03:18:38Z'>
		So I think this is some interaction between pypi's numpy and Ubuntu 14.04. Either upgrading to Ubuntu 16.04 or building numpy from source (pip install --no-binary) fixes it in every case I've tried (GPU/no GPU, jemalloc disabled/enabled, Python 2.7/3.4/3.5)
		</comment>
		<comment id='11' author='yaroslavvb' date='2017-02-11T17:09:48Z'>
		Hi, I got a similar Error in "python": double free or corruption (!prev) error. Using tf 0.12, built from source, today with Ubuntu 14.04.
This was solved with &lt;denchmark-link:https://github.com/dennybritz&gt;@dennybritz&lt;/denchmark-link&gt;
 's fix, four posts above. Thank you very much &lt;denchmark-link:https://github.com/dennybritz&gt;@dennybritz&lt;/denchmark-link&gt;

Will try upgrading to 16.04 as &lt;denchmark-link:https://github.com/jhseu&gt;@jhseu&lt;/denchmark-link&gt;
 recommends
		</comment>
		<comment id='12' author='yaroslavvb' date='2017-02-13T17:35:19Z'>
		Would it be possible to rebuild the Docker images on gcr.io? The underlying nvidia/cuda:8.0-cudnn5-devel uses Ubuntu 16.04 now, but the gcr.io/tensorflow/tensorflow:1.0.0-rc2-devel-gpu image still uses Ubuntu 14.04.
		</comment>
		<comment id='13' author='yaroslavvb' date='2017-02-13T17:49:22Z'>
		I'm updating all the Docker images, but the gcr.io ones won't be updated until after 1.0. In the meantime, you can start from nvidia/cuda:8.0-cudnn5-devel-ubuntu16.04 and just add RUN pip install tensorflow_gpu
		</comment>
		<comment id='14' author='yaroslavvb' date='2017-02-13T17:57:07Z'>
		Sorry, I mean the Docker images for the 1.0.0 RCs specifically â€“ or at least any upcoming ones?
This is actually a bit weird to me, because the nvidia/cuda:8.0-cudnn5-devel (with no OS suffix) image on Docker Hub has been pointing at Ubuntu 16.04 since at least 24 days ago, which predates all the TensorFlow 1.0.0 RC releases.
		</comment>
		<comment id='15' author='yaroslavvb' date='2017-02-13T19:39:48Z'>
		Yeah, Docker caches image versions and we had 14.04 cached. &lt;denchmark-link:https://github.com/caisq&gt;@caisq&lt;/denchmark-link&gt;
 is planning to upgrade those.
		</comment>
		<comment id='16' author='yaroslavvb' date='2017-02-15T01:37:39Z'>
		To narrow down the issue: someone internally noticed that this crash only happens when numpy is installed with OpenBLAS support on Ubuntu 14.04. I haven't tested whether upgrading libopenblas fixes it.
So, if you're on Ubuntu, the workaround is to make sure you don't have libopenblas-dev installed and pip install --no-binary=:all: numpy
If someone encounters this bug and has time to test out newer versions of libopenblas-dev, that'd be useful.
		</comment>
		<comment id='17' author='yaroslavvb' date='2017-02-23T20:22:31Z'>
		I found this problem when using 1.0.0 docker image gcr.io/tensorflow/tensorflow:1.0.0-devel-py3. Is this the source of the problem? Do I need to do anything to my containers/images/dockerfile to remove this issue?
		</comment>
		<comment id='18' author='yaroslavvb' date='2017-02-23T21:32:52Z'>
		The nightly docker images are on Ubuntu 16.04, which should make the problem go away.  For now, I'd recommend using the nightly.
The official builds will be on Ubuntu 16.04 when TensorFlow 1.1 is out.
		</comment>
		<comment id='19' author='yaroslavvb' date='2017-02-23T21:35:19Z'>
		nightly docker?
		</comment>
		<comment id='20' author='yaroslavvb' date='2017-02-23T21:38:13Z'>
		&lt;denchmark-link:https://github.com/brando90&gt;@brando90&lt;/denchmark-link&gt;
: Nightly TF docker images are pushed to Docker Hub. Example command line to use them:
docker run -it --rm tensorflow/tensorflow:nightly /bin/bash
nvidia-docker run -it --rm tensorflow/tensorflow:nightly-gpu /bin/bash
nvidia-docker run -it --rm tensorflow/tensorflow:nightly-gpu-py3 /bin/bash
		</comment>
		<comment id='21' author='yaroslavvb' date='2017-02-23T22:38:06Z'>
		ah ok, thanks. I will try nightly.
When is 1.1 out?
		</comment>
		<comment id='22' author='yaroslavvb' date='2017-02-24T00:20:08Z'>
		Sometime in March
		</comment>
		<comment id='23' author='yaroslavvb' date='2017-10-29T08:21:20Z'>
		I got this same issue trying to run real data benchmark with Horovod on TF 1.4.0rc1, with Open MPI OpenIB transport (which installs memory hooks). TCP transport is unaffected.
&lt;denchmark-code&gt;Generating model                                                                                                                                                 [992/9693]
*** Error in `python': double free or corruption (!prev): 0x0000000001f721f0 ***
[opusgpu39-wbu2:32804] *** Process received signal ***
[opusgpu39-wbu2:32804] Signal: Aborted (6)
[opusgpu39-wbu2:32804] Signal code:  (-6)
[opusgpu39-wbu2:32804] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0xf890)[0x7f0c06bba890]
[opusgpu39-wbu2:32804] [ 1] /lib/x86_64-linux-gnu/libc.so.6(gsignal+0x37)[0x7f0c05f12067]
[opusgpu39-wbu2:32804] [ 2] /lib/x86_64-linux-gnu/libc.so.6(abort+0x148)[0x7f0c05f13448]
[opusgpu39-wbu2:32804] [ 3] /lib/x86_64-linux-gnu/libc.so.6(+0x731b4)[0x7f0c05f501b4]
[opusgpu39-wbu2:32804] [ 4] /lib/x86_64-linux-gnu/libc.so.6(+0x7898e)[0x7f0c05f5598e]
[opusgpu39-wbu2:32804] [ 5] /lib/x86_64-linux-gnu/libc.so.6(+0x79696)[0x7f0c05f56696]
[opusgpu39-wbu2:32804] [ 6] /lib64/ld-linux-x86-64.so.2(_dl_deallocate_tls+0x58)[0x7f0c06dd9958]
[opusgpu39-wbu2:32804] [ 7] /lib/x86_64-linux-gnu/libpthread.so.0(+0x7107)[0x7f0c06bb2107]
[opusgpu39-wbu2:32804] [ 8] /lib/x86_64-linux-gnu/libpthread.so.0(+0x721f)[0x7f0c06bb221f]
[opusgpu39-wbu2:32804] [ 9] /lib/x86_64-linux-gnu/libpthread.so.0(pthread_join+0xe4)[0x7f0c06bb44d4]
[opusgpu39-wbu2:32804] [10] /usr/lib/x86_64-linux-gnu/libstdc++.so.6(_ZNSt6thread4joinEv+0x27)[0x7f0b69baa837]
[opusgpu39-wbu2:32804] [11] /home/asergeev/mpi/venv-nccl/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so(+0x5131d0)[0x7f0b708b61d0]
[opusgpu39-wbu2:32804] [12] /home/asergeev/mpi/venv-nccl/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow6thread10ThreadP$
ol4ImplD0Ev+0xbb)[0x7f0b7088d43b]
[opusgpu39-wbu2:32804] [13] /home/asergeev/mpi/venv-nccl/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow6thread10ThreadP$
olD1Ev+0x1a)[0x7f0b7088d73a]
[opusgpu39-wbu2:32804] [14] /home/asergeev/mpi/venv-nccl/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow10FileSystem16Ge$
MatchingPathsERKSsPSt6vectorISsSaISsEE+0x56c)[0x7f0b708b2aec]
[opusgpu39-wbu2:32804] [15] /home/asergeev/mpi/venv-nccl/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow3Env16GetMatchin$
PathsERKSsPSt6vectorISsSaISsEE+0xa3)[0x7f0b708aca43]
[opusgpu39-wbu2:32804] [16] /home/asergeev/mpi/venv-nccl/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_Z16GetMatchingFilesRKSsP9TF_S$
atus+0x4b)[0x7f0b7227090b]
[opusgpu39-wbu2:32804] [17] /home/asergeev/mpi/venv-nccl/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(+0x1080604)[0x7f0b72273604]
[opusgpu39-wbu2:32804] [18] python(PyEval_EvalFrameEx+0x614)[0x4cddf4]
[opusgpu39-wbu2:32804] [19] python(PyEval_EvalCodeEx+0x401)[0x4cc4f1]
[opusgpu39-wbu2:32804] [20] python(PyEval_EvalFrameEx+0x6500)[0x4d3ce0]
[opusgpu39-wbu2:32804] [21] python(PyEval_EvalCodeEx+0x401)[0x4cc4f1]
[opusgpu39-wbu2:32804] [22] python(PyEval_EvalFrameEx+0x5e0a)[0x4d35ea]
[opusgpu39-wbu2:32804] [23] python(PyEval_EvalCodeEx+0x401)[0x4cc4f1]
[opusgpu39-wbu2:32804] [24] python(PyEval_EvalFrameEx+0x5e0a)[0x4d35ea]
[opusgpu39-wbu2:32804] [25] python(PyEval_EvalCodeEx+0x401)[0x4cc4f1]
[opusgpu39-wbu2:32804] [26] python(PyEval_EvalFrameEx+0x6500)[0x4d3ce0]
[opusgpu39-wbu2:32804] [27] python(PyEval_EvalCodeEx+0x401)[0x4cc4f1]
[opusgpu39-wbu2:32804] [28] python(PyEval_EvalFrameEx+0x6500)[0x4d3ce0]
[opusgpu39-wbu2:32804] [29] python(PyEval_EvalCodeEx+0x401)[0x4cc4f1]
[opusgpu39-wbu2:32804] *** End of error message ***
&lt;/denchmark-code&gt;

I was able to make it work by adding -x LD_PRELOAD=/usr/local/lib/libtcmalloc.so.4.4.5.
Seems other folks are still hitting this issue in other use cases, too.  Any ideas or plans for the fix?
		</comment>
		<comment id='24' author='yaroslavvb' date='2017-11-02T01:41:41Z'>
		We still don't think there's a bug in TensorFlow here. Any Python module that messes with memory allocation can cause this, so perhaps trying importing those last?
		</comment>
		<comment id='25' author='yaroslavvb' date='2017-11-24T09:03:31Z'>
		I ran into this and other "Error in `python'" issues using the module mayavi.mlab with TensorFlow when creating multiple mlab figures. &lt;denchmark-link:https://github.com/jhseu&gt;@jhseu&lt;/denchmark-link&gt;
 's suggestion to import the module after TF fixed it instantly. Thank you.
		</comment>
		<comment id='26' author='yaroslavvb' date='2017-12-10T10:54:12Z'>
		I don't know why I send the command:
bazel-bin/im2txt/train --input_file_pattern="${MSCOCO_DIR}/train-?????-of-00256" --inception_checkpoint_file="${INCEPTION_CHECKPOINT}" --train_dir="${MODEL_DIR}/train" --train_inception=false --number_of_steps=1000000
there have a error:
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:126] Couldn't open CUDA library libcufft.so.8.0. LD_LIBRARY_PATH:
I tensorflow/stream_executor/cuda/cuda_fft.cc:344] Unable to load cuFFT DSO.
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
*** Error in `/usr/bin/python': double free or corruption (!prev): 0x000000000231f8e0 ***
I don't know the error
		</comment>
		<comment id='27' author='yaroslavvb' date='2018-03-26T18:37:44Z'>
		I was facing the same issue and later I found that I wrote a Destructor of the class which was clearing the memory but I had already cleared it in another function which i was calling before the Destructor and then repeated the same which caused same error when I removed the Destructor  Problem Solved. (I was loading my preTrained Model)
		</comment>
		<comment id='28' author='yaroslavvb' date='2018-04-06T00:25:28Z'>
		I am experiencing this issue while restoring partial weights/biases thru the method: tensorflow.contrib.framework.python.ops.assign_from_checkpoint. Currently working on CUDA 8/9, tf r1.4 w/ native source build, and horovod 0.11.2. I am also planning to upgrade tf to very recent version or successor (r1.5), and then I'll leave some progress about error.
Below is an example of process backtrace. (automatically generated)
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/1882253/process_dead_backtrace.txt&gt;process_dead_backtrace.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='29' author='yaroslavvb' date='2018-04-11T03:58:37Z'>
		I used jemalloc instead of tcmalloc and the problem was also solved on CentOS 6.
Just install jemalloc and run with LD_PRELOAD=/usr/local/lib/libjemalloc.so
(follow this post: &lt;denchmark-link:https://zapier.com/engineering/celery-python-jemalloc/&gt;https://zapier.com/engineering/celery-python-jemalloc/&lt;/denchmark-link&gt;
)
		</comment>
		<comment id='30' author='yaroslavvb' date='2019-02-05T12:56:25Z'>
		
Can confirm this. I ran into this on my CI server and the the following fixed it:
sudo apt-get install libtcmalloc-minimal4
export LD_PRELOAD="/usr/lib/libtcmalloc_minimal.so.4"


I am at Ubuntu16.04 machine. After following the above steps, it still gave the same error.
		</comment>
		<comment id='31' author='yaroslavvb' date='2019-06-04T06:41:39Z'>
		I use pip install --no-binary=:all: --force-reinstall numpy,
solve the problem
		</comment>
		<comment id='32' author='yaroslavvb' date='2019-08-14T08:56:43Z'>
		

Can confirm this. I ran into this on my CI server and the the following fixed it:
sudo apt-get install libtcmalloc-minimal4
export LD_PRELOAD="/usr/lib/libtcmalloc_minimal.so.4"


I am at Ubuntu16.04 machine. After following the above steps, it still gave the same error.

yes,me too
		</comment>
		<comment id='33' author='yaroslavvb' date='2019-10-21T21:03:23Z'>
		
I use pip install --no-binary=:all: --force-reinstall numpy,
solve the problem

This fixed the issue for me when running TF Object Detection. Numpy was updated to 1.17.3 from 1.14.6.
		</comment>
	</comments>
</bug>