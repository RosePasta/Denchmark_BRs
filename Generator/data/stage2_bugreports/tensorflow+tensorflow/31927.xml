<bug id='31927' author='daniele-sartiano' open_date='2019-08-23T12:24:37Z' closed_time='2019-10-10T20:46:29Z'>
	<summary>tf2 load model issue</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): google colab
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): tf-nightly-2.0-preview==2.0.0.dev20190818
Python version: 3
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

Describe the current behavior
When I try to load a model I obtain the following error:
&lt;denchmark-code&gt;ValueError: Unable to save the object ListWrapper([ListWrapper([]), ListWrapper([])]) (a list wrapper constructed to track trackable TensorFlow objects). A list element was replaced (__setitem__, __setslice__), deleted (__delitem__, __delslice__), or moved (sort). In order to support restoration on object creation, tracking is exclusively for append-only data structures.
&lt;/denchmark-code&gt;

Describe the expected behavior
I would like load the saved model.

Google Colab link: &lt;denchmark-link:https://colab.research.google.com/drive/1e1TPBzhSipaAGI38gZF00kZMSi5Pg8fp&gt;https://colab.research.google.com/drive/1e1TPBzhSipaAGI38gZF00kZMSi5Pg8fp&lt;/denchmark-link&gt;

Here a portion of source code:
&lt;denchmark-code&gt;import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow import feature_column
from tensorflow.python.keras import Input, Model
from tensorflow.python.keras.layers import Dense, Dropout
from tensorflow.python.keras.optimizers import Adam
from tensorflow.python.keras.models import load_model
n = 200
df = pd.DataFrame(data={'a': [x for x in range(n)], 'b': [x for x in range(n+10,n+n+10)], 'labels': [int(x%2==0) for x in range(n)]})
df = df.astype({'b': str})
def df_to_dataset(dataframe, shuffle=True, batch_size=32):
  dataframe = dataframe.copy()
  
  labels = dataframe.pop('labels')
  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
  if shuffle:
    ds = ds.shuffle(buffer_size=len(dataframe))
  ds = ds.batch(batch_size)
  return ds
train, test = train_test_split(df, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)
train_ds = df_to_dataset(train)
val_ds = df_to_dataset(val, shuffle=False)
test_ds = df_to_dataset(test, shuffle=False)
feature_columns = []
feature_layer_inputs = {}
for c in df.columns:
  if c == 'labels':
    continue
  elif c == 'b':
    el = feature_column.categorical_column_with_vocabulary_list(c, df[c].unique(), default_value=-10)
    el_one_hot = feature_column.indicator_column(el)
    feature_columns.append(el_one_hot)
    feature_layer_inputs[c] = tf.keras.Input(shape=(1,), name=c, dtype=tf.string)
  elif c == 'a':
    feature_columns.append(feature_column.numeric_column(c, default_value=-10))
    feature_layer_inputs[c] = Input(shape=(1,), name=c)
feature_layer = tf.keras.layers.DenseFeatures(feature_columns)
f_layer = feature_layer(feature_layer_inputs)
input = [v for v in feature_layer_inputs.values()]
x = Dense(2048, activation='relu')(f_layer)
x = Dropout(0.5)(x)
out = Dense(1, activation='sigmoid')(x)
model = Model(inputs=[input], outputs=out)
model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001), metrics=['binary_accuracy', 'AUC'])
model.fit(train_ds, validation_data=val_ds, epochs=10, verbose=0)
model.save('aaa.model')
new_model = load_model('aaa.model')
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='daniele-sartiano' date='2019-08-24T10:30:18Z'>
		Hi it is happening because you have put some part of model outside graph of your graph.
entry point of you model is input so feature layer is outside .
An op outside of the function building code is being passed a "Graph" tensor.
&lt;denchmark-code&gt;feature_layer = tf.keras.layers.DenseFeatures(feature_columns)
f_layer = feature_layer(feature_layer_inputs)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='daniele-sartiano' date='2019-08-26T05:47:19Z'>
		&lt;denchmark-link:https://github.com/daniele-sartiano&gt;@daniele-sartiano&lt;/denchmark-link&gt;
, did you get a chance to look at &lt;denchmark-link:https://github.com/akanyaani&gt;@akanyaani&lt;/denchmark-link&gt;
's comment. Thanks
		</comment>
		<comment id='3' author='daniele-sartiano' date='2019-08-26T07:12:00Z'>
		&lt;denchmark-link:https://github.com/akanyaani&gt;@akanyaani&lt;/denchmark-link&gt;
 thank you for your comment.
I tried to edit the code in this way:
&lt;denchmark-code&gt;train, test = train_test_split(df, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)
train_ds = df_to_dataset(train)
val_ds = df_to_dataset(val, shuffle=False)
test_ds = df_to_dataset(test, shuffle=False)
feature_columns = []
feature_layer_inputs = {}
for c in df.columns:
  if c == 'labels':
    continue
  elif c == 'b':
    el = feature_column.categorical_column_with_vocabulary_list(c, df[c].unique(), default_value=-10)
    el_one_hot = feature_column.indicator_column(el)
    feature_columns.append(el_one_hot)
    feature_layer_inputs[c] = tf.keras.Input(shape=(1,), name=c, dtype=tf.string)
  elif c == 'a':
    feature_columns.append(feature_column.numeric_column(c, default_value=-10))
    feature_layer_inputs[c] = Input(shape=(1,), name=c)

x = tf.keras.layers.DenseFeatures(feature_columns)(feature_layer_inputs)
x = Dense(2048, activation='relu')(x)
x = Dropout(0.5)(x)
out = Dense(1, activation='sigmoid')(x)
model = Model(inputs=[feature_layer_inputs], outputs=out)
model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001), metrics=['binary_accuracy', 'AUC'])
model.fit(train_ds, validation_data=val_ds, epochs=10, verbose=0)
model.save('aaa.model')
from tensorflow.python.keras.models import load_model
new_model = load_model('aaa.model')
&lt;/denchmark-code&gt;

but I obtain the same error.
Any suggestion to solve this issue?
		</comment>
		<comment id='4' author='daniele-sartiano' date='2019-08-27T09:06:53Z'>
		Was able to reproduce the issue with Tensorflow 2.0.0.dev20190818. Please see the gist &lt;denchmark-link:https://colab.research.google.com/drive/1T4pKIz9d6zqVsoZerjaquTKVhx4-Px4P&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='5' author='daniele-sartiano' date='2019-09-09T14:17:54Z'>
		Hello everyone gladly I found this issue.
I am facing the same problem.
I am using Python3 and Tensorflow 2.0.0 rc
I was able to replicate the issue both with the mnist dataset and the heart dataset.
I tried some model variations and I believe that the problem comes from the joint occurrence of feature columns and save model.
When I get it right the above code uses the functional api for keras. With the sequential model this issue happens too.
The issue is also occurring when using h5 or json for saving and then loading the model.
To be precise the error for h5 is
ValueError: ('We expected a dictionary here. Instead we got: ', &lt;tf.Tensor 'Placeholder:0' shape=(None,) dtype=float32&gt;)
If there are any information which I can provide for solving the issue please post here.
		</comment>
		<comment id='6' author='daniele-sartiano' date='2019-09-20T11:47:53Z'>
		I have the same error too.
		</comment>
		<comment id='7' author='daniele-sartiano' date='2019-09-25T13:29:22Z'>
		The issue seems to be solved for me with tensorflow 2.0 rc2
The change log states

Model saving changes
model.save and tf.saved_model.save may now save to the TensorFlow SavedModel format. The model can be restored using tf.keras.models.load_model. HDF5 files are still supported, and may be used by specifying save_format="h5" when



		</comment>
		<comment id='8' author='daniele-sartiano' date='2019-10-10T20:46:28Z'>
		&lt;denchmark-link:https://github.com/daniele-sartiano&gt;@daniele-sartiano&lt;/denchmark-link&gt;
 Looks like this was resolved in . Please check the &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/dec0d251573289e24d7e3aed4580818a/untitled1.ipynb&gt;gist here&lt;/denchmark-link&gt;
. Everything runs without any issue when I use . Thanks!
I am closing the issue as it was resolved. Please feel free to open it if the issue persists again. Thanks!
		</comment>
		<comment id='9' author='daniele-sartiano' date='2019-10-10T20:46:31Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31927&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31927&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='daniele-sartiano' date='2019-11-20T04:44:24Z'>
		After installing tf-nightly, still i am getting the same error message.
I am trying to read the CSV file from hdfs using tf.data API.
Following is my code sample,-
(code is referred from: #&lt;denchmark-link:https://www.tensorflow.org/tutorials/load_data/csv&gt;https://www.tensorflow.org/tutorials/load_data/csv&lt;/denchmark-link&gt;
)
train_file_paths = ["hdfs://..../tr/titanic/train/train.csv"]
test_file_paths = ["hdfs://...../tr/titanic/eval/test.csv"]
LABEL_COLUMN = 'survived'
LABELS = [0, 1]
COLUMNS = ["survived","sex","age","n_siblings_spouses","parch","fare","class","deck","embark_town","alone"]
def get_dataset(file_path, **kwargs):
dataset = tf.data.experimental.make_csv_dataset(
file_path,
batch_size=rowsin_batch, # 10- Artificially small to make examples easier to show.
column_names=COLUMNS,
label_name=LABEL_COLUMN,
na_value="?",
header=False,
num_epochs=1,
shuffle=False,
ignore_errors=True,
**kwargs)
return dataset
raw_train_data = get_dataset(train_file_paths)
raw_test_data = get_dataset(test_file_paths)
class PackNumericFeatures(object):
def init(self, names):
self.names = names
def call(self, features, labels):
numeric_freatures = [features.pop(name) for name in self.names]
numeric_features = [tf.cast(feat, tf.float32) for feat in numeric_freatures]
numeric_features = tf.stack(numeric_features, axis=-1)
features['numeric'] = numeric_features
&lt;denchmark-code&gt;return features, labels
&lt;/denchmark-code&gt;

NUMERIC_FEATURES = ['age','n_siblings_spouses','parch', 'fare']
packed_train_data = raw_train_data.map( PackNumericFeatures(NUMERIC_FEATURES))
packed_test_data = raw_test_data.map( PackNumericFeatures(NUMERIC_FEATURES))
numeric_column = tf.feature_column.numeric_column('numeric', shape=[len(NUMERIC_FEATURES)])
numeric_columns = [numeric_column]
feature_layer = tf.keras.layers.DenseFeatures(numeric_columns)
model = tf.keras.Sequential([
feature_layer,
layers.Dense(128, activation='relu'),
layers.Dense(128, activation='relu'),
layers.Dense(1, activation='sigmoid')
])
model.compile(
loss='binary_crossentropy',
optimizer='adam',
metrics=['accuracy'])
model.fit(packed_train_data, epochs=2)
print(model.summary())
test_data = packed_test_data
print("evaluating model..")
test_loss, test_accuracy = model.evaluate(test_data)
model.save('my_model.h5')
print("Printing new model created from saved model")
new_model = tf.keras.models.load_model('my_model.h5')
print("Show the model architecture")
print(new_model.summary())
Please let me know if i am doing it wrong way.
		</comment>
		<comment id='11' author='daniele-sartiano' date='2019-11-24T13:13:59Z'>
		
Hello everyone gladly I found this issue.
I am facing the same problem.
I am using Python3 and Tensorflow 2.0.0 rc
I was able to replicate the issue both with the mnist dataset and the heart dataset.
I tried some model variations and I believe that the problem comes from the joint occurrence of feature columns and save model.
When I get it right the above code uses the functional api for keras. With the sequential model this issue happens too.
The issue is also occurring when using h5 or json for saving and then loading the model.
To be precise the error for h5 is
ValueError: ('We expected a dictionary here. Instead we got: ', &lt;tf.Tensor 'Placeholder:0' shape=(None,) dtype=float32&gt;)
If there are any information which I can provide for solving the issue please post here.

This error still persists with today's tf-nightly (also tried 1.15.0 and 2.0.0).
I have DenseFeatures as the first layer.
I'm able to evaluate, predict and even save the model.
But on load_model I get:
ValueError: ('We expected a dictionary here. Instead we got: ', &lt;tf.Tensor 'Placeholder:0' shape=(None,) dtype=float32&gt;)
Any suggestions?
Edit: This only happens with a Sequential model.
		</comment>
		<comment id='12' author='daniele-sartiano' date='2019-11-25T18:15:40Z'>
		&lt;denchmark-link:https://github.com/eliadl&gt;@eliadl&lt;/denchmark-link&gt;
 Please open a new issue with details related to issue and a standalone code to reproduce the issue. Thanks!
		</comment>
		<comment id='13' author='daniele-sartiano' date='2020-01-24T08:57:28Z'>
		&lt;denchmark-link:https://github.com/eliadl&gt;@eliadl&lt;/denchmark-link&gt;
 Have you any solutions for the problem?
		</comment>
		<comment id='14' author='daniele-sartiano' date='2020-01-24T12:59:04Z'>
		&lt;denchmark-link:https://github.com/magiclevinho&gt;@magiclevinho&lt;/denchmark-link&gt;
 I ended up using the &lt;denchmark-link:https://www.tensorflow.org/guide/keras/functional&gt;Keras functional API&lt;/denchmark-link&gt;
, rather than a Sequential model.
Because that way the issue doesn't occur.
		</comment>
		<comment id='15' author='daniele-sartiano' date='2020-01-24T15:26:52Z'>
		&lt;denchmark-link:https://github.com/eliadl&gt;@eliadl&lt;/denchmark-link&gt;
 Thank You for answering!
I don't know if the following code does similar as you said, but it works for me!
I copied the code from an another website, (i don t know if I m allowed to post links?) the user was posting is, so the credit goes for him/her: Egor B Eremeev
`from  import absolute_import, division, print_function
import numpy as np
import pandas as pd
#!pip install tensorflow==2.0.0-alpha0
import tensorflow as tf
from tensorflow import feature_column
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
URL = '&lt;denchmark-link:https://storage.googleapis.com/applied-dl/heart.csv&gt;https://storage.googleapis.com/applied-dl/heart.csv&lt;/denchmark-link&gt;
'
dataframe = pd.read_csv(URL)
dataframe.head()
train, test = train_test_split(dataframe, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)
print(len(train), 'train examples')
print(len(val), 'validation examples')
print(len(test), 'test examples')
&lt;denchmark-h:h1&gt;A utility method to create a tf.data dataset from a Pandas Dataframe&lt;/denchmark-h&gt;

def df_to_dataset(dataframe, shuffle=True, batch_size=32):
dataframe = dataframe.copy()
labels = dataframe.pop('target')
ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
if shuffle:
ds = ds.shuffle(buffer_size=len(dataframe))
ds = ds.batch(batch_size)
return ds
batch_size = 5 # A small batch sized is used for demonstration purposes
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)
age = feature_column.numeric_column("age")
feature_columns = []
feature_layer_inputs = {}
&lt;denchmark-h:h1&gt;numeric cols&lt;/denchmark-h&gt;

for header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:
feature_columns.append(feature_column.numeric_column(header))
feature_layer_inputs[header] = tf.keras.Input(shape=(1,), name=header)
&lt;denchmark-h:h1&gt;bucketized cols&lt;/denchmark-h&gt;

age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])
feature_columns.append(age_buckets)
&lt;denchmark-h:h1&gt;indicator cols&lt;/denchmark-h&gt;

thal = feature_column.categorical_column_with_vocabulary_list(
'thal', ['fixed', 'normal', 'reversible'])
thal_one_hot = feature_column.indicator_column(thal)
feature_columns.append(thal_one_hot)
feature_layer_inputs['thal'] = tf.keras.Input(shape=(1,), name='thal', dtype=tf.string)
&lt;denchmark-h:h1&gt;embedding cols&lt;/denchmark-h&gt;

thal_embedding = feature_column.embedding_column(thal, dimension=8)
feature_columns.append(thal_embedding)
&lt;denchmark-h:h1&gt;crossed cols&lt;/denchmark-h&gt;

crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)
crossed_feature = feature_column.indicator_column(crossed_feature)
feature_columns.append(crossed_feature)
batch_size = 32
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)
feature_layer = tf.keras.layers.DenseFeatures(feature_columns)
feature_layer_outputs = feature_layer(feature_layer_inputs)
x = layers.Dense(128, activation='relu')(feature_layer_outputs)
x = layers.Dense(64, activation='relu')(x)
baggage_pred = layers.Dense(1, activation='sigmoid')(x)
model = keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=baggage_pred)
model.compile(optimizer='adam',
loss='binary_crossentropy',
metrics=['accuracy'])
model.fit(train_ds)`
		</comment>
		<comment id='16' author='daniele-sartiano' date='2020-05-11T18:52:12Z'>
		
@magiclevinho I ended up using the Keras functional API, rather than a Sequential model.
Because that way the issue doesn't occur.

For my purposes it does not work to use the Keras functional API, isn't there another solution to this problem? Having to switch to a different API will not work for every situation
		</comment>
		<comment id='17' author='daniele-sartiano' date='2020-06-14T17:34:07Z'>
		I also have the same issue. I tired with Tf-nightly. And also tried switching to functional API. Both do not seem to work.
		</comment>
		<comment id='18' author='daniele-sartiano' date='2020-07-21T21:49:38Z'>
		The newly added &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/CategoryEncoding&gt;experimental layers&lt;/denchmark-link&gt;
 seems to be an attempt to solve the issue.
In the old versions, feature_column blocks are defined outside of the model, resulting in the issue. These new experimental layers are placed inside model definition. I haven't tried it yet, but it looks promising.
		</comment>
	</comments>
</bug>