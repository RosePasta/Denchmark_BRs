<bug id='2502' author='Remper' open_date='2016-05-25T10:06:53Z' closed_time='2017-06-16T17:48:23Z'>
	<summary>Method to check for GPU op support</summary>
	<description>
So, given the basic word2vec example being bound to CPU (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/514&gt;#514&lt;/denchmark-link&gt;
), we can see that  doesn't work on GPU. Therefore, ops that use  internally doesn't support GPU either (for example, ).
Can we have explicit info in the documentation on which operations are currently GPU-capable and which are not?
For example, are  or  GPU-capable?
	</description>
	<comments>
		<comment id='1' author='Remper' date='2016-05-27T08:22:30Z'>
		&lt;denchmark-link:https://github.com/vrv&gt;@vrv&lt;/denchmark-link&gt;
 is there a way to check CPU/GPU capabilities of an operation from python? Something like print out a list of available backends for each operation?
Because I could update the documentation myself, but I was wondering if there is a more efficient way than just digging sources one operation at the time.
		</comment>
		<comment id='2' author='Remper' date='2016-05-27T15:35:10Z'>
		There doesn't seem to be a more efficient way. Kernel registrations are
done through C++ registry mechanism, so some non-trivial piping would need
to be done to expose it in Python.
		</comment>
		<comment id='3' author='Remper' date='2016-05-27T15:38:20Z'>
		BTW, when tracking capabilities through code, one thing to keep in mind is
that there are logical devices and "physical" devices that don't
necessarily coincide. For instance, the following
from core/kernels/cwise_op_add.cc shows that "Add" for int32 is registered
for GPU logical device, but the actual implementation will run on CPU.
(This was done to avoid crossing logical device boundary for efficiency
reasons)
REGISTER_KERNEL_BUILDER(Name("Add")
.Device(DEVICE_GPU)
.HostMemory("x")
.HostMemory("y")
.HostMemory("z")
.TypeConstraint("T"),
BinaryOp&lt;CPUDevice, functor::add&gt;);
		</comment>
		<comment id='4' author='Remper' date='2016-06-07T17:47:19Z'>
		The fact that word2vec is bound to CPU is historical: tf.gather works fine on GPU now.  I believe LogUniformCandidateSampler is still CPU-only, though.
Unfortunately making the docs not depend on kernel registrations helps significantly, and we'd lose those advantages if we presented this information in the docs.  However, a runtime feature for checking which ops have which kernels would be great, so I'll mark this as contributions welcome.
		</comment>
		<comment id='5' author='Remper' date='2016-06-09T10:10:41Z'>
		A tool that could produce an HTML table with an overview of

which ops exist
which dtypes are supported
which backends are supported

would be pretty amazing when starting out with tensorflow, and would also be useful for making sure that things are defined in a consistent way.
The table could then also be included in the documentation.
(Or should this rather be a feature of TensorBoard?)
This could be built on top of the runtime checking that's the topic of this issue.
		</comment>
		<comment id='6' author='Remper' date='2017-06-16T17:48:23Z'>
		Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
		<comment id='7' author='Remper' date='2017-08-11T15:39:54Z'>
		It's been more than a year, but I can't find any information on the subject. How can one check if a tensorflow operation is running/supported on CPU or GPU? Thanks!
		</comment>
		<comment id='8' author='Remper' date='2017-12-01T11:06:28Z'>
		I wrote such a function  &lt;denchmark-link:https://github.com/rwth-i6/returnn/blob/master/TFUtil.py&gt;here&lt;/denchmark-link&gt;
 (&lt;denchmark-link:https://github.com/rwth-i6/returnn/commit/ed47af101e0f750232c81944be154ff980eb70d9&gt;this commit&lt;/denchmark-link&gt;
).
Basically it uses the TF C++ function  which provides such information.
		</comment>
		<comment id='9' author='Remper' date='2018-09-20T13:00:30Z'>
		&lt;denchmark-link:https://github.com/dantkz&gt;@dantkz&lt;/denchmark-link&gt;
 another way to check where your ops are running is using:

		</comment>
	</comments>
</bug>