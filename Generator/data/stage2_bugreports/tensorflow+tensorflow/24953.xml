<bug id='24953' author='salemmohammed' open_date='2019-01-16T05:57:09Z' closed_time='2019-03-01T22:15:58Z'>
	<summary>Multiple parameter servers are not sharing the load when running TensorFlow distributed</summary>
	<description>
This is a performance issue question.
I have one, two, and four parameter servers experiments to test performance.
In the two and four parameter server networks utilization, only one parameter server has a highly utilized network and the rest is having low utilization.
For example:
in 4 parameter server tasks with 4 worker tasks. Each task is on one machine.
After running the experiments, I collected the network utlization:
first ps == 693.7K
second ps == 93.7 M
third ps == 15.5 M
fourth ps == 4.1
all four workers have 28.5 M utilization of the network.
if we take 28.5 * 4 = 114 M.
Then 114 M = (93.7+15.5+4.1+693.7K).
System information

I wrote my own example with MNIST dataset and fully connected layers
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): Pip3 installed
TensorFlow version (use command below): 1.11
Python version: 3.6
CPU
Bandwidth is 108 M for every connection.

My question here is why ps did not share tensor size equally.
This is the part of my code that relates to this question. In Tensorflow, there is a Round-Robin fashion for tf.Variables(...) to place variables on parameter servers.
with tf.device(tf.train.replica_device_setter(worker_device="/job:worker/task:%d" % (FLAGS.task_index),cluster=cluster)):
I am using MNIST dataset with minibatch 100. In my case, I will run 600 iterations to go through all dataset( 60000 samples in the data / 100 ) = 600 times. That means the worker will push and pull the model from each parameter server 600 times.
&lt;denchmark-code&gt;  with tf.name_scope('input'):
    X = tf.placeholder(dtype=tf.float32,shape=[None, 784])
    Y = tf.placeholder(dtype=tf.float32,shape=[None, 10])
    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob') # dropout (keep probability)

  with tf.name_scope("weights"):
    W1 = tf.Variable(tf.truncated_normal([784, 256], stddev=0.1)) 
    W2 = tf.Variable(tf.truncated_normal([256, 128], stddev=0.1))
    W3 = tf.Variable(tf.truncated_normal([128, 64], stddev=0.1)) 
    W4 = tf.Variable(tf.truncated_normal([64, 10], stddev=0.1)) 

  with tf.name_scope("biases"):
    b1 = tf.Variable(tf.zeros([256]))
    b2 = tf.Variable(tf.zeros([128]))
    b3 = tf.Variable(tf.zeros([64]))
    b4 = tf.Variable(tf.zeros([10]))

  with tf.name_scope("softmax"):
    XX = tf.reshape(X, [-1, 784])
    z2 = tf.nn.sigmoid(tf.matmul(XX, W1) + b1)
    z3 = tf.nn.sigmoid(tf.matmul(z2, W2) + b2)
    z4 = tf.nn.sigmoid(tf.matmul(z3, W3) + b3)
    z5 = tf.matmul(z4,W4) + b4
  
  # define the loss function ...
  with tf.name_scope('cross_entropy'):
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=z5, labels=Y))

  # create an optimizer then wrap it with SynceReplicasOptimizer
  with tf.name_scope("train"):
    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)

    # global_step tells the graph where it is in training
    global_step = tf.train.get_or_create_global_step()
    
    #Synchronization
    optimizer = tf.train.SyncReplicasOptimizer(optimizer, replicas_to_aggregate=num_workers, total_num_replicas=num_workers)
    
    # averages gradients
    opt = optimizer.minimize(loss, global_step=global_step)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='salemmohammed' date='2019-01-16T06:24:29Z'>
		
Round-Robin fashion for Variable to place variables on parameter servers

As tensors are of different size, this does not always mean perfect load balancing PS which divides the number of bytes equally. You could check out two alternatives:

Bin packing of tensors
Partitioned variables

Let me know if you need more details than that.
		</comment>
		<comment id='2' author='salemmohammed' date='2019-01-16T07:11:17Z'>
		+1 to &lt;denchmark-link:https://github.com/byronyi&gt;@byronyi&lt;/denchmark-link&gt;
 's suggestion. For a quick workaround, you can also try odd number of parameter servers.
		</comment>
		<comment id='3' author='salemmohammed' date='2019-03-01T22:15:58Z'>
		I think &lt;denchmark-link:https://github.com/byronyi&gt;@byronyi&lt;/denchmark-link&gt;
 has captured the explanation nicely: in this particular case, each of the four weight matrices will be placed on a different PS, and the imbalance in their sizes directly corresponds to the imbalance in network traffic.
In terms of better APIs for this problem, you might be interested in the &lt;denchmark-link:https://github.com/tensorflow/community/pull/55&gt;partitioned variable RFC&lt;/denchmark-link&gt;
 that &lt;denchmark-link:https://github.com/yuefengz&gt;@yuefengz&lt;/denchmark-link&gt;
 wrote, which will give a way to divide large variables across multiple PSs (primarily for embeddings, but you could make it work for weights like these as well).
		</comment>
	</comments>
</bug>