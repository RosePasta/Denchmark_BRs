<bug id='38375' author='17patelumang' open_date='2020-04-09T02:54:07Z' closed_time='2020-04-12T23:02:29Z'>
	<summary>tf.tile each dimension individual  operation not supported</summary>
	<description>
System information
&lt;denchmark-code&gt; OS Platform and Distribution : macOS Catalina 10.15.3

TensorFlow installed from : binary

TensorFlow version : 1.15.0

Python version: 3.7.3
&lt;/denchmark-code&gt;

Describe the current behavior
I have tensor
a = tf.Tensor( [[[ 16  15  16  15  87] [  3   4   3   4  87] [  3   4   8   9 133] [  3   4   8   9  871] [  3   4  14   95  87] [  3   7   9  250  87]] [[  3   4   3   41  87] [  3   4   8   93 133] [  3   4   18   9  87] [  3   4  141   5  87] [  3   7   19  25  87] [  0   0   0   0   0]]], shape=(2, 6, 5), dtype=int64)
b = tf.Tensor( [[[ 1] [2] [ 3] [ 1] [ 2] [2]] [[2] [ 2] [ 2] [ 2] [1] [ 0]]], shape=(2, 6, 1), dtype=int64)
Describe the expected behavior
I want vector c such that
[ 16  15  16  15  87] is repeated [1] times ,
[  3   4   3   4  87] is repeated [2] times ,
[  3   4   8   9 133] is repeated [3] times,
[  3   4   8   9  871] is repated [1] times,
[  3   4  14   95  87] is repeated [2] times,
[  3   7   9  250  87] is repated [2] times and so on ............
so c should be
c= tf.Tensor( [[[ 16  15  16  15  87] [  3   4   3   4  87] [  3   4   3   4  87] [  3   4   8   9 133] [  3   4   8   9 133] [  3   4   8   9 133] [  3   4   8   9  871] [  3   4  14   95  87] [  3   4  14   95  87] [  3   7   9  250  87] [  3   7   9  250  87]] [[  3   4   3   41  87] [  3   4   3   41  87] [  3   4   8   93 133] [  3   4   8   93 133] [  3   4   18   9  87] [  3   4   18   9  87] [  3   4  141   5  87] [  3   4  141   5  87] [  3   7   19  25  87] [  3   7   19  25  87] [  0   0   0   0   0]]])
I tried using tf.tile but it only takes same repeation for each dimension .
	</description>
	<comments>
		<comment id='1' author='17patelumang' date='2020-04-09T04:26:34Z'>
		&lt;denchmark-link:https://github.com/17patelumang&gt;@17patelumang&lt;/denchmark-link&gt;

please share a simple standalone code for us to replicate the issue faced along with the error.
		</comment>
		<comment id='2' author='17patelumang' date='2020-04-09T05:04:25Z'>
		import tensorflow as tf 
a = tf.constant([[[16,15,16,15,87],[3,4,3,4,87],[3,4,8,9,133],[3,4,8,9,871],[3,4,14,95,87],[3,7,9,250,87]],[[ 3,4,3,41,87],[3,4,8,93,133],[3,4,18,9,87],[3,4,141,5,87],[3,7,19,25,87],[0,0,0,0,0]]]) 
print(a)
b = tf.constant([[[1],[2],[3],[1],[2],[2]],[[2],[2],[2],[2],[1],[0]]])
print(b)
c = tf.tile(a, b)  ### this will fail
print(c)
####  c = I was c as described in questions

"""
Traceback (most recent call last):
File "tensor_tf.py", line 10, in 
c = tf.tile(a, b)
File "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py", line 11309, in tile
"Tile", input=input, multiples=multiples, name=name)
File "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py", line 794, in _apply_op_helper
op_def=op_def)
File "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
return func(*args, **kwargs)
File "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 3357, in create_op
attrs, op_def, compute_device)
File "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 3426, in _create_op_internal
op_def=op_def)
File "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 1770, in init
control_input_ops)
File "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 1610, in _create_c_op
raise ValueError(str(e))
ValueError: Shape must be rank 1 but is rank 3 for 'Tile' (op: 'Tile') with input shapes: [2,6,5], [2,6,1
"""

		</comment>
		<comment id='3' author='17patelumang' date='2020-04-09T10:02:39Z'>
		i was able to replicate this, please find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/da216942754e945a4865be16bb969dc1/untitled132.ipynb&gt;gist here&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='17patelumang' date='2020-04-10T04:14:29Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 any update ?
		</comment>
		<comment id='5' author='17patelumang' date='2020-04-10T13:05:36Z'>
		&lt;denchmark-link:https://github.com/17patelumang&gt;@17patelumang&lt;/denchmark-link&gt;
 you wrote in your example:
&lt;denchmark-code&gt;b = tf.constant([[[1],[2],[3],[1],[2],[2]],[[2],[2],[2],[2],[1],[0]]])
&lt;/denchmark-code&gt;

But according to the output you described, I believe you wanted:
&lt;denchmark-code&gt;b = tf.constant([[[1],[2],[3],[1],[2],[2]],[[2],[2],[2],[2],[1],[1]]])
&lt;/denchmark-code&gt;

(that is, the last element is 1, not 0).
The answer to that question is important because in the first case, you'd end up with a tensor with 11 elements on the first row, but only 10 elements on the second, which cannot be represented as a dense tensor and would require a ragged tensor. But in the second case, a dense tensor would be sufficient.
		</comment>
		<comment id='6' author='17patelumang' date='2020-04-10T13:55:13Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
  I want empty tenor where its zero in tensor c.  I agree I would need ragged tensor.
So basically let me ask exact question for more clarity
a1 = tf.ragged.constant([[[b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2','c3']], [[b'd1'], [b'e1', b'e2']]])
b1 = tf.ragged.constant([[[b't1', b't2', b't3'], [b'u1', b'u2'], [b'v1', b'v2']], [[b'w1'], [b'x1', b'x2', b'x3']]])
I have a1 , b1 as above , i want c as below
c1 = tf.ragged.constant([[[b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],[b'b1', b'b2', b'b3'],[b'b1', b'b2', b'b3'],[b'c1', b'c2','c3'],[b'c1', b'c2','c3']], [[b'd1'], [b'e1', b'e2'],[b'e1', b'e2'],[b'e1', b'e2']]])
ie [b'a1', b'a2', b'a3'] is repeated equal to number of elements in  [b't1', b't2', b't3'] ie 3 so in c1 we will have [b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],
similarly [b'b1', b'b2', b'b3'] is repeated equal to number of elements in  [b'u1', b'u2'] ie 2 so in c 1we will more have [b'b1', b'b2', b'b3'],[b'b1', b'b2', b'b3'],
similarly [b'c1', b'c2','c3'] is repeated equal to number of elements in  [[b'v1', b'v2'] ie 2 so in c1 we will more have [b'c1', b'c2','c3'],[b'c1', b'c2','c3']
till here all will be past of first tensor  ie [[b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],[b'b1', b'b2', b'b3'],[b'b1', b'b2', b'b3'],[b'c1', b'c2','c3'],[b'c1', b'c2','c3']] now for second ragged tensor
same process as above where [b'd1'] should be repeated 1 times as number of elements in [b'w1'] .....
I was trying by converting b1 into length as b in above ticket but a1,b1,c1 is the actual question
i am trying to fiddle with it since last 2 days , could you please help me with that.
		</comment>
		<comment id='7' author='17patelumang' date='2020-04-10T14:39:38Z'>
		So tf.tile is not powerful enough to handle this use case. It only knows how to repeat entire dimensions a constant number of times.
You'll need to build it manually instead, here's a way to do it. The code uses autograph and I've only tested it in TF 2, but let me know if you have trouble using it in TF 1.
&lt;denchmark-code&gt;a = tf.constant(
    [[[ 16, 15, 16, 15, 87],
      [ 3, 4, 3, 4, 87],
      [ 3, 4, 8, 9, 133],
      [ 3, 4, 8, 9, 871],
      [ 3, 4, 14, 95, 87],
      [ 3, 7, 9, 250, 87]],
     [[ 3, 4, 3, 41, 87],
      [ 3, 4, 8, 93, 133],
      [ 3, 4, 18, 9, 87],
      [ 3, 4, 141, 5, 87],
      [ 3, 7, 19, 25, 87],
      [ 0, 0, 0, 0, 0]]])

b = tf.constant(
    [[[1], [2], [3], [1], [2], [2]],
     [[2], [2], [2], [2], [1], [0]]])

@tf.function
def tile_nd_ragged(a, b):
  # Need a sentinel, otherwise it's hard to give it the initial shape we need.
  # We'll drop the sentinel at the end.
  acc = tf.ragged.constant([[[0] * a.shape[2]]])

  # Work one row at a time...
  for i1 in range(len(a)):    # Should be able to write `for a1, b1 in zip(a, b)` soon.
    a1 = a[i1]
    b1 = b[i1]
    acc1 = tf.TensorArray(dtype=a1.dtype, size=0, dynamic_size=True)
 
    # Do the actual tiling...
    for i2 in tf.range(len(a1)):
      a2 = a1[i2]
      b2 = b1[i2]
      for _ in range(b2[0]):
        acc1 = acc1.write(acc1.size(), a2)
    tmp = tf.expand_dims(acc1.stack(), 0)    # Now `tmp` is row `i1`, tiled.

    acc = tf.concat([acc, tmp], axis=0)    # Add the row to the final result.

  acc = acc[1:]  # Drop the sentinel.
  return acc

print(tile_nd_ragged(a, b))
&lt;/denchmark-code&gt;

Edit: added a few code comments.
		</comment>
		<comment id='8' author='17patelumang' date='2020-04-10T14:46:24Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
  thank you for reply . I asked the actual question for tensor a1,b1,c1 .

Could you please look into that so basically I want (apologies initially i was trying to convert to b tensor having length and some dummy tensor a)

a1 = tf.ragged.constant([[[b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2','c3']], [[b'd1'], [b'e1', b'e2']]])
b1 = tf.ragged.constant([[[b't1', b't2', b't3'], [b'u1', b'u2'], [b'v1', b'v2']], [[b'w1'], [b'x1', b'x2', b'x3']]])
`I have a1 , b1 as above , i want c as below
c1 = tf.ragged.constant([[[b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],[b'b1', b'b2', b'b3'],[b'b1', b'b2', b'b3'],[b'c1', b'c2','c3'],[b'c1', b'c2','c3']], [[b'd1'], [b'e1', b'e2'],[b'e1', b'e2'],[b'e1', b'e2']]])
ie [b'a1', b'a2', b'a3'] is repeated equal to number of elements in [b't1', b't2', b't3'] ie 3 so in c1 we will have [b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],
similarly [b'b1', b'b2', b'b3'] is repeated equal to number of elements in [b'u1', b'u2'] ie 2 so in c 1we will more have [b'b1', b'b2', b'b3'],[b'b1', b'b2', b'b3'],
similarly [b'c1', b'c2','c3'] is repeated equal to number of elements in [[b'v1', b'v2'] ie 2 so in c1 we will more have [b'c1', b'c2','c3'],[b'c1', b'c2','c3']
till here all will be past of first tensor ie [[b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],[b'b1', b'b2', b'b3'],[b'b1', b'b2', b'b3'],[b'c1', b'c2','c3'],[b'c1', b'c2','c3']] now for second ragged tensor
same process as above where [b'd1'] should be repeated 1 times as number of elements in [b'w1']

Also i am using this to convert raw signal to feature as part of my savedModel , so will for i2 in range(len(a1)): work in savedmodel ? or should we need to use tf API to do that (I am using  1.15.2) ?

		</comment>
		<comment id='9' author='17patelumang' date='2020-04-10T14:53:35Z'>
		

So I suspect in your original code b was the lengths of the rows in b1. Please give the code I showed you a try. You should be able to easily modify it so that the inner loop uses len(b2) instead of b2[0].


Yes, this should work with SavedModel, even in TF 1. The loop you indicated is converted to a tf.while_loop by autograph, see https://www.tensorflow.org/guide/function.


		</comment>
		<comment id='10' author='17patelumang' date='2020-04-10T15:16:19Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 thank you again ! :) .

Wrt to 2 thank you , so does it mean using @tf.function we can convert our python code feature processing into TF api feature processing and port it to saved model ?  If thats the case then we can ideally convert our python code to TF API feature transformation to port to saved model and then once we have savedmodel it will become platform independent correct ?

Could you please share more light on what we need to take care while writing customized python function so that it can be easily ported to TF API ? esp in above code range(len(a))

Wrt to I tried your code by replacing b2[0] with len(b2) it gives output as below

&lt;tf.RaggedTensor [[[16, 15, 16, 15, 87], [3, 4, 3, 4, 87], [3, 4, 8, 9, 133], [3, 4, 8, 9, 871], [3, 4, 14, 95, 87], [3, 7, 9, 250, 87]], [[3, 4, 3, 41, 87], [3, 4, 8, 93, 133], [3, 4, 18, 9, 87], [3, 4, 141, 5, 87], [3, 7, 19, 25, 87], [0, 0, 0, 0, 0]]]&gt; which is correct however when i try to convert this to raggedTensor  iin below code it fails
&lt;denchmark-code&gt;a1 = tf.ragged.constant([[[b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2','c3']], [[b'd1'], [b'e1', b'e2']]])
b1 = tf.ragged.constant([[[b't1', b't2', b't3'], [b'u1', b'u2'], [b'v1', b'v2']], [[b'w1'], [b'x1', b'x2', b'x3']]])

@tf.function
def tile_nd_ragged(a, b):
  # Need a sentinel, otherwise it's hard to give it the initial shape we need.
  # We'll drop the sentinel at the end.
  acc = tf.ragged.constant([[[0] * a.shape[2]]])

  # Work one row at a time...
  for i1 in range(len(a)):    # Should be able to write `for a1, b1 in zip(a, b)` soon.
    a1 = a[i1]
    b1 = b[i1]
    acc1 = tf.TensorArray(dtype=a1.dtype, size=0, dynamic_size=True)
 
    # Do the actual tiling...
    for i2 in tf.range(len(a1)):
      a2 = a1[i2]
      b2 = b1[i2]
      for _ in range(b2[0]):
        acc1 = acc1.write(acc1.size(), a2)
    tmp = tf.expand_dims(acc1.stack(), 0)    # Now `tmp` is row `i1`, tiled.

    acc = tf.concat([acc, tmp], axis=0)    # Add the row to the final result.

  acc = acc[1:]  # Drop the sentinel.
  return acc

print(tile_nd_ragged(a1, b1))
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;It gives error 

Traceback (most recent call last):
  File "tf_3.py", line 53, in &lt;module&gt;
    print(tile_nd_ragged(a1, b1))
  File "&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 449, in __call__
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File "&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 392, in _initialize
    *args, **kwds))
  File "&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1847, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File "&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 2147, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 2038, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File "&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 335, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py", line 905, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in converted code:

    tf_3.py:32 tile_nd_ragged  *
        acc = tf.ragged.constant([[[0] * a.shape[2]]])
    &lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py:446 __rmul__
        return self * other

    TypeError: __index__ returned non-int (type NoneType)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='17patelumang' date='2020-04-10T16:02:15Z'>
		A few good places to start are the documentation of &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/function&gt;tf.function&lt;/denchmark-link&gt;
, and the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/index.md&gt;autograph reference&lt;/denchmark-link&gt;
. The hardest part is until you get used to understanding which parts of Python work and which don't.
Re: (2), yes, this lets you write platform-independent code, but the caveat is that it's a (small) subset of Python. And in some cases there are missing features (you just encountered one) that we're still working on.
Re: (1), if you want the inputs to be RaggedTensor it's a bit more complicated, because they have more complex shape. The error that you saw is a bug - sorry about that! - we'll fix it soon. Anyway, even avoiding that bug, there are a few subtleties around dynamic tensor shapes that are tricky to deal with, so I rewrote the code instead.
I recommend carefully reading through it to understand what each operation does - it's a useful exercise.
&lt;denchmark-code&gt;a1 = tf.ragged.constant([[[b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2','c3']], [[b'd1'], [b'e1', b'e2']]])
b1 = tf.ragged.constant([[[b't1', b't2', b't3'], [b'u1', b'u2'], [b'v1', b'v2']], [[b'w1'], [b'x1', b'x2', b'x3']]])

@tf.function
def tile_nd_ragged(a, b):
  # Need a sentinel, otherwise it's hard to give it the initial shape we need.
  # We'll drop the sentinel at the end.
  acc = tf.ragged.constant([[[]]], dtype=a.dtype)

  # Work one row at a time...
  for i1 in range(len(a.nested_row_lengths()[0])):    # Should be able to write `for a1, b1 in zip(a, b)` soon.
    a1 = a[i1]
    b1 = b[i1]
    # If the components have variable length, we can't use a TensorArray anymore,
    # so use a RaggedTensor instead.
    acc1 = tf.ragged.constant([[]], dtype=a.dtype)
 
    # Do the actual tiling...
    for i2 in tf.range(a1.nested_row_lengths()[0][i1]):
      # Need this workaround to let tensors change shape in a loop.
      tf.autograph.experimental.set_loop_options(
          shape_invariants=[(acc1, tf.TensorShape([None, None]))]
      )

      a2 = a1[i2]
      b2 = b1[i2]
      for _ in range(len(b2)):
        acc1 = tf.concat([acc1, tf.expand_dims(a2, 0)], axis=0)

    acc1 = acc1[1:]  # Drop the sentinel.
    acc = tf.concat([acc, tf.expand_dims(acc1, 0)], axis=0)    # Add the row to the final result.

  acc = acc[1:]  # Drop the sentinel.
  return acc

tile_nd_ragged(a1, b1)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='12' author='17patelumang' date='2020-04-10T16:13:42Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 thank you.
Re 2: wrt to feature missing we are talking about range(len(a)) ?
Re1 : wrt to 2 , i understood your code but when i try to run it gives error -
   ValueError: Input tensor 'placeholder_2:0' enters the loop with shape (2,), but has shape (?,) after one iteration. To allow the shape to vary across iterations, use theshape_invariants argument of tf.while_loop to specify a less-specific shape
Also does it mean that since we are using range(len(b2)) it cannot be suceessfully ported into savedmodel ?
		</comment>
		<comment id='13' author='17patelumang' date='2020-04-10T16:30:59Z'>
		range(len(a)) works when a is a Tensor. But other things, like zip(a, b) when a or b are tensors, don't work yet. That kind of things. And all things that do work should be compatible with SavedModel. If they don't, then it's a bug :) . Does this answer your question?
That error is strange. The line tf.autograph.experimental.set_loop_options(shape_invariants=[(acc1, tf.TensorShape([None, None]))]) should have taken care of that, so it means that set_loop_options doesn't work properly in TF 1.15, which is unfortunate. I'd recommend switching to TF 2, but if that's not practical then you'd have to rewrite the for i2 loop using tf.while_loop, like this:
&lt;denchmark-code&gt;    ...
    acc1 = tf.ragged.constant([[]], dtype=a.dtype)
 
    # Do the actual tiling...
    def loop_test(i2, _):
      return i2 &lt; tf.cast(a1.nested_row_lengths()[0][i1], tf.int32)

    def loop_body(i2, acc1):
      a2 = a1[i2]
      b2 = b1[i2]
      for _ in range(len(b2)):
        acc1 = tf.concat([acc1, tf.expand_dims(a2, 0)], axis=0)
      return i2 + 1, acc1

    _, acc1 = tf.while_loop(
        loop_test, loop_body, [0, acc1],
        shape_invariants=[None, tf.TensorShape([None, None])])

    acc1 = acc1[1:]  # Drop the sentinel.
    ...
&lt;/denchmark-code&gt;

		</comment>
		<comment id='14' author='17patelumang' date='2020-04-10T16:52:42Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
  yes i got it  , thank you :) .
Also switching to TF 2.0 is not practical as of now , so i tried the code you suggested as below and i am seeing something strange .
When I try below
&lt;denchmark-code&gt;import tensorflow as tf

tf.enable_eager_execution()

def tile_nd_ragged2(a, b):
  # Need a sentinel, otherwise it's hard to give it the initial shape we need.
  # We'll drop the sentinel at the end.
  acc = tf.ragged.constant([[[]]], dtype=a.dtype)

  # Work one row at a time...
  for i1 in range(len(a.nested_row_lengths()[0])):    # Should be able to write `for a1, b1 in zip(a, b)` soon.
    a1 = a[i1]
    b1 = b[i1]
    # If the components have variable length, we can't use a TensorArray anymore,
    # so use a RaggedTensor instead.
    acc1 = tf.ragged.constant([[]], dtype=a.dtype)
    def loop_test(i2, _):
      return i2 &lt; tf.cast(a1.nested_row_lengths()[0][i1], tf.int32)
    def loop_body(i2, acc1):
      a2 = a1[i2]
      b2 = b1[i2]
      for _ in range(len(b2)):
        acc1 = tf.concat([acc1, tf.expand_dims(a2, 0)], axis=0)
      return i2 + 1, acc1

    _, acc1 = tf.while_loop(loop_test, loop_body, [0, acc1],shape_invariants=[None, tf.TensorShape([None, None])])
    acc1 = acc1[1:]  # Drop the sentinel.
    acc = tf.concat([acc, tf.expand_dims(acc1, 0)], axis=0)    # Add the row to the final result.

  acc = acc[1:]  # Drop the sentinel.
  return acc


x = tf.ragged.constant([[[b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2','c3']], [[b'd1'], [b'e1', b'e2']]])
y = tf.ragged.constant([[[b't1', b't2', b't3'], [b'u1', b'u2'], [b'v1', b'v2']], [[b'w1'], [b'x1', b'x2', b'x3']]])
print(x)
print(y)
print(tile_nd_ragged2(x, y))
&lt;/denchmark-code&gt;

I get below output
&lt;denchmark-code&gt;input 
a = &lt;tf.RaggedTensor [[[b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2', b'c3']], [[b'd1'], [b'e1', b'e2']]]&gt;
b = &lt;tf.RaggedTensor [[[b't1', b't2', b't3'], [b'u1', b'u2'], [b'v1', b'v2']], [[b'w1'], [b'x1', b'x2', b'x3']]]&gt;
output = &lt;tf.RaggedTensor [[[b'a1', b'a2', b'a3'], [b'a1', b'a2', b'a3'], [b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2', b'c3'], [b'c1', b'c2', b'c3']], [[b'd1'], [b'e1', b'e2'], [b'e1', b'e2'], [b'e1', b'e2']]]&gt;
&lt;/denchmark-code&gt;

But when i try to put @tf.function above the function and run exactly same way it gives below error
&lt;denchmark-code&gt;File "tf_3.py", line 101, in &lt;module&gt;
   print(tile_nd_ragged2(x, y))
 File "/&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 449, in __call__
   self._initialize(args, kwds, add_initializers_to=initializer_map)
 File "/&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 392, in _initialize
   *args, **kwds))
 File "/&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1847, in _get_concrete_function_internal_garbage_collected
   graph_function, _, _ = self._maybe_define_function(args, kwargs)
 File "/&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 2147, in _maybe_define_function
   graph_function = self._create_graph_function(args, kwargs)
 File "/&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 2038, in _create_graph_function
   capture_by_value=self._capture_by_value),
 File "/&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py", line 915, in func_graph_from_py_func
   func_outputs = python_func(*func_args, **func_kwargs)
 File "/&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 335, in wrapped_fn
   return weak_wrapped_fn().__wrapped__(*args, **kwds)
 File "/&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py", line 905, in wrapper
   raise e.ag_error_metadata.to_exception(e)
TypeError: in converted code:

   tf_3.py:88 tile_nd_ragged2  *
       _, acc1 = tf.while_loop(loop_test, loop_body, [0, acc1],shape_invariants=[None, tf.TensorShape([None, None])])
   /&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py:2675 while_loop
       back_prop=back_prop)
   /&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py:87 while_loop
       list(shape_invariants), expand_composites=False)
   /&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py:536 map_structure
       structure[0], [func(*x) for x in entries],
   /&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py:536 &lt;listcomp&gt;
       structure[0], [func(*x) for x in entries],
   /&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py:512 _shape_invariant_to_type_spec
       % shape)

   TypeError: Expected shape to be a TypeSpec or TensorShape, got None
&lt;/denchmark-code&gt;



I  need to port this as part of TF API transformation so i need to put @tf.function correct ? , if so then i think it fails when I try to run by keeping it as top @tf.function , am i missing anything ?


Wrt to set_loop_options doesn't work properly in TF 1.15, should I open a bug or feature request ?
(updated the code with import and tf.enable_eager_execution())


		</comment>
		<comment id='15' author='17patelumang' date='2020-04-10T17:00:20Z'>
		Ah, that's another bug fix that landed in TF 2 and not in TF 1.15.... previously, shape_invariants required a value for all loop vars, and only recently we allowed setting some of them to None. Could you try changing this line:
_, acc1 = tf.while_loop(loop_test, loop_body, [0, acc1],shape_invariants=[tf.TensorShape([]), tf.TensorShape([None, None])])
The error message it throws is quite bad, and that's worth filing a bug request.
As for &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/2&gt;#2&lt;/denchmark-link&gt;
, the problem is that there are no plans for a TF 1.16, and we can't backport fixes into 1.15 because it's already released. That's another good reason to consider upgrading when possible.
		</comment>
		<comment id='16' author='17patelumang' date='2020-04-10T17:11:44Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 thank you again ! :)
Just for other who wants working code below is the code
&lt;denchmark-code&gt;import tensorflow as tf

tf.enable_eager_execution()


@tf.function
def tile_nd_ragged2(a, b):
  # Need a sentinel, otherwise it's hard to give it the initial shape we need.
  # We'll drop the sentinel at the end.
  acc = tf.ragged.constant([[[]]], dtype=a.dtype)

  # Work one row at a time...
  for i1 in range(len(a.nested_row_lengths()[0])):    # Should be able to write `for a1, b1 in zip(a, b)` soon.
    a1 = a[i1]
    b1 = b[i1]
    # If the components have variable length, we can't use a TensorArray anymore,
    # so use a RaggedTensor instead.
    acc1 = tf.ragged.constant([[]], dtype=a.dtype)
    def loop_test(i2, _):
      return i2 &lt; tf.cast(a1.nested_row_lengths()[0][i1], tf.int32)
    def loop_body(i2, acc1):
      a2 = a1[i2]
      b2 = b1[i2]
      for _ in range(len(b2)):
        acc1 = tf.concat([acc1, tf.expand_dims(a2, 0)], axis=0)
      return i2 + 1, acc1

    _, acc1 = tf.while_loop(loop_test, loop_body, [0, acc1],shape_invariants=[tf.TensorShape([]), tf.TensorShape([None, None])])
    acc1 = acc1[1:]  # Drop the sentinel.
    acc = tf.concat([acc, tf.expand_dims(acc1, 0)], axis=0)    # Add the row to the final result.

  acc = acc[1:]  # Drop the sentinel.
  return acc

x = tf.ragged.constant([[[b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2','c3']], [[b'd1'], [b'e1', b'e2']]])
y = tf.ragged.constant([[[b't1', b't2', b't3'], [b'u1', b'u2'], [b'v1', b'v2']], [[b'w1'], [b'x1', b'x2', b'x3']]])


print(x)
print(y)
print(tile_nd_ragged2(x, y))
&lt;/denchmark-code&gt;

output
&lt;denchmark-code&gt;&lt;tf.RaggedTensor [[[b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2', b'c3']], [[b'd1'], [b'e1', b'e2']]]&gt;

&lt;tf.RaggedTensor [[[b't1', b't2', b't3'], [b'u1', b'u2'], [b'v1', b'v2']], [[b'w1'], [b'x1', b'x2', b'x3']]]&gt;
&lt;tf.RaggedTensor [[[b'a1', b'a2', b'a3'], [b'a1', b'a2', b'a3'], [b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2', b'c3'], [b'c1', b'c2', b'c3']], [[b'd1'], [b'e1', b'e2'], [b'e1', b'e2'], [b'e1', b'e2']]]&gt;
&lt;/denchmark-code&gt;


Wrt to filing feature/bug request , i will do it referencing this ticket . #38438 (feature/bug ticket)
Also Now the above function has all the function which will work in TF API correct ? (please correct me if i am wrong.) or I need to specially handle like zip(a,b) ? Could you please comment on that esp for i1 in range

(updated ticket with feature/bug request - &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/38438&gt;#38438&lt;/denchmark-link&gt;
)
		</comment>
		<comment id='17' author='17patelumang' date='2020-04-10T17:28:53Z'>
		If it works with this test than the function should work with SavedModel and all the other APIs - let me know if it doesn't. Be sure you keep the @tf.function decorator.
		</comment>
		<comment id='18' author='17patelumang' date='2020-04-10T17:39:05Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 i will try to see if it works in savedmodel in TF 1.15.x and will respond by Tuesday next week max.
		</comment>
		<comment id='19' author='17patelumang' date='2020-04-10T19:33:59Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 I think  there is  bug in script
for same  above script its giving wrong output
&lt;denchmark-code&gt;x = tf.ragged.constant([[[b'd1'], [b'e1', b'e2']],[[b'd1'], [b'e1', b'e2']]])
y = tf.ragged.constant([[[b'w1'], [b'x1', b'x2', b'x3']],[[b'w1'], [b'x1', b'x2', b'x3']]])

print(x)
print(y)
print(tile_nd_ragged2(x, y))
&lt;/denchmark-code&gt;

output
&lt;denchmark-code&gt;&lt;tf.RaggedTensor [[[b'd1'], [b'e1', b'e2']], [[b'd1'], [b'e1', b'e2']]]&gt;
&lt;tf.RaggedTensor [[[b'w1'], [b'x1', b'x2', b'x3']], [[b'w1'], [b'x1', b'x2', b'x3']]]&gt;
&lt;tf.RaggedTensor [[[b'd1']], [[b'd1'], [b'e1', b'e2'], [b'e1', b'e2'], [b'e1', b'e2']]]&gt;
&lt;/denchmark-code&gt;

also when I try
&lt;denchmark-code&gt;x = tf.ragged.constant([[[b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2','c3']], [[b'd1'], [b'e1', b'e2']       ],[[b'f2'],[b'g2']]])
y = tf.ragged.constant([[[b't1', b't2', b't3'], [b'u1', b'u2'],             [b'v1', b'v2']], [[b'w1'], [b'x1', b'x2', b'x3']],[[b'y2'],[b'z2']]])
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;it gives error 

  File "tf_3.py", line 43, in &lt;module&gt;
    print(tile_nd_ragged2(x, y))
  File "/&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 467, in __call__
    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
  File "/&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1141, in _filtered_call
    self.captured_inputs)
  File "/&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1224, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
  File "/&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 511, in call
    ctx=ctx)
  File "/&lt;path&gt;/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "&lt;string&gt;", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError:  slice index 2 of dimension 0 out of bounds.
	 [[{{node while_2/cond/_101/strided_slice}}]] [Op:__inference_tile_nd_ragged2_1422]

Function call stack:
tile_nd_ragged2
&lt;/denchmark-code&gt;

		</comment>
		<comment id='20' author='17patelumang' date='2020-04-10T20:44:22Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 what we need is for dynamic input -
eg -
x= &lt;tf.RaggedTensor [ [ [b'd1'], [b'e1', b'e2'] ]........ ]&gt;
taking  (just for example)
P = [b'd1']
Q = [ [b'd1'], [b'e1', b'e2'] ]
the number of elements in P and Q can change .
So to reiterate if x 's 1st level list has N elements , y's 1st level list will also have N elements , the number of elements in 2nd level list of x's will also be same as y's 2nd level list . The number elements in third level of x's list can differ  in y's third level list . Also there is not 4th level list ever at all .
So can we make it more generalised ?
(updated)
		</comment>
		<comment id='21' author='17patelumang' date='2020-04-11T08:20:00Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
  I found the bug in the code , instead of doing  we need to loop over number of elements in second array so 
		</comment>
		<comment id='22' author='17patelumang' date='2020-04-12T05:51:31Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 its not working in TF API , below is the code and error
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np

############### Function we wrote ############
@tf.function
def tile_nd_ragged2(a, b):
  # Need a sentinel, otherwise it's hard to give it the initial shape we need.
  # We'll drop the sentinel at the end.
  acc = tf.ragged.constant([[[]]], dtype=a.dtype)
  
  # Work one row at a time...
  for i1 in range(len(a.nested_row_lengths()[0])):    # Should be able to write `for a1, b1 in zip(a, b)` soon.
    a1 = a[i1]
    b1 = b[i1]
    # If the components have variable length, we can't use a TensorArray anymore,
    # so use a RaggedTensor instead.
    acc1 = tf.ragged.constant([[]], dtype=a.dtype)
    def loop_test(i2, _):
      return i2 &lt; tf.shape(a1.nested_row_lengths()[0])[0]
      #return i2 &lt; tf.cast(a1.nested_row_lengths()[0][i1], tf.int32)
    def loop_body(i2, acc1):
      a2 = a1[i2]
      b2 = b1[i2]
      for _ in range(len(b2)):
        acc1 = tf.concat([acc1, tf.expand_dims(a2, 0)], axis=0)
      return i2 + 1, acc1

    _, acc1 = tf.while_loop(loop_test, loop_body, [0, acc1],shape_invariants=[tf.TensorShape([]), tf.TensorShape([None, None])])
    acc1 = acc1[1:]  # Drop the sentinel.
    acc = tf.concat([acc, tf.expand_dims(acc1, 0)], axis=0)    # Add the row to the final result.
  acc = acc[1:]  # Drop the sentinel.
  return acc


###############  export_input_fn ############

def export_input_fn():
    serialized_tf_example = tf.placeholder(dtype=tf.string, name ="text") 

    s1Split = tf.strings.split([serialized_tf_example],result_type="RaggedTensor")
    result  = tile_nd_ragged2(s1Split,s1Split)
    result_tf = result.to_tensor()[:1,:1,:1][0][0] ### just take very first element for simplicity
    result_int = tf.strings.to_number(result_tf,out_type=tf.int32)
    
    features ={}
    features["f1"]=result_int
    
    reciever_tensor = {"text": serialized_tf_example}
    return tf.estimator.export.ServingInputReceiver(features, reciever_tensor)

########### Training ###############
x_feature = tf.feature_column.numeric_column('f1')
train_input_fn = tf.estimator.inputs.numpy_input_fn(
      x = {"f1": np.array([1., 2., 3., 4.])},      # Input features
      y = np.array([1.5, 3.5, 5.5, 7.5]),         # true labels
      batch_size=1,
      num_epochs=1,
      shuffle=True)

regressor = tf.estimator.LinearRegressor(feature_columns=[x_feature])
regressor.train(input_fn=train_input_fn, steps=10)

samples = np.array([1])
predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={"f1": samples},num_epochs=1,shuffle=False)
predictions = list(regressor.predict(input_fn=predict_input_fn))
print("---------")
print(predictions)


print("--------- training finished ---------")


regressor.export_saved_model("./model",export_input_fn,as_text=False)
&lt;/denchmark-code&gt;

Error is
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "saved_model.py", line 73, in &lt;module&gt;
    regressor.export_saved_model("./model",export_input_fn,as_text=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 735, in export_saved_model
    strip_default_attrs=True)
  File "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 859, in _export_all_saved_models
    strip_default_attrs=strip_default_attrs)
  File "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 925, in _add_meta_graph_for_mode
    input_receiver = input_receiver_fn()
  File "saved_model.py", line 41, in export_input_fn
    result  = tile_nd_ragged2(s1Split,s1Split)
  File "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 449, in __call__
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 392, in _initialize
    *args, **kwds))
  File "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1847, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 2147, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 2038, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 335, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py", line 905, in wrapper
    raise e.ag_error_metadata.to_exception(e)
AttributeError: in converted code:

    saved_model.py:19 loop_test  *
        return i2 &lt; tf.shape(a1.nested_row_lengths()[0])[0]
    /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py:2675 while_loop
        back_prop=back_prop)
    /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py:153 while_loop
        add_control_dependencies=add_control_dependencies)
    /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py:915 func_graph_from_py_func
        func_outputs = python_func(*func_args, **func_kwargs)
    /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py:132 wrapped_cond
        pred = cond(*_pack_sequence_as(orig_loop_vars, args))
    /var/folders/zl/fgh1kxxj7612sw7nv3v6c7hw0000gn/T/tmpl9mey_a1.py:30 loop_test
        retval__1 = loop_test_scope.mark_return_value(i2 &lt; ag__.converted_call(tf.shape, loop_test_scope.callopts, (ag__.converted_call(a1.nested_row_lengths, loop_test_scope.callopts, (), None, loop_test_scope)[0],), None, loop_test_scope)[0])

    AttributeError: 'Tensor' object has no attribute 'nested_row_lengths'
&lt;/denchmark-code&gt;

Its complaining at return i2 &lt; tf.shape(a1.nested_row_lengths()[0])[0] but a1 is suppose to be RaggedTensor but its coming to be Tensor.
What do we need to change here ?
		</comment>
		<comment id='23' author='17patelumang' date='2020-04-12T12:23:34Z'>
		It's possible that a[i1] returns a Tensor instead of Ragged when it can (that is when all dimensions are equal). Add a few print statements to see what you get. You can handle both by adding an if isinstance(a1, tf.RaggedTensor):.
		</comment>
		<comment id='24' author='17patelumang' date='2020-04-12T17:37:20Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
  I tried as you suggested but it still fails but now giving different error
&lt;denchmark-code&gt;    saved_model.py:14 tile_nd_ragged2  *
        for i1 in range(len(a.nested_row_lengths()[0])):    # Should be able to write `for a1, b1 in zip(a, b)` soon.
    /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/operators/control_flow.py:315 for_stmt
        composite_symbol_names)
    /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/operators/control_flow.py:478 _tf_range_for_stmt
        opts=opts,
    /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/operators/control_flow.py:769 _tf_while_stmt
        aug_init_vars, **opts)
    /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py:2675 while_loop
        back_prop=back_prop)
    /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py:234 while_loop
        len_orig_loop_vars], expand_composites=True))
    /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py:1068 _check_shapes_compat
        "specify a less-specific shape." % (input_t.name, shape, t.shape))

    ValueError: Input tensor 'RaggedConstant/Const_1:0' enters the loop with shape (2,), but has shape (3,) after one iteration. To allow the shape to vary across iterations, use the `shape_invariants` argument of tf.while_loop to specify a less-specific shape.
&lt;/denchmark-code&gt;

Code :-
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np

tf.enable_eager_execution()

############### Function we wrote ############
@tf.function
def tile_nd_ragged2(a, b):
  # Need a sentinel, otherwise it's hard to give it the initial shape we need.
  # We'll drop the sentinel at the end.
  acc = tf.ragged.constant([[[]]], dtype=a.dtype)
  
  # Work one row at a time...
  for i1 in range(len(a.nested_row_lengths()[0])):    # Should be able to write `for a1, b1 in zip(a, b)` soon.
    
    a1 = a[i1]
    b1 = b[i1]

    # If the components have variable length, we can't use a TensorArray anymore,
    # so use a RaggedTensor instead.
    acc1 = tf.ragged.constant([[]], dtype=a.dtype)
    def loop_test(i2, _):
    	shape = None
    	if isinstance(a1, tf.RaggedTensor):
    		print(" --- ragged --- ")
    		print(a1)
    		print(shape)
    		shape = tf.shape(a1.nested_row_lengths()[0])[0]
    	else:
    		print(" --- tensor --- ")
    		print(a1)
    		
    		shape = tf.shape(a1)[0]
    		print(tf.shape(a1))
    		print(shape)

    	return i2 &lt; shape
      #return i2 &lt; tf.cast(a1.nested_row_lengths()[0][i1], tf.int32)
    def loop_body(i2, acc1):
      #print(a1[i2])
      a2 = a1[i2]
      b2 = b1[i2]

      for _ in range(len(b2)):
        acc1 = tf.concat([acc1, tf.expand_dims(a2, 0)], axis=0)
      return i2 + 1, acc1

    _, acc1 = tf.while_loop(loop_test, loop_body, [0, acc1],shape_invariants=[tf.TensorShape([]), tf.TensorShape([None, None])])
    acc1 = acc1[1:]  # Drop the sentinel.
    acc = tf.concat([acc, tf.expand_dims(acc1, 0)], axis=0)    # Add the row to the final result.
  acc = acc[1:]  # Drop the sentinel.
  return acc


###############  export_input_fn ############

# def export_input_fn():
#     serialized_tf_example = tf.placeholder(dtype=tf.string, name ="text") 

#     s1Split = tf.strings.split([serialized_tf_example],result_type="RaggedTensor")
#     result  = tile_nd_ragged2(s1Split,s1Split)
#     result_tf = result.to_tensor()[:1,:1,:1][0][0] ### just take very first element for simplicity
#     result_int = tf.strings.to_number(result_tf,out_type=tf.int32)
    
#     features ={}
#     features["f1"]=result_int
    
#     reciever_tensor = {"text": serialized_tf_example}
#     return tf.estimator.export.ServingInputReceiver(features, reciever_tensor)

def export_input_fn():
    serialized_tf_example = tf.placeholder(dtype=tf.string, shape=(None), name ="text") 

    s1Split = tf.strings.split([serialized_tf_example],result_type="RaggedTensor")
    s1Split = tf.strings.split(s1Split,sep='@',result_type="RaggedTensor")
    result  = tile_nd_ragged2(s1Split,s1Split)
    result_tf = result.to_tensor()[:1,:,:][0][0]
    #result_tf = result.to_tensor()
    result_int = tf.strings.to_number(result_tf,out_type=tf.int32)

    features ={}
    features["f1"]=result_int ### this will be tf.Tensor([1], shape=(1,), dtype=int32)
    
    reciever_tensor = {"text": serialized_tf_example}
    print(reciever_tensor)   
    return tf.estimator.export.ServingInputReceiver(features, reciever_tensor)

########### Training ###############
x_feature = tf.feature_column.numeric_column('f1')
train_input_fn = tf.estimator.inputs.numpy_input_fn(
      x = {"f1": np.array([1., 2., 3., 4.])},      # Input features
      y = np.array([1.5, 3.5, 5.5, 7.5]),         # true labels
      batch_size=1,
      num_epochs=1,
      shuffle=True)

regressor = tf.estimator.LinearRegressor(feature_columns=[x_feature])
regressor.train(input_fn=train_input_fn, steps=10)

samples = np.array([1])
predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={"f1": samples},num_epochs=1,shuffle=False)
predictions = list(regressor.predict(input_fn=predict_input_fn))
print("---------")
print(predictions)


print("--------- training finished ---------")


regressor.export_saved_model("./model",export_input_fn,as_text=False)
&lt;/denchmark-code&gt;

Changes in code

Added specific handling as you suggested
bit changed export_input_fn
Also why does the code fail when we replace it by placeholders ?

&lt;denchmark-link:https://github.com/mdaley&gt;@mdaley&lt;/denchmark-link&gt;
 kindly please advice
		</comment>
		<comment id='25' author='17patelumang' date='2020-04-12T20:20:09Z'>
		I ran the code in TF 2 where the error message is cleaner. Looks like shape invariants once more, so the outer loop needs to be rewritten for TF 1 as well. Unfortunately that makes the code quite ugly now:
&lt;denchmark-code&gt;@tf.function
def tile_nd_ragged2(a, b):
  # Need a sentinel, otherwise it's hard to give it the initial shape we need.
  # We'll drop the sentinel at the end.
  acc = tf.ragged.constant([[[]]], dtype=a.dtype)
  
  print('acc is', acc)
  # Work one row at a time...
  # for i1 in tf.range(len(a.nested_row_lengths()[0])):    # Should be able to write `for a1, b1 in zip(a, b)` soon.
  def outer_loop_test(i1, _):
    return i1 &lt; len(a.nested_row_lengths()[0])

  def outer_loop_body(i1, acc):
    
    a1 = a[i1]
    b1 = b[i1]

    # If the components have variable length, we can't use a TensorArray anymore,
    # so use a RaggedTensor instead.
    acc1 = tf.ragged.constant([[]], dtype=a.dtype)
    def loop_test(i2, _):
    	shape = None
    	if isinstance(a1, tf.RaggedTensor):
    		print(" --- ragged --- ")
    		print(a1)
    		print(shape)
    		shape = tf.shape(a1.nested_row_lengths()[0])[0]
    	else:
    		print(" --- tensor --- ")
    		print(a1)
    		
    		shape = tf.shape(a1)[0]
    		print(tf.shape(a1))
    		print(shape)

    	return i2 &lt; shape
      #return i2 &lt; tf.cast(a1.nested_row_lengths()[0][i1], tf.int32)
    def loop_body(i2, acc1):
      #print(a1[i2])
      a2 = a1[i2]
      b2 = b1[i2]

      for _ in range(len(b2)):
        acc1 = tf.concat([acc1, tf.expand_dims(a2, 0)], axis=0)
      return i2 + 1, acc1

    _, acc1 = tf.while_loop(loop_test, loop_body, [0, acc1],shape_invariants=[tf.TensorShape([]), tf.TensorShape([None, None])])
    acc1 = acc1[1:]  # Drop the sentinel.
    acc = tf.concat([acc, tf.expand_dims(acc1, 0)], axis=0)    # Add the row to the final result.

    return i1 + 1, acc

  _, acc = tf.while_loop(
      outer_loop_test, outer_loop_body,
      [0, acc],
      shape_invariants=[
                        tf.TensorShape([]),
                        tf.TensorShape([None, None, None])
                        ]
      )
  acc = acc[1:]  # Drop the sentinel.
  return acc
&lt;/denchmark-code&gt;

		</comment>
		<comment id='26' author='17patelumang' date='2020-04-12T20:50:44Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
  thank you  .  Looks like it working now .
I have a question when we tested using placeholder why it failed and not when we input specific string ?
		</comment>
		<comment id='27' author='17patelumang' date='2020-04-12T23:02:29Z'>
		Glad to hear it! That's a good question. I think in the first tests a.nested_row_lengths()[0] was returning a Python value, the the loop was being unrolled in Python. Unrolled loops don't have the shape restrictions of tf.while_loop (they just repeat the graph ops). Once you added the placeholder, the result of that became a Tensor, causing the loop to become a tf.while_loop, which is more restrictive. Anyway, we should have written the loop with tf.range in the first place to avoid that unrolling.
		</comment>
		<comment id='28' author='17patelumang' date='2020-04-12T23:02:30Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38375&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38375&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='29' author='17patelumang' date='2020-04-13T23:45:56Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
  we are using  , does len(b2)  need to  be changed to tf api ?
		</comment>
		<comment id='30' author='17patelumang' date='2020-04-14T01:44:28Z'>
		Only if it gives an error. len(b2) works if b2 is a Tensor, but currently fails if it's a RaggedTensor. Working to support both.
		</comment>
		<comment id='31' author='17patelumang' date='2020-04-14T03:29:56Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
  thank you , what to use for RaggedTensor ?
		</comment>
		<comment id='32' author='17patelumang' date='2020-04-14T03:45:25Z'>
		b2.nrows() should work.
		</comment>
		<comment id='33' author='17patelumang' date='2020-04-14T03:49:45Z'>
		That said, check out &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/38438#issuecomment-613200554&gt;this comment&lt;/denchmark-link&gt;
 for a shorter solution that users some special tricks from RaggedTensors.
		</comment>
		<comment id='34' author='17patelumang' date='2020-04-14T04:37:24Z'>
		&lt;denchmark-link:https://github.com/mdaley&gt;@mdaley&lt;/denchmark-link&gt;
 which comment i should look at ? I only opened this 38412 ticket :) ?
		</comment>
		<comment id='35' author='17patelumang' date='2020-04-14T12:01:50Z'>
		Oops, wrong link - updated.
		</comment>
		<comment id='36' author='17patelumang' date='2020-04-14T17:46:43Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
  thank you
		</comment>
	</comments>
</bug>