<bug id='45401' author='grofte' open_date='2020-12-04T11:39:55Z' closed_time='2020-12-04T18:56:20Z'>
	<summary>TF 2.4rc3 removes names from ops in Model, unlike TF 2.3.1</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): 2.4rc3
Python version: 3.8.5

Describe the current behavior
In TF 2.4rc3
import tensorflow as tf
from tensorflow.keras.layers import Input
from tensorflow.keras.models import Model
 
inputs = Input(shape=(1,), name='input_layer')
outputs = tf.identity(inputs, name='test_layer')
model = Model(inputs, outputs)
 
print(model.output_names)
['tf.identity']
Describe the expected behavior
In TF 2.3.1 and earlier
import tensorflow as tf
from tensorflow.keras.layers import Input
from tensorflow.keras.models import Model
 
inputs = Input(shape=(1,), name='input_layer')
outputs = tf.identity(inputs, name='test_layer')
model = Model(inputs, outputs)
 
print(model.output_names)
['tf_op_layer_test_layer']
So you could create a model with a function from Tensorflow as the last layer and name it. If you needed to recover the name you can simply slice off the prefix.
Standalone code to reproduce the issue
See above.
Other info / logs
If the output is a layer from Keras it works in both versions:
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
 
inputs = Input(shape=(1,), name='input_layer')
outputs = Dense(1, name='test_layer')(inputs)
model = Model(inputs, outputs)
 
print(model.output_names)
['test_layer']
But that doesn't change that this is a code-breaking behavior change.
	</description>
	<comments>
		<comment id='1' author='grofte' date='2020-12-04T18:40:40Z'>
		I was able to reproduce it. Here is the &lt;denchmark-link:https://gist.github.com/geetachavan1/3c6510343e34e18aeae8de9bf7da4e69&gt;gist&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='grofte' date='2020-12-04T18:56:20Z'>
		Hi &lt;denchmark-link:https://github.com/grofte&gt;@grofte&lt;/denchmark-link&gt;
, what you're running into is actually covered by this bullet in the release notes:
"Code that relies on the exact number and names of the op layers that TensorFlow operations were converted into may have changed."
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/releases&gt;https://github.com/tensorflow/tensorflow/releases&lt;/denchmark-link&gt;

If you're looking for more context around this, the discussion around
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/43844&gt;#43844&lt;/denchmark-link&gt;

and
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/43840&gt;#43840&lt;/denchmark-link&gt;

should have more info.
Tl;dr: Before 2.4 the process that tried to automatically convert tf ops into keras layers in functional models was extremely problematic for many reasons (memory leaks, bugs, poor support, etc.). The price of improving it was that the names of these autogen'd layers would have to change, and that the name arg passed in to the op would not be able to affect the name of the generated layer.
Because we never made any API guarantees about how tf ops would be implicitly converted into layers, we judged that the very niche usage of relying on their names wasn't worth supporting. It can generally be worked around with slight restructuring of the code. (E.g. you can use a no-op keras lambda layer at the end to explicitly set a name in your case rather than using tf.Identity).
		</comment>
		<comment id='3' author='grofte' date='2020-12-04T18:56:22Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45401&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45401&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='grofte' date='2020-12-06T11:30:28Z'>
		Okay, so you prefer something along the lines of this if we are naming things:
import tensorflow as tf
from tensorflow.keras.layers import Input, Lambda
from tensorflow.keras.models import Model
 
inputs = Input(shape=(1,), name='input_layer')
x = tf.identity(inputs)
outputs = Lambda(lambda x: x, name='test_layer')(x)
model = Model(inputs, outputs)
 
print(model.output_names)
['test_layer']
Then you should remove the name parameter from ops. And since there's a method for getting layers by name in a model (very important in transfer learning) there is an implicit guarantee that layer names can be used as code bearing.
I will make sure to use the Lambda layer for any ops I want to name explicitly going forward.
		</comment>
		<comment id='5' author='grofte' date='2020-12-07T00:16:52Z'>
		If you are trying to explicitly name an output layer for the purpose of Keras APIs that rely on layer names then that is one option.
If you are explicitly constructing a tf.keras.layers layer as the last layer in your Keras model rather than relying on TF ops then you can use name=... there as well, without needing to create an identity lambda layer.
E.g.:
&lt;denchmark-code&gt;inputs = Input(shape=(1,), name='input_layer')
outputs = tf.keras.layers.Dense(5, name='test_layer')(inputs)
model = Model(inputs, outputs)
&lt;/denchmark-code&gt;

We have not changed the naming semantics for any tf.keras.layers layers that you explicitly construct, because just as you mention there are APIs that treat layer names as load-bearing.
What you're running into is that the Keras layer/model APIs and the lower-level TF apis are entirely different subsets of the API that have organically grown in different ways to serve different purposes. For a very long time it was completely impossible to use lower-level TF apis to create a Keras functional model, and you could only use tf.keras.layers.
At some point Keras started hooking into the lower-level TF ops so that if you use them in Keras functional model construction it would try to turn those lower-level tf api calls into keras layers under the hood and let you build Keras models with them. This was always a best-effort approach that tries to match the intent of the lower-level api as closely as possible, but it is not and has never been perfect. So, we can't remove an argument from the lower-level APIs (which specifies the name of the created tf op) just because Keras itself is unable to use that argument to set the name of the generated layer. (Again, the discussion in the bugs linked above explains why)
One of the positive side effects of the refactoring of the internals is that the best-effort conversions will work more reliably now and better match the intent of the hooked lower-level apis, with the exception of no longer being able to read the name argument to set the layer name.
Generally speaking if you're using implicit conversions of lower-level tf ops into Keras layers, you should not be relying on semantics around exactly what layers get auto-generated (e.g. the number of layers created for a specific TF api, their names, etc.), because that is all subject to change. We will do our best to make sure that calling the generated layers on concrete inputs will match the behavior of directly calling the low-level api on those same inputs.
		</comment>
	</comments>
</bug>