<bug id='34659' author='andrewstanfordjason' open_date='2019-11-27T20:44:38Z' closed_time='2020-09-30T19:13:47Z'>
	<summary>Dataset iterator hanging from within a dataset interleave</summary>
	<description>

== check python ===================================================
python version: 3.7.4
python branch:
python build version: ('default', 'Sep  7 2019 18:27:02')
python compiler version: Clang 10.0.1 (clang-1001.0.46.4)
python implementation: CPython
== check os platform ===============================================
os: Darwin
os kernel version: Darwin Kernel Version 19.0.0: Wed Sep 25 20:18:50 PDT 2019; root:xnu-6153.11.262/RELEASE_X86_64
os release version: 19.0.0
os platform: Darwin-19.0.0-x86_64-i386-64bit
linux distribution: ('', '', '')
linux os distribution: ('', '', '')
mac version: ('10.15', ('', '', ''), 'x86_64')
uname: uname_result(system='Darwin', node='Andrews-MacBook.local', release='19.0.0', version='Darwin Kernel Version 19.0.0: Wed Sep 25 20:18:50 PDT 2019; root:xnu-6153.11.262/RELEASE_X86_64', machine='x86_64', processor='i386')
architecture: ('64bit', '')
machine: x86_64
== are we in docker =============================================
No
== compiler =====================================================
xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun
== check pips ===================================================
numpy                1.17.4
protobuf             3.10.0
tensorflow           2.0.0
tensorflow-estimator 2.0.1
== check for virtualenv =========================================
True
== tensorflow import ============================================
tf.version.VERSION = 2.0.0
tf.version.GIT_VERSION = v2.0.0-rc2-26-g64c3d382ca
tf.version.COMPILER_VERSION = 4.2.1 Compatible Apple LLVM 10.0.0 (clang-1000.11.45.5)
== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset
== nvidia-smi ===================================================
./tf_env_collect.sh: line 147: nvidia-smi: command not found
== cuda libs  ===================================================
== tensorflow installed from info ==================
Name: tensorflow
Version: 2.0.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /Users/andrew/.local/share/virtualenvs/lib_andrew_scratch--yvJ8pLH/lib/python3.7/site-packages
Required-by:
== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 7, 4, 'final', 0)
== bazel version  ===============================================

Describe the current behavior
It seems all threads are waiting in a deadlock behaviour
Describe the expected behavior
To return a dataset iterator
Code to reproduce the issue
&lt;denchmark-code&gt;#!/usr/bin/env python
import tensorflow as tf

def gen( ):
	noise_tfrecord = "test.tfrecord"
	raw_noise_dataset = tf.data.TFRecordDataset(noise_tfrecord)
	noise_count = sum(1 for _ in raw_noise_dataset)
	yield 1.

def do_bug():
	signal_tfrecord = "test.tfrecord"
	raw_signal_dataset = tf.data.TFRecordDataset(signal_tfrecord)
	dataset = raw_signal_dataset.interleave(lambda x: tf.data.Dataset.from_generator(gen, 
			output_types=(tf.float32), 
			args=( )),
			cycle_length=2,
			block_length=1,
			num_parallel_calls = 1 )

	for d in dataset:
		print(d)

	return 

def _int_feature(value):
    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

def make_dataset():
	tfrecord_filename = "test.tfrecord"
	with tf.io.TFRecordWriter(tfrecord_filename) as writer:
		for i in range(4):
			feature = {'feat' : _int_feature(42)}

			example_proto = tf.train.Example(features=tf.train.Features(feature=feature))
			writer.write(example_proto.SerializeToString())  

if __name__== "__main__":
	make_dataset()
	do_bug()

&lt;/denchmark-code&gt;

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3898706/bug.py.zip&gt;bug.py.zip&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='andrewstanfordjason' date='2019-11-28T13:08:08Z'>
		Smaller example:
&lt;denchmark-code&gt;
#!/usr/bin/env python
import tensorflow as tf

def gen( ):
	raw_dataset = tf.data.TFRecordDataset("not_a_file")
	count = sum(1 for _ in raw_dataset)
	yield 1.

def do_bug():
	raw_signal_dataset = tf.data.Dataset.from_tensor_slices([1, 1, 1, 1])

	dataset = raw_signal_dataset.interleave(lambda x: tf.data.Dataset.from_generator(gen, 
			output_types=(tf.float32), args=()), num_parallel_calls = 1 )

	for d in dataset:
		print(d)

	return 

if __name__== "__main__":
	do_bug()

&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='andrewstanfordjason' date='2019-11-28T13:10:50Z'>
		Doesn't hang if:

num_parallel_calls = None
length of the list [1, 1, 1, 1] is shorter than 4
the dataset within the generator is not built from tf.data.TFRecordDataset

		</comment>
		<comment id='3' author='andrewstanfordjason' date='2019-11-29T09:01:58Z'>
		I have tried on colab with TF version 2.0,2.1.0-dev20191128 and was able to reproduce the issue.Thanks!
		</comment>
		<comment id='4' author='andrewstanfordjason' date='2019-12-09T22:03:05Z'>
		I can confirm that I can reproduce the issue. This seems to be a general TensorFlow runtime issue related to not properly cancelling TensorFlow computation which executes Python computation (your gen function) which in turns executes TensorFlow computation (the tf.data.TFRecordDataset call).
I do not foresee this being prioritized for a fix and would instead recommend not layering TensorFlow and Python computation this way. In this particular case, instead of doing from_generator, you could use tf.data.TFRecordDataset directly and follow it up with tf.data.Dataset.map that uses py_function, which performs the Python computation you previously did in from_generator.
		</comment>
		<comment id='5' author='andrewstanfordjason' date='2020-09-16T18:29:35Z'>
		&lt;denchmark-link:https://github.com/andrewstanfordjason&gt;@andrewstanfordjason&lt;/denchmark-link&gt;

Please let us know if this is still an issue.
		</comment>
		<comment id='6' author='andrewstanfordjason' date='2020-09-23T18:34:02Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='7' author='andrewstanfordjason' date='2020-09-30T19:13:45Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='8' author='andrewstanfordjason' date='2020-09-30T19:13:51Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34659&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34659&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>