<bug id='33920' author='zhisbug' open_date='2019-11-01T18:58:28Z' closed_time='2020-06-02T00:10:54Z'>
	<summary>Collective AllReduce ops cannot run with variables shared across tasks in between-graph replication</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): installed by pip install tensorflow-gpu
TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: CUDA 10.0 and cudnn 7.6
GPU model and memory:   Tesla V100, ~16Gb

Describe the current behavior
The collective ops executor will fail with the following program.
Describe the expected behavior
The expected behavior is that the program should run correctly in a between-graph replication as if the graph does not contain collective ops everything will run correctly.
Code to reproduce the issue
import threading
from multiprocessing import Process

import tensorflow as tf
from tensorflow.core.protobuf import config_pb2
from tensorflow.python import ops
from tensorflow.python.client.session import Session
from tensorflow.python.ops import collective_ops

cluster_spec = {
    "worker": [
        "localhost:14286",
        "localhost:14287"
    ]
}
inputs = [0.1, 1.1, 2.1, 3.1, 4.1, 5.1, 6.1, 7.1]
group_size = 4
group_key = 1
instance_key = 1
use_nccl = False
num_gpus_per_node = 2


def _configure(group_size):
    gpu_options = config_pb2.GPUOptions(
        visible_device_list='0,1',
        per_process_gpu_memory_fraction=0.7 / (group_size))
    experimental = config_pb2.ConfigProto.Experimental(collective_nccl=use_nccl)
    experimental.collective_group_leader = '/job:worker/replica:0/task:0'
    return config_pb2.ConfigProto(gpu_options=gpu_options, experimental=experimental)


class TFCluster(object):
    def __init__(self, cluster_spec):
        self._cluster_spec = cluster_spec
        self._num_worker = 0
        self._tf_servers = []

    def start(self):
        def server(job_name: str, task_index: int):
            server = tf.distribute.Server(self._cluster_spec,
                                          job_name=job_name,
                                          task_index=task_index,
                                          config=_configure(group_size))
            server.join()

        self._num_worker = len(cluster_spec.get("worker", []))
        assert self._num_worker &gt;= 1
        for i in range(self._num_worker):
            self._tf_servers.append(Process(target=server,
                                            args=("worker", i), daemon=True))
        for proc in self._tf_servers:
            proc.start()

    def stop(self):
        for proc in self._tf_servers:
            proc.terminate()


def between_graph_test():
    def run_between_graph_clients(client_fn, cluster_spec, num_gpus, *args,
                                  **kwargs):
        threads = []
        for task_type in ['chief', 'worker']:
            for task_id in range(len(cluster_spec.get(task_type, []))):
                t = threading.Thread(
                    target=test_reduction,
                    args=(task_type, task_id, num_gpus) + args,
                    kwargs=kwargs)
                t.start()
                threads.append(t)
        for t in threads:
            t.join()

    def test_reduction(task_type,
                       task_id,
                       num_gpus):
        worker_device = "/job:%s/task:%d" % (task_type, task_id)
        master_target = "grpc://" + cluster_spec[task_type][0]
        with ops.Graph().as_default(), Session(target=master_target) as sess:
            collectives = []
            for i in range(num_gpus):
                with ops.device('/job:worker/task:0/device:CPU:0'):  # make sure all use the same variable
                    t = tf.Variable(inputs)
                with ops.device(worker_device + '/device:GPU:' + str(i)):
                    collectives.append(collective_ops.all_reduce(
                        t, group_size, group_key, instance_key, 'Add', 'Div'))
            run_options = config_pb2.RunOptions()
            run_options.experimental.collective_graph_key = 6
            sess.run(tf.compat.v1.global_variables_initializer())
            res_m = sess.run(collectives, options=run_options)
            print(res_m)

    run_between_graph_clients(
        test_reduction,
        cluster_spec,
        num_gpus_per_node)

# launch in-process clusters
cluster = TFCluster(cluster_spec)
cluster.start()

# run between graph execution
between_graph_test()
cluster.stop()
Other info / logs
&lt;denchmark-code&gt;2019-11-01 18:37:48.066477: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:14286
2019-11-01 18:37:48.066556: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:14287
2019-11-01 18:37:48.733123: I tensorflow/core/distributed_runtime/base_rendezvous_mgr.cc:161] Skipping rendezvous re-initialization.
2019-11-01 18:37:50.385293: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Aborted: [_Derived_]Cleanup 95609632845199527
	 [[{{node CollectiveReduce}}]]
2019-11-01 18:37:50.385319: W tensorflow/core/distributed_runtime/rpc/grpc_worker_service.cc:510] RecvTensor cancelled for 95609632845199527
[array([0.1, 1.1, 2.1, 3.1, 4.1, 5.1, 6.1, 7.1], dtype=float32), array([0.1, 1.1, 2.1, 3.1, 4.1, 5.1, 6.1, 7.1], dtype=float32)]
Exception in thread Thread-2:
Traceback (most recent call last):
  File "/home/ubuntu/arion/arion-env/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1365, in _do_call
    return fn(*args)
  File "/home/ubuntu/arion/arion-env/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1350, in _run_fn
    target_list, run_metadata)
  File "/home/ubuntu/arion/arion-env/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1443, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.AbortedError: From /job:worker/replica:0/task:0:
[_Derived_]Cleanup 95609632845199527
	 [[{{node CollectiveReduce}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/anaconda3/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/home/ubuntu/anaconda3/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ubuntu/pycharm/examples/allreduce/test_collective.py", line 91, in test_reduction
    res_m = sess.run(collectives, options=run_options)
  File "/home/ubuntu/arion/arion-env/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 956, in run
    run_metadata_ptr)
  File "/home/ubuntu/arion/arion-env/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/ubuntu/arion/arion-env/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1359, in _do_run
    run_metadata)
  File "/home/ubuntu/arion/arion-env/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.AbortedError: From /job:worker/replica:0/task:0:
[_Derived_]Cleanup 95609632845199527
	 [[node CollectiveReduce (defined at /arion/arion-env/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]

Original stack trace for 'CollectiveReduce':
  File "/anaconda3/lib/python3.6/threading.py", line 884, in _bootstrap
    self._bootstrap_inner()
  File "/anaconda3/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/anaconda3/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "/pycharm/examples/allreduce/test_collective.py", line 87, in test_reduction
    t, group_size, group_key, instance_key, 'Add', 'Div'))
  File "/arion/arion-env/lib/python3.6/site-packages/tensorflow_core/python/ops/collective_ops.py", line 58, in all_reduce
    subdiv_offsets=subdiv_offsets)
  File "/arion/arion-env/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_collective_ops.py", line 349, in collective_reduce
    name=name)
  File "/arion/arion-env/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py", line 793, in _apply_op_helper
    op_def=op_def)
  File "/arion/arion-env/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/arion/arion-env/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 3360, in create_op
    attrs, op_def, compute_device)
  File "/arion/arion-env/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 3429, in _create_op_internal
    op_def=op_def)
  File "/arion/arion-env/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 1751, in __init__
    self._traceback = tf_stack.extract_stack()

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='zhisbug' date='2019-11-01T19:03:38Z'>
		I believe this issue is related to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33469&gt;#33469&lt;/denchmark-link&gt;
, where the fix there was to simply lower the log level to avoid repeatedly logging .
It seems that the collective executor cannot correctly interpret a shared variable (declared with the same name and device placement string) across two tasks in between-graph replication (as the example shown above).
		</comment>
		<comment id='2' author='zhisbug' date='2019-11-01T19:07:38Z'>
		I hope &lt;denchmark-link:https://github.com/dubey&gt;@dubey&lt;/denchmark-link&gt;
 can take a look at this issue, thanks!
		</comment>
		<comment id='3' author='zhisbug' date='2020-05-18T22:33:57Z'>
		Is this still an issue with latest tf version 2.2?
		</comment>
		<comment id='4' author='zhisbug' date='2020-05-25T23:26:24Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='5' author='zhisbug' date='2020-06-02T00:10:53Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='6' author='zhisbug' date='2020-06-02T00:10:55Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33920&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33920&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>