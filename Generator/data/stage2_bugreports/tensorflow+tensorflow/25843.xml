<bug id='25843' author='Threynaud' open_date='2019-02-18T15:58:57Z' closed_time='2019-03-18T22:40:39Z'>
	<summary>TF 2.0: Can't use tf.keras.layers.LSTM on GPU.</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
OS Platform and Distribution: Google Colab
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): tf-nightly-gpu-2.0-preview==2.0.0.dev20190218
Python version: 3.6.7
CUDA/cuDNN version: CUDA 10.0 CuDNN 7.4.2
GPU model and memory: ?

Describe the current behavior
Getting the following error when trying to fit a model using tf.keras.layers.LSTM with tf.keras:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
UnknownError                              Traceback (most recent call last)
&lt;ipython-input-30-145a5e1acc45&gt; in &lt;module&gt;()
     18 model.fit(x=dataset,
     19           epochs=1,
---&gt; 20           verbose=1)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    762           workers=0,
    763           shuffle=shuffle,
--&gt; 764           initial_epoch=initial_epoch)
    765 
    766     # Case 3: Symbolic tensors or Numpy array-like.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1482         shuffle=shuffle,
   1483         initial_epoch=initial_epoch,
-&gt; 1484         steps_name='steps_per_epoch')
   1485 
   1486   def evaluate_generator(self,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)
    244 
    245       is_deferred = not model._is_compiled
--&gt; 246       batch_outs = batch_function(*batch_data)
    247       if not isinstance(batch_outs, list):
    248         batch_outs = [batch_outs]

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)
   1226       else:
   1227         self._make_fit_function()
-&gt; 1228         outputs = self._fit_function(ins)  # pylint: disable=not-callable
   1229 
   1230     if reset_metrics:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)
   3207         value = math_ops.cast(value, tensor.dtype)
   3208       converted_inputs.append(value)
-&gt; 3209     outputs = self._graph_fn(*converted_inputs)
   3210     return nest.pack_sequence_as(self._outputs_structure,
   3211                                  [x.numpy() for x in outputs])

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
    438       raise TypeError("Keyword arguments {} unknown. Expected {}.".format(
    439           list(kwargs.keys()), list(self._arg_keywords)))
--&gt; 440     return self._call_flat(args)
    441 
    442   def _filtered_call(self, args, kwargs):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args)
    507     # Only need to override the gradient in graph mode and when we have outputs.
    508     if context.executing_eagerly() or not self.outputs:
--&gt; 509       outputs = self._inference_function.call(ctx, args)
    510     else:
    511       self._register_gradient()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args)
    295             attrs=("executor_type", executor_type,
    296                    "config_proto", config),
--&gt; 297             ctx=ctx)
    298       # Replace empty list with None
    299       outputs = outputs or None

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     64     else:
     65       message = e.message
---&gt; 66     six.raise_from(core._status_to_exception(e.code, message), None)
     67   except TypeError as e:
     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

UnknownError: Fail to find the dnn implementation.
	 [[{{node unified_lstm_8/CudnnRNN}}]]
	 [[loss_8/dense_12_loss/binary_crossentropy/weighted_loss/broadcast_weights/assert_broadcastable/is_valid_shape/else/_1/has_valid_nonscalar_shape/then/_47/has_invalid_dims/ExpandDims/_62]] [Op:__inference_keras_scratch_graph_9512]

&lt;/denchmark-code&gt;

Describe the expected behavior
It should train the model correctly. It is running on CPU.
Code to reproduce the issue
&lt;denchmark-code&gt;def create_model(vocab_size=10):
    model = tf.keras.Sequential([
        tf.keras.layers.Embedding(vocab_size, 80),
        tf.keras.layers.LSTM(64),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    return model

model = create_model(vocab_size=vocab_size)
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])


dataset1 = tf.data.Dataset.from_tensor_slices([[1, 2, 3], [4, 5,6]])
dataset2 = tf.data.Dataset.from_tensor_slices([1, 2])
dataset = tf.data.Dataset.zip((dataset1, dataset2))
model.fit(x=dataset,
          epochs=1,
          verbose=1)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Threynaud' date='2019-02-18T20:00:21Z'>
		Do you know where can we find tf.keras.layers.CuDNNLSTM in TF 2.0 ?
I did a search for my question and find your issue(related) at here.
		</comment>
		<comment id='2' author='Threynaud' date='2019-02-18T20:22:57Z'>
		&lt;denchmark-link:https://github.com/huan&gt;@huan&lt;/denchmark-link&gt;
 I believe  will be deprecated in 2.0 in favor of .
		</comment>
		<comment id='3' author='Threynaud' date='2019-02-19T03:31:42Z'>
		Yes, I'm using a UnifiedLSTM, but I still can see the following error message, which suggest me to use the CuDNNLSTM.
Should we get rid of those message about the CuDNNLSTM ?
&lt;denchmark-code&gt;W0219 03:21:45.296270 140664325056256 tf_logging.py:161] &lt;tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7fee9331e198&gt;: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.
&lt;/denchmark-code&gt;

BTW I'm using tf.keras.layers.LSTM on GPU with TF 2.0 without any problem in the following environment:

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution: 4.15.0-45-generic
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): tf-nightly-gpu-2.0-preview==2.0.0.dev20190218
Python version: 3.6.7
CUDA/cuDNN version: CUDA 10.0 CuDNN 7.4.2
GPU model and memory: GTX 1080, 8GB

		</comment>
		<comment id='4' author='Threynaud' date='2019-02-22T21:56:27Z'>
		I can't reproduce this in Colab, TF version '2.0.0-dev20190221'
		</comment>
		<comment id='5' author='Threynaud' date='2019-02-27T01:33:32Z'>
		&lt;denchmark-link:https://github.com/Threynaud&gt;@Threynaud&lt;/denchmark-link&gt;
 Could you check with recent version of tf-nightly-gpu-2.0-preview-2.0.0.dev20190226? Please let us know how it progresses. Thanks!
		</comment>
		<comment id='6' author='Threynaud' date='2019-03-10T13:54:14Z'>
		I had the same issue. It turned out that I did not have cuDNN.
After installing cuDNN v7.5.0.56 I no longer get this error.
TensorFlow version 2.0.0-dev20190310.
		</comment>
		<comment id='7' author='Threynaud' date='2019-03-18T22:40:39Z'>
		I think it was resolved. Closing due to lack of recent activity. Please open new ticket if you see similar issue. Thanks!
		</comment>
		<comment id='8' author='Threynaud' date='2019-04-05T05:34:47Z'>
		I had the same issue with huan using Tensorflow 2.0.0-gpu-alpha version. After I reinstalled TF 2.0.0-nightly-gpu-dev040404 version it is resolved.
		</comment>
		<comment id='9' author='Threynaud' date='2019-05-28T17:37:51Z'>
		Came across this and want to give my opinion that somebody may find it helpful. According to TF2.0 documentation: &lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/LSTM&gt;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/LSTM&lt;/denchmark-link&gt;
, TF automatically selects which layer implementation to use, based on the available environment. If there are GPUs available and you want to use cuDNN implementation (aka "CuDNNLSTM" in v1), just make sure your LSTM layers meet the following requirements:
&lt;denchmark-code&gt;1. activation == 'tanh'
2. recurrent_activation == 'sigmoid'
3. recurrent_dropout == 0
4. unroll is False
5. use_bias is True
6. No use of masking.
&lt;/denchmark-code&gt;

This worked in my case. I've been using tf.keras.layers.LSTM from TF2.0 and it was very slow while I was using activation='relu', once I removed the activation (by default it uses tanh) the training improved a lot (from speed perspective, although in my case there wasn't any difference in performance/accuracy if using relu or tanh) .
However, you keep seeing the warning tensorflow.python.keras.layers.recurrent.UnifiedLSTM, but that shouldn't be an issue!
		</comment>
		<comment id='10' author='Threynaud' date='2019-06-05T03:06:07Z'>
		&lt;denchmark-link:https://github.com/ljakupi&gt;@ljakupi&lt;/denchmark-link&gt;
 Thanks for you tip. I changed to  and it indeed utilizes the GPU. But it seems the inner structure is different from  as my model performs worse when changed to . Changed back to  (meanwhile back to tensorflow 1.13.1), it performs as before.
Or it might be related tensorflow 2.0 alpha? Not only LSTM thing?
		</comment>
		<comment id='11' author='Threynaud' date='2019-06-05T07:11:27Z'>
		&lt;denchmark-link:https://github.com/pennz&gt;@pennz&lt;/denchmark-link&gt;
 I don't believe there is such change in the core that it impacts your end results this much! What is "performs worse" in your case, slower in training or worse in quality (accuracy or whatever metric you are using)?
Quick try: If your model in TF 1.13 performs better and you are using relu, try switching the activation to tanh and see if it has an impact in the performance in your TF 1.13 model. Since TF 2.0 LSTM uses tanh by default, it could be the case! I'm not sure about this since there can be many reasons, but try it, if you already didn't.
		</comment>
		<comment id='12' author='Threynaud' date='2019-06-09T14:33:00Z'>
		
@pennz I don't believe there is such change in the core that it impacts your end results this much! What is "performs worse" in your case, slower in training or worse in quality (accuracy or whatever metric you are using)?
Quick try: If your model in TF 1.13 performs better and you are using relu, try switching the activation to tanh and see if it has an impact in the performance in your TF 1.13 model. Since TF 2.0 LSTM uses tanh by default, it could be the case! I'm not sure about this since there can be many reasons, but try it, if you already didn't.

&lt;denchmark-link:https://github.com/ljakupi&gt;@ljakupi&lt;/denchmark-link&gt;
 Thanks for your reminder, I will check with the  and  thing, to make sure what makes the difference. The "performs worse" meant that the accuracy is worse. I will do more experiment for this.
P.S. tensorflow 2.0.0-beta0 reports different error message for LSTM, and I think model won't run in GPU, but in CPU and batchsize needs to be scaled down by 4 in my environment, and it is really slow training in CPU...while in 2.0.0-alpha0, model can run in GPU and fast. And the code is the same.
The message in TF2.0.0-beta0 is followed (which might be related to my env setup for TF 2.0 beta0).
&lt;denchmark-code&gt;019-06-10 01:38:28.009143: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21
2019-06-10 01:38:28.535708: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21
2019-06-10 01:38:29.153903: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1483] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
&lt;/denchmark-code&gt;

Update:
Oh... I forgot to mention in TF 1.13, I use the keras.layers.CuDNNLSTM, not the one in tensorflow. I will do more experiment to compare the difference of keras and tensorflow version    models.
However, I thought the activation used in  in my code, which uses TF 1.13 as backend, is tanh (refer to #&lt;denchmark-link:https://github.com/keras-team/keras/issues/8510&gt;keras-team/keras#8510&lt;/denchmark-link&gt;
). So update to TF2.0, without any network configuration changed, the network should be the same, as I indeed use  in my TF 2.0.0-alpah0 code. However, use  in  in TF 2.0.0-alpah0 will be super slow.
And the problem is the accuracy metric is worse (dropped from 91% to 89%), and I tried change different learning rate, the model in TF 2.0.0-alpah0 cannot achieve 91% accuracy, which might owe to my poor tuning ability.
So I suspect there might be some other parts have changed their default parameter, so the network is different.
And my current resolution is stay in TF 1.13.
For my beta0 issue, detail can be seen &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29584&gt;#29584&lt;/denchmark-link&gt;

Update 2:
Refer &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29584&gt;#29584&lt;/denchmark-link&gt;
, it is closed. It must be some other parts make my model not able to train in GPU.
		</comment>
		<comment id='13' author='Threynaud' date='2020-02-23T16:25:41Z'>
		
@huan I believe tf.keras.layers.CuDNNLSTM will be deprecated in 2.0 in favor of tf.keras.layers.UnifiedLSTM.

Is this still the plan?
		</comment>
		<comment id='14' author='Threynaud' date='2020-11-18T21:34:27Z'>
		I know this is closed, but are there any good options right now to create a custom LSTM Cell that will use the GPU?  i.e. keras.layers.RNN(MyLSTMCell(units))  For example, what if I want to pass some custom states (calculated on the output of the cell) from an attention mechanism in time?
If I subclass LSTMCell I can do what I want, but I am forced onto the CPU.
		</comment>
	</comments>
</bug>