<bug id='44243' author='rposts' open_date='2020-10-22T21:48:27Z' closed_time='2020-10-30T18:26:06Z'>
	<summary>Unable to change tensor_content in SavedModel loader.cc on s390x</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): v1.8.0-49081-g13b54c02e4 2.2.0
Python version: Python 3.6.9
Bazel version (if compiling from source): 2.0.0- (@non-git)
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"

Describe the current behavior
On s390x architecture (big-endian),Tensorflow Serving is unable to serve model that were saved on little-endian systems containing tensor_content.  This happens because tensor_content data contains little-endian serialized data and there is no way of determining the endiness of the tensor_content data.
Describe the expected behavior
Tensorflow Serving should be able to serve models with tensor_content on big-endian systems.
Standalone code to reproduce the issue
Running this testcase on s390x Tensorflow Serving code will cause the problem:
&lt;denchmark-code&gt;bazel --host_jvm_args="-Xms1024m" --host_jvm_args="-Xmx2048m" test -c dbg --copt=-O -c opt --copt=-g --strip never --host_javabase="@local_jdk//:jdk" --test_tag_filters=-gpu,-benchmark-test,-v1only -k --test_timeout 300,450,1200,3600 --build_tests_only --test_output=errors --output_filter= -- //tensorflow_serving/model_servers:tensorflow_model_server_test
&lt;/denchmark-code&gt;


Here is another related &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/41652&gt;issue&lt;/denchmark-link&gt;
.
As part of understanding the flow, I have been trying to prototype a patch which will byte-swap  field of the tensor when &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/cc/saved_model/loader.cc#L316&gt;loader.cc&lt;/denchmark-link&gt;
 is done with loading the  from the SavedModel.
I can byte-swap tensor_content field in a function but the changes are not persisted for reasons I am unable to figure out.  I have used mutable Protobuf calls to make these changes to no avail.  Here is a rough snippet of what I thought would have worked but it did not:
&lt;denchmark-code&gt;// Swap tensor_content of Const Op Tensors in the named functions
static Status SwapTensorContent(MetaGraphDef* meta_graph_def) {
  GraphDef graph_def = *meta_graph_def-&gt;mutable_graph_def();
  auto library = graph_def.mutable_library();
  for (auto&amp; function : (*library-&gt;mutable_function())) {
    for (auto&amp; node : (*function.mutable_node_def())) {
            if (node.op() == "Const") {
                auto node_iterator = node.mutable_attr()-&gt;find("value");
                if (node_iterator != node.mutable_attr()-&gt;end()) {
                    AttrValue node_value = node_iterator-&gt;second;
                    if (node_value.has_tensor()) {
                        TensorProto* tproto = node_value.mutable_tensor();
                        auto tsize = tproto-&gt;tensor_content().size();
                        if (tsize != 0)
                        {
                                Tensor parsed(tproto-&gt;dtype());
                                bool success = parsed.FromProto(*tproto);
                                DCHECK(success);
				// Try DT_INT64 first
                                if ( tproto-&gt;dtype() == DT_INT64) {
					// swap and set the tensor_content LE/BE
                                        TF_RETURN_IF_ERROR(ByteSwapTensor(&amp;parsed));
                                        (*node.mutable_attr())["value"].mutable_tensor()-&gt;set_tensor_content(string(reinterpret_cast&lt;const char*&gt;(parsed.tensor_data().data()), parsed.tensor_data().size())); //&lt;--- this seems to work but not retained once loop exists
                                }
                        }
                    }
                }
            }
    }
  }  
  return Status::OK();
}

//loader.cc -&gt; https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/cc/saved_model/loader.cc#L316
Status LoadSavedModelInternal(const SessionOptions&amp; session_options,
                              const RunOptions&amp; run_options,
                              const string&amp; export_dir,
                              const std::unordered_set&lt;string&gt;&amp; tags,
                              SavedModelBundle* const bundle) {
  const uint64 read_start_microseconds = Env::Default()-&gt;NowMicros();
  TF_RETURN_IF_ERROR(ReadMetaGraphDefFromSavedModel(export_dir, tags,
                                                    &amp;bundle-&gt;meta_graph_def));
SwapTensorContent(&amp;meta_graph_def);
// Expected meta_graph_def to contain swapped tensor_content but no effect???
&lt;/denchmark-code&gt;

I may be missing some fundamental aspects around mutability of Nodes in a MetaGraphDef.  Would appreciate any pointers.
Thanks.
	</description>
	<comments>
		<comment id='1' author='rposts' date='2020-10-27T09:40:13Z'>
		&lt;denchmark-link:https://github.com/rposts&gt;@rposts&lt;/denchmark-link&gt;

This issue is more suitable for TensorFlow Serving repo. Please post it on Serving repo from &lt;denchmark-link:https://github.com/tensorflow/serving/issues/new&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='rposts' date='2020-10-28T18:22:25Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
 Thanks for the update.  While the issue surfaced in  the problem is in SavedModel component of Tensorflow.  As you can see above, SavedModel Loader does not check for endiness of the serialized tensor_content. So the question is more about how the protobuf entries backed up by SavedModel can be modified.  Protobuf docs seem to indicate using  methods but that is not working using the code above. It is possible that some protobuf messages are not being invoked correctly but it is not obvious which ones are those.
		</comment>
		<comment id='3' author='rposts' date='2020-10-29T03:03:16Z'>
		&lt;denchmark-link:https://github.com/rposts&gt;@rposts&lt;/denchmark-link&gt;

You are seeing the same behavior with recent TF versions like 2.3 and nightly versions? Thanks!
		</comment>
		<comment id='4' author='rposts' date='2020-10-30T18:26:06Z'>
		I think I have found a way of achieving this - closing.
		</comment>
		<comment id='5' author='rposts' date='2020-10-30T18:26:08Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44243&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44243&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>