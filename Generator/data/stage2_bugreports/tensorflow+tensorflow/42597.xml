<bug id='42597' author='yuanbopeng' open_date='2020-08-23T12:37:53Z' closed_time='2020-09-04T17:41:52Z'>
	<summary>[Bug]The file copied by TF from HDFS to local may be wrong, when HDFS file is being overwritten</summary>
	<description>
This is a issue from TaiJi AI platform in Tencent.
System information

OS Platform and Distribution : Linux version 4.14.105-1-tlinux3-0010
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.13.1（we use）and the latest version also has this problem
Python version: 3.6
C++ version: 11

Describe the current behavior
Our training sample data is generated by the spark program and stored on HDFS, an example of a training sample file: hdfs://xxxx/example/20200822/part-r-0000036.tfr.gz, and the file data is compressed by gzip.
The trigger condition of the training program is that the _SUCCESS file appears under hdfs://xxxx/example/20200822/. The training program first downloads the training samples on HDFS to the local, and then reads the local data for training. When the training program and the spark program are running at the same time, the downloaded HDFS file may be overwritten by the spark program, causing the gzip file downloaded to the local to be damaged. Once the gzip file is wrong, our tensorflow training program will always stay unzipped, and the CPU utilization rate is high.
The wrong local gzip file is composed of part of the data of the HDFS file before and after overwriting.
code:
&lt;denchmark-code&gt; auto env = tensorflow::Env::Default();
 auto st = env-&gt;CopyFile(src_file, des_file);
&lt;/denchmark-code&gt;

process pstack info：
&lt;denchmark-link:https://user-images.githubusercontent.com/70072713/90976846-30be5080-e573-11ea-9f02-dace76b15584.png&gt;&lt;/denchmark-link&gt;

top info:
&lt;denchmark-link:https://user-images.githubusercontent.com/70072713/90976850-3fa50300-e573-11ea-90bf-56ccf06172c7.png&gt;&lt;/denchmark-link&gt;

Describe the expected behavior
The local gzip file is consistent with the data of the HDFS file before overwriting, or the data of the HDFS file after overwriting, instead of containing the data of the HDFS file before and after overwriting

In order to solve the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/5438&gt;issue:5438&lt;/denchmark-link&gt;
 that the tensorboard needs to get the latest data written, the HDFS file is reopened in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L226&gt;the HDFSRandomAccessFile Read&lt;/denchmark-link&gt;
: when n&gt;0 r=0, call hdfsOpenFile to reopen the HDFS file. Please refer to this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/e6e8d8715552d8890c0dd10f49ec3dff931a9926&gt;commit&lt;/denchmark-link&gt;
 for details.
Before calling hdfsOpenFile, if the HDFS file is overwritten, a new HDFS file is generated.
After calling hdfsOpenFile, it will point to the new HDFS file. If the size of the new HDFS file is larger than the size of the old HDFS file, the HDFS file copied to the local file system by FileSystemCopyFile contains part of the data of the new and old HDFS files, causing the local gzip file to be wrong
: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/42598&gt;patch-1&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/env.cc#L466&gt;FileSystemCopyFile&lt;/denchmark-link&gt;
 avoids triggering the hdfsOpenFile operation of the HDFSRandomAccessFile Read. The size of the file copy is based on the file size, not based on kCopyFileBufferSize. The implementation principle of the temporary solution is the same as that of &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/env.cc#L423&gt;ReadFileToString&lt;/denchmark-link&gt;
, but it is still possible that the file copied to the local file is wrong. Because  and  cannot form an atomic operation. For example, when the file size is obtained through GetFileSize, the HDFS file is overwritten, and the data of the new file is read based on the size of the old file. However, the possibility that the local file of the temporary solution is wrong is far less than the original solution. Generally speaking, reading the file data to the end of the file is a time-consuming operation, and the time-consuming operation of obtaining the file size is negligible
: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/42599&gt;patch-2&lt;/denchmark-link&gt;

I'm not sure if this solution is a better solution. In some scenarios I don't know, it may require further discussion. RandomAccessFile READ is an abstraction of the operations supported by each file system, and the specific implementation is transparent to users. Adding the  to the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L226&gt;HDFSRandomAccessFile READ&lt;/denchmark-link&gt;
 to read the latest data is a hidden and dangerous behavior. Because hdfsOpenFile may point to new files, data inconsistencies may occur. More fatally, there are a large number of methods that depend on the READ, which may cause some behaviors that are not what we expect, which is the root of all errors. I think it is better for users to use READ and REOPEN to obtain the latest data in the program.
: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/42860&gt;patch-3&lt;/denchmark-link&gt;

Quoting mihaimaruseac's comment:
&lt;denchmark-code&gt;Patch-1 has the issue of breaking separation of concern design principles (what happens if there is a new scheme for hdfs? We would have a bug in there until someone remembers the additional if). 
Patch-2 has the issue of removing a test that was added for creating a bug.
&lt;/denchmark-code&gt;

In order to overcome the shortcomings of patch-1 and patch-2, a switch is added to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/42860&gt;patch-3&lt;/denchmark-link&gt;
,  the default HDFS_DISABLE_READ_EOF_RETRIED is false. &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/42860&gt;patch-3&lt;/denchmark-link&gt;
 will not remove the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system_test.cc#L202&gt;WriteWhileReading&lt;/denchmark-link&gt;
 test case, and it can also solve the problem we encountered. If you need to turn off the HDFS_READ_EOF_RETRIED, set the environment variable:
&lt;denchmark-code&gt;source HDFS_DISABLE_READ_EOF_RETRIED=1
&lt;/denchmark-code&gt;

For more details, please refer to the comments below.
	</description>
	<comments>
		<comment id='1' author='yuanbopeng' date='2020-08-24T16:33:45Z'>
		I think this is an issue that can be solved by &lt;denchmark-link:https://github.com/tensorflow/community/pull/245&gt;the transaction support introduced by the new RFC&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='yuanbopeng' date='2020-08-25T02:24:05Z'>
		&lt;denchmark-link:https://github.com/mihaimaruseac&gt;@mihaimaruseac&lt;/denchmark-link&gt;
 Thanks for the first  comment. I think this issue may have nothing to do with the transaction. Because adding  to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L226&gt;READ&lt;/denchmark-link&gt;
 may have destroyed the semantics of HDFS  and easily leads to program execution errors. Especially,  is &lt;denchmark-link:https://github.com/tensorflow/community/blob/9c248b5603256c0641a05dd202adb2942c8af500/rfcs/20190506-filesystem-plugin-modular-tensorflow.md#low-level-filesystem-api&gt;the low level filesystem API&lt;/denchmark-link&gt;
, users will not know that this will be different from HDFS READ.
The reopen operation is added to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L226&gt;HDFSRandomAccessFile READ&lt;/denchmark-link&gt;
  to solve the problem of reading the latest data in the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/5438&gt;issue:5438&lt;/denchmark-link&gt;
. I think that the reopen operation should not be added to &lt;denchmark-link:https://github.com/tensorflow/community/blob/9c248b5603256c0641a05dd202adb2942c8af500/rfcs/20190506-filesystem-plugin-modular-tensorflow.md#low-level-filesystem-api&gt;the low level filesystem API&lt;/denchmark-link&gt;
, and it should be consistent with the HDFS file system. Because this has destroyed the semantics of READ provided by HDFS,  If the file is just appended, this will work very well, like adding a retry to read, and getting the latest data through a new file handle. But the file is overwritten, and the data read may be wrong.
Furthermore, I think it is more reasonable to implement HDFS's WriteWhileReading feature to read the latest data in the convenience API or the application layer utils API.
&lt;denchmark-link:https://user-images.githubusercontent.com/70072713/91115229-573bd300-e6bc-11ea-8c9f-04ed27005a9e.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='yuanbopeng' date='2020-08-26T19:00:20Z'>
		One more, my understanding about &lt;denchmark-link:https://github.com/tensorflow/community/pull/245&gt;the transaction support introduced by the new RFC&lt;/denchmark-link&gt;
 I have read.
As &lt;denchmark-link:https://github.com/tensorflow/community/pull/245#issuecomment-628380634&gt;vnno2409 commented&lt;/denchmark-link&gt;
, HDFS does not support transaction, and transaction require file system support, such as s3 or gcs. Therefore, to less intrusive, &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/42050/commits/46a8319ee74337182c7aadf80acbeb7f01eb7ffd&gt;Moving rest of the filesystems to Transactional API about HDFS&lt;/denchmark-link&gt;
 only pass the transaction parameters, and there is no transaction implementation.
Because HDFS does not support transactions, I think the reopen method is used in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L255&gt;the HDFS read method&lt;/denchmark-link&gt;
 to support simultaneous reading and writing scenarios, and data accuracy cannot be guaranteed. We should circumvent this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L226&gt;read implementation&lt;/denchmark-link&gt;
, which undermines the semantics provided by HDFS and causes the read data to be wrong.
Finally, because this question is very important to us, I look forward to &lt;denchmark-link:https://github.com/mihaimaruseac&gt;@mihaimaruseac&lt;/denchmark-link&gt;
  your comments, thanks :).
		</comment>
		<comment id='4' author='yuanbopeng' date='2020-08-27T19:00:22Z'>
		So the big issue is that the PRs remove an existing test that was added for a different issue. This is not good, we should not break existing behavior if possible.
From what I gather, you want to support the scenario where you started reading from a file but it got overwritten. This is a classic TOCTOU scenario, not even POSIX handles it by itself:
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;unistd.h&gt;

int main() {
	FILE *f = fopen("file", "r");
	char buffer[10];
	fgets(buffer, 10, f);
	printf("Got \"%s\"\n", buffer);
	sleep(10);
	fgets(buffer, 10, f);
	printf("Got \"%s\"\n", buffer);
	return 0;
}
[mm] λ gcc -Wall -Wextra test.c -o ./test
[mm] λ echo "asdf" &gt; file
[mm] λ ./test 
Got "asdf
"
Got "asdf
"
vs with overwriting the file during the 10s sleep:
[mm] λ ./test 
Got "asdf
"
Got "fghijklmn"
vs appending to the file
[mm] λ ./test 
Got "asdf
"
Got "abcdefghi"
I think that if we want to handle these scenarios we need to add transaction support to the filesystem implementation. Even if HDFS itself does not support transactions, we can add this layer as a middleware on top of it.
		</comment>
		<comment id='5' author='yuanbopeng' date='2020-08-28T17:46:25Z'>
		&lt;denchmark-link:https://github.com/mihaimaruseac&gt;@mihaimaruseac&lt;/denchmark-link&gt;

Ok. I agree with you that to solve this problem and delete the test may cause other issues. Therefore, &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/42598&gt;patch-1&lt;/denchmark-link&gt;
 may be better. According to your suggestionIn, in the HDFS scenario, &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/env.cc#L466&gt;FileSystemCopyFile&lt;/denchmark-link&gt;
 adapted to avoid calling &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L255&gt;HDFSRandomAccessFile READ reopen&lt;/denchmark-link&gt;
.
In addition, I made some modifications and tests based on your POSIX test example, and I am not sure whether my conclusion is the same as yours. In POSIX, the appended data can be read, but the overwritten data cannot be read; but in HDFS read with reopen, both the appended data and the overwritten data can be read. The read data is a mixture before and after the overwrite file.
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;unistd.h&gt;

int main() {
	FILE *f = fopen("file", "r");
	char buffer[10];
	char *first = fgets(buffer, 10, f);
        printf("Got \"%s\"\n", first);
	sleep(20);
	char *second = fgets(buffer, 10, f);
        printf("Got \"%s\"\n", second);
	return 0;
}
&lt;denchmark-code&gt;$ echo "asdf" &gt; file
$ ./test_read
Got "asdf
"
Got "(null)"
&lt;/denchmark-code&gt;

vs with overwriting the file during the 20s sleep:
&lt;denchmark-code&gt;./test_read
Got "aaa
"
Got "(null)"
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;$ echo "aaa" &gt; file
--------during the 20s sleep--------
$ echo "bbb" &gt; file
$ cat file
bbb
&lt;/denchmark-code&gt;

vs appending to the file during the 20s sleep:
&lt;denchmark-code&gt;./test_read
Got "aaa
"
Got "bbb
"
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;$ echo "aaa" &gt; file
--------during the 20s sleep--------
$ echo "bbb" &gt;&gt; file
$ cat file
aaa
bbb
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='yuanbopeng' date='2020-08-31T03:38:55Z'>
		&lt;denchmark-link:https://github.com/mihaimaruseac&gt;@mihaimaruseac&lt;/denchmark-link&gt;

Since I did not receive your comment, it may be because my previous reply was not clear enough. Let me add one more thing.
I think &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/42598&gt;patch-1&lt;/denchmark-link&gt;
 can solve the problem of copying the file being overwritten, . Because the bug of reading the overwritten file is caused by calling  &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L255&gt;reopen&lt;/denchmark-link&gt;
. The size of the copied HDFS file is determined. You can avoid calling  &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L255&gt;reopen&lt;/denchmark-link&gt;
 by passing the size, and solve the bug of reading the overwritten file. In addition, I think &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/42598&gt;patch-1&lt;/denchmark-link&gt;
 will not add additional issues, because &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/42598&gt;patch-1&lt;/denchmark-link&gt;
 only restricts the copy scenario from reading appended data. The copied file is similar to a snapshot file and should not be modified. So if you want to read additional data in real time, it is strange to use the copy method.
The problem I want to solve is to avoid reading data from the overwritten file when copying the HDFS file. I think the biggest problem I need to emphasize is that . The original &lt;denchmark-link:https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/LibHdfs.html&gt;libhdfs&lt;/denchmark-link&gt;
 read does not support reading the content of the appended file. Tensorflow uses &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L255&gt;reopen&lt;/denchmark-link&gt;
 to read the appended file, but it will cause the overwritten file to be read. If the file is being overwritten, the final read data is wrong, mixing the file data before and after overwriting. Therefore, we need to fix it urgently. For details, please refer to the previous comment.
Can you help review the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/42598&gt;patch-1&lt;/denchmark-link&gt;
? I look forward to &lt;denchmark-link:https://github.com/mihaimaruseac&gt;@mihaimaruseac&lt;/denchmark-link&gt;
 your comments, thanks :).
		</comment>
		<comment id='7' author='yuanbopeng' date='2020-08-31T17:40:15Z'>
		We have the exact same test for POSIX behavior with exactly the same results.
POSIX reads from the overwritten file, from the current cursor position.
This should be solved via transaction support, I think. Patch-1 has the issue of breaking separation of concern design principles (what happens if there is a new scheme for hdfs? We would have a bug in there until someone remembers the additional if). Patch-2 has the issue of removing a test that was added for creating a bug.
If anything, I'd prefer patch 2 to patch 1 but I'd prefer adding transactional support instead of either of the two patches.
		</comment>
		<comment id='8' author='yuanbopeng' date='2020-09-01T12:49:51Z'>
		&lt;denchmark-link:https://github.com/mihaimaruseac&gt;@mihaimaruseac&lt;/denchmark-link&gt;

I want to confirm that the  here means: to migrate data from HDFS to the transaction file system, and then read and write data from the transaction file system to avoid the problem of overwritten files?
If not, maybe I misunderstood what you mean. Can you show me specific HDFS examples?  If it is, it may not be suitable for solving our issue, because all our training data is stored on HDFS, it is almost impossible for us to switch other transaction file systems. And our data volume is very huge, at least petabytes or more, storing in HDFS is a better choice.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L260&gt;Reading EOF&lt;/denchmark-link&gt;
 will try to open the file again. This is a serious bug in our scenario. For details, please refer to the previous comment.
How about &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/42860&gt;patch-3&lt;/denchmark-link&gt;
?
In order to overcome the shortcomings of patch-1 and patch-2, a switch is added to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/42860&gt;patch-3&lt;/denchmark-link&gt;
,  the default DISABLE_HDFS_READ_EOF_RETRIED is false. &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/42860&gt;patch-3&lt;/denchmark-link&gt;
 will not remove the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system_test.cc#L202&gt;WriteWhileReading&lt;/denchmark-link&gt;
 test case, and it can also solve the problem we encountered. If you need to turn off the HDFS_READ_EOF_RETRIED, set the environment variable:
&lt;denchmark-code&gt;source DISABLE_HDFS_READ_EOF_RETRIED=1
&lt;/denchmark-code&gt;

Can you help review &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/42860&gt;patch-3&lt;/denchmark-link&gt;
? I look forward to &lt;denchmark-link:https://github.com/mihaimaruseac&gt;@mihaimaruseac&lt;/denchmark-link&gt;
 your comments, thanks :).
		</comment>
		<comment id='9' author='yuanbopeng' date='2020-09-02T02:14:39Z'>
		&lt;denchmark-link:https://github.com/mihaimaruseac&gt;@mihaimaruseac&lt;/denchmark-link&gt;

Update &lt;denchmark-link:&gt;patch-3&lt;/denchmark-link&gt;
, refer to the naming of &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/s3/s3_file_system.cc#L482&gt;S3_DISABLE_MULTI_PART_DOWNLOAD&lt;/denchmark-link&gt;
,  was renamed to .
Can you help review &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/42860&gt;patch-3&lt;/denchmark-link&gt;
? I look forward to &lt;denchmark-link:https://github.com/mihaimaruseac&gt;@mihaimaruseac&lt;/denchmark-link&gt;
  your comments, thanks :).
		</comment>
		<comment id='10' author='yuanbopeng' date='2020-09-02T16:38:25Z'>
		
@mihaimaruseac
I want to confirm that the via transaction support  here means: to migrate data from HDFS to the transaction file system, and then read and write data from the transaction file system to avoid the problem of overwritten files?

I meant submitting a patch to convert the HDFS support to &lt;denchmark-link:https://github.com/tensorflow/community/pull/245&gt;tensorflow/community#245&lt;/denchmark-link&gt;
.

If not, maybe I misunderstood what you mean. Can you show me specific HDFS examples? If it is, it may not be suitable for solving our issue, because all our training data is stored on HDFS, it is almost impossible for us to switch other transaction file systems. And our data volume is very huge, at least petabytes or more, storing in HDFS is a better choice.

You won't need to change your data storage. All I am saying is that the best fix here is to implement transaction support to HDFS layer in TF.

Can you help review patch-3? I look forward to @mihaimaruseac your comments, thanks :).

Looking over the patch now
		</comment>
		<comment id='11' author='yuanbopeng' date='2020-09-02T17:04:16Z'>
		
You won't need to change your data storage. All I am saying is that the best fix here is to implement transaction support to HDFS layer in TF.

&lt;denchmark-link:https://github.com/mihaimaruseac&gt;@mihaimaruseac&lt;/denchmark-link&gt;
 Thank you so much for your reply, I figured out your suggestion. Our disagreement may be that I think that HDFS itself has no transactions, so it is impossible to implement transactions on Tensorflow side. I am not sure if you have more information to prove that it is possible to implement transactions on the Tensorflow side. If so, can you share it?
I look forward to &lt;denchmark-link:https://github.com/mihaimaruseac&gt;@mihaimaruseac&lt;/denchmark-link&gt;
 your comments, thanks :).
		</comment>
		<comment id='12' author='yuanbopeng' date='2020-09-02T17:08:26Z'>
		Basically, what you are doing with the retries is a component of transactions. TF could use the transaction token to know when to do the transaction retry, in all operations. A transaction is nothing more than a set of files which either all change at the same time or not.
In the end, it's the same as how TCP protocol is implemented over the IP protocol. TCP is connection oriented, requires handshakes, checksums, has a lot of flow control. IP has none of these, is just best-effort. The TCP implementation contains all the code needed to implement the additional layers. Contrast with UDP which just encapsulates the data and throws it over to the IP layer.
		</comment>
		<comment id='13' author='yuanbopeng' date='2020-09-02T17:12:28Z'>
		
Basically, what you are doing with the retries is a component of transactions. TF could use the transaction token to know when to do the transaction retry, in all operations. A transaction is nothing more than a set of files which either all change at the same time or not.

&lt;denchmark-link:https://github.com/mihaimaruseac&gt;@mihaimaruseac&lt;/denchmark-link&gt;

I have a doubt whether this implementation requires a third-party stateful central service to keep and check the transaction token, or implement it in HDFS NameNode. In addition, the HDFS protocol may also need to support restricting other client processes to read and write files in transactions, etc.

In the end, it's the same as how TCP protocol is implemented over the IP protocol. TCP is connection oriented, requires handshakes, checksums, has a lot of flow control. IP has none of these, is just best-effort. The TCP implementation contains all the code needed to implement the additional layers. Contrast with UDP which just encapsulates the data and throws it over to the IP layer.

As far as I know, the TCP protocol requires the client and server to implement the TCP protocol. If Tensorflow implements HDFS transaction, I guess Tensorflow is equivalent to the client, but does not sure what the server is?
I look forward to &lt;denchmark-link:https://github.com/mihaimaruseac&gt;@mihaimaruseac&lt;/denchmark-link&gt;
 your comments, thanks :).
		</comment>
		<comment id='14' author='yuanbopeng' date='2020-09-04T17:41:52Z'>
		This has been solved by the third patch
		</comment>
		<comment id='15' author='yuanbopeng' date='2020-09-04T17:41:54Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42597&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42597&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>