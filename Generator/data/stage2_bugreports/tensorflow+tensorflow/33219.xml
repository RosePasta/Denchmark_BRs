<bug id='33219' author='mikeymezher' open_date='2019-10-10T19:14:25Z' closed_time='2019-10-14T16:58:26Z'>
	<summary>regularization parameter being ignored in tf.keras dense layer initialization</summary>
	<description>
An example of this issue can easily be seen on the TF 2.0 colab advanced quickstart tutorial: &lt;denchmark-link:https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/advanced.ipynb#scrollTo=i-2pkctU_Ci7&gt;https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/advanced.ipynb#scrollTo=i-2pkctU_Ci7&lt;/denchmark-link&gt;

Changing the code block from
&lt;denchmark-code&gt;`class MyModel(Model):
  def __init__(self):
    super(MyModel, self).__init__()
    self.conv1 = Conv2D(32, 3, activation='relu')
    self.flatten = Flatten()
    self.d1 = Dense(128, activation='relu')
    self.d2 = Dense(10, activation='softmax')

  def call(self, x):
    x = self.conv1(x)
    x = self.flatten(x)
    x = self.d1(x)
    return self.d2(x)
model = MyModel()`
&lt;/denchmark-code&gt;

To
&lt;denchmark-code&gt;`class MyModel(Model):
  def __init__(self):
    super(MyModel, self).__init__()
    self.conv1 = Conv2D(32, 3, activation='relu',activity_regularizer=tf.keras.regularizers.l2(100000.))
    self.flatten = Flatten()
    self.d1 = Dense(128, activation='relu',activity_regularizer=tf.keras.regularizers.l2(100000.))
    self.d2 = Dense(10, activation='softmax',activity_regularizer=tf.keras.regularizers.l2(100000.))
  def call(self, x):
    x = self.conv1(x)
    x = self.flatten(x)
    x = self.d1(x)
    return self.d2(x)
model = MyModel()`
&lt;/denchmark-code&gt;

has no effect, despite the regularization parameter being inordinately large (we should expect learning to completely stop in this case, as the penalization should force outputs to 0).
This is also true for kernel_regularizer.
Manually adding tf.keras.layers.ActivityRegularization also has no effect.
	</description>
	<comments>
		<comment id='1' author='mikeymezher' date='2019-10-11T03:36:37Z'>
		As explained no error is faced after code is replaced, Thanks!
		</comment>
		<comment id='2' author='mikeymezher' date='2019-10-13T16:30:48Z'>
		The code in that file does not collect the regularisation losses.
I think that you need to write:
loss += tf.add_n(model.losses)
in the train_step function.
		</comment>
		<comment id='3' author='mikeymezher' date='2019-10-14T15:58:45Z'>
		&lt;denchmark-link:https://github.com/mikeymezher&gt;@mikeymezher&lt;/denchmark-link&gt;
 Let us know whether &lt;denchmark-link:https://github.com/georgesterpu&gt;@georgesterpu&lt;/denchmark-link&gt;
 suggestion helped you in resolving the issue? Thanks!
		</comment>
		<comment id='4' author='mikeymezher' date='2019-10-14T16:58:25Z'>
		Thanks &lt;denchmark-link:https://github.com/georgesterpu&gt;@georgesterpu&lt;/denchmark-link&gt;
, that recommendation worked. In retrospect, I understand why this step is needed, since regularization wouldn't effect the predict method of the model so loss due to regularization has to be added separately; but I couldn't find any documentation for this.
Might be worth mentioning/providing an example of this somewhere in the tf.keras.regularizers api-docs.
		</comment>
		<comment id='5' author='mikeymezher' date='2019-10-14T16:58:27Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33219&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33219&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>