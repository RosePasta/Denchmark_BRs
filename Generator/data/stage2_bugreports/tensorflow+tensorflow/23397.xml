<bug id='23397' author='jonasrauber' open_date='2018-10-31T07:44:09Z' closed_time='2018-11-07T20:59:06Z'>
	<summary>parallel_for: No converter defined for MaxPoolGradGrad</summary>
	<description>
I have written a fwd_gradients-based implementation of batch_jacobian (comparable to the tf.gradients-based tensorflow.python.ops.parallel_for.batch_jacobian). To efficiently implement batch_jacobian, TensorFlow added support parallel_for: unfortunately, pfor does not support the MaxPoolGradGrad op, requiring a fallback to a slow while loop. For performance reasons, it would be great to add a converter for MaxPoolGradGrad to parallel_for.
Code to reproduce the issue
The following code is a minimal example to reproduce the issue and can be run as is.
#!/usr/bin/env python3
import tensorflow as tf
from tensorflow.python.ops.parallel_for import control_flow_ops
from tensorflow.contrib.nn.python.ops.fwd_gradients import fwd_gradients
from tensorflow.python.platform import flags
import sys
import numpy as np


def main():
    sess = tf.InteractiveSession()

    print(tf.__version__)

    flags.FLAGS(sys.argv)

    x = tf.placeholder(tf.float32, (None, 2, 2, 1))
    y = tf.nn.max_pool(x, (1, 2, 2, 1), strides=(1, 1, 1, 1), padding='SAME')

    input_shape = tf.shape(x)
    batch_size = input_shape[0]

    input_size = 4
    one_hot_vectors = tf.eye(input_size)

    def loop_fn(i):
        one_hot = tf.gather(one_hot_vectors, i)
        one_hot = one_hot[tf.newaxis]
        grad_inputs = tf.tile(one_hot, (batch_size, 1))
        grad_inputs = tf.reshape(grad_inputs, input_shape)
        g = fwd_gradients([y], [x], [grad_inputs])
        assert len(g) == 1
        print(g[0])
        return g[0]

    J = control_flow_ops.pfor(loop_fn, 4)

    x_ = np.array([4, 1, 2, 3]).reshape(1, 2, 2, 1)
    y_, J_ = sess.run([y, J], feed_dict={x: x_})
    print(x_.squeeze())
    print(y_.squeeze())
    print(J_.squeeze())


if __name__ == '__main__':
    main()
Describe the current behavior
&lt;denchmark-code&gt;1.11.0
Tensor("loop_body/gradients_1/loop_body/gradients/MaxPool_grad/MaxPoolGrad_grad/MaxPoolGradGrad:0", shape=(?, 2, 2, 1), dtype=float32)
Traceback (most recent call last):
  File "./minimal_example.py", line 46, in &lt;module&gt;
    main()
  File "./minimal_example.py", line 36, in main
    J = control_flow_ops.pfor(loop_fn, 4)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py", line 129, in pfor
    outputs.append(converter.convert(loop_fn_output))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/parallel_for/pfor.py", line 1077, in convert
    output = self._convert_helper(y)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/parallel_for/pfor.py", line 1223, in _convert_helper
    "which may run slower" % (y_op.type, y_op, converted_inputs))
ValueError: No converter defined for MaxPoolGradGrad
name: "loop_body/gradients_1/loop_body/gradients/MaxPool_grad/MaxPoolGrad_grad/MaxPoolGradGrad"
op: "MaxPoolGradGrad"
input: "Placeholder"
input: "MaxPool"
input: "loop_body/gradients_1/grad_ys_0"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "data_format"
  value {
    s: "NHWC"
  }
}
attr {
  key: "ksize"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
attr {
  key: "padding"
  value {
    s: "SAME"
  }
}
attr {
  key: "strides"
  value {
    list {
      i: 1
      i: 1
      i: 1
      i: 1
    }
  }
}

inputs: [WrappedTensor(t=&lt;tf.Tensor 'Placeholder:0' shape=(?, 2, 2, 1) dtype=float32&gt;, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=&lt;tf.Tensor 'MaxPool:0' shape=(?, 2, 2, 1) dtype=float32&gt;, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=&lt;tf.Tensor 'loop_body/gradients_1/grad_ys_0/pfor/Identity:0' shape=(4, ?, 2, 2, 1) dtype=float32&gt;, is_stacked=True, is_sparse_stacked=False)]. 
Either add a converter or set --op_conversion_fallback_to_while_loop=True, which may run slower
&lt;/denchmark-code&gt;

Describe the expected behavior
(produced by running the above script with --op_conversion_fallback_to_while_loop)
&lt;denchmark-code&gt;1.11.0
Tensor("loop_body/gradients_1/loop_body/gradients/MaxPool_grad/MaxPoolGrad_grad/MaxPoolGradGrad:0", shape=(?, 2, 2, 1), dtype=float32)
WARNING:tensorflow:Using a while_loop for converting MaxPoolGradGrad
[[4 1]
 [2 3]]
[[4. 3.]
 [3. 3.]]
[[[1. 0.]
  [0. 0.]]

 [[0. 0.]
  [0. 0.]]

 [[0. 0.]
  [0. 0.]]

 [[0. 1.]
  [1. 1.]]]
&lt;/denchmark-code&gt;

P.S. If you are interested, I'd be happy to contribute my fwd_gradients-based batch_jacobian to TensorFlow once this performance issue with MaxPool ops is fixed and I've cleaned up my code.
	</description>
	<comments>
		<comment id='1' author='jonasrauber' date='2018-11-05T16:27:55Z'>
		&lt;denchmark-link:https://github.com/agarwal-ashish&gt;@agarwal-ashish&lt;/denchmark-link&gt;
 , can you take a look?
		</comment>
		<comment id='2' author='jonasrauber' date='2018-11-07T20:59:05Z'>
		A converter for MaxPoolGradGrad has been added and should make its way to a nightly build soon.
However I suspect there might be a bunch of other ops that would turn up as not having converters given that you are relying on second order gradients.
		</comment>
		<comment id='3' author='jonasrauber' date='2018-11-11T20:44:48Z'>
		Thanks &lt;denchmark-link:https://github.com/agarwal-ashish&gt;@agarwal-ashish&lt;/denchmark-link&gt;
 for fixing this so fast! Works :)
		</comment>
	</comments>
</bug>