<bug id='23376' author='eryshev' open_date='2018-10-30T13:34:17Z' closed_time='2018-11-10T11:21:01Z'>
	<summary>ParameterServerStrategy throws an exception when training locally with multiple GPUS</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos-release-7-4.1708.el7.centos.x86_64
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): compiled form source with flags

&lt;denchmark-code&gt;export TF_NEED_HDFS=1
export TF_NEED_KAFKA=1
export TF_ENABLE_XLA=1
export TF_NEED_CUDA=1
export TF_CUDA_COMPUTE_CAPABILITIES=5.2,7.0
export TF_CUDA_VERSION=9.2
export TF_CUDNN_VERSION=7
export TF_NCCL_VERSION=1.3

bazel build \
            --config=opt \
            --config=cuda \
            --copt=-msse4.2 \
            --copt=-mavx \
            --copt=-mavx2 \
            --copt=-mfma \
            --copt=-O3 \
            //tensorflow/tools/pip_package:build_pip_package
&lt;/denchmark-code&gt;


TensorFlow version (use command below): b'v1.11.0-0-gc19e293' 1.11.0
Python version: Python 3.6.6
Bazel version (if compiling from source): 0.18.0
GCC/Compiler version (if compiling from source): 4.8.5 20150623
CUDA/cuDNN version: 9.2, 7
GPU model and memory: Tesla V100-PCIE 16GB and Tesla V100-PCIE 16GB


I'm trying to run an official resnet model on cifar 10 dataset from &lt;denchmark-link:https://github.com/tensorflow/models/tree/master/official/resnet&gt;https://github.com/tensorflow/models/tree/master/official/resnet&lt;/denchmark-link&gt;

where I replaced MirroredStrategy by ParameterServerStrategy in &lt;denchmark-link:https://github.com/tensorflow/models/blob/master/official/utils/misc/distribution_utils.py#L48&gt;https://github.com/tensorflow/models/blob/master/official/utils/misc/distribution_utils.py#L48&lt;/denchmark-link&gt;

as following
&lt;denchmark-code&gt;# return tf.contrib.distribute.MirroredStrategy(num_gpus=num_gpus)
     return tf.contrib.distribute.ParameterServerStrategy(num_gpus)
&lt;/denchmark-code&gt;

It throws an exception
&lt;denchmark-code&gt;I1030 10:21:32.401904 139688013772544 tf_logging.py:115] Done calling model_fn.
Traceback (most recent call last):
  File "./models/official/resnet/cifar10_main.py", line 285, in &lt;module&gt;
    absl_app.run(main)
  File "/home/a.eryshev/miniconda3/envs/py36/lib/python3.6/site-packages/absl/app.py", line 300, in run
    _run_main(main, args)
  File "/home/a.eryshev/miniconda3/envs/py36/lib/python3.6/site-packages/absl/app.py", line 251, in _run_main
    sys.exit(main(argv))
  File "./models/official/resnet/cifar10_main.py", line 278, in main
    run_cifar(flags.FLAGS)
  File "./models/official/resnet/cifar10_main.py", line 273, in run_cifar
    shape=[_HEIGHT, _WIDTH, _NUM_CHANNELS])
  File "/home/a.eryshev/dev/tf-experiments/models/official/resnet/resnet_run_loop.py", line 565, in resnet_main
    hooks=train_hooks, max_steps=flags_obj.max_train_steps)
  File "/home/a.eryshev/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py", line 356, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File "/home/a.eryshev/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py", line 1179, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File "/home/a.eryshev/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py", line 1295, in _train_model_distributed
    destinations='/device:CPU:0'))[0]
  File "/home/a.eryshev/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/distribute.py", line 748, in reduce
    return self._reduce(aggregation, value, destinations)
  File "/home/a.eryshev/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/parameter_server_strategy.py", line 305, in _reduce
    self._verify_destinations_not_different_worker(destinations)
  File "/home/a.eryshev/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/parameter_server_strategy.py", line 302, in _verify_destinations_not_different_worker
    (d, self._worker_device))
AttributeError: 'ParameterServerStrategy' object has no attribute '_worker_device'
&lt;/denchmark-code&gt;

Describe the expected behavior
Replacing MirroredStrategy by ParameterServerStrategy launches a training with CPU acting as PS and 2 GPU workers.
Code to reproduce the issue
Run command:
&lt;denchmark-code&gt;python ./models/official/resnet/cifar10_main.py --data_dir=/data --model_dir=/data --benchmark_logger_type=BaseBenchmarkLogger --resnet_version=2 --clean --log_step_count_steps=10 --save_summary_steps=10 --epochs_between_evals=1820 --train_epochs=1820 --batch_size=512 --num_gpus=2
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='eryshev' date='2018-11-05T08:43:35Z'>
		Looks like there's an open PR that fix the problem &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/22713&gt;#22713&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='eryshev' date='2018-11-05T18:18:10Z'>
		Does &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/66dd1e21e7ab6e2aed8413880a7f2dd7f0a20e50&gt;66dd1e2&lt;/denchmark-link&gt;
 fix your issue?
		</comment>
		<comment id='3' author='eryshev' date='2018-11-10T11:21:01Z'>
		Yes, thank you.
		</comment>
	</comments>
</bug>