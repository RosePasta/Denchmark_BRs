<bug id='36250' author='gm-spacagna' open_date='2020-01-27T14:31:36Z' closed_time='2020-01-30T00:34:37Z'>
	<summary>Tensorflow keras metrics cannot be used straight into the keras compile method</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu
TensorFlow installed from (source or binary): using pip
TensorFlow version (use command below):  2.1.0
Keras version: 2.3.1
Python version: 3.6.4

tensorflow.version.GIT_VERSION, tensorflow.version.VERSION
('v2.1.0-rc2-17-ge5bf8de', '2.1.0')
Describe the current behavior
I found an anomalous behavior when specifying tensorflow.keras.metrics directly into the Keras compile API:
&lt;denchmark-code&gt;from tensorflow.keras.metrics import Recall, Precision
model.compile(..., metrics=[Recall(), Precision()]
&lt;/denchmark-code&gt;

When looking at the history track the precision and recall plots at each epoch (using keras.callbacks.History) I observe very similar performances to both the training set and the validation set.
The weirdest thing is that both Recall and Precision increase at each epoch while the loss is clearly not improving anymore.
I found the issue to be related to the statefulness of the Tensorflow metrics objects.
Everytime you call the metric object it will append a new batch of data that get mixed with both training and validation data and cumulates at each epoch.
Describe the expected behavior
The expected behavior is that the metrics object should be stateless and do not depend on previous calls. Each time we calculate the metric (precision, recall or anything else), the function should only depend on the specified y_true and y_pred.
To workaround the issue we need to have either have Keras to be smart enough to re-instantiate the metric object at every call or to provide a tensorflow wrapper that is stateless. Maybe a decorator?
Code to reproduce the issue
&lt;denchmark-code&gt;recall=Recall()

y_train =[[0, 1, 0, 1],
         [1,0,0,0]]

y_train_pred=[[0.1,0.50001,0.4,0.7],
       [0.5,0.51,1,0]]

y_test =[[1, 1, 0, 0],
         [0,0,0,1]]

y_test_pred=[[0.1,0.80,0.8,0.9],
            [0.1,0.4,0.99,0]]


print(recall(y_train, y_train_pred))
print(recall(y_test, y_test_pred))
recall=Recall()
print(recall(y_test, y_test_pred))

recall=Recall()
print(recall(y_test, y_test_pred))
print(recall(y_train, y_train_pred))
&lt;/denchmark-code&gt;

Other info / logs
The code above will print:
&lt;denchmark-code&gt;tf.Tensor(0.6666667, shape=(), dtype=float32)
tf.Tensor(0.5, shape=(), dtype=float32)
tf.Tensor(0.33333334, shape=(), dtype=float32)
tf.Tensor(0.33333334, shape=(), dtype=float32)
tf.Tensor(0.5, shape=(), dtype=float32)
&lt;/denchmark-code&gt;

As you can see the behavior is not stateless but is the concatenation of all of the apply calls since the object instantiation.
	</description>
	<comments>
		<comment id='1' author='gm-spacagna' date='2020-01-28T09:13:36Z'>
		I have even tried wrapping the tensorflow metric instances in a sort of decorator:
&lt;denchmark-code&gt;def tf_metric_stateless(name, metric_factory, *args, **kwargs):
    
    def metric_stateless(y_true, y_pred):
        metric = metric_factory(*args, **kwargs)
        return metric(y_true, y_pred)
    
    metric_stateless.__name__ = name
    return metric_stateless

model.compile(optimizer=optimizer, 
                           loss='binary_crossentropy', 
                           metrics=['accuracy',
                                    'binary_accuracy',
                                    tf_metric_stateless('tf_binary_accuracy', BinaryAccuracy),
                                    tf_metric_stateless('tf_precision', Precision),
                                    tf_metric_stateless('tf_recall', Recall),
                                    tf_metric_stateless('tf_precision_at_recall', PrecisionAtRecall, 0.9),
                                    tf_metric_stateless('tf_auc', AUC, multi_label=False)
                                   ])
&lt;/denchmark-code&gt;

The wrapped metrics instances work fine in eager mode in fact I can now get reproducible results when I calculate the recall in sequence on the toy data.
Nevertheless, when I collect the metrics calculated at each epoch via the History callback in Keras, the look like in the original case (without the wrapper). I believe it has something to do with the different execution modes. When the metric is compiled in the tensorflow graph, it becomes a singleton even if it is re-instantiated everytime from the python code. Am I wrong or missing something?
		</comment>
		<comment id='2' author='gm-spacagna' date='2020-01-28T09:42:07Z'>
		I have tried to train the model by proving random validation labels (y_val) in order to force a visible gap between training and validation data.
The metrics calculated natively in keras makes sense (loss and accuracy):
&lt;denchmark-link:https://user-images.githubusercontent.com/1311749/73252243-baaf6200-41ba-11ea-8d9c-ad2d8148e251.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/1311749/73252245-baaf6200-41ba-11ea-902d-729e903d2fb4.png&gt;&lt;/denchmark-link&gt;

But the tensorflow metrics looks fishy.
&lt;denchmark-link:https://user-images.githubusercontent.com/1311749/73252240-ba16cb80-41ba-11ea-9abc-be8866436481.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/1311749/73252241-baaf6200-41ba-11ea-90ac-9b8c66655001.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/1311749/73252242-baaf6200-41ba-11ea-8ea4-2c47e3c1ffe0.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='gm-spacagna' date='2020-01-28T13:30:13Z'>
		Was able to reproduce the issue. Please find the Gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/amahendrakar/40d8b36766062b0c2a9101af93ae8464/36250.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='4' author='gm-spacagna' date='2020-01-30T00:34:30Z'>
		&lt;denchmark-link:https://github.com/gm-spacagna&gt;@gm-spacagna&lt;/denchmark-link&gt;
 Thank you for the issue.
For some of the metrics such as MSE we have stateful and stateless versions:
stateful listed as classes here: &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/metrics&gt;https://www.tensorflow.org/api_docs/python/tf/keras/metrics&lt;/denchmark-link&gt;

stateless listed as functions: &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/metrics#functions&gt;https://www.tensorflow.org/api_docs/python/tf/keras/metrics#functions&lt;/denchmark-link&gt;

Usage with compile/fit API are always stateful. If you want to get batchwise values, you can write custom training loop using the train_on_batch API.
For metrics such as Precision/Recall there isn't really a stateless version. For standalone usage of these metrics, please use reset_state API for clearing the state between batches.
		</comment>
		<comment id='5' author='gm-spacagna' date='2020-01-30T00:34:39Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36250&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36250&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='gm-spacagna' date='2020-02-03T11:46:21Z'>
		&lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;
 your explanations are correct but there problem I think is elsewhere.
I see two issues:

It is hard to get aggregated metrics on the whole dataset instead of batchwise
It is hard to isolate the metrics on training set and validation set

You can reset the state between batches but i guess it won't help on finding metric on the whole validation data separately from the training data.
		</comment>
		<comment id='7' author='gm-spacagna' date='2020-02-03T18:17:04Z'>
		
It is hard to get aggregated metrics on the whole dataset instead of batchwise

With the stateful metrics you get the aggregated results across the entire dataset and not batchwise.

It is hard to isolate the metrics on training set and validation set

Can you call evaluate separately for this use case?
		</comment>
	</comments>
</bug>