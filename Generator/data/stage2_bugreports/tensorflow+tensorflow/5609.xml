<bug id='5609' author='sonalgupta' open_date='2016-11-15T02:20:04Z' closed_time='2017-06-16T18:16:57Z'>
	<summary>Add documentation on how to use bucketing functions</summary>
	<description>
Can someone please add documentation on how to use tensorflow.contrib.training.bucket and tensorflow.contrib.training.bucket_by_sequence_length functions?
NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.
For general support from the community, see &lt;denchmark-link:https://stackoverflow.com/questions/tagged/tensorflow&gt;StackOverflow&lt;/denchmark-link&gt;
.
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.
For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
&lt;denchmark-h:h3&gt;What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System: Mac OS X Sierra
Installed version of CUDA and cuDNN: None
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
If installed from binary pip package, provide:

A link to the pip package you installed: Mac OS X Python 2.7 CPU
The output from python -c "import tensorflow; print(tensorflow.__version__)".: 0.11.0rc1

If installed from source, provide

The commit hash (git rev-parse HEAD)
The output of bazel version

&lt;denchmark-h:h3&gt;If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;What other attempted solutions have you tried?&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Logs or other output that would be helpful&lt;/denchmark-h&gt;

(If logs are large, please upload as attachment or provide link).
	</description>
	<comments>
		<comment id='1' author='sonalgupta' date='2016-11-15T15:55:40Z'>
		Have you read the documentation?  &lt;denchmark-link:https://www.tensorflow.org/versions/r0.11/api_docs/python/contrib.training.html#bucket_by_sequence_length&gt;https://www.tensorflow.org/versions/r0.11/api_docs/python/contrib.training.html#bucket_by_sequence_length&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='sonalgupta' date='2016-11-15T16:05:25Z'>
		Yes, I read the documentation. I am confused about how to use the outputs of the function. If I do output.eval() it hangs the program.
		</comment>
		<comment id='3' author='sonalgupta' date='2016-11-15T16:26:42Z'>
		You probably need to start queue runners: &lt;denchmark-link:https://www.tensorflow.org/versions/r0.11/how_tos/threading_and_queues/index.html&gt;https://www.tensorflow.org/versions/r0.11/how_tos/threading_and_queues/index.html&lt;/denchmark-link&gt;
.
&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 It seems tedious, but should we mention queues everywhere we create one (or at least at the top of the bucketing doc)?  It's pretty easy to not know one has to start queue runners after calling an obscure function.
		</comment>
		<comment id='4' author='sonalgupta' date='2016-11-15T17:58:45Z'>
		Also, the following code doesn't function as I would expect. I'm sure I am missing something, which is not clear from the documentation. Update: seems like the function is meant to bucket one input at a time? the documentation says The list or dictionary of tensors, representing a single element, to bucket .. I don't understand, when would I want to bucket/batch a single input? Also, for bucketing/batching multiple inputs (e.g. multiple sentences), do I call the function multiple times with different inputs?
For the following code:
&lt;denchmark-code&gt;seq_lengths = np.array([6, 3, 2])
inputs = []
inputs.append(tf.convert_to_tensor(np.array([2,3,3,3,3,3])))
inputs.append(tf.convert_to_tensor(np.array([2, 3, 4])))
inputs.append(tf.convert_to_tensor(np.array([3, 4])))

sequences, output = bucket_by_sequence_length(input_length=seq_lengths, tensors= inputs, batch_size=2, bucket_boundaries =[1, 2], allow_smaller_final_batch=True,
                                              dynamic_pad=True, capacity=2)

init_op = tf.initialize_all_variables()

sess = tf.Session()

sess.run(init_op)

coord = tf.train.Coordinator()
threads = tf.train.start_queue_runners(sess=sess, coord=coord)

try:
    while not coord.should_stop():
        s, o= sess.run([sequences, output])

except tf.errors.OutOfRangeError:
    print('Done training -- epoch limit reached')
finally:
    coord.request_stop()

coord.join(threads)
sess.close()
&lt;/denchmark-code&gt;

o is [array([[2, 3, 3, 3, 3, 3], [2, 3, 3, 3, 3, 3]]), array([[2, 3, 4], [2, 3, 4]]), array([[3, 4], [3, 4]])] however, I was expecting the tensors of lengths 3 and 6 to be in the same bucket.
		</comment>
		<comment id='5' author='sonalgupta' date='2016-11-15T19:47:48Z'>
		Looking at the docs for the bucket_by_sequence_length function:
bucket_boundaries: int list, increasing non-negative numbers. The edges
of the buckets to use when bucketing tensors. Two extra buckets are
created, one for input_length &lt; bucket_boundaries[0] and one for
input_length &gt;= bucket_boundaries[-1].
In your case if you had sequence_lengths [2, 3, 6] then you get 5 buckets:
[len &lt; 2, 2 &lt;= len &lt; 3, 3 &lt;= len &lt; 6, len &gt;= 6]
either way, you would need to set the last of the lengths to 7 in order to
get 3 and 6 in the same bucket.
On Tue, Nov 15, 2016 at 9:59 AM, Sonal Gupta &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;

wrote:

Also, the following code doesn't function as I would expect. I'm sure I am
missing something, which is not clear from the documentation.
For the following code:
`seq_lengths = np.array([6, 3, 2])
inputs = []
inputs.append(tf.convert_to_tensor(np.array([2,3,3,3,3,3])))
inputs.append(tf.convert_to_tensor(np.array([2, 3, 4])))
inputs.append(tf.convert_to_tensor(np.array([3, 4])))
sequences, output = bucket_by_sequence_length(input_length=seq_lengths,
tensors= inputs, batch_size=2, bucket_boundaries =[1, 2],
allow_smaller_final_batch=True,
dynamic_pad=True, capacity=2)
Create the graph, etc.
init_op = tf.initialize_all_variables()
Create a session for running operations in the Graph.
sess = tf.Session()
Initialize the variables (like the epoch counter).
sess.run(init_op)
Start input enqueue threads.
coord = tf.train.Coordinator()
threads = tf.train.start_queue_runners(sess=sess, coord=coord)
try:
while not coord.should_stop():
Run training steps or whatever
s, o= sess.run([sequences, output])
except tf.errors.OutOfRangeError:
print('Done training -- epoch limit reached')
finally:
When done, ask the threads to stop.
coord.request_stop()
Wait for threads to finish.
coord.join(threads)
sess.close()
`
o is [array([[2, 3, 3, 3, 3, 3],
[2, 3, 3, 3, 3, 3]]), array([[2, 3, 4],
[2, 3, 4]]), array([[3, 4],
[3, 4]])] however, I was expecting the tensors of lengths 3 and 6 to be
in the same bucket.
—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub
#5609 (comment),
or mute the thread
https://github.com/notifications/unsubscribe-auth/ABtim8hi_DovDpG7cHR4h5ba_mikNts_ks5q-fL5gaJpZM4KyDNA
.

		</comment>
		<comment id='6' author='sonalgupta' date='2016-11-15T20:02:41Z'>
		Thanks for the reply. If I give bucket_boundaries=[2,3, 7] then I get the exception ValueError: Dimensions must be equal, but are 4 and 3. Can you please explain what tensors exactly mean? Documentation says the list or dictionary of tensors, representing a single element, to bucket... How do I bucket/batch multiple sentences?
		</comment>
		<comment id='7' author='sonalgupta' date='2017-01-23T02:11:50Z'>
		I don't understand these two bucketing methods neither. Could someone write a simple instruction?
		</comment>
		<comment id='8' author='sonalgupta' date='2017-01-23T03:00:13Z'>
		Perhaps I should add an example in the docstring.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Jan 22, 2017 6:12 PM, "Kuang R" ***@***.***&gt; wrote:
 I don't understand these two bucketing methods neither. Could someone
 write a simple instruction?

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#5609 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtim5YdvZU6xc_9lHcCI8AqaCMXQ9Lpks5rVAx5gaJpZM4KyDNA&gt;
 .



		</comment>
		<comment id='9' author='sonalgupta' date='2017-01-30T14:39:57Z'>
		I have the same issue as  sonalgupta  . I pass in a list of roughly 605742 sequence lengths, and I have 24 buckets, and it errors out with:
  File "./step_train_rnnlm.py", line 66, in &lt;module&gt; main() File "./step_train_rnnlm.py", line 61, in main n_epochs=10) File "/home/malinin/dnn_lib/rnnlm.py", line 138, in fit data = self._construct_bucket_queue(trn_data_list, batch_size, capacity=20000, num_threads=8) File "/home/malinin/dnn_lib/base_model.py", line 226, in _construct_bucket_queue name=None) File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/training/python/training/bucket_ops.py", line 347, in bucket_by_sequence_length math_ops.less_equal(buckets_min, input_length), File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 1189, in less_equal result = _op_def_lib.apply_op("LessEqual", x=x, y=y, name=name) File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 749, in apply_op op_def=op_def) File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 2382, in create_op set_shapes_for_outputs(ret) File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1783, in set_shapes_for_outputs shapes = shape_func(op) File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py", line 596, in call_cpp_shape_fn raise ValueError(err.message) ValueError: Dimensions must be equal, but are 25 and 605742
I have a feeling something is not being broadcast correctly, but can't put my finger on it.
		</comment>
		<comment id='10' author='sonalgupta' date='2017-01-30T16:46:13Z'>
		It does seem that way.  When you say you pass in a list, are you saying you
are literally passing in a python list of length 605742, each entry
containing a tensor?

If so, then what you want to be doing is passing in a list with a single
tensor whose leftmost dimension is 605742.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, Jan 30, 2017 at 6:40 AM, KaosEngineer ***@***.***&gt; wrote:
 I have the same issue as sonalgupta . I pass in a list of roughly 605742
 sequence lengths, and I have 24 buckets, and it errors out with:

 File "./step_train_rnnlm.py", line 66, in &lt;module&gt; main() File
 "./step_train_rnnlm.py", line 61, in main n_epochs=10) File
 "/home/malinin/dnn_lib/rnnlm.py", line 138, in fit data =
 self._construct_bucket_queue(trn_data_list, batch_size, capacity=20000,
 num_threads=8) File "/home/malinin/dnn_lib/base_model.py", line 226, in
 _construct_bucket_queue name=None) File "/usr/local/lib/python2.7/
 dist-packages/tensorflow/contrib/training/python/training/bucket_ops.py",
 line 347, in bucket_by_sequence_length math_ops.less_equal(buckets_min,
 input_length), File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py",
 line 1189, in less_equal result = _op_def_lib.apply_op("LessEqual", x=x,
 y=y, name=name) File "/usr/local/lib/python2.7/dist-packages/tensorflow/
 python/framework/op_def_library.py", line 749, in apply_op op_def=op_def)
 File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py",
 line 2382, in create_op set_shapes_for_outputs(ret) File
 "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py",
 line 1783, in set_shapes_for_outputs shapes = shape_func(op) File
 "/usr/local/lib/python2.7/dist-packages/tensorflow/
 python/framework/common_shapes.py", line 596, in call_cpp_shape_fn raise
 ValueError(err.message) ValueError: Dimensions must be equal, but are 25
 and 605742

 I have a feeling something is not being broadcast correctly, but can't put
 my finger on it.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#5609 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtim-De_Ce5VESTPZPfofZpx2B3Nifsks5rXfZngaJpZM4KyDNA&gt;
 .



		</comment>
		<comment id='11' author='sonalgupta' date='2017-03-01T13:56:56Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 Hi Eugene,
I have just watched your session in #tfdevsummit it was really good and informative. I am trying to use bucket_by_sequence_length. But I am not able to understand the functionality as others, it will be very helpful if you give an example on some classic data sets like WMT dataset used in seq2seq.
In seq2seq tutorial the bucketing is done manually so I am expecting that will be done automatically with these bucketing functions. But I am not clear about giving the correct parameters to the function.
It would be a great help if you can give us a tutorial in tensor flow website.
Thanks,
kushwanth naga goutham
		</comment>
		<comment id='12' author='sonalgupta' date='2017-03-01T17:00:34Z'>
		We are working on a new seq2seq tutorial. I'll try to ensure this new
bucketing operation is in there as well.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mar 1, 2017 5:57 AM, "Kushwanth Naga Goutham" ***@***.***&gt; wrote:
 @ebrevdo &lt;https://github.com/ebrevdo&gt; Hi Eugene,
 I have just watched your session in #tfdevsummit it was really good and
 informative. I am trying to use bucket_by_sequence_length. But I am not
 able to understand the functionality as others, it will be very helpful if
 you have give a example on some classic data sets like WMT dataset used in
 seq2seq.
 In seq2seq tutorial the bucketing is done manually so I am expecting that
 will be done automatically with these bucketing functions. But I am not
 clear about giving the correct parameters to the function.

 It would be a great help if you can give us a tutorial in tensor flow
 website.

 Thanks,
 kushwanth naga goutham

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#5609 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtimxNcyAqu0b6QILzm733XNJe_e0xRks5rhXk8gaJpZM4KyDNA&gt;
 .



		</comment>
		<comment id='13' author='sonalgupta' date='2017-03-04T11:37:55Z'>
		I finally got bucket_by_sequence_length to work, here is what I think was hard:

It is not clear that bucket_by_sequence_length needs input_length and tensors to be elements from a queue.
It is not clear that input_length just controls the bucketing, the padding works separately.
Creating a queue that contains tensors of different sequence lengths is not trivial.

Here is the example I got to work:
import numpy as np
import tensorflow as tf


class SequenceTable:
    def __init__(self, data):
        # A TensorArray is required as the sequences don't have the same
        # length. Alternatively a FIFOQueue can be used.
        # Because the data is read more than once by the queue,
        # clear_after_read is set to False (but I can't confirm an effect).
        # Because the items has diffrent sequence lengths the infer_shape
        # is set to False. The shape is then restored in the .read method.
        self.table = tf.TensorArray(size=len(data),
                                    dtype=data[0].dtype,
                                    dynamic_size=False,
                                    clear_after_read=False,
                                    infer_shape=False)

        # initialize table
        for i, datum in enumerate(data):
            self.table = self.table.write(i, datum)

        # setup infered element shape
        self.element_shape = tf.TensorShape((None, ) + data[0].shape[1:])

    def read(self, index):
        # read index from table and set infered shape
        read = self.table.read(index)
        read.set_shape(self.element_shape)
        return read


def shuffle_bucket_batch(input_length, tensors, shuffle=True, **kwargs):
    # bucket_by_sequence_length requires the input_length and tensors
    # arguments to be queues. Use a range_input_producer queue to shuffle
    # an index for sliceing the input_length and tensors laters.
    # This strategy is idendical to the one used in slice_input_producer.
    table_index = tf.train.range_input_producer(
        int(input_length.get_shape()[0]), shuffle=shuffle
    ).dequeue()

    # the first argument is the sequence length specifed in the input_length
    # I did not find a ue for it.
    _, batch_tensors = tf.contrib.training.bucket_by_sequence_length(
        input_length=tf.gather(input_length, table_index),
        tensors=[tensor.read(table_index) for tensor in tensors],
        **kwargs
    )

    return tuple(batch_tensors)


# these values specify the length of the sequence and this controls how
# the data is bucketed. The value is not required to be the acutal length,
# which is also problematic when using pairs of sequences that have diffrent
# length. In that case just specify a value that gives the best performance,
# for example "the max length".
length_table = tf.constant([2, 4, 3, 4, 3, 7], dtype=tf.int32)

source_table = SequenceTable([
    np.asarray([3, 4], dtype=np.int32),
    np.asarray([2, 3, 4], dtype=np.int32),
    np.asarray([1, 3, 4], dtype=np.int32),
    np.asarray([5, 3, 4], dtype=np.int32),
    np.asarray([6, 3, 4], dtype=np.int32),
    np.asarray([3, 3, 3, 3, 3, 3], dtype=np.int32)
])

target_table = SequenceTable([
    np.asarray([9], dtype=np.int32),
    np.asarray([9, 3, 4, 5], dtype=np.int32),
    np.asarray([9, 3, 4], dtype=np.int32),
    np.asarray([9, 3, 4, 6], dtype=np.int32),
    np.asarray([9, 3], dtype=np.int32),
    np.asarray([9, 3, 3, 3, 3, 3, 2], dtype=np.int32)
])

source_batch, target_batch = shuffle_bucket_batch(
    length_table, [source_table, target_table],
    batch_size=2,
    # devices buckets into [len &lt; 3, 3 &lt;= len &lt; 5, 5 &lt;= len]
    bucket_boundaries=[3, 5],
    # this will bad the source_batch and target_batch independently
    dynamic_pad=True,
    capacity=2
)

with tf.Session() as sess:
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(sess, coord)

    for i in range(6):
        source, target = sess.run((source_batch, target_batch))
        print(f'source_output[{i}]')
        print(source)
        print(f'target_output[{i}]')
        print(target)
        print('')

    coord.request_stop()
    coord.join(threads)
This outputs something like:
&lt;denchmark-code&gt;source_output[0]
[[6 3 4]
 [5 3 4]]
target_output[0]
[[9 3 0 0]
 [9 3 4 6]]

source_output[1]
[[1 3 4]
 [2 3 4]]
target_output[1]
[[9 3 4 0]
 [9 3 4 5]]

source_output[2]
[[6 3 4]
 [2 3 4]]
target_output[2]
[[9 3 0 0]
 [9 3 4 5]]

source_output[3]
[[3 3 3 3 3 3]
 [3 3 3 3 3 3]]
target_output[3]
[[9 3 3 3 3 3 2]
 [9 3 3 3 3 3 2]]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='14' author='sonalgupta' date='2017-03-04T21:18:12Z'>
		A simpler solution (not requiring TensorArray) is probably to create a
queue that you feed sequences into one at a time in a separate python
thread - the "reader"; and then in the main thread you read from that queue
and pass that to bucket_by_sequence_length.  I'll try to include that in
the new tutorial.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Sat, Mar 4, 2017 at 3:38 AM, Andreas Madsen ***@***.***&gt; wrote:
 I finally got bucket_by_sequence_length it to work, here is what I think
 was had:

    - It is not clear that bucket_by_sequence_length needs input_length
    and tensors to be elements from a queue.
    - It is not clear that input_length just controls the bucketing, the
    padding works separately.
    - Creating a queue that contains tensors of different sequence lengths
    is not trivial.

 Here is the example I got to work:

 import numpy as npimport tensorflow as tf

 class SequenceTable:
     def __init__(self, data):
         # A TensorArray is required as the sequences don't have the same
         # length. Alternatively a FIFO query can be used.
         # Because the data is read more than once by the queue,
         # clear_after_read is set to False (but I can't confirm an effect).
         # Because the items has diffrent sequence lengths the infer_shape
         # is set to False. The shape is then restored in the .read method.
         self.table = tf.TensorArray(size=len(data),
                                     dtype=data[0].dtype,
                                     dynamic_size=False,
                                     clear_after_read=False,
                                     infer_shape=False)

         # initialize table
         for i, datum in enumerate(data):
             self.table = self.table.write(i, datum)

         # setup infered element shape
         self.element_shape = tf.TensorShape((None, ) + data[0].shape[1:])

     def read(self, index):
         # read index from table and set infered shape
         read = self.table.read(index)
         read.set_shape(self.element_shape)
         return read

 def shuffle_bucket_batch(input_length, tensors, shuffle=True, **kwargs):
     # bucket_by_sequence_length requires the input_length and tensors
     # arguments to be queues. Use a range_input_producer queue to shuffle
     # an index for sliceing the input_length and tensors laters.
     # This strategy is idendical to the one used in slice_input_producer.
     table_index = tf.train.range_input_producer(
         int(input_length.get_shape()[0]), shuffle=shuffle
     ).dequeue()

     # the first argument is the sequence length specifed in the input_length
     # I did not find a ue for it.
     _, batch_tensors = tf.contrib.training.bucket_by_sequence_length(
         input_length=tf.gather(input_length, table_index),
         tensors=[tensor.read(table_index) for tensor in tensors],
         **kwargs
     )

     return tuple(batch_tensors)

 # these values specify the length of the sequence and this controls how# the data is bucketed. The value is not required to be the acutal length,# which is also problematic when using pairs of sequences that have diffrent# length. In that case just specify a value that gives the best performance,# for example "the max length".
 length_table = tf.constant([2, 4, 3, 4, 3, 7], dtype=tf.int32)

 source_table = SequenceTable([
     np.asarray([3, 4], dtype=np.int32),
     np.asarray([2, 3, 4], dtype=np.int32),
     np.asarray([1, 3, 4], dtype=np.int32),
     np.asarray([5, 3, 4], dtype=np.int32),
     np.asarray([6, 3, 4], dtype=np.int32),
     np.asarray([3, 3, 3, 3, 3, 3], dtype=np.int32)
 ])

 target_table = SequenceTable([
     np.asarray([9], dtype=np.int32),
     np.asarray([9, 3, 4, 5], dtype=np.int32),
     np.asarray([9, 3, 4], dtype=np.int32),
     np.asarray([9, 3, 4, 6], dtype=np.int32),
     np.asarray([9, 3], dtype=np.int32),
     np.asarray([9, 3, 3, 3, 3, 3, 2], dtype=np.int32)
 ])

 source_batch, target_batch = shuffle_bucket_batch(
     length_table, [source_table, target_table],
     batch_size=2,
     # devices buckets into [len &lt; 3, 3 &lt;= len &lt; 5, 5 &lt;= len]
     bucket_boundaries=[3, 5],
     # this will bad the source_batch and target_batch independently
     dynamic_pad=True,
     capacity=2
 )
 with tf.Session() as sess:
     coord = tf.train.Coordinator()
     threads = tf.train.start_queue_runners(sess, coord)

     for i in range(6):
         source, target = sess.run((source_batch, target_batch))
         print(f'source_output[{i}]')
         print(source)
         print(f'target_output[{i}]')
         print(target)
         print('')

     coord.request_stop()
     coord.join(threads)

 This outputs something like:

 source_output[0]
 [[6 3 4]
  [5 3 4]]
 target_output[0]
 [[9 3 0 0]
  [9 3 4 6]]

 source_output[1]
 [[1 3 4]
  [2 3 4]]
 target_output[1]
 [[9 3 4 0]
  [9 3 4 5]]

 source_output[2]
 [[6 3 4]
  [2 3 4]]
 target_output[2]
 [[9 3 0 0]
  [9 3 4 5]]

 source_output[3]
 [[3 3 3 3 3 3]
  [3 3 3 3 3 3]]
 target_output[3]
 [[9 3 3 3 3 3 2]
  [9 3 3 3 3 3 2]]

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#5609 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtim0qiVXzQWLANQo7XSsHobK_v_6hXks5riU09gaJpZM4KyDNA&gt;
 .



		</comment>
		<comment id='15' author='sonalgupta' date='2017-03-04T21:53:04Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 Thanks. I got something like that working for just one sequence, but I couldn't see how when using a pair of sequences. This is why I used the strategy from  (slice a  using a .
PS: Another thing I'm confused about is how to get a shuffling behavior similar to shuffle_batch*.
		</comment>
		<comment id='16' author='sonalgupta' date='2017-03-05T05:43:41Z'>
		Can you say more about how you want to combine pairs of sequences in a
batching mechanism?

To get shuffling behavior similar to shuffle_batch you can use a
RandomShuffleQueue object.  Keep in mind, because you're dealing with
variable-length tensors (before minibatching) and we don't have a Padding
version of this queue, you should use the .dequeue() method to pull out one
(shuffled) entry at a time, instead of dequeue_many().  Construct the
object with the argument shapes=None.  You can use this queue between your
input reader and batch_sequences_by_length in order to shuffle the
sequences before batching them.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Sat, Mar 4, 2017 at 1:53 PM, Andreas Madsen ***@***.***&gt; wrote:
 @ebrevdo &lt;https://github.com/ebrevdo&gt; Thanks. I got something like that
 working for just one sequence, but I couldn't see how when using a pair of
 sequences. This is why I used the strategy from slice_input_producer
 (slice a Tensor using a range_input_producer.

 Another thing I'm confused about is how to get a shuffling behavior
 similar to shuffle_batch*.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#5609 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtim9keLzftloqs6ExChMFCGDqXsmAgks5rid1TgaJpZM4KyDNA&gt;
 .



		</comment>
		<comment id='17' author='sonalgupta' date='2017-03-05T09:25:36Z'>
		
Can you say more about how you want to combine pairs of sequences in a
batching mechanism?

A pair of sequences like in train.batch([source, target]), which is useful in a machine translation model.

To get shuffling behavior similar to shuffle_batch you can use a
RandomShuffleQueue object. ...

Thanks for the tip, I will give that a go.
		</comment>
		<comment id='18' author='sonalgupta' date='2017-04-04T17:55:47Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 Thanks for working on a tutorial. Any idea when would it be posted? Is there a way to subscribe to notification for new tutorials?
		</comment>
		<comment id='19' author='sonalgupta' date='2017-04-04T18:18:06Z'>
		Probably it'll be posted to the google research blog and probably announced
by the &lt;denchmark-link:https://github.com/tensorflow&gt;@tensorflow&lt;/denchmark-link&gt;
 twitter handle.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Apr 4, 2017 at 10:56 AM, Sonal Gupta ***@***.***&gt; wrote:
 @ebrevdo &lt;https://github.com/ebrevdo&gt; Thanks for working on a tutorial.
 Any idea when would it be posted? Is there a way to subscribe to
 notification for new tutorials?

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#5609 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtimyzwhUwaqh9ZgL0UrUCnI-7cxEwuks5rsoQ9gaJpZM4KyDNA&gt;
 .



		</comment>
		<comment id='20' author='sonalgupta' date='2017-04-06T10:28:02Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/AndreasMadsen&gt;@AndreasMadsen&lt;/denchmark-link&gt;

I gave it a shot to do this with a queuerunner, however I got stuck at some point, this is my current code
&lt;denchmark-code&gt;    BUCKET_BOUNDARIES = [2,4]
    MAX_LEN = 6

    def _to_sparse_tensor(self, sequences):
 
          sp_indices = []
          sp_vals = []
          sp_shape = [len(sequences), self.MAX_LEN]
 
          for i in range(0, len(sequences)):
              for j in range(0, len(sequences[i])):
                  sp_indices.append([i, j])
                  sp_vals.append(sequences[i][j])
 
          return tf.SparseTensor(indices=sp_indices, values=sp_vals, shape=sp_shape)

      def __init__(self, batch_size=32, name='x-train'):
 
          # load train corpus
          sources, targets, lengths, validation_sources, validation_targets, validation_lengths = self._load_corpus()
 
          print("Finished corpus loading")
          # to a constant tensor
          source = self._to_sparse_tensor(sources)
          target = self._to_sparse_tensor(targets)
          length = tf.convert_to_tensor(np.asarray(lengths, dtype=np.int32))
 
          input_queue = tf.train.shuffle_batch([length, source, target], batch_size,
                                              batch_size*64, # capacity
                                              batch_size*32, # min_after_dequeue
                                              num_threads=32,
                                              allow_smaller_final_batch=False, name=name)
 
          lengths_t, sources_t, targets_t = input_queue

          source_batch, target_batch = self.shuffle_bucket_batch(
              lengths_t, [sources_t, targets_t],
              batch_size=batch_size,
              bucket_boundaries=self.BUCKET_BOUNDARIES,
              # this will pad the source_batch and target_batch independently
              dynamic_pad=True,
              capacity=batch_size*64,
              num_threads=32,
              allow_smaller_final_batch=False, name=name
          )
&lt;/denchmark-code&gt;

where shuffle_bucket_batch looks like this:
&lt;denchmark-code&gt;def shuffle_bucket_batch(input_length, tensors, shuffle=True, **kwargs):
    # the first argument is the sequence length specifed in the input_length
    # I did not find a ue for it.
    _, batch_tensors = tf.contrib.training.bucket_by_sequence_length(
        input_length=input_length,
        tensors=tensors,
        **kwargs
    )
    return tuple(batch_tensors)
&lt;/denchmark-code&gt;

I've put the sources and targets into SparseArray (doing it manually, since I couldn't find a tf method similar to convert_to_tensor but for sparse data) so I can feed them into shuffle_batch queue (it can operate on SparseArrays), then I pass the outputs to the shuffle_bucket_batch.
The problem that I've hit is that I get some sort of dimension mismatch between the input_length and the amounts of buckets:
ValueError: Dimensions must be equal, but are 3 and 31978 for 'x-train_1/LessEqual' (op:'LessEqual') with input shapes: [3], [32,31978].
the error happens at line 346 in this file: &lt;denchmark-link:http://git.hiddenunit.com/hakan/tersorflow/blob/42d26aa7001dda6928771db8b4244067c7924a01/tensorflow/contrib/training/python/training/bucket_ops.py&gt;http://git.hiddenunit.com/hakan/tersorflow/blob/42d26aa7001dda6928771db8b4244067c7924a01/tensorflow/contrib/training/python/training/bucket_ops.py&lt;/denchmark-link&gt;

The main difference with &lt;denchmark-link:https://github.com/AndreasMadsen&gt;@AndreasMadsen&lt;/denchmark-link&gt;
 is that I don't use  since the input_lengths, sources and targets are batched together.
Is it reasonable to use SparseArray, shuffle_batch and bucket_by_sequence_length? Any ideas why the dimension don't fit together?
		</comment>
		<comment id='21' author='sonalgupta' date='2017-06-19T21:45:53Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/itsmeolivia&gt;@itsmeolivia&lt;/denchmark-link&gt;
 has the tutorial been posted somewhere? I can't find it...
		</comment>
		<comment id='22' author='sonalgupta' date='2017-06-26T05:23:05Z'>
		&lt;denchmark-link:https://github.com/sonalgupta&gt;@sonalgupta&lt;/denchmark-link&gt;
 I've decided to open source an Atrous CNN architecture for text classification, there I have implemented the sequence with variable length reading from a file, you can see the project &lt;denchmark-link:https://github.com/randomrandom/deep-atrous-cnn-sentiment&gt;here&lt;/denchmark-link&gt;
, the code that you need is located in the &lt;denchmark-link:https://github.com/randomrandom/deep-atrous-cnn-sentiment/blob/master/data/base_data_loader.py&gt;BaseDataLoader class&lt;/denchmark-link&gt;
. This is the actual snippet that you need:
&lt;denchmark-code&gt;def __load_batch(self, file_names, record_defaults, data_column, bucket_boundaries, field_delim=_CSV_DELIM,
                     skip_header_lines=0,
                     num_epochs=None, shuffle=True):

        original_file_names = file_names[:]
        file_names = self.__generate_preprocessed_files(file_names, data_column, bucket_boundaries,
                                                        field_delim=field_delim)

        filename_queue = tf.train.string_input_producer(
            file_names, num_epochs=num_epochs, shuffle=shuffle
        )

        self.shuffle_queue = tf.RandomShuffleQueue(capacity=self._capacity, min_after_dequeue=self._min_after_dequeue,
                                                   dtypes=[tf.int64, tf.int32], shapes=None)


        example, label = self._read_file(filename_queue, record_defaults, field_delim, skip_header_lines)

        voca_path, voca_name = BaseDataLoader._split_file_to_path_and_name(
            original_file_names[0])  # TODO: will be break with multiple filenames
        voca_name = KagglePreprocessor.VOCABULARY_PREFIX + voca_name
        self.__vocabulary_file = voca_path + voca_name

        # load look up table that maps words to ids
        self.table = tf.contrib.lookup.index_table_from_file(vocabulary_file=voca_path + voca_name,
                                                             default_value=KagglePreprocessor.UNK_TOKEN_ID,
                                                             num_oov_buckets=0)

        # convert to tensor of strings
        split_example = tf.string_split([example], " ")

        # determine lengths of sequences
        line_number = split_example.indices[:, 0]
        line_position = split_example.indices[:, 1]
        lengths = (tf.segment_max(data=line_position,
                                  segment_ids=line_number) + 1).sg_cast(dtype=tf.int32)

        # convert sparse to dense
        dense_example = tf.sparse_tensor_to_dense(split_example, default_value="")
        dense_example = self.table.lookup(dense_example)

        # get the enqueue op to pass to a coordintor to be run
        self.enqueue_op = self.shuffle_queue.enqueue([dense_example, label])
        dense_example, label = self.shuffle_queue.dequeue()

        # add queue to queue runner
        self.qr = tf.train.QueueRunner(self.shuffle_queue, [self.enqueue_op] * self.num_threads)
        tf.train.queue_runner.add_queue_runner(self.qr)

        # reshape from &lt;unknown&gt; shape into proper form after dequeue from random shuffle queue
        # this is needed so next queue can automatically infer the shape properly
        dense_example = dense_example.sg_reshape(shape=[1, -1])
        label = label.sg_reshape(shape=[1])

        _, (padded_examples, label_examples) = tf.contrib.training.bucket_by_sequence_length(lengths,
                                                                                             [dense_example, label],
                                                                                             batch_size=self._batch_size,
                                                                                             bucket_boundaries=bucket_boundaries,
                                                                                             dynamic_pad=True,
                                                                                             capacity=self._capacity,
                                                                                             num_threads=self._num_threads)

        # reshape shape into proper form after dequeue from bucket queue
        padded_examples = padded_examples.sg_reshape(shape=[self._batch_size, -1])
        label_examples = label_examples.sg_reshape(shape=[self._batch_size])

        return padded_examples, label_examples
&lt;/denchmark-code&gt;

The above piece of code:

picks a file name from a queue with file name
reads single examples from the file via file reader
loads vocabulary of words from a preprocessed file
uses the vocabulary of words to turn the string tensor into tensor of ids
puts the single examples into a RandomShuffleQueue which allows all the examples to be shuffled
reads a single example from the RandomShuffleQueue and puts it to a bucket_by_sequence queue with dynamic padding
and finally reads batches of examples from the bucket_by_sequence queue

Hope this is helpful to you
		</comment>
		<comment id='23' author='sonalgupta' date='2017-08-03T15:43:19Z'>
		&lt;denchmark-link:https://github.com/AndreasMadsen&gt;@AndreasMadsen&lt;/denchmark-link&gt;
 Thanks for the code above using TensorArray. Did you ever manage to replace it with a FIFOQueue. I have tried multiple times to no avail.  Many thanks.
		</comment>
		<comment id='24' author='sonalgupta' date='2017-08-03T18:04:58Z'>
		&lt;denchmark-link:https://github.com/francotheengineer&gt;@francotheengineer&lt;/denchmark-link&gt;
 you can replace it with something like this:
class SequenceQueueExternal(SequenceQueue):
    data_file: str
    writer = tf.python_io.TFRecordWriter

    def __init__(self, external_encoding: str, *args, **kwargs):
        self.data_file = path.realpath(external_encoding)

        # detect if data file exists
        has_data = (path.exists(self.data_file) and
                    os.stat(self.data_file).st_size &gt; 0)
        super().__init__(not has_data, *args, **kwargs)

        if self.need_data:
            os.makedirs(path.dirname(self.data_file), exist_ok=True)
            with open(self.data_file, 'w'):
                self.writer = tf.python_io.TFRecordWriter(
                    self.data_file,
                    options=tf.python_io.TFRecordOptions(
                        tf.python_io.TFRecordCompressionType.ZLIB
                    )
                )

    def write(self,
              length: int, source: np.ndarray, target: np.ndarray) -&gt; None:
        if not self.need_data:
            raise RuntimeError(
                'queue.write should not be called when need_data is false'
            )

        example = make_sequence_example(length, source, target)
        self.writer.write(example.SerializeToString())

    def read(self) -&gt; Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:
        if self.need_data:
            self.writer.close()

        # create filename queue of one filename. TFRecordReader demands this.
        filename_queue = tf.train.string_input_producer(
            [self.data_file],
            num_epochs=None if self.repeat else 1,
            name=self.name
        )

        # read serialized data
        reader = tf.TFRecordReader(
            options=tf.python_io.TFRecordOptions(
                tf.python_io.TFRecordCompressionType.ZLIB
            )
        )
        reader_dequeue = reader.read(filename_queue)

        # parse data
        length, source, target = parse_sequence_example(reader_dequeue.value)

        # cast to original type
        length = tf.cast(length, dtype=tf.int32)
        source = tf.cast(source, dtype=self.dtype)
        target = tf.cast(target, dtype=self.dtype)

        # To get a continues shuffling behaviour similar to suffle_batch
        # put in a RandomShuffleQueue
        if self.shuffle:
            length, source, target = shuffle_tensor_list(
                (length, source, target),
                capacity=self.batch_size * 128,
                min_after_dequeue=self.batch_size * 64,
                seed=self.seed,
                name=self.name
            )

        return (length, source, target)
		</comment>
		<comment id='25' author='sonalgupta' date='2017-08-03T18:09:44Z'>
		This message was created automatically by mail delivery software.

A message that you sent could not be delivered to one or more of its
recipients. This is a temporary error. The following address(es) deferred:

  mazecreator@gmail.com
    Domain mazecreator.com has exceeded the max emails per hour (26/25 (104%)) allowed.  Message will be reattempted later
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


------- This is a copy of the message, including all the headers. ------
------ The body of the message is 14836 characters long; only the first
------ 5000 or so are included here.
Received: from o4.sgmail.github.com ([192.254.112.99]:55419)
	by server2.lowesthostingrates.com with esmtps (TLSv1.2:ECDHE-RSA-AES128-GCM-SHA256:128)
	(Exim 4.89)
	(envelope-from &lt;bounces+848413-e6d9-mazecreator=mazecreator.com@sgmail.github.com&gt;)
	id 1ddKNW-00048A-Fv
	for mazecreator@mazecreator.com; Thu, 03 Aug 2017 12:58:17 -0500
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=github.com;
	h=from:reply-to:to:cc:in-reply-to:references:subject:mime-version:content-type:content-transfer-encoding:list-id:list-archive:list-post:list-unsubscribe;
	s=s20150108; bh=B2DquR6IstTcRsdhjQeqo1ZdRQY=; b=TBqhTIoVIOwougWf
	p5TL8lkpdAChnGKeRPBQOoQQmp0fvwpWmR44i4zk3eWSJdbV1D4PXejPVdfRWFuj
	xKSDJzRL2sESrE8IHpXt9Nv+QbuHd2u/XJbHIHYoXXsKOXvCFO6S4TRXEP9CXRER
	ijCQs89ZtOqE/6Dn1fIfdOoJJAY=
Received: by filter0439p1mdw1.sendgrid.net with SMTP id filter0439p1mdw1-8131-59836697-54
        2017-08-03 18:08:23.727792632 +0000 UTC
Received: from github-smtp2a-ext-cp1-prd.iad.github.net (github-smtp2a-ext-cp1-prd.iad.github.net [192.30.253.16])
	by ismtpd0030p1mdw1.sendgrid.net (SG) with ESMTP id LAxiP_e0Rlupo7wGghbOCg
	for &lt;mazecreator@mazecreator.com&gt;; Thu, 03 Aug 2017 18:08:23.641 +0000 (UTC)
Date: Thu, 03 Aug 2017 18:08:23 +0000 (UTC)
From: Andreas Madsen &lt;notifications@github.com&gt;
Reply-To: tensorflow/tensorflow &lt;reply@reply.github.com&gt;
To: tensorflow/tensorflow &lt;tensorflow@noreply.github.com&gt;
Cc: Subscribed &lt;subscribed@noreply.github.com&gt;
Message-ID: &lt;tensorflow/tensorflow/issues/5609/320046002@github.com&gt;
In-Reply-To: &lt;tensorflow/tensorflow/issues/5609@github.com&gt;
References: &lt;tensorflow/tensorflow/issues/5609@github.com&gt;
Subject: Re: [tensorflow/tensorflow] Add documentation on how to use bucketing
 functions (#5609)
Mime-Version: 1.0
Content-Type: multipart/alternative;
 boundary="--==_mimepart_598366974122_66ae3ff2e60a5c303173a8";
 charset=UTF-8
Content-Transfer-Encoding: 7bit
Precedence: list
X-GitHub-Sender: AndreasMadsen
X-GitHub-Recipient: Mazecreator
X-GitHub-Reason: subscribed
List-ID: tensorflow/tensorflow &lt;tensorflow.tensorflow.github.com&gt;
List-Archive: https://github.com/tensorflow/tensorflow
List-Post: &lt;mailto:reply@reply.github.com&gt;
List-Unsubscribe: &lt;mailto:unsub+0118f3a0233cd248cebf628debe904ce04cc17dde108f66692cf00000001159b289792a169ce0b4836a1@reply.github.com&gt;,
 &lt;https://github.com/notifications/unsubscribe/ARjzoGUNhusBD4cFgBlx8CGVIkfpotc4ks5sUgyXgaJpZM4KyDNA&gt;
X-Auto-Response-Suppress: All
X-GitHub-Recipient-Address: mazecreator@mazecreator.com
X-SG-EID: BJ4qYjf5a3yL0lCrdDNghY4YYR+k1a+cluU6wEX1JqzbEs87w+mYm/unNG0159r/UEAApwEJBsj9up
 5/RFJG+hFL0CfCPELQXc3qRzY5x+PGzPzqlgb+gHP3WIKr5vDdVS/MY1IiP2yzfqcCTkgYjkpG38mG
 dqi0wpbITd75JwnNOdhAEnRMZ3nsOIBiGHi3DabG8lcZ32lfkkONuaTIrPgFjyJfyD45sIqHImBChB
 A=
X-Spam-Status: No, score=
X-Spam-Score:
X-Spam-Bar:
X-Ham-Report:
X-Spam-Flag: NO
----==_mimepart_598366974122_66ae3ff2e60a5c303173a8
Content-Type: text/plain;
 charset=UTF-8
Content-Transfer-Encoding: 7bit

@francotheengineer you can replace it with something like this:

```py

class SequenceQueueExternal(SequenceQueue):
    data_file: str
    writer = tf.python_io.TFRecordWriter

    def __init__(self, external_encoding: str, *args, **kwargs):
        self.data_file = path.realpath(external_encoding)

        # detect if data file exists
        has_data = (path.exists(self.data_file) and
                    os.stat(self.data_file).st_size &gt; 0)
        super().__init__(not has_data, *args, **kwargs)

        if self.need_data:
            os.makedirs(path.dirname(self.data_file), exist_ok=True)
            with open(self.data_file, 'w'):
                self.writer = tf.python_io.TFRecordWriter(
                    self.data_file,
                    options=tf.python_io.TFRecordOptions(
                        tf.python_io.TFRecordCompressionType.ZLIB
                    )
                )

    def write(self,
              length: int, source: np.ndarray, target: np.ndarray) -&gt; None:
        if not self.need_data:
            raise RuntimeError(
                'queue.write should not be called when need_data is false'
            )

        example = make_sequence_example(length, source, target)
        self.writer.write(example.SerializeToString())

    def read(self) -&gt; Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:
        if self.need_data:
            self.writer.close()

        # create filename queue of one filename. TFRecordReader demands this.
        filename_queue = tf.train.string_input_producer(
            [self.data_file],
            num_epochs=None if self.repeat else 1,
            name=self.name
        )

        # read serialized data
        reader = tf.TFRecordReader(
            options=tf.python_io.TFRecordOptions(
                tf.python_io.TFRecordCompressionType.ZLIB
            )
        )
        reader_dequeue = reader.read(filename_queue)

        # parse data
        length, source, target = parse_sequence_example(reader_dequeue.value)

        # cast to original type
        length = tf.cast(length, dtype=tf.int32)
        source = tf.cast(source, dtype=self.dtype)
        target = tf.cast(target, dtype=self.dtype)

        # To get a continues shuffling behaviour similar to suffle_batch
        # put in a RandomShuffleQueue
        if self.shuffle:
            length, source, target = shuffle_tensor_list(
                (length, source, target),
                capacity=self.batch_size * 128,
                min_after_dequeue=self.batch_size * 64,
                seed=self.seed,
                name=self.name
            )

        return (length, source, target)
```
-- 
You are receiving this because you are subscribed to this thread.
Reply to this email directly or view it on GitHub:
#5609 (comment)
----==_mimepart_598366974122_66ae3ff2e60a5c303173a8
Content-Type: text/html;
 charset=UTF-8
Content-Transfer-Encoding: quoted-printable

&lt;p&gt;&lt;a href=3D"https://github.com/francotheengineer" class=3D"user-mention"&gt;=
@francotheengineer&lt;/a&gt; you can replace it with something like this:&lt;/p&gt;
&lt;div class=3D"highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class=3D"pl-k"&gt;=
class&lt;/span&gt; &lt;span class=3D"pl-en"&gt;SequenceQueueExternal&lt;/span&gt;(&lt;span class=
=3D"pl-e"&gt;SequenceQueue&lt;/span&gt;):
    data_file: &lt;span class=3D"pl-c1"&gt;str&lt;/span&gt;
    writer &lt;span class=3D"pl-k"&gt;=3D&lt;/span&gt; tf.python_io.TFRecordWriter

    &lt;span class=3D"pl-k"&gt;def&lt;/span&gt; &lt;span class=3D"pl-c1"&gt;__init__&lt;/span&gt;(&lt;=
span class=3D"pl-smi"&gt;&lt;span class=3D"pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span clas=
s=3D"pl-smi"&gt;external_encoding&lt;/span&gt;: &lt;span class=3D"pl-c1"&gt;str&lt;/span&gt;, &lt;s=
pan class=3D"pl-k"&gt;*&lt;/span&gt;&lt;span class=3D"pl-smi"&gt;args&lt;/span&gt;, &lt;span class=
=3D"pl-k"&gt;**&lt;/span&gt;&lt;span class=3D"pl-smi"&gt;kwargs&lt;/span&gt;):
        &lt;span class=3D"pl-c1"&gt;self&lt;/span&gt;.data_file &lt;span class=3D"pl-k"&gt;=
=3D&lt;/span&gt; path.realpath(external_encoding)

        &lt;span class=3D"pl-c"&gt;&lt;span class=3D"pl-c"&gt;#&lt;/span&gt; detect if data f=
ile exists&lt;/span&gt;
        has_data &lt;span class=3D"pl-k"&gt;=3D&lt;/span&gt; (path.exists(&lt;span class=
=3D"pl-c1"&gt;self&lt;/span&gt;.data_file) &lt;span class=3D"pl-k"&gt;and&lt;/span&gt;
                    os.stat(&lt;span class=3D"pl-c1"&gt;self&lt;/span&gt;.data_file).st=
_size &lt;span class=3D"pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class=3D"pl-c1"&gt;0&lt;/span&gt;)
        &lt;span class=3D"pl-c1"&gt;super&lt;/span&gt;().&lt;span class=3D"pl-c1"&gt;__init__=
&lt;/span&gt;(&lt;span class=3D"pl-k"&gt;not&lt;/span&gt; has_data, &lt;span class=3D"pl-k"&gt;*&lt;/s=
pan&gt;args, &lt;span class=3D"pl-k"&gt;**&lt;/span&gt;kwargs)

        &lt;span class=3D"pl-k"&gt;if&lt;/span&gt; &lt;span class=3D"pl-c1"&gt;self&lt;/span&gt;.ne=
ed_data:
            os.makedirs(path.dirname(&lt;span class=3D"pl-c1"&gt;self&lt;/span&gt;.data=
_file), &lt;span class=3D"pl-v"&gt;exist_ok&lt;/span&gt;&lt;span class=3D"pl-k"&gt;=3D&lt;/span&gt;=
&lt;span class=3D"pl-c1"&gt;True&lt;/span&gt;)
            &lt;span class=3D"pl-k"&gt;with&lt;/span&gt; &lt;span class=3D"pl-c1"&gt;open&lt;/sp=
an&gt;(&lt;span c

		</comment>
		<comment id='26' author='sonalgupta' date='2017-08-22T14:42:59Z'>
		For those searching for solutions in the future: &lt;denchmark-link:https://github.com/francotheengineer/Bucket_by_sequence_length&gt;https://github.com/francotheengineer/Bucket_by_sequence_length&lt;/denchmark-link&gt;
    Thanks.
		</comment>
		<comment id='27' author='sonalgupta' date='2017-10-03T17:03:50Z'>
		tf.contrib.data (will be tf.data in 1.4) is moving to replace the queue based input pipelines.
It has a method that can allow this bucket-by-sequence-length functionality as well: &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/contrib/data/Dataset#group_by_window&gt;group_by_window&lt;/denchmark-link&gt;
.
It's not a one-liner, but this &lt;denchmark-link:https://github.com/mari-linhares/tensorflow-workshop/blob/master/code_samples/RNN/colorbot/colorbot.ipynb&gt;"colorbot"&lt;/denchmark-link&gt;
 demonstrates how you can use datasets and  to do your own .
		</comment>
		<comment id='28' author='sonalgupta' date='2017-10-03T19:07:32Z'>
		There's an additional example here
&lt;&lt;denchmark-link:https://github.com/tensorflow/nmt/blob/master/nmt/utils/iterator_utils.py#L81&gt;https://github.com/tensorflow/nmt/blob/master/nmt/utils/iterator_utils.py#L81&lt;/denchmark-link&gt;
&gt;
in
the tensorflow NMT tutorial.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Oct 3, 2017 at 10:05 AM, Mark Daoust ***@***.***&gt; wrote:
 tf.contrib.data (will be tf.data in 1.4) is moving to replace the queue
 based input pipelines.

 It has a method that can allow this bucket-by-sequence-length
 functionality as well: group_by_window
 &lt;https://www.tensorflow.org/api_docs/python/tf/contrib/data/Dataset#group_by_window&gt;
 .

 It's not a one-liner, but this "colorbot"
 &lt;https://github.com/mari-linhares/tensorflow-workshop/blob/master/code_samples/RNN/colorbot/colorbot.ipynb&gt;
 demonstrates how you can use datasets and group_by_window to do your own
 batch_by_sequence_length.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#5609 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtim4Kdvzk-9E_HY38B3c11VpzZPxvoks5somlkgaJpZM4KyDNA&gt;
 .



		</comment>
	</comments>
</bug>