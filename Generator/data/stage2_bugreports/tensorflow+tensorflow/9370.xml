<bug id='9370' author='alquraishi' open_date='2017-04-21T17:19:00Z' closed_time='2018-01-06T19:50:32Z'>
	<summary>Possibly serious bug in cuDNN RNNParamsSaveable</summary>
	<description>
RNNParamsSaveable appears to only save half of the weights when the RNN is bidirectional. See below.
When the RNN is unidirectional, model.params_size() matches the total size of weights + biases returned by model.params_to_canonical(params)
&lt;denchmark-code&gt;model = cudnn_rnn_ops.CudnnLSTM(num_layers=1, num_units=100, input_size=20, direction='unidirectional')
params = tf.get_variable('cudnn_rnn_params', initializer=tf.random_uniform([model.params_size()]), validate_shape=False)
model.params_size().eval(session=sess) # returns 48800
sum([wts.eval(session=sess).shape[0] for wtss in model.params_to_canonical(params) for wts in wtss]) # returns 48800
&lt;/denchmark-code&gt;

On the other hand, when the RNN is bidirectional, model.params_size() returns twice the size of the unidirectional case, which makes sense, but the size of model.params_to_canonical(params) is unchanged.
&lt;denchmark-code&gt;model = cudnn_rnn_ops.CudnnLSTM(num_layers=1, num_units=100, input_size=20, direction='bidirectional')
params = tf.get_variable('cudnn_rnn_params', initializer=tf.random_uniform([model.params_size()]), validate_shape=False)
model.params_size().eval(session=sess) # returns 97600
sum([wts.eval(session=sess).shape[0] for wtss in model.params_to_canonical(params) for wts in wtss]) # returns 48800
&lt;/denchmark-code&gt;

I believe this may have been missed by tests because, as this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/f0f7a1ef63c47075e3eb7eaeeb3588d057f3171d/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py#L37&gt;TODO&lt;/denchmark-link&gt;
 suggests, both the canonical and non-canonical versions are being saved and restored, and so even if only half the weights are being restored from the canonical version, the non-canonical weights can compensate and hide the problem.
Am I missing something?
	</description>
	<comments>
		<comment id='1' author='alquraishi' date='2017-04-22T17:06:37Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 any comment on this? Is this a documentation or usage issue?
		</comment>
		<comment id='2' author='alquraishi' date='2017-05-05T11:46:12Z'>
		And updates on this?
		</comment>
		<comment id='3' author='alquraishi' date='2017-05-06T21:43:02Z'>
		&lt;denchmark-link:https://github.com/alquraishi&gt;@alquraishi&lt;/denchmark-link&gt;
, thanks for reporting the issue. This does look like a bug/feature request. &lt;denchmark-link:https://github.com/boche&gt;@boche&lt;/denchmark-link&gt;
 also reported a related (potentially the same) issue here: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/6072&gt;#6072&lt;/denchmark-link&gt;

I looked into the code and currently the code only stores params for the forward direction, as you have observed. So I don't think checkpointing for the bidirectional case is currently working/supported.
The unknown param order (in the bidirectional case) problem &lt;denchmark-link:https://github.com/boche&gt;@boche&lt;/denchmark-link&gt;
 raised is a valid question. I looked into the cuDNN documentation. The order (in the bidirectional case) is not documented for function cudnnGetRNNLinLayerMatrixParams and  cudnnGetRNNLinLayerBiasParams. Another possibility is that the backward params are accessed by the "layer" parameter in these two functions. However, it is also not clear if this is the case in the cuDNN documentation.
I marked this as contribution welcome for interested people to take a deeper look from here and possibly propose and work on a fix.
		</comment>
		<comment id='4' author='alquraishi' date='2017-05-06T21:45:07Z'>
		We should consider also contacting nvidia for clarification in their
documentation.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Sat, May 6, 2017 at 2:43 PM, zhangyaobit ***@***.***&gt; wrote:
 @alquraishi &lt;https://github.com/alquraishi&gt;, thanks for reporting the
 issue. This does look like a bug/feature request. @boche
 &lt;https://github.com/boche&gt; also reported a related (potentially the same)
 issue here: #6072 &lt;#6072&gt;

 I looked into the code and currently the code only stores params for
 forward direction, as you have observed. So I don't think checkpointing for
 the bidirectional the case is currently working/supported.

 The unknown param order (in the bidirectional case) problem @boche
 &lt;https://github.com/boche&gt; raised is a valid question. I looked in the
 cuDNN documentation. The order (in the bidirectional case) is not
 documented for function cudnnGetRNNLinLayerMatrixParams and  cudnnGetRNNLinLayerBiasParams.
 Another possibility is that the backward params are accessed by the "layer"
 parameter in these two functions. However, it is also not clear if this is
 the case in the cuDNN documentation.

 I marked this as contribution welcome for interested people to take a
 deeper look from here and possibly propose and work on a fix.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#9370 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtim7L2tls7yl9HK82TWyLdUmS3PP1Uks5r3Ol5gaJpZM4NEkX1&gt;
 .



		</comment>
		<comment id='5' author='alquraishi' date='2017-05-06T21:57:14Z'>
		Good point. Assigning &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='alquraishi' date='2017-05-07T01:51:42Z'>
		May I suggest that this gets documented very clearly as a limitation, perhaps even preventing the user from accidentally using RNNParamsSaver with a bidirectional RNN? I understand the temptation to mark this as contributions welcome and set it aside, but I really do think it's a very serious bug. What makes it particularly problematic is that it's completely silent, so a user may suddenly see their model behaving inexplicably without any idea why. The only reason I stumbled onto this is because I noticed the size difference by accident, otherwise I would've merrily used it without realizing there's a hidden bug, and I routinely checkpoint models on one GPU type and resume on another, so it's not an unlikely scenario. Just my two cents. Of course ideally the problem would get solved.
		</comment>
		<comment id='7' author='alquraishi' date='2017-05-17T20:48:46Z'>
		Any updates on this?
		</comment>
		<comment id='8' author='alquraishi' date='2017-05-18T06:27:20Z'>
		&lt;denchmark-link:https://github.com/alquraishi&gt;@alquraishi&lt;/denchmark-link&gt;
, we haven't had time to look into this. We provided this checkpointing feature for cuDNN and I'm glad you find it useful and seem to be a heavy user of it. If you think you benefit from this feature, please consider registering as a contributor and contributing back to improve it. We could together make TensorFlow better. I like your idea of better documentation. And if you could help dive deeper (from my previous suggestions) into the bidirectional RNN issue you found, that would be even better. Please consider registering as NVIDIA developer as well to ask them about the bidirectional RNN layout.
We just haven't found a time to look into this, ebrevdo@ and zheng-xq@ will follow up when they get a chance.
		</comment>
		<comment id='9' author='alquraishi' date='2017-07-05T08:42:57Z'>
		This is not a bug in the checkpointing nor in RNNParamsSaveable.
This is a bug in the CudnnRNNParamsToCanonical op or in CudnnSupport::createRnnDescriptor or even in cuDNN itself.
Note that the test also seems to be not correct. &lt;denchmark-link:https://github.com/alquraishi&gt;@alquraishi&lt;/denchmark-link&gt;
, you are only summing . But you should sum . I wonder why that worked for you in the unidirectional case. It does not for me.
		</comment>
		<comment id='10' author='alquraishi' date='2017-12-22T07:31:22Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='11' author='alquraishi' date='2018-01-06T18:57:15Z'>
		Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the contributions welcome label. Thank you.
		</comment>
		<comment id='12' author='alquraishi' date='2018-01-06T19:50:32Z'>
		This class is deprecated. The new CudnnRNN layers manages vars and saveables for users.
		</comment>
	</comments>
</bug>