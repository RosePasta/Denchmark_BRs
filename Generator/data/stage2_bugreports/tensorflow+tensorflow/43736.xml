<bug id='43736' author='DhruvAwasthi' open_date='2020-10-02T14:21:12Z' closed_time='2020-10-27T11:49:20Z'>
	<summary>Unable to train the model</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Manjaro 5.4.64 kernel
TensorFlow installed from (source or binary): Pycharm
TensorFlow version: 2.3.1
Python version: 3.8
Installed using virtualenv? pip? conda?: virtualenv
GPU model and memory: No GPU

Describe the problem
I have built a model architecture that takes in multimodal inputs i.e. visual and textual. For visual inputs, I am using the embeddings generated by second to last layer of VGG-19 of 4096 dimension and passing them here. When I do start training the model, the following error shows up:
&lt;denchmark-code&gt;Epoch 1/300
Traceback (most recent call last):
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
TypeError: An op outside of the function building code is being passed
a "Graph" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: z_log_var/BiasAdd:0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/caesar/PycharmProjects/fake-news-detector/mvae.py", line 211, in &lt;module&gt;
    train(20, 4096, 64, 0.05, 0.3, 'models/vae_fnd_0.05_0.3')
  File "/home/caesar/PycharmProjects/fake-news-detector/mvae.py", line 158, in train
    model.autoencoder.fit(x=[text, im],
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 1098, in fit
    tmp_logs = train_function(iterator)
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 780, in __call__
    result = self._call(*args, **kwds)
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 840, in _call
    return self._stateless_fn(*args, **kwds)
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 2829, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 1843, in _filtered_call
    return self._call_flat(
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 1923, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 545, in call
    outputs = execute.execute(
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py", line 72, in quick_execute
    raise core._SymbolicException(
tensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [&lt;tf.Tensor 'z_log_var/BiasAdd:0' shape=(None, 64) dtype=float32&gt;, &lt;tf.Tensor 'z_mean/BiasAdd:0' shape=(None, 64) dtype=float32&gt;]

Process finished with exit code 1

&lt;/denchmark-code&gt;

Provide the exact sequence of commands / steps that you executed before running into the problem
&lt;denchmark-code&gt;import pdb

import tensorflow
from keras import regularizers
from keras import objectives, backend as K
from keras.layers import Dropout, Reshape, Concatenate, Flatten, Bidirectional, Dense, Embedding, Input, Lambda, LSTM, \
    RepeatVector, TimeDistributed
from keras.models import Model
from keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, ModelCheckpoint, TensorBoard
from keras.optimizers import Adam, RMSprop
import keras
import numpy as np
import os
from sklearn.metrics import precision_score, accuracy_score, precision_recall_fscore_support

# tensorflow.config.experimental_run_functions_eagerly(True)


class MVAE(object):

    def create(self, max_length, image_embed_size, latent_dim, reg_lambda, fnd_lambda, embed_matrix):
        self.encoder = None
        self.decoder = None
        self.fnd = None
        self.autoencoder = None
        self.embedding_matrix = embed_matrix
        self.vocab_size = self.embedding_matrix.shape[0]
        self.max_length = max_length
        self.latent_dim = latent_dim
        self.reg_lambda = reg_lambda
        self.fnd_lambda = fnd_lambda
        self.image_embed_size = image_embed_size

        input_txt = Input(shape=(self.max_length,), name='input_txt')
        input_img = Input((image_embed_size,), name='input_img')

        vae_ce_loss, vae_mse_loss, encoded = self._build_encoder(input_txt, input_img)
        self.encoder = Model(inputs=[input_txt, input_img], outputs=encoded)

        encoded_input = Input(shape=(self.latent_dim,))
        predicted_outcome = self._build_fnd(encoded_input)
        self.fnd = Model(encoded_input, predicted_outcome)

        decoded_txt, decoded_img = self._build_decoder(encoded_input)
        self.decoder = Model(encoded_input, [decoded_txt, decoded_img])

        decoder_output = self._build_decoder(encoded)

        self.autoencoder = Model(inputs=[input_txt, input_img],
                                 outputs=[decoder_output[0], decoder_output[1], self._build_fnd(encoded)])
        self.autoencoder.compile(optimizer=Adam(1e-5),
                                 loss=['sparse_categorical_crossentropy', vae_mse_loss, 'binary_crossentropy'],
                                 metrics=['accuracy'])
        self.get_features = K.function([input_txt, input_img], [encoded])
        print(self.autoencoder.summary())

    def _build_encoder(self, input_txt, input_img, latent_dim=64):
        txt_embed = Embedding(self.vocab_size, 32, input_length=self.max_length, name='txt_embed', trainable=False,
                              weights=[self.embedding_matrix])(input_txt)
        lstm_txt_1 = Bidirectional(LSTM(32, return_sequences=True, name='lstm_txt_1', activation='tanh',
                                        kernel_regularizer=regularizers.l2(self.reg_lambda)), merge_mode='concat')(
            txt_embed)
        lstm_txt_2 = Bidirectional(LSTM(32, return_sequences=False, name='lstm_txt_2', activation='tanh',
                                        kernel_regularizer=regularizers.l2(self.reg_lambda)), merge_mode='concat')(
            lstm_txt_1)
        fc_txt = Dense(32, activation='tanh', name='dense_txt', kernel_regularizer=regularizers.l2(self.reg_lambda))(
            lstm_txt_2)

        fc_img_1 = Dense(1024, name='fc_img_1', activation='tanh', kernel_regularizer=regularizers.l2(self.reg_lambda))(
            input_img)
        fc_img_2 = Dense(32, name='fc_img_2', activation='tanh', kernel_regularizer=regularizers.l2(self.reg_lambda))(
            fc_img_1)

        h = Concatenate(axis=-1, name='concat')([fc_txt, fc_img_2])
        h = Dense(64, name='shared', activation='tanh', kernel_regularizer=regularizers.l2(self.reg_lambda))(h)

        def sampling(args):
            z_mean_, z_log_var_ = args
            batch_size = K.shape(z_mean_)[0]
            epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=0.01)
            return z_mean_ + K.exp(0.5 * z_log_var_) * epsilon

        z_mean = Dense(latent_dim, name='z_mean', activation='linear')(h)
        z_log_var = Dense(latent_dim, name='z_log_var', activation='linear')(h)

        def vae_mse_loss(x, x_decoded_mean):
            mse_loss = objectives.mse(x, x_decoded_mean)
            kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)
            return mse_loss + kl_loss

        def vae_ce_loss(x, x_decoded_mean):
            x = K.flatten(x)
            x_decoded_mean = K.flatten(x_decoded_mean)
            xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)
            kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)
            return xent_loss + kl_loss

        return (
        vae_ce_loss, vae_mse_loss, Lambda(sampling, output_shape=(latent_dim,), name='lambda')([z_mean, z_log_var]))

    def _build_decoder(self, encoded):
        dec_fc_txt = Dense(32, name='dec_fc_txt', activation='tanh',
                           kernel_regularizer=regularizers.l2(self.reg_lambda))(encoded)
        repeated_context = RepeatVector(self.max_length)(dec_fc_txt)
        dec_lstm_txt_1 = LSTM(32, return_sequences=True, activation='tanh', name='dec_lstm_txt_1',
                              kernel_regularizer=regularizers.l2(self.reg_lambda))(repeated_context)
        dec_lstm_txt_2 = LSTM(32, return_sequences=True, activation='tanh', name='dec_lstm_txt_2',
                              kernel_regularizer=regularizers.l2(self.reg_lambda))(dec_lstm_txt_1)
        decoded_txt = TimeDistributed(Dense(self.vocab_size, activation='softmax'), name='decoded_txt')(dec_lstm_txt_2)

        dec_fc_img_1 = Dense(32, name='dec_fc_img_1', activation='tanh',
                             kernel_regularizer=regularizers.l2(self.reg_lambda))(encoded)
        dec_fc_img_2 = Dense(1024, name='dec_fc_img_2', activation='tanh',
                             kernel_regularizer=regularizers.l2(self.reg_lambda))(dec_fc_img_1)
        decoded_img = Dense(4096, name='decoded_img', activation='sigmoid')(dec_fc_img_2)

        return decoded_txt, decoded_img

    def _build_fnd(self, encoded):
        h = Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(self.fnd_lambda))(encoded)
        h = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(self.fnd_lambda))(h)
        return Dense(1, activation='sigmoid', name='fnd_output')(h)


def train(sequence_length, image_embed_size, latent_dim, reg_lambda, fnd_lambda, path):
    text = np.load('data/train_text.npy')
    im = np.load('data/train_image_embed.npy')
    label = np.load('data/train_label.npy')[:, 1]

    test_text = np.load('data/test_text.npy')
    test_im = np.load('data/test_image_embed.npy')
    test_label = np.load('data/test_label.npy')[:, 1]

    embed_matrix = np.load('data/embedding_matrix.npy')
    vocab_size = embed_matrix.shape[0]

    # temp = np.zeros((text.shape[0], sequence_length, vocab_size))
    # temp[np.expand_dims(np.arange(text.shape[0]), axis=0).reshape(text.shape[0], 1), np.repeat(np.array([np.arange(sequence_length)]), text.shape[0], axis=0), text] = 1
    # text_one_hot = temp
    #
    # temp = np.zeros((test_text.shape[0], sequence_length, vocab_size))
    # temp[np.expand_dims(np.arange(test_text.shape[0]), axis=0).reshape(test_text.shape[0], 1), np.repeat(np.array([np.arange(sequence_length)]), test_text.shape[0], axis=0), test_text] = 1
    # test_text_one_hot = temp

    if not os.path.exists(path):
        os.makedirs(path)
    if not os.path.exists(path + '/tb'):
        os.makedirs(path + '/tb')
    if not os.path.exists(path + '/weights'):
        os.makedirs(path + '/weights')
    tensorboard = TensorBoard(log_dir=path + '/tb', write_graph=True, write_images=True)
    checkpoint = ModelCheckpoint(path + '/weights/{epoch:02d}.hdf5', monitor='loss', verbose=1, save_best_only=True,
                                 mode='auto')
    reduce_lr = ReduceLROnPlateau(monitor='fnd_output_loss', factor=0.2, patience=6, min_lr=1e-7)

    model = MVAE()
    model.create(sequence_length, image_embed_size, latent_dim, reg_lambda, fnd_lambda, embed_matrix)
    model.autoencoder.fit(x=[text, im],
                          y={'decoded_txt': np.expand_dims(text, -1), 'decoded_img': im, 'fnd_output': label},
                          batch_size=128, epochs=300, callbacks=[checkpoint, tensorboard, reduce_lr], shuffle=True,
                          validation_data=([test_text, test_im],
                                           {'decoded_txt': np.expand_dims(test_text, -1), 'decoded_img': test_im,
                                            'fnd_output': test_label}))


def save_features(sequence_length, image_embed_size, latent_dim, reg_lambda, fnd_lambda, path):
    test_text = np.load('../data/test_text.npy')
    test_im = np.load('../data/test_image_embed.npy')

    embed_matrix = np.load('../data/embedding_matrix.npy')
    vocab_size = embed_matrix.shape[0]

    model = MVAE()
    model.create(sequence_length, image_embed_size, latent_dim, reg_lambda, fnd_lambda, embed_matrix)
    model.autoencoder.load_weights(path + '/weights/286.hdf5')

    if not os.path.exists(path + '/features'):
        os.makedirs(path + '/features')

    learnt_features = np.array([]).reshape(0, 64)
    for i in range(test_text.shape[0]):
        text_batch = test_text[i:i + 1]
        im_batch = test_im[i:i + 1]
        batch = model.get_features([text_batch, im_batch])[0]
        learnt_features = np.concatenate([learnt_features, batch])
    np.save(path + '/features/vae_fnd', learnt_features)


def test(sequence_length, image_embed_size, latent_dim, reg_lambda, fnd_lambda, path):
    test_text = np.load('data/test_text.npy')
    test_im = np.load('data/test_image_embed.npy')
    test_label = np.load('data/test_label.npy')[:, 1]

    embed_matrix = np.load('data/embedding_matrix.npy')
    vocab_size = embed_matrix.shape[0]

    model = MVAE()
    model.create(sequence_length, image_embed_size, latent_dim, reg_lambda, fnd_lambda, embed_matrix)
    model.autoencoder.load_weights(path + '/weights/224.hdf5')
    for i in range(10):
        pred = model.autoencoder.predict([test_text, test_im])[-1]
        pred[pred &gt; 0.5] = 1
        pred[pred &lt;= 0.5] = 0
        print(accuracy_score(test_label, pred))
        print(precision_recall_fscore_support(test_label, pred))

    pdb.set_trace()


if __name__ == '__main__':
    train(20, 4096, 64, 0.05, 0.3, 'models/vae_fnd_0.05_0.3')
    test(20, 4096, 64, 0.05, 0.3, 'models/vae_fnd_0.05_0.3')
    save_features(20, 4096, 64, 0.05, 0.3, '../models/vae_fnd_0.05_0.3')

&lt;/denchmark-code&gt;

Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
I tried adding tensorflow.config.experimental_run_functions_eagerly(True) to the code. This error went away but another error showed up later which I guess is because of adding this line only error -
&lt;denchmark-code&gt;Epoch 1/300
2020-10-02 19:48:34.816657: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 130744320 exceeds 10% of free system memory.
2020-10-02 19:48:35.015006: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 130744320 exceeds 10% of free system memory.
2020-10-02 19:48:35.073321: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 130744320 exceeds 10% of free system memory.
Traceback (most recent call last):
  File "/home/caesar/PycharmProjects/fake-news-detector/mvae.py", line 211, in &lt;module&gt;
    train(20, 4096, 64, 0.05, 0.3, 'models/vae_fnd_0.05_0.3')
  File "/home/caesar/PycharmProjects/fake-news-detector/mvae.py", line 158, in train
    model.autoencoder.fit(x=[text, im],
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 1098, in fit
    tmp_logs = train_function(iterator)
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 806, in train_function
    return step_function(self, iterator)
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 796, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py", line 1211, in run
    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py", line 2585, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py", line 2945, in _call_for_each_replica
    return fn(*args, **kwargs)
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py", line 275, in wrapper
    return func(*args, **kwargs)
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 789, in run_step
    outputs = model.train_step(data)
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 756, in train_step
    _minimize(self.distribute_strategy, tape, self.optimizer, loss,
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 2743, in _minimize
    optimizer.apply_gradients(
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py", line 545, in apply_gradients
    return distribute_ctx.get_replica_context().merge_call(
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py", line 2715, in merge_call
    return self._merge_call(merge_fn, args, kwargs)
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py", line 2722, in _merge_call
    return merge_fn(self._strategy, *args, **kwargs)
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py", line 275, in wrapper
    return func(*args, **kwargs)
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py", line 642, in _distributed_apply
    with ops.control_dependencies(update_ops):
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py", line 5359, in control_dependencies
    return get_default_graph().control_dependencies(control_inputs)
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py", line 360, in control_dependencies
    return super(FuncGraph, self).control_dependencies(filtered_control_inputs)
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py", line 4749, in control_dependencies
    c = self.as_graph_element(c)
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py", line 3670, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File "/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py", line 3758, in _as_graph_element_locked
    raise TypeError("Can not convert a %s into a %s." %
TypeError: Can not convert a NoneType into a Tensor or Operation.

Process finished with exit code 1

&lt;/denchmark-code&gt;

It is because of the some graph elements due to which previous error showed up.
Thank you for your time!
	</description>
	<comments>
		<comment id='1' author='DhruvAwasthi' date='2020-10-05T08:00:28Z'>
		&lt;denchmark-link:https://github.com/DhruvAwasthi&gt;@DhruvAwasthi&lt;/denchmark-link&gt;

Can you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster.Thanks!
		</comment>
		<comment id='2' author='DhruvAwasthi' date='2020-10-05T11:12:31Z'>
		Hey &lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
, thank you for your prompt response
Here are the attached files.
When you unzip this, the files in /data directory are the supporting files required and mvae.py contains the main code to build the architecture.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/5326971/architecture.zip&gt;architecture.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='DhruvAwasthi' date='2020-10-06T07:06:44Z'>
		&lt;denchmark-link:https://github.com/DhruvAwasthi&gt;@DhruvAwasthi&lt;/denchmark-link&gt;

I am not seeing any files in data folder. Can you please recheck and share with me. Thanks!
		</comment>
		<comment id='4' author='DhruvAwasthi' date='2020-10-06T07:41:50Z'>
		Hey, please unzip this into /data dir
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/5332091/data.zip&gt;data.zip&lt;/denchmark-link&gt;

Thank you!
		</comment>
		<comment id='5' author='DhruvAwasthi' date='2020-10-06T09:58:11Z'>
		&lt;denchmark-link:https://github.com/DhruvAwasthi&gt;@DhruvAwasthi&lt;/denchmark-link&gt;

I tried in colab with TF version 2.3 and i am seeing different error message().Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/4abcd0b094671bee05d80ced53626234/untitled426.ipynb#scrollTo=vJJJxMzFSnGL&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='6' author='DhruvAwasthi' date='2020-10-06T12:25:50Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;

&lt;denchmark-link:https://drive.google.com/file/d/1Wnj1TmUaOSlzDJx8LvyLVizrcm7QM0Kl/view?usp=sharing&gt;Here&lt;/denchmark-link&gt;
 is the updated zip, you can use this...
Thank you for the patience!
		</comment>
		<comment id='7' author='DhruvAwasthi' date='2020-10-07T07:55:18Z'>
		&lt;denchmark-link:https://github.com/DhruvAwasthi&gt;@DhruvAwasthi&lt;/denchmark-link&gt;

I tried in colab with TF version 2.3 and i am seeing the error message (
).Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/eb1e1b3f163c017ee07cd0ca950d7f8c/untitled431.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='8' author='DhruvAwasthi' date='2020-10-07T11:49:22Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;

I am sharing a &lt;denchmark-link:https://colab.research.google.com/drive/165C3QYuXOp8AQt796JN2_3loXay_GMd1?usp=sharing&gt;colab&lt;/denchmark-link&gt;
 with TF version 2.3.1
Please upload the data.zip file before running.
Thank you!
		</comment>
		<comment id='9' author='DhruvAwasthi' date='2020-10-13T10:32:02Z'>
		I have ran your codein tf-nightly and this is a user error. Please find the gist &lt;denchmark-link:https://colab.research.google.com/gist/gowthamkpr/95740bdaac8bb5f5ce473ff8e4cd1b55/untitled5.ipynb&gt;here&lt;/denchmark-link&gt;
. Also inorder to understand about symbolic tensors you can look at this &lt;denchmark-link:https://stackoverflow.com/questions/59707065/what-are-symbolic-tensors-in-tensorflow-and-keras&gt;issue&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='DhruvAwasthi' date='2020-10-20T11:22:54Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='11' author='DhruvAwasthi' date='2020-10-27T11:49:19Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='12' author='DhruvAwasthi' date='2020-10-27T11:49:21Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43736&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43736&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>