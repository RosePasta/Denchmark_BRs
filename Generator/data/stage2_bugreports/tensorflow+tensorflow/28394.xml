<bug id='28394' author='Atrus619' open_date='2019-05-04T15:40:05Z' closed_time='2019-06-06T17:03:52Z'>
	<summary>Custom TF 2.0 training loop performing considerably worse than keras fit_generator - can't understand why</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.2 LTS
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v1.12.0-9492-g2c319fb415 2.0.0-alpha0
Python version: 3.6
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: 10.1/V10.1.105
GPU model and memory: RTX 2080 ti 11gb

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
Currently trying to reproduce the results from keras model.fit_generator with a custom training loop in TF2.0. Code runs without any issue, but the MAE loss for the custom training loop is ~3.0, whereas the MAE loss for the keras model.fit_generator is significantly better, at ~2.0.
Describe the expected behavior
I would expect the losses to be roughly equivalent.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
The data set is extremely large, but each sample consists of 4 fields of length 14,995. Target is a single value, in seconds.
`
import tensorflow as tf
import numpy as np
import os
import pandas as pd
import time
from sklearn import preprocessing
import shutil
import my_classes_tf
&lt;denchmark-code&gt;# Import data and massage
os.chdir('/home/aj/Data/LANL-Earthquake-Prediction')
# cv_indices = pd.read_csv('./Current Data/cv_assignments.csv', delimiter=',', header=None).values.astype('int16')
evaluation_indices = pd.read_csv('./Current Data/Validation Indices Original.csv', delimiter=',', header=None).values.astype('int64')
eval_index, cv_index = np.hsplit(evaluation_indices, 2)
train = pd.read_csv('./Current Data/NewFeatures.csv', delimiter=',', header=None).values.astype('float32')
train_data, other_info = np.hsplit(train, 2)
targets, OG_row, EQ_ind, CV_ind = np.hsplit(other_info, 4)
targets = targets.astype('float16')
OG_row = OG_row.astype('int64')
EQ_ind = EQ_ind.astype('int64')
CV_ind = CV_ind.astype('int64')
mod_eval = pd.read_csv('./Current Data/Validation Indices Modified.csv', delimiter=',', header=None).values.astype('int64')
mod_eval_index, mod_cv_index, _, _ = np.hsplit(mod_eval, 4)

logtrain = pd.read_csv('./Current Data/NewFeatures_logtransformed.csv', delimiter=',', header=None).values.astype('float32')

log_std, log_skew, log_kurt, log_sixth, _, _, _ = np.hsplit(logtrain, 7)
train_data_logs = np.concatenate((log_std, log_skew, log_kurt, log_sixth), axis=1)

del logtrain, log_std, log_skew, log_kurt, log_sixth, other_info


def safe_mkdir(path):
    """ Create a directory if there isn't one already. """
    try:
        os.mkdir(path)
    except OSError:
        pass


def del_dir(name):
    if os.path.isdir('./Saved Models/{}'.format(name)):
        shutil.rmtree('./Saved Models/{}'.format(name))
    if os.path.isdir('./Error Plots/{}'.format(name)):
        shutil.rmtree('./Error Plots/{}'.format(name))
    if os.path.isdir('./Train and Test Losses/{}'.format(name)):
        shutil.rmtree('./Train and Test Losses/{}'.format(name))


fold = 1
boolz = CV_ind != fold
cv_train = train_data_logs[boolz.reshape(-1)]
cv_targets = targets[boolz.reshape(-1)]
cv_eqs = EQ_ind[boolz.reshape(-1)]

scaler = preprocessing.StandardScaler().fit(cv_train)
cv_train = scaler.transform(cv_train)
cv_val = scaler.transform(train_data_logs)

batch_size = 64
lookback = 14995
offset = 15000

if np.max(mod_eval_index) &gt; len(train_data_logs):  # Prevents from dividing twice on accident when re-running code
    mod_eval_index = mod_eval_index // 10
train_gen = my_classes_tf.DataGenerator(data=cv_train,
                                        targets=cv_targets,
                                        indices=cv_eqs,
                                        min_index=0,
                                        max_index=None,
                                        batch_size=batch_size,
                                        lookback=lookback,
                                        offset=offset,
                                        shuffle_start=True,
                                        shuffle_feed=True)

val_gen = my_classes_tf.ValDataGenerator(data=cv_val,
                                         targets=targets,
                                         eval_index=mod_eval_index,
                                         cv_index=mod_cv_index,
                                         cv=fold,
                                         batch_size=batch_size,
                                         lookback=lookback)


class CRNN(tf.keras.Model):
    def __init__(self):
        super(CRNN, self).__init__()
        # Consider LocallyConnected1D
        self.conv1 = tf.keras.layers.Conv1D(filters=32, kernel_size=50, strides=1, padding='same',
                                            activation=None, kernel_initializer='he_uniform', name='conv1a')
        self.pool1 = tf.keras.layers.MaxPool1D(pool_size=100, strides=None, name='pool1')
        self.gru1 = tf.keras.layers.GRU(units=32, name='gru1')
        self.dense1 = tf.keras.layers.Dense(units=16, activation=None, name='dense1')
        self.output1 = tf.keras.layers.Dense(units=1, activation='relu', name='output1')
        self.lrelu = tf.keras.layers.LeakyReLU(alpha=0.1)
        self.mae = tf.keras.losses.MeanAbsoluteError()
        self.optimizer = tf.keras.optimizers.SGD(lr=1e-3, momentum=0, nesterov=True)

    def call(self, inputs):
        x = self.conv1(inputs)
        x = self.lrelu(x)
        x = self.pool1(x)
        x = self.gru1(x)
        x = self.dense1(x)
        x = self.lrelu(x)
        return self.output1(x)

    def train_step(self, sample, label):
        with tf.GradientTape() as tape:
            predictions = self.call(sample)
            loss = self.mae(label, predictions)
        gradients = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))
        self.train_loss(loss)

    def eval_once(self, sample, label):
        predictions = self.call(sample)
        loss = self.mae(label, predictions)
        self.eval_loss(loss)

    def train(self, num_epochs):
        self.train_loss = tf.keras.metrics.Mean(name='train_loss')
        self.eval_loss = tf.keras.metrics.Mean(name='eval_loss')
        self.store_gradients = np.empty((num_epochs, ))
        for epoch in range(num_epochs):
            start_time = time.time()
            self.train_loss.reset_states()
            self.eval_loss.reset_states()
            for samples, labels in train_gen:
                self.train_step(samples, labels)
            train_gen.on_epoch_end()
            for samples, labels in val_gen:
                self.eval_once(samples, labels)
            print('Epoch: {0}, Time: {1:.2f}, Train Loss: {2:.2f}, Test Loss: {3:.2f}'.format(epoch + 1,
                                                                                              time.time() - start_time,
                                                                                              self.train_loss.result(),
                                                                                              self.eval_loss.result()))


tf.keras.backend.clear_session()
model = CRNN()
model.train(20)

model2 = CRNN()
model2.compile(optimizer=tf.keras.optimizers.SGD(lr=1e-3, momentum=0, nesterov=True),
               loss='mae')

history = model2.fit_generator(generator=train_gen,
                               validation_data=val_gen,
                               epochs=20,
                               workers=1,
                               use_multiprocessing=False,
                               verbose=2,
                               callbacks=[])

# https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/engine/training_eager.py
# Check this ^ to see what is different between keras fit_generator and your fit
model3 = CRNN()
model3.compile(optimizer=model3.optimizer,
               loss=model3.mae)
history3 = model3.fit_generator(generator=train_gen,
                                validation_data=val_gen,
                                epochs=20,
                                workers=1,
                                use_multiprocessing=False,
                                verbose=2,
                                callbacks=[])
&lt;/denchmark-code&gt;

`
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
I have tested to see that this model can overfit by training it on only a single sample, and it is able to overfit in both scenarios. I am using custom generators of the class tf.keras.utils.Sequence, but they work equivalently under each training scenario. Let me know if you need any additional information in order to help me out with this issue! Thanks!
Attached the custom generators as well as some of the data in text file format (the larger files are too large to upload, but you can use the data from here: &lt;denchmark-link:https://www.kaggle.com/c/LANL-Earthquake-Prediction/data&gt;https://www.kaggle.com/c/LANL-Earthquake-Prediction/data&lt;/denchmark-link&gt;
 along with the Build_Features R script I uploaded).
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3152705/Build_Features.txt&gt;Build_Features.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3152692/Validation.Indices.Original.txt&gt;Validation Indices Original.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3152693/Validation.Indices.Modified.txt&gt;Validation Indices Modified.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3152664/my_classes.txt&gt;my_classes.txt&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='Atrus619' date='2019-05-07T11:21:52Z'>
		&lt;denchmark-link:https://github.com/Atrus619&gt;@Atrus619&lt;/denchmark-link&gt;
 Please provide the whole set libraries included in this attached code. We request the complete code to reproduce the issue.
		</comment>
		<comment id='2' author='Atrus619' date='2019-05-07T12:56:19Z'>
		Updated with the requested information - let me know if you have any additional questions, thanks!
		</comment>
		<comment id='3' author='Atrus619' date='2019-05-07T14:14:45Z'>
		&lt;denchmark-link:https://github.com/Atrus619&gt;@Atrus619&lt;/denchmark-link&gt;
 I ran the code, it says ModuleNotFoundError: No module named 'my_classes_tf'. As this is custom written code, This question is better asked on &lt;denchmark-link:http://stackoverflow.com/questions/tagged/tensorflow&gt;StackOverflow&lt;/denchmark-link&gt;
 since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/new/choose&gt;issue template&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='4' author='Atrus619' date='2019-05-07T15:05:16Z'>
		To the best of my knowledge, this issue is independent of the custom generator (and the generator has been provided as an attachment, see my_classes.txt). I'm sure this could be replicated if the custom generator was replaced with a native tensorflow/keras generator or even something simpler. Would it be helpful to you if I pulled that together and reposted on here?
		</comment>
		<comment id='5' author='Atrus619' date='2019-05-09T14:18:51Z'>
		&lt;denchmark-link:https://github.com/Atrus619&gt;@Atrus619&lt;/denchmark-link&gt;
 Please provide a smaller reproducible code so that it will be easy to debug. Thanks!
		</comment>
		<comment id='6' author='Atrus619' date='2019-05-12T17:05:56Z'>
		&lt;denchmark-link:https://github.com/muddham&gt;@muddham&lt;/denchmark-link&gt;
 Here is one standalone script that replicates this issue. I am leaving the original code at the top for the time being for reference (to demonstrate the analogy at play), but if you would prefer I completely replace it, I have no problem doing just that.
Short description of analogy:
I wrote a very simple generator to replicate the tf.keras.utils.Sequence class I am using for my custom generator. The data set is MNIST for ease of use, since it is a tensorflow data set. Interestingly enough, I set up two modes for the CRNN class, one for mae and one for sparse categorical crossentropy, and it seems like this issue only occurs for mae. The results of a run I just did:
Custom loop and MAE error: 2.518
Keras loop and MAE error: 1.307
Custom loop and SCC error: 1.468
Keras loop and SCC error: 1.593
Essentially the custom loop with MAE error ceased to train after only an epoch or two. I realize that MNIST is technically a multilabel classification problem, but it seemed like a good example you will be more familiar with. Feel free to recommend an alternate data set to demonstrate this issue. Also, it is possible the initialization fails and the model ceases to train at all, but as long as you re-run it once it should be fine.
Code - This standalone script should run just fine as long as you are running tf 2.0:
import time
import numpy as np
import tensorflow_datasets as tfds
import tensorflow as tf
&lt;denchmark-code&gt;# Construct a tf.data.Dataset
ds_train, ds_test = tfds.load(name="mnist", split=["train", "test"])


# Generator
class SampleGenerator(tf.keras.utils.Sequence):
    def __init__(self, data, epoch_len, batch_size, shuffle=False):
        self.data = data
        self.epoch_len = epoch_len
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.this_isnt_efficient_images = np.empty([self.epoch_len, self.batch_size, 28, 28, 1])
        self.this_isnt_efficient_labels = np.empty([self.epoch_len, self.batch_size])
        self.on_epoch_end()

    def __len__(self):
        return self.epoch_len

    def on_epoch_end(self):
        data_temp = self.data
        if self.shuffle:
            data_temp = self.data.shuffle(60000)
        data_temp_batched = data_temp.batch(self.batch_size)
        i = 0
        for features in data_temp_batched:
            if i &lt; self.epoch_len:
                self.this_isnt_efficient_images[i], self.this_isnt_efficient_labels[i] = features['image'], features['label']
                i += 1

    def __getitem__(self, index):
        return tf.convert_to_tensor(self.this_isnt_efficient_images[index], dtype=tf.float32), \
               tf.convert_to_tensor(self.this_isnt_efficient_labels[index], dtype=tf.int16)


train_gen = SampleGenerator(ds_train, 400, 128, shuffle=True)
val_gen = SampleGenerator(ds_test, 20, 128)


# Build CRNN class
class CRNN(tf.keras.Model):
    def __init__(self, mode):
        super(CRNN, self).__init__()
        # Consider LocallyConnected1D
        self.conv1 = tf.keras.layers.Conv1D(filters=32, kernel_size=50, strides=1, padding='same',
                                            activation=None, kernel_initializer='he_uniform', name='conv1a')
        self.pool1 = tf.keras.layers.MaxPool1D(pool_size=100, strides=None, name='pool1')
        self.gru1 = tf.keras.layers.GRU(units=32, name='gru1')
        self.dense1 = tf.keras.layers.Dense(units=16, activation=None, name='dense1')

        self.lrelu = tf.keras.layers.LeakyReLU(alpha=0.1)
        self.flatten = tf.keras.layers.Flatten()
        if mode == "mae":
            self.output1 = tf.keras.layers.Dense(units=1, activation=tf.keras.activations.relu, name='output1')
            self.loss = tf.keras.losses.MeanAbsoluteError()
        elif mode == "scc":
            self.output1 = tf.keras.layers.Dense(units=10, activation=tf.keras.activations.softmax, name='output1')
            self.loss = tf.keras.losses.SparseCategoricalCrossentropy()
        self.optimizer = tf.optimizers.SGD(lr=1e-3, momentum=0, nesterov=True)

    def call(self, inputs):
        x = tf.reshape(self.flatten(inputs), [128, 784, 1])
        x = self.conv1(x)
        x = self.lrelu(x)
        x = self.pool1(x)
        x = self.gru1(x)
        x = self.dense1(x)
        x = self.lrelu(x)
        return self.output1(x)

    def train_step(self, sample, label):
        with tf.GradientTape() as tape:
            predictions = self.call(sample)
            loss = self.loss(label, predictions)
        gradients = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))
        self.train_loss(loss)

    def eval_once(self, sample, label):
        predictions = self.call(sample)
        loss = self.loss(label, predictions)
        self.eval_loss(loss)

    def train(self, num_epochs):
        self.train_loss = tf.metrics.Mean(name='train_loss')
        self.eval_loss = tf.metrics.Mean(name='eval_loss')
        self.store_gradients = np.empty((num_epochs,))
        for epoch in range(num_epochs):
            start_time = time.time()
            self.train_loss.reset_states()
            self.eval_loss.reset_states()
            for samples, labels in train_gen:
                self.train_step(samples, labels)
            train_gen.on_epoch_end()
            for samples, labels in val_gen:
                self.eval_once(samples, labels)
            print('Epoch: {0}, Time: {1:.2f}, Train Loss: {2:.2f}, Test Loss: {3:.2f}'.format(epoch + 1,
                                                                                              time.time() - start_time,
                                                                                              self.train_loss.result(),
                                                                                              self.eval_loss.result()))


# Train
# Custom loop and MAE error - 2.52
tf.keras.backend.clear_session()
model_custom_mae = CRNN(mode='mae')
model_custom_mae.train(20)
# Keras fit_generator and MAE error - 1.307
tf.keras.backend.clear_session()
model_keras_mae = CRNN(mode='mae')
model_keras_mae.compile(optimizer=tf.optimizers.SGD(lr=1e-3, momentum=0, nesterov=True),
                        loss='mae')
history1 = model_keras_mae.fit_generator(generator=train_gen,
                                         validation_data=val_gen,
                                         epochs=20,
                                         workers=1,
                                         use_multiprocessing=False,
                                         verbose=2,
                                         callbacks=[])
# Custom loop and SCC error - 1.468
tf.keras.backend.clear_session()
model_custom_scc = CRNN(mode='scc')
model_custom_scc.train(20)
# Keras fit_generator and SCC error - 1.593
tf.keras.backend.clear_session()
model_keras_scc = CRNN(mode='scc')
model_keras_scc.compile(optimizer=tf.optimizers.SGD(lr=1e-3, momentum=0, nesterov=True),
                        loss='sparse_categorical_crossentropy')
history2 = model_keras_scc.fit_generator(generator=train_gen,
                                         validation_data=val_gen,
                                         epochs=20,
                                         workers=1,
                                         use_multiprocessing=False,
                                         verbose=2,
                                         callbacks=[])
# Print outputs
print("Custom loop and MAE error: {0:.3f}".format(model_custom_mae.eval_loss.result().numpy()))
print("Keras loop and MAE error: {0:.3f}".format(history1.history['val_loss'][len(history1.history['val_loss'])-1]))
print("Custom loop and SCC error: {0:.3f}".format(model_custom_scc.eval_loss.result().numpy()))
print("Keras loop and SCC error: {0:.3f}".format(history2.history['val_loss'][len(history2.history['val_loss'])-1]))
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='Atrus619' date='2019-05-14T09:25:14Z'>
		&lt;denchmark-link:https://github.com/Atrus619&gt;@Atrus619&lt;/denchmark-link&gt;
 Able to reproduce the issue with the provided code in TF
Custom loop and MAE error: 2.516
Keras loop and MAE error: 1.274
Custom loop and SCC error: 1.518
Keras loop and SCC error: 1.240
		</comment>
		<comment id='8' author='Atrus619' date='2019-05-22T12:32:52Z'>
		Hi &lt;denchmark-link:https://github.com/Atrus619&gt;@Atrus619&lt;/denchmark-link&gt;
,
maybe the problem is caused by broadcasting in your loss function in the custom loop. Make sure that the dimensions of  and  is equal. At the moment (for MAE) they are  and . Just make use  of  or .
I've tested it with predictions = tf.squeeze(predictions) in the train_step and eval_once only for MAE. Results are after 20 epochs now:
Custom loop and MAE error: 1.413
Keras loop and MAE error: 1.261
Custom loop and SCC error: 1.327
Keras loop and SCC error: 1.390
Hope I could help,
Alex
		</comment>
		<comment id='9' author='Atrus619' date='2019-06-06T17:03:52Z'>
		Closing this out since I understand it to be resolved by &lt;denchmark-link:https://github.com/AlexanderBartler&gt;@AlexanderBartler&lt;/denchmark-link&gt;
 suggestion, but please let me know if I'm mistaken.
		</comment>
		<comment id='10' author='Atrus619' date='2019-06-06T17:03:55Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=28394&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=28394&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='Atrus619' date='2019-06-06T19:52:57Z'>
		Yes, I guess I didn't notice the response 15 days ago, but this was totally the issue, guess I learned a valuable lesson about broadcasting, thanks &lt;denchmark-link:https://github.com/AlexanderBartler&gt;@AlexanderBartler&lt;/denchmark-link&gt;
!
		</comment>
	</comments>
</bug>