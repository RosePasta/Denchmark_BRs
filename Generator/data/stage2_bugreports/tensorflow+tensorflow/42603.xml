<bug id='42603' author='sagar-m' open_date='2020-08-23T14:56:46Z' closed_time='2020-09-15T19:41:09Z'>
	<summary>NotImplementedError: TPUStrategy.run(fn, ...) does not support pure eager execution. please make sure the function passed into `strategy.run` is a `tf.function` or `strategy.run` is called inside a `tf.function` if eager behavior is enabled.</summary>
	<description>
Hi, I pretty new at this. The following is  my code and the error. Thank you for your help. Posted on stackoverflow too.
`
#WITH TPU
latent_dim = 2 #number of latent variables to learn
hidden_size = 32
input_dim = x_train.shape[1]
latent_dim = 3 # d, dimensionality of the latent code t.
intermediate_dim = 256 # Size of the hidden layer.
def create_model():
x = Input(shape=(input_dim,))
t = BatchNormalization()(x)
t = Dense(intermediate_dim, activation='relu' , name='encoder_hidden')(t)
t = BatchNormalization()(t)
z_mean = Dense(latent_dim, name='z_mean')(t)
z_log_var = Dense(latent_dim, name='z_log_var')(t)
def sampling(args):
z_mean, z_log_var = args
epsilon = tf.random.normal(shape=tf.shape(z_mean), mean=0., stddev=1.,name="epsilon")
return z_mean + tf.exp(z_log_var / 2) * epsilon
z = Lambda(sampling, name='z_sampled')([z_mean, z_log_var])
t = Dense(intermediate_dim, activation='relu', name='decoder_hidden')(z)
decoded_mean = Dense(input_dim, activation=None, name='decoded_mean')(t)
def kl_loss(y_true, y_pred):
return - 0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)
def rec_loss(y_true, y_pred):
return tf.reduce_sum(tf.square(y_true - y_pred), axis=-1)
def vae_loss(x, decoded_mean):
rec_loss = tf.reduce_sum(tf.square(x - decoded_mean), axis=-1)
kl_loss = - 0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)
return K.mean((rec_loss + kl_loss) / 2)
vae = Model(x, decoded_mean)
return vae
with strategy.scope():
vae = create_model()
#vae.compile(optimizer=tf.keras.optimizers.Nadam(), loss=negloglik)
vae.compile(optimizer=Adam(lr=1e-2), loss=vae_loss, metrics=[rec_loss, kl_loss])
vae.summary()
n_epochs = 30
batch_size = 128
early_stopping = EarlyStopping(monitor='loss', patience=10, min_delta=1e-5) #stop training if loss does not decrease with at least 0.00001
reduce_lr = ReduceLROnPlateau(monitor='loss', patience=5, min_delta=1e-5, factor=0.2) #reduce learning rate (divide it by 5 = multiply it by 0.2) if loss does not decrease with at least 0.00001
callbacks = [early_stopping, reduce_lr]
tf.config.experimental_run_functions_eagerly(True)
tf_train = tf.data.Dataset.from_tensor_slices((x_train, x_train)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE).shuffle(int(10e4))
tf_val = tf.data.Dataset.from_tensor_slices((x_val, x_val)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE).shuffle(int(10e4))
hist = vae.fit(tf_train,
validation_data=tf_val,
shuffle=True,
verbose=0,
#batch_size=batch_size,
epochs=n_epochs,
callbacks=callbacks)
plot_loss(hist)
 
Model: "functional_39"
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;Layer (type)                    Output Shape         Param #     Connected to&lt;/denchmark-h&gt;

input_21 (InputLayer)           [(None, 30)]         0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

batch_normalization_40 (BatchNo (None, 30)           120         input_21[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

encoder_hidden (Dense)          (None, 256)          7936        batch_normalization_40[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

batch_normalization_41 (BatchNo (None, 256)          1024        encoder_hidden[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

z_mean (Dense)                  (None, 3)            771         batch_normalization_41[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

z_log_var (Dense)               (None, 3)            771         batch_normalization_41[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

z_sampled (Lambda)              (None, 3)            0           z_mean[0][0]
z_log_var[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

decoder_hidden (Dense)          (None, 256)          1024        z_sampled[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;decoded_mean (Dense)            (None, 30)           7710        decoder_hidden[0][0]&lt;/denchmark-h&gt;

Total params: 19,356
Trainable params: 18,784
Non-trainable params: 572
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3350: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.
"Even though the tf.config.experimental_run_functions_eagerly "&lt;/denchmark-h&gt;

NotImplementedError                       Traceback (most recent call last)
 in ()
68                  #batch_size=batch_size,
69                  epochs=n_epochs,
---&gt; 70                  callbacks=callbacks)
71
72   plot_loss(hist)
5 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py in validate_run_function(fn)
95       and not (callable(fn) and isinstance(fn.call, def_function.Function)):
96     raise NotImplementedError(
---&gt; 97         "TPUStrategy.run(fn, ...) does not support pure eager "
98         "execution. please make sure the function passed into "
99         "strategy.run is a tf.function or "
NotImplementedError: TPUStrategy.run(fn, ...) does not support pure eager execution. please make sure the function passed into strategy.run is a tf.function or strategy.run is called inside a tf.function if eager behavior is enabled.
`
	</description>
	<comments>
		<comment id='1' author='sagar-m' date='2020-08-23T15:06:28Z'>
		&lt;denchmark-link:https://stackoverflow.com/questions/63548506/notimplementederror-tpustrategy-runfn-does-not-support-pure-eager-execut&gt;https://stackoverflow.com/questions/63548506/notimplementederror-tpustrategy-runfn-does-not-support-pure-eager-execut&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='sagar-m' date='2020-08-23T21:52:50Z'>
		Hi &lt;denchmark-link:https://github.com/sagar-m&gt;@sagar-m&lt;/denchmark-link&gt;
, as the error message states TPUStrategy does not work with pure eager execution. As a first step, try commenting out the line 
		</comment>
		<comment id='3' author='sagar-m' date='2020-08-24T07:05:34Z'>
		Ok I did comment out that line. I am getting a new error. Please can you guide why. Thank you!
&lt;denchmark-code&gt;#WITH TPU

latent_dim = 2 #number of latent variables to learn
hidden_size = 32
input_dim = x_train.shape[1]
latent_dim = 3 # d, dimensionality of the latent code t.
intermediate_dim = 256 # Size of the hidden layer.


def create_model():
  x = Input(shape=(input_dim,))
  t = BatchNormalization()(x)
  t = Dense(intermediate_dim, activation='relu' , name='encoder_hidden')(t)
  t = BatchNormalization()(t)

  z_mean = Dense(latent_dim, name='z_mean')(t)
  z_log_var = Dense(latent_dim, name='z_log_var')(t)

  def sampling(args):
    z_mean, z_log_var = args
    epsilon = tf.random.normal(shape=tf.shape(z_mean), mean=0., stddev=1.,name="epsilon")
    return z_mean + tf.exp(z_log_var / 2) * epsilon

  z = Lambda(sampling, name='z_sampled')([z_mean, z_log_var])

  t = Dense(intermediate_dim, activation='relu', name='decoder_hidden')(z)

  decoded_mean = Dense(input_dim, activation=None, name='decoded_mean')(t)

  def kl_loss(y_true, y_pred):
    kl_loss_ = - 0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)
    return kl_loss_

  def rec_loss(y_true, y_pred):
    rec_loss_ = tf.reduce_sum(tf.square(y_true - y_pred), axis=-1)
    return rec_loss_   

  def vae_loss(x, decoded_mean):
    rec_loss_ = tf.reduce_sum(tf.square(x - decoded_mean), axis=-1)
    kl_loss_ = - 0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)
    vae_loss_ = K.mean((rec_loss + kl_loss) / 2)
    return vae_loss_

  vae = Model(x, decoded_mean)

  return vae
  

with strategy.scope():
  vae = create_model()
  vae.compile(optimizer=Adam(lr=1e-2), loss=vae_loss_, metrics=[rec_loss, kl_loss])
  #vae.compile(optimizer=tf.keras.optimizers.Nadam(), loss=negloglik)
  vae.summary()
  n_epochs = 30
  batch_size = 128

  early_stopping = EarlyStopping(monitor='loss', patience=10, min_delta=1e-5) #stop training if loss does not decrease with at least 0.00001
  reduce_lr = ReduceLROnPlateau(monitor='loss', patience=5, min_delta=1e-5, factor=0.2) #reduce learning rate (divide it by 5 = multiply it by 0.2) if loss does not decrease with at least 0.00001

  callbacks = [early_stopping, reduce_lr]

  #tf.config.experimental_run_functions_eagerly(True)

  tf_train = tf.data.Dataset.from_tensor_slices((x_train, x_train)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE).shuffle(int(10e4))
  tf_val = tf.data.Dataset.from_tensor_slices((x_val, x_val)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE).shuffle(int(10e4))

  hist = vae.fit(tf_train,
                 validation_data=tf_val,
                 shuffle=True,
                 verbose=0,
                 #batch_size=batch_size, 
                 epochs=n_epochs,
                 callbacks=callbacks)
  
  plot_loss(hist)
&lt;/denchmark-code&gt;

Error...
&lt;denchmark-code&gt;---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
&lt;ipython-input-14-a9d277df43a1&gt; in &lt;module&gt;()
     49 with strategy.scope():
     50   vae = create_model()
---&gt; 51   vae.compile(optimizer=Adam(lr=1e-2), loss=vae_loss_, metrics=[rec_loss, kl_loss])
     52   #vae.compile(optimizer=tf.keras.optimizers.Nadam(), loss=negloglik)
     53   vae.summary()

NameError: name 'vae_loss_' is not defined
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='sagar-m' date='2020-08-24T14:21:27Z'>
		The error message says vae_loss_ is not defined and you'll notice that you have not defined vae_loss_ in your program. You have a function called vae_loss (ie without the extra underscore at the end), so perhaps you meant to pass vae_loss  to your compile function instead of vae_loss_.
		</comment>
		<comment id='5' author='sagar-m' date='2020-08-24T16:32:38Z'>
		Hi, thank you so much for pointing out... I fixed the above vae loss error now. Yet, it is getting stuck.
&lt;denchmark-code&gt;#WITH TPU

latent_dim = 2 #number of latent variables to learn
hidden_size = 32
input_dim = x_train.shape[1]
latent_dim = 3 # d, dimensionality of the latent code t.
intermediate_dim = 256 # Size of the hidden layer.


def create_model():
  x = Input(shape=(input_dim,))
  t = BatchNormalization()(x)
  t = Dense(intermediate_dim, activation='relu' , name='encoder_hidden')(t)
  t = BatchNormalization()(t)

  z_mean = Dense(latent_dim, name='z_mean')(t)
  z_log_var = Dense(latent_dim, name='z_log_var')(t)

  def sampling(args):
    z_mean, z_log_var = args
    epsilon = tf.random.normal(shape=tf.shape(z_mean), mean=0., stddev=1.,name="epsilon")
    return z_mean + tf.exp(z_log_var / 2) * epsilon

  z = Lambda(sampling, name='z_sampled')([z_mean, z_log_var])

  t = Dense(intermediate_dim, activation='relu', name='decoder_hidden')(z)

  decoded_mean = Dense(input_dim, activation=None, name='decoded_mean')(t)

  def kl_loss(y_true, y_pred):
    kl_loss_ = - 0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)
    return kl_loss_

  def rec_loss(y_true, y_pred):
    rec_loss_ = tf.reduce_sum(tf.square(y_true - y_pred), axis=-1)
    return rec_loss_   

  def vae_loss(x, decoded_mean):
    rec_loss_ = tf.reduce_sum(tf.square(x - decoded_mean), axis=-1)
    kl_loss_ = - 0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)
    vae_loss_ = K.mean((rec_loss + kl_loss) / 2)
    return vae_loss_

  vae = Model(x, decoded_mean)

  return vae
  

with strategy.scope():
  vae = create_model()
  vae.compile(optimizer=Adam(lr=1e-2), loss=vae_loss, metrics=[rec_loss, kl_loss])
  #vae.compile(optimizer=tf.keras.optimizers.Nadam(), loss=negloglik)
  vae.summary()
  n_epochs = 30
  batch_size = 128

  early_stopping = EarlyStopping(monitor='loss', patience=10, min_delta=1e-5) #stop training if loss does not decrease with at least 0.00001
  reduce_lr = ReduceLROnPlateau(monitor='loss', patience=5, min_delta=1e-5, factor=0.2) #reduce learning rate (divide it by 5 = multiply it by 0.2) if loss does not decrease with at least 0.00001

  callbacks = [early_stopping, reduce_lr]

  #tf.config.experimental_run_functions_eagerly(True)

  tf_train = tf.data.Dataset.from_tensor_slices((x_train, x_train)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE).shuffle(int(10e4))
  tf_val = tf.data.Dataset.from_tensor_slices((x_val, x_val)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE).shuffle(int(10e4))

  hist = vae.fit(tf_train,
                 validation_data=tf_val,
                 shuffle=True,
                 verbose=0,
                 #batch_size=batch_size, 
                 epochs=n_epochs,
                 callbacks=callbacks)
  
&lt;/denchmark-code&gt;

Error
&lt;denchmark-code&gt;Model: "functional_5"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            [(None, 30)]         0                                            
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 30)           120         input_3[0][0]                    
__________________________________________________________________________________________________
encoder_hidden (Dense)          (None, 256)          7936        batch_normalization_4[0][0]      
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 256)          1024        encoder_hidden[0][0]             
__________________________________________________________________________________________________
z_mean (Dense)                  (None, 3)            771         batch_normalization_5[0][0]      
__________________________________________________________________________________________________
z_log_var (Dense)               (None, 3)            771         batch_normalization_5[0][0]      
__________________________________________________________________________________________________
z_sampled (Lambda)              (None, 3)            0           z_mean[0][0]                     
                                                                 z_log_var[0][0]                  
__________________________________________________________________________________________________
decoder_hidden (Dense)          (None, 256)          1024        z_sampled[0][0]                  
__________________________________________________________________________________________________
decoded_mean (Dense)            (None, 30)           7710        decoder_hidden[0][0]             
==================================================================================================
Total params: 19,356
Trainable params: 18,784
Non-trainable params: 572
__________________________________________________________________________________________________
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---&gt; 60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:

TypeError: An op outside of the function building code is being passed
a "Graph" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: z_log_var/BiasAdd:0

During handling of the above exception, another exception occurred:

_SymbolicException                        Traceback (most recent call last)
9 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     72       raise core._SymbolicException(
     73           "Inputs to eager execution function cannot be Keras symbolic "
---&gt; 74           "tensors, but found {}".format(keras_symbolic_tensors))
     75     raise e
     76   # pylint: enable=protected-access

_SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [&lt;tf.Tensor 'z_log_var/BiasAdd:0' shape=(None, 3) dtype=float32&gt;, &lt;tf.Tensor 'z_mean/BiasAdd:0' shape=(None, 3) dtype=float32&gt;]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='sagar-m' date='2020-08-24T18:06:05Z'>
		Hi &lt;denchmark-link:https://github.com/sagar-m&gt;@sagar-m&lt;/denchmark-link&gt;
, I don't think this is a bug. It's difficult to tell where the error is in your code since it's not fully reproducible. But here are a few tips: firstly, I would suggest making sure your code runs properly without TPU strategy, that will help narrow down where the problem is. Secondy, I suggest you swap out your lambda layer for a custom layer. Lambda layers are good for experimentation and simple operations, but for a more complicated case subclassing tf.keras.layers.Layer is preferred.
As for the error you're seeing, I suspect that it's caused because you're passing in x (which is the model.input, and a symbolic tensor) to your loss function, which is expecting an eager tensor. Hope this helps you to debug the issue!
		</comment>
		<comment id='7' author='sagar-m' date='2020-08-25T03:07:53Z'>
		Hi &lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;
  thank you for all the tips! FYI - the code does run properly without TPU, i have tested it. Was really hoping to use the TPU!
		</comment>
		<comment id='8' author='sagar-m' date='2020-08-27T11:20:53Z'>
		Hi &lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;
  could you please share the exact line of code I could use to replace the Lambda. Thank you!
		</comment>
		<comment id='9' author='sagar-m' date='2020-08-28T04:49:20Z'>
		&lt;denchmark-link:https://github.com/sagar-m&gt;@sagar-m&lt;/denchmark-link&gt;

Can you please let us know the tf version you are facing this issue on.
		</comment>
		<comment id='10' author='sagar-m' date='2020-08-28T09:14:58Z'>
		print(tf.version)
print(tfp.version)
2.3.0
0.11.0
		</comment>
		<comment id='11' author='sagar-m' date='2020-09-01T18:20:33Z'>
		&lt;denchmark-link:https://github.com/sagar-m&gt;@sagar-m&lt;/denchmark-link&gt;
 it's difficult to provide the exact lines without having fully reproducible code. You can &lt;denchmark-link:https://www.tensorflow.org/tutorials/customization/custom_layers&gt;follow this tutorial&lt;/denchmark-link&gt;
 on how to write a custom layer in Tensorflow. In your custom layer you will compute the same thing you are computing with the lambda layer. Additionally, as I mentioned before, I suspect that the error you're seeing caused because you're passing in x (which is the model.input, and a symbolic tensor) to your loss function, which is expecting an eager tensor.
Github is a place for filing bugs. Since you are not facing a bug, you might have better luck on Stack Overflow as there is a larger community to provide support.
		</comment>
		<comment id='12' author='sagar-m' date='2020-09-08T19:16:25Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='13' author='sagar-m' date='2020-09-15T19:41:07Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='14' author='sagar-m' date='2020-09-15T19:41:10Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42603&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42603&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>