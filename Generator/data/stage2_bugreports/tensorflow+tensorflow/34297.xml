<bug id='34297' author='MikeOfZen' open_date='2019-11-15T01:59:48Z' closed_time='2019-11-21T23:39:00Z'>
	<summary>tf.function hangs on ragged tensor input</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):No
TensorFlow installed from (source or binary):Colab
TensorFlow version (use command below):2.0.0
Python version:3.6
GPU model and memory:None

Describe the current behavior
When using tf.function with a number of for loops on a RaggedTensor, the function call hangs (I stopped waiting after half hour).
-when running the function directly (no tf.function decorator), the function executes immediately
-when converting the ragged tensor to dense tensor, the function executes immediately.
I couldn't pinpoint the exact combination of operations that causes this Autograph behavior, but I tried to reduce my code to the simplest combination that still causes this behavior.
I struggled for hours with my own code, trying to get tf.function to work, until I figured it was due to the ragged tensor + for loops + tf.function hanging the kernel
I observed similar behavior on my machine and a colab machine as well.
Describe the expected behavior
Should execute in a comparable time to a dense tensor.
Code to reproduce the issue
&lt;denchmark-code&gt;%tensorflow_version 2.x
import tensorflow as tf
import numpy as np

inp=tf.ragged.constant(np.arange(1000,dtype=np.float32).reshape(10,10,10))

@tf.function    #if not using tf.function it runs well
def ragged_example(r_tensor):
  s=tf.constant(0.0)
  for i in tf.range(10):
    inner=r_tensor[i]
    for x in inner:
      b=tf.reduce_sum(x)
      s=s+b
  return s

#inp=inp.to_tensor() #if this is uncommented, it runs well

ragged_example(inp) #this hangs
&lt;/denchmark-code&gt;

Other observations
Also noted very large memory footprint as a result (9gb and growing)
	</description>
	<comments>
		<comment id='1' author='MikeOfZen' date='2019-11-18T11:22:31Z'>
		Could reproduce the issue with TF Version 2.0. Here is the &lt;denchmark-link:https://colab.sandbox.google.com/gist/rmothukuru/3a286f4b25e62d041d0030e80a4e1cb2/34297.ipynb&gt;Gist&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='MikeOfZen' date='2019-11-18T21:05:00Z'>
		This bug is a combination of several problems that we'll need to resolve, fopefully in 2.2. In the mean time, a workaround is to avoid writing for loops over ragged tensors (e.g. for x in ragged_tensor). This means rewriting your code like so:
&lt;denchmark-code&gt;def ragged_example(r_tensor):
  s = tf.constant(0.0)
  for i in tf.range(10):
    inner = r_tensor[i]
    for j in tf.range(inner.row_lengths()[0]):  #  `for x in inner` doesn't work yet.
      x = inner[j]
      b = tf.reduce_sum(x)
      s = s + b
  return s
&lt;/denchmark-code&gt;

A few more details on the cause of the bug --
Autograph doesn't currently support iterating over , and it thinks they are normal Python objects. We'll fix that in autograph, hopefully by TF 2.2.
Now, this should have normally generated an error (something in the lines of "cannot directly iterate over RaggedTensor in graph mode"). But it seems that the  class is iterable and the iterator it returns hangs when consumed in graph mode. &lt;denchmark-link:https://github.com/edloper&gt;@edloper&lt;/denchmark-link&gt;
 I think this iterator should error out in  instead?
Edit: fixed the index in the example
		</comment>
		<comment id='3' author='MikeOfZen' date='2019-11-18T21:19:38Z'>
		Thank you for the update and the details.
		</comment>
		<comment id='4' author='MikeOfZen' date='2019-11-21T23:39:01Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34297&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34297&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='MikeOfZen' date='2019-11-22T14:11:17Z'>
		The PR that just got merged should fix the issue in tf.function.
As a side note, we should still disallow RaggedTensor.iter in graph code (or make it return a proper graph-based iterator).
		</comment>
	</comments>
</bug>