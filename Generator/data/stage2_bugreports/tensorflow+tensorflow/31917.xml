<bug id='31917' author='jurneo' open_date='2019-08-23T06:40:04Z' closed_time='2019-09-18T01:33:46Z'>
	<summary>Intermittent crash with CUDA_ERROR_LAUNCH_FAILED failure</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution:
on (Linux-based) ML-engine runtime version 1.12, which means it is using TensorFlow 1.12, with Python 3.6.
CUDA/cuDNN version:
CUDA Version: 10.1, with cuDNN: libcudnn.so.7.4.2
GPU model and memory:
Tesla K80
Exact command to reproduce:
running custom code on object segmentation task.

&lt;denchmark-h:h3&gt;The problem&lt;/denchmark-h&gt;

The problem is the training crash after reaching thousands of steps (few hours). The crash is intermittent. The same code and dataset when rerun is OK on another occasion. This problems happened quite frequently (&gt;20x) in my testings. One observation is that this does not happen for earlier TensorFlow 1.8. Current workaround for me is to downgrade to TF 1.8. This could be a bug on the library or CUDA driver.  The same problem happened in workstation offline .
&lt;denchmark-h:h3&gt;The Logs&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;Instructions for updating:
Use standard file APIs to delete files with this prefix.
INFO:tensorflow:Recording summary at step 1239.
INFO:tensorflow:global step 1240: loss = 0.2247 (1.349 sec/step)
INFO:tensorflow:global step 1260: loss = 0.2513 (1.189 sec/step)
INFO:tensorflow:global step 1280: loss = 0.3016 (1.181 sec/step)
INFO:tensorflow:Recording summary at step 1291.
INFO:tensorflow:global step 1300: loss = 0.2227 (1.151 sec/step)
INFO:tensorflow:global step 1320: loss = 0.2227 (1.139 sec/step)
INFO:tensorflow:global step 1340: loss = 0.2224 (1.273 sec/step)
INFO:tensorflow:Recording summary at step 1342.
INFO:tensorflow:global step 1360: loss = 0.2220 (1.186 sec/step)
INFO:tensorflow:global step 1380: loss = 0.2297 (1.157 sec/step)
INFO:tensorflow:Recording summary at step 1393.
INFO:tensorflow:global step 1400: loss = 0.2217 (1.160 sec/step)
INFO:tensorflow:global step 1420: loss = 0.2213 (1.169 sec/step)
INFO:tensorflow:global step 1440: loss = 0.2209 (1.134 sec/step)
INFO:tensorflow:Recording summary at step 1443.
2019-08-01 01:43:15.655519: E tensorflow/stream_executor/cuda/cuda_driver.cc:1131] failed to enqueue async memcpy from host to device: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure; GPU dst: 0x71133f800; host src: 0x7ff986bfcc80; size: 131072=0x20000
2019-08-01 01:43:15.655586: E tensorflow/stream_executor/cuda/cuda_driver.cc:1000] could not wait stream on event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2019-08-01 01:43:15.655630: E tensorflow/stream_executor/cuda/cuda_driver.cc:1000] could not wait stream on event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2019-08-01 01:43:15.655630: E tensorflow/stream_executor/cuda/cuda_driver.cc:1000] could not wait stream on event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2019-08-01 01:43:15.655644: E tensorflow/stream_executor/cuda/cuda_event.cc:48] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2019-08-01 01:43:15.655731: I tensorflow/stream_executor/stream.cc:5027] [stream=0x5c23080,impl=0x5c23120] did not memcpy host-to-device; source: 0x7ff97e0fd7c0
2019-08-01 01:43:15.655731: I tensorflow/stream_executor/stream.cc:5027] [stream=0x5c23080,impl=0x5c23120] did not memcpy host-to-device; source: 0x7ff97e6241c0
2019-08-01 01:43:15.655717: I tensorflow/stream_executor/stream.cc:5027] [stream=0x5c23080,impl=0x5c23120] did not memcpy host-to-device; source: 0x7ff97e62e9c0
2019-08-01 01:43:15.655755: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:274] Unexpected Event status: 1
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Additional information logs:&lt;/denchmark-h&gt;

The ML team support kindly provides machine debug information as followings:
&lt;denchmark-code&gt;"The VM logs of failed jobs all have the same error, like
 I 2019-07-25T17:00:20.282471416Z [37582.281240] NVRM: Xid (PCI:0000:00:04): 13, Graphics Exception:  MISSING_INLINE_DATA\r\n 
I 2019-07-25T17:00:20.282477291Z [37582.289153] NVRM: Xid (PCI:0000:00:04): 13, Graphics Exception: ESR 0x404600=0x80000002\r\n 
I 2019-07-25T17:00:20.282557397Z [37582.297688] NVRM: Xid (PCI:0000:00:04): 13, Graphics Exception: ChID 0017, Class 0000a1c0, Offset 000001b4, Data 00002000\r\n "
&lt;/denchmark-code&gt;

FYI, I did not succeed to reproduce the error with  cuda-memcheck or cuda-gdb or CUDA_DEVICE_WAITS_ON_EXCEPTION=1.
This issue is probably similar to #&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/20356&gt;#20356&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='jurneo' date='2019-08-26T22:24:30Z'>
		Did you build TF from sources for cuda 10.1 support? Also what is your nvidia driver version?
		</comment>
		<comment id='2' author='jurneo' date='2019-08-27T00:33:32Z'>
		No, I did not build the TF from the source code. I supposed the TF is preinstalled (when running on the Google AI-Platform).
The nvidia driver version from running nvidia-smi:
&lt;denchmark-code&gt; nvcc: NVIDIA (R) Cuda compiler driver 
 Copyright (c) 2005-2017 NVIDIA Corporation 
 Built on Fri_Sep__1_21:08:03_CDT_2017 
 Cuda compilation tools, release 9.0, V9.0.176 
 Mon Aug  5 06:56:55 2019        
 +-----------------------------------------------------------------------------+ 
 | NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     | 
 |-------------------------------+----------------------+----------------------+ 
 | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC | 
 | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. | 
 |===============================+======================+======================| 
 |   0  Tesla K80           On   | 00000000:00:04.0 Off |                    0 | 
 | N/A   37C    P0    79W / 149W |      0MiB / 11441MiB |      0%      Default | 
 +-------------------------------+----------------------+----------------------+ 
                                                                                 
 +-----------------------------------------------------------------------------+ 
 | Processes:                                                       GPU Memory | 
 |  GPU       PID   Type   Process name                             Usage      | 
 |=============================================================================| 
 |  No running processes found                                                 | 
 +-----------------------------------------------------------------------------+ 
 -rwxr-xr-x 1 root root 302770160 Jul 27 11:37 /usr/local/cuda/lib64/libcudnn.so 
 -rwxr-xr-x 1 root root 302770160 Jul 27 11:37 /usr/local/cuda/lib64/libcudnn.so.7 
 -rwxr-xr-x 1 root root 302770160 Jul 27 11:37 /usr/local/cuda/lib64/libcudnn.so.7.3.1 
 -rwxr-xr-x 1 root root 314634338 Jul 27 11:37 /usr/local/cuda/lib64/libcudnn_static.a 
&lt;/denchmark-code&gt;

To add extra info: this happens also for TF 1.13.1 although the test was not as extensive as TF 1.12. The specs for machine with TF 1.13.1 from nvidia-smi is as follows:
&lt;denchmark-code&gt; nvcc: NVIDIA (R) Cuda compiler driver I  
 Copyright (c) 2005-2018 NVIDIA Corporation I  
 Built on Sat_Aug_25_21:08:01_CDT_2018 I  
 Cuda compilation tools, release 10.0, V10.0.130 I  
 Mon Aug  5 07:04:42 2019        I  
 +-----------------------------------------------------------------------------+ I  
 | NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     | I  
 |-------------------------------+----------------------+----------------------+ I  
 | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC | I  
 | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. | I  
 |===============================+======================+======================| I  
 |   0  Tesla K80           On   | 00000000:00:04.0 Off |                    0 | I  
 | N/A   38C    P0    78W / 149W |      0MiB / 11441MiB |      0%      Default | I  
 +-------------------------------+----------------------+----------------------+ I  
                                                                                 I  
 +-----------------------------------------------------------------------------+ I  
 | Processes:                                                       GPU Memory | I  
 |  GPU       PID   Type   Process name                             Usage      | I  
 |=============================================================================| I  
 |  No running processes found                                                 | I  
 +-----------------------------------------------------------------------------+ I  
 -rwxr-xr-x 1 root root 349141232 Jul 27 11:38 /usr/local/cuda/lib64/libcudnn.so I  
 -rwxr-xr-x 1 root root 349141232 Jul 27 11:38 /usr/local/cuda/lib64/libcudnn.so.7 I  
 -rwxr-xr-x 1 root root 349141232 Jul 27 11:38 /usr/local/cuda/lib64/libcudnn.so.7.4.2 I  
 -rwxr-xr-x 1 root root 346085818 Jul 27 11:38 /usr/local/cuda/lib64/libcudnn_static.a I  
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='jurneo' date='2019-09-17T15:44:05Z'>
		&lt;denchmark-link:https://github.com/jurneo&gt;@jurneo&lt;/denchmark-link&gt;
 could you provide a simple repro (e.g. python scripts, etc) for the problem? Also, could you try with TF &lt;denchmark-link:https://pypi.org/project/tensorflow-gpu/1.15.0rc1/&gt;1.15rc1&lt;/denchmark-link&gt;
 or &lt;denchmark-link:https://pypi.org/project/tensorflow-gpu/2.0.0rc1/&gt;2.0rc1&lt;/denchmark-link&gt;
?
Also &lt;denchmark-link:https://github.com/nluehr&gt;@nluehr&lt;/denchmark-link&gt;
, are you familiar with the  error mentioned above?
		</comment>
		<comment id='4' author='jurneo' date='2019-09-17T19:19:50Z'>
		The "Graphics Exception" is most likely a symptom of the "unspecified launch failure" which has left the application in a bad state. An unspecified launch failure often results from a Segfault within a GPU kernel. It is an asynchronous error in that it gets generated by a kernel executing on the GPU but isn't flagged on the CPU until the next CPU/GPU sync point (in this case, the sync point appears to be an attempt to run a host-to-device memcpy).
In addition to testing with the latest TF versions, I would also recommend updating cudnn to the latest version, 7.6.3.
		</comment>
		<comment id='5' author='jurneo' date='2019-09-18T01:33:46Z'>
		&lt;denchmark-link:https://github.com/aaroey&gt;@aaroey&lt;/denchmark-link&gt;
 It is likely the driver issue on linux. To share additional recent finding, this problem does not occur on TF 1.12 on Windows version. I will try your suggestion for later TF versions.
Meanwhile, related thread discussion is on issuetracker google 138398232, and the python script is similar to deeplab v3.
I would close this issue with the workaround (by upgrading to latest version of TF with latest update GPU on drivers), but feel free to reopen.
		</comment>
		<comment id='6' author='jurneo' date='2019-09-18T01:33:48Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=31917&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=31917&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>