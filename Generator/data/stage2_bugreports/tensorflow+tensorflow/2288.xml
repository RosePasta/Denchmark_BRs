<bug id='2288' author='sesse' open_date='2016-05-09T10:51:43Z' closed_time='2017-01-24T00:28:47Z'>
	<summary>NaN checker does not check outputs of optimizers</summary>
	<description>
Hi,
I noticed this while debugging fp16 support for the Adam optimizer, so I thought I'd write it down here before it was forgotten.
I had problems with NaNs in my pipeline, but couldn't really figure out where they came from. So I called tf.add_check_numerics_ops(), but it turns out that if the Adam optimizer creates a NaN (in my case, because the default epsilon of 1e-8 was too small for fp16), it is not actually checked.
Instead, you get the error on a later Read op (when the NaN is attempted used), which can be very confusing, especially since there's a lot of Read ops and it's not always easy to figure out which one the error message is about.
	</description>
	<comments>
		<comment id='1' author='sesse' date='2016-05-09T20:02:26Z'>
		Are you calling add_check_numerics_ops after calling AdamOptimizer? In add_check_numerics I see that it adds check numerics ops to all ops present in the graph
		</comment>
		<comment id='2' author='sesse' date='2016-05-10T09:24:59Z'>
		Yes, I called it after AdamOptimizer. I believe the optimizer is special because nothing reads from its outputs (before the next iteration), and that's why it falls through the cracks.
		</comment>
		<comment id='3' author='sesse' date='2017-01-24T00:28:47Z'>
		Closing due to inactivity. Feel free to open a new github issue if the problem still persists in recent versions.
		</comment>
	</comments>
</bug>