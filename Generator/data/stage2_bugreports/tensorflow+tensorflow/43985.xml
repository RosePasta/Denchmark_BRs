<bug id='43985' author='patcombe' open_date='2020-10-13T18:10:10Z' closed_time='2020-10-21T06:39:09Z'>
	<summary>TF to TF Lite Int 8 resulting in error: "Quantization not yet supported for op: %"</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes (a mix)
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): 2.3
Python version: 3

Describe the current behavior
Conversion to TF Lite Float 16 works and model runs well
Conversion to TF Lite Int 8 does NOT convert
Describe the expected behavior
Conversion to TF Lite Int 8 works
Standalone code to reproduce the issue
`
&lt;denchmark-h:h1&gt;TFLite model export&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;try:
    print('\nStarting TFLite export with TensorFlow %s...' % tf.__version__)
    if opt.no_tfl_detect:
        print("Don't export Detect module")
        m.training = True
        keras_model = keras.Model(inputs=inputs, outputs=tf_model.predict(inputs))

    # fp16 TFLite model export
    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_types = [tf.float16]
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]
    converter.allow_custom_ops = False
    converter.experimental_new_converter = True
    tflite_model = converter.convert()
    f = opt.weights.replace('.pt', '.tflite')  # filename
    open(f, "wb").write(tflite_model)
    print('\nTFLite export success, saved as %s' % f)

    # int8 TFLite model export
    if opt.tfl_int8:

        dataset = LoadImages(opt.source, img_size=opt.img_size, auto=False)
            
        def representative_data_gen():
            n = 0
            for path, img, im0s, vid_cap in dataset:
                # Get sample input data as a numpy array in a method of your choosing.
                n += 1
                input = np.transpose(img, [1, 2, 0])
                input = np.expand_dims(input, axis=0).astype(np.float32)
                input /= 255.0
                yield [input]
                if n &gt;= opt.ncalib:
                    break

        converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
        # This enables quantization
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        # This sets the representative dataset for quantization
        converter.representative_dataset = representative_data_gen
        # This ensures that if any ops can't be quantized, the converter throws an error
        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
        # For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.
        converter.target_spec.supported_types = [tf.int8]
        # These set the input and output tensors to uint8 (added in r2.3)
        converter.inference_input_type = tf.uint8
        converter.inference_output_type = tf.uint8
        tflite_model = converter.convert()

        with open('mobilenet_v2_1.0_224_quant.tflite', 'wb') as f:
          f.write(tflite_model)
&lt;/denchmark-code&gt;

`
 Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/5373243/logs.pdf&gt;logs.pdf&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='patcombe' date='2020-10-13T18:12:17Z'>
		&lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;
 Hey Abhilash, let me know if you need any more info! Thanks!
		</comment>
		<comment id='2' author='patcombe' date='2020-10-14T07:59:42Z'>
		The error looks strange. Could you try it again with tf-nightly?
&lt;denchmark-code&gt;TFLite export failure: Quantization not yet supported for op: %
Traceback (most recent call last):
 File "yolov5/models/tf.py", line 568, in &lt;module&gt;
 tflite_model = converter.convert()
 File "/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/lite.py", line 831, in convert
 self).convert(graph_def, input_tensors, output_tensors)
 File "/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/lite.py", line 638, in convert
 result = self._calibrate_quantize_model(result, **flags)
 File "/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/lite.py", line 452, in
_calibrate_quantize_model
 inference_output_type, allow_float, activations_type)
 File "/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/optimize/calibrator.py", line
98, in calibrate_and_quantize
 np.dtype(activations_type.as_numpy_dtype()).num)
RuntimeError: Quantization not yet supported for op: %
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='patcombe' date='2020-10-15T06:56:35Z'>
		&lt;denchmark-link:https://github.com/patcombe&gt;@patcombe&lt;/denchmark-link&gt;
,
On running the code, I am facing an error stating . Please find the gist of it &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/d943bd3f2e75d20a7d6112ea01b36efd/43985.ipynb&gt;here&lt;/denchmark-link&gt;
.
In order to reproduce the issue reported, could you please provide the complete code and the dataset you are using.
Also, as &lt;denchmark-link:https://github.com/terryheo&gt;@terryheo&lt;/denchmark-link&gt;
 suggested please try running the code with the latest TF-nightly and check if it works. Thanks!
		</comment>
		<comment id='4' author='patcombe' date='2020-10-15T15:04:06Z'>
		&lt;denchmark-link:https://github.com/terryheo&gt;@terryheo&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;
 using tf-nightly seemed to solve it! Thanks!
		</comment>
		<comment id='5' author='patcombe' date='2020-10-21T06:39:09Z'>
		&lt;denchmark-link:https://github.com/patcombe&gt;@patcombe&lt;/denchmark-link&gt;
,
Thank you for the update. Marking the issue as closed.
		</comment>
		<comment id='6' author='patcombe' date='2020-10-21T06:39:11Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43985&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43985&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>