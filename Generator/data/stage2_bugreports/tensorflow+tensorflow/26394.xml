<bug id='26394' author='kpe' open_date='2019-03-06T12:08:17Z' closed_time='2019-03-18T12:05:58Z'>
	<summary>Allow building TF + nvidia GPU targeting &amp;lt; sm35 if XLA is not enabled</summary>
	<description>
Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Gentoo
TensorFlow installed from (source or binary): source
TensorFlow version: 1.13.1
Python version: 3.6.4
Installed using virtualenv? pip? conda?: pip in venv
Bazel version (if compiling from source): 0.21
GCC/Compiler version (if compiling from source): 7.4.0
CUDA/cuDNN version: 10.0
GPU model and memory: GTX 650 Ti

I was able to successfully build TF from source with XLA enabled and compute capability 3.0.
However, when a session is created the python interpreter exits (complaining about insufficient compute capability):
&lt;denchmark-code&gt;&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; tf.Session()
2019-03-06 12:49:41.776396: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3500130000 Hz
2019-03-06 12:49:41.776727: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x556553ef5ee0 executing computations on platform Host. Devices:
2019-03-06 12:49:41.776741: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
2019-03-06 12:49:41.809556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-03-06 12:49:41.810593: I tensorflow/compiler/xla/service/platform_util.cc:194] StreamExecutor cuda device (0) is of insufficient compute capability: 3.5 required, device is 3.0
2019-03-06 12:49:41.810666: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA

&lt;/denchmark-code&gt;

if I reconfigure TF by disabling XLA, and rebuild (again with compute capability 3.0), than TF works fine.
So I guess, a simple check if compute capability &gt;= 3.5 when XLA is enabled, could at least prevent building non-functional TF.
	</description>
	<comments>
		<comment id='1' author='kpe' date='2019-03-06T21:28:06Z'>
		Hey, a similar issue was filed a while ago, and a fix has been deployed &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/25767&gt;here&lt;/denchmark-link&gt;
. Would be available in the master branch/the next release.
		</comment>
		<comment id='2' author='kpe' date='2019-03-07T10:52:59Z'>
		I actually found similar issues, but non of them was XLA related. And indeed as you pointed out (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/25767&gt;#25767&lt;/denchmark-link&gt;
) the config script does no longer allow building TF with compute capability bellow 3.5. Even it looks like if XLA is disabled TF could still be built and function properly with capability 3.0.
also just for referece - &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/24126&gt;#24126&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='kpe' date='2019-03-08T22:46:01Z'>
		
as you pointed out (#25767) the config script does no longer allow building TF with compute capability bellow 3.5.

If this is the case, I'm not sure what is the bug here?
		</comment>
		<comment id='4' author='kpe' date='2019-03-09T00:24:21Z'>
		The "bug" is - you could build with 3.0 and disabled XLA but the config.py does not allow this.
What leads to nonfunctional TF ist the combination of XLA and 3.0, and not 3.0 alone.
		</comment>
		<comment id='5' author='kpe' date='2019-03-09T00:30:26Z'>
		
The "bug" is - you could build with 3.0 and disabled XLA but the config.py does not allow this.
What leads to nonfunctional TF ist the combination of XLA and 3.0, and not 3.0 alone.

Sorry, I'm still confused.
In the second sentence, I'm understanding that TF built without XLA and run on sm30 does work?  But in the first sentence I'm understanding that you can't build TF with XLA disabled targeting sm30?  Those seem at odds.
		</comment>
		<comment id='6' author='kpe' date='2019-03-09T00:45:42Z'>
		Well, the first sentence is - you can build it if you patch configure.py to accept compute capability 3.0 (with disabled XLA). And in sentence two, there are two statements - a) XLA and 3.0 = broken-TF, and b) disabled XLA and 3.0 = working-TF.
		</comment>
		<comment id='7' author='kpe' date='2019-03-09T00:50:50Z'>
		....I mean, I had to patch config.py in order to build an TF version for my old GPU (compute capability 3.0).
		</comment>
		<comment id='8' author='kpe' date='2019-03-09T00:53:21Z'>
		OK, so what you'd like is for the TF build team to change config.py so that it doesn't require sm35, if XLA is disabled?
		</comment>
		<comment id='9' author='kpe' date='2019-03-09T00:59:38Z'>
		Yes, this is what the issue was about. (edit: at least after I learned there was a  &gt;= 3.5 check on the main branch already, preventing TF from being built with 3.0)
		</comment>
		<comment id='10' author='kpe' date='2019-03-09T01:04:20Z'>
		It may be the case that TF build team has decided that they do not support &lt; sm35 at all, even though you observed that it happened to work in your particular case.  Over to them, this is not really an XLA question.
		</comment>
		<comment id='11' author='kpe' date='2019-03-09T01:07:26Z'>
		Yes, sorry for the confusion, it was really about, a better config sanity check.
		</comment>
		<comment id='12' author='kpe' date='2019-03-10T21:21:03Z'>
		&lt;denchmark-link:https://github.com/chsigg&gt;@chsigg&lt;/denchmark-link&gt;
 may be able to confirm or deny, but here is what I can say:
Below compute capability 3.0 TF will NOT work. XLA, or no xla, TF needs features in compute capability 3.0
Below 3.5 is best effort, we do not test it, or even build it.  It may work, may not work, you are free to use at your own risk.
Above 3.5 will work.
6.0 is the most heavily tested one for us, and any issues we see for 6.0 or newer is promptly triaged.
		</comment>
		<comment id='13' author='kpe' date='2019-03-11T09:51:37Z'>
		We can probably change &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/25767&gt;#25767&lt;/denchmark-link&gt;
 to issue an error for compute capability &lt; 3.0 (down from currently &lt; 3.5).
For compute capability &lt; 3.5, we can issue a warning that XLA is not supported. XLA generates CUDA code for whatever GPU is present at runtime, so anything more than a warning during configuration seems prohibitive.
		</comment>
		<comment id='14' author='kpe' date='2019-03-11T14:07:56Z'>
		I can submit another PR if it is required, but I would appreciate it if you could specify the exact information that should be conveyed in the warning. The original PR did issue a warning and not an error. It was changed because it seemed like TensorFlow would surely fail to work below 3.5 (which is not really the case maybe?).
		</comment>
		<comment id='15' author='kpe' date='2019-03-18T12:05:58Z'>
		The compute capability check is now a warning for &lt; 3.5, and and error for &lt; 3.0. Closing.
		</comment>
	</comments>
</bug>