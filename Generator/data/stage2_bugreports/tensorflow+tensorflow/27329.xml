<bug id='27329' author='sidgan' open_date='2019-03-31T04:05:26Z' closed_time='2019-04-08T03:46:51Z'>
	<summary>Unable to convert frozen graph to usable model on iPhone</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): 1.13.1
Python version: 2.7.15rc1
Bazel version (if compiling from source): NA
GCC/Compiler version (if compiling from source): NA
CUDA/cuDNN version: 10.0
GPU model and memory: GeForce GTX 1080

You can collect some of this information using our environment capture &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
Describe the current behavior
I am fine-tuning the &lt;denchmark-link:http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz&gt;pretrained SSD-MobileNetV1&lt;/denchmark-link&gt;
 model using the  &lt;denchmark-link:https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_mobilenet_v1_pets.config&gt;config file&lt;/denchmark-link&gt;
 for detecting the bounding boxes of objects in an image.
&lt;denchmark-code&gt;python model_main.py --logtostderr --model_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config
&lt;/denchmark-code&gt;

After training, I am freezing the model using
&lt;denchmark-code&gt;python export_inference_graph.py --input_type=image_tensor --pipeline_config_path=training/ssd_mobilenet_v1_pets.config --output_directory=inference_graph --trained_checkpoint_prefix=training/model.ckpt-3740 
&lt;/denchmark-code&gt;

After the frozen_inference_graph.pb  is being generated, I'd like to convert the model to a model format that is compatible with iPhones. I tried the following approaches:
&lt;denchmark-h:h3&gt;Approach 1 - Convert to mlmodel&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;$ python convert_to_mlmodel.py 
WARNING:root:TensorFlow version 1.13.1 detected. Last version known to be fully compatible is 1.12.0 .

Loading the TF graph...
2019-03-30 18:39:48.956255: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-03-30 18:39:49.156054: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5592d6f7e880 executing computations on platform CUDA. Devices:
2019-03-30 18:39:49.156083: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2019-03-30 18:39:49.177932: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3398155000 Hz
2019-03-30 18:39:49.178509: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5592d6fdf420 executing computations on platform Host. Devices:
2019-03-30 18:39:49.178548: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
2019-03-30 18:39:49.178847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 10.92GiB freeMemory: 211.44MiB
2019-03-30 18:39:49.178889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-03-30 18:39:49.179849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-30 18:39:49.179872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-03-30 18:39:49.179882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-03-30 18:39:49.180081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 211 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
Graph Loaded.
Traceback (most recent call last):
  File "convert_to_mlmodel.py", line 4, in &lt;module&gt;
    output_feature_names = ['softmax:0'])
  File "/home/deepvision/.local/lib/python2.7/site-packages/tfcoreml/_tf_coreml_converter.py", line 586, in convert
    custom_conversion_functions=custom_conversion_functions)
  File "/home/deepvision/.local/lib/python2.7/site-packages/tfcoreml/_tf_coreml_converter.py", line 167, in _convert_pb_to_mlmodel
    OPS = _topological_sort_ops(OPS)
  File "/home/deepvision/.local/lib/python2.7/site-packages/tfcoreml/_tf_graph_transform.py", line 194, in _topological_sort_ops
    _push_stack(stack, node, in_stack)
  File "/home/deepvision/.local/lib/python2.7/site-packages/tfcoreml/_tf_graph_transform.py", line 38, in _push_stack
    raise ValueError('Graph has cycles.')
ValueError: Graph has cycles.
&lt;/denchmark-code&gt;

The conversion code looks like:
&lt;denchmark-code&gt;import tfcoreml as tf_converter
tf_converter.convert(tf_model_path = 'frozen_inference_graph.pb',
                     mlmodel_path = 'my_model.mlmodel',
                     output_feature_names = ['softmax:0'])
&lt;/denchmark-code&gt;

Tried the above with Faster RCNN ResNet101 and SSD MobileNet - both give the same error.
Not sure why the output_feature_names has to be a softmax as its a detection problem.
&lt;denchmark-h:h3&gt;Approach 2 - Convert to tflite&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;$ tflite_convert --output_file=output.tflite --saved_model_dir=/home/deepvision/code/models/research/object_detection/inference_graph/saved_model/
2019-03-30 19:22:30.354332: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-03-30 19:22:30.757830: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5592ab466420 executing computations on platform CUDA. Devices:
2019-03-30 19:22:30.757861: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2019-03-30 19:22:30.785890: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3398155000 Hz
2019-03-30 19:22:30.791145: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5592ab498d20 executing computations on platform Host. Devices:
2019-03-30 19:22:30.791161: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
2019-03-30 19:22:30.791279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 10.92GiB freeMemory: 69.38MiB
2019-03-30 19:22:30.791294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-03-30 19:22:30.791745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-30 19:22:30.791755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-03-30 19:22:30.791762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-03-30 19:22:30.791844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 69 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
WARNING:tensorflow:From /home/deepvision/.local/lib/python2.7/site-packages/tensorflow/lite/python/convert_saved_model.py:61: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
2019-03-30 19:22:31.298973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-03-30 19:22:31.299020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-30 19:22:31.299032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-03-30 19:22:31.299040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-03-30 19:22:31.299186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 69 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
WARNING:tensorflow:From /home/deepvision/.local/lib/python2.7/site-packages/tensorflow/lite/python/convert_saved_model.py:275: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.compat.v1.graph_util.convert_variables_to_constants
WARNING:tensorflow:From /home/deepvision/.local/lib/python2.7/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.compat.v1.graph_util.extract_sub_graph
Traceback (most recent call last):
  File "/home/deepvision/.local/bin/tflite_convert", line 11, in &lt;module&gt;
    sys.exit(main())
  File "/home/deepvision/.local/lib/python2.7/site-packages/tensorflow/lite/python/tflite_convert.py", line 442, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File "/home/deepvision/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py", line 125, in run
    _sys.exit(main(argv))
  File "/home/deepvision/.local/lib/python2.7/site-packages/tensorflow/lite/python/tflite_convert.py", line 438, in run_main
    _convert_model(tflite_flags)
  File "/home/deepvision/.local/lib/python2.7/site-packages/tensorflow/lite/python/tflite_convert.py", line 191, in _convert_model
    output_data = converter.convert()
  File "/home/deepvision/.local/lib/python2.7/site-packages/tensorflow/lite/python/lite.py", line 411, in convert
    "invalid shape '{1}'.".format(_tensor_name(tensor), shape_list))
ValueError: None is only supported in the 1st dimension. Tensor 'image_tensor' has invalid shape '[None, None, None, 3]'.
&lt;/denchmark-code&gt;

Also, tried the following:
 tflite_convert --output_file=output.tflite --graph_def_file=frozen_inference_graph.pb --input_arrays=input --output_arrays=
But, I am not sure what the last layer is, as the model is fairly convoluted. Tried visualizing with Netron but
Describe the expected behavior
Seamless conversion to tflite and mlmodel files.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
NA
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
NA
	</description>
	<comments>
		<comment id='1' author='sidgan' date='2019-04-04T19:39:14Z'>
		Hello. We've been stuck with this issue for 5 days now. Can you please provide an update on this?
cc: &lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='sidgan' date='2019-04-05T18:56:09Z'>
		When you use the tflite converter to convert from saved_model, the input_arrays/output_arrays are derived from the SignatureDef. &lt;denchmark-link:https://www.tensorflow.org/lite/convert/cmdline_examples#convert_a_tensorflow_savedmodel_&gt;1&lt;/denchmark-link&gt;
 And input_shape is automatically determined, but in this case it seems the 2nd, 3rd dimension is 'None'. I think TF Lite currently only support 'None' shape for the batch_size dimension.
Could you try using tflite converter to convert from the frozen graph (your third approach) and also supply the input_shape as well? The output_arrays are needed, which you could check from Netron, or from the SignatureDef of the SavedModel.
Let me know if you need any help further.
		</comment>
		<comment id='3' author='sidgan' date='2019-04-05T18:56:52Z'>
		[1]&lt;denchmark-link:https://www.tensorflow.org/lite/convert/cmdline_examples#convert_a_tensorflow_savedmodel&gt;https://www.tensorflow.org/lite/convert/cmdline_examples#convert_a_tensorflow_savedmodel&lt;/denchmark-link&gt;
_
		</comment>
		<comment id='4' author='sidgan' date='2019-04-08T03:46:51Z'>
		Solution: There is an export_tflite_ssd_graph.py file provided in the repository which needs to be used in place of the previous script export_inference_graph, which allows conversion to tflite format.
&lt;denchmark-code&gt;python export_tflite_ssd_graph.py --pipeline_config_path=training/pipeline.config --trained_checkpoint_prefix=training/model.ckpt-20000  --output_directory=tflite_model --add_postprocessing_op=true
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='sidgan' date='2019-04-08T03:46:52Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27329&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27329&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>