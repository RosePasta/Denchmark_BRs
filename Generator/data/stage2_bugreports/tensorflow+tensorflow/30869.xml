<bug id='30869' author='w4-sjcho' open_date='2019-07-19T04:49:37Z' closed_time='2019-09-05T00:41:15Z'>
	<summary>"'NoneType' object has no attribute '_fetch_cloud_tpu_metadata'" when using TPU</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): 1.14.0
Python version: 3.6
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: 10.0
GPU model and memory: Cloud TPU v2

Describe the current behavior
I've been using T2T training script using cloud TPU. After upgrading to TF 1.14.0 from TF 1.13.1, I'm getting the error below:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/app/poodle/asr/t2t/t2t_train_venv.image.binary.runfiles/skelterlabs/poodle/asr/t2t/py-pkg/tensorflow/python/tpu/preempted_hook.py", line 86, in run
    response = self._cluster._fetch_cloud_tpu_metadata()  # pylint: disable=protected-access
AttributeError: 'NoneType' object has no attribute '_fetch_cloud_tpu_metadata'
&lt;/denchmark-code&gt;

The training script continues w/o any further issue despite the error.
Describe the expected behavior
No error.

Run &lt;denchmark-link:https://github.com/tensorflow/tensor2tensor#speech-recognition&gt;https://github.com/tensorflow/tensor2tensor#speech-recognition&lt;/denchmark-link&gt;
 problems using transformer model on TPUs.
Other info / logs
N/A
	</description>
	<comments>
		<comment id='1' author='w4-sjcho' date='2019-08-20T07:35:09Z'>
		I also meet this problem, have you solved it?
		</comment>
		<comment id='2' author='w4-sjcho' date='2019-08-20T16:05:59Z'>
		&lt;denchmark-link:https://github.com/w4-sjcho&gt;@w4-sjcho&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/HeimingX&gt;@HeimingX&lt;/denchmark-link&gt;
 Do you have issues with this &lt;denchmark-link:https://colab.sandbox.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb#scrollTo=wfF8_cW-OXPN&gt;colab&lt;/denchmark-link&gt;
. Please let us know the location/item which is breaking. Thanks!
		</comment>
		<comment id='3' author='w4-sjcho' date='2019-09-03T15:45:57Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jhseu&gt;@jhseu&lt;/denchmark-link&gt;
 Also seeing this with tensorflow 1.14 and t2t 1.14, happy to help. This is running in batch on ctpu v3 with tf 1.14 runtime (in graph mode).
As you can see the error above  is failing because self._cluster is None; this is being passed to CloudTPUPreemptedHook &lt;denchmark-link:https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py#L3237&gt;in tpu_estimator.py&lt;/denchmark-link&gt;
.
See there has recently been a &lt;denchmark-link:https://github.com/tensorflow/estimator/commit/6a8b06637a73d070ca5ccbeeb5fdc280c8aa9994&gt;change to tpu_estimator.py&lt;/denchmark-link&gt;
 that modifies:
         if tpu_cluster_resolver.is_running_in_gce():
            hooks.extend(
                [preempted_hook.CloudTPUPreemptedHook(self._config.cluster)])
to
         if _check_add_preemption_hook(self._config.cluster):
            hooks.extend(
                [preempted_hook.CloudTPUPreemptedHook(self._config.cluster)])
where the latter wraps the former to include a check for the TPUClusterResolver object not being None, here:
def _check_add_preemption_hook(cluster):
  return (tpu_cluster_resolver.is_running_in_gce() and
          cluster and
          isinstance(cluster, tpu_cluster_resolver.TPUClusterResolver) and
          cluster._should_resolve)
which seems like one solution (i.e. avoid adding the problematic hook if we don't have a cluster resolver object).
Another would be to ensure the cluster resolver gets initialized when the run config is constructed in t2t trainer_lib.py, &lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/blob/2036ffe309b86bda367b1e687fafb114534500f9/tensor2tensor/utils/trainer_lib.py#L232&gt;here&lt;/denchmark-link&gt;
; looking at the logic there the portion initializing the cluster resolver won't be reached when using TPUs via GKE because KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS will be set.
Idk if it will break something else but I'm just going to try moving the initialization of the cluster resolver outside of the logic about whether master and KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS are set. Or re-build tf with the modification to tpu_estimator.py mentioned above. The tpu runtime tf version would still be stock 1.14 but afaict that pertains to ops not session hooks.
		</comment>
		<comment id='4' author='w4-sjcho' date='2019-09-03T20:02:47Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jhseu&gt;@jhseu&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/w4-sjcho&gt;@w4-sjcho&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/HeimingX&gt;@HeimingX&lt;/denchmark-link&gt;
 So it does look like there's a side effect in moving the cluster resolver instantiation from its current place but &lt;denchmark-link:https://gist.github.com/cwbeitel/2ab6bb9c5c418d390bedb23b8c959c0b#file-patch_tpu_estimator-py&gt;patching in&lt;/denchmark-link&gt;
 the  change mentioned above gets rid of the error and training appears to proceed normally.
		</comment>
		<comment id='5' author='w4-sjcho' date='2019-09-05T00:41:15Z'>
		Yeah, this'll be fixed in TF 1.15. You can also pass the cluster to the RunConfig instead of passing master directly and it should work.
		</comment>
		<comment id='6' author='w4-sjcho' date='2019-09-05T00:41:16Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30869&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30869&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>