<bug id='31550' author='sziem' open_date='2019-08-12T16:28:18Z' closed_time='2019-08-26T18:36:24Z'>
	<summary>Error: Check failed: dims == sizes.size() (5 vs. 4), when using CPU and MKL instead of Eigen or GPU.</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6 High Sierra
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.14
Python version: 3.7
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A

Describe the current behavior
Running my code on cpu with tensorflow 1.14 using mkl from anaconda throws the following error:
2019-08-12 17:42:33.158451: F ./tensorflow/core/util/mkl_util.h:636] Check failed: dims == sizes.size() (5 vs. 4)
Abort trap: 6
The error trace gives me no hint on how to localize the problem (see below). The issue does not occur when installing a tensorflow build using eigen.
Describe the expected behavior
The code should work using both MKL or Eigen
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
I was not able to localize the exact issue.
It occurs when running ndnet.py from &lt;denchmark-link:https://github.com/sziem/deconv_unet_2d3d&gt;https://github.com/sziem/deconv_unet_2d3d&lt;/denchmark-link&gt;

Other info / logs
Full output:
$ python ndnet.py
WARNING: Logging before flag parsing goes to stderr.
W0812 18:24:24.249456 140736187437952 deprecation.py:323] From /Users/Soenke/anaconda/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
testing training
2019-08-12 18:24:26.506695
2019-08-12 18:24:26.621728: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-08-12 18:24:26.691965: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.
W0812 18:24:26.750053 140736187437952 deprecation.py:323] From /Users/Soenke/anaconda/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support..wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0812 18:24:26.794230 140736187437952 deprecation.py:323] From /Users/Soenke/Documents/Uni/Master/Masterarbeit/gits/code/public/unet_deconv/dataset_handlers/tfdata_dataset_handlers.py:332: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
options available in V2.
- tf.py_function takes a python function which manipulates tf eager
tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
an ndarray (just call tensor.numpy()) but having access to eager tensors
means tf.py_functions can use accelerators such as GPUs as well as
being differentiable using a gradient tape.
- tf.numpy_function maintains the semantics of the deprecated tf.py_func
(it is not differentiable, and manipulates numpy arrays). It drops the
stateful argument making all functions stateful.
Cropping to nearest allowed input image size.
Cropping to nearest allowed input image size.
W0812 18:24:27.274060 140736187437952 deprecation.py:323] From /Users/Soenke/Documents/Uni/Master/Masterarbeit/gits/code/public/unet_deconv/dataset_handlers/tfdata_dataset_handlers.py:139: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use for ... in dataset: to iterate over a dataset. If using tf.estimator, return the Dataset object directly from your input function. As a last resort, you can use tf.compat.v1.data.make_one_shot_iterator(dataset).
input_shape: (?, 400, 100, 100, 1)
building Unet_v3 for training
net input shape (?, 398, 98, 98, 1)
W0812 18:24:27.390805 140736187437952 deprecation.py:323] From /Users/Soenke/Documents/Uni/Master/Masterarbeit/gits/code/public/unet_deconv/network_architectures/ops.py:173: conv3d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.keras.layers.Conv3D instead.
input_block1_2 (?, 396, 96, 96, 2)
W0812 18:24:27.957751 140736187437952 deprecation.py:323] From /Users/Soenke/Documents/Uni/Master/Masterarbeit/gits/code/public/unet_deconv/network_architectures/ops.py:116: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dropout instead.
W0812 18:24:28.167316 140736187437952 deprecation.py:323] From /Users/Soenke/Documents/Uni/Master/Masterarbeit/gits/code/public/unet_deconv/network_architectures/ops.py:232: average_pooling3d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.AveragePooling3D instead.
down_block2_4 (?, 196, 46, 46, 4)
bottom_block4_4 (?, 192, 42, 42, 4)
up_block4_2 (?, 380, 80, 80, 2)
output_block2_1 (?, 378, 78, 78, 1)
net output shape (?, 378, 78, 78, 1)
output_shape: (?, 378, 78, 78, 1)
loss is  l2loss
determining number of trainable vars (except batch_norm) for regularization...
done:  2183
saving new ckpt and logs in models/unetv3_small_valid_fp0_pp0_bn00_chlast/poisson_n1000_wl520/seed1_bs1_do0.0_loss=l2loss0_weightreg=0.001l2_loss_datareg=1e-08None_example/run8
Saving 2 logs per epoch by default.
Saving checkpoint every 2 epochs by default.
starting training with start_step 0
epoch 1 / 2
----&gt;saving 0
----&gt;summarizing 0
2019-08-12 18:24:44.145176: F ./tensorflow/core/util/mkl_util.h:636] Check failed: dims == sizes.size() (5 vs. 4)
2019-08-12 18:24:44.145177: F ./tensorflow/core/util/mkl_util.h:636] Check failed: dims == sizes.size() (5 vs. 4)
Abort trap: 6
	</description>
	<comments>
		<comment id='1' author='sziem' date='2019-08-15T05:05:17Z'>
		I got the exact same issue at the moment!
PS:



print(keras.version)
2.2.4
print(tf.version)
1.14.0



		</comment>
		<comment id='2' author='sziem' date='2019-08-22T16:44:50Z'>
		We submitted a few fixes to the 1.15 branch, could you please test your use case in 1.15 branch?
		</comment>
		<comment id='3' author='sziem' date='2019-08-26T18:36:24Z'>
		Great!  I was able to run the code without error after compiling the 1.15 branch with mkl.
Thanks!
		</comment>
		<comment id='4' author='sziem' date='2019-08-26T18:36:25Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=31550&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=31550&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='sziem' date='2020-06-11T20:04:16Z'>
		And if we can't use the 1.15 branch?
		</comment>
	</comments>
</bug>