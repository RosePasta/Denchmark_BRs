<bug id='22201' author='tfboyd' open_date='2018-09-10T22:49:30Z' closed_time='2018-09-18T20:42:36Z'>
	<summary>Fall 2018 Symposium Performance Talk Notes</summary>
	<description>
There were additional topics discussed that could not be turned into coherent notes after the fact.
&lt;denchmark-h:h2&gt;Action items&lt;/denchmark-h&gt;


TensorFlow team to create a block diagram and then go deeper.  Goal is to help people who want to contribute plugins or extensions. #22356
Make the NVIDIA Library to TF Version matrix more visible and consider better error messages. #22357
Document tips and methods to use for large batch scaling with TensorFlow.  Possibly mention where this is proven and unproven to work.  This would include adding optimizers or wrapper to simplify usage.  Researchers are large labs want to scale but do not have the knowledge to do it quickly. #22358

&lt;denchmark-h:h2&gt;General Questions and discussion results&lt;/denchmark-h&gt;


Should TensorFlow default builds move forward more aggressively with newer versions of CUDA/cuDNN?

Internally Google moves slowly to new versions of CUDA as the verification process is long and often includes multiple patches with NVIDIA until all edge cases are covered.  The bar is extremely high for obvious reasons.
Group thought was it doesn’t matter. Situation would be improved with better error messages indicating what version is needed and making the matrix showing what NVIDIA libs are needed for each TensorFlow version.


Provide more models with good performance and accuracy in github.com/tensorflow/models.  Too many different versions and unsure which one to use.  Example:  There are 3+ ResNet models.

Contact tfboyd@ or comment in this thread if there is a specific model of interest and why.  MLPerf may improve this situation.


Auto regressive models with long dependency chains are hard to optimize by doing fusion by hand.

Consider trying XLA and providing feedback. An objective of XLA is to reduce the need for custom fusion.


[Ask] Distributing the input pipeline across servers.  Some work has been done and rumored to have occured in production but not trivial to setup.  Consider an RFP and/or reach out to the tf.data team.
TensorCores are only used if you are using tf.16.  You can do Pseudo FP16 with an envar that should be in TF 1.11 from NVIDIA as an experimental feature.  You still need to do loss scaling.

&lt;denchmark-h:h2&gt;Debugging and performance investigations&lt;/denchmark-h&gt;


Hard to get timelines and profiler results.

New guide is on the way, not positive this will make it as easy as desired.


[Ask] Provide an interface to autotuning parameters such as intra and inter thread pools.

In hindsight, more information is needed to make this actionable.  Please add info to the comments.



&lt;denchmark-h:h2&gt;Distributed compute (multi-gpu and multi-node)&lt;/denchmark-h&gt;



All reduce API for multi-node and support MPI.

MPI would be useful for supercomputers where you need to use their MPI library to get good communication.
NCCL currently used by MirroredStrategy but only for multi-GPU.  TensorFlow’s own ops are used for multi-node all-reduce.



Document on how the distribution strategies is setup with the goal of showing how others can add their own solution and collective ops.  The API is the best place right now to figure this out.


[AI] TensorFlow team to create a block diagram and then go deeper.  Goal is to help people who want to contribute plugins or extensions.


Having a hard time with MPI.  People are using a newer versions of MPI


How do we plan to support fault tolerance? Will it allow dynamic addition and removal of machines to the collective. E.g. preemption but keep training with a smaller pool. Also increasing machines &amp; batch size as training progresses. Need to adjust the learning rate, so maybe get a callback, especially when you are going to lose or have just lost a machine. Response to call back is starting with revised set of machines an new learning rate.
*[AI] Document strategies for large batch training.  LARS looking to automatic learning rate, warmup rate and stuff.


&lt;denchmark-h:h2&gt;Model parallelism and Data parallelism&lt;/denchmark-h&gt;


Model parallelism is currently manual.  Work to automate it is on going and it can be phrased as an RL problem.
Data parallelism supported via MirroredStrategy discussed today

	</description>
	<comments>
		<comment id='1' author='tfboyd' date='2018-09-11T06:40:58Z'>
		Thank you for your post. We noticed you have not filled out the following field in the issue template.
		</comment>
		<comment id='2' author='tfboyd' date='2018-09-19T03:49:26Z'>
		I would be interested if &lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
 could share more information on this particular event; I cannot find any information online apart from this issue.
		</comment>
	</comments>
</bug>