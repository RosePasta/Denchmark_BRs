<bug id='19499' author='LionelCons' open_date='2018-05-23T13:47:21Z' closed_time='2018-05-30T21:59:33Z'>
	<summary>tf.data.Dataset iterators are not cleaned when the loop ends with a break</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):CentOS7
TensorFlow installed from (source or binary):binary
TensorFlow version (use command below):1.8.0
Python version: 2.7.5
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

tf.data.Dataset iterators are not cleaned when the loop ends with a break.
The code below opens one file per epoch. This eventually hits a system limit (maximum number of open files).
Replacing the break by a continue works better since the files are closed. However, this is inefficient if we only need to iterate over a small fraction of the data
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;dataset = tf.data.TextLineDataset(fp)
...
for epoch in xrange(epochs):
    ...
    batches = 0
    for (x, y) in dataset:
        batches += 1
        if batches &gt; MAX_BATCHES:
            break
        ...
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='LionelCons' date='2018-05-23T19:57:22Z'>
		&lt;denchmark-link:https://github.com/allenlavoie&gt;@allenlavoie&lt;/denchmark-link&gt;
 It looks like you've done something related to Eager resource garbage collection in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/309e340619ab922f1ecb51b8f142283e09bda07d&gt;309e340&lt;/denchmark-link&gt;
, and this is still used in the current :



tensorflow/tensorflow/python/data/ops/iterator_ops.py


        Lines 480 to 482
      in
      2c9d129






 # Delete the resource when this object is deleted 



 self._resource_deleter = resource_variable_ops.EagerResourceDeleter( 



 handle=self._resource, handle_device="/device:CPU:0") 





Can you please take a look and see if there's a reference leak here?
		</comment>
		<comment id='2' author='LionelCons' date='2018-05-24T00:13:30Z'>
		There is a reference leak, but I don't think it's in Python. DestroyResourceOp gets run and Unrefs the resource, but it looks like IteratorHandleOp is &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/f8b74d642420dcf2f5cab763b41884a05777ea45/tensorflow/core/kernels/data/iterator_ops.cc#L510&gt;keeping a reference to the resource in its OpKernel&lt;/denchmark-link&gt;
. AFAIK kernels are never deleted when executing eagerly, they just sit around in the kernel cache.
I've verified that removing the reference from IteratorHandleOp fixes the "files not closed" issue (they get closed when DestroyResourceOp runs). I can think of horrible hacks to get this to happen only when executing eagerly, but maybe we should discuss tomorrow.
		</comment>
		<comment id='3' author='LionelCons' date='2018-05-24T14:53:51Z'>
		Ugh, yes, whatever we do to that kernel implementation, the current version will still leak the  object and related guff for each iterator. As a strawman, we could solve it with (i) a new version of  that creates the handle doesn't retain a resource, and (ii) some API for creating-and-running-but-not-caching a kernel in eager mode. (CCing &lt;denchmark-link:https://github.com/asimshankar&gt;@asimshankar&lt;/denchmark-link&gt;
 since kernel lifetimes in eager is something we've talked about in the past.)
&lt;denchmark-link:https://github.com/LionelCons&gt;@LionelCons&lt;/denchmark-link&gt;
 In the meantime, here's a workaround that should alleviate the file handle leak:
dataset = tf.data.TextLineDataset(fp)
# ...
for epoch in xrange(epochs):
  # ...
  for (x, y) in dataset.take(MAX_BATCHES):
    # ...
		</comment>
	</comments>
</bug>