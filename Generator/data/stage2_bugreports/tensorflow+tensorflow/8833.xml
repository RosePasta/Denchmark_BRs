<bug id='8833' author='RobRomijnders' open_date='2017-03-30T09:54:57Z' closed_time='2017-04-01T00:17:48Z'>
	<summary>DynamicAttentionWrapper expects own state on the 0-th step</summary>
	<description>
Using Tensorflow 1.1rc0
AFAIK the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py&gt;DynamicAttentionWrapper&lt;/denchmark-link&gt;
 wraps an attention mechanism around the RNNCell. To do so, it passes around its own DynamicAttentionWrapperState in the ()
However, I think that on the 0-th step, the network cannot pass such state as an argument, because the function gets called by a general decoder. (which doesnt know what cell it will encounter)
I got the following error
Traceback (most recent call last):
  File "/home/rob/Documents/frosha/analysis-service/src/analysis_parser/analysis_parser/trainer_class.py", line 54, in __init__
    self.model = Model(self.tok_chr.dim)
  File "/home/rob/Documents/frosha/analysis-service/src/analysis_parser/analysis_parser/models/rnn_seq2seq_tf.py", line 164, in __init__
    outputs, _ = dynamic_decode(decoder, impute_finished = True, maximum_iterations = max_sl)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py", line 278, in dynamic_decode
    swap_memory=swap_memory)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py", line 2623, in while_loop
    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py", line 2456, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py", line 2406, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py", line 231, in body
    decoder_finished) = decoder.step(time, inputs, state)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py", line 140, in step
    cell_outputs, cell_state = self._cell(inputs, state)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/dynamic_attention_wrapper.py", line 530, in __call__
    cell_inputs = self._cell_input_fn(inputs, state.attention)
AttributeError: 'tuple' object has no attribute 'attention'
The error indicates that the current state has no attribute attention. But that's the final state of the encoder, which doesn't use this syntax
The following snippet shows the use of the wrapper. This was coded after &lt;denchmark-link:https://www.tensorflow.org/versions/r1.1/api_guides/python/contrib.seq2seq#Dynamic_Decoding&gt;this&lt;/denchmark-link&gt;
 explanation
encoder_outputs, encoder_state = core_rnn.static_rnn(
                encoder_cell, encoder_inputs, dtype=dtype, sequence_length=self.SL_enc)

#Some other code

attention_size = 10
attn_obj = BahdanauAttention(num_units=attention_size,
                                             memory=attention_states,
                                             memory_sequence_length=self.SL_enc,
                                             normalize=True,
                                             name='BahdanauAttentionObject')

wrapped_cell = DynamicAttentionWrapper(cell_dec_fw,
                                                       attn_obj,
                                                       D,
                                                       output_attention=False,
                                                       name='DynAttnWrap')

sampler = ScheduledEmbeddingTrainingHelper(decoder_inputs,
                                                        sequence_length=self.SL_dec,
                                                        embedding=embedding_in,
                                                        sampling_probability=self.samp_prob)
decoder = BasicDecoder(wrapped_cell,
                                       sampler,
                                       encoder_state)
outputs, _ = dynamic_decode(decoder)
	</description>
	<comments>
		<comment id='1' author='RobRomijnders' date='2017-03-30T23:04:23Z'>
		Where is DynamicAttentionWrapper? It does not seem to be in tf.contrib.seq2seq. Is AttentionWrapper now dynamic by default?
		</comment>
		<comment id='2' author='RobRomijnders' date='2017-03-31T08:37:29Z'>
		The dynamic attention wrapper is &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py&gt;here&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='RobRomijnders' date='2017-03-31T19:48:03Z'>
		Ok I see the confusion. According to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commits/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py&gt;this&lt;/denchmark-link&gt;
 it was renamed a few days ago from DynamicAttentionWrapper to AttentionWrapper. Many edits since, try to repull and test. I'll test and let you know.
		</comment>
		<comment id='4' author='RobRomijnders' date='2017-03-31T21:04:50Z'>
		I apologize, but we typically cannot support the contrib directories of tensorflow unless they are underlying tensorflow bugs.
&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='5' author='RobRomijnders' date='2017-04-01T00:17:48Z'>
		That's right, the decoder needs a new, separate, type of state than what
the encoder emitted.
As of today I pushed a new method to AttentionWrapperState object called
.clone().  You would use it thus:
initial_state = wrapper.zero_state(...).clone(cell_state=encoder_state)
and pass that initial_state to the decoder.
		</comment>
		<comment id='6' author='RobRomijnders' date='2017-04-01T04:11:18Z'>
		(i may consider adding an initial_cell_state= argument when constructing the wrapper; but that approach mixes concerns.  we'll see)
		</comment>
		<comment id='7' author='RobRomijnders' date='2017-04-02T22:18:06Z'>
		There's something I'm not getting, and it may be more theoretical. If the decoder's cell is an AttentionWrapper, which in turn attends to the output of the encoder RNN, why must the decoder's initial_state be an AttentionWrapperState?
Also I checked for the .clone(...) in attention_wrapper.py on master but couldn't find it. I'm probably blind though.
		</comment>
		<comment id='8' author='RobRomijnders' date='2017-04-02T22:31:47Z'>
		Oh my bad, it's just a "double" initialization, initilization for the hidden states and for the attention.
		</comment>
		<comment id='9' author='RobRomijnders' date='2017-04-02T23:10:37Z'>
		Ok, nope. I tried to set the initial cell state and attention, based on the other attention's zero_state method, but no go.
Here's what I tried
    # Attention Mechanisms. Bahdanau is additive style attention
    attn_mech = tf.contrib.seq2seq.BahdanauAttention(
        num_units = mem_units, # depth of query mechanism
        memory = attention_states, # hidden states to attend (output of RNN)
        memory_sequence_length=seq_len_enc, # masks false memories
        normalize=False, # normalize energy term
        name='BahdanauAttention')

    # Attention Wrapper: adds the attention mechanism to the cell
    attn_cell = tf.contrib.seq2seq.AttentionWrapper(
        cell = cell,# Instance of RNNCell
        attention_mechanism = attn_mech, # Instance of AttentionMechanism
        attention_size = attn_units, # Int, depth of attention (output) tensor
        attention_history=False, # whether to store history in final output
        name="attention_wrapper")

    # TrainingHelper does no sampling, only uses inputs
    helper = tf.contrib.seq2seq.TrainingHelper(
        inputs = x, # decoder inputs
        sequence_length = seq_len_dec, # decoder input length
        name = "decoder_training_helper")

    # Decoder setup
    batch_size = tf.shape(x)[0]
    attn_zero = attn_cell.zero_state(batch_size=batch_size, dtype=tf.float32)
    init_state = tf.contrib.seq2seq.AttentionWrapperState(\
                cell_state=encoder_state,
                attention=attn_zero,
                time=0,
                attention_history=())
    decoder = tf.contrib.seq2seq.BasicDecoder(
              cell = attn_cell,
              helper = helper, # A Helper instance
              initial_state = init_state, # initial state of decoder
              output_layer = None) # instance of tf.layers.Layer, like Dense

    # Perform dynamic decoding with decoder object
    outputs, final_state = tf.contrib.seq2seq.dynamic_decode(decoder)
But got an error on the dynamic_decode (below). Maybe you have instructions on how to properly set this up?
  File "/home/andre/projects/seq2seq_drr/enc_dec.py", line 232, in decoder_train_attn
    outputs, final_state = tf.contrib.seq2seq.dynamic_decode(decoder)
  File "/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py", line 278, in dynamic_decode
    swap_memory=swap_memory)
  File "/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2623, in while_loop
    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File "/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2456, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File "/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2406, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File "/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py", line 231, in body
    decoder_finished) = decoder.step(time, inputs, state)
  File "/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py", line 140, in step
    cell_outputs, cell_state = self._cell(inputs, state)
  File "/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py", line 532, in __call__
    cell_inputs = self._cell_input_fn(inputs, state.attention)
  File "/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py", line 443, in &lt;lambda&gt;
    lambda inputs, attention: array_ops.concat([inputs, attention], -1))
  File "/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py", line 1036, in concat
    name=name)
  File "/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py", line 519, in _concat_v2
    name=name)
  File "/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py", line 464, in apply_op
    raise TypeError("%s that don't all match." % prefix)
TypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [float32, &lt;NOT CONVERTIBLE TO TENSOR&gt;] that don't all match.
		</comment>
		<comment id='10' author='RobRomijnders' date='2017-04-03T02:28:18Z'>
		You want:

init_state = attn_zero.clone(cell_state=encoder_final_state)
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Apr 2, 2017 4:11 PM, "Andre Cianflone" ***@***.***&gt; wrote:
 Ok, nope. I tried to set the initial cell state and attention, based on
 the other attention's zero_state method, but no go.

 Here's what I tried

     # Attention Mechanisms. Bahdanau is additive style attention
     attn_mech = tf.contrib.seq2seq.BahdanauAttention(
         num_units = mem_units, # depth of query mechanism
         memory = attention_states, # hidden states to attend (output of RNN)
         memory_sequence_length=seq_len_enc, # masks false memories
         normalize=False, # normalize energy term
         name='BahdanauAttention')

     # Attention Wrapper: adds the attention mechanism to the cell
     attn_cell = tf.contrib.seq2seq.AttentionWrapper(
         cell = cell,# Instance of RNNCell
         attention_mechanism = attn_mech, # Instance of AttentionMechanism
         attention_size = attn_units, # Int, depth of attention (output) tensor
         attention_history=False, # whether to store history in final output
         name="attention_wrapper")

     # TrainingHelper does no sampling, only uses inputs
     helper = tf.contrib.seq2seq.TrainingHelper(
         inputs = x, # decoder inputs
         sequence_length = seq_len_dec, # decoder input length
         name = "decoder_training_helper")

     # Decoder setup
     batch_size = tf.shape(x)[0]
     attn_zero = attn_cell.zero_state(batch_size=batch_size, dtype=tf.float32)
     init_state = tf.contrib.seq2seq.AttentionWrapperState(\
                 cell_state=encoder_state,
                 attention=attn_zero,
                 time=0,
                 attention_history=())
     decoder = tf.contrib.seq2seq.BasicDecoder(
               cell = attn_cell,
               helper = helper, # A Helper instance
               initial_state = init_state, # initial state of decoder
               output_layer = None) # instance of tf.layers.Layer, like Dense

     # Perform dynamic decoding with decoder object
     outputs, final_state = tf.contrib.seq2seq.dynamic_decode(decoder)

 But got an error on the dynamic_decode (below). Maybe you have
 instructions on how to properly set this up?

   File "/home/andre/projects/seq2seq_drr/enc_dec.py", line 232, in decoder_train_attn
     outputs, final_state = tf.contrib.seq2seq.dynamic_decode(decoder)
   File "/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py", line 278, in dynamic_decode
     swap_memory=swap_memory)
   File "/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2623, in while_loop
     result = context.BuildLoop(cond, body, loop_vars, shape_invariants)
   File "/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2456, in BuildLoop
     pred, body, original_loop_vars, loop_vars, shape_invariants)
   File "/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2406, in _BuildLoop
     body_result = body(*packed_vars_for_body)
   File "/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py", line 231, in body
     decoder_finished) = decoder.step(time, inputs, state)
   File "/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py", line 140, in step
     cell_outputs, cell_state = self._cell(inputs, state)
   File "/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py", line 532, in __call__
     cell_inputs = self._cell_input_fn(inputs, state.attention)
   File "/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py", line 443, in &lt;lambda&gt;
     lambda inputs, attention: array_ops.concat([inputs, attention], -1))
   File "/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py", line 1036, in concat
     name=name)
   File "/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py", line 519, in _concat_v2
     name=name)
   File "/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py", line 464, in apply_op
     raise TypeError("%s that don't all match." % prefix)
 TypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [float32, &lt;NOT CONVERTIBLE TO TENSOR&gt;] that don't all match.

 —
 You are receiving this because you modified the open/close state.
 Reply to this email directly, view it on GitHub
 &lt;#8833 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtim59Wb4MPNiKHa_gNxHRQyMIlwWW8ks5rsCsWgaJpZM4MuICf&gt;
 .



		</comment>
		<comment id='11' author='RobRomijnders' date='2017-04-04T23:17:30Z'>
		Awesome, finally got it to work. Your clone method only popped up today when I pulled, for whatever reason.
		</comment>
		<comment id='12' author='RobRomijnders' date='2017-05-12T07:51:44Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 I tried to use your previous code as explained. But If I put the encoder_state of the previous level (2 bidirectional encoders with GRU), I have a memory leak on GPU.
More precisely, here my code:
&lt;denchmark-code&gt;        W_embedding_output = tf.Variable(tf.random_uniform([decoder_output_dim, self.vocab_size], -0.01, 0.01), name="embedding_output")
        W_t = tf.transpose(W_embedding_output, [1, 0])
        output_y_embedded = tf.nn.embedding_lookup(W_t, output_y)

        # base cell to use for the decoder (same of inference, also for attention technique)
        base_cell = DropoutWrapper(GRUCell(decoder_output_dim), self.dropout, 1.0, self.dropout)
        attention_technique = BahdanauAttention(decoder_output_dim, encoded)
        # helper to fit the previous state into decoder
        helper = TrainingHelper(output_y_embedded, output_batch_length)
        # cell to use, with attention method
        cell = DynamicAttentionWrapper(
            base_cell,
            attention_technique,
            self.encoder_dim,
            output_attention=False
        )
        # initial state of the decoder, should be use the last encoder state as initial point?
        zero_state = cell.zero_state(self.batch_size, tf.float32)

        # zero_state = zero_state.clone(cell_state=encoder_state)

        # decode
        (outputs, index), final_state = dynamic_decode(
            BasicDecoder(
                cell,
                helper,
                zero_state)
        )
&lt;/denchmark-code&gt;

So, If I remove use the line zero_state = zero_state.clone(cell_state=encoder_state) during the session.run the program continuously allocate memory on GPU and finally it crashes due to device memory become full.
Here the error.
&lt;denchmark-code&gt;ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[9024,1024]
	 [[Node: gradients/tower2/Decoder/MatMul_grad/MatMul = MatMul[T=DT_FLOAT, _class=["loc:@tower2/Decoder/MatMul"], transpose_a=false, transpose_b=true, _device="/job:localhost/replica:0/task:0/gpu:2"](gradients/tower2/Decoder/output_grad/Reshape, tower2/Decoder/embedding_output/read)]]
	 [[Node: gradients/Sub_2/_942 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/cpu:0", send_device="/job:localhost/replica:0/task:0/gpu:2", send_device_incarnation=1, tensor_name="edge_9973_gradients/Sub_2", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/cpu:0"](^_cloopgradients/tower2/Decoder/decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/_828)]]
&lt;/denchmark-code&gt;

Am I using these classes in a wrong way? Could it be a bug?
		</comment>
		<comment id='13' author='RobRomijnders' date='2017-05-18T15:13:30Z'>
		I'd need to see how you're calling session.run and the rest of your code to see if you're doing anything else wrong.  I suggest starting a thread on StackOverflow for this since it could be user error.  This thread is closed since the originally reported issue has been fixed in TensorFlow.
		</comment>
		<comment id='14' author='RobRomijnders' date='2017-06-09T21:38:55Z'>
		AttributeError: 'DynamicAttentionWrapperState' object has no attribute 'clone'
&lt;denchmark-link:https://github.com/ProximaCent&gt;@ProximaCent&lt;/denchmark-link&gt;
 Do you know why I'm getting this error or how I can fix it? It's specifically happening on my line:
init_state = attention_zero.clone(cell_state=encoder_final_states)
(I pluralize encoder final states because I'm using a bidirectional RNN. The two states are concatenated, though.)
		</comment>
		<comment id='15' author='RobRomijnders' date='2017-06-12T17:02:22Z'>
		For anyone else coming along, I think I found the problem*. &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 's comments are based on TensorFlow 1.2, not TensorFlow 1.1, so if you want to solve this problem, you'll need to switch. Then, you'll want to replace DynamicAttentionWrapper with AttentionWrapper.
*if I'm wrong, anyone can feel free to correct me.
		</comment>
		<comment id='16' author='RobRomijnders' date='2017-06-12T17:11:38Z'>
		&lt;denchmark-link:https://github.com/RylanSchaeffer&gt;@RylanSchaeffer&lt;/denchmark-link&gt;
 , yes that's correct. At the time, ebrevdo's code was pre 1.2 but post 1.1. In the 1.2 release, some other naming has changed as well. For example AttentionWrapper's  is now . If you are using my above code, check the &lt;denchmark-link:https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/seq2seq/AttentionWrapper&gt;AttentionWrapper page&lt;/denchmark-link&gt;
 for the new  arguments.
		</comment>
		<comment id='17' author='RobRomijnders' date='2017-06-12T17:32:59Z'>
		&lt;denchmark-link:https://github.com/ProximaCent&gt;@ProximaCent&lt;/denchmark-link&gt;
 , I'm running into a new problem. Can I email you? There's not much (up-to-date) material online, and it sounds like you've already done something similar to what I'm working on.
		</comment>
		<comment id='18' author='RobRomijnders' date='2017-06-12T17:35:46Z'>
		My problem is that somewhere inside my decoder, the following line produces this error: TypeError: 'Tensor' object is not iterable.
c, h = state
I've stepped through my code, and the failure seems to be generated when something causes the state variable to be a tensor (instead of a tuple) with the name "define_model/define_decoder/decoder/while/Identity_3"
define_model() and define_decoder() are functions I wrote, but I don't know what inside decoder would be creating this Identity_3 and why it isn't a tuple.
I know this isn't the right place for this. &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 , perhaps I could email you directly?
		</comment>
		<comment id='19' author='RobRomijnders' date='2017-06-12T17:51:52Z'>
		&lt;denchmark-link:https://github.com/RylanSchaeffer&gt;@RylanSchaeffer&lt;/denchmark-link&gt;
, what's , what is the output of ? If I had to guess, maybe you changed from LSTM cell to GRU. Some functions, like tf.nn.dynamic_rnn, return a pair of (output, state), where  is LSTMStateTuple if you are using an LSTM cell, (which have  and ), but a Tensor if using a GRU cell.
		</comment>
		<comment id='20' author='RobRomijnders' date='2017-06-12T20:56:13Z'>
		&lt;denchmark-link:https://github.com/ProximaCent&gt;@ProximaCent&lt;/denchmark-link&gt;
 , it depends on when I evaluate . For the first two times that  is evaluated, state has type . Then, (just before my code breaks), state has type .
I am using LSTM cells, not GRU cells (specifically BasicLSTMCell from tensorflow.contrib.rnn). Why and where would my code be switching RNN cell type?
Also, thanks for the help!
		</comment>
		<comment id='21' author='RobRomijnders' date='2017-06-13T16:36:32Z'>
		&lt;denchmark-link:https://github.com/RylanSchaeffer&gt;@RylanSchaeffer&lt;/denchmark-link&gt;
 , please paste some code. Should have , the code where  evaluates to LSTMStateTuple, Tensor, and code in between.
		</comment>
		<comment id='22' author='RobRomijnders' date='2017-06-13T16:55:50Z'>
		&lt;denchmark-link:https://github.com/ProximaCent&gt;@ProximaCent&lt;/denchmark-link&gt;
 , here's some of the code that I wrote:
&lt;denchmark-code&gt;            bahdanau_attention = BahdanauAttention(num_units=DECODER_NUM_UNITS,
                                                   memory=encoder_outputs)

            attention_cell = AttentionWrapper(cell=self._create_lstm_cell(),
                                              attention_mechanism=bahdanau_attention)

            attention_zero = attention_cell.zero_state(batch_size=self.x.shape[0], dtype=tf.float32)

            init_state = attention_zero.clone(cell_state=encoder_final_states)

            training_helper = TrainingHelper(inputs=self.y,  # feed in ground truth
                                             sequence_length=length)  # feed in sequence length

            decoder = BasicDecoder(cell=attention_cell,
                                   helper=training_helper,
                                   initial_state=init_state)
&lt;/denchmark-code&gt;

The error is raised in line 379 c, h = state of TensorFlow's rnn_cell_impl.py. If you want, I can paste that code.
If I set a breakpoint right before line 379, the first two times I reach the breakpoint, type(state) evaluates to &lt;class 'tensorflow.python.ops.rnn_cell_impl.LSTMStateTuple'&gt;. The third time, type(state) evaluates to &lt;class 'tensorflow.python.framework.ops.Tensor'&gt;.
		</comment>
		<comment id='23' author='RobRomijnders' date='2017-06-13T18:04:59Z'>
		I'm away this week but hope one of my collaborators can help with this
issue.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Jun 13, 2017 9:56 AM, "Rylan Schaeffer" ***@***.***&gt; wrote:
 Here's some of the code that I wrote:
 `

         bahdanau_attention = BahdanauAttention(num_units=DECODER_NUM_UNITS,
                                                memory=encoder_outputs)

         attention_cell = AttentionWrapper(cell=self._create_lstm_cell(),
                                           attention_mechanism=bahdanau_attention)

         attention_zero = attention_cell.zero_state(batch_size=self.x.shape[0], dtype=tf.float32)

         init_state = attention_zero.clone(cell_state=encoder_final_states)

         training_helper = TrainingHelper(inputs=self.y,  # feed in ground truth
                                          sequence_length=length)  # feed in sequence length

         decoder = BasicDecoder(cell=attention_cell,
                                helper=training_helper,
                                initial_state=init_state)

 `

 The error is raised in line 379 c, h = state of TensorFlow's
 rnn_cell_impl.py.

 If I set a breakpoint right before this line, the first two times I reach
 the breakpoint, type(state) evaluates to &lt;class
 'tensorflow.python.ops.rnn_cell_impl.LSTMStateTuple'&gt;. The third time,
 type(state) evaluates to &lt;class 'tensorflow.python.framework.ops.Tensor'&gt;.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#8833 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtim2rkcUu1rNuBoBGGn5y2g8W4sGDzks5sDr9LgaJpZM4MuICf&gt;
 .



		</comment>
		<comment id='24' author='RobRomijnders' date='2017-06-13T18:44:25Z'>
		Hi &lt;denchmark-link:https://github.com/RylanSchaeffer&gt;@RylanSchaeffer&lt;/denchmark-link&gt;
,
Let me see if I can help you. What is the structure of your decoder? Is it a MultiRNNCell with several BasicLSTMCells? Can you print what is exactly in the encoder_final_states? (Is it something like (LSTMStateTuple, LSTMStateTuple, Tensor)?)
		</comment>
		<comment id='25' author='RobRomijnders' date='2017-06-13T19:47:48Z'>
		&lt;denchmark-link:https://github.com/oahziur&gt;@oahziur&lt;/denchmark-link&gt;
 , thank you! My decoder is not a MultiRNNCell - it's just one BasicLSTMCell with an AttentionWrapper. Here is my method I use to create an LSTM cell:
&lt;denchmark-code&gt;@staticmethod  
def _create_lstm_cell():  
    return BasicLSTMCell(LSTM_SIZE)
&lt;/denchmark-code&gt;

Here is my function call to wrap the cell with a Bahdanau Attention mechanism:
&lt;denchmark-code&gt;attention_cell = AttentionWrapper(cell=self._create_lstm_cell(),
                           attention_mechanism=bahdanau_attention)
&lt;/denchmark-code&gt;

And finally here is how I pass the attention cell BasicDecoder:
&lt;denchmark-code&gt;decoder = BasicDecoder(cell=attention_cell,
                  helper=training_helper,
                  initial_state=init_state)
&lt;/denchmark-code&gt;

My encoder_final_states has type &lt;class 'tensorflow.python.framework.ops.Tensor'&gt; and shape=(2, ?, 128); this makes sense as I'm using a Bidirectional RNN and my LSTM_SIZE = 64. encoder_final_states one of the two values returned by the following method:
&lt;denchmark-code&gt;def _define_encoder(self):
    with tf.name_scope('define_encoder'):
        outputs, final_states = bidirectional_dynamic_rnn(cell_fw=self._create_lstm_cell(),
                                              cell_bw=self._create_lstm_cell(),
                                              inputs=self.x,
                                              dtype=tf.float32)
        outputs = tf.concat(outputs, axis=-1)
        final_states = tf.concat(final_states, axis=-1)

    return outputs, final_states
&lt;/denchmark-code&gt;

Sorry about the formatting. I tried to clean it up best I could.
		</comment>
		<comment id='26' author='RobRomijnders' date='2017-06-13T20:41:22Z'>
		&lt;denchmark-link:https://github.com/RylanSchaeffer&gt;@RylanSchaeffer&lt;/denchmark-link&gt;
 , if your LSTM_SIZE=64, the final state should be (2, ?, 256) tensor if you set &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell&gt;state_is_tuple=False in BasicLSTM&lt;/denchmark-link&gt;
, since each state is 64, so a single state should have 128.
I think you shouldn't do  if  (the default behavior) since the return value of &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/nn/bidirectional_dynamic_rnn&gt;bidirectional_dynamic_rnn&lt;/denchmark-link&gt;
 will be a tuple of LSTMStateTuple instead of a tuple of Tensor. Concatenate c state and h state separately and then create a   to pass to the decoder should fix your problem.
		</comment>
		<comment id='27' author='RobRomijnders' date='2017-06-13T21:15:42Z'>
		&lt;denchmark-link:https://github.com/oahziur&gt;@oahziur&lt;/denchmark-link&gt;
 , I'm a little confused. My understanding was that  isn't concatenating the c state and the h state for an LSTM cell, but rather concatenating states of the two RNNs that together compose the bidirectional RNN i.e. c1 concatenated with c2, h1 concatenated with h2. Can you clarify?
		</comment>
		<comment id='28' author='RobRomijnders' date='2017-06-13T21:20:55Z'>
		&lt;denchmark-link:https://github.com/RylanSchaeffer&gt;@RylanSchaeffer&lt;/denchmark-link&gt;
 , I think the bidirectional_dynamic_rnn returns outputs and final states.
You have use outputs = tf.concat(outputs, axis=-1) to concat the outputs, which isn't a problem.
If you print final_states before final_states = tf.concat(final_states, axis=-1), it should be (LSTMStateTuple, LSTMStateTuple) because you have one fw_cell and one bw_cell.
		</comment>
		<comment id='29' author='RobRomijnders' date='2017-06-13T21:29:15Z'>
		&lt;denchmark-link:https://github.com/oahziur&gt;@oahziur&lt;/denchmark-link&gt;
 ok, that makes sense! I'm now confused by what you mean when you say, "then create a tf.contrib.rnn.LSTMStateTuple to pass to the decoder should fix your problem." Can you explain more?
		</comment>
		<comment id='30' author='RobRomijnders' date='2017-06-13T21:32:40Z'>
		&lt;denchmark-link:https://github.com/oahziur&gt;@oahziur&lt;/denchmark-link&gt;
 , my confusion stems from the fact that  accepts  Since I pass in , where  and  is a 2-tuple of LSTMStateTuples, it doesn't seem like I should need to make any modifications.
And yet dynamic_decode(decoder=decoder) now gives me the following error: AttributeError: 'LSTMStateTuple' object has no attribute 'get_shape'
		</comment>
		<comment id='31' author='RobRomijnders' date='2017-06-13T21:41:43Z'>
		&lt;denchmark-link:https://github.com/RylanSchaeffer&gt;@RylanSchaeffer&lt;/denchmark-link&gt;
 The problem is that you decoder only have 1 BasicLSTMCell so it can only accept a single LSTMStateTuple, but you have 2 LSTMStateTuples from the .
Is this helpful?
		</comment>
		<comment id='32' author='RobRomijnders' date='2017-06-13T22:20:45Z'>
		&lt;denchmark-link:https://github.com/oahziur&gt;@oahziur&lt;/denchmark-link&gt;
 , let me try to explain what my current understanding is, and you can tell me if I'm correct. A BasicDecoder accepts only one cell. One cell corresponds to one LSTMStateTuple (consisting of c and h). My problem is that I have two LSTMStateTuples. To reduce the two LSTMStateTuples down to one LSTMStateTuple, I need to separately concatenate the c states (i.e. c = c1 concat c2) and the h states (i.e. h = h1 concat h2), and then pass the newly created c and h states into a new LSTMStateTuple.
If that's correct, my next question is where I would use this new LSTMStateTuple? BasicDecoder accepts a cell, not a LSTMStateTuple. Would I pass the LSTMStateTuple in as the initial state? That makes the most sense. But then how do I do this? Do I change my code from init_state = attention_zero.clone(cell_state=encoder_final_states) to init_state = attention_zero.clone(cell_state=new_lstm_state_tuple)?
		</comment>
		<comment id='33' author='RobRomijnders' date='2017-06-13T22:23:55Z'>
		&lt;denchmark-link:https://github.com/RylanSchaeffer&gt;@RylanSchaeffer&lt;/denchmark-link&gt;
 Yes. your understanding is correct.
Just a reminder, I believe your decoder lstm cell needs to have 2*LSTM_SIZE in order to match the shape of the new concatenated state.
		</comment>
		<comment id='34' author='RobRomijnders' date='2017-06-13T22:25:05Z'>
		Makes sense. &lt;denchmark-link:https://github.com/oahziur&gt;@oahziur&lt;/denchmark-link&gt;
 Thank you so much!
		</comment>
		<comment id='35' author='RobRomijnders' date='2017-06-14T18:29:43Z'>
		&lt;denchmark-link:https://github.com/oahziur&gt;@oahziur&lt;/denchmark-link&gt;
 , I have an unrelated, general question about best practices. I'm currently trying to use RNNs to solve the problem I'm working on, but I'd also like to consider using fully convolutional models as well. How would you recommend storing and processing my data in a way conducive to training both types of models?
Currently, I've written a script to parse all of my data (originally in .csv files) into tf.train.SequenceExamples and write those to TFRecords. Then I use a TFRecordReader in conjunction with tf.parse_single_sequence_example to pipe data into my RNN models for training. However, I don't think that this will work for convolutional models.
I don't know if asking here makes sense. Creating a new issue wouldn't make sense, but people on StackOverflow don't seem to respond to TensorFlow questions with the same alacrity (if at all).
		</comment>
		<comment id='36' author='RobRomijnders' date='2017-06-15T01:45:51Z'>
		&lt;denchmark-link:https://github.com/RylanSchaeffer&gt;@RylanSchaeffer&lt;/denchmark-link&gt;
 I am not very familiar with modeling sequence data with convolutional models and your use cases, but if you data is fixed length, I think you can always reshape the input Tensor to fit your convolutional model after you parse the example.
		</comment>
		<comment id='37' author='RobRomijnders' date='2017-06-15T15:24:50Z'>
		&lt;denchmark-link:https://github.com/oahziur&gt;@oahziur&lt;/denchmark-link&gt;
 , can I try asking the question another way? Is there a protocol buffer other than  that makes feeding data to a dynamic RNN easy?
To give more information, my data has the following shape: [number of samples per minibatch, time steps per sample, number of features]. Both number of samples per minibatch and number of features are fixed, but time steps per sample can vary (hence the use of a dynamic RNN). At least conceptually, it should be possible to reshape my data as [number of samples per minibatch, time steps per sample, number of features, 1] and treat this akin to a single-channel image (which can then be passed through convolutional layers). However, I'm confused by how I would parse data stored as a SequenceExample in a way that would let me do this reshaping.
		</comment>
		<comment id='38' author='RobRomijnders' date='2017-06-15T15:27:15Z'>
		I spent more time reading up on , and I realized that I was incorrectly following a &lt;denchmark-link:http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/&gt;tutorial&lt;/denchmark-link&gt;
. His example uses sequences that have single features, but I can't find a tutorial that uses sequences with multiple features.
I think the reason why I couldn't directly reshape my data is because I was improperly structuring my sequences in SequenceExample. How do I correctly structure sequential data with multiple features in a SequenceExample?
		</comment>
		<comment id='39' author='RobRomijnders' date='2017-06-15T18:14:26Z'>
		Never mind. Everything just clicked. Thanks for the help!
		</comment>
		<comment id='40' author='RobRomijnders' date='2017-06-21T22:42:16Z'>
		Funny, I was playing with my code and I think I got the exact error &lt;denchmark-link:https://github.com/RylanSchaeffer&gt;@RylanSchaeffer&lt;/denchmark-link&gt;
 got. Basically I was switching back and forth between GRU and LSTM which don't have the same outputs from the  function. The problem arises from:
&lt;denchmark-code&gt;      outputs, state = tf.nn.bidirectional_dynamic_rnn(\
                  cell_fw=cell_fw,
                  cell_bw=cell_bw,
                  inputs=x,
                  sequence_length=seq_len,
                  initial_state_fw=init_state_fw,
                  initial_state_bw=init_state_bw,
                  dtype=self.float_type)
&lt;/denchmark-code&gt;

If your cells are LSTM, then state is a tuple of 2 LSTMStateTuple, and if your cells are GRU then state is a tuple of Tensor. As mentioned, for LSTMStateTuple you need to "hack" concat state. I also have to set_shape to the state Tensor otherwise you can get issues with decoding. The code below works for both GRU and LSTM:
&lt;denchmark-code&gt;      # If LSTM cell, then "state" is not a tuple of Tensors but an
      # LSTMStateTuple of "c" and "h". Need to concat separately then new
      if "LSTMStateTuple" in str(type(state[0])):
        c = tf.concat([state[0][0],state[1][0]],axis=1)
        h = tf.concat([state[0][1],state[1][1]],axis=1)
        state = tf.contrib.rnn.LSTMStateTuple(c,h)
      else:
        state = tf.concat(state,1)
        state.set_shape([None, bi_encoder_size])
&lt;/denchmark-code&gt;

To me this seems a little hackish, but works for now. I think tf.nn.bidirectional_dynamic_rnn should have an optional "concat" arg which checks in the background what type of cell was passed and concats accordingly.
Cheers
		</comment>
		<comment id='41' author='RobRomijnders' date='2017-06-22T19:55:13Z'>
		&lt;denchmark-link:https://github.com/ProximaCent&gt;@ProximaCent&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/oahziur&gt;@oahziur&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;

I'm trying to train a sequence to sequence model, but the training times are monotonically increasing. I posted on &lt;denchmark-link:https://stackoverflow.com/questions/44706150/tensorflow-seq2seq-training-time-per-minibatch-monotonically-increases&gt;StackOverflow&lt;/denchmark-link&gt;
, but I thought I'd ask in case any of you have an idea of what I might be doing wrong.
		</comment>
	</comments>
</bug>