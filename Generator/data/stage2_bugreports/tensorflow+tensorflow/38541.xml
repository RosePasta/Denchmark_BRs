<bug id='38541' author='remifan' open_date='2020-04-14T16:03:05Z' closed_time='2020-06-04T17:50:35Z'>
	<summary>Issues of Adam Optimizer on Complex domain</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):  Yes
OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04):  Linux Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device:
TensorFlow installed from (source or
binary): conda install -c conda-forge tensorflow-gpu=2.1
TensorFlow version (use command below):  2.1.0
Python version: - 3.7.7
version (if compiling from source):
GCC/Compiler version (if compiling from
source):
CUDA/cuDNN version: - GPU model and memory: 10.1.243/7.6.5

Describe the current behavior
I'm building a custom model that does computation in Complex value domain, we find the performance in accuracy sense by using Adam is worse than that by translating the whole model into a real domain counterpart (2 times network parameters, real and imag parts ), and use same Adam optimizer.
I dig into the sources codes of &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/f270180a6caa8693f2b2888ac7e6b8e69c4feaa8/tensorflow/core/kernels/training_ops_gpu.cu.cc#L57&gt;Adam&lt;/denchmark-link&gt;
, line 57:
v_i += one_minus_beta2 * (g_i * g_i - v_i);
for real numbers, it is correct as g_i * g_i standards for 2nd order moments, while g_i * g_i in the Complex domain is pseudo-variance. Andy M. Sarrof also discuss this behavior in his &lt;denchmark-link:https://andysarroff.com/papers/sarroff2018a.pdf&gt;thesis&lt;/denchmark-link&gt;
 (page 35, equation 3.56)
However, it is expected that this behavior does not throw errors because it's valid operation.
Describe the expected behavior
I suppose in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/f270180a6caa8693f2b2888ac7e6b8e69c4feaa8/tensorflow/core/kernels/training_ops_gpu.cu.cc#L57&gt;line 57&lt;/denchmark-link&gt;
,   should be replace by  as pointed out by Andy M. Sarrof in this thesis [https://andysarroff.com/papers/sarroff2018a.pdf] (page 35, equation 3.56)
	</description>
	<comments>
		<comment id='1' author='remifan' date='2020-04-15T05:20:34Z'>
		&lt;denchmark-link:https://github.com/remifan&gt;@remifan&lt;/denchmark-link&gt;

Request you to share colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!
		</comment>
		<comment id='2' author='remifan' date='2020-04-22T06:08:32Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='3' author='remifan' date='2020-04-22T07:59:30Z'>
		&lt;denchmark-link:https://github.com/remifan&gt;@remifan&lt;/denchmark-link&gt;

Any update on this issue please. Thanks!
		</comment>
		<comment id='4' author='remifan' date='2020-04-25T03:47:07Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
 Sorry for being inactive so long, I'm preparing a simple test case to reproduce the issue, I'll update soon, thanks!!
		</comment>
		<comment id='5' author='remifan' date='2020-04-25T09:59:20Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
 I've made a simple demo to illustrate the concept, please check the this &lt;denchmark-link:https://github.com/remifan/complex_adam/blob/master/complex_adam_example.ipynb&gt;notebook&lt;/denchmark-link&gt;

 is my modified Adam using  as "variance".
In this simple demo, the x trace of TF2's Adam is biased from the obviously optimal trace that theory predicts. I suppose the 2nd-moment term in Adam is intended to regularize the step size in "intensity" sense for reasonable heuristics, rather than touching the direction of the gradients based on g * g whose angle is not quite related to the intuitive "variance".
Another possible issue of native Adam is that it does not implement the amsgrad=True (it'd raise an errorNo registered 'ResourceApplyAdamWithAmsgrad' OpKernel ....) version on complex numbers. This issue is expected because 2nd-moment v_t is complex (since g*g is complex), comparing complex numbers makes no sense, While using g*conj(g) as "variance", v_t becomes real numbers, amsgrad is then valid by replacing maximum function with function like cxmax = lambda a,b: math_ops.cast(math_ops.maximum(math_ops.real(a), math_ops.real(b)), var_dtype) to be valid in complex dtype.
		</comment>
		<comment id='6' author='remifan' date='2020-04-27T07:02:28Z'>
		&lt;denchmark-link:https://github.com/remifan&gt;@remifan&lt;/denchmark-link&gt;

I tried in colab with TF-GPU 2.1.0 and i am seeing below error (ModuleNotFoundError: No module named 'myadam').Request you to provide colab link or reproducible code in our environment.It helps us in localizing the issue faster.Thanks!
		</comment>
		<comment id='7' author='remifan' date='2020-04-27T08:40:41Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
 I have updated that &lt;denchmark-link:https://github.com/remifan/complex_adam/blob/master/complex_adam_example.ipynb&gt;notebook &lt;/denchmark-link&gt;
to be working inside Colab, or please check this &lt;denchmark-link:https://colab.research.google.com/github/remifan/complex_adam/blob/master/complex_adam_example.ipynb&gt;shared colab&lt;/denchmark-link&gt;
 for your convenience, thanks!
		</comment>
		<comment id='8' author='remifan' date='2020-05-25T23:28:54Z'>
		&lt;denchmark-link:https://github.com/gowthamkpr&gt;@gowthamkpr&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
 Hey, any updates on this issue? Is this issue confirmed from your side?
		</comment>
		<comment id='9' author='remifan' date='2020-06-04T17:50:35Z'>
		Thank you for this feature request. Because we don't have widespread support for complex numbers in Keras, we can't fix this in one place and ignore the maintenance implications across the rest of the codebase. That said, an Adam Optimizer that handles complex numbers might be of interest to the TF Addons community repo, where special subclasses of Optimizers can live: &lt;denchmark-link:https://github.com/tensorflow/addons&gt;https://github.com/tensorflow/addons&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='remifan' date='2020-06-04T17:50:36Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38541&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38541&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>