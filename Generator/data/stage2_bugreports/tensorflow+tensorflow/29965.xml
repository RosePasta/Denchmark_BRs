<bug id='29965' author='timakro' open_date='2019-06-19T14:56:41Z' closed_time='2019-06-26T00:31:24Z'>
	<summary>"mean_squared_error" gives incorrect results in conjunction with sample weights</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): 1.13.1
Python version: 3.7.3

There is a difference between passing the string "mean_squared_error" or passing tf.keras.losses.mean_squared_error as loss function in conjunction with sample weights.
import numpy as np
import tensorflow as tf

def test_loss_function(loss_function):
    print("Testing {}".format(loss_function))

    layer = tf.keras.layers.Input(shape=(1,))
    model = tf.keras.Model(inputs=layer, outputs=layer)

    model.compile(optimizer='adam',
                  loss=loss_function)

    weights = np.array([1., 1., 1., 1., 1., 0., 0., 0., 0., 0.])
    model.evaluate(np.zeros(10), np.ones(10), sample_weight=weights)

test_loss_function('mean_squared_error')
test_loss_function(tf.keras.losses.mean_squared_error)
Output:
&lt;denchmark-code&gt;Testing mean_squared_error
10/10 [==============================] - 0s 4ms/sample - loss: 0.5000
Testing &lt;function mean_squared_error at 0x7f657d0d6a60&gt;
10/10 [==============================] - 0s 2ms/sample - loss: 1.0000
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='timakro' date='2019-06-19T15:09:58Z'>
		It seems like "mean_squared_error" is simply not weighted by sample_weight. It gives the same result as tf.keras.losses.mean_squared_error as a (non weighted) metric.
		</comment>
		<comment id='2' author='timakro' date='2019-06-20T22:01:42Z'>
		In TensorFlow 2.0 Beta all loss functions seem to behave in the way "mean_squared_error" behaves. This is the output in TensorFlow 2.0.0-beta1:
&lt;denchmark-code&gt;Testing mean_squared_error
10/10 [==============================] - 0s 2ms/sample - loss: 0.5000
Testing &lt;function mean_squared_error at 0x7fb89b5a8ea0&gt;
10/10 [==============================] - 0s 2ms/sample - loss: 0.5000
&lt;/denchmark-code&gt;

I don't know much about the internal workings and I didn't look at the code but this is my interpretation:
The behavior of "mean_squared_error" in TensorFlow 1.13 and all loss functions in TensorFlow 2.0 can be described the following way: The losses per sample (and per time step if sample_weight_mode='temporal') are weighted by sample_weight. Then they are used for SGD, everything good so far. Now the losses are averaged to provide them as a metric to the user. This is done by adding them up and dividing them by the number of losses (the batch size or the batch size times the number of time steps if sample_weight_mode='temporal').
Consider a fixed batch size, the last batch is just containing a few samples and is filled up with zero entries. Sample weights are used to mask these zero entries from SGD. The weighted losses are now consisting of the few losses from the samples and a lot of zeros. The user will see a much lower loss on the last batch. Or in my case with less and less samples per batch (I sort the samples by sequence length before batching) it looked like the loss was going down - if only! Although confusing for the user this is true to the mathematics behind the model.
Loss functions in TensorFlow 1.13 (except "mean_squared_error") instead of dividing by the number of losses divide by the sum of all sample weights. This avoids misinterpretation from the user but also hides the true loss value.
You can add your loss functions additionally as weighted_metrics and they will be calculated the intuitive way. So the TensorFlow 2.0 behavior is superior because it allows for both. Although a warning in the documentation of the sample_weight parameter explaining how to interpret the loss values would be helpful.
		</comment>
		<comment id='3' author='timakro' date='2019-06-26T00:03:33Z'>
		&lt;denchmark-link:https://github.com/timakro&gt;@timakro&lt;/denchmark-link&gt;
 I was able to reproduce the issue with TF1.13. But, in the most recent version TF1.14, the loss function is working as expected. Please check the &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/07cb3ebf319020ff34764bfc568f5ff4/tf29965_metric_error.ipynb&gt;gist&lt;/denchmark-link&gt;
 here. Please let me know what you think. Thanks!
		</comment>
		<comment id='4' author='timakro' date='2019-06-26T00:31:24Z'>
		Thanks, seems fixed.
		</comment>
		<comment id='5' author='timakro' date='2019-06-26T00:31:26Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29965&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29965&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>