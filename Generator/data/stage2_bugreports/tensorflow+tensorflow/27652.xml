<bug id='27652' author='8bitmp3' open_date='2019-04-08T21:03:17Z' closed_time='2019-07-29T17:11:05Z'>
	<summary>[TF 2.0 API Docs] tf.keras.activations.elu</summary>
	<description>
System information

TensorFlow version: 2.0 alpha
Doc Link:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations/elu
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/activations.py

- Links
Links exist. Should format the link to the original paper (delete "-"). Also, should add authors and year of publishing
E.g.
&lt;denchmark-code&gt;Reference: "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)" (Clevert et al, 2015)
&lt;/denchmark-code&gt;


Since it just says:  we can modify it to the following - similar to the &lt;denchmark-link:https://arxiv.org/abs/1511.07289&gt;original paper&lt;/denchmark-link&gt;
 :
&lt;denchmark-code&gt;The exponential linear unit (ELU) with `alpha` &gt; 0 is:
`x` if `x &gt; 0` and `alpha * (exp(x)-1)` if `x &lt; 0`
The ELU hyperparameter `alpha` (α) controls the value to which an ELU saturates for negative net inputs.
ELUs diminish the vanishing gradient effect.
&lt;/denchmark-code&gt;

Followed by word-for-word stuff from the &lt;denchmark-link:https://arxiv.org/abs/1511.07289&gt;original paper&lt;/denchmark-link&gt;
:
&lt;denchmark-code&gt;ELUs have negative values which pushes the mean of the activations closer to zero. 
Mean activations that are closer to zero enable faster learning as they bring the gradient closer to the natural gradient. 
ELUs saturate to a negative value when the argument gets smaller. 
Saturation means a small derivative which decreases the variation and the information that is propagated to the next layer."
&lt;/denchmark-code&gt;

- Examples
No examples given. Can add a modified example from the Intro to CNNs tutorial (where elu replaces ReLU - relu)
E.g.
&lt;denchmark-code&gt;model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='elu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='elu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='elu'))
&lt;/denchmark-code&gt;

- Parameters
Both params defined already but to aid the user it should state that alpha (α) should be set at 1.0  by default and that:
&lt;denchmark-code&gt;`alpha` controls the value to which an ELU saturates for negative net inputs.
&lt;/denchmark-code&gt;

(source: &lt;denchmark-link:https://arxiv.org/abs/1511.07289&gt;paper&lt;/denchmark-link&gt;
)
... instead of just
&lt;denchmark-code&gt;`alpha`: A scalar, slope of negative section
&lt;/denchmark-code&gt;

- Returns
Defined but for clarity should say:
&lt;denchmark-code&gt;The exponential linear unit (ELU) activation function: `x` if `x &gt; 0` and `alpha * (exp(x) - 1)` if `x &lt; 0`
&lt;/denchmark-code&gt;

instead of simply The exponential linear activation:...[equation]
- Raises
Not defined.

Should be added to help the user. Example - see p.5 of the original paper: &lt;denchmark-link:https://arxiv.org/pdf/1511.07289.pdf&gt;https://arxiv.org/pdf/1511.07289.pdf&lt;/denchmark-link&gt;
.
We welcome contributions by users. Will you be able to update submit a PR (use the doc style guide) to fix the doc Issue?
Yes
	</description>
	<comments>
		<comment id='1' author='8bitmp3' date='2019-07-29T17:11:05Z'>
		thank you &lt;denchmark-link:https://github.com/Bharat123rox&gt;@Bharat123rox&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/kyscg&gt;@kyscg&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 . Closing now 
		</comment>
	</comments>
</bug>