<bug id='31446' author='Uiuran' open_date='2019-08-08T13:23:01Z' closed_time='2019-08-15T22:50:05Z'>
	<summary>Init operation is not added automatically to collections with tf.GraphKeys.INIT_OP</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): conda/pip
TensorFlow version (use command below): 1.14
Python version: 2.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
In:
graph = tf.Graph()
with graph.as_default():

  with tf.variable_scope('signal_in'):
    signal_in = tf.placeholder(tf.float32, shape=(10,40,2,1))

  with tf.variable_scope('dascope1'):
    conv_linear = tf.keras.layers.Conv2D( 8, (8,2), padding='valid', name='conv_linear', use_bias=True, kernel_initializer=tf.initializers.lecun_normal(seed=137), bias_initializer=tf.initializers.lecun_normal(seed=137) )(signal_in)
  
  with tf.variable_scope('softmax'):
    logits = tf.contrib.layers.fully_connected(conv_linear, 2, activation_fn=None, normalizer_fn=None, normalizer_params=None, weights_initializer=tf.initializers.lecun_normal(seed=731), weights_regularizer=None, biases_initializer=tf.initializers.lecun_normal(seed=777), biases_regularizer=None, reuse=None, variables_collections=None, outputs_collections=None, trainable=True, scope='logit')
    softmax = tf.nn.softmax(logits,axis=0)            
    
  with tf.variable_scope('loss'):
    l_vec = tf.placeholder(tf.float32, shape=(10,2))
    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=0)(l_vec, softmax)         
    minimize_op = tf.train.AdamOptimizer(learning_rate=0.05).minimize(loss)
    tf.global_variables_initializer()

print(graph.get_collection_ref(tf.GraphKeys.INIT_OP))
returns [] to stdout
Describe the expected behavior
it must return the value of the collection keyed by tf.GraphKeys.INIT_OP, in this case it should be something like loss/init  type NoOp
Code to reproduce the issue
Given above.
Other info / logs
this must be a easy to circumvent bug, but for coherence, i think must be corrected.
	</description>
	<comments>
		<comment id='1' author='Uiuran' date='2019-08-09T09:12:03Z'>
		&lt;denchmark-link:https://github.com/Uiuran&gt;@Uiuran&lt;/denchmark-link&gt;
 ,
When tried executing the given code i got output as per the screenshot, can you confirm if the same warnings are received.Thanks!
&lt;denchmark-link:https://user-images.githubusercontent.com/52397990/62768120-d3647800-bab3-11e9-9942-bfff8effdd16.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='Uiuran' date='2019-08-09T14:38:58Z'>
		yes, but i feel that's only a version issue, not related to the issue i reffered
		</comment>
		<comment id='3' author='Uiuran' date='2019-08-15T22:50:05Z'>
		There is no loss/init_op op. Variable initializers don't end up in INIT_OP unless you add it yourself. This is working as intended.
		</comment>
		<comment id='4' author='Uiuran' date='2019-08-15T22:50:07Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=31446&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=31446&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='Uiuran' date='2019-08-15T23:27:04Z'>
		
There is no loss/init_op op. Variable initializers don't end up in INIT_OP unless you add it yourself. This is working as intended.

Is INIT_OP  GraphKey added for something else ? just curious if iam missusing or Is it for it, but is faculted to the user as a suggestion ? what do you think ?
		</comment>
		<comment id='6' author='Uiuran' date='2019-08-16T15:23:46Z'>
		If you're using Supervisor (deprecated) the
tf.global_variables_initializer() op gets added to GraphKeys.INIT_OP. If
you're using MonitoredSession (not deprecated, implied by Estimator), if
you the user put something in INIT_OP the framework will run it once to
initialize things, but by default it remains unpopulated.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Aug 15, 2019 at 4:33 PM Daniel Penalva ***@***.***&gt; wrote:
 There is no loss/init_op op. Variable initializers don't end up in INIT_OP
 unless you add it yourself. This is working as intended.

 Is INIT_OP GraphKey added for something else ? just curious if iam
 missusing or Is it for it, but is faculted to the user as a suggestion ?
 what do you think ?

 —
 You are receiving this because you modified the open/close state.
 Reply to this email directly, view it on GitHub
 &lt;#31446?email_source=notifications&amp;email_token=AAABHRJZWKOS35LVAPXF4ZDQEXRVXA5CNFSM4IKKKP6KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD4NIKFQ#issuecomment-521831702&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAABHRN2TUWPYXFVY6XQNRDQEXRVXANCNFSM4IKKKP6A&gt;
 .


-- 
 - Alex

		</comment>
		<comment id='7' author='Uiuran' date='2019-08-16T16:22:26Z'>
		ok, nice, thank u

On Fri, Aug 16, 2019 at 12:31 PM Alexandre Passos &lt;notifications@github.com&gt;
wrote:
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


 If you're using Supervisor (deprecated) the
 tf.global_variables_initializer() op gets added to GraphKeys.INIT_OP. If
 you're using MonitoredSession (not deprecated, implied by Estimator), if
 you the user put something in INIT_OP the framework will run it once to
 initialize things, but by default it remains unpopulated.

 On Thu, Aug 15, 2019 at 4:33 PM Daniel Penalva ***@***.***&gt;
 wrote:

 &gt; There is no loss/init_op op. Variable initializers don't end up in
 INIT_OP
 &gt; unless you add it yourself. This is working as intended.
 &gt;
 &gt; Is INIT_OP GraphKey added for something else ? just curious if iam
 &gt; missusing or Is it for it, but is faculted to the user as a suggestion ?
 &gt; what do you think ?
 &gt;
 &gt; —
 &gt; You are receiving this because you modified the open/close state.
 &gt; Reply to this email directly, view it on GitHub
 &gt; &lt;
 #31446?email_source=notifications&amp;email_token=AAABHRJZWKOS35LVAPXF4ZDQEXRVXA5CNFSM4IKKKP6KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD4NIKFQ#issuecomment-521831702
 &gt;,
 &gt; or mute the thread
 &gt; &lt;
 https://github.com/notifications/unsubscribe-auth/AAABHRN2TUWPYXFVY6XQNRDQEXRVXANCNFSM4IKKKP6A
 &gt;
 &gt; .
 &gt;


 --
 - Alex

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#31446?email_source=notifications&amp;email_token=AAOOCX2XJPCB4HAQRVTGE2TQE3B3TA5CNFSM4IKKKP6KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD4O5H7Y#issuecomment-522048511&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAOOCX2UZBFVAOXZTN3GWXLQE3B3TANCNFSM4IKKKP6A&gt;
 .



		</comment>
	</comments>
</bug>