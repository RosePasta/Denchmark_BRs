<bug id='28580' author='monkeypride' open_date='2019-05-10T01:16:14Z' closed_time='2019-05-28T21:54:28Z'>
	<summary>keras.layers.BatchNormalization.call() with default training param does not reflect K.learning_phase() Tensor input when fed into feed_dict</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 update 1809
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.12.0
Python version: 3.6.8
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: CUDAv9.0.176 / cuDNNv7.4.2
GPU model and memory: NVIDIA GeForce GTX 1060 6GB RAM

&lt;denchmark-h:h2&gt;Describe the problem&lt;/denchmark-h&gt;

When using keras.layers.BatchNormalization, K.learning_phase() does not seem to have any affect during training when BatchNormalization.__call__ is called with training as default param. However, if K.learning_phase() is explicitly passed in as BatchNormalization.__call__(training=K.learning_phase()), the model is able to train batch norm's moving mean and moving variance ops.
&lt;denchmark-h:h4&gt;Expected Behavior&lt;/denchmark-h&gt;

Keras.layers.BatchNormalization is expected to fill in K.learning_phase() if training=None and ops are updated during training.
&lt;denchmark-h:h2&gt;Exact command to reproduce&lt;/denchmark-h&gt;

Running below code snippet for BatchNormalization.__call__(input) with non-supplied training param:
import tensorflow as tf
from tensorflow.keras.layers import Dense, BatchNormalization, Activation, Input
from tensorflow.train import AdamOptimizer
import numpy as np
from keras import backend as K

shape = (100, 4)
data = np.random.random(size=shape)

tf.reset_default_graph()
graph = tf.Graph()

with graph.as_default():
  is_training = K.learning_phase()
  input_tensor = tf.placeholder(tf.float32, shape=shape)
  x = Input(tensor=input_tensor)
  layer = Dense(units=128, activation='relu')(x)
  layer = Dense(units=32, activation='relu')(layer)
  layer = Dense(units=4, activation='linear')(layer)
  bn = BatchNormalization()
  # line of interest
  layer = bn(layer)
  tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, bn.updates)
  logits = Activation('relu')(layer)
  loss = tf.losses.mean_squared_error(logits, x)
  opt = AdamOptimizer()
  grads_and_vars = opt.compute_gradients(loss)
  opt = opt.apply_gradients(grads_and_vars)
  variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,
                          scope='batch_normalization')

print([var.name for var in variables])
with tf.Session(graph=graph) as sess:
  sess.run(tf.global_variables_initializer())
  for idx in range(50):
    out = sess.run([opt] + bn.updates, feed_dict={input_tensor:np.array(data), is_training:1})
    if not idx % 5: 
      outs = [sess.run(var, feed_dict={input_tensor:np.array(data), is_training:0}) for var in variables]
      outs = np.linalg.norm(outs, axis=1)
      print(f'Running {idx}: {outs}')
Produces with the following output (notice that moving_mean=0 and moving_variance=2 throughout) :
&lt;denchmark-code&gt;['batch_normalization_v1/gamma:0', 'batch_normalization_v1/beta:0', 'batch_normalization_v1/moving_mean:0', 'batch_normalization_v1/moving_variance:0', 'batch_normalization_v1/gamma/Adam:0', 'batch_normalization_v1/gamma/Adam_1:0', 'batch_normalization_v1/beta/Adam:0', 'batch_normalization_v1/beta/Adam_1:0']
Running 0: [2.0005007e+00 1.7320185e-03 0.0000000e+00 2.0000000e+00 6.5435906e-04
 3.3886202e-08 1.7219670e-02 2.0985259e-05]
Running 5: [2.0065410e+00 8.4475391e-03 0.0000000e+00 2.0000000e+00 1.4017796e-02
 3.9132146e-06 1.1373939e-01 2.7749091e-04]
Running 10: [2.01387644e+00 1.56976394e-02 0.00000000e+00 2.00000000e+00
 2.52754204e-02 1.10197625e-05 1.15519769e-01 3.48794798e-04]
Running 15: [2.01952314e+00 2.05438156e-02 0.00000000e+00 2.00000000e+00
 1.88015848e-02 1.29885275e-05 7.03223869e-02 3.55401076e-04]
Running 20: [2.0201447e+00 2.0984445e-02 0.0000000e+00 2.0000000e+00 1.1408665e-02
 1.6400263e-05 3.1892959e-02 3.6953186e-04]
Running 25: [2.0176027e+00 1.9309264e-02 0.0000000e+00 2.0000000e+00 1.3336688e-02
 1.8967023e-05 3.1269908e-02 3.8181755e-04]
Running 30: [2.0159297e+00 1.7820496e-02 0.0000000e+00 2.0000000e+00 6.4963060e-03
 1.9346602e-05 2.3151563e-02 3.8251214e-04]
Running 35: [2.0171123e+00 1.7269025e-02 0.0000000e+00 2.0000000e+00 6.5930812e-03
 2.0723272e-05 6.8678367e-03 3.8147590e-04]
Running 40: [2.0202377e+00 1.7495051e-02 0.0000000e+00 2.0000000e+00 1.2473603e-02
 2.2621665e-05 7.9890825e-03 3.8188352e-04]
Running 45: [2.0230916e+00 1.8221466e-02 0.0000000e+00 2.0000000e+00 1.3377106e-02
 2.3886721e-05 1.5546208e-02 3.8239476e-04]
&lt;/denchmark-code&gt;

However, after explicitly adding training parameter to __call__ with K.learning_phase(), training seems to be updating batch norm ops:
import tensorflow as tf
from tensorflow.keras.layers import Dense, BatchNormalization, Activation, Input
from tensorflow.train import AdamOptimizer
import numpy as np
from keras import backend as K

shape = (100, 4)
data = np.random.random(size=shape)

tf.reset_default_graph()
graph = tf.Graph()

with graph.as_default():
  is_training = K.learning_phase()
  input_tensor = tf.placeholder(tf.float32, shape=shape)
  x = Input(tensor=input_tensor)
  layer = Dense(units=128, activation='relu')(x)
  layer = Dense(units=32, activation='relu')(layer)
  layer = Dense(units=4, activation='linear')(layer)
  bn = BatchNormalization()
  # line of interest
  layer = bn(layer, training=is_training)
  tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, bn.updates)
  logits = Activation('relu')(layer)
  loss = tf.losses.mean_squared_error(logits, x)
  opt = AdamOptimizer()
  grads_and_vars = opt.compute_gradients(loss)
  opt = opt.apply_gradients(grads_and_vars)
  variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,
                          scope='batch_normalization')

print([var.name for var in variables])
with tf.Session(graph=graph) as sess:
  sess.run(tf.global_variables_initializer())
  for idx in range(50):
    out = sess.run([opt] + bn.updates, feed_dict={input_tensor:np.array(data), is_training:1})
    if not idx % 5: 
      outs = [sess.run(var, feed_dict={input_tensor:np.array(data), is_training:0}) for var in variables]
      outs = np.linalg.norm(outs, axis=1)
      print(f'Running {idx}: {outs}')
Now moving_mean and moving_variance both seem to be updating during training as expected.
&lt;denchmark-code&gt;['batch_normalization_v1/gamma:0', 'batch_normalization_v1/beta:0', 'batch_normalization_v1/moving_mean:0', 'batch_normalization_v1/moving_variance:0', 'batch_normalization_v1/gamma/Adam:0', 'batch_normalization_v1/gamma/Adam_1:0', 'batch_normalization_v1/beta/Adam:0', 'batch_normalization_v1/beta/Adam_1:0']
Running 0: [1.9980000e+00 1.9999903e-03 2.0461062e-03 1.9801079e+00 2.8676972e-02
 4.9131999e-05 1.5716424e-02 1.4908865e-05]
Running 5: [1.9882594e+00 1.1786965e-02 1.1019228e-02 1.8835746e+00 9.9898919e-02
 1.8135854e-04 6.1359003e-02 7.0034890e-05]
Running 10: [1.9793329e+00 2.0299187e-02 2.0462982e-02 1.7917910e+00 1.1684086e-01
 2.4303599e-04 6.9242075e-02 9.5857220e-05]
Running 15: [1.9711298e+00 2.7165966e-02 2.9484417e-02 1.7045155e+00 1.1539625e-01
 2.8652389e-04 6.3280165e-02 1.0698218e-04]
Running 20: [1.9639944e+00 3.2249879e-02 3.6651909e-02 1.6214770e+00 9.9982716e-02
 3.0927311e-04 5.2701045e-02 1.1211485e-04]
Running 25: [1.95816207e+00 3.52966003e-02 4.23905924e-02 1.54244149e+00
 8.03107098e-02 3.20362276e-04 3.72414254e-02 1.13687325e-04]
Running 30: [1.9536946e+00 3.6224511e-02 4.7054645e-02 1.4672419e+00 6.2270045e-02
 3.2455754e-04 3.2511890e-02 1.1524249e-04]
Running 35: [1.9503421e+00 3.6185008e-02 5.0670151e-02 1.3957475e+00 5.0311577e-02
 3.2821088e-04 3.6015689e-02 1.1753518e-04]
Running 40: [1.9479088e+00 3.5928987e-02 5.3476196e-02 1.3277912e+00 4.1749101e-02
 3.3096061e-04 3.5941143e-02 1.1917475e-04]
Running 45: [1.9461093e+00 3.5778154e-02 5.5816881e-02 1.2631813e+00 3.3145458e-02
 3.3147351e-04 3.4038719e-02 1.2038982e-04]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='monkeypride' date='2019-05-14T12:23:48Z'>
		&lt;denchmark-link:https://github.com/monkeypride&gt;@monkeypride&lt;/denchmark-link&gt;
 Able to reproduce two kinds of  output as mentioned in the template with the provided code.
		</comment>
		<comment id='2' author='monkeypride' date='2019-05-28T21:54:28Z'>
		Before I get to the central point of the issue, it should be clarified that keras and tf.keras are distinct libraries. Specifically, both implement the Keras API; however interchanging keras.thing and tf.keras.thing is not supported since they have different implementations.
I took the liberty of replacing keras with tf.keras, and pruning the repro down to something that only exercised batch norm:
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow.keras.layers import BatchNormalization, Input
import numpy as np
from tensorflow.keras import backend as K

shape = (100, 1)
data = np.ones(shape=shape)

tf.reset_default_graph()
graph = tf.Graph()

with graph.as_default():
  is_training = K.learning_phase()
  input_tensor = tf.placeholder(tf.float32, shape=shape)
  x = Input(tensor=input_tensor)

  bn = BatchNormalization()
  # line of interest
  layer = bn(x)
  tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, bn.updates)
  variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,
                                scope='batch_normalization')

print([var.name for var in variables])
with tf.Session(graph=graph) as sess:
  sess.run(tf.global_variables_initializer())
  for idx in range(100):
    out = sess.run([layer] + bn.updates, feed_dict={input_tensor:np.array(data) * idx, is_training:1})
    if not (idx + 1) % 5:
      outs = [sess.run(var, feed_dict={input_tensor:np.array(data) * idx, is_training:0}) for var in variables]
      outs = " ".join("{:.2f}".format(float(i)).ljust(8) for i in outs)
      print('Running {idx}: {outs}'.format(idx=idx, outs=outs))
&lt;/denchmark-code&gt;

That said, I don't think this is a supported pattern. (Relying on high level library concepts like training templating, but not using tf.keras.backend.function or model.fit/evaluate/predict/...) But in the example above it does at least use is_training, so I'm going to close this as intended behavior.
		</comment>
		<comment id='3' author='monkeypride' date='2019-05-28T21:54:29Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=28580&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=28580&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='monkeypride' date='2019-05-28T22:02:27Z'>
		Yeah this looks like it's just because you're feeding the keras.backend.learning_phase() placeholder rather thanthe tf.keras.backend.learning_phase() placeholder.
It works fine for me if I replace
from keras import backend as K
with
from tensorflow.keras import backend as K
		</comment>
	</comments>
</bug>