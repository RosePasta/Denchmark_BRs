<bug id='7625' author='kirk86' open_date='2017-02-17T14:48:02Z' closed_time='2017-03-03T23:41:42Z'>
	<summary>tf.train.import_meta_graph should be parallelized</summary>
	<description>
Hey guys, I've been wondering whether it's possible the tf.train.import_meta_graph to be parallelized across multiple cores. When I load a complex model it takes too long and I only see one of my 24 cores being utilized to 100%.
&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System: CentOS
Installed version of CUDA and cuDNN:
Not applicable
Minimal example:
&lt;denchmark-code&gt;sess = tf.Session()
new_saver = tf.train.import_meta_graph('my-model.meta')
new_saver.restore(sess, tf.train.latest_checkpoint('./'))
all_vars = tf.get_collection('vars')
for v in all_vars:
    v_ = sess.run(v)
    print(v_)
&lt;/denchmark-code&gt;

Any plans on changing that in the near future?
	</description>
	<comments>
		<comment id='1' author='kirk86' date='2017-02-17T16:11:55Z'>
		When I see one core being utilized, my first suspicion is that work is done in Python. Python has chosen to remain in the punch-card age and only use 1 core for its processing, citing GIL issues. So the way to troubleshoot this is:
&lt;denchmark-code&gt;python -m cProfile -o slow.prof script.py
pip install snakeviz
snakeviz slow.prof
&lt;/denchmark-code&gt;

This may help you find a bottleneck, possibly something obviously inefficient is being done in Python (I used this technique to speed up session.run from 120 usec to 60 usec by cutting out some unneeded Python calls). Longer term solution may be to rewrite metagraph loading in C++, someone from TF team may know if there are plans
		</comment>
		<comment id='2' author='kirk86' date='2017-02-17T16:24:46Z'>
		&lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 thanks for the input. I'll definitely have a look into it.
		</comment>
		<comment id='3' author='kirk86' date='2017-02-17T16:31:52Z'>
		PS, we also run into MetaGraph slowness issue (on the saving side), we have to turn it off in our reinforcement learning scripts -- &lt;denchmark-link:https://github.com/openai/universe-starter-agent/blob/master/worker.py#L18&gt;https://github.com/openai/universe-starter-agent/blob/master/worker.py#L18&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='kirk86' date='2017-03-03T23:41:42Z'>
		Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
	</comments>
</bug>