<bug id='23878' author='javiermas' open_date='2018-11-20T11:31:19Z' closed_time='2018-12-18T16:50:57Z'>
	<summary>Bug: instantiating dynamic_rnn with tf.int32 in input and state raises TypeError</summary>
	<description>
System information

OS Platform: OS X 10.13.3
Custom code
tensorflow version: 1.12.0
python version: 3.6.5

Describe the current behavior
Tensorflow raises a TypeError when creating a dynamic_rnn with tf.int32 type in its input and state. When changing the type to tf.float32 the error is not raised.
Describe the expected behavior
Ideally, a dynamic_rnn should support tf.in32 types. If there's any reason why instantiating a dynamic_rnn with tf.int32 type in its input and state should not be allowed, a custom error should be raised.
Code to reproduce the issue
The code below reproduces the error:
&lt;denchmark-code&gt;import tensorflow as tf

X = tf.placeholder(tf.int32, [None, 10, 1])
cell = tf.nn.rnn_cell.LSTMCell(1, dtype=tf.int32)
output, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.int32)

&lt;/denchmark-code&gt;

The code below doesn't:
&lt;denchmark-code&gt;
import tensorflow as tf

X = tf.placeholder(tf.float32, [None, 10, 1])
cell = tf.nn.rnn_cell.LSTMCell(1, dtype=tf.float32)
output, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.float32)

&lt;/denchmark-code&gt;

Note the change in dtype.
&lt;denchmark-h:h2&gt;Other info / logs
TRACEBACK:&lt;/denchmark-h&gt;

TypeError                                 Traceback (most recent call last)
 in ()
2 X = tf.placeholder(tf.int32, [None, 10, 1])
3 cell = tf.nn.rnn_cell.LSTMCell(1, dtype=tf.int32)
----&gt; 4 output, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.int32)#, initial_state=state)
5
~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in dynamic_rnn(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)
662         swap_memory=swap_memory,
663         sequence_length=sequence_length,
--&gt; 664         dtype=dtype)
665
666     # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].
~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in _dynamic_rnn_loop(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)
870       parallel_iterations=parallel_iterations,
871       maximum_iterations=time_steps,
--&gt; 872       swap_memory=swap_memory)
873
874   # Unpack final output if not using output tuples.
~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)
3289       ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)
3290     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,
-&gt; 3291                                     return_same_structure)
3292     if maximum_iterations is not None:
3293       return result[1]
~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants, return_same_structure)
3002       with ops.get_default_graph()._mutation_lock():  # pylint: disable=protected-access
3003         original_body_result, exit_vars = self._BuildLoop(
-&gt; 3004             pred, body, original_loop_vars, loop_vars, shape_invariants)
3005     finally:
3006       self.Exit()
~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)
2937         flat_sequence=vars_for_body_with_tensor_arrays)
2938     pre_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access
-&gt; 2939     body_result = body(*packed_vars_for_body)
2940     post_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access
2941     if not nest.is_sequence(body_result):
~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in (i, lv)
3258         cond = lambda i, lv: (  # pylint: disable=g-long-lambda
3259             math_ops.logical_and(i &lt; maximum_iterations, orig_cond(*lv)))
-&gt; 3260         body = lambda i, lv: (i + 1, orig_body(*lv))
3261
3262     if context.executing_eagerly():
~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in _time_step(time, output_ta_t, state)
838           skip_conditionals=True)
839     else:
--&gt; 840       (output, new_state) = call_cell()
841
842     # Keras cells always wrap state as list, even if it's a single tensor.
~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in ()
824     if is_keras_rnn_cell and not nest.is_sequence(state):
825       state = [state]
--&gt; 826     call_cell = lambda: cell(input_t, state)
827
828     if sequence_length is not None:
~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state, scope, *args, **kwargs)
368     # method.  See the class docstring for more details.
369     return base_layer.Layer.call(self, inputs, state, scope=scope,
--&gt; 370                                      *args, **kwargs)
371
372
~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/layers/base.py in call(self, inputs, *args, **kwargs)
372
373       # Actually call layer
--&gt; 374       outputs = super(Layer, self).call(inputs, *args, **kwargs)
375
376     if not context.executing_eagerly():
~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in call(self, inputs, *args, **kwargs)
755       if not in_deferred_mode:
756         self._in_call = True
--&gt; 757         outputs = self.call(inputs, *args, **kwargs)
758         self._in_call = False
759         if outputs is None:
~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state)
1003            sigmoid(i + self._w_i_diag * c_prev) * self._activation(j))
1004     else:
-&gt; 1005       c = (sigmoid(f + self._forget_bias) * c_prev + sigmoid(i) *
1006            self._activation(j))
1007
TypeError: unsupported operand type(s) for +: 'Tensor' and 'float'
	</description>
	<comments>
		<comment id='1' author='javiermas' date='2018-11-26T20:33:53Z'>
		Thanks for the clear repro example. Probably this is a duplicate of &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/14729&gt;#14729&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='javiermas' date='2018-11-27T15:30:06Z'>
		Thanks for the report, I will take a look today or tomorrow.
		</comment>
		<comment id='3' author='javiermas' date='2018-12-17T18:25:48Z'>
		Sorry for the very late reply. After some digging, the root cause is because of the default forget gate bias being initialized as float. However, even casting it to be int32 will still causing the code to fail down the road since the activation function for LSTM (tanh and sigmoid) only support floating numbers. So the conclusion is that LSTM will only support floating numbers as the dtype for input and states.
I will update the code to have a clear error message when the input dtype is not float number.
		</comment>
	</comments>
</bug>