<bug id='18528' author='ming-c' open_date='2018-04-15T06:43:03Z' closed_time='2018-06-10T18:33:45Z'>
	<summary>Gradient Inconsistency</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04.5
TensorFlow installed from (source or binary): pip install from binary
TensorFlow version (use command below): v1.4.0-19-ga52c8d9 1.4.1
Python version: Python3.5 (Anaconda)
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: CUDA-8.0
GPU model and memory: GeForce GTX 1070(8GB) &amp; GeForce GTX 770(4GB)
Exact command to reproduce: N/A

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

When use operation tf.transpose(), tf.gather(), tf.cholesky() all together in a row over instance of tf.Variable(), the backward gradient computation may seems inconsistent. By using 'inconsistent', I mean that after run the same script multiple times with fixed random seeds, the computed gradient of result of tf.cholesky() over input of tf.transpose() are not always identical.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

import tensorflow as tf
import numpy as np

np.random.seed(1024)
tf.set_random_seed(1024)

N = 10

# indices for `tf.gather()`
indices_for_gather = tf.constant(np.concatenate((np.zeros(N), np.ones(N))).reshape(-1,).astype(np.int32))

# build a 2-by-2 PSD matrix as `param` for 'tf.gather()'
W = tf.constant(np.random.rand(2, 1), dtype=tf.float32)
W = tf.Variable(W)

PSD2x2 = tf.matmul(W, W, transpose_b=True)

# do the `tf.gather()`
M_temp = tf.gather(PSD2x2, indices_for_gather)

# Then transpose the M_Temp and tf.gather() again (to ensure the result is a PSD for cholesky)
M_temp_T = tf.transpose(M_temp)

PSD = tf.gather(M_temp_T, indices_for_gather) + tf.eye(2*N, dtype=tf.float32) * 0.01

# cholesky
L = tf.cholesky(PSD)

# compute the gradient of `L` over `M_temp`
grad = tf.gradients(L, M_temp)

# build a session and compute the gradient
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
results = []
with tf.Session(config=config) as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(10):
        result = sess.run(grad)
        results.append(result[0]) 
        
# check the results are whether identical or not
for i in range(len(results) - 1): 
    if isinstance(results[i], np.ndarray):
        print(np.all(np.equal(results[i], results[i+1])))
    else:
        print(np.all(np.equal(results[i].values, results[i+1].values)))
The output result is
&lt;denchmark-code&gt;False
False
False
False
False
False
False
False
False
&lt;/denchmark-code&gt;

If I comment the line W = tf.Variable(W) making the W a constant tensor, the results are ideally all True. And I've also tried to compute the gradient ofL over M_temp_T and PSD, both of them are all True. So I think the problem lies in the using of tf.transpose(), tf.gather(), tf.cholesky() all together over instance of tf.Variable().
	</description>
	<comments>
		<comment id='1' author='ming-c' date='2018-04-24T20:52:06Z'>
		I suspect that reinitializing the variables will reinitialize them inconsistently. Is it consistent if you tf.reset_deafult_graph() and rebuidl the graph every iteration of checking?
		</comment>
		<comment id='2' author='ming-c' date='2018-05-10T01:04:13Z'>
		It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='3' author='ming-c' date='2018-05-26T19:01:23Z'>
		It has been 31 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
	</comments>
</bug>