<bug id='37757' author='TimCapes' open_date='2020-03-20T17:09:42Z' closed_time='2020-03-20T18:20:32Z'>
	<summary>TFLite - Experimental New Converter, incorrect model for Bidirectional GRU</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):
Yes Provided Below.
OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04):
Mac OS 10.12.6
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: Samsung Note9
TensorFlow installed from (source or
binary): - TensorFlow version (use command below):
2.1.0
Python version: - Bazel
version (if compiling from source):
GCC/Compiler version (if compiling from
source):
CUDA/cuDNN version: - GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
TFLite produces a graph with 1 output of size 1(*4 bytes)
Describe the expected behavior
TFLite produces a graph with 1 output of size 64(*4 bytes)
Standalone code to reproduce the issue
import tensorflow as tf
word = tf.keras.Input(shape=(1,), name='word', dtype=tf.int32)
embedding_output = tf.keras.layers.Embedding(8000,100,input_length=1)(word)
modified_embedding = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32))(embedding_output)
model = tf.keras.Model(inputs=[word], outputs=[modified_embedding])
model._make_predict_function()
model.summary()
model.save("testmodel")
converter = tf.lite.TFLiteConverter.from_saved_model("testmodel")
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]
converter.experimental_new_converter = True
tflite_model = converter.convert()
open("testmodel.tflite", "wb").write(tflite_model)
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='TimCapes' date='2020-03-20T17:29:41Z'>
		Have you actually tried executing the graph in the TFLite interpreter? Shape inference during conversion isn't guaranteed to propagate to output nodes, especially when using SELECT_TF_OPS. It should be resolved at runtime when you run inference.
		</comment>
		<comment id='2' author='TimCapes' date='2020-03-20T18:20:32Z'>
		Good to know. It works on device with the correct size, closing.
		</comment>
		<comment id='3' author='TimCapes' date='2020-03-20T18:20:33Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37757&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37757&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>