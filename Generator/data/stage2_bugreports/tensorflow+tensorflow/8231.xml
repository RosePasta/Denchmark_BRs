<bug id='8231' author='erikchwang' open_date='2017-03-09T06:21:46Z' closed_time='2017-03-30T20:07:03Z'>
	<summary>memory leak when training complex neural networks</summary>
	<description>
When training some complex (seq-to-seq) neural networks, the memory cost of my program will keep growing, and this only happens on GPU...(On CPU, everything is OK)
I used to report this problem in this issue: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/6599&gt;#6599&lt;/denchmark-link&gt;

After that, I solved this problem by encapsulating my encoding method as an RNN cell, so this issue was closed, though no one knows the reason...
But now, this problem happens again, because I changed the structure of my network...
I do not think this is due to the bugs in my program, because it runs very well on CPU...
	</description>
	<comments>
		<comment id='1' author='erikchwang' date='2017-03-09T17:21:47Z'>
		Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!
In your previous issue report you did not describe how you measured or diagnosed the 'memory leak'.   What tool or statistic are you using?
Also, if you believe there is a specific GPU operation which is somehow leaking tensors the this would be difficult to triage without a repro - either code or a GraphDef - which would allow us to indentify which ops you are running. Are you able to provide either of these?
		</comment>
		<comment id='2' author='erikchwang' date='2017-03-10T19:05:28Z'>
		@carltonwang you can use &lt;denchmark-link:https://github.com/yaroslavvb/memory_util&gt;https://github.com/yaroslavvb/memory_util&lt;/denchmark-link&gt;
 to get a timeline of all tensor allocations/deallocations, determine which tensors are getting allocated that are taking so much space, and then decide if this is intentional (requested by your program), or not
		</comment>
		<comment id='3' author='erikchwang' date='2017-03-24T07:48:34Z'>
		Look at this code:
result = tf.contrib.layers.bias_add(inputs=tf.matmul(a=left, b=right), activation_fn=tf.nn.softmax)
If I use "softmax" as the activation function, there will be a memory leak, but if I change the activation function to "relu" or "softplus" or "sigmoid", the memory leak will disappear.
By the way, this "softmax" is not the one used at the last layer for classification.
Platform: CentOS 7, x86_64
TensorFlow version: 1.0_gpu
		</comment>
		<comment id='4' author='erikchwang' date='2017-03-28T22:04:35Z'>
		@carltonwang, did you try &lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
's memory analyzer. Perhaps with and without using tf.nn.softmax to get a baseline.
		</comment>
		<comment id='5' author='erikchwang' date='2017-03-29T04:59:56Z'>
		After I update from TF 1.0.0 to TF 1.0.1, this problem disappear...
		</comment>
		<comment id='6' author='erikchwang' date='2017-03-30T20:07:03Z'>
		Thanks, closing since it seems to be resolved in newer version of software.
		</comment>
		<comment id='7' author='erikchwang' date='2017-05-09T03:45:06Z'>
		I also find the same problem. After update TF from 1.0.0 to 1.0.1, the memory leak still exists.
So we implement the softmax mannally use other TF operators such as tf.exp , etc. The the memory leak disappear.
		</comment>
		<comment id='8' author='erikchwang' date='2017-05-09T05:08:32Z'>
		we have submit a new open issue here &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/9779&gt;#9779&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>