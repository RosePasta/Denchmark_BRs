<bug id='21081' author='snownus' open_date='2018-07-24T08:11:59Z' closed_time='2019-04-16T00:25:28Z'>
	<summary>Tensort RT Engine Object Detection</summary>
	<description>
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Requested batch size is not available and engine cache is full
[[Node: import/my_trt_op_16 = TRTEngineOp[InT=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], OutT=[DT_FLOAT], cached_engine_batches=[1], calibration_data="", fixed_input_size=true, input_shapes=[[?,1], [?,1], [?,1], [?,1]], max_cached_engines_count=1, output_shapes=[[?,4]], precision_mode="FP32", segment_funcdef_name="my_trt_op_16_native_segment", serialized_segment="\2007\000...00\000\000", static_engine=true, workspace_size_bytes=65075264, _device="/job:localhost/replica:0/task:0/device:GPU:0"](import/MultiClassNonMaxSuppression/ClipToWindow/split, import/MultiClassNonMaxSuppression/ClipToWindow/split:2, import/MultiClassNonMaxSuppression/ClipToWindow/split:1, import/MultiClassNonMaxSuppression/ClipToWindow/split:3)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
&lt;denchmark-code&gt; [[Node: import/SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField/strided_slice/_89 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1900_...ided_slice", tensor_type=DT_INT32, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
&lt;/denchmark-code&gt;

Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
	</description>
	<comments>
		<comment id='1' author='snownus' date='2018-07-24T19:07:30Z'>
		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
Have I written custom code
OS Platform and Distribution
TensorFlow installed from
TensorFlow version
Bazel version
CUDA/cuDNN version
GPU model and memory
Exact command to reproduce
Mobile device
		</comment>
		<comment id='2' author='snownus' date='2018-08-08T00:04:56Z'>
		&lt;denchmark-link:https://github.com/samikama&gt;@samikama&lt;/denchmark-link&gt;
 not clear whether this is a bug or working-as-intended. Would you please take a look?
		</comment>
		<comment id='3' author='snownus' date='2018-08-08T00:11:32Z'>
		It looks like &lt;denchmark-link:https://github.com/snownus&gt;@snownus&lt;/denchmark-link&gt;
 created the engine with a batch size smaller than the execution time. Either a bigger batch size should be used or dynamic op should be generated.
		</comment>
		<comment id='4' author='snownus' date='2018-08-24T01:19:57Z'>
		&lt;denchmark-link:https://github.com/samikama&gt;@samikama&lt;/denchmark-link&gt;
 , in faster rcnn, the second stage will have one more dimension. It seems tensorflow/tensorrt see another dimension as batch size and in fact, it isn't. Faster RCNN, at the second stage have 300 boxes.
		</comment>
		<comment id='5' author='snownus' date='2018-09-22T07:28:11Z'>
		I have same error. If no use tensorflow.contrib.tensorrt.create_inference_graph then no error.
ResourceExhaustedError (see above for traceback): Requested batch size is not available and engine cache is full
[[Node: model_1/prior_layer_1/my_trt_op_11 = TRTEngineOp&lt;denchmark-link:model_1/prior_layer_1/ToFloat&gt;InT=[DT_FLOAT], OutT=[DT_FLOAT], cached_engine_batches=[10], calibration_data="", fixed_input_size=true, input_shapes=[[?,2]], max_cached_engines_count=1, output_shapes=[[?,2]], precision_mode="FP32", segment_funcdef_name="model_1/prior_layer_1/my_trt_op_11_native_segment", serialized_segment="\270\020\0...00\000\000", static_engine=true, workspace_size_bytes=18292682, _device="/job:localhost/replica:0/task:0/device:GPU:0"&lt;/denchmark-link&gt;
]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
&lt;denchmark-code&gt;     [[Node: while/Switch_2/_85 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_2429_while/Switch_2", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"](^_cloopwhile/strided_slice_5/stack/_1)]]
&lt;/denchmark-code&gt;

Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
System:
ubuntu 16.04
tensorflow v1.10.1 from source
cuda 9.0, cudnn 7.1
tensorrt 4.1.2
python 3.5
		</comment>
		<comment id='6' author='snownus' date='2018-09-22T10:20:30Z'>
		use create_inference_graph with maximum_cached_engines=3 no error
		</comment>
		<comment id='7' author='snownus' date='2018-10-22T23:07:49Z'>
		&lt;denchmark-link:https://github.com/snownus&gt;@snownus&lt;/denchmark-link&gt;
,
In your case it is a known issue of integration. Since TF doesn't have batch dimension concept and TRT requires a batch dimension, our assumption of Rank0 being the batch dimension is failing in FasterRCNN. It is being worked on.
		</comment>
		<comment id='8' author='snownus' date='2018-12-15T01:25:27Z'>
		There will be an NMS op implementation soon which  should resolve this.
		</comment>
		<comment id='9' author='snownus' date='2019-03-01T02:48:43Z'>
		&lt;denchmark-link:https://github.com/trevor-m&gt;@trevor-m&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/pooyadavoodi&gt;@pooyadavoodi&lt;/denchmark-link&gt;
  Can you PTAL
		</comment>
		<comment id='10' author='snownus' date='2019-04-02T21:35:41Z'>
		It has been 32 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='11' author='snownus' date='2019-04-03T00:05:27Z'>
		Our (somewhat) recent improvements to the engine cache fixes this issue. It is available in the master branch of tensorflow (tf-nightly-gpu).
		</comment>
		<comment id='12' author='snownus' date='2019-04-16T00:25:28Z'>
		I think it was resolved. I am closing the issue. If you think I made a mistake, please open another issue.
		</comment>
		<comment id='13' author='snownus' date='2019-04-16T00:25:30Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=21081&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=21081&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>