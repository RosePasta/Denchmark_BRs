<bug id='31421' author='devstein' open_date='2019-08-07T19:46:52Z' closed_time='2019-08-13T00:43:01Z'>
	<summary>[TF 2.0] Issues Serializing SavedModel Serving Default Signature</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX + Linux
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): pipenv install --pre tensorflow==2.0.0-beta1 --python=3.6.8
TensorFlow version (use command below): 2.0.0-beta1
Python version: 3.6.8
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A

Describe the current behavior
I want to use a  to run batch inference in . A few libraries implement this functionality, such as &lt;denchmark-link:https://github.com/databricks/spark-deep-learning&gt;Databrick's Spark Deep Learning&lt;/denchmark-link&gt;
, however, they are implemented using  or lower. I am using  to train and save my models, so I would like to use the same version for deploying my model.
To use an object in PySpark it must be serializable, but I am getting a pickle issue when trying to use a SavedModel's signatures['serving_default] concrete function.
Previously, this call was serializable
# TensorFlow 1.X
outputs = session.run(f(placeholder), feed_dict={placeholder: input})
Now, this is giving issues
# TensorFlow 2.0
outputs = f(input)
This behavior can be reproduced without using Spark. You only need to try pickle.dump the signatures['serving_default'] concrete function.
Describe the expected behavior
I expect both the TF 1.X and TF 2.0 ways of predicting from a SavedModel to be serializable
# TensorFlow 1.X
outputs = session.run(f(placeholder), feed_dict={placeholder: input})
# TensorFlow 2.0
outputs = f(input)
Let me know if I am approaching this problem the wrong way! Any help is appreciated.
Code to reproduce the issue
# Example
import os
import pickle

from pathlib import Path

import tensorflow as tf
from tensorflow.keras import layers


# Create a simple model
inputs = tf.keras.Input(shape=(784,), name='img')

x = layers.Dense(64, activation='relu')(inputs)
x = layers.Dense(64, activation='relu')(x)
outputs = layers.Dense(10, activation='softmax')(x)

model = tf.keras.Model(inputs=inputs, outputs=outputs, name='mnist_model')

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train = x_train.reshape(60000, 784).astype('float32') / 255

# Compile
model.compile(loss='sparse_categorical_crossentropy',
              optimizer=tf.keras.optimizers.RMSprop(),
              metrics=['accuracy'])

# Train
history = model.fit(x_train, y_train,
                    batch_size=64,
                    epochs=5,
                    validation_split=0.2)

# Save to SavedModel 
output_dir = "/tmp/workdir/v1/"
output_directory = Path(output_dir)
output_directory.mkdir(parents=True, exist_ok=True)

model_save_path = os.path.join(output_dir,'model')

model.save(tf_model_save_path, save_format="tf")


# Load the SavedModel

saved_model = tf.saved_model.load(model_save_path, tags=['serve'])

# Get the 'predict' concrete function
infer = saved_model.signatures['serving_default']

pickle_file = os.path.join(output_dir, 'serialized')

# Try serialize the function 
with open(pickle_file, 'wb') as f:
    pickle.dump(infer, f)

# Expect error 
# _pickle.PickleError: can't pickle repeated message fields, convert to list first
Other info / logs
&lt;denchmark-code&gt;_pickle.PickleError: can't pickle repeated message fields, convert to list first
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='devstein' date='2019-08-08T08:44:02Z'>
		I have tried in Jupyter notebook with TF version 2.0 beta1 and was able to reproduce the issue.Please, find the gist in attachment.Thanks!
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3480677/savedmodel.ipynb.tar.gz&gt;savedmodel.ipynb.tar.gz&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='devstein' date='2019-08-12T23:26:07Z'>
		I'm not sure I see the relation between pickling an object and Spark. Spark directly uses the SavedModel by calling tf.saved_model.loader, without re-serializing it to another format (from an admittedly brief look at the library - let met know if I missed something). Can you clarify?
		</comment>
		<comment id='3' author='devstein' date='2019-08-13T00:01:46Z'>
		&lt;denchmark-link:https://github.com/k-w-w&gt;@k-w-w&lt;/denchmark-link&gt;
 Thanks for taking a look at this.
When you create a &lt;denchmark-link:https://docs.databricks.com/spark/latest/spark-sql/udf-python.html&gt;user defined function&lt;/denchmark-link&gt;
 in Spark, it serializes (pickles) the function in order to distribute it across workers. In this case, I want to create a &lt;denchmark-link:https://docs.databricks.com/spark/latest/spark-sql/udf-python.html&gt;user defined function&lt;/denchmark-link&gt;
 that predicts on each row with a SavedModel, but the method for predicting in 
# Get the 'predict' concrete function
infer = saved_model.signatures['serving_default']
is no longer serializable. Whereas,
outputs = session.run(f(placeholder), feed_dict={placeholder: input})
was serializable, so it worked with &lt;denchmark-link:https://docs.databricks.com/spark/latest/spark-sql/udf-python.html&gt;user defined functions&lt;/denchmark-link&gt;

Let me know if that makes sense!
		</comment>
		<comment id='4' author='devstein' date='2019-08-13T00:43:01Z'>
		That makes sense, thank you for the explanation!
infer is a ConcreteFunction which is an object defined in Tensorflow, which isn't compatible with pickle. You can file a separate issue requesting this feature, or submit a PR to add this functionality.
A possible workaround is to create a regular python function that loads the SavedModel and calls the default signature function, and register that as a user-defined function.
I'll close this issue because it is not a bug, but please do file the feature request or try the workaround suggested!
		</comment>
		<comment id='5' author='devstein' date='2019-08-13T00:43:03Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=31421&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=31421&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='devstein' date='2019-08-13T15:05:46Z'>
		&lt;denchmark-link:https://github.com/k-w-w&gt;@k-w-w&lt;/denchmark-link&gt;

Thanks! I'll make a feature request if this workaround doesn't work 
		</comment>
		<comment id='7' author='devstein' date='2019-08-17T21:49:06Z'>
		&lt;denchmark-link:https://github.com/k-w-w&gt;@k-w-w&lt;/denchmark-link&gt;
 Workaround worked 
For all those that run into this issue, read the guide below!
&lt;denchmark-h:h1&gt;Guide to Deploying a TF 2.0 SavedModel on Spark using PySparK&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Load Dependencies&lt;/denchmark-h&gt;

import tensorflow as tf

from pyspark import SparkFiles
from pyspark.sql.functions import udf
import pyspark.sql.types as T
from pyspark.sql import Row

print(tf.__version__)
&lt;denchmark-h:h2&gt;Fetch SavedModel from S3/GCS and Distribute to Nodes&lt;/denchmark-h&gt;

S3_PREFIX = "s3://"

MODEL_BUCKET = "my-models-bucket"
MODEL_PATH = "path/to/my/model/dir"
MODEL_NAME = "model"

S3_MODEL = f"{S3_PREFIX}{MODEL_BUCKET}/{MODEL_PATH}/{MODEL_NAME}"

print("Fetching model", S3_MODEL)

# Add model to all workers
spark.sparkContext.addFile(S3_MODEL, recursive=True)
&lt;denchmark-h:h2&gt;Create the Input Dataframe&lt;/denchmark-h&gt;

# In this example, the SavedModel has the following format:

# inputs = tf.keras.Input(shape=(784,), name='img')
# x = layers.Dense(64, activation='relu')(inputs)
# x = layers.Dense(64, activation='relu')(x)
# outputs = layers.Dense(10, activation='softmax')(x)
# model = tf.keras.Model(inputs=inputs, # outputs=outputs, name='mnist_model')

(_, _), (x_test, _) = tf.keras.datasets.mnist.load_data()
x_test = x_test.reshape(10000, 784).astype('float32') / 255

rows = list(map(lambda n: Row(img=[n.tolist()]), x_test))

schema = T.StructType([T.StructField('img',T.ArrayType(T.ArrayType(T.FloatType())))])

input_df = spark.createDataFrame(rows, schema=schema)
&lt;denchmark-h:h2&gt;Memoize Retrieval of the Saved Model&lt;/denchmark-h&gt;

# Simple memoization helper with a single cache key
def compute_once(f):
    K = '0'
    cache = {}
    
    def wrapper(x):
        # Set on first call
        if K not in cache:
            cache[K] = f(x)
        
        return cache[K]

    return wrapper
    

def load_model(model_name):
    # Models are saved under the SparkFiles root directory
    root_dir = SparkFiles.getRootDirectory()
    export_dir = f"{root_dir}/{model_name}"
    
    return tf.saved_model.load(export_dir, tags=['serve'])
    

# Only load the model once per worker!
# The reduced disk IO makes prediction much faster
memo_model_load = compute_once(load_model)

def get_model_prediction(model_name, input):
    """
    Note: 
        TF session is scoped to where the model is loaded.
        All calls to the model's ConcreteFunciton must be in the same scope as
        the loaded model (i.e in the same function!)
        
        If not, TF will throw errors for undefined/ variables
    """
    # Load the predict function (from disk or cache)
    m = memo_model_load(model_name)
    
    # Save the predict signature
    pred_func = m.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]
    
    return pred_func(input)
&lt;denchmark-h:h2&gt;Create the Predict UDF&lt;/denchmark-h&gt;

# Decorator with return type of UDF
@udf("array&lt;array&lt;float&gt;&gt;")
def infer(data):
    # Cast the input to a Tensor
    input_data = tf.constant(data)
    
    # Returns a dict of the form { TENSOR_NAME: Tensor }
    outputs = get_model_prediction(MODEL_NAME, input_data)

    # Assuming we have a single output
    output_tensor = list(outputs.values())[0]
    
    # Convert back to regular python
    output_value = output_tensor.numpy().tolist()
    
    return output_value
&lt;denchmark-h:h2&gt;Infer on the Dataset 🎉&lt;/denchmark-h&gt;

predictions_df = input_df.withColumn("predictions", infer("img"))

# All done :) 
predictions_df.show(vertical=True)
		</comment>
	</comments>
</bug>