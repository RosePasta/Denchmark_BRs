<bug id='31040' author='kwaegel' open_date='2019-07-25T18:25:42Z' closed_time='2020-02-14T02:20:02Z'>
	<summary>TFLite conversion fails when using BatchNorm after Reshape (Check failed: dim_x == dim_y)</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 &amp; Windows 10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v1.12.1-6931-g2b5ece29d3 1.15.0-dev20190724
Python version: 3.6.8
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A

Describe the current behavior
Trying to convert a graph containing a reshape layer followed by batch normalization appears to trigger an incorrect op reordering. In particular, the toco converter relocates the mul operation from the BN layer to before the reshape layer, at which point the layers do not have compatible dimensions. This results in the Check failed: dim_x == dim_y error.
From looking at the Graphviz video, this change is introduced in frame 38.
Describe the expected behavior
The converter should not reorder operations across reshape if it would cause the dimensions to no longer match.
In addition, the check failure message should provide more information about the location of the error, such as the originating layer names (which are visible in the Graphviz outputs).
Code to reproduce the issue
&lt;denchmark-code&gt;#!/usr/bin/env python3.6

import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.layers as KL
import tensorflow.keras.backend as K

keras.backend.set_learning_phase(0)  # Build in test mode.

# Construct a minimal graph with Reshape followed by BatchNormalization
input_tensor = KL.Input(shape=[10, 15, 512], name="input_tensor")
r = KL.Reshape(target_shape=[10, 30, 256], name="reshape")(input_tensor)
output_bn = KL.BatchNormalization(axis=3, name="BN")(r)
keras_model = keras.models.Model([input_tensor], [output_bn])

# Convert the graph to TFLite.
converter = tf.lite.TFLiteConverter.from_session(K.get_session(), keras_model.inputs, keras_model.outputs)
#converter.dump_graphviz_dir = "./graphviz"
#converter.dump_graphviz_video = True
tflite_model = converter.convert()
&lt;/denchmark-code&gt;

Other info / logs
This occurs on all version of TF I've tested (1.13.1, 1.14.0, tf-nightly). It's possible that this sequence of operations just isn't supported, but this should be explicitly stated if so. The error message is quite vague and made the problematic sequence of ops extremely difficult to track down in a large graph.
Relevant log messages:
&lt;denchmark-code&gt;2019-07-25 11:06:27.322822: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count &gt;= 8, compute capability &gt;= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2019-07-25 11:06:27.360607: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2019-07-25 11:06:27.370031: E tensorflow/core/grappler/grappler_item_builder.cc:656] Init node BN/gamma/Assign doesn't exist in graph
Traceback (most recent call last):
  File "./bug_report.py", line 21, in &lt;module&gt;
    tflite_model = converter.convert()
  File "[venv]/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py", line 983, in convert
    **converter_kwargs)
  File "[venv]/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py", line 438, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File "[venv]/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py", line 189, in toco_convert_protos
    raise ConverterError("See console for info.\n%s\n%s\n" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2019-07-25 11:06:31.071452: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 12 operators, 24 arrays (0 quantized)
2019-07-25 11:06:31.071874: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 12 operators, 24 arrays (0 quantized)
2019-07-25 11:06:31.072261: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 2 operators, 5 arrays (0 quantized)
2019-07-25 11:06:31.072428: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:118] Check failed: dim_x == dim_y (512 vs. 256)Dimensions must match
Fatal Python error: Aborted

Current thread 0x00007f026b9a0740 (most recent call first):
  File "[venv]/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py", line 52 in execute
  File "[venv]/lib/python3.6/site-packages/absl/app.py", line 251 in _run_main
  File "[venv]/lib/python3.6/site-packages/absl/app.py", line 300 in run
  File "[venv]/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py", line 40 in run
  File "[venv]/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py", line 89 in main
  File "[venv]/bin/toco_from_protos", line 10 in &lt;module&gt;
Aborted (core dumped)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='kwaegel' date='2019-11-02T17:15:24Z'>
		I also meet this problem when use dense→reshape→BN→activation ,when swap the order of BN and reshape it will work well. It refer to &lt;denchmark-link:https://stackoverflow.com/questions/56110234/dimensions-must-match-error-in-tflite-conversion-with-toco&gt;QAQ&lt;/denchmark-link&gt;
 But I don't want this.
Win10
tensorflow 1.13.1
and I use 'tf.lite.TFLiteConverter.from_session' to convert.
Do you have any other way to solve thie problem?
		</comment>
		<comment id='2' author='kwaegel' date='2019-11-12T16:53:01Z'>
		Same here.
I've a sort of Unet with embedding. After embedding (linear layer) I reshape the tensor to square (16x16x162) and proceed.
I faced two problems, actually: one is addressed by &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/30124&gt;#30124&lt;/denchmark-link&gt;

but just changing the [None, 16, 16, 162] to [-1, 16, 16, 162] did the trick (as follows)
x = tf.reshape(x, (-1, xsize[1], xsize[2], xsize[3]))
instead of
x = tf.reshape(x, xsize)
But then I add a BNorm and the
Check failed: dim_x == dim_y (162 vs. 41472)Dimensions must match
thing happened.
Looking at the numbers (162 and 41472 = 16x16x162) it seems that it has something to do with the bnorm probably acting before the reshape, not after, as stated in the issue.
I tried also to disentangle the two adding some variant of
x = x * 1
x = x + 1 (and -1)
but no luck.
Of course in TF everything is fine, this happens only during the conversion.
		</comment>
		<comment id='3' author='kwaegel' date='2020-02-01T00:21:04Z'>
		hey,
TOCO will be deprecated soon. Could you try use the new MLIR converter and see if issue still persist?
To use it, please download the latest tf-nightly or recent stable release of tf, and then add this to your code:
converter.experimental_new_converter = True.
(see user guide here: &lt;denchmark-link:https://www.tensorflow.org/lite/convert/python_api&gt;https://www.tensorflow.org/lite/convert/python_api&lt;/denchmark-link&gt;
)
		</comment>
		<comment id='4' author='kwaegel' date='2020-02-12T09:32:19Z'>
		I just tried with tf2.1 (on windows), updated a few days ago.
Inserting the
converter.experimental_new_converter = True
that error disappeared.
Now I have other issues like
error: 'tf.ResizeBilinear' op is neither a custom op nor a flex op
which I believe depends on other things.
Without the
converter.experimental_new_converter = True
the error is still there so it definitely makes a difference (but I'm still not able to convert my model :)).
		</comment>
		<comment id='5' author='kwaegel' date='2020-02-12T18:57:45Z'>
		TF lite has ResizeBilinear op:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/resize_bilinear.cc&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/resize_bilinear.cc&lt;/denchmark-link&gt;

I'm guessing that the TF 2.1 doesn't contain the relevant converter changes to support this op, could you try convert your model with tf-nightly? Thanks.
		</comment>
		<comment id='6' author='kwaegel' date='2020-02-13T09:27:11Z'>
		Unfortunately I can't install the tf-nightly here, so I can't help.
For me the issue was solved just by adding this (found somewhere on the internet):
&lt;denchmark-code&gt;converter.experimental_new_converter = True
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                                       tf.lite.OpsSet.SELECT_TF_OPS]
&lt;/denchmark-code&gt;

And now the model is converted. I have yet to check for correctness, but at least I have a float32 tflite model. Float16 has some other problems which i'll try to investigate later.
		</comment>
		<comment id='7' author='kwaegel' date='2020-02-14T02:20:02Z'>
		I see.
tf.lite.OpsSet.SELECT_TF_OPS tells the converter to use the TF ResizeBilinear op instead. So it can convert. But the binary size may grow a bit.
		</comment>
		<comment id='8' author='kwaegel' date='2020-02-14T02:20:04Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31040&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31040&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>