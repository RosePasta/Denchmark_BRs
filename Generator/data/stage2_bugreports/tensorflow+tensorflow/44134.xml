<bug id='44134' author='DominikStachura' open_date='2020-10-19T09:02:56Z' closed_time='2020-11-10T00:19:49Z'>
	<summary>Invoke returns NaN values when performing on floating points</summary>
	<description>
&lt;denchmark-link:https://github.com/orgs/tensorflow/teams/micro&gt;@tensorflow/micro&lt;/denchmark-link&gt;

System information

Host Platform: Ubuntu 18.04.4 LTS
TensorFlow installed from source,
Tensorflow version: 86726ad
Target platform: STM32H745 (ARM M7)

I have Keras model trained on host platform. I want to use this model on my target device, after conversion to .tflite format. Once I perform conversion with quantization and obtain INT8 weights values, model works fine and gives correct predictions.
But when I try to convert model without quantization and try to run it with floating points values, inout vector and output vector contain NaN values after calling Invoke().
Input vector after calling invoke on INT8 weights:
&lt;denchmark-link:https://user-images.githubusercontent.com/58625554/96422788-c5060600-11f8-11eb-8d23-6e7a12bd7c27.PNG&gt;&lt;/denchmark-link&gt;

Input vector after calling invoke on Floating Points weights:
&lt;denchmark-link:https://user-images.githubusercontent.com/58625554/96422840-d5b67c00-11f8-11eb-9cf6-13be10c915a2.PNG&gt;&lt;/denchmark-link&gt;

Code that I use to convert with quantization:
import tensorflow as tf
import tensorflow.keras as keras
import numpy as np
import pickle

model = keras.models.load_model('data/models/best_model.hdf5')

X_train = pickle.load(open("dataset_from_target.pickle", "rb"))['x_train']

converter = tf.lite.TFLiteConverter.from_keras_model(model)
model_quant_tflite = converter.convert()


def representative_dataset():
  for i in range(1000):
    yield([np.expand_dims(X_train[i].astype(np.float32), axis=0)])


converter.optimizations = [tf.lite.Optimize.DEFAULT]

converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]

converter.representative_dataset = representative_dataset
model_tflite = converter.convert()

open("model_target.tflite", "wb").write(model_tflite)
Code that I use to convert with no quantization:
import tensorflow as tf
import tensorflow.keras as keras


model = keras.models.load_model('data/models/best_model.hdf5')

converter = tf.lite.TFLiteConverter.from_keras_model(model)
model_no_quant_tflite = converter.convert()

# Set the optimization flag.
converter.optimizations = [tf.lite.Optimize.DEFAULT]
model_tflite = converter.convert()

open("model_target_new_data_no_quant.tflite", "wb").write(model_tflite)
Problem is not with RAM memory, which I have already checked. NaN values in 'input-&gt;data.f' results in all NaN values in 'output-&gt;data.f' which makes predictions impossible. Before calling invoke, input vector contains correct values.
	</description>
	<comments>
		<comment id='1' author='DominikStachura' date='2020-10-19T09:41:40Z'>
		I noticed that the prolem disappear when I get rid of optimization during conversion:
&lt;denchmark-code&gt;model = keras.models.load_model('data/models/best_model.hdf5')


converter = tf.lite.TFLiteConverter.from_keras_model(model)

model_tflite = converter.convert()

open("model_target_new_data_no_quant.tflite", "wb").write(model_tflite)
&lt;/denchmark-code&gt;

But then the model weight is twice as big as with the optimization.
		</comment>
		<comment id='2' author='DominikStachura' date='2020-10-26T22:35:32Z'>
		Can you try enforcing full integer quantization for all ops including the input and output?
See &lt;denchmark-link:https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only&gt;https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only&lt;/denchmark-link&gt;

converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8  # or tf.uint8
converter.inference_output_type = tf.int8  # or tf.uint8
model_tflite = converter.convert()
		</comment>
		<comment id='3' author='DominikStachura' date='2020-11-02T23:34:02Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='4' author='DominikStachura' date='2020-11-10T00:19:48Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='5' author='DominikStachura' date='2020-11-10T00:19:51Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44134&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44134&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>