<bug id='35226' author='manish-kumar-garg' open_date='2019-12-18T13:53:38Z' closed_time='2020-01-07T08:09:12Z'>
	<summary>ValueError: `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call.</summary>
	<description>
I am not able to run training using tf.distribute.Strategy
However, it works fine without distribution.
Below is the code block for training loop
&lt;denchmark-code&gt;from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from absl import app
import os
import tensorflow as tf # TF2
import model_timit as model
import kaldi_io
from DataLoader_timit import SequentialLoader
from warprnnt_tensorflow import rnnt_loss
assert tf.__version__.startswith('2')

class Train(object):

  def __init__(self, epochs, decoder,batch_size):
    self.epochs = epochs
    self.decoder = decoder
    self.batch_size = batch_size
    self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.0004)
    self.train_loss_metric = tf.keras.metrics.Mean(name='train_loss')
    self.checkpoint = tf.train.Checkpoint(
            decoder=self.decoder,
            optimizer=self.optimizer)

  def loss_function(self, pred,real,xlen,ylen):
    loss_ = rnnt_loss(pred,real,xlen,ylen,0)
    return tf.reduce_sum(loss_) * 1. / self.batch_size

  def train_step(self, inputs):
    loss = 0
    inp, targ,xlen,ylen = inputs

    with tf.GradientTape() as tape:
      xs_1,xs,predictions = self.decoder(
            inp, targ)
      time_dim = tf.shape(predictions)[1]
      loss += self.loss_function(predictions,targ,xlen,ylen)

    batch_loss = (loss / int(targ.shape[1]))
    variables = (self.decoder.trainable_variables)
    gradients = tape.gradient(loss, variables)
    self.optimizer.apply_gradients(zip(gradients, variables))
    #self.optimizer.apply_gradients(list(zip(gradients, variables)))

    self.train_loss_metric(batch_loss)

    return self.train_loss_metric.result().numpy()

class DistributedTrain(Train):
  def __init__(self, epochs, decoder, batch_size, local_batch_size):
    Train.__init__(
        self, epochs, decoder, local_batch_size)

  def training_loop(self, train_ds, test_ds, strategy):
    def distributed_train(inp, targ, xlen, ylen):
      returnstrategy.experimental_run_v2(self.train_step((inp, targ, xlen, ylen)))

    distributed_train = tf.function(distributed_train)
    template = 'Epoch: {}, Train Loss: {}, Test Loss: {}'
    for epoch in range(self.epochs):
      self.train_loss_metric.reset_states()
      for i, (inp, targ, xlen, ylen) in enumerate(train_ds):
        distributed_train(inp, targ, xlen, ylen)

def main(epochs=200, batch_size=16, num_examples=70000, embedding_dim=256, enc_units=1024, dec_units=1024):

  strategy = tf.distribute.MirroredStrategy(devices=["/gpu:0","/gpu:1"])
  num_replicas = strategy.num_replicas_in_sync

  train_ds = SequentialLoader('train', batch_size)
  test_ds = SequentialLoader('test', batch_size)

  with strategy.scope():
    decoder = model.Transducer(39, 62, 250, 3, 0.5,bidirectional=False)
    train_obj = DistributedTrain(10, decoder, batch_size, 8)

    print ('Training ...')
    return train_obj.training_loop(train_ds, test_ds, strategy)

if __name__ == '__main__':
  app.run(main)


&lt;/denchmark-code&gt;

This is the error.
&lt;denchmark-code&gt;train_timit_distributed.py:97 distributed_train  *
        per_example_loss = strategy.experimental_run_v2(self.train_step((inp, targ, xlen, ylen)))
    train_timit_distributed.py:61 train_step  *
        gradients = tape.gradient(loss, variables)
    /home/ubuntu/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py:996 gradient
        flat_sources = [_handle_or_self(x) for x in flat_sources]
    /home/ubuntu/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py:996 &lt;listcomp&gt;
        flat_sources = [_handle_or_self(x) for x in flat_sources]
    /home/ubuntu/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py:687 _handle_or_self
        x = x.handle
    /home/ubuntu/tf2/lib/python3.6/site-packages/tensorflow_core/python/distribute/values.py:717 handle
        raise ValueError("`handle` is not available outside the replica context"

    ValueError: `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call.

&lt;/denchmark-code&gt;

How to fix this?
	</description>
	<comments>
		<comment id='1' author='manish-kumar-garg' date='2019-12-20T05:43:04Z'>
		&lt;denchmark-link:https://github.com/manish-kumar-garg&gt;@manish-kumar-garg&lt;/denchmark-link&gt;

Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Thanks!
		</comment>
		<comment id='2' author='manish-kumar-garg' date='2019-12-23T02:46:50Z'>
		I get the same error trying to create a Resnet50 model with mirrored distribution across 2 GPUs.
I am training on:

Ubuntu 18
Anaconda 2019 Python 3.7
Tensorflow 2.0
Keras

		</comment>
		<comment id='3' author='manish-kumar-garg' date='2019-12-23T02:47:59Z'>
		`&gt;&gt;&gt; with strategy.scope():
...     parallel_model =  EAST_model(FLAGS.input_size)
...
Traceback (most recent call last):
File "", line 2, in 
File "/d/src/pid/EAST/model.py", line 30, in init
resnet = ResNet50(input_tensor=input_image, weights='imagenet', include_top=False, pooling=None)
File "/d/home/aisrv/.conda/envs/aidev/lib/python3.7/site-packages/keras/applications/init.py", line 20, in wrapper
return base_fun(*args, **kwargs)
File "/d/home/aisrv/.conda/envs/aidev/lib/python3.7/site-packages/keras/applications/resnet50.py", line 11, in ResNet50
return resnet50.ResNet50(*args, **kwargs)
File "/d/home/aisrv/.conda/envs/aidev/lib/python3.7/site-packages/keras_applications/resnet50.py", line 231, in ResNet50
x = layers.BatchNormalization(axis=bn_axis, name='bn_conv1')(x)
File "/d/home/aisrv/.conda/envs/aidev/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py", line 75, in symbolic_fn_wrapper
return func(*args, **kwargs)
File "/d/home/aisrv/.conda/envs/aidev/lib/python3.7/site-packages/keras/engine/base_layer.py", line 489, in call
output = self.call(inputs, **kwargs)
File "/d/home/aisrv/.conda/envs/aidev/lib/python3.7/site-packages/keras/layers/normalization.py", line 199, in call
self.momentum),
File "/d/home/aisrv/.conda/envs/aidev/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py", line 75, in symbolic_fn_wrapper
return func(*args, **kwargs)
File "/d/home/aisrv/.conda/envs/aidev/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py", line 1296, in moving_average_update
with tf_ops.colocate_with(x):
File "/d/home/aisrv/.conda/envs/aidev/lib/python3.7/contextlib.py", line 112, in enter
return next(self.gen)
File "/d/home/aisrv/.conda/envs/aidev/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 4220, in _colocate_with_for_gradient
with self.colocate_with(op, ignore_existing):
File "/d/home/aisrv/.conda/envs/aidev/lib/python3.7/contextlib.py", line 112, in enter
return next(self.gen)
File "/d/home/aisrv/.conda/envs/aidev/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 4269, in colocate_with
op = _op_to_colocate_with(op, self)
File "/d/home/aisrv/.conda/envs/aidev/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 6603, in _op_to_colocate_with
if hasattr(v, "handle") and hasattr(v.handle, "op") and isinstance(
File "/d/home/aisrv/.conda/envs/aidev/lib/python3.7/site-packages/tensorflow_core/python/distribute/values.py", line 717, in handle
raise ValueError("handle is not available outside the replica context"
ValueError: handle is not available outside the replica context or a tf.distribute.Strategy.update() call.`
		</comment>
		<comment id='4' author='manish-kumar-garg' date='2019-12-23T13:19:16Z'>
		hahaï¼ŒI also have same problem.
I am training on:
&lt;denchmark-code&gt;Ubuntu 16.04.2
Python 3.7
Tensorflow 2.0 (install from the source code)
cuda 9.0
cudnn 7
&lt;/denchmark-code&gt;

my codes are not able to run training using tf.distribute.Strategy
However, it works fine without distribution.
Below is the error.
Traceback (most recent call last):
  File "main.py", line 134, in &lt;module&gt;
    train_step(inp, tar, enc_hidden)
  File "/disk1/lx/conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 457, in __call__
    result = self._call(*args, **kwds)
  File "/disk1/lx/conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 503, in _call
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File "/disk1/lx/conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 408, in _initialize
    *args, **kwds))
  File "/disk1/lx/conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1848, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File "/disk1/lx/conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 2150, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/disk1/lx/conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 2041, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File "/disk1/lx/conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/disk1/lx/conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 358, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "/disk1/lx/conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py", line 905, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in converted code:

    main.py:90 train_step  *
        gradients = tape.gradient(loss, variables)
    /disk1/lx/conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py:996 gradient
        flat_sources = [_handle_or_self(x) for x in flat_sources]
    /disk1/lx/conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py:996 &lt;listcomp&gt;
        flat_sources = [_handle_or_self(x) for x in flat_sources]
    /disk1/lx/conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py:687 _handle_or_self
        x = x.handle
    /disk1/lx/conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/distribute/values.py:717 handle
        raise ValueError("`handle` is not available outside the replica context"

    ValueError: `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call.
and if my code works without distribution, the gpu:0 used 11000MB, the gpu:1 used 128Mb.
		</comment>
		<comment id='5' author='manish-kumar-garg' date='2019-12-27T00:31:03Z'>
		manish-kumar-garg@ Is this the complete stack trace? Does the failure happen at the tape.gradient call? Can you also provide a smaller reproducible snippet which will help in debugging?
		</comment>
		<comment id='6' author='manish-kumar-garg' date='2019-12-27T01:14:31Z'>
		It looks like the issue is how you are using experimental_run_v2. Can you specify the args argument like this:
return strategy.experimental_run_v2(self.train_step, args=((inp, targ, xlen, ylen)))
		</comment>
		<comment id='7' author='manish-kumar-garg' date='2019-12-27T18:30:47Z'>
		&lt;denchmark-link:https://github.com/SmileTM&gt;@SmileTM&lt;/denchmark-link&gt;
  I could not find the  call in the repo above. Can you point me to that?
		</comment>
		<comment id='8' author='manish-kumar-garg' date='2019-12-29T07:24:12Z'>
		
@SmileTM I could not find the experimental_run_v2 call in the repo above. Can you point me to that?

ðŸ˜¯ohï¼Œ thank you.
I thought it was as simple as keras,  just add two lines of code.
I have rebuilt my codeï¼Œ it works well in two GPU  .
thank you.
		</comment>
		<comment id='9' author='manish-kumar-garg' date='2020-01-02T16:40:41Z'>
		I get the same error with 4 lines of code
&lt;denchmark-code&gt;import sys
print(sys.version)
# 3.6.9 (default, Nov  7 2019, 10:44:02) 
# [GCC 8.3.0]

import tensorflow 


print(f"version = {tensorflow.__version__}")
# version = 2.1.0-rc2

# Crop of real program, just to show it does not crash.
# Real program has much more, and fully works with a single gpu
input_data = tensorflow.keras.layers.Input(name="input", shape=(256, 64, 1))

cnn = tensorflow.keras.layers.Conv2D(filters=16, kernel_size=(3,3), strides=(2,2), padding="same", kernel_initializer="he_uniform")(input_data)
cnn = tensorflow.keras.layers.PReLU(shared_axes=[1,2])(cnn)
cnn = tensorflow.keras.layers.BatchNormalization(renorm=True)(cnn)

print(f"Everything good up to here, shape={cnn.shape}")
# Everything good up to here, shape=(None, 128, 32, 16)


################################################################
# same code inside MirroredStrategy crashes
strategy = tensorflow.distribute.MirroredStrategy()
with strategy.scope():

    input_data = tensorflow.keras.layers.Input(name="input", shape=(256, 64, 1))

    cnn = tensorflow.keras.layers.Conv2D(filters=16, kernel_size=(3,3), strides=(2,2), padding="same", kernel_initializer="he_uniform")(input_data)
    cnn = tensorflow.keras.layers.PReLU(shared_axes=[1,2])(cnn)
    cnn = tensorflow.keras.layers.BatchNormalization(renorm=True)(cnn)

    print("crash in step above")
&lt;/denchmark-code&gt;

My environment is docker. Nothing special, but just in case here is my Dockerfile:
&lt;denchmark-code&gt;FROM tensorflow/tensorflow:2.1.0rc2-gpu-py3-jupyter

RUN pip3 install ptvsd

# see bug tensorflow/tensorflow #35434 
ARG CUDA=10.1
RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends --allow-downgrades \
        cuda-nvrtc-${CUDA/./-} \
        cuda-nvrtc-dev-${CUDA/./-} \
        libcublas10=10.2.1.243-1 \ 
        libcublas-dev=10.2.1.243-1 

# kludge in main.py needs to be looked at
# when driver changes. Search for 
# https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth

# Stuff needed for the ocr app
RUN pip install autopep8==1.4.4
RUN pip install editdistance==0.5.3
RUN pip install flake8==3.7.9
RUN pip install kaldiio==2.15.0
RUN pip install numba==0.46.0
RUN ["apt-get", "install", "-y", "libsm6", "libxext6", "libxrender-dev"]
RUN pip install opencv-python==4.1.2.30

RUN apt-get update

# for the visual debugger
RUN pip3 install ptvsd

# A place for tensorboard logs. Not using presently.
RUN mkdir /tmp/logs

# replicate the tensorflow/tensorflow launch command
CMD ["bash", "-c", "source /etc/bash.bashrc &amp;&amp; jupyter notebook --NotebookApp.iopub_data_rate_limit=1000000000000 --notebook-dir=/tf --ip 0.0.0.0 --no-browser --allow-root"]

&lt;/denchmark-code&gt;

I don't understand the solution SmileTM is referring to. Can someone elaborate?
		</comment>
		<comment id='10' author='manish-kumar-garg' date='2020-01-03T12:44:44Z'>
		
I get the same error with 4 lines of code
import sys
print(sys.version)
# 3.6.9 (default, Nov  7 2019, 10:44:02) 
# [GCC 8.3.0]

import tensorflow 


print(f"version = {tensorflow.__version__}")
# version = 2.1.0-rc2

# Crop of real program, just to show it does not crash.
# Real program has much more, and fully works with a single gpu
input_data = tensorflow.keras.layers.Input(name="input", shape=(256, 64, 1))

cnn = tensorflow.keras.layers.Conv2D(filters=16, kernel_size=(3,3), strides=(2,2), padding="same", kernel_initializer="he_uniform")(input_data)
cnn = tensorflow.keras.layers.PReLU(shared_axes=[1,2])(cnn)
cnn = tensorflow.keras.layers.BatchNormalization(renorm=True)(cnn)

print(f"Everything good up to here, shape={cnn.shape}")
# Everything good up to here, shape=(None, 128, 32, 16)


################################################################
# same code inside MirroredStrategy crashes
strategy = tensorflow.distribute.MirroredStrategy()
with strategy.scope():

    input_data = tensorflow.keras.layers.Input(name="input", shape=(256, 64, 1))

    cnn = tensorflow.keras.layers.Conv2D(filters=16, kernel_size=(3,3), strides=(2,2), padding="same", kernel_initializer="he_uniform")(input_data)
    cnn = tensorflow.keras.layers.PReLU(shared_axes=[1,2])(cnn)
    cnn = tensorflow.keras.layers.BatchNormalization(renorm=True)(cnn)

    print("crash in step above")

My environment is docker. Nothing special, but just in case here is my Dockerfile:
FROM tensorflow/tensorflow:2.1.0rc2-gpu-py3-jupyter

RUN pip3 install ptvsd

# see bug tensorflow/tensorflow #35434 
ARG CUDA=10.1
RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends --allow-downgrades \
        cuda-nvrtc-${CUDA/./-} \
        cuda-nvrtc-dev-${CUDA/./-} \
        libcublas10=10.2.1.243-1 \ 
        libcublas-dev=10.2.1.243-1 

# kludge in main.py needs to be looked at
# when driver changes. Search for 
# https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth

# Stuff needed for the ocr app
RUN pip install autopep8==1.4.4
RUN pip install editdistance==0.5.3
RUN pip install flake8==3.7.9
RUN pip install kaldiio==2.15.0
RUN pip install numba==0.46.0
RUN ["apt-get", "install", "-y", "libsm6", "libxext6", "libxrender-dev"]
RUN pip install opencv-python==4.1.2.30

RUN apt-get update

# for the visual debugger
RUN pip3 install ptvsd

# A place for tensorboard logs. Not using presently.
RUN mkdir /tmp/logs

# replicate the tensorflow/tensorflow launch command
CMD ["bash", "-c", "source /etc/bash.bashrc &amp;&amp; jupyter notebook --NotebookApp.iopub_data_rate_limit=1000000000000 --notebook-dir=/tf --ip 0.0.0.0 --no-browser --allow-root"]

I don't understand the solution SmileTM is referring to. Can someone elaborate?

&lt;denchmark-link:https://github.com/johngrabner&gt;@johngrabner&lt;/denchmark-link&gt;

I think you should remove renorm=True.
Maybe,  renorm=True will have some conflicts  in GPU distribute.
cnn = tensorflow.keras.layers.BatchNormalization(renorm=True)(cnn)
change to
cnn = tensorflow.keras.layers.BatchNormalization()(cnn)
		</comment>
		<comment id='11' author='manish-kumar-garg' date='2020-01-07T08:09:11Z'>
		&lt;denchmark-link:https://github.com/manish-kumar-garg&gt;@manish-kumar-garg&lt;/denchmark-link&gt;
 Passing experimental_run_v2 arguments using the args tuple should fix the issue. Please reopen if the suggested solution does not work.
		</comment>
		<comment id='12' author='manish-kumar-garg' date='2020-01-07T08:09:13Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35226&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35226&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='manish-kumar-garg' date='2020-01-13T03:51:12Z'>
		Sorry, I don't understand "Passing experimental_run_v2 arguments using the args tuple"
How do I modify this code to use your suggestion?
&lt;denchmark-code&gt;strategy = tensorflow.distribute.MirroredStrategy()
with strategy.scope():

    input_data = tensorflow.keras.layers.Input(name="input", shape=(256, 64, 1))

    cnn = tensorflow.keras.layers.Conv2D(filters=16, kernel_size=(3,3), strides=(2,2), padding="same", kernel_initializer="he_uniform")(input_data)
    cnn = tensorflow.keras.layers.PReLU(shared_axes=[1,2])(cnn)
    cnn = tensorflow.keras.layers.BatchNormalization(renorm=True)(cnn)

    print("crash in step above")
&lt;/denchmark-code&gt;

		</comment>
		<comment id='14' author='manish-kumar-garg' date='2020-03-20T17:30:48Z'>
		Same here, can someone  elaborate the solution further?
		</comment>
		<comment id='15' author='manish-kumar-garg' date='2020-03-30T05:39:05Z'>
		Same here...
		</comment>
		<comment id='16' author='manish-kumar-garg' date='2020-04-02T08:52:14Z'>
		&lt;denchmark-link:https://github.com/johngrabner&gt;@johngrabner&lt;/denchmark-link&gt;
 you might be interested in this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/31531&gt;#31531&lt;/denchmark-link&gt;
.
and for &lt;denchmark-link:https://github.com/ElPapi42&gt;@ElPapi42&lt;/denchmark-link&gt;
 &amp; &lt;denchmark-link:https://github.com/fzyzcjy&gt;@fzyzcjy&lt;/denchmark-link&gt;
 the doco is here &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#experimental_run_v2&gt;https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#experimental_run_v2&lt;/denchmark-link&gt;
 and its referenced because the original issue is raised around use of this API.
		</comment>
		<comment id='17' author='manish-kumar-garg' date='2020-06-10T01:37:27Z'>
		Has there been any update on this?
		</comment>
		<comment id='18' author='manish-kumar-garg' date='2020-07-03T18:38:32Z'>
		I'm having the same issue. Perhaps there is something with using BatchNormalization in the strategy scope. If you remove this normalization layer, the training goes well....
		</comment>
		<comment id='19' author='manish-kumar-garg' date='2020-07-10T13:04:42Z'>
		I'm also having the same issue. I have a U-Net architecture with multiple BatchNormalization layers, but whenever I remove this layer the problem no longer exists.
A code snippet is something like this:
pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
renorm=True is not included!
How to fix it?
		</comment>
		<comment id='20' author='manish-kumar-garg' date='2020-07-10T13:06:29Z'>
		
I'm having the same issue. Perhaps there is something with using BatchNormalization in the strategy scope. If you remove this normalization layer, the training goes well....

Did you find a solution to this problem without removing the BatchNormalization  layers?
&lt;denchmark-link:https://github.com/samcaetano&gt;@samcaetano&lt;/denchmark-link&gt;

		</comment>
		<comment id='21' author='manish-kumar-garg' date='2020-07-14T14:59:04Z'>
		

I'm having the same issue. Perhaps there is something with using BatchNormalization in the strategy scope. If you remove this normalization layer, the training goes well....

Did you find a solution to this problem without removing the BatchNormalization layers?
@samcaetano

I couldn't find a solution for that. I really didn't understand this bug. By now, I'm running the same model with the strategy scope on cpu, WITH batchNormalization, and the model doesn't complain about this ValueError no longer.
I still have something in mind which is: maybe the strategy scope on gpu has problem on normalizing the batches.
But actually, I am not sure.
		</comment>
	</comments>
</bug>