<bug id='9757' author='luxiaolei' open_date='2017-05-08T13:16:22Z' closed_time='2017-05-15T11:50:46Z'>
	<summary>Large page fault causes slow performance while using gpu</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
TensorFlow installed from (source or binary):
binary, using pip install tensorflow-gpu==1.0.1
TensorFlow version (use command below):
1.0.1
Bazel version (if compiling from source):
CUDA/cuDNN version:
8.0/5.1
GPU model and memory:
nvidia gtx1080, 8g

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

I have observed a large amount of page fault while running the provided sample code on gpu, and this causes a serious performance drawdown.
The key parts of the output of /usr/bin/time -v python sample.py are:
&lt;denchmark-code&gt;System time (seconds): 7.28  
Percent of CPU this job got: 85%  
Elapsed (wall clock) time (h:mm:ss or m:ss): 0:22.41 
Minor (reclaiming a frame) page faults: 684695  
Involuntary context switches: 164 
File system inputs: 0  
File system outputs: 8  
&lt;/denchmark-code&gt;

There are 684k page faults,  and the gpu-volatile usage is only about 30%.
I am very hesitating to ask for help here, because on another system with exact os, software and gpu, this issue does not appears, I have posted on stackoverflow to compare two systems &lt;denchmark-link:http://stackoverflow.com/questions/43842731/two-exactly-same-systems-have-very-different-performances-when-running-tensorflo&gt;here&lt;/denchmark-link&gt;

Is that possible that tensorflow handles different hardwares differently? It looks to me that the gpu-cpu I/O may have caused this issue, and I suspect that I need to configure my hardware settings somewhere, but don't know how.
Things I have tried:

Upgrade BIOS to the latest version and reset default settings.
Call Asus(my motherboard and gpu vendor) customer service for help.
Inject LD_PRELOAD="/usr/lib/libtcmalloc.so" to .bashrc file.

&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

Here is the sample code I used to test
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np
from tqdm import trange

np.random.seed(111)
h,w = 3000, 2000
steps = 1000

x = tf.placeholder(dtype=tf.float32, shape=[h, w], name='x')
t = tf.constant(np.random.random(size=[w, w]), dtype=tf.float32)
m = tf.matmul(x,t)

x0 = np.random.random(size=[h, w])
sess = tf.Session()
for i in trange(steps):
    x0 = sess.run(m, feed_dict={x: x0})
&lt;/denchmark-code&gt;

The attachment contains:  output,  output, hardware specs in html format, chrome trace timeline.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/983550/sysB.zip&gt;sysB.zip&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='luxiaolei' date='2017-05-09T00:09:30Z'>
		CC: &lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='luxiaolei' date='2017-05-12T09:50:25Z'>
		Just to provide another information, my memory Bank Locator is  reported by using  , whereas for a issue free computer it is . Dont know if its making any differences. &lt;denchmark-link:https://superuser.com/questions/1208443/what-is-ram-bank-locator/1208473#1208473&gt;Here is my post on Superuser discussing this.&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='luxiaolei' date='2017-05-14T16:30:05Z'>
		&lt;denchmark-link:https://github.com/luxiaolei&gt;@luxiaolei&lt;/denchmark-link&gt;
 you are using  which means that tensorflow copies array from Python to TensorFlow runtime at each step.
There's a way to avoid the copy in latest TF version if your data is aligned. Can you try latest TensorFlow nightly + align the data to 64 bits using recipe from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/9690#issuecomment-299521465&gt;#9690 (comment)&lt;/denchmark-link&gt;
 to see if this speeds things up?
		</comment>
		<comment id='4' author='luxiaolei' date='2017-05-15T11:50:38Z'>
		&lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 Sorry for the late reply. I upgraded Tensorflow to version 1.1.0-rc2, ran the sample code with and without align the first input to 64 bits, both observed a 2.3 times speed boost! WOW, what a huge boost! I have noticed that the output of  is 64 bits aligned, whereas in version 1.0.1 is not.
Many thanks ! Really saved my day! My problem solved, closing the issue.
		</comment>
		<comment id='5' author='luxiaolei' date='2017-05-15T11:51:35Z'>
		If you need me to do more tests, I am very glad to:)
		</comment>
		<comment id='6' author='luxiaolei' date='2017-05-15T14:59:32Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
  -- looks like a happy customer of your copy-free numpy arrays :)
		</comment>
		<comment id='7' author='luxiaolei' date='2017-06-17T04:55:04Z'>
		Minor nitpick in case anyone references this thread in the future: the speedup came by aligning with 64 bytes, not 64 bits (=8 bytes).
The EIGEN_MAX_ALIGN variable depends on the vector architecture with which Eigen was compiled. As far as I know, AVX-512 (= 64 bytes) is the largest vector size (and largest value for EIGEN_MAX_ALIGN) in the wild.
		</comment>
		<comment id='8' author='luxiaolei' date='2019-05-16T02:28:29Z'>
		why does version 1.2 still have many page faults?
&lt;denchmark-link:https://user-images.githubusercontent.com/3445470/57821998-343a2800-77c5-11e9-96a1-6182b232a75d.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='luxiaolei' date='2019-05-16T17:29:59Z'>
		Version 1.2 is way too old.
		</comment>
	</comments>
</bug>