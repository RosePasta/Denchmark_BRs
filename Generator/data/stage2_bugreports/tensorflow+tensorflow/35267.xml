<bug id='35267' author='tomwphillips' open_date='2019-12-19T16:21:25Z' closed_time='2020-04-14T08:47:25Z'>
	<summary>Training fails when a multi-output Keras model has one output without a loss function</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see minimal example.
OS Platform and Distribution: Ubuntu 18.04.3 LTS
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  N/A
TensorFlow installed from (source or binary): binary (specifically, tensorflow/tensorflow:nightly-py3 Docker image)
TensorFlow version (use command below): 2.1.0-dev20191216
Python version: 3.6.9
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A

Describe the current behavior
A multi-output Keras model compiled so that one output doesn't have a loss function raises an exception when calling .fit.
Describe the expected behavior
Training should minimise the losses defined for the other output(s).
Code to reproduce the issue
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf
import tensorflow.keras as keras

input_a = keras.layers.Input(shape=(10,), name="input_a")
input_b = keras.layers.Input(shape=(20,), name="input_b")
output_a = keras.layers.Dense(1, name="output_a")(input_a)
output_b = keras.layers.Dense(1, name="output_b")(input_b)
model = keras.Model(inputs=[input_a, input_b], outputs=[output_a, output_b])
model.compile(optimizer="sgd", loss={"output_a": None, "output_b": "mse"})

n = 128
input_a = np.ones((n, 10))
input_b = np.ones((n, 20))
output_a = np.ones((n, 1))
output_b = np.ones((n, 1))

dataset = tf.data.Dataset.from_tensor_slices(
    ((input_a, input_b), (output_a, output_b))
).batch(64)

model.fit(dataset)
&lt;/denchmark-code&gt;

Raises:
&lt;denchmark-code&gt;ValueError: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), for inputs ['output_b'] but instead got the following list of 2 arrays: [&lt;tf.Tensor 'args_2:0' shape=(None, 1) dtype=float64&gt;, &lt;tf.Tensor 'args_3:0' shape=(None, 1) dtype=float64&gt;]...
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='tomwphillips' date='2019-12-20T09:20:44Z'>
		Was able to reproduce the issue with TF version 2.1.0-rc1. Please find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/amahendrakar/2c1f1df27ea0beffcf85ce142a1d21ce/35267.ipynb&gt;Gist&lt;/denchmark-link&gt;
 here. Thanks!
		</comment>
		<comment id='2' author='tomwphillips' date='2020-01-09T19:54:35Z'>
		You need to define two dictionaries since you have two outputs. Perhaps you can try defining a dummy loss function for one of the inputs.
		</comment>
		<comment id='3' author='tomwphillips' date='2020-01-13T11:01:51Z'>
		Yes, that's what I did in the end:
&lt;denchmark-code&gt;import tensorflow.keras.backend as K

def null_loss(y_true, y_pred):
    return K.zeros_like(y_true)
&lt;/denchmark-code&gt;

Perhaps a more descriptive error message is neccesary to guide users in the right direction? It took me a while to figure out what was going on.
		</comment>
		<comment id='4' author='tomwphillips' date='2020-02-07T14:50:18Z'>
		&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
 There is a bug in the  block of &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_eager.py#L145&gt;in training_eager.py&lt;/denchmark-link&gt;
.  The block:    losses from .  But then the enumerated index  in  then no longer matches positions of the  losses in .  E.g., if the first loss is  and excluded from , and , then when , the second loss callable gets paired with the name of the first loss.
		</comment>
		<comment id='5' author='tomwphillips' date='2020-02-08T04:05:24Z'>
		&lt;denchmark-link:https://github.com/tomwphillips&gt;@tomwphillips&lt;/denchmark-link&gt;
 this is by design. You do not need to feed target data for the output for which there is no loss function during training. You can pass a dictionary with just the other output like .
Target data is used for computing loss so in this case it is not required.
		</comment>
		<comment id='6' author='tomwphillips' date='2020-02-08T16:01:48Z'>
		&lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;
 I think the following modification of &lt;denchmark-link:https://github.com/tomwphillips&gt;@tomwphillips&lt;/denchmark-link&gt;
 's example more clearly shows there is in fact a bug in training_eager.py, please see the comments in code:
&lt;denchmark-code&gt;import tensorflow as tf

# a model that forks into two independent heads 'one' and 'two'
inputs = tf.keras.layers.Input(shape=(1,))
one = tf.keras.layers.Dense(units=1, name='one')(inputs)
two = tf.keras.layers.Dense(units=1, name='two')(inputs)
model = tf.keras.Model(inputs=inputs, outputs=dict(one=one, two=two))

losses = dict(one=None, two='mse')  # 'one' loss is None, training should not affect weights of 'one'

model.compile(loss=losses, optimizer='adam')

x = tf.data.Dataset.from_tensor_slices([0., 1.])
y = tf.data.Dataset.from_tensor_slices([1., 2.])
yy = tf.data.Dataset.zip(dict(one=y, two=y))  # targets on both heads, but ONLY 'two' has loss
dataset = tf.data.Dataset.zip((x, yy)).batch(1).repeat()

model.fit(dataset, steps_per_epoch=1000, epochs=10)  # should NOT fit 'one' weights, only 'two' weights

# printout shows that, unexpectedly, 'one' weights WERE fitted, 'two' weights WERE NOT fitted
for var in model.trainable_weights: 
    print(f'{var.name}: {var.numpy()}')
&lt;/denchmark-code&gt;

The incorrect behavior in this example depends on the lexicographic ordering of loss names.  The fit trains on an incorrect loss when a None loss precedes non-None loss.  This is the case when 'one' is None and 'one' &lt; 'two'.  Replacing 'one' with 'z_one', so that 'z_one' &gt; 'two', results in a correct training of 'two', since now the None loss is the last loss.
		</comment>
		<comment id='7' author='tomwphillips' date='2020-02-08T22:25:53Z'>
		&lt;denchmark-link:https://github.com/mmilosav&gt;@mmilosav&lt;/denchmark-link&gt;
 you are right.
		</comment>
		<comment id='8' author='tomwphillips' date='2020-02-08T22:27:08Z'>
		 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/36044&gt;#36044&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='tomwphillips' date='2020-04-14T08:47:25Z'>
		&lt;denchmark-link:https://github.com/tomwphillips&gt;@tomwphillips&lt;/denchmark-link&gt;
 Closing this issue as it was the expected behavior. Please feel free to re-open the issue if you have any concerns still. Thanks!
		</comment>
		<comment id='10' author='tomwphillips' date='2020-04-14T08:47:27Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35267&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35267&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>