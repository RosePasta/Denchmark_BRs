<bug id='29535' author='Nitinsiwach' open_date='2019-06-07T14:41:59Z' closed_time='2019-07-08T17:54:26Z'>
	<summary>model.trainable=False does nothing in tensorflow keras</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):source
TensorFlow version (use command below):1.13
Python version:2.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:10.1.105
GPU model and memory:m60,8gb

It seems setting model.trainable=False in tensorflow keras does nothing except for to print a wrong model.summary(). Here is the code to reproduce the issue:
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np
IMG_SHAPE = (160, 160, 3)

# Create the base model from the pre-trained model MobileNet V2
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False, 
                                               weights='imagenet')
base_model.trainable = False
# for layer in base_model.layers:
#     layer.trainable=False
bc=[] #before compile
ac=[] #after compile
for layer in base_model.layers:
    bc.append(layer.trainable)
print(np.all(bc)) #True
print(base_model.summary()) ##this changes to show no trainable parameters but that  is wrong given the output to previous np.all(bc)
base_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001), 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])
for layer in base_model.layers:
    ac.append(layer.trainable)
print(np.all(ac)) #True
print(base_model.summary()) #this changes to show no trainable parameters but that  is wrong given the output to previous np.all(ac)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Nitinsiwach' date='2019-06-07T19:03:58Z'>
		tf.keras.models.Model documentation does not mention this attribute.
&gt;&gt;&gt; hasattr(base_model, 'trainable')
True
tf.keras.models.Model inherits attribute from Network.
Network inherits the attribute from Layer
Attribute seems to be defined here:



tensorflow/tensorflow/python/keras/engine/network.py


        Lines 194 to 195
      in
      9699b90






 # This acts just like the `trainable` attribute of any layer instance. 



 self._trainable = kwargs.get('trainable', True) 





and here:



tensorflow/tensorflow/python/keras/engine/base_layer.py


        Lines 164 to 166
      in
      c572ea2






 # Indicates whether the layer's weights are updated during training 



 # and whether the layer's updates are run during training. 



 self._trainable = trainable 





		</comment>
		<comment id='2' author='Nitinsiwach' date='2019-06-07T19:14:14Z'>
		Setting the trainable attribute to False affects the models' trainable_weights attribute.
		</comment>
		<comment id='3' author='Nitinsiwach' date='2019-06-07T19:31:37Z'>
		


tensorflow/tensorflow/python/keras/engine/base_layer.py


        Lines 766 to 772
      in
      c572ea2






 @property 



 def trainable_weights(self): 



 if self.trainable: 



 nested = self._gather_children_attribute('trainable_weights') 



 return self._trainable_weights + nested 



 else: 



 return [] 





This Property seems to be the cause
		</comment>
		<comment id='4' author='Nitinsiwach' date='2019-06-07T19:35:57Z'>
		When trainable is set to false, the trainable_weights property resolve to empty list. This causes the layer summary to display the parameter count to display incorrectly.



tensorflow/tensorflow/python/keras/utils/layer_utils.py


        Lines 234 to 239
      in
      9699b90






 trainable_count = count_params(model.trainable_weights) 



 



 non_trainable_count = count_params(model.non_trainable_weights) 



 



 print_fn('Total params: {:,}'.format(trainable_count + non_trainable_count)) 



 print_fn('Trainable params: {:,}'.format(trainable_count)) 





		</comment>
		<comment id='5' author='Nitinsiwach' date='2019-06-07T20:31:41Z'>
		setting  is the suggested way in even the Tensorflow documentation for transfer learning &lt;denchmark-link:https://www.tensorflow.org/tutorials/images/transfer_learning&gt;https://www.tensorflow.org/tutorials/images/transfer_learning&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='Nitinsiwach' date='2019-06-07T22:04:11Z'>
		
When trainable is set to false, the trainable_weights property resolve to empty list. This causes the layer summary to display the parameter count to display incorrectly.
It is unclear to me how trainable_weights being set to an empty list makes layer summary to display the parameter count incorrectly. Can you explain a bit more please?



tensorflow/tensorflow/python/keras/utils/layer_utils.py


        Lines 234 to 239
      in
      9699b90






 trainable_count = count_params(model.trainable_weights) 



 



 non_trainable_count = count_params(model.non_trainable_weights) 



 



 print_fn('Total params: {:,}'.format(trainable_count + non_trainable_count)) 



 print_fn('Trainable params: {:,}'.format(trainable_count)) 






		</comment>
		<comment id='7' author='Nitinsiwach' date='2019-06-07T22:33:25Z'>
		I think the problem is actually that the individual layers in the model stay trainable when the model is set to be non trainable.
&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False, 
                                               weights='imagenet')
&gt;&gt;&gt; base_model.trainable = False
&gt;&gt;&gt; base_model.layers[0].trainable
True
		</comment>
		<comment id='8' author='Nitinsiwach' date='2019-06-07T23:02:28Z'>
		i mentioned that in the code bit i posted as minimal reproducible example. All layers remain trainable both before and after compiling the model
		</comment>
		<comment id='9' author='Nitinsiwach' date='2019-06-11T06:28:05Z'>
		Tried on Colab with TF 1.13.1 and was able to get mentioned model.summary with trainable params as 0.
		</comment>
		<comment id='10' author='Nitinsiwach' date='2019-06-11T09:16:30Z'>
		I can confirm that all three model.trainable=False, layer.trainable=False for layer in model.layers and model.trainable=True result in different behavior
		</comment>
		<comment id='11' author='Nitinsiwach' date='2019-06-13T07:29:36Z'>
		


tensorflow/tensorflow/python/keras/engine/base_layer.py


        Lines 750 to 754
      in
      c572ea2






 @trainable.setter 



 def trainable(self, value): 



 self._trainable = value 



 for layer in getattr(self, '_layers', []): 



 layer.trainable = value 





This observer pattern does not seem to be functioning as expected.
		</comment>
		<comment id='12' author='Nitinsiwach' date='2019-06-26T07:26:50Z'>
		Any update on this?
		</comment>
		<comment id='13' author='Nitinsiwach' date='2019-06-26T16:08:02Z'>
		This is fixed in 1.14, can you update the version and try it again?
		</comment>
		<comment id='14' author='Nitinsiwach' date='2019-06-26T19:55:32Z'>
		&lt;denchmark-link:https://github.com/Nitinsiwach&gt;@Nitinsiwach&lt;/denchmark-link&gt;
 Please check a gist with 1.14 &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/936cbc0184d3ae96bdc682f34a32d9a2/tf29535_model_tranables.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='15' author='Nitinsiwach' date='2019-07-08T17:54:26Z'>
		Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!
		</comment>
		<comment id='16' author='Nitinsiwach' date='2019-07-08T17:54:27Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29535&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29535&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='17' author='Nitinsiwach' date='2019-09-29T15:21:17Z'>
		Encountered this error in tensorflow 1.14 from a colab gpu runtime with the following code:
`import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (Dense, Activation, BatchNormalization,
Reshape, Conv2D, LeakyReLU, Input)
from tensorflow.keras.optimizers import Adam
print(tf.version)
network = Sequential([
Conv2D(64, 4, strides=(2, 2), padding='same', input_shape=(28,28,1)),
LeakyReLU(),
Conv2D(128, 4, strides=(2, 2), padding='same'),
BatchNormalization(axis=-1),
LeakyReLU(),
Reshape((77128,)),
Dense(1024),
BatchNormalization(axis=-1),
LeakyReLU(),
Dense(1),
Activation('sigmoid')
])
network.compile(loss='binary_crossentropy', optimizer=Adam(2e-4, beta_1=0.5))
network.summary()
network.trainable = False
network.summary()`
		</comment>
		<comment id='18' author='Nitinsiwach' date='2019-09-29T23:43:52Z'>
		&lt;denchmark-link:https://github.com/Aladoro&gt;@Aladoro&lt;/denchmark-link&gt;
 Please create a new issue by providing your platform details and the standalone code. Thanks!
		</comment>
		<comment id='19' author='Nitinsiwach' date='2019-10-05T21:58:38Z'>
		&lt;denchmark-code&gt;from keras.models import Model
from keras.applications.vgg16 import VGG16

model = VGG16(weights='imagenet', include_top=True, input_shape=(224, 224, 3))
model.trainable = False

model.summary()

model = Model(inputs=model.input, outputs=model.output)

model.summary()
&lt;/denchmark-code&gt;

This first summary shows there is no trainable weights. Wheras second summary shows all weights are trainable.
&lt;denchmark-code&gt;for layer in model.layers:
        layer.trainable = False
&lt;/denchmark-code&gt;

If doing it this way, both summaries show no trainable weights.
		</comment>
		<comment id='20' author='Nitinsiwach' date='2019-10-07T16:45:28Z'>
		&lt;denchmark-link:https://github.com/don-tpanic&gt;@don-tpanic&lt;/denchmark-link&gt;
 Please create a new issue with more details on the issue and also provide platform details, version used etc. Thanks!
		</comment>
		<comment id='21' author='Nitinsiwach' date='2019-10-16T19:16:22Z'>
		
from keras.models import Model
from keras.applications.vgg16 import VGG16

model = VGG16(weights='imagenet', include_top=True, input_shape=(224, 224, 3))
model.trainable = False

model.summary()

model = Model(inputs=model.input, outputs=model.output)

model.summary()

This first summary shows there is no trainable weights. Wheras second summary shows all weights are trainable.
for layer in model.layers:
        layer.trainable = False

If doing it this way, both summaries show no trainable weights.

yes this is because confusingly, the behavior of model.trainable is different in keras vs tf.keras: in keras it only impacts the variable of the model layer, without impacting the variable of all the sub layers, contrary to what happens in tf.keras. (Is it expected to have such different behavior between keras and tf.keras??)
In the second summary, you effectively creates a new model which set model.trainable = True by default, which explains the different behavior.
I find setting the trainable flag of the whole model very confusing, so I now only set it per sub layers. For example in tf.keras, if you do model.trainable = False and then model.layers[-4].trainable = True, the latter instruction has no effect as it does not change the trainable flag at the top of the model.
		</comment>
		<comment id='22' author='Nitinsiwach' date='2020-07-10T11:29:45Z'>
		&lt;denchmark-link:https://github.com/babaish&gt;@babaish&lt;/denchmark-link&gt;
: Thank you, good point, useful to know!
		</comment>
	</comments>
</bug>