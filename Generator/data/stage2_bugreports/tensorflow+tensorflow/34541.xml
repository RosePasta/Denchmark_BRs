<bug id='34541' author='zwenju' open_date='2019-11-23T08:00:08Z' closed_time='2020-01-08T05:35:21Z'>
	<summary>RNN with cell  get_initial_state  and state_size incompatible</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
TensorFlow version (use command below): TF2.0
Python version: py3.74
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
ValueError: An initial_state was passed that is not compatible with cell.state_size. Received state_spec=ListWrapper([InputSpec(shape=(None, 100, 1), ndim=3)]); however cell.state_size is [100]
Describe the expected behavior
Code to reproduce the issue
&lt;denchmark-code&gt;# coding:utf-8
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
import tensorflow.keras.layers as layers
import numpy as np
from tensorflow.keras.utils import plot_model

from tensorflow.keras.utils import plot_model
from tensorflow.keras import layers
tf.keras.backend.set_floatx('float64')
#%%###################################################################
# y(n) = func_g(x)y(n-1) + X\beta + \alpha
###############################################################
def func_g (x):
    v = np.sin(x)
    return v
########################################################
#%% deine the neutron for X\beta + alpha
############################################
class Linear(layers.Layer):

  def __init__(self, CityNum, CityFactorNum):   
    self.CityNum = CityNum    
    self.CityFactorNum = CityFactorNum
    super(Linear, self).__init__()
    
  def build(self, input_shape):
    self.beta  = self.add_weight(shape=(self.CityFactorNum, 1),  initializer='random_normal', trainable=True)
    self.alpha = self.add_weight(shape=(self.CityNum,),        initializer='random_normal', trainable=True)

  def call(self, X):
    v = tf.matmul( X[:,1:], self.beta)  + self.alpha[ tf.dtypes.cast( X[0,0], tf.int32 ) ]
    return v
#############################################33
#%% define rnn cell node
###############################################
class MinimalRNNCell(tf.keras.layers.Layer):
    def __init__(self, units, CityNum, CityFactorNum, **kwargs):
        self.units = units
        self.state_size = 100
        self.CityNum = CityNum    
        self.CityFactorNum = CityFactorNum
        super(MinimalRNNCell, self).__init__(**kwargs)

    def build(self, input_shape):
        
        self.beta  = self.add_weight(shape=(self.CityNum,),  initializer='random_normal', trainable=True)
        self.alpha = self.add_weight(shape=(self.CityFactorNum-1,), initializer='random_normal', trainable=True)
        
        self.dense_1  = layers.Dense(32, activation='tanh')
        self.dense_2  = layers.Dense(64, activation='tanh')
        self.dense_3  = layers.Dense(64, activation='tanh')
        self.dense_4  = layers.Dense(64, activation='tanh')
        self.dense_5= layers.Dense(1)

        
        self.Xbeta_Add_alpha = Linear(self.CityNum, self.CityFactorNum)
                
        self.built = True
    
    def get_initial_state(self, inputs, batch_size=None, dtype=None):
        initial_states = []
        initial_states = [tf.ones(1)]

        return tuple(initial_states)
        

    def call(self, inputs, states):
 
        X_input = inputs[0]
        U_input = inputs[1]
     
        s1 = states[0] 

        gU = self.dense_1(U_input)
        gU = self.dense_2(gU)
        gU = self.dense_3(gU)
        gU = self.dense_4(gU)
        gU = self.dense_5(gU)
        X = self.Xbeta_Add_alpha(U_input)

        gUZ  = layers.dot( [gU, s1], axes=1, name = 'dot')
        gUZX = layers.add( [gUZ, X], name = 'add')
    
        output = [gUZ, gUZX]
        new_state = [gUZX]
        
        return output, new_state

#########################################
#%% define model 
sample_num = 2000
time_step = 100
CityNum, CityFactorNum = 100, 1
UFactorNum = 1
#############################################
def rnn_model (CityNum, CityFactorNum, time_step):
    
    input_X = tf.keras.Input(shape=[time_step, CityFactorNum])
    input_U = tf.keras.Input(shape=[time_step, UFactorNum])
    
    
    
    cells = MinimalRNNCell(1, CityNum, CityFactorNum)
    
    #rnn = tf.keras.layers.RNN(cells, return_sequences=True)(input)
    #out = tf.keras.layers.Dense(units=1)(rnn)

    out = tf.keras.layers.RNN(cells, return_sequences=True)([input_X, input_U])
    out = tf.keras.layers.Dense(1)(out)
    
    
    
    model = tf.keras.Model(inputs=[input_X, input_U], outputs=out)
    plot_model(model, to_file='model.png')
    
    model.summary()
    model.compile(optimizer='rmsprop',  loss=['mse'],  metrics=['mse'])
    
    return model 

################################################################
#%% set the test data
##(sample_num,  time_step_num, units)
######################################################################
sample_num = 2000
time_step = 100
CityNum, CityFactorNum = 100, 1

X = np.random.uniform(-10,10, size=(sample_num,time_step,1))
Y = np.zeros((sample_num,time_step,1))

#for i1 in range()


for i1 in range(sample_num):
    for i2 in range(1,time_step):
        for i3 in range(1):
            Y[i1, i2, i3] = func_g(X[i1, i2, i3]) +  Y[i1, i2-1, i3]

#-----------------------------------------------------------------------
model = rnn_model (CityNum, CityFactorNum, time_step)
model.fit(X, Y, batch_size = 100, epochs = 50)
#----------------------------------------------------------------------
start = np.random.uniform(-10,10, size=(1,time_step,1))
start = np.linspace(-10,10,time_step)
start = np.reshape(start, (1,time_step,1))
next = model.predict(start)


plt.plot(start[0,:,0], next[0,:,0],'rs')
################################################################
#%% true solution 
##############################################################
next = next
for i1 in range(1):
    for i2 in range(1,time_step):
        for i3 in range(1):
            next[i1, i2, i3] = func_g(start[i1, i2, i3]) + next[i1, i2-1, i3]

plt.plot(start[0,:,0], next[0,:,0],'bo')

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='zwenju' date='2019-11-24T16:19:25Z'>
		it seems that the error occurs  with the following  (([input_X, input_U]))
out = tf.keras.layers.RNN(cells, return_sequences=True)([input_X, input_U])
		</comment>
		<comment id='2' author='zwenju' date='2019-11-25T05:31:25Z'>
		I have tried on colab with TF version 2.0 ,2.1.0-dev20191124 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/e17b1d46e3672796971d566ba519b171/untitled400.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='3' author='zwenju' date='2020-01-08T05:35:20Z'>
		The issue happens because you override the method of get_initial_state(), which returns a state that has a wrong shape. Usually the state tensor should have shape (batch, state_size) with all zeros. If you need to provide custom values (one in this case), you should return tf.ones((batch_size, state_size)), rather than tf.ones(1).
		</comment>
		<comment id='4' author='zwenju' date='2020-01-08T05:35:22Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34541&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34541&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='zwenju' date='2020-06-30T05:33:13Z'>
		&lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
 hiï¼Œi want train a ctc model, and i use some code like
`

def rnn_layer(self, inputs, states=None):

          gru = tf.keras.layers.GRUCell(1024)



         if states is None:



             states = gru.get_initial_state(inputs=inputs)



             layers = tf.keras.layers



             inner, last_state = tf.keras.layers.RNN(



                  cell=gru,



                  return_sequences=True,



                  return_state=True



            )(inputs=[inputs], initial_state=[states])



`
when i run , and i get error" assert initial_state is None and constants is None",  hope some advice, thxs
		</comment>
		<comment id='6' author='zwenju' date='2020-06-30T17:30:21Z'>
		Please unwrap the [inputs] since rnn layer only expect one input, and it shouldn't be a list.
		</comment>
	</comments>
</bug>