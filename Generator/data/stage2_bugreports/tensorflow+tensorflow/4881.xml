<bug id='4881' author='chenghuige' open_date='2016-10-10T20:33:07Z' closed_time='2017-06-16T17:45:12Z'>
	<summary>Multi gpu cifa10 example, putting ops outside of towerloss to cpu actually hurt performance</summary>
	<description>
From cifa 10 example tutorial  in train.py  "with tf.Graph().as_default(), tf.device('/cpu:0'):" putting all the ops but in tower loss to cpu.
as in the tutorial &lt;denchmark-link:https://www.tensorflow.org/versions/r0.11/tutorials/deep_cnn/index.html#cifar-10-model&gt;cifa 10 tutorial&lt;/denchmark-link&gt;

"This setup requires that all GPUs share the model parameters. A well-known fact is that transferring data to and from GPUs is quite slow. For this reason, we decide to store and update all model parameters on the CPU (see green box). A fresh set of model parameters is transferred to the GPU when a new batch of data is processed by all GPUs."
However I find in my rnn application(tower loss 4GPU), without setting  tf.device('/cpu:0'): actually is faster.
Setting   tf.device('/cpu:0'):
about 44s per 100 step, take 9min43s to finish the first 1000 steps
2016-10-10 17:37:53 epoch:0.1664 train_step:100 duration:0.474 elapsed:56.103 train_avg_metrics:['loss:0.496']  ['loss:0.486']
2016-10-10 17:38:38 epoch:0.3328 train_step:200 duration:0.520 elapsed:45.064 train_avg_metrics:['loss:0.464']  ['loss:0.448']
2016-10-10 17:39:25 epoch:0.4992 train_step:300 duration:0.408 elapsed:46.612 train_avg_metrics:['loss:0.383']  ['loss:0.238']
2016-10-10 17:40:10 epoch:0.6656 train_step:400 duration:0.471 elapsed:44.819 train_avg_metrics:['loss:0.308']  ['loss:0.317']
2016-10-10 17:40:54 epoch:0.8319 train_step:500 duration:0.431 elapsed:44.716 train_avg_metrics:['loss:0.269']  ['loss:0.245']
2016-10-10 17:41:39 epoch:0.9983 train_step:600 duration:0.418 elapsed:44.256 train_avg_metrics:['loss:0.242']  ['loss:0.234']
2016-10-10 17:42:35 epoch:1.1647 train_step:700 duration:0.424 elapsed:56.733 train_avg_metrics:['loss:0.214']  ['loss:0.168']
2016-10-10 17:43:19 epoch:1.3311 train_step:800 duration:0.402 elapsed:43.769 train_avg_metrics:['loss:0.209']  ['loss:0.213']
2016-10-10 17:44:01 epoch:1.4975 train_step:900 duration:0.475 elapsed:41.747 train_avg_metrics:['loss:0.201']  ['loss:0.193']
2016-10-10 17:44:43 epoch:1.6639 train_step:1000 duration:0.423 elapsed:42.351 train_avg_metrics:['loss:0.190']  ['loss:0.238']
2016-10-10 17:44:43 0:08:14 epoch:1.6639 eval_step: 1000 train_avg_loss:['loss:0.298']
Not setting tf.device('/cpu:0'):
about 35s per 100 step, take 6min48s to finsish the first 1000 steps
2016-10-11 04:24:43 epoch:0.1664 train_step:100 duration:0.371 elapsed:47.976 train_avg_metrics:['loss:0.496']  ['loss:0.485']
2016-10-11 04:25:19 epoch:0.3328 train_step:200 duration:0.385 elapsed:36.630 train_avg_metrics:['loss:0.470']  ['loss:0.408']
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 1680058 get requests, put_count=1680141 evicted_count=3000 eviction_rate=0.00178556 and unsatisfied allocation rate=0.00195172
2016-10-11 04:25:56 epoch:0.4992 train_step:300 duration:0.319 elapsed:37.185 train_avg_metrics:['loss:0.386']  ['loss:0.336']
2016-10-11 04:26:32 epoch:0.6656 train_step:400 duration:0.411 elapsed:35.670 train_avg_metrics:['loss:0.320']  ['loss:0.303']
2016-10-11 04:27:07 epoch:0.8319 train_step:500 duration:0.360 elapsed:35.204 train_avg_metrics:['loss:0.268']  ['loss:0.272']
2016-10-11 04:27:43 epoch:0.9983 train_step:600 duration:0.333 elapsed:35.526 train_avg_metrics:['loss:0.248']  ['loss:0.167']
2016-10-11 04:28:31 epoch:1.1647 train_step:700 duration:0.378 elapsed:48.393 train_avg_metrics:['loss:0.218']  ['loss:0.170']
2016-10-11 04:29:06 epoch:1.3311 train_step:800 duration:0.334 elapsed:34.472 train_avg_metrics:['loss:0.208']  ['loss:0.264']
2016-10-11 04:29:38 epoch:1.4975 train_step:900 duration:0.322 elapsed:32.613 train_avg_metrics:['loss:0.199']  ['loss:0.198']
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 7824854 get requests, put_count=7824942 evicted_count=13000 eviction_rate=0.00166135 and unsatisfied allocation rate=0.00169639
2016-10-11 04:30:11 epoch:1.6639 train_step:1000 duration:0.320 elapsed:32.889 train_avg_metrics:['loss:0.199']  ['loss:0.219']
2016-10-11 04:30:11 0:06:48 epoch:1.6639 eval_step: 1000 train_avg_loss:['loss:0.301']
	</description>
	<comments>
		<comment id='1' author='chenghuige' date='2016-10-10T20:35:47Z'>
		I've noticed same thing on cirrascale GPU machines - putting parameters on gpu:0 and using gpu-&gt;gpu transfer was a bit faster. I suppose this depends on particular details of hardware -- if you don't have p2p connectivity between your video cards then keeping parameters on CPU:0 gives faster training.
		</comment>
		<comment id='2' author='chenghuige' date='2016-10-10T20:38:54Z'>
		&lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
  I see, thanks for your quick reply. I'm not sure p2p connetctivity, testing on 4 K40 gpu cards.
Another thing can you help me look at this ?
I find average_gradients(tower_grads) costing lots of time.
&lt;denchmark-link:http://stackoverflow.com/questions/39952878/tensorflow-multi-gpu-much-slower-than-single-gpu-if-embeddingvocabulary-is-la&gt;average gradients problem&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='chenghuige' date='2016-10-10T20:42:50Z'>
		It's printed when you start TensorFlow. So if you have 8 GPU's that can all talk to each other, you'll see something like this in your console.
&lt;denchmark-code&gt;I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 2 3 4 5 6 7 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y Y Y Y Y Y Y Y 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   Y Y Y Y Y Y Y Y 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 2:   Y Y Y Y Y Y Y Y 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 3:   Y Y Y Y Y Y Y Y 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 4:   Y Y Y Y Y Y Y Y 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 5:   Y Y Y Y Y Y Y Y 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 6:   Y Y Y Y Y Y Y Y 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 7:   Y Y Y Y Y Y Y Y 
&lt;/denchmark-code&gt;

I'm not sure this would qualify as a bug, a person who doesn't have fancy motherboard where GPU-&gt;GPU transfers have to go through QPI link might still benefit from pinning parameters on CPU:0, such optimizations are hardware dependent
		</comment>
		<comment id='4' author='chenghuige' date='2016-10-10T20:44:27Z'>
		&lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;

I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 2 3
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y Y N N
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   Y Y N N
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 2:   N N Y Y
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 3:   N N Y Y
here is mine, so this tells why in my exp, 3 gpu performance is similar to 2 gpu, not improve much?
May be not as a bug, but we might put this info to the tutorial.
		</comment>
		<comment id='5' author='chenghuige' date='2016-10-21T09:38:26Z'>
		&lt;denchmark-link:https://github.com/chenghuige&gt;@chenghuige&lt;/denchmark-link&gt;
 : If you'd like to send a pull request to update the tutorial with this observation, we'd be glad to take it
		</comment>
		<comment id='6' author='chenghuige' date='2017-06-16T17:45:12Z'>
		This issue is quite old and hasn't had recent activity. Please check the recently published performance guide and benchmarked example models.
		</comment>
	</comments>
</bug>