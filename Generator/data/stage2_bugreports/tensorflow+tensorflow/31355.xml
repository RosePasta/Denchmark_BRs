<bug id='31355' author='MKFMIKU' open_date='2019-08-06T03:36:04Z' closed_time='2020-07-31T07:18:54Z'>
	<summary>Custom OPS with registering gradient functions error in eager backprop</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): PIP
TensorFlow version (use command below): 2.0.0-beta1
Python version: 3.6.9
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: V10.0.130 / V7.0.5
GPU model and memory: NVIDIA TITAN Xp

Describe the current behavior
If I run the code with directly
&lt;denchmark-code&gt;2019-08-06 11:32:33.687311: E tensorflow/stream_executor/cuda/cuda_driver.cc:1003] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2019-08-06 11:32:33.687356: E tensorflow/stream_executor/gpu/gpu_timer.cc:78] Invalid argument: error recording CUDA event on stream 0x7fce8a062590: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2019-08-06 11:32:33.687558: F ./tensorflow/core/kernels/reduction_gpu_kernels.cu.h:644] Non-OK-status: CudaLaunchKernel( ColumnReduceKernel&lt;IN_T, T*, Op&gt;, grid_dim, block_dim, 0, cu_stream, in, (T*)temp_storage.flat&lt;int8_t&gt;().data(), extent_x, extent_y, op, init) status: Internal: an illegal memory access was encountered
Aborted (core dumped)
&lt;/denchmark-code&gt;

If I run the code with cuda-memcheck --binary-patching no python train_simple.py
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "train_simple.py", line 71, in &lt;module&gt;
    grads = tape.gradient(loss, model.trainable_variables)
  File "/media/disk1/fordata/web_server/kangfu/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py", line 1002, in gradient
    unconnected_gradients=unconnected_gradients)
  File "/media/disk1/fordata/web_server/kangfu/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py", line 76, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File "/media/disk1/fordata/web_server/kangfu/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py", line 137, in _gradient_function
    return grad_fn(mock_op, *out_grads)
  File "/media/disk4/fordata/web_server/kangfu/HDRNet-TF/hdrnet_ops.py", line 24, in _bilateral_slice_grad
    grid_tensor, guide_tensor, input_tensor, grad, has_offset=has_offset)
  File "&lt;string&gt;", line 355, in bilateral_slice_apply_grad
  File "&lt;string&gt;", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: Failed launch BilateralSliceApplyGradKernel. [Op:BilateralSliceApplyGrad]
&lt;/denchmark-code&gt;

Describe the expected behavior
The code should calculate gradient properly without error. The custom ops run correctly in inferring mode, however, it throws bug during model.fit or below:
&lt;denchmark-code&gt;with tf.GradientTape() as tape:
    output = model(_full_res)
    loss = tf.keras.losses.mse(output, _full_res)
grads = tape.gradient(loss, model.trainable_variables)
&lt;/denchmark-code&gt;

Code to reproduce the issue
I define the custom ops like below:
&lt;denchmark-code&gt;REGISTER_KERNEL_BUILDER(Name("BilateralSlice").Device(DEVICE_GPU), BilateralSliceOp);
REGISTER_KERNEL_BUILDER(Name("BilateralSliceGrad").Device(DEVICE_GPU), BilateralSliceGradOp);
REGISTER_KERNEL_BUILDER(Name("BilateralSliceApply").Device(DEVICE_GPU), BilateralSliceApplyOp);
REGISTER_KERNEL_BUILDER(Name("BilateralSliceApplyGrad").Device(DEVICE_GPU), BilateralSliceApplyGradOp);
&lt;/denchmark-code&gt;

It seems like the custom ops not work OK in eager mode
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='MKFMIKU' date='2019-08-07T11:57:55Z'>
		In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!
		</comment>
		<comment id='2' author='MKFMIKU' date='2019-08-08T09:53:20Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
 I commit the code used to reproduce the error in &lt;denchmark-link:https://github.com/MKFMIKU/Tensorflow-Bug-Reproduce&gt;https://github.com/MKFMIKU/Tensorflow-Bug-Reproduce&lt;/denchmark-link&gt;
.
To get the error, you can:
git clone https://github.com/MKFMIKU/Tensorflow-Bug-Reproduce.git
cd Tensorflow-Bug-Reproduce
make
python run.py
then it outputs:
&lt;denchmark-code&gt;2019-08-08 17:50:50.689623: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-08-08 17:50:51.605504: E tensorflow/stream_executor/cuda/cuda_driver.cc:1003] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2019-08-08 17:50:51.605560: E tensorflow/stream_executor/gpu/gpu_timer.cc:78] Invalid argument: error recording CUDA event on stream 0x7fe3505d0550: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2019-08-08 17:50:51.605661: F ./tensorflow/core/kernels/reduction_gpu_kernels.cu.h:605] Non-OK-status: CudaLaunchKernel(ColumnReduceMax16ColumnsKernel&lt;IN_T, T*, Op&gt;, grid_dim, block_dim, 0, cu_stream, in, (T*)temp_storage.flat&lt;int8_t&gt;().data(), extent_x, extent_y, op, init) status: Internal: an illegal memory access was encountered
Aborted (core dumped)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='MKFMIKU' date='2019-08-09T05:30:46Z'>
		&lt;denchmark-link:https://github.com/MKFMIKU&gt;@MKFMIKU&lt;/denchmark-link&gt;

I am trying to reproduce the issue with your instructions.
I am able to run the below two commands:
git clone &lt;denchmark-link:https://github.com/MKFMIKU/Tensorflow-Bug-Reproduce.git&gt;https://github.com/MKFMIKU/Tensorflow-Bug-Reproduce.git&lt;/denchmark-link&gt;

cd Tensorflow-Bug-Reproduce
but make command is throwing some error
&lt;denchmark-link:https://user-images.githubusercontent.com/51902062/62756296-cbe1a680-ba94-11e9-89e4-d83ab28fb658.png&gt;&lt;/denchmark-link&gt;

Thanks!
		</comment>
		<comment id='4' author='MKFMIKU' date='2019-08-09T05:44:52Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
 Seems like you don't have  in your environment. The  should be installed before run .
		</comment>
		<comment id='5' author='MKFMIKU' date='2020-07-31T05:02:00Z'>
		&lt;denchmark-link:https://github.com/MKFMIKU&gt;@MKFMIKU&lt;/denchmark-link&gt;
,
Is this still an issue? Could you please upgrade to TensorFlow v2.3 and check if you are still facing the error?

git clone https://github.com/MKFMIKU/Tensorflow-Bug-Reproduce.git
cd Tensorflow-Bug-Reproduce
make

Also while trying to reproduce the issue, on running the  step I am facing an error stating . Please find the gist of it &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/cf1d67a0b1595ce316946c04e93aa6d0/31355.ipynb#scrollTo=4iyuE954ERri&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='6' author='MKFMIKU' date='2020-07-31T07:18:54Z'>
		It is solved after upgrade
		</comment>
		<comment id='7' author='MKFMIKU' date='2020-07-31T07:18:56Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31355&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31355&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>