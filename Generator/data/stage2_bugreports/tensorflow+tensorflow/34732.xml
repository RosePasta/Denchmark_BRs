<bug id='34732' author='hoangcuong2011' open_date='2019-12-01T09:30:22Z' closed_time='2020-02-28T17:53:43Z'>
	<summary>Bug with embedding layer for position of sequence using tf.cumsum</summary>
	<description>
Greetings,
I wanted to create an embedding layer for token position along with an embedding layer for token
in the sequence.
The first way I did was that I create a pos_ids input tensor, and feed an numpy array of position into the input. I then use embedding layer for the input as usual:
word_ids = Input(dtype='int32', batch_shape=(batch_size, max_seq_length), name='word_ids') pos_ids = Input(dtype='int32', batch_shape=(batch_size, max_seq_length), name='pos_ids') pos_embedding_layer = tensorflow.keras.layers.Embedding(input_dim=(max_seq_length+1),  output_dim=word_embedding_size,  input_length=max_seq_length, mask_zero=True, trainable=True) embedding_layer = tensorflow.keras.layers.Embedding(input_dim=(len(d)+1),  output_dim=word_embedding_size,  input_length=max_seq_length, mask_zero=True, trainable=True) pos_ids_embeddings = pos_embedding_layer(pos_ids)
With this I noticed the network is as usual, with parameters (18432) for the embedding layer. Here is what I get when I print the model summary.
&lt;denchmark-link:https://user-images.githubusercontent.com/8759715/69912311-129b4800-145a-11ea-98c2-b9947e0c6bb9.png&gt;&lt;/denchmark-link&gt;

However, I wanted to use tf.cumsum as another way to create the index of position. By doing that I can avoid having pos_ids as the another Input. My code is as follows, which I believe it is correct.
word_ids = Input(dtype='int32', batch_shape=(batch_size, max_seq_length), name='word_ids') pos_embedding_layer = tensorflow.keras.layers.Embedding(input_dim=(max_seq_length+1),  output_dim=word_embedding_size,  input_length=max_seq_length, mask_zero=True, trainable=True) embedding_layer = tensorflow.keras.layers.Embedding(input_dim=(len(d)+1),  output_dim=word_embedding_size,  input_length=max_seq_length, mask_zero=True, trainable=True) pos_tensor = tf.cumsum(tf.ones_like(word_ids, 'int32'), axis=-1) pos_ids_embeddings = pos_embedding_layer(pos_tensor)
However, with this, I noticed pos_embedding_layer does not have any param when I print the model summary (model.summary()). The layer also does not connect to anything in the network.
&lt;denchmark-link:https://user-images.githubusercontent.com/8759715/69912147-cb13bc80-1457-11ea-8ca5-833821041199.png&gt;&lt;/denchmark-link&gt;

I guess this is a bug, and please correct me if it is the case. Also, how to fix this problem?
Thx.
	</description>
	<comments>
		<comment id='1' author='hoangcuong2011' date='2019-12-02T08:06:31Z'>
		&lt;denchmark-link:https://github.com/hoangcuong2011&gt;@hoangcuong2011&lt;/denchmark-link&gt;
, Please provide the complete standalone code to reproduce the reported issue and also include the Tensorflow version. Thanks!
		</comment>
		<comment id='2' author='hoangcuong2011' date='2019-12-02T09:03:46Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
: Thx. Trying to create a minimal version of this, I found 2 versions as below.
version 1 - no parameters
version 2 - having parameters.
The only difference between them is the way I declare Input ( batch_shape=(32, max_seq_length), or shape(max_seq_length,). Why is this the case?
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np
def main():
    max_seq_length = 35
    learning_rate = 2e-4
    word_embedding_size = 512
    def compile_new_model():
        optimizer = tf.keras.optimizers.Adam(lr = learning_rate, clipnorm=1.0)
        word_ids = tf.keras.layers.Input(dtype='int32', 
            batch_shape=(32, max_seq_length),
            name='word_ids')

        pos_embedding_layer = tf.keras.layers.Embedding(input_dim=(max_seq_length+1), 
            output_dim=word_embedding_size, 
            input_length=max_seq_length, trainable=True)
        pos_idss = tf.cumsum(tf.ones_like(word_ids, 'int32'), axis=-1)
        pos_ids_embeddings = pos_embedding_layer(pos_idss)
        model = tf.keras.models.Model(inputs=[word_ids], outputs=pos_ids_embeddings)
        model.compile(
            optimizer,
            loss=tf.keras.losses.sparse_categorical_crossentropy)
        return model
model = compile_new_model()
print(model.summary())
&lt;/denchmark-code&gt;

-&gt; output:
&lt;denchmark-link:https://user-images.githubusercontent.com/38926056/69945429-168f9e80-151c-11ea-861f-b518641af2bd.png&gt;&lt;/denchmark-link&gt;

Version 2:
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np
def main():
    max_seq_length = 35
    learning_rate = 2e-4
    word_embedding_size = 512
    def compile_new_model():
        optimizer = tf.keras.optimizers.Adam(lr = learning_rate, clipnorm=1.0)
        word_ids = tf.keras.layers.Input(dtype='int32', 
            shape=(max_seq_length,), 
            name='word_ids')

        pos_embedding_layer = tf.keras.layers.Embedding(input_dim=(max_seq_length+1), 
            output_dim=word_embedding_size, 
            input_length=max_seq_length, trainable=True)
        pos_idss = tf.cumsum(tf.ones_like(word_ids, 'int32'), axis=-1)
        pos_ids_embeddings = pos_embedding_layer(pos_idss)
        model = tf.keras.models.Model(inputs=[word_ids], outputs=pos_ids_embeddings)
        model.compile(
            optimizer,
            loss=tf.keras.losses.sparse_categorical_crossentropy)
        return model
model = compile_new_model()
print(model.summary())
&lt;/denchmark-code&gt;

-&gt; output:
&lt;denchmark-link:https://user-images.githubusercontent.com/38926056/69945474-27d8ab00-151c-11ea-8c7e-2ded24a0f323.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='hoangcuong2011' date='2019-12-03T08:36:49Z'>
		I could replicate the issue with Tensorflow 1.15 on colab.
Please find the colab gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/ee879e7ae5672c214ae3c674d0728b9a/untitled289.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='4' author='hoangcuong2011' date='2020-01-08T05:43:13Z'>
		Assigning to Tom who worked on tf ops layer. Also, please note that we are not going to fix anything in 1.x since there won't be any new releases. We will still fix the issue if the same error happens in 2.x as well.
		</comment>
		<comment id='5' author='hoangcuong2011' date='2020-02-28T17:53:42Z'>
		&lt;denchmark-link:https://github.com/hoangcuong2011&gt;@hoangcuong2011&lt;/denchmark-link&gt;
 Thanks for the issue!
Tested this and it is fixed in the latest tf-nightly
pip install -U tf-nightly
		</comment>
		<comment id='6' author='hoangcuong2011' date='2020-02-28T17:53:44Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34732&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34732&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>