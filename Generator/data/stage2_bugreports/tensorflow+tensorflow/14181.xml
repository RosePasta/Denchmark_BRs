<bug id='14181' author='JulianStier' open_date='2017-11-02T11:11:28Z' closed_time='2018-01-26T01:19:36Z'>
	<summary>Tensorflow or python having memory cleanup issues when using multiple models in iterative loop</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code: yes
OS Platform and Distribution: Linux Ubuntu 17.04
TensorFlow installed from (source or binary): binary
TensorFlow version: v1.3.0-rc2-20-g0787eee 1.3.0
Python version: Python 3.6.1 :: Anaconda 4.4.0 (64-bit)
CUDA/cuDNN version: none
GPU model and memory: none

== cat /etc/issue ===============================================
Linux Bragi 4.10.0-37-generic &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/41&gt;#41&lt;/denchmark-link&gt;
-Ubuntu SMP Fri Oct 6 20:20:37 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION="17.04 (Zesty Zapus)"
VERSION_ID="17.04"
VERSION_CODENAME=zesty
== are we in docker =============================================
No
== compiler =====================================================
c++ (Ubuntu 6.3.0-12ubuntu2) 6.3.0 20170406
Copyright (C) 2016 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
== uname -a =====================================================
Linux Bragi 4.10.0-37-generic &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/41&gt;#41&lt;/denchmark-link&gt;
-Ubuntu SMP Fri Oct 6 20:20:37 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
== check pips ===================================================
numpy (1.12.1)
numpydoc (0.6.0)
protobuf (3.4.0)
tensorflow (1.3.0)
tensorflow-tensorboard (0.1.8)
== check for virtualenv =========================================
False
== tensorflow import ============================================
tf.VERSION = 1.3.0
tf.GIT_VERSION = v1.3.0-rc2-20-g0787eee
tf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee
Sanity check: array([1], dtype=int32)
== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset
== nvidia-smi ===================================================
./tf_env_collect.sh: line 105: nvidia-smi: command not found
&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

I am working on a tensorflow model which takes pretty much RAM. It is executed iteratively to process given tasks.
However, with increasing time the whole process starts consuming more and more RAM although it should clean it up. This sounds like as if I'd keep data of one graph over the iterations, but I am almost sure that the graphs are cleanly separated.
&lt;denchmark-h:h2&gt;Problem&lt;/denchmark-h&gt;

I reduced the code to the following:
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np

reps = 30
for i in range(reps):
    with tf.Graph().as_default() as graph:
        with tf.Session(graph=graph) as sess:
            tf.constant(np.random.random((1000,1000,200,1)))
&lt;/denchmark-code&gt;

I have 32GB RAM available, working on a ubuntu 17.04 with CPU Tensorflow 1.3. This will give following error message after about the 25th or 27th iteration:

terminate called after throwing an instance of 'std::bad_alloc'
what():  std::bad_alloc

Giving the process some time after each iteration results in no improvement:
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np
import time

reps = 30
for i in range(reps):
    with tf.Graph().as_default() as graph:
        with tf.Session(graph=graph) as sess:
            tf.constant(np.random.random((1000,1000,200,1)))
    time.sleep(1)
&lt;/denchmark-code&gt;

However, it works if I force garbage collection invocation after each repetition:
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np
import gc

reps = 30
for i in range(reps):
    with tf.Graph().as_default() as graph:
        with tf.Session(graph=graph) as sess:
            tf.constant(np.random.random((1000,1000,200,1)))
    gc.collect()
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Question&lt;/denchmark-h&gt;

Now I wonder why I need to force garbage collection to run even though tensorflow should have closed the session and de-referenced the graph object.
Back to my original model I am not sure, yet, if the gc invocation actually helps. The memory usage grows pretty intense, especially when I am about to persist the model to disk.
Thanks for any insights.
	</description>
	<comments>
		<comment id='1' author='JulianStier' date='2017-11-03T06:48:40Z'>
		First of all, thanks for filling in the the issue template with all the details and providing clear instructions to reproduce the problem. This is very helpful!
I modified your snippet a bit to reduce iteration time and dump out memory stats inline (so avoid generating a new random numpy array on each iteration and I see issues even without creating a session):
import tensorflow as tf
import numpy as np
import resource

reps = 30
x = np.random.random((1000, 1000, 200, 1))
for i in range(reps):
  with tf.Graph().as_default() as graph:
    tf.constant(x)
  print('Iteration ', i, ' maxrss: ', resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)
The snippet above does show maxrss constantly increasing, while if I add the gc as you suggested:
import tensorflow as tf
import numpy as np
import resource
import gc

reps = 30
x = np.random.random((1000, 1000, 200, 1))
for i in range(reps):
  with tf.Graph().as_default() as graph:
    tf.constant(x)
  print('Iteration ', i, ' maxrss: ', resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)
  gc.collect()
Then maxrss remains stable.
Thanks for the report.
&lt;denchmark-link:https://github.com/allenlavoie&gt;@allenlavoie&lt;/denchmark-link&gt;
 has been looking at some other garbage generation and might have some thoughts.
&lt;denchmark-link:https://github.com/skye&gt;@skye&lt;/denchmark-link&gt;
 might also know a bit about graph construction code.
&lt;denchmark-link:https://github.com/allenlavoie&gt;@allenlavoie&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/skye&gt;@skye&lt;/denchmark-link&gt;
 : Mind taking a look? Thanks!
		</comment>
		<comment id='2' author='JulianStier' date='2017-11-03T16:50:45Z'>
		As long as we have reference cycles we're at the mercy of Python's garbage collection strategy. We could add some explicit gc.collect() calls in places (when popping the graph stack?), but for graph mode in TF 1.x I think that's the best we can do.
graph.get_operations()[0].graph is one definite reference cycle. I don't think this is something we can solve with weakref for graph mode while maintaining API compatibility; we're pretty explicit about holding on to an operation being enough to execute it (needs the graph) and holding on to the graph being enough to execute it (needs the operation).
So eager mode may help, where we will (eventually) try to avoid most reference cycles and so will hopefully not need the garbage collector much. But there are definitely reference cycles there too at the moment.
		</comment>
		<comment id='3' author='JulianStier' date='2017-12-20T19:20:20Z'>
		It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='4' author='JulianStier' date='2018-01-04T19:10:15Z'>
		It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='5' author='JulianStier' date='2018-01-23T23:16:39Z'>
		A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.
		</comment>
		<comment id='6' author='JulianStier' date='2018-01-26T00:34:17Z'>
		Assigning to &lt;denchmark-link:https://github.com/allenlavoie&gt;@allenlavoie&lt;/denchmark-link&gt;
, feel free to re-assign.
		</comment>
		<comment id='7' author='JulianStier' date='2018-01-26T01:19:36Z'>
		I don't think this bug has a reasonable solution, so I'm going to close.
I don't want to run gc.collect() manually. We make Graphs for Defuns (and therefore Dataset map_funcs) and every time a ResourceVariable is created. It requires a sweep through all objects and could slow down graph building significantly.
It is &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/8a87518ae7074d5a0da779089e7024cd0920bba4/tensorflow/python/ops/resource_variable_ops.py#L77&gt;possible to allow a graph's memory to be freed&lt;/denchmark-link&gt;
 without running the garbage collector, but it's sufficiently niche that I don't think it warrants a new "manual graph delete" API (but if someone wants to factor that into python.util as a non-public-API utility I'm happy to review a pull request).
Otherwise eager execution allows continually redefining what gets executed without creating work for the garbage collector.
		</comment>
		<comment id='8' author='JulianStier' date='2019-03-04T09:04:35Z'>
		I ran into this issue when I tried to run a bunch of experiments. So I tried calling gc.collect() manually with sess.close() and printing out resource.getrusage(resource.RUSAGE_SELF).ru_maxrss it just keeps increasing...
		</comment>
	</comments>
</bug>