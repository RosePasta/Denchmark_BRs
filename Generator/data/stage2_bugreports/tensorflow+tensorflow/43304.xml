<bug id='43304' author='HaraBeron' open_date='2020-09-17T16:59:37Z' closed_time='2020-09-18T08:22:17Z'>
	<summary>Model and layers set to non-trainable, but weights still adjusted</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom Code
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
TensorFlow installed from (source or binary): Google Colab
TensorFlow version (use command below): Google Colab 2.3.0
Python version: 3.6.9 (default, Jul 17 2020, 12:50:27) \n[GCC 8.4.0]
GPU model and memory: Google Colab GPU

I want to freeze a model. That means, set it to non-trainable. I do so by setting model.trainable and the individual layers to non-trainable. However, when I then fit this model (with just 1 epoch) I can see afterwards that the model weights changed. I can see this by checking the model.evaluate output and by comparing the model weights with print(model.trainable_variables) prior and afterwards the model fitting.
Standalone code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow_datasets as tfds
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

(train_x, train_labels), (test_x, test_labels) = tf.keras.datasets.imdb.load_data(num_words=10000)

x_train_padded = pad_sequences(train_x, maxlen=500)
x_test_padded = pad_sequences(test_x, maxlen=500)

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(10000, 128, input_length=500),
    tf.keras.layers.Conv1D(128, 5, activation='relu'),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1)
])

model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),optimizer='adam', metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')])

history = model.fit(x=x_train_padded,
                      y=train_labels,
                      validation_data=(x_test_padded , test_labels),
                      epochs=4, batch_size=128)
&lt;/denchmark-code&gt;

This gives the output:

Epoch 4/4
196/196 [==============================] - 9s 48ms/step - loss: 0.1202 - accuracy: 0.9578 - val_loss: 0.3663 - val_accuracy: 0.8669

I can confirm this with:
model.evaluate(x_test_padded , test_labels)

782/782 [==============================] - 3s 4ms/step - loss: 0.3663 - accuracy: 0.8669
[0.36633849143981934, 0.866919994354248]

Now I set the model and individual layers to non-trainable:
&lt;denchmark-code&gt;model.trainable=False

for layer in model.layers:
  layer.trainable=False
&lt;/denchmark-code&gt;

I check with model.summary() that

Trainable params: 0

And fit the model again:
&lt;denchmark-code&gt;history = model.fit(x=x_train_padded,
                      y=train_labels,
                      validation_data=(x_test_padded , test_labels),
                      epochs=1, batch_size=128)
&lt;/denchmark-code&gt;

This gives:

196/196 [==============================] - 10s 52ms/step - loss: 0.0911 - accuracy: 0.9677 - val_loss: 0.4298 - val_accuracy: 0.8648

I can again check this with
model.evaluate(x_test_padded , test_labels)

782/782 [==============================] - 4s 5ms/step - loss: 0.4298 - accuracy: 0.8648
[0.4298333525657654, 0.8648399710655212]

Which clearly shows that the model was adjusted. The val_loss and val_accuracy is not equal to loss: 0.3663 - accuracy: 0.8669. Moreover, when I check the model weights with
print(model.trainable_variables) before and afterwards I can see that the model weights got adjusted. However, actually all parameters should be set to non-trainable.
	</description>
	<comments>
		<comment id='1' author='HaraBeron' date='2020-09-17T19:24:21Z'>
		It is not a bug you need to model.compile again after you set to False
		</comment>
		<comment id='2' author='HaraBeron' date='2020-09-18T08:22:19Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43304&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43304&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>