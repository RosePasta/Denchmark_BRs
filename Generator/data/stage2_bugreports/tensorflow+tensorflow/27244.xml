<bug id='27244' author='liumilan' open_date='2019-03-28T12:52:26Z' closed_time='2019-06-14T17:46:26Z'>
	<summary>TensorFlow estimator train_and_evaluate loss is None model does not train when reading data from s3</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):


OS Platform and Distribution (e.g., Linux Ubuntu 16.04):


Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:


TensorFlow installed from (source or binary):
Anaconda


TensorFlow version (use command below):
1.12


Python version:
Python 3.6.8 :: Anaconda, Inc.


Bazel version (if compiling from source):


GCC/Compiler version (if compiling from source):


CUDA/cuDNN version:


GPU model and memory:


You can collect some of this information using our environment capture &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
Describe the current behavior
i have build tensorflow distribution code using estimator api.And it worked well when reading data from local using train_and_evaluate.But if data is located in s3,it just checkpoints at step 0, the loss being None and moves to the evaluate phase. I'm not sure why this is happening, and any suggestions about what to look for would be great
Describe the expected behavior
it should training normally when reading data from s3
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
&lt;denchmark-code&gt;train_input = lambda: fetch_train_data(FLAGS.train_data, FLAGS.train_epoch,batch_size=FLAGS.train_batch_size,is_eval=False,shuffle=True)
    eval_input  = lambda: fetch_eval_data (FLAGS.eval_data,  FLAGS.eval_epoch, batch_size=FLAGS.eval_batch_size, is_eval=True, shuffle=True)

    train_spec = tf.estimator.TrainSpec(train_input,max_steps=9223372036854775807)
    eval_spec = tf.estimator.EvalSpec(eval_input,steps=1)
    tf.estimator.train_and_evaluate(model, train_spec, eval_spec)
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;def fetch_train_data(filename, epochs=1, batch_size=256,is_eval=False,shuffle=False):
    print ("fetch_train_data");
    blocks = get_blocks()
    #files = __list_files(filename)

    def chunks(arr, m):
      n = int(math.floor(len(arr) / float(m)))
      return [arr[i:i + n] for i in range(0, len(arr), n)]
    print ("is_eval is %d"%(is_eval))
    data_file = tf_record_pattern(filename, is_eval)
    #random.shuffle(data_file)
    num_shards = FLAGS.gpu_nums
    if FLAGS.gpu_nums!=0 and not is_eval:
        data_file = chunks(data_file, num_shards)

    for file in data_file:
        print (file)

    sys.stdout.flush()
    ds = [[] for i in range(num_shards)]
    feature_shards = [[] for i in range(num_shards)]
    label_shards = [[] for i in range(num_shards)]
    for i in range(num_shards):
      if not is_eval:
        ds[i] = tf.data.TFRecordDataset.list_files(data_file[i])
        ds[i] = ds[i].interleave(lambda filename: tf.data.TFRecordDataset(filename, buffer_size = 20*1024*1024), cycle_length = 20)
      else:
        ds[i] = tf.data.TFRecordDataset(data_file[i], buffer_size = 20*1024*1024)
      ds[i] = ds[i].prefetch(buffer_size=batch_size)
      if shuffle:
          ds[i] = ds[i].shuffle(buffer_size = 10000)
      if FLAGS.sync == 0:
        ds[i] = ds[i].repeat(epochs)
      else:
        ds[i] = ds[i].repeat()

      ds[i] = ds[i].batch(batch_size).map(__parse_ins(blocks), num_parallel_calls = 20)
      ds[i] = ds[i].prefetch(FLAGS.gpu_nums)
      feature_shards[i], label_shards[i] = ds[i].make_one_shot_iterator().get_next()
    return feature_shards, label_shards

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='liumilan' date='2019-04-03T09:14:20Z'>
		who can help to fix it?
		</comment>
		<comment id='2' author='liumilan' date='2019-04-08T17:44:59Z'>
		&lt;denchmark-link:https://github.com/yongtang&gt;@yongtang&lt;/denchmark-link&gt;
 Could you please take a look at this issue? Thanks!
		</comment>
		<comment id='3' author='liumilan' date='2019-06-08T17:19:12Z'>
		I have a similar situation. Unable to train an image classification model while streaming image data from S3.
		</comment>
		<comment id='4' author='liumilan' date='2019-06-09T00:25:13Z'>
		This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at &lt;denchmark-link:https://stackoverflow.com/questions/tagged/tensorflow&gt;Stackoverflow&lt;/denchmark-link&gt;
. There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!
		</comment>
		<comment id='5' author='liumilan' date='2019-06-14T17:46:26Z'>
		Please follow the suggestion from jvishnuvardhan@. Will close this.
		</comment>
		<comment id='6' author='liumilan' date='2019-06-14T17:46:28Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27244&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27244&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>