<bug id='34221' author='xgzeng' open_date='2019-11-13T11:10:51Z' closed_time='2020-03-12T23:28:42Z'>
	<summary>Got different result if train Keras model eagerly</summary>
	<description>
Problem
I got unexpected result while trying to train a simple Keras model in eagerly mode (so I can debug).  The problem is reproducible on local machine and Colab.
Describe the current behavior
Same keras model trained in eager mode, but got different result compared to non-eager mode.
The model didn't convergent.
Describe the expected behavior
Result should not depend on eagerly mode On or Off.

&lt;denchmark-link:https://colab.research.google.com/drive/1kzCj9vzrOuUnl90eo8pRoJvoeNwqVPP_&gt;Colab notebook that reproduce problem &lt;/denchmark-link&gt;

&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow_datasets as tfds

## model don't work if uncomment following line
# tf.config.experimental_run_functions_eagerly(True)

TRAIN_DATASET = tfds.load(name="cifar10")['train']

model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(32,32, 3)),
    tf.keras.layers.Dense(10, activation='softmax'),
])

BATCH_SIZE = 50

model.compile(loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])

train_set = TRAIN_DATASET.map(lambda item: (item['image'], item['label'])).batch(BATCH_SIZE)
model.fit(train_set, epochs = 5)
&lt;/denchmark-code&gt;

Other info / logs
Correct Output:
&lt;denchmark-code&gt;Epoch 1/5
1000/1000 [==============================] - 19s 19ms/step - loss: 330.7991 - accuracy: 0.1935
Epoch 2/5
1000/1000 [==============================] - 15s 15ms/step - loss: 299.7854 - accuracy: 0.2237
Epoch 3/5
1000/1000 [==============================] - 15s 15ms/step - loss: 292.7084 - accuracy: 0.2317
&lt;/denchmark-code&gt;

Output in eagerly mode
&lt;denchmark-code&gt;Epoch 1/5
1000/1000 [==============================] - 30s 30ms/step - loss: 14.5070 - accuracy: 0.0999
Epoch 2/5
1000/1000 [==============================] - 28s 28ms/step - loss: 14.5060 - accuracy: 0.1000
Epoch 3/5
1000/1000 [==============================] - 28s 28ms/step - loss: 14.5060 - accuracy: 0.1000
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='xgzeng' date='2019-11-14T05:55:53Z'>
		&lt;denchmark-link:https://github.com/xgzeng&gt;@xgzeng&lt;/denchmark-link&gt;

Can you try running the code in latest - version? Issue seemed to be fixed, kindly find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/c69588901a7fb90c7e0c22f61ff74eb4/untitled367.ipynb&gt;gist &lt;/denchmark-link&gt;
for the same.Thanks!
		</comment>
		<comment id='2' author='xgzeng' date='2019-11-14T10:51:35Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
  Thanks!
But tf-nightly 2.1.0.dev20191111 don't work. I tried on local machine and &lt;denchmark-link:https://colab.research.google.com/drive/1kzCj9vzrOuUnl90eo8pRoJvoeNwqVPP_&gt;colab&lt;/denchmark-link&gt;
.
The issue is:
When tf.config.experimental_run_functions_eagerly(False) , we can get accuracy around 0.23.
When tf.config.experimental_run_functions_eagerly(True) , accuracy stuck around 0.1.
		</comment>
		<comment id='3' author='xgzeng' date='2019-11-15T19:28:01Z'>
		&lt;denchmark-link:https://github.com/xgzeng&gt;@xgzeng&lt;/denchmark-link&gt;
 Thanks for reporting this issue. I could reproduce the issue with . However, when I ran &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/cbb0b6512150bf612441fb84aa9ded0f/untitled649.ipynb&gt;your code&lt;/denchmark-link&gt;
 with  (eager mode is default in TF2.x) without , then the results matches well with the code where  is enabled (Graph mode).
I think this issue may be more related to  than . The  is defined &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/eager/def_function.py#L254-L271&gt;here&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 adding you as it may be of interest to you. Thanks!
		</comment>
		<comment id='4' author='xgzeng' date='2019-11-15T19:50:04Z'>
		This looks like a bug in the keras training loop. &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 am I correct in reading your comment that tf-nightly shows no difference in behavior? If so then it's likely this bug has already been fixed.
		</comment>
		<comment id='5' author='xgzeng' date='2019-11-15T20:44:17Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 The discrepancy in the results appear only when we use  to run the model Eagerly.
If we don't use tf.config.experimental_run_functions_eagerly(True), then the model runs in Eager mode and there is no discrepancy in the results.
Based on docs, tf.config.experimental_run_functions_eagerly(True) is used for debugging the code the code. Thanks!
		</comment>
		<comment id='6' author='xgzeng' date='2019-11-15T20:51:55Z'>
		Ah so there is indeed a bug in the eager version of the keras training loop.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Fri, Nov 15, 2019 at 12:50 PM Vishnuvardhan Janapati &lt; ***@***.***&gt; wrote:
 @alextp &lt;https://github.com/alextp&gt; The discrepancy in the results appear
 only when we use tf.config.experimental_run_functions_eagerly(True) to
 run the model Eagerly.

 If we don't use tf.config.experimental_run_functions_eagerly(True), then
 the model runs in Eager mode and there is no discrepancy in the results.

 Based on docs, tf.config.experimental_run_functions_eagerly(True) is used
 for debugging the code the code. Thanks!

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#34221?email_source=notifications&amp;email_token=AAABHRNMAELE3U4QQJS3CDTQT4DS5A5CNFSM4JMZJABKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEEGVF4Y#issuecomment-554521331&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AAABHRKMOZJCPUKTLDIU74DQT4DS5ANCNFSM4JMZJABA&gt;
 .


-- 
 - Alex

		</comment>
		<comment id='7' author='xgzeng' date='2019-11-16T02:31:09Z'>
		The lost during training is totally different for two modes. Maybe the model is not evaluated in same way.
		</comment>
		<comment id='8' author='xgzeng' date='2019-11-16T03:21:27Z'>
		I wrote some &lt;denchmark-link:https://colab.research.google.com/drive/1d5d5XpeQ7fW0b7K34HntkD57o4jKLLZ_&gt;code&lt;/denchmark-link&gt;
 to calculate model lost for a batch of data without any training loop.
The result is similar to training result. Model lost value is totally different in eager/graph mode.
So I think the problem is not in training loop.
		</comment>
		<comment id='9' author='xgzeng' date='2019-11-18T06:09:01Z'>
		&lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
  I though the problem lies in  loss function.
Following code in sparse_categorical_crossentropy function ('keras/backend.py' file), cause different behavior in eager/graph mode.
&lt;denchmark-code&gt;  if not from_logits:
    if (isinstance(output, (ops.EagerTensor, variables_module.Variable)) or
        output.op.type != 'Softmax'):
      epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)
      output = clip_ops.clip_by_value(output, epsilon_, 1 - epsilon_)
      output = math_ops.log(output)
    else:
      # When softmax activation function is used for output operation, we
      # use logits from the softmax function directly to compute loss in order
      # to prevent collapsing zero when training.
      # See b/117284466
      assert len(output.op.inputs) == 1
      output = output.op.inputs[0]
&lt;/denchmark-code&gt;

If we rewrite model code to skip the optimization, the problem is gone.
&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow_datasets as tfds

TRAIN_DATASET = tfds.load(name="cifar10")['train']

tf.config.experimental_run_functions_eagerly(True) # result won't be affected by eager/graph mode

model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(32,32, 3)),
    tf.keras.layers.Dense(10), # no softmax in the layer
])

BATCH_SIZE = 50

model.compile(loss = tf.losses.SparseCategoricalCrossentropy(from_logits = True), # do softmax in loss function
              metrics=['accuracy'])

train_set = TRAIN_DATASET.map(lambda item: (item['image'], item['label'])).batch(BATCH_SIZE)
model.fit(train_set, epochs = 5)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='xgzeng' date='2020-03-09T18:08:46Z'>
		Hi,
There are a few things going on here, that combine top make this happen:
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;



crossentropy(y,softmax(x)) will overflow and return NANs easily.


The root of the difference is that, to help beginners, Keras tries to patching this to use the safe "from_logits=True" form when it is running in graph mode (inside a tf.function). If it's not in graph mode is cannot patch the calculation and falls back to including an epsilon to prevent the calculation from overflowing.


We recommend that people people never use activation=softmax of activation=sigmoid for classification outputs, To avoid exactly this problem.


Often the difference is tiny, as it's only extremely badly classified examples that can trigger this behavior, like assigning a probability of 1e-7 to the true class. And usually only shows up  later in training (or in models with huge numbers of class outputs) because a well initialized classification model should be uncertain about the classifications, and to trigger this you need to be sure of the wrong answer.
A well initialized model should have an initial crossentropy error around log(num_classes) or in this case: log(10) == 2.3.
Your error starts at ~300. The only way it's possible is if the model is too sure of the wrong answer from the start. it's assigning a probability of ~0 to the true class in many cases. It is very sure of the wrong answer. Why?


Usually the default layer initializations are fine. But these typically assume the inputs have a small mean and variance, typically like mean=0, var=1 or a uniform distribution on [-1,1] or [0,1]. With small inputs they return small outputs.
You're passing the image data without normalizing it. This has an input range of [0, 255].
So after running the model the inputs to the softmax function, instead of being similar mean=0, var=1,  they're huge  mean=0, std=150.  And the softmax of that is very certain of a random class.


&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

For your case, to fix this:

Don't include softmax in the oputput layer.
Normalize your input data.

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

I'm not sure what we can do to fix this without breaking a lot of existing code.
Having the keras losses print some warnings to explain what they're doing and why might be a good start.
		</comment>
		<comment id='11' author='xgzeng' date='2020-03-12T23:28:41Z'>
		
For your case, to fix this:
Don't include softmax in the oputput layer.

Actually, what I meant to say was:  "nobody should ever use softmax in the output layer during training. ever.".
But other than that advice, everything is working as intended, so I'm closing this.
		</comment>
		<comment id='12' author='xgzeng' date='2020-03-12T23:28:43Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34221&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34221&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>