<bug id='44417' author='jnd77' open_date='2020-10-29T02:17:52Z' closed_time='2020-11-16T01:34:59Z'>
	<summary>Random error with conv_grad_filter_ops</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): TF 2.2
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 11.0.194 / 8.0.1
GPU model and memory: GeForce RTX 2070 SUPER

Describe the current behavior
We have a repo, with a few hundreds "unit" tests.
During the CI, in 5-10% of the runs, one of the largest tests (always the same one) is failing with the following log (repeated many times):
tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at conv_grad_filter_ops.cc:1101 : Not found: No algorithm worked!
The test suite is a mix of eager execution and graph mode tests.
This particular failing test is building a network built with CNNs (Resnet backbone, then object detection), and it's using the estimator API. Its only exotic feature is the use of RaggedTensors.
Another test is very similar (but no RaggedTensor) and has no issue.
Both these tests are running towards the end of the suite, so after a lot of models/variables have been instantiated and deleted.
Describe the expected behavior
A more informative error message.
Standalone code to reproduce the issue
Unfortunately, this error can be reproduced only randomly and with the whole repo (even changing the order of the tests can have an impact).
I'm aware this ticket is not super helpful, but I hope someone will have an idea about what's happening ...
Other info / logs
Stack trace looks like:
&lt;denchmark-code&gt;[2020-10-27T07:40:30.393Z] tensorflow.python.framework.errors_impl.NotFoundError: No algorithm worked!
[2020-10-27T07:40:30.393Z] 	 [[node SGD/gradients/gradients/.../Conv2D_grad/Conv2DBackpropFilter (defined at ...) ]]
[2020-10-27T07:40:30.393Z] 
[2020-10-27T07:40:30.393Z] Errors may have originated from an input operation.
[2020-10-27T07:40:30.393Z] Input Source operations connected to node SGD/gradients/gradients/.../Conv2D_grad/Conv2DBackpropFilter:
[2020-10-27T07:40:30.393Z]  .../BiasAdd (defined at ...)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='jnd77' date='2020-10-30T10:50:41Z'>
		&lt;denchmark-link:https://github.com/jnd77&gt;@jnd77&lt;/denchmark-link&gt;

Please provide with minimal reproducible code or a colab gist with the error reported.
		</comment>
		<comment id='2' author='jnd77' date='2020-10-31T05:30:06Z'>
		&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 Thanks for looking at it.
As I mention in the ticket, it's impossible for me to reproduce the error with some minimal code. It's happening randomly even with a stable code base and the same machine.
I'm more hoping on someone being able to suggest what situation could create such error (since the message is not that helpful).
I guess in some cases, CUDA/Cudnn is in some particular state due to the accumulation of past ops (RaggedTensors in particular have changing shapes from one training loop to the next).
If you believe not much can be done to help me, then I will close the ticket.
		</comment>
		<comment id='3' author='jnd77' date='2020-11-02T22:53:36Z'>
		You may want to try this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/42728#issuecomment-682479892&gt;hack&lt;/denchmark-link&gt;
 suggested by the user.
		</comment>
		<comment id='4' author='jnd77' date='2020-11-09T23:19:48Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='5' author='jnd77' date='2020-11-10T01:20:01Z'>
		&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
 Thanks a lot for your message. I will try revert  to an older version of Cudnn and monitor.
		</comment>
		<comment id='6' author='jnd77' date='2020-11-16T01:34:59Z'>
		&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
 I ended up updating Cudnn to 8.0.4 (looks like the first versions 8.0 and 8.0.1 were unstable), and it looks good so far.
Thanks.
		</comment>
		<comment id='7' author='jnd77' date='2020-11-16T01:35:01Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44417&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44417&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='jnd77' date='2020-11-26T00:51:49Z'>
		&lt;denchmark-link:https://github.com/jnd77&gt;@jnd77&lt;/denchmark-link&gt;
 I just posted this workaround on the other linked issue. It may or may not be relevant for you too.
Put this at the start of your code.
&lt;denchmark-code&gt;from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='jnd77' date='2020-11-27T02:12:51Z'>
		Thanks &lt;denchmark-link:https://github.com/maxeonyx&gt;@maxeonyx&lt;/denchmark-link&gt;
 but that wasn't the fix for me.
allow_growth is always true in my setup via the env variable (no choice for RTX cards).
		</comment>
	</comments>
</bug>