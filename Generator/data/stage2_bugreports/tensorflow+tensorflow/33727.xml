<bug id='33727' author='ThinerDAS' open_date='2019-10-25T17:19:13Z' closed_time='2020-04-27T22:00:27Z'>
	<summary>Disabling eager execution while training ~90 layers of BatchNormalization exhausts RAM</summary>
	<description>
Disabling eager execution while training a network with a number of BatchNormalization will take unreasonable amount of RAM before training happens. It might be related that BatchNormalization lacks an _XlaCompiler attribute.
I find that I am in an awkward state: my script is a long-running script, and if I enable eager execution, GPU RAM will be used up due to some unknown memory leakage + my script will run ULTRA SLOWLY due to unbatched nature of queries of model.predict (GPU usage 30% in disabling eager execution mode v.s. 9% in enabling eager execution mode); if I disable eager execution, then this bug takes up my CPU RAM.
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS
TensorFlow installed from (source or binary): docker: tensorflow/tensorflow latest-gpu-py3       f7932d1761bd
TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
Python version: 3.6.8

Describe the current behavior
Writing ~90 layers of BatchNormalization takes around 8G RAM before training when disabling eager mode, even with matrix dimension small. Like 100M per BatchNormalization in python3.6.8, regardless of dimensions.
EDIT: I did some experiment and found that the memory usage is approximately linear to the number of BatchNormalizations.
Describe the expected behavior
Writing ~90 layers of BatchNormalization should take smaller amount of RAM.
Code to reproduce the issue
&lt;denchmark-code&gt;from tensorflow.python.framework.ops import disable_eager_execution
import numpy as np
DISABLE_EAGER = 1
resnet_depth = 96

if DISABLE_EAGER:
    disable_eager_execution()
if True:
    from tensorflow.keras.optimizers import *
    from tensorflow.keras.layers import *
    from tensorflow.keras.models import *


def init():
    # game params
    board_x, board_y = 3, 3
    action_size = 10
    depth_dim = 2

    input_boards = Input(
        shape=(board_x, board_y, depth_dim))
    num_chan = 4
    h_conv1 = Activation('relu')(BatchNormalization(axis=3)(
        Conv2D(num_chan, 1, padding='same', use_bias=False)(input_boards)))
    for i in range(resnet_depth):
        h_conv1 = Activation('relu')(BatchNormalization(axis=3)(
            Conv2D(num_chan, 1, padding='same', use_bias=False)(h_conv1)))
    hf = Flatten()(h_conv1)
    s_fc1 = Dropout(0.3)(Activation('relu')(BatchNormalization(axis=1)(
        Dense(16, use_bias=False)(hf))))
    pi = Dense(action_size, activation='softmax', name='pi')(
        s_fc1)

    model = Model(inputs=input_boards, outputs=pi)
    model.compile(
        loss=['categorical_crossentropy'], optimizer=Adam(0.001))
    return model


m = init()
print('inited model')
m.fit(x=np.zeros((1, 3, 3, 2)), y=np.zeros((1, 10)))

&lt;/denchmark-code&gt;

Other info / logs
None
	</description>
	<comments>
		<comment id='1' author='ThinerDAS' date='2019-10-29T08:35:45Z'>
		When tried running the given code for TF-2.0, it consumed more RM as per &lt;denchmark-link:https://github.com/ThinerDAS&gt;@ThinerDAS&lt;/denchmark-link&gt;
  explanation .
Kindly find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/oanush/2047078b9ef3140b7d38a1f2148b278b/33727.ipynb&gt;gist&lt;/denchmark-link&gt;
 of collab.
		</comment>
		<comment id='2' author='ThinerDAS' date='2020-04-14T21:12:26Z'>
		&lt;denchmark-link:https://github.com/ThinerDAS&gt;@ThinerDAS&lt;/denchmark-link&gt;
 Is this still an issue? I tried with  and I don't see any difference between graph mode and eager mode in RAM usage. Please check the &lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/414870a8731cd8139ad0f9c62786ad69/33727.ipynb&gt;gist here&lt;/denchmark-link&gt;
.
Please verify once and close the issue if this was resolved for you. Thanks!
		</comment>
		<comment id='3' author='ThinerDAS' date='2020-04-21T22:08:31Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='4' author='ThinerDAS' date='2020-04-27T22:00:27Z'>
		I am closing this issue as it was resolved. Please feel free to reopen if the issue persists again. Thanks!
		</comment>
		<comment id='5' author='ThinerDAS' date='2020-04-27T22:00:29Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33727&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33727&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>