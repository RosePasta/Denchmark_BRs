<bug id='8624' author='gaohuazuo' open_date='2017-03-22T13:22:29Z' closed_time='2017-06-16T21:14:14Z'>
	<summary>Constant folding does not work across devices?</summary>
	<description>
I've been trying to understand tensorflow internals recently. I found in , if I understand it correctly, that constant folding only take place at &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/ef56133461079f28b61b5a83a62685051408aadb/tensorflow/core/common_runtime/direct_session.cc#L1051&gt;#L1051&lt;/denchmark-link&gt;
 after graph partitioning, so constants won't propagate through device boundary.
This is also evidenced by a simple experiment:
&lt;denchmark-code&gt;with tf.device('gpu'):
    a = tf.constant(0)
with tf.device('cpu'):
    b = a + 1
    c = b + 1
&lt;/denchmark-code&gt;

Resulting computation time graph is
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/10446514/24199102/62c291ae-0f43-11e7-8689-a595ded2c7ed.png&gt;&lt;/denchmark-link&gt;

Whereas placing all ops on GPU gives a fully shaded graph.
Did I miss something? Or is there any consideration not to run constant folding before partitioning the graph?
	</description>
	<comments>
		<comment id='1' author='gaohuazuo' date='2017-03-22T18:58:18Z'>
		Right, from your example it looks like it should have been folded all the way up to c = 2 on cpu in the ideal case.
If you're confident, that might be a good PR to send. Also, I'm curious to know what happens with XLA enabled.
		</comment>
		<comment id='2' author='gaohuazuo' date='2017-03-25T15:17:50Z'>
		I have tried enabling XLA with both session config and jit_scope. With the session config approach, nothing changed. With jit_scope, all nodes became shaded, but even when a was fed, so it is more likely an incompatibility between XLA and tracing than an indication that constant folding is working.
Global constant folding has its own problem, in that a full graph can only be run using a session, while a partition graph requires just an executor. Therefore, constant folding sounds more suitable as an JIT optimization to me. For example, a session triggers constant folding at the second run, then it identifies constant foldable tensors and add them to fetches, and finally substitute them at the third run.
		</comment>
		<comment id='3' author='gaohuazuo' date='2017-03-25T15:22:17Z'>
		Right. When xla is enabled, all that is jit'ed shows as a single node. So
it may or may not be folded inside.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mar 25, 2017 8:19 AM, "Huazuo Gao" ***@***.***&gt; wrote:
 I have tried enabling XLA with both session config and jit_scope. With the
 session config approach, nothing changed. With jit_scope, all nodes became
 shaded, but even when a was fed, so it is more likely an incompatibility
 between XLA and tracing than an indication that constant folding is working.

 Global constant folding has its own problem, in that a full graph can only
 be run using a session, while a partition graph requires just an executor.
 Therefore, constant folding sounds more suitable as an JIT optimization to
 me. For example, a session triggers constant folding at the second run,
 then it identifies constant foldable tensors and add them to fetches, and
 finally substitute them at the third run.

 —
 You are receiving this because you commented.
 Reply to this email directly, view it on GitHub
 &lt;#8624 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AT_SbbebDW61-pTcM9cp9tgmg3z2UEzlks5rpTBogaJpZM4MlL8n&gt;
 .



		</comment>
		<comment id='4' author='gaohuazuo' date='2017-06-16T21:14:14Z'>
		Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
	</comments>
</bug>