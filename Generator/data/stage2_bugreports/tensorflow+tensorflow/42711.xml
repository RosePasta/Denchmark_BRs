<bug id='42711' author='caffeine-lab' open_date='2020-08-27T16:51:04Z' closed_time='2020-09-15T17:41:15Z'>
	<summary>TPU Pods: Batches not scaling according to the TPU pod size</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
TensorFlow version (use command below):
Python version:
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"

Describe the current behavior
I am using a custom training loop to train a model on TPUs. To iterate over the dataset I'm enumerating over it. I'm not using any steps here.
for One TPU device: TPU v3-8 the total batches found to be 800 per epoch
so for a TPU pod: TPU v3-32 the total batches should be 200 per epoch but still, the epoch completes after 800 batches I don't quite understand why.
attached a snippet of the code
Standalone code to reproduce the issue
&lt;denchmark-code&gt;#Connecting to TPU
tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='node-2')
print('Running on TPU ', tpu.master())

tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)
strategy = tf.distribute.TPUStrategy(tpu)



---------------------------------------------------
BATCH_SIZE = 128 * strategy.num_replicas_in_sync
per_replica_batch_size = BATCH_SIZE // strategy.num_replicas_in_sync

train_dataset = strategy.experimental_distribute_datasets_from_function(
	lambda _: get_training_dataset(per_replica_batch_size))

#the loss_plot array will be reset many times
loss_plot = []

@tf.function
def train_step(iterator):
	def step_fn(inputs):

		images, labels = inputs
		with tf.GradientTape() as tape:
			logits = model(images, training=True)
			loss = tf.keras.losses.sparse_categorical_crossentropy(
			labels, logits, from_logits=True)
			loss = tf.nn.compute_average_loss(loss, global_batch_size=batch_size)
		grads = tape.gradient(loss, model.trainable_variables)
		optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))
		training_loss.update_state(loss * strategy.num_replicas_in_sync)
		training_accuracy.update_state(labels, logits)
		return training_loss, training_accuracy

	per_replica_losses, l_loss = strategy.run(step_fn, args=(iterator,))
	return strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_losses,axis=None),strategy.reduce(tf.distribute.ReduceOp.MEAN, l_loss,axis=None)

for epoch in range(start_epoch, EPOCHS):
	start = time.time()
	total_loss = 0

	for (batch, (img_tensor, target)) in enumerate(train_dataset):
		print(target)
		training_loss, training_accuracy = train_step((img_tensor, target))
		total_loss += t_loss

		if batch % 50 == 0:
			print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy() / BATCH_SIZE), flush=True)
&lt;/denchmark-code&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='caffeine-lab' date='2020-08-28T08:31:31Z'>
		&lt;denchmark-link:https://github.com/caffeine-lab&gt;@caffeine-lab&lt;/denchmark-link&gt;
,
On running the code I am facing an error stating .
In order to expedite the trouble-shooting process, could you please provide the TensorFlow version, the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!
		</comment>
		<comment id='2' author='caffeine-lab' date='2020-08-28T10:44:12Z'>
		&lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;
 It's difficult to share the dataset, But the question is quite straightforward, it doesn’t have anything to do with the dataset, The problematic part of the code also given as a snippet. I hope that's enough?
		</comment>
		<comment id='3' author='caffeine-lab' date='2020-09-01T16:51:33Z'>
		
@amahendrakar It's difficult to share the dataset, But the question is quite straightforward, it doesn’t have anything to do with the dataset, The problematic part of the code also given as a snippet. I hope that's enough?

&lt;denchmark-link:https://github.com/caffeine-lab&gt;@caffeine-lab&lt;/denchmark-link&gt;
,
In this case, could you please provide dummy data with similar shape.
Also, without a minimal reproducible code it would be difficult for us to determine the source of the issue. Thanks!
		</comment>
		<comment id='4' author='caffeine-lab' date='2020-09-08T17:16:25Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='5' author='caffeine-lab' date='2020-09-15T17:41:13Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='6' author='caffeine-lab' date='2020-09-15T17:41:16Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42711&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42711&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>