<bug id='26108' author='worthy7' open_date='2019-02-26T00:56:42Z' closed_time='2019-06-10T16:14:35Z'>
	<summary>2.0 Compile is leaking memory.</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 (2 different machines)
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): gpu 2.0.0-dev20190214, gpu 2.0.0-dev20190224
Python version: 3.6.6
CUDA/cuDNN version: 10
GPU model and memory: 7gb  K5200, 4gb GTX 970

Describe the current behavior
Compile leaks memory.
Describe the expected behavior
Compile clears the memory when overwritten
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
&lt;denchmark-code&gt;import tensorflow as tf

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(512, activation=tf.nn.relu),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])

for i in range(100000):
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
&lt;/denchmark-code&gt;

Memory slowly increases about 20mb at a time.
I discovered this whilst working on a genetic algorithm which generates models which are then compiled and tested, but it fills my memory and I have narrowed it down to this simple code.
	</description>
	<comments>
		<comment id='1' author='worthy7' date='2019-02-26T01:15:32Z'>
		Changing the parameters of compile seems to makes no difference. And the size of the network also doesn't seem to make a difference.
		</comment>
		<comment id='2' author='worthy7' date='2019-04-21T02:29:11Z'>
		Me too... Hope this can get solved soon, for now I have to use an old version
		</comment>
		<comment id='3' author='worthy7' date='2019-05-05T03:48:46Z'>
		Same issue...  And K.clear_session() doesn't work.
		</comment>
		<comment id='4' author='worthy7' date='2019-06-06T21:36:53Z'>
		&lt;denchmark-link:https://github.com/worthy7&gt;@worthy7&lt;/denchmark-link&gt;
 Can you try this &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/6cf3a9ee853a805963e461df92e224bf/oom_tf26108.ipynb&gt;gist&lt;/denchmark-link&gt;
 and let me know if the issue still persists. I don't see any issue if I add  to the end of your code.
&lt;denchmark-code&gt;import tensorflow as tf
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(512, activation=tf.nn.relu),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])

for i in range(100000):
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    tf.keras.backend.clear_session()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='worthy7' date='2019-06-10T02:49:13Z'>
		That seems to work in that notebook on the nightly build.
In my original 2.0a the clear_session() doesn't fix it.
So I assume you fixed a bug somewhere in clear_session() which seems to work.
I think that clear_session shouldn't be necessary really... don't you?
I feed there should be no residue from a previous compile when running another compile on the same model.
		</comment>
		<comment id='6' author='worthy7' date='2019-06-10T16:14:35Z'>
		I'm not able to replicate your 20 mb figure; I see an increase of just under 0.5 mb / compile:
&lt;denchmark-link:https://colab.sandbox.google.com/gist/robieta/9820655313efc01f15bd864110e98a45/sequential_leak_test.ipynb&gt;https://colab.sandbox.google.com/gist/robieta/9820655313efc01f15bd864110e98a45/sequential_leak_test.ipynb&lt;/denchmark-link&gt;

Given that compile does add ops to the graph (gradient ops, metric variables, etc.), I think this probably reasonable since compile is an infrequent call. (And yes, there was a bug with clear_session in the 2.0 alpha. It should be fixed in the beta.) See &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/28844&gt;#28844&lt;/denchmark-link&gt;
 for an explanation of why there is some unnecessarily persisted state for the time being.
		</comment>
		<comment id='7' author='worthy7' date='2019-06-10T16:14:37Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=26108&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=26108&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='worthy7' date='2019-06-11T00:03:17Z'>
		&lt;denchmark-link:https://github.com/robieta&gt;@robieta&lt;/denchmark-link&gt;
 I said "at a time", I didn't mean "per compile" :) I was just watching the task manager window, probably per second.
Thanks for sorting it!
		</comment>
		<comment id='9' author='worthy7' date='2019-07-03T02:50:13Z'>
		
That seems to work in that notebook on the nightly build.
In my original 2.0a the clear_session() doesn't fix it.
So I assume you fixed a bug somewhere in clear_session() which seems to work.
I think that clear_session shouldn't be necessary really... don't you?
I feed there should be no residue from a previous compile when running another compile on the same model.

I agree with you &lt;denchmark-link:https://github.com/worthy7&gt;@worthy7&lt;/denchmark-link&gt;
 . A similar situation, if I train the model in the main thread and load the model in another thread . All things is in a loop.   I test  and  , and  why???  amazing tensorflow!!!  I think that clear_session shouldn't be necessary.
		</comment>
	</comments>
</bug>