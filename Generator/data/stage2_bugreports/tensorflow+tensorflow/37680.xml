<bug id='37680' author='jdlamstein' open_date='2020-03-18T03:03:41Z' closed_time='2020-04-14T19:45:04Z'>
	<summary>Guided Backprop Gradcam with TF 2.0 for Transfer Learning</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information


Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):
Yes


OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04):
Centos 7


Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device:


TensorFlow installed from (source or
binary): - TensorFlow version (use command below):
Tensorflow 2.0 from source


Python version: - Bazel
version (if compiling from source):
Python 3.6


GCC/Compiler version (if compiling from
source):


CUDA/cuDNN version: - GPU model and memory:
Cuda 10.0, Titan V 12gb


You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
I am trying to do gradient visualization using gradcam with guided backpropagation. I can do it on VGG16, but when I add layers to VGG16, I get an error that the layer is not found.
Describe the expected behavior
I want to visualize activation layers using gradcam with guided guided backprop.
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Here's the Colab code with both the working example using just VGG16 and the broken example using VGG16 with added layers.
&lt;denchmark-link:https://colab.research.google.com/drive/1098Hp2icvleIQelkaLmPoIqAKuahs7JH&gt;https://colab.research.google.com/drive/1098Hp2icvleIQelkaLmPoIqAKuahs7JH&lt;/denchmark-link&gt;

I'm attaching an image of a cat which is needed to run the colab.
&lt;denchmark-link:https://user-images.githubusercontent.com/36546688/76920921-ef21cf00-6889-11ea-8465-9b061a8f44c7.jpg&gt;&lt;/denchmark-link&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
Here's the traceback:
&lt;denchmark-code&gt;
Tensor("block5_conv3_6/Identity:0", shape=(None, 14, 14, 512), dtype=float32)
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
&lt;ipython-input-7-511b9293dd65&gt; in &lt;module&gt;()
     60 # Get the score for target class
     61 with tf.GradientTape() as tape:
---&gt; 62     conv_outputs, predictions = grad_model(np.array([img]))
     63     loss = predictions[:, 1]
     64 print('Prediction shape:', predictions.get_shape())

2 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    889           with base_layer_utils.autocast_context_manager(
    890               self._compute_dtype):
--&gt; 891             outputs = self.call(cast_inputs, *args, **kwargs)
    892           self._handle_activity_regularization(inputs, outputs)
    893           self._set_mask_metadata(inputs, outputs, input_masks)

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py in call(self, inputs, training, mask)
    706     return self._run_internal_graph(
    707         inputs, training=training, mask=mask,
--&gt; 708         convert_kwargs_to_constants=base_layer_utils.call_context().saving)
    709 
    710   def compute_output_shape(self, input_shape):

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask, convert_kwargs_to_constants)
    868     output_shapes = []
    869     for x in self.outputs:
--&gt; 870       assert str(id(x)) in tensor_dict, 'Could not compute output ' + str(x)
    871       tensor = tensor_dict[str(id(x))]
    872       output_shapes.append(x.shape)

AssertionError: Could not compute output Tensor("block5_conv3_6/Identity:0", shape=(None, 14, 14, 512), dtype=float32)

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='jdlamstein' date='2020-03-18T04:10:48Z'>
		Can you please share the colab notebook via github? Since permission is getting denied.
Also, one main reason can be because you are passing a Numpy array to the model during graph mode execution. One thing you can do is either pass a Tensor of image, or execute it in Eager mode.(May be removing the @tf.function decorator from the training step?)
		</comment>
		<comment id='2' author='jdlamstein' date='2020-03-18T09:45:09Z'>
		&lt;denchmark-link:https://github.com/jdlamstein&gt;@jdlamstein&lt;/denchmark-link&gt;
,Please provide the access to view the colab notebook. and also check the &lt;denchmark-link:https://github.com/captain-pool&gt;@captain-pool&lt;/denchmark-link&gt;
's workaround and let us know how it progresses. Thanks
		</comment>
		<comment id='3' author='jdlamstein' date='2020-03-18T18:03:48Z'>
		Thanks &lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/captain-pool&gt;@captain-pool&lt;/denchmark-link&gt;
 , I added the sharable link for colab. I tried &lt;denchmark-link:https://github.com/captain-pool&gt;@captain-pool&lt;/denchmark-link&gt;
 's suggestion to change the input from numpy array to tensor, but I got the same error.
		</comment>
		<comment id='4' author='jdlamstein' date='2020-03-19T06:02:44Z'>
		I was able to replicate the issue issue.
Please find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/d05a2511db244ed3e39103ed7c5e049d/untitled468.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='5' author='jdlamstein' date='2020-04-13T21:01:26Z'>
		Hey, I think I found a workaround. If you append layers directly to the base model rather than insert the base model as a layer into your new model, it's easier to grab the layers.
When setting up the model, I had to set the input_tensor when initializing the Model.
&lt;denchmark-code&gt;inp = layers.Input(shape=(imsize[0], imsize[1], imsize[2]))
base_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_tensor=inp,
                                          input_shape=(imsize[0], imsize[1], imsize[2]))
&lt;/denchmark-code&gt;

I set up the vgg16 base model and grabbed the output of the pooling layer from block5.
&lt;denchmark-code&gt;base_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_tensor=inp,
                                          input_shape=(imsize[0], imsize[1], imsize[2]))
block5_pool = base_model.get_layer('block5_pool')
x = global_average_layer(block5_pool.output)
x = fc1(x)
# x = fc2(x)
x = prediction(x)

model = tf.keras.models.Model(inputs = inp, outputs = x)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
&lt;/denchmark-code&gt;

Then I created a new model to output both the prediction and layer I want to visualize.
&lt;denchmark-code&gt;# Create a graph that outputs target convolution and output
grad_model = tf.keras.models.Model(inputs = [model.input], 
                                   outputs=[model.output, model.get_layer(LAYER_NAME).output])

print(model.get_layer(LAYER_NAME).output)
# Get the score for target class

# Get the score for target class
with tf.GradientTape() as tape:
    predictions, conv_outputs = grad_model(img)
    loss = predictions[:, 1]
print('Prediction shape:', predictions.get_shape())
# Extract filters and gradients
output = conv_outputs[0]
grads = tape.gradient(loss, conv_outputs)[0]
&lt;/denchmark-code&gt;

I edited the colab and it worked. I added a fully connected layer, which is untrained so visualization will look at little random, but I did it a few times and it still seems to preserve the gist of it.
&lt;denchmark-link:https://colab.research.google.com/drive/1098Hp2icvleIQelkaLmPoIqAKuahs7JH&gt;https://colab.research.google.com/drive/1098Hp2icvleIQelkaLmPoIqAKuahs7JH&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='jdlamstein' date='2020-04-14T19:45:05Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37680&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37680&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>