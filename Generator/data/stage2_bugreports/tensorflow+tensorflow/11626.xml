<bug id='11626' author='saman-aghazadeh' open_date='2017-07-20T00:07:02Z' closed_time='2019-09-09T16:07:52Z'>
	<summary>SparseSoftmaxCrossEntropyWithLogits error while calculating gradient in c++ mode</summary>
	<description>
I'm trying to use the C++ API to train a CNN model. My last layer is using SparseSoftmaxCrossEntropyWithLogits. Here is the python code which generates the prototxt of the model:
&lt;denchmark-code&gt;import tensorflow as tf 
from tensorflow.python.framework import ops 
from tensorflow.python.framework import dtypes

import random  import numpy as np

NUM_CLASSES = 102  IMAGE_HEIGHT = 224  IMAGE_WIDTH = 224  BATCH_SIZE = 25  NUM_CHANNELS = 3  LEARNING_RATE = 0.0001

with tf.Session() as sess:



images_placeholder = tf.placeholder (tf.float32,
                                                shape=(BATCH_SIZE, IMAGE_HEIGHT,
                                                IMAGE_WIDTH, NUM_CHANNELS), name="input")   
labels_placeholder = tf.placeholder (tf.float32,
                                                shape=(BATCH_SIZE), name="label")

    with tf.name_scope("conv1_1") as scope:         
                 kernel = tf.Variable (tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32, stddev=1e-2),
                                                  name="weights")         
                 conv = tf.nn.conv2d (images_placeholder, kernel, [1, 1, 1, 1], padding='SAME')      
                 biases = tf.Variable (tf.constant(0.0, shape=[64], dtype=tf.float32),
                                                  trainable=True, name='biases')      
                 out = tf.nn.bias_add (conv, biases)         
                 conv1_1 = tf.nn.relu (out, name=scope)

    pool1 = tf.nn.max_pool (conv1_1,
                            ksize=[1, 2, 2, 1],
                            strides=[1, 2, 2, 1],
                            padding='SAME',
                            name='pool1')

    with tf.name_scope('conv2_1') as scope:         
                 kernel = tf.Variable (tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32, stddev=1e-2),
                                                  name='weights')       
                 conv = tf.nn.conv2d (pool1, kernel, [1, 1, 1, 1], padding='SAME')       
                 biases = tf.Variable (tf.constant(0.0, shape=[128], dtype=tf.float32),
                                                  trainable=True, name='biases')      
                 out = tf.nn.bias_add (conv, biases)         
                 conv2_1 = tf.nn.relu (out, name=scope)

    pool2 = tf.nn.max_pool (conv2_1,
                            ksize=[1, 2, 2, 1],
                            strides=[1, 2, 2, 1],
                            padding='SAME',
                            name='pool2')

    with tf.name_scope('conv3_1') as scope:         
                 kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32, stddev=1e-2),
                                                name='weights')       
                 conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')        
                 biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),
                                                 trainable=True, name='biases')      
                 out = tf.nn.bias_add(conv, biases)      
                 conv3_1 = tf.nn.relu(out, name=scope)

    pool3 = tf.nn.max_pool (conv3_1,
                            ksize=[1, 2, 2, 1],
                            strides=[1, 2, 2, 1],
                            padding='SAME',
                            name='pool3')

    with tf.name_scope('conv4_1') as scope:         
                 kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32, stddev=1e-2),
                                                name='weights')       
                conv = tf.nn.conv2d(pool3, kernel, [1, 1, 1, 1], padding='SAME')        
                biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),
                                                trainable=True, name='biases')      
                out = tf.nn.bias_add(conv, biases)      
                conv4_1 = tf.nn.relu(out, name=scope)

    pool4 = tf.nn.max_pool (conv4_1,
                            ksize=[1, 2, 2, 1],
                            strides=[1, 2, 2, 1],
                            padding='SAME',
                            name='pool4')   

    with tf.name_scope('mentee_conv5_1') as scope:      
                 kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32, stddev=1e-2),
                                                name='weights')       
                 conv = tf.nn.conv2d(pool4, kernel, [1, 1, 1, 1], padding='SAME')        
                 biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32), trainable=True,
                                                name='biases')         
                 out = tf.nn.bias_add(conv, biases)      
                 conv5_1 = tf.nn.relu(out, name=scope)

    pool5 = tf.nn.max_pool (conv5_1,
                            ksize=[1, 2, 2, 1],
                            strides=[1, 2, 2, 1],
                            padding='SAME',
                            name='pool5')

    with tf.name_scope('fc1') as scope:         
                 shape = int(np.prod(pool5.get_shape()[1:]))         
                 fc1w = tf.Variable(tf.truncated_normal([shape, 4096], dtype=tf.float32, 
                                             stddev=1e-2), name='weights')       
                 fc1b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),
                                             trainable=True, name='biases')      
                 pool5_flat = tf.reshape(pool5, [-1, shape])         
                 fc1l = tf.nn.bias_add(tf.matmul(pool5_flat, fc1w), fc1b)        
                 fc1 = tf.nn.relu(fc1l)
                 fc1 = tf.nn.dropout(fc1, 0.5)


    labels = tf.to_int64(labels_placeholder)    
    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits (labels=labels,
                        logits=fc1, name="xentropy")    
    loss = tf.reduce_mean (cross_entropy, name='loss')

    optimizer = tf.train.AdamOptimizer (LEARNING_RATE)  
    global_step = tf.Variable (0, name='global_step', trainable=False)  
    train_op = optimizer.minimize (loss, global_step=global_step, name="train")

    init = tf.initialize_variables (tf.all_variables(), name='init_all_vars_op')    
    tf.train.write_graph (sess.graph_def, "models/", "graph.pb", as_text=False)
&lt;/denchmark-code&gt;

Unfortunately when I call the c++ code to run the train_op node, it'll throw below error:
E tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'message' not in Op&lt;name=PreventGradient; signature=input:T -&gt; output:T; attr=T:type&gt;; NodeDef: gradients/xentropy/xentropy_grad/PreventGradient = PreventGradient[T=DT_FLOAT, message="Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation\'s interaction with tf.gradients()", _device="/job:localhost/replica:0/task:0/cpu:0"](xentropy/xentropy:1) 
I'm still confused whether the error comes from a bug inside the TF code or not.
	</description>
	<comments>
		<comment id='1' author='saman-aghazadeh' date='2017-07-25T00:58:21Z'>
		Does it work if you train from within Python?
&lt;denchmark-link:https://github.com/skye&gt;@skye&lt;/denchmark-link&gt;
, do you expect that a generated graphdef should be runnable for training in c++?
		</comment>
		<comment id='2' author='saman-aghazadeh' date='2017-07-25T19:16:42Z'>
		Basically yes. I have seen C++ examples (even I tried it myself) that can train a graph. To me it seems like gradient calculation functions generally does work on C++ mode. But still based on the description, that gradient ops are only available in python, I'm not sure how they are being ported from python to C++, when we wrap the whole TF core as a shared object library.
		</comment>
		<comment id='3' author='saman-aghazadeh' date='2017-07-27T02:05:36Z'>
		Some gradients certainly are not available, but you are generating the graph first, and the weird thing is it is remarking about a 2nd derivate. Which suggests you model is double differentiating somewhere or there is a bug in the C++ layer.
		</comment>
		<comment id='4' author='saman-aghazadeh' date='2017-07-27T23:18:12Z'>
		Alright, so here I have one more question. If I have a model with specific operators, which gradients have not been implemented in C++ version, does it mean that during training it's gonna fail? Or it's silently continue execution with no update on the variables? I'm asking this because I'm trying to validate how much using C++ interface is practical for training models!
		</comment>
		<comment id='5' author='saman-aghazadeh' date='2017-07-28T20:55:11Z'>
		If the gradient is not implemented in C++, we don't register a kernel, so it will definitely give you a "no kernel found" type error.
		</comment>
		<comment id='6' author='saman-aghazadeh' date='2017-10-28T08:22:15Z'>
		&lt;denchmark-link:https://github.com/saman-aghazadeh&gt;@saman-aghazadeh&lt;/denchmark-link&gt;
 Could you please share the sources from where you could create a graph in c++
I am trying to create a graph in c++ from python code available at &lt;denchmark-link:https://github.com/wolfib/image-classification-CIFAR10-tf/blob/master/softmax.py&gt;https://github.com/wolfib/image-classification-CIFAR10-tf/blob/master/softmax.py&lt;/denchmark-link&gt;
 but unable to do so as I coudn't find few of the python tensorflow function in C++ , for Eg tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,
labels=labels_placeholder))
		</comment>
		<comment id='7' author='saman-aghazadeh' date='2018-10-08T04:16:01Z'>
		This issue is reproducible with the python API as well.
In my case, I have added an additional term to the loss function (gradient of the cross entropy with respect to the input). In this case, second derivative of sparse_softmax_cross_entropy_with_logits needs to be computed during backprop and the following error is thrown:
LookupError: Gradient explicitly disabled. Reason: b"Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation's interaction with tf.gradients()"
It is surprising that support for second order derivative is still missing for some of the key operations in TF.
		</comment>
		<comment id='8' author='saman-aghazadeh' date='2018-10-23T17:30:44Z'>
		&lt;denchmark-link:https://github.com/tensorflowbutler&gt;@tensorflowbutler&lt;/denchmark-link&gt;
 Yes, this is still an issue.
		</comment>
		<comment id='9' author='saman-aghazadeh' date='2019-02-08T17:14:46Z'>
		I am getting this error in Python as well, similar to &lt;denchmark-link:https://github.com/vipinpillai&gt;@vipinpillai&lt;/denchmark-link&gt;
. Is there any update on this issue? Is there any short-term workaround for it?
		</comment>
		<comment id='10' author='saman-aghazadeh' date='2019-08-28T21:16:19Z'>
		&lt;denchmark-link:https://github.com/mmkamani7&gt;@mmkamani7&lt;/denchmark-link&gt;
 Can you check with recent TF versions ( or , ) and let us know whether it was resolved or not. There were several improvements in the last couple of months. Thanks!
		</comment>
		<comment id='11' author='saman-aghazadeh' date='2019-09-09T16:07:52Z'>
		Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
		<comment id='12' author='saman-aghazadeh' date='2019-09-09T16:07:53Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=11626&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=11626&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='saman-aghazadeh' date='2019-10-10T09:30:06Z'>
		When I use jupyter with tf2.0beta, there has no problem. But when I use tf2.0.0, the error 'LookupError: Gradient explicitly disabled. ' appears.
		</comment>
		<comment id='14' author='saman-aghazadeh' date='2020-09-05T15:22:05Z'>
		
When I use jupyter with tf2.0beta, there has no problem. But when I use tf2.0.0, the error 'LookupError: Gradient explicitly disabled. ' appears.

&lt;denchmark-code&gt;Did you have solved this problem?,I met the same error that is "LookupError: Gradient explicitly disabled. Reason: b"Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation's interaction with tf.gradients()""
&lt;/denchmark-code&gt;

		</comment>
		<comment id='15' author='saman-aghazadeh' date='2020-09-05T15:30:33Z'>
		
This issue is reproducible with the python API as well.
In my case, I have added an additional term to the loss function (gradient of the cross entropy with respect to the input). In this case, second derivative of sparse_softmax_cross_entropy_with_logits needs to be computed during backprop and the following error is thrown:
LookupError: Gradient explicitly disabled. Reason: b"Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation's interaction with tf.gradients()"
It is surprising that support for second order derivative is still missing for some of the key operations in TF.

Excuse me, can you tell me how to deal with this error? Look forward to you reply, thanks
		</comment>
	</comments>
</bug>