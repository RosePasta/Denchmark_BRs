<bug id='35441' author='yourtheron' open_date='2019-12-27T07:48:14Z' closed_time='2020-01-23T20:31:37Z'>
	<summary>kears.layers.concatenate Does Not Work when Saving a Model</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): A custom model
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows7
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary):
TensorFlow version (use command below): 2.0.0
Python version:  3.75
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.0.1
GPU model and memory: Quadro K620M

Describe the current behavior
I built a custom model as follows
&lt;denchmark-code&gt;	class C3BR(tf.keras.Model):
		''' 3D Convolution + Batch Normalisation + Relu '''
		def __init__(self, filterNum, kSize, strSize, padMode):
			super(C3BR, self).__init__()
			self.conv = layers.Conv3D(filters=filterNum, kernel_size=kSize, strides=strSize, padding=padMode, data_format='channels_first')
			self.BN = layers.BatchNormalization(axis=1)
		
		def call(self, inputs, ifTrain=True):
			x = self.conv(inputs)
			if ifTrain == True:
				x= self.BN(x)
			return activations.relu(x)

		def build_model(self, input_shape):
			''' A work-around to define dimensions of signals through the NN'''
			self.build(input_shape)
			inputs = tf.keras.Input(shape=input_shape[1:])
			_ = self.call(inputs) 

	class SimpleUNet1(tf.keras.Model):
		"""
		Serialise basic units so as to build up a double-layered encoder-decoder U-Net
		Input:
			inDim: (for initialisation) [modaility/channel, tensor dimensions]
			classNum: background included
			name: name for the net
			inputs: 5D tf tensor of [mbSize, modaility/channel, tensor dimensions]. Inputs must be organised into channel first order
			input_shape: a 1X5 tuple [mbSize, modaility/channel, tensor dimensions]
			ifTrain: True for training, and False for validation and testing
		Returns:
			outputs: 5D tf tensor of [mbSize, classNum, tensor dimensions]
		"""
		def __init__(self, inDim, classNum, name='SimpleUNet', **kwarg):
			super(SimpleUNet1, self).__init__(name=name, **kwarg)
			self.inDim = inDim
			self.classNum = classNum
			dimEnSt1End = np.array(inDim)[2:]-2-2
			dimEnSt2Ed = dimEnSt1End/2-2-2
			dimBridgeEnd = (dimEnSt2Ed/2-2-2)*2
			dimDEStd1End = (dimBridgeEnd-2-2)*2
			self.outDim = dimDEStd1End-2-2-2
			temp = ((dimEnSt2Ed - dimBridgeEnd)/2).astype('int32')
			crop3d1 = tuple(np.tile(temp, (2, 1)).T)
			temp = ((dimEnSt1End - dimDEStd1End)/2).astype('int32')
			crop3d2 = tuple(np.tile(temp, (2, 1)).T)

			self.en_st1_cbr1 = C3BR(32, 3, 1, 'valid')
			self.en_st1_cbr2 = C3BR(64, 3, 1, 'valid')
			self.en_st2_mp = layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid', data_format='channels_first')
			self.en_st2_cbr1 = C3BR(64, 3, 1, 'valid')
			self.en_st2_cbr2 = C3BR(128, 3, 1, 'valid')
			self.bridge_mp = layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid', data_format='channels_first')
			self.bridge_cbr1 = C3BR(128, 3, 1, 'valid')
			self.bridge_cbr2 = C3BR(256, 3, 1, 'valid')    
			self.bridge_tconv1 = layers.Conv3DTranspose(256, 2, strides=2, padding='valid', data_format='channels_first')
			self.de_3dcrop1 = layers.Cropping3D(crop3d1, data_format='channels_first')
			self.de_st1_concat = layers.Concatenate(axis=1)
			self.de_st1_cbr1 = C3BR(256, 3, 1, 'valid')
			self.de_st1_cbr2 = C3BR(128, 3, 1, 'valid')    
			self.de_st1_tconv1 = layers.Conv3DTranspose(128, 2, strides=2, padding='valid', data_format='channels_first')
			self.de_3dcrop2 = layers.Cropping3D(crop3d2, data_format='channels_first')
			self.de_st2_concat = layers.Concatenate(axis=1)
			self.de_st2_cbr1 = C3BR(64, 3, 1, 'valid')
			self.de_st2_cbr2 = C3BR(64, 3, 1, 'valid') 
			self.final_conv3D = layers.Conv3D(filters=self.classNum, kernel_size=3, strides=1, padding='valid', data_format='channels_first')
					
		#@tf.function
		def call(self, inputs, ifTrain=True):
			x0 = self.en_st1_cbr1(inputs, ifTrain)
			xEnSt1End = self.en_st1_cbr2(x0, ifTrain)
			x1 = self.en_st2_mp(xEnSt1End)
			x2 = self.en_st2_cbr1(x1, ifTrain)
			xEnSt2Ed = self.en_st2_cbr2(x2, ifTrain)
			x3 = self.bridge_mp(xEnSt2Ed)  
			x4 = self.bridge_cbr1(x3, ifTrain)
			x5 = self.bridge_cbr2(x4, ifTrain)    
			xBridgeEnd = self.bridge_tconv1(x5)
			xCrop1 = self.de_3dcrop1(xEnSt2Ed)
			print(xBridgeEnd.shape)
			print(xCrop1.shape)
			x6 = self.de_st1_concat([xBridgeEnd, xCrop1])
			print(x6.shape)
			x7 = self.de_st1_cbr1(x6, ifTrain)
			x8 = self.de_st1_cbr2(x7, ifTrain)
			xDeSt1End = self.de_st1_tconv1(x8)
			xCrop2 = self.de_3dcrop2(xEnSt1End)
			x9 = self.de_st2_concat([xDeSt1End, xCrop2])
			x10 = self.de_st2_cbr1(x9, ifTrain)
			x11 = self.de_st2_cbr2(x10, ifTrain)
			x12 = self.final_conv3D(x11)
			outputs = activations.softmax(x12, axis=1)
			
			return outputs
			
		def build_model(self, input_shape):
			''' A work-around to define dimensions of signals through the NN'''
			self.build(input_shape)
			inputs = tf.keras.Input(shape=input_shape[1:])

			_ = self.call(inputs)
			
		def compute_output_shape(self):
			# Override this function if one expects to use the subclassed model in Kera's fit() method; Otherwise, it is optional.
			return tf.TensorShape(np.append(self.classNum, self.outDim))    

&lt;/denchmark-code&gt;

Please pay attention to the following two definitions. If I use concatenate layer in this way (the C is a capitalised one), when I try and save the model, it works
&lt;denchmark-code&gt;	self.de_st1_concat = layers.Concatenate(axis=1)
	self.de_st2_concat = layers.Concatenate(axis=1)
&lt;/denchmark-code&gt;

For instance,
&lt;denchmark-code&gt;	modelInDim = (4, 64, 64, 64)
	classNum = 2
	mbSize = 2
	TUNet = SimpleUNet1(modelInDim, classNum)
	TUNet.build_model(input_shape=(mbSize,)+modelInDim)
	x=tf.random.uniform((mbSize, 4, 64, 64, 64))
	y=TUNet(x)
	TUNet.summary()
	TUNet._set_inputs(x)
	TUNet.save(r'...\TTweight', save_format='tf') 
&lt;/denchmark-code&gt;

But if, as per &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/layers/concatenate?hl=en&amp;version=stable&gt;this page&lt;/denchmark-link&gt;
 I use layers.concatenate (small 'c') to generate signals x6 and x9, respectively, that is
&lt;denchmark-code&gt;	x6 = layers.concatenate([xBridgeEnd, xCrop1], axis=1)
	x9 = layers.concatenate([xDeSt1End, xCrop2], axis=1)
&lt;/denchmark-code&gt;

Then save the model in the same way above, it raised an error
&lt;denchmark-code&gt;	  ValueError: A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 256, None, None, None), (None, 128, 18, 18, 18)]
&lt;/denchmark-code&gt;

The detailed log is &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/19241&gt;here&lt;/denchmark-link&gt;

It takes me almost an entire day to figure out the root cause. In conclusion, I think the second one may have to be deprecated; otherwise users like me will be confused and misguided.
	</description>
	<comments>
		<comment id='1' author='yourtheron' date='2020-01-02T07:57:00Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/gowthamkpr&gt;@gowthamkpr&lt;/denchmark-link&gt;

I encountered a further issue when trying to save the model under the mixed precision version. It quite confusing. I followed instructions at &lt;denchmark-link:https://www.tensorflow.org/guide/keras/mixed_precision#training_the_model_with_a_custom_training_loop&gt;here&lt;/denchmark-link&gt;

So I did the following
&lt;denchmark-code&gt;from tensorflow.keras.mixed_precision import experimental as mixed_precision
curOpt = tf.keras.optimizers.Adam(learning_rate=lr)
curOpt = mixed_precision.LossScaleOptimizer(curOpt, loss_scale='dynamic')
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_policy(policy)
# query if it has been well set
print('Compute dtype: %s' % policy.compute_dtype)
print('Variable dtype: %s' % policy.variable_dtype)
from MedNN import SimpleUNet
TUNet = SimpleUNet(modelInDim, classNum)
TUNet.build_model(input_shape=(mbSize,)+modelInDim)
&lt;/denchmark-code&gt;

Then if I use a test signal x0 of tf.float32 to do the following:
&lt;denchmark-code&gt;x0 =tf.random.uniform((mbSize, 4, 64, 64, 64), dtype=tf.float32)
y=TUNet(x0, True)
TUNet.summary()
TUNet._set_inputs(x0, True)
# Method #1
tf.saved_model.save(TUNet, r'...\TUNet_Test')
# Method #2
TUNet.save(r'...\TUNet_Test', save_format='tf') 
&lt;/denchmark-code&gt;

Whether I ran method &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/1&gt;#1&lt;/denchmark-link&gt;
 or 2, an error was raised. It seems the dtype does not match
&lt;denchmark-code&gt;ValueError: Python inputs incompatible with input_signature: inputs ((&lt;tf.Tensor 'conv3d/Cast:0' shape=(None, 4, 64, 64, 64) dtype=float16&gt;,)), input_signature ((TensorSpec(shape=(None, 4, None, None, None), dtype=tf.float32, name=None),))
&lt;/denchmark-code&gt;

Even if I cast x0 into tf.float16, the same error occurs.
So my questions are:

Is it a bug about tf's functionality of saving custom model or mixed precision?
Can I use mixed precision for custom models safely?
If it is neither, how can I fix the problem?
I am aware that multiple issues are involved here, and I might need to report them in different issues, Yet they are tightly coupled as presented above, hence could they be scrutinised here in this issue altogether? Many thanks, indeed!

		</comment>
		<comment id='2' author='yourtheron' date='2020-01-02T08:10:22Z'>
		At the same time, I would like to say that I surmise that loads of people tend to use custom layers, custom loops, custom models, and custom training, but tf's doc seems not quite to keep apace well. For example, in the mixed precision guide (the link above), it uses the toy example of digit recognition, then states the following:
&lt;denchmark-link:https://user-images.githubusercontent.com/51472988/71657145-9e7bea80-2d0c-11ea-8a97-4c907a9d8fe2.JPG&gt;&lt;/denchmark-link&gt;


Why can't the doc use a more explicit or articulating example to state such a delicate and important case?
The underlined sentence is quite confusing. Will the model do it automatically, or shall the user do it, for example in the call() function of a custom model?

		</comment>
		<comment id='3' author='yourtheron' date='2020-01-02T10:03:46Z'>
		Sorry, I mistakenly closed the issue by pressing the wrong keys....
Another issue about loading custom models:
After saving it in tf format as I mentioned in the first post, I load it by
&lt;denchmark-code&gt;newModel= tf.keras.models.load_model(...path)
&lt;/denchmark-code&gt;

The path contains the following
&lt;denchmark-link:https://user-images.githubusercontent.com/51472988/71661707-105c3000-2d1d-11ea-8d66-37722cd4e8a5.png&gt;&lt;/denchmark-link&gt;

Please note that my model above can run and can be trained--working properly. Yet tf prompted
&lt;denchmark-code&gt;ValueError: Could not find matching function to call loaded from the SavedModel. Got:
  Positional arguments (1 total):
    * Tensor("input_1_5:0", shape=(None, 4, 64, 64, 64), dtype=float32)
  Keyword arguments: {'training': &lt;tf.Tensor 'keras_learning_phase:0' shape=() dtype=bool&gt;}

Expected these arguments to match one of the following 2 option(s):

Option 1:
  Positional arguments (3 total):
    * TensorSpec(shape=(None, 4, 64, 64, 64), dtype=tf.float32, name='input_1')
    * False
    * False
  Keyword arguments: {}

Option 2:
  Positional arguments (3 total):
    * TensorSpec(shape=(None, 4, 64, 64, 64), dtype=tf.float32, name='input_1')
    * False
    * True
  Keyword arguments: {}
&lt;/denchmark-code&gt;

May you be kind to take a look? Many thanks.
		</comment>
		<comment id='4' author='yourtheron' date='2020-01-02T10:03:49Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35441&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35441&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='yourtheron' date='2020-01-22T01:02:17Z'>
		When I try to run the code in the original post, I get the error:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "issue35441.py", line 118, in &lt;module&gt;
    TUNet = SimpleUNet1(modelInDim, classNum)
  File "issue35441.py", line 63, in __init__
    self.de_3dcrop1 = layers.Cropping3D(crop3d1, data_format='channels_first')
  File "/usr/local/google/home/reedwm/venvs/tf2_0/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/convolutional.py", line 2565, in __init__
    'Found: ' + str(cropping))
ValueError: `cropping` should have 3 elements. Found: (array([4, 4], dtype=int32), array([4, 4], dtype=int32))
&lt;/denchmark-code&gt;

I am unfamiliar with UNet so I'm not sure what's wrong. Also, it would help if you post a single self-contained example to reproduce the issue. In your original post, you split the code over several code blocks, and you don't have the import statements like import tensorflow as tf.
Can you post a self-contained example to reproduce the original issue?
As for your mixed-precision related questions:


Is it a bug about tf's functionality of saving custom model or mixed precision?


It's probably a bug with the combination of the two features. But this may have been fixed by &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/cd6184047e9e497955c473b88387b54818ff23a0&gt;cd61840&lt;/denchmark-link&gt;
 so try again with the latest nightly build. You can install a nightly build with .


Can I use mixed precision for custom models safely?


Yes, this should work.

Why can't the doc use a more explicit or articulating example to state such a delicate and important case?

Agreed this point is not emphasized much. I did try to make this clear, as earlier in the doc, there is a sentence stating the same thing: "They cast their inputs to float16 in order to do float16 computations, which causes their outputs to be float16 as a result." After that sentence, there is an example showing the output is float16.
It was tough to balance having a lot of explanation on details like that, and having the doc be too long. I will see if I can make this more clear but I also don't want to add another paragraph as the doc is already very long.

The underlined sentence is quite confusing. Will the model do it automatically, or shall the user do it, for example in the call() function of a custom model?

The model will automatically do it. I'll also see if I can add a sentence stating this to the doc. This is useful information, but it's tough to find a good place to mention this.
		</comment>
		<comment id='6' author='yourtheron' date='2020-01-22T07:25:59Z'>
		&lt;denchmark-link:https://github.com/reedwm&gt;@reedwm&lt;/denchmark-link&gt;
 Sorry for the confusion, here is the complete code, which I have tested via Colab.
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np
from tensorflow.keras import layers, activations

class C3BR(tf.keras.Model):
		''' 3D Convolution + Batch Normalisation + Relu '''
		def __init__(self, filterNum, kSize, strSize, padMode):
			super(C3BR, self).__init__()
			self.conv = layers.Conv3D(filters=filterNum, kernel_size=kSize, strides=strSize, padding=padMode, data_format='channels_first')
			self.BN = layers.BatchNormalization(axis=1)
		
		def call(self, inputs, ifTrain=True):
			x = self.conv(inputs)
			if ifTrain == True:
				x= self.BN(x)
			return activations.relu(x)

		def build_model(self, input_shape):
			''' A work-around to define dimensions of signals through the NN'''
			self.build(input_shape)
			inputs = tf.keras.Input(shape=input_shape[1:])
			_ = self.call(inputs) 

class SimpleUNet1(tf.keras.Model):
	"""
	Serialise basic units so as to build up a double-layered encoder-decoder U-Net
	Input:
		inDim: (for initialisation) [modaility/channel, tensor dimensions]
		classNum: background included
		name: name for the net
		inputs: 5D tf tensor of [mbSize, modaility/channel, tensor dimensions]. Inputs must be organised into channel first order
		input_shape: a 1X5 tuple [mbSize, modaility/channel, tensor dimensions]
		ifTrain: True for training, and False for validation and testing
	Returns:
		outputs: 5D tf tensor of [mbSize, classNum, tensor dimensions]
	"""
	def __init__(self, inDim, classNum, name='SimpleUNet', **kwarg):
		super(SimpleUNet1, self).__init__(name=name, **kwarg)
		self.inDim = inDim
		self.classNum = classNum
		dimEnSt1End = np.array(inDim)[1:]-2-2
		dimEnSt2Ed = dimEnSt1End/2-2-2
		dimBridgeEnd = (dimEnSt2Ed/2-2-2)*2
		dimDEStd1End = (dimBridgeEnd-2-2)*2
		self.outDim = dimDEStd1End-2-2-2
		temp = ((dimEnSt2Ed - dimBridgeEnd)/2).astype('int32')
		crop3d1 = tuple(np.tile(temp, (2, 1)).T)
		temp = ((dimEnSt1End - dimDEStd1End)/2).astype('int32')
		crop3d2 = tuple(np.tile(temp, (2, 1)).T)

		self.en_st1_cbr1 = C3BR(32, 3, 1, 'valid')
		self.en_st1_cbr2 = C3BR(64, 3, 1, 'valid')
		self.en_st2_mp = layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid', data_format='channels_first')
		self.en_st2_cbr1 = C3BR(64, 3, 1, 'valid')
		self.en_st2_cbr2 = C3BR(128, 3, 1, 'valid')
		self.bridge_mp = layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid', data_format='channels_first')
		self.bridge_cbr1 = C3BR(128, 3, 1, 'valid')
		self.bridge_cbr2 = C3BR(256, 3, 1, 'valid')    
		self.bridge_tconv1 = layers.Conv3DTranspose(256, 2, strides=2, padding='valid', data_format='channels_first')
		self.de_3dcrop1 = layers.Cropping3D(crop3d1, data_format='channels_first')
		self.de_st1_concat = layers.Concatenate(axis=1)
		self.de_st1_cbr1 = C3BR(256, 3, 1, 'valid')
		self.de_st1_cbr2 = C3BR(128, 3, 1, 'valid')    
		self.de_st1_tconv1 = layers.Conv3DTranspose(128, 2, strides=2, padding='valid', data_format='channels_first')
		self.de_3dcrop2 = layers.Cropping3D(crop3d2, data_format='channels_first')
		self.de_st2_concat = layers.Concatenate(axis=1)
		self.de_st2_cbr1 = C3BR(64, 3, 1, 'valid')
		self.de_st2_cbr2 = C3BR(64, 3, 1, 'valid') 
		self.final_conv3D = layers.Conv3D(filters=self.classNum, kernel_size=3, strides=1, padding='valid', data_format='channels_first')
				
	#@tf.function
	def call(self, inputs, ifTrain=True):
		x0 = self.en_st1_cbr1(inputs, ifTrain)
		xEnSt1End = self.en_st1_cbr2(x0, ifTrain)
		x1 = self.en_st2_mp(xEnSt1End)
		x2 = self.en_st2_cbr1(x1, ifTrain)
		xEnSt2Ed = self.en_st2_cbr2(x2, ifTrain)
		x3 = self.bridge_mp(xEnSt2Ed)  
		x4 = self.bridge_cbr1(x3, ifTrain)
		x5 = self.bridge_cbr2(x4, ifTrain)    
		xBridgeEnd = self.bridge_tconv1(x5)
		xCrop1 = self.de_3dcrop1(xEnSt2Ed)
		print(xBridgeEnd.shape)
		print(xCrop1.shape)
		x6 = self.de_st1_concat([xBridgeEnd, xCrop1])
		print(x6.shape)
		x7 = self.de_st1_cbr1(x6, ifTrain)
		x8 = self.de_st1_cbr2(x7, ifTrain)
		xDeSt1End = self.de_st1_tconv1(x8)
		xCrop2 = self.de_3dcrop2(xEnSt1End)
		x9 = self.de_st2_concat([xDeSt1End, xCrop2])
		x10 = self.de_st2_cbr1(x9, ifTrain)
		x11 = self.de_st2_cbr2(x10, ifTrain)
		x12 = self.final_conv3D(x11)
		outputs = activations.softmax(x12, axis=1)
		
		return outputs
		
	def build_model(self, input_shape):
		''' A work-around to define dimensions of signals through the NN'''
		self.build(input_shape)
		inputs = tf.keras.Input(shape=input_shape[1:])

		_ = self.call(inputs)
		
	def compute_output_shape(self):
		# Override this function if one expects to use the subclassed model in Kera's fit() method; Otherwise, it is optional.
		return tf.TensorShape(np.append(self.classNum, self.outDim))    

modelInDim = (4, 64, 64, 64)
classNum = 2
mbSize = 2
TUNet = SimpleUNet1(modelInDim, classNum)
TUNet.build_model(input_shape=(mbSize,)+modelInDim)
TUNet.summary()
TUNet.save(r'e:\TTweight', save_format='tf') 
&lt;/denchmark-code&gt;

The above works in saving the moel
If we change the code above into
&lt;denchmark-code&gt;x6 = layers.concatenate([xBridgeEnd, xCrop1], axis=1)
x9 = layers.concatenate([xDeSt1End, xCrop2], axis=1)
&lt;/denchmark-code&gt;

An error was raised as stated before.
Has the solution for saving models under mixed precision been submitted to Tf2.1 as well?
By the way, at present I am saving neither model nor weight. I simply save everything to a checkpoint, and it can be loaded back. It is fine as long as I work in Python environment and have full access to original code for building up the model, but not export model to C++ etc.
Finally, I would say as I proceed in the doc (basically I read all of the guide, the tutorial, as well as 2.0.0 API except the RNN and reinforcement learning part that are not my field), I have got a strong feeling that the writing style in different parts differ a lot. Sometimes semi-formal, sometimes informal, even colloquial. I, being a non-native English speaker, am afraid to say there are even some grammatical mistakes and pdigin English, which do not hinder engineers from understanding the stuff though, are flaws. Hope Google will pay some attentions to it.
		</comment>
		<comment id='7' author='yourtheron' date='2020-01-22T20:43:35Z'>
		Thanks for the example to reproduce! However, when I try running the new example (without changing the x6 and x9 lines to the version), I get the error:
&lt;denchmark-code&gt;ValueError: Model &lt;__main__.SimpleUNet1 object at 0x7f7ea76ec690&gt; cannot be saved because the input shapes have not been set. Usually, input shapes are automatically determined from calling .fit() or .predict(). To manually set the shapes, call model._set_inputs(inputs).
&lt;/denchmark-code&gt;

Can you double check the example? I got that above error in TF 2.0, TF 2.1, and the nightly TF.

Has the solution for saving models under mixed precision been submitted to Tf2.1 as well?

Unfortunately no. The fix will only be in TF 2.2 and above.

Finally, I would say as I proceed in the doc (basically I read all of the guide, the tutorial, as well as 2.0.0 API except the RNN and reinforcement learning part that are not my field), I have got a strong feeling that the writing style in different parts differ a lot. Sometimes semi-formal, sometimes informal, even colloquial. I, being a non-native English speaker, am afraid to say there are even some grammatical mistakes and pdigin English, which do not hinder engineers from understanding the stuff though, are flaws. Hope Google will pay some attentions to it.

Thanks for the feedback. Ideally all documentation should conform to the &lt;denchmark-link:https://developers.google.com/style&gt;Google documentation style guide&lt;/denchmark-link&gt;
. Feel free to point out different docs where you think the writing style differs a lot, or where there are grammatical mistakes. You can also file a new issue for this. The docs are written by different people so style may differ, but we try to keep the style consistent.
		</comment>
		<comment id='8' author='yourtheron' date='2020-01-23T08:46:48Z'>
		&lt;denchmark-link:https://github.com/reedwm&gt;@reedwm&lt;/denchmark-link&gt;
 I see. Well, for some reason I do not know, we have to run a dummy signal through the net as follows before saving the model
&lt;denchmark-code&gt;x=tf.random.uniform((mbSize, 4, 64, 64, 64))
y=TUNet(x)
# use your directory in lieu of r'...\TTweight'
TUNet.save(r'...\TTweight', save_format='tf') 
&lt;/denchmark-code&gt;

or
&lt;denchmark-code&gt;x=tf.random.uniform((mbSize, 4, 64, 64, 64))
TUNet._set_inputs(x)
TUNet.save(r'...\TTweight', save_format='tf') 
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='yourtheron' date='2020-01-23T20:31:37Z'>
		With your second code sample, with TUNet._set_inputs(x), I was able to get the example to work. Once I replace x6 and x9 though, I get a different error on the TUNet.summary() line. If I comment the summary line out, I can reproduce your original error in TF 2.0 ("A Concatenate layer requires inputs with matching shapes..."). However, I cannot reproduce in TF 2.1. Therefore, I'm assuming the original issue is fixed so I'm closing it.
However, there still may be a bug, as the summary doesn't work. Feel free to file another bug about that if you think that is an issue, with a self-contained example to reproduce. If you do file such a bug, consider CCing me (add the line "/CC &lt;denchmark-link:https://github.com/reedwm&gt;@reedwm&lt;/denchmark-link&gt;
") as I have some context and can therefore more easily triage to someone else.
For reference, here is the code sample I ran that works in TF 2.0 but not TF 2.1. It is the same as the code sample you provided, except i called _set_inputs as you suggested, I commented out TUNet.summary(), and I replaced x6 and x9 as you suggested.
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np
from tensorflow.keras import layers, activations

class C3BR(tf.keras.Model):
  ''' 3D Convolution + Batch Normalisation + Relu '''
  def __init__(self, filterNum, kSize, strSize, padMode):
    super(C3BR, self).__init__()
    self.conv = layers.Conv3D(filters=filterNum, kernel_size=kSize, strides=strSize, padding=padMode, data_format='channels_first')
    self.BN = layers.BatchNormalization(axis=1)

  def call(self, inputs, ifTrain=True):
    x = self.conv(inputs)
    if ifTrain == True:
      x= self.BN(x)
    return activations.relu(x)

  def build_model(self, input_shape):
    ''' A work-around to define dimensions of signals through the NN'''
    self.build(input_shape)
    inputs = tf.keras.Input(shape=input_shape[1:])
    _ = self.call(inputs)

class SimpleUNet1(tf.keras.Model):
  """
  Serialise basic units so as to build up a double-layered encoder-decoder U-Net
  Input:
    inDim: (for initialisation) [modaility/channel, tensor dimensions]
    classNum: background included
    name: name for the net
    inputs: 5D tf tensor of [mbSize, modaility/channel, tensor dimensions]. Inputs must be organised into channel first order
    input_shape: a 1X5 tuple [mbSize, modaility/channel, tensor dimensions]
    ifTrain: True for training, and False for validation and testing
  Returns:
    outputs: 5D tf tensor of [mbSize, classNum, tensor dimensions]
  """
  def __init__(self, inDim, classNum, name='SimpleUNet', **kwarg):
    super(SimpleUNet1, self).__init__(name=name, **kwarg)
    self.inDim = inDim
    self.classNum = classNum
    dimEnSt1End = np.array(inDim)[1:]-2-2
    dimEnSt2Ed = dimEnSt1End/2-2-2
    dimBridgeEnd = (dimEnSt2Ed/2-2-2)*2
    dimDEStd1End = (dimBridgeEnd-2-2)*2
    self.outDim = dimDEStd1End-2-2-2
    temp = ((dimEnSt2Ed - dimBridgeEnd)/2).astype('int32')
    crop3d1 = tuple(np.tile(temp, (2, 1)).T)
    temp = ((dimEnSt1End - dimDEStd1End)/2).astype('int32')
    crop3d2 = tuple(np.tile(temp, (2, 1)).T)

    self.en_st1_cbr1 = C3BR(32, 3, 1, 'valid')
    self.en_st1_cbr2 = C3BR(64, 3, 1, 'valid')
    self.en_st2_mp = layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid', data_format='channels_first')
    self.en_st2_cbr1 = C3BR(64, 3, 1, 'valid')
    self.en_st2_cbr2 = C3BR(128, 3, 1, 'valid')
    self.bridge_mp = layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid', data_format='channels_first')
    self.bridge_cbr1 = C3BR(128, 3, 1, 'valid')
    self.bridge_cbr2 = C3BR(256, 3, 1, 'valid')
    self.bridge_tconv1 = layers.Conv3DTranspose(256, 2, strides=2, padding='valid', data_format='channels_first')
    self.de_3dcrop1 = layers.Cropping3D(crop3d1, data_format='channels_first')
    self.de_st1_concat = layers.Concatenate(axis=1)
    self.de_st1_cbr1 = C3BR(256, 3, 1, 'valid')
    self.de_st1_cbr2 = C3BR(128, 3, 1, 'valid')
    self.de_st1_tconv1 = layers.Conv3DTranspose(128, 2, strides=2, padding='valid', data_format='channels_first')
    self.de_3dcrop2 = layers.Cropping3D(crop3d2, data_format='channels_first')
    self.de_st2_concat = layers.Concatenate(axis=1)
    self.de_st2_cbr1 = C3BR(64, 3, 1, 'valid')
    self.de_st2_cbr2 = C3BR(64, 3, 1, 'valid')
    self.final_conv3D = layers.Conv3D(filters=self.classNum, kernel_size=3, strides=1, padding='valid', data_format='channels_first')

  #@tf.function
  def call(self, inputs, ifTrain=True):
    x0 = self.en_st1_cbr1(inputs, ifTrain)
    xEnSt1End = self.en_st1_cbr2(x0, ifTrain)
    x1 = self.en_st2_mp(xEnSt1End)
    x2 = self.en_st2_cbr1(x1, ifTrain)
    xEnSt2Ed = self.en_st2_cbr2(x2, ifTrain)
    x3 = self.bridge_mp(xEnSt2Ed)
    x4 = self.bridge_cbr1(x3, ifTrain)
    x5 = self.bridge_cbr2(x4, ifTrain)
    xBridgeEnd = self.bridge_tconv1(x5)
    xCrop1 = self.de_3dcrop1(xEnSt2Ed)
    print(xBridgeEnd.shape)
    print(xCrop1.shape)
    x6 = layers.concatenate([xBridgeEnd, xCrop1], axis=1)
    print(x6.shape)
    x7 = self.de_st1_cbr1(x6, ifTrain)
    x8 = self.de_st1_cbr2(x7, ifTrain)
    xDeSt1End = self.de_st1_tconv1(x8)
    xCrop2 = self.de_3dcrop2(xEnSt1End)
    x9 = layers.concatenate([xDeSt1End, xCrop2], axis=1)
    x10 = self.de_st2_cbr1(x9, ifTrain)
    x11 = self.de_st2_cbr2(x10, ifTrain)
    x12 = self.final_conv3D(x11)
    outputs = activations.softmax(x12, axis=1)

    return outputs

  def build_model(self, input_shape):
    ''' A work-around to define dimensions of signals through the NN'''
    self.build(input_shape)
    inputs = tf.keras.Input(shape=input_shape[1:])

    _ = self.call(inputs)

  def compute_output_shape(self):
    # Override this function if one expects to use the subclassed model in Kera's fit() method; Otherwise, it is optional.
    return tf.TensorShape(np.append(self.classNum, self.outDim))

modelInDim = (4, 64, 64, 64)
classNum = 2
mbSize = 2
TUNet = SimpleUNet1(modelInDim, classNum)
TUNet.build_model(input_shape=(mbSize,)+modelInDim)
# TUNet.summary()

x=tf.random.uniform((mbSize, 4, 64, 64, 64))
TUNet._set_inputs(x)
# use your directory in lieu of r'...\TTweight'
TUNet.save(r'TTweight', save_format='tf')
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='yourtheron' date='2020-01-23T20:31:39Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35441&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35441&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>