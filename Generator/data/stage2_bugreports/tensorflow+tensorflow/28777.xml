<bug id='28777' author='AakashKumarNain' open_date='2019-05-16T19:48:10Z' closed_time='2019-08-05T18:02:28Z'>
	<summary>Conv2D breaking when used with keras Model subclassing</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.0 alpha gpu
Python version: 3.6
Bazel version (if compiling from source):N/A
GCC/Compiler version (if compiling from source):N/A
CUDA/cuDNN version:
GPU model and memory:

Describe the current behavior
I am trying to use tf.keras.Model subclassing to define my model but it is failing on the first conv layer. Here is the code I am using:
class CustomModel(tf.keras.Model):
  def __init__(self, **kwargs):
    super(CustomModel, self).__init__(**kwargs)
    self.conv1   = Conv2D(32, (3, 3), padding='same')
    self.conv2   = Conv2D(64, (3, 3), padding='same')
    self.pool    = MaxPooling2D(pool_size=(2, 2))
    self.bn      = BatchNormalization()
    self.relu    = Activation("relu")
    self.softmax = Activation("softmax")
    self.drop1   = Dropout(0.25)
    self.drop2   = Dropout(0.5)
    self.dense1  = Dense(512)
    self.dense2  = Dense(10)
    self.flat    = Flatten()
    
  
  
  def call(self, inputs, train):
    z = self.conv1(inputs)
    z = self.bn(z, training=train)
    z = self.relu(z)
    
    z = self.conv1(z)
    z = self.bn(z, training=train)
    z = self.relu(z)
    z = self.pool(z)
    z = self.drop1(z, training=train)
    
    z = self.conv2(z)
    z = self.bn(z, training=train)
    z = self.relu(z)
    
    z = self.conv2(z)
    z = self.bn(z, training=train)
    z = self.relu(z)
    z = self.pool(z)
    z = self.drop1(z, training=train)
    
    z = self.flat(z)
    z = self.dense1(z)
    z = self.relu(z)
    z = self.drop2(z, training=train)
    z = self.dense2(z)
    z = self.softmax(z)
    
    return z
In order to check if the model is working fine, I am passing a random input to the model in this way:
&lt;denchmark-code&gt;random_input = np.random.rand(32,32, 3).astype(np.float32)
random_input = np.expand_dims(random_input, axis=0)
preds = model(random_input, train=False)
&lt;/denchmark-code&gt;

But this throws the following error: InvalidArgumentError: input depth must be evenly divisible by filter depth: 32 vs 3 [Op:Conv2D]
The same model works fine with Sequential/Functional API. So either I am missing out on something or there is something wrong with the subclassing. Can you please look into it?
	</description>
	<comments>
		<comment id='1' author='AakashKumarNain' date='2019-05-17T08:00:03Z'>
		&lt;denchmark-link:https://github.com/AakashKumarNain&gt;@AakashKumarNain&lt;/denchmark-link&gt;
 In order to expedite the trouble-shooting process, please provide a minimum code snippet to reproduce the issue reported here. Thanks!
		</comment>
		<comment id='2' author='AakashKumarNain' date='2019-05-17T09:40:50Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 except for the  statements, what I have provided is the code snippet. Anyways, here is the full code:
import os
import glob
import h5py
import math
import shutil


import numpy as np 
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.pylab as pl


from tensorflow.keras.models import Sequential, Model, load_model
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Activation
from tensorflow.keras.layers import Input, Flatten, SeparableConv2D, BatchNormalization
from tensorflow.keras.optimizers import Adam, SGD, RMSprop
from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping
from tensorflow.keras.utils import to_categorical

from tensorflow.keras import backend as K
import tensorflow as tf

class CustomModel(tf.keras.Model):
  def __init__(self, **kwargs):
    super(CustomModel, self).__init__(**kwargs)
    self.conv1   = Conv2D(32, (3, 3), padding='same')
    self.conv2   = Conv2D(64, (3, 3), padding='same')
    self.pool    = MaxPooling2D(pool_size=(2, 2))
    self.bn      = BatchNormalization()
    self.relu    = Activation("relu")
    self.softmax = Activation("softmax")
    self.drop1   = Dropout(0.25)
    self.drop2   = Dropout(0.5)
    self.dense1  = Dense(512)
    self.dense2  = Dense(10)
    self.flat    = Flatten()
    
  
  
  def call(self, inputs, train):
    z = self.conv1(inputs)
    z = self.bn(z, training=train)
    z = self.relu(z)
    
    z = self.conv1(z)
    z = self.bn(z, training=train)
    z = self.relu(z)
    z = self.pool(z)
    z = self.drop1(z, training=train)
    
    z = self.conv2(z)
    z = self.bn(z, training=train)
    z = self.relu(z)
    
    z = self.conv2(z)
    z = self.bn(z, training=train)
    z = self.relu(z)
    z = self.pool(z)
    z = self.drop1(z, training=train)
    
    z = self.flat(z)
    z = self.dense1(z)
    z = self.relu(z)
    z = self.drop2(z, training=train)
    z = self.dense2(z)
    z = self.softmax(z)
    
    return z

model =CustomModel()
random_input = np.random.rand(32,32, 3).astype(np.float32)
random_input = np.expand_dims(random_input, axis=0)
preds = model(random_input, train=False)
		</comment>
		<comment id='3' author='AakashKumarNain' date='2019-05-17T10:08:33Z'>
		&lt;denchmark-link:https://github.com/AakashKumarNain&gt;@AakashKumarNain&lt;/denchmark-link&gt;
 Thanks for the information.
		</comment>
		<comment id='4' author='AakashKumarNain' date='2019-05-17T10:40:10Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 You are welcome
		</comment>
		<comment id='5' author='AakashKumarNain' date='2019-05-17T10:42:13Z'>
		I am able to reproduce the issue on colab with TF 2.0alpha . This is the error message I got InvalidArgumentError: input depth must be evenly divisible by filter depth: 32 vs 3 [Op:Conv2D]
		</comment>
		<comment id='6' author='AakashKumarNain' date='2019-05-19T07:15:46Z'>
		Any updates on this? I think this is a major bug
		</comment>
		<comment id='7' author='AakashKumarNain' date='2019-08-05T03:40:30Z'>
		Not sure what do you mean by it's working in Functional API...
The problem here is you call:
z = conv1(inputs)
which builds the layer to accepts an input of 3 channels
then you call:
z = conv1(z)
which is an input of 32 channels
This is confusing conv1, I think it fails with all subclass/functional/sequential, can you double check?
The error definitely needs improvement, but using colab I do see that the error message is much clearer in 1.14.
		</comment>
		<comment id='8' author='AakashKumarNain' date='2019-08-05T17:44:08Z'>
		When I say
z = conv1(z)
I mean that I want to add another layer that has 32 output channels and a filter of size (3,3). So, conv1 is a layer with 32 output channels which I should be able to use at multiplt places in my net. Is it clear now?
		</comment>
		<comment id='9' author='AakashKumarNain' date='2019-08-05T18:02:28Z'>
		Like mentioned above, we don't have a layer that accepts dynamic number of channels. If you run conv1 with input of (1, 32, 32, 3), the weight inside layer will be shape of (3, 3, 3, 32), which stands for (kernel_size, kernel_size, input_channels, output_channels). And now you pass another input which is 32 channels, so the weight will have to change shape to (3, 3, 32, 32), it's not possible.
		</comment>
		<comment id='10' author='AakashKumarNain' date='2019-08-05T18:02:30Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=28777&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=28777&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='AakashKumarNain' date='2019-08-05T18:05:01Z'>
		Instead make another conv layer would help fix this
		</comment>
		<comment id='12' author='AakashKumarNain' date='2019-08-05T18:17:35Z'>
		But then I have to write all the layers in __init__ and then call every layer in call method which doesn't seem optimal to me. Is there any example you can point to for ref?
		</comment>
		<comment id='13' author='AakashKumarNain' date='2019-08-05T18:36:58Z'>
		You can write smaller blocks that does this, for reference, checkout keras applications:
&lt;denchmark-link:https://github.com/keras-team/keras-applications/blob/master/keras_applications/densenet.py#L93-L120&gt;https://github.com/keras-team/keras-applications/blob/master/keras_applications/densenet.py#L93-L120&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='AakashKumarNain' date='2019-08-05T18:47:07Z'>
		I am aware of that but that is Functional API which works fine. This one is the case of subclassing and eager mode
		</comment>
		<comment id='15' author='AakashKumarNain' date='2019-08-05T19:58:05Z'>
		Do you have a minimal code snippet for functional model?
		</comment>
		<comment id='16' author='AakashKumarNain' date='2019-08-07T07:53:57Z'>
		Yeah sure. Here is the code for functional model:
&lt;denchmark-code&gt;def build_functional_model():
  img_input = Input(shape=(224,224,3))
  x = Conv2D(32, (3,3), activation='relu')(img_input)
  x = Conv2D(32, (3,3), activation='relu')(x)
  x = MaxPooling2D((2,2))(x)
  
  x = Conv2D(64, (3,3), activation='relu')(x)
  x = Conv2D(64, (3,3), activation='relu')(x)
  x = MaxPooling2D((2,2))(x)
  
  x = Flatten()(x)
  x = Dropout(0.25)(x)
  x = Dense(512, activation='relu')(x)
  x = Dropout(0.5)(x)
  x = Dense(10, activation='softmax')(x)
  
  model= Model(img_input, x)
  return model

&lt;/denchmark-code&gt;

If I write the  as I have shown above in the original post, which I think is the right way to do it, then it throws the error. If you need to run it by yourself, then you can check out this notbeook: &lt;denchmark-link:https://colab.research.google.com/drive/1mBf1mErwN0lDEFqu6fQI-SpK7PpSp5Xl&gt;https://colab.research.google.com/drive/1mBf1mErwN0lDEFqu6fQI-SpK7PpSp5Xl&lt;/denchmark-link&gt;

		</comment>
		<comment id='17' author='AakashKumarNain' date='2019-08-07T14:33:42Z'>
		This functional model is completely legitimate because you indeed created two different Conv2D layers, one for input shape (224,224,3), one for (224,224,32)....The subclass model will not work because you're reusing the same layer
		</comment>
		<comment id='18' author='AakashKumarNain' date='2019-08-07T15:21:54Z'>
		I think we are still not on the same page. Let me try to elaborate it again:

I want to build a model using subclassing
In my model, I can have one particular type of layer repeated. For example, a Conv2D layer with 32 filters can be used in multiple places as shown in the above example.
If this isn't the right way to declare all the types of layer I am going to use in my model, can you please show me an example of how to do that?

		</comment>
		<comment id='19' author='AakashKumarNain' date='2019-08-07T20:47:00Z'>
		&lt;denchmark-link:https://github.com/AakashKumarNain&gt;@AakashKumarNain&lt;/denchmark-link&gt;
 Based on comments from &lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
 , here is the model (modified couple of lines in your code) that works. Please let's know what you think.
&lt;denchmark-code&gt;import os
import glob
import h5py
import math
import shutil


import numpy as np 
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.pylab as pl


from tensorflow.keras.models import Sequential, Model, load_model
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Activation
from tensorflow.keras.layers import Input, Flatten, SeparableConv2D, BatchNormalization
from tensorflow.keras.optimizers import Adam, SGD, RMSprop
from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping
from tensorflow.keras.utils import to_categorical

from tensorflow.keras import backend as K
import tensorflow as tf

class CustomModel(tf.keras.Model):
  def __init__(self, **kwargs):
    super(CustomModel, self).__init__(**kwargs)
    self.conv1   = Conv2D(32, (3, 3), padding='same')
    self.conv2   = Conv2D(32, (3, 3), padding='same')
    self.conv3   = Conv2D(64, (3, 3), padding='same')
    self.conv4   = Conv2D(64, (3, 3), padding='same')
    self.pool    = MaxPooling2D(pool_size=(2, 2))
    self.bn      = BatchNormalization()
    self.relu    = Activation("relu")
    self.softmax = Activation("softmax")
    self.drop1   = Dropout(0.25)
    self.drop2   = Dropout(0.5)
    self.dense1  = Dense(512)
    self.dense2  = Dense(10)
    self.flat    = Flatten()
    
  
  
  def call(self, inputs, train):
    z = self.conv1(inputs)
    z = self.bn(z, training=train)
    z = self.relu(z)
    
    z = self.conv2(z)
    z = self.bn(z, training=train)
    z = self.relu(z)
    z = self.pool(z)
    z = self.drop1(z, training=train)
    
    z = self.conv3(z)
    z = self.bn(z, training=train)
    z = self.relu(z)
    
    z = self.conv4(z)
    z = self.bn(z, training=train)
    z = self.relu(z)
    z = self.pool(z)
    z = self.drop1(z, training=train)
    
    z = self.flat(z)
    z = self.dense1(z)
    z = self.relu(z)
    z = self.drop2(z, training=train)
    z = self.dense2(z)
    z = self.softmax(z)
    
    return z

model =CustomModel()
random_input = np.random.rand(32,32, 3).astype(np.float32)
random_input = np.expand_dims(random_input, axis=0)
preds = model(random_input, train=False)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='20' author='AakashKumarNain' date='2019-08-08T05:55:29Z'>
		Thanks. I got that. The only thing is that it is not a very good design IMO because you are declaring  same thing with two diff variables in __init__.
Maybe, it's just the limitation as we have to check for the shapes. So, I will digest it for now anyways. Thanks again
		</comment>
		<comment id='21' author='AakashKumarNain' date='2020-08-04T09:36:16Z'>
		Hi &lt;denchmark-link:https://github.com/AakashKumarNain&gt;@AakashKumarNain&lt;/denchmark-link&gt;
, I was struggling with the same problem and it was strange to me to define "same" layer multiple times, too. However after realising I should do this, I implemented a workaround (which is not really nice, but it works). What I did was:
self.conv   = [Conv2D(32, (3, 3), padding='same') for layer in range(4)]
and then do
&lt;denchmark-code&gt;z = self.conv[0](z)
z = self.conv[1](z)
z = self.conv[2](z)
z = self.conv[3](z)
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>