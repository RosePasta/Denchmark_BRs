<bug id='44708' author='yakovdan' open_date='2020-11-09T17:08:56Z' closed_time='2020-12-05T01:51:06Z'>
	<summary>TFLite set_tensor fails unexpectedly in Tensorflow version 2.3.1</summary>
	<description>
System information

Have I written custom code: Yes
OS Platform and Distribution: Windows 10
Mobile device: Snapdragon 855
TensorFlow installed from: binary
TensorFlow version: 2.3.1
Python version: 3.7
Bazel version: None
GCC/Compiler version:None
CUDA/cuDNN version: None
GPU model and memory: None

I have a Keras model which I am able to convert to tflite both without quantization and with full integer quantization.
I'm then able to run inference using the interpreter and get the expected output, all using the code below. In addition, this works as expected when using the Hexagon delegate on a mobile device
When using Tensorflow version 2.2.0 this works. When using Tensorflow version 2.3.1, it does not work, with the following error:

ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type INT8 for input 0, name: input_1

I also have the following logs of the "input details" of the tflite intepreter:
TF version 2.3.1
non quantized:
input details:  [{'name': 'input_1', 'index': 0, 'shape': array([  1, 224, 224,   3]), 'shape_signature': array([ -1, 224, 224,   3]), 'dtype': &lt;class 'numpy.float32'&gt;, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
input shape:  [  1 224 224   3]
quantized:
input details:  [{'name': 'input_1', 'index': 0, 'shape': array([  1, 224, 224,   3]), 'shape_signature': array([ -1, 224, 224,   3]), 'dtype': &lt;class 'numpy.int8'&gt;, 'quantization': (0.007843137718737125, -1), 'quantization_parameters': {'scales': array([0.00784314], dtype=float32), 'zero_points': array([-1]), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
input shape:  [  1 224 224   3]
TF version 2.2.0
non quantized:
input details:  [{'name': 'input_1', 'index': 0, 'shape': array([  1, 224, 224,   3]), 'shape_signature': array([  1, 224, 224,   3]), 'dtype': &lt;class 'numpy.float32'&gt;, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
input shape:  [  1 224 224   3]
quantized:
input details:  [{'name': 'input_1', 'index': 235, 'shape': array([  1, 224, 224,   3]), 'shape_signature': array([  1, 224, 224,   3]), 'dtype': &lt;class 'numpy.float32'&gt;, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
input shape:  [  1 224 224   3]
also, see attached the input image, the reference images and the model.
I see that with version 2.2.0, the input layer of the quantized version is not quantized. However, the quantized model works fine on Hexagon DSP with Hexagon delegate. What's going on?
`
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/5512065/norm_images.zip&gt;norm_images.zip&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/5512071/dataset.zip&gt;dataset.zip&lt;/denchmark-link&gt;

&lt;denchmark-link:https://drive.google.com/file/d/12ORteaMaDInKpJl3xhlTwyoi0m_V2yvV/view?usp=sharing&gt;model.zip&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np
import cv2

def read_and_process_image(fname):
    img = cv2.imread(fname)
    img = cv2.resize(img,dsize = (224,224),interpolation = cv2.INTER_LANCZOS4)
    img = img / 127.5
    img = img - 1
    img = img.astype(np.float32)
    img = img.reshape(1,224,224,3)
    return img

def rep_data_gen0():
    a = []
    for i in range(128):
        img = np.fromfile('norm_images/image_'+str(i)+".bin",dtype=np.float32)
        img = img.reshape(224,224,3)
        a.append(img)
    
    a = np.array(a)
    img = tf.data.Dataset.from_tensor_slices(a).batch(1)
    for i in img.take(128):
        yield [i]


 def convert(model):
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    tflite_model = converter.convert()
    return tflite_model

def quantize(model):
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.representative_dataset = rep_data_gen0
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.inference_input_type = tf.int8  # or tf.uint8
    converter.inference_output_type = tf.int8  # or tf.uint8

    print("Converting using full integer quantization")
    quant_model = converter.convert()
    return quant_model

def excute_tflite(model,image):
    interpreter = tf.lite.Interpreter(model_content=model)
    interpreter.allocate_tensors()
    # Get input and output tensors.
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    input_shape = input_details[0]['shape']
    print ("input details: ", input_details)
    print("input shape: ", input_shape)
    interpreter.set_tensor(input_details[0]['index'], image)
    interpreter.invoke()

    tflite_results = interpreter.get_tensor(output_details[0]['index'])
    tflite_results_img = tflite_results[0, :, :, 0]
    return tflite_results_img

def main_func():
    img = read_and_process_image("dataset/COCO_train2014_000000003860.jpg")
    keras_model = tf.keras.models.load_model('sod_latest.hdf5')
    tf_model = convert(keras_model)
    quant01_model = quantize(keras_model)

    result = keras_model.predict(img)
    result_img = result[0,:,:,0]

    result_tf_img = excute_tflite(tf_model,img)
    result_quant_img0 = excute_tflite(quant01_model, img)

    print(np.min(result_img), np.max(result_img))
    print(np.min(result_tf_img), np.max(result_tf_img))
    print(np.min(result_quant_img0), np.max(result_quant_img0))

    cv2.imshow('keras',result_img)
    cv2.resizeWindow('keras', 400, 400)
    cv2.imshow('tflite', result_tf_img)
    cv2.resizeWindow('tflite', 400, 400)
    cv2.imshow('quantized tflite', result_quant_img0)
    cv2.resizeWindow('quantized tflite', 400, 400)
    cv2.waitKey(0)

if __name__ == "__main__":
    print(tf.__version__)
    main_func()
&lt;/denchmark-code&gt;

`
	</description>
	<comments>
		<comment id='1' author='yakovdan' date='2020-11-10T03:13:08Z'>
		I have tried in colab with TF version 2.3, nightly version() and was able to reproduce the issue. Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/0c936ae628894ef6cefba54a0ade47c6/untitled503.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='2' author='yakovdan' date='2020-11-12T04:29:15Z'>
		In TF 2.2 i am not seeing any value error.Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/064c1b4e0ba8cde69cef8446e26028a9/untitled515.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='3' author='yakovdan' date='2020-11-12T17:16:06Z'>
		I've attached the wrong model by mistake. Please try this:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/5532127/test_model.zip&gt;test_model.zip&lt;/denchmark-link&gt;

There is a second issue here: note that when converting to a quantized tflite model in TF 2.2.0, the input layer's type is float32 despite setting the input layer to be int8
		</comment>
		<comment id='4' author='yakovdan' date='2020-11-17T13:20:40Z'>
		Is there any update on this?
		</comment>
		<comment id='5' author='yakovdan' date='2020-11-17T19:08:44Z'>
		Your code is passing float input to the Interpreter as the error message shows.
You need to pass int8 input for the quantized model.
		</comment>
		<comment id='6' author='yakovdan' date='2020-11-17T19:10:14Z'>
		One more thing (not sure it is causing problem or not) but you should do same preprocessing while getting representative data, looks like you're not (compare read_and_process_image and  rep_data_gen0)
		</comment>
		<comment id='7' author='yakovdan' date='2020-11-28T01:26:03Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='8' author='yakovdan' date='2020-12-05T01:51:04Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='9' author='yakovdan' date='2020-12-05T01:51:07Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44708&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44708&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>