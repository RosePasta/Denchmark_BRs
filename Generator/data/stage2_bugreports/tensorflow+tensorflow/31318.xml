<bug id='31318' author='AlinaYablokova' open_date='2019-08-04T09:51:50Z' closed_time='2020-03-04T00:42:18Z'>
	<summary>InvalidArgumentError: Cannot assign a device for operation embedding_1</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

I have written custom code (as opposed to using a stock example script provided in TensorFlow):
OS version: Fedora 29.5.1.18 (also tested Ubuntu 18.10)
TensorFlow installed from (source or binary): tensorflow/tensorflow:latest-gpu-py3-jupyter
TensorFlow version (use command below): 1.14.0
Python version: 3.6.8
CUDA/cuDNN version: 10.0.130
GPU model and memory: GeForce RTX 2080 ti, 11 Gb

Describe the current behavior
I'm using keras. I try to to fit model that contains Embedding layer. When I call model.fit_generator(...) I get an error:
InvalidArgumentError: Cannot assign a device for operation embedding/embeddings/Initializer/random_uniform/sub: Could not satisfy explicit device specification '' because the node {{colocation_node embedding/embeddings/Initializer/random_uniform/sub}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:GPU:0].
Colocation Debug Info:
Colocation group had the following types and supported devices:
Root Member(assigned_device_name_index=1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: GPU CPU XLA_CPU XLA_GPU
Const: GPU CPU XLA_CPU XLA_GPU
ResourceSparseApplyRMSProp: CPU
RandomUniform: GPU CPU XLA_CPU XLA_GPU
ReadVariableOp: GPU CPU XLA_CPU XLA_GPU
Sub: GPU CPU XLA_CPU XLA_GPU
Add: GPU CPU XLA_CPU XLA_GPU
Mul: GPU CPU XLA_CPU XLA_GPU
VarIsInitializedOp: GPU CPU XLA_CPU XLA_GPU
VarHandleOp: GPU CPU XLA_CPU XLA_GPU
AssignVariableOp: GPU CPU XLA_CPU XLA_GPU
ResourceGather: GPU CPU XLA_CPU XLA_GPU_
ResourceSparseApplyRMSProp looks strange for me.
After getting this error I cannot fit new (simplified) model, because I get this error again. I get this error even I run tensorflow.keras.backend.get_value(model.optimizer.lr)
Describe the expected behavior
Model fits without any problems, like the same model without Embedding layer (no dest_input).
Code to reproduce the issue
&lt;denchmark-code&gt;lstm_input = Input(shape=(30, 5,))
steady_input = Input(shape=(3,),
                    name='steady_float') #тут None в shape не нужен?
dest_input = Input(shape=(1,), name='steady_dest')
ns_input = Input(shape=(1,))

x1 = layers.Bidirectional(layers.LSTM(512, activation='relu', return_sequences=True))(lstm_input)
x1 = layers.Bidirectional(layers.LSTM(256, activation='relu'))(x1)
x1 = Model(inputs=lstm_input, outputs=x1)

x2 = layers.Dense(512, activation="relu")(steady_input)
x2 = Model(inputs=steady_input, outputs=x2)

x3 = layers.Embedding(12, 3)(dest_input)
x3 = layers.Flatten()(x3)
x3 = layers.Dense(512, activation="relu")(x3)
x3 = Model(inputs=dest_input, outputs=x3)

x = layers.concatenate([x1.output, x2.output, x3.output])
x = layers.Dense(128, activation='relu')(x)

y1_output_tensor = layers.Dense(5, name='y1')(x)
y2_output_tensor = layers.Dense(5, name='y2')(x)
model = Model(inputs=[x1.input, x2.input, x3.input],
                         outputs=[y1_output_tensor, y2_output_tensor])

ep_n = 200
learning_rate = 0.001
decay_rate = learning_rate / ep_n
momentum = 0.7
model.compile(optimizer=RMSprop(lr=learning_rate, momentum=momentum, decay=decay_rate), loss=['mae', 'mae'])

#train_gen and test_get - simple generatora with shuffle
batch_size = 128
history = model.fit_generator(train_gen,
                              steps_per_epoch=1000,
                              epochs=ep_n,
                              validation_data=test_gen,
                              validation_steps=X_test.shape[0]//batch_size)
&lt;/denchmark-code&gt;

Other info / logs
Some times I get this error on simplified model:
&lt;denchmark-code&gt;# define two sets of inputs
inputA = Input(shape=(1,))
inputB = Input(shape=(128,))
 
# the first branch operates on the first input
x = Embedding(1000, 3)(inputA)
x = layers.Flatten()(x)
x = layers.Dense(4096, activation="relu")(x)
x = layers.Dense(2048, activation="relu")(x)
x = layers.Dense(1024, activation="relu")(x)
x = layers.Dense(512, activation="relu")(x)
x = layers.Dense(256, activation="relu")(x)
x = Dense(128, activation="relu")(x)
x = Dense(64, activation="relu")(x)
x = Model(inputs=inputA, outputs=x)
 
# the second branch opreates on the second input
y = Dense(1024, activation="relu")(inputB)
y = Dense(512, activation="relu")(y)
y = Dense(256, activation="relu")(y)
y = Dense(128, activation="relu")(y)
y = Dense(64, activation="relu")(y)
y = Model(inputs=inputB, outputs=y)
 
# combine the output of the two branches
combined = layers.concatenate([x.output, y.output])
 
# apply a FC layer and then a regression prediction on the
# combined outputs
z = Dense(2, activation="relu")(combined)
z = Dense(1, activation="linear")(z)
 
# our model will accept the inputs of the two branches and
# then output a single value
model = Model(inputs=[x.input, y.input], outputs=z)

ep_n = 10
learning_rate = 0.001
decay_rate = learning_rate / ep_n
momentum = 0.7
model.compile(optimizer=RMSprop(lr=learning_rate, momentum=momentum, decay=decay_rate

input_array_a = np.random.randint(1000, size=(500000, 1))
input_array_b = np.random.randint(32, size=(500000, 128))
output_array = np.random.randint(9, size=(500000, 1))

def generator_shuffle(x_a, x_b, y, batch_size=1024):
    max_index = len(x_a) - 1
    while 1:
        rows = np.random.randint(0, max_index, batch_size)
        yield [x_a[rows], x_b[rows]], y[rows]

tr_gen = generator_shuffle(input_array_a,
                           input_array_b,
                           output_array)

history = model.fit_generator(tr_gen, steps_per_epoch=3, epochs=10)
&lt;/denchmark-code&gt;

Probably, the issue occuring depends on CPU usage.
I'm attaching full log.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3464780/gpu_error.txt&gt;gpu_error.txt&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='AlinaYablokova' date='2019-08-04T09:57:33Z'>
		I tried yo use
&lt;denchmark-code&gt;sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,  log_device_placement=True))
KB.set_session(sess)
&lt;/denchmark-code&gt;

and got new error:
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

NotFoundError                             Traceback (most recent call last)
 in 
3                               epochs=ep_n,
4                               validation_data=test_gen,
----&gt; 5                               validation_steps=X_test.shape[0]//batch_size)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
1431         shuffle=shuffle,
1432         initial_epoch=initial_epoch,
-&gt; 1433         steps_name='steps_per_epoch')
1434
1435   def evaluate_generator(self,
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)
262
263       is_deferred = not model._is_compiled
--&gt; 264       batch_outs = batch_function(*batch_data)
265       if not isinstance(batch_outs, list):
266         batch_outs = [batch_outs]
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)
1173       self._update_sample_weight_modes(sample_weights=sample_weights)
1174       self._make_train_function()
-&gt; 1175       outputs = self.train_function(ins)  # pylint: disable=not-callable
1176
1177     if reset_metrics:
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in call(self, inputs)
3290
3291     fetched = self._callable_fn(*array_vals,
-&gt; 3292                                 run_metadata=self.run_metadata)
3293     self._call_fetch_callbacks(fetched[-len(self._fetches):])
3294     output_structure = nest.pack_sequence_as(
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in call(self, *args, **kwargs)
1456         ret = tf_session.TF_SessionRunCallable(self._session._session,
1457                                                self._handle, args,
-&gt; 1458                                                run_metadata_ptr)
1459         if run_metadata:
1460           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)
&lt;denchmark-h:h2&gt;NotFoundError: 2 root error(s) found.
(0) Not found: Resource localhost/embedding_1/embeddings/N10tensorflow3VarE does not exist.
[[{{node embedding_1/embedding_lookup}}]]
(1) Not found: Resource localhost/embedding_1/embeddings/N10tensorflow3VarE does not exist.
[[{{node embedding_1/embedding_lookup}}]]
[[RMSprop_1/RMSprop/update_embedding_1/embeddings/ResourceSparseApplyRMSProp/_184]]
0 successful operations.
0 derived errors ignored.&lt;/denchmark-h&gt;

		</comment>
		<comment id='2' author='AlinaYablokova' date='2019-08-04T10:03:17Z'>
		Probably, the same issue: &lt;denchmark-link:https://github.com/fizyr/keras-maskrcnn/issues/39&gt;fizyr/keras-maskrcnn#39&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='AlinaYablokova' date='2019-08-04T13:23:10Z'>
		Finaly, I solved my problem. I removed the definition of momentum for RMSProp.
Previous run (with error):
&lt;denchmark-code&gt;ep_n = 200
learning_rate = 0.001
decay_rate = learning_rate / ep_n
momentum = 0.7
model.compile(optimizer=RMSprop(lr=learning_rate, momentum=momentum, decay=decay_rate), loss=['mae', 'mae'])
&lt;/denchmark-code&gt;

Current run (successful):
&lt;denchmark-code&gt;model.compile(optimizer=RMSprop(), loss=['mae', 'mae'])
KB.get_value(model.optimizer.lr)
&lt;/denchmark-code&gt;

Another successful run:
&lt;denchmark-code&gt;ep_n = 200
learning_rate = 0.001
decay_rate = learning_rate / ep_n
model.compile(optimizer=RMSprop(lr=learning_rate, decay=decay_rate), loss=['mae', 'mae'])
&lt;/denchmark-code&gt;

So the problem is in momentum parameter.
		</comment>
		<comment id='4' author='AlinaYablokova' date='2019-08-05T09:15:32Z'>
		&lt;denchmark-link:https://github.com/AlinaYablokova&gt;@AlinaYablokova&lt;/denchmark-link&gt;
 Looks like you found workaround. Are you happy to close the issue. Thanks!
		</comment>
		<comment id='5' author='AlinaYablokova' date='2019-08-05T09:33:39Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 , yes, I have found workaround and I can continue my work now. But in my opinion, there are still a number of problems:

It's impossible to use Embedding layer and RMSProp with non-zero momentum together.
First workaround

&lt;denchmark-code&gt;sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,  log_device_placement=True))
KB.set_session(sess)
&lt;/denchmark-code&gt;

did not work:
_"NotFoundError: 2 root error(s) found.
(0) Not found: Resource localhost/embedding_1/embeddings/N10tensorflow3VarE does not exist.
[[{{node embedding_1/embedding_lookup}}]]
(1) Not found: Resource localhost/embedding_1/embeddings/N10tensorflow3VarE does not exist.
[[{{node embedding_1/embedding_lookup}}]]
[[RMSprop_1/RMSprop/update_embedding_1/embeddings/ResourceSparseApplyRMSProp/184]]
0 successful operations.
0 derived errors ignored."
		</comment>
		<comment id='6' author='AlinaYablokova' date='2019-08-06T11:34:59Z'>
		&lt;denchmark-link:https://github.com/AlinaYablokova&gt;@AlinaYablokova&lt;/denchmark-link&gt;
 Could you please provide the complete code to reproduce the reported issue. Thanks!
		</comment>
		<comment id='7' author='AlinaYablokova' date='2019-08-06T11:58:04Z'>
		&lt;denchmark-code&gt;import numpy as np

import tensorflow as tf
from tensorflow.keras import backend as KB
from tensorflow.keras import layers
from tensorflow.keras import Input
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import RMSprop

inputA = Input(shape=(1,))
inputB = Input(shape=(128,3,))
 
x = layers.Embedding(1000, 3)(inputA)
x = layers.Flatten()(x)
x = layers.Dense(256, activation="relu")(x)
x = layers.Dense(128, activation="relu")(x)
x = layers.Dense(64, activation="relu")(x)
x = Model(inputs=inputA, outputs=x)
 
y = layers.Bidirectional(layers.LSTM(128, activation='relu', return_sequences=True))(inputB)
y = layers.Bidirectional(layers.LSTM(64, activation='relu'))(y)
y = Model(inputs=inputB, outputs=y)

combined = layers.concatenate([x.output, y.output])
 
z = layers.Dense(2, activation="relu")(combined)
z = layers.Dense(1, activation="linear")(z)

t = layers.Dense(2, activation="relu")(combined)
t = layers.Dense(1, activation="linear")(t)
 
model = Model(inputs=[x.input, y.input], outputs=[z, t])
model.compile(RMSprop(lr=0.001, decay=0.05, momentum=0.7), ['mse', 'mse'])
#model.compile(RMSprop(lr=0.001, decay=0.05, momentum=0.0), ['mse', 'mse']) OK

input_array_a = np.random.randint(1000, size=(500000, 1))
input_array_b = np.random.randint(32, size=(500000, 128, 3))
output_array1 = np.random.randint(9, size=(500000, 1))
output_array2 = np.random.randint(9, size=(500000, 1))

def generator_shuffle(x_a, x_b, y1, y2, batch_size=128):
    max_index = len(x_a) - 1
    while 1:
        rows = np.random.randint(0, max_index, batch_size)
        yield [x_a[rows], x_b[rows]], [y1[rows], y2[rows]]

tr_gen = generator_shuffle(input_array_a,
                           input_array_b,
                           output_array1, output_array2)

history = model.fit_generator(tr_gen, steps_per_epoch=3, epochs=10)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='AlinaYablokova' date='2019-08-07T09:08:31Z'>
		I am able to reproduce the issue on Colab with Tensorflow 1.14.0 and TF-Nightly version '1.15.0-dev20190821'. Please take a look at colab &lt;denchmark-link:https://colab.research.google.com/drive/1P35gSdWJJ6IemiwkUqKuQa071oaJS5Xe&gt;gist&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='9' author='AlinaYablokova' date='2020-03-04T00:42:18Z'>
		This is fixed with tf-nightly version '2.2.0-dev20200303'. Thanks!
		</comment>
		<comment id='10' author='AlinaYablokova' date='2020-03-04T00:42:20Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31318&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31318&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='AlinaYablokova' date='2020-03-06T19:45:43Z'>
		
I tried yo use
sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,  log_device_placement=True))
KB.set_session(sess)

and got new error:
NotFoundError Traceback (most recent call last)
in
3 epochs=ep_n,
4 validation_data=test_gen,
----&gt; 5 validation_steps=X_test.shape[0]//batch_size)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
1431 shuffle=shuffle,
1432 initial_epoch=initial_epoch,
-&gt; 1433 steps_name='steps_per_epoch')
1434
1435 def evaluate_generator(self,
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)
262
263 is_deferred = not model._is_compiled
--&gt; 264 batch_outs = batch_function(*batch_data)
265 if not isinstance(batch_outs, list):
266 batch_outs = [batch_outs]
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)
1173 self._update_sample_weight_modes(sample_weights=sample_weights)
1174 self._make_train_function()
-&gt; 1175 outputs = self.train_function(ins) # pylint: disable=not-callable
1176
1177 if reset_metrics:
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in call(self, inputs)
3290
3291 fetched = self._callable_fn(*array_vals,
-&gt; 3292 run_metadata=self.run_metadata)
3293 self._call_fetch_callbacks(fetched[-len(self._fetches):])
3294 output_structure = nest.pack_sequence_as(
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in call(self, *args, **kwargs)
1456 ret = tf_session.TF_SessionRunCallable(self._session._session,
1457 self._handle, args,
-&gt; 1458 run_metadata_ptr)
1459 if run_metadata:
1460 proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)
NotFoundError: 2 root error(s) found.
(0) Not found: Resource localhost/embedding_1/embeddings/N10tensorflow3VarE does not exist.
[[{{node embedding_1/embedding_lookup}}]]
(1) Not found: Resource localhost/embedding_1/embeddings/N10tensorflow3VarE does not exist.
[[{{node embedding_1/embedding_lookup}}]]
[[RMSprop_1/RMSprop/update_embedding_1/embeddings/ResourceSparseApplyRMSProp/_184]]
0 successful operations.
0 derived errors ignored.

InvalidArgumentError: Cannot assign a device for operation conv2d_1/kernel/IsInitialized/VarIsInitializedOp: node conv2d_1/kernel/IsInitialized/VarIsInitializedOp (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748)  was explicitly assigned to /job:worker/replica:0/task:0/device:TPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0 ]. Make sure the device specification refers to a valid device.
[[conv2d_1/kernel/IsInitialized/VarIsInitializedOp]]
		</comment>
		<comment id='12' author='AlinaYablokova' date='2020-03-06T19:46:29Z'>
		
@AlinaYablokova Could you please provide the complete code to reproduce the reported issue. Thanks!
help-me

InvalidArgumentError: Cannot assign a device for operation conv2d_1/kernel/IsInitialized/VarIsInitializedOp: node conv2d_1/kernel/IsInitialized/VarIsInitializedOp (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748)  was explicitly assigned to /job:worker/replica:0/task:0/device:TPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0 ]. Make sure the device specification refers to a valid device.
[[conv2d_1/kernel/IsInitialized/VarIsInitializedOp]]
		</comment>
	</comments>
</bug>