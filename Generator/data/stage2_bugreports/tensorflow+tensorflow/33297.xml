<bug id='33297' author='ageron' open_date='2019-10-13T04:16:30Z' closed_time='2020-01-10T18:52:21Z'>
	<summary>Surprising random seed behavior when using @tf.function</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOSX 10.14.6
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0
Python version: 3.7.4
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A

Describe the current behavior
Random seeds work in surprising ways in TF 2.0 when using @tf.function. In particular, the value of the global random seed is only taken into account when a function is traced, not when it is called. This is surprising and different from TF 1.x behavior.
Describe the expected behavior
I expect the value of the global random seed to be taken into account every time a pseudo-random number is generated.
Code to reproduce the issue
@tf.function
def rnd():
    return tf.random.uniform(shape=[])

tf.random.set_seed(42)
print(rnd()) # The rnd() function's seed is generated randomly now, based on
print(rnd()) # the current random seed (which is 42).
print()

tf.random.set_seed(43) # resets the random sequence but ignores this seed!
print(rnd())
print(rnd())
print()

tf.random.set_seed(42) # resets the random sequence but ignores this seed!
print(rnd())
print(rnd())
print()
The output value is:
&lt;denchmark-code&gt;tf.Tensor(0.63789964, shape=(), dtype=float32)
tf.Tensor(0.8774011, shape=(), dtype=float32)

tf.Tensor(0.63789964, shape=(), dtype=float32)
tf.Tensor(0.8774011, shape=(), dtype=float32)

tf.Tensor(0.63789964, shape=(), dtype=float32)
tf.Tensor(0.8774011, shape=(), dtype=float32)
&lt;/denchmark-code&gt;

Notice that we get the same sequence of random numbers every time, ignoring the value of the global random seed. The only value that matters is the first one (when the function gets traced).
More code and examples of surprising behavior in &lt;denchmark-link:https://colab.research.google.com/drive/1C3LZkt5hfO6T2Uo2xaYVG8hiNQL8c3xu&gt;this colab&lt;/denchmark-link&gt;
.
Other info / logs
Specifically, I would expect the output to look the same as when the function is not decorated with @tf.function:
&lt;denchmark-code&gt;tf.Tensor(0.6645621, shape=(), dtype=float32)
tf.Tensor(0.68789124, shape=(), dtype=float32)

tf.Tensor(0.2733041, shape=(), dtype=float32)
tf.Tensor(0.5168259, shape=(), dtype=float32)

tf.Tensor(0.6645621, shape=(), dtype=float32)
tf.Tensor(0.68789124, shape=(), dtype=float32)
&lt;/denchmark-code&gt;

Note that the second sequence is different, as expected (in fact, the pseudo-random numbers should be identical whether the function is decorated or not, but that's a nice-to-have).
	</description>
	<comments>
		<comment id='1' author='ageron' date='2019-10-14T09:35:00Z'>
		&lt;denchmark-link:https://github.com/ageron&gt;@ageron&lt;/denchmark-link&gt;
,
Can you please refer the issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33034&gt;#33034&lt;/denchmark-link&gt;
 and let us know if it helps. Thanks!
		</comment>
		<comment id='2' author='ageron' date='2019-10-15T05:03:54Z'>
		Hi &lt;denchmark-link:https://github.com/rmothukuru&gt;@rmothukuru&lt;/denchmark-link&gt;
 ,
I looked at issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33034&gt;#33034&lt;/denchmark-link&gt;
 before filing this issue, I think it's a different problem. I am not using per-operation seeds in this issue.
		</comment>
		<comment id='3' author='ageron' date='2019-10-15T09:46:41Z'>
		Could reproduce the issue with TF Version 2.0. Here is the &lt;denchmark-link:https://colab.sandbox.google.com/gist/rmothukuru/faae8f0424cd0bcfcee29854140cab8c/random-seeds-in-tf-2-0.ipynb&gt;Gist&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='4' author='ageron' date='2019-10-15T12:01:54Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/wangpengmit&gt;@wangpengmit&lt;/denchmark-link&gt;
 FYI
Unfortunately, this is a known issue of tf.random.seed. We'll fix the documentation to help avoid the issue, and I'll post an example for how the original example can be updated so that it consistently works in tf.function as well..
		</comment>
		<comment id='5' author='ageron' date='2019-10-15T21:22:55Z'>
		Update: here is a method that currently works in TF2, and which will give you the expected results. Unfortunately, tf.set_seed and tf.random.uniform (and similar ops) are currently broken, although they will probably be either fixed or removed in the future:
&lt;denchmark-code&gt;@tf.function
def rnd(rng):
  return rng.uniform(shape=[])

rng = tf.random.experimental.Generator.from_seed(seed=42)
print(rnd(rng))
print(rnd(rng))
print()

rng = tf.random.experimental.Generator.from_seed(seed=43)
print(rnd(rng))
print(rnd(rng))
print()

rng = tf.random.experimental.Generator.from_seed(seed=42)
print(rnd(rng))
print(rnd(rng))
print()
&lt;/denchmark-code&gt;

Note: currently, calling the function with a new generator will causing it to be retraced (that is, will be slow the first time you call it); this will be fixed soon.
&lt;denchmark-link:https://github.com/yashk2810&gt;@yashk2810&lt;/denchmark-link&gt;
 it would be nice to make sure this snippet or something similar is visible in one of our tutorials or perhaps the docs for tf.random?
		</comment>
		<comment id='6' author='ageron' date='2019-10-15T21:33:41Z'>
		Its fixed now:

https://www.tensorflow.org/api_docs/python/tf/random/set_seed
https://www.tensorflow.org/api_docs/python/tf/random/uniform

		</comment>
		<comment id='7' author='ageron' date='2019-10-15T21:36:48Z'>
		We also link off to the notebooks where the particular symbol is being used.
See this for example: &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/random/uniform#used_in_the_guide&gt;https://www.tensorflow.org/api_docs/python/tf/random/uniform#used_in_the_guide&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='ageron' date='2019-10-16T01:50:52Z'>
		Thanks &lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/yashk2810&gt;@yashk2810&lt;/denchmark-link&gt;
 , the documentation is clearer now, and it's nice to have a workaround. I really like the  solution, and in fact I think I'll stop using  altogether, as well as op-level seeds. Their behavior is way too complicated and error-prone, IMHO.
Ideally, a function's behavior should not change when you decorate it with @tf.function, and things should be as simple and intuitive as possible. In NumPy, there is a single global random number generator (RNG), and all random ops use it by default. If users need other RNGs, they just create them. Works fine.
Here's a solution that works fine and which I find very simple and intuitive:
import tensorflow as tf

tf.random.experimental.Generator.set_seed = tf.random.experimental.Generator.reset_from_seed
tf.random_v2 = tf.random.experimental.Generator.from_non_deterministic_state()

@tf.function
def rng():
    return tf.random_v2.uniform(shape=[])

print(rng()) # prints ?1, different for every run of this program
print(rng()) # prints ?2

tf.random_v2.set_seed(42)
print(rng()) # prints A1
print(rng()) # prints A2

tf.random_v2.set_seed(43)
print(rng()) # prints B1
print(rng()) # prints B2

tf.random_v2.set_seed(42)
print(rng()) # prints A1
print(rng()) # prints A2
Everything works exactly like you would expect, and it gives the exact same result whether the rng() function is decorated with @tf.function or not.
The only problem is that reset_from_seed() does not support None. That's really something that should be fixed IMHO, it should generate a random state like in from_non_deterministic_state(), using non_deterministic_ints().
Next, if I want to use a specific RNG, I can do so easily:
@tf.function
def my_fn(rng):
    return rng.uniform(shape=[])

rng1 = tf.random.experimental.Generator.from_seed(42)
rng2 = tf.random.experimental.Generator.from_seed(43)

print(my_fn(rng1)) # prints A1
print(my_fn(rng1)) # prints A2

print(my_fn(rng2)) # prints B1
print(my_fn(rng2)) # prints B2

print(my_fn(rng1)) # prints A3
print(my_fn(rng1)) # prints A4

rng1.set_seed(42)
print(my_fn(rng1)) # prints A1
print(my_fn(rng1)) # prints A2
Again, this is simple and behaves exactly like you would expect, and just the same way with or without decorating the function with @tf.function.
It's a pity the reset() method explodes if I don't pass it a state argument. I would expect it to revert to its initial state instead. It should just save its initial state upon creation, and revert to that state when reset() is called without any argument.
Wdyt?
		</comment>
		<comment id='9' author='ageron' date='2019-10-16T12:24:37Z'>
		Thank you for the thorough suggestion!
Yes, it's absolutely crucial that code works the same way with or without tf.function - anything that doesn't is a bug.
I'll let Peng weigh in on a solution, he's the RNG expert. Personally I like your proposal because it's clean and minimizes the symbols we'd need to export: Generator is the sole RNG, the "easy" tf.random.uniform functions just offer access to the default generator and we can omit tf.stateless.stateless_uniform.
		</comment>
		<comment id='10' author='ageron' date='2019-10-16T13:59:16Z'>
		One thing I'm not sure about is how to add this to TF 2.0 while keeping backward compatibility. I see a few options:

Keep the existing tf.random and add tf.random_v2 in parallel, as I did. But it feels wrong to add yet a *_v2 just a few weeks after 2.0 is released, which had finally removed all the *_v2s that had accumulated in TF1.
Add tf.random.enable_simple_behavior() to make the new behavior opt-in. Not very elegant: shouldn't the simple behavior be the default? Perhaps make it the default behavior in TF 2.1?
Or just update tf.random to use the new behavior and consider that we're just fixing a bug. Since the output differs when you decorate the function with @tf.function, it is a bug. Since TF 2 was just released, there's probably not much production code out there anyway, and the impact is limited: it will just change the random sequences. But in that case the recently added documentation should be updated quickly.

		</comment>
		<comment id='11' author='ageron' date='2019-10-16T21:52:30Z'>
		Hi &lt;denchmark-link:https://github.com/ageron&gt;@ageron&lt;/denchmark-link&gt;
, your code is roughly what we have in mind for the future  and alike, if we choose to keep them.  seems like a useful feature which I'll consider. I'm hesitant to make  save the initial state to support an argument-less , since for basic facilities such  we need to be careful about memory usage. After all, the user can retrieve and store the initial state herself and use it to reset the generator later.
		</comment>
		<comment id='12' author='ageron' date='2019-10-17T03:40:00Z'>
		Hi &lt;denchmark-link:https://github.com/wangpengmit&gt;@wangpengmit&lt;/denchmark-link&gt;
 ,
Thanks for your feedback. Just to be clear, are you planning to go for option 3 in my previous comment? In other words, will the new (simpler) behavior replace the old one or will they live side by side (if so, how)?
Also, I'd like to suggest providing a set_seed() method in the Generator class that calls reset_from_seed() if a seed is provided or reset_from_non_deterministic_state() if not. IMHO, it would be more convenient, more intuitive and less verbose.
Regarding the initial state, I see your point, it makes sense. Perhaps the initial state could be saved by default when creating the Generator (since most people won't create millions of Generators, RAM usage should be fine) and have an option in the constructor such as save_initial_state=True so that people who need the extra RAM can set this to False if they want? Or perhaps set it to False by default and make the reset() method print our a clear error message if the user doesn't provide a state: "You need to set save_initial_state=True when constructing the Generator if you want to be able to call reset without any argument". Not sure about this.
		</comment>
		<comment id='13' author='ageron' date='2019-10-17T12:16:58Z'>
		&lt;denchmark-link:https://github.com/ageron&gt;@ageron&lt;/denchmark-link&gt;
 Your @ not me :)
		</comment>
		<comment id='14' author='ageron' date='2019-10-17T20:45:25Z'>
		We are pretty sure the old behavior is wrong and will go away. We haven't decided whether to remove symbols such as tf.random.uniform and only provide tf.random.Generator, or to give tf.random.uniform the new behavior. Thank you for the other suggestions! We'll consider them.
		</comment>
		<comment id='15' author='ageron' date='2019-10-18T05:44:54Z'>
		Thanks for your feedback. Please don't remove tf.random.uniform() and other random functions: there's really no need to break everyone's existing code, IMHO. Most people won't even notice the change. I think this modification is mostly an implementation detail and a bug fix, it doesn't justify changing the API much. Just adding a warning in the release notes explaining what's changed and the motivation would be sufficient.
Moreover, I would recommend making the default Generator available directly through the API. Something like tf.random.default_generator() or tf.random.get_default_generator(), for example in case someone needs to pass a Generator to a function, and they want to pass the default one.
Thanks for your work, it's a really nice improvement!
		</comment>
		<comment id='16' author='ageron' date='2019-10-18T18:21:27Z'>
		We already exported tf.random.experimental.get_global_generator (


tensorflow/tensorflow/python/ops/stateful_random_ops.py


         Line 682
      in
      ba96c40






 def get_global_generator(): 




). Is it the same as the tf.random.get_default_generator you have in mind?
		</comment>
		<comment id='17' author='ageron' date='2019-10-19T01:55:50Z'>
		Ah yes, that's exactly what I had in mind, thanks &lt;denchmark-link:https://github.com/wangpengmit&gt;@wangpengmit&lt;/denchmark-link&gt;
 . 
		</comment>
		<comment id='18' author='ageron' date='2019-10-29T22:54:41Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33297&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33297&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='19' author='ageron' date='2019-10-30T05:38:01Z'>
		Hey &lt;denchmark-link:https://github.com/wangpengmit&gt;@wangpengmit&lt;/denchmark-link&gt;
 ,
When do you expect  functions to be moved to , and when will  (and other random functions) move to using the global generator by default?
		</comment>
		<comment id='20' author='ageron' date='2019-10-30T17:56:43Z'>
		We don't have a timeline yet.
		</comment>
		<comment id='21' author='ageron' date='2019-10-31T06:30:26Z'>
		I understand. In the meantime, don't you want to leave this issue open? The Generator class is a great workaround, but as long as the default random functions don't use it, they're still kind of broken, IMHO.
		</comment>
		<comment id='22' author='ageron' date='2019-10-31T18:27:59Z'>
		Sure, I've reopened the issue.
		</comment>
		<comment id='23' author='ageron' date='2019-11-06T22:19:54Z'>
		Hi &lt;denchmark-link:https://github.com/ageron&gt;@ageron&lt;/denchmark-link&gt;
, there is a discussion about RNG migration plan: &lt;denchmark-link:https://groups.google.com/a/tensorflow.org/d/topic/developers/WLBbnQMB2Rw/discussion&gt;https://groups.google.com/a/tensorflow.org/d/topic/developers/WLBbnQMB2Rw/discussion&lt;/denchmark-link&gt;
 . We welcome your comments!
		</comment>
		<comment id='24' author='ageron' date='2019-11-07T02:59:25Z'>
		Thanks &lt;denchmark-link:https://github.com/wangpengmit&gt;@wangpengmit&lt;/denchmark-link&gt;
 ,
It looks good to me. I added a note about making it possible to revert to the old behavior if needed, but I understand if that's hard to do.
		</comment>
		<comment id='25' author='ageron' date='2020-01-10T18:52:21Z'>
		I'm closing this bug because after many discussions we've decided that we will leave the old RNGs unchanged, and discourage and eventually deprecate them in favor of the new RNGs and stateless RNGs. The main reasons include (1) we can't find a way to fully reproduce the old RNGs' behavior using the new mechanism and (2) we can't afford to change the old RNGs' behavior which will break a lot of users' code.
BTW there is a new guide for the recommended ways to generate random numbers in TF2: &lt;denchmark-link:https://www.tensorflow.org/guide/random_numbers&gt;https://www.tensorflow.org/guide/random_numbers&lt;/denchmark-link&gt;

		</comment>
		<comment id='26' author='ageron' date='2020-01-10T18:52:27Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33297&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33297&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>