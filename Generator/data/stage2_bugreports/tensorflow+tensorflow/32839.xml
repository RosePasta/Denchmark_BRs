<bug id='32839' author='kartik4949' open_date='2019-09-26T13:32:25Z' closed_time='2019-10-09T06:13:21Z'>
	<summary>Inputs to eager execution function cannot be Keras symbolic tensors</summary>
	<description>
Version:
2.0.0-rc
Python Version:
python 3.7
this is the shape check function!
'''@tf.function
def shape_check(input_channels,filters,bottom ,second):
shortcut = tf.cond(
tf.equal(input_channels, filters),
lambda :bottom,
lambda :second
)
return shortcut'''
The Error is poping here
this is a class function and i tried writing tf.cond() but it got different error!
'''def _basic_block(self, bottom, filters):
input_channels = tf.shape(bottom)[-1]
conv = self._conv_bn_activation(bottom, filters, 3, 1)
conv = self._conv_bn_activation(conv, filters, 3, 1)
input_channels = tf.shape(bottom)[-1]
shortcut = shape_check(input_channels,filters,bottom,self._conv_bn_activation(bottom, filters, 1, 1))
&lt;denchmark-code&gt;    return conv + shortcut'''
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='kartik4949' date='2019-09-27T06:09:02Z'>
		&lt;denchmark-link:https://github.com/kartik4949&gt;@kartik4949&lt;/denchmark-link&gt;
, In order to expedite the trouble-shooting process, please provide a standalone code to reproduce the issue reported here. Thanks!
		</comment>
		<comment id='2' author='kartik4949' date='2019-09-27T06:23:09Z'>
		&lt;denchmark-code&gt;# -*- coding: utf-8 -*-
"""
Created on Thu Sep 26 15:23:42 2019

@author: ACIPLE1088
"""
import tensorflow as tf
print(tf.__version__)
@tf.function
def shape_check(input_channels,filters,bottom ,second):
    bottom = tf.convert_to_tensor(bottom)
    second = tf.convert_to_tensor(second)
    shortcut = tf.cond(
                            tf.equal(input_channels, filters),
                            lambda :bottom,
                            lambda :second 
                        )
    return shortcut

class CenterNet:
    def __init__(self, input_shape=(300, 300, 3), activation=None, l2_reg=0.001, num_classes=3, weight_decay=None, batch_size=16, score_threshold=0.60, mode='train'):
        self.input_shape = input_shape
        self.num_classes = num_classes
        self.weight_decay = weight_decay
        self.batch_size = batch_size
        self.score_threshold = score_threshold
        self.mode = mode
        self.l2_reg = tf.keras.regularizers.l2(0.0005)
        if(activation is None):
            self.activation = self.mish
        else:
            self.activation = activation
    def build_model(self):
        #with tf.name_scope('backbone'):
        input_layer = tf.keras.layers.Input(shape=self.input_shape)
        conv = self._conv_bn_activation(
            input_layer, filters=16, kernel_size=7, strides=1)

        conv = self._conv_bn_activation(
            conv, filters=16, kernel_size=3, strides=1)

        conv = self._conv_bn_activation(
            conv, filters=32, kernel_size=3, strides=2)

        intermediate1 = self._dla_generator(conv, 64, 1, self._basic_block)
        intermediate1 = self._max_pooling(intermediate1, 2, 2)
        print(intermediate1.shape)


        dla_stage4 = self._dla_generator(intermediate1, 128, 1, self._basic_block)
        residual = self._conv_bn_activation(intermediate1, 128, 1, 1)
        residual = self._avg_pooling(residual, 2, 2)
        dla_stage4 = self._max_pooling(dla_stage4, 2, 2)

        dla_stage4 = dla_stage4 + residual
        print('dla_stage4 =',dla_stage4.shape)

        dla_stage5 = self._dla_generator(dla_stage4, 256, 1, self._basic_block)
        print('dla_stage5 =',dla_stage5.shape)
        residual = self._conv_bn_activation(dla_stage4, 256, 1, 1)
        residual = self._avg_pooling(residual, 2, 2)
        dla_stage5 = self._max_pooling(dla_stage5, 2, 2)
        dla_stage5 = dla_stage5 + residual
        print('dla_stage5 =',dla_stage5.shape)

        dla_stage6 = self._dla_generator(dla_stage5, 512, 1, self._basic_block)
        residual = self._conv_bn_activation(dla_stage5, 512, 1, 1)
        residual = self._avg_pooling(residual, 2, 2)
        dla_stage6 = self._max_pooling(dla_stage6, 2, 2)
        dla_stage6 = dla_stage6 + residual
        print('dla_stage6 =',dla_stage6.shape)
   #with tf.name_scope('upsampling'):
        dla_stage6 = self._conv_bn_activation(dla_stage6, 256, 1, 1)
        print('dla_stage6--- =',dla_stage6.shape)
        dla_stage6_5 = self._dconv_bn_activation(dla_stage6, 256, 4, 2)
        print('dla_stage6_5--- =',dla_stage6_5.shape)
        dla_stage6_4 = self._dconv_bn_activation(dla_stage6_5, 256, 4, 2)
        print('dla_stage6_4--- =',dla_stage6_4.shape)
        dla_stage6_3 = self._dconv_bn_activation(dla_stage6_4, 256, 4, 2)
        print('dla_stage6_3--- =',dla_stage6_3.shape)
        #dla_stage5 = self._conv_bn_activation(dla_stage5, 256, 1, 1)
        print('dla_stage--- =',dla_stage5.shape)
        dla_stage5 = tf.keras.layers.ZeroPadding2D()(dla_stage5)
        print('dla_stage--- =',dla_stage5.shape)

        #print( dla_stage4.shape,dla_stage5.shape,dla_stage6.shape ,dla_stage5.shape , dla_stage6_5.shape)
        dla_stage5_4 = self._conv_bn_activation(dla_stage5+dla_stage6_5, 256, 3, 1)


        dla_stage5_4 = self._dconv_bn_activation(dla_stage5_4, 256, 4, 2)
        dla_stage5_3 = self._dconv_bn_activation(dla_stage5_4, 256, 4, 2)

        dla_stage4 = self._conv_bn_activation(dla_stage4, 256, 1, 1)
        dla_stage4_3 = self._conv_bn_activation(dla_stage4+dla_stage5_4+dla_stage6_4, 256, 3, 1)
        dla_stage4_3 = self._dconv_bn_activation(dla_stage4_3, 256, 4, 2)

        features = self._conv_bn_activation(dla_stage6_3+dla_stage5_3+dla_stage4_3, 256, 3, 1)
        features = self._conv_bn_activation(features, 256, 1, 1)
        stride = 4.0

    #with tf.name_scope('center_detector'):
        keypoints = self._conv_bn_activation(features, self.num_classes, 3, 1, None)
        offset = self._conv_bn_activation(features, 2, 3, 1, None)
        size = self._conv_bn_activation(features, 2, 3, 1, None)

        model = tf.keras.Model(inputs = input_layer , outputs = [keypoints,offset,size])
        model.compile(experimental_run_tf_function=False)
        return model



    def visiualize_model(self):
        model = self.build_model()
        model.summary()

    def mish(self,inputs):
        return inputs * tf.math.tanh(tf.math.softplus(inputs))

    



    #@tf.function
    def _conv_bn_activation(self, input_layer, filters=16, kernel_size=7, strides=1):
        input_layer = tf.keras.layers.Conv2D(filters=filters,
                                             kernel_size=kernel_size, padding='same',
                                             strides=strides, kernel_initializer='he_uniform', kernel_regularizer=self.l2_reg)(input_layer)

        input_layer = tf.keras.layers.BatchNormalization()(input_layer)
        activation = tf.keras.layers.Activation(
            activation=self.activation)(input_layer)
        return activation
    
    #@tf.function    
    def _basic_block(self, bottom, filters):
        conv = self._conv_bn_activation(bottom, filters, 3, 1)
        conv = self._conv_bn_activation(conv, filters, 3, 1)
        input_channels = tf.shape(bottom)[-1]
        shortcut = shape_check(input_channels,filters,bottom,self._conv_bn_activation(bottom, filters, 1, 1))
        

        '''shortcut = tf.cond(
            tf.equal(input_channels, filters),
            lambda :bottom,
            lambda : self._conv_bn_activation(bottom, filters, 1, 1)
        )'''
        return conv + shortcut

    

    #@tf.function
    def _dla_generator(self, bottom, filters, levels, stack_block_fn):
        if levels == 1:
            block1 = stack_block_fn(bottom, filters)
            block2 = stack_block_fn(block1, filters)
            aggregation = block1 + block2
            aggregation = self._conv_bn_activation(aggregation, filters, 3, 1)
        else:
            block1 = self._dla_generator(bottom, filters, levels-1, stack_block_fn)
            block2 = self._dla_generator(block1, filters, levels-1, stack_block_fn)
            aggregation = block1 + block2
            aggregation = self._conv_bn_activation(aggregation, filters, 3, 1)
        return aggregation
    #@tf.function
    def _max_pooling(self , inputs , pool_size ,strides):
        
        return tf.keras.layers.MaxPool2D( pool_size = pool_size ,strides = strides, padding = 'same')(inputs)
    
    def _dconv_bn_activation(self, bottom, filters, kernel_size, strides):
        conv = tf.keras.layers.Conv2DTranspose(
            filters=filters,
            kernel_size=kernel_size,
            strides=strides,
            padding='same',
        )(bottom)

        bn = self._bn(conv)
        bn = self.mish(bn)
        return bn
    def _bn(self,inputs):
        return tf.keras.layers.BatchNormalization()(inputs)

    def _separable_conv_layer(self, bottom, filters, kernel_size, strides, activation=None):
        conv = tf.keras.layers.SeparableConv2D(
            filters=filters,
            kernel_size=kernel_size,
            strides=strides,
            padding='same',
            use_bias=False,
        )(bottom)
        bn = self._bn(conv)
        bn = self.mish(bn)
        return bn

    #@tf.function
    def _avg_pooling(self,inputs , pool_size ,strides):
         return tf.keras.layers.AveragePooling2D(pool_size = pool_size ,strides = strides , padding = 'same')(inputs)
    #@tf.function
    def _dropout(self,inputs,prob):
        return tf.keras.layers.Dropout(prob)(inputs)


if __name__ == '__main__':
    centernet = CenterNet()
    centernet.visiualize_model()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='kartik4949' date='2019-09-27T06:25:32Z'>
		
@kartik4949, In order to expedite the trouble-shooting process, please provide a standalone code to reproduce the issue reported here. Thanks!

&lt;denchmark-code&gt;# -*- coding: utf-8 -*-
"""
Created on Thu Sep 26 15:23:42 2019

@author: ACIPLE1088
"""
import tensorflow as tf
print(tf.__version__)
@tf.function
def shape_check(input_channels,filters,bottom ,second):
    bottom = tf.convert_to_tensor(bottom)
    second = tf.convert_to_tensor(second)
    shortcut = tf.cond(
                            tf.equal(input_channels, filters),
                            lambda :bottom,
                            lambda :second 
                        )
    return shortcut

class CenterNet:
    def __init__(self, input_shape=(300, 300, 3), activation=None, l2_reg=0.001, num_classes=3, weight_decay=None, batch_size=16, score_threshold=0.60, mode='train'):
        self.input_shape = input_shape
        self.num_classes = num_classes
        self.weight_decay = weight_decay
        self.batch_size = batch_size
        self.score_threshold = score_threshold
        self.mode = mode
        self.l2_reg = tf.keras.regularizers.l2(0.0005)
        if(activation is None):
            self.activation = self.mish
        else:
            self.activation = activation
    def build_model(self):
        #with tf.name_scope('backbone'):
        input_layer = tf.keras.layers.Input(shape=self.input_shape)
        conv = self._conv_bn_activation(
            input_layer, filters=16, kernel_size=7, strides=1)

        conv = self._conv_bn_activation(
            conv, filters=16, kernel_size=3, strides=1)

        conv = self._conv_bn_activation(
            conv, filters=32, kernel_size=3, strides=2)

        intermediate1 = self._dla_generator(conv, 64, 1, self._basic_block)
        intermediate1 = self._max_pooling(intermediate1, 2, 2)
        print(intermediate1.shape)


        dla_stage4 = self._dla_generator(intermediate1, 128, 1, self._basic_block)
        residual = self._conv_bn_activation(intermediate1, 128, 1, 1)
        residual = self._avg_pooling(residual, 2, 2)
        dla_stage4 = self._max_pooling(dla_stage4, 2, 2)

        dla_stage4 = dla_stage4 + residual
        print('dla_stage4 =',dla_stage4.shape)

        dla_stage5 = self._dla_generator(dla_stage4, 256, 1, self._basic_block)
        print('dla_stage5 =',dla_stage5.shape)
        residual = self._conv_bn_activation(dla_stage4, 256, 1, 1)
        residual = self._avg_pooling(residual, 2, 2)
        dla_stage5 = self._max_pooling(dla_stage5, 2, 2)
        dla_stage5 = dla_stage5 + residual
        print('dla_stage5 =',dla_stage5.shape)

        dla_stage6 = self._dla_generator(dla_stage5, 512, 1, self._basic_block)
        residual = self._conv_bn_activation(dla_stage5, 512, 1, 1)
        residual = self._avg_pooling(residual, 2, 2)
        dla_stage6 = self._max_pooling(dla_stage6, 2, 2)
        dla_stage6 = dla_stage6 + residual
        print('dla_stage6 =',dla_stage6.shape)
   #with tf.name_scope('upsampling'):
        dla_stage6 = self._conv_bn_activation(dla_stage6, 256, 1, 1)
        print('dla_stage6--- =',dla_stage6.shape)
        dla_stage6_5 = self._dconv_bn_activation(dla_stage6, 256, 4, 2)
        print('dla_stage6_5--- =',dla_stage6_5.shape)
        dla_stage6_4 = self._dconv_bn_activation(dla_stage6_5, 256, 4, 2)
        print('dla_stage6_4--- =',dla_stage6_4.shape)
        dla_stage6_3 = self._dconv_bn_activation(dla_stage6_4, 256, 4, 2)
        print('dla_stage6_3--- =',dla_stage6_3.shape)
        #dla_stage5 = self._conv_bn_activation(dla_stage5, 256, 1, 1)
        print('dla_stage--- =',dla_stage5.shape)
        dla_stage5 = tf.keras.layers.ZeroPadding2D()(dla_stage5)
        print('dla_stage--- =',dla_stage5.shape)

        #print( dla_stage4.shape,dla_stage5.shape,dla_stage6.shape ,dla_stage5.shape , dla_stage6_5.shape)
        dla_stage5_4 = self._conv_bn_activation(dla_stage5+dla_stage6_5, 256, 3, 1)


        dla_stage5_4 = self._dconv_bn_activation(dla_stage5_4, 256, 4, 2)
        dla_stage5_3 = self._dconv_bn_activation(dla_stage5_4, 256, 4, 2)

        dla_stage4 = self._conv_bn_activation(dla_stage4, 256, 1, 1)
        dla_stage4_3 = self._conv_bn_activation(dla_stage4+dla_stage5_4+dla_stage6_4, 256, 3, 1)
        dla_stage4_3 = self._dconv_bn_activation(dla_stage4_3, 256, 4, 2)

        features = self._conv_bn_activation(dla_stage6_3+dla_stage5_3+dla_stage4_3, 256, 3, 1)
        features = self._conv_bn_activation(features, 256, 1, 1)
        stride = 4.0

    #with tf.name_scope('center_detector'):
        keypoints = self._conv_bn_activation(features, self.num_classes, 3, 1, None)
        offset = self._conv_bn_activation(features, 2, 3, 1, None)
        size = self._conv_bn_activation(features, 2, 3, 1, None)

        model = tf.keras.Model(inputs = input_layer , outputs = [keypoints,offset,size])
        model.compile(experimental_run_tf_function=False)
        return model



    def visiualize_model(self):
        model = self.build_model()
        model.summary()

    def mish(self,inputs):
        return inputs * tf.math.tanh(tf.math.softplus(inputs))

    



    #@tf.function
    def _conv_bn_activation(self, input_layer, filters=16, kernel_size=7, strides=1):
        input_layer = tf.keras.layers.Conv2D(filters=filters,
                                             kernel_size=kernel_size, padding='same',
                                             strides=strides, kernel_initializer='he_uniform', kernel_regularizer=self.l2_reg)(input_layer)

        input_layer = tf.keras.layers.BatchNormalization()(input_layer)
        activation = tf.keras.layers.Activation(
            activation=self.activation)(input_layer)
        return activation
    
    #@tf.function    
    def _basic_block(self, bottom, filters):
        conv = self._conv_bn_activation(bottom, filters, 3, 1)
        conv = self._conv_bn_activation(conv, filters, 3, 1)
        input_channels = tf.shape(bottom)[-1]
        shortcut = shape_check(input_channels,filters,bottom,self._conv_bn_activation(bottom, filters, 1, 1))
        

        '''shortcut = tf.cond(
            tf.equal(input_channels, filters),
            lambda :bottom,
            lambda : self._conv_bn_activation(bottom, filters, 1, 1)
        )'''
        return conv + shortcut

    

    #@tf.function
    def _dla_generator(self, bottom, filters, levels, stack_block_fn):
        if levels == 1:
            block1 = stack_block_fn(bottom, filters)
            block2 = stack_block_fn(block1, filters)
            aggregation = block1 + block2
            aggregation = self._conv_bn_activation(aggregation, filters, 3, 1)
        else:
            block1 = self._dla_generator(bottom, filters, levels-1, stack_block_fn)
            block2 = self._dla_generator(block1, filters, levels-1, stack_block_fn)
            aggregation = block1 + block2
            aggregation = self._conv_bn_activation(aggregation, filters, 3, 1)
        return aggregation
    #@tf.function
    def _max_pooling(self , inputs , pool_size ,strides):
        
        return tf.keras.layers.MaxPool2D( pool_size = pool_size ,strides = strides, padding = 'same')(inputs)
    
    def _dconv_bn_activation(self, bottom, filters, kernel_size, strides):
        conv = tf.keras.layers.Conv2DTranspose(
            filters=filters,
            kernel_size=kernel_size,
            strides=strides,
            padding='same',
        )(bottom)

        bn = self._bn(conv)
        bn = self.mish(bn)
        return bn
    def _bn(self,inputs):
        return tf.keras.layers.BatchNormalization()(inputs)

    def _separable_conv_layer(self, bottom, filters, kernel_size, strides, activation=None):
        conv = tf.keras.layers.SeparableConv2D(
            filters=filters,
            kernel_size=kernel_size,
            strides=strides,
            padding='same',
            use_bias=False,
        )(bottom)
        bn = self._bn(conv)
        bn = self.mish(bn)
        return bn

    #@tf.function
    def _avg_pooling(self,inputs , pool_size ,strides):
         return tf.keras.layers.AveragePooling2D(pool_size = pool_size ,strides = strides , padding = 'same')(inputs)
    #@tf.function
    def _dropout(self,inputs,prob):
        return tf.keras.layers.Dropout(prob)(inputs)


if __name__ == '__main__':
    centernet = CenterNet()
    centernet.visiualize_model()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='kartik4949' date='2019-09-27T08:47:48Z'>
		&lt;denchmark-link:https://github.com/kartik4949&gt;@kartik4949&lt;/denchmark-link&gt;
, Thanks for providing the full code. I could replicate the issue on colab. Please see the &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/747ad1d17997ca02678ce45dcee2cc26/untitled167.ipynb&gt;gist here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='5' author='kartik4949' date='2019-09-27T10:27:46Z'>
		
@kartik4949, Thanks for providing the full code. I could replicate the issue on colab. Please see the gist here. Thanks!
Sweet!
i tried like everything to make it work!

		</comment>
		<comment id='6' author='kartik4949' date='2019-09-27T10:57:05Z'>
		
@kartik4949, Thanks for providing the full code. I could replicate the issue on colab. Please see the gist here. Thanks!

&lt;denchmark-code&gt;# -*- coding: utf-8 -*-
"""
Created on Thu Sep 26 15:23:42 2019

@author: ACIPLE1088
"""
import tensorflow as tf
print(tf.__version__)
l2_reg = tf.keras.regularizers.l2(0.0005)


def mish(inputs):
    return inputs * tf.math.tanh(tf.math.softplus(inputs))


class _conv_bn_activation(tf.keras.Model):
    def __init__(self, filters = 16 ,kernel_size = 7 ,strides=1 , activation=None):
        super().__init__()
        self.model = tf.keras.models.Sequential([
             tf.keras.layers.Conv2D(filters, kernel_size, strides=strides, padding='same', kernel_initializer='he_uniform', kernel_regularizer=l2_reg),
             tf.keras.layers.BatchNormalization(),
             tf.keras.layers.Activation(activation=mish)

                                                    ])
        
    def call(self,x,training=None):
        x = self.model(x,training)
        return x




class _basic_block(tf.keras.Model):
    def __init__(self, filters,**kwargs):
        super().__init__(**kwargs)
        self.filters = filters
        self.conv1 = _conv_bn_activation(self.filters, 3, 1)
        self.conv2 = _conv_bn_activation(self.filters, 3, 1)
        conv3 = _conv_bn_activation( self.filters, 1, 1)

        
    def call(self,x):
        x = self.conv1(x)
        input_channels = tf.shape(x)[-1]
        shortcut = tf.cond(tf.equal(input_channels,self.filters),
            lambda : x,
            lambda :self.conv2(x))
        '''if(input_channels==self.filters):
            shortcut = conv2
        else:
            shortcut = self.conv3(x ,training=training)'''
        return x + shortcut




class _dla_generator(tf.keras.layers.Layer):
    def __init__(self,   filters, levels, stack_block_fn,**kwargs):
        super().__init__(**kwargs)
        self.filters = filters
        self.levels = levels
        self.stack_block_fn = stack_block_fn( filters)
        self.conv1 = _conv_bn_activation( filters, 3, 1)
        

    def call(self,x):
        block1 = self.stack_block_fn(x)
        aggregation = self.conv1(block1 )
        return aggregation


class CenterNet(tf.keras.Model):
    def __init__(self ,l2_reg=0.001, num_classes=3, weight_decay=None, batch_size=16, score_threshold=0.60, mode='train',**kwargs):
        super().__init__(**kwargs)
        self.num_classes = num_classes
        self.weight_decay = weight_decay
        self.batch_size = batch_size
        self.score_threshold = score_threshold
        self.mode = mode
        self.l2_reg = tf.keras.regularizers.l2(0.0005)
        self.conv1 = _conv_bn_activation()
        self.basic =_basic_block(16)
        
        #self._dla_generator1 = _dla_generator(16,1,_basic_block)
        #self.model = tf.keras.models.Sequential(name='final')
        #self.model.add(basic)
    def call(self,x , training =None):
        #x = self.model(x,training = training)

        x = self.conv1(x,training = training)
        """
        If We donot build the model is gives error in tensorflow 2.0
        
        """
        self.basic.build(input_shape = x.shape)
        x = self.basic(x)
        return x

model = CenterNet()

model.build(input_shape = (None,300,300,3)) 
model.summary()
&lt;/denchmark-code&gt;

IS THIS GOOD??
THIS IS THE ONLY WORKING SOLUTION I CAN MAKE AT THIS MOMENT
SO BASICALLY IS MADE COMPOSITION LAYERS CLASSES RATHER THAN FUNCTION AS PREVIOUS
AND I MADE THEM AS SPERATE MODEL!
PLEASE REVIEW THEM!
,THANKS.
		</comment>
		<comment id='7' author='kartik4949' date='2019-10-02T22:40:08Z'>
		The above structure looks good. You can also refer to &lt;denchmark-link:https://www.tensorflow.org/guide/keras/custom_layers_and_models&gt;Writing custom layers and models with Keras&lt;/denchmark-link&gt;
 article.
Thanks!
		</comment>
		<comment id='8' author='kartik4949' date='2019-10-03T08:54:04Z'>
		Ok...so there is no issue in implementing model structure like this right? i mean performance?
		</comment>
		<comment id='9' author='kartik4949' date='2019-10-09T06:13:22Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32839&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32839&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>