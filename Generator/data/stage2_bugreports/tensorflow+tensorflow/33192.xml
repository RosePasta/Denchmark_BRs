<bug id='33192' author='charmasaur' open_date='2019-10-10T04:07:07Z' closed_time='2019-12-21T21:22:46Z'>
	<summary>Incorrect result when subtracting 1 from exponential of Variable</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.6
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
Python version: 3.7.3
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: Intel Iris Plus Graphics 640 1536 MB

Describe the current behavior
With the following setup
&lt;denchmark-code&gt;session = tf.Session()
variable = tf.Variable(4j)
exp = tf.exp(variable)
session.run(tf.global_variables_initializer())
&lt;/denchmark-code&gt;

the following code
&lt;denchmark-code&gt;print(session.run(exp-1))
print(session.run(exp)-1)
&lt;/denchmark-code&gt;

produces two different outputs, specifically:
&lt;denchmark-code&gt;(2.897081749192498+1.3258713481172573j)
(-1.6536436208636118-0.7568024953079282j)
&lt;/denchmark-code&gt;

Describe the expected behavior
That code should print identical arrays (the latter is correct, the former is not).
Code to reproduce the issue
&lt;denchmark-code&gt;session = tf.Session()
variable = tf.Variable(4j)
exp = tf.exp(variable)
session.run(tf.global_variables_initializer())

print(session.run(exp-1))
print(session.run(exp)-1)
&lt;/denchmark-code&gt;

Also see Colab notebook here: &lt;denchmark-link:https://colab.research.google.com/drive/1KT2gfuWeezhWOr3zfyHhk9HvCH7JcShm&gt;https://colab.research.google.com/drive/1KT2gfuWeezhWOr3zfyHhk9HvCH7JcShm&lt;/denchmark-link&gt;

Other info / logs
I have no idea what's happening, but some observations:

there needs to be a Variable involved. If I use a constant instead of a Variable, all works as expected.
the exponent value is important. If I change the 4's to 1's then all works as expected, for example.
the TF exponential is important. If instead I calculate the exponential with numpy and then set the Variable to that exponential, subtracting 1 works correctly.
specifically subtracting 1 seems important. If instead I subtract 2, or 0.1, or 1j, all works as expected.

	</description>
	<comments>
		<comment id='1' author='charmasaur' date='2019-10-11T09:14:46Z'>
		I have tried on colab with TF version 1.14, 1.15.0-rc3 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/dd87c7a8f850af52f1d667d1eac67899/untitled257.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='charmasaur' date='2019-10-13T08:00:30Z'>
		Exp and Exp-1 are two different values. How can they produce the same result.  print(session.run(exp-1)) produces the result of value which is equal to exp - 1.
print(session.run(exp)-1) produces the result equal to one subtracted from the result of value which equals exp. Hence they are two different. I guess there is no issue. It is working properly.
		</comment>
		<comment id='3' author='charmasaur' date='2019-10-13T09:44:03Z'>
		Sorry, I don't follow. In both cases you describe, the eventual value printed should be the value of e^(4j)-1.
		</comment>
		<comment id='4' author='charmasaur' date='2019-10-24T13:07:48Z'>
		It looks like a bug in Eigen, specifically expm1 seems to produce incorrect results for complex numbers with imaginary part 4 or greater. I've asked a few folks internally who are familiar with the code, but it would be nice to confirm this with the Eigen team.
The reason why the bug only manifests for variables is most likely a consequence of how the optimizer simplifies mathematical expressions.
		</comment>
		<comment id='5' author='charmasaur' date='2019-10-24T21:33:42Z'>
		Ah, nice find! It hadn't even occurred to me that exp(x)-1 might get simplified to a single function call, but in hindsight that makes sense.
		</comment>
		<comment id='6' author='charmasaur' date='2019-12-21T21:22:46Z'>
		Hi, I submitted
&lt;denchmark-link:https://gitlab.com/libeigen/eigen/merge_requests/18&gt;https://gitlab.com/libeigen/eigen/merge_requests/18&lt;/denchmark-link&gt;

in Eigen, which fixes Eigen's implementation of expm1.
This is being used in the current tf-nightly:



tf.math.expm1(4j)
&lt;tf.Tensor: shape=(), dtype=complex128, numpy=(-1.653643620863612-0.7568024953079282j)&gt;



This issue should be fixed now.
		</comment>
		<comment id='7' author='charmasaur' date='2019-12-21T21:22:48Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33192&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33192&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='charmasaur' date='2019-12-22T22:15:23Z'>
		Thanks!
		</comment>
		<comment id='9' author='charmasaur' date='2019-12-22T22:17:45Z'>
		Should we expect the next minor release (2.1.0, I guess) to have the fix?
		</comment>
		<comment id='10' author='charmasaur' date='2020-06-02T08:20:27Z'>
		Turns out 2.2.0 has the fix
		</comment>
	</comments>
</bug>