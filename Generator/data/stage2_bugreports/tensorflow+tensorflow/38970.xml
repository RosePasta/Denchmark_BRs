<bug id='38970' author='atodniAr' open_date='2020-04-28T06:45:57Z' closed_time='2020-08-26T15:25:47Z'>
	<summary>RaggedTensor raises error with Keras TimeDistributed Layer</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary, pip installed
TensorFlow version (use command below): pip install tensorflow-gpu==2.2.0rc3
Python version: 3.7.0
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: CUDA 10.1 / cuDNN 7.6.5
GPU model and memory: GTX 1080 Ti 11G / 128G RAM

Describe the current behavior
I builded a hierarchical lstm model for binary classification:
model = Sequential()
model.add(layers.TimeDistributed(layers.Masking(-1),input_shape=(None,20,1)))
model.add(layers.TimeDistributed(layers.LSTM(num_units_1,dropout=0.4)))
model.add(layers.LSTM(num_units_2))
model.add(layers.Dense(1))
model.summary()
I then generated my x_train as a tf.RaggedTensor, with shape [10000, None, 20, 1], each slice over the out most dimension of this ragged tensor is a tf.Tensor with shape [x, 20, 1].
The reason I have to use RaggedTensor is I can't figure out a way to do padding over the 2nd dimension. My each input sequence is a variable length sequence of variable length sequences, I padded the lower time dimension and then mask it with time distributed masking, but there seems no apparent way how to do masking before the 2nd lstm layer.
Now if run it with:
history = model.fit(train_data, epochs=epochs, verbose=1, steps_per_epoch=-(-sample_count//batch_size))
I'd get:

ValueError: All inputs to ConcreteFunctions must be Tensors; on invocation of __backward_standard_lstm_1179546, the 0-th input (IndexedSlices(indices=Tensor("gradient_tape/sequential_29/lstm_55/RaggedToTensor/boolean_mask_1/GatherV2:0", shape=(None,), dtype=int32), values=Tensor("gradient_tape/sequential_29/lstm_55/RaggedToTensor/boolean_mask/GatherV2:0", shape=(None, 32), dtype=float32), dense_shape=Tensor("gradient_tape/sequential_29/lstm_55/RaggedToTensor/Shape:0", shape=(2,), dtype=int32))) was not a Tensor.

Describe the expected behavior
TimeDistributed layer should support RaggedTensor if the underlying model can process slice of RaggedTensor and output dimension is matched.
Standalone code to reproduce the issue
import tensorflow as tf
from tensorflow.keras import layers, Model, Sequential
import numpy as np

x = tf.RaggedTensor.from_row_splits(np.ones((100,20,1)),[0,4,20,100])
y = np.ones((3,1))

model = Sequential()
model.add(layers.TimeDistributed(layers.LSTM(32,dropout=0.4),input_shape=(None,20,1)))
model.add(layers.LSTM(24))
model.add(layers.Dense(1,activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam')
history = model.fit(x=x, y=y, epochs=10, verbose=1)
	</description>
	<comments>
		<comment id='1' author='atodniAr' date='2020-04-28T14:27:31Z'>
		&lt;denchmark-link:https://github.com/atodniAr&gt;@atodniAr&lt;/denchmark-link&gt;
,
Was able to reproduce the issue with TF v2.1, &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/e17377657a6ae2efbaf000993cb9bd94/38970.ipynb#scrollTo=0W_BpvlRGGH1&gt;TF v2.2.0-rc3&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/daff2b4e3797d5729698833f2b67cd4a/38970-tf-nightly.ipynb&gt;TF-nightly&lt;/denchmark-link&gt;
. Please find the attached gist. Thanks!
		</comment>
		<comment id='2' author='atodniAr' date='2020-07-28T09:06:29Z'>
		&lt;denchmark-link:https://github.com/atodniAr&gt;@atodniAr&lt;/denchmark-link&gt;
  I can confirm that it is solved in TF v2.3 with a minor addition:
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow.keras import layers, Model, Sequential
import numpy as np

x = tf.RaggedTensor.from_row_splits(np.ones((100, 20, 1)), [0, 4, 20, 100])
y = np.ones((3, 1))

model = Sequential()
model.add(layers.Input(shape=(None, 20, 1), ragged=True)) #this input with ragged=True is important
model.add(layers.TimeDistributed(layers.LSTM(32, dropout=0.4)))
model.add(layers.LSTM(24))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam')
history = model.fit(x=x, y=y, epochs=10, verbose=1)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='atodniAr' date='2020-08-12T14:26:17Z'>
		&lt;denchmark-link:https://github.com/atodniAr&gt;@atodniAr&lt;/denchmark-link&gt;

I ran the code on tf-nightly (2.4.0-dev20200812) and the issue seems resolved, please verify with &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/dfd2481aa8142d108e34dc079d840805/untitled367.ipynb&gt;this gist&lt;/denchmark-link&gt;
, and confirm if the issue can be moved to closed status.
		</comment>
		<comment id='4' author='atodniAr' date='2020-08-19T14:27:35Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='5' author='atodniAr' date='2020-08-26T15:25:46Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='6' author='atodniAr' date='2020-08-26T15:25:48Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38970&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38970&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>