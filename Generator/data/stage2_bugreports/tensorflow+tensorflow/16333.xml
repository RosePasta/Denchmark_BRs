<bug id='16333' author='mholzel' open_date='2018-01-23T19:36:43Z' closed_time='2019-08-06T22:21:57Z'>
	<summary>Computing gradients of loop variables return None</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Windows 10 x64
Installed from binary
TensorFlow 1.3.0
Python 3.6.3
CuDNN 6.4.6, CUDA 8.0
NVIDIA GeForce 940M

&lt;denchmark-h:h3&gt;Bug Description&lt;/denchmark-h&gt;

Issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/15403&gt;15403&lt;/denchmark-link&gt;
 was closed since it was labelled as "not a bug or feature request". I believe it is indeed a bug.
The fundamental problem is that gradients do not seem to be working inside of tf.while_loops. Here is a demonstration of code run inside and outside of a while loop. The code outside of the loop produces a result, whereas the gradient inside of the loop returns None:
&lt;denchmark-code&gt;import tensorflow as tf

i = tf.constant(0)
x = tf.constant(3.0)
print("External gradient:", tf.gradients(x, x)[0])     # Prints Tensor("gradients/Fill:0", shape=(), dtype=float32)

def loop_body(i, x, y):
    print("internal gradient:", tf.gradients(x, y)[0]) # Prints None
    return i + 1, x, y

tf.while_loop( lambda i,x,y: tf.less(i, 5), loop_body, [i, x, x]);
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='mholzel' date='2018-01-23T20:50:39Z'>
		In the loop, x and y has no relationship.
&lt;denchmark-code&gt;i = tf.constant(0)
x = tf.constant(3.0)

# now x and i has no computation graph between them
print(tf.gradients(x, i)[0])    # prints None

def loop_body(i, x, y):
    y = x                       # if you add computation graph between x and y
    g = tf.gradients(x, y)[0]
    print(g)                    # prints something
    return i + 1, x, y

tf.while_loop(lambda i, *_: tf.less(i, 5), loop_body, [i, x, x])
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='mholzel' date='2018-01-23T21:05:05Z'>
		Then I am confused about something...
In the above code, the loop_vars are set to [i,x,x]. So I assumed that y=x in the loop_body. Why isn't that the case?
Is the while loop copying in the loop_vars instead of referencing them?
		</comment>
		<comment id='3' author='mholzel' date='2018-01-24T19:26:56Z'>
		&lt;denchmark-link:https://github.com/skye&gt;@skye&lt;/denchmark-link&gt;
 do you understand the Python gradient creation code well enough to understand what is/should be going on here?
		</comment>
		<comment id='4' author='mholzel' date='2018-01-27T19:05:53Z'>
		I think this is closed related to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/13616&gt;#13616&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='mholzel' date='2019-08-06T22:21:57Z'>
		This is fixed with the latest version of TensorFlow. Feel free to reopen if the issue still persists. Thanks!
		</comment>
	</comments>
</bug>