<bug id='41351' author='pbontrager' open_date='2020-07-13T21:38:41Z' closed_time='2020-07-16T05:05:26Z'>
	<summary>tf.image.random_saturation cpu implementation crashes on some images</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   Ubuntu 18.04.3 LTS
TensorFlow installed from (source or binary): Docker Image 'tensorflow/tensorflow:2.2.0-gpu'
TensorFlow version (use command below): 2.2.0
Python version: 3.6.9
CUDA/cuDNN version: CUDA Version 10.1.243
GPU model and memory: NVIDIA Titan RTX with 24gb of memory

Describe the current behavior
When using tf.image.random_saturation on the cpu (as part of a custom image augmentation pipeline), certain images will cause tf.image.random_saturation to crash. There is no error, the session simply freezes and cannot be exited. Ctr C has no effect and the terminal session needs to be quit. When running the function in GPU this error does not happen.
The images being fed into this function are imagenet images that have already been augmented with a Gaussian blur and it only seems to happen on less than 1% of the data. The images that cause the crash do not appear to be out of the ordinary, with all the values being float32 values between 0 and 1. The only notable pattern being that they all have at least one pixel equal to exactly 0.0. This alone though is not enough to recreate the failure.
Describe the expected behavior
The expected behavior is that this function behaves the same as on the gpu and does not crash from an rgb image where all the values are floats between 0 and 1.

Download and extract the file provided to get bad_image.npy to use with the code snippet below.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4915306/bad_image.npy.zip&gt;bad_image.npy.zip&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np
inp = np.load("bad_image.npy")
with tf.device('/cpu:0'):
    tf_inp = tf.convert_to_tensor(inp)
    test = tf.image.random_saturation(tf_inp, .2, 1.8)
print(test)
&lt;/denchmark-code&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
Current workaround: wrap the input to tf.image.random_saturation with tf.clip_by_value(x, 1e-6, 1)
	</description>
	<comments>
		<comment id='1' author='pbontrager' date='2020-07-14T15:13:42Z'>
		&lt;denchmark-link:https://github.com/pbontrager&gt;@pbontrager&lt;/denchmark-link&gt;

I ran the code on gpu and cpu and do not find any difference on the colab, please find the gist for &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/89c4a7626425ff4cad0e352e37e519f7/untitled276.ipynb&gt;gpu&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/86a481f09ddb68e83665a8d370e83b1a/untitled275.ipynb&gt;cpu here&lt;/denchmark-link&gt;
. I do not face any freezing, please verify and let me know if i am doing something incorrect.
		</comment>
		<comment id='2' author='pbontrager' date='2020-07-14T20:12:41Z'>
		I too didn't face any issues while running the code on both CPU and GPU.

check this link and see if you can find anything, Maybe this is happening because of the docker image.

		</comment>
		<comment id='3' author='pbontrager' date='2020-07-14T20:45:58Z'>
		I'll try to reproduce this on another machine. It may be the docker image, or a different dependency that tf random_saturation relies on
		</comment>
		<comment id='4' author='pbontrager' date='2020-07-15T08:39:22Z'>
		&lt;denchmark-link:https://github.com/pbontrager&gt;@pbontrager&lt;/denchmark-link&gt;

Please update us after you verify.
		</comment>
		<comment id='5' author='pbontrager' date='2020-07-15T22:07:18Z'>
		Okay, after testing this on a few machines, using the identical docker image and NVIDIA Drivers, I cannot reproduce this issue unless I'm on the original machine. As I cannot reproduce this but outside of one machine, I guess there is no more that can be done with this at this time.
		</comment>
		<comment id='6' author='pbontrager' date='2020-07-16T05:05:26Z'>
		&lt;denchmark-link:https://github.com/pbontrager&gt;@pbontrager&lt;/denchmark-link&gt;

Thank you for the update. Closing this issue for now. Please feel free to reopen the issue if any new information is found. Thanks!
		</comment>
		<comment id='7' author='pbontrager' date='2020-07-16T05:05:28Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41351&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41351&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>