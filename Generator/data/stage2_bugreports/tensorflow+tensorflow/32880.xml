<bug id='32880' author='durandg12' open_date='2019-09-27T17:06:50Z' closed_time='2019-10-01T17:02:41Z'>
	<summary>deprecation warning when training a simple embedding with custom function</summary>
	<description>

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6
TensorFlow installed from (source or binary): pip install tensorflow==2.0.0-beta1
TensorFlow version (use command below): v2.0.0-beta1-5101-gc75bb66a99 2.0.0-rc0
Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14

Describe the current behavior
The following code trains a simple embedding on temporal indices, Z, to match a given time series X:
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow.keras import Model, losses, optimizers
from tensorflow.keras.layers import Embedding
import numpy as np


def extract(indices, X): 
    # extract the lines of X given by the indices
    # if indices.shape = 4
    # and X.shape = (20, 5)
    # then the output has shape (4, 5)
    indices = tf.reshape(indices, [-1, 1])
    return tf.gather_nd(X, indices)


class Test(Model):

    def __init__(self, X, **kwargs):
        super(Test, self).__init__(**kwargs)
        self.X = X

        self.optimizer = optimizers.Adam(learning_rate=1e-3)
        self.loss = losses.CategoricalCrossentropy()

        self.Z = Embedding(X.shape[0], X.shape[1])

    def call(self, inputs, training=None):
        # inputs are temporal indices
        Z_t = self.Z(inputs)
        return Z_t

    def custom_train(self):

        @tf.function
        def _train_Step(index):
            with tf.GradientTape() as tape:
                Z_t = self(index, training=True)
                X_t = extract(index, self.X)
                loss = self.loss(X_t, Z_t)

            grads = tape.gradient(loss, self.trainable_variables)
            self.optimizer.apply_gradients(zip(grads, self.trainable_variables))
            return
        # this is a toy exemple: we simply make one training step on the index 0
        _train_Step(tf.constant([0]))
        return 
    
T = 20
n = 5
X = np.random.rand(T, n)
model = Test(X=X)
model.custom_train()
&lt;/denchmark-code&gt;

and I get the following warning about the use of a deprecated method:
&lt;denchmark-code&gt;WARNING: Logging before flag parsing goes to stderr.
W0927 19:04:48.957805 140735485318016 deprecation.py:323] From /path/to/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where

&lt;/denchmark-code&gt;

This reminds me about &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29881&gt;an other issue&lt;/denchmark-link&gt;
 I opened with a lot of deprecation warnings arising when I use the API. This issue is closed but the warnings are still here (and, in fact, the release of the rc0 has brought a new one, see my last comment there).
	</description>
	<comments>
		<comment id='1' author='durandg12' date='2019-09-27T20:01:12Z'>
		Was able to reproduce the issue. The github gist is &lt;denchmark-link:https://colab.sandbox.google.com/gist/gowthamkpr/3b8b4b959ee44861ce8c20c3ff399559/untitled150.ipynb&gt;here&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='durandg12' date='2019-10-01T17:02:40Z'>
		The warning is not here in tf2.0.0 so I can close this issue.
		</comment>
		<comment id='3' author='durandg12' date='2019-10-01T17:02:42Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=32880&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=32880&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>