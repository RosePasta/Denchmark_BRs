<bug id='22497' author='eliorc' open_date='2018-09-25T09:06:29Z' closed_time='2018-10-23T10:29:01Z'>
	<summary>DataLoss error on TFRecords - happens on one machine doesn't on other</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;

&lt;denchmark-h:h4&gt;System 1 (Bug DOESN'T occur)&lt;/denchmark-h&gt;

All details are from inside the docker which the codes run in


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): True


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.1, 4.15.0-29-generic


TensorFlow installed from (source or binary): pip install tensorflow_gpu


TensorFlow version (use command below): v1.10.0-0-g656e7a2b34 1.10.0


Mobile device: N/A


Bazel version (if compiling from source): N/A


Python version: 3.6.6


GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609


CUDA/cuDNN version: CUDA: V9.0.176, cuDNN 7.1.4


GPU model and memory: GTX 1080 8GB + GTX 980 Ti 6GB


Docker details:
NVIDIA Docker: 2.0.2
Client:
Version:           17.12.0-ce
API version:       1.35
Go version:        go1.9.2
Git commit:        c97c6d6
Built:             Wed Dec 27 20:11:19 2017
OS/Arch:          linux/amd64
Experimental:      false
Server:
Engine:
Version:          17.12.0-ce
API version:      1.35 (minimum version 1.12)
Go version:       go1.9.2
Git commit:       c97c6d6
Built:            Wed Dec 27 20:09:53 2017
OS/Arch:          linux/amd64
Experimental:     false


Exact command to reproduce: See below


&lt;denchmark-h:h4&gt;System 2 (Bug DOES occur)&lt;/denchmark-h&gt;

All details are from inside the docker which the codes run in

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): True
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.1, 4.15.0-34-generic
TensorFlow installed from (source or binary): pip install tensorflow_gpu
TensorFlow version (use command below): v1.10.1-0-g4dcfddc5d1 1.10.1
Mobile device: N/A
Bazel version (if compiling from source): N/A
Python version: 3.6.6
GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CUDA/cuDNN version: CUDA: V9.0.176, cuDNN 7.2.1
GPU model and memory: GTX 1080 Ti 12GB x 2
Docker details:
NVIDIA Docker: 2.0.3
Client:
Version:           18.06.0-ce
API version:       1.38
Go version:        go1.10.3
Git commit:        0ffa825
Built:             Wed Jul 18 19:11:02 2018
OS/Arch:           linux/amd64
Experimental:      false
Server:
Engine:
Version:          18.06.0-ce
API version:      1.38 (minimum version 1.12)
Go version:       go1.10.3
Git commit:       0ffa825
Built:            Wed Jul 18 19:09:05 2018
OS/Arch:          linux/amd64
Experimental:     false
Exact command to reproduce: See below

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Using the same docker image, which is an altered version of &lt;denchmark-link:https://github.com/ufoym/deepo/blob/master/docker/Dockerfile.tensorflow-py36-cu90&gt;this image&lt;/denchmark-link&gt;
 running the same code over the same TFRecords on one machine (System 2) I get a DataLoss exception (corrupted record) while on the other system (System 1) I don't.
I have verified using checksum that the files were transferred safely. I have also tried to retransfer the files at least two times. I have also have tried using other sets of TFRecords I created. The problem is the same in all cases, it happens on System 2, but does not happen on System 1.
The TFRecords were written on System 1 and are over 150GB in size.
I have also tried rebuilding the image and restarting containers.
I have also tried restarting the host machine.
The code is running on one GPU, I tried using each, all fail.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

Due to sensitivity of the code I can't share all of it, but I'll paste the relevant lines.
In the training code I have 3 places where I iterate over the dataset, once when I count the number of records
train_size = sum(1 for _ in tf.python_io.tf_record_iterator(meta['train_tfr_path'])).
Second time when I feed the training loop
&lt;denchmark-code&gt;while True:
    try:
        ...
        _, batch_summary = sess.run([opt, merged], feed_dict={handle: train_handle})
        ...
    except tf.errors.OutOfRangeError:
        ...
&lt;/denchmark-code&gt;

And third time very similar code for the validation set.
After I recreate image + containers and restarting the machine, I will successfully finish the calculation of train_size (full loop over TFRecords), then I would start training, and somewhere in the training loop I would fail with the traceback attached below. If then I run the code again, I will fail on the first call to the TFRecords, with the same traceback - meaning this time it would happen on the train_size calculation. It will now forever fail on this call, until I restart the machine and the scenario repeats.
As I said, same code on different machine (System 1) never fails.
Traceback:

Traceback (most recent call last):
File "train.py", line 884, in 
main()
File "train.py", line 878, in main
train(graph, model_dir, tensorboard_dir, meta, is_recovering=recovering)
File "train.py", line 846, in train
'sequential']})
File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py", line 877, in run
run_metadata_ptr)
File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
feed_dict_tensor, options, run_metadata)
File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
run_metadata)
File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 6720637495
[[Node: data/IteratorGetNext = IteratorGetNextoutput_shapes=[[?,?], [?,1], [?,?,?], [?,?,?], [?,?], ..., [?,?], [?,1], [?,?,?], [?,?,?], [?,?]], output_types=[DT_FLOAT, DT_INT64, DT_INT64, DT_FLOAT, DT_INT64, ..., DT_INT64, DT_INT64, DT_FLOAT, DT_INT64, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"]]
Caused by op 'data/IteratorGetNext', defined at:
File "train.py", line 884, in 
main()
File "train.py", line 874, in main
graph, meta = build_model(available_gpus)
File "train.py", line 502, in build_model
return build_classifier(available_gpus)
File "train.py", line 572, in build_classifier
n_gpus=FLAGS.n_gpus)
File "/opt/code/utils/architecture/data/v4.py", line 309, in data_prep

File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py", line 410, in get_next
name=name)), self._output_types,
File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py", line 2069, in iterator_get_next
output_shapes=output_shapes, name=name)
File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
op_def=op_def)
File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
return func(*args, **kwargs)
File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
op_def=op_def)
File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py", line 1717, in init
self._traceback = tf_stack.extract_stack()
DataLossError (see above for traceback): corrupted record at 6720637495
[[Node: data/IteratorGetNext = IteratorGetNextoutput_shapes=[[?,?], [?,1], [?,?,?], [?,?,?], [?,?], ..., [?,?], [?,1], [?,?,?], [?,?,?], [?,?]], output_types=[DT_FLOAT, DT_INT64, DT_INT64, DT_FLOAT, DT_INT64, ..., DT_INT64, DT_INT64, DT_FLOAT, DT_INT64, DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"]]

	</description>
	<comments>
		<comment id='1' author='eliorc' date='2018-09-25T19:07:50Z'>
		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
Bazel version
Mobile device
		</comment>
		<comment id='2' author='eliorc' date='2018-09-25T19:59:41Z'>
		Editted ü§¶‚Äç‚ôÇÔ∏è
		</comment>
		<comment id='3' author='eliorc' date='2018-09-26T17:31:21Z'>
		&lt;denchmark-link:https://github.com/eliorc&gt;@eliorc&lt;/denchmark-link&gt;
 Hi, just to confirm have you tried reading your saved data using &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator&gt;generator&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='eliorc' date='2018-09-26T18:00:16Z'>
		Yep, due to sensitivity I can't post the entire code but this is censored version
&lt;denchmark-code&gt;dataset = tf.data.TFRecordDataset(tfrecord_path)
dataset = dataset.map(...., num_parallel_calls=n_workers)
dataset = dataset.map(....,  num_parallel_calls=n_workers)
dataset = dataset.map(...., num_parallel_calls=n_workers)

dataset = dataset.map(...,  num_parallel_calls=n_workers)

dataset = dataset.shuffle(buffer_size=10000)

dataset = dataset.apply(tf.contrib.data.bucket_by_sequence_length(element_length_func=general.element_length_func,
                                                                  bucket_batch_sizes=[batch_size] * (
                                                                          len(bucket_boundaries) + 1),
                                                                  # +2 for the below [0] and above [-1]
                                                                  bucket_boundaries=bucket_boundaries,
                                                                  padded_shapes={.....}))

dataset = dataset.prefetch(buffer_size=prefetch)

handle = tf.placeholder(tf.string, shape=[], name='handle')
iterator = tf.data.Iterator.from_string_handle(handle,
                                                   dataset.output_types,
                                                   dataset.output_shapes)

train_iterator = dataset.make_initializable_iterator()

iterator.get_next()
....
&lt;/denchmark-code&gt;

The code runs on a 16 core machine
		</comment>
		<comment id='5' author='eliorc' date='2018-09-28T18:03:10Z'>
		&lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
  Could you please look into this.
		</comment>
		<comment id='6' author='eliorc' date='2018-09-28T21:16:59Z'>
		Hi &lt;denchmark-link:https://github.com/eliorc&gt;@eliorc&lt;/denchmark-link&gt;
! I'm going to need some more details to get to the bottom of this. Can you please answer the following questions?

Is the corrupted record always at the same offset, or does it vary?
Does the error reproduce every time you run on worker 2, or only sometimes?
How was the file generated?
Are you using any compression options?

		</comment>
		<comment id='7' author='eliorc' date='2018-09-28T21:23:33Z'>
		Also:

What file system are you using? (e.g. Is it a local file, or in GCS, S3, etc.?)

		</comment>
		<comment id='8' author='eliorc' date='2018-09-30T06:43:55Z'>
		Hi &lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;


Is the corrupted record always at the same offset, or does it vary? - I'm using the .shuffle call, so I believe even though the number of the record is different each time I restart the machine, it might be the same one.
Does the error reproduce every time you run on worker 2, or only sometimes? - The error always reproduces on worker 2, if I restart the host machine it will go through the counting process and somewhere during the training it will fail, and then it will always fail upon trying to count
How was the file generated? - Pretty standard code, these are tf.SequenceExample, pretty similar in technique but more complex than code that can be found here
Are you using any compression options? - Not that I know of, unless there is some kind of default compression. What I did do is compress the file before transferring it between systems using zip.
What file system are you using? (e.g. Is it a local file, or in GCS, S3, etc.?) - Local filesystems

P.S. thanks for your great support on the SO platform, it helps a lot
		</comment>
		<comment id='9' author='eliorc' date='2018-10-03T10:26:14Z'>
		Furthering investigating this issue, I used the same TFRecord and the same code again on an AWS instance and it worked perfectly.
System 3 (Bug DOESN'T occur - AWS instance)
All details are from inside the docker which the codes run in
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): True
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 4.4.0-1069-aws &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/79&gt;#79&lt;/denchmark-link&gt;
-Ubuntu x86_64 x86_64 x86_64 GNU/Linux
TensorFlow installed from (source or binary): pip install tensorflow_gpu
TensorFlow version (use command below): v1.10.0-0-g656e7a2b34 1.10.0
Mobile device: N/A
Bazel version (if compiling from source): N/A
Python version: 3.6.6
GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CUDA/cuDNN version: CUDA: V9.0.176, cuDNN 7.1.4
GPU model and memory: Tesla V100-SXM2-16GB
Docker details:
NVIDIA Docker: 2.0.3
Client:
Version:      18.03.1-ce
API version:  1.37
Go version:   go1.9.5
Git commit:   9ee9f40
Built:        Thu Apr 26 07:17:20 2018
OS/Arch:      linux/amd64
Experimental: false
Orchestrator: swarm
Server:
Engine:
Version:      18.03.1-ce
API version:  1.37 (minimum version 1.12)
Go version:   go1.9.5
Git commit:   9ee9f40
Built:        Thu Apr 26 07:15:30 2018
OS/Arch:      linux/amd64
Experimental: false
Exact command to reproduce: See below
		</comment>
		<comment id='10' author='eliorc' date='2018-10-16T20:35:28Z'>
		&lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 Hi, can you please look into this.
		</comment>
		<comment id='11' author='eliorc' date='2018-10-20T05:44:59Z'>
		Another step we took; we have just formatted the machine that fails, rebuilt the images and tried again - it still fails on the DataLoss error.
This week we will run a memtest.
		</comment>
		<comment id='12' author='eliorc' date='2018-10-23T10:29:01Z'>
		PROBLEM IS SOLVED
We ran a memtest on our 4 memory cards and one of the cards was faulty. We removed that card, reran the code and now everything works smoothly.
		</comment>
		<comment id='13' author='eliorc' date='2019-02-22T23:04:43Z'>
		Hi &lt;denchmark-link:https://github.com/eliorc&gt;@eliorc&lt;/denchmark-link&gt;
, this is probably a dumb question but would you mind elaborating on which type of memory was failing for you (RAM, hard drive, etc.)? And/or what you mean by memtest? Was it the memtest86 program that comes with Ubuntu, thus suggesting RAM was the problem...?
I'm experiencing the same issue (data corruption occurring during training on one Ubuntu machine, but not on the other), and I suspect there's an issue with the memory, but I ran memtest86 and it didn't find any problems. That being the case, would appreciate any extra information about your findings ‚Äì¬†thanks so much!
		</comment>
		<comment id='14' author='eliorc' date='2019-02-23T09:10:04Z'>
		&lt;denchmark-link:https://github.com/owenwork&gt;@owenwork&lt;/denchmark-link&gt;
 I'll try to investigate that, since the IT guys ran all the test I don't have that information on hand - if I'll be able to get some answers I'll be sure to update
		</comment>
		<comment id='15' author='eliorc' date='2019-10-12T11:36:27Z'>
		how to solute it?
		</comment>
	</comments>
</bug>