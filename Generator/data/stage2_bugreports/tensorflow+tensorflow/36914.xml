<bug id='36914' author='vdouet' open_date='2020-02-20T01:40:54Z' closed_time='2020-07-26T02:31:58Z'>
	<summary>Models with tf.lite.Optimize cause abort() when interpreter-&amp;gt;Invoke()</summary>
	<description>
@tensorflow/micro
System information

Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina 10.15.2
TensorFlow installed from (source or binary): Installed with pip pip install --upgrade tensorflow
Tensorflow version (commit SHA if source): Version 2.1.0 and TFLITE_SCHEMA_VERSION (3)
Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ESP32

Describe the problem
When using a Tensorflow Lite converted model with tf.lite.Optimize the ESP32 will reboot (abort()) if interpreter-&gt;Invoke() is called for inference.
However if we convert the exact same model without using tf.lite.Optimize the inference will run just fine.
Please provide the exact sequence of commands/steps when you ran into the problem
Training and converting a model:
#Converting a simple model.
model = Sequential()
model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(10,6)))
model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))
model.add(Dropout(0.5))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(100, activation='relu'))
model.add(Dense(y_train.shape[1], activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=2)

converter = lite.TFLiteConverter.from_keras_model(model)

#The problem is caused by the following line
converter.optimizations = [lite.Optimize.DEFAULT]

tfmodel = converter.convert()
open(PATH+'/model.tflite',"wb").write(tfmodel)
Converting to C array:
xxd -i converted_model.tflite &gt; model_data.cc
In ESP32:
//abort() when:
if(interpreter-&gt;Invoke() != kTfLiteOk) {
        Serial.println("There was an error invoking the interpreter!");
        return;
        }
The inference will run just fine if we convert the same model without converter.optimizations = [lite.Optimize.DEFAULT]
I tried 'DEFAULT', 'OPTIMIZE_FOR_SIZE', and 'OPTIMIZE_FOR_LATENCY' same problem every time.
I also tried using converter.experimental_new_converter = True but it did not work.
Here is the trace from the ESP32:
&lt;denchmark-code&gt;abort() was called at PC 0x400ddd39 on core 1

Backtrace: 0x4008c49c:0x3ffb1e00 0x4008c6cd:0x3ffb1e20 0x400ddd39:0x3ffb1e40 0x400de09d:0x3ffb1e90 0x400e60ca:0x3ffb1f70 0x400d2a0d:0x3ffb1f90 0x400edf0d:0x3ffb1fb0 0x40088bd9:0x3ffb1fd0

Rebooting...
ets Jun  8 2016 00:22:57

rst:0xc (SW_CPU_RESET),boot:0x13 (SPI_FAST_FLASH_BOOT)
configsip: 0, SPIWP:0xee
clk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00
mode:DIO, clock div:1
load:0x3fff0018,len:4
load:0x3fff001c,len:1216
ho 0 tail 12 room 4
load:0x40078000,len:9720
ho 0 tail 12 room 4
load:0x40080400,len:6352
entry 0x400806b8
&lt;/denchmark-code&gt;

Edit: Adding decoded stack result
&lt;denchmark-code&gt;Decoding stack results
0x4008c49c: invoke_abort at /home/runner/work/esp32-arduino-lib-builder/esp32-arduino-lib-builder/esp-idf/components/esp32/panic.c line 155
0x4008c6cd: abort at /home/runner/work/esp32-arduino-lib-builder/esp32-arduino-lib-builder/esp-idf/components/esp32/panic.c line 170

# Error comes from here:
0x400ddd39: tflite::reference_integer_ops::FullyConnected(tflite::FullyConnectedParams const&amp;, tflite::RuntimeShape const&amp;, signed char const*, tflite::RuntimeShape const&amp;, signed char const*, tflite::RuntimeShape const&amp;, int const*, tflite::RuntimeShape const&amp;, signed char*) at lib/tfmicro/tensorflow/lite/kernels/internal/reference/integer_ops/fully_connected.h line 35
0x400de09d: tflite::ops::micro::fully_connected::Eval(TfLiteContext*, TfLiteNode*) at lib/tfmicro/tensorflow/lite/micro/kernels/fully_connected.cc line 102
0x400e60ca: tflite::MicroInterpreter::Invoke() at lib/tfmicro/tensorflow/lite/micro/micro_interpreter.cc line 190

0x400d2a0d: loop() at /Users/vdouet/Deep Learning/HAR_ESP32_Tensorflow/Firmware/src/main.ino line 560
0x400edf0d: loopTask(void*) at /Users/vdouet/.platformio/packages/framework-arduinoespressif32/cores/esp32/main.cpp line 17
0x40088bd9: vPortTaskWrapper at /home/runner/work/esp32-arduino-lib-builder/esp32-arduino-lib-builder/esp-idf/components/freertos/port.c line 143
&lt;/denchmark-code&gt;

I tried using different models with different input shapes, even the same one from examples/magic_wand but every time the ESP32 will abort() when it reaches interpreter-&gt;Invoke().
Edit for completeness:
The optimized converted models work absolutely fine using Tensorflow Lite with Python3:
interpreter = tf.lite.Interpreter(model_path=PATH+"/model.tflite")
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)

interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()
Best regards,
Victor
	</description>
	<comments>
		<comment id='1' author='vdouet' date='2020-02-24T15:28:39Z'>
		After cloning the newest version of the Tensorflow repository I got the same error but with a new information before abort():
assertion "exponent &lt;= 31" failed: file "lib/tfmicro/fixedpoint/fixedpoint.h", line 359, function: IntegerType gemmlowp::RoundingDivideByPOT(IntegerType, int) [with IntegerType = int]
ESP32 trace:
&lt;denchmark-code&gt;0x4008cc88: invoke_abort at /home/runner/work/esp32-arduino-lib-builder/esp32-arduino-lib-builder/esp-idf/components/esp32/panic.c line 155
0x4008ceb9: abort at /home/runner/work/esp32-arduino-lib-builder/esp32-arduino-lib-builder/esp-idf/components/esp32/panic.c line 170
0x40128297: scalbn at ../../../.././newlib/libm/common/s_scalbn.c line 101
0x400de2c2: tflite::reference_ops::Conv(tflite::ConvParams const&amp;, tflite::RuntimeShape const&amp;, float const*, tflite::RuntimeShape const&amp;, float const*, tflite::RuntimeShape const&amp;, float const*, tflite::RuntimeShape const&amp;, float*, tflite::RuntimeShape const&amp;, float*) at lib/tfmicro/tensorflow/lite/kernels/internal/reference/conv.h line 52
0x400e0acd: tflite::ops::micro::fully_connected::Eval(TfLiteContext*, TfLiteNode*) at lib/tfmicro/tensorflow/lite/micro/kernels/fully_connected.cc line 64
0x400e0da1: tflite::ops::micro::fully_connected::Eval(TfLiteContext*, TfLiteNode*) at lib/tfmicro/tensorflow/lite/micro/kernels/fully_connected.cc line 136
0x400e8d07: tflite::MicroInterpreter::Invoke() at lib/tfmicro/tensorflow/lite/micro/micro_interpreter.cc line 186
0x400d4282: loop() at /Users/TotoryDort/Lassena/Projects/IDEaS/IbNav/Deep Learning/HAR_ESP32_Tensorflow/Firmware/src/main.ino line 229
0x400f23c5: uartDetectBaudrate at /Users/TotoryDort/.platformio/packages/framework-arduinoespressif32/cores/esp32/esp32-hal-uart.c line 563
0x4008939d: vPortTaskWrapper at /home/runner/work/esp32-arduino-lib-builder/esp32-arduino-lib-builder/esp-idf/components/freertos/port.c line 143
&lt;/denchmark-code&gt;

This seems to be related to the issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/36943&gt;#36943&lt;/denchmark-link&gt;

Edit:
I'm trying to use a representative dataset as mentionned here: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/36943#issuecomment-589887984&gt;#36943 (comment)&lt;/denchmark-link&gt;

But with this my ESP32 gives me another error when I use 'AllOpsResolver':
&lt;denchmark-code&gt;Didn't find op for builtin opcode 'MAX_POOL_2D' version '2'

Failed to get registration from op code  d

AllocateTensors() failed
Guru Meditation Error: Core  1 panic'ed (LoadProhibited). Exception was unhandled.
&lt;/denchmark-code&gt;

I don't know if it matter but I only use a MaxPooling1D in my model.
However if I use 'MicroOpResolver' like this:
tflite::MicroOpResolver&lt;6&gt; micro_op_resolver;
    micro_op_resolver.AddBuiltin(tflite::BuiltinOperator_FULLY_CONNECTED,
                              tflite::ops::micro::Register_FULLY_CONNECTED(), 1, 4);
    micro_op_resolver.AddBuiltin(tflite::BuiltinOperator_SOFTMAX,
                              tflite::ops::micro::Register_SOFTMAX());
    micro_op_resolver.AddBuiltin(tflite::BuiltinOperator_RELU,
                              tflite::ops::micro::Register_RELU());
    micro_op_resolver.AddBuiltin(tflite::BuiltinOperator_QUANTIZE,
                              tflite::ops::micro::Register_QUANTIZE());
    micro_op_resolver.AddBuiltin(tflite::BuiltinOperator_RESHAPE,
                              tflite::ops::micro::Register_RESHAPE());
    micro_op_resolver.AddBuiltin(tflite::BuiltinOperator_CONV_2D,
                              tflite::ops::micro::Register_CONV_2D(), 1, 3);
I get the following error:
&lt;denchmark-code&gt;Didn't find op for builtin opcode 'QUANTIZE' version '1'

Failed to get registration from op code  d

AllocateTensors() failed
&lt;/denchmark-code&gt;

I tried with the latest version of Tensorflow and tf-nightly and it did not make a difference.
		</comment>
		<comment id='2' author='vdouet' date='2020-02-24T17:36:33Z'>
		We're looking into this and will let you know once we have an update!
		</comment>
		<comment id='3' author='vdouet' date='2020-02-25T17:24:23Z'>
		I did some more testing and successfully made the inference work by removing the MaxPooling1D layer in my model and by using  AllOpsResolver.
Previously when I was using a MaxPooling1D layer with AllOpsResolver I got the error message:
&lt;denchmark-code&gt;Didn't find op for builtin opcode 'MAX_POOL_2D' version '2'
Failed to get registration from op code  d
&lt;/denchmark-code&gt;

The model I am using:
model = Sequential()
        model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(n_timestep,6)))
        model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))
        model.add(Dropout(0.5))
        #model.add(MaxPooling1D(pool_size=2))
        model.add(Flatten())
        model.add(Dense(100, activation='relu'))
        model.add(Dense(3, activation='softmax'))
        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
The converter:
converter = lite.TFLiteConverter.from_keras_model(model)
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]#, tf.lite.OpsSet.SELECT_TF_OPS]
    converter.optimizations = [lite.Optimize.DEFAULT]
    converter.experimental_new_converter = True
    converter.representative_dataset = representative_dataset_gen
    tfmodel = converter.convert()
    open(PATH+'/model.tflite',"wb").write(tfmodel)
		</comment>
		<comment id='4' author='vdouet' date='2020-03-03T11:36:09Z'>
		Hi, the same happens with my setup with Arduino ESP32, TensorFlowLite library v2.1.0-ALPHA, and generating the tflite model in colab with TF version 2.1.0
When i convert to TFLite with optimization = tf.lite.Optimize.OPTIMIZE_FOR_SIZE, the ESP32 crashes at inference (see &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/37088#issuecomment-593812080&gt;trace&lt;/denchmark-link&gt;
). Without optimizer, it works perfectly. The model.tflite is 260KB. With optimizer it is 258KB so seems not worth it.
I'm using the Fashion MNIST dataset with 28x28 images, and my model is as follows:
&lt;denchmark-code&gt;model = tf.keras.Sequential([
  tf.keras.layers.Conv2D(16, 3, activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(16, 3, activation='relu'),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(32, activation='relu'),
  tf.keras.layers.Dense(10, activation='softmax')
])
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='vdouet' date='2020-07-12T01:32:03Z'>
		&lt;denchmark-link:https://github.com/tomtobback&gt;@tomtobback&lt;/denchmark-link&gt;
 Can you try optimizing your model again using the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/train/train_hello_world_model.ipynb&gt;updated hello_world example&lt;/denchmark-link&gt;
 and let us know if you face this issue.
We recommend testing with  input and output type as given in the example and then update it to  as given in &lt;denchmark-link:https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only&gt;this code snippet&lt;/denchmark-link&gt;
. For int8/uint8 input/output type, you need TF Version &gt;= 2.3
		</comment>
		<comment id='6' author='vdouet' date='2020-07-19T01:52:02Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='7' author='vdouet' date='2020-07-26T02:31:56Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='8' author='vdouet' date='2020-07-26T02:31:59Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36914&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36914&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='vdouet' date='2020-12-17T15:01:23Z'>
		
I did some more testing and successfully made the inference work by removing the MaxPooling1D layer in my model and by using AllOpsResolver.
Previously when I was using a MaxPooling1D layer with AllOpsResolver I got the error message:
Didn't find op for builtin opcode 'MAX_POOL_2D' version '2'
Failed to get registration from op code  d

The model I am using:
model = Sequential()
        model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(n_timestep,6)))
        model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))
        model.add(Dropout(0.5))
        #model.add(MaxPooling1D(pool_size=2))
        model.add(Flatten())
        model.add(Dense(100, activation='relu'))
        model.add(Dense(3, activation='softmax'))
        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
The converter:
converter = lite.TFLiteConverter.from_keras_model(model)
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]#, tf.lite.OpsSet.SELECT_TF_OPS]
    converter.optimizations = [lite.Optimize.DEFAULT]
    converter.experimental_new_converter = True
    converter.representative_dataset = representative_dataset_gen
    tfmodel = converter.convert()
    open(PATH+'/model.tflite',"wb").write(tfmodel)

thanks for your share. help me a lot.
		</comment>
	</comments>
</bug>