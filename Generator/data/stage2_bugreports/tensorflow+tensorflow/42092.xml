<bug id='42092' author='shalushajan95' open_date='2020-08-06T11:35:18Z' closed_time='2020-08-08T08:42:28Z'>
	<summary>getting input/output dtype as  float32  after converting  keras mnist model to integer quantized tflite model</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu 16.04
TensorFlow installed from (source or binary):
TensorFlow version (or github SHA if from source): 2.2

Command used to run the converter or code if youâ€™re using the Python API
If possible, please share a link to Colab/Jupyter/any notebook.
&lt;denchmark-code&gt;# Copy and paste here the exact command
&lt;/denchmark-code&gt;

import tensorflow as tf
import numpy as np
&lt;denchmark-h:h1&gt;Load MNIST dataset&lt;/denchmark-h&gt;

mnist = tf.keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
&lt;denchmark-h:h1&gt;Normalize the input image so that each pixel value is between 0 to 1.&lt;/denchmark-h&gt;

train_images = train_images.astype(np.float32) / 255.0
test_images = test_images.astype(np.float32) / 255.0
&lt;denchmark-h:h1&gt;Define the model architecture&lt;/denchmark-h&gt;

model = tf.keras.Sequential([
tf.keras.layers.InputLayer(input_shape=(28, 28)),
tf.keras.layers.Reshape(target_shape=(28, 28, 1)),
tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),
tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
tf.keras.layers.Flatten(),
tf.keras.layers.Dense(10)
])
&lt;denchmark-h:h1&gt;Train the digit classification model&lt;/denchmark-h&gt;

model.compile(optimizer='adam',
loss=tf.keras.losses.SparseCategoricalCrossentropy(
from_logits=True),
metrics=['accuracy'])
model.fit(
train_images,
train_labels,
epochs=5,
validation_data=(test_images, test_labels)
)
model.save("mnist.h5")
model_ = tf.keras.models.load_model("mnist.h5")
def representative_data_gen():
for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):
# Model has only one input so each data point has one element.
yield [input_value]
converter = tf.lite.TFLiteConverter.from_keras_model(model_)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen
&lt;denchmark-h:h1&gt;Ensure that if any ops can't be quantized, the converter throws an error&lt;/denchmark-h&gt;

converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
&lt;denchmark-h:h1&gt;Set the input and output tensors to uint8 (APIs added in r2.3)&lt;/denchmark-h&gt;

converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
tflite_model_quant = converter.convert()
interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)
input_type = interpreter.get_input_details()[0]['dtype']
print('input: ', input_type)
output_type = interpreter.get_output_details()[0]['dtype']
print('output: ', output_type)
The output from the converter invocation
&lt;denchmark-code&gt;# Copy and paste the output here.

&lt;class 'numpy.float32'&gt;
&lt;output:  &lt;class 'numpy.float32'&gt;

&lt;/denchmark-code&gt;

Also, please include a link to the saved model or GraphDef
&lt;denchmark-code&gt;# Put link here or attach to the issue.
&lt;/denchmark-code&gt;

Failure details
If the conversion is successful, but the generated model is wrong,
state what is wrong:

Producing wrong results and/or decrease in accuracy
Producing correct results, but the model is slower than expected (model generated from old converter)

RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.
Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Hi
Please help me to identify this issue, After successful conversion of mnist model to integer quantized tflite model i am getting the input dtype as float32 instead  of uint8.
I followed the same post training interger quantization steps mentioned in the tensorflows official website, but getting input/output dtype  as float32
Attaching the link: &lt;denchmark-link:https://www.tensorflow.org/lite/performance/post_training_integer_quant&gt;https://www.tensorflow.org/lite/performance/post_training_integer_quant&lt;/denchmark-link&gt;

I also followed the command line tflite_converter command too,  still getting quantized float model only
command used :
tflite_convert --output_file mnist_tflite_cmd.tflite --keras_model_file mnist.h5 --input_arrays "reshape_input" --input_shapes "1,28,28" --output_arrays "Identity" --output_format TFLITE --inference_type QUANTIZED_UINT8 --inference_input_type QUANTIZED_UINT8
Thanks
	</description>
	<comments>
		<comment id='1' author='shalushajan95' date='2020-08-06T18:38:33Z'>
		&lt;denchmark-link:https://github.com/shalushajan95&gt;@shalushajan95&lt;/denchmark-link&gt;

Please provide indented code such that we can replicate the issue or if possible share a colab gist with the error reported.
		</comment>
		<comment id='2' author='shalushajan95' date='2020-08-07T05:43:31Z'>
		&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;

Code is attached
Than you
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/5039527/minst_tflite_converter.py.zip&gt;minst_tflite_converter.py.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='shalushajan95' date='2020-08-07T14:40:00Z'>
		&lt;denchmark-link:https://github.com/shalushajan95&gt;@shalushajan95&lt;/denchmark-link&gt;

I ran the code shared and i am able to get unit8 [on tf 2.3 and tf nightly]  as expected, please find the &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/d0f3c92e2b94fee80139339e28075735/untitled335.ipynb&gt;gist here&lt;/denchmark-link&gt;
.
In case you are still facing the issue on tf 2.3 please share a colab gist.
		</comment>
	</comments>
</bug>