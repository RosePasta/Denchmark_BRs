<bug id='26178' author='rickstaa' open_date='2019-02-27T16:40:51Z' closed_time='2019-06-26T08:05:12Z'>
	<summary>Output_dir inconsitancy between "model_to_estimator" and "export_savedmodel"</summary>
	<description>
Problem explanation
Since I want to train my neural networks into the Google ML Cloud, I am trying to convert a Keras model to a TensorFlow (TF) estimator.
A tutorial explaining how to do this can be found on &lt;denchmark-link:https://www.youtube.com/watch?v=4pC97HRhK9E&gt;Training Keras with GPUs &amp; Serving Predictions with Cloud ML Engine (Google Cloud AI Huddle)&lt;/denchmark-link&gt;
. The Jupyter notebook that accompanies this tutorial can be found on &lt;denchmark-link:https://www.kaggle.com/yufengg/emnist-gpu-keras-to-tf&gt;kaggle.com&lt;/denchmark-link&gt;
.
Unfortunately, while following this tutorial, I ran into some problems.
When trying to convert a Keras model into a TF estimator (using the model_to_estimator function while supplying the output_dir = &lt;USER_DIR&gt; argument) and following saving this estimator (using the export_savedmodel module_ function), I received the following error:
ValueError: Couldn't find trained model at ./estimator_model.
A solution to overcome this problem has been given on the following &lt;denchmark-link:https://stackoverflow.com/questions/54615708/exporting-a-keras-model-as-a-tf-estimator-trained-model-cannot-be-found&gt;stackoverflow post&lt;/denchmark-link&gt;
. However I am reporting the issue here in case it was not yet solved.
System information

Tested on Kaggle kernel and Windows 10 Pro
TF installed from source: v1.12.0
Python version: 3.6.8
CUDA/cuDNN version: 9.0
GPU model and memory:  NVidia K80 GPUs, 13 GB RAM

Describe the current behaviour
Currently because model_to_estimator module saves the trained model in a "Keras" subfolder under the user specified output_dir = '' while the export_savedmodel module uses the model.output_dir parameter and thus the user specified parent folder to look for the trained model I get the following error:
ValueError: Couldn't find trained model at ./estimator_model.
Describe the expected behaviour
I expected the export_savedmodel module to successfully find the trained model in the  instead of the /keras folder and save the TF model.
Current workaround
Currently, I need to move the model files from the /keras folder to the  to successfully save the model.
Code to reproduce the issue
The issue can be reproduced by using the code provided by
Create a Keras model and save it:
&lt;denchmark-code&gt;import keras
model = keras.Sequential()
model.add(keras.layers.Dense(units=1,
                                activation='sigmoid',
                                input_shape=(10, )))
model.compile(loss='binary_crossentropy', optimizer='sgd')
model.save('./model.h5')
&lt;/denchmark-code&gt;

Next, convert the model to an estimator with tf.keras.estimator.model_to_estimator, add an input receiver function and export it in the Savedmodel format with estimator.export_savedmodel:
&lt;denchmark-code&gt;# Convert keras model to TF estimator
tf_files_path = './tf'
estimator =\
    tf.keras.estimator.model_to_estimator(keras_model=model,
                                          model_dir=tf_files_path)
def serving_input_receiver_fn():
    return tf.estimator.export.build_raw_serving_input_receiver_fn(
        {model.input_names[0]: tf.placeholder(tf.float32, shape=[None, 10])})

# Export the estimator
export_path = './export'
estimator.export_savedmodel(
    export_path,
    serving_input_receiver_fn=serving_input_receiver_fn())

&lt;/denchmark-code&gt;

Error Log
&lt;denchmark-code&gt;
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-25-fa76b1124e44&gt; in &lt;module&gt;()
----&gt; 1 export_path = estimator_model.export_savedmodel('./export', serving_input_receiver_fn=serving_input_receiver_fn)
      2 export_path

/opt/conda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in export_savedmodel(self, export_dir_base, serving_input_receiver_fn, assets_extra, as_text, checkpoint_path, strip_default_attrs)
    661         checkpoint_path=checkpoint_path,
    662         strip_default_attrs=strip_default_attrs,
--&gt; 663         mode=model_fn_lib.ModeKeys.PREDICT)
    664 
    665   def export_saved_model(

/opt/conda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _export_saved_model_for_mode(self, export_dir_base, input_receiver_fn, assets_extra, as_text, checkpoint_path, strip_default_attrs, mode)
    787         as_text=as_text,
    788         checkpoint_path=checkpoint_path,
--&gt; 789         strip_default_attrs=strip_default_attrs)
    790 
    791   def _export_all_saved_models(

/opt/conda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _export_all_saved_models(self, export_dir_base, input_receiver_fn_map, assets_extra, as_text, checkpoint_path, strip_default_attrs)
    876             self._model_dir)
    877       if not checkpoint_path:
--&gt; 878         raise ValueError("Couldn't find trained model at %s." % self._model_dir)
    879 
    880       export_dir = export_helpers.get_timestamped_export_dir(export_dir_base)

ValueError: Couldn't find trained model at ./estimator_model.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='rickstaa' date='2019-03-01T00:26:10Z'>
		&lt;denchmark-link:https://github.com/rickstaa&gt;@rickstaa&lt;/denchmark-link&gt;
 Is it possible for you to test whether this bug persists with latest version of TF? Thanks!
		</comment>
		<comment id='2' author='rickstaa' date='2019-03-01T14:28:14Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 Thanks for your response. I tested it in the newest TF build and the error still seems to be there.
OUTPUT LOG
&lt;denchmark-code&gt;(base) [15:13:50] ricks@ricks-HP-ZBook:~/OneDrive/Education/Other/Deep Learning/Other/Keras2tf $ source /home/ricks/miniconda3/bin/activate(base) [15:13:50] ricks@ricks-HP-ZBook:~/OneDrive/Education/Other/Deep Learning/Other/Keras2tf $ conda activate datascience_tf_new
(datascience_tf_new) [15:13:51] ricks@ricks-HP-ZBook:~/OneDrive/Education/Other/Deep Learning/Other/Keras2tf $ /home/ricks/miniconda3/envs/datascience_tf_new/bin/python "/home/ricks/OneDrive/Education/Other/Deep Learning/Other/Keras2tf/keras_cloud_conversion.py"
WARNING: Logging before flag parsing goes to stderr.
W0301 15:14:05.699281 140146440931136 deprecation.py:506] From /home/ricks/miniconda3/envs/datascience_tf_new/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 12, 12, 12)        312       
_________________________________________________________________
dropout (Dropout)            (None, 12, 12, 12)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 5, 5, 18)          1962      
_________________________________________________________________
dropout_1 (Dropout)          (None, 5, 5, 18)          0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 4, 4, 24)          1752      
_________________________________________________________________
flatten (Flatten)            (None, 384)               0         _________________________________________________________________
dense (Dense)                (None, 150)               57750     
_________________________________________________________________
dense_1 (Dense)              (None, 47)                7097      
=================================================================
Total params: 68,873
Trainable params: 68,873
Non-trainable params: 0
_________________________________________________________________
[None, 12, 12, 12]
[None, 12, 12, 12]
[None, 5, 5, 18]
[None, 5, 5, 18]
[None, 4, 4, 24]
[None, 384]
[None, 150]
[None, 47]
(112800, 28, 28, 1)
2019-03-01 15:14:14.039015: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-03-01 15:14:14.044111: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-03-01 15:14:14.128575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1009] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-03-01 15:14:14.185376: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x5584828ecca0 executing computations on platform CUDA. Devices:
2019-03-01 15:14:14.185400: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): Quadro M1000M, Compute Capability 5.0
2019-03-01 15:14:14.206354: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2592000000 Hz
2019-03-01 15:14:14.206920: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x558457b11fa0 executing computations on platform Host. Devices:
2019-03-01 15:14:14.206975: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
2019-03-01 15:14:14.207346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 0 with properties: 
name: Quadro M1000M major: 5 minor: 0 memoryClockRate(GHz): 1.0715
pciBusID: 0000:01:00.0
totalMemory: 3,95GiB freeMemory: 3,39GiB
2019-03-01 15:14:14.207361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0
2019-03-01 15:14:14.207437: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-03-01 15:14:14.208699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-01 15:14:14.208712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0 
2019-03-01 15:14:14.208718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N 
2019-03-01 15:14:14.209027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3168 MB memory) -&gt; physical GPU (device: 0, name: Quadro M1000M, pci bus id: 0000:01:00.0, compute capability: 5.0)
^C      
(datascience_tf_new) [15:15:31] ricks@ricks-HP-ZBook:~/OneDrive/Education/Other/Deep Learning/Other/Keras2tf $ /home/ricks/miniconda3/envs/datascience_tf_new/bin/python "/home/ricks/OneDrive/Education/Other/Deep Learning/Other/Keras2tf/keras_cloud_conversion.py"
WARNING: Logging before flag parsing goes to stderr.
W0301 15:24:53.037093 140332255565632 deprecation.py:506] From /home/ricks/miniconda3/envs/datascience_tf_new/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 12, 12, 12)        312       
_________________________________________________________________
dropout (Dropout)            (None, 12, 12, 12)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 5, 5, 18)          1962      
_________________________________________________________________
dropout_1 (Dropout)          (None, 5, 5, 18)          0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 4, 4, 24)          1752      
_________________________________________________________________
flatten (Flatten)            (None, 384)               0         
_________________________________________________________________
dense (Dense)                (None, 150)               57750     
_________________________________________________________________
dense_1 (Dense)              (None, 47)                7097      
=================================================================
Total params: 68,873
Trainable params: 68,873
Non-trainable params: 0
_________________________________________________________________
[None, 12, 12, 12]
[None, 12, 12, 12]
[None, 5, 5, 18]
[None, 5, 5, 18]
[None, 4, 4, 24]
[None, 384]
[None, 150]
[None, 47]
(112800, 28, 28, 1)
2019-03-01 15:25:03.184462: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-03-01 15:25:03.192966: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-03-01 15:25:03.260354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1009] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-03-01 15:25:03.261681: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x560aedec8ff0 executing computations on platform CUDA. Devices:
2019-03-01 15:25:03.261702: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): Quadro M1000M, Compute Capability 5.0
2019-03-01 15:25:03.282273: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2592000000 Hz
2019-03-01 15:25:03.282672: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x560aedec5dc0 executing computations on platform Host. Devices:
2019-03-01 15:25:03.282691: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
2019-03-01 15:25:03.287303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 0 with properties: 
name: Quadro M1000M major: 5 minor: 0 memoryClockRate(GHz): 1.0715
pciBusID: 0000:01:00.0
totalMemory: 3,95GiB freeMemory: 3,39GiB
2019-03-01 15:25:03.287336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0
2019-03-01 15:25:03.287445: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-03-01 15:25:03.288709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-01 15:25:03.288739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0 
2019-03-01 15:25:03.288747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N 
2019-03-01 15:25:03.288993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3173 MB memory) -&gt; physical GPU (device: 0, name: Quadro M1000M, pci bus id: 0000:01:00.0, compute capability: 5.0)
Epoch 1/5
2019-03-01 15:25:04.878610: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-03-01 15:25:04.995056: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
500/500 [==============================] - 5s 11ms/step - loss: 2.5097 - acc: 0.3254 - val_loss: 1.4320 - val_acc: 0.6009
Epoch 2/5
500/500 [==============================] - 4s 9ms/step - loss: 1.4519 - acc: 0.5789 - val_loss: 1.0212 - val_acc: 0.6947
Epoch 3/5
500/500 [==============================] - 5s 9ms/step - loss: 1.1432 - acc: 0.6476 - val_loss: 0.8468 - val_acc: 0.7394
Epoch 4/5
500/500 [==============================] - 5s 9ms/step - loss: 1.0161 - acc: 0.6908 - val_loss: 0.7605 - val_acc: 0.7589
Epoch 5/5
500/500 [==============================] - 4s 8ms/step - loss: 0.9246 - acc: 0.7104 - val_loss: 0.6986 - val_acc: 0.7811
Prediction:  9 , Char:  9
Label:  16
Prediction:  14 , Char:  E
Label:  14
Prediction:  26 , Char:  Q
Label:  26
Prediction:  21 , Char:  L
Label:  12
Prediction:  37 , Char:  b
Label:  37
Prediction:  9 , Char:  9
Label:  16
Prediction:  9 , Char:  9
Label:  16
Prediction:  2 , Char:  2
Label:  2
Prediction:  17 , Char:  H
Label:  20
Prediction:  26 , Char:  Q
Label:  26
conv2d_input
Traceback (most recent call last):
  File "/home/ricks/OneDrive/Education/Other/Deep Learning/Other/Keras2tf/keras_cloud_conversion.py", line 190, in &lt;module&gt;
    export_path = estimator_model.export_savedmodel('./export', serving_input_receiver_fn=serving_input_receiver_fn)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py", line 1645, in export_savedmodel
    experimental_mode=model_fn_lib.ModeKeys.PREDICT)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py", line 723, in export_saved_model
    checkpoint_path=checkpoint_path)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py", line 801, in experimental_export_all_saved_models
    raise ValueError("Couldn't find trained model at %s." % self._model_dir)
ValueError: Couldn't find trained model at ./model_estimator.
(datascience_tf_new) [15:25:41] ricks@ricks-HP-ZBook:~/OneDrive/Education/Other/Deep Learning/Other/Keras2tf $ python --versionPython 3.6.8 :: Anaconda, Inc.(datascience_tf_new) [15:26:08] ricks@ricks-HP-ZBook:~/OneDrive/Education/Other/Deep Learning/Other/Keras2tf $ pip list | grep tensorflow
tensorflow           1.13.1              
tensorflow-estimator 1.13.0              
tensorflow-gpu       1.13.1              
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='rickstaa' date='2019-03-01T16:53:23Z'>
		&lt;denchmark-link:https://github.com/rickstaa&gt;@rickstaa&lt;/denchmark-link&gt;
 Thanks for checking the bug with latest version of TF. Also, thanks for the workaround you provided in stackoverflow. Thanks!
		</comment>
		<comment id='4' author='rickstaa' date='2019-03-15T05:46:43Z'>
		Summarizing what the stackoverflow content covers:
It looks like what's happening here is that model_to_estimator is writing the model to a sub-folder called keras. So when you choose "./model_estimator" as your model folder, it actually puts the files in "./model_estimator/keras". But it's not reflecting that in the model_dir attribute, which remains as "./model_estimator".
Not sure what the right thing to do here is -- the nested "keras" folder behavior was not present in the past, so it appears that this is desired behavior. The move in that case would be ensure that the model_dir value is appropriately populated.
The workaround I've been (shamefully) using is setting the underlying private variable directly, rather than moving the files around:

Note that this approach also technically answers &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/25772&gt;#25772&lt;/denchmark-link&gt;
 , though it's not even close to a "good" approach...
		</comment>
		<comment id='5' author='rickstaa' date='2019-03-15T06:19:53Z'>
		it appears the 'keras' subdirectory is being added &lt;denchmark-link:https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/keras.py#L313&gt;here&lt;/denchmark-link&gt;
, during "warm start". I'm not quite sure of its purpose, but I suspect the intent here was originally for the conversions to be for "untrained" keras models, and for training of the converted estimators to be done using  , which would yield checkpoint files in the appropriate top-level folder.
However, the use case outlined here is such that folks are converting and then immediately exporting. In this case, nothing shows up in the top-level folder, and we're forced to resort to somewhat absurdist means of rectifying the situation.
		</comment>
		<comment id='6' author='rickstaa' date='2019-05-13T12:34:14Z'>
		
Summarizing what the stackoverflow content covers:
It looks like what's happening here is that model_to_estimator is writing the model to a sub-folder called keras. So when you choose "./model_estimator" as your model folder, it actually puts the files in "./model_estimator/keras". But it's not reflecting that in the model_dir attribute, which remains as "./model_estimator".
Not sure what the right thing to do here is -- the nested "keras" folder behavior was not present in the past, so it appears that this is desired behavior. The move in that case would be ensure that the model_dir value is appropriately populated.
The workaround I've been (shamefully) using is setting the underlying private variable directly, rather than moving the files around:
estimator_model._model_dir = './model_estimator/keras'
Note that this approach also technically answers #25772 , though it's not even close to a "good" approach...

Thanks for the monkey patch suggestion. Ugly but saving me a lot of trouble.
This issue is most definitely needed to be solved. It is a crucial step in the process of serving a keras model using tf.serve for example
		</comment>
		<comment id='7' author='rickstaa' date='2019-05-21T16:12:18Z'>
		If you train the estimator at all, it should create the appropriate model_dir, if I understand correctly, and therefore save out. If you are not training the estimator at all, is the purpose of conversion merely to save the TF SavedModel? If so, I would recommend exporting the savedmodel directly from Keras: &lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/experimental/export_saved_model&gt;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/experimental/export_saved_model&lt;/denchmark-link&gt;

Note that we are also working on making Keras models save to TF SavedModel by default; see the RFC at: &lt;denchmark-link:https://github.com/tensorflow/community/pull/102&gt;tensorflow/community#102&lt;/denchmark-link&gt;

If I am misinterpreting, and there is some reason that you specifically need to pass through Estimator without training the estimator, let me know.
		</comment>
		<comment id='8' author='rickstaa' date='2019-05-21T17:21:51Z'>
		Agreed that this method would work; this issue was filed before r2.0 existed, so at the time using the old tf calls was the only way to export an already trained Keras model.
Since the new export_saved_model is only available in TF 2.0's experimental, it will not be accessible in many cases (non-1.xx users, non-experimental code teams/companies). Question: in the RFC, I notice that export_saved_model is not mentioned. It appears that the use case is covered by model.save(), is my understanding accurate?
In terms of reasons for conversion Keras to Estimator and immediately exporting, rather than using tf.train(), I believe the primary reason is due to the simpler nature (or legacy code) of the keras.fit() api. For users who are familiar with Keras, it is much more natural to train using Keras APIs since that's what all their other calls have been using, rather than suddenly switch over to using TF APIs for just the training portion of things.
Additionally, if one were to convert a trained keras model into a tf model, this is how they would do it, as there's not guarantee that there is necessarily any additional training (data) that is needed to be passed through the model. To that end, a utility for coverting a keras model to a TF model may be useful (several community-made examples exist currently). Though this may be obviated by the move to the TF SavedModel for Keras models (though I see that in the RFC, h5 format continues to be supported for backward compatibility reasons, suggesting that the legacy of this format will cause many to continue using it for some time during the migration period)
		</comment>
		<comment id='9' author='rickstaa' date='2019-05-21T18:25:26Z'>
		To clarify, the code is available now as experimental, and will remain so for 1.x. So if you are in 1.x AND don't use experimental, you will have to stick with a workaround, but 2.x users or 1.x+experimental-okay users can save directly. WRT the RFC, the experimental export_saved_model functionality will be moved into model.save in 2.0, such that there is no need for the separate method/experimental label.
WRT to not training with Estimator, you are right-- the community has spoken, and it prefers Keras. Hopefully, with the direct option in model.save in 2.0 (and the experimental API for 1.x), you will not need to pass through estimator at all.
A utility for moving from h5 to TF SavedModel-- you should be able to load in the h5 to tf.keras, and then export directly as TF SavedModel. eg:
&lt;denchmark-code&gt;model = tf.keras.models.load_model('path_to_model.h5')

# In 1.x
tf.keras.experimental.export_saved_model(model, 'path/to/saved/model')

# After the RFC is accepted/implemented in 2.x
model.save('path/to/saved/model')

&lt;/denchmark-code&gt;

There are various args around compilation and optimizers that you may want to explore, but I believe that should just work. Is that what you had in mind?
		</comment>
		<comment id='10' author='rickstaa' date='2019-05-21T21:22:39Z'>
		And thanks for clarifying that the 'experimental' module is available in 1.x as well.
Re: your example, this is pretty much what I imagined it would look like, given the new APIs available.
Thanks for writing this up!
		</comment>
		<comment id='11' author='rickstaa' date='2019-05-28T13:41:09Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=26178&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=26178&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='rickstaa' date='2019-05-28T14:03:34Z'>
		
If you train the estimator at all, it should create the appropriate model_dir, if I understand correctly, and therefore save out. If you are not training the estimator at all, is the purpose of conversion merely to save the TF SavedModel? If so, I would recommend exporting the savedmodel directly from Keras: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/experimental/export_saved_model
Note that we are also working on making Keras models save to TF SavedModel by default; see the RFC at: tensorflow/community#102
If I am misinterpreting, and there is some reason that you specifically need to pass through Estimator without training the estimator, let me know.

The reason to pass a keras model through an Estimator without training is the .export_saved_model() of the estimator:
&lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#export_saved_model&gt;https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#export_saved_model&lt;/denchmark-link&gt;

This is the only production-ready way to merge a preprocessing transformation graph (using the serving_input_receiver_fn) into model graph inside the estimator. (in order to be used as a servable under tf.serving production server) This is a must to any keras production code that have any meaningful runtime preprocessing (all of them)
if you know any other way let me know.
		</comment>
		<comment id='13' author='rickstaa' date='2019-05-30T22:37:45Z'>
		This is coming soon for keras: &lt;denchmark-link:https://github.com/tensorflow/community/pull/102/files#diff-79c7f63112904922da09631e4a9a902bR96&gt;https://github.com/tensorflow/community/pull/102/files#diff-79c7f63112904922da09631e4a9a902bR96&lt;/denchmark-link&gt;
 , and is already available using the lower-level tf.saved_model API: &lt;denchmark-link:https://github.com/tensorflow/community/blob/master/rfcs/20181116-saved-model.md#specifying-signatures&gt;https://github.com/tensorflow/community/blob/master/rfcs/20181116-saved-model.md#specifying-signatures&lt;/denchmark-link&gt;
 (You would just do what is done to the Net tf.Module there to your Keras Model, and it should work as expected for inference).
		</comment>
		<comment id='14' author='rickstaa' date='2019-06-26T08:05:13Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=26178&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=26178&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>