<bug id='32477' author='mshlis' open_date='2019-09-12T20:03:01Z' closed_time='2020-03-19T02:54:23Z'>
	<summary>BatchNormalization doesn't work in graph mode in tf2</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow:
OS: MacOS
TensorFlow version (use command below): '2.0.0-rc0'
Python version: 3.7

tf.keras.layers.BatchNormalization layer does not work in graph mode
code to reproduce:
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow.keras.layers import BatchNormalization
import numpy as np

keras = tf.keras

class check_bn_model(keras.Model):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.bn = BatchNormalization()
    
    @tf.function
    def call(self, x):
        x = self.bn(x)
        return x
    
X = np.ones((10,5)).astype('float32')
model = check_bn_model()
model.compile('adam', 'mse')
model.fit(X, X, batch_size=2, epochs=2)
&lt;/denchmark-code&gt;

Error stack:
&lt;denchmark-code&gt;InaccessibleTensorError                   Traceback (most recent call last)
&lt;ipython-input-142-0234ba2796e1&gt; in &lt;module&gt;
     12 model = check_bn_model()
     13 model.compile('adam', 'mse')
---&gt; 14 model.fit(X, X, batch_size=2, epochs=2)

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    732         max_queue_size=max_queue_size,
    733         workers=workers,
--&gt; 734         use_multiprocessing=use_multiprocessing)
    735 
    736   def evaluate(self,

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    222           validation_data=validation_data,
    223           validation_steps=validation_steps,
--&gt; 224           distribution_strategy=strategy)
    225 
    226       total_samples = _get_total_number_of_samples(training_data_adapter)

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in _process_training_inputs(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)
    545         max_queue_size=max_queue_size,
    546         workers=workers,
--&gt; 547         use_multiprocessing=use_multiprocessing)
    548     val_adapter = None
    549     if validation_data:

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in _process_inputs(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)
    591         batch_size=batch_size,
    592         check_steps=False,
--&gt; 593         steps=steps)
    594   adapter = adapter_cls(
    595       x,

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)
   2382     # First, we build the model on the fly if necessary.
   2383     if not self.inputs:
-&gt; 2384       all_inputs, y_input, dict_inputs = self._build_model_with_inputs(x, y)
   2385       is_build_called = True
   2386     else:

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _build_model_with_inputs(self, inputs, targets)
   2585     else:
   2586       cast_inputs = inputs
-&gt; 2587     self._set_inputs(cast_inputs)
   2588     return processed_inputs, targets, is_dict_inputs
   2589 

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _set_inputs(self, inputs, outputs, training)
   2672           kwargs['training'] = training
   2673       try:
-&gt; 2674         outputs = self(inputs, **kwargs)
   2675       except NotImplementedError:
   2676         # This Model or a submodel is dynamic and hasn't overridden

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    800                     not base_layer_utils.is_in_eager_or_tf_function()):
    801                   with auto_control_deps.AutomaticControlDependencies() as acd:
--&gt; 802                     outputs = call_fn(cast_inputs, *args, **kwargs)
    803                     # Wrap Tensors in `outputs` in `tf.identity` to avoid
    804                     # circular dependencies.

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    437         # Lifting succeeded, so variables are initialized and we can run the
    438         # stateless function.
--&gt; 439         return self._stateless_fn(*args, **kwds)
    440     else:
    441       canon_args, canon_kwds = \

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)
   1820     """Calls a graph function specialized to the inputs."""
   1821     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-&gt; 1822     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   1823 
   1824   @property

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)
   1139          if isinstance(t, (ops.Tensor,
   1140                            resource_variable_ops.BaseResourceVariable))),
-&gt; 1141         self.captured_inputs)
   1142 
   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1228           {"PartitionedCall": gradient_name,
   1229            "StatefulPartitionedCall": gradient_name}):
-&gt; 1230         flat_outputs = forward_function.call(ctx, args)
   1231     if isinstance(flat_outputs, ops.Operation) or flat_outputs is None:
   1232       # We only record function calls which have outputs.

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    538                 executing_eagerly=executing_eagerly,
    539                 config=config,
--&gt; 540                 executor_type=executor_type)
    541 
    542     if executing_eagerly:

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/ops/functional_ops.py in partitioned_call(args, f, tout, executing_eagerly, config, executor_type)
    857           f=f,
    858           config_proto=config,
--&gt; 859           executor_type=executor_type)
    860     else:
    861       outputs = gen_functional_ops.partitioned_call(

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_functional_ops.py in stateful_partitioned_call(args, Tout, f, config, config_proto, executor_type, name)
    670         "StatefulPartitionedCall", args=args, Tout=Tout, f=f, config=config,
    671                                    config_proto=config_proto,
--&gt; 672                                    executor_type=executor_type, name=name)
    673   _result = _op.outputs[:]
    674   if not _result:

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    791         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,
    792                          input_types=input_types, attrs=attr_protos,
--&gt; 793                          op_def=op_def)
    794       return output_structure, op_def.is_stateful, op
    795 

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in create_op(***failed resolving arguments***)
    542       if ctxt is not None and hasattr(ctxt, "AddValue"):
    543         inp = ctxt.AddValue(inp)
--&gt; 544       inp = self.capture(inp)
    545       inputs[i] = inp
    546     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in capture(self, tensor, name)
    601               " explicit Python locals or TensorFlow collections to access"
    602               " it. Defined in: %s; accessed from: %s.\n"
--&gt; 603               % (tensor, tensor.graph, self))
    604         inner_graph = inner_graph.outer_graph
    605       return self._capture_helper(tensor, name)

InaccessibleTensorError: The tensor 'Tensor("batch_normalization_194/batch_normalization_194_trainable:0", dtype=bool)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=call, id=5982930640); accessed from: FuncGraph(name=keras_graph, id=5269648784).
&lt;/denchmark-code&gt;

If you remove the @tf.function decorator or if you pass dynamic=True to the model instantialization it will work. Otherwise it fails. Note that this is specific to BatchNormalization, if you replace it with any other layer it will work (even other normalization layers like layer norm/instancenorm)
	</description>
	<comments>
		<comment id='1' author='mshlis' date='2019-09-13T09:26:57Z'>
		Looks like code is incomplete. Request you to provide simple standalone code snippet to reproduce the issue in our environment.Thanks!
		</comment>
		<comment id='2' author='mshlis' date='2019-09-15T00:13:55Z'>
		
Looks like code is incomplete. Request you to provide simple standalone code snippet to reproduce the issue in our environment.Thanks!

Just added the import statements. Sorry for the exclusion.
		</comment>
		<comment id='3' author='mshlis' date='2019-09-16T11:42:17Z'>
		I have tried on colab with TF version 2.0.0-rc0, 2.0.0-rc1 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/20ab0b70683608e4d33dd82b2934835d/untitled190.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='4' author='mshlis' date='2019-09-20T00:55:37Z'>
		bump, also have this issue. code works with instance norm from TFA but fails with batch norm, using tf.function and functional API...
		</comment>
		<comment id='5' author='mshlis' date='2019-09-26T16:50:58Z'>
		seems the issue only happens when custom layers or subclassed models use batch norm. if batch norm is used as a standalone layer, it works &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/robieta&gt;@robieta&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/mshlis&gt;@mshlis&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='mshlis' date='2019-09-26T18:27:13Z'>
		&lt;denchmark-link:https://github.com/bionicles&gt;@bionicles&lt;/denchmark-link&gt;
 yeah i noticed that, i shifted to use the functional api (as it works there) and explicitly creating the block i was using with BN, but i feel its still important this gets fixed given that higher level layer abstraction and model sub-classing are some of convenient interfaces of the framework
		</comment>
		<comment id='7' author='mshlis' date='2019-10-24T13:29:37Z'>
		Any updates/news on this issue ?
		</comment>
		<comment id='8' author='mshlis' date='2019-11-20T14:40:57Z'>
		As a workaround you can set the class variable _USE_V2_BEHAVIOR to false.
import tensorflow as tf
from tensorflow.keras.layers import BatchNormalization
BatchNormalization._USE_V2_BEHAVIOR = False
import numpy as np

keras = tf.keras


class check_bn_model(keras.Model):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.bn = BatchNormalization()

    @tf.function
    def call(self, x, training=None, mask=None):
        x = self.bn(x)
        return x


X = np.ones((10, 5)).astype('float32')
model = check_bn_model()
model.compile('adam', 'mse')
model.fit(X, X, batch_size=2, epochs=2)
		</comment>
		<comment id='9' author='mshlis' date='2019-11-20T19:18:00Z'>
		&lt;denchmark-link:https://github.com/kkimdev&gt;@kkimdev&lt;/denchmark-link&gt;
 The original code is now failing due to the assert in  which comes from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/cafc2b641f62d00ba00c8fb7742f3c2ebc80c387&gt;cafc2b6&lt;/denchmark-link&gt;
. Can you take a look?
		</comment>
		<comment id='10' author='mshlis' date='2019-12-16T16:49:53Z'>
		Will this issue be fixed in tf2.1?
		</comment>
		<comment id='11' author='mshlis' date='2020-01-14T22:17:08Z'>
		
Will this issue be fixed in tf2.1?

Just checked and this still appears in 2.1
		</comment>
		<comment id='12' author='mshlis' date='2020-01-14T22:38:03Z'>
		After a little more digging you can fix this by going into the batchnormalisation constructor and replacing self._trainable_var = None with  self._trainable_var = K.freezable_variable(trainable, name=self.name + '_trainable'). Not sure why but in the unaltered version this var only get created on the first call to the batch normalisation layer, which I guess messes with some .fit magic.
		</comment>
		<comment id='13' author='mshlis' date='2020-02-17T17:20:27Z'>
		Getting the same problem.
		</comment>
		<comment id='14' author='mshlis' date='2020-03-03T18:05:56Z'>
		&lt;denchmark-link:https://github.com/mshlis&gt;@mshlis&lt;/denchmark-link&gt;
 Is this still an issue? I am not able to reproduce the issue with . Please check the &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/18aab600662fbcf67620aa4844bd1abd/32477.ipynb&gt;gist here&lt;/denchmark-link&gt;
. Thanks!
Please close the issue if it was already resolved for you. Thanks!
		</comment>
		<comment id='15' author='mshlis' date='2020-03-19T01:16:21Z'>
		It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='16' author='mshlis' date='2020-03-19T02:54:23Z'>
		I am closing this issue as it was resolved in recent tf-nightly. Please feel free to reopen if It was not resolved for you. Thanks!
		</comment>
		<comment id='17' author='mshlis' date='2020-03-19T02:54:25Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32477&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32477&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>