<bug id='15746' author='MarkSonn' open_date='2017-12-31T09:02:50Z' closed_time='2018-01-31T02:39:47Z'>
	<summary>CIFAR10 slows down every 100th step</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code: No
OS Platform and Distribution: Linux Ubuntu 16.04
TensorFlow installed from: Have tried both binary and source
TensorFlow version: 1.4.0-19-ga52c8d9, 1.4.1
Python version: 2.7.12
CUDA/cuDNN version: 8.0.61
Hardware: GPU: NVIDIA GeForce GTX 1080 Ti (11GB), RAM: 64GB, CPU: Intel i7-6850K
Exact command to reproduce: python cifar10_train.py

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

The &lt;denchmark-link:https://www.tensorflow.org/tutorials/deep_cnn&gt;CIFAR10 Tensorflow tutorial&lt;/denchmark-link&gt;
 seems to have a few odd patterns when it comes to the number of examples per second it can compute:

Step 0 is extremely slow
Every 100th step is significantly slow
The step after a really slow step is either a little slow or average
The next 10-30 steps after that are slightly boosted (faster than average)
The rest of the steps are average speed

I'm hoping for (in order of importance):

An explanation and fix for every 100th step being so slow
An explanation and instructions showing me how to make every step run at the boosted speed (the speed shortly after a slow step)
An explanation and fix for the slow 0th and 1st step

I can't find any additional logging or processing that happens on every 100th step. Could it be tf.train.MonitoredSession?
&lt;denchmark-h:h3&gt;Reproducible:&lt;/denchmark-h&gt;


when training on CPU rather than GPU
independent of batch size
on MacBook Pro (Retina, 13-inch, Mid 2014)

&lt;denchmark-h:h3&gt;Hardware utilization:&lt;/denchmark-h&gt;


Average:


CPU: 82-84%
GPU: 70-85%
RAM: 3.7GB


Every 100th step:


CPU: 9%
GPU: 0%
RAM: 3.7GB


Boosted after slow step:


GPU: 92%
CPU: 82-84%
RAM: 3.7GB


Idle:


CPU: 1%
GPU: 0%
RAM: 1.6GB

:
&lt;denchmark-link:https://user-images.githubusercontent.com/7654904/34998310-6b153646-fae7-11e7-8e0c-00ccf01349bf.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Logs excerpt (full logs):&lt;/denchmark-h&gt;


step 0: (587.3 examples/sec; 6.974 sec/batch)
step 1: (22630.6 examples/sec; 0.181 sec/batch)
step 2: (36253.6 examples/sec; 0.113 sec/batch)
step 3: (37966.0 examples/sec; 0.108 sec/batch)
step 4: (38511.4 examples/sec; 0.106 sec/batch)
step 5: (38554.6 examples/sec; 0.106 sec/batch)
step 6: (32112.4 examples/sec; 0.128 sec/batch)
step 7: (38912.4 examples/sec; 0.105 sec/batch)
step 8: (39377.0 examples/sec; 0.104 sec/batch)
step 9: (38206.2 examples/sec; 0.107 sec/batch)
step 10: (38222.1 examples/sec; 0.107 sec/batch)
step 11: (38757.5 examples/sec; 0.106 sec/batch)
step 12: (38833.1 examples/sec; 0.105 sec/batch)
step 13: (39774.8 examples/sec; 0.103 sec/batch)
step 14: (39795.9 examples/sec; 0.103 sec/batch)
step 15: (37850.5 examples/sec; 0.108 sec/batch)
step 16: (38443.5 examples/sec; 0.107 sec/batch)
step 17: (39194.6 examples/sec; 0.105 sec/batch)
step 18: (39164.0 examples/sec; 0.105 sec/batch)
step 19: (39057.5 examples/sec; 0.105 sec/batch)
step 20: (33268.7 examples/sec; 0.123 sec/batch)
step 21: (39459.7 examples/sec; 0.104 sec/batch)
step 22: (39336.2 examples/sec; 0.104 sec/batch)
step 23: (39207.1 examples/sec; 0.104 sec/batch)
step 24: (39330.5 examples/sec; 0.104 sec/batch)
step 25: (38783.9 examples/sec; 0.106 sec/batch)
step 26: (39038.9 examples/sec; 0.105 sec/batch)
step 27: (39214.2 examples/sec; 0.104 sec/batch)
step 28: (39525.9 examples/sec; 0.104 sec/batch)
step 29: (37209.0 examples/sec; 0.110 sec/batch)
step 30: (38356.7 examples/sec; 0.107 sec/batch)
step 31: (36077.0 examples/sec; 0.114 sec/batch)
step 32: (37143.8 examples/sec; 0.110 sec/batch)
step 33: (35961.1 examples/sec; 0.114 sec/batch)
step 34: (33378.4 examples/sec; 0.123 sec/batch)
step 35: (37830.3 examples/sec; 0.108 sec/batch)
step 36: (36789.5 examples/sec; 0.111 sec/batch)
step 37: (36638.2 examples/sec; 0.112 sec/batch)
step 38: (36848.1 examples/sec; 0.111 sec/batch)
step 39: (36041.4 examples/sec; 0.114 sec/batch)
step 40: (36612.0 examples/sec; 0.112 sec/batch)
step 41: (35623.9 examples/sec; 0.115 sec/batch)
step 42: (37589.3 examples/sec; 0.109 sec/batch)
step 43: (37462.9 examples/sec; 0.109 sec/batch)
step 44: (35823.6 examples/sec; 0.114 sec/batch)
step 45: (35911.8 examples/sec; 0.114 sec/batch)
step 46: (36073.8 examples/sec; 0.114 sec/batch)
step 47: (36930.2 examples/sec; 0.111 sec/batch)
step 48: (36142.9 examples/sec; 0.113 sec/batch)
...
step 99: (36434.8 examples/sec; 0.112 sec/batch)
step 100: (1215.0 examples/sec; 3.371 sec/batch)
step 101: (35952.9 examples/sec; 0.114 sec/batch)
step 102: (38422.5 examples/sec; 0.107 sec/batch)
step 103: (39315.8 examples/sec; 0.104 sec/batch)
step 104: (38989.1 examples/sec; 0.105 sec/batch)
step 105: (39091.4 examples/sec; 0.105 sec/batch)
step 106: (39247.6 examples/sec; 0.104 sec/batch)
step 107: (38009.7 examples/sec; 0.108 sec/batch)
step 108: (38746.7 examples/sec; 0.106 sec/batch)
step 109: (39505.4 examples/sec; 0.104 sec/batch)
step 110: (39340.0 examples/sec; 0.104 sec/batch)
step 111: (39065.0 examples/sec; 0.105 sec/batch)
step 112: (38561.1 examples/sec; 0.106 sec/batch)
step 113: (39109.0 examples/sec; 0.105 sec/batch)
step 114: (39203.7 examples/sec; 0.104 sec/batch)
step 115: (39144.4 examples/sec; 0.105 sec/batch)
step 116: (38317.6 examples/sec; 0.107 sec/batch)
step 117: (33757.5 examples/sec; 0.121 sec/batch)
step 118: (34115.4 examples/sec; 0.120 sec/batch)
step 119: (35671.4 examples/sec; 0.115 sec/batch)
step 120: (35297.2 examples/sec; 0.116 sec/batch)
step 121: (36152.8 examples/sec; 0.113 sec/batch)
step 122: (35780.1 examples/sec; 0.114 sec/batch)
step 123: (35847.1 examples/sec; 0.114 sec/batch)
step 124: (36888.9 examples/sec; 0.111 sec/batch)
step 125: (36946.2 examples/sec; 0.111 sec/batch)
...

	</description>
	<comments>
		<comment id='1' author='MarkSonn' date='2018-01-03T02:07:55Z'>
		Perhaps a checkpoint is being written or something? &lt;denchmark-link:https://github.com/nealwu&gt;@nealwu&lt;/denchmark-link&gt;
 do you know who owns the CIFAR10 tutorial?
		</comment>
		<comment id='2' author='MarkSonn' date='2018-01-03T02:14:10Z'>
		I don't think anyone is particularly maintaining that code. For CIFAR-10 I would recommend looking at our &lt;denchmark-link:https://github.com/tensorflow/models/blob/master/official/resnet/cifar10_main.py&gt;example in the official models&lt;/denchmark-link&gt;
 instead. As a plus, it also achieves much higher accuracy.
		</comment>
		<comment id='3' author='MarkSonn' date='2018-01-03T02:33:21Z'>
		If you're looking for multi-GPU, you can also check out &lt;denchmark-link:https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator&gt;cifar10_estimator&lt;/denchmark-link&gt;
.
And for the future you should open models issues in the &lt;denchmark-link:https://github.com/tensorflow/models&gt;models repo&lt;/denchmark-link&gt;
.
Does the code slow down every 100 steps or every 10 steps? 10 steps would make more sense because the file has an argument called &lt;denchmark-link:https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_train.py#L57&gt;log_frequency&lt;/denchmark-link&gt;
 which defaults to 10.
		</comment>
		<comment id='4' author='MarkSonn' date='2018-01-03T08:55:13Z'>
		Thanks &lt;denchmark-link:https://github.com/skye&gt;@skye&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/nealwu&gt;@nealwu&lt;/denchmark-link&gt;
 I have searched through every file for what is written every 100 steps (even used the search functionality for things like "100" and "%" in cifar10_train.py, cifar10.py and cifar10_input.py which are the 3 files that run) and haven't found anything useful.
Good idea &lt;denchmark-link:https://github.com/nealwu&gt;@nealwu&lt;/denchmark-link&gt;
 I've looked at those models briefly but I'll consider switching over to them. It's not the log frequency, I have set my log frequency to 1 for those logs above so that you can see that the issue is every 100 steps.
		</comment>
		<comment id='5' author='MarkSonn' date='2018-01-16T12:52:03Z'>
		Now that I have optimized my algorithm this slows it by over 30%. Is it possible to assign someone please?
		</comment>
		<comment id='6' author='MarkSonn' date='2018-01-23T23:04:43Z'>
		A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.
		</comment>
		<comment id='7' author='MarkSonn' date='2018-01-31T00:33:59Z'>
		&lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
 is that something that we're aware of?
		</comment>
		<comment id='8' author='MarkSonn' date='2018-01-31T01:39:04Z'>
		With queues deprecated, I would highly suggest using the model Neal suggested which has multi-gpu done decently as well as flipping data formats for CPU vs. GPU and uses tf.data.  It also likely trains to a better accuracy.  I do not want to make any wild guesses about step 100.
Step 0 likely includes starting up the queue and preparing part of the data.  Debugging this on the cifar10_estimator would be better, it also has better support all around.  I worked to get the cifar10_estimator version created because I didn't want to look at this example anymore.  :-)
		</comment>
		<comment id='9' author='MarkSonn' date='2018-01-31T02:39:47Z'>
		Thanks &lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
 ! Closing then, since we have a better recommended way.
		</comment>
	</comments>
</bug>