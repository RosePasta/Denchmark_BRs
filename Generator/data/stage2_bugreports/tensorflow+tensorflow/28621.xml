<bug id='28621' author='SunYanCN' open_date='2019-05-11T10:36:19Z' closed_time='2019-05-21T21:27:00Z'>
	<summary>About transformer</summary>
	<description>
transformer:&lt;denchmark-link:https://www.tensorflow.org/alpha/tutorials/text/transformer#top_of_page&gt;https://www.tensorflow.org/alpha/tutorials/text/transformer#top_of_page&lt;/denchmark-link&gt;

Has anyone run this experiment, and the results of my run have not reached the official results. I posted my code and helped me find the reason.
import tensorflow_datasets as tfds
import tensorflow as tf

import time
import numpy as np
import matplotlib.pyplot as plt

from tqdm.auto import tqdm
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

print(tf.__version__)

if tf.test.is_gpu_available():
    device = "/gpu:0"
else:
    device = "/cpu:0"


print("(1):Reading dataset and token......")
examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,as_supervised=True)
train_examples, val_examples = examples['train'], examples['validation']

tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(
    (en.numpy() for pt, en in train_examples), target_vocab_size=2 ** 13)

tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(
    (pt.numpy() for pt, en in train_examples), target_vocab_size=2 ** 13)


BUFFER_SIZE = 20000
BATCH_SIZE = 64

"""Add a start and end token to the input and target."""


def encode(lang1, lang2):
    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(
        lang1.numpy()) + [tokenizer_pt.vocab_size + 1]

    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(
        lang2.numpy()) + [tokenizer_en.vocab_size + 1]

    return lang1, lang2


"""Note: To keep this example small and relatively fast, drop examples with a length of over 40 tokens."""

MAX_LENGTH = 40


def filter_max_length(x, y, max_length=MAX_LENGTH):
    return tf.logical_and(tf.size(x) &lt;= max_length,
                          tf.size(y) &lt;= max_length)


"""Operations inside `.map()` run in graph mode and receive a graph tensor that do not have a numpy attribute. The `tokenizer` expects a string or Unicode symbol to encode it into integers. Hence, you need to run the encoding inside a `tf.py_function`, which receives an eager tensor having a numpy attribute that contains the string value."""


def tf_encode(pt, en):
    return tf.py_function(encode, [pt, en], [tf.int64, tf.int64])

print("(2):Encode and padded batch......")
train_dataset = train_examples.map(tf_encode)
train_dataset = train_dataset.filter(filter_max_length)
# cache the dataset to memory to get a speedup while reading from it.
train_dataset = train_dataset.cache()
train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(
    BATCH_SIZE, padded_shapes=([-1], [-1]))
train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)

val_dataset = val_examples.map(tf_encode)
val_dataset = val_dataset.filter(filter_max_length).padded_batch(
    BATCH_SIZE, padded_shapes=([-1], [-1]))



def get_angles(pos, i, d_model):
    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))
    return pos * angle_rates


def positional_encoding(position, d_model):
    angle_rads = get_angles(np.arange(position)[:, np.newaxis],
                            np.arange(d_model)[np.newaxis, :],
                            d_model)

    # apply sin to even indices in the array; 2i
    sines = np.sin(angle_rads[:, 0::2])

    # apply cos to odd indices in the array; 2i+1
    cosines = np.cos(angle_rads[:, 1::2])

    pos_encoding = np.concatenate([sines, cosines], axis=-1)

    pos_encoding = pos_encoding[np.newaxis, ...]

    return tf.cast(pos_encoding, dtype=tf.float32)


def create_padding_mask(seq):
    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)

    # add extra dimensions so that we can add the padding
    # to the attention logits.
    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)



def create_look_ahead_mask(size):
    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)
    return mask  # (seq_len, seq_len)



def scaled_dot_product_attention(q, k, v, mask):
    """Calculate the attention weights.
    q, k, v must have matching leading dimensions.
    The mask has different shapes depending on its type(padding or look ahead)
    but it must be broadcastable for addition.

    Args:
      q: query shape == (..., seq_len_q, depth)
      k: key shape == (..., seq_len_k, depth)
      v: value shape == (..., seq_len_v, depth)
      mask: Float tensor with shape broadcastable
            to (..., seq_len_q, seq_len_k). Defaults to None.

    Returns:
      output, attention_weights
    """

    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)

    # scale matmul_qk
    dk = tf.cast(tf.shape(k)[-1], tf.float32)
    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)

    # add the mask to the scaled tensor.
    if mask is not None:
        scaled_attention_logits += (mask * -1e9)

    # softmax is normalized on the last axis (seq_len_k) so that the scores
    # add up to 1.
    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)

    output = tf.matmul(attention_weights, v)  # (..., seq_len_v, depth)

    return output, attention_weights

class MultiHeadAttention(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.num_heads = num_heads
        self.d_model = d_model

        assert d_model % self.num_heads == 0

        self.depth = d_model // self.num_heads

        self.wq = tf.keras.layers.Dense(d_model)
        self.wk = tf.keras.layers.Dense(d_model)
        self.wv = tf.keras.layers.Dense(d_model)

        self.dense = tf.keras.layers.Dense(d_model)

    def split_heads(self, x, batch_size):
        """Split the last dimension into (num_heads, depth).
        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)
        """
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, v, k, q, mask):
        batch_size = tf.shape(q)[0]

        q = self.wq(q)  # (batch_size, seq_len, d_model)
        k = self.wk(k)  # (batch_size, seq_len, d_model)
        v = self.wv(v)  # (batch_size, seq_len, d_model)

        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)
        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)
        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)

        # scaled_attention.shape == (batch_size, num_heads, seq_len_v, depth)
        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)
        scaled_attention, attention_weights = scaled_dot_product_attention(
            q, k, v, mask)

        scaled_attention = tf.transpose(scaled_attention,
                                        perm=[0, 2, 1, 3])  # (batch_size, seq_len_v, num_heads, depth)

        concat_attention = tf.reshape(scaled_attention,
                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_v, d_model)

        output = self.dense(concat_attention)  # (batch_size, seq_len_v, d_model)

        return output, attention_weights


def point_wise_feed_forward_network(d_model, dff):
    return tf.keras.Sequential([
        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)
        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)
    ])

class EncoderLayer(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, dff, rate=0.1):
        super(EncoderLayer, self).__init__()

        self.mha = MultiHeadAttention(d_model, num_heads)
        self.ffn = point_wise_feed_forward_network(d_model, dff)

        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)

        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)

    def call(self, x, training, mask):
        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)

        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)
        ffn_output = self.dropout2(ffn_output, training=training)
        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)

        return out2


class DecoderLayer(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, dff, rate=0.1):
        super(DecoderLayer, self).__init__()

        self.mha1 = MultiHeadAttention(d_model, num_heads)
        self.mha2 = MultiHeadAttention(d_model, num_heads)

        self.ffn = point_wise_feed_forward_network(d_model, dff)

        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)

        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)
        self.dropout3 = tf.keras.layers.Dropout(rate)

    def call(self, x, enc_output, training,
             look_ahead_mask, padding_mask):
        # enc_output.shape == (batch_size, input_seq_len, d_model)

        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)
        attn1 = self.dropout1(attn1, training=training)
        out1 = self.layernorm1(attn1 + x)

        attn2, attn_weights_block2 = self.mha2(
            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)
        attn2 = self.dropout2(attn2, training=training)
        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)

        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)
        ffn_output = self.dropout3(ffn_output, training=training)
        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)

        return out3, attn_weights_block1, attn_weights_block2


class Encoder(tf.keras.layers.Layer):
    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,
                 rate=0.1):
        super(Encoder, self).__init__()

        self.d_model = d_model
        self.num_layers = num_layers

        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)
        self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)

        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)
                           for _ in range(num_layers)]

        self.dropout = tf.keras.layers.Dropout(rate)

    def call(self, x, training, mask):
        seq_len = tf.shape(x)[1]

        # adding embedding and position encoding.
        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)
        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        x += self.pos_encoding[:, :seq_len, :]

        x = self.dropout(x, training=training)

        for i in range(self.num_layers):
            x = self.enc_layers[i](x, training, mask)

        return x  # (batch_size, input_seq_len, d_model)


class Decoder(tf.keras.layers.Layer):
    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,
                 rate=0.1):
        super(Decoder, self).__init__()

        self.d_model = d_model
        self.num_layers = num_layers

        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)
        self.pos_encoding = positional_encoding(target_vocab_size, self.d_model)

        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)
                           for _ in range(num_layers)]
        self.dropout = tf.keras.layers.Dropout(rate)

    def call(self, x, enc_output, training,
             look_ahead_mask, padding_mask):
        seq_len = tf.shape(x)[1]
        attention_weights = {}

        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)
        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        x += self.pos_encoding[:, :seq_len, :]

        x = self.dropout(x, training=training)

        for i in range(self.num_layers):
            x, block1, block2 = self.dec_layers[i](x, enc_output, training,
                                                   look_ahead_mask, padding_mask)

            attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1
            attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2

        # x.shape == (batch_size, target_seq_len, d_model)
        return x, attention_weights


"""## Create the Transformer

Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned.
"""


class Transformer(tf.keras.Model):
    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,
                 target_vocab_size, rate=0.1):
        super(Transformer, self).__init__()

        self.encoder = Encoder(num_layers, d_model, num_heads, dff,
                               input_vocab_size, rate)

        self.decoder = Decoder(num_layers, d_model, num_heads, dff,
                               target_vocab_size, rate)

        self.final_layer = tf.keras.layers.Dense(target_vocab_size)

    def call(self, inp, tar, training, enc_padding_mask,
             look_ahead_mask, dec_padding_mask):
        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)

        # dec_output.shape == (batch_size, tar_seq_len, d_model)
        dec_output, attention_weights = self.decoder(
            tar, enc_output, training, look_ahead_mask, dec_padding_mask)

        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)

        return final_output, attention_weights


num_layers = 4
d_model = 128
dff = 512
num_heads = 8

input_vocab_size = tokenizer_pt.vocab_size + 2
target_vocab_size = tokenizer_en.vocab_size + 2
dropout_rate = 0.1


class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, d_model, warmup_steps=4000):
        super(CustomSchedule, self).__init__()

        self.d_model = d_model
        self.d_model = tf.cast(self.d_model, tf.float32)

        self.warmup_steps = warmup_steps

    def __call__(self, step):
        arg1 = tf.math.rsqrt(step)
        arg2 = step * (self.warmup_steps ** -1.5)

        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)


learning_rate = CustomSchedule(d_model)

optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,
                                     epsilon=1e-9)

temp_learning_rate_schedule = CustomSchedule(d_model)

plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))
plt.ylabel("Learning Rate")
plt.xlabel("Train Step")

"""## Loss and metrics

Since the target sequences are padded, it is important to apply a padding mask when calculating the loss.
"""

loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none')


def loss_function(real, pred):
    mask = tf.math.logical_not(tf.math.equal(real, 0))
    loss_ = loss_object(real, pred)

    mask = tf.cast(mask, dtype=loss_.dtype)
    loss_ *= mask

    return tf.reduce_mean(loss_)


train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')
val_loss = tf.keras.metrics.Mean(name='val_loss')
val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')

"""## Training and checkpointing"""

transformer = Transformer(num_layers, d_model, num_heads, dff,
                          input_vocab_size, target_vocab_size, dropout_rate)


def create_masks(inp, tar):
    # Encoder padding mask
    enc_padding_mask = create_padding_mask(inp)

    # Used in the 2nd attention block in the decoder.
    # This padding mask is used to mask the encoder outputs.
    dec_padding_mask = create_padding_mask(inp)

    # Used in the 1st attention block in the decoder.
    # It is used to pad and mask future tokens in the input received by
    # the decoder.
    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])
    dec_target_padding_mask = create_padding_mask(tar)
    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)

    return enc_padding_mask, combined_mask, dec_padding_mask


"""Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every `n` epochs."""

checkpoint_path = "./checkpoints/train"

ckpt = tf.train.Checkpoint(transformer=transformer,
                           optimizer=optimizer)

ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)

# if a checkpoint exists, restore the latest checkpoint.
if ckpt_manager.latest_checkpoint:
    ckpt.restore(ckpt_manager.latest_checkpoint)
    print('Latest checkpoint restored!!')


EPOCHS = 200


train_num = len([1 for _, _ in train_dataset])

@tf.function
def train_step(inp, tar):
    tar_inp = tar[:, :-1]
    tar_real = tar[:, 1:]

    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)

    with tf.GradientTape() as tape:
        predictions, _ = transformer(inp, tar_inp,
                                     True,
                                     enc_padding_mask,
                                     combined_mask,
                                     dec_padding_mask)
        loss = loss_function(tar_real, predictions)

    gradients = tape.gradient(loss, transformer.trainable_variables)
    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))

    train_loss(loss)
    train_accuracy(tar_real, predictions)

@tf.function
def val_step(inp, tar):
    tar_inp = tar[:, :-1]
    tar_real = tar[:, 1:]
    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)
    predictions, _ = transformer(inp,tar_inp,
                                enc_padding_mask=enc_padding_mask,
                                look_ahead_mask=combined_mask,
                                dec_padding_mask=dec_padding_mask,
                                training=False)
    loss = loss_function(tar_real, predictions)
    ppl = tf.exp(loss)
    val_loss(ppl)
    val_accuracy(tar_real, predictions)

print("(3):Traning model......")
"""Portuguese is used as the input language and English is the target language."""

for epoch in range(EPOCHS):
    train_loss.reset_states()
    train_accuracy.reset_states()
    val_loss.reset_states()
    val_accuracy.reset_states()

    print('Epoch {}'.format(epoch + 1))
    start = time.time()
    # inp -&gt; portuguese, tar -&gt; english
    with tqdm(total=train_num * BATCH_SIZE) as pbar:
        for inp, tar in train_dataset:
            train_step(inp, tar)
            pbar.update(BATCH_SIZE)

    for inp, tar in val_dataset:
        val_step(inp, tar)

    end = time.time()
    print('train_loss {:.4f}\ttrain_acc {:.2f}\t'
          'val_loss {:.4f}\tval_acc {:.2f}\t'
          'time {:.2f}s'.format(train_loss.result(),
                                train_accuracy.result() * 100,
                                val_loss.result(),
                                val_accuracy.result() * 100,
                                end - start,
                                ))


def evaluate(inp_sentence):
    start_token = [tokenizer_pt.vocab_size]
    end_token = [tokenizer_pt.vocab_size + 1]

    # inp sentence is portuguese, hence adding the start and end token
    inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token
    encoder_input = tf.expand_dims(inp_sentence, 0)

    # as the target is english, the first word to the transformer should be the
    # english start token.
    decoder_input = [tokenizer_en.vocab_size]
    output = tf.expand_dims(decoder_input, 0)

    for i in range(MAX_LENGTH):
        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(
            encoder_input, output)

        # predictions.shape == (batch_size, seq_len, vocab_size)
        predictions, attention_weights = transformer(encoder_input,
                                                     output,
                                                     False,
                                                     enc_padding_mask,
                                                     combined_mask,
                                                     dec_padding_mask)

        # select the last word from the seq_len dimension
        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)

        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)

        # return the result if the predicted_id is equal to the end token
        if tf.equal(predicted_id, tokenizer_en.vocab_size + 1):
            return tf.squeeze(output, axis=0), attention_weights

        # concatentate the predicted_id to the output which is given to the decoder
        # as its input.
        output = tf.concat([output, predicted_id], axis=-1)

    return tf.squeeze(output, axis=0), attention_weights


def plot_attention_weights(attention, sentence, result, layer):
    fig = plt.figure(figsize=(16, 8))

    sentence = tokenizer_pt.encode(sentence)

    attention = tf.squeeze(attention[layer], axis=0)

    for head in range(attention.shape[0]):
        ax = fig.add_subplot(2, 4, head + 1)

        # plot the attention weights
        ax.matshow(attention[head][:-1, :], cmap='viridis')

        fontdict = {'fontsize': 10}

        ax.set_xticks(range(len(sentence) + 2))
        ax.set_yticks(range(len(result)))

        ax.set_ylim(len(result) - 1.5, -0.5)

        ax.set_xticklabels(
            ['&lt;start&gt;'] + [tokenizer_pt.decode([i]) for i in sentence] + ['&lt;end&gt;'],
            fontdict=fontdict, rotation=90)

        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result
                            if i &lt; tokenizer_en.vocab_size],
                           fontdict=fontdict)

        ax.set_xlabel('Head {}'.format(head + 1))

    plt.tight_layout()
    plt.show()


def translate(sentence, plot=''):
    result, attention_weights = evaluate(sentence)

    predicted_sentence = tokenizer_en.decode([i for i in result
                                              if i &lt; tokenizer_en.vocab_size])

    print('Input: {}'.format(sentence))
    print('Predicted translation: {}'.format(predicted_sentence))

    if plot:
        plot_attention_weights(attention_weights, sentence, result, plot)

print("(4):Evaluate model......")
translate("este é um problema que temos que resolver.")
print("Real translation: this is a problem we have to solve .")

translate("os meus vizinhos ouviram sobre esta ideia.")
print("Real translation: and my neighboring homes heard about this idea .")

translate("vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.")
print(
    "Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .")

"""You can pass different layers and attention blocks of the decoder to the `plot` parameter."""

translate("este é o primeiro livro que eu fiz.", plot='decoder_layer4_block2')
print("Real translation: this is the first book i've ever done.")
	</description>
	<comments>
		<comment id='1' author='SunYanCN' date='2019-05-13T09:43:41Z'>
		&lt;denchmark-link:https://github.com/SunYanCN&gt;@SunYanCN&lt;/denchmark-link&gt;
 Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?
Make sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/new/choose&gt;the Github new issue template&lt;/denchmark-link&gt;
.
We ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!
		</comment>
		<comment id='2' author='SunYanCN' date='2019-05-13T13:35:11Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 I run &lt;denchmark-link:https://www.tensorflow.org/alpha/tutorials/text/transformer&gt;https://www.tensorflow.org/alpha/tutorials/text/transformer&lt;/denchmark-link&gt;
 in Calab with GPU.
		</comment>
		<comment id='3' author='SunYanCN' date='2019-05-14T08:35:24Z'>
		&lt;denchmark-link:https://github.com/SunYanCN&gt;@SunYanCN&lt;/denchmark-link&gt;
 Could you provide more details about the issue and context? Thanks!.
		</comment>
		<comment id='4' author='SunYanCN' date='2019-05-14T13:04:51Z'>
		Thank you for your reply, I will give more detailed results.
First run very slowly, this may be the reason for my machine - TeslaP4 8G GPU. The second official did not add a validation set, my code above added a validation set to see if the model is over-fitting. The result of the third run was that the fit began after about 20 epoch, and the correct rate was only about 28%. I cannot guarantee the complete reproducibility of the results due to some random factors. Below are the results of my run on colab and the results of the local run, the parameters are consistent with the official except for epoch. The version of TF is: tf-nightly-gpu-2.0-preview.
official:
Epoch 1 Batch 0 Loss 4.4091 Accuracy 0.0000
Epoch 1 Batch 500 Loss 2.8389 Accuracy 0.4932
Epoch 1 Loss 2.4228 Accuracy 0.5203
Time taken for 1 epoch: 319.307568073 secs

Epoch 2 Batch 0 Loss 1.3543 Accuracy 0.6036
Epoch 2 Batch 500 Loss 1.1689 Accuracy 0.6405
Epoch 2 Loss 1.1440 Accuracy 0.6460
Time taken for 1 epoch: 251.259027004 secs

Epoch 3 Batch 0 Loss 1.1281 Accuracy 0.6554
Epoch 3 Batch 500 Loss 1.0244 Accuracy 0.6723
Epoch 3 Loss 1.0115 Accuracy 0.6754
Time taken for 1 epoch: 70.5513730049 secs

Epoch 4 Batch 0 Loss 1.0113 Accuracy 0.6764
Epoch 4 Batch 500 Loss 0.9192 Accuracy 0.6975
Epoch 4 Loss 0.9037 Accuracy 0.7019
Time taken for 1 epoch: 70.917550087 secs

Epoch 5 Batch 0 Loss 0.9030 Accuracy 0.7027
Epoch 5 Batch 500 Loss 0.8099 Accuracy 0.7260
Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1
Epoch 5 Loss 0.7974 Accuracy 0.7293
Time taken for 1 epoch: 73.0342350006 secs

Epoch 6 Batch 0 Loss 0.8077 Accuracy 0.7360
Epoch 6 Batch 500 Loss 0.7201 Accuracy 0.7475
Epoch 6 Loss 0.7084 Accuracy 0.7503
Time taken for 1 epoch: 70.6219291687 secs

Epoch 7 Batch 0 Loss 0.7275 Accuracy 0.7451
Epoch 7 Batch 500 Loss 0.6304 Accuracy 0.7688
Epoch 7 Loss 0.6182 Accuracy 0.7719
Time taken for 1 epoch: 72.2072319984 secs

Epoch 8 Batch 0 Loss 0.6404 Accuracy 0.7730
Epoch 8 Batch 500 Loss 0.5517 Accuracy 0.7887
Epoch 8 Loss 0.5430 Accuracy 0.7911
Time taken for 1 epoch: 70.9613239765 secs

Epoch 9 Batch 0 Loss 0.5784 Accuracy 0.7829
Epoch 9 Batch 500 Loss 0.4962 Accuracy 0.8035
Epoch 9 Loss 0.4900 Accuracy 0.8052
Time taken for 1 epoch: 68.5947010517 secs

Epoch 10 Batch 0 Loss 0.5201 Accuracy 0.7956
Epoch 10 Batch 500 Loss 0.4545 Accuracy 0.8145
Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2
Epoch 10 Loss 0.4493 Accuracy 0.8161
Time taken for 1 epoch: 68.6737399101 secs

Epoch 11 Batch 0 Loss 0.4883 Accuracy 0.8055
Epoch 11 Batch 500 Loss 0.4206 Accuracy 0.8240
Epoch 11 Loss 0.4164 Accuracy 0.8251
Time taken for 1 epoch: 69.4070420265 secs

Epoch 12 Batch 0 Loss 0.4413 Accuracy 0.8195
Epoch 12 Batch 500 Loss 0.3937 Accuracy 0.8317
Epoch 12 Loss 0.3902 Accuracy 0.8328
Time taken for 1 epoch: 70.7441010475 secs

Epoch 13 Batch 0 Loss 0.4223 Accuracy 0.8236
Epoch 13 Batch 500 Loss 0.3716 Accuracy 0.8380
Epoch 13 Loss 0.3685 Accuracy 0.8389
Time taken for 1 epoch: 71.3240449429 secs

Epoch 14 Batch 0 Loss 0.4037 Accuracy 0.8265
Epoch 14 Batch 500 Loss 0.3511 Accuracy 0.8442
Epoch 14 Loss 0.3483 Accuracy 0.8450
Time taken for 1 epoch: 75.1278469563 secs

Epoch 15 Batch 0 Loss 0.3782 Accuracy 0.8331
Epoch 15 Batch 500 Loss 0.3339 Accuracy 0.8493
Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3
Epoch 15 Loss 0.3318 Accuracy 0.8499
Time taken for 1 epoch: 71.4256718159 secs

Epoch 16 Batch 0 Loss 0.3577 Accuracy 0.8409
Epoch 16 Batch 500 Loss 0.3195 Accuracy 0.8537
Epoch 16 Loss 0.3172 Accuracy 0.8544
Time taken for 1 epoch: 70.8179049492 secs

Epoch 17 Batch 0 Loss 0.3447 Accuracy 0.8466
Epoch 17 Batch 500 Loss 0.3055 Accuracy 0.8579
Epoch 17 Loss 0.3033 Accuracy 0.8587
Time taken for 1 epoch: 68.7967669964 secs

Epoch 18 Batch 0 Loss 0.3385 Accuracy 0.8487
Epoch 18 Batch 500 Loss 0.2931 Accuracy 0.8620
Epoch 18 Loss 0.2910 Accuracy 0.8626
Time taken for 1 epoch: 67.865557909 secs

Epoch 19 Batch 0 Loss 0.3198 Accuracy 0.8503
Epoch 19 Batch 500 Loss 0.2818 Accuracy 0.8657
Epoch 19 Loss 0.2797 Accuracy 0.8665
Time taken for 1 epoch: 67.9785480499 secs

Epoch 20 Batch 0 Loss 0.3110 Accuracy 0.8557
Epoch 20 Batch 500 Loss 0.2726 Accuracy 0.8684
Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4
Epoch 20 Loss 0.2706 Accuracy 0.8692
Time taken for 1 epoch: 71.4560930729 secs
Colab(No validation set):
Epoch 1 Batch 0 Loss 4.4481 Accuracy 0.0000
Epoch 1 Batch 500 Loss 3.6211 Accuracy 0.0405
Epoch 1 Loss 3.3870 Accuracy 0.0524
Time taken for 1 epoch: 630.2422263622284 secs

Epoch 2 Batch 0 Loss 2.7912 Accuracy 0.1001
Epoch 2 Batch 500 Loss 2.4382 Accuracy 0.1176
Epoch 2 Loss 2.3831 Accuracy 0.1230
Time taken for 1 epoch: 495.0642912387848 secs

Epoch 3 Batch 0 Loss 2.3392 Accuracy 0.1482
Epoch 3 Batch 500 Loss 2.1384 Accuracy 0.1470
Epoch 3 Loss 2.1120 Accuracy 0.1501
Time taken for 1 epoch: 148.81153535842896 secs

Epoch 4 Batch 0 Loss 2.1304 Accuracy 0.1727
Epoch 4 Batch 500 Loss 1.9199 Accuracy 0.1714
Epoch 4 Loss 1.8868 Accuracy 0.1758
Time taken for 1 epoch: 125.57066106796265 secs

Epoch 5 Batch 0 Loss 1.8778 Accuracy 0.2010
Epoch 5 Batch 500 Loss 1.6968 Accuracy 0.2003
Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1
Epoch 5 Loss 1.6701 Accuracy 0.2036
Time taken for 1 epoch: 116.66682600975037 secs

Epoch 6 Batch 0 Loss 1.6858 Accuracy 0.2192
Epoch 6 Batch 500 Loss 1.5010 Accuracy 0.2218
Epoch 6 Loss 1.4778 Accuracy 0.2238
Time taken for 1 epoch: 86.72514414787292 secs

Epoch 7 Batch 0 Loss 1.4574 Accuracy 0.2340
Epoch 7 Batch 500 Loss 1.3128 Accuracy 0.2439
Epoch 7 Loss 1.2905 Accuracy 0.2460
Time taken for 1 epoch: 87.45780897140503 secs

Epoch 8 Batch 0 Loss 1.2689 Accuracy 0.2669
Epoch 8 Batch 500 Loss 1.1475 Accuracy 0.2630
Epoch 8 Loss 1.1349 Accuracy 0.2645
Time taken for 1 epoch: 101.89115715026855 secs

Epoch 9 Batch 0 Loss 1.1215 Accuracy 0.2834
Epoch 9 Batch 500 Loss 1.0331 Accuracy 0.2781
Epoch 9 Loss 1.0247 Accuracy 0.2788
Time taken for 1 epoch: 86.37149453163147 secs

Epoch 10 Batch 0 Loss 1.0124 Accuracy 0.2965
Epoch 10 Batch 500 Loss 0.9445 Accuracy 0.2893
Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2
Epoch 10 Loss 0.9383 Accuracy 0.2893
Time taken for 1 epoch: 79.97530317306519 secs

Epoch 11 Batch 0 Loss 0.9529 Accuracy 0.3049
Epoch 11 Batch 500 Loss 0.8746 Accuracy 0.2988
Epoch 11 Loss 0.8715 Accuracy 0.2988
Time taken for 1 epoch: 81.03851366043091 secs

Epoch 12 Batch 0 Loss 0.8880 Accuracy 0.3129
Epoch 12 Batch 500 Loss 0.8198 Accuracy 0.3067
Epoch 12 Loss 0.8160 Accuracy 0.3060
Time taken for 1 epoch: 101.49828553199768 secs

Epoch 13 Batch 0 Loss 0.8088 Accuracy 0.3197
Epoch 13 Batch 500 Loss 0.7700 Accuracy 0.3118
Epoch 13 Loss 0.7684 Accuracy 0.3118
Time taken for 1 epoch: 86.86038994789124 secs

Epoch 14 Batch 0 Loss 0.7651 Accuracy 0.3243
Epoch 14 Batch 500 Loss 0.7282 Accuracy 0.3185
Epoch 14 Loss 0.7279 Accuracy 0.3180
Time taken for 1 epoch: 81.0619957447052 secs

Epoch 15 Batch 0 Loss 0.7379 Accuracy 0.3349
Epoch 15 Batch 500 Loss 0.6923 Accuracy 0.3238
Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3
Epoch 15 Loss 0.6937 Accuracy 0.3234
Time taken for 1 epoch: 80.88019394874573 secs

Epoch 16 Batch 0 Loss 0.6983 Accuracy 0.3412
Epoch 16 Batch 500 Loss 0.6617 Accuracy 0.3287
Epoch 16 Loss 0.6611 Accuracy 0.3279
Time taken for 1 epoch: 79.34229493141174 secs

Epoch 17 Batch 0 Loss 0.6588 Accuracy 0.3476
Epoch 17 Batch 500 Loss 0.6292 Accuracy 0.3320
Epoch 17 Loss 0.6326 Accuracy 0.3317
Time taken for 1 epoch: 79.55837631225586 secs

Epoch 18 Batch 0 Loss 0.6609 Accuracy 0.3509
Epoch 18 Batch 500 Loss 0.6054 Accuracy 0.3357
Epoch 18 Loss 0.6078 Accuracy 0.3356
Time taken for 1 epoch: 88.51137733459473 secs

Epoch 19 Batch 0 Loss 0.6138 Accuracy 0.3450
Epoch 19 Batch 500 Loss 0.5845 Accuracy 0.3404
Epoch 19 Loss 0.5851 Accuracy 0.3398
Time taken for 1 epoch: 79.66434621810913 secs

Epoch 20 Batch 0 Loss 0.5793 Accuracy 0.3594
Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4
Epoch 20 Loss 0.5634 Accuracy 0.3428
Time taken for 1 epoch: 79.7306261062622 secs
Localization (validation set):
Epoch 1
100%|█████████████████████████████████████| 44992/44992 [11:27&lt;00:00, 12.72it/s]
train_ppl 34.2444	train_acc 4.97	val_ppl 14.3158	val_acc 9.88	time 705.11s
Epoch 2
100%|████████████████████████████████████| 44992/44992 [10:28&lt;00:00, 127.29it/s]
train_ppl 10.8922	train_acc 12.51	val_ppl 9.3847	val_acc 14.07	time 630.37s
Epoch 3
100%|████████████████████████████████████| 44992/44992 [03:52&lt;00:00, 232.76it/s]
train_ppl 8.2826	train_acc 15.14	val_ppl 7.7277	val_acc 16.09	time 234.72s
Epoch 4
100%|████████████████████████████████████| 44992/44992 [03:46&lt;00:00, 199.87it/s]
train_ppl 6.5978	train_acc 17.71	val_ppl 6.1918	val_acc 18.77	time 228.85s
Epoch 5
100%|████████████████████████████████████| 44992/44992 [03:49&lt;00:00, 200.55it/s]
train_ppl 5.2749	train_acc 20.39	val_ppl 5.1585	val_acc 21.00	time 231.67s
Saving checkpoint for epoch 5 at ./checkpoints/o_train/ckpt-1
Epoch 6
100%|████████████████████████████████████| 44992/44992 [03:43&lt;00:00, 213.87it/s]
train_ppl 4.3767	train_acc 22.41	val_ppl 4.5751	val_acc 22.49	time 225.89s
Epoch 7
100%|████████████████████████████████████| 44992/44992 [03:42&lt;00:00, 218.24it/s]
train_ppl 3.6294	train_acc 24.53	val_ppl 3.9920	val_acc 24.09	time 224.32s
Epoch 8
100%|████████████████████████████████████| 44992/44992 [03:46&lt;00:00, 232.85it/s]
train_ppl 3.0980	train_acc 26.47	val_ppl 3.7746	val_acc 24.79	time 228.97s
Epoch 9
100%|████████████████████████████████████| 44992/44992 [03:44&lt;00:00, 239.29it/s]
train_ppl 2.7678	train_acc 27.87	val_ppl 3.5705	val_acc 25.64	time 226.23s
Epoch 10
100%|████████████████████████████████████| 44992/44992 [03:43&lt;00:00, 230.09it/s]
train_ppl 2.5436	train_acc 28.94	val_ppl 3.4663	val_acc 26.20	time 225.23s
Saving checkpoint for epoch 10 at ./checkpoints/o_train/ckpt-2
Epoch 11
100%|████████████████████████████████████| 44992/44992 [03:40&lt;00:00, 245.01it/s]
train_ppl 2.3776	train_acc 29.83	val_ppl 3.4482	val_acc 26.31	time 222.26s
Epoch 12
100%|████████████████████████████████████| 44992/44992 [03:41&lt;00:00, 257.37it/s]
train_ppl 2.2500	train_acc 30.59	val_ppl 3.4032	val_acc 26.55	time 223.20s
Epoch 13
100%|████████████████████████████████████| 44992/44992 [03:39&lt;00:00, 255.28it/s]
train_ppl 2.1497	train_acc 31.18	val_ppl 3.3787	val_acc 26.71	time 221.46s
Epoch 14
100%|████████████████████████████████████| 44992/44992 [03:38&lt;00:00, 233.30it/s]
train_ppl 2.0664	train_acc 31.75	val_ppl 3.3953	val_acc 26.88	time 220.44s
Epoch 15
100%|████████████████████████████████████| 44992/44992 [03:36&lt;00:00, 232.35it/s]
train_ppl 1.9903	train_acc 32.33	val_ppl 3.4401	val_acc 26.74	time 218.78s
Saving checkpoint for epoch 15 at ./checkpoints/o_train/ckpt-3
Epoch 16
100%|████████████████████████████████████| 44992/44992 [03:38&lt;00:00, 251.13it/s]
train_ppl 1.9304	train_acc 32.77	val_ppl 3.4418	val_acc 26.79	time 220.73s
Epoch 17
100%|████████████████████████████████████| 44992/44992 [03:39&lt;00:00, 258.43it/s]
train_ppl 1.8757	train_acc 33.18	val_ppl 3.4877	val_acc 26.71	time 221.12s
Epoch 18
100%|████████████████████████████████████| 44992/44992 [03:37&lt;00:00, 254.54it/s]
train_ppl 1.8315	train_acc 33.56	val_ppl 3.5823	val_acc 26.55	time 219.96s
Epoch 19
100%|████████████████████████████████████| 44992/44992 [03:38&lt;00:00, 257.17it/s]
train_ppl 1.7889	train_acc 33.94	val_ppl 3.6283	val_acc 26.21	time 220.20s
Epoch 20
100%|████████████████████████████████████| 44992/44992 [03:37&lt;00:00, 227.05it/s]
train_ppl 1.7540	train_acc 34.22	val_ppl 3.6021	val_acc 26.83	time 219.68s
Saving checkpoint for epoch 20 at ./checkpoints/o_train/ckpt-4

Process finished with exit code 0
I want to confirm if there is a problem with my code or if there is a problem with the official results.
		</comment>
		<comment id='5' author='SunYanCN' date='2019-05-14T13:06:02Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensorflowbutler&gt;@tensorflowbutler&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ry&gt;@ry&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jmhodges&gt;@jmhodges&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='SunYanCN' date='2019-05-14T13:06:56Z'>
		more details:&lt;denchmark-link:https://github.com/SDBurt/Transformer-TF/issues/1&gt;SDBurt/Transformer-TF#1&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='SunYanCN' date='2019-05-15T04:44:19Z'>
		&lt;denchmark-link:https://github.com/SunYanCN&gt;@SunYanCN&lt;/denchmark-link&gt;
 Thanks for the detailed information.
		</comment>
		<comment id='8' author='SunYanCN' date='2019-05-21T21:27:00Z'>
		Closing since a duplicate &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/28405&gt;#28405&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='SunYanCN' date='2019-05-21T21:27:03Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=28621&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=28621&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='SunYanCN' date='2019-06-27T14:39:38Z'>
		pt.numpy()
en.numpy()
they don't have attribute numpy()
somebody can help is the same transformer example of SunYan
		</comment>
	</comments>
</bug>