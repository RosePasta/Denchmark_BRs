<bug id='30882' author='durandg12' open_date='2019-07-19T16:35:05Z' closed_time='2020-02-11T00:18:14Z'>
	<summary>Dense does not flatten inputs with rank &amp;gt;2 and behaves exactly like TimeDistributed(Dense)</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6
TensorFlow installed from (source or binary): from pip install
TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14


A note in  &lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dense&gt;documentation&lt;/denchmark-link&gt;
 says that

Note: If the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel.

I don't see this happening in real life. Instead, Dense behaves on a 3-rank tensor as it would behave if it was wrapped in a TimeDistributed layer, making me question the utility of TimeDistributed at all.
Describe the expected behavior
Dense should flatten its input like the documentation says. In the first example bellow, the shape of the kernel weights of dense should be (5 * 3, 2) = (15, 2) instead of (3, 2), which is the shape of dense2 (as expected in the case of dense2).
Code to reproduce the issue
First example:
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np

print('Using Tensorflow version {} (git version {})'.format(tf.version.VERSION, tf.version.GIT_VERSION))

tf.random.set_seed(12)
np.random.seed(12)

init = tf.keras.initializers.GlorotUniform(seed=12)

inp = tf.constant(np.random.normal(0, 1, (1, 5, 6)))
inp = tf.cast(inp, dtype=tf.float32)

gru = tf.keras.layers.GRU(3, return_sequences=True)(inp)
print(gru.shape)
#(1, 5, 3)

dense = tf.keras.layers.Dense(2, kernel_initializer=init, bias_initializer=init)
print(dense(gru))
#tf.Tensor(
#[[[ 1.5456871  -0.5280464 ]
#  [ 0.11647969 -0.20553198]
#  [ 0.58126366 -0.16031623]
#  [-0.22882831 -0.22649539]
#  [ 0.62777793 -0.32470667]]], shape=(1, 5, 2), dtype=float32)

for w in dense.weights:
    print(w.shape)
#(3, 2) instead of (5 * 3, 2) if Dense indeed flattened its input
#(2,)

tddense = tf.keras.layers.TimeDistributed(dense)
print(tddense(gru))
#tf.Tensor(
#[[[ 1.5456871  -0.5280464 ]
#  [ 0.11647969 -0.20553198]
#  [ 0.58126366 -0.16031623]
#  [-0.22882831 -0.22649539]
#  [ 0.62777793 -0.32470667]]], shape=(1, 5, 2), dtype=float32)
# if Dense kernel had shape (15, 2), this should result in the following error:
# InvalidArgumentError: Matrix size-incompatible: In[0]: [5,3], In[1]: [15,2] [Op:MatMul]
# but instead what we get is the same output
# than without TimeDistributed, without error

dense2 = tf.keras.layers.Dense(2, kernel_initializer=init, bias_initializer=init)
tddense = tf.keras.layers.TimeDistributed(dense2)
print(tddense(gru))
#tf.Tensor(
#[[[ 1.5456871  -0.5280464 ]
#  [ 0.11647969 -0.20553198]
#  [ 0.58126366 -0.16031623]
#  [-0.22882831 -0.22649539]
#  [ 0.62777793 -0.32470667]]], shape=(1, 5, 2), dtype=float32)

for w in dense2.weights:
    print(w.shape)
#(3, 2) as expected
#(2,)
&lt;/denchmark-code&gt;

Second example, with a rank even larger than 3:
&lt;denchmark-code&gt;import tensorflow as tf

print('Using Tensorflow version {} (git version {})'.format(tf.version.VERSION, tf.version.GIT_VERSION))

inp = tf.keras.Input(shape=(10, 25, 25, 3))
dense_layer1 = tf.keras.layers.Dense(78)
x = dense_layer1(inp)
print('Output shape without TimeDistributed:')
print(x.shape)

dense_layer2 = tf.keras.layers.Dense(78)
y=tf.keras.layers.TimeDistributed(dense_layer2)(inp)
print('Output shape with TimeDistributed:')
print(y.shape)

print('Weight shapes without TimeDistributed:')
for weight in dense_layer1.trainable_weights:
    if len(weight.shape) == 2:
        print('    kernel shape:')
    else:
        print('    bias shape:')
    print(weight.shape)
    
print('Weight shapes with TimeDistributed:')
for weight in dense_layer2.trainable_weights:
    if len(weight.shape) == 2:
        print('    kernel shape:')
    else:
        print('    bias shape:')
    print(weight.shape)
&lt;/denchmark-code&gt;

which outputs is:
&lt;denchmark-code&gt;Using Tensorflow version 2.0.0-beta1 (git version v2.0.0-beta0-16-g1d91213fe7)
Output shape without TimeDistributed:
(None, 10, 25, 25, 78)
Output shape with TimeDistributed:
(None, 10, 25, 25, 78)
Weight shapes without TimeDistributed:
    kernel shape:
(3, 78)
    bias shape:
(78,)
Weight shapes with TimeDistributed:
    kernel shape:
(3, 78)
    bias shape:
(78,)
&lt;/denchmark-code&gt;

We see, in this example, that Dense and TimeDistributed(Dense) behave the same in that they only touch to the last dimension of the input.
	</description>
	<comments>
		<comment id='1' author='durandg12' date='2019-07-30T08:17:35Z'>
		Issue replicating with TF version-2.0.0beta1, please find the gist of &lt;denchmark-link:https://colab.sandbox.google.com/gist/oanush/864dcb1fde8c517b6f7beacc5a968178/30882.ipynb&gt;collab&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='durandg12' date='2019-08-01T22:32:33Z'>
		@anush-o  It says access denied. Please change your settings to enable it.
		</comment>
		<comment id='3' author='durandg12' date='2019-10-01T17:07:04Z'>
		The issue is still here in tf2.0.0.
		</comment>
		<comment id='4' author='durandg12' date='2020-02-10T20:49:13Z'>
		The note is incorrect, we will update the note to reflect the code behavior.
		</comment>
		<comment id='5' author='durandg12' date='2020-02-11T00:18:14Z'>
		Commit &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/2e6a3c58e4b96cac864f244e4886ef00b3184986#diff-5fb1fa5fa46d0ec9a01d5a60b7d8acc8&gt;2e6a3c5#diff-5fb1fa5fa46d0ec9a01d5a60b7d8acc8&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='durandg12' date='2020-02-11T00:18:16Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30882&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30882&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='durandg12' date='2020-02-11T11:10:45Z'>
		I appreciate how the new note is comprehensive &lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;
 .
But now I don't get the purpose of TimeDistributed anymore, as I don't see the difference between Dense and TimeDistributed(Dense).
		</comment>
		<comment id='8' author='durandg12' date='2020-02-11T20:44:51Z'>
		They are the same, TimeDistributed  doesn't just apply to Dense layers but i see that the main example in the TimeDistributed docs is using Dense layer. i'll update that.
		</comment>
		<comment id='9' author='durandg12' date='2020-02-13T01:17:42Z'>
		The change to TimeDistributed docs have also been submitted. Thank you!
		</comment>
	</comments>
</bug>