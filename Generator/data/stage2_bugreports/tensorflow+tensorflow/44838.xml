<bug id='44838' author='Moldoteck' open_date='2020-11-13T12:49:42Z' closed_time='2020-11-30T18:26:09Z'>
	<summary>Strange behavior for tf.keras.backend.function in tf 2.3.1 for intermediate values</summary>
	<description>
I was training an Xception model with custom output layers. So, clasic Xception model + dropout layer + dense layer alpha + dropout layer alpha + softmax dense layer for output.
After training, my goal is to split the model in half: Xception+dropout and dense layer alpha...softmax output.
For this, i am doing&gt;
&lt;denchmark-code&gt;model = load_model(path)
base_model= Model(inputs=model.input, outputs=model.get_layer('base_drop').output) - Works fine
second_half = K.function([model.get_layer('dense_alpha').input, K.learning_phase()],
                            [model.get_layer('dense_out').output]) - Error
&lt;/denchmark-code&gt;

The error is:
&lt;denchmark-code&gt;WARNING:tensorflow:Functional inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to "functional_4" was not an Input tensor, it was generated by layer base_drop.
Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.
The tensor that caused the issue was: base_drop/cond/Identity:0

ValueError: Input tensors to a Functional must come from `tf.keras.Input`. Received: 0 (missing previous layer metadata).

&lt;/denchmark-code&gt;

Also, if i am doing
&lt;denchmark-code&gt;second_half  = Model(inputs=model.get_layer('dense_alpha').input,
                            outputs=model.get_layer('dense_out').output)
&lt;/denchmark-code&gt;

I get the following error
Graph disconnected: cannot obtain value for tensor Tensor("input_1:0", shape=(None, 300, 480, 3), dtype=float32) at layer "xception". The following previous layers were accessed without issue: []
I didn't got these errors with tf 2.2.0 + i have read that the behavior of tf.keras.backend.function has changed in the 2.3.0
Is this a bug or it's something expected. In the second case, what should i change in order to achieve previous functionality?
I need this split in order to run base_model only one time with my test dataset and pass the results through the function multiple times with active second dropout
	</description>
	<comments>
		<comment id='1' author='Moldoteck' date='2020-11-13T13:23:43Z'>
		Do you have a very, very minimal standalone code (or Colab) code snippet that we could just copy and run to reproduce this?
		</comment>
		<comment id='2' author='Moldoteck' date='2020-11-13T13:34:35Z'>
		
Do you have a very, very minimal standalone code (or Colab) code snippet that we could just copy and run to reproduce this?

Is this ok?&gt;&gt;&gt;
&lt;denchmark-code&gt;
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.applications.xception import Xception
from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import backend as K

inp = Input((300,480,3))
base_model = Xception(include_top=False,
                              weights='imagenet',
                              input_tensor=inp,
                              pooling='avg')

for layer in base_model.layers[:65]:
	layer.trainable = False
for layer in base_model.layers[65:]:
	layer.trainable = True
	
	
x = base_model(inp)

x = Dropout(0.5, name='base_drop')(x)
dense_alpha = Dense(1024, activation='relu', name='dense_alpha')(x)
dense_alpha = Dropout(0.5, name='dense_alpha_drop')(dense_alpha)
dense_alpha = Dense(2, activation='softmax', name='dense_out')(dense_alpha)

losses = [tf.keras.losses.BinaryCrossentropy()]

model = Model(inputs=inp, outputs=[dense_alpha])
model.compile(optimizer=Adam(lr=0.0001),
			  loss=losses,
			  metrics=['accuracy'])

model_base = Model(inputs=model.input, outputs=model.get_layer('base_drop').output)
                
second_part = K.function([model.get_layer('dense_alpha').input, K.learning_phase()],
                            [model.get_layer('dense_out').output])
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='Moldoteck' date='2020-11-13T13:47:20Z'>
		The example it is enough thanks. It seems similar to &lt;denchmark-link:https://stackoverflow.com/questions/63238203/how-to-get-intermediate-outputs-in-tf-2-3-eager-with-learning-phase&gt;https://stackoverflow.com/questions/63238203/how-to-get-intermediate-outputs-in-tf-2-3-eager-with-learning-phase&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='Moldoteck' date='2020-11-13T13:54:13Z'>
		
The example it is enough thanks. It seems similar to https://stackoverflow.com/questions/63238203/how-to-get-intermediate-outputs-in-tf-2-3-eager-with-learning-phase

Yes. The thing is that if i will replace last line with
&lt;denchmark-code&gt;second_part = Model([model.get_layer('dense_alpha').input],
                            [model.get_layer('dense_out').output])
&lt;/denchmark-code&gt;

I'll get this error:
 ValueError: Graph disconnected: cannot obtain value for tensor Tensor("input_1:0", shape=(None, 300, 480, 3), dtype=float32) at layer "xception". The following previous layers were accessed without issue: []
So, the question is: is this a bug, or it is expected behavior. If it is expected, why it worked with previous tf versions?
P.S. from tensorflow.python.keras.backend import eager_learning_phase_scope will solve/hide the error, but still, it looks like a workaround instead of an explanation why it doesn't work now
		</comment>
		<comment id='5' author='Moldoteck' date='2020-11-13T14:02:32Z'>
		EDIT: from tensorflow.python.keras.backend import eager_learning_phase_scope is not helping, since the error is happening in this part:
second_part = K.function([model.get_layer('dense_alpha').input, K.learning_phase()],
[model.get_layer('dense_out').output])
In other words, solution from stackoverflow doesn't work
		</comment>
		<comment id='6' author='Moldoteck' date='2020-11-13T14:26:11Z'>
		When you use
&lt;denchmark-code&gt;second_part = Model([model.get_layer('dense_alpha').input],
                            [model.get_layer('dense_out').output])
&lt;/denchmark-code&gt;

You are in the same case as
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/43025&gt;#43025&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='Moldoteck' date='2020-11-13T14:42:44Z'>
		
When you use
second_part = Model([model.get_layer('dense_alpha').input],
                            [model.get_layer('dense_out').output])

You are in the same case as
#43025

Well, as i understand, in current implementation, the method with Model is a bit harder to use if i have branches in the network(which is my case) and at the same time, the Backend.function will not behave as it was in the past, so i will not be able to use it. In other words, in my case the best method would be to downgrade the tf version OR disable the eager execution and use backend.function, since it will go to another branch
		</comment>
		<comment id='8' author='Moldoteck' date='2020-11-13T14:47:48Z'>
		You can explore to reorganize with subclassed as suggested in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/43025#issuecomment-688957850&gt;#43025 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='Moldoteck' date='2020-11-23T17:41:56Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='10' author='Moldoteck' date='2020-11-30T18:26:06Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='11' author='Moldoteck' date='2020-11-30T18:26:10Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44838&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44838&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>