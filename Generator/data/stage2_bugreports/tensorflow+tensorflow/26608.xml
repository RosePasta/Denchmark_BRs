<bug id='26608' author='meyerjo' open_date='2019-03-12T13:32:21Z' closed_time='2020-04-10T13:10:50Z'>
	<summary>Subsequent gather_nd Operation leads to error: Tensor.graph is meaningless when eager execution is enabled.</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.0.0-alpha0
Python version: python 3.5
CUDA/cuDNN version: 10.
GPU model and memory: TITAN

Describe the current behavior
Gradient computation fails with the message: "Tensor.graph is meaningless when eager execution is enabled."
The gradient becomes an IndexedSlices through the second gather_nd operation. On the IndexedSlices the check in 


tensorflow/tensorflow/python/framework/ops.py


        Lines 5956 to 5957
      in
      8cb1aa1






 if (isinstance(op_input, (Operation, _TensorLike)) and 



     ((not isinstance(op_input, Tensor)) or type(op_input) == Tensor)):  # pylint: disable=unidiomatic-typecheck 




 evaluates to True and in line 5965 the .graph attribute is called which in EagerExecution causes the error mentioned above.



tensorflow/tensorflow/python/framework/ops.py


         Line 5965
      in
      8cb1aa1






 graph = graph_element.graph 





A workaround/fix for the problem can be achieved by adding another check to the if clause making sure that op_input is not an IndexedSlices: and not isinstance(op_input, IndexedSlices)
    if (isinstance(op_input, (Operation, _TensorLike)) and
       ((not isinstance(op_input, Tensor)) or type(op_input) == Tensor) and
       (not isinstance(op_input, IndexedSlices))):  # pylint: disable=unidiomatic-typecheck
Describe the expected behavior
Gradients to be computed correctly
Code to reproduce the issue
import tensorflow as tf


class DebugModel(tf.keras.Model):
  def __init__(self):
    super(DebugModel, self).__init__()
    self.dense = tf.keras.layers.Dense(20, activation='relu', input_shape=(5,))

  def __call__(self, input, *args, **kwargs):
    x = self.dense(input)
    ind = [[0], [1], [2]]
    x = tf.gather_nd(x, ind)
    ind = [[0], [1]]
    x = tf.gather_nd(x, ind)
    return x

my_model = DebugModel()

my_optim = tf.keras.optimizers.Adam(1e-3)
with tf.GradientTape() as tape:
  out = my_model(tf.random.uniform((16,5)))
  my_loss = -tf.reduce_mean(out)
  grads = tape.gradient(my_loss, my_model.trainable_weights)
  my_optim.apply_gradients(zip(grads, my_model.trainable_weights))
	</description>
	<comments>
		<comment id='1' author='meyerjo' date='2019-03-14T07:47:05Z'>
		&lt;denchmark-link:https://github.com/josh11b&gt;@josh11b&lt;/denchmark-link&gt;
 You are mentioned in a TODO above the problematic if statement. Can you check whether this is a valid fix?
		</comment>
		<comment id='2' author='meyerjo' date='2019-03-21T13:07:24Z'>
		&lt;denchmark-link:https://github.com/goldiegadde&gt;@goldiegadde&lt;/denchmark-link&gt;
 This is still present in 2.0.0-dev20190319
		</comment>
		<comment id='3' author='meyerjo' date='2019-03-21T15:23:47Z'>
		It seems plausible but I'm not certain I understand the implications of this change.
		</comment>
		<comment id='4' author='meyerjo' date='2019-03-25T23:26:43Z'>
		We should probably drop this validating that everything is in the same graph bit. It'll likely be false with functions anyway.
		</comment>
		<comment id='5' author='meyerjo' date='2019-03-26T15:42:32Z'>
		&lt;denchmark-link:https://github.com/meyerjo&gt;@meyerjo&lt;/denchmark-link&gt;
 do you feel like sending a pull request which adds a unit test based on your code and deletes this validation block in ops.py?
		</comment>
		<comment id='6' author='meyerjo' date='2019-03-27T07:22:31Z'>
		Do you feel like this whole validation block is not necessary? I will try to create a pull request beginning of April. Till then I am rather occupied.
		</comment>
		<comment id='7' author='meyerjo' date='2019-03-27T15:34:44Z'>
		Yes, I feel like it's wrong more often than not now.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Wed, Mar 27, 2019 at 12:25 AM Johannes Meyer ***@***.***&gt; wrote:
 Do you feel like this whole validation block is not necessary? I will try
 to create a pull request beginning of April. Till then I am rather occupied.

 —
 You are receiving this because you were assigned.
 Reply to this email directly, view it on GitHub
 &lt;#26608 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAATxYRqeL06Lw4JLnIFuEJaY1dRhkaqks5vaxzygaJpZM4bq6HM&gt;
 .


-- 
 - Alex

		</comment>
		<comment id='8' author='meyerjo' date='2019-07-19T21:47:58Z'>
		&lt;denchmark-link:https://github.com/meyerjo&gt;@meyerjo&lt;/denchmark-link&gt;
 did you ever send a PR to fix this?
		</comment>
		<comment id='9' author='meyerjo' date='2019-10-01T17:00:10Z'>
		&lt;denchmark-link:https://github.com/meyerjo&gt;@meyerjo&lt;/denchmark-link&gt;
 did you had a chance to send a PR to fix this? Thanks!
		</comment>
		<comment id='10' author='meyerjo' date='2019-10-06T14:23:00Z'>
		For now, the only fix I found is to multiply the tensor by 1. In between both the operations
But not sure if that is the best way to get rid of this error
&lt;denchmark-code&gt;    x = self.dense(input)
    ind = [[0], [1], [2]]
    x = tf.gather_nd(x, ind)
    x = x*tf.constant(1.0)
    ind = [[0], [1]]
    x = tf.gather_nd(x, ind)
    return x
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='meyerjo' date='2019-10-07T07:07:59Z'>
		Unfortunately, I did not find time yet to work on the pull request. However, the quick-fix mentioned in the initial issue is still working in TF2.0 . Hopefully, by mid of this week I will have some more time to take a deeper look at it.
		</comment>
		<comment id='12' author='meyerjo' date='2020-03-27T20:14:19Z'>
		&lt;denchmark-link:https://github.com/meyerjo&gt;@meyerjo&lt;/denchmark-link&gt;
,
I was able to reprocude the error with TF v2.0. However, the issue seems to be fixed with &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/55ed1dfb50678c98c263f9a0c5d2e098/2-1-template.ipynb&gt;TF v2.1&lt;/denchmark-link&gt;
. Please find the attached gist. Thanks!
		</comment>
		<comment id='13' author='meyerjo' date='2020-04-03T12:36:24Z'>
		&lt;denchmark-link:https://github.com/meyerjo&gt;@meyerjo&lt;/denchmark-link&gt;
,
Any updates regarding this issue? Thanks!
		</comment>
		<comment id='14' author='meyerjo' date='2020-04-10T13:10:38Z'>
		Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
		<comment id='15' author='meyerjo' date='2020-04-10T13:10:51Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26608&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26608&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>