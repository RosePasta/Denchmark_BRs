<bug id='6132' author='ashern' open_date='2016-12-06T23:24:50Z' closed_time='2017-03-27T19:56:44Z'>
	<summary>contrib.learn.Estimator does not work with multiple GPU</summary>
	<description>
Attempting to assign ops to a GPU within model_fn passed to an Estimator produces the following error:
&lt;denchmark-code&gt;tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'save/ShardedFilename_2': Could not satisfy explicit device specification '/device:GPU:1' because no supported kernel for GPU devices is available.
Colocation Debug Info:
Colocation group had the following types and devices: 
Identity: CPU 
ShardedFilename: CPU 
	 [[Node: save/ShardedFilename_2 = ShardedFilename[_device="/device:GPU:1"](save/Const, save/ShardedFilename_2/shard, save/num_shards)]]
&lt;/denchmark-code&gt;

This can be reproduced by running the example in examples/learn/multiple_gpu.py
	</description>
	<comments>
		<comment id='1' author='ashern' date='2016-12-08T16:34:10Z'>
		A quick bump on this - am I making fundamentally incorrect assumptions on how this should work?
I'm not eager to replicate the Estimator's functionality, which is great overall, but it's important for me to get my models working in TF w/ multiple GPUs. I'd be happy to help w/ a PR if some work is needed to get this functioning.
I had also posted on SO &lt;denchmark-link:http://stackoverflow.com/questions/41003989/tensorflow-contrib-learn-estimator-multi-gpu&gt;here&lt;/denchmark-link&gt;
.
Thank you - wonderful library.
		</comment>
		<comment id='2' author='ashern' date='2016-12-08T19:07:09Z'>
		Thanks for filing this issue. We know about the problem and are in the process of fixing it. Specifically, and as a stopgap, we will allow you do (optionally) create a Saver yourself, so you can control where its ops land. We'll update this thread once that has landed (&lt;denchmark-link:https://github.com/ispirmustafa&gt;@ispirmustafa&lt;/denchmark-link&gt;
 FYI)
		</comment>
		<comment id='3' author='ashern' date='2016-12-08T19:20:33Z'>
		Thank you, &lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
. I'll keep an eye out.
In the meanwhile, were I to dig into the source and add allow_soft_placement=True on the session creation, is that likely to work-around the problem or is there another blocking issue (recognizing the can of worms I'm opening w/ that option setting)?
		</comment>
		<comment id='4' author='ashern' date='2016-12-08T19:45:07Z'>
		I believe if you did that, you should be fine.
		</comment>
		<comment id='5' author='ashern' date='2017-01-23T16:12:09Z'>
		Hi,
I get the same errors when using contrib.learn package (Regressors) where only the GPU memory is allocated but the GPU processing is at zero. I tried allow_soft_placement=True but still get the same errors.
Is this fixed yet?
		</comment>
		<comment id='6' author='ashern' date='2017-01-24T00:44:00Z'>
		We're making "allow_soft_placement" default in Estimator implementation.
That should fix this issue.
		</comment>
		<comment id='7' author='ashern' date='2017-01-24T00:49:53Z'>
		&lt;denchmark-link:https://github.com/vvpreetham&gt;@vvpreetham&lt;/denchmark-link&gt;
 it's surprising that allow_soft_placement would not have fixed this. How did you set it?
		</comment>
		<comment id='8' author='ashern' date='2017-01-24T01:08:56Z'>
		Thanks for the quick response Wicke.

I have done the following: tensorflow (v0.12)

In
the tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/run_config.py

Line 241: I have changed the code to:
    self.tf_config = ConfigProto(allow_soft_placement=True,....

Then in my files:
I am using a LinearRegressor as follows:
       config_ = run_config.RunConfig()
       m = tf.contrib.learn.LinearRegressor(model_dir=model_dir,
                                          config=config_,
                                          feature_columns=wide_columns,
                                          optimizer=tf.train.FtrlOptimizer(
            learning_rate=0.2,
            l1_regularization_strength=1.0,
            l2_regularization_strength=1.0))

Which has a feed function:
def feed_fn(df):
    with tf.device('/gpu:2'):
        categorical_cols = {k: tf.SparseTensor(
         indices=[[i, 0] for i in range(df[k].size)],
         values=df[k].values,
         shape=[df[k].size, 1]) for k in CATEGORICAL_COLUMNS}
        label = tf.constant(df[REGRESSOR_LABEL_COLUMN].values)
    return categorical_cols, label

I get the error:
InvalidArgumentError (see above for traceback): Cannot assign a device to
node 'SparseTensor_55/values': Could not satisfy explicit device
specification '/device:GPU:2' because no supported kernel for GPU devices
is available.
         [[Node: SparseTensor_55/values = Const[dtype=DT_STRING,
value=Tensor&lt;type: string shape: [149999] values:
6b76ea8a9a6046649c021c2c26f9e2e7 bf9319d59ac94dd396de82601d3ccba0
4028cbe0384137e1013846732b4e009d...&gt;, _device="/device:GPU:2"]()]]

Should I use the scope for the variable (Given that SparseTensor is a
variable and not a constant?)
		</comment>
		<comment id='9' author='ashern' date='2017-01-24T01:14:30Z'>
		Also, I am using the estimator as follows:
&lt;denchmark-code&gt;    estimator.fit(input_fn=lambda: feed_fn(batch_df_train), steps=train_steps)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='ashern' date='2017-01-24T19:09:31Z'>
		Also as an update, the reason I am trying out the specific device allocation is that, when I normally use the program without tf.device then, I am seeing a strange behavior where only the Titan-X GPU memory is allocated but the GPU processor is 0% used. (I have 3 Titan-X GPU on the same box)
		</comment>
		<comment id='11' author='ashern' date='2017-01-24T19:13:51Z'>
		&lt;denchmark-link:https://github.com/vvpreetham&gt;@vvpreetham&lt;/denchmark-link&gt;
  I think what you're seeing is expected behavior, and unrelated to Estimator. Tensorflow will automatically claim all available memory for all of the GPUs that it sees, unless you tell it otherwise. It will do this whether you are using them for your model or not. Take a look at CUDA_VISIBLE_DEVICES environment setting to specify GPUs to use, or you can change the memory settings (see using GPUs in the docs)
		</comment>
		<comment id='12' author='ashern' date='2017-01-24T19:16:35Z'>
		&lt;denchmark-link:https://github.com/ashern&gt;@ashern&lt;/denchmark-link&gt;
 I am guessing your comment is specific to my last comment (memory allocation). How do I ensure that tensorflow is actually using all the GPU processors I have?
Does CUDA_VISIBLE_DEVICE also ensure the GPU processor usage?
		</comment>
		<comment id='13' author='ashern' date='2017-01-24T19:20:35Z'>
		&lt;denchmark-link:https://github.com/vvpreetham&gt;@vvpreetham&lt;/denchmark-link&gt;
 That's a pretty big topic. Take a look here: &lt;denchmark-link:https://www.tensorflow.org/how_tos/using_gpu/&gt;https://www.tensorflow.org/how_tos/using_gpu/&lt;/denchmark-link&gt;
 - and I think you'll find more help if you post your questions to StackOverflow for a wider audience to answer.
		</comment>
		<comment id='14' author='ashern' date='2017-01-24T19:23:12Z'>
		&lt;denchmark-link:https://github.com/ashern&gt;@ashern&lt;/denchmark-link&gt;
 Thanks. I shall post on StackOverflow, meanwhile setting CUDA_VISIBLE_DEVICES still does not enable my processor usage.
(I have tried everything on the link you have specified and still no luck. Hence posted here)
		</comment>
		<comment id='15' author='ashern' date='2017-01-24T20:08:12Z'>
		&lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
 bump on my original question (so that this thread is not lost) :)
		</comment>
		<comment id='16' author='ashern' date='2017-01-31T22:54:39Z'>
		The problem seems to be the SparseTensor. Note that the following piece of code works:
&lt;denchmark-code&gt;categorical_cols = {k: tf.SparseTensor(
        indices=[[i, 0] for i in range(df[k].size)],
        values=df[k].values,
        shape=[df[k].size, 1]) for k in CATEGORICAL_COLUMNS}
for d in ['/gpu:0','/gpu:1','/gpu:2']:
    with tf.device(d):
        # Converts the label column into a constant Tensor.
        label = tf.constant(df[REGRESSOR_LABEL_COLUMN].values)
        # Returns the feature columns and the label.
&lt;/denchmark-code&gt;

BUT if I do the following (that-is, assign the SparseTensor to the devices, it fails)
&lt;denchmark-code&gt;for d in ['/gpu:0','/gpu:1','/gpu:2']:
    with tf.device(d):
        # Creates a dictionary mapping from each categorical feature column name (k)
        # to the values of that column stored in a tf.SparseTensor.
        categorical_cols = {k: tf.SparseTensor(
                indices=[[i, 0] for i in range(df[k].size)],
                values=df[k].values,
                shape=[df[k].size, 1]) for k in CATEGORICAL_COLUMNS}
        # Converts the label column into a constant Tensor.
        label = tf.constant(df[REGRESSOR_LABEL_COLUMN].values)
        # Returns the feature columns and the label.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='17' author='ashern' date='2017-02-03T00:36:53Z'>
		If you're still getting the same error message (cannot put string tensor on GPU), then you still haven't enabled soft placement (or soft placement does not consider colocation constraints? &lt;denchmark-link:https://github.com/vrv&gt;@vrv&lt;/denchmark-link&gt;
 do you know?). You cannot put string tensors on GPUs, so that particular tensor has to live on the CPU.
		</comment>
		<comment id='18' author='ashern' date='2017-02-03T01:42:43Z'>
		I am still getting the error. I am super certain that soft placement is enabled as I have dry run the code with log.info and also breakpoints. I also add the gpu_fraction and soft placement directly in the session (The GPU memory fraction works, and soft placement works as I have stated for constants). My modified code for session is as follows:
&lt;denchmark-code&gt;def get_session(gpu_fraction=DEFAULT_GPU_FRACTION):
    num_threads = os.environ.get('OMP_NUM_THREADS')
    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)
    if num_threads:
        return tf.Session(config=tf.ConfigProto(
                    allow_soft_placement=True,
                    log_device_placement=True,
                    gpu_options=gpu_options, 
                    intra_op_parallelism_threads=num_threads))
    else:
        return tf.Session(config=tf.ConfigProto(allow_soft_placement=True,
                                                log_device_placement=True,
                                                gpu_options=gpu_options))
&lt;/denchmark-code&gt;

I setup the session as follows:
&lt;denchmark-code&gt;def main(_):
    with get_session() as sess:
        m, df_test = wide_batch_train()
&lt;/denchmark-code&gt;

The problem seems to be the tf.SparseTensor
		</comment>
		<comment id='19' author='ashern' date='2017-03-27T19:29:21Z'>
		I see in the repo that Estimator has been promoted to core from contrib for 1.1 - great news.
Briefly checking in on this issue - will the deployed implementation handle multiple GPU device assignment &amp; soft placement?
Many thanks for the hard work!
		</comment>
		<comment id='20' author='ashern' date='2017-03-27T19:50:33Z'>
		​Yes, the core estimator uses soft placement, and multi-GPU training should
work.
		</comment>
		<comment id='21' author='ashern' date='2017-03-27T19:56:44Z'>
		Excellent. Time to put my homespun solution to rest...
Many thanks, Martin. Appreciated your overview at the summit!
		</comment>
		<comment id='22' author='ashern' date='2017-04-04T21:16:15Z'>
		&lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
: any examples of multi-gpu for tf.learn now that it will be on core besides the one provided in the example section? The one from example section only does parallel-model as opposed to data-parallel &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/multiple_gpu.py&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/multiple_gpu.py&lt;/denchmark-link&gt;

Thanks.
		</comment>
		<comment id='23' author='ashern' date='2017-04-05T20:16:45Z'>
		I don't think creating sparse string Tensors on the GPU will work since the GPU does not support strings.
It's best to structure your model so the string to int conversion happens on the CPU, and the GPU just processes the dense part of your model.
That said, the linear regression canned estimator will probably see no benefit from running on the GPU (not enough matrix multiplications to offset the data transfer cost).
		</comment>
	</comments>
</bug>