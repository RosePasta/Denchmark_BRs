<bug id='43252' author='andrescodas' open_date='2020-09-15T22:59:25Z' closed_time='2020-09-22T19:31:10Z'>
	<summary>tf.saved_model.save very slow with second-order tf.autodiff.ForwardAccumulator</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  dockerhub container 'latest' Digest: 7bc36fe0ca1a051a808122e87f5438614b371263515df4794abef9a78440af8b
GPU model and memory: No gpu
4xIntel(R) Core(TM) i7-8650U CPU @ 1.90GHz
32 GB RAM

Describe the current behavior
Saving a tf.module involving second-order tf.autodiff.ForwardAccumulator takes too much time; 1 hour for the example below
Describe the expected behavior
Saving the graph in the example should take few seconds
Standalone code to reproduce the issue
import os
import tensorflow as tf
import time

class Issue_fwd(tf.Module):

    @tf.function(input_signature=[tf.TensorSpec([None, 1], tf.float64)] * 3 +
                                 [tf.TensorSpec([None, 3], tf.float64)] +
                                 [tf.TensorSpec([1, None], tf.float64)] * 4)
    def f(self, x1, x2, x3, c, v1, v2, v3, v4):

        with tf.autodiff.ForwardAccumulator(x1, tf.ones_like(x1)) as fwd_acc_x1_2, \
                tf.autodiff.ForwardAccumulator(x2, tf.ones_like(x2)) as fwd_acc_x2_2:

            with tf.autodiff.ForwardAccumulator(x1, tf.ones_like(x1)) as fwd_acc_x1, \
                 tf.autodiff.ForwardAccumulator(x2, tf.ones_like(x2)) as fwd_acc_x2, \
                 tf.autodiff.ForwardAccumulator(x3, tf.ones_like(x3)) as fwd_acc_x3:

                p = tf.concat([x1, x2, x3], axis=1)
                pe = tf.transpose(a=p[:, :, None], perm=[0, 2, 1])
                ce = tf.transpose(a=c[:, :, None], perm=[2, 0, 1])
                r = tf.reduce_sum(input_tensor=tf.square(ce - pe), axis=2)
                G = tf.exp(-r / 2)

                p = tf.reduce_sum(input_tensor=G * v1, axis=1, keepdims=True)
                b = tf.reduce_sum(input_tensor=G * v2, axis=1, keepdims=True)
                u = tf.reduce_sum(input_tensor=G * v3, axis=1, keepdims=True)
                w = tf.reduce_sum(input_tensor=G * v4, axis=1, keepdims=True)

            dpdx = fwd_acc_x1.jvp(p, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dbdx = fwd_acc_x1.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dudx = fwd_acc_x1.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dwdx = fwd_acc_x1.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)

            dpdz = fwd_acc_x2.jvp(p, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dbdz = fwd_acc_x2.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dudz = fwd_acc_x2.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dwdz = fwd_acc_x2.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)

            dbdt = fwd_acc_x3.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dudt = fwd_acc_x3.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dwdt = fwd_acc_x3.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)

        d2ud2x = fwd_acc_x1_2.jvp(dudx, unconnected_gradients=tf.UnconnectedGradients.ZERO)
        d2ud2z = fwd_acc_x2_2.jvp(dudz, unconnected_gradients=tf.UnconnectedGradients.ZERO)

        d2wd2x = fwd_acc_x1_2.jvp(dwdx, unconnected_gradients=tf.UnconnectedGradients.ZERO)
        d2wd2z = fwd_acc_x2_2.jvp(dwdz, unconnected_gradients=tf.UnconnectedGradients.ZERO)

        d2bd2x = fwd_acc_x1_2.jvp(dbdx, unconnected_gradients=tf.UnconnectedGradients.ZERO)
        d2bd2z = fwd_acc_x2_2.jvp(dbdz, unconnected_gradients=tf.UnconnectedGradients.ZERO)

        return dudx, dudz, dudt, dwdx, dwdz, dwdt, dbdx, dbdz, dbdt, dpdx, dpdz,  d2ud2x, d2ud2z, d2wd2x, d2wd2z, d2bd2x, d2bd2z,


f = Issue_fwd()
saving_path = 'save_path'
os.makedirs(saving_path, exist_ok=True)

start_time = time.clock()
tf.saved_model.save(f, saving_path)
delta_time = time.clock() - start_time
print('saving took {:f} seconds'.format(delta_time))
print('tf.version.GIT_VERSION={}'.format(tf.version.GIT_VERSION))
print('tf.version.VERSION={}'.format(tf.version.VERSION))
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
&lt;denchmark-code&gt;saving took 3942.697280 seconds
tf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087
tf.version.VERSION=2.3.0
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='andrescodas' date='2020-09-15T23:15:10Z'>
		Could you try:
&lt;denchmark-code&gt;import os
import tensorflow as tf
import time

class Issue_fwd(tf.Module):

    input_signature=([tf.TensorSpec([None, 1], tf.float64)] * 3 +
                                 [tf.TensorSpec([None, 3], tf.float64)] +
                                 [tf.TensorSpec([1, None], tf.float64)] * 4)
    @tf.function
    def f(self, x1, x2, x3, c, v1, v2, v3, v4):

        with tf.autodiff.ForwardAccumulator(x1, tf.ones_like(x1)) as fwd_acc_x1_2, \
                tf.autodiff.ForwardAccumulator(x2, tf.ones_like(x2)) as fwd_acc_x2_2:

            with tf.autodiff.ForwardAccumulator(x1, tf.ones_like(x1)) as fwd_acc_x1, \
                 tf.autodiff.ForwardAccumulator(x2, tf.ones_like(x2)) as fwd_acc_x2, \
                 tf.autodiff.ForwardAccumulator(x3, tf.ones_like(x3)) as fwd_acc_x3:

                p = tf.concat([x1, x2, x3], axis=1)
                pe = tf.transpose(a=p[:, :, None], perm=[0, 2, 1])
                ce = tf.transpose(a=c[:, :, None], perm=[2, 0, 1])
                r = tf.reduce_sum(input_tensor=tf.square(ce - pe), axis=2)
                G = tf.exp(-r / 2)

                p = tf.reduce_sum(input_tensor=G * v1, axis=1, keepdims=True)
                b = tf.reduce_sum(input_tensor=G * v2, axis=1, keepdims=True)
                u = tf.reduce_sum(input_tensor=G * v3, axis=1, keepdims=True)
                w = tf.reduce_sum(input_tensor=G * v4, axis=1, keepdims=True)

            dpdx = fwd_acc_x1.jvp(p, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dbdx = fwd_acc_x1.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dudx = fwd_acc_x1.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dwdx = fwd_acc_x1.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)

            dpdz = fwd_acc_x2.jvp(p, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dbdz = fwd_acc_x2.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dudz = fwd_acc_x2.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dwdz = fwd_acc_x2.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)

            dbdt = fwd_acc_x3.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dudt = fwd_acc_x3.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dwdt = fwd_acc_x3.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)

        d2ud2x = fwd_acc_x1_2.jvp(dudx, unconnected_gradients=tf.UnconnectedGradients.ZERO)
        d2ud2z = fwd_acc_x2_2.jvp(dudz, unconnected_gradients=tf.UnconnectedGradients.ZERO)

        d2wd2x = fwd_acc_x1_2.jvp(dwdx, unconnected_gradients=tf.UnconnectedGradients.ZERO)
        d2wd2z = fwd_acc_x2_2.jvp(dwdz, unconnected_gradients=tf.UnconnectedGradients.ZERO)

        d2bd2x = fwd_acc_x1_2.jvp(dbdx, unconnected_gradients=tf.UnconnectedGradients.ZERO)
        d2bd2z = fwd_acc_x2_2.jvp(dbdz, unconnected_gradients=tf.UnconnectedGradients.ZERO)

        return dudx, dudz, dudt, dwdx, dwdz, dwdt, dbdx, dbdz, dbdt, dpdx, dpdz,  d2ud2x, d2ud2z, d2wd2x, d2wd2z, d2bd2x, d2bd2z,


f = Issue_fwd()
saving_path = 'save_path'
os.makedirs(saving_path, exist_ok=True)

start_time = time.clock()
tf.saved_model.save(f, saving_path)
delta_time = time.clock() - start_time
print('saving took {:f} seconds'.format(delta_time))
print('tf.version.GIT_VERSION={}'.format(tf.version.GIT_VERSION))
print('tf.version.VERSION={}'.format(tf.version.VERSION))
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='andrescodas' date='2020-09-16T00:07:04Z'>
		Can you post a sample input for f()?
		</comment>
		<comment id='3' author='andrescodas' date='2020-09-16T13:14:08Z'>
		Hi &lt;denchmark-link:https://github.com/bhack&gt;@bhack&lt;/denchmark-link&gt;
 .  This is the output of &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/43252#issuecomment-693027496&gt;your suggestion&lt;/denchmark-link&gt;
:
&lt;denchmark-code&gt;saving took 0.061019 seconds
tf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087
tf.version.VERSION=2.3.0
&lt;/denchmark-code&gt;

I included the  argument following the recommendation in &lt;denchmark-link:https://www.tensorflow.org/guide/function#controlling_retracing&gt;this guide&lt;/denchmark-link&gt;
 .  Now I'm confused, who should I prevent retracing but still manage to avoid this excessive time to save the graph?
		</comment>
		<comment id='4' author='andrescodas' date='2020-09-16T13:31:55Z'>
		&lt;denchmark-link:https://github.com/andrescodas&gt;@andrescodas&lt;/denchmark-link&gt;
 I've tried to simplify your example to perimeter a little bit the main problem.
I've tried to remove the nested  on the same variable (x1,x2).
Can you try to run it?
&lt;denchmark-code&gt;import os
import tensorflow as tf
import time

class Issue_fwd(tf.Module):

    @tf.function(input_signature=[tf.TensorSpec([None, 1], tf.float64)] * 3 +
                                 [tf.TensorSpec([None, 3], tf.float64)] +
                                 [tf.TensorSpec([1, None], tf.float64)] * 4)
    def f(self, x1, x2, x3, c, v1, v2, v3, v4):

        with tf.autodiff.ForwardAccumulator(x1, tf.ones_like(x1)) as fwd_acc_x1_2, \
                tf.autodiff.ForwardAccumulator(x2, tf.ones_like(x2)) as fwd_acc_x2_2:

            with tf.autodiff.ForwardAccumulator(x3, tf.ones_like(x3)) as fwd_acc_x3:

                p = tf.concat([x1, x2, x3], axis=1)
                pe = tf.transpose(a=p[:, :, None], perm=[0, 2, 1])
                ce = tf.transpose(a=c[:, :, None], perm=[2, 0, 1])
                r = tf.reduce_sum(input_tensor=tf.square(ce - pe), axis=2)
                G = tf.exp(-r / 2)

                p = tf.reduce_sum(input_tensor=G * v1, axis=1, keepdims=True)
                b = tf.reduce_sum(input_tensor=G * v2, axis=1, keepdims=True)
                u = tf.reduce_sum(input_tensor=G * v3, axis=1, keepdims=True)
                w = tf.reduce_sum(input_tensor=G * v4, axis=1, keepdims=True)
             
            """dpdx = fwd_acc_x1.jvp(p, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dbdx = fwd_acc_x1.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dudx = fwd_acc_x1.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dwdx = fwd_acc_x1.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)

            dpdz = fwd_acc_x2.jvp(p, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dbdz = fwd_acc_x2.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dudz = fwd_acc_x2.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dwdz = fwd_acc_x2.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)"""

            dbdt = fwd_acc_x3.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dudt = fwd_acc_x3.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dwdt = fwd_acc_x3.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)

        #d2ud2x = fwd_acc_x1_2.jvp(dudx, unconnected_gradients=tf.UnconnectedGradients.ZERO)
        #d2ud2z = fwd_acc_x2_2.jvp(dudz, unconnected_gradients=tf.UnconnectedGradients.ZERO)

        #d2wd2x = fwd_acc_x1_2.jvp(dwdx, unconnected_gradients=tf.UnconnectedGradients.ZERO)
        #d2wd2z = fwd_acc_x2_2.jvp(dwdz, unconnected_gradients=tf.UnconnectedGradients.ZERO)

        #d2bd2x = fwd_acc_x1_2.jvp(dbdx, unconnected_gradients=tf.UnconnectedGradients.ZERO)
        #d2bd2z = fwd_acc_x2_2.jvp(dbdz, unconnected_gradients=tf.UnconnectedGradients.ZERO)

        return dudt, dwdt, dbdt
f = Issue_fwd()
saving_path = 'save_path'
os.makedirs(saving_path, exist_ok=True)

start_time = time.clock()
tf.saved_model.save(f, saving_path)
delta_time = time.clock() - start_time
print('saving took {:f} seconds'.format(delta_time))
print('tf.version.GIT_VERSION={}'.format(tf.version.GIT_VERSION))
print('tf.version.VERSION={}'.format(tf.version.VERSION))
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='andrescodas' date='2020-09-16T13:36:52Z'>
		the output for &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/43252#issuecomment-693407426&gt;the example above&lt;/denchmark-link&gt;
 is:
`
saving took 15.386245 seconds
tf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087
tf.version.VERSION=2.3.0
`
		</comment>
		<comment id='6' author='andrescodas' date='2020-09-16T13:40:17Z'>
		Can you give a sample input args for def f(self, x1, x2, x3, c, v1, v2, v3, v4)?
		</comment>
		<comment id='7' author='andrescodas' date='2020-09-16T13:40:57Z'>
		
Can you post a sample input for f()?

import tensorflow as tf
import time

class Issue_fwd(tf.Module):

    input_signature=([tf.TensorSpec([None, 1], tf.float64)] * 3 +
                                 [tf.TensorSpec([None, 3], tf.float64)] +
                                 [tf.TensorSpec([1, None], tf.float64)] * 4)
    @tf.function
    def f(self, x1, x2, x3, c, v1, v2, v3, v4):

        with tf.autodiff.ForwardAccumulator(x1, tf.ones_like(x1)) as fwd_acc_x1_2, \
                tf.autodiff.ForwardAccumulator(x2, tf.ones_like(x2)) as fwd_acc_x2_2:

            with tf.autodiff.ForwardAccumulator(x1, tf.ones_like(x1)) as fwd_acc_x1, \
                 tf.autodiff.ForwardAccumulator(x2, tf.ones_like(x2)) as fwd_acc_x2, \
                 tf.autodiff.ForwardAccumulator(x3, tf.ones_like(x3)) as fwd_acc_x3:

                p = tf.concat([x1, x2, x3], axis=1)
                pe = tf.transpose(a=p[:, :, None], perm=[0, 2, 1])
                ce = tf.transpose(a=c[:, :, None], perm=[2, 0, 1])
                r = tf.reduce_sum(input_tensor=tf.square(ce - pe), axis=2)
                G = tf.exp(-r / 2)

                p = tf.reduce_sum(input_tensor=G * v1, axis=1, keepdims=True)
                b = tf.reduce_sum(input_tensor=G * v2, axis=1, keepdims=True)
                u = tf.reduce_sum(input_tensor=G * v3, axis=1, keepdims=True)
                w = tf.reduce_sum(input_tensor=G * v4, axis=1, keepdims=True)

            dpdx = fwd_acc_x1.jvp(p, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dbdx = fwd_acc_x1.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dudx = fwd_acc_x1.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dwdx = fwd_acc_x1.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)

            dpdz = fwd_acc_x2.jvp(p, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dbdz = fwd_acc_x2.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dudz = fwd_acc_x2.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dwdz = fwd_acc_x2.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)

            dbdt = fwd_acc_x3.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dudt = fwd_acc_x3.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dwdt = fwd_acc_x3.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)

        d2ud2x = fwd_acc_x1_2.jvp(dudx, unconnected_gradients=tf.UnconnectedGradients.ZERO)
        d2ud2z = fwd_acc_x2_2.jvp(dudz, unconnected_gradients=tf.UnconnectedGradients.ZERO)

        d2wd2x = fwd_acc_x1_2.jvp(dwdx, unconnected_gradients=tf.UnconnectedGradients.ZERO)
        d2wd2z = fwd_acc_x2_2.jvp(dwdz, unconnected_gradients=tf.UnconnectedGradients.ZERO)

        d2bd2x = fwd_acc_x1_2.jvp(dbdx, unconnected_gradients=tf.UnconnectedGradients.ZERO)
        d2bd2z = fwd_acc_x2_2.jvp(dbdz, unconnected_gradients=tf.UnconnectedGradients.ZERO)

        return dudx, dudz, dudt, dwdx, dwdz, dwdt, dbdx, dbdz, dbdt, dpdx, dpdz,  d2ud2x, d2ud2z, d2wd2x, d2wd2z, d2bd2x, d2bd2z,


issue_fwd = Issue_fwd()

n = 10
x1 = tf.random.uniform((n, 1), dtype=tf.float64)
x2 = tf.random.uniform((n, 1), dtype=tf.float64)
x3 = tf.random.uniform((n, 1), dtype=tf.float64)

c = tf.random.uniform((n,3), dtype=tf.float64)

v1 = tf.random.uniform((1, n), dtype=tf.float64)
v2 = tf.random.uniform((1, n), dtype=tf.float64)
v3 = tf.random.uniform((1, n), dtype=tf.float64)
v4 = tf.random.uniform((1, n), dtype=tf.float64)


start_time = time.clock()
dudx, dudz, dudt, dwdx, dwdz, dwdt, dbdx, dbdz, dbdt, dpdx, dpdz, d2ud2x, d2ud2z, d2wd2x, d2wd2z, d2bd2x, d2bd2z = issue_fwd.f(x1, x2, x3, c, v1, v2, v3, v4)
delta_time = time.clock() - start_time

print('running took {:f} seconds'.format(delta_time))
print('tf.version.GIT_VERSION={}'.format(tf.version.GIT_VERSION))
print('tf.version.VERSION={}'.format(tf.version.VERSION))
I`m trying to run this concrete case and it is taking many minutes already
		</comment>
		<comment id='8' author='andrescodas' date='2020-09-16T13:45:52Z'>
		Yes in your last example I removed:
&lt;denchmark-code&gt;
    input_signature=([tf.TensorSpec([None, 1], tf.float64)] * 3 +
                                 [tf.TensorSpec([None, 3], tf.float64)] +
                                 [tf.TensorSpec([1, None], tf.float64)] * 4)
    @tf.function
&lt;/denchmark-code&gt;

and as you can see with a run it is not the (tf.function) tracing with the input_signature but it is f.
		</comment>
		<comment id='9' author='andrescodas' date='2020-09-16T13:53:41Z'>
		the output of &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/43252#issuecomment-693412767&gt;this concrete case&lt;/denchmark-link&gt;
  is:
&lt;denchmark-code&gt;running took 738.183305 seconds
tf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087
tf.version.VERSION=2.3.0
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='andrescodas' date='2020-09-16T14:10:00Z'>
		Yes but I suppose tracing a 738.183305 second function it will be quite slow. Just if you execute/trace the function 5 times you will something similar to the original 3942.697280.
So I want to understand here if you think/expect that f needs to run faster (on CPU on GPU?) or something else.
		</comment>
		<comment id='11' author='andrescodas' date='2020-09-16T14:19:36Z'>
		
So I want to understand here if you think/expect that f needs to run faster (on CPU on GPU?) or something else.

Sure! ... That is why I raised the issue.  I'm wondering how to best use the computing resources to run these computations.  This is an excerpt of a larger code I have, and I'm failing to scale it up, for the reasons being discussed here.
		</comment>
		<comment id='12' author='andrescodas' date='2020-09-16T14:24:36Z'>
		&lt;denchmark-link:https://github.com/allenlavoie&gt;@allenlavoie&lt;/denchmark-link&gt;
 Can you give any feedback about the  use in this function?
		</comment>
		<comment id='13' author='andrescodas' date='2020-09-16T15:48:43Z'>
		I think the outer ForwardAccumulators will end up watching the inner accumulator's output; that's how you get higher-order forward autodiff, which in my experience blows up the graph size if iterated more than 4ish times (even for something simple like tf.cos).
Maybe the docstring needs an example, but you can pass a list of primals and a list of tangents: 


tensorflow/tensorflow/python/eager/forwardprop.py


        Lines 344 to 347
      in
      3da9cc8






       primals: A tensor or nested structure of tensors to watch. 



       tangents: A tensor or nested structure of tensors, with the same nesting 



         structure as `primals`, with each element being a vector with the same 



         size as the corresponding primal element. 





If you do that and only have two levels of nesting it shouldn't create huge graphs. (Note: I've only skimmed the thread, so let me know if the 5-level nesting was intended and I can take another look.)
		</comment>
		<comment id='14' author='andrescodas' date='2020-09-16T16:18:14Z'>
		
If you do that and only have two levels of nesting it shouldn't create huge graphs. (Note: I've only skimmed the thread, so let me know if the 5-level nesting was intended and I can take another look.)

thanks &lt;denchmark-link:https://github.com/allenlavoie&gt;@allenlavoie&lt;/denchmark-link&gt;
 .   5-level nesting was  intended.  , ,  are independent root-variables of the graph, and I want to compute the partial derivatives an Laplacians of ,  ,,  with respect to , , .  For that, second-order nesting should be sufficient.
		</comment>
		<comment id='15' author='andrescodas' date='2020-09-16T16:24:12Z'>
		I didn't pass a list of primals to the ForwardAccumulators because I couldn't figure out how to provide multiple right-hand-sides (or tangents -- the 'p' of jvp).  Then I created multiple ForwardAccumulators that I didn't know where nested.
		</comment>
		<comment id='16' author='andrescodas' date='2020-09-16T16:28:38Z'>
		Yeah that should be documented. This test has an example: 


tensorflow/tensorflow/python/eager/forwardprop_test.py


        Lines 639 to 640
      in
      3da9cc8






 with forwardprop.ForwardAccumulator( 



 primals=[m1, m2], tangents=[tangent1, tangent2]) as acc: 





The lists are zipped together; the first element of the primals list has its tangent recorded as the first element of the tangents list and so on.
		</comment>
		<comment id='17' author='andrescodas' date='2020-09-16T16:35:51Z'>
		I don't know if I was clear. When I said multiple right-hand-sides I meant several instances of tangents=[tangent1, tangent2], e.g.:
tangents_1=[tf.ones_like(m1), tf.zeros_like(m2)], and;
tangents_2=[tf.zeros_like(m1), tf.ones_like(m2)].
Could you point me to an example that would be efficient for this case?
		</comment>
		<comment id='18' author='andrescodas' date='2020-09-16T16:43:20Z'>
		Ah, batching multiple tangents associated with one primal? A GSoC student started on built-in tf.vectorized_map integration, but unfortunately it's not complete (e.g. won't work for higher-order yet because its tf.function integration has issues).
You can use it with tf.vectorized_map yourself for sure: 


tensorflow/tensorflow/python/eager/forwardprop_test.py


        Lines 93 to 97
      in
      3da9cc8






 def _jvp_batch(f, primal, tangents): 



 tf_function = def_function.function(f) 



 



 return control_flow_ops.vectorized_map( 



 functools.partial(_jvp, tf_function, primal), tangents) 





One issue is that tf.vectorized_map traces the function at the moment rather than executing it eagerly.
You can also run forwardprop in a Python for loop if there aren't many tangents. That'd happen eagerly.
		</comment>
		<comment id='19' author='andrescodas' date='2020-09-22T19:00:11Z'>
		&lt;denchmark-link:https://github.com/allenlavoie&gt;@allenlavoie&lt;/denchmark-link&gt;
 Do we want to maintain this issue to track/update some development activity for the subscribers or do you think that we can close it?
		</comment>
		<comment id='20' author='andrescodas' date='2020-09-22T19:31:10Z'>
		If someone wants to re-purpose it as a request for the built-in vectorization support to be completed I think that's fine. Please re-open (or comment) if so.
		</comment>
		<comment id='21' author='andrescodas' date='2020-09-22T19:31:12Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43252&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43252&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='22' author='andrescodas' date='2020-09-22T20:56:34Z'>
		Hi again &lt;denchmark-link:https://github.com/bhack&gt;@bhack&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/allenlavoie&gt;@allenlavoie&lt;/denchmark-link&gt;
 , sorry for taking so long to answer back.  I tried the solution you suggested:
import os
import tensorflow as tf
import time


def _jvp(f, primals, tangents):
    with tf.autodiff.ForwardAccumulator(primals, tangents) as acc:
        primals_out = f(*primals)
    return primals_out, acc.jvp(
        primals_out, unconnected_gradients=tf.UnconnectedGradients.ZERO)


input_signature = [tf.TensorSpec([None, 1], tf.float64)] * 3 + \
                     [tf.TensorSpec([None, 3], tf.float64)] + \
                     [tf.TensorSpec([1, None], tf.float64)] * 4

@tf.function(input_signature=input_signature)
def ff(x, z, t, c, v1, v2, v3, v4):

    p = tf.concat([x, z, t], axis=1)
    pe = tf.transpose(a=p[:, :, None], perm=[0, 2, 1])
    ce = tf.transpose(a=c[:, :, None], perm=[2, 0, 1])
    d = ce - pe
    r = tf.reduce_sum(input_tensor=tf.square(d), axis=2)
    G = tf.exp(-r / 2)

    p = tf.reduce_sum(input_tensor=G * v1, axis=1, keepdims=True)
    b = tf.reduce_sum(input_tensor=G * v2, axis=1, keepdims=True)
    u = tf.reduce_sum(input_tensor=G * v3, axis=1, keepdims=True)
    w = tf.reduce_sum(input_tensor=G * v4, axis=1, keepdims=True)

    return p, b, u, w

class Issue_fwd(tf.Module):
    @tf.function(input_signature=input_signature)
    def f(self, x, z, t, c, v1, v2, v3, v4):

        fi = lambda xi, zi, ti: ff(xi, zi, ti, c, v1, v2, v3, v4)
        primals = [x, z, t]
        tangent_mask = [tf.zeros_like(primal) for primal in primals]

        with tf.autodiff.ForwardAccumulator(primals=[x], tangents=[tf.ones_like(x)]) as fwd_outer:
            i = 0
            primals = [x, z, t]
            [dpdx, dbdx, dudx, dwdx] = _jvp(fi, primals, tangent_mask[:i] + [tf.ones_like(primals[i])] + tangent_mask[i + 1:])[1]
        [d2bd2x, d2ud2x, d2wd2x] = fwd_outer.jvp([dbdx, dudx, dwdx], tf.UnconnectedGradients.ZERO)

        with tf.autodiff.ForwardAccumulator(primals=[z], tangents=[tf.ones_like(z)]) as fwd_outer:
            i = 1
            primals = [x, z, t]
            [dpdz, dbdz, dudz, dwdz] = _jvp(fi, primals, tangent_mask[:i] + [tf.ones_like(primals[i])] + tangent_mask[i + 1:])[1]
        [d2bd2z, d2ud2z, d2wd2z] = fwd_outer.jvp([dbdz, dudz, dwdz], tf.UnconnectedGradients.ZERO)

        i = 2
        [p, b, u, w], [dpdt, dbdt, dudt, dwdt] = _jvp(fi, primals, tangent_mask[:i] + [tf.ones_like(primals[i])] + tangent_mask[i + 1:])

        return dudx, dudz, dudt, dwdx, dwdz, dwdt, dbdx, dbdz, dbdt, dpdx, dpdz, d2ud2x, d2ud2z, d2wd2x, d2wd2z, d2bd2x, d2bd2z,


issue_fwd = Issue_fwd()
saving_path = 'save_path'
os.makedirs(saving_path, exist_ok=True)

start_time = time.clock()
tf.saved_model.save(issue_fwd, saving_path)
delta_time = time.clock() - start_time
print('saving took {:f} seconds'.format(delta_time))
print('tf.version.GIT_VERSION={}'.format(tf.version.GIT_VERSION))
print('tf.version.VERSION={}'.format(tf.version.VERSION))


n = 5000
x = tf.random.uniform((n, 1), dtype=tf.float64)
z = tf.random.uniform((n, 1), dtype=tf.float64)
t = tf.random.uniform((n, 1), dtype=tf.float64)

c = tf.random.uniform((n,3), dtype=tf.float64)

v1 = tf.random.uniform((1, n), dtype=tf.float64)
v2 = tf.random.uniform((1, n), dtype=tf.float64)
v3 = tf.random.uniform((1, n), dtype=tf.float64)
v4 = tf.random.uniform((1, n), dtype=tf.float64)


start_time = time.clock()
p, b, u, w = ff(x, z, t, c, v1, v2, v3, v4)
delta_time = time.clock() - start_time
print('running ff took {:f} seconds'.format(delta_time))



start_time = time.clock()
dudx, dudz, dudt, dwdx, dwdz, dwdt, dbdx, dbdz, dbdt, dpdx, dpdz, d2ud2x, d2ud2z, d2wd2x, d2wd2z, d2bd2x, d2bd2z = issue_fwd.f(x, z, t, c, v1, v2, v3, v4)
delta_time = time.clock() - start_time
print('running Issue_fwd.f took {:f} seconds'.format(delta_time))
the output of the script is:
&lt;denchmark-code&gt;saving took 5.672405 seconds
tf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087
tf.version.VERSION=2.3.0
running ff took 3.102973 seconds
running Issue_fwd.f took 262.284672 seconds
&lt;/denchmark-code&gt;

Please let me if I'm wrong.  I was expecting Issue_fwd.f to take not much more than 3x ff.  Further it seems that Issue_fwd.f takes all of the 32GB RAM memory of machine and steps a bit into the swap. The largest variables are d, r and G in ff which should take at most 1GB of memory if they are all kept simultaneously in memory.
		</comment>
		<comment id='23' author='andrescodas' date='2020-09-22T21:05:43Z'>
		There's a decent amount going on inside that tf.function which could be parallelized by the executor.
I'd check if eager execution also uses that much peak memory. It probably uses less, I'd just check whether it matches your expectation. If yes you can get that by adding tf.control_dependencies; tf.function tries to execute everything at once if data dependencies don't prevent it.
You could also turn on tf.function(experimental_compile=True). XLA executes closer to sequentially (and may apply other memory-saving optimiztions).
		</comment>
		<comment id='24' author='andrescodas' date='2020-09-22T21:27:04Z'>
		Commenting the decorator  @tf.function(input_signature=input_signature) (I'm assuming that is equivalent to running eagerly):
&lt;denchmark-code&gt;saving took 0.066272 seconds
tf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087
tf.version.VERSION=2.3.0
running ff took 3.957520 seconds
running Issue_fwd.f took 100.743039 seconds
&lt;/denchmark-code&gt;

It didn't blow the memory too.
using  @tf.function(experimental_compile=True) I get this error:
LookupError: No gradient defined for operation 'gradients/gradients/concat_grad/Slice_grad/XlaDynamicUpdateSlice' (op type: XlaDynamicUpdateSlice)
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

I will check how to use tf.control_dependencies thanks for the suggestion.
I'll appreciate if you have other guidelines on how to get this code optimized, I thought that running it with @tf.function would help.
After &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/43252#issuecomment-693527200&gt;this comment&lt;/denchmark-link&gt;
 I'm reluctant to dig into 
		</comment>
		<comment id='25' author='andrescodas' date='2020-09-22T21:40:21Z'>
		You're changing how ff is decorated? Unfortunate that experimental_compile doesn't support higher-order gradients (I'm guessing), but in your case I'd suggest changing the tf.function around f instead. A bit awkward, but if you want an apples-to-apples comparison you could have f call an un-decorated version of ff and then have a decorated version of ff to benchmark standalone.
Compiling all of f is where you'd get the most benefit from XLA.
		</comment>
		<comment id='26' author='andrescodas' date='2020-09-22T21:59:58Z'>
		
You're changing how ff is decorated?

Yes on &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/43252#issuecomment-696991585&gt;these&lt;/denchmark-link&gt;
 experiments I changed both ff and Issue_fwd.f decorators accordingly, i.e, the first experiment commenting out both instances, and in the second experiment including  to both functions

in your case I'd suggest changing the tf.function around f instead. A bit awkward, but if you want an apples-to-apples comparison you could have f call an un-decorated version of ff and then have a decorated version of ff to benchmark standalone.
Compiling all of f is where you'd get the most benefit from XLA.

Sorry, I got confused with your explanation, then I tried all the alternatives:

 @tf.function(experimental_compile=True)  raises the error even if I use it alone in ff or in  Issue_fwd.f.
I got the best performance so far decorating  Issue_fwd.f with @tf.function(input_signature=input_signature) and ff with no decorator:

&lt;denchmark-code&gt;saving took 2.310552 seconds
tf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087
tf.version.VERSION=2.3.0
running ff took 3.793245 seconds
running Issue_fwd.f took 69.214752 seconds
&lt;/denchmark-code&gt;

It would be nice to understand why because I tend to use @tf.fuction with input_signature whenever possible following &lt;denchmark-link:https://www.tensorflow.org/guide/function&gt;this guide&lt;/denchmark-link&gt;

		</comment>
		<comment id='27' author='andrescodas' date='2020-09-22T22:15:13Z'>
		I think the short answer is that tf.function produces function call operations, and the gradients/jvps of call operations are more complicated than the equivalent eager operations, especially if you're nesting to take higher-order gradients. Usually graph optimizations re-simplify, but apparently not in this case. There's a plan to unify these and always do the thing we do in eager right now, but it won't happen very quickly.
I see why you get that error from experimental_compile with  but not  decorated. It looks like it was added in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/6a6261c0a0e803891af95f5e754180739df1897d&gt;6a6261c&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/yunxing&gt;@yunxing&lt;/denchmark-link&gt;
 do we need to register a gradient for xla_dynamic_update_slice? Is there a bug for it?
		</comment>
	</comments>
</bug>