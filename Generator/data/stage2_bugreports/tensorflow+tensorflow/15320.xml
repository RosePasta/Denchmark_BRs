<bug id='15320' author='BogdanRuzh' open_date='2017-12-12T19:43:31Z' closed_time='2018-01-31T21:28:18Z'>
	<summary>Multi-core CPU performance dropped for MKL TF build</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 LTS (64-bit)
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): Tensorflow r1.4
Python version: Python version: 2.7.12
Bazel version (if compiling from source): Bazel release 0.7.0
GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
CUDA/cuDNN version: no CUDA
GPU model and memory: no GPU, but i7-6850K with 32Gb ddr4
Exact command to reproduce: run the script below

Tested on two machines:

i7-6850K with 32Gb ddr4
two Xeon x5650 with 24Gb ddr3

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

When I build Tensorflow with MKL it dropped CPU performance in a strange way. Performance of individual core is much higher, but for multicore is much worse.
It's a big epic bottleneck for my project and I can't solve it by myself. I will appreciate any help!

TF installation from sources with MKL support


Tensorflow r1.4 installed from source. Configured with jemalloc as malloc support and other configure settings ignored.
$ bazel build --config=mkl -c opt //tensorflow/tools/pip_package:build_pip_package
$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
$ pip install /tmp/tensorflow_pkg/tensorflow-1.4.1-cp27-cp27mu-linux_x86_64.whl

Run tests:
one core: 0.03s
all cores: 0.12s

TF installation with pip


$ pip install tensorflow
(tensorflow-1.4.1-cp27-cp27mu-manylinux1_x86_64.whl installed)

Run tests:
one core: 0.16s
all cores: 0.03s
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;    import time
    import numpy as np
    import tensorflow as tf

    from tensorflow.contrib import slim
    from tensorflow.contrib.slim.python.slim.nets.inception_v1 import inception_v1, inception_v1_arg_scope


    input_shape = (1, 224, 224, 3)
    features = tf.placeholder(tf.float32, input_shape)

    with slim.arg_scope(inception_v1_arg_scope()):
        predictions, end_points = inception_v1(features, is_training=False)

    # remove to utilize all cores
    session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,
                                  inter_op_parallelism_threads=1)

    with tf.Session(config=session_conf) as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(tf.local_variables_initializer())

        images = np.random.random(input_shape)
        consumption = []
        for i in range(10):
            tick = time.time()
            sess.run(predictions, feed_dict={features: images})
            consumption.append(time.time() - tick)

        print np.mean(consumption)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='BogdanRuzh' date='2017-12-13T07:30:56Z'>
		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
Have I written custom code
OS Platform and Distribution
TensorFlow installed from
TensorFlow version
Bazel version
CUDA/cuDNN version
GPU model and memory
Exact command to reproduce
		</comment>
		<comment id='2' author='BogdanRuzh' date='2017-12-13T10:04:08Z'>
		&lt;denchmark-link:https://github.com/tensorflowbutler&gt;@tensorflowbutler&lt;/denchmark-link&gt;
 Fixed
		</comment>
		<comment id='3' author='BogdanRuzh' date='2017-12-13T18:33:04Z'>
		So the problem in parallelization between cores (and maybe also between CPUs). I'm going to compile TF with OpenCL. Hope it will help.
		</comment>
		<comment id='4' author='BogdanRuzh' date='2017-12-13T18:43:52Z'>
		Make sure you've set all the environment variables correctly. The &lt;denchmark-link:https://www.tensorflow.org/performance/performance_guide#tensorflow_with_intel_mkl_dnn&gt;TensorFlow performance guide&lt;/denchmark-link&gt;
 has a section on "Tuning MKL for the best performance". I would strongly recommend taking a look. Once you try out these settings, I'd be happy to help with tuning. Do mention what settings you're using when replying.
		</comment>
		<comment id='5' author='BogdanRuzh' date='2017-12-13T19:32:36Z'>
		&lt;denchmark-link:https://github.com/vivek-rane&gt;@vivek-rane&lt;/denchmark-link&gt;
 Thanks for your response.
I'm testing performance on another machine (two Xeon X5650, 6 physical cores per CPU), so my current measurements will not match to previously reported. I've tried these variables and get some performance gain.
Tested local variables:

KMP_BLOCKTIME=0 KMP_AFFINITY=granularity=fine,verbose,compact,1,0 KMP_SETTINGS=1


MKL Tensorflow:

Without local variables: 0.74s
With local variables: 0.27s



Also with OMP_NUM_THREADS=2 I have the best performance: 0.22s
With more or less threads performance dropped.

Pip Tensorflow (all cores): 0.04s

For all these tests I've removed session_conf.
Still, pip installed Tensorflow achieve the best performance on all cores. Is it possible to outperform it?
		</comment>
		<comment id='6' author='BogdanRuzh' date='2017-12-13T19:48:31Z'>
		Btw, I did the same tests for MKL Tensorflow compiled with OpenCL, and there is no difference in performance between this and pure MKL TF build.
		</comment>
		<comment id='7' author='BogdanRuzh' date='2017-12-13T21:01:51Z'>
		I would recommend setting inter-op to 2, intra-op to 6 and OMP_NUM_THREADS to 6. Then try increasing inter-op to 3 and/or intra-op to 8/10/12. Make sure OMP_NUM_THREADS is set to the same value as intra-op.
If this doesn't work, please post the timeline (or a screenshot of it) listing the top few ops and how long they take. You can find how to create a timeline here: &lt;denchmark-link:https://towardsdatascience.com/howto-profile-tensorflow-1a49fb18073d&gt;https://towardsdatascience.com/howto-profile-tensorflow-1a49fb18073d&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='BogdanRuzh' date='2017-12-15T17:46:51Z'>
		&lt;denchmark-link:https://github.com/vivek-rane&gt;@vivek-rane&lt;/denchmark-link&gt;

Sorry for late response. I did a bunch of tests and have good results with improving performance for MKL tf in comparison with pip tf - it's 3x faster for my segmentation model and 2x faster for inception v3.
Also, I'm curious about how it tf parallelize inception blocks. Check timelines below. So inception blocks can be processed much faster than straightforward cnn?
Segmentation model, very straightforward, no parallelization showed:
&lt;denchmark-link:https://user-images.githubusercontent.com/20704139/34053133-38ee8456-e1ce-11e7-835d-d23f616e3ce2.png&gt;&lt;/denchmark-link&gt;

Best run with :

OMP_NUM_THREADS=8
KMP_BLOCKTIME=0
KMP_AFFINITY=granularity=fine,verbose,compact,1,0
KMP_SETTINGS=1
intra_op_parallelism_threads=8
inter_op_parallelism_threads=3

In comparison, Inception V3 have good parallelization because of Inception blocks:
&lt;denchmark-link:https://user-images.githubusercontent.com/20704139/34053606-1cab1eb0-e1d0-11e7-95e1-c8bdaa203f35.png&gt;&lt;/denchmark-link&gt;

Best run with :

OMP_NUM_THREADS=12
KMP_BLOCKTIME=0
KMP_AFFINITY=granularity=fine,verbose,compact,1,0
KMP_SETTINGS=1
intra_op_parallelism_threads=12
inter_op_parallelism_threads=3

Tested on i7-6850K (6 cores, 3.60GHz)
		</comment>
		<comment id='9' author='BogdanRuzh' date='2017-12-29T10:49:34Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/14496&gt;#14496&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='BogdanRuzh' date='2018-01-13T18:55:10Z'>
		Nagging Awaiting Response: It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='11' author='BogdanRuzh' date='2018-01-23T22:54:24Z'>
		The original poster has replied to this issue after the stat:awaiting response label was applied.
		</comment>
		<comment id='12' author='BogdanRuzh' date='2018-01-31T19:37:21Z'>
		&lt;denchmark-link:https://github.com/tatianashp&gt;@tatianashp&lt;/denchmark-link&gt;
 could you maybe take a look?
		</comment>
		<comment id='13' author='BogdanRuzh' date='2018-01-31T21:28:18Z'>
		&lt;denchmark-link:https://github.com/BogdanRuzh&gt;@BogdanRuzh&lt;/denchmark-link&gt;
  If you look at your models in TensorBoard you should be able to see dependencies between the operations. TensorFlow may execute independent operations in parallel. Based on the timelines that you posted, segmentation model likely has all big operations depending on each other, while inception has operations that can be executed in parallel.
As I can see, you solved your reported problem of MKL performance  by choosing appropriate values of inter-op, intra-op and OMP_NUM_THREADS.
&lt;denchmark-link:https://github.com/hendra-herviawan&gt;@hendra-herviawan&lt;/denchmark-link&gt;
 If you have comments on &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/14496&gt;#14496&lt;/denchmark-link&gt;
 please post there. I am closing this issue.
		</comment>
	</comments>
</bug>