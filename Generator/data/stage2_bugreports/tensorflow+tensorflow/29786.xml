<bug id='29786' author='idealboy' open_date='2019-06-14T08:26:13Z' closed_time='2019-10-09T15:22:47Z'>
	<summary>Why does tensorflow occupy different gpu memory for the same inference process on different gpu card?</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes, my code is similar to examples/label_image.cc in c++ with *.pb(frozen model)
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):centos7.3
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: sorry, I dont't know
TensorFlow installed from (source or binary): source, just compile the c++ api library
TensorFlow version (use command below):v1.13.1
Python version:python2.7
Bazel version (if compiling from source):0.21.0
GCC/Compiler version (if compiling from source):4.8.5
CUDA/cuDNN version:10.0/7.5
GPU model and memory: p40(24G * 4), v100(24G*8)

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
when I run the inference process (video action recognition with I3D, the input shape (32 * 224 * 224 * 3)) on different card, I got following infomation:
(1) on p40, on device:0, it occupy 4235MiB memory(the available free memory is still about 6G), some other process have been existing on the same card;
(2) on p40, on device:1, it occupy 8300MiB, only the I3D inference model is running on the card
(3) on v100, on device:0, it only occupy about 1G memory,  only the I3D inference model is running on the card
And where is the relative code with allocator in tensorflow ?
And I want to know clearly the gpu memory allocator mechanism, than you very much!
And I set allow_growth = true in my c++ inference program, and I set CUDA_VISIBLE_DEVICE to the current device id at each running!
Thank you!
Describe the expected behavior
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='idealboy' date='2019-10-09T15:22:47Z'>
		&lt;denchmark-link:https://github.com/idealboy&gt;@idealboy&lt;/denchmark-link&gt;
 the allocation logic is &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/bfc_allocator.cc#L49&gt;here&lt;/denchmark-link&gt;
. When we set allow_growth to True, it'll start by allocating 1MB and double the allocation size when running out of memory. One possibility is that for different GPU cards it picks different algorithms for the gpu ops which will require different amount of memory. I'm closing this one, feel free to reopen and/or file a new issue if you encounter further problems.
Thanks.
		</comment>
		<comment id='2' author='idealboy' date='2019-10-09T15:22:48Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29786&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29786&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>