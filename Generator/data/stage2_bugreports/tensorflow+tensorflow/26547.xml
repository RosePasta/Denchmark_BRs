<bug id='26547' author='elithrar' open_date='2019-03-10T15:58:24Z' closed_time='2020-02-20T22:03:21Z'>
	<summary>Tutorial: TF 2.0 - Text generation with keras.layer.LSTM - Fails to Generate Sensible Text</summary>
	<description>
System information

TensorFlow version: '2.0.0-alpha0'
Doc Link: https://www.tensorflow.org/alpha/tutorials/sequences/text_generation#generate_text

Describe the documentation issue
The model, after 10 epochs (the default), generates "unreadable" text. This is through running the default Colab notebook via the docs/as loaded from GitHub: &lt;denchmark-link:https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/text_generation.ipynb&gt;https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/text_generation.ipynb&lt;/denchmark-link&gt;

print(generate_text(model, start_string=u"ROMEO: "))
# Output
ROMEO: VINLNKENKJVENVJNKJ3NKJKENqUKCJUKENQENVENJVENKNQKJJNKJQNK$$zzzzzzzzzzz33YYWK3JVAYYYY3NJV3NKJVJQNUKKENJNVAJJNKKxJJNKJKKJJJNKYYYJJNUJNUKC$UJVENKEVBWENVQNKJjUSJHJJJNKDENVKENVQVQVKENU$UJHENJJNJNXQGDVxjUKAQCHENKYYYJJ3NVJVMANQNKJQKENVENVJNNJNHJQUKENXENKENV3NJNUMENSENVVENVENVANQNVJQKQVHMJJNJNJNYUKENJVJNKNVANUKEVxx3NXUKENVQVUJKEN3NKJHJUKENX$qUKENQKJJNKJNNUKENKENVNQNVJKYJJNUJHENVJHYJJVANJJKYV3NVYWQJK3KJVJQUKKCJKYNVJQKEQNKEENNKJQJJNKYV3NKJJNUQDNKJKzzxJ3NV3NQNUKENKEQNKENV3NKYVQVANCHENKJJNKYVJYAN$YY33YYVJXMANQNKENVNVAVDNVANKJQNLYVQVHIVAJKJQUKENKENKJJqUKEJNNQSKCjUKENJHIVEN3NENVUVENVNVANJNVAYYVENQVKEJV3NKJx3NKYJJxQVNVHENVANMJVYYJQNUSQ3VKENVYYVNVAQXKENNKYVJJNKJQNKENX3NJXYNVJVUKJQKKJxJQNQKKENV3NKYVANJjUJQVENNKEMx3NXJNQUVQNKENRQKENQKEXKNRJNKYYYVQNVQVOVE3NHJJXKKENNQVHENVANJKENJJNN$VQUJKHENVQCHINQEVK:JVJHYYYYWqUKEMKYVJJNQVQVKAJENJQQKKENUSK$JJHEV:NVENV3NVJNJVENVENKYVJVMAYB$YYVJJNJJNUKCUKExxx3NVAQYEYYYYY3Y3VJYYYVJMJNJKK3JJNKKYV3NJNKYVQVJQNKEKYCjUKYV3NVEVJKYYYJJJjUKCENDNzzzzRNUVENVVQLNJVAKKENKJNNEQKEJNNKYYV$
Training output w/ loss:
history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])
Epoch 1/10
172/172 [==============================] - 37s 213ms/step - loss: 2.6031
Epoch 2/10
172/172 [==============================] - 33s 190ms/step - loss: 1.9073
Epoch 3/10
172/172 [==============================] - 34s 198ms/step - loss: 1.6539
Epoch 4/10
172/172 [==============================] - 33s 194ms/step - loss: 1.5161
Epoch 5/10
172/172 [==============================] - 34s 196ms/step - loss: 1.4312
Epoch 6/10
172/172 [==============================] - 33s 189ms/step - loss: 1.3695
Epoch 7/10
172/172 [==============================] - 33s 190ms/step - loss: 1.3196
Epoch 8/10
172/172 [==============================] - 34s 199ms/step - loss: 1.2751
Epoch 9/10
172/172 [==============================] - 34s 196ms/step - loss: 1.2337
Epoch 10/10
172/172 [==============================] - 34s 196ms/step - loss: 1.1941
I've validated:

The source data on GCS is OK - https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt
The char2idx mapping is correct / reversible (as expected; no typos)
Reduced the temperature - predictions are still invalid (as expected; they aren't really close to what we attempted to train)
Ran for 30 epochs (loss: 0.6435)

Continuing to debug (far from an expert) but filing this as a heads-up.
	</description>
	<comments>
		<comment id='1' author='elithrar' date='2019-03-10T19:01:10Z'>
		OK - I diffed this against &lt;denchmark-link:https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/sequences/text_generation.ipynb&gt;the notebook on master&lt;/denchmark-link&gt;
 (targeting v1.13).
The only real difference is the use of a CuDNNGRU layer in the v1.13 version vs. the unified LSTM layer in the v2.0.0-alpha version.
Changing the layer in v2.0.0-alpha to the GRU layer yields expected results from the prediction:
-    tf.keras.layers.LSTM(rnn_units, 
-                        return_sequences=True, 
-                        stateful=True, 
-                        recurrent_initializer='glorot_uniform'),
+    tf.keras.layers.GRU(rnn_units, 
+                        return_sequences=True, 
+                        stateful=True, 
+                        recurrent_initializer='glorot_uniform'),
ROMEO: Through drops of ease,
May young lip and tear it.

MARCAUDINA:
I'll tell not, there lies Murcidous sought subject.

TRANIO:
Triel, to Finst Ricelent is ready. See 'twixtwer to Buckingham--
To make your patren: say the king set us heavy sakning well o'
&lt;snip&gt;
Thoughts:

Is there a bug with the unified LSTM layer here?
Does this manifest on CPU (about to force CPU only and test...) - Update: No issues - predicted text output is as expected after a couple of epochs.

		</comment>
		<comment id='2' author='elithrar' date='2019-03-11T22:04:02Z'>
		I also tried this locally on a 20-series GPU:
&lt;denchmark-code&gt;nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2018 NVIDIA Corporation
Built on Sat_Aug_25_21:08:04_Central_Daylight_Time_2018
Cuda compilation tools, release 10.0, V10.0.130 # Same patch version as Colab has installed
&lt;/denchmark-code&gt;

... with the same nonsensical results.
Changing the layer to the UnifiedGRU layer resolved the problem as well - which indicates there is definitely an issue with how the UnifiedLSTM layer is implemented on GPUs (only).
		</comment>
		<comment id='3' author='elithrar' date='2019-05-09T23:21:29Z'>
		Thank for reporting the issue. let me take a look.
		</comment>
		<comment id='4' author='elithrar' date='2019-05-10T21:25:50Z'>
		I have tried with unmodified code with GPU setting for predication, its interesting to see that it will randomly generate some text first and then suddenly produce reasonable text afterwards. Its like there are some junk text in memory need to flush first before it can probably work. Not sure why, need to dig more.
=======================================================
ROMEO: NG&amp;UZSEkzjjUPCQHOUJKUNSKOT:!KQWIUS;
WISTETHAMNIAN:
I pray thee thy king is prosper thee:
But I bristle rascals intainted lost!
Or Clarence is good nor increase, I see, the comfort is but stay.
WERWICK:
There were they never with a house of conseat:
Down, misfave you do not best.
MANCANIUS:
What, may I think
Your perform friends. I'll have no suffer:
Livelour to Mercutio's same to see him.
First Lord:
No, not wash their weeping good which is the
which in a lowly adwise to our age! Where
you come to be
Of once towards Lach:
As that do private write resort to hard
From their friend of thine eyes I go to
her, now ades might other.
O horse
To give him, back'd with belons, an unsign to think
Yourselves I see she's Romeo! enjoy's very
liesk:
This darks give her head to yard and wretches
Masterness not likeway.
LEONTES:
What, was my special queen?'s not so pack as for youth,
The thought to bear me less than he is fare
As you can stay.
DUKE VINCENTIO:
Out; because as porthless chessen nobl
==================================================
ROMEO: w3SxBREY:
WHORTHER:
No, Signior Balingbrock's soldiers purson me
writhering up his gorder, womb; thou art too sour age
onjurying: I respect I said yet he brears your king.
And is a hundard man whose pertiace
Showorigate to be wish'd his speaches to shave
To save him drunk,--why, thys; and I go shall inter?
Tut spiders for Rome; I shall revost it still as Jupe this?
Half thou to chose happy bark an ack of nature
Was strew
And call me pawn; the charges be gote to thee.
YORK:
Exe once asked it in their deeds: where thou wert wonder'd here,
Hath yet not with the king news: they say.
=========================================================
ROMEO: 'OTBMNWQOURIDQU3KCFvIUKXWMISMKSTILISSzUTZNGZKBYUETzXHXV&amp;MATUdKFNOXRHMNYUK:
MIXSSSTCENUT:Cus fight in him, for Gredio.
BUCKINGHAM:
You do not come to intember, nothing have fought
In her ristress'd, it most unknown shall four high
Subjection: that thou tert it is in heavy,
To vengered Berauade.
CLO:
My father set bey made
To see a town, a maident, revenge,, they
hard, unluckity.
ANGELO:
I had as you lough; I do resalve thyself
Have taken. They have Warwick;
And, if I very tribut of revenuman's crown,
You are thereof.
SLY:
Age to calt my father; he hast thou give me stip of.
TYRREL:
Proth, that's by no chearing thee,
========================================================
		</comment>
		<comment id='5' author='elithrar' date='2019-05-10T22:25:04Z'>
		Also, it seems that the model works properly with the custom training loop. The prediction output for the model from custom training loop always produce reasonable result.
		</comment>
		<comment id='6' author='elithrar' date='2019-05-10T23:34:07Z'>
		Also noticed the difference between custom training loop and standard keras fit. The model.reset_states() is not call at the beginning of per epoch. After adding that, the model is producing reasonable text quite constantly.
Having said that, this still doesn't explain why the CPU and V1 code works properly without reset_states().
		</comment>
		<comment id='7' author='elithrar' date='2019-05-13T16:18:15Z'>
		Spent some time debugging this with &lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 last Friday, it seems that cudnn kernel will spit out junk result sometime. The chances are reduced when change to use a smaller number of LSTM size.
		</comment>
		<comment id='8' author='elithrar' date='2020-02-20T22:03:21Z'>
		Sorry for the long wait. I think there was a backend kernel error somewhere. I rerun the same code in tutoral with LSTM layer and TF 2.1, it was able to generate reasonable result without issue. Please verify this on ur end as well. I am closing this bug and feel free to reopen it if you still see same error.
		</comment>
		<comment id='9' author='elithrar' date='2020-02-20T22:03:23Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26547&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26547&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>