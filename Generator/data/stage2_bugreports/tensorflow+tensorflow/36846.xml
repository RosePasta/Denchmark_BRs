<bug id='36846' author='ianferreira' open_date='2020-02-18T03:48:35Z' closed_time='2020-04-16T23:26:45Z'>
	<summary>There aren't enough elements in this dataset for each shard to have at least one element (# elems = 1, # shards = 2)</summary>
	<description>
Environment
Python 3.7.3 (default, Dec 20 2019, 18:57:59)
[GCC 8.3.0] on linux
tensorflow(cpu) 2.1.0
tensorflow_datasets 2.0.0
Description:    Raspbian GNU/Linux 10 (buster)

&lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras&gt;https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras&lt;/denchmark-link&gt;

TFConfig below
{'cluster': {'worker': ['RpiCluster1:2222', 'RpiCluster2:2222']}, 'task': {'type': 'worker', 'index': 0}} {'cluster': {'worker': ['RpiCluster1:2222', 'RpiCluster2:2222']}, 'task': {'type': 'worker', 'index': 1}}
Error below seems to suggest sharding is not working
2020-02-18 03:41:12.788327: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; RpiCluster1:2222, 1 -&gt; localhost:2222}
2020-02-18 03:41:12.789117: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:2222
2020-02-18 03:41:21.023441: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:2222, 1 -&gt; RpiCluster2:2222}
2020-02-18 03:41:21.024802: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:2222
WARNING:tensorflow:eval_fn is not passed in. The worker_fn will be used if an "evaluator" task exists in the cluster.
WARNING:tensorflow:eval_fn is not passed in. The worker_fn will be used if an "evaluator" task exists in the cluster.
WARNING:tensorflow:eval_strategy is not passed in. No distribution strategy will be used for evaluation.
WARNING:tensorflow:eval_strategy is not passed in. No distribution strategy will be used for evaluation.
WARNING:tensorflow:eval_fn is not passed in. The worker_fn will be used if an "evaluator" task exists in the cluster.
WARNING:tensorflow:eval_fn is not passed in. The worker_fn will be used if an "evaluator" task exists in the cluster.
WARNING:tensorflow:eval_strategy is not passed in. No distribution strategy will be used for evaluation.
WARNING:tensorflow:eval_strategy is not passed in. No distribution strategy will be used for evaluation.
2020-02-18 03:41:34.450011: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
[[{{node IteratorGetNext}}]]
WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least steps_per_epoch * epochs batches (in this case, 15 batches). You may need to use the repeat() function when building your dataset.
WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least steps_per_epoch * epochs batches (in this case, 15 batches). You may need to use the repeat() function when building your dataset.
2020-02-18 03:41:35.775198: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Invalid argument: There aren't enough elements in this dataset for each shard to have at least one element (# elems = 1, # shards = 2). If you are using datasets with distribution strategy, considering setting the auto sharding policy to either DATA or OFF using the experimental_distribute.auto_shard_policy optionof tf.data.Options().
[[{{node MultiDeviceIteratorGetNextFromShard}}]]
[[RemoteCall]]
[[IteratorGetNext]]
2020-02-18 03:41:35.795862: I tensorflow/core/profiler/lib/profiler_session.cc:225] Profiler session started.
Found Cluster spec  {'worker': ['RpiCluster1:2222', 'RpiCluster2:2222']}
Found TFConfig {'cluster': {'worker': ['RpiCluster1:2222', 'RpiCluster2:2222']}, 'task': {'type': 'worker', 'index': 0}}
Created strategy  &lt;tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x66fb7e70&gt;
Number of workers  2
Creating datasets inside scope...
Creating model inside scope...
Starting to fit model....
Train for 5 steps
Epoch 1/3
2020-02-18 03:41:36.589112: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.init (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.init (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *constraint arguments to layers.
2020-02-18 03:41:39.502892: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2020-02-18 03:41:39.615315: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Cancelled: RPC Request was cancelled
2020-02-18 03:41:39.615446: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Cancelled: RPC Request was cancelled
2020-02-18 03:41:39.615763: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at collective_ops.cc:253 : Cancelled: RPC Request was cancelled
2020-02-18 03:41:39.615850: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Cancelled: RPC Request was cancelled
[[{{node CollectiveReduce}}]]
Found Cluster spec  {'worker': ['RpiCluster1:2222', 'RpiCluster2:2222']}
Found TFConfig {'cluster': {'worker': ['RpiCluster1:2222', 'RpiCluster2:2222']}, 'task': {'type': 'worker', 'index': 1}}
Created strategy  &lt;tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x670a3c10&gt;
Number of workers  2
Creating datasets inside scope...
Creating model inside scope...
Starting to fit model....
Train for 5 steps
Epoch 1/3
Traceback (most recent call last):
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.py", line 753, in on_start
yield
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.py", line 397, in fit
prefix='val')
File "/usr/lib/python3.7/contextlib.py", line 130, in exit
self.gen.throw(type, value, traceback)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.py", line 771, in on_epoch
self.callbacks.on_epoch_end(epoch, epoch_logs)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/callbacks.py", line 302, in on_epoch_end
callback.on_epoch_end(epoch, logs)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/callbacks.py", line 990, in on_epoch_end
self._save_model(epoch=epoch, logs=logs)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/callbacks.py", line 1040, in _save_model
self.model.save(filepath, overwrite=True)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/network.py", line 1008, in save
signatures, options)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/saving/save.py", line 115, in save_model
signatures, options)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/saving/saved_model/save.py", line 78, in save
save_lib.save(model, filepath, signatures, options)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/saved_model/save.py", line 916, in save
object_saver.save(utils_impl.get_variables_path(export_dir))
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/tracking/util.py", line 1168, in save
file_prefix=file_prefix_tensor, object_graph_tensor=object_graph_tensor)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/tracking/util.py", line 1116, in _save_cached_when_graph_building
save_op = saver.save(file_prefix)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saving/functional_saver.py", line 230, in save
sharded_saves.append(saver.save(shard_prefix))
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saving/functional_saver.py", line 69, in save
tensors.append(spec.tensor)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saving/saveable_object.py", line 52, in tensor
return self._tensor() if callable(self._tensor) else self._tensor
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/values.py", line 1252, in tensor
return strategy.extended.read_var(sync_on_read_variable)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py", line 769, in read_var
return replica_local_var._get_cross_replica()  # pylint: disable=protected-access
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/values.py", line 1347, in _get_cross_replica
self, axis=None)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_lib.py", line 808, in reduce
return self._extended._reduce(reduce_op, value)  # pylint: disable=protected-access
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_lib.py", line 1449, in _reduce
device_util.current() or "/device:CPU:0"))[0]
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/collective_all_reduce_strategy.py", line 528, in _reduce_to
reduce_op, value, destinations=destinations)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 282, in reduce
destinations)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 1038, in reduce_implementation
all_reduced = self._batch_all_reduce(reduce_op, [per_replica_value])[0]
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 1118, in _batch_all_reduce
dense_results = self._do_batch_all_reduce_dense(reduce_op, dense_values)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 1160, in _do_batch_all_reduce_dense
"Id", communication_hint)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/cross_device_utils.py", line 368, in build_collective_reduce
return collective_all_reduce()
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py", line 568, in call
result = self._call(*args, **kwds)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py", line 638, in _call
return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py", line 1611, in _filtered_call
self.captured_inputs)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py", line 1692, in _call_flat
ctx, args, cancellation_manager=cancellation_manager))
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py", line 545, in call
ctx=ctx)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/execute.py", line 67, in quick_execute
six.raise_from(core._status_to_exception(e.code, message), None)
File "", line 3, in raise_from
tensorflow.python.framework.errors_impl.CancelledError:  RPC Request was cancelled
[[node CollectiveReduce (defined at /usr/lib/python3.7/contextlib.py:130) ]] [Op:__inference_collective_all_reduce_1457]
Function call stack:
collective_all_reduce
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
File "main_2.py", line 98, in 
multi_worker_model.fit(x=train_datasets, epochs=3, steps_per_epoch=5, callbacks = callbacks)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training.py", line 819, in fit
use_multiprocessing=use_multiprocessing)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 790, in fit
*args, **kwargs)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 777, in wrapper
mode=dc.CoordinatorMode.INDEPENDENT_WORKER)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_coordinator.py", line 853, in run_distribute_coordinator
task_id, session_config, rpc_layer)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_coordinator.py", line 360, in _run_single_worker
return worker_fn(strategy)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 772, in worker_fn
return method(model, **kwargs)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.py", line 397, in fit
prefix='val')
File "/usr/lib/python3.7/contextlib.py", line 130, in exit
self.gen.throw(type, value, traceback)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.py", line 757, in on_start
self.callbacks._call_end_hook(mode)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/callbacks.py", line 262, in _call_end_hook
self.on_train_end()
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/callbacks.py", line 379, in on_train_end
callback.on_train_end(logs)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/callbacks.py", line 966, in on_train_end
self._training_state.delete_backup()
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/distribute/multi_worker_training_state.py", line 173, in delete_backup
tracking.AutoTrackable.delattr(self._model, CKPT_SAVED_EPOCH)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/tracking/tracking.py", line 94, in delattr
super(AutoTrackable, self).delattr(name)
AttributeError: _ckpt_saved_epoch
WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.init (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.init (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *constraint arguments to layers.
2020-02-18 03:41:41.376299: W tensorflow/core/common_runtime/eager/context.cc:349] Unable to destroy server object, so releasing instead. Servers don't support clean shutdown.
srun: error: RpiCluster2: task 1: Exited with exit code 1
2020-02-18 03:41:45.023032: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Invalid argument: [Derived]There aren't enough elements in this dataset for each shard to have at least one element (# elems = 1, # shards = 2). If you are using datasets with distribution strategy, considering setting the auto sharding policy to either DATA or OFF using the experimental_distribute.auto_shard_policy optionof tf.data.Options().
[[{{node MultiDeviceIteratorGetNextFromShard}}]]
[[RemoteCall]]
[[IteratorGetNext]]
2020-02-18 03:41:45.023246: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Invalid argument: [Derived]There aren't enough elements in this dataset for each shard to have at least one element (# elems = 1, # shards = 2). If you are using datasets with distribution strategy, considering setting the auto sharding policy to either DATA or OFF using the experimental_distribute.auto_shard_policy optionof tf.data.Options().
[[{{node MultiDeviceIteratorGetNextFromShard}}]]
[[RemoteCall]]
[[IteratorGetNext]]
2020-02-18 03:41:45.023715: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at collective_ops.cc:253 : Invalid argument: [Derived]There aren't enough elements in this dataset for each shard to have at least one element (# elems = 1, # shards = 2). If you are using datasets with distribution strategy, considering setting the auto sharding policy to either DATA or OFF using the experimental_distribute.auto_shard_policy optionof tf.data.Options().
[[{{node MultiDeviceIteratorGetNextFromShard}}]]
[[RemoteCall]]
[[IteratorGetNext]]
2020-02-18 03:41:45.024006: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Invalid argument: [Derived]There aren't enough elements in this dataset for each shard to have at least one element (# elems = 1, # shards = 2). If you are using datasets with distribution strategy, considering setting the auto sharding policy to either DATA or OFF using the experimental_distribute.auto_shard_policy optionof tf.data.Options().
[[{{node MultiDeviceIteratorGetNextFromShard}}]]
[[RemoteCall]]
[[IteratorGetNext]]
[[CollectiveReduce]]
Traceback (most recent call last):
File "main_2.py", line 98, in 
multi_worker_model.fit(x=train_datasets, epochs=3, steps_per_epoch=5, callbacks = callbacks)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training.py", line 819, in fit
use_multiprocessing=use_multiprocessing)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 790, in fit
*args, **kwargs)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 777, in wrapper
mode=dc.CoordinatorMode.INDEPENDENT_WORKER)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_coordinator.py", line 853, in run_distribute_coordinator
task_id, session_config, rpc_layer)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_coordinator.py", line 360, in _run_single_worker
return worker_fn(strategy)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 772, in worker_fn
return method(model, **kwargs)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.py", line 397, in fit
prefix='val')
File "/usr/lib/python3.7/contextlib.py", line 130, in exit
self.gen.throw(type, value, traceback)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.py", line 771, in on_epoch
self.callbacks.on_epoch_end(epoch, epoch_logs)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/callbacks.py", line 302, in on_epoch_end
callback.on_epoch_end(epoch, logs)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/callbacks.py", line 990, in on_epoch_end
self._save_model(epoch=epoch, logs=logs)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/callbacks.py", line 1040, in _save_model
self.model.save(filepath, overwrite=True)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/network.py", line 1008, in save
signatures, options)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/saving/save.py", line 115, in save_model
signatures, options)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/saving/saved_model/save.py", line 78, in save
save_lib.save(model, filepath, signatures, options)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/saved_model/save.py", line 916, in save
object_saver.save(utils_impl.get_variables_path(export_dir))
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/tracking/util.py", line 1168, in save
file_prefix=file_prefix_tensor, object_graph_tensor=object_graph_tensor)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/tracking/util.py", line 1116, in _save_cached_when_graph_building
save_op = saver.save(file_prefix)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saving/functional_saver.py", line 230, in save
sharded_saves.append(saver.save(shard_prefix))
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saving/functional_saver.py", line 69, in save
tensors.append(spec.tensor)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saving/saveable_object.py", line 52, in tensor
return self._tensor() if callable(self._tensor) else self._tensor
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/values.py", line 1252, in tensor
return strategy.extended.read_var(sync_on_read_variable)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py", line 769, in read_var
return replica_local_var._get_cross_replica()  # pylint: disable=protected-access
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/values.py", line 1347, in _get_cross_replica
self, axis=None)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_lib.py", line 808, in reduce
return self._extended._reduce(reduce_op, value)  # pylint: disable=protected-access
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_lib.py", line 1449, in _reduce
device_util.current() or "/device:CPU:0"))[0]
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/collective_all_reduce_strategy.py", line 528, in _reduce_to
reduce_op, value, destinations=destinations)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 282, in reduce
destinations)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 1038, in reduce_implementation
all_reduced = self._batch_all_reduce(reduce_op, [per_replica_value])[0]
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 1118, in _batch_all_reduce
dense_results = self._do_batch_all_reduce_dense(reduce_op, dense_values)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 1160, in _do_batch_all_reduce_dense
"Id", communication_hint)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/cross_device_utils.py", line 368, in build_collective_reduce
return collective_all_reduce()
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py", line 568, in call
result = self._call(*args, **kwds)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py", line 638, in _call
return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py", line 1611, in _filtered_call
self.captured_inputs)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py", line 1692, in _call_flat
ctx, args, cancellation_manager=cancellation_manager))
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py", line 545, in call
ctx=ctx)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/execute.py", line 67, in quick_execute
six.raise_from(core._status_to_exception(e.code, message), None)
File "", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError:  [Derived]There aren't enough elements in this dataset for each shard to have at least one element (# elems = 1, # shards = 2). If you are using datasets with distribution strategy, considering setting the auto sharding policy to either DATA or OFF using the experimental_distribute.auto_shard_policy optionof tf.data.Options().
[[{{node MultiDeviceIteratorGetNextFromShard}}]]
[[RemoteCall]]
[[IteratorGetNext]]
[[CollectiveReduce]] [Op:__inference_collective_all_reduce_1477]
Function call stack:
collective_all_reduce
2020-02-18 03:41:47.749541: W tensorflow/core/common_runtime/eager/context.cc:349] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.
1/5 [=====&gt;........................] - ETA: 22ssrun: error: RpiCluster1: task 0: Exited with exit code 1
	</description>
	<comments>
		<comment id='1' author='ianferreira' date='2020-02-19T12:54:01Z'>
		I have the exact same issue using tensorflow/tensorflow:2.0.0-py3
		</comment>
		<comment id='2' author='ianferreira' date='2020-02-21T06:43:29Z'>
		Hi - this error suggests that your input dataset is reading from one file? When you use Multi worker strategy with keras model.fit, we try to automatically shard your input dataset at the file level across workers. but if you have 2 workers and only one file, it's not possible to shard that. Hence this error - the recommendation is to set  on your input dataset and that will turn automatic sharding off. (in absence of sharding, make sure you have shuffle in your input dataset. You can read more about this here: &lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#dataset_sharding_and_batch_size&gt;https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#dataset_sharding_and_batch_size&lt;/denchmark-link&gt;

If this is not the case in your dataset, please provide code to repro the problem.
		</comment>
		<comment id='3' author='ianferreira' date='2020-02-24T10:55:34Z'>
		I get this error when I used a TensorFlow Dataset, which is supposed to support automatic sharding...
		</comment>
		<comment id='4' author='ianferreira' date='2020-02-24T14:13:56Z'>
		How to manage the files, is that the work of TDFS and why it needs to be within the strategy scope?
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


________________________________
From: Karl Schriek &lt;notifications@github.com&gt;
Sent: Monday, February 24, 2020 2:55:39 AM
To: tensorflow/tensorflow &lt;tensorflow@noreply.github.com&gt;
Cc: Ian Ferreira &lt;ianferreira@hotmail.com&gt;; Author &lt;author@noreply.github.com&gt;
Subject: Re: [tensorflow/tensorflow] There aren't enough elements in this dataset for each shard to have at least one element (# elems = 1, # shards = 2) (#36846)


I get this error when I used a TensorFlow Dataset, which is supposed to support automatic sharding...

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub&lt;https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F36846%3Femail_source%3Dnotifications%26email_token%3DAABIJ47SOJ2DEJQVCKBSBCDREORSXA5CNFSM4KW4OCPKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMXLYGI%23issuecomment-590265369&amp;data=02%7C01%7C%7C2ff945fa548c4e531cdf08d7b918167d%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637181385407681783&amp;sdata=iBjHcGCJJucA3Fug1oefHiq5usiErPFftcUx3xAvpt0%3D&amp;reserved=0&gt;, or unsubscribe&lt;https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAABIJ45IOA4VJLXA5XJKTH3REORSXANCNFSM4KW4OCPA&amp;data=02%7C01%7C%7C2ff945fa548c4e531cdf08d7b918167d%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637181385407691778&amp;sdata=P8gwSgW4ZMqQvLVSYjAGSA%2Bi6VVzhFz%2BBv0NqqHMSUE%3D&amp;reserved=0&gt;.

		</comment>
		<comment id='5' author='ianferreira' date='2020-03-02T07:38:00Z'>
		&lt;denchmark-link:https://github.com/karlschriek&gt;@karlschriek&lt;/denchmark-link&gt;
 if the TFDS dataset you are using is reading from only one file, then it will have the same problem. Please try out &lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#dataset_sharding_and_batch_size&gt;https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#dataset_sharding_and_batch_size&lt;/denchmark-link&gt;
 which you can set on the dataset object you get from TFDS.
&lt;denchmark-link:https://github.com/ianferreira&gt;@ianferreira&lt;/denchmark-link&gt;
 Currently the dataset needs to be created after the strategy object is created due to some limitation in the TF2 runtime, we are actively working on removing this restriction. It doesn't need to be in the scope, but putting it in the scope is an easy way to ensure you're creating it after the strategy. Please try it outside the scope and let us know if that doesn't work.
		</comment>
		<comment id='6' author='ianferreira' date='2020-03-13T14:42:43Z'>
		&lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
 same error outside scope. There is something really weird going on the TFDS.
`
BUFFER_SIZE = 10000
BATCH_SIZE = 10000
def make_datasets_unbatched():
def scale(image, label):
image = tf.cast(image, tf.float32)
image /= 255
return image, label
datasets, info = tfds.load(name='mnist',
with_info=True,
as_supervised=True)
if my_task_index == 0:
print(info)
assert info.features['image'].shape == (28, 28, 1)
assert info.features['label'].num_classes == 10
assert info.splits['train'].num_examples == 60000
ds = datasets['train'].repeat().batch(BATCH_SIZE)
return ds
def build_and_compile_cnn_model():
model = tf.keras.Sequential([
tf.keras.layers.Flatten(input_shape=(28,28,1)),
tf.keras.layers.Dense(128, activation='relu'),
tf.keras.layers.Dropout(0.5),
tf.keras.layers.Dense(10, activation='softmax')
])
model.compile(
loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
metrics=['accuracy'])
return model
callbacks = [tf.keras.callbacks.TensorBoard(log_dir='logs')]
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
print("Created strategy " , strategy)
NUM_WORKERS = strategy.num_replicas_in_sync
print("Number of workers " ,NUM_WORKERS )
GLOBAL_BATCH_SIZE = 16 * NUM_WORKERS
with strategy.scope():
print("Creating model inside scope...")
model = build_and_compile_cnn_model()
print(model.summary())
print("Creating datasets inside scope...")
train_datasets = make_datasets_unbatched()
print("Created datasets " , train_datasets)
print("Starting to fit model....")
verbose = 1 if my_task_index == 0 else 0
model.fit(train_datasets, epochs=3, steps_per_epoch=5, callbacks = callbacks, verbose=verbose)
print("Model fit completed")
**ERRORS like so**
Found Cluster spec  {'worker': ['RpiCluster1:2222', 'RpiCluster2:2222', 'RpiCluster3:2222']}
Found TFConfig {'cluster': {'worker': ['RpiCluster1:2222', 'RpiCluster2:2222', 'RpiCluster3:2222']}, 'task': {'type': 'worker', 'index': 0}}
Created strategy  &lt;tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x767012b0&gt;
Number of workers  3
Creating model inside scope...
Model: "sequential"
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;Layer (type)                 Output Shape              Param #&lt;/denchmark-h&gt;

flatten (Flatten)            (None, 784)               0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

dense (Dense)                (None, 128)               100480
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

dropout (Dropout)            (None, 128)               0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;dense_1 (Dense)              (None, 10)                1290&lt;/denchmark-h&gt;

Total params: 101,770
Trainable params: 101,770
Non-trainable params: 0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

None
Creating datasets inside scope...
tfds.core.DatasetInfo(
name='mnist',
version=3.0.0,
description='The MNIST database of handwritten digits.',
homepage='&lt;denchmark-link:http://yann.lecun.com/exdb/mnist/&gt;http://yann.lecun.com/exdb/mnist/&lt;/denchmark-link&gt;
',
features=FeaturesDict({
'image': Image(shape=(28, 28, 1), dtype=tf.uint8),
'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),
}),
total_num_examples=70000,
splits={
'test': 10000,
'train': 60000,
},
supervised_keys=('image', 'label'),
citation="""&lt;denchmark-link:https://github.com/Article&gt;@Article&lt;/denchmark-link&gt;
{lecun2010mnist,
title={MNIST handwritten digit database},
author={LeCun, Yann and Cortes, Corinna and Burges, CJ},
journal={ATT Labs [Online]. Available: &lt;denchmark-link:http://yann&gt;http://yann&lt;/denchmark-link&gt;
. lecun. com/exdb/mnist},
volume={2},
year={2010}
}""",
redistribution_info=,
)
Created datasets  &lt;DatasetV1Adapter shapes: ((None, 28, 28, 1), (None,)), types: (tf.uint8, tf.int64)&gt;
Starting to fit model....
Train for 5 steps
Epoch 1/3
WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least steps_per_epoch * epochs batches (in this case, 15 batches). You may need to use the repeat() function when building your dataset.
WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least steps_per_epoch * epochs batches (in this case, 15 batches). You may need to use the repeat() function when building your dataset.
WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least steps_per_epoch * epochs batches (in this case, 15 batches). You may need to use the repeat() function when building your dataset.
WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least steps_per_epoch * epochs batches (in this case, 15 batches). You may need to use the repeat() function when building your dataset.
`
		</comment>
		<comment id='7' author='ianferreira' date='2020-03-18T11:27:45Z'>
		I meet the same problem when I use this tutorial:
&lt;denchmark-link:https://tensorflow.google.cn/tutorials/distribute/multi_worker_with_keras?hl=en&gt;https://tensorflow.google.cn/tutorials/distribute/multi_worker_with_keras?hl=en&lt;/denchmark-link&gt;

I fixed this according to the doc:
&lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#dataset_sharding_and_batch_size&gt;https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#dataset_sharding_and_batch_size&lt;/denchmark-link&gt;

As a newcomer to use Tensorflow, this problem cost a lot of my time, to help others who meet the same problem, I paste the runnable code with some annotations.
&lt;denchmark-code&gt;from __future__ import absolute_import, division, print_function, unicode_literals

import json
import tensorflow_datasets as tfds
import tensorflow as tf
import os

BUFFER_SIZE = 10000
BATCH_SIZE = 32


os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        'worker': ["worker1:12345", "worker2:23456"]
    },
    'task': {'type': 'worker', 'index': 0}      # the index is the first machine, the second machine's index should be 1
})

strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

def make_datasets_unbatched():
  #Scaling MNIST data from (0, 255] to (0., 1.]
  def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255
    return image, label

  datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)

  return datasets['train'].map(scale, num_parallel_calls=tf.data.experimental.AUTOTUNE).cache().shuffle(BUFFER_SIZE)

def build_and_compile_cnn_model():
  model = tf.keras.Sequential([
      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(10)
  ])
  model.compile(
      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
      optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
      metrics=['accuracy'])
  return model
  
#single_worker_model = build_and_compile_cnn_model()
#single_worker_model.fit(x=train_datasets, epochs=3, steps_per_epoch=5)

NUM_WORKERS = 2
#Here the batch size scales up by number of workers since 
#`tf.data.Dataset.batch` expects the global batch size. Previously we used 64, 
#and now this becomes 128.
GLOBAL_BATCH_SIZE = 64 * NUM_WORKERS

#Creation of dataset needs to be after MultiWorkerMirroredStrategy object
#is instantiated.
train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)

#next three line is the key point to fix this problem
options = tf.data.Options()
options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA  # AutoShardPolicy.OFF can work too.
train_datasets_no_auto_shard = train_datasets.with_options(options)

with strategy.scope():
  #Model building/compiling need to be within `strategy.scope()`.
  multi_worker_model = build_and_compile_cnn_model()

#Keras' `model.fit()` trains the model with specified number of epochs and
#number of steps per epoch. Note that the numbers here are for demonstration
#purposes only and may not sufficiently produce a model with good quality.

#attention:   x=train_datasets_no_auto_shard , not x = train_datasets
multi_worker_model.fit(x=train_datasets_no_auto_shard, epochs=3, steps_per_epoch=5)

&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='ianferreira' date='2020-04-10T05:15:11Z'>
		&lt;denchmark-link:https://github.com/maqy1995&gt;@maqy1995&lt;/denchmark-link&gt;
 Good job! And I would like to add a few more explanations for this issue. In multi worker training modes, the distribution strategy will "shard" the dataset, i.e., splitting the dataset and sending each worker a different part.
According to &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/data/experimental/DistributeOptions&gt;documentation of tf.data.experimental.DistributeOptions&lt;/denchmark-link&gt;
,  defaults to , which first tries to shard the files. If the  is created from TFRecord files, then the TFRecord files will be split into groups of number of workers. However, for a very small dataset (like MNIST), the whole dataset is stored in only one TFRecord file, and this is why the file-based auto sharding will fail and throw an exception saying "There aren't enough elements in this dataset for each shard to have at least one element"!
Therefore, when using a multi-worker strategy to train on a very small dataset, we need to manually set the auto_shard_policy to DATA or OFF. Though, multi-worker strategies are designed for accelerating the training of huge datasets that are supposed to contain enough number of files!
		</comment>
		<comment id='9' author='ianferreira' date='2020-04-10T15:44:27Z'>
		&lt;denchmark-link:https://github.com/BinyanHu&gt;@BinyanHu&lt;/denchmark-link&gt;
  please update your examples, because the example uses MNIST :)
		</comment>
		<comment id='10' author='ianferreira' date='2020-04-16T23:26:45Z'>
		&lt;denchmark-link:https://github.com/ianferreira&gt;@ianferreira&lt;/denchmark-link&gt;
 in your pasted code, you have . is that intentional? MNIST only has 60k examples, which is probably why it is running out of data when you try to run a few steps. The tutorial shows BATCH_SIZE=64.
		</comment>
		<comment id='11' author='ianferreira' date='2020-04-16T23:26:47Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36846&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36846&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='ianferreira' date='2020-04-20T11:45:53Z'>
		Is there a reason why this has been closed? I don't see that the problem has been resolved!
		</comment>
		<comment id='13' author='ianferreira' date='2020-04-21T17:34:45Z'>
		&lt;denchmark-link:https://github.com/zhuzilin&gt;@zhuzilin&lt;/denchmark-link&gt;
 in that example, only the buffer size is 10000, not batch size. Those are 2 very different things.
&lt;denchmark-link:https://github.com/karlschriek&gt;@karlschriek&lt;/denchmark-link&gt;
 please provide reproducible code if you're still seeing this issue with latest TF nightly.
		</comment>
		<comment id='14' author='ianferreira' date='2020-04-22T12:11:17Z'>
		&lt;denchmark-link:https://github.com/ianferreira&gt;@ianferreira&lt;/denchmark-link&gt;
 I also used MNIST to test and I applied exactly the same solution as &lt;denchmark-link:https://github.com/maqy1995&gt;@maqy1995&lt;/denchmark-link&gt;
. I was just adding some explainations.
		</comment>
		<comment id='15' author='ianferreira' date='2020-04-28T18:07:22Z'>
		This should still be open because the tutorial doesn't explicitly mention needing this for the tutorial to work:
&lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#dataset_sharding_and_batch_size&gt;https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#dataset_sharding_and_batch_size&lt;/denchmark-link&gt;

Can we fix the documentation to explicitly add this?
		</comment>
		<comment id='16' author='ianferreira' date='2020-04-29T23:57:35Z'>
		Agreed, &lt;denchmark-link:https://github.com/rchao&gt;@rchao&lt;/denchmark-link&gt;
 can you help update the tutorial?
		</comment>
	</comments>
</bug>