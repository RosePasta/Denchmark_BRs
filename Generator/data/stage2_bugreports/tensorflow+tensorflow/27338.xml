<bug id='27338' author='henrysky' open_date='2019-03-31T07:52:07Z' closed_time='2019-04-06T03:24:44Z'>
	<summary>Sequence generator as validation_data not working if Flatten() involved in model</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.13.1
Python version: 3.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10
GPU model and memory: GTX1060 6GB

Describe the current behavior
Sequence generator as validation_data not working if Flatten() layer involved in model
Describe the expected behavior
I expect it to work as expected
Code to reproduce the issue
Define basic stuffs
import numpy as np

import tensorflow as tf
import tensorflow.keras as tfk
Sequence = tfk.utils.Sequence

Dense = tfk.layers.Dense
Input = tfk.layers.Input
Flatten = tfk.layers.Flatten
Model = tfk.models.Model


class CustomGenerator(Sequence):
    def __init__(self, batch_size, shuffle, steps_per_epoch, data):
        self.inputs = data[0]
        self.labels = data[1]
        self.shuffle = shuffle
        self.batch_size = batch_size
        self.steps_per_epoch = steps_per_epoch

        # initial idx
        self.idx_list = self._get_exploration_order(range(self.inputs.shape[0]))
        self.current_idx = 0

    def __len__(self):
        return self.steps_per_epoch

    def _get_exploration_order(self, idx_list):
        """
        :param idx_list:
        :return:
        """
        # shuffle (if applicable) and find exploration order
        if self.shuffle is True:
            idx_list = np.copy(idx_list)
            np.random.shuffle(idx_list)

        return idx_list

    def _data_generation(self, inputs, labels, idx_list_temp):
        x = inputs[idx_list_temp]
        y = labels[idx_list_temp]
        return x, y

    def __getitem__(self, index):
        x, y = self._data_generation(self.inputs,
                                     self.labels,
                                     self.idx_list[self.current_idx:self.current_idx + self.batch_size])
        self.current_idx += self.batch_size
        return x, y

    def on_epoch_end(self):
        # shuffle the list when epoch ends for the next epoch
        self.idx_list = self._get_exploration_order(range(self.inputs.shape[0]))
        # reset counter
        self.current_idx = 0

# Model 1 which does not have Flatten
input_tensor = Input(shape=[200], name='input')
output_tensor = Dense(units=10, name='output')(input_tensor)
neuralnet = Model(inputs=input_tensor, outputs=output_tensor)
neuralnet.compile(loss='mse', optimizer='adam')

# Model 2 which has Flatten
input_tensor = Input(shape=[200, 1], name='input')
flat = Flatten()(input_tensor)
output_tensor = Dense(units=10, name='output')(flat)
neuralnet_flat = Model(inputs=input_tensor, outputs=output_tensor)
neuralnet_flat.compile(loss='mse', optimizer='adam')
This works as expected
predgen = CustomGenerator(batch_size=64, shuffle=True, steps_per_epoch=10, data=[np.random.normal(size=(700, 200, 1)), np.random.normal(size=(700, 10))])

neuralnet_flat.fit_generator(generator=predgen, epochs=5)
This also works
predgen = CustomGenerator(batch_size=64, shuffle=True, steps_per_epoch=10, data=[np.random.normal(size=(700, 200)), np.random.normal(size=(700, 10))])
valgen = CustomGenerator(batch_size=64, shuffle=True, steps_per_epoch=1, data=[np.random.normal(size=(64, 200)), np.random.normal(size=(64, 10))])

neuralnet.fit_generator(generator=predgen, validation_data=valgen, epochs=5)
But this is not working
predgen = CustomGenerator(batch_size=64, shuffle=True, steps_per_epoch=10, data=[np.random.normal(size=(700, 200, 1)), np.random.normal(size=(700, 10))])
valgen = CustomGenerator(batch_size=64, shuffle=True, steps_per_epoch=1, data=[np.random.normal(size=(64, 200, 1)), np.random.normal(size=(64, 10))])

# does not work
neuralnet_flat.fit_generator(generator=predgen, validation_data=valgen, epochs=5)
Other info / logs
&lt;denchmark-code&gt;InvalidArgumentError                      Traceback (most recent call last)
&lt;ipython-input-2-ced9d06428c4&gt; in &lt;module&gt;
      3
      4 # does not work
----&gt; 5 neuralnet_flat.fit_generator(generator=predgen, validation_data=valgen, epochs=5)

~\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1424         use_multiprocessing=use_multiprocessing,
   1425         shuffle=shuffle,
-&gt; 1426         initial_epoch=initial_epoch)
   1427
   1428   def evaluate_generator(self,

~\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)
    223           use_multiprocessing=use_multiprocessing,
    224           max_queue_size=max_queue_size,
--&gt; 225           mode='test')
    226
    227       if not isinstance(val_results, list):

~\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)
    189       progbar.on_batch_begin(step, batch_logs)
    190
--&gt; 191       batch_outs = batch_function(*batch_data)
    192       if not isinstance(batch_outs, list):
    193         batch_outs = [batch_outs]

~\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in test_on_batch(self, x, y, sample_weight, reset_metrics)
   1254       else:
   1255         self._make_eval_function()
-&gt; 1256         outputs = self._eval_function(inputs)  # pylint: disable=not-callable
   1257
   1258     if reset_metrics:

~\Anaconda3\lib\site-packages\tensorflow\python\keras\backend.py in __call__(self, inputs)
   3074
   3075     fetched = self._callable_fn(*array_vals,
-&gt; 3076                                 run_metadata=self.run_metadata)
   3077     self._call_fetch_callbacks(fetched[-len(self._fetches):])
   3078     return nest.pack_sequence_as(self._outputs_structure,

~\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in __call__(self, *args, **kwargs)
   1437           ret = tf_session.TF_SessionRunCallable(
   1438               self._session._session, self._handle, args, status,
-&gt; 1439               run_metadata_ptr)
   1440         if run_metadata:
   1441           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    526             None, None,
    527             compat.as_text(c_api.TF_Message(self.status.status)),
--&gt; 528             c_api.TF_GetCode(self.status.status))
    529     # Delete the underlying status object from memory otherwise it stays alive
    530     # as there is a reference to status from this from the traceback due to

InvalidArgumentError: Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero
         [[{{node flatten/Reshape}}]]
         [[{{node ConstantFoldingCtrl/loss_1/output_loss/MeanSquaredError/weighted_loss/broadcast_weights/assert_broadcastable/AssertGuard/Switch_0}}]]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='henrysky' date='2019-04-06T03:24:44Z'>
		I found out the cause of error. Turns out on_epoch_end() is not reached in validation generation so it yield empty data. Nothing is wrong on Tensorflow side.
		</comment>
		<comment id='2' author='henrysky' date='2019-04-06T03:24:45Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27338&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27338&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='henrysky' date='2019-11-10T21:45:53Z'>
		
I found out the cause of error. Turns out on_epoch_end() is not reached in validation generation so it yield empty data. Nothing is wrong on Tensorflow side.

Hi &lt;denchmark-link:https://github.com/henrysky&gt;@henrysky&lt;/denchmark-link&gt;
, I'm facing a similar issue that validation's on_epoch_end is not reached, do you recall why and how to fix that? Thanks!
		</comment>
	</comments>
</bug>