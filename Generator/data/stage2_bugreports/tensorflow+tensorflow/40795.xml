<bug id='40795' author='jdonier' open_date='2020-06-25T10:21:34Z' closed_time='2020-09-28T08:41:46Z'>
	<summary>Error when computing gradients of a reloaded SavedModel containing an if clause</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): 2.2.0, 2.3.0-dev20200622
Python version: 3.7.6
CUDA/cuDNN version: CUDA 10.1.243 - cuDNN 7.6.4
GPU model and memory: Tesla K80, 12GB

Describe the current behavior
Computing gradients of a reloaded SavedModel containing an if clause raises an error.
When I save a model containing an if clause as follows:
import tensorflow as tf

class CondModel(tf.keras.models.Model):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.dense_layer = tf.keras.layers.Dense(units=1)

    def call(self, inputs, training=None):
        if training:
            return self.dense_layer(inputs)
        else:
            return self.dense_layer(inputs)


m = CondModel()
m.__call__ = tf.function(m.__call__)
m.__call__.get_concrete_function(
    inputs=tf.TensorSpec(shape=[1, 1]), 
    training=tf.TensorSpec(shape=None, dtype=tf.bool)
)
tf.saved_model.save(m, 'saved_model')
and I try to load it back and get gradients in a new process as:
import tensorflow as tf

reloaded = tf.saved_model.load('saved_model')

with tf.GradientTape() as tape:
    reloaded(inputs=[[1]], training=False)
then error message below happens. If however I execute the following code it works all right:
import tensorflow as tf

# --- HACK ---
def f(x):
    if x:
        pass 
    
tf.function(f).get_concrete_function(
    x=tf.TensorSpec(shape=None)
)
# -------------

reloaded = tf.saved_model.load('saved_model')

with tf.GradientTape() as tape:
    reloaded(inputs=[[1]], training=False)
Also if I reload in the same process in which I did the saving no error happens (so to reproduce the error the above snippets should be excuted in different processes).
Error message:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in get_attr(self, name)
   2485       with c_api_util.tf_buffer() as buf:
-&gt; 2486         pywrap_tf_session.TF_OperationGetAttrValueProto(self._c_op, name, buf)
   2487         data = pywrap_tf_session.TF_GetBuffer(buf)

InvalidArgumentError: Operation 'StatefulPartitionedCall' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)
    330     try:
--&gt; 331       xla_compile = op.get_attr("_XlaCompile")
    332       xla_separate_compiled_gradients = op.get_attr(

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in get_attr(self, name)
   2489       # Convert to ValueError for backwards compatibility.
-&gt; 2490       raise ValueError(str(e))
   2491     x = attr_value_pb2.AttrValue()

ValueError: Operation 'StatefulPartitionedCall' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

LookupError                               Traceback (most recent call last)
~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)
    606           try:
--&gt; 607             grad_fn = ops.get_gradient_function(op)
    608           except LookupError:

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in get_gradient_function(op)
   2654     op_type = op.type
-&gt; 2655   return _gradient_registry.lookup(op_type)
   2656 

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/framework/registry.py in lookup(self, name)
     96       raise LookupError(
---&gt; 97           "%s registry has no entry for: %s" % (self._name, name))

LookupError: gradient registry has no entry for: If

During handling of the above exception, another exception occurred:

LookupError                               Traceback (most recent call last)
&lt;ipython-input-1-c7239ff2b139&gt; in &lt;module&gt;
      4 
      5 with tf.GradientTape() as tape:
----&gt; 6     reloaded(inputs=[[1]], training=False)

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in _call_attribute(instance, *args, **kwargs)
    494 
    495 def _call_attribute(instance, *args, **kwargs):
--&gt; 496   return instance.__call__(*args, **kwargs)
    497 
    498 

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    778       else:
    779         compiler = "nonXla"
--&gt; 780         result = self._call(*args, **kwds)
    781 
    782       new_tracing_count = self._get_tracing_count()

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    844               *args, **kwds)
    845       # If we did not create any variables the trace we have is good enough.
--&gt; 846       return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
    847 
    848     def fn_with_cond(*inner_args, **inner_kwds):

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs, cancellation_manager)
   1840                            resource_variable_ops.BaseResourceVariable))],
   1841         captured_inputs=self.captured_inputs,
-&gt; 1842         cancellation_manager=cancellation_manager)
   1843 
   1844   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1921         possible_gradient_type,
   1922         executing_eagerly)
-&gt; 1923     forward_function, args_with_tangents = forward_backward.forward()
   1924     if executing_eagerly:
   1925       flat_outputs = forward_function.call(

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in forward(self)
   1431     """Builds or retrieves a forward function for this call."""
   1432     forward_function = self._functions.forward(
-&gt; 1433         self._inference_args, self._input_tangents)
   1434     return forward_function, self._inference_args + self._input_tangents
   1435 

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in forward(self, inference_args, input_tangents)
   1187       (self._forward, self._forward_graph, self._backward,
   1188        self._forwardprop_output_indices, self._num_forwardprop_outputs) = (
-&gt; 1189            self._forward_and_backward_functions(inference_args, input_tangents))
   1190     return self._forward
   1191 

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _forward_and_backward_functions(self, inference_args, input_tangents)
   1339     outputs = self._func_graph.outputs[:self._num_inference_outputs]
   1340     return self._build_functions_for_outputs(
-&gt; 1341         outputs, inference_args, input_tangents)
   1342 
   1343 

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _build_functions_for_outputs(self, outputs, inference_args, input_tangents)
    897             self._func_graph.inputs,
    898             grad_ys=gradients_wrt_outputs,
--&gt; 899             src_graph=self._func_graph)
    900 
    901       captures_from_forward = [

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)
    667                 # functions.
    668                 in_grads = _MaybeCompile(grad_scope, op, func_call,
--&gt; 669                                          lambda: grad_fn(op, *out_grads))
    670               else:
    671                 # For function call ops, we add a 'SymbolicGradient'

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)
    334       xla_scope = op.get_attr("_XlaScope").decode()
    335     except ValueError:
--&gt; 336       return grad_fn()  # Exit early
    337 
    338   if not xla_compile:

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py in &lt;lambda&gt;()
    667                 # functions.
    668                 in_grads = _MaybeCompile(grad_scope, op, func_call,
--&gt; 669                                          lambda: grad_fn(op, *out_grads))
    670               else:
    671                 # For function call ops, we add a 'SymbolicGradient'

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _rewrite_forward_and_call_backward(self, op, *doutputs)
    710   def _rewrite_forward_and_call_backward(self, op, *doutputs):
    711     """Add outputs to the forward call and feed them to the grad function."""
--&gt; 712     forward_function, backwards_function = self.forward_backward(len(doutputs))
    713     if not backwards_function.outputs:
    714       return backwards_function.structured_outputs

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in forward_backward(self, num_doutputs)
    619     if forward_backward is not None:
    620       return forward_backward
--&gt; 621     forward, backward = self._construct_forward_backward(num_doutputs)
    622     self._cached_function_pairs[num_doutputs] = (forward, backward)
    623     return forward, backward

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _construct_forward_backward(self, num_doutputs)
    667           args=[], kwargs={},
    668           signature=signature,
--&gt; 669           func_graph=backwards_graph)
    670       backwards_graph_captures = backwards_graph.external_captures
    671       captures_from_forward = [

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    977         _, original_func = tf_decorator.unwrap(python_func)
    978 
--&gt; 979       func_outputs = python_func(*func_args, **func_kwargs)
    980 
    981       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _backprop_function(*grad_ys)
    657             self._func_graph.inputs,
    658             grad_ys=grad_ys,
--&gt; 659             src_graph=self._func_graph)
    660 
    661     with self._func_graph.as_default():

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)
    621               raise LookupError(
    622                   "No gradient defined for operation '%s' (op type: %s)" %
--&gt; 623                   (op.name, op.type))
    624         if loop_state:
    625           loop_state.EnterGradWhileContext(op, before=False)

LookupError: No gradient defined for operation 'cond_model/cond' (op type: If)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='jdonier' date='2020-06-30T07:48:11Z'>
		&lt;denchmark-link:https://github.com/jdonier&gt;@jdonier&lt;/denchmark-link&gt;

I have tried in colab with TF version 2.2 and i am not seeing any issue.Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/a929d95732c51e3c2bde8ba17b4e3de3/untitled70.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='2' author='jdonier' date='2020-06-30T08:34:41Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
 I have just tried and it does fail. You need to restart the runtime after saving the model and then execute the cell:
import tensorflow as tf

reloaded = tf.saved_model.load('saved_model')

with tf.GradientTape() as tape:
    reloaded(inputs=[[1]], training=False)
and you will get
&lt;denchmark-code&gt;LookupError: No gradient defined for operation 'cond_model/cond' (op type: If)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='jdonier' date='2020-07-02T09:15:10Z'>
		Restarting the runtime after saving the model I could reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/341f427659e4ff5aae7d55481631ca7f/untitled73.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='4' author='jdonier' date='2020-07-06T13:24:37Z'>
		For a workaround, setting the training to a Python constant, rather than a Tensor, avoids the error:
&lt;denchmark-code&gt;m.__call__.get_concrete_function(
    inputs=tf.TensorSpec(shape=[1, 1]), 
    training=False
)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='jdonier' date='2020-07-17T12:27:33Z'>
		&lt;denchmark-link:https://github.com/jdonier&gt;@jdonier&lt;/denchmark-link&gt;

Any update on this issue please. Thanks!
		</comment>
		<comment id='6' author='jdonier' date='2020-07-20T10:22:13Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 interesting, however this won't work for us as we don't want to set the training flag to a constant. Can you tell if this will be fixed in a near future?
		</comment>
		<comment id='7' author='jdonier' date='2020-07-20T12:58:07Z'>
		&lt;denchmark-link:https://github.com/omalleyt12&gt;@omalleyt12&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 would have the best answer for this question.
		</comment>
		<comment id='8' author='jdonier' date='2020-07-20T17:38:44Z'>
		I don't have full context on this issue, but it seems like the error is caused bc training is specified as a Tensor in the concrete_function, but then passed as a constant? Maybe passing training as a boolean tf.Tensor would fix this?
		</comment>
		<comment id='9' author='jdonier' date='2020-07-20T19:33:53Z'>
		No I get the same error if I run reloaded(inputs=[[1]], training=tf.convert_to_tensor(False, dtype=tf.bool)) .
		</comment>
		<comment id='10' author='jdonier' date='2020-07-20T19:36:09Z'>
		Hm ok, thanks for the info I'll take a look
		</comment>
		<comment id='11' author='jdonier' date='2020-09-16T22:22:46Z'>
		This is fixed with latest tf-nightly version 2.4.0-dev20200916
See the &lt;denchmark-link:https://colab.research.google.com/gist/ymodak/c1f02bba476ca5b6b302c8ada8d58676/untitled70.ipynb&gt;gist&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='jdonier' date='2020-09-25T20:49:51Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='13' author='jdonier' date='2020-09-28T08:41:46Z'>
		Indeed it works, thank you!
		</comment>
		<comment id='14' author='jdonier' date='2020-09-28T08:41:48Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40795&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40795&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>