<bug id='3287' author='alexatknit' open_date='2016-07-13T00:01:09Z' closed_time='2016-08-24T04:11:40Z'>
	<summary>tf.cond and tf.case execute all branches</summary>
	<description>
When I run:
import tensorflow as tf


X1 = tf.Variable(1.)
X2 = tf.Variable(1.)

cond_value = tf.Variable(True)

assign_1 = tf.assign(X1, 2.)
assign_2 = tf.assign(X2, 2.)

with tf.control_dependencies([assign_1]):
    result_1 = tf.identity(X1)
with tf.control_dependencies([assign_2]):
    result_2 = tf.identity(X2)

cond_result = tf.cond(cond_value, lambda: result_1, lambda: result_2)

with tf.Session() as sesh:
    sesh.run(tf.initialize_all_variables())
    sesh.run(cond_result)
    print(sesh.run(X1), sesh.run(X2))
my expected output is: 2.0 1.0 but my actual output is 2.0 2.0. I need control flow in my graph that actually controls which nodes get executed, not just selects from a list of tensors. Am I missing some sort of operation here?
	</description>
	<comments>
		<comment id='1' author='alexatknit' date='2016-07-13T00:06:28Z'>
		Assigning to Yuan -- this is expected behavior, but we still need more documentation on this.
I believe if you define your control dependencies as a function, and the lambda calls the function, it will work.
e.g.,
&lt;denchmark-code&gt;def fn1():
  with tf.control_dependencies....

cond_result = tf.cond(cond_value, lambda: fn1(), lambda: fn2())
&lt;/denchmark-code&gt;

or something like that.  Essentially the ops have to be defined in the function passed to the lambda for them to not be executed.
		</comment>
		<comment id='2' author='alexatknit' date='2016-07-13T00:10:05Z'>
		ah, I see. This may require a large amount of refactoring on my part
		</comment>
		<comment id='3' author='alexatknit' date='2016-07-13T00:10:40Z'>
		the assigns need to be in the method as well:
import tensorflow as tf


X1 = tf.Variable(1.)
X2 = tf.Variable(1.)

cond_value = tf.Variable(True)


def result_1():
    assign_1 = tf.assign(X1, 2.)
    with tf.control_dependencies([assign_1]):
        return tf.identity(X1)
def result_2():
    assign_2 = tf.assign(X2, 2.)
    with tf.control_dependencies([assign_2]):
        return tf.identity(X2)

cond_result = tf.cond(cond_value, result_1, result_2)

with tf.Session() as sesh:
    sesh.run(tf.initialize_all_variables())
    sesh.run(cond_result)
    print(sesh.run(X1), sesh.run(X2))
		</comment>
		<comment id='4' author='alexatknit' date='2016-07-13T00:15:07Z'>
		Lifting this requirement would allow me to have more modular code when designing resnet structures:
...
        with tf.variable_scope('resnet1'):
            for i in range(5):
                with tf.variable_scope(str(i)):
                    X = self.conv(elu(X1), 3, 3, 16, name='conv1')                # 128 -&gt; 128
                    X = self.conv(elu(X), 3, 3, 16, name='conv2')                 # 128 -&gt; 128
                    X1 = self.feedback(X, X1, dropout=0.7)
...
In this case, feedback will add the two tensors with a 70% chance or simply pass X1 with a 30% chance.
		</comment>
		<comment id='5' author='alexatknit' date='2016-07-13T01:17:10Z'>
		For your simple example, I believe that you could simply do:
&lt;denchmark-code&gt;      X1 = tf.Variable(1.)
      X2 = tf.Variable(1.)
      cond_value = tf.Variable(True)
      cond_result = tf.cond(cond_value, lambda: tf.assign(X1, 2.), lambda: tf.assign(X2, 2.))

      sess.run(tf.initialize_all_variables())
      sess.run(cond_result)
      print(sess.run(X1), sess.run(X2))
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='alexatknit' date='2016-07-13T16:31:04Z'>
		sure, I get that, I threw it together to test the unexpected behavior I was seeing running rnns, wasn't supposed to be a polished example. Now I'm getting some weird behavior when converting my trainable variables to constants. It might be a separate issue but the stack looks like:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/usr/local/bin/finalize_graph", line 9, in &lt;module&gt;
    load_entry_point('KnitNN==0.5.3', 'console_scripts', 'finalize_graph')()
  File "/usr/local/lib/python3.5/site-packages/KnitNN-0.5.3-py3.5.egg/nn/tools/finalize_graph.py", line 59, in finalize_graph
    variable_names_whitelist=variables)
  File "/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/graph_util.py", line 224, in convert_variables_to_constants
    returned_variables = sess.run(variable_names)
  File "/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 382, in run
    run_metadata_ptr)
  File "/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 598, in _run
    processed_fetches = self._process_fetches(fetches)
  File "/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 553, in _process_fetches
    'Tensor. (%s)' % (subfetch, fetch, str(e)))
ValueError: Fetch argument 'base_model/cond/resnet1/Assign/Switch:1:0' of 'base_model/cond/resnet1/Assign/Switch:1:0' cannot be interpreted as a Tensor. (The name 'base_model/cond/resnet1/Assign/Switch:1:0' looks a like a Tensor name, but is not a valid one. Tensor names must be of the form "&lt;op_name&gt;:&lt;output_index&gt;".)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='alexatknit' date='2019-06-05T21:37:16Z'>
		&lt;denchmark-link:https://github.com/vrv&gt;@vrv&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/yuanbyu&gt;@yuanbyu&lt;/denchmark-link&gt;
  is there other way to avoid evaluating all branches other than put the operator creation inside the lambda?
There are many cases like:
# Complex code creating two subgraphs
tf.case(predicate, true_fn=lambda: subgraph_a, false_fn=lambda: subgraph_b)
It would be hard to refactor the creation of subgraph_a and subgraph_b into the lambda for true_fn and false_fn. Maybe add an additional argument that avoids evaluating all branches?
This feels like a limitation of the expressiveness of TF Python API, rather than an intended feature.
		</comment>
	</comments>
</bug>