<bug id='37969' author='akanyaani' open_date='2020-03-27T11:13:29Z' closed_time='2020-04-06T19:40:32Z'>
	<summary>Memory leak in TensorFlow 2.0 DataSet when using group_by_window.</summary>
	<description>
System information

OS Platform: - Google Cloud Linux Ubuntu 16.04
TensorFlow installed from (source or binary): binary
TensorFlow version: 2.0.0
Python version: 3.7
CUDA/cuDNN version: 10.01
GPU model and memory: Tesla P100, 16GB
Running on GRAPH MODE:- YES

This is creating memory leak.
&lt;denchmark-code&gt;def pairwise_batch_iterator(tf_records,
                           no_threads=14,
                           batch_size=64,
                           num_epochs=50):
    
    dataset = make_dataset(tf_records, no_threads)
    dataset = dataset.repeat(num_epochs)
    
    dataset = dataset.apply(tf.data.experimental.group_by_window(
        key_func=lambda elem, *args: elem,
        reduce_func=lambda _, window: window.batch(batch_size),
        window_size=batch_size))
    
    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    
    return dataset
&lt;/denchmark-code&gt;

This works fine
&lt;denchmark-code&gt;def pairwise_batch_iterator(tf_records,
                           no_threads=14,
                           batch_size=64,
                           num_epochs=50):
    
    dataset = make_dataset(tf_records, no_threads)
    dataset = dataset.repeat(num_epochs)
    
    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    
    return dataset
&lt;/denchmark-code&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

I am training a pairwise ranking model, where I have 100k TF Records each file has all pairs belongs to one query id, While training I have to group by query id that's why I am using tf.data.experimental.group_by_window and this is creating a memory leak. (This only happens when I have a huge number of TF records ) If I use the second version of code I don't face any issue but I have to group by query id.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='akanyaani' date='2020-03-27T11:36:55Z'>
		&lt;denchmark-link:https://github.com/akanyaani&gt;@akanyaani&lt;/denchmark-link&gt;

Looks like code is incomplete. Please, share colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!
		</comment>
		<comment id='2' author='akanyaani' date='2020-03-27T12:27:47Z'>
		Hi &lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;

I am facing some issues in the Colab notebook, so I am posting a sample code here.
&lt;denchmark-code&gt;dense = tf.random.uniform([500000, 300])
ids = tf.random.uniform([500000], maxval=1000, dtype=tf.int64)

batch_size = 768


dense = tf.data.Dataset.from_tensor_slices(dense) 
ids = tf.data.Dataset.from_tensor_slices(ids) 

dataset = tf.data.Dataset.zip((ids, dense))

dataset = dataset.apply(tf.data.experimental.group_by_window(
       key_func=lambda elem, *args: elem,
      reduce_func=lambda _, window: window.batch(batch_size),
      window_size=batch_size))

dataset = dataset.repeat(100)
dataset = dataset.prefetch(50)

for i in dataset:
    train_func()
&lt;/denchmark-code&gt;

But I am unable to replicate the issue when creating dataset using random vectors. Still, you can replicate the issue if you read data from huge numbers of TF records.
In my case, I have 100k TF records and every file contains training data for one query id.
I am still facing the issue when reading these many TF records and finally grouping all pairs for particular id in one batch.
		</comment>
		<comment id='3' author='akanyaani' date='2020-03-30T07:38:45Z'>
		&lt;denchmark-link:https://github.com/akanyaani&gt;@akanyaani&lt;/denchmark-link&gt;

I tried to reproduce the issue but i am seeing the below error message NameError: name 'train_func' is not defined.Please, share colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!
		</comment>
		<comment id='4' author='akanyaani' date='2020-04-01T16:26:18Z'>
		Hi &lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
,
Please run this code you will be able to generate the issue, I am still facing some issue with Colab notebook that's why I have written normal python code.
First I am creating TF records with similar data which I have and then just iterating over. Using this code you will see a continuous increase in RAM usage and after couple of iteration, it will get killed because of this.
&lt;denchmark-code&gt;import tensorflow as tf
from datetime import datetime
import random
import numpy as np
import glob

output_dir = "./test/"

def create_int_feature(values):
    feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))
    return feature


def create_float_feature(values):
    feature = tf.train.Feature(float_list=tf.train.FloatList(value=list(values)))
    return feature


def serialize_example(q_id, q, s1, s2):
    feature = {
        'q_id' : create_int_feature([q_id]),
        'q': create_float_feature(q),
        's1' : create_float_feature(s1),
        's2' : create_float_feature(s1)
    }
    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))
    return example_proto.SerializeToString()

def writ_tf(q_id, q, s1, s2):
    filename = output_dir + str(datetime.now().timestamp()) + ".tfrecord"
    tf_writer = tf.io.TFRecordWriter(filename)
    for i in range(len(q_id)):
        example = serialize_example(q_id[i], q[i], s1[i], s2[i])
        tf_writer.write(example)
    tf_writer.close()

for i in range(5500):
    batch = random.randrange(250, 768)
    q_id = [i+1]*batch
    q = np.random.rand(batch, 786)
    s1 = np.random.rand(batch, 1536)
    s2 = np.random.rand(batch, 1536)
    
    writ_tf(q_id, q, s1, s2)

NO_THREADS = 12

def load_tf_records(filenames, no_threads):
    if type(filenames) is str:
        filenames = [filenames]
    return tf.data.TFRecordDataset(filenames, buffer_size=500, num_parallel_reads=no_threads)


def parse_example(serialized_example):
    data_fields = {
        "q_id": tf.io.VarLenFeature(tf.int64),
        "q": tf.io.VarLenFeature(tf.float32),
        "s1": tf.io.VarLenFeature(tf.float32),
        "s2": tf.io.VarLenFeature(tf.float32)}
    
    parsed = tf.io.parse_single_example(serialized_example, data_fields)
    q_id = tf.cast(tf.sparse.to_dense(parsed["q_id"]), tf.int64)[0]
    q = tf.cast(tf.sparse.to_dense(parsed["q"]), tf.float32)
    p_su = tf.cast(tf.sparse.to_dense(parsed["s1"]), tf.float32)
    n_su = tf.cast(tf.sparse.to_dense(parsed["s2"]), tf.float32)

    return (q_id, q, p_su, n_su)


def make_dataset(tf_files, no_threads=NO_THREADS):
    dataset = load_tf_records(tf_files, no_threads)
    dataset = dataset.apply(tf.data.experimental.ignore_errors())
    dataset = dataset.map(parse_example, num_parallel_calls=no_threads)
    return dataset
   
    
def batch_iterator(tf_records,
                   no_threads=12,
                   batch_size=768,
                   num_epochs=100):
    
    dataset = make_dataset(tf_records, no_threads)

    
    
    dataset = dataset.apply(tf.data.experimental.group_by_window(
        key_func=lambda elem, *args: elem,
        reduce_func=lambda _, window: window.batch(batch_size),
        window_size=batch_size,
    ))
    
    dataset = dataset.repeat(num_epochs)
    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)
    
    return dataset

tf_records = glob.glob((output_dir + "*tfrecord"))
dataset = batch_iterator(tf_records)

#Just iterate over dataset
for i in dataset:
    q_id, q, p_su, n_su = i
    target_p = tf.reshape(p_su, [-1])
    target_n = tf.reshape(n_su, [-1])
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='akanyaani' date='2020-04-03T07:12:35Z'>
		I have tried to reproduce the issue in colab  but it is taking forever to run. Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/87e63e3218b52873242f3e837951a910/untitled761.ipynb&gt;here.&lt;/denchmark-link&gt;
 Thanks!
		</comment>
		<comment id='6' author='akanyaani' date='2020-04-05T13:24:12Z'>
		Hi &lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;

Ya, it takes some time to create 5500 TF Records because it does not give an error with less number of the file.
I ran this Colab and was able to produce memory error I have attached the screenshot of the error.
Colab Gist:- &lt;denchmark-link:https://colab.research.google.com/drive/1j2chtHRpLHjq-dWRsn7Lktgm3I3MgKul&gt;https://colab.research.google.com/drive/1j2chtHRpLHjq-dWRsn7Lktgm3I3MgKul&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/11317416/78499407-263f1e00-776e-11ea-87ba-33341e37b311.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='akanyaani' date='2020-04-05T15:37:35Z'>
		This is not a memory leak and is working as intended. group_by_window will maintain up to num_keys * window_size elements buffered in memory. If you have too many keys or too many windows, you will use a lot of memory.
IIUC, your program is effectively trying to do an in-memory sort using the query ID as the key. If your dataset does not fit into memory this will not be possible.
What is not clear to me is why do you need to use group_by_window in the first place, if your individual TFRecord files are already grouped by query ID. Is it perhaps because your make_dataset method is interleaving entries from different files? If so, I suggest changing that part of your input pipeline.
		</comment>
		<comment id='8' author='akanyaani' date='2020-04-05T19:29:43Z'>
		Hi &lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;

Yes, But I have another reason also, make_datet is interleaving entries from different files, and the second problem is all queries can have a variable number of documents but I want to use all the documents which belong to one query in a batch. Could you please help me out.
		</comment>
		<comment id='9' author='akanyaani' date='2020-04-05T19:59:28Z'>
		The easiest option is to specify num_parallel_reads=None when calling the TFRecordDataset. That way your input pipeline will be processing files in a sequential order instead of interleaving their elements.
If you would like to batch contents of each file into a single batch, you can do something along the following lines:
&lt;denchmark-code&gt;filenames = ...

def file_as_batch(filename):
  ds = tf.data.Dataset.TFRecordDataset(filename, ...)
  return ds.batch(batch_size=MAX_INT)

dataset = filenames.flat_map(file_as_batch)
# At this point `dataset` produces one batch for each input file
&lt;/denchmark-code&gt;

Note that the downstream computation will need to be able to handle variable length batches.
		</comment>
		<comment id='10' author='akanyaani' date='2020-04-06T09:20:52Z'>
		Hi &lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
,
I have updated the code according to your suggestion and it returns the output as an expectation but this pipeline is very slow and GPU has to wait after every step.
&lt;denchmark-code&gt;_READ_RECORD_BUFFER = 1000

def parse_example(serialized_example):
    data_fields = {
        "q_id": tf.io.VarLenFeature(tf.int64),
        "q": tf.io.VarLenFeature(tf.float32),
        "s1": tf.io.VarLenFeature(tf.float32),
        "s2": tf.io.VarLenFeature(tf.float32)}
    
    parsed = tf.io.parse_single_example(serialized_example, data_fields)
    q_id = tf.cast(tf.sparse.to_dense(parsed["q_id"]), tf.int64)[0]
    q = tf.cast(tf.sparse.to_dense(parsed["q"]), tf.float32)
    p_su = tf.cast(tf.sparse.to_dense(parsed["s1"]), tf.float32)
    n_su = tf.cast(tf.sparse.to_dense(parsed["s2"]), tf.float32)

    return (q_id, q, p_su, n_su)

def file_as_batch(filename):
    dataset = tf.data.TFRecordDataset(filename, buffer_size=_READ_RECORD_BUFFER,
                                      num_parallel_reads=12)
    dataset = dataset.map(parse_example, num_parallel_calls=12)
    return dataset.batch(batch_size=768)




dataset = tf.data.Dataset.list_files(path + "/*tfrecord")
dataset = dataset.flat_map(file_as_batch)
dataset = dataset.filter(lambda x, *args: tf.shape(x)[0] &gt;= 64)
dataset = dataset.repeat(100)
dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='akanyaani' date='2020-04-06T17:09:36Z'>
		You should be able to do something along the following lines:
&lt;denchmark-code&gt;PARALLELISM = 10

filenames = tf.data.Dataset.list_files(path + "/*tfrecord")
indices = tf.data.Dataset.range(PARALLELISM)

def make_dataset(shard_index):
  dataset = filenames.shard(PARALLELISM, index)
  dataset = dataset.flat_map(file_as_batch)
  dataset = dataset.filter(lambda x, *args: tf.shape(x)[0] &gt;= 64)
  return dataset.repeat(100)

dataset = indices.interleave(make_dataset, num_parallel_calls=PARALLELISM)
dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
&lt;/denchmark-code&gt;

The above will run PARALLELISM copies of the input pipeline, each reading from a different shard of filenames, interleaving the output of these parallel pipelines. Greater values of PARALLELISM are expected to speed up your input pipeline (up to the number of available CPU cores) but will also result in high memory usage.
		</comment>
		<comment id='12' author='akanyaani' date='2020-04-06T19:40:32Z'>
		Hi &lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;

This one worked perfectly, Thankyou very much for your help.
		</comment>
		<comment id='13' author='akanyaani' date='2020-04-06T19:40:34Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37969&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37969&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>