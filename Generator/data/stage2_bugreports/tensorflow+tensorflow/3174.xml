<bug id='3174' author='vasusharma' open_date='2016-07-03T20:06:25Z' closed_time='2016-08-17T16:48:44Z'>
	<summary>C Stack smashing from inside python when using tensorflow</summary>
	<description>
I am trying to implement Fully Connected Convolutional Network in python using tensorflow and I am getting a Stack Smashing error in one of the Convolution operators. I have checked and all the filter sizes match up and are perfectly in sync with the layer shapes. I did my research on the error and found that this error is caused when you overflow the alloted buffer but I cant see any place where I exceed this buffer and the most weird part is that it's running for 10 layers (7 convolutions and 3 max pool ones of the VGG network) and crashing on the next convolution.
(Posted this on Stack Overflow and was told that this error is due to a bug in tensorflow and hence should be posted here. The reason I was told was that the stack smashing is happening in the C++ code and not the python code and I havent written any C++ code myself so the most likely place for the error to have happened is inside tensorflow)
The error I get is:
-&gt; _, loss_value = sess.run([train_op, loss],feed_dict=feed_dict) (Pdb) c I tensorflow/core/kernels/logging_ops.cc:79] Shape of input image: [1 375 1242 3] I tensorflow/core/kernels/logging_ops.cc:79] Shape of conv1_1[1 375 1242 64] I tensorflow/core/kernels/logging_ops.cc:79] Shape of conv1_2[1 375 1242 64] I tensorflow/core/kernels/logging_ops.cc:79] Shape of pool1[1 188 621 64] I tensorflow/core/kernels/logging_ops.cc:79] Shape of conv2_1[1 188 621 128] I tensorflow/core/kernels/logging_ops.cc:79] Shape of conv2_2[1 188 621 128] I tensorflow/core/kernels/logging_ops.cc:79] Shape of pool2[1 94 311 128] I tensorflow/core/kernels/logging_ops.cc:79] Shape of conv3_1[1 94 311 256] I tensorflow/core/kernels/logging_ops.cc:79] Shape of conv3_2[1 94 311 256] I tensorflow/core/kernels/logging_ops.cc:79] Shape of conv3_3[1 94 311 256] I tensorflow/core/kernels/logging_ops.cc:79] Shape of pool3[1 47 156 256] *** stack smashing detected ***: python terminated Aborted (core dumped)
Here is the relevant part of my code:
&lt;denchmark-code&gt;`class FCN8VGG:
def __init__(self, vgg16_npy_path=None):
    if vgg16_npy_path is None:
        path = sys.modules[self.__class__.__module__].__file__
        # print path
        path = os.path.abspath(os.path.join(path, os.pardir))
        # print path
        path = os.path.join(path, "vgg16.npy")
        print(path)
        vgg16_npy_path = path

    self.data_dict = np.load(vgg16_npy_path).item()
    self.wd = 5e-4
    print("npy file loaded")
def build(self, rgb, train=True, num_classes=14, random_init_fc8=True,
          debug=True):
    """
    Build the VGG model using loaded weights
    Parameters
    ----------
    rgb: image batch tensor
        Image in rgb shap. Scaled to Intervall [0, 255]
    train: bool
        Whether to build train or inference graph
    num_classes: int
        How many classes should be predicted (by fc8)
    random_init_fc8 : bool
        Whether to initialize fc8 layer randomly.
        Finetuning is required in this case.
    debug: bool
        Whether to print additional Debug Information.
    """
    # Convert RGB to BGR

    with tf.name_scope('Processing'):
        #import pdb;pdb.set_trace()
        red, green, blue = tf.split(3, 3, rgb)
        # assert red.get_shape().as_list()[1:] == [224, 224, 1]
        # assert green.get_shape().as_list()[1:] == [224, 224, 1]
        # assert blue.get_shape().as_list()[1:] == [224, 224, 1]
        bgr = tf.concat(3, [
            blue - VGG_MEAN[0],
            green - VGG_MEAN[1],
            red - VGG_MEAN[2],
        ])

        if debug:
            bgr = tf.Print(bgr, [tf.shape(bgr)],
                           message='Shape of input image: ',
                           summarize=4, first_n=1)

    self.conv1_1 = self._conv_layer(bgr, "conv1_1")
    self.conv1_2 = self._conv_layer(self.conv1_1, "conv1_2")
    self.pool1 = self._max_pool(self.conv1_2, 'pool1', debug)

    print("Pool1 layer ready")

    self.conv2_1 = self._conv_layer(self.pool1, "conv2_1")
    self.conv2_2 = self._conv_layer(self.conv2_1, "conv2_2")
    self.pool2 = self._max_pool(self.conv2_2, 'pool2', debug)
    self.conv3_1 = self._conv_layer(self.pool2, "conv3_1")
    self.conv3_2 = self._conv_layer(self.conv3_1, "conv3_2")
    self.conv3_3 = self._conv_layer(self.conv3_2, "conv3_3")
    self.pool3 = self._max_pool(self.conv3_3, 'pool3', debug)

    print("Pool 3 layer ready")
    #pdb.set_trace()
    self.conv4_1 = self._conv_layer(self.pool3, "conv4_1")
    self.conv4_2 = self._conv_layer(self.conv4_1, "conv4_2")
    self.conv4_3 = self._conv_layer(self.conv4_2, "conv4_3")
    self.pool4 = self._max_pool(self.conv4_3, 'pool4', debug)

    print("Pool 4 layer ready")
    #pdb.set_trace()
    self.conv5_1 = self._conv_layer(self.pool4, "conv5_1")
    self.conv5_2 = self._conv_layer(self.conv5_1, "conv5_2")
    self.conv5_3 = self._conv_layer(self.conv5_2, "conv5_3")
    self.pool5 = self._max_pool(self.conv5_3, 'pool5', debug)

    print("Pool 5 layer ready")
    #pdb.set_trace()
    self.fc6 = self._fc_layer(self.pool5, "fc6")

    if train:
        self.fc6 = tf.nn.dropout(self.fc6, 0.5)

    self.fc7 = self._fc_layer(self.fc6, "fc7")
    if train:
        self.fc7 = tf.nn.dropout(self.fc7, 0.5)

    if random_init_fc8:
        self.score_fr = self._score_layer(self.fc7, "score_fr",
                                          num_classes)
    else:
        self.score_fr = self._fc_layer(self.fc7, "score_fr",
                                       num_classes=num_classes,
                                       relu=False)

    self.pred = tf.argmax(self.score_fr, dimension=3)

    self.upscore2 = self._upscore_layer(self.score_fr,
                                        shape=tf.shape(self.pool4),
                                        num_classes=num_classes,
                                        debug=debug, name='upscore2',
                                        ksize=4, stride=2)
    self.score_pool4 = self._score_layer(self.pool4, "score_pool4",
                                         num_classes=num_classes)
    self.fuse_pool4 = tf.add(self.upscore2, self.score_pool4)
    self.upscore4 = self._upscore_layer(self.fuse_pool4,
                                        shape=tf.shape(self.pool3),
                                        num_classes=num_classes,
                                        debug=debug, name='upscore4',
                                        ksize=4, stride=2)
    self.score_pool3 = self._score_layer(self.pool3, "score_pool3",
                                         num_classes=num_classes)
    self.fuse_pool3 = tf.add(self.upscore4, self.score_pool3)

    self.upscore32 = self._upscore_layer(self.fuse_pool3,
                                         shape=tf.shape(bgr),
                                         num_classes=num_classes,
                                         debug=debug, name='upscore32',
                                         ksize=16, stride=8)

    self.pred_up = tf.argmax(self.upscore32, dimension=3)

def _max_pool(self, bottom, name, debug):
    pool = tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],
                          padding='SAME', name=name)

    if debug:
        pool = tf.Print(pool, [tf.shape(pool)],
                        message='Shape of %s' % name,
                        summarize=4, first_n=1)
    return pool

def _conv_layer(self, bottom, name):
    with tf.variable_scope(name) as scope:
        filt = self.get_conv_filter(name)
        conv = tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding='SAME')

        conv = tf.Print(conv, [tf.shape(conv)], message='Shape of %s' % name, summarize=4, first_n=1)

        conv_biases = self.get_bias(name)
        bias = tf.nn.bias_add(conv, conv_biases)
        if relu:
            bias = tf.nn.relu(bias)
        _activation_summary(bias)

        if debug:
            bias = tf.Print(bias, [tf.shape(bias)],
                            message='Shape of %s' % name,
                            summarize=4, first_n=1)
        return bias`

The driver tensorflow code has this command which produces the error:

` _, loss_value = sess.run([train_op, loss],feed_dict=feed_dict)`

Where loss is the tensorflow op for:

`def loss(logits, labels):
"""Calculate the loss from the logits and the labels.

Args:
  logits: Logits tensor, float - [batch_size, height, width, NUM_CLASSES].  Must be unsclaed and unsoftmaxed.
  labels: Labels tensor, int32 - [batch_size, height, width].

Returns:
  loss: Loss tensor of type float.
"""
with tf.name_scope('loss'):
    reshaped_logits = tf.reshape(logits, [-1, 14])  # shape [batch_size*height*width, 14]
    reshaped_labels = tf.reshape(labels, [-1])  # shape [batch_size*height*width]   #Need fix size images for this
    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(reshaped_logits, reshaped_labels)
return loss   `

And training is the driver training function:

`def training(loss, learning_rate):
  """Sets up the training Ops.

  Creates a summarizer to track the loss over time in TensorBoard.

  Creates an optimizer and applies the gradients to all trainable variables.

  The Op returned by this function is what must be passed to the
  `sess.run()` call to cause the model to train.

  Args:
    loss: Loss tensor, from loss().
    learning_rate: The learning rate to use for gradient descent.

  Returns:
    train_op: The Op for training.
  """
  # Add a scalar summary for the snapshot loss.
  tf.scalar_summary(loss.op.name, loss)
  # Create the gradient descent optimizer with the given learning rate.
  #optimizer = tf.train.GradientDescentOptimizer(learning_rate)
  optimizer = tf.train.AdamOptimizer(1e-6)
  # Create a variable to track the global step.
  print("Setting step size to 1e-6 for Adam Optimizer")
  global_step = tf.Variable(0, name='global_step', trainable=False)
  # Use the optimizer to apply the gradients that minimize the loss
  # (and also increment the global step counter) as a single training step.
  print("Running optimizer")
  train_op = optimizer.minimize(loss, global_step=global_step)
  return train_op `
&lt;/denchmark-code&gt;

The debug log from the building of the network is: This contains details of filter and layer shapes etc.
&lt;denchmark-code&gt;`Layer name: conv1_1
Layer shape: (3, 3, 3, 64)
INFO:tensorflow:Created variable conv1_1/filter:0 with shape (3, 3, 3, 64) and init &lt;function _initializer at 0x7fcc27c85ed8&gt;
2016-06-30 14:15:53,939 INFO Created variable conv1_1/filter:0 with shape (3, 3, 3, 64) and init &lt;function _initializer at 0x7fcc27c85ed8&gt;

INFO:tensorflow:Created variable conv1_1/biases:0 with shape (64,) and init &lt;function _initializer at 0x7fcc27c85ed8&gt;
2016-06-30 14:15:53,944 INFO Created variable conv1_1/biases:0 with shape (64,) and init &lt;function _initializer at 0x7fcc27c85ed8&gt;
Layer name: conv1_2
Layer shape: (3, 3, 64, 64)

INFO:tensorflow:Created variable conv1_2/filter:0 with shape (3, 3, 64, 64) and init &lt;function _initializer at 0x7fcc27c96488&gt;
2016-06-30 14:15:53,952 INFO Created variable conv1_2/filter:0 with shape (3, 3, 64, 64) and init &lt;function _initializer at 0x7fcc27c96488&gt;

INFO:tensorflow:Created variable conv1_2/biases:0 with shape (64,) and init &lt;function _initializer at 0x7fcc27c96488&gt;
2016-06-30 14:15:53,956 INFO Created variable conv1_2/biases:0 with shape (64,) and init &lt;function _initializer at 0x7fcc27c96488&gt;
Pool1 layer ready
Layer name: conv2_1
Layer shape: (3, 3, 64, 128)

INFO:tensorflow:Created variable conv2_1/filter:0 with shape (3, 3, 64, 128) and init &lt;function _initializer at 0x7fcc27c9dc08&gt;
2016-06-30 14:15:53,967 INFO Created variable conv2_1/filter:0 with shape (3, 3, 64, 128) and init &lt;function _initializer at 0x7fcc27c9dc08&gt;

INFO:tensorflow:Created variable conv2_1/biases:0 with shape (128,) and init &lt;function _initializer at 0x7fcc27c9dc08&gt;
2016-06-30 14:15:53,972 INFO Created variable conv2_1/biases:0 with shape (128,) and init &lt;function _initializer at 0x7fcc27c9dc08&gt;
Layer name: conv2_2
Layer shape: (3, 3, 128, 128)

INFO:tensorflow:Created variable conv2_2/filter:0 with shape (3, 3, 128, 128) and init &lt;function _initializer at 0x7fcbb3012488&gt;
2016-06-30 14:15:53,980 INFO Created variable conv2_2/filter:0 with shape (3, 3, 128, 128) and init &lt;function _initializer at 0x7fcbb3012488&gt;

INFO:tensorflow:Created variable conv2_2/biases:0 with shape (128,) and init &lt;function _initializer at 0x7fcbb2f8af50&gt;
2016-06-30 14:15:53,985 INFO Created variable conv2_2/biases:0 with shape (128,) and init &lt;function _initializer at 0x7fcbb2f8af50&gt;
Pool 2 layer ready
Layer name: conv3_1
Layer shape: (3, 3, 128, 256)

INFO:tensorflow:Created variable conv3_1/filter:0 with shape (3, 3, 128, 256) and init &lt;function _initializer at 0x7fcbb3012488&gt;
2016-06-30 14:15:53,995 INFO Created variable conv3_1/filter:0 with shape (3, 3, 128, 256) and init &lt;function _initializer at 0x7fcbb3012488&gt;

INFO:tensorflow:Created variable conv3_1/biases:0 with shape (256,) and init &lt;function _initializer at 0x7fcbb1a7cb90&gt;
2016-06-30 14:15:54,000 INFO Created variable conv3_1/biases:0 with shape (256,) and init &lt;function _initializer at 0x7fcbb1a7cb90&gt;
Layer name: conv3_2
Layer shape: (3, 3, 256, 256)

INFO:tensorflow:Created variable conv3_2/filter:0 with shape (3, 3, 256, 256) and init &lt;function _initializer at 0x7fcbb3012488&gt;
2016-06-30 14:15:54,010 INFO Created variable conv3_2/filter:0 with shape (3, 3, 256, 256) and init &lt;function _initializer at 0x7fcbb3012488&gt;

INFO:tensorflow:Created variable conv3_2/biases:0 with shape (256,) and init &lt;function _initializer at 0x7fcbb3012488&gt;
2016-06-30 14:15:54,015 INFO Created variable conv3_2/biases:0 with shape (256,) and init &lt;function _initializer at 0x7fcbb3012488&gt;
Layer name: conv3_3
Layer shape: (3, 3, 256, 256)

INFO:tensorflow:Created variable conv3_3/filter:0 with shape (3, 3, 256, 256) and init &lt;function _initializer at 0x7fcbb1a8aaa0&gt;
2016-06-30 14:15:54,024 INFO Created variable conv3_3/filter:0 with shape (3, 3, 256, 256) and init &lt;function _initializer at 0x7fcbb1a8aaa0&gt;

INFO:tensorflow:Created variable conv3_3/biases:0 with shape (256,) and init &lt;function _initializer at 0x7fcbb1a8aaa0&gt;
2016-06-30 14:15:54,029 INFO Created variable conv3_3/biases:0 with shape (256,) and init &lt;function _initializer at 0x7fcbb1a8aaa0&gt;
Pool 3 layer ready
Layer name: conv4_1
Layer shape: (3, 3, 256, 512)

INFO:tensorflow:Created variable conv4_1/filter:0 with shape (3, 3, 256, 512) and init &lt;function _initializer at 0x7fcbb1a4c7d0&gt;
2016-06-30 14:15:54,043 INFO Created variable conv4_1/filter:0 with shape (3, 3, 256, 512) and init &lt;function _initializer at 0x7fcbb1a4c7d0&gt;

INFO:tensorflow:Created variable conv4_1/biases:0 with shape (512,) and init &lt;function _initializer at 0x7fcbb1a4c7d0&gt;
2016-06-30 14:15:54,048 INFO Created variable conv4_1/biases:0 with shape (512,) and init &lt;function _initializer at 0x7fcbb1a4c7d0&gt;
Layer name: conv4_2
Layer shape: (3, 3, 512, 512)

INFO:tensorflow:Created variable conv4_2/filter:0 with shape (3, 3, 512, 512) and init &lt;function _initializer at 0x7fcbb1a12500&gt;
2016-06-30 14:15:54,063 INFO Created variable conv4_2/filter:0 with shape (3, 3, 512, 512) and init &lt;function _initializer at 0x7fcbb1a12500&gt;

INFO:tensorflow:Created variable conv4_2/biases:0 with shape (512,) and init &lt;function _initializer at 0x7fcbb1a12500&gt;
2016-06-30 14:15:54,068 INFO Created variable conv4_2/biases:0 with shape (512,) and init &lt;function _initializer at 0x7fcbb1a12500&gt;
Layer name: conv4_3
Layer shape: (3, 3, 512, 512)

INFO:tensorflow:Created variable conv4_3/filter:0 with shape (3, 3, 512, 512) and init &lt;function _initializer at 0x7fcbb19d89b0&gt;
2016-06-30 14:15:54,083 INFO Created variable conv4_3/filter:0 with shape (3, 3, 512, 512) and init &lt;function _initializer at 0x7fcbb19d89b0&gt;

INFO:tensorflow:Created variable conv4_3/biases:0 with shape (512,) and init &lt;function _initializer at 0x7fcbb19d89b0&gt;
2016-06-30 14:15:54,088 INFO Created variable conv4_3/biases:0 with shape (512,) and init &lt;function _initializer at 0x7fcbb19d89b0&gt;
Pool 4 layer ready
Layer name: conv5_1
Layer shape: (3, 3, 512, 512)

INFO:tensorflow:Created variable conv5_1/filter:0 with shape (3, 3, 512, 512) and init &lt;function _initializer at 0x7fcbb199f6e0&gt;
2016-06-30 14:15:54,103 INFO Created variable conv5_1/filter:0 with shape (3, 3, 512, 512) and init &lt;function _initializer at 0x7fcbb199f6e0&gt;

INFO:tensorflow:Created variable conv5_1/biases:0 with shape (512,) and init &lt;function _initializer at 0x7fcbb199f6e0&gt;
2016-06-30 14:15:54,108 INFO Created variable conv5_1/biases:0 with shape (512,) and init &lt;function _initializer at 0x7fcbb199f6e0&gt;
Layer name: conv5_2
Layer shape: (3, 3, 512, 512)

INFO:tensorflow:Created variable conv5_2/filter:0 with shape (3, 3, 512, 512) and init &lt;function _initializer at 0x7fcbb1961488&gt;
2016-06-30 14:15:54,122 INFO Created variable conv5_2/filter:0 with shape (3, 3, 512, 512) and init &lt;function _initializer at 0x7fcbb1961488&gt;

INFO:tensorflow:Created variable conv5_2/biases:0 with shape (512,) and init &lt;function _initializer at 0x7fcbb18daf50&gt;
2016-06-30 14:15:54,127 INFO Created variable conv5_2/biases:0 with shape (512,) and init &lt;function _initializer at 0x7fcbb18daf50&gt;
Layer name: conv5_3
Layer shape: (3, 3, 512, 512)

INFO:tensorflow:Created variable conv5_3/filter:0 with shape (3, 3, 512, 512) and init &lt;function _initializer at 0x7fcbb1961488&gt;
2016-06-30 14:15:54,141 INFO Created variable conv5_3/filter:0 with shape (3, 3, 512, 512) and init &lt;function _initializer at 0x7fcbb1961488&gt;

INFO:tensorflow:Created variable conv5_3/biases:0 with shape (512,) and init &lt;function _initializer at 0x7fcbb1961488&gt;
2016-06-30 14:15:54,146 INFO Created variable conv5_3/biases:0 with shape (512,) and init &lt;function _initializer at 0x7fcbb1961488&gt;
Pool 5 layer ready
Layer name: fc6
Layer shape: [7, 7, 512, 4096]

INFO:tensorflow:Created variable fc6/weights:0 with shape (7, 7, 512, 4096) and init &lt;function _initializer at 0x7fcbb186b5f0&gt;
2016-06-30 14:15:54,435 INFO Created variable fc6/weights:0 with shape (7, 7, 512, 4096) and init &lt;function _initializer at 0x7fcbb186b5f0&gt;

INFO:tensorflow:Created variable fc6/biases:0 with shape (4096,) and init &lt;function _initializer at 0x7fcbb186b5f0&gt;
2016-06-30 14:15:54,438 INFO Created variable fc6/biases:0 with shape (4096,) and init &lt;function _initializer at 0x7fcbb186b5f0&gt;
Layer name: fc7
Layer shape: [1, 1, 4096, 4096]

INFO:tensorflow:Created variable fc7/weights:0 with shape (1, 1, 4096, 4096) and init &lt;function _initializer at 0x7fcbb189fed8&gt;
2016-06-30 14:15:54,492 INFO Created variable fc7/weights:0 with shape (1, 1, 4096, 4096) and init &lt;function _initializer at 0x7fcbb189fed8&gt;

INFO:tensorflow:Created variable fc7/biases:0 with shape (4096,) and init &lt;function _initializer at 0x7fcbb189fed8&gt;
2016-06-30 14:15:54,495 INFO Created variable fc7/biases:0 with shape (4096,) and init &lt;function _initializer at 0x7fcbb189fed8&gt;

INFO:tensorflow:Created variable score_fr/weights:0 with shape (1, 1, 4096, 14) and init &lt;function _initializer at 0x7fcbb186b5f0&gt;
2016-06-30 14:15:54,506 INFO Created variable score_fr/weights:0 with shape (1, 1, 4096, 14) and init &lt;function _initializer at 0x7fcbb186b5f0&gt;

INFO:tensorflow:Created variable score_fr/biases:0 with shape (14,) and init &lt;function _initializer at 0x7fcbb186b5f0&gt;
2016-06-30 14:15:54,510 INFO Created variable score_fr/biases:0 with shape (14,) and init &lt;function _initializer at 0x7fcbb186b5f0&gt;

INFO:tensorflow:Created variable upscore2/up_filter:0 with shape (4, 4, 14, 14) and init &lt;function _initializer at 0x7fcbb17a48c0&gt;
2016-06-30 14:15:54,525 INFO Created variable upscore2/up_filter:0 with shape (4, 4, 14, 14) and init &lt;function _initializer at 0x7fcbb17a48c0&gt;

INFO:tensorflow:Created variable score_pool4/weights:0 with shape (1, 1, 512, 14) and init &lt;function _initializer at 0x7fcbb17a48c0&gt;
2016-06-30 14:15:54,536 INFO Created variable score_pool4/weights:0 with shape (1, 1, 512, 14) and init &lt;function _initializer at 0x7fcbb17a48c0&gt;

INFO:tensorflow:Created variable score_pool4/biases:0 with shape (14,) and init &lt;function _initializer at 0x7fcbb17a48c0&gt;
2016-06-30 14:15:54,540 INFO Created variable score_pool4/biases:0 with shape (14,) and init &lt;function _initializer at 0x7fcbb17a48c0&gt;

INFO:tensorflow:Created variable upscore4/up_filter:0 with shape (4, 4, 14, 14) and init &lt;function _initializer at 0x7fcbb16c0aa0&gt;
2016-06-30 14:15:54,555 INFO Created variable upscore4/up_filter:0 with shape (4, 4, 14, 14) and init &lt;function _initializer at 0x7fcbb16c0aa0&gt;

INFO:tensorflow:Created variable score_pool3/weights:0 with shape (1, 1, 256, 14) and init &lt;function _initializer at 0x7fcbb16c0aa0&gt;
2016-06-30 14:15:54,566 INFO Created variable score_pool3/weights:0 with shape (1, 1, 256, 14) and init &lt;function _initializer at 0x7fcbb16c0aa0&gt;

INFO:tensorflow:Created variable score_pool3/biases:0 with shape (14,) and init &lt;function _initializer at 0x7fcbb16c0aa0&gt;
2016-06-30 14:15:54,570 INFO Created variable score_pool3/biases:0 with shape (14,) and init &lt;function _initializer at 0x7fcbb16c0aa0&gt;

INFO:tensorflow:Created variable upscore32/up_filter:0 with shape (16, 16, 14, 14) and init &lt;function _initializer at 0x7fcbb15eb758&gt;
2016-06-30 14:15:54,585 INFO Created variable upscore32/up_filter:0 with shape (16, 16, 14, 14) and init &lt;function _initializer at 0x7fcbb15eb758&gt;`
&lt;/denchmark-code&gt;

Been stuck on this for quite some time and cant figure out whats wrong... Any help will be appreciated :)
	</description>
	<comments>
		<comment id='1' author='vasusharma' date='2016-08-05T17:53:42Z'>
		Is this still occurring (try the 0.10RC for example).  If it is still occurring, it will be difficult to help you any further without you providing a self-contained reproducible test case. I understand these errors are frustrating.
		</comment>
		<comment id='2' author='vasusharma' date='2016-08-17T16:48:44Z'>
		Automatically closing due to lack of recent activity. Please reopen when additional becomes available.
		</comment>
	</comments>
</bug>