<bug id='33043' author='inscite' open_date='2019-10-04T07:18:38Z' closed_time='2019-10-09T08:44:43Z'>
	<summary>Dumping XLA compiled results in specific directory not working</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution : CentOS 7.6 x86_64
TensorFlow installed from (source or binary):
TensorFlow version (use command below): r1.12.3
Python version: 3.6.7
Bazel version (if compiling from source): 0.19.1
GCC/Compiler version (if compiling from source): gcc 7.4.0
CUDA/cuDNN version: CUDA 10.1 / cuDNN 7.5
GPU model and memory: Tesla V100-PCIE-32GB / Titan Xp 12GB

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior

XLA compiled model files are not saved in the desired directory, even though I declared the xla dump directory in an explicit manner
According to this link (https://www.tensorflow.org/xla), a compiled model must be saved as module_XXXX.ptx or something, which is not visible.
Meanwhile, I only can see the graph descriptions about the model under /tmp, even though I did already declared a different directory far from /tmp.

&lt;denchmark-code&gt;$ ls /tmp
before_mark_for_compilation_1.pbtxt
before_mark_for_compilation_2.pbtxt
before_mark_for_compilation_3.pbtxt
before_mark_for_compilation_4.pbtxt
before_mark_for_compilation_5.pbtxt
before_mark_for_compilation_6.pbtxt
before_mark_for_compilation.pbtxt
mark_for_compilation_1.pbtxt
mark_for_compilation_2.pbtxt
mark_for_compilation_3.pbtxt
mark_for_compilation_4.pbtxt
mark_for_compilation_5.pbtxt
mark_for_compilation_6.pbtxt
mark_for_compilation.pbtxt
&lt;/denchmark-code&gt;

Describe the expected behavior
&lt;denchmark-code&gt;# XLA compiled module files may be saved in /somewhere/xladump
$ ls /somewhere/xladump
&lt;/denchmark-code&gt;

Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
&lt;denchmark-code&gt;TF_XLA_FLAGS="--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit --tf_xla_clustering_debug" TF_DUMP_GRAPH_PREFIX=/somewhere/xladump XLA_FLAGS="--dump_hlo_as_text --xla_dump_to=/somewhere/xladump" PYTHONDONTWRITEBYTECODE=1 time python blahblah.py
&lt;/denchmark-code&gt;

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
[Build Flags]
&lt;denchmark-code&gt;- Build with Ignite? Y
- Build with XLA JIT? Y
...
&lt;/denchmark-code&gt;

[Runtime Log]
&lt;denchmark-code&gt;2019-10-04 15:10:36.868003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-04 15:10:36.868683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.911
pciBusID: 0000:09:00.0
totalMemory: 11.91GiB freeMemory: 11.75GiB
2019-10-04 15:10:36.868705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-10-04 15:10:37.195028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-10-04 15:10:37.195073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0
2019-10-04 15:10:37.195082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N
2019-10-04 15:10:37.195174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11367 MB memory) -&gt; physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:09:00.0, compute capability: 6.1)
2019-10-04 15:10:37.784171: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /tmp//before_mark_for_compilation.pbtxt
2019-10-04 15:10:37.794203: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /tmp//mark_for_compilation.pbtxt
2019-10-04 15:10:37.847682: I tensorflow/compiler/xla/service/service.cc:149] XLA service 0x7f237c001c70 executing computations on platform CUDA. Devices:
2019-10-04 15:10:37.847749: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (0): TITAN Xp, Compute Capability 6.1
2019-10-04 15:10:37.903711: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:402] *** WARNING *** You are using ptxas 10.0.145, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.
2019-10-04 15:10:40.008334: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /tmp//before_mark_for_compilation_1.pbtxt
2019-10-04 15:10:40.009382: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /tmp//mark_for_compilation_1.pbtxt
2019-10-04 15:10:40.548377: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /tmp//before_mark_for_compilation_2.pbtxt
2019-10-04 15:10:40.549325: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /tmp//mark_for_compilation_2.pbtxt
2019-10-04 15:10:40.783753: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /tmp//before_mark_for_compilation_3.pbtxt
2019-10-04 15:10:40.784706: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /tmp//mark_for_compilation_3.pbtxt
[I] TF_XLA_FLAGS=--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit --tf_xla_clustering_debug
[I] XLA_FLAGS=--dump_hlo_as_text --xla_dump_to=sponge
[I] TF_DUMP_GRAPH_PREFIX=sponge
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='inscite' date='2019-10-08T14:16:36Z'>
		My guess is that XLA is not compiling anything in your model.
The XLA cluster nodes should be marked as _XlaCluster in the mark_for_compilation_1.pbtxt graphs, are you seeing any?
		</comment>
		<comment id='2' author='inscite' date='2019-10-09T05:58:17Z'>
		Dear &lt;denchmark-link:https://github.com/sanjoy&gt;@sanjoy&lt;/denchmark-link&gt;
,
Some parameters containing XLA cluster found in multiple mark_for_compilation_*.pbtxt graphs.
Furthermore, in /tmp, I could find multiple outputs from tensorflow, such as...
&lt;denchmark-code&gt;before_mark_for_compilation_10.pbtxt  ccY1v2Nx.s
before_mark_for_compilation_11.pbtxt  ccY3BvQB.s
before_mark_for_compilation_12.pbtxt  ccYOO5oC.s
before_mark_for_compilation_13.pbtxt  ccYzOwnP.s
before_mark_for_compilation_14.pbtxt  cczFaENP.s
...
before_mark_for_compilation_28.pbtxt  tmpxft_00004a65_00000000
before_mark_for_compilation_29.pbtxt  tmpxft_00004a65_00000000-0
before_mark_for_compilation_2.pbtxt   tmpxft_00004a65_00000000-1.cpp
before_mark_for_compilation_30.pbtxt  tmpxft_00004a65_00000000-2_cwise_op_gpu_xdivy.cu.fatbin.c
before_mark_for_compilation_31.pbtxt  tmpxft_00004a65_00000000-3_cwise_op_gpu_xdivy.cu.module_id
before_mark_for_compilation_32.pbtxt  tmpxft_00004a65_00000000-4_cwise_op_gpu_xdivy.cu.cpp4.ii
before_mark_for_compilation_33.pbtxt  tmpxft_00004a65_00000000-5_cwise_op_gpu_xdivy.cu.cudafe1.c
before_mark_for_compilation_34.pbtxt  tmpxft_00004a65_00000000-5_cwise_op_gpu_xdivy.cu.cudafe1.cpp
before_mark_for_compilation_35.pbtxt  tmpxft_00004a65_00000000-5_cwise_op_gpu_xdivy.cu.cudafe1.gpu
before_mark_for_compilation_36.pbtxt  tmpxft_00004a65_00000000-5_cwise_op_gpu_xdivy.cu.cudafe1.stub.c
before_mark_for_compilation_37.pbtxt  tmpxft_00004a65_00000000-5_cwise_op_gpu_xdivy.cu.ptx
before_mark_for_compilation_38.pbtxt  tmpxft_00004a65_00000000-6_cwise_op_gpu_xdivy.cu.cpp1.ii
before_mark_for_compilation_39.pbtxt  tmpxft_00004a65_00000000-7_cwise_op_gpu_xdivy.cu.sm_70.cubin
&lt;/denchmark-code&gt;

I believe xla optimization is working by all means and outputs are exported in /tmp,
which is not expected behavior when I declare the flags such as --xla_dump_to=/somewhere/xladump.
Meanwhile, I am revising my runtime code now to initiate an explicit XLA compiling as well as avoiding AUTO_JIT.
		</comment>
		<comment id='3' author='inscite' date='2019-10-09T08:44:43Z'>
		After a deep survey on the source code of r1.12.3, especially a part of 'compiler',
I found a valid configuration for the explicit target directory selection.
For further reference, --tf_dump_graph_prefix=/path/to/somewhere must be declared,
in part of XLA environment variables as TF_XLA_FLAGS.
(e.g. TF_XLA_FLAGS="--something --tf_dump_graph_prefix=/path/to/somewhere")
(This hint was found in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/5b900cfe4b3b848f577315a0dde09a729f770e95/tensorflow/compiler/tf2xla/dump_graph.h&gt;tensorflow/compiler/tf2xla/dump_graph.h&lt;/denchmark-link&gt;
 of commit &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/5b900cfe4b3b848f577315a0dde09a729f770e95&gt;5b900cf&lt;/denchmark-link&gt;
 - r1.12.3)
As a result, the documentation of XLA usage is far different from actual behavior of the source code.
In other words, using env TF_DUMP_GRAPH_PREFIX does NOTING in tf-r1.12.3 described as below.
&lt;denchmark-code&gt;$ TF_DUMP_GRAPH_PREFIX=/tmp/generated \
  TF_XLA_FLAGS="--tf_xla_clustering_debug --tf_xla_auto_jit=2" \
  XLA_FLAGS="--xla_dump_hlo_as_text --xla_dump_to=/tmp/generated" \
    my/tensorflow/program"
&lt;/denchmark-code&gt;

As a result, here's some log from tensorflow runtime,
&lt;denchmark-code&gt;...
2019-10-09 17:23:37.144396: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /path/to/somewhere/mark_for_compilation_57.pbtxt
...
&lt;/denchmark-code&gt;

as well as directory listing under /path/to/somewhere.
&lt;denchmark-code&gt;$ ls /path/to/somewhere/
before_mark_for_compilation_10.pbtxt  mark_for_compilation_10.pbtxt
before_mark_for_compilation_11.pbtxt  mark_for_compilation_11.pbtxt
...
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='inscite' date='2019-10-09T08:44:44Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33043&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33043&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>