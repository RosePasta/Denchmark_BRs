<bug id='22088' author='jrabary' open_date='2018-09-05T10:23:21Z' closed_time='2019-05-03T01:14:29Z'>
	<summary>distribute.MirroredStrategy fails with Resource exhausted: OOM when allocating tensor with shape</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):  source r1.10
TensorFlow version (use command below): 1.10
Python version:
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:

You can collect some of this information using our environment capture script:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&lt;/denchmark-link&gt;

You can obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

We have a training code based on tf.Estimator that works well on single GPU with tf.contrib.distribute.OneDeviceStrategy("device:GPU:0"). But when we add another GPU and change the distribution type to tf.contrib.distribute.MirroredStrategy(num_gpus=num_gpus) the training code doesn't run anymore and raise an ugly memory allocation error. Even if we reduce drastically the batch size (from 128 to 64). Below you'll find a sample of the output error.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;2018-09-05 12:06:36.826713: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fa445486000 of size 2013265920
2018-09-05 12:06:36.826719: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fa4bd486000 of size 2013265920
2018-09-05 12:06:36.826725: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0x7fa535486000 of size 2013265920
2018-09-05 12:06:36.826730: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fa5ad486000 of size 2013501696
2018-09-05 12:06:36.826735: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0x7fa6254bf900 of size 1855833856
2018-09-05 12:06:36.826741: I tensorflow/core/common_runtime/bfc_allocator.cc:671]      Summary of in-use Chunks by size:
2018-09-05 12:06:36.826748: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 17 Chunks of size 256 totalling 4.2KiB
2018-09-05 12:06:36.826754: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 1280 totalling 2.5KiB
2018-09-05 12:06:36.826760: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 2048 totalling 6.0KiB
2018-09-05 12:06:36.826766: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 2304 totalling 2.2KiB
2018-09-05 12:06:36.826772: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 9728 totalling 19.0KiB
2018-09-05 12:06:36.826778: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 73728 totalling 144.0KiB
2018-09-05 12:06:36.826784: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 147456 totalling 288.0KiB
2018-09-05 12:06:36.826791: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 13516800 totalling 12.89MiB
2018-09-05 12:06:36.826797: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 2013265920 totalling 3.75GiB
2018-09-05 12:06:36.826803: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 2013501696 totalling 1.88GiB
2018-09-05 12:06:36.826809: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Sum Total of in-use chunks: 5.64GiB
2018-09-05 12:06:36.826818: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats:
Limit:                 11922948096
InUse:                  6054027520
MaxInUse:               9980828416
NumAllocs:                     153
MaxAllocSize:           3507027968

2018-09-05 12:06:36.826832: W tensorflow/core/common_runtime/bfc_allocator.cc:279] *_______________***********************************________________******************_______________
2018-09-05 12:06:36.826879: W tensorflow/core/framework/op_kernel.cc:1275] OP_REQUIRES failed at nccl_ops.cc:96 : Resource exhausted: OOM when allocating tensor with shape[503375361] and type float on /job:localhost/replica:0/task:0/device:GPU:1 by allocator GPU_1_bfc

...

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[503375361] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: NcclAllReduce = NcclAllReduce[T=DT_FLOAT, _class=["loc:@Reshape_28"], num_devices=2, reduction="sum", shared_name="c0", _device="/job:localhost/replica:0/task:0/device:GPU:0"](concat)]]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='jrabary' date='2018-09-05T18:53:41Z'>
		&lt;denchmark-link:https://github.com/josh11b&gt;@josh11b&lt;/denchmark-link&gt;
 Can you help here?
		</comment>
		<comment id='2' author='jrabary' date='2018-09-14T20:09:18Z'>
		&lt;denchmark-link:https://github.com/jrabary&gt;@jrabary&lt;/denchmark-link&gt;
 can you provide code to reproduce this?
&lt;denchmark-link:https://github.com/chsigg&gt;@chsigg&lt;/denchmark-link&gt;
 looks like the OOM is coming from the Nccl op, could you help take look as well?
		</comment>
		<comment id='3' author='jrabary' date='2018-09-16T15:38:36Z'>
		&lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
 here is an example of code to reproduce this. Tested on Nvidia Titan X. The model is big enough to completely fill the memory of a single GPU but it still works in one device strategy.
You get the problem when you change the number of GPU to 2 for example.
&lt;denchmark-code&gt;import tensorflow as tf
layers = tf.keras.layers

class MyModel(tf.keras.Model):
    """
    Simple CNN model.
    """

    def __init__(self, name=''):
        super(MyModel, self).__init__(name=name)

        self.flatten = layers.Flatten()

        kernel_initializer = tf.variance_scaling_initializer(scale=1.0 / 3.0, distribution='uniform')

        self.conv1 = layers.Conv2D(32, (3, 3), name='conv1', activation=tf.nn.relu6,
                                   kernel_initializer=kernel_initializer)
        self.conv2 = layers.Conv2D(64, (3, 3), name='conv2', activation=tf.nn.relu6,
                                   kernel_initializer=kernel_initializer)
        self.conv3 = layers.Conv2D(64, (3, 3), name='conv3', activation=tf.nn.relu6,
                                   kernel_initializer=kernel_initializer)
        self.fc1 = layers.Dense(512, name='fc1', activation=tf.nn.relu6)
        self.steer_predictor = layers.Dense(1, name='steer_predictor')

    def call(self, inputs, training=True):
        y = self.conv1(inputs)

        y = self.conv2(y)

        y = self.conv3(y)
        y = self.flatten(y)

        y = self.fc1(y)
        y = self.steer_predictor(y)

        return y


def get_distribution_strategy(num_gpus, all_reduce_alg=None):
    """Return a DistributionStrategy for running the model.
    Args:
      num_gpus: Number of GPUs to run this model.
      all_reduce_alg: Specify which algorithm to use when performing all-reduce.
        See tf.contrib.distribute.AllReduceCrossTowerOps for available algorithms.
        If None, DistributionStrategy will choose based on device topology.
    Returns:
      tf.contrib.distribute.DistibutionStrategy object.
    """
    if num_gpus == 0:
        return tf.contrib.distribute.OneDeviceStrategy("device:CPU:0")
    elif num_gpus == 1:
        return tf.contrib.distribute.OneDeviceStrategy("device:GPU:0")
    else:
        if all_reduce_alg:
            return tf.contrib.distribute.MirroredStrategy(
                num_gpus=num_gpus,
                cross_tower_ops=tf.contrib.distribute.AllReduceCrossTowerOps(
                    all_reduce_alg, num_packs=num_gpus))
        else:
            return tf.contrib.distribute.MirroredStrategy(num_gpus=num_gpus)


def model_fn(features, labels, mode, params):
    """ Model function to be used by the estimator.

    Returns:
      An EstimatorSpec object
    """

    is_training = mode == tf.estimator.ModeKeys.TRAIN

    model = MyModel()

    predictions = model(features, training=is_training)

    loss = tf.losses.mean_squared_error(labels, predictions)

    if mode == tf.estimator.ModeKeys.TRAIN:

        global_step = tf.train.get_or_create_global_step()

        learning_rate = tf.train.linear_cosine_decay(0.0001,
                                                     global_step,
                                                     10000,
                                                     beta=0.01)

        optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)

        train_op = tf.contrib.training.create_train_op(loss,
                                                       optimizer,
                                                       global_step,
                                                       summarize_gradients=False)
        # summaries
        tf.summary.image('inputs', features, max_outputs=6)
        tf.summary.scalar('training/learning_rate', learning_rate)

        return tf.estimator.EstimatorSpec(mode=mode,
                                          predictions=None,
                                          loss=loss,
                                          train_op=train_op,
                                          training_hooks=None)

    if mode == tf.estimator.ModeKeys.EVAL:
        eval_metric_ops = {
            'mae': tf.metrics.mean_absolute_error(labels, predictions),
        }
        return tf.estimator.EstimatorSpec(
            mode=mode,
            loss=loss,
            eval_metric_ops=eval_metric_ops
        )


def create_input_fn():

    def input_fn():
        features = tf.random_uniform([100, 88, 200, 3])
        labels = tf.random_uniform([100, 1])
        data = tf.data.Dataset.from_tensor_slices((features, labels)).repeat().batch(128)
        return data

    return input_fn



def main(_):

    num_gpus = 2

    # run configuration
    distribution = get_distribution_strategy(num_gpus)

    # tf session config
    session_config = tf.ConfigProto(inter_op_parallelism_threads=64,
                                    intra_op_parallelism_threads=64,
                                    allow_soft_placement=True)

    run_config = tf.estimator.RunConfig(save_summary_steps=100,
                                        train_distribute=distribution,
                                        session_config=session_config)

    # Create estimator that trains and evaluates the model
    ml_estimator = tf.estimator.Estimator(
        model_fn=model_fn,
        model_dir='/tmp/model',
        config=run_config,
        params={}
    )

    ml_estimator.train(input_fn=create_input_fn(), steps=100)


if __name__ == '__main__':
    # Set tensorflow verbosity
    tf.logging.set_verbosity(tf.logging.INFO)

    # Run the experiment
    tf.app.run()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='jrabary' date='2018-10-11T04:02:39Z'>
		&lt;denchmark-link:https://github.com/yuefengz&gt;@yuefengz&lt;/denchmark-link&gt;
 I have encountered a similar problem, any updates?
		</comment>
		<comment id='5' author='jrabary' date='2018-10-18T00:33:03Z'>
		&lt;denchmark-link:https://github.com/jrabary&gt;@jrabary&lt;/denchmark-link&gt;
 thank you for sharing the code. Could you try setting  when you define the  using ? My hypothesis is that it is packing all the gradients into 2 tensors (with num_packs=2) and this is too big for nccl to handle.
		</comment>
		<comment id='6' author='jrabary' date='2018-11-02T07:12:21Z'>
		&lt;denchmark-link:https://github.com/jrabary&gt;@jrabary&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/fanshiqing&gt;@fanshiqing&lt;/denchmark-link&gt;
 did any of you get a chance to try out the suggestion &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/22088#issuecomment-430835141&gt;above&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='7' author='jrabary' date='2018-11-07T16:03:09Z'>
		Hi &lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
, I did and setting  seems to work. So what does it really mean if we set num_packs to zero ? And what are the side effects w.r.t optimisation results ?
		</comment>
		<comment id='8' author='jrabary' date='2018-11-09T07:04:02Z'>
		&lt;denchmark-link:https://github.com/jrabary&gt;@jrabary&lt;/denchmark-link&gt;
 thanks for trying it out. num_packs=0 means that we will reduce each gradient separately, instead of trying to combine all of them into a small number of tensors first. Performance impact will depend on the use case. In the cases where the gradient tensors are large though (like it seems in your use case), it is not feasible to combine them given the limited memory.
&lt;denchmark-link:https://github.com/yuefengz&gt;@yuefengz&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/dubey&gt;@dubey&lt;/denchmark-link&gt;
 it seems like we should not try to combine gradients if there isn't enough memory. Can we do this in MirroredStrategy? Does CollectiveAllReduceStrategy check this?
		</comment>
		<comment id='9' author='jrabary' date='2018-11-09T16:38:18Z'>
		From what I can understand, packing and splitting shouldn't affect overall memory usage.  This logic concats many (small) tensors into num_packs (larger) tensors.  But overall memory usage should remain the same.
I'm not aware of a size limitation when using nccl.
Looking at the logs above, the  failure reports OOM at , but the  reports OOM at .  &lt;denchmark-link:https://github.com/yuefengz&gt;@yuefengz&lt;/denchmark-link&gt;
 could there be an issue with placing the concat and split ops on correct devices?  I noticed that we use  which was recently deprecated.
Collectives has this logic built into the C++ backend via the ScopedAllocator.  Conceptually it does a similar thing.  We haven't seen any OOMs due to ScopedAllocator.
		</comment>
		<comment id='10' author='jrabary' date='2018-11-09T19:22:21Z'>
		The concat op has to create a memory block to store the concatenated result. In our our nccl packing algorithm, we concat all gradients into one large tensor. We can switch to a different tensor aggregation method (specifying non-zero agg_small_grads_max_bytes and agg_small_grads_max_group and set num_packs to 0) or even disable tensor aggregation. CollectiveAllReduceStrategy is another option which I think avoids creating large concatenated tensors as well.
		</comment>
		<comment id='11' author='jrabary' date='2019-05-03T01:12:39Z'>
		Does the physical second GPU that you are adding have the same amount of memory as the first?
		</comment>
		<comment id='12' author='jrabary' date='2019-05-03T01:14:29Z'>
		It's been a while and we have suggested work-arounds.  Please re-open if you still have this problem at newer versions of TF.
		</comment>
		<comment id='13' author='jrabary' date='2019-05-03T01:14:30Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=22088&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=22088&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>