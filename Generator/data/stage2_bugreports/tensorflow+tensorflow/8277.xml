<bug id='8277' author='cancan101' open_date='2017-03-10T17:02:19Z' closed_time='2017-12-22T18:10:43Z'>
	<summary>Different Code Path Taken in conv2d for Constant vs Variable Filter</summary>
	<description>
It appears that a different code path is taken in conv2d for Constant vs Variable filters.
On a box with GPU, this works:
images = tf.constant(np.arange(2*15. * 15*2).reshape((2,2,15,15)).astype(np.float32))
filters = tf.Variable(initial_value=1 * np.ones((1,1,2,1), np.float32))
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    output = nn_ops.conv2d(
      images,
      filters,
      strides=(1,1,1,1),
      padding='VALID',
      data_format='NCHW',
  ).eval()
however this fails with Check failed: data_format == FORMAT_NHWC Generic conv implementation only supports NHWC tensor format for now.:
images = tf.constant(np.arange(2*15. * 15*2).reshape((2,2,15,15)).astype(np.float32))
filters = tf.constant(1 * np.ones((1,1,2,1), np.float32))
with tf.Session() as sess:
    output = nn_ops.conv2d(
      images,
      filters,
      strides=(1,1,1,1),
      padding='VALID',
      data_format='NCHW',
  ).eval()
Both of these work (the data_format is supported):
images = tf.constant(np.arange(2*15. * 15*2).reshape((2,15,15,2)).astype(np.float32))
filters = tf.Variable(initial_value=1 * np.ones((1,1,2,1), np.float32))
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    output = nn_ops.conv2d(
      images,
      filters,
      strides=(1,1,1,1),
      padding='VALID',
  ).eval()
and
images = tf.constant(np.arange(2*15. * 15*2).reshape((2,15,15,2)).astype(np.float32))
filters = tf.constant(1 * np.ones((1,1,2,1), np.float32))
with tf.Session() as sess:
    output = nn_ops.conv2d(
      images,
      filters,
      strides=(1,1,1,1),
      padding='VALID',
  ).eval()
I don't see a reason for the Constant filter to take what I am guessing is a less efficient code path (not using GPU op) than the Variable Filter.
To make things even weirder, it seems like even though the GPU is not being used for the Constant, there is still cuda memcpy. Run on 20 calls:
&lt;denchmark-code&gt;Time(%)      Time     Calls       Avg       Min       Max  Name
 52.45%  90.108us        20  4.5050us  4.0950us  6.0800us  [CUDA memcpy HtoD]
 42.91%  73.726us        20  3.6860us  3.5520us  4.5120us  [CUDA memcpy DtoH]
  4.64%  7.9680us         1  7.9680us  7.9680us  7.9680us  [CUDA memset]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='cancan101' date='2017-03-24T02:19:49Z'>
		Is there any way to convert a pre-trained NCHW weight into NHWC and load it into the same graph?
		</comment>
		<comment id='2' author='cancan101' date='2017-03-24T03:04:00Z'>
		What do you mean? Weights are always stored the same way.
		</comment>
		<comment id='3' author='cancan101' date='2017-06-16T20:30:21Z'>
		&lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
 Any ideas?
		</comment>
		<comment id='4' author='cancan101' date='2017-09-19T05:06:08Z'>
		Hi &lt;denchmark-link:https://github.com/cancan101&gt;@cancan101&lt;/denchmark-link&gt;
 ,
I've a net (graph) with some conv2D and maxPool operations declared using NCHW format. I've trained the net on GPUs and created some checkpoints. The graph and the model works well on GPUs when you test by restoring the checkpoints.
Now I want to test the same code/graph on CPU.  The same net/graph is not working because the conv2D and maxPool operations do not run since they were declared using NCHW.
I'm getting the following error:
&lt;denchmark-code&gt;2017-09-18 17:57:48.890761: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: Default MaxPoolingOp only supports NHWC.
         [[Node: text_box_300/pool1/MaxPool = MaxPool[T=DT_FLOAT, data_format="NCHW", ksize=[1, 1, 2, 2], padding="SAME", strides=[1, 1, 2, 2], _device="/job:localhost/replica:0/task:0/cpu:0"](text_box_300/conv1/conv1_2/Relu)]]
2017-09-18 17:57:48.892768: E main_textd_test.cc:457] Running model failed: Invalid argument: Default MaxPoolingOp only supports NHWC.
         [[Node: text_box_300/pool1/MaxPool = MaxPool[T=DT_FLOAT, data_format="NCHW", ksize=[1, 1, 2, 2], padding="SAME", strides=[1, 1, 2, 2], _device="/job:localhost/replica:0/task:0/cpu:0"](text_box_300/conv1/conv1_2/Relu)]]
&lt;/denchmark-code&gt;

So I've modified the net so that the operations are declared using NHWC format. Is it possible to restore the same checkpoint weights with this new net/graph without retraining the new graph (that has  NHWC ops) ?
finally I want to freeze the graph and have a standalone graph with weights to run/test forward pass.  I was not able to create a standalone graph using checkpoints I've got when I trained me NCHW graph and use the standalone graph on CPU for testing (running forward pass) ? Is there a way to do it ?
		</comment>
		<comment id='5' author='cancan101' date='2017-12-22T07:49:46Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='6' author='cancan101' date='2017-12-22T18:10:43Z'>
		I tried with tensorflow-gpu v1.4.0, and wasn't able to reproduce this issue. Close for now. Please feel free to reopen it if you think the issue still exists.
		</comment>
	</comments>
</bug>