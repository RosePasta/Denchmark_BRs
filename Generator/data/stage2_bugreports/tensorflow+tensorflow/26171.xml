<bug id='26171' author='amelroua' open_date='2019-02-27T12:43:05Z' closed_time='2019-03-15T13:08:57Z'>
	<summary>RuntimeError: Graph is finalized and cannot be modified.</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a
TensorFlow installed from (source or binary):n/a
TensorFlow version (use command below):1.12.0
Python version:2.7.x
Bazel version (if compiling from source):n/a
GCC/Compiler version (if compiling from source):(Ubuntu 5.4.0-6ubuntu1~16.04.11)5.4.0
CUDA/cuDNN version:n/a
GPU model and memory:n/a

Describe the problem
I use tfrecords file to train a stacked autoencoder. I want to train the encoding layer for 1000 steps. I tried to create batches from features and labels and use them to train my network.  However, when I run my code I get an error (RuntimeError: Graph is finalized and cannot be modified.) indicating that the problem comes from the following function:
&lt;denchmark-code&gt; def train_layer(output_layer, layer_loss,optimizer):
 """Train each encoding layer for 1000 steps"""
 layer_name = output_layer.name.split('/')[0]
 print('Pretraining {}'.format(layer_name))
 num_steps = 1000
  step=1
 features, labels=train_input_fn()
 input_l = tf.reshape(features, [-1, FLAGS.image_rows, FLAGS.image_cols, 1])
  while step &lt;= num_steps:
 instance_batch, label_batch = tf.train.shuffle_batch([input_l], batch_size=5, capacity=200, min_after_dequeue=100)
_out_layer, _layer_loss,_ = sess.run([output_layer, layer_loss, optimizer],
 feed_dict ={features:instance_batch,labels:label_batch})
 #print(_layer_loss)
 step += 1
 print('layer finished')`
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='amelroua' date='2019-02-27T12:49:05Z'>
		I have a tfrecords file from which I am looking to create batches of data. I want to train the encoding layer of my model many steps while calling the batches. For this purpose I used the below code to create batches and get the next batch:
&lt;denchmark-code&gt; `def write_and_encode(data_list, tfrecord_filename):
  writer = tf.python_io.TFRecordWriter(tfrecord_filename)
  for label, data_matrix in data_list:
    example = tf.train.Example(features=tf.train.Features(
        feature={
            "label": tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),
            "data_raw": tf.train.Feature(bytes_list=tf.train.BytesList(value=[data_matrix.tostring()]))
        }
    ))
    writer.write(example.SerializeToString())
writer.close()
def read_and_decode(tfrecord_filename):
reader = tf.TFRecordReader()
 filename_queue = tf.train.string_input_producer([tfrecord_filename],)
_, serialized_example = reader.read(filename_queue)
feature = tf.parse_single_example(serialized_example,
                                  features={
                                      "label": tf.FixedLenFeature([], tf.int64),
                                      "data_raw": tf.FixedLenFeature([], tf.string)
                                  })
data = tf.decode_raw(feature["data_raw"], tf.float64)
data = tf.reshape(data, [FLAGS.image_rows, FLAGS.image_cols])
return data, feature["label"]`
def train_input_fn():
tfrecord_file = "../resources/train_tfrecord"  
dataset = tf.data.TFRecordDataset(tfrecord_file)
dataset = dataset.map(parser)
train_dataset = dataset.repeat(FLAGS.num_epochs).batch(FLAGS.batch_size)
train_iterator = train_dataset.make_one_shot_iterator()
features, labels = train_iterator.get_next()
return features, labels

def parser(record_line):

features = {
    "label": tf.FixedLenFeature([], tf.int64),
    "data_raw": tf.FixedLenFeature([], tf.string)
}
parsed = tf.parse_single_example(record_line, features=features)
label = tf.cast(parsed["label"], tf.int32) - 1  
data = tf.decode_raw(parsed["data_raw"], tf.float64)
data = tf.reshape(data, [FLAGS.image_rows, FLAGS.image_cols])
data = tf.cast(data, tf.float32)
return data, label`
&lt;/denchmark-code&gt;

To train the encoding layer of an autoencoder, I perfrom as follows:
&lt;denchmark-code&gt; `def train_layer(output_layer, layer_loss,optimizer):
"""Train each encoding layer for 1000 steps"""
layer_name = output_layer.name.split('/')[0]
print('Pretraining {}'.format(layer_name))
num_steps = 1000
step=1
features, labels=train_input_fn()
input_l = tf.reshape(features, [-1, FLAGS.image_rows, FLAGS.image_cols, 1])
while step &lt;= num_steps:

     instance_batch, label_batch = tf.train.shuffle_batch([input_l], batch_size=5, capacity=200, min_after_dequeue=100)

_out_layer, _layer_loss,_ = sess.run([output_layer, layer_loss, optimizer],
  feed_dict ={features:instance_batch,labels:label_batch})
#print(_layer_loss)
step += 1
print('layer finished')`
&lt;/denchmark-code&gt;

For the configuration of the Monitored Training Session, I implement it as follows:
&lt;denchmark-code&gt; `"""Use a MonitoredTrainingSession for running the computations.  It makes running on distributed    systems possible, handles checkpoints, saving summaries, and restoring from crashes easy."""

   #create hooks to pass to the session.  These can be used for adding additional calculations, loggin, etc.
   #This hook simply tells the session how many steps to run
   hooks=[tf.train.StopAtStepHook(last_step=10000)]
   #This command collects all summary ops that have been added to the graph and prepares them to   run in the next session
  tf.summary.merge_all()
  logs_dir = 'logs'
  with tf.train.MonitoredTrainingSession(hooks=hooks,  checkpoint_dir=logs_dir,save_summaries_steps=100) as sess:

start_time = time.time()

"""First train each layer one at a time, freezing weights from previous layers.
This was accomplished by declaring which variables to update when each layer optimizer was defined."""
    for layer_dict in model_layers:
         output_layer = layer_dict['output_layer']
          layer_loss = layer_dict['layer_loss']
          optimizer = layer_dict['optimizer']
           train_layer( output_layer, layer_loss, optimizer)
  #Now train the whole network for classification allowing all weights to change.
    while not sess.should_stop():
              _y, _cross_entropy, _net_op, _accuracy = sess.run([y, cross_entropy, net_op, accuracy], feed_dict={x:instance_batch,y_labels:label_batch})
   print(_accuracy)
   print('Training complete\n')`
&lt;/denchmark-code&gt;

When I run my code, I get an error:

File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 2919, in _check_not_finalized
raise RuntimeError("Graph is finalized and cannot be modified.")
RuntimeError: Graph is finalized and cannot be modified.

The source of the problem is the Train_layer:

Pretraining layer_0
Traceback (most recent call last):
File "aut.py", line 222, in 
train_layer( output_layer, layer_loss, optimizer)
File "aut.py", line 111, in train_layer
features, labels=train_input_fn()
File "aut.py", line 67, in train_input_fn
dataset = tf.data.TFRecordDataset(tfrecord_file)

		</comment>
		<comment id='2' author='amelroua' date='2019-02-28T00:42:34Z'>
		&lt;denchmark-link:https://github.com/amelroua&gt;@amelroua&lt;/denchmark-link&gt;

Please fill the issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/new?template=00-bug-performance-issue.md&gt;template&lt;/denchmark-link&gt;
. Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. Thanks!
		</comment>
		<comment id='3' author='amelroua' date='2019-03-15T12:54:06Z'>
		It has been 15 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='4' author='amelroua' date='2019-03-15T13:08:57Z'>
		Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!
		</comment>
	</comments>
</bug>