<bug id='23903' author='koenhelwegen' open_date='2018-11-21T16:18:24Z' closed_time='2019-01-10T15:28:39Z'>
	<summary>Poor performance when scaling up data pipeline in multi CPU environments</summary>
	<description>
System information
AWS Deep Learning AMI (ubuntu 16.04, tensorflow 12) on AWS p3.8 and p3.16 instances.
Describe the current behavior
I am trying to optimize the speed of my code when training on Imagenet and started by looking at the data pipeline. I am using TFRecordDataset(), and use both prefetch() and num_parallel_calls for mappings. Here I just load in the batches without performing any action on them.
I seem to get reasonably good results on AWS p3.8 (32 vCPU, 4 Nvidia Tesla V100 GPUs), although it's still a bit below the ~3000 images/second that I've seen mentioned elsewhere. However, if I run exactly the same code on AWS p3.16 (64 vCPU, 8 GPUs), throughput decreases significantly, even while CPU use is higher.
The two systems perform as expected on other tasks.  I did some very elementary benchmarking of both systems:

calculating the number of primes, spawning a number of threads using multiprocessing.Process(). As expected, performance between p3.8 and p3.16 appears to be identical when n_threads &lt; 32, and 3.16 performs better when n_threads = 128.
reading a file with 1,000,000 lines (and repeat 100 times): identical performance

This suggests to me the problem is with the TFRecordDataset, but if there is any other benchmarking that makes sense, please let me know.
Both systems allow two threads for each CPU. I launched another p3.16 instance with a single thread per CPU, so that the number of virtual CPUs is identical to the p3.8 instance. This did not appear to make any difference.
Describe the expected behavior
Performance should improve, or at least remain the same, on p3.16 as compared to p3.8, as there are more CPUs available.
Code to reproduce the issue
Taking only the data pipeline part and skipping all shuffling, preprocessing, etcetera I am left with this as a minimal working example.
(Note that I use multiple prefetch statements; this significantly improves performance on both systems for me, but removing them does not change their relative performance. Similarly, using 128 parallel calls in the mapping function gives better results than 32 or 64, but changing that or making it dependent on the actual number of vCPUs doesn't solve the observed behaviour.)
&lt;denchmark-code&gt;import tensorflow as tf
import os
import time

DATA_FOLDER = '/home/ubuntu/datasets/imagenet-data'
BATCH_SIZE = 256
N_TRIALS = 10
STEPS_PER_TRIAL = 100

def decode_imagenet(serialized_example):
    feature_map = {
        'image/encoded': tf.FixedLenFeature([], dtype=tf.string, default_value=''),
        'image/class/label': tf.FixedLenFeature([1], dtype=tf.int64, default_value=-1)
    }
    features = tf.parse_single_example(serialized_example, feature_map)

    image = features['image/encoded']
    image = tf.image.decode_jpeg(image, channels=3)
    image = tf.image.convert_image_dtype(image, tf.float32)
    image = tf.image.resize_images(image, [224, 224])

    label = tf.squeeze(tf.cast(features['image/class/label'], tf.int32))
    
    return image, label

training_files = [os.path.join(DATA_FOLDER, fn) for fn in os.listdir(DATA_FOLDER) if 'train' in fn]

dataset = tf.data.TFRecordDataset(training_files)
dataset = dataset.repeat()
dataset = dataset.prefetch(BATCH_SIZE*16)
dataset = dataset.map(decode_imagenet, num_parallel_calls=128)
dataset = dataset.prefetch(BATCH_SIZE*4)
dataset = dataset.batch(BATCH_SIZE)
dataset = dataset.prefetch(1)


iterator = dataset.make_initializable_iterator()

batch_x, batch_y = iterator.get_next()

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    sess.run(iterator.initializer)

    res = []
    for trial in range(N_TRIALS):
        t0 = time.time()
        for _ in range(STEPS_PER_TRIAL):
            sess.run([batch_x, batch_y])
        t1 = time.time()
        images_per_second = BATCH_SIZE*STEPS_PER_TRIAL/(t1-t0)
        res.append(images_per_second)
        print('Trial %i: %f ips' % (trial, images_per_second))
    print('Overall average: %f ips' % (sum(res)/N_TRIALS))
&lt;/denchmark-code&gt;

Other info / logs
Output of above code on p3.8:
&lt;denchmark-code&gt;Trial 0: 2687.172262 ips
Trial 1: 2861.731855 ips
Trial 2: 2832.192681 ips
Trial 3: 2826.211587 ips
Trial 4: 2851.585431 ips
Trial 5: 2809.069081 ips
Trial 6: 2869.325024 ips
Trial 7: 2858.402548 ips
Trial 8: 2833.678433 ips
Trial 9: 2859.654147 ips
Overall average: 2828.902305 ips
&lt;/denchmark-code&gt;

Meanwhile, %CPU (observed using the top command) is 2300-2400.
On p3.16:
&lt;denchmark-code&gt;Trial 0: 1585.289254 ips
Trial 1: 2077.237178 ips
Trial 2: 2165.624391 ips
Trial 3: 2183.211334 ips
Trial 4: 2226.638648 ips
Trial 5: 2217.900892 ips
Trial 6: 2217.662143 ips
Trial 7: 2232.781854 ips
Trial 8: 2178.928816 ips
Trial 9: 2222.856626 ips
Overall average: 2130.813114 ips
&lt;/denchmark-code&gt;

%CPU is between 3700-4000.
	</description>
	<comments>
		<comment id='1' author='koenhelwegen' date='2019-01-08T18:55:19Z'>
		&lt;denchmark-link:https://github.com/koenhelwegen&gt;@koenhelwegen&lt;/denchmark-link&gt;

Based on the information available on &lt;denchmark-link:https://aws.amazon.com/ec2/instance-types/p3/&gt;AWS website&lt;/denchmark-link&gt;
, the machines use Intel Skylake architecture which generally has &lt;denchmark-link:https://software.intel.com/en-us/articles/intel-xeon-processor-scalable-family-technical-overview&gt;multiple sockets&lt;/denchmark-link&gt;
. What likely happens when you go from 32vCPUs to 64vCPUs, you end up using more sockets, copying large amounts of data between them. This would explain the reduced throughput and increased CPU usage (in fact, I have seen similar issue on Google Cloud).
Here are my suggestions:


Compare NUMA setting between p3.8 and p3.16 using numactl --hardware


Simplify your pipeline to:


&lt;denchmark-code&gt;  dataset = tf.data.TFRecordDataset(training_files)
  dataset = dataset.map(decode_imagenet, num_parallel_calls=tf.data.experimental.AUTOTUNE)
  dataset = dataset.batch(BATCH_SIZE)
  dataset = dataset.prefetch(8)
&lt;/denchmark-code&gt;

I expect this to boost performance for both p3.8 and p3.16 and reduce the cross-NUMA traffic for p3.16. This is because the tf.data runtime will detect the  +  sequence and apply a more efficient fused implementation of the two (discussed &lt;denchmark-link:https://www.tensorflow.org/guide/performance/datasets&gt;here&lt;/denchmark-link&gt;
).

Consider pinning your program to a NUMA socket, discussed here.

		</comment>
		<comment id='2' author='koenhelwegen' date='2019-01-10T15:28:39Z'>
		Thanks for taking this up!
When I run numactl --hardware I get:
&lt;denchmark-code&gt;# p3.8
available: 1 nodes (0)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31
node 0 size: 245854 MB
node 0 free: 215328 MB
node distances:
node   0 
  0:  10

# p3.16
available: 2 nodes (0-1)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
node 0 size: 245864 MB
node 0 free: 243849 MB
node 1 cpus: 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63
node 1 size: 245941 MB
node 1 free: 245151 MB
node distances:
node   0   1 
  0:  10  20 
  1:  20  10  
&lt;/denchmark-code&gt;

So we indeed move from 1 to 2 nodes, which explains the observed behaviour. On p3.8, if I switch to the nightly build (was using tf-1.12) and use the pipeline you suggest performance improves to about 3200 ips; if I fix the num_parallel_calls to anything between 64 and 512 the performance increases further to ~3500 ips. On p3.16 performance is ~4200 ips; here too the AUTOTUNE option slightly underperforms as compared to fixing some value manually.
The main difference is in the nightly build vs tf 1.12: running the same code with tensorflow==1.12 (not tensorflow-gpu) gon p3.16 gives ~2400 ips.
Behaviour is now as expected, thanks a lot! Now I just need to get the nightly build working with gpu support on AWS ;)
		</comment>
	</comments>
</bug>