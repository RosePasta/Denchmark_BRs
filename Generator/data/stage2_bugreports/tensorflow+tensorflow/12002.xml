<bug id='12002' author='ghost(ghost)' open_date='2017-08-03T14:17:14Z' closed_time='2018-05-17T01:00:14Z'>
	<summary>tf.nn.sparse_softmax_cross_entropy_with_logits() seems to return bad values !</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): from pip
TensorFlow version (use command below): 'v1.2.0-5-g435cdfc', '1.2.1'
Python version: Python 2.7.12
Bazel version (if compiling from source):
CUDA/cuDNN version: cudNN v8.0
GPU model and memory: 2* NVidia GeForce 1080Ti (11Go each)
Exact command to reproduce: Following code

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

It seems that  and  are returning bad values. According to this &lt;denchmark-link:https://stackoverflow.com/questions/36078411/tensorflow-are-my-logits-in-the-right-format-for-cross-entropy-function/36086477#36086477&gt;stackOverflow post&lt;/denchmark-link&gt;
,  is almost equivalent to .
But when I'm using the provided optimized function, I don't get the same results. It appears that its come from the log function when logits is equal to 0. But I've read that  handle that case, and that's the case, cause I would have some  output. But instead I have huge numbers, so I (naturally) thought that to avoid  a small constant must have been added to the problematic numbers. So I tried to reproduce this tip (with 1e-10) and I don't still have the same result. So I tried to read the code &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/ops/nn_ops.py&gt;here&lt;/denchmark-link&gt;
 to understand what's going on. But I can't find in the repo the gen_nn_ops module to understand why this function returns such a "strange" result. I will be pleased to contribute to understand what happens (and to correct it, if needed of course !).
Here is a really simple piece of code to reproduce the "error" (if it's indeed one). And we have the same behavior with the sparse version of the function (with proper labels).
Thanks,
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;graph = tf.Graph()

features = np.array([[6.83324017e-02, 4.55211316e-01,-1.41892820e-01, 6.41751984e-01, -5.45895865e-01, 5.38657679e-01, 1.93379897e-01, 1.60154529e-01, 1.57859872e-02, 1.36758294e-02, 4.40859703e+00, 4.96067050e+03, -5.95230431e+01, 2.29624126e+00, 4.02069655e+00], [8.82284599e-01, 6.42900440e-01,-4.27639642e-02, 1.83567706e-01, 7.52404702e-01, -6.32605771e-01, 5.40391531e-01, 5.84584613e-01,-7.15044264e-03,-8.23328268e-02, 6.29273115e+00,-4.32369561e+01, 7.07259958e+00,-1.02810233e+00,-7.04034886e-01], [5.48660773e-01, 8.08794529e-01,-5.96924524e-02,-7.26052964e-01,-2.70772000e-02, 6.87105464e-01, 5.68913359e-01, 4.76252594e-01, 4.14203699e-02,-5.79935485e-03, 9.40232256e+00,-2.01665599e+04, 1.34500232e+01,-2.24989629e-01, 2.52753983e-01], [7.46613308e-01, 8.23272733e-01,-1.04753678e-01, 7.87653516e-01, 5.33736860e-01, 3.07777360e-01, 8.51814816e-01, 7.29870149e-01,-5.67521706e-03, 2.37203887e-02, 6.33280960e+00, 4.08845288e+05, 4.48007235e+01, 5.33139458e-02, 2.37384134e-02], [4.47498908e-01, 1.49080014e-01,-9.07106172e-03,-2.67174181e-01,-5.21700457e-01, 8.10213916e-01, 9.18038857e-01, 8.36740457e-01,-7.64173908e-03,-1.18870530e-02, 6.18394833e+00, 7.37307204e+01,-5.58432681e+01, 3.83996968e-01, 9.18497562e-01], [4.71607629e-01, 1.31179570e-01,-4.56846546e-02,-9.27597302e-01,-3.63639607e-01,-8.56123912e-02, 3.32925650e-01, 2.86999292e-01,-1.37396795e-01,-2.39745171e-01, 6.28318531e+00,-9.03421275e+04,-9.83543039e+03,-1.09839821e+00, 1.05041514e+00], [4.71613040e-01, 1.31166299e-01,-4.56797268e-02,-9.27775404e-01,-3.64117510e-01,-8.15551274e-02, 3.32854008e-01, 2.86979856e-01,-1.36950051e-01,-2.39623484e-01, 6.28318531e+00,-2.85787226e+05, 1.02588457e+05,-1.09795489e+00, 1.05020120e+00], [1.72510574e-01, 3.40244123e-02,-1.78258372e-01,-1.78623912e-01, 9.82406854e-01,-5.45001987e-02, 6.49133952e-01, 4.58514334e-01,-1.05587941e-01,-1.50382361e-01, 6.56445597e+00,-7.39915259e+01,-3.39043636e+01, 8.32312454e-01, 1.66266815e+00]])

labels = np.array([[ 1., 0.], [ 0., 1.], [ 0., 1.], [ 1., 0.], [ 0., 1.], [ 0., 1.], [ 0., 1.], [ 1., 0.]])

totalLoss = 0
totalTest = 0

with graph.as_default():
    x = tf.placeholder("float", [None, 15], name = "x")
    y = tf.placeholder("int64", [None, 2], name = "y")

    h1 = tf.Variable(tf.truncated_normal([15, 100], stddev = 0.1), name = "h1") 
    out = tf.Variable(tf.truncated_normal([100, 2], stddev = 0.1), name = "out")
    b1 =  tf.Variable(tf.truncated_normal([100], stddev = 0.1), name = "b1")
    bout = tf.Variable(tf.truncated_normal([2], stddev = 0.1), name = "bout")


    def model(x):
        layer_1 = tf.add(tf.matmul(x, h1), b1)
        layer_1 = tf.nn.relu(layer_1)

        out_layer = tf.matmul(layer_1, out) + bout
        return out_layer

    logits = model(x)
    
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y))
    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)

    with tf.Session(graph = graph) as session:
        for i in xrange(10):
            tf.global_variables_initializer().run()
            
            _ = session.run(optimizer, feed_dict = {x : features, y : labels})
            l = session.run(loss, feed_dict = {x : features, y : labels})
            test = session.run(tf.reduce_mean(-tf.reduce_sum(labels * tf.log(tf.nn.softmax(logits) + 1e-10), 1)), feed_dict = {x : features})
            totalLoss += l 
            totalTest += test

print("mathematical : ", totalTest *1. /10)
print("sparse_softmax_cross_entropy : ", totalLoss *1. /10)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='ghost(ghost)' date='2017-08-03T18:41:55Z'>
		tf.log(tf.nn.softmax(logits) + 1e-10) is known to be unstable.
If I replace it by tf.nn.log_softmax(logits) I can see consistent results.
		</comment>
		<comment id='2' author='ghost(ghost)' date='2017-08-04T07:24:22Z'>
		Could you please put some of the consistent results you have please ? Because I don't have the same results at all ... Here are the results when I replace tf.log(tf.nn.softmax(logits) + 1e-10) by tf.nn.log_softmax(logits) on 3 different runs :
&lt;denchmark-h:h4&gt;run 1&lt;/denchmark-h&gt;

mathematical :  9671.87484341
sparse_softmax_cross_entropy :  1208.98436916
&lt;denchmark-h:h4&gt;run 2&lt;/denchmark-h&gt;

mathematical :  30372.4364977
sparse_softmax_cross_entropy :  3796.55453706
&lt;denchmark-h:h4&gt;run 3&lt;/denchmark-h&gt;

mathematical :  9671.87484341
sparse_softmax_cross_entropy :  1208.98436916
Any Idea why ? A numerical stability issue ?
		</comment>
		<comment id='3' author='ghost(ghost)' date='2017-08-04T10:39:04Z'>
		&lt;denchmark-code&gt;mathematical :  1244.12398391
sparse_softmax_cross_entropy :  1244.12398391

mathematical :  2525.72263083
sparse_softmax_cross_entropy :  2525.72263083

mathematical :  4146.75835724
sparse_softmax_cross_entropy :  4146.75835724

mathematical :  1013.51154659
sparse_softmax_cross_entropy :  1013.51154659

mathematical :  2610.52162684
sparse_softmax_cross_entropy :  2610.52162684
&lt;/denchmark-code&gt;

Using TF 1.3.0rc1, btw.
		</comment>
		<comment id='4' author='ghost(ghost)' date='2017-08-04T21:51:11Z'>
		So here is the source of the difference between your and my results ! So it seems that between our two versions of TF, the way of coding these 2 functions have changed !
Wouldn't it nice to warn v 1.2 users that a huge difference exists ? (And maybe an error so ...)
		</comment>
		<comment id='5' author='ghost(ghost)' date='2017-08-04T22:07:27Z'>
		I didn't mean that TF version is the problem. It was just an FYI.
In fact the code (with log_softmax) still produces correct result here with TF 1.2.0rc0.
		</comment>
		<comment id='6' author='ghost(ghost)' date='2017-08-04T22:10:46Z'>
		This message was created automatically by mail delivery software.

A message that you sent could not be delivered to one or more of its
recipients. This is a temporary error. The following address(es) deferred:

  mazecreator@gmail.com
    Domain mazecreator.com has exceeded the max emails per hour (25/25 (100%)) allowed.  Message will be reattempted later
&lt;denchmark-link:#&gt;â€¦&lt;/denchmark-link&gt;


------- This is a copy of the message, including all the headers. ------
Received: from github-smtp2-ext6.iad.github.net ([192.30.252.197]:52170 helo=github-smtp2b-ext-cp1-prd.iad.github.net)
	by server2.lowesthostingrates.com with esmtps (TLSv1.2:ECDHE-RSA-AES256-GCM-SHA384:256)
	(Exim 4.89)
	(envelope-from &lt;noreply@github.com&gt;)
	id 1ddkcH-0000Hn-Ds
	for mazecreator@mazecreator.com; Fri, 04 Aug 2017 16:59:15 -0500
Date: Fri, 04 Aug 2017 15:09:31 -0700
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=github.com;
	s=pf2014; t=1501884571;
	bh=SypO/JhO5lnky6YYSSkgVxcutkL9roA2yInEbZlWXqQ=;
	h=From:Reply-To:To:Cc:In-Reply-To:References:Subject:List-ID:
	 List-Archive:List-Post:List-Unsubscribe:From;
	b=U4v3RW4uRTyff9y8OCHZPTyEY0RiSmfwM9UD1sPt5tHGxf9L2GMQBHcVaMRPp5ikD
	 QbDUx1htRZdj4hW06ileJ4Pq0SkxwJQ1wq12EjHe4tLIdZovi1/VYEFo+/rgBQ52Vg
	 c5E1DOhnVWqiCW8w9zZMQ4rfCmbxwdgVR3xhLEFE=
From: Yuxin Wu &lt;notifications@github.com&gt;
Reply-To: tensorflow/tensorflow &lt;reply@reply.github.com&gt;
To: tensorflow/tensorflow &lt;tensorflow@noreply.github.com&gt;
Cc: Subscribed &lt;subscribed@noreply.github.com&gt;
Message-ID: &lt;tensorflow/tensorflow/issues/12002/320363688@github.com&gt;
In-Reply-To: &lt;tensorflow/tensorflow/issues/12002@github.com&gt;
References: &lt;tensorflow/tensorflow/issues/12002@github.com&gt;
Subject: Re: [tensorflow/tensorflow]
 tf.nn.sparse_softmax_cross_entropy_with_logits() seems to return bad values !
 (#12002)
Mime-Version: 1.0
Content-Type: multipart/alternative;
 boundary="--==_mimepart_5984f09ba80f_562f3fa08329dc3c3211aa";
 charset=UTF-8
Content-Transfer-Encoding: 7bit
Precedence: list
X-GitHub-Sender: ppwwyyxx
X-GitHub-Recipient: Mazecreator
X-GitHub-Reason: subscribed
List-ID: tensorflow/tensorflow &lt;tensorflow.tensorflow.github.com&gt;
List-Archive: https://github.com/tensorflow/tensorflow
List-Post: &lt;mailto:reply@reply.github.com&gt;
List-Unsubscribe: &lt;mailto:unsub+0118f3a070872b7681c27cc414163f618de8744dfc4b54d692cf00000001159cb29b92a169ce0ec3ee17@reply.github.com&gt;,
 &lt;https://github.com/notifications/unsubscribe/ARjzoAa4s9Afp3Xz0VcL31zSaAYJ7AQ-ks5sU5abgaJpZM4OsgT5&gt;
X-Auto-Response-Suppress: All
X-GitHub-Recipient-Address: mazecreator@mazecreator.com
X-Spam-Status: No, score=
X-Spam-Score:
X-Spam-Bar:
X-Ham-Report:
X-Spam-Flag: NO
----==_mimepart_5984f09ba80f_562f3fa08329dc3c3211aa
Content-Type: text/plain;
 charset=UTF-8
Content-Transfer-Encoding: 7bit

I didn't mean that TF version is the problem. It was just an FYI.
In fact the code (with log_softmax) still produces correct result here with TF 1.2.0rc0.
-- 
You are receiving this because you are subscribed to this thread.
Reply to this email directly or view it on GitHub:
#12002 (comment)
----==_mimepart_5984f09ba80f_562f3fa08329dc3c3211aa
Content-Type: text/html;
 charset=UTF-8
Content-Transfer-Encoding: 7bit

&lt;p&gt;I didn't mean that TF version is the problem. It was just an FYI.&lt;br&gt;
In fact the code (with log_softmax) still produces correct result here with TF 1.2.0rc0.&lt;/p&gt;

&lt;p style="font-size:small;-webkit-text-size-adjust:none;color:#666;"&gt;&amp;mdash;&lt;br /&gt;You are receiving this because you are subscribed to this thread.&lt;br /&gt;Reply to this email directly, &lt;a href="#12002 (comment)"&gt;view it on GitHub&lt;/a&gt;, or &lt;a href="https://github.com/notifications/unsubscribe-auth/ARjzoOv9XkmzvloD_t0StSlpiMJpwtLMks5sU5abgaJpZM4OsgT5"&gt;mute the thread&lt;/a&gt;.&lt;img alt="" height="1" src="https://github.com/notifications/beacon/ARjzoCKP9tmNdQ-sgniF4uhFHvkjvyjXks5sU5abgaJpZM4OsgT5.gif" width="1" /&gt;&lt;/p&gt;
&lt;div itemscope itemtype="http://schema.org/EmailMessage"&gt;
&lt;div itemprop="action" itemscope itemtype="http://schema.org/ViewAction"&gt;
  &lt;link itemprop="url" href="#12002 (comment)"&gt;&lt;/link&gt;
  &lt;meta itemprop="name" content="View Issue"&gt;&lt;/meta&gt;
&lt;/div&gt;
&lt;meta itemprop="description" content="View this Issue on GitHub"&gt;&lt;/meta&gt;
&lt;/div&gt;

&lt;script type="application/json" data-scope="inboxmarkup"&gt;{"api_version":"1.0","publisher":{"api_key":"05dde50f1d1a384dd78767c55493e4bb","name":"GitHub"},"entity":{"external_key":"github/tensorflow/tensorflow","title":"tensorflow/tensorflow","subtitle":"GitHub repository","main_image_url":"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png","avatar_image_url":"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png","action":{"name":"Open in GitHub","url":"https://github.com/tensorflow/tensorflow"}},"updates":{"snippets":[{"icon":"PERSON","message":"@ppwwyyxx in #12002: I didn't mean that TF version is the problem. It was just an FYI.\r\nIn fact the code (with log_softmax) still produces correct result here with TF 1.2.0rc0."}],"action":{"name":"View Issue","url":"#12002 (comment)"}}}&lt;/script&gt;
----==_mimepart_5984f09ba80f_562f3fa08329dc3c3211aa--

		</comment>
		<comment id='7' author='ghost(ghost)' date='2017-08-05T05:27:28Z'>
		I reproduced the same code on 4 different computers (2 MAC, 2 Ubuntu) with GPU compatibility and CPU based on the same tensorflow version  'v1.2.0-5-g435cdfc', '1.2.1' and I still have this big difference ... I would just like to understand what's going on. With exactly the same code I don't have the same output's behavior ...
		</comment>
		<comment id='8' author='ghost(ghost)' date='2017-08-07T11:48:22Z'>
		Hi, I've just installed tensorflow with the following command : sudo pip install tensorflow. I ran some example codes to get how it works. Once I understood how it works I tried on my own dataset and it turns out that computing hand made cross entropy (with log_softmax()) does not match with the output of softmax_cross_entropy_with_logits(). My dataset is strictly different than the one proposed in the code. When I run the source code of this issue, I have the same kind of results than bdenoun's. Results don't match.
Thanks for your help !
		</comment>
		<comment id='9' author='ghost(ghost)' date='2017-08-08T06:41:41Z'>
		Hi, after cmontassier's comment, I reinstalled tf from pip and I still have the same issue. And same thing for the other 4 computers... so I really think it comes from this particular version !
		</comment>
		<comment id='10' author='ghost(ghost)' date='2017-08-28T03:30:55Z'>
		Thank you &lt;denchmark-link:https://github.com/bdenoun&gt;@bdenoun&lt;/denchmark-link&gt;
, for reaching out!
&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
, are you able to take a look?
		</comment>
		<comment id='11' author='ghost(ghost)' date='2017-12-20T01:17:45Z'>
		It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='12' author='ghost(ghost)' date='2018-01-03T19:11:48Z'>
		It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='13' author='ghost(ghost)' date='2018-01-18T19:15:09Z'>
		Nagging Awaiting TensorFlower: It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='14' author='ghost(ghost)' date='2018-02-06T07:44:54Z'>
		Nagging Awaiting TensorFlower: It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='15' author='ghost(ghost)' date='2018-02-20T19:41:41Z'>
		Nagging Awaiting TensorFlower: It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='16' author='ghost(ghost)' date='2018-02-20T19:59:49Z'>
		sparse_softmax_cross_entropy_with_logits &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/754048a0453a04a761e112ae5d99c149eb9910dd/tensorflow/core/kernels/sparse_xent_op.h#L201&gt;subtracts the max of the logits&lt;/denchmark-link&gt;
 before calculating the cross entropy, for numerical stability.  could that be the reason you're getting different results?
		</comment>
		<comment id='17' author='ghost(ghost)' date='2018-03-07T13:19:27Z'>
		Nagging Awaiting Response: It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='18' author='ghost(ghost)' date='2018-03-25T00:58:20Z'>
		It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='19' author='ghost(ghost)' date='2018-04-17T12:45:34Z'>
		It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='20' author='ghost(ghost)' date='2018-05-02T18:46:33Z'>
		It has been 29 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='21' author='ghost(ghost)' date='2018-05-17T01:00:14Z'>
		Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
	</comments>
</bug>