<bug id='32500' author='LuisSaybe' open_date='2019-09-13T15:45:47Z' closed_time='2019-11-22T16:55:44Z'>
	<summary>Memory continues to grow after repeated calls to model.predict(tf.one_hot(states, dtype='float32', depth=3))</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave 10.14.6
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): tensorflow==2.0.0-rc1
TensorFlow version (use command below): v2.0.0-rc0-101-gd2d2566 2.0.0-rc1
Python version: 3.7.4
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory: Intel Iris Plus Graphics 640 1536 MB

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
Running the below code in Docker (version 19.03.2) causes the memory to grow without limit. This is visible in docker stats, eventually crashing docker.
Describe the expected behavior
The memory should not grow indefinitely
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
import tensorflow as tf

rows = 6
columns = 7

model = tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=[rows * columns, 3]),
  tf.keras.layers.Dense(7, input_shape=[rows * columns * 3]),
])

model.compile(
  optimizer=tf.keras.optimizers.SGD(lr=0.01),
  loss='mean_squared_error',
  metrics=['accuracy']
)

states = [ [ 1 ] * rows * columns for i in range(20) ]

for iteration in range(1000000):
    print('iteration', iteration)
    model.predict(tf.one_hot(states, dtype='float32', depth=3))
The aforementioned code runs in an image generated by the following Dockerfile
FROM centos:7

ENV SOURCE_DIRECTORY /tmp/tf-connect4

ENV PYTHON_VERSION 3.7.4

RUN yum -y groupinstall -y "Development Tools" &amp;&amp; \
    yum -y update &amp;&amp; \
    yum -y install openssl-devel zlib-devel libffi libffi-devel wget &amp;&amp; \
    wget https://www.python.org/ftp/python/$PYTHON_VERSION/Python-$PYTHON_VERSION.tar.xz &amp;&amp; \
    tar -xJf Python-$PYTHON_VERSION.tar.xz &amp;&amp; \
    cd Python-$PYTHON_VERSION &amp;&amp; \
    ./configure &amp;&amp; \
    make &amp;&amp; \
    make install &amp;&amp; \
    pip3 install --upgrade pip &amp;&amp; \
    pip3 install tensorflow==2.0.0-rc1 tensorflow_probability==0.8.0-rc0 numpy falcon jsonschema
	</description>
	<comments>
		<comment id='1' author='LuisSaybe' date='2019-09-16T06:10:17Z'>
		I replicated the issue on colab with Tf 2.0.0.rc1. Please take a look at colab &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/cc35a1c57a228661157480b1ba307116/untitled150.ipynb&gt;gist&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='LuisSaybe' date='2019-09-21T03:59:33Z'>
		I can reproduce this issue with the same setting. If I add keras.backend.clear_session() after each call to model.predict(), it is getting super slow.
		</comment>
		<comment id='3' author='LuisSaybe' date='2019-10-01T10:55:37Z'>
		still a bug after upgrading to tensorflow==2.0.0
		</comment>
		<comment id='4' author='LuisSaybe' date='2019-11-08T09:54:45Z'>
		&lt;denchmark-link:https://github.com/LuisSaybe&gt;@LuisSaybe&lt;/denchmark-link&gt;
 Thank you for the repro case- I added some memory tracking to it and found a pretty clear linear increase in memory utilization of the model object you've defined. It does seem to be fixed in TF 2.1.0, so I guess I'll be waiting until that's released to move off of 1.14.
&lt;denchmark-link:https://user-images.githubusercontent.com/16674595/68467202-65773c00-01db-11ea-907d-d15d133654b7.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/16674595/68467208-6a3bf000-01db-11ea-97e5-c9cc00d5eb61.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='LuisSaybe' date='2019-11-08T16:36:10Z'>
		This issue has been fixed recently. &lt;denchmark-link:https://github.com/LuisSaybe&gt;@LuisSaybe&lt;/denchmark-link&gt;
 Could you try tf-nightly to verify it? Thanks!
		</comment>
		<comment id='6' author='LuisSaybe' date='2019-11-22T01:22:36Z'>
		&lt;denchmark-link:https://github.com/LuisSaybe&gt;@LuisSaybe&lt;/denchmark-link&gt;
 Is this still an issue. I ran it in colab for 25000 iterations without any issue. Thanks!
		</comment>
		<comment id='7' author='LuisSaybe' date='2019-11-22T12:47:41Z'>
		will check today, sorry was on vacation
		</comment>
		<comment id='8' author='LuisSaybe' date='2019-11-22T16:55:44Z'>
		&lt;denchmark-link:https://github.com/yhliang2018&gt;@yhliang2018&lt;/denchmark-link&gt;
 yes it is fixed now in tf-nightly, thanks
		</comment>
		<comment id='9' author='LuisSaybe' date='2019-11-22T16:55:46Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32500&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32500&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>