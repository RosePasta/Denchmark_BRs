<bug id='10765' author='chenghuige' open_date='2017-06-16T13:52:17Z' closed_time='2018-09-05T01:51:53Z'>
	<summary>[Performance] contirb.seq2seq.attention_wrapper slower due to using matmul instead of reduce_sum</summary>
	<description>
tf version '1.2.0-rc0' contirb.seq2seq.attention_wrapper is great, it make using attention much easier.
However I found using attention_wrapper will be much slower then tf version 1.0.
After some experiment I found it is due to using matmul instead of reduce_sum.
from attetntion_wrapper.py 731
&lt;denchmark-code&gt;  expanded_alignments = array_ops.expand_dims(alignments, 1)
  attention_mechanism_values = self._attention_mechanism.values
  context = math_ops.matmul(expanded_alignments, attention_mechanism_values)
  context = array_ops.squeeze(context, [1])
&lt;/denchmark-code&gt;

Using above code for one of my application got 2.2 batch/s, after changing to use reduce_sum(as tf version 1.0 did), the speed is 3.4 batch/s, improve a lot.
&lt;denchmark-code&gt;  expanded_alignments = array_ops.expand_dims(alignments, 2)
  attention_mechanism_values = self._attention_mechanism.values
  context = math_ops.reduce_sum(expanded_alignments * attention_mechanism_values, [1])
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='chenghuige' date='2018-08-24T21:39:40Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
: It has been 433 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='2' author='chenghuige' date='2018-09-05T01:51:53Z'>
		Closing this as prefer to use eager mode seq2seq.
		</comment>
	</comments>
</bug>