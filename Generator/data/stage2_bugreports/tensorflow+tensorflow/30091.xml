<bug id='30091' author='MyRespect' open_date='2019-06-24T13:46:59Z' closed_time='2019-08-12T04:22:20Z'>
	<summary>Cannot reuse the self.Dense() defined in __init__(self) in call(self, x)</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.0.0-beta1
Python version:  Python 3.5.2
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
When I define the following:
&lt;denchmark-code&gt;class MNISTModel(Model):
    def __init__(self):
        super(MNISTModel, self).__init__()
        self.conv1 = Conv2D(32, (3, 3), input_shape=(28, 28, 1))
        self.activ = Activation('relu')
        self.flatt = Flatten()
        self.dense = Dense(200) #Here I want to reuse this Dense() layer
        self.dense1 = Dense(200) #However, I need to re-define the same layer again to use in call()
        self.dens2 = Dense(10)
&lt;/denchmark-code&gt;

Describe the expected behavior
I think that we only need to define the self.Dense(200) only once, but we can reuse it in the  def call(self, x)
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.data import Dataset
from tensorflow.keras import datasets, Model, losses, optimizers, metrics
from tensorflow.keras.layers import Dense, Flatten, Conv2D, Activation, MaxPooling2D


class MNISTModel(Model):
    def __init__(self):
        super(MNISTModel, self).__init__()
        self.conv1 = Conv2D(32, (3, 3), input_shape=(28, 28, 1))
        self.activ = Activation('relu')
        self.conv2 = Conv2D(32, (3, 3))
        self.maxpo = MaxPooling2D(pool_size=(2, 2))
        self.conv3 = Conv2D(64, (3, 3))
        self.flatt = Flatten()
        self.dense = Dense(200)
        self.dense1 = Dense(200)
        self.dens2 = Dense(10)

    def call(self, x):
        x = self.conv1(x)
        x = self.activ(x)
        x = self.conv2(x)
        x = self.activ(x)
        x = self.maxpo(x)

        x = self.conv3(x)
        x = self.activ(x)
        x = self.conv3(x)
        x = self.activ(x)
        x = self.maxpo(x)

        x = self.flatt(x)
        x = self.dense(x)
        x = self.activ(x)
        x = self.dense1(x)
        x = self.activ(x)
        return self.dens2(x)


(train_data, train_labels), (test_data, test_labels) = datasets.mnist.load_data()
train_data, test_data = train_data / 255.0, test_data / 255.0
train_data = train_data[..., tf.newaxis]
test_data = test_data[..., tf.newaxis]
print(train_data.shape, test_data.shape, type(train_data))
train_data = Dataset.from_tensor_slices(
    (train_data, train_labels)).shuffle(60000).batch(128)
test_data = Dataset.from_tensor_slices((test_data, test_labels)).batch(128)


model = MNISTModel()
loss_object = losses.SparseCategoricalCrossentropy()
optimizer = optimizers.Adam()

train_loss = metrics.Mean(name='train_loss')
train_accuracy = metrics.SparseCategoricalAccuracy(name='train_accuracy')
test_loss = metrics.Mean(name='test_loss')
test_accuracy = metrics.SparseCategoricalAccuracy(name='test_accuracy')


@tf.function
def train_step(images, labels):
    with tf.GradientTape() as tape:
        logits = model(images)
        loss_value = loss_object(labels, logits)
    grads = tape.gradient(loss_value, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
    train_loss(loss_value)
    train_accuracy(labels, logits)


@tf.function
def test_step(images, labels):
    logits = model(images)
    tloss_value = loss_object(labels, logits)
    test_loss(tloss_value)
    test_accuracy(labels, logits)


EPOCHS = 5

for epoch in range(EPOCHS):
    for images, labels in train_data:
        train_step(images, labels)

    for test_images, test_labels in test_data:
        test_step(test_images, test_labels)

    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'
    print (template.format(epoch + 1, train_loss.result(), train_accuracy.result()
                           * 100, test_loss.result(), test_accuracy.result() * 100))
&lt;/denchmark-code&gt;

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='MyRespect' date='2019-06-25T05:13:51Z'>
		&lt;denchmark-link:https://github.com/MyRespect&gt;@MyRespect&lt;/denchmark-link&gt;
 After executing the above code on Colab with Tensorflow 2.0.0.beta1 I got the following error
.
		</comment>
		<comment id='2' author='MyRespect' date='2019-06-25T05:32:06Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;

Yes, you reproduced my bug. This is because I use

in the (), but if you define

in the (), and use it in the ():

you can avoid the problem. So here I think it is a bug to be fixed.
		</comment>
		<comment id='3' author='MyRespect' date='2019-08-12T04:22:20Z'>
		&lt;denchmark-link:https://github.com/MyRespect&gt;@MyRespect&lt;/denchmark-link&gt;
 Thanks for the bug! You can reuse layers, you just have to make sure that the shapes of your second inputs are compatible with the shapes of your first inputs to the layer, in this case it looks like that is not true for your model
		</comment>
		<comment id='4' author='MyRespect' date='2019-08-12T04:22:21Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30091&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30091&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>