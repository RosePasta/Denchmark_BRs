<bug id='41308' author='PINTO0309' open_date='2020-07-11T15:48:26Z' closed_time='2020-08-29T12:58:26Z'>
	<summary>[TFLite] Two consecutive Dequantize OPs are generated in the structure of the model after integer quantization</summary>
	<description>
&lt;denchmark-h:h2&gt;1. System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 x86_64
TensorFlow installed from (source or binary): binary
TensorFlow version (or github SHA if from source): tensorflow==2.3.0-rc1

&lt;denchmark-h:h2&gt;2. Command used to run the converter, and code using the Python API&lt;/denchmark-h&gt;

Here's a minimal Google Colaboratory that can reproduce the situation.
&lt;denchmark-h:h3&gt;(1) Integer Quantization&lt;/denchmark-h&gt;

Performs integer quantization. The resources used for quantization are obtained at Google Colaboratory. This step will successfully generate an integer quantized .tflite file.
### tensorflow==2.3.0-rc1

import tensorflow as tf
import numpy as np

def representative_dataset_gen():
  for image in raw_test_data:
    image = tf.image.resize(image, (512, 512))
    image = image[np.newaxis,:,:,:]
    image = image - 127.5
    image = image * 0.007843
    yield [image]

raw_test_data = np.load('calibration_data_img_coco_512x512.npy', allow_pickle=True)

# Integer Quantization - Input/Output=float32
converter = tf.lite.TFLiteConverter.from_saved_model('saved_model')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen
tflite_quant_model = converter.convert()
with open('efficientdet_d0_512x512_integer_quant.tflite', 'wb') as w:
    w.write(tflite_quant_model)
print("Integer Quantization complete! - efficientdet_d0_512x512_integer_quant.tflite")
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
  :
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Issue encountered when serializing global_step.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
to_proto not supported in EAGER mode.
Integer Quantization complete! - efficientdet_d0_512x512_integer_quant.tflite
&lt;denchmark-h:h3&gt;(2) Checking the operation of the model&lt;/denchmark-h&gt;

The next step is to check the execution of the model with a minimal amount of test code. When you run this test code, you'll see an error in the input geometry mismatch for the Dequantize OP occurs.
import numpy as np
import tensorflow as tf

interpreter = tf.lite.Interpreter(model_path="efficientdet_d0_512x512_integer_quant.tflite")
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

print('input:', input_details)
print('')
print('output:', output_details)

input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])
print('output_data.shape:', output_data.shape)
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-7-c9ae37dddd9b&gt; in &lt;module&gt;()
      3 
      4 interpreter = tf.lite.Interpreter(model_path="efficientdet_d0_512x512_integer_quant.tflite")
----&gt; 5 interpreter.allocate_tensors()
      6 input_details = interpreter.get_input_details()
      7 output_details = interpreter.get_output_details()

/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py in allocate_tensors(self)
    241   def allocate_tensors(self):
    242     self._ensure_safe()
--&gt; 243     return self._interpreter.AllocateTensors()
    244 
    245   def _safe_to_run(self):

RuntimeError: tensorflow/lite/kernels/dequantize.cc:61 op_context.input-&gt;type == kTfLiteUInt8 || op_context.input-&gt;type == kTfLiteInt8 || op_context.input-&gt;type == kTfLiteInt16 || op_context.input-&gt;type == kTfLiteFloat16 was not true.Node number 782 (DEQUANTIZE) failed to prepare.
&lt;denchmark-h:h2&gt;3. Links to saved models, GraphDef, and test datasets&lt;/denchmark-h&gt;

The links below contain EfficientDet D0's GraphDef and saved_model and test datasets.
https://drive.google.com/file/d/1ymLnGUebTPTRtGLPy539w0P0r3UZfcA_/view?usp=sharing
The integer quantized .tflite file can be obtained from the link below.
https://drive.google.com/file/d/12S6GgRn4I2jl1c5omP023i4zgKACyDe5/view?usp=sharing
&lt;denchmark-h:h2&gt;4. Failure details&lt;/denchmark-h&gt;

In some places, a double Dequantize OP is generated in duplicate. Due to this issue, running the Python API interpreter.allocate_tensors() causes an error indicating that the structure of the model is flawed.
RuntimeError: tensorflow/lite/kernels/dequantize.cc:61 op_context.input-&gt;type == kTfLiteUInt8 || op_context.input-&gt;type == kTfLiteInt8 || op_context.input-&gt;type == kTfLiteInt16 || op_context.input-&gt;type == kTfLiteFloat16 was not true.Node number 782 (DEQUANTIZE) failed to prepare.
&lt;denchmark-link:https://user-images.githubusercontent.com/33194443/87227676-9eedf000-c3d7-11ea-952c-5375a67b4f36.png&gt;&lt;/denchmark-link&gt;

The content of the error message and the INDEX number of the OP indicates that it is the second Dequantize OP that is causing this problem.
&lt;denchmark-link:https://user-images.githubusercontent.com/33194443/87227799-276c9080-c3d8-11ea-948a-ce7799cdc3a5.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;5. Related Issues&lt;/denchmark-h&gt;


tflite: Slicing isn't compatible with quantisation #29571
TFLite Interpreter, allocate_tensors() failed to prepare, not kTFLiteInt8/Uint8 #31053

	</description>
	<comments>
		<comment id='1' author='PINTO0309' date='2020-07-31T03:17:37Z'>
		Same error for me, Have you find the way to fix this error &lt;denchmark-link:https://github.com/PINTO0309&gt;@PINTO0309&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='PINTO0309' date='2020-07-31T03:25:02Z'>
		&lt;denchmark-link:https://github.com/Julius-ZCJ&gt;@Julius-ZCJ&lt;/denchmark-link&gt;

No. I am waiting for the issue to be fixed.
		</comment>
		<comment id='3' author='PINTO0309' date='2020-08-21T00:14:56Z'>
		&lt;denchmark-link:https://github.com/liyunlu0618&gt;@liyunlu0618&lt;/denchmark-link&gt;
 Do you have any idea when we would expect to fix this issue? Thanks
		</comment>
		<comment id='4' author='PINTO0309' date='2020-08-29T12:58:26Z'>
		The problem seems to be resolved in 2.4.0-dev20200829. Inference performed correctly using Integer Quantized models.
&lt;denchmark-link:https://user-images.githubusercontent.com/33194443/91637339-b42fe400-ea42-11ea-8073-7c9ce1a0c147.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/33194443/91637298-7d59ce00-ea42-11ea-8d3f-d8266009057e.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='PINTO0309' date='2020-08-29T12:58:28Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41308&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41308&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>