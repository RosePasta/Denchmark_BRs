<bug id='5516' author='wjaskowski' open_date='2016-11-10T12:23:06Z' closed_time='2017-06-16T20:19:37Z'>
	<summary>TensorFlow is 1.3-7 times slower than Theano for small models</summary>
	<description>
I created a small benchmark (&lt;denchmark-link:https://github.com/wjaskowski/tensorflow-vs-theano-benchmark&gt;https://github.com/wjaskowski/tensorflow-vs-theano-benchmark&lt;/denchmark-link&gt;
) which shows that, depending on the model architecture, Tensorflow is 1.3-7 times slower than Theano depending on the model architecture. The question is whether this effect is due to a design decision or a performance bug, which may be solved in the future?
Related: &lt;denchmark-link:https://github.com/keras-team/keras/issues/4287&gt;keras-team/keras#4287&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/5422&gt;#5422&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='wjaskowski' date='2016-11-11T17:57:41Z'>
		&lt;denchmark-link:https://github.com/zhangyaobit&gt;@zhangyaobit&lt;/denchmark-link&gt;
, please take a look.
		</comment>
		<comment id='2' author='wjaskowski' date='2016-11-11T18:19:14Z'>
		One first thing I'd check is that for convolution if NCHW (faster) format or NHWC format is used. To find out the causes of performance discrepancies, you will need to profile Tensorflow and Theano and compare the two in terms of where the time is spent. For TensorFlow, you can use Timeline (&lt;denchmark-link:http://stackoverflow.com/questions/36123740/is-there-a-way-of-determining-how-much-gpu-memory-is-in-use-by-tensorflow/37931964#37931964&gt;http://stackoverflow.com/questions/36123740/is-there-a-way-of-determining-how-much-gpu-memory-is-in-use-by-tensorflow/37931964#37931964&lt;/denchmark-link&gt;
) to find out the time breakdown for all nodes/operations in a TensorFlow graph.
		</comment>
		<comment id='3' author='wjaskowski' date='2016-11-11T19:04:27Z'>
		I use tf.contrib.layers.convolution2d(input,...), where input is [batch_size, height, width, channels] according to the documentation. Do you suggest that this is wrong, and I should use [batch_size, channels, height, width]?
The code for tensorflow is dead simple:
def create_tf(image_shape, output_dim, layers_conf):
    from tensorflow.contrib import layers

    x = tf.placeholder(tf.float32, shape=[None, image_shape[0], image_shape[1], 1], name="input")
    t = tf.placeholder(tf.float32, shape=[None, output_dim], name="target")

    net = x
    for num_filters in layers_conf[:-1]:
        net = layers.conv2d(net, num_outputs=num_filters, kernel_size=3, stride=3, activation_fn=tf.nn.relu,
                weights_initializer=layers.xavier_initializer_conv2d(), biases_initializer=tf.constant_initializer(0.1))
    net = layers.flatten(net)
    net = layers.fully_connected(net, num_outputs=layers_conf[-1], activation_fn=tf.nn.relu, weights_initializer=layers.xavier_initializer(),
            biases_initializer=tf.constant_initializer(0.1))
    y = layers.fully_connected(net, num_outputs=output_dim, activation_fn=None, weights_initializer=layers.xavier_initializer(),
            biases_initializer=tf.constant_initializer(0.1))
    mean_square_error = 0.5*tf.reduce_mean((y - t)**2)
    optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss=mean_square_error)

    def backprop(x_batch, y_batch):
        optimizer.run({x: x_batch, t: y_batch})

    def fwd_pass(x_batch):
        return y.eval({x: x_batch})

    return fwd_pass, backprop
I am willing to profile it I this could help you. I would not spend time on it, however, if you acknowledge that such performance gap is expected. So is this expected or not?
		</comment>
		<comment id='4' author='wjaskowski' date='2016-11-11T19:53:43Z'>
		NHWC is not wrong. I'm just saying it might be a slower option than NCHW. You can try HCHW by adding data_format="NCHW" to layers.conv2d, and change the shape accordingly, and also make sure other ops can accept this format.
This kind of slow down (1.3-7x) is common when it comes to comparing NCHW and NHWC, and might be the reason why it is slower than Theano.
While we are working on hiding this kind of manual tuning from users (e.g. selecting a better data_format for a op and a device), users are still expected to spend time to understand and tune the  performance, and the first basic step towards that is profiling.
		</comment>
		<comment id='5' author='wjaskowski' date='2016-11-11T19:55:39Z'>
		Btw. TF is ca. 3 times slower than Theano also where the model involves only dense layers (no convolutions).
		</comment>
		<comment id='6' author='wjaskowski' date='2016-11-11T20:02:00Z'>
		Could you post the code and the Timeline profiling results?
		</comment>
		<comment id='7' author='wjaskowski' date='2016-11-12T08:08:45Z'>
		HCHW did not help. The benchmark code is here: &lt;denchmark-link:https://github.com/wjaskowski/tensorflow-vs-theano-benchmark&gt;https://github.com/wjaskowski/tensorflow-vs-theano-benchmark&lt;/denchmark-link&gt;
. Here are the Timeline results:

&lt;denchmark-link:https://cloud.githubusercontent.com/assets/4952605/20236490/fc13f594-a8b6-11e6-98b3-440fd65d305d.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/587125/timeline-128.128.32.32.32.128.txt&gt;timeline-128 128 32 32 32 128.txt&lt;/denchmark-link&gt;


&lt;denchmark-link:https://cloud.githubusercontent.com/assets/4952605/20236491/08ed25b0-a8b7-11e6-9e15-f2389686b81d.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/587126/timeline-32.32.32.32.32.512.txt&gt;timeline-32 32 32 32 32 512.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='wjaskowski' date='2016-11-14T21:57:27Z'>
		&lt;denchmark-link:https://github.com/wjaskowski&gt;@wjaskowski&lt;/denchmark-link&gt;
, could you make sure you exclude the first few iterations from the benchmark? This is how convnet-benchmark does it.
&lt;denchmark-link:https://github.com/soumith/convnet-benchmarks/blob/master/tensorflow/benchmark_alexnet.py#L139&gt;https://github.com/soumith/convnet-benchmarks/blob/master/tensorflow/benchmark_alexnet.py#L139&lt;/denchmark-link&gt;

In the first round, TensorFlow does autotuning to find out the best algorithm, so it might have more overhead. Following up rounds should not have them. For convnet benchmarks, important things are:

NCHW format
Excluding first few rounds, typically 10.
Make sure your session run didn't fetch anything unnecessarily.

		</comment>
		<comment id='9' author='wjaskowski' date='2016-11-14T22:08:17Z'>
		&lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
:

I exclude the first 50 rounds (https://github.com/wjaskowski/tensorflow-vs-theano-benchmark/blob/master/benchmark.py#L74).
I have checked that NCHW improves the results only slightly (as a margin note, I find it strange that you recommend NCHW while NHWC is the default one).
I cannot imagine what unnecessary could I possibly fetch in this simple example.

		</comment>
		<comment id='10' author='wjaskowski' date='2016-11-14T22:25:22Z'>
		Looks like your Timeline is a profiling run that tries multiple conv2d implementations to find the best one, rather than a later run that uses the previously found best implementation.
It might be that somehow the benchmark is not written in a standard way, and as a result it is always running the profiling runs.
As you see, each conv layers actually invoked multiple runs:
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/1034716/20285252/b46c6896-aa75-11e6-815d-25be068acf6c.png&gt;&lt;/denchmark-link&gt;

After this is fixed, you can try NCHW again. Because I'm thinking your previous comparison might be between  NCHW and NHWC in the profiling mode, rather than in the regular mode.
		</comment>
		<comment id='11' author='wjaskowski' date='2016-11-14T22:44:39Z'>
		&lt;denchmark-link:https://github.com/wjaskowski&gt;@wjaskowski&lt;/denchmark-link&gt;
 there's a fixed per-run overhead in TensorFlow which is higher than in Theano, and when your session.run only takes 1.6ms, this overhead is significant. One source of overhead is "crossing TensorFlow/Python" boundary which happens in TF that has C++ runtime, but not Theano which has Python runtime. For your TF experiment, every 1.6ms you release GIL, copy data from Python runtime to C++ TensorFlow runtime, then copy the result back and reacquire GIL. For an op that does no memory transfers and sees no graph modifications , each eval call will add 0.2ms overhead.
TF has been optimized for cases when session run call takes 20ms-2000ms, so these tiny run call scenarios didn't get as much attention, however there are some tricks you can do. For instance to remove the extra Python-&gt;TF copy, you can keep the data in tensorflow runtime using variables or queues. To eliminate extra TF-&gt;Python copies, you can keep data in tensorflow runtime by doing sess.run(optimizer_node.op) instead of sess.run(optimizer_node) in your loop
		</comment>
		<comment id='12' author='wjaskowski' date='2016-11-15T00:24:17Z'>
		I run the benchmark and found out your profiling results is fine, which is an actually a regular run, instead of a profiling run, which I thought earlier it is.
This now indeed looks like a TensorFlow internal issue; the CPU overhead seems high for smaller shapes (32x32) and we will investigate that. We will also look at the performance for NCHW format.
		</comment>
		<comment id='13' author='wjaskowski' date='2016-11-15T09:49:48Z'>
		&lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 I see. Indeed, my first profiling results shows the overhead of fetching the data. When I removed it, TF/TH time ratio decreased to 2-2.5. I also confirm that when the model is larger (e.g. 512 filters, the difference between TH and TF shrinks. E.g., for "128x128 + conv[512,512,256,128] + 512") I got a ratio of 1.5 for forward pass and 0.85 for backprop (i.e. TF is actually faster).
But still, it would be nice to make TF also quick for smaller models. In reinforcement learning, I cannot afford huge models. It is also not so easy to prefetch data to TF since the data are generated on-the-fly (but maybe with the queues?).
		</comment>
		<comment id='14' author='wjaskowski' date='2016-11-15T16:46:15Z'>
		&lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/wjaskowski&gt;@wjaskowski&lt;/denchmark-link&gt;
 +1 regarding the fact that one can't dismiss poor performance for small models just because no one cares about MNIST and CIFAR-10 (and their corresponding small convnets) anymore. The state-of-the-art RL models are rather small and so the importance of adequate performance for small models is still an issue to address for current research and production use cases.
		</comment>
		<comment id='15' author='wjaskowski' date='2016-11-22T02:09:02Z'>
		I did more CPU profiling and found that the majority of the CPU overhead is actually from the host code inside the conv op, rather than from the executor. The CPU time is distributed across pre- and post- processing steps such as padding and filter transform, as well as in the cuDNN kernel invocation. There doesn't seem to be a single "performance bug" that could be fixed here, but the overall CPU side of the implementation of conv could be improved.
Conv actually internally converts NHWC to NCHW, so using NCHW directly would avoid the conversion. However, for small models, as the performance is bottlenecked by the CPU code, using NCHW makes little performance difference.
I marked the issue as contribution welcome for interested people to take a deeper look.
		</comment>
		<comment id='16' author='wjaskowski' date='2016-11-22T02:55:19Z'>
		&lt;denchmark-link:https://github.com/benoitsteiner&gt;@benoitsteiner&lt;/denchmark-link&gt;
, here is an interesting example the performance of NCHW and NHWC is about the same, as the CPU is the bottleneck.
		</comment>
		<comment id='17' author='wjaskowski' date='2017-06-16T20:19:37Z'>
		Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
		<comment id='18' author='wjaskowski' date='2017-11-18T15:51:40Z'>
		I am using Tensorflow 1.3 and it is %25 slower than MatConvNet, an alternative deep learning library for Matlab. I can fit more batches of data(2x) into GPU in the "MatConvNet" implementation and i think this is one of the most important factor in the performance difference.
In tensorflow implementation, queue runners, multi tower gpu implementation and all best practices are used for getting the best performance.
Tried many different scenarios to improve results such as using different data formats, NHWC and NCHW, allow_growth etc..
I expect Tensorflow as way much faster according to the cumbersome "Matlab" architecture. Currently, TF with python, consumes much more memory and uses GPU less efficient than Matlab alternative. This is very interesting.
Training Data : 2165x5x5x145
Model :
conv2d([1x1, 1200])
relu
conv2d([3x3, 600])
relu
conv2d([5x5, 300])
relu
fc
This issue should be reopened as TF performs worse many other libraries out there regardless of their platforms
		</comment>
		<comment id='19' author='wjaskowski' date='2017-11-18T19:03:49Z'>
		There are many cases when TensorFlow is slower than something else. Sometimes it's due to user error, sometimes not. A self-contained benchmark would be needed to troubleshoot this correctly.
BTW, queue runners don't have great performance, tf.data should be better
		</comment>
		<comment id='20' author='wjaskowski' date='2017-11-18T20:16:37Z'>
		&lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 if you think the tf.data should close the %25 performance gap between other frameworks, i can try it. But how could you explain the 2x batch size difference between two frameworks ? Is there any hints, data pinning or something like that ?
Excuse me but, my comparison is not between top-competitors(Caffe, Caffe2, CNTK, Theano etc..) of the arena. This is a Matlab based third party library not backed by an large scale company or community. Also Matlab is another computation platform which is heavier and slower according other native or managed platforms. I think this issue should be considered more seriously and any inspection regarding such a critical performance issue should not be handled case case but on the overall implementation of the TF library.
		</comment>
		<comment id='21' author='wjaskowski' date='2017-11-19T07:09:39Z'>
		&lt;denchmark-link:https://github.com/aligokalppeker&gt;@aligokalppeker&lt;/denchmark-link&gt;
 I do not find it surprising that TensorFlow code may run slower than Matlab for a particular example, TensorFlow gives a lot more opportunities for the user shooting themselves in the foot
		</comment>
		<comment id='22' author='wjaskowski' date='2017-11-19T18:26:14Z'>
		&lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 I think, the your latest comment has been written with no development experience/knowledge on Matlab(or you write such a meaningless thing intentionaly). With such a unacceptable approach, decline on TF's performance and quality will continue. This bug is an evidence of such a bad progress; &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/14107&gt;#14107&lt;/denchmark-link&gt;

At least they accepted the problem in the bug, they did not bury their heads in to the sand...
		</comment>
		<comment id='23' author='wjaskowski' date='2017-11-19T23:48:14Z'>
		"Low-performance" is a bit of an over-simplification. Improvements in TensorFlow performance translates directly to datacenter cost savings, so Google spends significant resources in this area. What's happening is that some kinds of computations  are optimized, while others, especially ones small enough to run in Matlab, are not. I think eventually, most computations will be possible to do in TensorFlow without incurring high performance/human cost, but what can help in getting there faster is providing self-contained benchmarks that demonstrate the problem.
		</comment>
	</comments>
</bug>