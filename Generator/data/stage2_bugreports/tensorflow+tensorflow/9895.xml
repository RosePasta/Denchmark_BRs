<bug id='9895' author='thomasquintana' open_date='2017-05-14T19:15:34Z' closed_time='2017-06-15T18:55:44Z'>
	<summary>XLA "Aborted (core dumped)"</summary>
	<description>
OS: Ubuntu/Linux (16.04)
TensorFlow: Compiled from source
TensorFlow Version: r1.1
Bazel Version: 0.4.5
CUDA/CuDNN Versions: 8.0/5.1
GPU Model/Memory: TitanX/12Gb
After turning on XLA JIT compiling, TF fails with a core dump.
&lt;denchmark-code&gt;2017-05-14 14:50:38.673877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:06:00.0
Total memory: 11.90GiB
Free memory: 11.75GiB
2017-05-14 14:50:38.673951: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x3cb6960
2017-05-14 14:50:38.900484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 1 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:05:00.0
Total memory: 11.90GiB
Free memory: 11.67GiB
2017-05-14 14:50:38.901416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 1 
2017-05-14 14:50:38.901427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y Y 
2017-05-14 14:50:38.901430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 1:   Y Y 
2017-05-14 14:50:38.901437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: TITAN X (Pascal), pci bus id: 0000:06:00.0)
2017-05-14 14:50:38.901441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: TITAN X (Pascal), pci bus id: 0000:05:00.0)
2017-05-14 14:50:38.979726: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 2 visible devices
2017-05-14 14:50:38.979748: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 12 visible devices
2017-05-14 14:50:38.981294: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x4a7a7b0 executing computations on platform Host. Devices:
2017-05-14 14:50:38.981305: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
2017-05-14 14:50:38.981438: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 2 visible devices
2017-05-14 14:50:38.981446: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 12 visible devices
2017-05-14 14:50:38.982608: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x4a42c80 executing computations on platform CUDA. Devices:
2017-05-14 14:50:38.982619: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): TITAN X (Pascal), Compute Capability 6.1
2017-05-14 14:50:38.982623: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (1): TITAN X (Pascal), Compute Capability 6.1
2017-05-14 14:51:53.443289: F tensorflow/compiler/xla/service/algebraic_simplifier.cc:768] Check failed: user-&gt;operand(reshape_or_broadcast_operand_index) == reshape_or_broadcast (0x7f30bc938e70 vs. 0x7f30bcb3e340)
Aborted (core dumped)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='thomasquintana' date='2017-05-15T17:46:17Z'>
		Thanks for reporting! Do you have a simple script to repro?
CC: &lt;denchmark-link:https://github.com/tatatodd&gt;@tatatodd&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='thomasquintana' date='2017-05-15T18:43:24Z'>
		I am implementing the &lt;denchmark-link:https://arxiv.org/pdf/1504.00941v2.pdf&gt;IRNN&lt;/denchmark-link&gt;
 paper but the RNN layer runs really slow which prompted me to give XLA a try. I have done my best to simplify the code as much as possible. I hope this helps.
import os
import sys
import time
import urllib.request

import numpy as np
import tensorflow as tf


def fetch(destination, source):
  if os.path.exists(destination):
    return destination
  target_dir = os.path.dirname(destination)
  if not os.path.exists(target_dir):
    os.makedirs(target_dir)
  def progress(count, block_size, total_size):
    bytes_downloaded = float(count * block_size)
    percent_downloaded = int(bytes_downloaded / float(total_size) * 100.0)
    sys.stdout.write('\rDownloading from {} {}%'.format(
      source, percent_downloaded
    ))
    sys.stdout.flush()
  target_path, _ = urllib.urlretrieve(source, destination, progress)
  target_info = os.stat(target_path)
  sys.stdout.write(
    '\nSuccessfully downloaded {} {} bytes.\n'.format(
      target_path, target_info.st_size
    )
  )
  sys.stdout.flush()
  return target_path

def load_data():
  path = fetch(
    '/tmp/mnist.npz', 'https://s3.amazonaws.com/img-datasets/mnist.npz'
  )
  inputs = np.load(path)
  try:
    return (inputs['x_train'], inputs['y_train']), \
           (inputs['x_test'], inputs['y_test'])
  finally:
    inputs.close()


epochs = 900
batch_count = 600
batch_sz = 100
# Load the training data.
(x_train, y_train), (x_test, y_test) = load_data()
# Cast the examples.
x_train = x_train.astype(np.float32)
x_test = x_test.astype(np.float32)
# Subtract the mean and scale the examples.
x_train = (x_train - np.mean(x_train)) / \
          (np.max(x_train) - np.min(x_train))
x_test = (x_test - np.mean(x_test)) / \
         (np.max(x_test) - np.min(x_test))
# Reshape the inputs for the rnn layer.
x_train = np.reshape(x_train, [60000, 784, 1])
x_train = np.transpose(x_train, [1, 0, 2])
x_test = np.reshape(x_test, [10000, 784, 1])
x_test = np.transpose(x_test, [1, 0, 2])
# Build the tensorflow graph.
graph = tf.Graph()
with graph.as_default():
  x = tf.placeholder(tf.float32, [None, None, 1])
  y = tf.placeholder(tf.int32)
  with tf.variable_scope('rnn_1'):
    weights = tf.get_variable('weights', initializer=tf.truncated_normal(
      [1, 100], 0.0, 0.01, dtype=tf.float32
    ))
    state = tf.get_variable(
      'state', 
      shape=[100, 100],
      initializer=tf.constant_initializer(np.identity(100), dtype=tf.float32))
    bias = tf.get_variable(
      'bias', shape=[100], 
      initializer=tf.constant_initializer(0.0, dtype=tf.float32)
    )
    x_shape = tf.shape(x)
    timesteps = x_shape[0]
    batch_size = x_shape[1]
    # Compute the linear transformation for every time step.
    output = tf.reshape(x, [-1, 1])
    output = tf.add(tf.matmul(output, weights), bias)
    output = tf.reshape(output, [timesteps, batch_size, 100])
    # Create a tensor array to hold the output of our rnn layer.
    temp = tf.TensorArray(dtype=tf.float32, size=timesteps)

    def step(c, t, h, s, i, o):
      h = tf.nn.relu(tf.add(tf.matmul(h, s), i[c]))
      o = o.write(c, h)
      return [c + 1, t, h, s, i, o]

    def step_condition(c, t, h, s, i, o):
      return tf.less(c, t)

    # Create a counter to track the number of timesteps.
    count = tf.constant(0)
    # Define the initial hidden state.
    hidden = tf.zeros([batch_size, 100], dtype=tf.float32)

    _, _, _, _, _, output = tf.while_loop(
      step_condition,
      step,
      [count, timesteps, hidden, state, output, temp]
    )

    output = output.stack()
  # We're only interested in the last output from the RNN layer.
  output = output[-1, :, :]
  with tf.variable_scope('ff_1'):
    weights = tf.get_variable(
      'weights',
      shape=[100, 10],
      initializer=tf.contrib.layers.xavier_initializer(dtype=tf.float32)
    )
    bias = tf.get_variable(
      'bias',
      shape=[10], 
      initializer=tf.constant_initializer(0.0, dtype=tf.float32)
    )
    output = tf.add(tf.matmul(output, weights), bias)
  predictions = tf.nn.softmax(output)
  error = tf.nn.sparse_softmax_cross_entropy_with_logits(
    logits=output, labels=y
  )
  error = tf.reduce_mean(error)
  optimizer = tf.train.RMSPropOptimizer(1e-8)
  gradients = optimizer.compute_gradients(error)
  train_op = optimizer.apply_gradients(
    [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]
  )
# Train the model.
config = tf.ConfigProto()
config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1
with tf.Session(config=config, graph=graph) as session:
  session.run(tf.global_variables_initializer())
  for epoch in range(epochs):
    epoch_duration = 0.0
    epoch_error = 0.0
    start_time = time.time()
    for batch in range(batch_count):
      batch_error, _ = session.run([error, train_op], {
        x: x_train[:, batch * batch_sz:batch * batch_sz + batch_sz, :],
        y: y_train[batch * batch_sz:batch * batch_sz + batch_sz]
      })
      epoch_error += batch_error
    end_time = time.time()
    epoch_duration = end_time - start_time
    print('Epoch %d: loss = %.2f (%.3f sec)' % (
      epoch + 1, epoch_error, epoch_duration
    ))
		</comment>
		<comment id='3' author='thomasquintana' date='2017-05-15T23:54:24Z'>
		Hey &lt;denchmark-link:https://github.com/tatatodd&gt;@tatatodd&lt;/denchmark-link&gt;
, could you take a look when you get a chance? (:
Thanks,
Ali
		</comment>
		<comment id='4' author='thomasquintana' date='2017-05-19T18:36:11Z'>
		Hi Guys,
Just checking in :) Any progress on this issue?
Cheers,
Tom
		</comment>
		<comment id='5' author='thomasquintana' date='2017-05-20T13:18:01Z'>
		I'm also running into this problem.
		</comment>
		<comment id='6' author='thomasquintana' date='2017-05-22T02:16:50Z'>
		I'm running into this problem too when I run the mnist_softmax_xla.py.
		</comment>
		<comment id='7' author='thomasquintana' date='2017-06-07T10:33:41Z'>
		I'm also seeing this problem on Ubuntu 16.04, Linux ppc64le platform with tensorflow 1.1.0 built with XLA enabled.
		</comment>
		<comment id='8' author='thomasquintana' date='2017-06-15T13:23:17Z'>
		Any update on this? It's an easily reproduced issue that makes using XLA impossible, but there hasn't been anyone assigned to this ticket.
		</comment>
		<comment id='9' author='thomasquintana' date='2017-06-15T15:55:26Z'>
		Apologies on the long delay.  I'm taking a look at this now.
		</comment>
		<comment id='10' author='thomasquintana' date='2017-06-15T15:58:30Z'>
		BTW - if anyone who's running into this problem has a smaller repro script, that would be appreciated.
		</comment>
		<comment id='11' author='thomasquintana' date='2017-06-15T16:23:18Z'>
		I believe @wj1066 reported that he's having the same issue with &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax_xla.py&gt;mnist_softmax_xla.py&lt;/denchmark-link&gt;
. I tried to simplify the script I provided above but not sure I can make it any simpler. Thanks for looking into this issue.
		</comment>
		<comment id='12' author='thomasquintana' date='2017-06-15T18:55:44Z'>
		&lt;denchmark-link:https://github.com/thomasquintana&gt;@thomasquintana&lt;/denchmark-link&gt;
 I replicated your issue on TF 1.1.0 build with XLA enabled, using the python program you provided.  FYI the CHECK failure indicates a logical error in the compiler:
&lt;denchmark-code&gt;2017-05-14 14:51:53.443289: F tensorflow/compiler/xla/service/algebraic_simplifier.cc:768] Check failed: user-&gt;operand(reshape_or_broadcast_operand_index) == reshape_or_broadcast (0x7f30bc938e70 vs. 0x7f30bcb3e340)
&lt;/denchmark-code&gt;

I also tried the same thing on TF 1.2.0-rc2, and the failure goes away.  Looking through the commit history of algebraic_simplifier.cc, there have been many changes to the code.  I did not identify exactly which change fixed the problem.
So the solution is to use TF 1.2.0-rc2 or later.  As a reminder, different TF releases can be found here:
&lt;denchmark-link:https://hub.docker.com/r/tensorflow/tensorflow/tags/&gt;https://hub.docker.com/r/tensorflow/tensorflow/tags/&lt;/denchmark-link&gt;

I'm closing this out; if you feel this doesn't resolve the issue, just comment on this thread and I'll re-open.
		</comment>
		<comment id='13' author='thomasquintana' date='2019-03-09T09:02:17Z'>
		Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-03-09 16:58:39.308046: W tensorflow/compiler/xla/service/platform_util.cc:240] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 11523260416
2019-03-09 16:58:39.308242: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA
Aborted (core dumped)
(tensorflow) deeplab@deeplab-X399-DESIGNARE-EX:~/Tracy/FasterRCNN/tf-faster-rcnn-master$ Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA
Attempting: command not found
		</comment>
		<comment id='14' author='thomasquintana' date='2019-07-11T10:46:28Z'>
		Deprecated in favor of operator or tf.math.divide.
2019-07-11 12:42:06.524160: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-11 12:42:11.226602: W tensorflow/compiler/xla/service/platform_util.cc:240] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 11523260416
2019-07-11 12:42:11.399046: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA
Aborted (core dumped)
this is the error appearing need  help!!
		</comment>
		<comment id='15' author='thomasquintana' date='2019-10-31T14:01:01Z'>
		2019-10-31 14:55:52.525459: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-10-31 14:55:52.625544: W tensorflow/compiler/xla/service/platform_util.cc:240] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 4236312576
2019-10-31 14:55:52.625882: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA
Aborted (core dumped)
		</comment>
	</comments>
</bug>