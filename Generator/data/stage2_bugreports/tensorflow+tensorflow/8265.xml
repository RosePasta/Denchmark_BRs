<bug id='8265' author='falaktheoptimist' open_date='2017-03-10T06:16:04Z' closed_time='2017-04-11T05:38:32Z'>
	<summary>Memory leak when writing to Logfile with Tensorboard summarywriter</summary>
	<description>
&lt;denchmark-h:h3&gt;Issue:&lt;/denchmark-h&gt;

Memory occupied after call to Filewriter not being freed till python termination. This causes accumulation of data and subsequent filling up of RAM which is freed only when the entire script completes execution and python is terminated.
&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System: Ubuntu 16.04
CUDA/cuDNN : Not installed
Link to the pip package you : &lt;denchmark-link:https://github.com/tensorflow/tensorflow/releases/tag/v1.0.0&gt;https://github.com/tensorflow/tensorflow/releases/tag/v1.0.0&lt;/denchmark-link&gt;

Tensorflow version: 1.0.0
&lt;denchmark-h:h3&gt;A small script replicating the issue&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import os
import numpy as np
import tensorflow as tf
from memory_profiler import profile
import time

@profile
def _write_into_log(images):
    path_logdir = os.path.join("./MiniExample")
    if not os.path.exists(path_logdir):
        os.makedirs(path_logdir)

    with tf.Graph().as_default() as g:
        image = tf.placeholder(tf.float32, shape = [None, None, None, 3])

        image_summary = tf.summary.image(name = "Images", tensor = image, max_outputs = 2000)

        with tf.Session() as sess:
            summary = sess.run(image_summary, feed_dict = {image : np.concatenate(images, axis = 0)})
            file_writer = tf.summary.FileWriter(path_logdir, g)
            file_writer.add_summary(summary)
            file_writer.close()

@profile
def main():
    out = [np.random.random((1, 224, 224, 3)) for i in range(2000)]
    _write_into_log(out)
    out = None
    time.sleep(10)

main()
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Logs or other output that would be helpful&lt;/denchmark-h&gt;

Python memory profiler output
&lt;denchmark-code&gt;$ python test.py 
Filename: test.py

Line #    Mem usage    Increment   Line Contents
================================================
     7   2394.5 MiB      0.0 MiB   @profile
     8                             def _write_into_log(images):
     9   2394.5 MiB      0.0 MiB       path_logdir = os.path.join("./MiniExample")
    10   2394.5 MiB      0.0 MiB       if not os.path.exists(path_logdir):
    11                                     os.makedirs(path_logdir)
    12                             
    13   2394.6 MiB      0.0 MiB       with tf.Graph().as_default() as g:
    14   2399.0 MiB      4.4 MiB           image = tf.placeholder(tf.float32, shape = [None, None, None, 3])
    15                             
    16   2399.1 MiB      0.1 MiB           image_summary = tf.summary.image(name = "Images", tensor = image, max_outputs = 2000)
    17                             
    18   2407.9 MiB      8.9 MiB           with tf.Session() as sess:
    19   2978.8 MiB    570.9 MiB               summary = sess.run(image_summary, feed_dict = {image : np.concatenate(images, axis = 0)})
    20   2980.8 MiB      2.0 MiB               file_writer = tf.summary.FileWriter(path_logdir, g)
    21   3269.7 MiB    288.9 MiB             file_writer.add_summary(summary)
    22   3269.7 MiB      0.0 MiB               file_writer.close()


Filename: test.py

Line #    Mem usage    Increment   Line Contents
================================================
    24     89.6 MiB      0.0 MiB   @profile
    25                             def main():
    26   2394.5 MiB   2304.9 MiB       out = [np.random.random((1, 224, 224, 3)) for i in range(2000)]
    27   2981.4 MiB    586.8 MiB       _write_into_log(out)
    28    676.7 MiB  -2304.7 MiB       out = None
    29    676.7 MiB      0.0 MiB       time.sleep(10)
&lt;/denchmark-code&gt;

As can be seen above, the additional 586.8 MB occupied after call to the _write_into_log is never cleared.
	</description>
	<comments>
		<comment id='1' author='falaktheoptimist' date='2017-03-17T06:04:25Z'>
		We are experiencing the same issue, but with histograms.  Eventually training script takes up 24G and is killed because of OOM.
		</comment>
		<comment id='2' author='falaktheoptimist' date='2017-03-22T12:02:41Z'>
		Digging a bit more into the first case &lt;denchmark-link:https://github.com/falaktheoptimist&gt;@falaktheoptimist&lt;/denchmark-link&gt;
  shared above, we found that the memory hogging is happening at two stages - first when computing the  (using session.run) and second by the  function of Filewriter. Below are the profiler outputs showing half of the memory(~285 MB) occupied by  and other half being occupied by  call.
Case-1:
&lt;denchmark-code&gt;Filename: test.py

Line #    Mem usage    Increment   Line Contents
================================================
     7   2394.4 MiB      0.0 MiB   @profile
     8                             def _write_into_log(images):
     9   2394.4 MiB      0.0 MiB       with tf.Graph().as_default() as g:
    10   2398.7 MiB      4.3 MiB           image = tf.placeholder(tf.float32, shape = [None, None, None, 3])
    11   2398.8 MiB      0.1 MiB           image_summary = tf.summary.image(name = "Images", tensor = image, max_outputs = 2000)
    12                             
    13   2407.3 MiB      8.5 MiB           with tf.Session() as sess:
    14   2691.9 MiB    284.6 MiB               sess.run(image_summary, feed_dict = {image : np.concatenate(images, axis = 0)})


Filename: test.py

Line #    Mem usage    Increment   Line Contents
================================================
    17     89.4 MiB      0.0 MiB   @profile
    18                             def main():
    19   2394.2 MiB   2304.8 MiB       out = [np.random.random((1, 224, 224, 3)) for i in range(2000)]
    20   2691.9 MiB    297.7 MiB       _write_into_log(out)
    21    387.2 MiB  -2304.7 MiB       out = None
&lt;/denchmark-code&gt;

Case-2:
&lt;denchmark-code&gt;Filename: test.py

Line #    Mem usage    Increment   Line Contents
================================================
     7   2394.2 MiB      0.0 MiB   @profile
     8                             def _write_into_log(images):
     9   2394.2 MiB      0.0 MiB       path_logdir = os.path.join("./MiniExample")
    10   2394.2 MiB      0.0 MiB       if not os.path.exists(path_logdir):
    11                                     os.makedirs(path_logdir)
    12                             
    13   2394.2 MiB      0.1 MiB       with tf.Graph().as_default() as g:
    14   2398.6 MiB      4.4 MiB           image = tf.placeholder(tf.float32, shape = [None, None, None, 3])
    15                             
    16   2398.7 MiB      0.1 MiB           image_summary = tf.summary.image(name = "Images", tensor = image, max_outputs = 2000)
    17                             
    18   2409.4 MiB     10.7 MiB           with tf.Session() as sess:
    19   2409.7 MiB      0.3 MiB               file_writer = tf.summary.FileWriter(path_logdir, g)
    20   2982.4 MiB    572.7 MiB               sss = sess.run(image_summary, feed_dict = {image : np.concatenate(images, axis = 0)})
    21   3271.2 MiB    288.8 MiB               file_writer.add_summary(sss)
    22   3271.2 MiB      0.0 MiB               file_writer.close()
    23   3271.2 MiB      0.0 MiB               file_writer = None
    24   3271.2 MiB      0.0 MiB               image_summary = None
    25   2983.0 MiB   -288.2 MiB               sss = None

Filename: test.py

Line #    Mem usage    Increment   Line Contents
================================================
    27     89.3 MiB      0.0 MiB   @profile
    28                             def main():
    29   2394.2 MiB   2304.9 MiB       out = [np.random.random((1, 224, 224, 3)) for i in range(2000)]
    30   2983.0 MiB    588.8 MiB       _write_into_log(out)
    31    678.3 MiB  -2304.7 MiB       out = None
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='falaktheoptimist' date='2017-03-28T08:09:21Z'>
		We have found the fix to this memory leak issue- it's a minor tweak. We'll send in the pull request as soon as our CLA gets approved (we're just waiting for that).
		</comment>
		<comment id='4' author='falaktheoptimist' date='2017-04-11T05:38:32Z'>
		This has been resolved with &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/8981&gt;#8981&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='falaktheoptimist' date='2017-04-17T14:27:58Z'>
		&lt;denchmark-link:https://github.com/falaktheoptimist&gt;@falaktheoptimist&lt;/denchmark-link&gt;
 , can you explain why it will cause memory leak when queue item is not passed directly?
		</comment>
		<comment id='6' author='falaktheoptimist' date='2017-04-18T04:37:16Z'>
		The thread was not being killed till python termination and hence the reference to event (queue item) was never being freed.
		</comment>
		<comment id='7' author='falaktheoptimist' date='2017-04-18T04:58:13Z'>
		&lt;denchmark-link:https://github.com/falaktheoptimist&gt;@falaktheoptimist&lt;/denchmark-link&gt;
 , I get what you mean. The reference count of the last element in the queue will never decrease, because  will block forever.
		</comment>
		<comment id='8' author='falaktheoptimist' date='2017-04-18T05:01:42Z'>
		&lt;denchmark-link:https://github.com/suiyuan2009&gt;@suiyuan2009&lt;/denchmark-link&gt;
 Exactly.. Thanks to your PR- it's fixed now..
		</comment>
	</comments>
</bug>