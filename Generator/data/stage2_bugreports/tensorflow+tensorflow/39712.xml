<bug id='39712' author='qianzhang613' open_date='2020-05-20T11:24:56Z' closed_time='2020-09-28T17:22:54Z'>
	<summary>Non-OK-status: tensorflow::Env::Default()-&amp;gt;DeleteFile(ptx_path) status: Not found:</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
NO
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Centos7.7
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
pip install officail
TensorFlow version (use command below):
1.15.2 with gpu
Python version:
3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
10.0/ 7.6
GPU model and memory:
v100 32G

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"

Describe the current behavior
Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -&gt; sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version.
2020-04-07 16:28:20.691087: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Searched for CUDA in the following directories:
2020-04-07 16:28:20.691210: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:73]   ./cuda_sdk_lib
2020-04-07 16:28:20.691315: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:73]   /usr/local/cuda
2020-04-07 16:28:20.691454: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:73]   .
2020-04-07 16:28:20.691571: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:75] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2020-04-07 16:28:20.725596: F tensorflow/stream_executor/cuda/ptxas_utils.cc:181] Non-OK-status: tensorflow::Env::Default()-&gt;DeleteFile(ptx_path) status: Not found: /tmp/tempfile-72d9c7c8-2841-4447-bd7d-3947098f8e24-6a7fc700-2624-5a2af2a888f80; No such file or directory
Fatal Python error: Aborted
Describe the expected behavior

the code in follows log the mislead warning?



tensorflow/tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc


         Line 405
      in
      4386a66






 if (log_warning) { 






&lt;denchmark-code&gt;       bool log_warning = true;
          if (maybe_cubin.status().code() ==
              tensorflow::error::Code::NOT_FOUND) {
            // Missing ptxas is expected in some environments where CUDA SDK
            // binaries are not available. We don't want to spam logs with
            // identical warnings in this case.

            // TODO(jlebar): we should implement a LOG_FIRST_N and LOG_EVERY_N
            // for more general usage.
            static std::atomic&lt;bool&gt; warning_done(false);
            log_warning = !warning_done.exchange(true);
          }
          if (log_warning) {
            PrintCantFindCudaMessage(
&lt;/denchmark-code&gt;


if some exception that the ptx_path did not create, there will rasie a error that : 2020-04-07 16:28:20.725596: F tensorflow/stream_executor/cuda/ptxas_utils.cc:181] Non-OK-status: tensorflow::Env::Default()-&gt;DeleteFile(ptx_path) status: Not found:




tensorflow/tensorflow/stream_executor/cuda/ptxas_utils.cc


         Line 184
      in
      4386a66






 TF_RETURN_IF_ERROR( 





&lt;denchmark-code&gt;
  // Write ptx into a temporary file.
  string ptx_path;
  if (!env-&gt;LocalTempFilename(&amp;ptx_path)) {
    return port::InternalError("couldn't get temp PTX file name");
  }
  auto ptx_cleaner = tensorflow::gtl::MakeCleanup([&amp;ptx_path] {
    TF_CHECK_OK(tensorflow::Env::Default()-&gt;DeleteFile(ptx_path));
  });

  TF_RETURN_IF_ERROR(
      tensorflow::WriteStringToFile(env, ptx_path, ptx_contents));
  VLOG(2) &lt;&lt; "ptx written to: " &lt;&lt; ptx_path;

&lt;/denchmark-code&gt;

Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='qianzhang613' date='2020-05-26T11:08:32Z'>
		&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
  could you look at this problem?
		</comment>
		<comment id='2' author='qianzhang613' date='2020-06-12T06:49:38Z'>
		Does this reproduce on tensorflow-nightly?
		</comment>
		<comment id='3' author='qianzhang613' date='2020-06-19T20:52:53Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='4' author='qianzhang613' date='2020-06-24T02:40:47Z'>
		
Does this reproduce on tensorflow-nightly?

i didnot try , because we are using tf v1.15
		</comment>
		<comment id='5' author='qianzhang613' date='2020-06-24T22:12:13Z'>
		Most likely it has been solved in nightly. If it has, we can attempt a backport when doing a patch release on 1.15.
In the end, we are doing development only on master branch and the releases are snapshots of the code at that time, with some cherry-picks. So you will have to move to newer versions at one point
		</comment>
		<comment id='6' author='qianzhang613' date='2020-09-24T02:10:23Z'>
		Has it been solved? I have a same problem using TF2.0.0. Please tell me the solution, thank you !
		</comment>
		<comment id='7' author='qianzhang613' date='2020-09-25T16:42:23Z'>
		&lt;denchmark-link:https://github.com/GuoYL36&gt;@GuoYL36&lt;/denchmark-link&gt;
 can you try reproducer with ?
		</comment>
		<comment id='8' author='qianzhang613' date='2020-09-26T10:17:38Z'>
		
@GuoYL36 can you try reproducer with tf-nightly?

No, I did not try. Now I can run code normally without doing anything, and I just want to say it is amazing.&lt;^_^&gt;
		</comment>
		<comment id='9' author='qianzhang613' date='2020-09-28T17:22:54Z'>
		So it seems it has been solved. I think we can close the issue and reopen if there is a resurgence
		</comment>
		<comment id='10' author='qianzhang613' date='2020-09-28T17:22:56Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39712&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39712&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>