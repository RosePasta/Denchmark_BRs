<bug id='32889' author='hitlic' open_date='2019-09-28T13:55:10Z' closed_time='2019-10-15T20:57:59Z'>
	<summary>TypeError: An op outside of the function building code is being passed</summary>
	<description>
I have realized a code to take previous lstm outputs as current inputs as a sequence generator. But when I want to pass states of lstm cells to next batch by take states as a member of the generator class, it rise an exception "TypeError: An op outside of the function building code is being passed". When add @tf.function to main function in my code, the exception became "AttributeError: 'Tensor' object has no attribute '_numpy'". I suspect this is about the compute graph, but I can't understand the reason.
My tensorflow version is 2.0.0-rc1.
&lt;denchmark-code&gt;In [4]: tf.__version__                                                                                                                                
Out[4]: '2.0.0-rc1'
&lt;/denchmark-code&gt;

I don't know whether it is a bug or not. Thank you for your attention.
The code is:
import tensorflow as tf
from tensorflow import keras
import numpy as np


class RNNGenerator(keras.layers.Layer):
    def __init__(self, rnn_dim, rnn_layer_num, seq_len, seq_width, batch_size, **kwargs):
        self.rnn_dim = rnn_dim              # lstm hidden dim
        self.rnn_layer_num = rnn_layer_num  # number of lstm cell layers
        self.seq_len = seq_len              # length of squence
        self.seq_width = seq_width          # width of squence
        self.batch_size = batch_size

        self._cells = {}

        super().__init__(**kwargs)

    def build(self, input_shape):
        # input dense
        self._dense_in = keras.layers.Dense(self.rnn_dim)

        # many lstm cell layers
        for i in range(self.rnn_layer_num):
            cell = keras.layers.LSTMCell(units=self.rnn_dim)
            self._cells[i] = cell

        # output dense
        self._dens_out = keras.layers.Dense(self.seq_width, activation="sigmoid")

        super().build(input_shape)

    def call(self, inputs):
        inputs = tf.squeeze(inputs, 1)

        # init cells' states
        states = getattr(self, 'states', None)
        if states is None:
            states = {}
            for i, cell in self._cells.items():
                init_cell_states = [tf.random.uniform([self.batch_size, self.rnn_dim]),
                                    tf.random.uniform([self.batch_size, self.rnn_dim])]
                states[i] = init_cell_states

        # --- prev outputs as current inputs
        rand_prev = tf.random.uniform([self.batch_size, self.seq_width], dtype=tf.float32)
        prev_inputs = tf.concat([rand_prev, inputs], -1)
        outputs = []
        for _ in range(self.seq_len):
            cell_inputs = self._dense_in(prev_inputs)
            for i, cell in self._cells.items():
                cell_outputs, states[i] = cell(cell_inputs, states[i])
                cell_inputs = cell_outputs
            step_outputs = self._dens_out(cell_outputs)
            prev_inputs = tf.concat([step_outputs, inputs], -1)

            outputs.append(tf.expand_dims(step_outputs, 1))

        # reserve cells' state in current batch
        self.states = states

        return tf.concat(outputs, 1)

# @tf.function
def main():
    batch_size = 17
    seq_width = 4
    rand_input_dim = 3
    inputs = keras.layers.Input(shape=[1, rand_input_dim])
    outputs = RNNGenerator(rnn_dim=64, rnn_layer_num=2, seq_len=10, seq_width=seq_width, batch_size=batch_size)(inputs)
    outputs = keras.layers.Flatten()(outputs)
    outputs = keras.layers.Dense(1)(outputs)
    outputs = tf.nn.sigmoid(outputs)
    model = keras.models.Model(inputs=inputs, outputs=outputs)

    # model.summary()

    X = np.random.rand(batch_size, 1, rand_input_dim).astype(np.float32)
    y = np.zeros([batch_size, 1])

    model.compile(loss="sparse_categorical_crossentropy", optimizer="sgd", metrics=["accuracy"])
    model.fit(X, y, batch_size=32, epochs=10)


if __name__ == "__main__":
    main()
Running info without @tf.function before main:
2019-09-28 21:43:30.966344: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-09-28 21:43:30.980391: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f88282a2f60 executing computations on platform Host. Devices:
2019-09-28 21:43:30.980415: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
Train on 17 samples
Epoch 1/10
WARNING:tensorflow:From /Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
17/17 [==============================] - 2s 98ms/sample
Traceback (most recent call last):
  File "/Users/xxxxxx/Desktop/GAN-seq/test.py", line 85, in &lt;module&gt;
    main()
  File "/Users/xxxxxx/Desktop/GAN-seq/test.py", line 81, in main
    model.fit(X, y, batch_size=32, epochs=10)
  File "/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 728, in fit
    use_multiprocessing=use_multiprocessing)
  File "/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 324, in fit
    total_epochs=epochs)
  File "/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 123, in run_one_epoch
    batch_outs = execution_function(iterator)
  File "/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", line 86, in execution_function
    distributed_function(input_fn))
  File "/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 457, in __call__
    result = self._call(*args, **kwds)
  File "/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 520, in _call
    return self._stateless_fn(*args, **kwds)
  File "/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 1823, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 1141, in _filtered_call
    self.captured_inputs)
  File "/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 1224, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
  File "/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 511, in call
    ctx=ctx)
  File "/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py", line 76, in quick_execute
    raise e
  File "/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py", line 61, in quick_execute
    num_outputs)
TypeError: An op outside of the function building code is being passed
a "Graph" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: model/rnn_generator/lstm_cell_10/mul_2:0
Running info with @tf.function before main:
2019-09-28 21:48:39.227708: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-09-28 21:48:39.242540: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fdda3d85620 executing computations on platform Host. Devices:
2019-09-28 21:48:39.242575: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
WARNING:tensorflow:From /Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Train on 17 samples
Epoch 1/10
Traceback (most recent call last):
  File "/Users/xxxxxx/Desktop/GAN-seq/test.py", line 85, in &lt;module&gt;
    main()
  File "/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 457, in __call__
    result = self._call(*args, **kwds)
  File "/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 503, in _call
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File "/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 408, in _initialize
    *args, **kwds))
  File "/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 1848, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File "/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 2150, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 2041, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File "/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 358, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py", line 905, in wrapper
    raise e.ag_error_metadata.to_exception(e)
AttributeError: in converted code:
    relative to /Users/xxxxxx:

    Desktop/GAN-seq/test.py:81 main  *
        model.fit(X, y, batch_size=32, epochs=10)
    anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py:728 fit
        use_multiprocessing=use_multiprocessing)
    anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py:674 fit
        steps_name='steps_per_epoch')
    anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py:393 model_iteration
        batch_outs = f(ins_batch)
    anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py:3635 __call__
        [x._numpy() for x in outputs],  # pylint: disable=protected-access
    anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py:3635 &lt;listcomp&gt;
        [x._numpy() for x in outputs],  # pylint: disable=protected-access

    AttributeError: 'Tensor' object has no attribute '_numpy'

	</description>
	<comments>
		<comment id='1' author='hitlic' date='2019-09-30T08:21:45Z'>
		I have tried on colab with TF version 2.0.0-rc1,2.0.0-rc2 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/c3f66cbde5677fa47653892374f09871/untitled234.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='2' author='hitlic' date='2019-10-04T11:48:32Z'>
		I am experiencing the exact same issue (just with different model) in TF version 2.0.0 and the error message is by no means suggestive of what may be done wrongly on the layer of abstraction I'm programming in to fix this (I'm using tf.keras Functional API to build the model and tf.data.Dataset to train with model.fit()).
		</comment>
		<comment id='3' author='hitlic' date='2019-10-15T00:29:51Z'>
		I think this issue is very likely related to the self.states field  captured in the call() context, this leaked reference is used under multiple training context, which could cause the issue. Let me see how to walk around the issue.
		</comment>
		<comment id='4' author='hitlic' date='2019-10-15T00:33:45Z'>
		Btw, you probably shouldn't annotation the main function with tf.function since keras.fit() already convert the model into graph execution under the hood. The tf.function annotation is only useful for training function that handles tensor inputs (your main function doesn't take any tensor as inputs).
		</comment>
		<comment id='5' author='hitlic' date='2019-10-15T00:53:49Z'>
		
Btw, you probably shouldn't annotation the main function with tf.function since keras.fit() already convert the model into graph execution under the hood. The tf.function annotation is only useful for training function that handles tensor inputs (your main function doesn't take any tensor as inputs).

Thank you very much. Your explanation is very useful to me. It would be grateful if the issue were solved.
		</comment>
		<comment id='6' author='hitlic' date='2019-10-15T20:57:02Z'>
		So there are few issues in your code, which is bit hard to diagnose:


Creation of the state. Since the state is tracked and attached to the model, most likely you want to treat them like a data/eager tensor, rather than a symbolic tensor (graph tensor). Since the call() method is executed in the graph context, if you init the state in call(), they will just be graph tensor, and the final self.states = states will set the graph tensor as an attribute on the model, which will just causing issue in the when it proceed next batch. You will need to move the init of the state to build() method.


Record the states at end of the batch. Currently your are doing "self.states = states", which just assign the self.states to a dict with symbolic tensors. The correct way is actually assign the value of the states to the individual state in the self.states.


The changed code is like below, I have just applied the change to how the states is handled, and it works fine.
import tensorflow as tf
from tensorflow import keras
import numpy as np


class RNNGenerator(keras.layers.Layer):
  def __init__(self, rnn_dim, rnn_layer_num, seq_len, seq_width, batch_size, **kwargs):
    self.rnn_dim = rnn_dim              # lstm hidden dim
    self.rnn_layer_num = rnn_layer_num  # number of lstm cell layers
    self.seq_len = seq_len              # length of squence
    self.seq_width = seq_width          # width of squence
    self.batch_size = batch_size

    self._cells = {}

    super().__init__(**kwargs)

  def build(self, input_shape):
    # input dense
    self._dense_in = keras.layers.Dense(self.rnn_dim)

    # many lstm cell layers
    for i in range(self.rnn_layer_num):
      cell = keras.layers.LSTMCell(units=self.rnn_dim)
      self._cells[i] = cell

    # output dense
    self._dens_out = keras.layers.Dense(self.seq_width, activation="sigmoid")

    # init cells' states
    states = getattr(self, 'states', None)
    if states is None:
      states = {}
      for i, cell in self._cells.items():
        init_cell_states = [tf.random.uniform([self.batch_size, self.rnn_dim]),
                            tf.random.uniform([self.batch_size, self.rnn_dim])]
        states[i] = init_cell_states
      self.states = states

    super().build(input_shape)

  # @tf.function
  def call(self, inputs):
    inputs = tf.squeeze(inputs, 1)

    states = self.states.copy()

    # --- prev outputs as current inputs
    rand_prev = tf.random.uniform([self.batch_size, self.seq_width], dtype=tf.float32)
    prev_inputs = tf.concat([rand_prev, inputs], -1)
    outputs = []
    for _ in range(self.seq_len):
      cell_inputs = self._dense_in(prev_inputs)
      for i, cell in self._cells.items():
        cell_outputs, states[i] = cell(cell_inputs, states[i])
        cell_inputs = cell_outputs
      step_outputs = self._dens_out(cell_outputs)
      prev_inputs = tf.concat([step_outputs, inputs], -1)

      outputs.append(tf.expand_dims(step_outputs, 1))

    # reserve cells' state in current batch
    for state_, state in zip(tf.nest.flatten(self.states), tf.nest.flatten(states)):
      state_ = state

    return tf.concat(outputs, 1)

# @tf.function
def main():
  batch_size = 17
  seq_width = 4
  rand_input_dim = 3
  inputs = keras.layers.Input(shape=[1, rand_input_dim])
  outputs = RNNGenerator(rnn_dim=64, rnn_layer_num=2, seq_len=10, seq_width=seq_width, batch_size=batch_size)(inputs)
  outputs = keras.layers.Flatten()(outputs)
  outputs = keras.layers.Dense(1)(outputs)
  outputs = tf.nn.sigmoid(outputs)
  model = keras.models.Model(inputs=inputs, outputs=outputs)

  # model.summary()

  X = np.random.rand(batch_size, 1, rand_input_dim).astype(np.float32)
  y = np.zeros([batch_size, 1])

  model.compile(loss="sparse_categorical_crossentropy", optimizer="sgd", metrics=["accuracy"])
  model.fit(X, y, batch_size=32, epochs=10)


if __name__ == "__main__":
  main()
		</comment>
		<comment id='7' author='hitlic' date='2019-10-15T20:58:00Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32889&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32889&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='hitlic' date='2019-10-16T01:58:17Z'>
		This method is very nice! My problem has been solved. Thanks very much! &lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='hitlic' date='2019-11-12T12:59:37Z'>
		Faced a similar issue. No more error raised using the solution given by &lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
. But my question is, how the below lines update the new state to self.states for the next batch? It looks like the  have never changed after initialized. Please correct me if I'm wrong.
&lt;denchmark-code&gt;# reserve cells' state in current batch
for state_, state in zip(tf.nest.flatten(self.states), tf.nest.flatten(states)):
   state_ = state
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='hitlic' date='2020-08-02T08:07:47Z'>
		&lt;denchmark-code&gt;from keras import backend as K
from keras.utils import conv_utils
from keras.engine import InputSpec
from keras.layers import Conv2D


class myPConv2D(Conv2D):

    def __init__(self, last_layer=False, *args,  **kwargs):
        print("-----args", args, kwargs)
        super().__init__(*args, **kwargs)
        self.last_layer = last_layer
        self.input_spec = [InputSpec(ndim=4), InputSpec(ndim=4)]

    def build(self, input_shape):
        """
        Adapted from original _Conv() layer of Keras.
        Parameters
            input_shape: list of dimensions for [img, mask].
        """
        assert isinstance(input_shape, list)
        assert self.data_format == 'channels_last', "data format should be `channels_last`"
        channel_axis = -1

        if input_shape[0][channel_axis] is None:
            raise ValueError('The channel dimension of the inputs should be defined. Found `None`.')

        self.input_dim = input_shape[0][channel_axis]

        # Image kernel:
        kernel_shape = self.kernel_size + (self.input_dim, self.filters)
        self.kernel  = self.add_weight(shape=kernel_shape,
                                      initializer=self.kernel_initializer,
                                      name='img_kernel',
                                      regularizer=self.kernel_regularizer,
                                      constraint=self.kernel_constraint)
        # Image bias:
        if self.use_bias:
            self.bias = self.add_weight(shape=(self.filters,),
                                        initializer=self.bias_initializer,
                                        name='bias',
                                        regularizer=self.bias_regularizer,
                                        constraint=self.bias_constraint)
        else:
            self.bias = None

        # Mask kernel:
        self.kernel_mask = K.ones(shape=self.kernel_size + (self.input_dim, self.filters))

        self.built = True

    def call(self, inputs):
        assert isinstance(inputs, list) and len(inputs) == 2
        #img, mask = inputs

        # Masked convolution:
        img_output = K.conv2d(inputs[0] * inputs[1],
                              self.kernel,
                              strides=self.strides,
                              padding=self.padding,
                              data_format=self.data_format)

        # Image scaling:
        sum_m = K.conv2d(inputs[1],
                         self.kernel_mask,
                         strides=self.strides,
                         padding=self.padding,
                         data_format=self.data_format)
        # Note, sum_1 does not need to be created via conv2d (as sum_m), it can be generated straight away:
        sum_1i = self.kernel_size[0] * self.kernel_size[1] * self.input_dim
        sum_1 = sum_1i * K.ones(K.shape(sum_m))
        # Prevent division by zero:
        sum_m_clip = K.clip(sum_m, 1., None)
        # Scale the image:
        img_output = img_output * (sum_1 / sum_m_clip)

        # Apply bias only to the image (if chosen to do so):
        if self.use_bias:
            img_output = K.bias_add(img_output,
                                    self.bias,
                                    data_format=self.data_format)

        # Apply activation if needed. Note, in the paper, activation is applied after BatchNormalization.
        if self.activation is not None:
            img_output = self.activation(img_output)

        if self.last_layer:
            return img_output

        # Update the mask:
        mask_output = K.clip(sum_m, 0., 1.)

        return [img_output, mask_output]

    def compute_output_shape(self, input_shape):
        assert isinstance(input_shape, list)
        assert self.data_format == 'channels_last'
        space = input_shape[0][1:-1]
        new_space = []
        for i in range(len(space)):
            new_dim = conv_utils.conv_output_length(
                space[i],
                self.kernel_size[i],
                padding=self.padding,
                stride=self.strides[i],
                dilation=self.dilation_rate[i])
            new_space.append(new_dim)
        new_shape = (input_shape[0][0],) + tuple(new_space) + (self.filters,)
        if self.last_layer:
            return new_shape
        return [new_shape, new_shape]

&lt;/denchmark-code&gt;

Same issue... Please help &lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;

Complete StackTrace error
&lt;denchmark-code&gt;2020-08-02 04:01:27.519864: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-08-02 04:01:27.541115: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2599990000 Hz
2020-08-02 04:01:27.541850: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5578c71a0d30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-02 04:01:27.541891: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-08-02 04:01:27.541986: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2020-08-02 04:01:28.536401: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.
WARNING:tensorflow:From inpainter_main.py:47: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
Please use Model.fit, which supports generators.
Traceback (most recent call last):
  File "inpainter_main.py", line 47, in &lt;module&gt;
    model.fit_generator(
  File "/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py", line 324, in new_func
    return func(*args, **kwargs)
  File "/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 1465, in fit_generator
    return self.fit(
  File "/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 848, in fit
    tmp_logs = train_function(iterator)
  File "/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 580, in __call__
    result = self._call(*args, **kwds)
  File "/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 627, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File "/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 505, in _initialize
    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
  File "/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 2446, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File "/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 2777, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 2657, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File "/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 441, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in user code:

    /home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    /home/bitsy-chuck/Downloads/PConv2D-2ndimp/inpainter_utils/pconv2d_layer.py:71 call  *
        sum_1 = sum_1i * K.ones(K.shape(sum_m))
    /home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:1365 ones  **
        v = array_ops.ones(shape=shape, dtype=tf_dtype, name=name)
    /home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:2979 ones
        output = fill(shape, constant(one, dtype=dtype), name=name)
    /home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:234 fill
        result = gen_array_ops.fill(dims, value, name=name)
    /home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:3311 fill
        return fill_eager_fallback(
    /home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:3338 fill_eager_fallback
        _result = _execute.execute(b"Fill", 1, inputs=_inputs_flat, attrs=_attrs,
    /home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:75 quick_execute
        raise e
    /home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:59 quick_execute
        tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,

    TypeError: An op outside of the function building code is being passed
    a "Graph" tensor. It is possible to have Graph tensors
    leak out of the function building context by including a
    tf.init_scope in your function building code.
    For example, the following function will fail:
      @tf.function
      def has_init_scope():
        my_constant = tf.constant(1.)
        with tf.init_scope():
          added = my_constant * 2
    The graph tensor has name: model/pconv2d_enc_1/Shape:0
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>