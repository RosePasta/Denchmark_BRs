<bug id='35384' author='KANGRuipeng' open_date='2019-12-24T11:40:59Z' closed_time='2020-02-01T06:12:41Z'>
	<summary>TF1.15 Distribute Mode Error</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos7.6
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): TF1.15
Python version: python3.6
CUDA/cuDNN version: 10.0/7.6.2

Code
mirror_strategy = tf.distribute.MirroredStrategy()
with mirror_strategy.scope():
model = test_net()
sgd = tf.keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=1e-4)
model.compile(
optimizer=sgd, loss="sparse_categorical_crossentropy", metrics=["accuracy"]
)
&lt;denchmark-code&gt;model.fit(
    x_train, 
    y_train,
    batch_size=256,
    epochs=10,
    validation_data=(x_test, y_test),
    verbose=2,
)
&lt;/denchmark-code&gt;

The Error Information
No registered 'MultiDeviceIteratorGetNextFromShard' OpKernel for GPU devices compatible with node {{node MultiDeviceIteratorGetNextFromShard}}
.  Registered:  device='CPU'
	</description>
	<comments>
		<comment id='1' author='KANGRuipeng' date='2019-12-24T11:54:15Z'>
		You have fixed this bug in TF2.0. Is there any update of TF1.15
		</comment>
		<comment id='2' author='KANGRuipeng' date='2019-12-26T04:12:16Z'>
		&lt;denchmark-link:https://github.com/KANGRuipeng&gt;@KANGRuipeng&lt;/denchmark-link&gt;

Looks like code is incomplete.Can you please provide simple standalone code to reproduce the issue in our environment. It helps in localizing the issue faster. Thanks!
		</comment>
		<comment id='3' author='KANGRuipeng' date='2019-12-26T06:26:49Z'>
		
@KANGRuipeng
Looks like code is incomplete.Can you please provide simple standalone code to reproduce the issue in our environment. It helps in localizing the issue faster. Thanks!

The test_net is a simple conv-net. You can define it by yourself.
		</comment>
		<comment id='4' author='KANGRuipeng' date='2019-12-26T06:30:41Z'>
		def block(x):
&lt;denchmark-code&gt;x =tf.keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding="same")(x)
x =tf.keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding="same")(x)
x =tf.keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding="same")(x)
x =tf.keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding="same")(x)

x = tf.keras.layers.GlobalAveragePooling2D()(x)
x = tf.keras.layers.Flatten()(x)

x = tf.keras.layers.Dense(10)(x)
x = tf.keras.layers.Activation(activation=tf.nn.softmax)(x)

return x
&lt;/denchmark-code&gt;

def FCN():  # pylint: disable = invalid-name
in_tensor = tf.keras.Input(shape=(32, 32, 3))
out_tensors = block(in_tensor)
model = tf.keras.Model(inputs=in_tensor, outputs=out_tensors, name="FCN")
return model
		</comment>
		<comment id='5' author='KANGRuipeng' date='2019-12-26T06:31:09Z'>
		
def block(x):
x =tf.keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding="same")(x)
x =tf.keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding="same")(x)
x =tf.keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding="same")(x)
x =tf.keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding="same")(x)

x = tf.keras.layers.GlobalAveragePooling2D()(x)
x = tf.keras.layers.Flatten()(x)

x = tf.keras.layers.Dense(10)(x)
x = tf.keras.layers.Activation(activation=tf.nn.softmax)(x)

return x

def FCN(): # pylint: disable = invalid-name
in_tensor = tf.keras.Input(shape=(32, 32, 3))
out_tensors = block(in_tensor)
model = tf.keras.Model(inputs=in_tensor, outputs=out_tensors, name="FCN")
return model

This is the test_net
		</comment>
		<comment id='6' author='KANGRuipeng' date='2019-12-26T06:31:33Z'>
		The data we use is CIFAR10
		</comment>
		<comment id='7' author='KANGRuipeng' date='2019-12-26T23:39:26Z'>
		KANGRuipeng @ can you point me to the specific PR/bug that was fixed for TF 2?
Can you add the end to end example(input data included) which reproduces this issue? I am unable to repro it with the example I have. Thanks!
		</comment>
		<comment id='8' author='KANGRuipeng' date='2020-01-03T06:17:20Z'>
		cifar10 = tf.keras.datasets.cifar10
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
x_train, x_test = (x_train - 128.0) / 128.0, (x_test - 128.0) / 128.0
		</comment>
		<comment id='9' author='KANGRuipeng' date='2020-01-03T06:17:39Z'>
		
cifar10 = tf.keras.datasets.cifar10
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
x_train, x_test = (x_train - 128.0) / 128.0, (x_test - 128.0) / 128.0

this is the input data that I use
		</comment>
		<comment id='10' author='KANGRuipeng' date='2020-01-10T08:26:53Z'>
		
KANGRuipeng @ can you point me to the specific PR/bug that was fixed for TF 2?
Can you add the end to end example(input data included) which reproduces this issue? I am unable to repro it with the example I have. Thanks!

any feedback?
		</comment>
		<comment id='11' author='KANGRuipeng' date='2020-01-10T08:42:59Z'>
		from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import sys
import tensorflow as tf

def block(x):
    x =tf.keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding="same")(x)
    x =tf.keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding="same")(x)
    x =tf.keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding="same")(x)
    x =tf.keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding="same")(x)

    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Flatten()(x)

    x = tf.keras.layers.Dense(10)(x)
    x = tf.keras.layers.Activation(activation=tf.nn.softmax)(x)

    return x
def FCN():
    in_tensor = tf.keras.Input(shape=(32, 32, 3))
    out_tensors = block(in_tensor)
    model = tf.keras.Model(inputs=in_tensor, outputs=out_tensors, name="FCN")
    return model
		</comment>
		<comment id='12' author='KANGRuipeng' date='2020-01-10T08:43:54Z'>
		from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os
import numpy as np
import tensorflow as tf

from test_net import FCN

os.environ["CUDA_VISIBLE_DEVICES"] = "1"

# load data and norm to [-1,1]
cifar10 = tf.keras.datasets.cifar10
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
x_train, x_test = (x_train - 128.0) / 128.0, (x_test - 128.0) / 128.0

x_test= x_test[0:9984]
y_test = y_test[0:9984]

# config the used GPU memory size
config = tf.ConfigProto(allow_soft_placement=True)
config.gpu_options.per_process_gpu_memory_fraction = 0.3
tf.keras.backend.set_session(tf.Session(config=config))


mirror_strategy = tf.distribute.MirroredStrategy()
with mirror_strategy.scope():
    model = FCN()
    sgd = tf.keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=1e-4)
    model.compile(
    optimizer=sgd, loss="sparse_categorical_crossentropy", metrics=["accuracy"]
    )

    model.fit(
        x_train, 
        y_train,
        batch_size=256,
        epochs=10,
        validation_data=(x_test, y_test),
        verbose=2,
    )
		</comment>
		<comment id='13' author='KANGRuipeng' date='2020-01-10T08:49:06Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/anj-s&gt;@anj-s&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensorflowbutler&gt;@tensorflowbutler&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
  The up two replies are the net-defination code and the training script.
		</comment>
		<comment id='14' author='KANGRuipeng' date='2020-01-16T09:24:04Z'>
		Is there any one following this issue?
		</comment>
		<comment id='15' author='KANGRuipeng' date='2020-02-01T06:12:41Z'>
		&lt;denchmark-link:https://github.com/KANGRuipeng&gt;@KANGRuipeng&lt;/denchmark-link&gt;
 I am unable to repro this error. If you can share a colab link to the code and error message we can take a look. Also I have seen this message as a warning but not an error so I am surprised that this was a hard error. You can also try moving to TF 2 is that is possible.
		</comment>
		<comment id='16' author='KANGRuipeng' date='2020-02-01T06:12:42Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35384&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35384&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='17' author='KANGRuipeng' date='2020-02-05T07:40:29Z'>
		
@KANGRuipeng I am unable to repro this error. If you can share a colab link to the code and error message we can take a look. Also I have seen this message as a warning but not an error so I am surprised that this was a hard error. You can also try moving to TF 2 is that is possible.

Is there any report performance issue of TF1.15 under distributed mode?
		</comment>
	</comments>
</bug>