<bug id='40032' author='orsharir' open_date='2020-05-31T18:36:13Z' closed_time='2020-07-28T00:31:58Z'>
	<summary>Regression on tf-nightly when using Cloud TPU and writing to GCS</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 4.19.118-2
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): tf-nightly==2.3.0.dev20200531
Python version: 3.7.3
GPU model and memory: v3-8 TPU on Google Cloud


Following the &lt;denchmark-link:https://cloud.google.com/tpu/docs/quickstart&gt;MNIST with TPUs tutorial&lt;/denchmark-link&gt;
 using tf-nightly + tf-models-nightly I get the following exception when training the model with a TPU (see full log at end of issue):


Using a CPU/GPU the above code works fine.
Using TF=2.2.0 and its respective TF-Models' tag works fine as well.

I should stress that I'm linking to the tutorial because it's a simple script that reproduces the issue. This also affects my own project, and I have spent the last 24 hours going through my own code before thinking that it might be a bug in tensorflow / TPU drivers. I would also like to add that it might still be a misconfiguration on my part related to my Google Cloud setup (though I've already went through the basics of setting the correct permissions), but even if that's the case I believe TF's exceptions should be more relevant than the cryptic "Location header not returned" string, which seems too low-level.
Looking through other issues, the only one in the past that seems relevant is &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29304&gt;#29304&lt;/denchmark-link&gt;
, though there the exception is always thrown when trying to write to the bucket, while here it's only when using TPUs (as mentioned above, it works fine with CPU/GPU).
Standalone code to reproduce the issue
See above tutorial for reproducing error while using the latest tf-nightly and tf-models-nightly.
Other info / logs Include any logs or source code that would be helpful to
Full log when running mnist_main.py (replacing my actual bucket name with ):
&lt;denchmark-code&gt;(venv) or@instance-6-tf-nightly:~/models/official/vision/image_classification$ python3 mnist_main.py \
&gt;   --tpu=$TPU_NAME \
&gt;   --model_dir=$MODEL_DIR \
&gt;   --data_dir=$DATA_DIR \
&gt;   --train_epochs=10 \
&gt;   --distribution_strategy=tpu \
&gt;   --download
2020-05-31 18:12:25.684123: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2020-05-31 18:12:25.684174: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
I0531 18:12:27.662264 139671896655680 transport.py:157] Attempting refresh to obtain initial access_token
I0531 18:12:27.766179 139671896655680 transport.py:157] Attempting refresh to obtain initial access_token
2020-05-31 18:12:27.836011: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2020-05-31 18:12:27.836062: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)
2020-05-31 18:12:27.836083: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (instance-6-tf-nightly): /proc/driver/nvidia/version does not exist
I0531 18:12:27.865279 139671896655680 transport.py:157] Attempting refresh to obtain initial access_token
I0531 18:12:27.951599 139671896655680 transport.py:157] Attempting refresh to obtain initial access_token
I0531 18:12:28.011914 139671896655680 remote.py:218] Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0
2020-05-31 18:12:28.012461: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-05-31 18:12:28.020191: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz
2020-05-31 18:12:28.020458: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x59b9fe0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-31 18:12:28.020490: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-05-31 18:12:28.030120: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; 10.149.197.2:8470}
2020-05-31 18:12:28.030185: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -&gt; {0 -&gt; localhost:34872}
2020-05-31 18:12:48.223171: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; 10.149.197.2:8470}
2020-05-31 18:12:48.223227: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -&gt; {0 -&gt; localhost:34872}
2020-05-31 18:12:48.223761: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:422] Started server with target: grpc://localhost:34872
INFO:tensorflow:Initializing the TPU system: node-2-test
I0531 18:12:48.225129 139671896655680 tpu_strategy_util.py:72] Initializing the TPU system: node-2-test
INFO:tensorflow:Clearing out eager caches
I0531 18:12:48.369003 139671896655680 tpu_strategy_util.py:100] Clearing out eager caches
INFO:tensorflow:Finished initializing TPU system.
I0531 18:12:54.483889 139671896655680 tpu_strategy_util.py:123] Finished initializing TPU system.
I0531 18:12:54.511550 139671896655680 transport.py:157] Attempting refresh to obtain initial access_token
I0531 18:12:54.605852 139671896655680 transport.py:157] Attempting refresh to obtain initial access_token
INFO:tensorflow:Found TPU system:
I0531 18:12:54.659763 139671896655680 tpu_system_metadata.py:159] Found TPU system:
INFO:tensorflow:*** Num TPU Cores: 8
I0531 18:12:54.659997 139671896655680 tpu_system_metadata.py:160] *** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Workers: 1
I0531 18:12:54.660263 139671896655680 tpu_system_metadata.py:161] *** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
I0531 18:12:54.660351 139671896655680 tpu_system_metadata.py:163] *** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)
I0531 18:12:54.660465 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
I0531 18:12:54.660613 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)
I0531 18:12:54.660671 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)
I0531 18:12:54.660733 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)
I0531 18:12:54.660809 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)
I0531 18:12:54.660887 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)
I0531 18:12:54.660967 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)
I0531 18:12:54.661044 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)
I0531 18:12:54.661120 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)
I0531 18:12:54.661195 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)
I0531 18:12:54.661268 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)
I0531 18:12:54.661341 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
I0531 18:12:54.661427 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
I0531 18:12:54.974422 139671896655680 dataset_info.py:430] Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: mnist/3.0.1
I0531 18:12:55.118813 139671896655680 dataset_info.py:361] Load dataset info from /tmp/tmpq933cjqntfds
I0531 18:12:55.120393 139671896655680 dataset_info.py:401] Field info.citation from disk and from code do not match. Keeping the one from code.
I0531 18:12:55.181865 139671896655680 dataset_builder.py:333] Generating dataset mnist (gs://&lt;bucket name&gt;/data/mnist/3.0.1)
Downloading and preparing dataset mnist/3.0.1 (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to gs://&lt;bucket name&gt;/data/mnist/3.0.1...
W0531 18:12:56.779903 139671896655680 dataset_builder.py:357] Dataset mnist is hosted on GCS. It will automatically be downloaded to your
local data directory. If you'd instead prefer to read directly from our public
GCS bucket (recommended if you're running on GCP), you can instead pass
`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.

Dl Completed...: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:00&lt;00:00,  4.35 file/s]
Dl Completed...: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:00&lt;00:00,  5.19 file/s]
I0531 18:12:57.596850 139671896655680 dataset_info.py:361] Load dataset info from gs://&lt;bucket name&gt;/data/mnist/3.0.1.incompleteIAR91Z
I0531 18:12:57.898775 139671896655680 dataset_info.py:401] Field info.citation from disk and from code do not match. Keeping the one from code.
Dataset mnist downloaded and prepared to gs://&lt;bucket name&gt;/data/mnist/3.0.1. Subsequent calls will reuse this data.
I0531 18:12:58.717931 139671896655680 dataset_builder.py:478] Constructing tf.data.Dataset for split ['train', 'test'], from gs://&lt;bucket name&gt;/data/mnist/3.0.1
2020-05-31 18:12:59.741571: I tensorflow/core/profiler/lib/profiler_session.cc:163] Profiler session started.
Traceback (most recent call last):
  File "mnist_main.py", line 171, in &lt;module&gt;
    app.run(main)
  File "/home/or/models/venv/lib/python3.7/site-packages/absl/app.py", line 299, in run
    _run_main(main, args)
  File "/home/or/models/venv/lib/python3.7/site-packages/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "mnist_main.py", line 164, in main
    stats = run(flags.FLAGS)
  File "mnist_main.py", line 135, in run
    validation_freq=flags_obj.epochs_between_evals)
  File "/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py", line 1066, in fit
    steps=data_handler.inferred_steps)
  File "/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py", line 226, in __init__
    self.set_model(model)
  File "/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py", line 277, in set_model
    callback.set_model(model)
  File "/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py", line 1951, in set_model
    self._write_keras_model_graph()
  File "/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py", line 1989, in _write_keras_model_graph
    summary_ops_v2.keras_model('keras', self.model, step=0)
  File "/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py", line 1155, in keras_model
    metadata=summary_metadata)
  File "/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py", line 681, in write
    _should_record_summaries_v2(), record, _nothing, name="summary_cond")
  File "/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/framework/smart_cond.py", line 51, in smart_cond
    pred_value = smart_constant_value(pred)
  File "/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/framework/smart_cond.py", line 75, in smart_constant_value
    pred_value = tensor_util.constant_value(pred)
  File "/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py", line 829, in constant_value
    return tensor.numpy()
  File "/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py", line 1071, in numpy
    maybe_arr = self._numpy()  # pylint: disable=protected-access
  File "/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py", line 1039, in _numpy
    six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access
  File "&lt;string&gt;", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: Unexpected response from GCS when writing to gs://&lt;bucket name&gt;/mnist/: 'Location' header not returned.
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/eager/context.py", line 2270, in async_wait
    context().sync_executors()
  File "/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/eager/context.py", line 652, in sync_executors
    pywrap_tfe.TFE_ContextSyncExecutors(self._context_handle)
tensorflow.python.framework.errors_impl.InternalError: Unexpected response from GCS when writing to gs://&lt;bucket name&gt;/mnist/: 'Location' header not returned.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='orsharir' date='2020-07-13T23:27:12Z'>
		This is fixed with latest tf-nightly 2.4.0-dev20200712 version
Please can you confirm at your end? Thanks!
		</comment>
		<comment id='2' author='orsharir' date='2020-07-20T23:52:03Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='3' author='orsharir' date='2020-07-28T00:31:57Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='4' author='orsharir' date='2020-07-28T00:32:00Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40032&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40032&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>