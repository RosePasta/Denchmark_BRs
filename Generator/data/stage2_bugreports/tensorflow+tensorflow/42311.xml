<bug id='42311' author='micahreich' open_date='2020-08-13T09:08:00Z' closed_time='2020-09-02T16:22:21Z'>
	<summary>Lambda layer with custom function gives gradient error</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): pip install
TensorFlow version (use command below): 2.2.0
Python version: 3.5.2
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.2
GPU model and memory: Geforce RTX 2080

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"

Describe the current behavior
I have created a GAN style model with a weightless transformation layer as the final layer in the generator. This layer is a Lambda layer, and the function it calls uses only Tensorflow ops. When running a training loop using train_on_batch, I get the following error (the layers mentioned in the error are layers of the generator):
&lt;denchmark-code&gt;ValueError: No gradients provided for any variable: ['dense_1/kernel:0', 'dense_1/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0'].
&lt;/denchmark-code&gt;

The model is meant to take in latent noise and produce 4 outputs corresponding to two (x, y) pairs representing the top left and bottom right corners of a square on a canvas. The Lambda layer converts these coordinates into an image of a square which is then fed into the discriminator.
Describe the expected behavior
I expect the model to be able to train using my training loop with no issues and to be able to use the custom Lambda function / layer that I wrote to transform the Dense layer outputs into an image Tensor.
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
I have included two main GitHub gists with my code. One file is the training file and the other builds the GAN models:
Train.py: &lt;denchmark-link:https://gist.github.com/micahreich/13208980a22be2f53e3f8eba05c0db72&gt;https://gist.github.com/micahreich/13208980a22be2f53e3f8eba05c0db72&lt;/denchmark-link&gt;

Models.py: &lt;denchmark-link:https://gist.github.com/micahreich/80e44710417707bf3fb6327cb9946b79&gt;https://gist.github.com/micahreich/80e44710417707bf3fb6327cb9946b79&lt;/denchmark-link&gt;

It should be noted that the train.py training file loads a numpy file which contains the dataset. If you want to create some data, use the data loader: &lt;denchmark-link:https://gist.github.com/micahreich/fde2024365ec91c818af6e5203d4f98e&gt;https://gist.github.com/micahreich/fde2024365ec91c818af6e5203d4f98e&lt;/denchmark-link&gt;
 (change n_samples to a small number for testing)
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
The full error traceback is below:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "train.py", line 95, in &lt;module&gt;
    TL.train()
  File "train.py", line 83, in train
    g_loss = self.gan.train_on_batch(noise, valid)
  File "/nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py", line 1348, in train_on_batch
    logs = train_function(iterator)
  File "/nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/eager/def_function.py", line 580, in __call__
    result = self._call(*args, **kwds)
  File "/nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/eager/def_function.py", line 627, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File "/nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/eager/def_function.py", line 506, in _initialize
    *args, **kwds))
  File "/nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/eager/function.py", line 2446, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File "/nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/eager/function.py", line 2777, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/eager/function.py", line 2667, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File "/nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/framework/func_graph.py", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/eager/def_function.py", line 441, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "/nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/framework/func_graph.py", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    /nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    /nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/distribute/mirrored_strategy.py:770 _call_for_each_replica
        fn, args, kwargs)
    /nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/distribute/mirrored_strategy.py:201 _call_for_each_replica
        coord.join(threads)
    /nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py:389 join
        six.reraise(*self._exc_info_to_raise)
    /nethome/mreich8/.local/lib/python3.5/site-packages/six.py:703 reraise
        raise value
    /nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py:297 stop_on_exception
        yield
    /nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/distribute/mirrored_strategy.py:998 run
        self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
    /nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py:541 train_step  **
        self.trainable_variables)
    /nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py:1804 _minimize
        trainable_variables))
    /nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:521 _aggregate_gradients
        filtered_grads_and_vars = _filter_grads(grads_and_vars)
    /nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1219 _filter_grads
        ([v.name for _, v in grads_and_vars],))

    ValueError: No gradients provided for any variable: ['dense_1/kernel:0', 'dense_1/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0'].
&lt;/denchmark-code&gt;

Thank you.
	</description>
	<comments>
		<comment id='1' author='micahreich' date='2020-08-13T10:07:57Z'>
		I believe that the issue is caused by the fact that the Lambda layer isn't differentiable because of the tf ops used within the layer -- is this correct? If so, how can I either bypass this or find a workaround?
		</comment>
		<comment id='2' author='micahreich' date='2020-08-16T05:44:00Z'>
		&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 I'm now able to compile the model and get it to start training, but the loss does not change -- something in the Lambda layer is preventing the gradients from updating or changing... how can I fix this? I tried adding @tf.custom_gradient decorator and setting the gradient to be 1.0 and 0.0, but no change. Any advice?
		</comment>
		<comment id='3' author='micahreich' date='2020-08-19T14:42:02Z'>
		&lt;denchmark-link:https://github.com/micahreich&gt;@micahreich&lt;/denchmark-link&gt;

Please provide with latest indented code for me to replicate the issue, or if possible share a colab gist with the error.
		</comment>
		<comment id='4' author='micahreich' date='2020-08-26T15:25:42Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='5' author='micahreich' date='2020-09-02T16:22:19Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='6' author='micahreich' date='2020-09-02T16:22:23Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42311&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42311&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='micahreich' date='2020-11-10T07:09:35Z'>
		
@Saduf2019 I'm now able to compile the model and get it to start training, but the loss does not change -- something in the Lambda layer is preventing the gradients from updating or changing... how can I fix this? I tried adding @tf.custom_gradient decorator and setting the gradient to be 1.0 and 0.0, but no change. Any advice?

Hi, what are the changes you have made to start the training?
		</comment>
	</comments>
</bug>