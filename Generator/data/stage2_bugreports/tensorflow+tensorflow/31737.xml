<bug id='31737' author='anferico' open_date='2019-08-18T09:37:58Z' closed_time='2019-08-29T17:36:47Z'>
	<summary>Unable to learn model weights using `tf.nn.nce_loss`</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): KDE Neon 5.16 (based on Ubuntu 18.04)
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
TensorFlow installed from (source or binary): source
TensorFlow version: 1.12.0 (GIT: v1.12.0-0-ga6d8ffae09)
Python version: 3.6.8
Bazel version (if compiling from source): 0.19.1
GCC/Compiler version (if compiling from source): GCC 8.0.1 20180414 (experimental) [trunk revision 259383]
CUDA/cuDNN version: CUDA=10.0, cuDNN=7.4.2
GPU model and memory: NVIDIA GeForce GTX 1050 Ti Max-Q, with 4GB of memory


I was trying to build a word2vec model by following &lt;denchmark-link:https://www.tensorflow.org/tutorials/representation/word2vec&gt;this TensorFlow tutorial&lt;/denchmark-link&gt;
. I've changed the code little bit, mainly in the parts where I have to load training data. The problem is that the model won't learn anything. The loss keeps fluctuating up and down, and the weights of my network stay constant throughout the training process (I've checked this using TensorBoard). I am convinced there's something wrong with either the code posted in the tutorial or with the  function, as I don't see any problem with the code I wrote.
Describe the expected behavior
I'm expecting the model to learn something - which doesn't mean that I'm expecting it to achieve a specific accuracy. In other words, I'm expecting the network's weights to be updated after a training step.
Code to reproduce the issue
import tensorflow as tf
import numpy as np

vocabulary_size = 13046
embedding_size = 256
num_noise = 1
learning_rate = 1e-3
batch_size = 1024
epochs = 10

def make_hparam_string(embedding_size, num_noise, learning_rate, batch_size, epochs):
    return f'es={embedding_size}_nn={num_noise}_lr={learning_rate}_bs={batch_size}_e={epochs}'

# These are the hidden layer weights
embeddings = tf.get_variable(name='embeddings', initializer=tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), trainable=True)

# 'nce' stands for 'Noise-contrastive estimation' and represents a particular loss function.
# Check https://www.tensorflow.org/tutorials/representation/word2vec for more details.
# 'nce_weights' and 'nce_biases' are simply the output weights and biases.
# NOTE: for some reason, even though output weights will have shape (embedding_size, vocabulary_size),
#       we have to initialize them with the shape (vocabulary_size, embedding_size)
nce_weights = tf.get_variable(name='output_weights',
                              initializer=tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / np.sqrt(embedding_size)), 
                              trainable=True)
nce_biases = tf.get_variable(name='output_biases', initializer=tf.constant_initializer(0.1), shape=[vocabulary_size], trainable=True)

# Placeholders for inputs
train_inputs = tf.placeholder(tf.int32, shape=[None])    # [batch_size]
train_labels = tf.placeholder(tf.int32, shape=[None, 1]) # [batch_size, 1]

# This allows us to quickly retrieve the corresponding word embeddings for each word in 'train_inputs'
matched_embeddings = tf.nn.embedding_lookup(embeddings, train_inputs)

# Compute the NCE loss, using a sample of the negative labels each time.
loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,
                                     biases=nce_biases,
                                     labels=train_labels,
                                     inputs=matched_embeddings,
                                     num_sampled=num_noise,
                                     num_classes=vocabulary_size))

# Use the SGD optimizer to minimize the loss function
optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)

# Add some summaries for TensorBoard
loss_summary = tf.summary.scalar('nce_loss', loss)
input_embeddings_summary = tf.summary.histogram('input_embeddings', embeddings)
output_embeddings_summary = tf.summary.histogram('output_embeddings', nce_weights)

################################################################################

# Load data
target_words = np.genfromtxt('target_words.txt', dtype=int, delimiter='\n').reshape((-1, 1))
context_words = np.genfromtxt('context_words.txt', dtype=int, delimiter='\n').reshape((-1, 1))

# Convert to tensors
target_words_tensor = tf.convert_to_tensor(target_words)
context_words_tensor = tf.convert_to_tensor(context_words)

# Create a tf.data.Dataset object representing our dataset
dataset = tf.data.Dataset.from_tensor_slices((target_words_tensor, context_words_tensor))
dataset = dataset.shuffle(buffer_size=target_words.shape[0])
dataset = dataset.batch(batch_size)

# Create an iterator to iterate over the dataset
iterator = dataset.make_initializable_iterator()
next_batch = iterator.get_next()

# Train the model
with tf.Session() as session:

    # Initialize variables
    session.run( tf.global_variables_initializer() )

    merged_summary = tf.summary.merge_all()

    # File writer for TensorBoard
    hparam_string = make_hparam_string(embedding_size, num_noise, learning_rate, batch_size, epochs)
    loss_writer = tf.summary.FileWriter(f'./tensorboard/{hparam_string}')

    global_step = 0
    for epoch in range(epochs):

        session.run(iterator.initializer)
        while True:
            try:
                inputs, labels = session.run(next_batch)

                feed_dict = {train_inputs: inputs[:, 0], train_labels: labels}
                _, cur_loss, all_summaries = session.run([optimizer, loss, merged_summary], feed_dict=feed_dict)

                # Write sumaries to disk
                loss_writer.add_summary(all_summaries, global_step=global_step)
                global_step += 1

                print(f'Current loss: {cur_loss}')

            except tf.errors.OutOfRangeError:
                print(f'Finished epoch {epoch}.')
                break

I'm attaching the training samples (target_words.txt) and the corresponding labels (context_words.txt) in case you wanted to reproduce this issue.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3512860/target_words.txt&gt;target_words.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3512859/context_words.txt&gt;context_words.txt&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='anferico' date='2019-08-19T08:49:29Z'>
		Issue replicating for TF version-1.12,please find the &lt;denchmark-link:https://colab.sandbox.google.com/drive/1wUjGwmsR0NaL4VaT1Jtz05vzwi_1PfIQ#scrollTo=SBTDGV0TCr5u&gt;gist&lt;/denchmark-link&gt;
 of colab. Thanks
		</comment>
		<comment id='2' author='anferico' date='2019-08-19T09:00:52Z'>
		Are you going to replicate the issue by yourself or do you want me to do that for you?
		</comment>
		<comment id='3' author='anferico' date='2019-08-20T21:49:26Z'>
		The tutorial you are referencing was built and tested with TF 2.X version.
You may try using TF 2.X
		</comment>
		<comment id='4' author='anferico' date='2019-08-21T10:05:48Z'>
		Well, this is kind of a makeshift solution. Would you be able to tell me whether tf.nn.nce_loss is bugged or not in TF 1.12?
		</comment>
		<comment id='5' author='anferico' date='2019-08-23T18:07:03Z'>
		I tried your code snippet with latest version of tf-nightly ('1.15.0-dev20190821') and was able to execute it successfully.
I cross checked nce_loss function with TF  version 1.12 and TF version 1.14. There isn't any change in its definition. Thus doesn't look like a root cause in this scenario.
		</comment>
		<comment id='6' author='anferico' date='2019-08-23T22:21:39Z'>
		What do you mean by "executing that successfully"? I was able to execute it successfully as well, but the problem was that the model wasn't learning anything, and the loss function kept oscillating up and down instead of decreasing with each training step. That's the whole point of this issue
		</comment>
		<comment id='7' author='anferico' date='2019-08-29T17:36:47Z'>
		There is still a fair amount of isolation and debugging that needs to happen here before we can call this a bug. This question is better asked on &lt;denchmark-link:http://stackoverflow.com/questions/tagged/tensorflow&gt;StackOverflow&lt;/denchmark-link&gt;
 since it is not a bug or feature request. There is also a larger community that reads questions there.
If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/new&gt;issue template&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='8' author='anferico' date='2019-08-29T17:36:49Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=31737&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=31737&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>