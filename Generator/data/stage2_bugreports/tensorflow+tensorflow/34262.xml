<bug id='34262' author='honeytidy' open_date='2019-11-14T06:51:35Z' closed_time='2019-12-12T19:24:33Z'>
	<summary>train with muliple gpu get error: Out of range:  End of sequence</summary>
	<description>
I am training with tensorflow2.0 with multiple GPU. It got the following errors. But if I use only one GPU it ran without any error. My tensorflow version is tensorflow-gpu-2.0.0:
&lt;denchmark-code&gt;tensorflow.python.framework.errors_impl.CancelledError: 4 root error(s) found.
  (0) Cancelled:  Operation was cancelled
     [[{{node cond_6/else/_59/IteratorGetNext}}]]
  (1) Out of range:  End of sequence
     [[{{node cond_4/else/_37/IteratorGetNext}}]]
  (2) Out of range:  End of sequence
     [[{{node cond_7/else/_70/IteratorGetNext}}]]
     [[metrics/accuracy/div_no_nan/ReadVariableOp_6/_154]]
  (3) Out of range:  End of sequence
     [[{{node cond_7/else/_70/IteratorGetNext}}]]
0 successful operations.
1 derived errors ignored. [Op:__inference_distributed_function_83325]

Function call stack:
distributed_function -&gt; distributed_function -&gt; distributed_function -&gt; distributed_function
&lt;/denchmark-code&gt;

and this is my code:
&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow_datasets as tfds

data_name = 'uc_merced'
dataset = tfds.load(data_name)
train_data, test_data = dataset['train'], dataset['train']

def parse(img_dict):
    img = tf.image.resize_with_pad(img_dict['image'], 256, 256)
    label = img_dict['label']
    return img, label

train_data = train_data.map(parse)
train_data = train_data.batch(96)

test_data = test_data.map(parse)
test_data = test_data.batch(96)

strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    model = tf.keras.applications.ResNet50(weights=None, classes=21, input_shape=(256, 256, 3))
    model.compile(optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy'])


model.fit(train_data, epochs=50, verbose=2, validation_data=test_data)
model.save('model/resnet_{}.h5'.format(data_name))
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='honeytidy' date='2019-11-17T00:17:36Z'>
		Can you please provide the full stack trace so we can debug this further? How many GPUs did you run this with?
Also can you test with the latest TF 2 nightly?
(For reference, I tried with the latest TF and not able to repro)
		</comment>
		<comment id='2' author='honeytidy' date='2019-11-18T02:06:26Z'>
		I ran with 4 GPUs. This thefull stack trace:
&lt;denchmark-code&gt;Epoch 1/50
22/22 - 78s - loss: 3.4754 - accuracy: 0.2224 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00
Epoch 2/50
22/22 - 9s - loss: 2.1286 - accuracy: 0.3386
Traceback (most recent call last):
  File "simple.py", line 76, in &lt;module&gt;
    model.fit(train_data, epochs=50, verbose=2, validation_data=test_data)
  File "/home/soft/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py", line 728, in fit
    use_multiprocessing=use_multiprocessing)
  File "/home/soft/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 370, in fit
    total_epochs=1)
  File "/home/soft/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 123, in run_one_epoch
    batch_outs = execution_function(iterator)
  File "/home/soft/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", line 86, in execution_function
    distributed_function(input_fn))
  File "/home/soft/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 457, in __call__
    result = self._call(*args, **kwds)
  File "/home/soft/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 494, in _call
    results = self._stateful_fn(*args, **kwds)
  File "/home/soft/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1823, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/home/soft/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1141, in _filtered_call
    self.captured_inputs)
  File "/home/soft/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1224, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
  File "/home/soft/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 511, in call
    ctx=ctx)
  File "/home/soft/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "&lt;string&gt;", line 3, in raise_from
tensorflow.python.framework.errors_impl.CancelledError: 5 root error(s) found.
  (0) Cancelled:  Operation was cancelled
	 [[{{node cond_7/else/_70/IteratorGetNext}}]]
  (1) Out of range:  End of sequence
	 [[{{node cond_6/else/_59/IteratorGetNext}}]]
	 [[replica_3/metrics/accuracy/AssignAddVariableOp_1/_103]]
  (2) Out of range:  End of sequence
	 [[{{node cond_5/else/_48/IteratorGetNext}}]]
  (3) Out of range:  End of sequence
	 [[{{node cond_6/else/_59/IteratorGetNext}}]]
	 [[metrics/accuracy/div_no_nan/ReadVariableOp_4/_150]]
  (4) Out of range:  End of sequence
	 [[{{node cond_6/else/_59/IteratorGetNext}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_distributed_function_83325]

Function call stack:
distributed_function -&gt; distributed_function -&gt; distributed_function -&gt; distributed_function -&gt; distributed_function
&lt;/denchmark-code&gt;

I will try tf-nightly later.
		</comment>
		<comment id='3' author='honeytidy' date='2019-11-18T12:20:38Z'>
		&lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
 I can not repro for tf-nightly-gpu. But for tensorflow-gpu-2.0 I reproduce the error every time. Is this a bug of tensorflow2.0?
		</comment>
		<comment id='4' author='honeytidy' date='2019-11-26T07:37:13Z'>
		&lt;denchmark-link:https://github.com/honeytidy&gt;@honeytidy&lt;/denchmark-link&gt;
 that's possible, we've fixed a number of issues since TF 2.0 branch was cut.
		</comment>
		<comment id='5' author='honeytidy' date='2019-12-12T19:24:35Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34262&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34262&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>