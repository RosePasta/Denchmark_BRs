<bug id='26812' author='galeone' open_date='2019-03-17T21:19:19Z' closed_time='2019-03-19T21:40:49Z'>
	<summary>Can't declare tf.Variable in @tf.function decorated function</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Archlinux
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.0.0-dev20190317
Python version: 3.6

Describe the current behavior
A function that correctly works in eager execution can't be decorated with @tf.function if declares a tf.Variable in the function body.
The error message, reported below, is misleading since it talks about a non-first invocation when the function is invoked only once.
&lt;denchmark-code&gt;ValueError: tf.function-decorated function tried to create variables on non-first call.
&lt;/denchmark-code&gt;

Describe the expected behavior
Calling a function decorated with the @tf.function should produce the same output as the same function without the decoration.
Code to reproduce the issue
import tensorflow as tf
import tensorflow as tf

@tf.function
def f():
    a = tf.constant([[10, 10], [11., 1.]])
    x = tf.constant([[1., 0.], [0., 1.]])
    b = tf.Variable(12.)
    y = tf.matmul(a, x) + b
    return y

print(f())
	</description>
	<comments>
		<comment id='1' author='galeone' date='2019-03-18T01:17:03Z'>
		see &lt;denchmark-link:https://github.com/tensorflow/community/blob/master/rfcs/20180918-functions-not-sessions-20.md#functions-that-create-state&gt;this&lt;/denchmark-link&gt;

and rewrite your code to
&lt;denchmark-code&gt;import tensorflow as tf

b = None


@tf.function
def f():
    a = tf.constant([[10, 10], [11., 1.]])
    x = tf.constant([[1., 0.], [0., 1.]])
    global b
    if b is None:
        b = tf.Variable(12.)
    y = tf.matmul(a, x) + b
    return y

print(f())
&lt;/denchmark-code&gt;

or
&lt;denchmark-code&gt;class F(object):
    def __init__(self):
        self.b = None

    @tf.function
    def __call__(self):
        a = tf.constant([[10, 10], [11., 1.]])
        x = tf.constant([[1., 0.], [0., 1.]])
        if self.b is None:
            self.b = tf.Variable(12.)
        y = tf.matmul(a, x) + self.b
        return y


f = F()
print(f())
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='galeone' date='2019-03-18T07:44:36Z'>
		HI &lt;denchmark-link:https://github.com/zakizhou&gt;@zakizhou&lt;/denchmark-link&gt;
 ! I'm aware that defining a function that declares a variable inside creates a status and it has to be handled differently.
As you suggested an alternative I to wrap the function in a class and make the variable a private attribute, or declaring the variable as global and check if it is None.
However, as stated in the RFC:

State (like tf.Variable objects) are only created the first time the function f is called.

Thus, when I invoke f() for the first time, I should have the graph definition with a tf.Variable object created and executed once. And it should work according to what is written in the RFC.
Instead, what happens is that even though I call the function only once, and thus this is the first call, it seems like tf.function is evaluating the function twice (like calling f(), f()), making what is stated in the RFC false.
		</comment>
		<comment id='3' author='galeone' date='2019-03-19T21:40:49Z'>
		tf.function may evaluate your python function more than once.
What the RFC states instead is that you are allowed to create variables as long as variable creation only happens the first time your python function is evaluated.
		</comment>
		<comment id='4' author='galeone' date='2019-03-19T21:41:56Z'>
		And the reason for this is that if you write python code which unconditionally creates variables then I can't tell whether you mean to create a new variable every time the python function is called (eager behavior) or reuse the existing variables (what happens in graph tf 1.x) so an error felt safer.
		</comment>
		<comment id='5' author='galeone' date='2019-03-20T17:25:17Z'>
		Thank you for the explanation &lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 - now everything is clearer. The RFC just describes the first call, but there is no guarantee that tf.function won't call the function more than once while converting it as a graph and for this reason, the developer should take care of handling the variables creation; thus If I need to reuse or not a variable it makes no difference, I have to take care of its status manually.
		</comment>
		<comment id='6' author='galeone' date='2019-03-20T17:38:47Z'>
		Yes, exactly. With TF 2.0 in general we're trying to get TF out of your way
as much as possible, instead of pushing every possible thing you'd want to
do inside TF. We think this is simpler, more modular, and easier to
integrate with larger codebases.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Wed, Mar 20, 2019 at 10:29 AM Paolo Galeone ***@***.***&gt; wrote:
 Thank you for the explanation @alextp &lt;https://github.com/alextp&gt; - now
 everything is clearer. The RFC just describes the first call, but there is
 no guarantee that tf.function won't call the function more than once while
 converting it as a graph and for this reason, the developer should take
 care of handling the variables creation; thus If I need to reuse or not a
 variable it makes no difference, I have to take care of its status manually.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#26812 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAATxSQYBidgnZS8sTPjSbwXUWcZqKmLks5vYm_rgaJpZM4b4pkS&gt;
 .


-- 
 - Alex

		</comment>
		<comment id='7' author='galeone' date='2019-03-22T12:22:21Z'>
		I have an exact issue like this, but with using different optimizers:
&lt;denchmark-code&gt;@tf.function
def apply_gradients_once(optimizer, grads, vars):
    optimizer.apply_gradients(zip(
        grads, vars))


def apply_gradients(self, use_fast, grads_per_model):
    for i in range(self.nmodels):
        grads = grads_per_model[i]
        vars = self.model[i].get_trainable_variables()

        if use_fast[i]:
            optimizer = self.fast_optimizer
            apply_gradients_once(optimizer, grads, vars)
        else:
            optimizer = self.slow_optimizer
            apply_gradients_once(optimizer, grads, vars)

&lt;/denchmark-code&gt;

On a first epoch, I have use_fast = [True, False], and then on a second one I have
use_fast = [False, True].
With this, I have a huge error trace on the apply_gradients_once call, ending with ValueError: tf.function-decorated function tried to create variables on non-first call..
From my understanding, since the function is always called with different arguments, and there are no external dependencies, this error should not be happening.
		</comment>
		<comment id='8' author='galeone' date='2019-03-25T10:07:24Z'>
		&lt;denchmark-link:https://github.com/ericpts&gt;@ericpts&lt;/denchmark-link&gt;
 this is an interesting use case I hadn't thought of.
As a workaround I recommend you use two instances of tf.function here.
Let's open another issue to discuss this?
		</comment>
		<comment id='9' author='galeone' date='2019-03-25T10:25:03Z'>
		One alternative workaround is to make the boolean parameter a tensor; then
TF will trace both sides of the conditional once and pre-create all
variables.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, Mar 25, 2019 at 4:11 AM Alexandre Passos ***@***.***&gt; wrote:
 @ericpts &lt;https://github.com/ericpts&gt; this is an interesting use case I
 hadn't thought of.

 As a workaround I recommend you use two instances of tf.function here.

 Let's open another issue to discuss this?

 —
 You are receiving this because you are subscribed to this thread.
 Reply to this email directly, view it on GitHub
 &lt;#26812 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAATxWcd8Q8-bfQrUYBDZzMCgTVMmbE_ks5vaKDWgaJpZM4b4pkS&gt;
 .


-- 
 - Alex

		</comment>
		<comment id='10' author='galeone' date='2019-11-22T20:31:31Z'>
		Hello, Is there a workaround for a situation like this. Here I need to create several w, b variables using add_layer method. But I get the variable reused error.
def add_layer(input, c_kdimension, c_kstrides):
w = tf.get_variable(name="w", shape=c_kdimension, initializer = tf.contrib.layers.xavier_initializer()
b = tf.get_variable(name="b", shape=c_kdimension[-1], initializer=tf.contrib.layers.xavier_initializer())
return tf.nn.conv2d(input, w, strides=c_kstrides, padding="SAME")
@tf.function
def inference(input):
model = Model()
layer_1 = add_layer(input, c_kdimension=[3, 3, 1, 32], c_kstrides=[1, 1, 1, 1])
layer_2 = add_layer(layer_1, c_kdimension=[3, 3, 32, 32], c_kstrides=[1, 1, 1, 1])
return layer2
		</comment>
		<comment id='11' author='galeone' date='2019-12-06T08:31:12Z'>
		Hello &lt;denchmark-link:https://github.com/galeone&gt;@galeone&lt;/denchmark-link&gt;
 ,
As you are using TensorFlow 2.0 you must add

after import and then run the code.
		</comment>
		<comment id='12' author='galeone' date='2020-04-27T10:20:25Z'>
		
Hello @galeone ,
As you are using TensorFlow 2.0 you must add
tf.config.experimental_run_functions_eagerly(True)
after import and then run the code.

Sorry, this is wrong, do not use it. This disables the graph construction and runs everything eagerly, like numpy. This can be wanted for debugging or in certain cases, but it is surely not the workaround.
&lt;denchmark-link:https://github.com/praveen-14&gt;@praveen-14&lt;/denchmark-link&gt;
 read again above about creating them as class variables or in the global scope.
		</comment>
	</comments>
</bug>