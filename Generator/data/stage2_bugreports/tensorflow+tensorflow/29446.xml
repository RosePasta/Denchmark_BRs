<bug id='29446' author='captain-pool' open_date='2019-06-05T17:00:42Z' closed_time='2019-06-14T17:18:26Z'>
	<summary>[TF 2.0] CrossShardOptimizer not working with global_step parameter</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Ubuntu 14.04
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): tensorflow==2.0.0.a
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
I'm trying to run CrossShardOptimizer with tf.optimizers.Adam, however, tf.Optimizer.Adam is updated to v2 (with no global_step_parameter), and CrossShardOptimizer is still in version 1 and still have that global_step parameter. So, CrossShardOptimizer is failing Badly.
Describe the expected behavior
The CrossShardOptimizer shouldn't break.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
def model_fn(features, labels, mode, params):
    model = tf.keras.Sequential([
        tf.keras.layers.Flatten(input_shape=[28, 28, 1]),
        tf.keras.layers.Dense(128),
        tf.keras.layers.Dense(10, activation="softmax")
    ])
    optimizer = None
    if mode == tf.estimator.ModeKeys.TRAIN:
        optimizer = tf.optimizers.Adam(params.get("learing_rate", 1e-3))
        if params.get("use_tpu", True):
          optimizer = tpu_optimizer.CrossShardOptimizer(optimizer)

    with tf.GradientTape() as tape:
        logits = model(features)
        if mode == tf.estimator.ModeKeys.PREDICT:
            preds = {
                "predictions": logits
            }
            return tpu_estimator.TPUEstimatorSpec(mode, predictions=preds)
        loss = tf.keras.losses.SparseCategoricalCrossentropy(
            from_logits=True)(labels, logits)
    if mode == tf.estimator.ModeKeys.EVAL:
        return tpu_estimator.TPUEstimatorSpec(mode, loss=loss)

    def train_fn():
        assert optimizer is not None
        gradient = tape.gradient(loss, model.trainable_variables)
        global_step = tf.compat.v1.train.get_global_step()
        update_global_step = tf.compat.v1.assign(global_step, global_step + 1, name='update_global_step')
        with tf.control_dependencies([update_global_step]):
          apply_grads = optimizer.apply_gradients(zip(gradient, model.trainable_variables))
        return apply_grads

    if mode == tf.estimator.ModeKeys.TRAIN:
return tpu_estimator.TPUEstimatorSpec(mode, loss=loss, train_op=train_fn())
The complete code can be found at:
&lt;denchmark-link:https://github.com/captain-pool/tf2.0-tpu-sample/blob/master/image_retraining_tpu.py&gt;https://github.com/captain-pool/tf2.0-tpu-sample/blob/master/image_retraining_tpu.py&lt;/denchmark-link&gt;


This issue gets fixed when:


line is modified to

and updating  manually.
Like this:
&lt;denchmark-link:https://github.com/captain-pool/tf2.0-tpu-sample/blob/39d2af5caf1dfbb28d0821b5143a9c534aef6861/image_retraining_tpu.py#L75-L78&gt;https://github.com/captain-pool/tf2.0-tpu-sample/blob/39d2af5caf1dfbb28d0821b5143a9c534aef6861/image_retraining_tpu.py#L75-L78&lt;/denchmark-link&gt;

Do you want to contribute?
Yes.
	</description>
	<comments>
		<comment id='1' author='captain-pool' date='2019-06-06T10:19:26Z'>
		CC: &lt;denchmark-link:https://github.com/vbardiovskyg&gt;@vbardiovskyg&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/srjoglekar246&gt;@srjoglekar246&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/dynamicwebpaige&gt;@dynamicwebpaige&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='captain-pool' date='2019-06-10T07:53:46Z'>
		&lt;denchmark-link:https://github.com/captain-pool&gt;@captain-pool&lt;/denchmark-link&gt;
 Looks like Complete code link provided above is pointing to error 404.Can you please check and point to the correct link. Thanks!
		</comment>
		<comment id='3' author='captain-pool' date='2019-06-10T08:02:47Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 Done 
		</comment>
		<comment id='4' author='captain-pool' date='2019-06-14T00:31:10Z'>
		I'm not sure it would be a good idea to support this because of the changes in optimizer would make its behavior confusing. It definitely could use a better error message, though, which I'll add.
You can do this instead to sum the gradients across replicas:
&lt;denchmark-code&gt;gradient = [tf.compat.v1.tpu.cross_replica_sum(grad) for grad in gradient]
apply_grads = optimizer.apply_gradients(zip(gradient, model.trainable_variables))
&lt;/denchmark-code&gt;

If you want to average the gradients, you'll need to also scale the loss down (divide by number of replicas).
The reason it would be confusing with OptimizerV2 is:

OptimizerV2 doesn't have compute_gradients, where we rescale the loss.
OptimizerV2 when used with TPUStrategy will already aggregate the gradients across replicas for you, and so it may end up being done twice.

		</comment>
		<comment id='5' author='captain-pool' date='2019-06-14T17:18:26Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/06349f1973ccdd25c9d12ed495c519ebf6f3d797&gt;06349f1&lt;/denchmark-link&gt;

We throw an exception now with the suggested workaround.
		</comment>
		<comment id='6' author='captain-pool' date='2019-06-14T17:18:28Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29446&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29446&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>