<bug id='36764' author='pkan2' open_date='2020-02-14T21:52:45Z' closed_time='2020-12-05T02:51:08Z'>
	<summary>tf.linalg.svd not supporting tf.half as dtype on Windows CPU with python3.5</summary>
	<description>
Hello!
I am currently writing a test file for an optimizer toward tensorflow_addons. I use tf.linalg.svd function in the test file and I test with the case of using tf.half as dtype.
The test can pass the given CI test with Mac CPU python 3.5 and Ubuntu CPU python 3. But for the given CI test on Windows CPU with python3.5, it throws following error message:
"
ERROR:tensorflow:No OpKernel was registered to support Op 'Svd' used by node Svd (defined at \?\C:\Users\RUNNER~1\AppData\Local\Temp\Bazel.runfiles_pt5sug_g\runfiles_main_\tensorflow_addons\optimizers\conditional_gradient_test.py:37) with these attrs: [full_matrices=false, compute_uv=true, T=DT_HALF]
Registered devices: [CPU]
Registered kernels:
device='CPU'; T in [DT_FLOAT]
device='CPU'; T in [DT_DOUBLE]
device='CPU'; T in [DT_COMPLEX64]
device='CPU'; T in [DT_COMPLEX128]
device='GPU'; T in [DT_FLOAT]
device='GPU'; T in [DT_DOUBLE]
&lt;denchmark-code&gt; [[Svd]]
&lt;/denchmark-code&gt;

"
The code I am running with is as following:
"
def top_singular_vector( m):
# handle the case where m is a tensor of rank 0 or rank 1.
n = tf.cond(
tf.equal(tf.rank(m), 0),
lambda: tf.expand_dims(tf.expand_dims(m, [0]), [0]),
lambda: m,
)
n = tf.cond(tf.equal(tf.rank(m), 1), lambda: tf.expand_dims(m, [0]), lambda: n,)
st, ut, vt = tf.linalg.svd(n, full_matrices=False)
m_size = tf.shape(n)
ut = tf.reshape(ut[:, 0], [m_size[0], 1])
vt = tf.reshape(vt[:, 0], [m_size[1], 1])
st = tf.matmul(ut, tf.transpose(vt))
# when we return the top singular vector, we have to remove the
# dimension we have added on
st = tf.cond(
tf.equal(tf.rank(m), 0),
lambda: tf.squeeze(tf.squeeze(st, [0]), [0]),
lambda: st,
)
st = tf.cond(tf.equal(tf.rank(m), 1), lambda: tf.squeeze(st, [0]), lambda: st,)
return st
grads0 = tf.constant([0.1, 0.1], dtype=tf.half)
grads1 = tf.constant([0.01, 0.01], dtype=tf.half)
top_singular_vector0 = top_singular_vector(grads0)
top_singular_vector1 = top_singular_vector(grads1)
"
It will be great if tf.linalg.svd function can have more compatible or similar behavior on dtype over different systems.
Thank you!
	</description>
	<comments>
		<comment id='1' author='pkan2' date='2020-02-17T12:26:23Z'>
		&lt;denchmark-link:https://github.com/pkan2&gt;@pkan2&lt;/denchmark-link&gt;
 Please provide the code snippet  in proper format  as  we are facing indentation error.  Also, please provide the TF version. Thanks!
		</comment>
		<comment id='2' author='pkan2' date='2020-02-17T15:32:04Z'>
		Hello @sai110594 !
The code is as following:
&lt;denchmark-code&gt;def top_singular_vector(m):
        # handle the case where m is a tensor of rank 0 or rank 1.
        n = tf.cond(
            tf.equal(tf.rank(m), 0),
            lambda: tf.expand_dims(tf.expand_dims(m, [0]), [0]),
            lambda: m,
        )
        n = tf.cond(tf.equal(tf.rank(m), 1), lambda: tf.expand_dims(m, [0]), lambda: n,)
        st, ut, vt = tf.linalg.svd(n, full_matrices=False)
        m_size = tf.shape(n)
        ut = tf.reshape(ut[:, 0], [m_size[0], 1])
        vt = tf.reshape(vt[:, 0], [m_size[1], 1])
        st = tf.matmul(ut, tf.transpose(vt))
        # when we return the top singular vector, we have to remove the
        # dimension we have added on
        st = tf.cond(
            tf.equal(tf.rank(m), 0),
            lambda: tf.squeeze(tf.squeeze(st, [0]), [0]),
            lambda: st,
        )
        st = tf.cond(tf.equal(tf.rank(m), 1), lambda: tf.squeeze(st, [0]), lambda: st,)
        return st

grads0 = tf.constant([0.1, 0.1], dtype=tf.half)
grads1 = tf.constant([0.01, 0.01], dtype=tf.half)
top_singular_vector0 = top_singular_vector(grads0)
top_singular_vector1 = top_singular_vector(grads1)
&lt;/denchmark-code&gt;

And I think the version used by the Windows CPU Python3 test is:
&lt;denchmark-code&gt;Collecting tensorflow==2.1.0
Downloading tensorflow-2.1.0-cp35-cp35m-win_amd64.whl (355.8 MB)
&lt;/denchmark-code&gt;

Thank you!
		</comment>
		<comment id='3' author='pkan2' date='2020-02-18T09:10:50Z'>
		I tried to reproduce the issue in colab  with TF 2.1 and didn't face any issues. Please find the &lt;denchmark-link:https://colab.sandbox.google.com/drive/1oI3bNG72gHc7MjnpcwHiv6i3M_jDCVO9#scrollTo=l4K9fKHwfwci&gt;gist&lt;/denchmark-link&gt;
 here. Thanks!
		</comment>
		<comment id='4' author='pkan2' date='2020-02-18T15:20:46Z'>
		Is it possibly caused by the fact the code is tested with graph mode rather than eager execution mode?
Here is the full file:
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow_addons.utils.types import FloatTensorLike

from typeguard import typechecked
from typing import Union, Callable


@tf.keras.utils.register_keras_serializable(package="Addons")
class ConditionalGradient(tf.keras.optimizers.Optimizer):
    """Optimizer that implements the Conditional Gradient optimization.
    This optimizer helps handle constraints well.
    Currently only supports frobenius norm constraint or nuclear norm
    constraint.
    See https://arxiv.org/pdf/1803.06453.pdf
    ```
    variable -= (1-learning_rate) * (variable + lambda_ * gradient
        / (frobenius_norm(gradient) + epsilon))
    ```
    Note that `lambda_` here refers to the constraint "lambda" in
    the paper. `epsilon` is constant with tiny value as compared to
    the value of frobenius norm of gradient. The purpose of `epsilon`
    here is to avoid the case that the value of frobenius norm of
    gradient is 0.
    In this implementation, `epsilon` defaults to $10^{-7}$.
    For nucler norm constraint, the formula is as following:
    ```
    variable -= (1-learning_rate) * (variable
    + lambda_ * top_singular_vector(gradient))
    ```
    """

    @typechecked
    def __init__(
        self,
        learning_rate: Union[FloatTensorLike, Callable],
        lambda_: Union[FloatTensorLike, Callable] = 0.01,
        epsilon: FloatTensorLike = 1e-7,
        ord: str = "fro",
        use_locking: bool = False,
        name: str = "ConditionalGradient",
        **kwargs
    ):
        """Construct a new conditional gradient optimizer.
        Args:
            learning_rate: A `Tensor` or a floating point value. or a schedule
                that is a `tf.keras.optimizers.schedules.LearningRateSchedule`
                The learning rate.
            lambda_: A `Tensor` or a floating point value. The constraint.
            epsilon: A `Tensor` or a floating point value. A small constant
                for numerical stability when handling the case of norm of
                gradient to be zero.
            ord: Order of the norm. Supported values are `'fro'`
                and `'nuclear'`. Default is `'fro'`, which is frobenius norm.
            use_locking: If `True`, use locks for update operations.
            name: Optional name prefix for the operations created when
                applying gradients. Defaults to 'ConditionalGradient'.
            **kwargs: keyword arguments. Allowed to be {`clipnorm`,
                `clipvalue`, `lr`, `decay`}. `clipnorm` is clip gradients
                by norm; `clipvalue` is clip gradients by value, `decay` is
                included for backward compatibility to allow time inverse
                decay of learning rate. `lr` is included for backward
                compatibility, recommended to use `learning_rate` instead.
        """
        super().__init__(name=name, **kwargs)
        self._set_hyper("learning_rate", kwargs.get("lr", learning_rate))
        self._set_hyper("lambda_", lambda_)
        self.epsilon = epsilon or tf.keras.backend.epsilon()
        supported_norms = ["fro", "nuclear"]
        if ord not in supported_norms:
            raise ValueError(
                "'ord' must be a supported matrix norm in %s, got '%s' instead"
                % (supported_norms, ord)
            )
        self.ord = ord
        self._set_hyper("use_locking", use_locking)

    def get_config(self):
        config = {
            "learning_rate": self._serialize_hyperparameter("learning_rate"),
            "lambda_": self._serialize_hyperparameter("lambda_"),
            "epsilon": self.epsilon,
            "ord": self.ord,
            "use_locking": self._serialize_hyperparameter("use_locking"),
        }
        base_config = super().get_config()
        return {**base_config, **config}

    def _create_slots(self, var_list):
        for v in var_list:
            self.add_slot(v, "conditional_gradient")

    def _prepare_local(self, var_device, var_dtype, apply_state):
        super()._prepare_local(var_device, var_dtype, apply_state)
        apply_state[(var_device, var_dtype)]["learning_rate"] = tf.identity(
            self._get_hyper("learning_rate", var_dtype)
        )
        apply_state[(var_device, var_dtype)]["lambda_"] = tf.identity(
            self._get_hyper("lambda_", var_dtype)
        )
        apply_state[(var_device, var_dtype)]["epsilon"] = tf.convert_to_tensor(
            self.epsilon, var_dtype
        )

    def _resource_apply_dense(self, grad, var, apply_state=None):
        def frobenius_norm(m):
            return tf.math.reduce_sum(m ** 2) ** 0.5

        def top_singular_vector(m):
            # handle the case where m is a tensor of rank 0 or rank 1.
            n = tf.cond(
                tf.equal(tf.rank(m), 0),
                lambda: tf.expand_dims(tf.expand_dims(m, [0]), [0]),
                lambda: m,
            )
            n = tf.cond(
                tf.equal(tf.rank(m), 1), lambda: tf.expand_dims(m, [0]), lambda: n,
            )
            st, ut, vt = tf.linalg.svd(n, full_matrices=False)
            m_size = tf.shape(n)
            ut = tf.reshape(ut[:, 0], [m_size[0], 1])
            vt = tf.reshape(vt[:, 0], [m_size[1], 1])
            st = tf.matmul(ut, tf.transpose(vt))
            # when we return the top singular vector, we have to remove the
            # dimension we have added on
            st = tf.cond(
                tf.equal(tf.rank(m), 0),
                lambda: tf.squeeze(tf.squeeze(st, [0]), [0]),
                lambda: st,
            )
            st = tf.cond(
                tf.equal(tf.rank(m), 1), lambda: tf.squeeze(st, [0]), lambda: st,
            )
            return st

        ord = self.ord
        if ord == "fro":
            var_device, var_dtype = var.device, var.dtype.base_dtype
            coefficients = (apply_state or {}).get(
                (var_device, var_dtype)
            ) or self._fallback_apply_state(var_device, var_dtype)
            norm = tf.convert_to_tensor(
                frobenius_norm(grad), name="norm", dtype=var.dtype.base_dtype
            )
            lr = coefficients["learning_rate"]
            lambda_ = coefficients["lambda_"]
            epsilon = coefficients["epsilon"]
            var_update_tensor = tf.math.multiply(var, lr) - (
                1 - lr
            ) * lambda_ * grad / (norm + epsilon)
            var_update_kwargs = {
                "resource": var.handle,
                "value": var_update_tensor,
            }
            var_update_op = tf.raw_ops.AssignVariableOp(**var_update_kwargs)
            return tf.group(var_update_op)

        else:
            var_device, var_dtype = var.device, var.dtype.base_dtype
            coefficients = (apply_state or {}).get(
                (var_device, var_dtype)
            ) or self._fallback_apply_state(var_device, var_dtype)
            top_singular_vector = tf.convert_to_tensor(
                top_singular_vector(grad),
                name="top_singular_vector",
                dtype=var.dtype.base_dtype,
            )
            lr = coefficients["learning_rate"]
            lambda_ = coefficients["lambda_"]
            epsilon = coefficients["epsilon"]
            var_update_tensor = (
                tf.math.multiply(var, lr) - (1 - lr) * lambda_ * top_singular_vector
            )
            var_update_kwargs = {
                "resource": var.handle,
                "value": var_update_tensor,
            }
            var_update_op = tf.raw_ops.AssignVariableOp(**var_update_kwargs)
            return tf.group(var_update_op)

    def _resource_apply_sparse(self, grad, var, indices, apply_state=None):
        def frobenius_norm(m):
            return tf.reduce_sum(m ** 2) ** 0.5

        def top_singular_vector(m):
            # handle the case where m is a tensor of rank 0 or rank 1.
            n = tf.cond(
                tf.equal(tf.rank(m), 0),
                lambda: tf.expand_dims(tf.expand_dims(m, [0]), [0]),
                lambda: m,
            )
            n = tf.cond(
                tf.equal(tf.rank(m), 1), lambda: tf.expand_dims(m, [0]), lambda: n,
            )
            st, ut, vt = tf.linalg.svd(n, full_matrices=False)
            m_size = tf.shape(n)
            ut = tf.reshape(ut[:, 0], [m_size[0], 1])
            vt = tf.reshape(vt[:, 0], [m_size[1], 1])
            st = tf.matmul(ut, tf.transpose(vt))
            # when we return the top singular vector, we have to remove the
            # dimension we have added on
            st = tf.cond(
                tf.equal(tf.rank(m), 0),
                lambda: tf.squeeze(tf.squeeze(st, [0]), [0]),
                lambda: st,
            )
            st = tf.cond(
                tf.equal(tf.rank(m), 1), lambda: tf.squeeze(st, [0]), lambda: st,
            )
            return st

        ord = self.ord
        if ord == "fro":
            var_device, var_dtype = var.device, var.dtype.base_dtype
            coefficients = (apply_state or {}).get(
                (var_device, var_dtype)
            ) or self._fallback_apply_state(var_device, var_dtype)
            norm = tf.convert_to_tensor(
                frobenius_norm(grad), name="norm", dtype=var.dtype.base_dtype
            )
            lr = coefficients["learning_rate"]
            lambda_ = coefficients["lambda_"]
            epsilon = coefficients["epsilon"]
            var_slice = tf.gather(var, indices)
            var_update_value = tf.math.multiply(var_slice, lr) - (
                1 - lr
            ) * lambda_ * grad / (norm + epsilon)
            var_update_kwargs = {
                "resource": var.handle,
                "indices": indices,
                "updates": var_update_value,
            }
            var_update_op = tf.raw_ops.ResourceScatterUpdate(**var_update_kwargs)
            return tf.group(var_update_op)

        else:
            var_device, var_dtype = var.device, var.dtype.base_dtype
            coefficients = (apply_state or {}).get(
                (var_device, var_dtype)
            ) or self._fallback_apply_state(var_device, var_dtype)
            top_singular_vector = tf.convert_to_tensor(
                top_singular_vector(grad),
                name="top_singular_vector",
                dtype=var.dtype.base_dtype,
            )
            lr = coefficients["learning_rate"]
            lambda_ = coefficients["lambda_"]
            var_slice = tf.gather(var, indices)
            var_update_value = (
                tf.math.multiply(var_slice, lr)
                - (1 - lr) * lambda_ * top_singular_vector
            )
            var_update_kwargs = {
                "resource": var.handle,
                "indices": indices,
                "updates": var_update_value,
            }
            var_update_op = tf.raw_ops.ResourceScatterUpdate(**var_update_kwargs)
            return tf.group(var_update_op)
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow_addons.utils import test_utils
import numpy as np
import conditional_gradient as cg_lib


@test_utils.run_all_in_graph_and_eager_modes
class ConditionalGradientTest(tf.test.TestCase):
    def _update_conditional_gradient_numpy(self, var, norm, g, lr, lambda_):
        var = var * lr - (1 - lr) * lambda_ * g / norm
        return var

    def top_singular_vector(self, m):
        # handle the case where m is a tensor of rank 0 or rank 1.
        n = tf.cond(
            tf.equal(tf.rank(m), 0),
            lambda: tf.expand_dims(tf.expand_dims(m, [0]), [0]),
            lambda: m,
        )
        n = tf.cond(tf.equal(tf.rank(m), 1), lambda: tf.expand_dims(m, [0]), lambda: n,)
        st, ut, vt = tf.linalg.svd(n, full_matrices=False)
        m_size = tf.shape(n)
        ut = tf.reshape(ut[:, 0], [m_size[0], 1])
        vt = tf.reshape(vt[:, 0], [m_size[1], 1])
        st = tf.matmul(ut, tf.transpose(vt))
        # when we return the top singular vector, we have to remove the
        # dimension we have added on
        st = tf.cond(
            tf.equal(tf.rank(m), 0),
            lambda: tf.squeeze(tf.squeeze(st, [0]), [0]),
            lambda: st,
        )
        st = tf.cond(tf.equal(tf.rank(m), 1), lambda: tf.squeeze(st, [0]), lambda: st,)
        return st

    def doTestBasicNuclear(self, use_resource=False, use_callable_params=False):
        for i, dtype in enumerate([tf.half, tf.float32, tf.float64]):
            if use_resource:
                var0 = tf.Variable([1.0, 2.0], dtype=dtype, name="var0_%d" % i)
                var1 = tf.Variable([3.0, 4.0], dtype=dtype, name="var1_%d" % i)
            else:
                var0 = tf.Variable([1.0, 2.0], dtype=dtype)
                var1 = tf.Variable([3.0, 4.0], dtype=dtype)

            grads0 = tf.constant([0.1, 0.1], dtype=dtype)
            grads1 = tf.constant([0.01, 0.01], dtype=dtype)
            top_singular_vector0 = self.top_singular_vector(grads0)
            top_singular_vector1 = self.top_singular_vector(grads1)

            def learning_rate():
                return 0.5

            def lambda_():
                return 0.01

            ord = "nuclear"

            if not use_callable_params:
                learning_rate = learning_rate()
                lambda_ = lambda_()

            cg_opt = cg_lib.ConditionalGradient(
                learning_rate=learning_rate, lambda_=lambda_, ord=ord
            )
            cg_update = cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))

            if not tf.executing_eagerly():
                self.evaluate(tf.compat.v1.global_variables_initializer())
                # Fetch params to validate initial values
                self.assertAllClose([1.0, 2.0], self.evaluate(var0))
                self.assertAllClose([3.0, 4.0], self.evaluate(var1))

            # Check we have slots
            self.assertEqual(["conditional_gradient"], cg_opt.get_slot_names())
            slot0 = cg_opt.get_slot(var0, "conditional_gradient")
            self.assertEquals(slot0.get_shape(), var0.get_shape())
            slot1 = cg_opt.get_slot(var1, "conditional_gradient")
            self.assertEquals(slot1.get_shape(), var1.get_shape())

            if not tf.executing_eagerly():
                self.assertFalse(slot0 in tf.compat.v1.trainable_variables())
                self.assertFalse(slot1 in tf.compat.v1.trainable_variables())

            if not tf.executing_eagerly():
                self.evaluate(cg_update)

            top_singular_vector0 = self.evaluate(top_singular_vector0)
            top_singular_vector1 = self.evaluate(top_singular_vector1)

            self.assertAllCloseAccordingToType(
                np.array(
                    [
                        1.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[0],
                        2.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[1],
                    ]
                ),
                self.evaluate(var0),
            )
            self.assertAllCloseAccordingToType(
                np.array(
                    [
                        3.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[0],
                        4.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[1],
                    ]
                ),
                self.evaluate(var1),
            )

            # Step 2: the conditional_gradient contain the previous update.
            if tf.executing_eagerly():
                cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))
            else:
                self.evaluate(cg_update)
            self.assertAllCloseAccordingToType(
                np.array(
                    [
                        (1.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[0]) * 0.5
                        - (1 - 0.5) * 0.01 * top_singular_vector0[0],
                        (2.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[1]) * 0.5
                        - (1 - 0.5) * 0.01 * top_singular_vector0[1],
                    ]
                ),
                self.evaluate(var0),
            )
            self.assertAllCloseAccordingToType(
                np.array(
                    [
                        (3.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[0]) * 0.5
                        - (1 - 0.5) * 0.01 * top_singular_vector1[1],
                        (4.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[0]) * 0.5
                        - (1 - 0.5) * 0.01 * top_singular_vector1[1],
                    ]
                ),
                self.evaluate(var1),
            )


    def testBasicNuclear(self):
        with self.cached_session():
            self.doTestBasicNuclear(use_resource=False)

    def testResourceBasicNuclear(self):
        self.doTestBasicNuclear(use_resource=True)

    def testBasicCallableParamsNuclear(self):
        self.doTestBasicNuclear(use_resource=True, use_callable_params=True)

    def testVariablesAcrossGraphsNuclear(self):
        optimizer = cg_lib.ConditionalGradient(0.01, 0.5, ord="nuclear")
        with tf.Graph().as_default():
            var0 = tf.Variable([1.0, 2.0], dtype=tf.float32, name="var0")
            var1 = tf.Variable([3.0, 4.0], dtype=tf.float32, name="var1")

            def loss():
                return tf.math.reduce_sum(var0 + var1)

            optimizer.minimize(loss, var_list=[var0, var1])
            optimizer_variables = optimizer.variables()
            # There should be three items. The first item is iteration,
            # and one item for each variable.
            self.assertStartsWith(
                optimizer_variables[1].name, "ConditionalGradient/var0"
            )
            self.assertStartsWith(
                optimizer_variables[2].name, "ConditionalGradient/var1"
            )
            self.assertEqual(3, len(optimizer_variables))

    # Based on issue #347 in the following link,
    #        "https://github.com/tensorflow/addons/issues/347"
    # tf.half is not registered for 'ResourceScatterUpdate' OpKernel
    # for 'GPU' devices.
    # So we have to remove tf.half when testing with gpu.
    # The function "_DtypesToTest" is from
    #       "https://github.com/tensorflow/tensorflow/blob/5d4a6cee737a1dc6c20172a1dc1
    #        5df10def2df72/tensorflow/python/kernel_tests/conv_ops_3d_test.py#L53-L62"

    def _DtypesToTest(self, use_gpu):
        if use_gpu:
            return [tf.float32, tf.float64]
        else:
            return [tf.half, tf.float32, tf.float64]

    def testMinimizeSparseResourceVariableNuclear(self):
        # This test invokes the ResourceSparseApplyConditionalGradient
        # operation. And it will call the 'ResourceScatterUpdate' OpKernel
        # for 'GPU' devices. However, tf.half is not registered in this case,
        # based on issue #347.
        # Thus, we will call the "_DtypesToTest" function.
        #
        # TODO:
        #       Wait for the solving of issue #347. After that, we will test
        #       for the dtype to be tf.half, with 'GPU' devices.
        for dtype in self._DtypesToTest(use_gpu=tf.test.is_gpu_available()):
            var0 = tf.Variable([[1.0, 2.0]], dtype=dtype)

            def loss():
                x = tf.constant([[4.0], [5.0]], dtype=dtype)
                pred = tf.matmul(tf.nn.embedding_lookup([var0], [0]), x)
                return pred * pred

            # the gradient based on the current loss function
            grads0_0 = 32 * 1.0 + 40 * 2.0
            grads0_1 = 40 * 1.0 + 50 * 2.0
            grads0 = tf.constant([[grads0_0, grads0_1]], dtype=dtype)
            top_singular_vector0 = self.top_singular_vector(grads0)

            learning_rate = 0.1
            lambda_ = 0.1
            ord = "nuclear"
            opt = cg_lib.ConditionalGradient(
                learning_rate=learning_rate, lambda_=lambda_, ord=ord
            )
            cg_op = opt.minimize(loss, var_list=[var0])
            self.evaluate(tf.compat.v1.global_variables_initializer())

            # Run 1 step of cg_op
            self.evaluate(cg_op)

            # Validate updated params
            top_singular_vector0 = self.evaluate(top_singular_vector0)
            self.assertAllCloseAccordingToType(
                [
                    [
                        1.0 * learning_rate
                        - (1 - learning_rate) * lambda_ * top_singular_vector0[0][0],
                        2.0 * learning_rate
                        - (1 - learning_rate) * lambda_ * top_singular_vector0[0][1],
                    ]
                ],
                self.evaluate(var0),
            )

    def testMinimizeWith2DIndiciesForEmbeddingLookupNuclear(self):
        # This test invokes the ResourceSparseApplyConditionalGradient
        # operation.
        var0 = tf.Variable(tf.ones([2, 2]))

        def loss():
            return tf.math.reduce_sum(tf.nn.embedding_lookup(var0, [[1]]))

        # the gradient for this loss function:
        grads0 = tf.constant([[0, 0], [1, 1]], dtype=tf.float32)
        top_singular_vector0 = self.top_singular_vector(grads0)

        learning_rate = 0.1
        lambda_ = 0.1
        ord = "nuclear"
        opt = cg_lib.ConditionalGradient(
            learning_rate=learning_rate, lambda_=lambda_, ord=ord
        )
        cg_op = opt.minimize(loss, var_list=[var0])
        self.evaluate(tf.compat.v1.global_variables_initializer())

        # Run 1 step of cg_op
        self.evaluate(cg_op)
        top_singular_vector0 = self.evaluate(top_singular_vector0)
        self.evaluate(var0)
        self.assertAllCloseAccordingToType(
            [
                learning_rate * 1
                - (1 - learning_rate) * lambda_ * top_singular_vector0[1][0],
                learning_rate * 1
                - (1 - learning_rate) * lambda_ * top_singular_vector0[1][1],
            ],
            self.evaluate(var0[1]),
        )

    def testTensorLearningRateAndConditionalGradientNuclear(self):
        for dtype in [tf.half, tf.float32, tf.float64]:
            with self.cached_session():
                var0 = tf.Variable([1.0, 2.0], dtype=dtype)
                var1 = tf.Variable([3.0, 4.0], dtype=dtype)
                grads0 = tf.constant([0.1, 0.1], dtype=dtype)
                grads1 = tf.constant([0.01, 0.01], dtype=dtype)
                top_singular_vector0 = self.top_singular_vector(grads0)
                top_singular_vector1 = self.top_singular_vector(grads1)
                ord = "nuclear"
                cg_opt = cg_lib.ConditionalGradient(
                    learning_rate=tf.constant(0.5), lambda_=tf.constant(0.01), ord=ord
                )
                cg_update = cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))
                if not tf.executing_eagerly():
                    self.evaluate(tf.compat.v1.global_variables_initializer())
                    # Fetch params to validate initial values
                    self.assertAllClose([1.0, 2.0], self.evaluate(var0))
                    self.assertAllClose([3.0, 4.0], self.evaluate(var1))

                # Check we have slots
                self.assertEqual(["conditional_gradient"], cg_opt.get_slot_names())
                slot0 = cg_opt.get_slot(var0, "conditional_gradient")
                self.assertEquals(slot0.get_shape(), var0.get_shape())
                slot1 = cg_opt.get_slot(var1, "conditional_gradient")
                self.assertEquals(slot1.get_shape(), var1.get_shape())

                if not tf.executing_eagerly():
                    self.assertFalse(slot0 in tf.compat.v1.trainable_variables())
                    self.assertFalse(slot1 in tf.compat.v1.trainable_variables())

                if not tf.executing_eagerly():
                    self.evaluate(cg_update)
                # Check that the parameters have been updated.
                top_singular_vector0 = self.evaluate(top_singular_vector0)
                top_singular_vector1 = self.evaluate(top_singular_vector1)
                self.assertAllCloseAccordingToType(
                    np.array(
                        [
                            1.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[0],
                            2.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[1],
                        ]
                    ),
                    self.evaluate(var0),
                )
                self.assertAllCloseAccordingToType(
                    np.array(
                        [
                            3.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[0],
                            4.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[1],
                        ]
                    ),
                    self.evaluate(var1),
                )
                # Step 2: the conditional_gradient contain the
                # previous update.
                if tf.executing_eagerly():
                    cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))
                else:
                    self.evaluate(cg_update)
                # Check that the parameters have been updated.
                self.assertAllCloseAccordingToType(
                    np.array(
                        [
                            (1.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[0])
                            * 0.5
                            - (1 - 0.5) * 0.01 * top_singular_vector0[0],
                            (2.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[1])
                            * 0.5
                            - (1 - 0.5) * 0.01 * top_singular_vector0[1],
                        ]
                    ),
                    self.evaluate(var0),
                )
                self.assertAllCloseAccordingToType(
                    np.array(
                        [
                            (3.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[0])
                            * 0.5
                            - (1 - 0.5) * 0.01 * top_singular_vector1[0],
                            (4.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[1])
                            * 0.5
                            - (1 - 0.5) * 0.01 * top_singular_vector1[1],
                        ]
                    ),
                    self.evaluate(var1),
                )

    def _dbParamsNuclearCG01(self):
        """Return dist-belief conditional_gradient values.
        Return values been generated from the dist-belief
        conditional_gradient unittest, running with a learning rate of 0.1
        and a lambda_ of 0.1.
        These values record how a parameter vector of size 10, initialized
        with 0.0, gets updated with 10 consecutive conditional_gradient
        steps.
        It uses random gradients.
        Returns:
            db_grad: The gradients to apply
            db_out: The parameters after the conditional_gradient update.
        """
        db_grad = [[]] * 10
        db_out = [[]] * 10
        db_grad[0] = [
            0.00096264342,
            0.17914793,
            0.93945462,
            0.41396621,
            0.53037018,
            0.93197989,
            0.78648776,
            0.50036013,
            0.55345792,
            0.96722615,
        ]
        db_out[0] = [
            -4.1552783e-05,
            -7.7334875e-03,
            -4.0554535e-02,
            -1.7870164e-02,
            -2.2895109e-02,
            -4.0231861e-02,
            -3.3951234e-02,
            -2.1599628e-02,
            -2.3891764e-02,
            -4.1753381e-02,
        ]
        db_grad[1] = [
            0.17075552,
            0.88821375,
            0.20873757,
            0.25236958,
            0.57578111,
            0.15312378,
            0.5513742,
            0.94687688,
            0.16012503,
            0.22159521,
        ]
        db_out[1] = [
            -0.00961733,
            -0.0507779,
            -0.01580694,
            -0.01599489,
            -0.03470477,
            -0.01264373,
            -0.03443632,
            -0.05546713,
            -0.01140388,
            -0.01665068,
        ]
        db_grad[2] = [
            0.35077485,
            0.47304362,
            0.44412705,
            0.44368884,
            0.078527533,
            0.81223965,
            0.31168157,
            0.43203235,
            0.16792089,
            0.24644311,
        ]
        db_out[2] = [
            -0.02462724,
            -0.03699233,
            -0.03154433,
            -0.03153357,
            -0.00876844,
            -0.05606324,
            -0.02447166,
            -0.03469437,
            -0.0124694,
            -0.01829169,
        ]
        db_grad[3] = [
            0.9694621,
            0.75035888,
            0.28171822,
            0.83813518,
            0.53807181,
            0.3728098,
            0.81454384,
            0.03848977,
            0.89759839,
            0.93665648,
        ]
        db_out[3] = [
            -0.04124615,
            -0.03371741,
            -0.0144246,
            -0.03668303,
            -0.02240246,
            -0.02052062,
            -0.03503307,
            -0.00500922,
            -0.03715545,
            -0.0393002,
        ]
        db_grad[4] = [
            0.38578293,
            0.8536852,
            0.88722926,
            0.66276771,
            0.13678469,
            0.94036359,
            0.69107032,
            0.81897682,
            0.5433259,
            0.67860287,
        ]
        db_out[4] = [
            -0.01979207,
            -0.0380417,
            -0.03747472,
            -0.0305847,
            -0.00779536,
            -0.04024221,
            -0.03156913,
            -0.0337613,
            -0.02578116,
            -0.03148951,
        ]
        db_grad[5] = [
            0.27885768,
            0.76100707,
            0.24625534,
            0.81354135,
            0.18959245,
            0.48038563,
            0.84163809,
            0.41172323,
            0.83259648,
            0.44941229,
        ]
        db_out[5] = [
            -0.01555188,
            -0.04084422,
            -0.01573331,
            -0.04265549,
            -0.01000746,
            -0.02740575,
            -0.04412147,
            -0.02341569,
            -0.0431026,
            -0.02502293,
        ]
        db_grad[6] = [
            0.27233034,
            0.056316052,
            0.5039115,
            0.24105175,
            0.35697976,
            0.75913221,
            0.73577434,
            0.16014607,
            0.57500273,
            0.071136251,
        ]
        db_out[6] = [
            -0.01890448,
            -0.00767214,
            -0.03367592,
            -0.01962219,
            -0.02374278,
            -0.05110246,
            -0.05128598,
            -0.01254396,
            -0.04094184,
            -0.00703416,
        ]
        db_grad[7] = [
            0.58697265,
            0.2494842,
            0.08106143,
            0.39954534,
            0.15892942,
            0.12683646,
            0.74053431,
            0.16033,
            0.66625422,
            0.73515922,
        ]
        db_out[7] = [
            -0.03772915,
            -0.01599993,
            -0.00831695,
            -0.0263572,
            -0.01207801,
            -0.01285448,
            -0.05034329,
            -0.01104364,
            -0.04477356,
            -0.04558992,
        ]
        db_grad[8] = [
            0.8215279,
            0.41994119,
            0.95172721,
            0.68000203,
            0.79439718,
            0.43384039,
            0.55561525,
            0.22567581,
            0.93331909,
            0.29438227,
        ]
        db_out[8] = [
            -0.03919835,
            -0.01970845,
            -0.04187151,
            -0.03195836,
            -0.03546333,
            -0.01999326,
            -0.02899324,
            -0.01083582,
            -0.04472339,
            -0.01725317,
        ]
        db_grad[9] = [
            0.68297005,
            0.67758518,
            0.1748755,
            0.13266537,
            0.70697063,
            0.055731893,
            0.68593478,
            0.50580865,
            0.12602448,
            0.093537711,
        ]
        db_out[9] = [
            -0.04510314,
            -0.04282944,
            -0.0147322,
            -0.0111956,
            -0.04617687,
            -0.00535998,
            -0.0442614,
            -0.031584,
            -0.01207165,
            -0.00736567,
        ]
        return db_grad, db_out

    def testLikeDistBeliefNuclearCG01(self):
        with self.cached_session():
            db_grad, db_out = self._dbParamsNuclearCG01()
            num_samples = len(db_grad)
            var0 = tf.Variable([0.0] * num_samples)
            grads0 = tf.constant([0.0] * num_samples)
            ord = "nuclear"
            cg_opt = cg_lib.ConditionalGradient(learning_rate=0.1, lambda_=0.1, ord=ord)
            if not tf.executing_eagerly():
                cg_update = cg_opt.apply_gradients(zip([grads0], [var0]))
                self.evaluate(tf.compat.v1.global_variables_initializer())

            for i in range(num_samples):
                if tf.executing_eagerly():
                    grads0 = tf.constant(db_grad[i])
                    cg_opt.apply_gradients(zip([grads0], [var0]))
                else:
                    cg_update.run(feed_dict={grads0: db_grad[i]})
                self.assertAllClose(np.array(db_out[i]), self.evaluate(var0))

    def testSparseNuclear(self):
        # TODO:
        #       To address the issue #347.
        for dtype in self._DtypesToTest(use_gpu=tf.test.is_gpu_available()):
            with self.cached_session():
                var0 = tf.Variable(tf.zeros([4, 2], dtype=dtype))
                var1 = tf.Variable(tf.constant(1.0, dtype, [4, 2]))
                grads0 = tf.IndexedSlices(
                    tf.constant([[0.1, 0.1]], dtype=dtype),
                    tf.constant([1]),
                    tf.constant([4, 2]),
                )
                grads1 = tf.IndexedSlices(
                    tf.constant([[0.01, 0.01], [0.01, 0.01]], dtype=dtype),
                    tf.constant([2, 3]),
                    tf.constant([4, 2]),
                )
                top_singular_vector0 = tf.constant(
                    [[0.0, 0.0], [0.7071067, 0.7071067], [0.0, 0.0], [0.0, 0.0]],
                    dtype=dtype,
                )
                top_singular_vector1 = tf.constant(
                    [
                        [-4.2146844e-08, -4.2146844e-08],
                        [0.0000000e00, 0.0000000e00],
                        [4.9999994e-01, 4.9999994e-01],
                        [4.9999994e-01, 4.9999994e-01],
                    ],
                    dtype=dtype,
                )
                learning_rate = 0.1
                lambda_ = 0.1
                ord = "nuclear"
                cg_opt = cg_lib.ConditionalGradient(
                    learning_rate=learning_rate, lambda_=lambda_, ord=ord
                )
                cg_update = cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))

                if not tf.executing_eagerly():
                    self.evaluate(tf.compat.v1.global_variables_initializer())
                    # Fetch params to validate initial values
                    self.assertAllClose([0, 0], self.evaluate(var0)[0])
                    self.assertAllClose([0, 0], self.evaluate(var0)[1])
                    self.assertAllClose([1, 1], self.evaluate(var1)[2])
                # Check we have slots
                self.assertEqual(["conditional_gradient"], cg_opt.get_slot_names())
                slot0 = cg_opt.get_slot(var0, "conditional_gradient")
                self.assertEquals(slot0.get_shape(), var0.get_shape())
                slot1 = cg_opt.get_slot(var1, "conditional_gradient")
                self.assertEquals(slot1.get_shape(), var1.get_shape())
                if not tf.executing_eagerly():
                    self.assertFalse(slot0 in tf.compat.v1.trainable_variables())
                    self.assertFalse(slot1 in tf.compat.v1.trainable_variables())

                # Step 1:
                if not tf.executing_eagerly():
                    self.evaluate(cg_update)
                # Check that the parameters have been updated.

                top_singular_vector0 = self.evaluate(top_singular_vector0)
                top_singular_vector1 = self.evaluate(top_singular_vector1)

                self.assertAllCloseAccordingToType(
                    np.array(
                        [
                            0
                            - (1 - learning_rate)
                            * lambda_
                            * top_singular_vector0[0][0],
                            0
                            - (1 - learning_rate)
                            * lambda_
                            * top_singular_vector0[0][1],
                        ]
                    ),
                    self.evaluate(var0)[0],
                )
                self.assertAllCloseAccordingToType(
                    np.array(
                        [
                            0
                            - (1 - learning_rate)
                            * lambda_
                            * top_singular_vector0[1][0],
                            0
                            - (1 - learning_rate)
                            * lambda_
                            * top_singular_vector0[1][1],
                        ]
                    ),
                    self.evaluate(var0)[1],
                )
                self.assertAllCloseAccordingToType(
                    np.array(
                        [
                            1.0 * learning_rate
                            - (1 - learning_rate)
                            * lambda_
                            * top_singular_vector1[2][0],
                            1.0 * learning_rate
                            - (1 - learning_rate)
                            * lambda_
                            * top_singular_vector1[2][1],
                        ]
                    ),
                    self.evaluate(var1)[2],
                )
                # Step 2: the conditional_gradient contain the
                # previous update.
                if tf.executing_eagerly():
                    cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))
                else:
                    self.evaluate(cg_update)
                # Check that the parameters have been updated.
                self.assertAllClose(np.array([0, 0]), self.evaluate(var0)[0])
                self.assertAllCloseAccordingToType(
                    np.array(
                        [
                            (
                                0
                                - (1 - learning_rate)
                                * lambda_
                                * top_singular_vector0[1][0]
                            )
                            * learning_rate
                            - (1 - learning_rate)
                            * lambda_
                            * top_singular_vector0[1][0],
                            (
                                0
                                - (1 - learning_rate)
                                * lambda_
                                * top_singular_vector0[1][1]
                            )
                            * learning_rate
                            - (1 - learning_rate)
                            * lambda_
                            * top_singular_vector0[1][1],
                        ]
                    ),
                    self.evaluate(var0)[1],
                )
                self.assertAllCloseAccordingToType(
                    np.array(
                        [
                            (
                                1.0 * learning_rate
                                - (1 - learning_rate)
                                * lambda_
                                * top_singular_vector1[2][0]
                            )
                            * learning_rate
                            - (1 - learning_rate)
                            * lambda_
                            * top_singular_vector1[2][0],
                            (
                                1.0 * learning_rate
                                - (1 - learning_rate)
                                * lambda_
                                * top_singular_vector1[2][1]
                            )
                            * learning_rate
                            - (1 - learning_rate)
                            * lambda_
                            * top_singular_vector1[2][1],
                        ]
                    ),
                    self.evaluate(var1)[2],
                )

    def testSharingNuclear(self):
        for dtype in [tf.half, tf.float32, tf.float64]:
            with self.cached_session():
                var0 = tf.Variable([1.0, 2.0], dtype=dtype)
                var1 = tf.Variable([3.0, 4.0], dtype=dtype)
                grads0 = tf.constant([0.1, 0.1], dtype=dtype)
                grads1 = tf.constant([0.01, 0.01], dtype=dtype)
                top_singular_vector0 = self.top_singular_vector(grads0)
                top_singular_vector1 = self.top_singular_vector(grads1)
                learning_rate = 0.1
                lambda_ = 0.1
                ord = "nuclear"
                cg_opt = cg_lib.ConditionalGradient(
                    learning_rate=learning_rate, lambda_=lambda_, ord=ord
                )
                cg_update1 = cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))
                cg_update2 = cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))
                if not tf.executing_eagerly():
                    self.evaluate(tf.compat.v1.global_variables_initializer())
                    # Fetch params to validate initial values
                    self.assertAllClose([1.0, 2.0], self.evaluate(var0))
                    self.assertAllClose([3.0, 4.0], self.evaluate(var1))

                # Check we have slots
                self.assertEqual(["conditional_gradient"], cg_opt.get_slot_names())
                slot0 = cg_opt.get_slot(var0, "conditional_gradient")
                self.assertEquals(slot0.get_shape(), var0.get_shape())
                slot1 = cg_opt.get_slot(var1, "conditional_gradient")
                self.assertEquals(slot1.get_shape(), var1.get_shape())

                if not tf.executing_eagerly():
                    self.assertFalse(slot0 in tf.compat.v1.trainable_variables())
                    self.assertFalse(slot1 in tf.compat.v1.trainable_variables())
                # Because in the eager mode, as we declare two cg_update
                # variables, it already altomatically finish executing them.
                # Thus, we cannot test the param value at this time for
                # eager mode. We can only test the final value of param
                # after the second execution.
                if not tf.executing_eagerly():
                    self.evaluate(cg_update1)
                    # Check that the parameters have been updated.
                    top_singular_vector0 = self.evaluate(top_singular_vector0)
                    top_singular_vector1 = self.evaluate(top_singular_vector1)
                    self.assertAllCloseAccordingToType(
                        np.array(
                            [
                                1.0 * learning_rate
                                - (1 - learning_rate)
                                * lambda_
                                * top_singular_vector0[0],
                                2.0 * learning_rate
                                - (1 - learning_rate)
                                * lambda_
                                * top_singular_vector0[1],
                            ]
                        ),
                        self.evaluate(var0),
                    )
                    self.assertAllCloseAccordingToType(
                        np.array(
                            [
                                3.0 * learning_rate
                                - (1 - learning_rate)
                                * lambda_
                                * top_singular_vector1[0],
                                4.0 * learning_rate
                                - (1 - learning_rate)
                                * lambda_
                                * top_singular_vector1[1],
                            ]
                        ),
                        self.evaluate(var1),
                    )

                # Step 2: the second conditional_gradient contain
                # the previous update.
                if not tf.executing_eagerly():
                    self.evaluate(cg_update2)
                # Check that the parameters have been updated.
                self.assertAllCloseAccordingToType(
                    np.array(
                        [
                            (
                                1.0 * learning_rate
                                - (1 - learning_rate)
                                * lambda_
                                * top_singular_vector0[0]
                            )
                            * learning_rate
                            - (1 - learning_rate) * lambda_ * top_singular_vector0[0],
                            (
                                2.0 * learning_rate
                                - (1 - learning_rate)
                                * lambda_
                                * top_singular_vector0[1]
                            )
                            * learning_rate
                            - (1 - learning_rate) * lambda_ * top_singular_vector0[1],
                        ]
                    ),
                    self.evaluate(var0),
                )
                self.assertAllCloseAccordingToType(
                    np.array(
                        [
                            (
                                3.0 * learning_rate
                                - (1 - learning_rate)
                                * lambda_
                                * top_singular_vector1[0]
                            )
                            * learning_rate
                            - (1 - learning_rate) * lambda_ * top_singular_vector1[0],
                            (
                                4.0 * learning_rate
                                - (1 - learning_rate)
                                * lambda_
                                * top_singular_vector1[1]
                            )
                            * learning_rate
                            - (1 - learning_rate) * lambda_ * top_singular_vector1[1],
                        ]
                    ),
                    self.evaluate(var1),
                )


if __name__ == "__main__":
    tf.test.main()
&lt;/denchmark-code&gt;

The code is from the following page:
&lt;denchmark-link:url&gt;https://github.com/pkan2/addons/blob/master/tensorflow_addons/optimizers/conditional_gradient.py&lt;/denchmark-link&gt;

&lt;denchmark-link:url&gt;https://github.com/pkan2/addons/blob/master/tensorflow_addons/optimizers/conditional_gradient_test.py&lt;/denchmark-link&gt;

And the error message and system info is from here:
&lt;denchmark-link:url&gt;https://github.com/pkan2/addons/runs/446766976?check_suite_focus=true&lt;/denchmark-link&gt;

Thank you!
		</comment>
		<comment id='5' author='pkan2' date='2020-11-21T01:52:54Z'>
		&lt;denchmark-link:https://github.com/pkan2&gt;@pkan2&lt;/denchmark-link&gt;
 Is this still an issue for you? Can you please test with recent  and share a simple standalone code to reproduce the issue. It is difficult to follow long codes. Thanks!
		</comment>
		<comment id='6' author='pkan2' date='2020-11-28T02:26:03Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='7' author='pkan2' date='2020-12-05T02:51:06Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='8' author='pkan2' date='2020-12-05T02:51:11Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36764&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36764&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>