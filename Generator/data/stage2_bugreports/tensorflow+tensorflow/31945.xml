<bug id='31945' author='demmerichs' open_date='2019-08-24T11:44:39Z' closed_time='2019-12-08T12:21:54Z'>
	<summary>tf.custom_gradient does not handle variables correctly</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): tf 1.13.1 and 1.13.2 tested
Python version: 3.6.8
CUDA/cuDNN version: 10.0/7.3.1 and 7.4.2
GPU model and memory: Nvidia Quadro P2000

When applying the custom gradient to a function which uses/creates variables, then it recognizes it through the gradient tape, however it does not find the variables=None argument in the gradient function, which results in an error.
TypeError: If using @custom_gradient with a function that uses variables, then grad_fn must accept a keyword argument 'variables'.
The expected behavior would be of course, that the found variables are passed correctly to the underlying gradient function. In the following code snippet I expect the print statement to display [&lt;tf.Variable 'outside:0' shape=() dtype=float32&gt;]. In fact, I achieved this already by changing the custom_gradient functor at one line though I am not sure if this is wanted.
import tensorflow as tf
# from custom_gradient import custom_gradient  # my corrected version
from tensorflow import custom_gradient


def layer(t, name):
    var = tf.Variable(1.0, dtype=tf.float32, use_resource=True, name=name)
    return t * var


@custom_gradient
def custom_gradient_layer(t):
    result = layer(t, name='outside')

    def grad(*grad_ys, variables=None):
        assert variables is not None
        print(variables)
        grads = tf.gradients(
            layer(t, name='inside'),
            [t, *variables],
            grad_ys=grad_ys,
        )
        grads = (grads[:1], grads[1:])
        return grads

    return result, grad


var = tf.Variable(0.0, dtype=tf.float32)
result = custom_gradient_layer(var)
grads = tf.gradients(result, var)[0]
The line I changed is 


tensorflow/tensorflow/python/ops/custom_gradient.py


        Lines 197 to 198
      in
      00fad90






 variables_in_signature = ("variables" in grad_argspec.args or 



 grad_argspec.varkw) 





to
  variables_in_signature = ("variables" in grad_argspec.args or
                            grad_argspec.varkw or "variables" in grad_argspec.kwonlyargs)
	</description>
	<comments>
		<comment id='1' author='demmerichs' date='2019-08-26T06:14:43Z'>
		@DavidS3141, Thanks for finding the issue.
I could able to reproduce the issue on Colab with Tensorflow 1.13.1 . Please take a look at gist &lt;denchmark-link:https://colab.research.google.com/drive/1lG5pJtuX7IDxxH-Sxt8SzqQ4S8e3A_Wk&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='demmerichs' date='2019-08-26T22:02:29Z'>
		This looks like an improvement. Want to send a PR with your change and a unit test?
		</comment>
		<comment id='3' author='demmerichs' date='2019-10-24T08:12:54Z'>
		I came across the same issue in TensorFlow 1.14. As a result,

*grad_ys cannot be used. It has to be replaced by a fixed number of arguments;
then the custom-gradient function must output a fixed number of tensors.

However, in my case, the number of inputs and outputs to custom-gradient function should be changeable.
Workaround
After a deeper look at tf_inspect.getfullargspec, I got the below:
from tensorflow.python.util.tf_inspect import getfullargspec

def grad_fn(*grad_ys, variables=None):
    pass

def grad_fn2(grad_ys, variables=None):
    pass

def grad_fn3(grad_ys, **variables):
    pass

def grad_fn4(*grad_ys, **variables):
    pass

# The code below are used by TF to check variables
grad_argspec = getfullargspec(grad_fn)
grad_argspec2 = getfullargspec(grad_fn2)
grad_argspec3 = getfullargspec(grad_fn3)
grad_argspec4 = getfullargspec(grad_fn4)
variables_in_signature = ("variables" in grad_argspec.args or grad_argspec.varkw)
variables_in_signature2 = ("variables" in grad_argspec2.args or grad_argspec2.varkw)
variables_in_signature3 = ("variables" in grad_argspec3.args or grad_argspec3.varkw)
variables_in_signature4 = ("variables" in grad_argspec3.args or grad_argspec4.varkw)

# note if "not variables_in_signature" is True, we'd have the TypeError
print(not variables_in_signature)  # True
print(not variables_in_signature2)  # False
print(not variables_in_signature3)  # False
print(not variables_in_signature4)  # False
Thus, a workaround is to define:
def grad_fn(*grad_ys, **kwargs):
    variables = kwargs.get('variables', None)
This would result in a warning in case no variable is used, but no error at all time.
&lt;denchmark-code&gt;@custom_gradient grad_fn has 'variables' in signature, but no ResourceVariables were used on the forward pass.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='demmerichs' date='2019-12-08T12:21:55Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31945&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31945&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='demmerichs' date='2019-12-08T12:22:06Z'>
		We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!
		</comment>
		<comment id='6' author='demmerichs' date='2020-04-16T08:42:32Z'>
		Was this bug ever fixed?
I'm happy to do a PR if someone can tell me what unit test needs to be written.
I'm having exact the same issue with custom gradients: my loss requires instantiation of a temp variable. The workaround above didn't work for me.
Wasn't able to fix locally and try it out because I can't do a local build due to some bazel issue similar to here, even though I've just downgraded to bazel 0.21.0: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/21362&gt;#21362&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='demmerichs' date='2020-04-16T15:03:30Z'>
		Can you file a separate issue with instructions to reproduce your problem?
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Apr 16, 2020 at 1:42 AM Andre Zapico ***@***.***&gt; wrote:
 Was this bug ever fixed?

 I'm happy to do a PR if someone can tell me what unit test needs to be
 written.

 I'm having exact the same issue with custom gradients: my loss requires
 instantiation of a temp variable. The workaround above didn't work for me.

 Wasn't able to fix locally and try it out because I can't do a local build
 due to some bazel issue similar to here, even though I've just downgraded
 to bazel 0.21.0: #21362
 &lt;#21362&gt;

 —
 You are receiving this because you commented.
 Reply to this email directly, view it on GitHub
 &lt;#31945 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AAABHROGRRQHSPS7RWC5DTTRM3AI3ANCNFSM4IPF7DHA&gt;
 .


-- 
 - Alex

		</comment>
		<comment id='8' author='demmerichs' date='2020-04-18T07:35:12Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 Thanks, I've opened an issue with a minimal working example.
		</comment>
	</comments>
</bug>