<bug id='34635' author='ikhatri' open_date='2019-11-27T06:04:33Z' closed_time='2019-12-21T05:28:19Z'>
	<summary>optimizer.apply_gradients() logs warnings using Tensor.name which is not supported by eager execution</summary>
	<description>
System information

Have I written custom code: Yes
OS Platform and Distribution: Ubuntu 18.04
TensorFlow installed from (source or binary): binary
TensorFlow version: v2.0.0-rc2-26-g64c3d38 2.0.0
Python version: 3.7.4
CUDA/cuDNN version: 10.1/7.6.5
GPU model and memory: RTX 2070 super 8gb


When using a gradient tape in eager mode, if the gradient computation fails and returns , the  function will attempt to log a warning using  which isn't supported in eager execution. The exact line can be found &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L1043&gt;here&lt;/denchmark-link&gt;
. This is a breaking issue because it is simply a logged warning, and the code should continue to execute; however in eager mode it raises an  due to . A similar issue can be found above on &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L1039&gt;line 1039&lt;/denchmark-link&gt;
 however that one is less serious, as the code would terminate due to the  anyway.
Describe the expected behavior
A warning is logged and the code continues to execute.

There is a workaround for RTX GPUs at the top per the comment in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/24828&gt;#24828&lt;/denchmark-link&gt;

import tensorflow as tf
import numpy as np

conv1_filters = 32
conv1_window = 3
input_dims = 50
num_classes = 10

random_normal = tf.initializers.RandomNormal()

# Magic fix for RTX GPUs
# gpus = tf.config.experimental.list_physical_devices('GPU')
# for gpu in gpus:
#   tf.config.experimental.set_memory_growth(gpu, True)

weights = {
  'wc1': tf.Variable(random_normal([conv1_window, input_dims, conv1_filters])),
  'out': tf.Variable(random_normal([conv1_filters, num_classes]))
}

biases = {
  # This line is the one that's wrong. Here I forgot to wrap the tf.zeros in a tf.Variable which is how I discovered the issue.
  'bc1': tf.zeros(conv1_filters),
  'out': tf.Variable(tf.zeros(num_classes))
}

def conv1d(x, W, b, stride=1):
  """
  Conv1D wrapper, with bias and relu activation.
  """
  x = tf.nn.conv1d(x, W, stride=stride, padding='SAME')
  x = tf.nn.bias_add(x, b)
  return tf.nn.relu(x)

def model(inputs):
  x = inputs
  x = conv1d(x, weights['wc1'], biases['bc1'])
  x = tf.add(tf.matmul(x, weights['out']), biases['out'])
  return tf.nn.softmax(x)

def cross_entropy(y_pred, y_true):
  y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)
  return -tf.reduce_sum(y_true * tf.math.log(y_pred)) / tf.reduce_sum(y_true)

def train_one_batch(optimizer, minibatch_x, minibatch_y):
  with tf.GradientTape() as g:
    pred = model(minibatch_x)
    loss = cross_entropy(pred, minibatch_y)
  trainable_variables = list(weights.values()) + list(biases.values())
  gradients = g.gradient(loss, trainable_variables)
  optimizer.apply_gradients(zip(gradients, trainable_variables))

batch_size = 2
sequence_len = 4
x = tf.zeros([batch_size, sequence_len, input_dims], dtype=tf.float32)
y = tf.ones([batch_size, sequence_len], dtype=tf.int64)
y_onehot = tf.one_hot(y, depth=num_classes)
optimizer = tf.optimizers.Adam()
train_one_batch(optimizer, x, y_onehot)
Other info / logs
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "code/tf_testcase.py", line 58, in &lt;module&gt;
    train_one_batch(optimizer, x, y_onehot)
  File "code/tf_testcase.py", line 50, in train_one_batch
    optimizer.apply_gradients(zip(gradients, trainable_variables))
  File "/home/ikhatri/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py", line 427, in apply_gradients
    grads_and_vars = _filter_grads(grads_and_vars)
  File "/home/ikhatri/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py", line 1029, in _filter_grads
    ([v.name for v in vars_with_empty_grads]))
  File "/home/ikhatri/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py", line 1029, in &lt;listcomp&gt;
    ([v.name for v in vars_with_empty_grads]))
  File "/home/ikhatri/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 1090, in name
    "Tensor.name is meaningless when eager execution is enabled.")
AttributeError: Tensor.name is meaningless when eager execution is enabled.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='ikhatri' date='2019-11-28T06:01:38Z'>
		I have tried on colab with TF version 2.0 and was able to reproduce the issue.If i disable eager execution ( and run the code i am not seeing any issue.I have tried using TF  2.1.0dev20191127 version and i am seeing different error message.. Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/9d44e8521891131fb3a3d4a179d3cf01/untitled413.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='ikhatri' date='2019-11-28T06:52:16Z'>
		&lt;denchmark-link:https://github.com/ikhatri&gt;@ikhatri&lt;/denchmark-link&gt;
,
Can you please check Stack Overflow &lt;denchmark-link:https://stackoverflow.com/questions/55552538/tensorflow-2-0-attributeerror-tensor-name-is-meaningless-when-eager-execution?rq=1&gt;Link1&lt;/denchmark-link&gt;
  and &lt;denchmark-link:https://stackoverflow.com/questions/52340101/tensor-name-is-meaningless-in-eager-execution&gt;Link2&lt;/denchmark-link&gt;
 and let us know if it resolves your issue. Thanks!
		</comment>
		<comment id='3' author='ikhatri' date='2019-11-28T22:59:46Z'>
		&lt;denchmark-link:https://github.com/rmothukuru&gt;@rmothukuru&lt;/denchmark-link&gt;
 The code sample above is intentionally incorrect in order to demonstrate a bug with the functionality of the  under eager execution. The issue can be "resolved" in the code sample by wrapping the call to  in a  which will allow the auto-grad to differentiate the full network and provide gradients for every variable.
However the intention with the above code snippet is to demonstrate the bug where the apply_gradients() function calls _filter_grads() which logs a warning using Tensor.name. The intended functionality of this function is to filter out variables that have un-computable gradients and warn the user (in case they have made a mistake, as I did above). The issue is that under eager execution, this warning message fails to print due to eager tensors not having a name. This behavior should be fixed to be consistent under both eager execution and non-eager execution so that the warning message does not crash the users code and is instead logged.
I'm not sure what the best way to go about this would be, as I'm unsure what the intended design pattern for code under eager execution should be. Having if eager_execution: blocks everywhere seems like the wrong way to do it, however it would fix the issue.
The below code snippet is from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L1040&gt;line 1040 of optimizer_v2.py&lt;/denchmark-link&gt;
 and I've modified it to fix the issue, though again I'm unsure if this is the best way to do so.
if vars_with_empty_grads:
	if context.executing_eagerly():
		logging.warning(
		"%i of your variables had that gradients could not be computed",
		len(vars_with_empty_grads))
	else:
		logging.warning(
			("Gradients do not exist for variables %s when minimizing the loss."),
			([v.name for v in vars_with_empty_grads]))
		</comment>
		<comment id='4' author='ikhatri' date='2019-12-21T05:28:16Z'>
		This is not really an optimizer issue when the "variable" is actually a "tensor".
		</comment>
		<comment id='5' author='ikhatri' date='2019-12-21T05:28:21Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34635&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34635&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='ikhatri' date='2020-06-28T01:45:54Z'>
		I have the same issue, but only when I try to train on the TPU, it works fine with GPU
This is the full error message:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-24-1f67c891bad4&gt; in &lt;module&gt;()
      5         validation_steps=val_steps,
      6         validation_freq=1,
----&gt; 7         callbacks=callbacks)
      8 
      9 # else:

10 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
     64   def _method_wrapper(self, *args, **kwargs):
     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
---&gt; 66       return method(self, *args, **kwargs)
     67 
     68     # Running inside `run_distribute_coordinator` already.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
    846                 batch_size=batch_size):
    847               callbacks.on_train_batch_begin(step)
--&gt; 848               tmp_logs = train_function(iterator)
    849               # Catch OutOfRangeError for Datasets of unknown size.
    850               # This blocks until the batch has finished executing.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    578         xla_context.Exit()
    579     else:
--&gt; 580       result = self._call(*args, **kwds)
    581 
    582     if tracing_count == self._get_tracing_count():

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    625       # This is the first call of __call__, so we have to initialize.
    626       initializers = []
--&gt; 627       self._initialize(args, kwds, add_initializers_to=initializers)
    628     finally:
    629       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    504     self._concrete_stateful_fn = (
    505         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--&gt; 506             *args, **kwds))
    507 
    508     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2444       args, kwargs = None, None
   2445     with self._lock:
-&gt; 2446       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2447     return graph_function
   2448 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2775 
   2776       self._function_cache.missed.add(call_context_key)
-&gt; 2777       graph_function = self._create_graph_function(args, kwargs)
   2778       self._function_cache.primary[cache_key] = graph_function
   2779       return graph_function, args, kwargs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2665             arg_names=arg_names,
   2666             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 2667             capture_by_value=self._capture_by_value),
   2668         self._function_attributes,
   2669         # Tell the ConcreteFunction to clean up its graph once it goes out of

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    979         _, original_func = tf_decorator.unwrap(python_func)
    980 
--&gt; 981       func_outputs = python_func(*func_args, **func_kwargs)
    982 
    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    439         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    440         # the function a weak reference to itself to avoid a reference cycle.
--&gt; 441         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    442     weak_wrapped_fn = weakref.ref(wrapped_fn)
    443 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    966           except Exception as e:  # pylint:disable=broad-except
    967             if hasattr(e, "ag_error_metadata"):
--&gt; 968               raise e.ag_error_metadata.to_exception(e)
    969             else:
    970               raise

AttributeError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    &lt;ipython-input-7-9641c6148b59&gt;:21 train_step  *
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
    /usr/local/lib/python3.6/dist-packages/tensorflow_addons/optimizers/weight_decay_optimizers.py:149 apply_gradients  *
        return super().apply_gradients(grads_and_vars, name=name)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:472 apply_gradients  **
        grads_and_vars = _filter_grads(grads_and_vars)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1223 _filter_grads
        ([v.name for v in vars_with_empty_grads]))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1223 &lt;listcomp&gt;
        ([v.name for v in vars_with_empty_grads]))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1123 name
        "Tensor.name is meaningless when eager execution is enabled.")

    AttributeError: Tensor.name is meaningless when eager execution is enabled.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='ikhatri' date='2020-07-02T15:50:49Z'>
		Yeah this issue has been closed prematurely in my opinion. I guess a better way to explain it would be like so:
The user (me!) makes an error by switching a "variable" and a "tensor". Just like &lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
 said, it's not an optimizer issue because you the user has made the mistake. TensorFlow tries to inform the user of their error. It does so by printing an error message stating that the "Gradients do not exist for xyz". This is a perfectly adequate error message because "variables" don't have gradients and if this message was printed the user could be expected to fix their code.
The actual problem here though is that the error message itself never prints because in eager mode tensors don't have names. So if you are in eager mode and you make this mistake you'll be left with a cryptic error message about "Tensor.name is meaningless" that has nothing to do with your actual mistake. The bug here is with the error message not the actual optimizer and I still think it should be fixed, but that's not my call 🤷 .
EDIT: A quick clarification that I should probably make is that this error message could be printed for a variety of reasons that aren't the user switching a "tensor" and a "variable" like I did in my original post. The fact that the error messaging is broken is the issue regardless of what code-path the user takes to get there. &lt;denchmark-link:https://github.com/Santosh-Gupta&gt;@Santosh-Gupta&lt;/denchmark-link&gt;
 it looks like you got here via different code path (did you have eager execution enabled on both the GPU and TPU?)
		</comment>
		<comment id='8' author='ikhatri' date='2020-07-02T18:33:22Z'>
		
Yeah this issue has been closed prematurely in my opinion. I guess a better way to explain it would be like so:
The user (me!) makes an error by switching a "variable" and a "tensor". Just like @tanzhenyu said, it's not an optimizer issue because you the user has made the mistake. TensorFlow tries to inform the user of their error. It does so by printing an error message stating that the "Gradients do not exist for xyz". This is a perfectly adequate error message because "variables" don't have gradients and if this message was printed the user could be expected to fix their code.
The actual problem here though is that the error message itself never prints because in eager mode tensors don't have names. So if you are in eager mode and you make this mistake you'll be left with a cryptic error message about "Tensor.name is meaningless" that has nothing to do with your actual mistake. The bug here is with the error message not the actual optimizer and I still think it should be fixed, but that's not my call 🤷 .
EDIT: A quick clarification that I should probably make is that this error message could be printed for a variety of reasons that aren't the user switching a "tensor" and a "variable" like I did in my original post. The fact that the error messaging is broken is the issue regardless of what code-path the user takes to get there. @Santosh-Gupta it looks like you got here via different code path (did you have eager execution enabled on both the GPU and TPU?)

I am using TF 2.0 where eager is on by default, and is discouraged not to turn it off.
I made a minimal example of my code for others to debug, but in my minimal example, I am getting a completely different error message.

NotImplementedError: TPUStrategy.run(fn, ...) does not support pure eager execution. please make sure the function passed into strategy.run is a tf.function or strategy.run is called inside a tf.function if eager behavior is enabled.

Even though I believe I removed all pytorch functions in the code, and I am only using tensorflow operations.
I detail the issue in this stackoverflow post
&lt;denchmark-link:https://stackoverflow.com/questions/62650379/error-when-running-on-tpu-notimplementederror-tpustrategy-runfn-does-n&gt;https://stackoverflow.com/questions/62650379/error-when-running-on-tpu-notimplementederror-tpustrategy-runfn-does-n&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>