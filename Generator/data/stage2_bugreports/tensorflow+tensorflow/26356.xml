<bug id='26356' author='alvaroslm' open_date='2019-03-05T10:30:51Z' closed_time='2019-12-31T23:07:49Z'>
	<summary>Low GPU usage for inference when multithreaded vs multiprocess c++</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7 64-bit
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.13.1
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source): vs2015
CUDA/cuDNN version: 10.0 / 7.5
GPU model and memory: RTX 2060 6GB

Describe the current behavior
I'm using Python to build a graph (using tf.keras.layers) that contains cudnnlstm, timedistributed, dense layers. It crashes the RTX 2060 (there's a bug report for that already, this is not this issue) if I don't set options allow_growth  or limit ram usage.
I'm using the model for inference from C++, my batch size is known as soon as the program starts but it can vary each time it's started so the graph doesn't have a fixed input size. I'm creating several threads with one Session on each. Tensorflow GPU usage is low (38%) and it eventually gobbles up all gpu ram available if it's not manually limited, I can't simply increase the batch sizes, because different weights are loaded on each Run().
Now, the issue here is that it doesn't matter much how many parallel sessions I run in multiple threads, GPU utilization is still low, but if I limit gpu RAM usage so that I can run two separate processes (not threads), then they can both use about half the ram and increase GPU utilization to 80%. Why can't Tensorflow figure out a way to do that in just one process with multiple threads?  It's the same inputs and results, just multi-threaded vs multi-process.
Describe the expected behavior
TF should utilize at least 80%+ GPU in my case since I'm just running inference sessions in parallel in one process, instead of having to resort to limiting ram usage and starting several processes.
Code to reproduce the issue
Too much code to extract to make a barebones example, but I could do it if absolutely necessary.
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='alvaroslm' date='2019-03-12T23:46:29Z'>
		&lt;denchmark-link:https://github.com/alvaroslm&gt;@alvaroslm&lt;/denchmark-link&gt;
 can you confirm that when two processes are used, you are actually getting double the inference examples/sec? Measuring GPU utilization is not always accurate, and perhaps the increased utilization comes from extra overhead of running two processes.
		</comment>
		<comment id='2' author='alvaroslm' date='2019-12-31T23:07:49Z'>
		Closing due to lack of activity.  &lt;denchmark-link:https://github.com/alvaroslm&gt;@alvaroslm&lt;/denchmark-link&gt;
 please feel free to reopen with a reply to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/26356#issuecomment-472224051&gt;Reed's question above&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='3' author='alvaroslm' date='2019-12-31T23:07:51Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26356&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26356&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>