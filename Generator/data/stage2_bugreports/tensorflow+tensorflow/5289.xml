<bug id='5289' author='deeptigp' open_date='2016-10-30T20:25:16Z' closed_time='2017-03-03T23:39:27Z'>
	<summary>TF freezes and gets killed while training /saving a network</summary>
	<description>
I am trying to train a deep network from scratch (a 4 layer &lt;denchmark-link:https://www.tensorflow.org/versions/r0.11/tutorials/deep_cnn/index.html#cifar-10-model&gt;CIFAR&lt;/denchmark-link&gt;
 network) on an image collection of 100K images. The TF instance hangs (while training or while saving using tf.Saver) and then gets killed without any error message.
I've tried the following things without any use:
a. Reduced the batch size from 32 to 8.
b. Set config's allow GPU growth option to True
But the problem still persists.
Has anybody else faced this issue? Is this because of insufficient memory? Is there a way to train a model under constrained memory conditions (although, 12 GB isn't bad)?
Any tips to avoid this would be very helpful.
&lt;denchmark-h:h3&gt;What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?&lt;/denchmark-h&gt;

I've looked at other similar issues posted but haven't found any useful solution.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/2121&gt;#2121&lt;/denchmark-link&gt;

&lt;denchmark-link:http://stackoverflow.com/questions/38958737/tensorflow-training-got-stuck-after-some-steps-how-to-investigate&gt;http://stackoverflow.com/questions/38958737/tensorflow-training-got-stuck-after-some-steps-how-to-investigate&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/1962&gt;#1962&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

GPU details: I am running this model on a  Tesla K40c (12GB memory).
Operating System:  4.7.0-1-amd64 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/1&gt;#1&lt;/denchmark-link&gt;
 SMP Debian 4.7.6-1 (2016-10-07) x86_64 GNU/Linux
Installed version of CUDA and cuDNN:
/opt/cuda-8.0/lib64/libcudnn.so.5
/opt/cuda-8.0/lib64/libcudart.so -&gt; libcudart.so.8.0
(Cuda version: 8.0 and cuDNN version 5)

The output from python -c "import tensorflow; print(tensorflow.__version__)".
0.11.0rc1

If installed from source, provide

The commit hash (git rev-parse HEAD)
ec7f37e
The output of bazel version

Build label: 0.3.2
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Oct 7 17:25:10 2016 (1475861110)
Build timestamp: 1475861110
Build timestamp as int: 1475861110
&lt;denchmark-h:h3&gt;If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)&lt;/denchmark-h&gt;

This issue is happening when I am trying to train/ save a model
	</description>
	<comments>
		<comment id='1' author='deeptigp' date='2016-10-30T21:24:27Z'>
		There should always be a log somewhere when you get killed, unless you actually run out of memory.
Try setting the env var GLOG_logtostderr=1, and also look at dmesg to see if you have been sacrificed.
		</comment>
		<comment id='2' author='deeptigp' date='2016-10-30T22:28:17Z'>
		Does GLOG_logtostderr do anything? I thought that was for gflags only which
isn't in TF. BTW, you can get hangs when the system isn't allowing me to write the file (ie, if you write to NFS and network is down)
On Sun, Oct 30, 2016 at 2:24 PM, drpngx &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;
 wrote:

There should always be a log somewhere when you get killed, unless you
actually run out of memory.
Try setting the env var GLOG_logtostderr=1, and also look at dmesg to see
if you have been sacrificed.
—
You are receiving this because you are subscribed to this thread.
Reply to this email directly, view it on GitHub
#5289 (comment),
or mute the thread
https://github.com/notifications/unsubscribe-auth/AABaHIAvqUfR_EbS-amExjPhdcN1i5aiks5q5QsOgaJpZM4KkfA-
.

		</comment>
		<comment id='3' author='deeptigp' date='2016-10-30T22:49:06Z'>
		Oh, we don't use glog, right.
		</comment>
		<comment id='4' author='deeptigp' date='2016-11-01T21:45:04Z'>
		I am seeing what I think is a similar issue, but am only training on CPU; when I sample the process in a hung state, I see get the following info. In my case, I just see an indefinite hang. Seems to happen randomly, but consistently if I run the program for a few hours, unfortunately. I am also on version, 0.11.0rc0, if that matters.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/565125/tflow_hang.txt&gt;tflow_hang.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='deeptigp' date='2016-11-01T22:08:19Z'>
		&lt;denchmark-link:https://github.com/nroth1&gt;@nroth1&lt;/denchmark-link&gt;
 -- a common way for "random hangs" is by causing a deadlock with
queues, partial solution  is to set operation timeout as &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/2130#issuecomment-215180165&gt;here&lt;/denchmark-link&gt;

and retry on error. More fundamentally is to analyze your queues and
prevent deadlock from occuring, as &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/4917&gt;here&lt;/denchmark-link&gt;

On Tue, Nov 1, 2016 at 2:45 PM, nroth1 &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;
 wrote:

I am seeing what I think is a similar issue, but am only training on CPU;
when I sample the process in a hung state, I see get the following info. In
my case, I just see an indefinite hang. Seems to happen randomly, but
consistently if I run the program for a few hours, unfortunately. I am also
on version, 0.11.0rc0, if that matters.
tflow_hang.txt
https://github.com/tensorflow/tensorflow/files/565125/tflow_hang.txt
—
You are receiving this because you commented.
Reply to this email directly, view it on GitHub
#5289 (comment),
or mute the thread
https://github.com/notifications/unsubscribe-auth/AABaHBm5M5lHsKvIUr-8rrAXgy2CARPhks5q57LjgaJpZM4KkfA-
.

		</comment>
		<comment id='6' author='deeptigp' date='2016-11-01T23:22:34Z'>
		Hi, thanks for writing back!
I agree that the partial solution would maybe be a work around, but I was hoping to understand what was happening and maybe get a cleaner fix. I am actually producing my issue without queues, in the strictest sense; I just have some python threads running in parallel (I am aware that I won't get true multithreading in python). In pseudo code this just boils down to:
thread1: my_session.run(some_op) thread2: saver.save(my_session) 
In this case, there is no dequeue op or queue set of threads that I explicitly call, so it seems like I ought not get the deadlocking issue as described in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/4917&gt;#4917&lt;/denchmark-link&gt;
? But maybe under the covers tensorflow parallelizes access to this shared session object in a way which still leaves this problem open even if you aren't using tensorflow threads/queues to make your calls?
Thanks a bunch for your help!!
		</comment>
		<comment id='7' author='deeptigp' date='2016-11-01T23:48:24Z'>
		If you don't have queues, you shouldn't get deadlocks like this. The only thing I can think of is that your write to disk is hanging on the OS level, so the saving op never terminates
		</comment>
		<comment id='8' author='deeptigp' date='2016-11-01T23:50:15Z'>
		hmm, ok, thanks a bunch! I will poke around and update if I find any more information.
		</comment>
		<comment id='9' author='deeptigp' date='2016-11-03T01:40:14Z'>
		So, I have been able to isolate the issue to roughly when I call save on a session at the same time that I call session.run(my_op). Interestingly, it is the session.run call that hangs, not the saver. When I do so, I get the following backtrace:
"
2592 _wrap_TF_Run(object, object)  (in _pywrap_tensorflow.so) + 1301  [0x106599365]
2592 tensorflow::TF_Run_wrapper(TF_Session_, TF_Buffer const_, object, tensorflow::gtl::InlinedVector&lt;char const_, 8&gt; const&amp;, tensorflow::gtl::InlinedVector&lt;char const*, 8&gt; const&amp;, TF_Status_, tensorflow::gtl::InlinedVector&lt;object, 8&gt;, TF_Buffer)  (in _pywrap_tensorflow.so) + 55  [0x1065b25e7]
2592 tensorflow::TF_Run_wrapper_helper(TF_Session_, char const_, TF_Buffer const_, object, tensorflow::gtl::InlinedVector&lt;char const*, 8&gt; const&amp;, tensorflow::gtl::InlinedVector&lt;char const*, 8&gt; const&amp;, TF_Status_, tensorflow::gtl::InlinedVector&lt;object, 8&gt;, TF_Buffer)  (in _pywrap_tensorflow.so) + 4930  [0x1065b1492]
2592 PyEval_RestoreThread  (in Python) + 62  [0x10577defa]
2592 PyThread_acquire_lock  (in Python) + 101  [0x1057ad790]
2592 _pthread_cond_wait  (in libsystem_pthread.dylib) + 712  [0x7fffaed7c97a]
"
I also saw a system log stating:
'pthread_cond_wait: Resource busy'
It looks like the session run call is trying to acquire a lock? Is that expected?
Is it not safe to a save a session at the same time it is evaluating some other op? I can't find this discussed explicitly in the docs. Thanks again so much for the help!
		</comment>
		<comment id='10' author='deeptigp' date='2016-11-03T02:38:26Z'>
		"PyThread_acquire_lock" is how Python GIL is acquired. If GIL is the issue,
there must be another Python thread holding the lock. However, session.run
is supposed to be releasing the lock because of line below in tf_session.i,
so running saver ops shouldn't block evaluation.
// Release the Python GIL for the duration of most methods.
%exception {
Py_BEGIN_ALLOW_THREADS;
$action
Py_END_ALLOW_THREADS;
}



tensorflow/tensorflow/python/client/tf_session.i


         Line 45
      in
      47dd089






 Py_BEGIN_ALLOW_THREADS; 





You could look at existing Python threads to see which other thread is
hanging onto the GIL
def stacktraces(self):
code = []
for threadId, stack in sys._current_frames().items():
code.append("\n thread: %s" % threadId)
for filename, lineno, name, line in
traceback.extract_stack(stack):
code.append('\n***  File: "%s", line %d, in %s' % (filename,
lineno,
name))
if line:
code.append("  %s" % (line.strip()))
&lt;denchmark-code&gt;    for line in code:
        sys.stderr.write(line)
    sys.stderr.write("\n")
&lt;/denchmark-code&gt;

On Wed, Nov 2, 2016 at 6:40 PM, nroth1 &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;
 wrote:

So, I have been able to isolate the issue to roughly when I call save on a
session at the same time that I evaluate an op on the session. When I do
so, I get the following backtrace:
2592 wrap_TF_Run(object, object) (in pywrap_tensorflow.so) + 1301
[0x106599365]
2592 tensorflow::TF_Run_wrapper(TF_Session, TF_Buffer const, object,
tensorflow::gtl::InlinedVector&lt;char const_, 8&gt; const&amp;, tensorflow::gtl::InlinedVector&lt;char
const*, 8&gt; const&amp;, TF_Status_, tensorflow::gtl::InlinedVector&lt;object,
8&gt;, TF_Buffer) (in pywrap_tensorflow.so) + 55 [0x1065b25e7]
2592 tensorflow::TF_Run_wrapper_helper(TF_Session, char const_,
TF_Buffer const_, object, tensorflow::gtl::InlinedVector&lt;char const*,
8&gt; const&amp;, tensorflow::gtl::InlinedVector&lt;char const*, 8&gt; const&amp;,
TF_Status_, tensorflow::gtl::InlinedVector&lt;object, 8&gt;, TF_Buffer) (in
_pywrap_tensorflow.so) + 4930 [0x1065b1492]
2592 PyEval_RestoreThread (in Python) + 62 [0x10577defa]
2592 PyThread_acquire_lock (in Python) + 101 [0x1057ad790]
2592 _pthread_cond_wait (in libsystem_pthread.dylib) + 712 [0x7fffaed7c97a]
2592 __psynch_cvwait (in libsystem_kernel.dylib) + 10 [0x7fffaec93c8a]
It looks like the session run call is trying to acquire a lock? Is that
expected?
Is it not safe to a save a session at the same time it is being evaluated?
I can't find this discussed explicitly in the docs. Thanks again so much
for the help!
—
You are receiving this because you commented.
Reply to this email directly, view it on GitHub
#5289 (comment),
or mute the thread
https://github.com/notifications/unsubscribe-auth/AABaHFYzkqwrsl4YRsFOKP16sAAIUWIcks5q6TuEgaJpZM4KkfA-
.

		</comment>
		<comment id='11' author='deeptigp' date='2016-11-09T18:58:40Z'>
		So I am still able to occasionally reproduce the freezing. This only occurs when a save operation and a session.run operation occur at roughly the same time. I see the  following backtrace (abbreviated to just show the tensorflow call where the deadlock happens), which seems to indicate that the hang is happening when tflow tries to acquire a lock in ops.py:
`
***  File: "/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 717, in run  run_metadata_ptr)
***  File: "/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 869, in _run  allow_operation=False)
***  File: "/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 2457, in as_graph_element  with self._lock:`
Any idea on why we would see deadlock there? Or what else could be holding that graph lock. As before, the save op seems to successfully terminate.
For what it is worth, I had a very similar version of this code running before I updated to the latest tensorflow (was all the way back on .8) , and did not observe this behavior.
Thanks a bunch for the time and help!
		</comment>
		<comment id='12' author='deeptigp' date='2017-01-18T17:12:46Z'>
		&lt;denchmark-link:https://github.com/nroth1&gt;@nroth1&lt;/denchmark-link&gt;
 could this be a freeze connected to you running out of memory? If you switch from  to the new MonitoredTrainingSession framework, saving happens in the same Python thread as everything else, so you don't have unlucky parallel  exploding the memory occasionally
		</comment>
		<comment id='13' author='deeptigp' date='2017-02-17T00:35:28Z'>
		Hi, sorry for the delayed response. I don't believe the freeze correlated to high memory usage. I have recently upgraded to tensorflow 1.0, so will write back with more details if the problem reproduces itself.
		</comment>
		<comment id='14' author='deeptigp' date='2017-02-17T19:16:49Z'>
		I am able to reproduce the issue on version 1.0, and will follow up with more details. Thanks again for all your time!
		</comment>
		<comment id='15' author='deeptigp' date='2017-02-19T21:26:59Z'>
		I'm able to reproduce the hangs on a smaller example, and have logs attached. I'm not sure if this is necessarily the same hang as I was seeing above, as the freeze seems to happen in a numpy call, not at the tensorflow level. The example is pretty silly, but I basically just make and evaluate two graphs, while saving them in the background on separate threads. Eventually, it hangs at:
***  File: "crash_eg.py", line 93, in   main_thread2()
***  File: "crash_eg.py", line 79, in main_thread2  session2.run(outputs2,feed_dict = fd)
***  File: "/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 767, in run  run_metadata_ptr)
***  File: "/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 938, in _run  np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)
***  File: "/usr/local/lib/python2.7/site-packages/numpy/core/numeric.py", line 531, in asarray  return array(a, dtype, copy=False, order=order)
I have attached the full logs below. Need to investigate more about whether this is the same issue as reported above, but thought I would leave the info just in case. I should mention I am on a mac, and have read that there can be some issues with numpy and threading on Mac OS, so maybe that is involved. I am not sure if anyone else has ever seen a similar issue? Thanks again for all your time!
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/786206/hang_logs.txt&gt;hang_logs.txt&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;import tensorflow as tf
import threading
import numpy as np
import time
import sys
import traceback

g1 = tf.Graph()
g2 = tf.Graph()

embedding_size = 150
pad_len = 100

with g1.as_default():
    batch_sentence = [tf.placeholder(tf.float32, shape = [None,embedding_size],name= 'tokens') for _ in range(pad_len)]
    cell = tf.contrib.rnn.GRUCell(embedding_size) 
    output1, states1 = tf.contrib.rnn.static_rnn(cell, batch_sentence, dtype = tf.float32)
    loss = tf.reduce_sum(output1[0] - output1[99])
    optimizer = tf.train.AdamOptimizer(1, epsilon=1e-5)
    optimizer.minimize(loss)
    session1 = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1))
    session1.run(tf.initialize_all_variables())
    saver1 = tf.train.Saver()

with g2.as_default():
    batch_sentence2 = [tf.placeholder(tf.float32, shape = [None,embedding_size],name= 'tokens') for _ in range(pad_len)]
    cell = tf.contrib.rnn.GRUCell(embedding_size) 
    outputs2, states2 = tf.contrib.rnn.static_rnn(cell, batch_sentence2, dtype = tf.float32)
    loss = tf.reduce_sum(outputs2[0] - outputs2[99])
    optimizer = tf.train.AdamOptimizer(1, epsilon=1e-5)
    optimizer.minimize(loss)
    session2 = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1))
    session2.run(tf.initialize_all_variables())
    saver2 = tf.train.Saver()

lock_map = [threading.Lock(),threading.Lock()]


def report_thread():
    while(True):
        def stacktraces():
            code = []
            for threadId, stack in sys._current_frames().items():
                code.append("\n thread: %s" % threadId)
                for filename, lineno, name, line in traceback.extract_stack(stack):
                    code.append('\n***  File: "%s", line %d, in %s' % (filename, lineno, name))
                    if line:
                        code.append("  %s" % (line.strip()))
            for line in code:
                sys.stderr.write(line)
            sys.stderr.write("\n")
        stacktraces()
        time.sleep(300)


def save_wrapper():
    print time.time()
    curr_lock = lock_map[0]
    with curr_lock:
        saver1.save(session1,'out1')
    print time.time()

def save_wrapper2():
    print time.time()
    curr_lock = lock_map[1]
    with curr_lock:
        saver2.save(session2,'out2')
    print time.time()

def main_thread2():
    X = np.random.random((5,150))
    while(True):
        print '------'
        for _ in range(60):
            fd = {}

            for i in range(pad_len):
                fd[batch_sentence2[i]] = X
            session2.run(outputs2,feed_dict = fd)
        save_thread = threading.Thread(target = save_wrapper2)
        save_thread.start()

        for _ in range(60):
            fd = {}
            for i in range(pad_len):
                fd[batch_sentence[i]] = X
            session1.run(output1,feed_dict = fd)
        save_thread = threading.Thread(target = save_wrapper)
        save_thread.start()
        
t1 = threading.Thread(target = report_thread)
t1.start()
main_thread2()


&lt;/denchmark-code&gt;

		</comment>
		<comment id='16' author='deeptigp' date='2017-03-03T23:39:27Z'>
		Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
	</comments>
</bug>