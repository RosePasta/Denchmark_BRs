<bug id='19570' author='jianlong-yuan' open_date='2018-05-26T09:27:35Z' closed_time='2018-08-25T03:37:15Z'>
	<summary>[Bug] Very slow operation of Cumsum in tensorflow 1.6</summary>
	<description>
It seems in tensorflow the tf.cumsum operation is extremely slow on GPU, and takes a huge amount of time (~99% of the total time).
In Pytorch, as expected, the sort operation is the one that takes the most time, cumsum is virtually instant on GPU.
I give an issues on other‘s’ gitHub, we get conclusion that is cumsum is very slow aginst pytorch.
we issue is &lt;denchmark-link:https://github.com/bermanmaxim/LovaszSoftmax/issues/6&gt;bermanmaxim/LovaszSoftmax#6&lt;/denchmark-link&gt;

code is &lt;denchmark-link:https://github.com/bermanmaxim/LovaszSoftmax/tree/master/tensorflow&gt;https://github.com/bermanmaxim/LovaszSoftmax/tree/master/tensorflow&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;def lovasz_grad(gt_sorted):
    """
    Computes gradient of the Lovasz extension w.r.t sorted errors
    See Alg. 1 in paper
    """
    gts = tf.reduce_sum(gt_sorted)
    **intersection = gts - tf.cumsum(gt_sorted)
    union = gts + tf.cumsum(1. - gt_sorted)**
    jaccard = 1. - intersection / union
    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)
    return jaccard
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='jianlong-yuan' date='2018-05-26T09:33:41Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/18137551/40574769-e04b3f30-610a-11e8-8308-e162e7365a16.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='jianlong-yuan' date='2018-06-02T00:04:39Z'>
		@yjl9122 Can you provide the extra details about your setup as well as full code to replicate in docker from the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/new&gt;New Issue Template&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='3' author='jianlong-yuan' date='2018-06-02T10:20:08Z'>
		@angersson
I use tensorflow1.6 and 1.8, gpu is tesla P40.
		</comment>
		<comment id='4' author='jianlong-yuan' date='2018-06-02T10:20:19Z'>
		&lt;denchmark-code&gt;In [1]:
import numpy as np
import tensorflow as tf
import time
import matplotlib.pyplot as plt
%matplotlib inline
In [2]:
N = 5 * 500 * 500
In [3]:
x = np.array(np.random.randn(N), dtype=np.float32)
pytorch ops
In [4]:
import torch
In [9]:
%%timeit -o
Xt = torch.tensor(x).cuda()
torch.cuda.synchronize()
1.36 ms ± 48.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Out[9]:
&lt;TimeitResult : 1.36 ms ± 48.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)&gt;
In [10]:
pytorch = {}
In [11]:
torch_gputransfert = np.array(_.timings)
In [12]:
%%timeit -o
Xt = torch.tensor(x).cuda()
torch.cumsum(Xt, 0)
torch.cuda.synchronize()
1.44 ms ± 11.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Out[12]:
&lt;TimeitResult : 1.44 ms ± 11.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)&gt;
In [13]:
pytorch['cumsum'] = np.array(_.timings) - torch_gputransfert
In [14]:
%%timeit -o
Xt = torch.tensor(x).cuda()
torch.sort(Xt, 0)
torch.cuda.synchronize()
5.21 ms ± 75.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
Out[14]:
&lt;TimeitResult : 5.21 ms ± 75.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)&gt;
In [15]:
pytorch['sort'] = np.array(_.timings) - torch_gputransfert
Tensorflow ops
In [16]:
X = tf.placeholder(tf.float32, shape=(None,), name=None)
Y = tf.cumsum(X)
Z = tf.nn.top_k(X, tf.shape(X)[0])
A = tf.identity(X)

config = tf.ConfigProto()
config.gpu_options.allow_growth=True

sess = tf.Session(config=config)
In [17]:
%%timeit -o
sess.run(A, feed_dict={X: x})
1.59 ms ± 44.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Out[17]:
&lt;TimeitResult : 1.59 ms ± 44.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)&gt;
In [18]:
tensf = {}
In [19]:
tf_gputransfert = np.array(_.timings)
In [20]:
%%timeit -o
sess.run(Y, feed_dict={X: x})
330 ms ± 1.36 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
Out[20]:
&lt;TimeitResult : 330 ms ± 1.36 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)&gt;
In [21]:
tensf['cumsum'] = np.array(_.timings) - tf_gputransfert
In [22]:
%%timeit -o
sess.run(Z, feed_dict={X: x})
10.9 ms ± 36.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
Out[22]:
&lt;TimeitResult : 10.9 ms ± 36.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)&gt;
In [23]:
tensf['sort'] = np.array(_.timings) - tf_gputransfert
Summary
In [40]:
r = 1/2
x = np.arange(len(pytorch))
funs = pytorch.keys()

plt.bar(x - r/4, [v.mean() for v in pytorch.values()], width=r/2, yerr=[v.std() for v in pytorch.values()], label='pytorch')
plt.bar(x + r/4, [v.mean() for v in tensf.values()], width=r/2, yerr=[v.std() for v in tensf.values()], label='tensorflow')
plt.legend()
plt.gca().set_yscale('log')
plt.xticks(x, funs);
plt.ylabel('time (s)')
plt.savefig('timings.svg')
plt.title('timings ({} pixels)'.format(N))
Out[40]:
Text(0.5,1,'timings (1250000 pixels)')
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='jianlong-yuan' date='2018-06-02T10:21:03Z'>
		the code is provided at &lt;denchmark-link:url&gt;https://github.com/bermanmaxim/LovaszSoftmax/blob/master/tensorflow/profile_ops.ipynb&lt;/denchmark-link&gt;

so you can use it
		</comment>
		<comment id='6' author='jianlong-yuan' date='2018-06-15T09:36:15Z'>
		@angersson Any news?
		</comment>
		<comment id='7' author='jianlong-yuan' date='2018-06-21T19:43:21Z'>
		Thanks for the update and the code, @yjl9122. &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
, can you take a look at this?
		</comment>
		<comment id='8' author='jianlong-yuan' date='2018-06-30T03:30:00Z'>
		Having the same issue here with cumsum
		</comment>
		<comment id='9' author='jianlong-yuan' date='2018-07-13T05:24:04Z'>
		@angersson &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 Any news
		</comment>
		<comment id='10' author='jianlong-yuan' date='2018-08-06T09:10:40Z'>
		No news &lt;denchmark-link:https://github.com/tensorflowbutler&gt;@tensorflowbutler&lt;/denchmark-link&gt;
 @angersson &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='jianlong-yuan' date='2018-08-22T19:19:08Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='12' author='jianlong-yuan' date='2018-09-03T20:16:50Z'>
		Was this ever resolved? I'm also running into performance issues using tf.cumsum on GPU
		</comment>
		<comment id='13' author='jianlong-yuan' date='2018-09-07T03:29:45Z'>
		@yjl9122 Have you solved this? I am also facing this issue when using LovaszSoftmax. Thx.
		</comment>
		<comment id='14' author='jianlong-yuan' date='2018-11-04T02:22:55Z'>
		Up
		</comment>
		<comment id='15' author='jianlong-yuan' date='2018-11-17T05:33:40Z'>
		Should be reasonably fixed now.
		</comment>
		<comment id='16' author='jianlong-yuan' date='2020-07-07T03:17:14Z'>
		As of Tensorflow 2.2 this op is still extremely slow on GPU with tf.complex128 inputs
Splitting into real and imag parts and doing the cumsums separately, then merging to complex again did the trick, as this sped up my forward propagation by a factor of 10.
		</comment>
	</comments>
</bug>