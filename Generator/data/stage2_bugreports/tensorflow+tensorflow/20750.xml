<bug id='20750' author='ppwwyyxx' open_date='2018-07-12T19:33:31Z' closed_time='2018-10-26T22:33:31Z'>
	<summary>RunOptions.FULL_TRACE produce wrong timestamps</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04
TensorFlow installed from (source or binary):binary
TensorFlow version (use command below):v1.9.0-0-g25c197e023 1.9.0
Python version: 3.6
Bazel version (if compiling from source):n/a
GCC/Compiler version (if compiling from source):n/a
CUDA/cuDNN version:9.0.176/7.1.1
GPU model and memory:P100 16G (can reproduce on other GPU model as well)
Exact command to reproduce: see below

#!/usr/bin/env python
# -*- coding: utf-8 -*-

import sys
import os
import tqdm
from datetime import datetime
import tensorflow as tf
from tensorflow.python.client import timeline

def build():
    image = tf.random_normal(shape=[64,32,32,3], dtype=tf.float32)
    label = tf.random_uniform(shape=[64], maxval=10, dtype=tf.int32)
    l = tf.transpose(image, [0, 3, 1, 2])
    for k in range(1, 100):
        l = tf.layers.conv2d(l, 16, 3, data_format='channels_first',
                name='conv{}'.format(k), padding='SAME')
    l = tf.reduce_mean(l, [2, 3])

    logits = tf.layers.dense(l, 10, name='linear')
    cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)
    cost = tf.reduce_mean(cost, name='cross_entropy_loss')
    return cost

if __name__ == '__main__':
    with tf.device('/gpu:0'):
        cost1 = build()
    with tf.device('/gpu:1'), tf.variable_scope(tf.get_variable_scope(), reuse=True):
        cost2 = build()
    cost = cost1 + cost2
    opt = tf.train.GradientDescentOptimizer(0.1)
    train_op = opt.minimize(cost)

    def write_tracing(idx, metadata):
        tl = timeline.Timeline(step_stats=metadata.step_stats)
        fname = os.path.join(
            '.', 'chrome-trace-{}.json'.format(idx))
        with open(fname, 'w') as f:
            f.write(tl.generate_chrome_trace_format(
                show_dataflow=True, show_memory=True))

    config = tf.ConfigProto()
    config.allow_soft_placement = True
    with tf.Session(config=config) as sess:
        sess.run(tf.global_variables_initializer())
        opt = tf.RunOptions()
        opt.trace_level = tf.RunOptions.FULL_TRACE

        for k in tqdm.trange(100):
            meta = tf.RunMetadata()
            sess.run(train_op, options=opt, run_metadata=meta)

            write_tracing(k, meta)

            for devst in meta.step_stats.dev_stats:
                for ns in devst.node_stats:
                    micro = timestamp = ns.all_start_micros // 1000000
                    timestamp = datetime.fromtimestamp(timestamp)
                    diff = timestamp - datetime.now()
                    if diff.days &gt; 100:
                        print(k, micro, timestamp)
                        #import IPython as IP; IP.embed()
                        #sys.exit()
The code above trains a CNN on two GPUs with FULL_TRACE enabled. The returned profiling information contains correct timestamps, but sometimes contains timestamps that are many years in the future. It prints the following output:
&lt;denchmark-code&gt;...
13 18446744073 2554-07-21 16:34:33                                                                                                                                                                  
13 18446744073 2554-07-21 16:34:33               
13 18446744073 2554-07-21 16:34:33                                                                                                                                                                  
13 18446744073 2554-07-21 16:34:33               
13 18446744073 2554-07-21 16:34:33               
13 18446744073 2554-07-21 16:34:33                                                                                                                                                                  
13 18446744073 2554-07-21 16:34:33               
13 18446744073 2554-07-21 16:34:33                                                                                                                                                                  
13 18446744073 2554-07-21 16:34:33               
13 18446744073 2554-07-21 16:34:33               
13 18446744073 2554-07-21 16:34:33
...
&lt;/denchmark-code&gt;

The issue was originally reported at &lt;denchmark-link:https://github.com/tensorpack/tensorpack/issues/819&gt;tensorpack/tensorpack#819&lt;/denchmark-link&gt;
.
The issue is more likely to happen after running about 50 steps.
The issue seems to disappear when training on one GPU, or training a small model.
	</description>
	<comments>
		<comment id='1' author='ppwwyyxx' date='2018-08-10T20:26:22Z'>
		&lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 Can you have a look at this? Seems possibly GPU-related.
		</comment>
		<comment id='2' author='ppwwyyxx' date='2018-08-21T11:55:13Z'>
		&lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensorflowbutler&gt;@tensorflowbutler&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ppwwyyxx&gt;@ppwwyyxx&lt;/denchmark-link&gt;

It seems like a problem in calling the cupti func:
tensorflow\core\platform\default\gpu\cupti_wrapper.cc
CuptiWrapper::ActivityGetNextRecord -&gt; cuptiActivityGetNextRecord
As the cupti-api reference mentioned, the reason is lack of device memory,is there anyway to resolve this problem?
Or am I miss anything?
Look forward to your reply.
It will return 0 timestamp on some CUPTI_ACTIVITY_KIND_CONCURRENT_KERNEL kernels in my testing.
LOG:
2018-08-21 11:37:10.099749: W tensorflow/core/platform/default/device_tracer.cc:583] ActivityCallback kernel-&gt;start:0 kernel-&gt;end:0 kernel-&gt;deviceId:0 kernel-&gt;streamId:13 kernel-&gt;correlationId:8928781
2018-08-21 11:37:10.099759: W tensorflow/core/platform/default/device_tracer.cc:583] ActivityCallback kernel-&gt;start:0 kernel-&gt;end:0 kernel-&gt;deviceId:0 kernel-&gt;streamId:13 kernel-&gt;correlationId:8928789
Cupti func reference:
&lt;denchmark-link:https://www.cs.rit.edu/~ark/cuda/doc/html/cupti/group__CUPTI__ACTIVITY__API.html&gt;https://www.cs.rit.edu/~ark/cuda/doc/html/cupti/group__CUPTI__ACTIVITY__API.html&lt;/denchmark-link&gt;

Inputs the previous record returned by cuptiActivityGetNextRecord and returns the next activity record from the buffer. If input value is NULL, returns the first activity record in the buffer. Records of kind CUPTI_ACTIVITY_KIND_CONCURRENT_KERNEL may contain invalid (0) timestamps, indicating that no timing information could be collected for lack of device memory.
		</comment>
		<comment id='3' author='ppwwyyxx' date='2018-10-05T19:08:21Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='4' author='ppwwyyxx' date='2018-10-25T23:05:46Z'>
		CC &lt;denchmark-link:https://github.com/nluehr&gt;@nluehr&lt;/denchmark-link&gt;

This looks like an issue with CUPTI. Nathan, could you help us triage this issue?
Thanks in advance!
		</comment>
		<comment id='5' author='ppwwyyxx' date='2018-10-26T21:54:49Z'>
		I can reproduce this on CUDA 9.0, but it appears to be fixed in CUDA 10.0.
		</comment>
		<comment id='6' author='ppwwyyxx' date='2018-10-26T22:33:31Z'>
		Thanks Nathan for the quick update. Closing this issue.
&lt;denchmark-link:https://github.com/ppwwyyxx&gt;@ppwwyyxx&lt;/denchmark-link&gt;
 feel free to re-open if the issue persists with CUDA 10 as well.
		</comment>
	</comments>
</bug>