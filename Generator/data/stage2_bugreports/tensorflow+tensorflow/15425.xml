<bug id='15425' author='ppwwyyxx' open_date='2017-12-17T10:31:02Z' closed_time='2018-03-21T01:27:50Z'>
	<summary>TypeError: broadcast() takes 1 positional argument but 2 were given</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
TensorFlow installed from (source or binary):source
TensorFlow version (use command below): b'v1.3.0-rc1-6044-g0b80606' 1.4.0    (2 days ago)
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:

&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

I ran tensorflow/benchmarks, and got the following error.
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "tf_cnn_benchmarks.py", line 47, in &lt;module&gt;
    tf.app.run()
  File "/MYHOME/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py", line 124, in run
    _sys.exit(main(argv))
  File "tf_cnn_benchmarks.py", line 43, in main
    bench.run()
  File "/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py", line 956, in run
    return self._benchmark_cnn()
  File "/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py", line 1046, in _benchmark_cnn
    self._build_model_single_session())
  File "/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py", line 1563, in _build_model_single_session                                                                      all_top_5_ops, phase_train)
  File "/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py", line 1370, in _build_fetches
    self.variable_mgr.preprocess_device_grads(device_grads))
  File "/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/variable_mgr.py", line 386, in preprocess_device_grads
    agg_small_grads_max_group=self._agg_small_grads_max_group)
  File "/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/allreduce.py", line 332, in sum_gradients_all_reduce
    if is_hierarchical else aux_device_groups[group_index], num_shards))
  File "/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/allreduce.py", line 236, in sum_grad_and_var_all_reduce
    tf.add)
  File "/MYHOME/.local/lib/python3.6/site-packages/tensorflow/contrib/all_reduce/python/all_reduce.py", line 780, in build_nccl_then_ring
    return _build_nccl_hybrid(input_tensors, red_op, upper_level_f)
  File "/MYHOME/.local/lib/python3.6/site-packages/tensorflow/contrib/all_reduce/python/all_reduce.py", line 748, in _build_nccl_hybrid
    send_op, dst_tensors = nccl.broadcast(level_2_output[w], dst_devices)
TypeError: broadcast() takes 1 positional argument but 2 were given
&lt;/denchmark-code&gt;

The reason is that the invocation of nccl.broadcast is different from its signature:



tensorflow/tensorflow/contrib/all_reduce/python/all_reduce.py


         Line 748
      in
      17e725c






 send_op, dst_tensors = nccl.broadcast(level_2_output[w], dst_devices) 








tensorflow/tensorflow/contrib/nccl/python/ops/nccl_ops.py


        Lines 173 to 182
      in
      17e725c






 def broadcast(tensor): 



 """Returns a tensor that can be efficiently transferred to other devices. 



  



   Args: 



     tensor: The tensor to send; must be assigned to a GPU device. 



  



   Returns: 



     A tensor with the value of `src_tensor`, which can be used as input to 



     ops on other GPU devices. 



   """ 





Problems still exists in current HEAD.
	</description>
	<comments>
		<comment id='1' author='ppwwyyxx' date='2017-12-20T22:49:42Z'>
		&lt;denchmark-link:https://github.com/chsigg&gt;@chsigg&lt;/denchmark-link&gt;
 contrib/all_reduce needs to be updated due to the contrib/nccl API change made in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/ca8af1d0dbb605087a4f8ae076188f2b9a26b1ba&gt;ca8af1d&lt;/denchmark-link&gt;
 which broke some benchmarks. cc: &lt;denchmark-link:https://github.com/poxvoculi&gt;@poxvoculi&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='ppwwyyxx' date='2017-12-26T14:52:53Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/15642&gt;#15642&lt;/denchmark-link&gt;
 same problem
		</comment>
		<comment id='3' author='ppwwyyxx' date='2018-01-10T18:56:40Z'>
		Nagging Awaiting TensorFlower: It has been 14 days with no activityand the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='4' author='ppwwyyxx' date='2018-01-18T00:37:27Z'>
		nccl.broadcast got changed without updating its uses.  The immediate problem has been fixed in the internal build and should roll out with the next update.
		</comment>
		<comment id='5' author='ppwwyyxx' date='2018-01-18T02:00:03Z'>
		&lt;denchmark-link:https://github.com/poxvoculi&gt;@poxvoculi&lt;/denchmark-link&gt;
  thanks very much!
how do i get this fix now?
one more question:  not support embedding lookup that has IndexedSlice gradients, can you give some advice?
		</comment>
		<comment id='6' author='ppwwyyxx' date='2018-01-18T05:38:10Z'>
		The fix is easy, you can patch your own client now if you're eager.
In all_reduce.py remove these lines
&lt;denchmark-code&gt;  dst_devices = per_worker_devices[w][1:]
  send_op, dst_tensors = nccl.broadcast(level_2_output[w], dst_devices)
  # NOTE: need control dependency to ensure send_op executes
  with ops.control_dependencies([send_op]):
    with ops.device(per_worker_devices[w][0]):
      dst_tensors.insert(0, array_ops.identity(level_2_output[w]))
      down_values[w] = dst_tensors
&lt;/denchmark-code&gt;

and replace with these lines:
&lt;denchmark-code&gt;  dst_tensors = []
  with ops.device(per_worker_devices[w][0]):
    broadcast_src = nccl.broadcast(array_ops.identity(level_2_output[w]))
  for d in per_worker_devices[w]:
    with ops.device(d):
      dst_tensors.append(array_ops.identity(broadcast_src))
  down_values[w] = dst_tensors
&lt;/denchmark-code&gt;

I'm not expert at NCCL, but I wouldn't be surprised if it only works on dense tensors of numeric types.  IndexedSlices are a sparse representation, right?
		</comment>
		<comment id='7' author='ppwwyyxx' date='2018-01-19T06:09:48Z'>
		&lt;denchmark-link:https://github.com/poxvoculi&gt;@poxvoculi&lt;/denchmark-link&gt;
 thanks, i had already tried same fix with you before this issue.
yesterday i tried your code, but failed again at same error: (use tf_benchmark:
variable_update: distributed_all_reduce
all_reduce_spec: nccl/xring)
UnimplementedError (see above for traceback): This op should be replaced during graph optimization.
caused by
File python2.7/site-packages/tensorflow/contrib/all_reduce/python/all_reduce.py", line 781, in build_nccl_then_ring
return _build_nccl_hybrid(input_tensors, red_op, upper_level_f)
File "python2.7/site-packages/tensorflow/contrib/all_reduce/python/all_reduce.py", line 749, in _build_nccl_hybrid
broadcast_src = nccl.broadcast(array_ops.identity(level_2_output[w]))
File "python2.7/site-packages/tensorflow/contrib/nccl/python/ops/nccl_ops.py", line 187, in broadcast
return gen_nccl_ops.nccl_broadcast(input=tensor, shape=tensor.shape)
File "python2.7/site-packages/tensorflow/contrib/nccl/ops/gen_nccl_ops.py", line 98, in nccl_broadcast
"NcclBroadcast", input=input, shape=shape, name=name)
which i guess rewrite graph maybe happen in nccl send/recv op.
IndexedSlices are a sparse representation, right?
Yes,  excuse me for that i sayed that nccl in the issue above, actually it's contrib/all_reduce module, i fix the issue content error now, thanks.
		</comment>
		<comment id='8' author='ppwwyyxx' date='2018-01-20T02:14:22Z'>
		Hi All, I meet the same issue.
Need your help. Thanks.
		</comment>
		<comment id='9' author='ppwwyyxx' date='2018-01-20T04:01:29Z'>
		&lt;denchmark-link:https://github.com/Agoniii&gt;@Agoniii&lt;/denchmark-link&gt;
  can you post detail? thanks
		</comment>
		<comment id='10' author='ppwwyyxx' date='2018-01-20T06:24:12Z'>
		&lt;denchmark-link:https://github.com/anpark&gt;@anpark&lt;/denchmark-link&gt;
  I run tf_cnn_benchmarks with the commands . My problem is almost as same as yours.
&lt;denchmark-code&gt;  File "/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py", line 1545, in _build_fetches
    self.variable_mgr.preprocess_device_grads(device_grads))
  File "/root/benchmarks/scripts/tf_cnn_benchmarks/variable_mgr.py", line 386, in preprocess_device_grads
    agg_small_grads_max_group=self._agg_small_grads_max_group)
  File "/root/benchmarks/scripts/tf_cnn_benchmarks/allreduce.py", line 332, in sum_gradients_all_reduce
    if is_hierarchical else aux_device_groups[group_index], num_shards))
  File "/root/benchmarks/scripts/tf_cnn_benchmarks/allreduce.py", line 236, in sum_grad_and_var_all_reduce
    tf.add)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/all_reduce/python/all_reduce.py", line 780, in build_nccl_then_ring
    return _build_nccl_hybrid(input_tensors, red_op, upper_level_f)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/all_reduce/python/all_reduce.py", line 749, in _build_nccl_hybrid
    broadcast_src = nccl.broadcast(array_ops.identity(level_2_output[w]))
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/nccl/python/ops/nccl_ops.py", line 187, in broadcast
    return gen_nccl_ops.nccl_broadcast(input=tensor, shape=tensor.shape)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/nccl/ops/gen_nccl_ops.py", line 98, in nccl_broadcast
    "NcclBroadcast", input=input, shape=shape, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3172, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1617, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

UnimplementedError (see above for traceback): This op should be replaced during graph optimization.
	 [[Node: allreduce_160/NcclBroadcast = NcclBroadcast[T=DT_FLOAT, shape=[1001], _device="/job:worker/replica:0/task:0/device:GPU:0"](allreduce_160/Slice)]]
	 [[Node: group_deps_3_G19865 = _Recv[client_terminated=false, recv_device="/job:worker/replica:0/task:0/device:CPU:0", send_device="/job:worker/replica:0/task:0/device:GPU:0", send_device_incarnation=-2613134839557745882, tensor_name="edge_55804_group_deps_3", tensor_type=DT_FLOAT, _device="/job:worker/replica:0/task:0/device:CPU:0"]()]]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='ppwwyyxx' date='2018-01-31T06:16:48Z'>
		I've run into this UnimplementedError as well with 1.5. A small repro:
&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow.contrib.nccl as nccl

if __name__ == '__main__':
    devices = ['/gpu:0', '/gpu:0']
    with tf.device(devices[0]):
        data0 = tf.constant([1.1, 2.2, 3.3, 4.4])
        received_tensor = nccl.broadcast(data0)
    received_tensors = []
    for device in devices[1:]:
        with tf.device(device):
            received_tensors.append(tf.identity(received_tensor))
    sess = tf.Session()
    sess.run(received_tensors)
&lt;/denchmark-code&gt;

It looks like the registration of the optimization pass at this line never happens:



tensorflow/tensorflow/contrib/nccl/kernels/nccl_rewrite.cc


         Line 270
      in
      bf1fad2






 REGISTER_OPTIMIZATION(OptimizationPassRegistry::POST_PLACEMENT, 0, 





This may be because _nccl_ops.so doesn't include kernels/nccl_rewrite.cc:



tensorflow/tensorflow/contrib/nccl/BUILD


         Line 24
      in
      bf1fad2






 name = "python/ops/_nccl_ops.so", 





Unfortunately Bazel throws a fit when I try to add the source file to this target.
		</comment>
		<comment id='12' author='ppwwyyxx' date='2018-01-31T18:57:34Z'>
		&lt;denchmark-link:https://github.com/chsigg&gt;@chsigg&lt;/denchmark-link&gt;
 can you clarify as to whether this test program is using contrib/nccl in the anticipated way?
		</comment>
		<comment id='13' author='ppwwyyxx' date='2018-02-02T01:53:39Z'>
		&lt;denchmark-link:https://github.com/benbarsdell&gt;@benbarsdell&lt;/denchmark-link&gt;
 Great!
i add the same issue in tensorflow/benchmarks repo, but &lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
  closed it and ask me to  run with tf1.5.
Hope anyone can help!
		</comment>
		<comment id='14' author='ppwwyyxx' date='2018-02-16T13:36:55Z'>
		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='15' author='ppwwyyxx' date='2018-03-03T15:03:30Z'>
		Nagging Assignees &lt;denchmark-link:https://github.com/chsigg&gt;@chsigg&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/poxvoculi&gt;@poxvoculi&lt;/denchmark-link&gt;
: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='16' author='ppwwyyxx' date='2018-03-18T16:05:51Z'>
		Nagging Assignees &lt;denchmark-link:https://github.com/chsigg&gt;@chsigg&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/poxvoculi&gt;@poxvoculi&lt;/denchmark-link&gt;
: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='17' author='ppwwyyxx' date='2018-03-19T20:32:54Z'>
		Reassigning to &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;

		</comment>
		<comment id='18' author='ppwwyyxx' date='2018-03-21T01:27:50Z'>
		Seems to be fixed already.
		</comment>
		<comment id='19' author='ppwwyyxx' date='2018-03-23T07:47:35Z'>
		&lt;denchmark-link:https://github.com/ppwwyyxx&gt;@ppwwyyxx&lt;/denchmark-link&gt;
  which version do you use?  Have you tested it ok?
		</comment>
		<comment id='20' author='ppwwyyxx' date='2018-03-23T07:58:45Z'>
		There might be other issues, but at least my original one seems fixed. As I can run tensorflow/benchmarks with TF1.6.
		</comment>
		<comment id='21' author='ppwwyyxx' date='2018-04-04T06:22:55Z'>
		failed again at same error: (use tf_benchmark:
variable_update: distributed_all_reduce
all_reduce_spec: nccl/xring)
TF1.6
&lt;denchmark-link:https://github.com/ppwwyyxx&gt;@ppwwyyxx&lt;/denchmark-link&gt;
  what's your params?
		</comment>
	</comments>
</bug>