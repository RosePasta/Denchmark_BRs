<bug id='29253' author='vsriram23' open_date='2019-06-01T17:58:11Z' closed_time='2019-06-03T17:58:20Z'>
	<summary>[TF 2.0] - freeze_graph not working with converted keras to tensorflow serving model using tf.keras.experimental.export_savedmodel</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution : Linux Ubuntu 16.04
TensorFlow installed from: binary
TensorFlow version (use command below): 2.0
Python version: 3.6
CUDA/cuDNN version: 10.0
GPU model and memory:

Describe the current behavior
Model saved as tf.keras .hdf5 converted to tensorflow model using  tf.keras.experimental.export_savedmodel and when using freeze_graph to get a single .pb file running into error
Describe the expected behavior
Code to reproduce the issue
import tensorflow as tf
model_path = '/home/vsrira10/Desktop/model.hdf5'
model = tf.keras.models.load_model(model_path)
tf.keras.experimental.export_savedmodel(model,newdir)
After this a variables folder with files [checkpoint,variables.data-00000-of-00001,variables.index], saved_model.pb and assests folder created in newdir.
I am trying to use saved_model.pb and variables.data-00000-of-00001 files to get single .pb frozen_graph
Other info / logs
python /home/vsrira10/anaconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py --input_graph=/home/vsrira10/Desktop/tf_models/saved_model.pb --input_checkpoint=/home/vsrira10/Desktop/tf_models/variables/variables.data-00000-of-00001 --output_graph=/home/vsrira10/Desktop/tf_models/frozen_graph.pb --output_node_names=classes,corners --input_binary=true
Traceback (most recent call last):
File "/home/vsrira10/anaconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py", line 492, in 
run_main()
File "/home/vsrira10/anaconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py", line 489, in run_main
app.run(main=my_main, argv=[sys.argv[0]] + unparsed)
File "/home/vsrira10/anaconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/platform/app.py", line 40, in run
_run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
File "/home/vsrira10/anaconda2/envs/tf2/lib/python3.6/site-packages/absl/app.py", line 300, in run
_run_main(main, args)
File "/home/vsrira10/anaconda2/envs/tf2/lib/python3.6/site-packages/absl/app.py", line 251, in _run_main
sys.exit(main(argv))
File "/home/vsrira10/anaconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py", line 488, in 
my_main = lambda unused_args: main(unused_args, flags)
File "/home/vsrira10/anaconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py", line 382, in main
flags.saved_model_tags, checkpoint_version)
File "/home/vsrira10/anaconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py", line 341, in freeze_graph
input_graph_def = _parse_input_graph_proto(input_graph, input_binary)
File "/home/vsrira10/anaconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py", line 252, in _parse_input_graph_proto
input_graph_def.ParseFromString(f.read())
google.protobuf.message.DecodeError: Error parsing message
	</description>
	<comments>
		<comment id='1' author='vsriram23' date='2019-06-03T17:58:20Z'>
		This is expected behavior. freeze_graph is not supported in 2.0. It will be removed from the pip in the near future.
		</comment>
		<comment id='2' author='vsriram23' date='2019-06-03T17:58:22Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29253&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29253&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='vsriram23' date='2019-06-03T18:10:47Z'>
		
This is expected behavior. freeze_graph is not supported in 2.0. It will be removed from the pip in the near future.

If freeze_graph is not supported what should I use instead?
		</comment>
		<comment id='4' author='vsriram23' date='2019-06-03T18:33:40Z'>
		In 2.0, the primary export format is &lt;denchmark-link:https://www.tensorflow.org/alpha/guide/saved_model&gt;SavedModels&lt;/denchmark-link&gt;
 so APIs are built to directly support SavedModels. Inference on existing frozen graphs can be run using the  path.
If there is a tool that is currently not supporting SavedModels in 2.0, please report that as a separate issue.
		</comment>
		<comment id='5' author='vsriram23' date='2019-07-01T09:49:36Z'>
		I'll try to comment here about this post:

This is expected behavior. freeze_graph is not supported in 2.0. It will be removed from the pip in the near future.

Well, I need to use freeze_graph to have a saved model compatible with opencv cv::dnn::readNetFromTensorflow in C++. Have you have ever considered this situation? What are your plans to continue supporting this?
Thank you.
&lt;denchmark-link:https://github.com/gargn&gt;@gargn&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='vsriram23' date='2019-07-01T20:12:05Z'>
		Also, coreml doesn't work with TF 2.0 now, only with frozen graph. But I need to convert my model to it.
		</comment>
		<comment id='7' author='vsriram23' date='2019-09-06T15:58:57Z'>
		Seriously, this is kind of crazy.
An entire ecosystem was built around using protocol buffers - none of those tools work with TF 2.0 since TF 2.0 can't export PB's as far as I can tell, especially if TF 2.0 has bug fixes for models that won't export properly from TF 1.14 -
So right now I

can build a  model in TF 1.14 or 2.0
Cant export said model Keras h5 since it used TF.Layers rather than Keras layers
Cant export a PB since said model has  some weird bug thats yet to be fixed in TF 1.1.4
Cant export a PB in 2.0 runtime, using 1.x combat runtime in TF 2.0 triggers the TF 1.x bug.
Cant export SavedModel format to any other format
SavedModel PB defaults to some very weird SERVING PB which, I never asked for.
Tensorflow project doesn't provide a method of back porting a SavedModel to a PB in an external tool.

I guess I can't use TF?
		</comment>
		<comment id='8' author='vsriram23' date='2019-09-12T09:17:40Z'>
		I was able to generate a frozen tensorflow model from a TF 2.0 model. First step is to save the weights of the tf.keras model with model.save_weights('xxx.h5').
With a conda environment or another computer with TF 1.xx installed (I had 1.14), you generate an untrained tf.keras model identical to the model you want to freeze and load the weights with model.load_weights('xxx.h5').
At this point you can save the full model in a h5 file with model.save(...) and then freeze it.
		</comment>
		<comment id='9' author='vsriram23' date='2019-09-12T10:51:58Z'>
		&lt;denchmark-link:https://github.com/efournie&gt;@efournie&lt;/denchmark-link&gt;
 if it doesnâ€™t take a lot of time could you provide small code example?
		</comment>
		<comment id='10' author='vsriram23' date='2019-09-12T11:27:34Z'>
		I use a create_model() function to generate an untrained tf.keras model:
&lt;denchmark-code&gt;def create_model():
    # Define your tf.keras model here, used also to train your network in the TF 2.0 environment
    # ...
    return Model(m_input, m_output)
&lt;/denchmark-code&gt;

In a conda environment with TF 2.0.0rc0 installed, I train the network generated with model = create_model(). Once trained, the model weights are saved with model.save_weights('result_w.h5').
I then activate another conda environment with TF 1.14 installed (in my case it is named tf1) and transfer the weights to a blank model:
&lt;denchmark-code&gt;D:\dl&gt;conda activate tf1
(tf1) D:\dl&gt;python
Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; from create_model import create_model
&gt;&gt;&gt; final_model = create_model()
&gt;&gt;&gt; final_model.load_weights('result_w.h5')
&gt;&gt;&gt; final_model.save('trained_model.h5')
&lt;/denchmark-code&gt;

To freeze the model, I use the following freeze_model.py script:
&lt;denchmark-code&gt;import tensorflow as tf
import argparse
import uuid
import tensorflow.keras.backend as K
import tensorflow.keras as keras
import shutil
import os
import subprocess
from pathlib import Path
from constants import *

anaconda_path = Path("C:/Program Files (x86)/Microsoft Visual Studio/Shared/Anaconda3_64/")
tf_tools_path = os.path.join(anaconda_path, "Lib/site-packages/tensorflow/python/tools/")
python_cmd = os.path.join(anaconda_path, "python.exe")
freeze_script = os.path.join(tf_tools_path, "freeze_graph.py")
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

parser = argparse.ArgumentParser(description="Convert Keras .h5 file to Tensorflow .pb frozen model.")
parser.add_argument("--h5", type=str, default="model.h5", help="Keras .h5 file (input)")
parser.add_argument("--pb", type=str, default="model.pb", help="Tensorflow frozen model (output)")
args = parser.parse_args()

K.set_learning_phase(0)

model = keras.models.load_model(args.h5)
model_output = model.output.op.name
tempdir = str(uuid.uuid4())
os.mkdir(tempdir)
try:
    temp_tf_file = str(os.path.join(tempdir, "tf.ckpt"))
    saver = tf.train.Saver()
    saver.save(K.get_session(), temp_tf_file)
    tmp_meta_file = str(temp_tf_file) + ".meta"

    saver = tf.train.import_meta_graph(tmp_meta_file, clear_devices=True)
    K.get_session().run(tf.global_variables_initializer())
    K.get_session().run(tf.local_variables_initializer())
    sess = K.get_session()
    saver.restore(sess, tf.train.latest_checkpoint(os.path.expanduser(tempdir)))

    output_node_names = model_output

    # for fixing the bug of batch norm
    gd = sess.graph.as_graph_def()
    for node in gd.node:
        if node.op == 'RefSwitch':
            node.op = 'Switch'
            for index in xrange(len(node.input)):
                if 'moving_' in node.input[index]:
                    node.input[index] = node.input[index] + '/read'
        elif node.op == 'AssignSub':
            node.op = 'Sub'
            if 'use_locking' in node.attr: del node.attr['use_locking']
        elif node.op == 'AssignAdd':
            node.op = 'Add'
            if 'use_locking' in node.attr: del node.attr['use_locking']

    converted_graph_def = tf.graph_util.convert_variables_to_constants(sess, gd, output_node_names.split(","))
    tf.train.write_graph(converted_graph_def, ".", args.pb, as_text=False)
    print("Done.")
finally:
    shutil.rmtree(tempdir)
&lt;/denchmark-code&gt;

I can now convert the trained model to a frozen pb file with python freeze_model.py --h5 trained_model.h5 --pb trained_model.pb
I can't avoid saving the weights and transferring them to an untrained model because saving the full model in TF 2.0 and loading it in TF 1.14 doesn't work.
		</comment>
	</comments>
</bug>