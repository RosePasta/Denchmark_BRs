<bug id='8330' author='MicaelCarvalho' open_date='2017-03-12T19:47:52Z' closed_time='2017-06-16T17:12:55Z'>
	<summary>Uncatchable exception messages when using slice_input_producer and batch</summary>
	<description>
Hello,
Out of range messages are being thrown by the code below, and they seem to be uncatchable — I even tried enclosing the whole code with try:, except: pass, but the messages are still printed. While they don't really seem to affect the rest of the code, since we are taking the exact amount of examples available, these messages are quite annoying because with a big pipeline they can get really messy and ruin real-time visualization of logs (the number of errors has a relation with the number of threads).
If I try to evaluate an extra epoch, then TF raises an exception I can catch, because this time I tried to evaluate an example I don't have, but this isn't the case here.
Operating System: Debian 4.8.15-2
Installed version of CUDA and cuDNN: CUDA 8, cuDNN 5
python3 -c "import tensorflow; print(tensorflow.version)": 1.0.0
&lt;denchmark-h:h3&gt;Reproducible example&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import tensorflow as tf
import time

num_epochs = 6

a = ([tf.constant(i) for i in range(2)],[tf.constant(i) for i in range(2)])

q1 = tf.train.slice_input_producer(a, num_epochs=num_epochs, shuffle=True, capacity=4)
q2 = tf.train.batch(q1, batch_size=2, num_threads=2, enqueue_many=False, capacity=4, allow_smaller_final_batch=True)

init = [tf.global_variables_initializer(), tf.local_variables_initializer()]
sess = tf.Session()
coord = tf.train.Coordinator()
sess.run(init)
threads = tf.train.start_queue_runners(coord=coord, sess=sess)

test_number = 1
for i in range(num_epochs):
	print('Testing %d' % test_number)
	ignore = sess.run(q2)
	test_number = test_number + 1
	time.sleep(3)
print('Done.')
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Output&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;(...initialization messages...)
Testing 1
W tensorflow/core/framework/op_kernel.cc:993] Out of range: Reached limit of 6
         [[Node: input_producer/input_producer/fraction_of_4_full/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=["loc:@input_producer/input_producer/fraction_of_4_full/limit_epochs/epochs"], limit=6, _device="/job:localhost/replica:0/task:0/cpu:0"](input_producer/input_producer/fraction_of_4_full/limit_epochs/epochs)]]
Testing 2
Testing 3
Testing 4
W tensorflow/core/framework/op_kernel.cc:993] Out of range: FIFOQueue '_0_input_producer/input_producer/fraction_of_4_full/fraction_of_4_full' is closed and has insufficient elements (requested 1, current size 0)
         [[Node: input_producer/fraction_of_4_full_Dequeue = QueueDequeueV2[component_types=[DT_INT32], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](input_producer/input_producer/fraction_of_4_full/fraction_of_4_full)]]
W tensorflow/core/framework/op_kernel.cc:993] Out of range: FIFOQueue '_0_input_producer/input_producer/fraction_of_4_full/fraction_of_4_full' is closed and has insufficient elements (requested 1, current size 0)
         [[Node: input_producer/fraction_of_4_full_Dequeue = QueueDequeueV2[component_types=[DT_INT32], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](input_producer/input_producer/fraction_of_4_full/fraction_of_4_full)]]
Testing 5
Testing 6
Done.
&lt;/denchmark-code&gt;

These messages may vary: on multiple runs of the same code, the first one sometimes isn't printed, and the number of messages after Testing 4 also changes.
The documentation of batch states:

The returned operation is a dequeue operation and will throw tf.errors.OutOfRangeError if the input queue is exhausted. If this operation is feeding another input queue, its queue runner will catch this exception, however, if this operation is used in your main thread you are responsible for catching this yourself.

However, no further instruction is given. And since a try: except: didn't work, I'm guessing this is a bug. Could anyone clarify this behavior?
Thanks in advance.
	</description>
	<comments>
		<comment id='1' author='MicaelCarvalho' date='2017-03-12T19:55:19Z'>
		Complementary information: These messages are being thrown by the threads created with start_queue_runners, that is why they aren't linked to the main program and cannot be catched/traced. There seems to be no parameter of start_queue_runners to treat this. There are a few options I can think of in order to contour the problem, but they are waaaaaaaay too hacky — like manually creating threads to feed the queues or intercepting the threads created by TF before they start running and encapsulating them.
		</comment>
		<comment id='2' author='MicaelCarvalho' date='2017-03-22T23:10:01Z'>
		&lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
, could you please take a look?
		</comment>
		<comment id='3' author='MicaelCarvalho' date='2017-06-15T19:28:25Z'>
		&lt;denchmark-link:https://github.com/MicaelCarvalho&gt;@MicaelCarvalho&lt;/denchmark-link&gt;

I had the same uncatchable error with tf.train.string_input_producer.
It was solved when I set num_epochs to None.
From the &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/train/string_input_producer&gt;documentation&lt;/denchmark-link&gt;
, it seems to have an 'automation' effect:

num_epochs: An integer (optional). If specified, string_input_producer produces each string from string_tensor num_epochs times before generating an OutOfRange error. If not specified, string_input_producer can cycle through the strings in string_tensor an unlimited number of times.

But I am not yet sure that this change is correct from an application perspective and I don't have an explanation of why it solved the uncatchable exception.
Looking forward to any precise insights on this issue..
		</comment>
		<comment id='4' author='MicaelCarvalho' date='2017-06-15T19:40:21Z'>
		&lt;denchmark-link:https://github.com/ymrabet&gt;@ymrabet&lt;/denchmark-link&gt;
 that is not the solution. :-)
Setting num_epochs to None makes the threads continuously feed the queue, without ever stopping. This way, the queue never gets emptied and the problem doesn't happen.
However, in some situations it is desirable to precisely control the number of epochs, specially when shuffle is enabled, to ensure every sample was seen N times; and in these situations the uncatchable exception messages occur. It won't actually pose big problems, since it's just a message being printed, but it seems to be impossible to block the printing of it (except with some hacky solution like setting high log levels throughout the application, and losing all important logs of tensorflow).
		</comment>
		<comment id='5' author='MicaelCarvalho' date='2017-06-15T20:34:45Z'>
		&lt;denchmark-link:https://github.com/MicaelCarvalho&gt;@MicaelCarvalho&lt;/denchmark-link&gt;
 Thanks for the explanation :-)
How can we be sure that it's only a display issue with no other impact?
		</comment>
		<comment id='6' author='MicaelCarvalho' date='2017-06-15T22:07:00Z'>
		&lt;denchmark-link:https://github.com/ymrabet&gt;@ymrabet&lt;/denchmark-link&gt;
 the problem happens because  only generates the specified number of epochs of input, while  has no parameter to control the number of epochs, therefore it "runs forever".
Internally, batch creates threads to feed an input queue, from which batches are taken. These threads keep pulling samples from the input tensor indefinitely. The problem occurs when they try to pull a sample and slice_input_producer has finished its job (i.e. there are no samples left, slice_input_producer's pool is empty).
The exception is then uncatchable and intractable because it is raised inside each of these threads, when they try to get a new sample and there is none left. Each thread, after raising the exception, has no predefined way of treating it (and that's the problem), so it goes all the way up to the interpreter, which by default prints the exception and kills "the program". Since each thread is individually handled, "the program" means "the thread"; so the thread gets terminated and the error message printed.
That being said, the number of messages one sees should match exactly the number of threads created by batch, because it is only printed when a thread dies (but weirdly it doesn't always match, and that's something to be investigated maybe). It has no other impact because each thread dies individually, without affecting the main program, and the expected behavior in this case would be closing the threads, which is happening anyway because of the exception.
		</comment>
		<comment id='7' author='MicaelCarvalho' date='2017-06-16T17:12:54Z'>
		We're replacing Queues with tf.contrib.datasets in the near future. This should take care of weaknesses such as this.
I'm closing this issue, thanks &lt;denchmark-link:https://github.com/MicaelCarvalho&gt;@MicaelCarvalho&lt;/denchmark-link&gt;
 for the great explanations!
		</comment>
	</comments>
</bug>