<bug id='45786' author='bibbygoodwin' open_date='2020-12-17T09:59:53Z' closed_time='2021-01-07T20:17:40Z'>
	<summary>Keras docs wrongly advise not to pass tf.keras.layers activations to a layer creation</summary>
	<description>
&lt;denchmark-h:h2&gt;URL(s) with the issue:&lt;/denchmark-h&gt;

&lt;denchmark-link:https://keras.io/api/layers/activations/&gt;https://keras.io/api/layers/activations/&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Description of issue (what needs changing):&lt;/denchmark-h&gt;

In tf.keras, there are 2** ways of adding activation functions to your models. In the first instance, objects from tf.keras.activations can be passed as the activation argument in the creation of a layer as in:
model.add(layers.Dense(64, activation=tf.nn.tanh))
The 2nd API for activations is to explicitly add them as a layer, as in:
x = layers.Dense(10)(x)
x = layers.LeakyReLU()(x)
The problem with the docs is that they explicitly say (at &lt;denchmark-link:https://keras.io/api/layers/activations/&gt;https://keras.io/api/layers/activations/&lt;/denchmark-link&gt;
) that:

you should not pass activation layers instances as the activation argument of a layer. They're meant to be used just like regular layers

This runs contrary to current usage in tf2, where can can do exactly this. See, for instance, &lt;denchmark-link:https://stackoverflow.com/a/56869141/7858285&gt;this stackoverflow answer&lt;/denchmark-link&gt;
 which advises the following:
model.add(Conv2D(..., activation=tf.keras.layers.LeakyReLU(alpha=0.1), ...)
in place of the lambda function that is otherwise needed if one is to follow the keras docs advice.
**you might say there are 3 ways, as in the '1st' method one can also request an activation by its string identifier.
&lt;denchmark-h:h3&gt;Submit a pull request?&lt;/denchmark-h&gt;

Unless I'm mistaken, the &lt;denchmark-link:https://keras.io/&gt;https://keras.io/&lt;/denchmark-link&gt;
 docs are not open source. The keras repo currently requests that all issues be opened here in the tf repo.
	</description>
	<comments>
		<comment id='1' author='bibbygoodwin' date='2020-12-23T02:28:12Z'>
		&lt;denchmark-link:https://github.com/bibbygoodwin&gt;@bibbygoodwin&lt;/denchmark-link&gt;
,
This is an issue related to  and not .  The statement:

you should not pass activation layers instances as the activation argument of a layer. They're meant to be used just like regular layers

is neither present in the TF Keras Documentation for &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/layers/LeakyReLU&gt;LeakyRelu&lt;/denchmark-link&gt;
 nor in &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/layers/PReLU&gt;PReLU&lt;/denchmark-link&gt;
.
So, request you to raise an issue in &lt;denchmark-link:https://github.com/keras-team/keras/issues/new&gt;Keras Github Repository&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='bibbygoodwin' date='2020-12-30T02:55:35Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='3' author='bibbygoodwin' date='2021-01-03T17:41:42Z'>
		&lt;denchmark-link:https://github.com/rmothukuru&gt;@rmothukuru&lt;/denchmark-link&gt;
 Thank you for your reply. The README of the keras repo currently states:

In the near future, this repository will be used once again for developing the Keras codebase. For the time being, the Keras codebase is being developed at tensorflow/tensorflow, and any PR or issue should be directed there.

That was why I created this issue here and not in the keras repo.
Also, the docs I linked to which have the problem (&lt;denchmark-link:https://keras.io/api/layers/activations/&gt;https://keras.io/api/layers/activations/&lt;/denchmark-link&gt;
) are indeed the keras documentation, but are for , as you can see on the linked page. Thus, I think there is ample scope for confusion here if things are not ammended.
Thanks again.
		</comment>
		<comment id='4' author='bibbygoodwin' date='2021-01-07T20:17:40Z'>
		It was always possible to do so (if you use the layer as such, it's used like a function), but it leads to potential issues (most importantly the issue that your activation layer could include trainable weights, which would be ignored by the model if you used it like this).
Layers are meant to be nodes in a graph of layers, and so should be used as such, like in the example:
&lt;denchmark-code&gt;x = layers.Dense(10)(x)
x = layers.LeakyReLU()(x)
&lt;/denchmark-code&gt;


Unless I'm mistaken, the https://keras.io/ docs are not open source. The keras repo currently requests that all issues be opened here in the tf repo.

Keras docs are open-source and can be modified at &lt;denchmark-link:https://github.com/keras-team/keras-io&gt;https://github.com/keras-team/keras-io&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>