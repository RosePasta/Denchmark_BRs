<bug id='34073' author='jpviguerasguillen' open_date='2019-11-07T15:54:27Z' closed_time='2019-11-18T22:20:43Z'>
	<summary>Problem with fit_generator when using a generator for training and a fixed set for validation.</summary>
	<description>
System information

I have written custom code. Code is available here as a Google Colab notebook: https://drive.google.com/open?id=1xp5LES0HiEEPWozrD4HR5DPPa-dce1mH
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): The code was run in Google Colab, with the version of all packages defined by Google Colab at this date (07 Nov 2019).
TensorFlow version: 2.0.0
Python version: 3.6.7

Describe the current behavior
The provided code, originally written by Xifeng Guo for Keras with Tensorflow 1.2 as backend, was updated to run in Tensorflow 2.0. The code builds a Capsule Network as defined by Hinton's paper  'Dynamic Routing Between Capsules'. It is trained/tested with MNIST dataset.
The code gives two options: either training with model.fit (it works perfectly) or with model.fit_generator, where a generator makes only shifting augmentation (the problem is here!).
This part of the code remained as it was originally written by Mr. Guo, as I think it does not need to be updated (see below, where the part of model.fit was commented out). Note that in the original implementation of CapsNets there is an encoder (x is input, y is output) and a decoder (y is input, x is output), thus the input of the network is x=[x_train, y_train] and the output is y=[y_train, x_train].
&lt;denchmark-code&gt;"""
# Training without data augmentation:
model.fit([x_train, y_train], [y_train, x_train], 
          batch_size=args.batch_size, 
          epochs=args.epochs,
          validation_data=[[x_test, y_test], [y_test, x_test]], 
          callbacks=[log, tb, checkpoint, lr_decay])
"""

# Begin: Training with data augmentation ----------------------------------#
def train_generator(x, y, batch_size, shift_fraction=0.):
  train_datagen = ImageDataGenerator(width_shift_range=shift_fraction,
                                      height_shift_range=shift_fraction)  
  generator = train_datagen.flow(x, y, batch_size=batch_size)
  while True:
    x_batch, y_batch = generator.next()
    yield ([x_batch, y_batch], [y_batch, x_batch])

# Training with data augmentation. If shift_fraction=0 then no augmentation.
model.fit_generator(generator=train_generator(x_train, y_train, 
                                              args.batch_size, 
                                              args.shift_fraction),
                    steps_per_epoch=5, #int(y_train.shape[0] / args.batch_size),
                    epochs=args.epochs,
                    validation_data=[[x_test, y_test], [y_test, x_test]],
                    callbacks=[log, tb, checkpoint, lr_decay])
# End: Training with data augmentation ------------------------------------#
&lt;/denchmark-code&gt;

If fit_generator is used (with the indicated generator), there is a problem when the function ends generating training samples and starts using the validation set to test the model, giving this error:

ValueError: could not broadcast input array from shape (100,28,28,1) into shape (100)

Remember that the MNIST dataset contains gray-scale images of 28x28 pixels, and the batch_size here is 100 images.
Interestingly, if I change the validation_data line from ...

validation_data=[[x_test, y_test], [y_test, x_test]],

... to a generator with no augmentation...

validation_data=train_generator(x_test, y_test, args.batch_size)

... it works perfectly.
And, again, if I do not use the fit_generator but the model.fit, it also works fine.
Describe the expected behavior
As described, fit_generator should be able to deal with validation_data correctly.

The issue can be easily reproducible in the following notebook (just run all the cells!). I have reduced the steps_per_epoch=5 to go fast to the point where the code breaks.
&lt;denchmark-link:https://drive.google.com/open?id=1xp5LES0HiEEPWozrD4HR5DPPa-dce1mH&gt;https://drive.google.com/open?id=1xp5LES0HiEEPWozrD4HR5DPPa-dce1mH&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='jpviguerasguillen' date='2019-11-08T09:23:25Z'>
		Issue replicating for the given code in TF-2.0.Thanks!
		</comment>
		<comment id='2' author='jpviguerasguillen' date='2019-11-18T22:20:43Z'>
		I think this is now obsolete. As of &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/ac20030c96d37e980333b604402ef6dba48ef5e2&gt;ac20030&lt;/denchmark-link&gt;
,  just calls  (which supports generators by turning them into Datasets). Previously  was using a separate, somewhat antiquated code path. With the consolidation both endpoints are now using the more modern codepath, and when I test your colab with the most recent  it runs without issue. I'm going to close this as fixed; feel free to re-open if you still encounter issues.
		</comment>
		<comment id='3' author='jpviguerasguillen' date='2019-11-18T22:20:45Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34073&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34073&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='jpviguerasguillen' date='2019-11-19T12:06:48Z'>
		Thanks!
You are right, the nightly version works perfectly. I did not occur to me to try tf-nightly before submitting the issue.
		</comment>
	</comments>
</bug>