<bug id='40249' author='DarrenRuan' open_date='2020-06-07T21:33:12Z' closed_time='2020-10-12T23:14:18Z'>
	<summary>The parameters for BatchNormalization are not updated</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OS
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): Official Website
TensorFlow version (use command below): 2.2.0
Python version: 3.7.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: No
GPU model and memory: No

Describe the current behavior
I am simply changing to codes from tf.layers to tf.keras.layers, based on the migration instructions on the official website. But it turned out the parameters for tf.keras.layers.BatchNormalization are not updated properly, while everything works fine for tf.layers.BatchNormalization(). I do not know the reasons.
Describe the expected behavior
The parameters for tf.keras.layers.BatchNormalization should be updated just like what have been done for tf.layers.BatchNormalization.
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Codes:
&lt;denchmark-code&gt;import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import numpy as np

def get_positive(batch_size):
    train_images = np.zeros((50, 28, 28, 1), dtype=np.float32) + 1.
    train_labels = np.int8(np.zeros((50, 1)) + 1)
    dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))
    dataset = dataset.repeat(1)
    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(1)
    return dataset.make_one_shot_iterator().get_next()

def get_negative(batch_size):
    train_images = np.zeros((50, 28, 28, 1), dtype=np.float32)
    train_labels = np.int8(np.zeros((50, 1)))
    dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))
    dataset = dataset.repeat(1)
    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(1)
    return dataset.make_one_shot_iterator().get_next()

def tf_model(feature, is_training):
    with tf.variable_scope("tf_model", reuse=tf.AUTO_REUSE):
        net = tf.layers.Conv2D(
            8, (3, 3), strides=(2, 2),
            activation=tf.nn.relu)(feature)  # 13x13x8
        net = tf.layers.BatchNormalization()(net, is_training)
        net = tf.layers.Conv2D(
            1, (3, 3), strides=(2, 2), activation=tf.nn.relu)(net)  # 6x6x1
        net = tf.layers.Flatten()(net)  # 36
        net = tf.layers.Dense(1)(net)
        return net

def tf_keras_model(feature, is_training):
    with tf.variable_scope("tf_model", reuse=tf.AUTO_REUSE):
        net = tf.keras.layers.Conv2D(
            8, (3, 3), strides=(2, 2),
            activation=tf.nn.relu)(feature)  # 13x13x8
        net = tf.keras.layers.BatchNormalization()(net, is_training)
        net = tf.keras.layers.Conv2D(
            1, (3, 3), strides=(2, 2), activation=tf.nn.relu)(net)  # 6x6x1
        net = tf.keras.layers.Flatten()(net)  # 36
        net = tf.keras.layers.Dense(1)(net)
        return net

def get_bn_vars(collection):
    moving_mean, moving_variance = None, None
    for var in collection:
        name = var.name.lower()
        if "variance" in name:
            moving_variance = var
        if "mean" in name:
            moving_mean = var

    if moving_mean is not None and moving_variance is not None:
        return moving_mean, moving_variance
    raise ValueError("Unable to find moving mean and variance")

def main_layers(case):
    positive, positive_labels = get_positive(10)
    negative, negative_labels = get_negative(10)

    model_true = tf_model(positive, True)

    loss = tf.losses.sigmoid_cross_entropy(positive_labels, model_true)
    if case == 2:
        model_false = tf_model(negative, True)
        loss += tf.losses.sigmoid_cross_entropy(negative_labels, model_false)

    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    with tf.control_dependencies(update_ops):
        train_op = tf.train.GradientDescentOptimizer(1e-3).minimize(loss)

    mean, variance = get_bn_vars(tf.global_variables())
    init = tf.global_variables_initializer()
    with tf.Session() as sess:
        sess.run(init)
        while True:
            try:
                loss_value, _ = sess.run([loss, train_op])
                print("loss: ", loss_value)
            except tf.errors.OutOfRangeError:
                break
        print(sess.run([mean, variance]))

def main_tf_keras_layers(case):
    tf.keras.backend.set_learning_phase(True)
    positive, positive_labels = get_positive(10)
    negative, negative_labels = get_negative(10)

    model_true = tf_keras_model(positive, True)

    loss = tf.losses.sigmoid_cross_entropy(positive_labels, model_true)
    if case == 2:
        model_false = tf_model(negative, True)
        loss += tf.losses.sigmoid_cross_entropy(negative_labels, model_false)

    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    with tf.control_dependencies(update_ops):
        train_op = tf.train.GradientDescentOptimizer(1e-3).minimize(loss)

    mean, variance = get_bn_vars(tf.global_variables())
    init = tf.global_variables_initializer()
    with tf.Session() as sess:
        sess.run(init)
        while True:
            try:
                loss_value, _ = sess.run([loss, train_op])
                print("loss: ", loss_value)
            except tf.errors.OutOfRangeError:
                break
        print(sess.run([mean, variance]))
&lt;/denchmark-code&gt;

If we run main_layers(1), we would get:
&lt;denchmark-code&gt;loss:  0.69315034
loss:  0.6928972
loss:  0.69264734
loss:  0.6923976
loss:  0.6921481
[array([0.        , 0.        , 0.        , 0.01786391, 0.02162646,
       0.        , 0.00420831, 0.        ], dtype=float32), array([0.95099014, 0.95099014, 0.95099014, 0.95099014, 0.95099014,
       0.95099014, 0.95099014, 0.95099014], dtype=float32)]
&lt;/denchmark-code&gt;

However, if we run main_tf_keras_layers(1), we would get:
&lt;denchmark-code&gt;loss:  0.6930456
loss:  0.69101626
loss:  0.688996
loss:  0.68698376
loss:  0.68497455
[array([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)]
&lt;/denchmark-code&gt;

It turns out that parameters for tf.keras.layers.BatchNormalization are not updated properly. They should be expected to be similar to  tf.layers.BatchNormalization.
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='DarrenRuan' date='2020-06-07T21:38:37Z'>
		My further question would be like: is it safe that we could simply replace tf.layers with tf.keras.layers? If it not safe, how should we deal with the codes written in tf1.x, and we want to write them in tf2.x (I do know that there is a guide about migrations)?
		</comment>
		<comment id='2' author='DarrenRuan' date='2020-06-08T11:42:19Z'>
		Was able to reproduce the issue with &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/636025752e44225cf3773dd6fb7bf5ed/40249.ipynb&gt;TF v2.2&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/f78949f716361e77a039a6095c4161ae/40249-tf-nightly.ipynb&gt;TF-nightly&lt;/denchmark-link&gt;
. Please find the attached gist. Thanks!
		</comment>
		<comment id='3' author='DarrenRuan' date='2020-10-12T23:14:18Z'>
		The code and repro in in TF1 and v2_behavior disabled. The issue should be gone if it's migrated to TF2. Could you follow this &lt;denchmark-link:https://www.tensorflow.org/guide/migrate&gt;migration guide&lt;/denchmark-link&gt;
 to update the code to TF2? Thanks.
		</comment>
		<comment id='4' author='DarrenRuan' date='2020-10-12T23:14:20Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40249&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40249&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>