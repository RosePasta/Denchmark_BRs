<bug id='6019' author='facundoq' open_date='2016-12-01T17:16:36Z' closed_time='2016-12-19T15:51:31Z'>
	<summary>Computer freeze when feeding a large numpy array as input in MNIST tutorial</summary>
	<description>
&lt;denchmark-h:h3&gt;What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?&lt;/denchmark-h&gt;

None
&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

intel i5, 8gb ram
Operating System:
Ubuntu 14.04.5:
Linux 4.4.0-45-generic &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/66&gt;#66&lt;/denchmark-link&gt;
~14.04.1-Ubuntu SMP Wed Oct 19 15:05:38 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
Installed version of CUDA and cuDNN:
No cuda or cuDNN, running on CPU
If installed from binary pip package, provide:

A link to the pip package you installed:
TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0-cp34-cp34m-linux_x86_64.whl
The output from python -c "import tensorflow; print(tensorflow.__version__)".
0.11.0rc2

&lt;denchmark-h:h3&gt;If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)&lt;/denchmark-h&gt;

Use the mnist tutorial, replace
train_accuracy = accuracy.eval(feed_dict={ x:batch[0], y_: batch[1], keep_prob: 1.0})
with
train_accuracy = accuracy.eval(feed_dict={ x:mnist.train.images, y_: mnist.train.labels, keep_prob: 1.0})
So that the accuracy is evaluated over the entire training set. I've reproduced this using another dataset, and the problem goes away when using a smaller number of examples.
(pastebin for entire file with this modification: &lt;denchmark-link:http://pastebin.com/THNqB4ws&gt;http://pastebin.com/THNqB4ws&lt;/denchmark-link&gt;
)
&lt;denchmark-h:h3&gt;What other attempted solutions have you tried?&lt;/denchmark-h&gt;

None, there's no error message and the computer hangs the first time it executes accuracy.eval
	</description>
	<comments>
		<comment id='1' author='facundoq' date='2016-12-01T20:00:18Z'>
		The documentation (&lt;denchmark-link:https://www.tensorflow.org/versions/r0.12/tutorials/mnist/beginners/index.html&gt;https://www.tensorflow.org/versions/r0.12/tutorials/mnist/beginners/index.html&lt;/denchmark-link&gt;
) says that mnist.test.images is 10k images, while mnist.train.images is 55k images.  By feeding the entire training set at once, it seems likely that you're causing too much memory to be allocated, and that may be your problem.  8GB is not a lot of RAM these days.   In linux you can use ulimit to bound the memory permitted to a process.
		</comment>
		<comment id='2' author='facundoq' date='2016-12-01T23:23:46Z'>
		&lt;denchmark-link:https://github.com/poxvoculi&gt;@poxvoculi&lt;/denchmark-link&gt;
 Thanks, I guessed as much, but just before the freeze I still see 3gbs free of ram; maybe a big chunk gets allocated just before the freeze and causes it. Also, I thought having no swap would cause an out-of-memory error, my bad, so maybe that could be it.  I'll check tomorrow from the computer at work if limiting the memory for the process helps to avoid the freeze.
Nonetheless, the entire training set is 160mb if in float32. The output of the convolutional layers are 1.3gb and 650mb respectively. 214mb for the first fully connected and just 2 megs for the output, for a total of ~2.5gb total. The memory required for the weights is negligible. It seems weird the activations of the intermediate layers would take that much space.
		</comment>
		<comment id='3' author='facundoq' date='2016-12-02T18:21:15Z'>
		Indeed limiting the memory of the process avoids the OOM. TF is trying to use about 11gbs of memory to evaluate the entire training set. While this can be workaround quite easily (evaluating the accuracy in smaller batches), isn't 11gbs a bit overmuch? Is it keeping the activations of every layer even when not doing a train_step.run()? If the activations are not saved for the backward pass, no more than 1.3gb+160mb should be used by the forward pass. Is there any way to avoid storing the activations?
		</comment>
		<comment id='4' author='facundoq' date='2016-12-02T18:56:35Z'>
		Further testing with a really simple network: conv(4) - pool(2x2) - fc(10) (that's 4 filters for the convolutional layer) full code available in &lt;denchmark-link:http://pastebin.com/Zh14pS3C&gt;http://pastebin.com/Zh14pS3C&lt;/denchmark-link&gt;
.
I removed the training, accuracy evaluation, and most layers from the net, while also reducing the filters for the convolutional layer and the relu. For the whole training dataset this still requires  of memory. There's a 1gb decrease if I evaluate with 10000 less training examples, 2gb for 20000, and so on. Is this normal?
		</comment>
		<comment id='5' author='facundoq' date='2016-12-02T19:42:32Z'>
		What happens if you run with tcmalloc? There's been some memory leaks
reported earlier which disappear when you do tcmalloc (as specified here
&lt;&lt;denchmark-link:http://goog-perftools.sourceforge.net/doc/tcmalloc.html&gt;http://goog-perftools.sourceforge.net/doc/tcmalloc.html&lt;/denchmark-link&gt;
&gt;)
Also, you can see which ops are allocating memory by doing something like
this

run_metadata = tf.RunMetadata()
sess.run([C.op],
             options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),
             run_metadata=run_metadata)
print run_metadata

Or more visually in Chrome Trace Viewer (see
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/1824#issuecomment-225754659&gt;#1824 (comment)&lt;/denchmark-link&gt;
)

trace = timeline.Timeline(step_stats=run_metadata.step_stats)
with open('timeline.ctf.json', 'w') as trace_file:
  trace_file.write(trace.generate_chrome_trace_format(show_memory=True))
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Fri, Dec 2, 2016 at 10:56 AM, Pepe Mandioca ***@***.***&gt; wrote:
 Further testing with a really simple network: conv(4) - pool(2x2) - fc(10)
 (that's 4 filters for the convolutional layer) full code available in
 http://pastebin.com/Zh14pS3C.
 I removed the training, accuracy evaluation, and most layers from the net,
 while also reducing the filters for the convolutional layer and the relu.
 For the whole training dataset this still requires *6.5gb* of memory.
 There's a roughly 1gb decrease if I evaluate with 10000 less training
 examples, 2gb for 20000, and so on. Is this normal?

 —
 You are receiving this because you are subscribed to this thread.
 Reply to this email directly, view it on GitHub
 &lt;#6019 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AABaHPtgmRHA9TlLuX2xxFdyyzookUQOks5rEGnkgaJpZM4LBp_t&gt;
 .



		</comment>
		<comment id='6' author='facundoq' date='2016-12-05T17:07:41Z'>
		&lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 Running with tcmalloc decreased the memory usage slightly but not entirely.
I tried the tracer but it seems there's only cpu use info.
Here's the output from , &lt;denchmark-link:http://pastebin.com/tXwB7Lfn&gt;http://pastebin.com/tXwB7Lfn&lt;/denchmark-link&gt;
 when testing with  training examples (so that the run() completes).
The relevant? bits from the memory subsection:
The Conv2D allocates total_bytes: 25088000 (~25mb) (2828400042)
The Add allocates total_bytes: 25088000 (~25mb) (2828400042)
The MaxPool allocates total_bytes: 6272000 (~6mb) (1414400042)
The MatMul allocates total_bytes: 160000 (~160k) (1040004)
The Add_1 allocates total_bytes: 160000 (~160k) (1040004)
The SoftmaxCrossEntropyWithLogits allocates total_bytes: 192000 (~192k) (1040004+4000*8)
There are also a lot of  "Shape", "Slice", most of which allocate just 4, 8, 16 or 200 bytes. Other nodes of the sort allocate bytes 15680, 15680, 12544000, 6272000, 160000, 160000, but all of these add up to at most 20mb. So if with 4000 samples the "use" is less than 100mb, with 55000 it would be about 1350mb or 1.35gb.
Note: Are the 'total bytes' and 'requested_bytes' added up? if so the conv, add and maxpool layers occupy twice as much.
		</comment>
		<comment id='7' author='facundoq' date='2016-12-16T05:48:56Z'>
		Uncompressed MNIST dataset is about 50MB, and you are feeding the whole dataset at each step for 20k steps. This &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/python/client/tf_session_helper.cc#L445&gt;line&lt;/denchmark-link&gt;
 in tf_session_helper.cc does a copy of numpy data into TensorFlow
std::memcpy(data, PyArray_DATA(array), size)
So during the run of this program you are trying to copy 1TB worth of numpy arrays into TensorFlow, how much memory does your machine have?
		</comment>
		<comment id='8' author='facundoq' date='2016-12-18T16:41:26Z'>
		Well, if you check the MWE you'll see that I removed the for loop and I'm doing a single eval, so I guess that theory is irrelevant for this issue.
Regardless, you are telling me that each time I feed a numpy array to a graph memory gets allocated and never released?
		</comment>
		<comment id='9' author='facundoq' date='2016-12-18T18:01:48Z'>
		I'm not able to reproduce freezing after single eval. When I ran your example from &lt;denchmark-link:http://pastebin.com/THNqB4ws&gt;http://pastebin.com/THNqB4ws&lt;/denchmark-link&gt;
 it runs for &gt;100 iterations, at which point I killed it. I've seen cases of a machine freezing to a point of requiring reboot when TensorFlow allocated too much memory.
If I put a pause after 100 iterations in your script, the memory usage returns to normal, so I guess that memory gets garbage collected eventually.
		</comment>
		<comment id='10' author='facundoq' date='2016-12-18T19:25:01Z'>
		Here's a version of your script with some extra info printed: &lt;denchmark-link:http://pastebin.com/Sfhn3sAP&gt;http://pastebin.com/Sfhn3sAP&lt;/denchmark-link&gt;

When I run it, I see this:
&lt;denchmark-code&gt;Iteration 0 time 81.24 3.31
tcmalloc: large alloc 5017829376 bytes == 0xc3be0000 @  0x7f5cb6236c4c 0x7f5cb6255040 0x7f5c50ac253f 0x7f5c517604f8 0x7f5c5176cd7d 0x7f5c5177ba14 0x7f5c5177dd23 0x7f5c51d2a49e 0x7f5c51d1dca0 0x7f5c51efb668 0x7f5c51efae42 0x7f5cb49bf870 0x7f5cb5b1e184 0x7f5cb4f3637d (nil)
test accuracy 0.1052
EIteration 0 time 1790.80 10.95
Iteration 10 time 41.54 10.95
Iteration 20 time 41.43 10.95
Iteration 30 time 40.35 10.95
Iteration 40 time 41.02 10.95
Iteration 50 time 39.61 10.95
Iteration 60 time 42.82 10.95
Iteration 70 time 41.60 10.95
Iteration 80 time 39.79 10.95
Iteration 90 time 41.15 10.95
Iteration 100 time 42.20 10.95
tcmalloc: large alloc 5017829376 bytes == 0x6825e000 @  0x7f5cb6236c4c 0x7f5cb6255040 0x7f5c50ac253f 0x7f5c517604f8 0x7f5c5176cd7d 0x7f5c5177ba14 0x7f5c5177dd23 0x7f5c51d2a49e 0x7f5c51d1dca0 0x7f5c51efb668 0x7f5c51efae42 0x7f5cb49bf870 0x7f5cb5b1e184 0x7f5cb4f3637d (nil)
test accuracy 0.7846
EIteration 100 time 1668.47 10.95
Iteration 110 time 43.09 10.95
Iteration 120 time 47.44 10.95
Iteration 130 time 49.22 10.95
Iteration 140 time 45.06 10.95
Iteration 150 time 45.78 10.95
Iteration 160 time 44.96 10.95
Iteration 170 time 45.14 10.95
Iteration 180 time 45.68 10.95
Iteration 190 time 40.74 10.95
Iteration 200 time 41.44 10.95
tcmalloc: large alloc 5017829376 bytes == 0xc3bde000 @  0x7f5cb6236c4c 0x7f5cb6255040 0x7f5c50ac253f 0x7f5c517604f8 0x7f5c5176cd7d 0x7f5c5177ba14 0x7f5c5177dd23 0x7f5c51d2a49e 0x7f5c51d1dca0 0x7f5c51efb668 0x7f5c51efae42 0x7f5cb49bf870 0x7f5cb5b1e184 0x7f5cb4f3637d (nil)
test accuracy 0.8924
EIteration 200 time 1707.17 10.95
Iteration 210 time 40.57 10.95
Iteration 220 time 44.48 10.95
Iteration 230 time 42.95 10.95
Iteration 240 time 40.86 10.95
Iteration 250 time 42.96 10.95
Iteration 260 time 42.18 10.95
Iteration 270 time 40.89 10.95
Iteration 280 time 42.90 10.95
Iteration 290 time 40.68 10.95
Iteration 300 time 41.88 10.95
tcmalloc: large alloc 5017829376 bytes == 0xc3bde000 @  0x7f5cb6236c4c 0x7f5cb6255040 0x7f5c50ac253f 0x7f5c517604f8 0x7f5c5176cd7d 0x7f5c5177ba14 0x7f5c5177dd23 0x7f5c51d2a49e 0x7f5c51d1dca0 0x7f5c51efb668 0x7f5c51efae42 0x7f5cb49bf870 0x7f5cb5b1e184 0x7f5cb4f3637d (nil)
test accuracy 0.9203
EIteration 300 time 1684.26 10.95
Iteration 310 time 51.82 10.95
Iteration 320 time 41.72 10.95
Iteration 330 time 40.68 10.95
Iteration 340 time 43.42 10.95
Iteration 350 time 48.30 10.95
Iteration 360 time 45.63 10.95
Iteration 370 time 43.82 10.95
Iteration 380 time 41.13 10.95
Iteration 390 time 41.07 10.95
Iteration 400 time 40.19 10.95
tcmalloc: large alloc 5017829376 bytes == 0xc3bde000 @  0x7f5cb6236c4c 0x7f5cb6255040 0x7f5c50ac253f 0x7f5c517604f8 0x7f5c5176cd7d 0x7f5c5177ba14 0x7f5c5177dd23 0x7f5c51d2a49e 0x7f5c51d1dca0 0x7f5c51efb668 0x7f5c51efae42 0x7f5cb49bf870 0x7f5cb5b1e184 0x7f5cb4f3637d (nil)
&lt;/denchmark-code&gt;

So it seems to be executing normally without slowdown.
		</comment>
		<comment id='11' author='facundoq' date='2016-12-19T04:54:22Z'>
		I took a &lt;denchmark-link:https://github.com/yaroslavvb/notebooks/blob/master/mnist-memory.ipynb&gt;closer look&lt;/denchmark-link&gt;
 at your memory usage and the main source of memory usage is the activation in your first conv layer.
Your convolution has 32 filters, so activations for first conv layer takes 4x28x28x32 = 100k per example. This is followed by broadcasting add which makes it 200k total. To run this over whole dataset you need 60k x 200k = 12GB of RAM
		</comment>
		<comment id='12' author='facundoq' date='2016-12-19T15:51:31Z'>
		Thanks Yarsolav, that may be it. I thought the forward pass would be smarter and discard intermediate computations or do them in-place when not needed for the backward pass, but thinking about it a bit more that may be hard to do with a framework as general as tf.
		</comment>
		<comment id='13' author='facundoq' date='2016-12-19T17:37:25Z'>
		Actually if you look at &lt;denchmark-link:https://github.com/yaroslavvb/notebooks/blob/master/mnist-memory.ipynb&gt;memory timeline&lt;/denchmark-link&gt;
 you'll see that activations for  are discarded as soon as  completes. This is because it's eval pass, so they are not needed for derivatives. If  could be done in place this would essentially lower the peak usage from 12GB to 6GB, and that's perhaps what the upcoming XLA framework could do. However, it would be trickier to lower usage for the backward pass as well since  activations are needed until much later -- you would need an implementation of fused  op, and a corresponding gradient.
		</comment>
		<comment id='14' author='facundoq' date='2016-12-19T18:18:35Z'>
		Yes, a fused conv+add could store the activation A(b_conv1), and do A(b_conv1)-b_conv1 in the backward pass (again, in-place) to obtain A(h_conv1) again! That would surely help for big models even with small batch sizes.
Anyhow, in this case I guess the sane thing to do is to use a tf input queue that would automatically batch stuff or calculate the full training set accuracy by feeding small batches which is easy since it's a reduction op.
		</comment>
		<comment id='15' author='facundoq' date='2018-08-25T06:05:32Z'>
		I have the same problem. I cannot load the numpy array into memory (it's not mnist) it freezes. I think I have to split my numpy array into some smaller arrays and use a generator to load every one of them and feed to my model but I think it's too slow. Is there any way to handle this problem?
		</comment>
		<comment id='16' author='facundoq' date='2018-12-17T13:50:04Z'>
		It is the best solution for OOM, the activations occupied the great majority of GPU memory, rather than images of test dataset.  But too much batch size will make activations become huge, and get the OOM.
		</comment>
	</comments>
</bug>