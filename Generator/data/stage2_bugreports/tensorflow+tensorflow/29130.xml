<bug id='29130' author='nikitos9000' open_date='2019-05-29T13:51:11Z' closed_time='2020-01-13T20:10:29Z'>
	<summary>Mixed precision LossScaleOptimizer doesn't work with Keras</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): PyPi
TensorFlow version (use command below): 1.13.1
Python version: 3.5.2

Describe the current behavior
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/mixed_precision/python/loss_scale_optimizer.py#L102&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/mixed_precision/python/loss_scale_optimizer.py#L102&lt;/denchmark-link&gt;

tf.contrib.mixed_precision.loss_scale_optimizer.LossScaleOptimizer doesn't implement variables() method (because it doesn't call base class __init__ nor override it), so it fails when used within Keras model training with the following stack trace on model.fit_generator call:
&lt;denchmark-code&gt;  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py", line 1426, in fit_generator
    initial_epoch=initial_epoch)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_generator.py", line 164, in model_iteration
    callbacks._call_begin_hook(mode)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/callbacks.py", line 212, in _call_begin_hook
    self.on_train_begin()
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/callbacks.py", line 279, in on_train_begin
    callback.on_train_begin(logs)
  File "/usr/local/lib/python3.5/dist-packages/horovod-0.16.0-py3.5-linux-x86_64.egg/horovod/_keras/callbacks.py", line 30, in on_train_begin
    self.backend.get_session().run(bcast_op)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py", line 482, in get_session
    _initialize_variables(session)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py", line 749, in _initialize_variables
    variables = _get_variables(get_graph())
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py", line 743, in _get_variables
    variables.update(opt.optimizer.variables())
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py", line 946, in variables
    optimizer_variables = [v for v in self._non_slot_variables()
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py", line 1025, in _non_slot_variables
    return self._non_slot_dict.values()
AttributeError: 'LossScaleOptimizer' object has no attribute '_non_slot_dict'
&lt;/denchmark-code&gt;

Describe the expected behavior
Wrapping the optimizer in a LossScaleOptimizer should work transparently with Keras models, as it implements tf.optimizers.Optimizer base class.
To solve this problem, there's a need to either pass undelying optimizer variables() method (together with passing LossScaleManager variables too) or fill slot- and non-slot variables some other way.
Workaround example:
&lt;denchmark-code&gt;class MyExponentialUpdateLossScaleManager(ExponentialUpdateLossScaleManager):
    def variables(self):
        return [self._loss_scale, self._num_good_steps, self._num_bad_steps]

class MyLossScaleOptimizer(LossScaleOptimizer):
    def variables(self):
        return self._opt.variables() + self._loss_scale_manager.variables()
&lt;/denchmark-code&gt;

Code to reproduce the issue
&lt;denchmark-code&gt;from tensorflow.train import AdamOptimizer 
from tensorflow.contrib.mixed_precision import LossScaleOptimizer, ExponentialUpdateLossScaleManager
...
optimizer = AdamOptimizer()
loss_scale_manager = ExponentialUpdateLossScaleManager(init_loss_scale=2 ** 32, incr_every_n_steps=1000)
optimizer = LossScaleOptimizer(optimizer, loss_scale_manager)
...
model.compile(optimizer=optimizer, ...)
&lt;/denchmark-code&gt;

Full example
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf
from tensorflow.train import AdamOptimizer
from tensorflow.contrib.mixed_precision import LossScaleOptimizer, ExponentialUpdateLossScaleManager

input = tf.keras.layers.Input(shape=(16,))
output = tf.keras.layers.Dense(1)(input)
model = tf.keras.models.Model(input, output)

optimizer = AdamOptimizer()
# Works without these two lines below
loss_scale_manager = ExponentialUpdateLossScaleManager(init_loss_scale=2 ** 32, incr_every_n_steps=1000)
optimizer = LossScaleOptimizer(optimizer, loss_scale_manager)

model.compile(optimizer=optimizer, loss='binary_crossentropy')
model.fit(np.zeros((16, 16)), np.zeros((16,)))
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='nikitos9000' date='2019-05-30T10:57:38Z'>
		Thanks for the detailed explanation. Will it be possible to provide a minimal exact full code snippet that can reproduce the issue. It would be easy for us to proceed faster. Thanks!
		</comment>
		<comment id='2' author='nikitos9000' date='2019-05-30T13:14:00Z'>
		Updated the description above with the full code example, thanks!
		</comment>
		<comment id='3' author='nikitos9000' date='2019-05-31T06:28:08Z'>
		Thanks for sharing the information. I have tried on Colab with TensorFlow version 1.13.1 and was able to reproduce the issue.
		</comment>
		<comment id='4' author='nikitos9000' date='2019-06-13T17:56:50Z'>
		&lt;denchmark-link:https://github.com/reedwm&gt;@reedwm&lt;/denchmark-link&gt;
 -- can you advise on the best combination of APIs to use here?
		</comment>
		<comment id='5' author='nikitos9000' date='2019-06-13T18:06:45Z'>
		The contrib LossScaleOptimizer is deprecated. Try using tf.train.experimental.MixedPrecisionLossScaleOptimizer instead.
		</comment>
		<comment id='6' author='nikitos9000' date='2019-06-28T12:11:13Z'>
		
The contrib LossScaleOptimizer is deprecated. Try using tf.train.experimental.MixedPrecisionLossScaleOptimizer instead.

The version of this API is tensorflow 1.14, how about tensorflow 1.13.1 ?
		</comment>
		<comment id='7' author='nikitos9000' date='2019-06-28T18:07:22Z'>
		Unfortunately, no LossScaleOptimizer can be used in tf.keras with 1.13.1.
		</comment>
		<comment id='8' author='nikitos9000' date='2020-01-13T20:10:29Z'>
		Closing this as contrib has been removed in TF 2.
		</comment>
		<comment id='9' author='nikitos9000' date='2020-01-13T20:10:31Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29130&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29130&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>