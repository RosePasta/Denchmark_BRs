<bug id='34313' author='kazuimotn' open_date='2019-11-15T15:59:12Z' closed_time='2020-09-18T10:41:17Z'>
	<summary>A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x70 in tid 28007</summary>
	<description>
There was a problem when calling converted custom model (LSTM.tflite) in Android Studio.
with
converter.experimental_new_converter = True
System information
model training machine

Ubuntu 16.04 (LTS)
GTX1080
tensorflow 1.13.1
Keras 2.2.4
Python 3.6.8

model converting &amp; loading machine

MacOS Mojave 10.14.5
Python 3.7.1
tensorflow 2.0.0, tf-nightly 2.1.0.dev20191113
Android Studio 3.4.1
targetSdkVersion 28

Smartphone

Android 8.0.0
SONY SO-04J (docomo xperia)

I can convert custom LSTM model from .h5 to .tflite with following code:
import tensorflow as tf
model = tf.keras.models.load_model("LSTM.h5")
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.experimental_new_converter = True
tflite_model = converter.convert()
open("LSTM.tflite", "wb").write(tflite_model)
Next I want to load this model (LSTM.tflite) into my Android APK.
I already have class that can perform inference using 1D-CNN custom model (1DCNN.tflite).
package iis.kmjlab.kazuimotn.sartips.others;
import android.content.res.AssetFileDescriptor;
import android.content.res.AssetManager;
import org.tensorflow.lite.Interpreter;
import java.io.FileInputStream;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.MappedByteBuffer;
import java.nio.channels.FileChannel;

public class TensorFlowLiteClassifier {

    private Interpreter interpreter;
    public static final String MODEL_FILE = "1DCNN.tflite";
    public static final int LABEL_NUM = 2;

    /**
     * Registers interpreter
     */
    private TensorFlowLiteClassifier(Interpreter interpreter) {
        this.interpreter = interpreter;
    }

    /**
     * Loads model into interpreter
     */
    public static TensorFlowLiteClassifier classifier(AssetManager assetManager, String modelPath) throws IOException {
        ByteBuffer byteBuffer = loadModelFile(assetManager, modelPath);
        Interpreter interpreter = new Interpreter(byteBuffer);
        return new TensorFlowLiteClassifier(interpreter);
    }

    /**
     * Loads model
     */
    private static MappedByteBuffer loadModelFile(AssetManager assets, String path) throws IOException {
        AssetFileDescriptor file = assets.openFd(path);
        FileInputStream stream = new FileInputStream(file.getFileDescriptor());
        FileChannel channel = stream.getChannel();
        long startOffset = file.getStartOffset();
        long declaredLength = file.getDeclaredLength();
        return channel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);
    }

    /**
     * Function that actually performs inference
     */
    public float[][] predictProbabilities(float[][] input) {
        float[][] output = new float[1][LABEL_NUM];
        interpreter.run(input, output);
        return output;
    }
}
When I use 1DCNN.tflite, this class works completely. However, when I change the model into LSTM.tflite, getting ERROR at this line
Interpreter interpreter = new Interpreter(byteBuffer);
And ERROR logcat is following
A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x70 in tid 28007
How can I deal with this unknown error?
After my some investigation, this error seems to occur when the prepared array is exceeded for some reason.
	</description>
	<comments>
		<comment id='1' author='kazuimotn' date='2019-11-15T21:19:14Z'>
		I'm wondering if it's an issue with the Android APK environment. Does the same lstm flatbuffer model run on desktop?
		</comment>
		<comment id='2' author='kazuimotn' date='2019-11-16T00:17:40Z'>
		I can infer probabilities using same LSTM.tflite model with this Python code:
import tensorflow as tf
interpreter = tf.compat.v2.lite.Interpreter("LSTM.tflite")
interpreter.allocate_tensors()
input  = interpreter.tensor(interpreter.get_input_details()[0]["index"])
output = interpreter.tensor(interpreter.get_output_details()[0]["index"])
for i in range(10):
  input().fill(1)
  interpreter.invoke()
  print("inference %s" % output())
Output is following:
&lt;denchmark-code&gt;inference [[9.5466515e-09 1.0000000e+00]]
inference [[9.5466515e-09 1.0000000e+00]]
inference [[9.5466515e-09 1.0000000e+00]]
inference [[9.5466515e-09 1.0000000e+00]]
inference [[9.5466515e-09 1.0000000e+00]]
inference [[9.5466515e-09 1.0000000e+00]]
inference [[9.5466515e-09 1.0000000e+00]]
inference [[9.5466515e-09 1.0000000e+00]]
inference [[9.5466515e-09 1.0000000e+00]]
inference [[9.5466515e-09 1.0000000e+00]]
&lt;/denchmark-code&gt;

My model is for binary classification, it means this output is what I want.
In other words, does it mean that the Android APK environment is incorrect?
		</comment>
		<comment id='3' author='kazuimotn' date='2019-11-16T05:32:31Z'>
		It might be due to some native methods &lt;denchmark-link:https://github.com/lizhangqu/TensorflowLite/blob/master/library/src/main/java/org/tensorflow/lite/NativeInterpreterWrapper.java&gt;here&lt;/denchmark-link&gt;
:
TensorflowLite/library/src/main/java/org/tensorflow/lite/NativeInterpreterWrapper.java
private static native long createErrorReporter(int size);
private static native long createModelWithBuffer(MappedByteBuffer modelBuffer, long errorHandle);
private static native long createInterpreter(long modelHandle);
/**
   * Initializes a {@code NativeInterpreterWrapper} with a {@code MappedByteBuffer}. The
   * MappedByteBuffer should not be modified after the construction of a {@code
   * NativeInterpreterWrapper}.
   */
  NativeInterpreterWrapper(MappedByteBuffer mappedByteBuffer) {
    modelByteBuffer = mappedByteBuffer;
    errorHandle = createErrorReporter(ERROR_BUFFER_SIZE);
    modelHandle = createModelWithBuffer(modelByteBuffer, errorHandle);
    interpreterHandle = createInterpreter(modelHandle);
  }
		</comment>
		<comment id='4' author='kazuimotn' date='2020-05-30T05:07:37Z'>
		Hi!, could any of you solve this issue?
I'm having the same error with an LSTM clasifier.
In fact, not exactly the same... actually I'm getting this:
java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model
		</comment>
		<comment id='5' author='kazuimotn' date='2020-06-13T18:48:28Z'>
		I solved a similar issue following &lt;denchmark-link:https://www.tensorflow.org/lite/guide/ops_select#android_aar&gt;this guide&lt;/denchmark-link&gt;
.
Particularly adding this into build.gradle:
android { defaultConfig { ndk { abiFilters 'armeabi-v7a', 'arm64-v8a' } } }
to filter unnecessary ABI dependencies.
		</comment>
		<comment id='6' author='kazuimotn' date='2020-09-04T09:57:53Z'>
		&lt;denchmark-link:https://github.com/kazuimotn&gt;@kazuimotn&lt;/denchmark-link&gt;

Please update as per above comment.
		</comment>
		<comment id='7' author='kazuimotn' date='2020-09-11T10:16:25Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='8' author='kazuimotn' date='2020-09-18T10:41:13Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='9' author='kazuimotn' date='2020-09-18T10:41:20Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34313&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34313&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>