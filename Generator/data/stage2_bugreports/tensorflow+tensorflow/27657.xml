<bug id='27657' author='8bitmp3' open_date='2019-04-08T22:46:17Z' closed_time='2019-04-29T20:09:40Z'>
	<summary>[TF 2.0 API Docs] tf.keras.activations.selu</summary>
	<description>
System information

TensorFlow version: 2.0 alpha
Doc Link:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations/selu
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/activations.py

- Links
Should format the referenced paper - get rid of "-" and add the authors/year of publishing:
"Self-Normalizing Neural Networks" (Klambauer et al, 2017)"

The current definition does not specify the fixed values for  and  constants. It also assumes knowledge of the ELU activation function. We should also include a link (i.e. &lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations/elu&gt;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations/elu&lt;/denchmark-link&gt;
).
Proposed modified definition:
&lt;denchmark-code&gt;The Scaled Exponential Linear Unit (SELU) activation function is:

`scale` * `x` if `x &gt; 0` and `scale * alpha * (exp(x)-1)` if `x &lt; 0`

where `alpha` and `scale` are pre-defined constants (`alpha = 1.6732632423543772848170429916717` and `scale = 1.0507009873554804934193349852946`.
The SELU activation function multiplies  `scale` &gt; 1 with the `[elu](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations/elu)` (Exponential Linear Unit (ELU)) to ensure a slope larger than one for positive net inputs. 
&lt;/denchmark-code&gt;

Followed by what is already in the docs with the formatted  initialization bit and a link to it: &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/initializers/lecun_normal&gt;https://www.tensorflow.org/api_docs/python/tf/keras/initializers/lecun_normal&lt;/denchmark-link&gt;
:
&lt;denchmark-code&gt;...The values of alpha and scale are chosen so that the mean and variance of the inputs are preserved between two consecutive layers as long as the weights are initialized correctly (see `[lecun_normal` initialization](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/lecun_normal)) and the number of inputs is "large enough" (see references for more information).
&lt;/denchmark-code&gt;

- Examples
Can add a modified example fro the Intro to CNNs tutorials (use selu instead of relu)
&lt;denchmark-code&gt;model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='selu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='selu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='selu'))
&lt;/denchmark-code&gt;

- Returns
Can modify to include the word function:
&lt;denchmark-code&gt;The scaled exponential unit activation function`: `scale * elu(x, alpha)`.
&lt;/denchmark-code&gt;

and format markdown for plain text.
- Raises
Not defined.

Should be added, similar to: &lt;denchmark-link:https://cdn-images-1.medium.com/max/1600/1*WyQS-lnoemRA3_FpRL7r5w.png&gt;https://cdn-images-1.medium.com/max/1600/1*WyQS-lnoemRA3_FpRL7r5w.png&lt;/denchmark-link&gt;

We welcome contributions by users. Will you be able to update submit a PR (use the doc style guide) to fix the doc Issue?
Yes
	</description>
	<comments>
		<comment id='1' author='8bitmp3' date='2019-04-09T08:56:35Z'>
		SELU was designed for standard feed-forward neural networks, I'd use a standard FNN for the SELU example instead of a CNN.
		</comment>
		<comment id='2' author='8bitmp3' date='2019-04-09T13:14:44Z'>
		&lt;denchmark-link:https://github.com/dynamicwebpaige&gt;@dynamicwebpaige&lt;/denchmark-link&gt;
 can you help me with this i would love to work on this.
		</comment>
	</comments>
</bug>