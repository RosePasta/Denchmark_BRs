<bug id='7854' author='cw1204772' open_date='2017-02-24T17:15:14Z' closed_time='2017-04-05T01:35:48Z'>
	<summary>mnist.input_data.read_data_sets() cause Segmentation Fault upon exiting</summary>
	<description>
&lt;denchmark-h:h3&gt;What related GitHub issues or StackOverflow threads have you found by searching the web for your&lt;/denchmark-h&gt;

While I was trying the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py&gt;MNIST tutorial&lt;/denchmark-link&gt;
, I found my tensorflow output Segmentation Fault (Core Dumped).
However, my mnist_softmax.py can still operates normally, outputing correct output on the terminal. It prints out the core dumped when the program exits.
After some try and error, I think it's this line
mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)
causing program to Core dumped upon exiting. Maybe this is a r1.0 bug?
&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System: Ubuntu 14.04.5 LTS
CUDA 8.0
CuDNN 5.1.5
Tensorflow r1.0 installed from source:

The commit hash: 29a6b46
The output of bazel version
Build label: 0.4.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Feb 1 18:54:21 2017 (1485975261)
Build timestamp: 1485975261
Build timestamp as int: 1485975261

&lt;denchmark-h:h3&gt;If possible, provide a minimal reproducible example&lt;/denchmark-h&gt;

The same code as &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py&gt;MNIST tutorial&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;What other attempted solutions have you tried?&lt;/denchmark-h&gt;

I've tried out the &lt;denchmark-link:https://www.tensorflow.org/get_started/get_started#complete_program&gt;Getting Started tutorial&lt;/denchmark-link&gt;
, and it worked perfectly.
I've try to comment each line to figure out which line cause the error. The result is
 seems to cause the problem.
&lt;denchmark-h:h3&gt;Logs or other output that would be helpful&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;$ python example.py
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:
name: GeForce GTX 770
major: 3 minor: 0 memoryClockRate (GHz) 1.1105
pciBusID 0000:01:00.0
Total memory: 1.95GiB
Free memory: 1.66GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 770, pci bus id: 0000:01:00.0)
0.917
Segmentation fault (core dumped)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='cw1204772' date='2017-02-27T05:24:50Z'>
		The same problem hit me
		</comment>
		<comment id='2' author='cw1204772' date='2017-02-27T21:21:49Z'>
		I tried with cuda 8.0 and cudnn 5.0 on ubuntu 14, and it works fine for me on the 1.0 release version. &lt;denchmark-link:https://github.com/gunan&gt;@gunan&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
, have you seen any reports of this happening? Could you try from take r1.0 or from a pre-build binary.
		</comment>
		<comment id='3' author='cw1204772' date='2017-03-02T12:06:17Z'>
		While I was trying "&lt;denchmark-link:https://www.tensorflow.org/get_started/mnist/beginners&gt;https://www.tensorflow.org/get_started/mnist/beginners&lt;/denchmark-link&gt;
" and trying to run below commands and facing time out exception. (I am using window 7 and tensorflow v.1
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)
		</comment>
		<comment id='4' author='cw1204772' date='2017-03-02T22:39:33Z'>
		&lt;denchmark-link:https://github.com/aselle&gt;@aselle&lt;/denchmark-link&gt;
 I have not seen any other reports of this.
&lt;denchmark-link:https://github.com/cw1204772&gt;@cw1204772&lt;/denchmark-link&gt;
 Could you try to get a stack trace for us to inspect?
&lt;denchmark-link:https://github.com/mkamranmalik&gt;@mkamranmalik&lt;/denchmark-link&gt;
 You are seeing a different error, on a different Operating system. Your report here will just confuse us when trying to help out with the actual issue reported at the top.
You need to file a new issue by filling in all the information we are requesting for us to be able to help you out.
		</comment>
		<comment id='5' author='cw1204772' date='2017-03-03T12:08:04Z'>
		Back trace result using gdb
&lt;denchmark-code&gt;#0  0x00007ffff782dc37 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007ffff7831028 in __GI_abort () at abort.c:89
#2  0x00007ffff786a2a4 in __libc_message (do_abort=do_abort@entry=1, fmt=fmt@entry=0x7ffff79786b0 "*** Error in `%s': %s: 0x%s ***\n") at ../sysdeps/posix/libc_fatal.c:175
#3  0x00007ffff787655e in malloc_printerr (ptr=&lt;optimized out&gt;, str=0x7ffff7974801 "free(): invalid pointer", action=1) at malloc.c:4996
#4  _int_free (av=&lt;optimized out&gt;, p=&lt;optimized out&gt;, have_lock=0) at malloc.c:3840
#5  0x00007fffe19b53e3 in ufunc_dealloc (ufunc=0xa4c570) at numpy/core/src/umath/ufunc_object.c:4992
#6  0x000000000055a657 in ?? ()
#7  0x000000000055a52f in ?? ()
#8  0x000000000045b2cf in _PyImport_Fini ()
#9  0x0000000000437d51 in Py_Finalize ()
#10 0x000000000044f993 in Py_Main ()
#11 0x00007ffff7818f45 in __libc_start_main (main=0x44f9c2 &lt;main&gt;, argc=2, argv=0x7fffffffdde8, init=&lt;optimized out&gt;, fini=&lt;optimized out&gt;, rtld_fini=&lt;optimized out&gt;, 
    stack_end=0x7fffffffddd8) at libc-start.c:287
#12 0x0000000000578c4e in _start ()
&lt;/denchmark-code&gt;

Terminal message during execution: (only use 10 epoch for inferencing mode)
&lt;denchmark-code&gt;Starting program: /usr/bin/python mnist.py
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
[New Thread 0x7ffff3647700 (LWP 9936)]
[New Thread 0x7ffff2e46700 (LWP 9937)]
[New Thread 0x7ffff0645700 (LWP 9938)]
[New Thread 0x7fffede44700 (LWP 9939)]
[New Thread 0x7fffeb643700 (LWP 9940)]
[New Thread 0x7fffe8e42700 (LWP 9941)]
[New Thread 0x7fffe6641700 (LWP 9942)]
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
[Thread 0x7fffe6641700 (LWP 9942) exited]
[Thread 0x7ffff3647700 (LWP 9936) exited]
[Thread 0x7ffff2e46700 (LWP 9937) exited]
[Thread 0x7fffe8e42700 (LWP 9941) exited]
[Thread 0x7fffeb643700 (LWP 9940) exited]
[Thread 0x7fffede44700 (LWP 9939) exited]
[Thread 0x7ffff0645700 (LWP 9938) exited]
Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
[New Thread 0x7fffe6641700 (LWP 9945)]
[New Thread 0x7fffe8e42700 (LWP 9946)]
[New Thread 0x7fffeb643700 (LWP 9947)]
[New Thread 0x7fffede44700 (LWP 9948)]
[New Thread 0x7fffbbf85700 (LWP 9949)]
[New Thread 0x7fffbb784700 (LWP 9950)]
[New Thread 0x7fffbaf83700 (LWP 9951)]
[New Thread 0x7fffba782700 (LWP 9952)]
[New Thread 0x7fffb9f81700 (LWP 9953)]
[New Thread 0x7fffb9780700 (LWP 9954)]
[New Thread 0x7fffb8f7f700 (LWP 9955)]
[New Thread 0x7fffb877e700 (LWP 9956)]
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 770
major: 3 minor: 0 memoryClockRate (GHz) 1.1105
pciBusID 0000:01:00.0
Total memory: 1.95GiB
Free memory: 1.55GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 770, pci bus id: 0000:01:00.0)
[New Thread 0x7fffb7f7d700 (LWP 9957)]
[New Thread 0x7fffb5756700 (LWP 9958)]
[New Thread 0x7fffb4f55700 (LWP 9959)]
[New Thread 0x7fffb4754700 (LWP 9960)]
[New Thread 0x7fffb3f53700 (LWP 9961)]
[New Thread 0x7fffb3752700 (LWP 9962)]
[New Thread 0x7fffb2f51700 (LWP 9963)]
[New Thread 0x7fffb2750700 (LWP 9964)]
[New Thread 0x7fffb1f4f700 (LWP 9965)]
[New Thread 0x7fffb174e700 (LWP 9966)]
[New Thread 0x7fffb09ff700 (LWP 9967)]
0/1000
0.458
[Thread 0x7fffb7f7d700 (LWP 9957) exited]
[Thread 0x7fffb5756700 (LWP 9958) exited]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='cw1204772' date='2017-03-05T16:48:25Z'>
		What really boggles me is when I try to back traced today, it come up with a slightly different back traced message. (Core dumped at the exit of the program)
&lt;denchmark-code&gt;Program received signal SIGSEGV, Segmentation fault.
0x00007ffff787601f in _int_free (av=0x7ffff7bb5760 &lt;main_arena&gt;, p=&lt;optimized out&gt;,
    have_lock=0) at malloc.c:3996
3996    malloc.c: No such file or directory.
(gdb) bt
#0  0x00007ffff787601f in _int_free (av=0x7ffff7bb5760 &lt;main_arena&gt;, p=&lt;optimized out&gt;,
    have_lock=0) at malloc.c:3996
#1  0x00000000004f7c6c in ?? ()
#2  0x00000000004f78c4 in ?? ()
#3  0x000000000055a657 in ?? ()
#4  0x000000000059fc62 in ?? ()
#5  0x0000000000507eb0 in PyDict_SetItem ()
#6  0x00000000004f7f71 in _PyModule_Clear ()
#7  0x00000000004c7d61 in PyImport_Cleanup ()
#8  0x0000000000437d4c in Py_Finalize ()
#9  0x000000000044f993 in Py_Main ()
#10 0x00007ffff7818f45 in __libc_start_main (main=0x44f9c2 &lt;main&gt;, argc=2,
    argv=0x7fffffffe438, init=&lt;optimized out&gt;, fini=&lt;optimized out&gt;,
    rtld_fini=&lt;optimized out&gt;, stack_end=0x7fffffffe428) at libc-start.c:287
#11 0x0000000000578c4e in _start ()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='cw1204772' date='2017-03-06T18:53:16Z'>
		&lt;denchmark-link:https://github.com/jhseu&gt;@jhseu&lt;/denchmark-link&gt;
 Any ideas on this one?
		</comment>
		<comment id='8' author='cw1204772' date='2017-03-06T18:55:27Z'>
		Same issue as &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/6968&gt;#6968&lt;/denchmark-link&gt;
. Known issue with Ubuntu 14.04.
You can either upgrade Ubuntu or install numpy from source with OpenBLAS disabled.
		</comment>
		<comment id='9' author='cw1204772' date='2017-04-05T01:35:48Z'>
		Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
	</comments>
</bug>