<bug id='4820' author='wyx-cornell' open_date='2016-10-07T13:25:39Z' closed_time='2017-06-16T18:09:37Z'>
	<summary>Thread created by SummaryWriter not killed</summary>
	<description>
Hi,
I noticed that the _EventLoggerThread created by summary writer does not get killed by the close method, which will make the number of threads keep increasing until it exceeds the system capacity. Is there any particular reason to not kill these threads?
Best
	</description>
	<comments>
		<comment id='1' author='wyx-cornell' date='2016-10-07T20:41:11Z'>
		&lt;denchmark-link:https://github.com/wyx-cornell&gt;@wyx-cornell&lt;/denchmark-link&gt;
 Thanks for filing this issue!
Looking at the code, I believe you're right, and this is a bug.  Have you run into a real-world problem because of this, or did you just notice via code inspection as well?
Would you like to file a pull request to fix this?  Contributions are welcome!
		</comment>
		<comment id='2' author='wyx-cornell' date='2017-01-02T19:16:54Z'>
		Hi, I am working on this, since haosdent is not working on it anymore.
		</comment>
		<comment id='3' author='wyx-cornell' date='2017-04-03T15:30:35Z'>
		I'll try to reproduce this, and I'll work on this if I can reproduce this.
		</comment>
		<comment id='4' author='wyx-cornell' date='2017-04-05T12:56:13Z'>
		&lt;denchmark-link:https://github.com/wyx-cornell&gt;@wyx-cornell&lt;/denchmark-link&gt;
 , according to the latest code and I have tested, number of threads will increase only when you create a new writer every time you want to add_summary. Also, if you kill the thread when close the writer, there will be no thread serving you when you reopen this writer.
how about kill the thread when close the writer and create a new thread when reopen the writer?
		</comment>
		<comment id='5' author='wyx-cornell' date='2017-08-06T21:04:00Z'>
		I was directed to this particular issue from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/2788&gt;#2788&lt;/denchmark-link&gt;
. I had been using tf.trainLoggingTensorHook which I am assuming uses the lower level summary writers under the hood. I am on tensorflow version 1.2 and upon seeing this I removed the logging and I went from intermittent hanging on start of run to now consistent successful training runs upon every start. I was seeing similar gdb traces on all my threads to the above linked issue and it seems all my threads were in lock. If this isn't the same issue I'm more than willing to open a new one, but it at least seemed related.
		</comment>
		<comment id='6' author='wyx-cornell' date='2017-08-14T05:47:48Z'>
		@GrandathePanda , can you provide a minimal test case so we can reproduce?
		</comment>
		<comment id='7' author='wyx-cornell' date='2018-08-20T07:03:39Z'>
		&lt;denchmark-link:https://github.com/suiyuan2009&gt;@suiyuan2009&lt;/denchmark-link&gt;

Below is a test that can reproduce:
We want to make a call from C++ to python and TF. Thus, an embedded python interpreter is used.
However, the Py_EndInterpreter call fails with error message:

Fatal Python error: Py_EndInterpreter: not the last thread

This is because there is a daemon thread _EventLoggerThread is still running after the python script finishes.
The output of this test is:

[33, 67.0]
[&lt;_MainThread(MainThread, started 140735723737984)&gt;, &lt;_EventLoggerThread(Thread-1, started daemon 123145450090496)&gt;]
Fatal Python error: Py_EndInterpreter: not the last thread

test.py file:
# import logging as logger
import sys
import time
import tensorflow as tf
from tensorflow.python.platform import tf_logging as logging
from tensorflow.python.client import timeline
import threading

class Train(object):

    def run(self):
        v = tf.Variable(dtype=tf.float32, initial_value=tf.constant(1.0))
        a = tf.placeholder(tf.float32, shape=None, name="a")
        b = tf.reduce_mean(a, name="b")
        c = tf.add(b, v, name="c")
        add = tf.assign(v, c, name="assign")
        global_step = tf.contrib.framework.get_or_create_global_step()
        sum = tf.summary.scalar(name="sum", tensor=c)
        global_step_inc = tf.assign_add(global_step, 1)
        hooks = [tf.train.StopAtStepHook(last_step=20)]
        ckpt_dir = './tmp/s1/'
        logging.info('begin run')
        with tf.train.MonitoredTrainingSession(checkpoint_dir=ckpt_dir, hooks=hooks) as mon_sess:
            while not mon_sess.should_stop():
                print (mon_sess.run([global_step_inc, add], feed_dict={a: [1.0, 2.0, 3.0]}))
        logging.info('[finish run]')


if __name__ == "__main__":
    print(sys.version)
    Train().run()
    print threading.enumerate()
And the test cpp code:
#include "Python.h"
#include&lt;iostream&gt;

int main(int argc, char* argv[]) {
    // initialize Python
    Py_Initialize() ;
    PyEval_InitThreads() ; // nb: creates and locks the GIL
    PyThreadState* state = PyThreadState_Get();
    PyEval_ReleaseThread(state);

    PyEval_AcquireThread(state);

    const char* pythonFileName = argv[1];
    FILE* to_run_script= fopen(pythonFileName, "r");
    std::cout &lt;&lt; "Run file " &lt;&lt; pythonFileName &lt;&lt; std::endl;
    PyObject * main_module = PyImport_ImportModule("__main__");
    PyObject* global_symbol_table = PyModule_GetDict(main_module);;
    if (to_run_script != NULL) {
        PyRun_File(to_run_script, pythonFileName, Py_file_input, global_symbol_table, global_symbol_table);

        // if python side has daemon thread. Py_EndInterpreter will throw 'not the last thread' exception.
        Py_EndInterpreter(state);

        if (!PyErr_Occurred()) {
            return 0;
        }else {
            PyErr_PrintEx(1);
        }
    }
    PyEval_ReleaseThread(state);
    Py_Finalize() ;
}
		</comment>
		<comment id='8' author='wyx-cornell' date='2019-03-04T23:00:26Z'>
		I think this is still up-to-date.
See &lt;denchmark-link:https://github.com/rwth-i6/returnn/blob/a923504d7265214efb74ef67509db9ac72b84d1c/tests/test_TFUtil.py#L678&gt;here&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/rwth-i6/returnn/blob/3f1e9a235e28f41e695686a345410f32e929a663/TFUtil.py#L4466&gt;here&lt;/denchmark-link&gt;
. This test case still runs with latest TF, e.g. &lt;denchmark-link:https://travis-ci.org/rwth-i6/returnn/jobs/501581634&gt;here&lt;/denchmark-link&gt;
.
( Just noticed that the test case actually does not check whether this is still really the case.)
		</comment>
		<comment id='9' author='wyx-cornell' date='2019-03-11T05:01:24Z'>
		&lt;denchmark-link:https://github.com/whjiang&gt;@whjiang&lt;/denchmark-link&gt;
 it's a long time, I'll take a look in this week.
		</comment>
	</comments>
</bug>