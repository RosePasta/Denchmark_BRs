<bug id='38349' author='0x0badc0de' open_date='2020-04-08T09:30:26Z' closed_time='2020-10-02T16:47:50Z'>
	<summary>`nan` gradient when `tf.where` is used</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):  Yes
OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Debian GNU/Linux 10 (buster)
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device:
TensorFlow installed from (source or
binary): binary
TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0 / v1.12.1-29016-g38797a1c8b 2.2.0-dev20200407
Python version: 3.7.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: - GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
Well-defined function with tf.where has nan gradients at points where tf.where inactive branch is undefined.
Describe the expected behavior
Inactive branch should be ignored in gradients calculations.
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
&lt;denchmark-code&gt;import tensorflow as tf

for ex in range(-3, 3):
    x = tf.convert_to_tensor(10.**ex)
    with tf.GradientTape() as g:
        g.watch(x)
        y = tf.where(x &gt;= -1., x, tf.math.log1p(-x))
#         y = tf.where(x &gt;= -1., x, tf.math.log(1.-x))
#         y = tf.where(x &gt;= -1., x, 1./(1.-x))
    dy_dx = g.gradient(y, x)
    print(f'y({x})={y}, dy/dx({x})={dy_dx}')
&lt;/denchmark-code&gt;

All 3 functions above are well defined for positive values used for testing. Still they show no gradient at point 1.. while it has to be equal to 1.
&lt;denchmark-code&gt;y(0.0010000000474974513)=0.0010000000474974513, dy/dx(0.0010000000474974513)=1.0
y(0.009999999776482582)=0.009999999776482582, dy/dx(0.009999999776482582)=1.0
y(0.10000000149011612)=0.10000000149011612, dy/dx(0.10000000149011612)=1.0
y(1.0)=1.0, dy/dx(1.0)=nan
y(10.0)=10.0, dy/dx(10.0)=1.0
y(100.0)=100.0, dy/dx(100.0)=1.0
&lt;/denchmark-code&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='0x0badc0de' date='2020-04-08T16:16:55Z'>
		I have tried on colab with TF version 2.1.0 , 2.2.0-rc2 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/806f63f2cf04070a4601289d7003cf0a/untitled24.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='0x0badc0de' date='2020-04-08T21:21:52Z'>
		This is due to a limitation limitation in how gradients are calculated. Unfortunately, it is unlikely to be fixed in the foreseable future.
You can find more detail here, along with a recipe for how to avoid it: &lt;denchmark-link:https://stackoverflow.com/questions/33712178/tensorflow-nan-bug/42497444#42497444&gt;https://stackoverflow.com/questions/33712178/tensorflow-nan-bug/42497444#42497444&lt;/denchmark-link&gt;

In short, if the input to a tf.where contains NaNs, the gradient will always be NaN, regardless whether the input is actually used or not, and the workaround is to prevent the inputs from ever containing NaNs.
		</comment>
		<comment id='3' author='0x0badc0de' date='2020-04-08T21:21:54Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38349&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38349&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='0x0badc0de' date='2020-04-08T21:27:19Z'>
		Shouldn't this be documented with big warning in tf.where docs in this case?
		</comment>
		<comment id='5' author='0x0badc0de' date='2020-04-08T22:25:09Z'>
		Indeed it should.
		</comment>
		<comment id='6' author='0x0badc0de' date='2020-04-09T02:42:48Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 Hello, this is my first time contributing to TensofFlow lib. From the thread I gather you would require the  be updated. If it is so can I work on this?
		</comment>
		<comment id='7' author='0x0badc0de' date='2020-04-11T18:53:42Z'>
		Hello &lt;denchmark-link:https://github.com/0x0badc0de&gt;@0x0badc0de&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;

Should the updated doc contain a something like a warning? or will a small note at the end, about the input not being Nan will do? Also should the workaround for avoiding it also be added to the doc?
		</comment>
		<comment id='8' author='0x0badc0de' date='2020-04-11T19:16:08Z'>
		&lt;denchmark-link:https://github.com/joemaren&gt;@joemaren&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/anorak-k&gt;@anorak-k&lt;/denchmark-link&gt;

Sorry for the delay. Feel free to send a PR - it's only a matter of adding a paragraph to the docstring.
The text should be more in the lines of a warning. Something like: Important: if any of the inputs contain NaN values, etc.. And yes, it should include the workaround as well, which is something in the lines of: instead of tf.where(x, ops_that_can_nan(z), ...), write tf.where(x, ops_that_can_nan(tf.where(x, z, safe_value)), ...).
		</comment>
		<comment id='9' author='0x0badc0de' date='2020-04-13T10:02:31Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 I have added the change and raised a PR &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/38467&gt;#38467&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='0x0badc0de' date='2020-04-18T22:25:55Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 Thanks for your reply. However, I would like to mention that this behavior also happens when the generated value in the inactive branch  (i.e.  or ). Here is a minimal reproducible example:
import tensorflow as tf

a = tf.Variable(10.)
with tf.GradientTape() as tape:
  out = tf.where(a &lt; 15., a, tf.math.pow(10.0, tf.math.exp(a)))
  grads = tape.gradient(out, a)

print(grads)
# tf.Tensor(nan, shape=(), dtype=float32)
And also if we reverse the condition such that the branch with infinite value is selected, the gradient would be infinite (which is a bit surprising that it does not generate nan instead, like above):
with tf.GradientTape() as tape:
  out = tf.where(a &gt; 15., a, tf.math.pow(10.0, tf.math.exp(a)))
  grads = tape.gradient(out, a)

print(grads)
# tf.Tensor(inf, shape=(), dtype=float32)
So this behavior happens for both nan and infinite values in inactive branch. I wish it wasn't like this, because it's a bit unreasonable and makes it impossible to use user-defined ops/functions which generate extremely large values for some input values; hence, that inner tf.where workaround may not be practical always (unfortunately, even gradient clipping does not help with this, because clipping a nan value produces nan in TF).
CC: &lt;denchmark-link:https://github.com/anorak-k&gt;@anorak-k&lt;/denchmark-link&gt;
 for potential consideration in your PR after &lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 confirms this.
		</comment>
		<comment id='11' author='0x0badc0de' date='2020-04-19T02:27:57Z'>
		&lt;denchmark-link:https://github.com/mkaze&gt;@mkaze&lt;/denchmark-link&gt;
 that's true - nan, inf and any other special FP value will disrupt the gradient calculation.
What happens internally is that the gradients are aggregated in this fashion: 1 * &lt;grad of branch taken&gt; + 0 * &lt;grad of branch not taken&gt;. In the former case, you have 0 * inf = nan. In the latter case, you have 1 * inf = inf. I agree it's very confusing, unfortunately a naive fix would add significant overhead to gradient calculations.
Moreover, the forward calculation doesn't need to result in a nan or inf. You can also get weird results if the gradient alone is nan or inf. For example, the cube root function is defined and well-behaved everywhere, but its derivative at zero is infinite. So this will give you a nan gradient too:
&lt;denchmark-code&gt;a = tf.Variable(0.0)
with tf.GradientTape() as tape:
  out = tf.where(a &lt; 1, a, tf.pow(a, 1.0/3.0))
  grads = tape.gradient(out, a)
print(grads)
&lt;/denchmark-code&gt;

I think the tf.where workaround is useful with infinite values as well, so long as the branch not taken is forced to take a gradient that can be safely multiplied by 0. For your example, it would be something like this:
&lt;denchmark-code&gt;dummy_safe_value = 0
safe_a = tf.where(a &gt; 15., dummy_safe_value, a)
out = tf.where(a &gt; 15., a, tf.math.pow(10.0, tf.math.exp(safe_a)))
&lt;/denchmark-code&gt;

I agree that it sometimes can be impractical to do, but in principle it should always be possible as long as you control the inputs to the sensitive functions - all they have to do is force finite values in all the elements that are dropped.
		</comment>
		<comment id='12' author='0x0badc0de' date='2020-05-07T14:42:20Z'>
		I want to fix the issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/38349&gt;#38349&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='0x0badc0de' date='2020-05-14T15:47:54Z'>
		
This is due to a limitation limitation in how gradients are calculated. Unfortunately, it is unlikely to be fixed in the foreseable future.
You can find more detail here, along with a recipe for how to avoid it: https://stackoverflow.com/questions/33712178/tensorflow-nan-bug/42497444#42497444
In short, if the input to a tf.where contains NaNs, the gradient will always be NaN, regardless whether the input is actually used or not, and the workaround is to prevent the inputs from ever containing NaNs.

You can simply have it raise a value error if its getting Nan inputs. Or does it not work like that?
		</comment>
		<comment id='14' author='0x0badc0de' date='2020-05-29T11:17:54Z'>
		Can I work on this issue if someone isn't now?
		</comment>
		<comment id='15' author='0x0badc0de' date='2020-05-29T14:36:50Z'>
		&lt;denchmark-link:https://github.com/tushar-dalal&gt;@tushar-dalal&lt;/denchmark-link&gt;
 The challenge is that verifying for such NaN inputs can be taking on performance. When debugging,  can indeed help with that.
&lt;denchmark-link:https://github.com/unicorn-io&gt;@unicorn-io&lt;/denchmark-link&gt;
 Feel free to tackle it, but note that it's extremely challenging to solve. That said, there was a PR (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/38467&gt;#38467&lt;/denchmark-link&gt;
) to add a warning message to the docs of tf.where, it would be useful to revive it.
		</comment>
		<comment id='16' author='0x0badc0de' date='2020-05-29T16:19:20Z'>
		I am motivated to do this can you give me some tips to start with I will try my best to understand and resolve this issue.
		</comment>
		<comment id='17' author='0x0badc0de' date='2020-06-02T01:38:13Z'>
		
I am motivated to do this can you give me some tips to start with I will try my best to understand and resolve this issue. @mdanatg

		</comment>
		<comment id='18' author='0x0badc0de' date='2020-06-02T12:11:03Z'>
		&lt;denchmark-link:https://github.com/unicorn-io&gt;@unicorn-io&lt;/denchmark-link&gt;
 You can start by looking at the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/eager/tape.h#L149&gt;gradient code&lt;/denchmark-link&gt;
 and understanding how it works. Then you can reproduce when happens in the case of a where with bad gradients.
		</comment>
		<comment id='19' author='0x0badc0de' date='2020-06-02T15:26:53Z'>
		Cool I'll get to it
		</comment>
		<comment id='20' author='0x0badc0de' date='2020-06-08T09:39:52Z'>
		Hey i would like to work on it. can also help please
		</comment>
		<comment id='21' author='0x0badc0de' date='2020-06-17T11:34:28Z'>
		
Cool I'll get to it

This bug cannot be fixed as of now it seems.
		</comment>
		<comment id='22' author='0x0badc0de' date='2020-06-17T11:51:13Z'>
		It's indeed very challenging to fix. However, the documentation of affected ops, like tf.where can still be updated to alert the users about it.
		</comment>
		<comment id='23' author='0x0badc0de' date='2020-06-18T05:00:16Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 isn't &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/38497&gt;#38497&lt;/denchmark-link&gt;
 addressing this and is closed?
		</comment>
		<comment id='24' author='0x0badc0de' date='2020-06-18T11:40:47Z'>
		You mean &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/38467&gt;#38467&lt;/denchmark-link&gt;
? It's closed due to staleness, and it would be useful to revive. By the looks of it it's safe to assume noone else is working on it.
		</comment>
		<comment id='25' author='0x0badc0de' date='2020-07-01T16:44:28Z'>
		Seems like its a long time since the last activity. Is this issue still open to be worked on?
		</comment>
		<comment id='26' author='0x0badc0de' date='2020-07-01T16:47:47Z'>
		I think so. There are two parts to it: (1) updating the docs of tf.where, which is fairly straightforward, and (2) actually trying to address the issue, which is a significant undertaking because it involves a rather fundamental issue.
		</comment>
		<comment id='27' author='0x0badc0de' date='2020-07-08T11:41:29Z'>
		Is this issue still addressable ?
		</comment>
		<comment id='28' author='0x0badc0de' date='2020-07-25T07:42:20Z'>
		Nice to be part of the group.
Please, have a look to my pull request for the workaround: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/41721&gt;#41721&lt;/denchmark-link&gt;

I'm going to work on the main issue too.
I'll be happy to cooperate with anybody else interested.
		</comment>
		<comment id='29' author='0x0badc0de' date='2020-07-27T13:18:53Z'>
		&lt;denchmark-link:https://github.com/codeadmin-peritiae&gt;@codeadmin-peritiae&lt;/denchmark-link&gt;
 The PR appears to be empty. Perhaps there's an issue with the git client?
		</comment>
		<comment id='30' author='0x0badc0de' date='2020-08-14T14:30:40Z'>
		Just to follow up on the events. It looks like codeadmin-peritiae had an issue with his original PR &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/41721&gt;#41721&lt;/denchmark-link&gt;
 where he had trouble with his SSH certificate. He then opened up another PR &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/41775&gt;#41775&lt;/denchmark-link&gt;
 which is currently blocked since some of the checks haven't completed. By the looks of it, the documentation update part of this problem is almost completed.
		</comment>
		<comment id='31' author='0x0badc0de' date='2020-09-28T18:49:30Z'>
		Is it issue solvable for a beginner ? if yes can I work on it? &lt;denchmark-link:https://github.com/Harsh188&gt;@Harsh188&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;

		</comment>
		<comment id='32' author='0x0badc0de' date='2020-09-29T02:25:02Z'>
		I think this issue should be closed, &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/41775&gt;#41775&lt;/denchmark-link&gt;
 got merged and fixed the issue with the documentation.
		</comment>
		<comment id='33' author='0x0badc0de' date='2020-10-02T16:47:51Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38349&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38349&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>