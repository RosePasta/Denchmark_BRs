<bug id='39110' author='jazzsir' open_date='2020-05-03T05:02:32Z' closed_time='2020-05-17T14:19:35Z'>
	<summary>MultiWorkerMirroredStrategy doesn't work without memory_growth</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): Container image tensorflow/tensorflow:2.1.0-gpu-py3 (tf-nightly-gpu installed)
TensorFlow version (use command below): 2.2.0-dev20200501
Python version: 3.6.9
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CUDA/cuDNN version: release 10.1, V10.1.243, but I can't find cuDNN libraries using a command cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2
GPU model and memory: (NVIDIA Tesla T4(16G) * 4) * 2 nodes on GCP
kubeflow 1.0.2 (kubernetes 1.15.11, tf-operator 1.0)

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4569678/tf_env.txt&gt;tf_env.txt&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"
TensorFlow version:  v1.12.1-31004-g203aa8b634 2.2.0-dev20200501

Describe the current behavior
if memory_growth of each GPU doesn't set to true as below
&lt;denchmark-code&gt;gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)

    except RuntimeError as e:
        print(e)
&lt;/denchmark-code&gt;

Out of memory errors occur
&lt;denchmark-code&gt;2020-05-03 02:05:59.765039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1686] Adding visible gpu devices: 0, 1, 2, 3
2020-05-03 02:05:59.769309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1085] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-05-03 02:05:59.769338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1091]      0 1 2 3
2020-05-03 02:05:59.769346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 0:   N Y N N
2020-05-03 02:05:59.769350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 1:   Y N N N
2020-05-03 02:05:59.769354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 2:   N N N Y
2020-05-03 02:05:59.769359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 3:   N N Y N
2020-05-03 02:05:59.770642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-03 02:05:59.774976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-03 02:05:59.779921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-03 02:05:59.785132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-03 02:05:59.789412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-03 02:05:59.793147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1230] Created TensorFlow device (/job:worker/replica:0/task:4/device:GPU:0 with 13297 MB memory) -&gt; physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)
2020-05-03 02:05:59.793269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-03 02:05:59.801442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1230] Created TensorFlow device (/job:worker/replica:0/task:4/device:GPU:1 with 13297 MB memory) -&gt; physical GPU (device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5)
2020-05-03 02:05:59.801564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-03 02:05:59.817011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1230] Created TensorFlow device (/job:worker/replica:0/task:4/device:GPU:2 with 13297 MB memory) -&gt; physical GPU (device: 2, name: Tesla T4, pci bus id: 0000:00:06.0, compute capability: 7.5)
2020-05-03 02:05:59.817149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-03 02:05:59.831155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1230] Created TensorFlow device (/job:worker/replica:0/task:4/device:GPU:3 with 13323 MB memory) -&gt; physical GPU (device: 3, name: Tesla T4, pci bus id: 0000:00:07.0, compute capability: 7.5)
2020-05-03 02:05:59.834444: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; dist-worker-0.default.svc:2222, 1 -&gt; dist-worker-1.default.svc:2222, 2 -&gt; dist-worker-2.default.svc:2222, 3 -&gt; dist-worker-3.default.svc:2222, 4 -&gt; localhost:2222}
2020-05-03 02:05:59.834974: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:2222
WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your
local data directory. If you'd instead prefer to read directly from our public
GCS bucket (recommended if you're running on GCP), you can instead pass
`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.

Dl Completed...: 100%|██████████| 4/4 [00:00&lt;00:00,  7.41 file/s]2020-05-03 02:06:02.069644: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 12.99G (13943597824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:02.072746: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 11.69G (12549237760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:02.075769: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 10.52G (11294313472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:02.078466: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 9.47G (10164881408 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:02.081365: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 8.52G (9148393472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:02.084128: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 7.67G (8233553920 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:02.086712: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 6.90G (7410198528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:02.089443: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 6.21G (6669178368 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:02.092258: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 5.59G (6002260480 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:02.094925: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 5.03G (5402034176 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:02.097778: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 4.53G (4861830656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
.
.
.
.
2020-05-03 02:06:12.431390: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 5.0K (5120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.433206: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 4.5K (4608 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.435134: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 4.2K (4352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.437051: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 4.0K (4096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.438725: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 3.8K (3840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.440388: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 3.5K (3584 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.442053: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 3.2K (3328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.444054: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 3.0K (3072 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.445745: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.8K (2816 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.447422: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.5K (2560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.449275: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.451169: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.453220: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.454900: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.456610: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.458291: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.460314: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.462209: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.464132: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
.
.
.
&lt;/denchmark-code&gt;


nvidia-smi info

&lt;denchmark-code&gt;## Node 1
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla T4            On   | 00000000:00:04.0 Off |                    0 |
| N/A   64C    P0    31W /  70W |  14220MiB / 15109MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla T4            On   | 00000000:00:05.0 Off |                    0 |
| N/A   66C    P0    31W /  70W |  14220MiB / 15109MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla T4            On   | 00000000:00:06.0 Off |                    0 |
| N/A   64C    P0    29W /  70W |  14220MiB / 15109MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla T4            On   | 00000000:00:07.0 Off |                    0 |
| N/A   64C    P0    30W /  70W |  14220MiB / 15109MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     11553      C   python                                     14209MiB |
|    1     11553      C   python                                     14209MiB |
|    2     11553      C   python                                     14209MiB |
|    3     11553      C   python                                     14209MiB |
+-----------------------------------------------------------------------------+


## Node 2
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla T4            On   | 00000000:00:04.0 Off |                    0 |
| N/A   59C    P0    28W /  70W |  15103MiB / 15109MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla T4            On   | 00000000:00:05.0 Off |                    0 |
| N/A   64C    P0    29W /  70W |  15103MiB / 15109MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla T4            On   | 00000000:00:06.0 Off |                    0 |
| N/A   68C    P0    32W /  70W |  15103MiB / 15109MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
^C|   3  Tesla T4            On   | 00000000:00:07.0 Off |                    0 |
| N/A   66C    P0    30W /  70W |  15103MiB / 15109MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     18804      C   python                                     13541MiB |
|    0     18917      C   python                                      1013MiB |
|    0     18972      C   python                                       243MiB |
|    0     19285      C   python                                       295MiB |
|    1     18804      C   python                                       241MiB |
|    1     18917      C   python                                       301MiB |
|    1     18972      C   python                                     13537MiB |
|    1     19285      C   python                                      1013MiB |
|    2     18804      C   python                                       301MiB |
|    2     18917      C   python                                      1013MiB |
|    2     18972      C   python                                       243MiB |
|    2     19285      C   python                                     13535MiB |
|    3     18804      C   python                                       251MiB |
|    3     18917      C   python                                     13589MiB |
|    3     18972      C   python                                       239MiB |
|    3     19285      C   python                                      1013MiB |
+-----------------------------------------------------------------------------+
&lt;/denchmark-code&gt;


Most of all, I would like to know the reason why MultiWorkerMirroredStrategy doesn't work without 
and whether it is normal operation or not.
if it adversely affects performance, the strategy should work without memory_growth as the example code &lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras&gt;here&lt;/denchmark-link&gt;

Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

My simple code

&lt;denchmark-code&gt;import os, json
os.environ['NCCL_DEBUG'] = 'INFO'
import tensorflow_datasets as tfds
import tensorflow as tf
tfds.disable_progress_bar()

gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)

    except RuntimeError as e:
        print(e)


strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(communication=tf.distribute.experimental.CollectiveCommunication.NCCL) # NCCL vs RING

BATCH_SIZE = 64
GLOBAL_BATCH_SIZE = BATCH_SIZE * 2
BUFFER_SIZE = 10000

print('Number of devices: {}'.format(strategy.num_replicas_in_sync))

def make_datasets_unbatched():
  # Scaling MNIST data from (0, 255] to (0., 1.]
  def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255
    return image, label

  datasets, info = tfds.load(name='mnist',
                            with_info=True,
                            as_supervised=True)

  return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)

def build_and_compile_cnn_model():
  model = tf.keras.Sequential([
      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(10)
  ])
  model.compile(
      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
      optimizer=tf.keras.optimizers.Adam(),
      metrics=['accuracy'])
  return model

with strategy.scope():
  train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE).repeat()
  options = tf.data.Options()
  options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA
  train_datasets = train_datasets.with_options(options)
  multi_worker_model = build_and_compile_cnn_model()

multi_worker_model.fit(x=train_datasets, epochs=3, steps_per_epoch=5)
&lt;/denchmark-code&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='jazzsir' date='2020-05-04T17:14:09Z'>
		&lt;denchmark-link:https://github.com/jazzsir&gt;@jazzsir&lt;/denchmark-link&gt;
 Can you please check these two resources &lt;denchmark-link:https://stackoverflow.com/questions/36838770/how-to-interpret-tensorflow-output&gt;one&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://stackoverflow.com/questions/44232898/memoryerror-in-tensorflow-and-successful-numa-node-read-from-sysfs-had-negativ&gt;two&lt;/denchmark-link&gt;
. I think setting the following might help you.
config.gpu_options.per_process_gpu_memory_fraction=0.3
Based on the warning, can you input try_gcs=True to tfds.load

WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your
local data directory. If you'd instead prefer to read directly from our public
GCS bucket (recommended if you're running on GCP), you can instead pass
try_gcs=True to tfds.load or set data_dir=gs://tfds-data/datasets.

		</comment>
		<comment id='2' author='jazzsir' date='2020-05-05T06:03:26Z'>
		Does this happen consistently? Or is it after multiple runs?
This can happen if you are sharing GPU resources with another process and hence run into errors during initialization. The set_memory_growth option should not be related to using MultiWorkerMirroredStrategy. Have you tried running this with MirroredStrategy on a single machine(4GPUs)?
		</comment>
		<comment id='3' author='jazzsir' date='2020-05-12T06:08:10Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='4' author='jazzsir' date='2020-05-17T14:19:32Z'>
		I figured it out, I ran the Strategy on Kubeflow platform where GPUs are allocated to multiple workers.
Thanks.
		</comment>
		<comment id='5' author='jazzsir' date='2020-05-17T14:19:36Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39110&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39110&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>