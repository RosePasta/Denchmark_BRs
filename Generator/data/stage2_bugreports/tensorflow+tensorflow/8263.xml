<bug id='8263' author='CarbonComputed' open_date='2017-03-10T03:46:16Z' closed_time='2017-05-19T18:53:25Z'>
	<summary>Documentation incorrect for RNN tutorial?</summary>
	<description>
Is it possible the documentation is incorrect on &lt;denchmark-link:https://www.tensorflow.org/tutorials/recurrent&gt;https://www.tensorflow.org/tutorials/recurrent&lt;/denchmark-link&gt;
 ?
&lt;denchmark-code&gt;words = tf.placeholder(tf.int32, [batch_size, num_steps])
for i in range(num_steps):
    # The value of state is updated after processing each batch of words.
    output, state = lstm(words[:, i], state)
&lt;/denchmark-code&gt;

It seems the correct output should be:
    output, state = lstm(words[i, :], state)
Since you want to process the words in the same sequence correct?
E.g.
[[The, quick, brown]
[fox, jumped, over]]
words[:,0] == [The, fox]
Whereas what you want is [The, quick, brown]  == words[0,:]
Please correct me if I'm wrong, thanks.
	</description>
	<comments>
		<comment id='1' author='CarbonComputed' date='2017-03-10T03:53:31Z'>
		This looks wrong to my admittedly untrained eye (and doesn't match &lt;denchmark-link:https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py#L146&gt;https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py#L146&lt;/denchmark-link&gt;
)
&lt;denchmark-link:https://github.com/xmbrst&gt;@xmbrst&lt;/denchmark-link&gt;
 Who is the author of this tutorial?
		</comment>
		<comment id='2' author='CarbonComputed' date='2017-03-10T16:16:01Z'>
		Looks like Rafal Jozefowicz, originally.
		</comment>
		<comment id='3' author='CarbonComputed' date='2017-03-10T19:08:39Z'>
		Upon looking over this, I see whats being done here. They process in batches and store the running state for each batch. However, what led me to this confusion was the first example in the tutorial takes a batch of words as input, so it doesn't seem to make sense when compared to the Truncated Backprop example.  The initial example probably should be more clear on what a batch of words looks like in this example. Mainly I think many would appreciate just a bit more detail than the pseudo code provided.
		</comment>
		<comment id='4' author='CarbonComputed' date='2017-04-28T21:55:44Z'>
		Since you have understood it, could you add annotations (comments) in a PR?
		</comment>
		<comment id='5' author='CarbonComputed' date='2017-04-28T22:05:08Z'>
		&lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
 Sure. I'm not sure where the code for the tutorials lives however. (&lt;denchmark-link:https://www.tensorflow.org/tutorials/recurrent&gt;https://www.tensorflow.org/tutorials/recurrent&lt;/denchmark-link&gt;
). Any thoughts?
		</comment>
		<comment id='6' author='CarbonComputed' date='2017-04-28T22:14:34Z'>
		The code lives here: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/tutorials/recurrent.md&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/tutorials/recurrent.md&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='CarbonComputed' date='2017-05-08T05:50:06Z'>
		&lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/9746&gt;#9746&lt;/denchmark-link&gt;

Hopefully my changes make sense.
		</comment>
	</comments>
</bug>