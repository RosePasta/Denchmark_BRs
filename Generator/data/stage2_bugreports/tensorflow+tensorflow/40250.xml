<bug id='40250' author='fengyang0317' open_date='2020-06-07T21:50:07Z' closed_time='2020-06-27T12:46:34Z'>
	<summary>tf.raw_ops.CollectivePermute bug caused by strange device numbering</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.2.0
Python version: 3.6

Describe the current behavior
The 8 cores are numbered as 0,1,2,3,6,7,4,5 by CollectivePermute and  tensorflow.python.tpu.ops.tpu_ops.all_to_all.
Describe the expected behavior
They should be numbered as 0,1,2,3,4,5,6,7.
Standalone code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(
  tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.experimental.TPUStrategy(resolver)

@tf.function
def step_fn():
  context = tf.distribute.get_replica_context()
  v = context.replica_id_in_sync_group
  v = tf.raw_ops.CollectivePermute(
      input=v,
      source_target_pairs=[[0,1],[1,2],[2,3],[3,4],[4,5],[5,6],[6,7],[7,0]])
  return v

ret = strategy.run(step_fn)
print(ret)
&lt;/denchmark-code&gt;

Other info / logs Include any logs or source code that would be helpful to
The following is the output.
&lt;denchmark-code&gt;PerReplica:{
  0: tf.Tensor(5, shape=(), dtype=int32),
  1: tf.Tensor(0, shape=(), dtype=int32),
  2: tf.Tensor(1, shape=(), dtype=int32),
  3: tf.Tensor(2, shape=(), dtype=int32),
  4: tf.Tensor(7, shape=(), dtype=int32),
  5: tf.Tensor(4, shape=(), dtype=int32),
  6: tf.Tensor(3, shape=(), dtype=int32),
  7: tf.Tensor(6, shape=(), dtype=int32)
}
&lt;/denchmark-code&gt;

The correct output should be 7,0,1,2,3,4,5,6
	</description>
	<comments>
		<comment id='1' author='fengyang0317' date='2020-06-08T07:12:04Z'>
		I have tried in colab with TF version 2.2 and was able to reproduce the issue.But in NIghtly version(2.3.0-dev20200605) i am seeing the below error message.(InvalidArgumentError: NodeDef expected inputs 'string' do not match 0 inputs specified; Op&lt;name=_Send; signature=tensor:T -&gt; ; attr=T:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true&gt;; NodeDef: {{node _Send}}).Thanks!
		</comment>
		<comment id='2' author='fengyang0317' date='2020-06-09T04:10:23Z'>
		I think this is expected behavior as XLA could interpret replica id differently than user's replica id.
See comment here: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/distribute/distribute_lib.py#L2444-L2445&gt;https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/distribute/distribute_lib.py#L2444-L2445&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='fengyang0317' date='2020-06-09T04:45:44Z'>
		&lt;denchmark-link:https://github.com/hthu&gt;@hthu&lt;/denchmark-link&gt;
 For the following code
&lt;denchmark-code&gt;import os
import tensorflow as tf
from tensorflow.python.distribute.values import PerReplica

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(
  tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.experimental.TPUStrategy(resolver)

@tf.function
def gen_id():
  context = tf.distribute.get_replica_context()
  return context.replica_id_in_sync_group

t = strategy.run(gen_id)
print(t)

@tf.function
def step_fn(v):
  v = tf.raw_ops.CollectivePermute(
      input=v,
      source_target_pairs=[[0,1],[1,2],[2,3],[3,4],[4,5],[5,6],[6,7],[7,0]])
  return v

ret = strategy.run(step_fn, args=[t])
print(ret)
&lt;/denchmark-code&gt;

The output is
&lt;denchmark-code&gt;PerReplica:{
  0: tf.Tensor(0, shape=(), dtype=int32),
  1: tf.Tensor(1, shape=(), dtype=int32),
  2: tf.Tensor(2, shape=(), dtype=int32),
  3: tf.Tensor(3, shape=(), dtype=int32),
  4: tf.Tensor(4, shape=(), dtype=int32),
  5: tf.Tensor(5, shape=(), dtype=int32),
  6: tf.Tensor(6, shape=(), dtype=int32),
  7: tf.Tensor(7, shape=(), dtype=int32)
}
PerReplica:{
  0: tf.Tensor(5, shape=(), dtype=int32),
  1: tf.Tensor(0, shape=(), dtype=int32),
  2: tf.Tensor(1, shape=(), dtype=int32),
  3: tf.Tensor(2, shape=(), dtype=int32),
  4: tf.Tensor(7, shape=(), dtype=int32),
  5: tf.Tensor(4, shape=(), dtype=int32),
  6: tf.Tensor(3, shape=(), dtype=int32),
  7: tf.Tensor(6, shape=(), dtype=int32)
}
&lt;/denchmark-code&gt;

Are these outputs still expected behavior?
		</comment>
		<comment id='4' author='fengyang0317' date='2020-06-09T05:39:53Z'>
		Unfortunately yes.
For the first part, we are merely exercising replica id in context.
The point being, when you execute collectivePermute, xla would view the IDs differently.
More specifically, the id 4 in context might not be viewed as id 4 when performing collectivePermute.
You can also do a quick check by just providing single pair [0,3] and see what that returns.
That said, I do think we should be more clear on this.
I'll follow up tomorrow and see what we can do here.
LMK if you have any questions!
		</comment>
		<comment id='5' author='fengyang0317' date='2020-06-09T10:50:09Z'>
		The input and output of strategy.run are sent to and received from the 8 cores, respectively. Are the order of sending and receiving related to XLA replica ID?
		</comment>
		<comment id='6' author='fengyang0317' date='2020-06-09T17:47:29Z'>
		Yes. And that ID isn't really the same as the context replica id here.
		</comment>
		<comment id='7' author='fengyang0317' date='2020-06-10T00:28:41Z'>
		In this call in the code above, there is no replica id involved. But the order is still strange.
&lt;denchmark-code&gt;ret = strategy.run(step_fn, args=[t])
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='fengyang0317' date='2020-06-10T04:11:28Z'>
		I think you are running this with the replica_id_in_sync_group from context, right?
replica_id_in_sync_group 0 isn't necessarily isn't necessarily the 0 in XLA world.
You could try this to verify the mapping:
@tf.function
def step_fn():
  context = tf.distribute.get_replica_context()
  v = context.replica_id_in_sync_group
  v = tf.raw_ops.CollectivePermute(
      input=v,
      source_target_pairs=[[1,4]])
  return v
		</comment>
		<comment id='9' author='fengyang0317' date='2020-06-10T10:54:03Z'>
		I have figured out the current mapping and switching 4 &lt;-&gt; 6, 5 &lt;-&gt; 7 can lead to the result I want.
Let us get rid of replica_id_in_sync_group. My question is that the output of the following code looks strange.
&lt;denchmark-code&gt;import os
import tensorflow as tf
from tensorflow.python.distribute.values import PerReplica

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(
  tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.experimental.TPUStrategy(resolver)

t = tf.range(8)
t = tf.unstack(t)

@tf.function
def step_fn(v):
  v = tf.raw_ops.CollectivePermute(
      input=v,
      source_target_pairs=[[0,1],[1,2],[2,3],[3,4],[4,5],[5,6],[6,7],[7,0]])
  return v

ret = strategy.run(step_fn, args=[PerReplica(t)])
print(ret)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='fengyang0317' date='2020-06-26T22:34:44Z'>
		Sorry for the late reply!
This is the due to the same reason as the ones with the Id.
TpuStrategy has a context that records replica_id_in_sync_group. When performing XLA operations, the participating TPU group in the strategy will inherently have a id, which again, might not map to the XLA id.
		</comment>
		<comment id='11' author='fengyang0317' date='2020-06-27T12:46:36Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40250&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40250&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>