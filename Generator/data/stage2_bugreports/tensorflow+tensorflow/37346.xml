<bug id='37346' author='padoremu' open_date='2020-03-05T09:59:54Z' closed_time='2020-03-06T07:15:32Z'>
	<summary>Training of tf.keras.layers.RNN with preceding Reshape using tf.shape fails</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution: Linux Ubuntu 18.04
Mobile device if the issue happens on mobile device: -
TensorFlow installed from: binary
TensorFlow version: 2.2.0-dev20200303
Python version: 3.6.9
Bazel version (if compiling from source): -
GCC/Compiler version (if compiling from source): -
CUDA/cuDNN version: CPU only
GPU model and memory: CPU only

Describe the current behavior
The model compiles, but training fails.
Describe the expected behavior
I would expect training to succeed.
Code to reproduce the issue
import tensorflow as tf
import numpy as np

batch_size = 1
num_units = 1
dim_other = 10

# build model
model = tf.keras.Sequential()

model.add(tf.keras.layers.Input(shape=(None, dim_other)))

dim_time_read = tf.shape(model.output)[1]
model.add(tf.keras.layers.Reshape(target_shape=(dim_time_read, dim_other)))

model.add(tf.keras.layers.RNN(cell=tf.keras.layers.GRUCell(units=num_units)))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# train
training_input1 = np.zeros([batch_size, 1, dim_other])
training_input2 = np.zeros([batch_size, 2, dim_other])
training_output = np.zeros([batch_size, num_units])

model.fit(training_input1, training_output)
model.fit(training_input2, training_output)
Other info / logs
Apparently, the Reshape layer is not required in this stripped down example. If one leaves it out, training succeeds. However, in my actual setup I need a Reshape layer before the RNN layer due to a preceding Conv2D layer.
The context is training a CRNN with variable length input. Using padding and masking is not an option unfortunately, since tf.keras.layers.Conv2D does currently not support masking.
If my usage of tf.shape is wrong, if there is an alternative or workaround, please let me know.
Traceback in case of failure:
Traceback (most recent call last):
  File "/home/test/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
TypeError: An op outside of the function building code is being passed
a "Graph" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: strided_slice:0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "test.py", line 27, in &lt;module&gt;
    model.fit(training_input, training_output)
  File "/home/test/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py", line 62, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/home/test/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py", line 775, in fit
    tmp_logs = train_function(iterator)
  File "/home/test/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 580, in __call__
    result = self._call(*args, **kwds)
  File "/home/test/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 644, in _call
    return self._stateless_fn(*args, **kwds)
  File "/home/test/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 2420, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/home/test/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1665, in _filtered_call
    self.captured_inputs)
  File "/home/test/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1746, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/test/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 598, in call
    ctx=ctx)
  File "/home/test/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 74, in quick_execute
    "tensors, but found {}".format(keras_symbolic_tensors))
tensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [&lt;tf.Tensor 'strided_slice:0' shape=() dtype=int32&gt;]

	</description>
	<comments>
		<comment id='1' author='padoremu' date='2020-03-05T12:09:48Z'>
		I could replicate the issue please find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/ecddb668c05648186c1f19484fdffc48/untitled73.ipynb&gt;gist here&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='padoremu' date='2020-03-05T18:28:10Z'>
		I think this error is expected since it you are feeding a symbolic tensor from model.output.shape to tf.keras.layers.Reshape(). Note that target_shape param for Reshape layer is expected to be tuple of integers, rather than tensors. The error message is also quite explicit about that slide op which is produced by dim_time_read = tf.shape(model.output)[1]
When u need reshape the output from previous layer, you could specify -1 if a particular dim is unknown. I can comment further if you have your model code posted.
		</comment>
		<comment id='3' author='padoremu' date='2020-03-06T07:15:32Z'>
		&lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
 Thank you very much, I wasn't aware of the possibility to specify -1 for a particular unknown dimension. Setting  works perfectly. Below you find sample code for the CRNN, in case one is interested. I am closing the ticket.
import tensorflow as tf
import numpy as np


batch_size = 1
num_units = 2
num_filters = 1
dim_other = 10

model = tf.keras.Sequential()

model.add(tf.keras.layers.Input(shape=(None, dim_other, 1)))

model.add(tf.keras.layers.Conv2D(filters=num_filters, kernel_size=(10, 4), strides=[2, 1]))

conv_output_width = model.output.shape[2]

model.add(tf.keras.layers.Reshape(target_shape=(-1, conv_output_width * num_filters)))

model.add(tf.keras.layers.RNN(cell=tf.keras.layers.GRUCell(units=num_units)))

model.add(tf.keras.layers.Dense(units=num_units))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# train
training_input1 = np.zeros([batch_size, 10, dim_other, 1])
training_input2 = np.zeros([batch_size, 12, dim_other, 1])
training_output = np.zeros([batch_size, num_units])

model.fit(training_input1, training_output)
model.fit(training_input2, training_output)
		</comment>
		<comment id='4' author='padoremu' date='2020-03-06T07:15:34Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37346&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37346&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>