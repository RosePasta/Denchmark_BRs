<bug id='29122' author='weiminson' open_date='2019-05-29T10:06:00Z' closed_time='2019-07-22T14:37:26Z'>
	<summary>Entity &amp;lt;&amp;gt; could not be transformed and will be staged without change</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 2.0 alpha
Python version: 3.6
CUDA/cuDNN version: 10
GPU model and memory: GTX2080Ti

Hi there, could you please have a look at the issue? A customized network module (in Keras) cannot work as for gradient backpropagation.
Log:
W0529 19:29:46.658907 140071302989632 tf_logging.py:161] Entity &lt;bound method Layer.call of &lt;tensorflow.python.keras.engine.training.Model object at 0x7f6404415fd0&gt;&gt; could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY &gt;= 1. Please report this to the AutoGraph team. Cause: Unexpected error transforming &lt;bound method Layer.call of &lt;tensorflow.python.keras.engine.training.Model object at 0x7f6404415fd0&gt;&gt;. If you believe this is due to a bug, please set the verbosity to 10 (on Linux, export AUTOGRAPH_VERBOSITY=10) and attach the full output when filing the bug report. Caused by: node &lt;gast.gast.Subscript object at 0x7f61f416e4e0&gt; has ctx unset
W0529 19:29:52.344057 140071302989632 tf_logging.py:161] Entity &lt;function train_G at 0x7f64c79a9ae8&gt; could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY &gt;= 1. Please report this to the AutoGraph team. Cause: Unexpected error transforming &lt;function train_G at 0x7f64c79a9ae8&gt;. If you believe this is due to a bug, please set the verbosity to 10 (on Linux, export AUTOGRAPH_VERBOSITY=10) and attach the full output when filing the bug report. Caused by: node &lt;gast.gast.Name object at 0x7f61ad7cb588&gt; has ctx unset
W0529 19:29:52.960978 140071302989632 optimizer_v2.py:928] Gradients does not exist for variables ['conv2d_24/kernel:0', 'conv2d_24/bias:0', 'conv2d_25/kernel:0', 'conv2d_25/bias:0', 'conv2d_26/kernel:0', 'conv2d_26/bias:0', 'conv2d_27/kernel:0', 'conv2d_27/bias:0', 'conv2d_28/kernel:0', 'conv2d_28/bias:0', 'conv2d_29/kernel:0', 'conv2d_29/bias:0', 'conv2d_30/kernel:0', 'conv2d_30/bias:0', 'conv2d_31/kernel:0', 'conv2d_31/bias:0', 'conv2d_32/kernel:0', 'conv2d_32/bias:0', 'conv2d_33/kernel:0', 'conv2d_33/bias:0', 'conv2d_34/kernel:0', 'conv2d_34/bias:0', 'conv2d_35/kernel:0', 'conv2d_35/bias:0', 'conv2d_36/kernel:0', 'conv2d_36/bias:0', 'conv2d_37/kernel:0', 'conv2d_37/bias:0', 'conv2d_38/kernel:0', 'conv2d_38/bias:0', 'conv2d_39/kernel:0', 'conv2d_39/bias:0'] when minimizing the loss.
x
	</description>
	<comments>
		<comment id='1' author='weiminson' date='2019-05-30T05:08:36Z'>
		&lt;denchmark-link:https://github.com/weiminson&gt;@weiminson&lt;/denchmark-link&gt;
 In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Also Could you provide more details about the issue and context?. Thanks!
		</comment>
		<comment id='2' author='weiminson' date='2019-05-30T05:29:40Z'>
		
@weiminson In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Also Could you provide more details about the issue and context?. Thanks!

&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 thanks for your reply.
I was trying to implement a network with the following structure:
&lt;denchmark-link:https://user-images.githubusercontent.com/41808551/58610418-bf034280-82ee-11e9-9a69-a90a45d4c248.png&gt;&lt;/denchmark-link&gt;

each of its decoder branches are connected to a decoder like this:
&lt;denchmark-link:https://user-images.githubusercontent.com/41808551/58610718-d7c02800-82ef-11e9-8524-ad94246eddf5.png&gt;&lt;/denchmark-link&gt;

However, when I try to update its gradient in a function called train_G using:
with tf.GradientTape(persistent=True) as t:
[TAB]        ground_truth = get_Mask(x_real)
[TAB]        x1_fake, x2_fake = G(ground_truth, training=True)
[TAB]        x1_fake_d_logit = D1(x1_fake, training=True)
[TAB]        x2_fake_d_logit = D2(x2_fake, training=True)
[TAB]        G1_loss = g_loss_fn(x1_fake_d_logit)
[TAB]        G2_loss = g_loss_fn(x2_fake_d_logit)
G2_grad = t.gradient(G2_loss, G.trainable_variables)
G_optimizer.apply_gradients(zip(G2_grad, G.trainable_variables))
the warning described above occurs, saying that no gradient will be obtained from the branches.
is there anything wrong with my code?
		</comment>
		<comment id='3' author='weiminson' date='2019-05-30T10:47:28Z'>
		&lt;denchmark-link:https://github.com/weiminson&gt;@weiminson&lt;/denchmark-link&gt;
 I tried to reproduce the issue but it looks code snippet is incomplete. Please provide complete code to. Thanks!
		</comment>
		<comment id='4' author='weiminson' date='2019-05-31T05:52:07Z'>
		
@weiminson I tried to reproduce the issue but it looks code snippet is incomplete. Please provide complete code to. Thanks!
@gadagashwini Thanks for your reply, to reproduce the issue, I've uploaded the entire coding to
https://github.com/weiminson/debug-SplitU
Plz run tain.py to test. for the running environment, plz refer to README.

		</comment>
		<comment id='5' author='weiminson' date='2019-06-04T07:04:36Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 Could you please help to try this case? I've been stuck on this issue for more than one week... Appreciate your reply.
		</comment>
		<comment id='6' author='weiminson' date='2019-07-11T19:53:25Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
: Is this autograph related? There is an "Unexpected error transforming" in the logs
		</comment>
		<comment id='7' author='weiminson' date='2019-07-11T20:15:17Z'>
		Yes, the warning is definitely related to autograph and indicates a bug, judging from the error message.
That said, I believe the gradient error is unrelated - the warning simply states that autograph will leave that code unchanged, and it's unlikely to cause that error.
So I think we're dealing with two separate issues here - it would be great if you had a code snippet (or a simplified model of it) that can reproduce the error.
		</comment>
		<comment id='8' author='weiminson' date='2019-07-11T20:17:06Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
: I believe the code has been uploaded to &lt;denchmark-link:https://github.com/weiminson/debug-SplitU&gt;https://github.com/weiminson/debug-SplitU&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='weiminson' date='2019-07-12T18:31:46Z'>
		The warning from optimizer is as expected.
&lt;denchmark-link:https://github.com/weiminson&gt;@weiminson&lt;/denchmark-link&gt;
 your code seems to be:
G2_grad = t.gradient(G2_loss, G.trainable_variables)
G_optimizer.apply_gradients(zip(G2_grad, G.trainable_variables))
G_optimizer is getting "all trainable variables" with G.trainable_variables, while you are providing gradients only for the ones on the G2 branch, hence it is warning you that it doesn't see any gradients for the variables in the G1 branch. The warning is to make sure you aren't doing this by mistake.
&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 the autograph warning seems unrelated, but may be worth checking if it needs to be there.
		</comment>
		<comment id='10' author='weiminson' date='2019-07-13T02:45:19Z'>
		
The warning from optimizer is as expected.
@weiminson your code seems to be:
G2_grad = t.gradient(G2_loss, G.trainable_variables)
G_optimizer.apply_gradients(zip(G2_grad, G.trainable_variables))
G_optimizer is getting "all trainable variables" with G.trainable_variables, while you are providing gradients only for the ones on the G2 branch, hence it is warning you that it doesn't see any gradients for the variables in the G1 branch. The warning is to make sure you aren't doing this by mistake.
@mdanatg the autograph warning seems unrelated, but may be worth checking if it needs to be there.

&lt;denchmark-link:https://github.com/rajatmonga&gt;@rajatmonga&lt;/denchmark-link&gt;
 , Thanks for your reply. Actually, I did try out a training mode where one of the branches was disabled and the other was left enabled (mode 2). Consequently, it was still showing the same error. Besides, I checked the name of the layers from that Error messages, finding that these layers are names of layers of BOTH branches, which implied that neither of the branches actually worked.
Correct me if I m wrong. BTW, do you know what this error message means? Why train G could not be transformed? I believe this is the root cause of the issue.
W0529 19:29:52.344057 140071302989632 tf_logging.py:161] Entity &lt;function train_G at 0x7f64c79a9ae8&gt; could not be transformed and will be staged without change.
		</comment>
		<comment id='11' author='weiminson' date='2019-07-13T13:40:43Z'>
		&lt;denchmark-link:https://github.com/weiminson&gt;@weiminson&lt;/denchmark-link&gt;
 I tried to run the code you uploaded to the github repo, but it fails with this error:
&lt;denchmark-code&gt;python3 train.py
Traceback (most recent call last):
  File "train.py", line 82, in &lt;module&gt;
    dataset, shape, len_dataset = data.make_PETCT_dataset(img_paths, args.batch_size, args.input_size)
  File "/home/mdan/debug-SplitU/data.py", line 95, in make_PETCT_dataset
    repeat=repeat)
  File "/home/mdan/debug-SplitU/tf2lib/data/dataset.py", line 127, in disk_image_batch_dataset
    repeat=repeat)
  File "/home/mdan/debug-SplitU/tf2lib/data/dataset.py", line 77, in memory_data_batch_dataset
    repeat=repeat)
  File "/home/mdan/debug-SplitU/tf2lib/data/dataset.py", line 32, in batch_dataset
    dataset = dataset.map(map_fn, num_parallel_calls=n_map_threads)
  File "/home/mdan/.local/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py", line 1146, in map
    self, map_func, num_parallel_calls, preserve_cardinality=True)
  File "/home/mdan/.local/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py", line 3264, in __init__
    use_legacy_function=use_legacy_function)
  File "/home/mdan/.local/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py", line 2591, in __init__
    self._function = wrapper_fn._get_concrete_function_internal()
  File "/home/mdan/.local/lib/python3.5/site-packages/tensorflow/python/eager/function.py", line 1366, in _get_concrete_function_internal
    *args, **kwargs)
  File "/home/mdan/.local/lib/python3.5/site-packages/tensorflow/python/eager/function.py", line 1360, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File "/home/mdan/.local/lib/python3.5/site-packages/tensorflow/python/eager/function.py", line 1648, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/home/mdan/.local/lib/python3.5/site-packages/tensorflow/python/eager/function.py", line 1541, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File "/home/mdan/.local/lib/python3.5/site-packages/tensorflow/python/framework/func_graph.py", line 716, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/home/mdan/.local/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py", line 2585, in wrapper_fn
    ret = _wrapper_helper(*args)
  File "/home/mdan/.local/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py", line 2530, in _wrapper_helper
    ret = func(*nested_args)
  File "/home/mdan/debug-SplitU/tf2lib/data/dataset.py", line 113, in map_fn_
    return map_fn(*parse_fn(*args))
  File "/home/mdan/debug-SplitU/tf2lib/data/dataset.py", line 107, in parse_fn
    img = tf.io.read_file(path)
  File "/home/mdan/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_io_ops.py", line 626, in read_file
    "ReadFile", filename=filename, name=name)
  File "/home/mdan/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py", line 550, in _apply_op_helper
    (prefix, dtypes.as_dtype(input_arg.type).name))
TypeError: Input 'filename' of 'ReadFile' Op has type float32 that does not match expected type of string.
&lt;/denchmark-code&gt;

It seems to require the PET &amp; CT dataset, but it's unclear how to install that.
At any rate, the two warnings are very unlikely to be related. To verify that, please change line 156 as follows:
&lt;denchmark-code&gt;@tf.function(autograph=False)
def train_G(x_real):
&lt;/denchmark-code&gt;

		</comment>
		<comment id='12' author='weiminson' date='2019-07-13T13:52:58Z'>
		&lt;denchmark-link:https://github.com/weiminson&gt;@weiminson&lt;/denchmark-link&gt;

I had a closer look at , and I believe the bug that caused the  warning is already fixed in the beta1 release:
pip3 install -U tensorflow==2.0.0-beta1
I think that will require upgrading tensorflow-addons, too:
pip3 install -U tensorflow-addons
Could you try running your code under these versions and check if any of the warnings remain?
		</comment>
		<comment id='13' author='weiminson' date='2019-07-13T15:17:06Z'>
		+1 with what &lt;denchmark-link:https://github.com/rajatmonga&gt;@rajatmonga&lt;/denchmark-link&gt;
 said. The warning from optimizer only suggests branch 2 is not included in the backprop path, and seems to be legit in your use case. If you want to avoid such warning, the simplest way is to change your code to use G.trainable_variables to tape.watched_variables so that it only updates branch 1.
The actual error seems to be coming from NodeTransformer:
assert node.ctx is not None, 'node {} has ctx unset'.format(node)
		</comment>
		<comment id='14' author='weiminson' date='2019-07-22T14:37:26Z'>
		Since we expect the warnings to clear in the beta or nightly builds, but we can't confirm from our side whether the warnings clear in the latest build, I will close this issue for now. Please re-open it if they still appear in the nightly build though. Thanks!
		</comment>
		<comment id='15' author='weiminson' date='2019-07-22T14:37:27Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29122&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29122&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='16' author='weiminson' date='2019-07-31T09:14:28Z'>
		
@weiminson
I had a closer look at train_G, and I believe the bug that caused the autograph warning is already fixed in the beta1 release:
pip3 install -U tensorflow==2.0.0-beta1
I think that will require upgrading tensorflow-addons, too:
pip3 install -U tensorflow-addons
Could you try running your code under these versions and check if any of the warnings remain?

&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 : I had a similar issue. The warnings disappeared after using the beta1 release.
Thanks.
		</comment>
	</comments>
</bug>