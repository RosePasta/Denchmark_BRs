<bug id='44560' author='houseofai' open_date='2020-11-03T19:09:37Z' closed_time='2020-11-05T22:17:03Z'>
	<summary>Custom LearningRateSchedule not called within a Mirrored Strategy</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
From the tutorial Custom training with tf.distribute.Strategy with a Custom LearningRateSchedule
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
TensorFlow version (use command below): 2.3.0
Python version: 3.6.9
GPU model and memory: GPU from Colab

Describe the current behavior
In a distributed environment, when adding a custom LearningRateSchedule to Adam optimizer to decay the learning rate over epochs, the learning rate is not decayed as expected.
I tested the same code in an environment without the distributed scope and the LR decay over the epochs.
Describe the expected behavior
The optimizer to call the LearningRateSchedule call method to decay the learning rate after applying the gradients

Please check this Colab from the TF tutorial: &lt;denchmark-link:https://colab.research.google.com/drive/1sT3MZ5hdGvsPhFQDrCFXzMR760HPCSsc?usp=sharing&gt;https://colab.research.google.com/drive/1sT3MZ5hdGvsPhFQDrCFXzMR760HPCSsc?usp=sharing&lt;/denchmark-link&gt;

See chapter: Training loop
Other info / logs
&lt;denchmark-code&gt;class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self):
      super(CustomSchedule, self).__init__()
      self.lr = 0.01

    def __call__(self, step):
      self.lr = self.lr/10
      print("NEW LR:", self.lr)
      return self.lr

with strategy.scope():
    model = create_model()
    optimizer = tf.keras.optimizers.Adam(learning_rate=CustomSchedule())
    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)
&lt;/denchmark-code&gt;

The learning_rate=CustomSchedule() is the only line I changed from the tutorial.
See the Colab for the train_step
	</description>
	<comments>
		<comment id='1' author='houseofai' date='2020-11-04T19:02:49Z'>
		&lt;denchmark-link:https://github.com/houseofai&gt;@houseofai&lt;/denchmark-link&gt;

Please share simple stand alone code to replicate the issue or if possible share a colab gist with the error reported.
		</comment>
		<comment id='2' author='houseofai' date='2020-11-05T22:17:03Z'>
		After a deeper analysis, I was able to see that the scheduler was called
		</comment>
		<comment id='3' author='houseofai' date='2020-11-05T22:17:05Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44560&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44560&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>