<bug id='25564' author='SungchulLee' open_date='2019-02-07T01:07:54Z' closed_time='2019-02-15T05:13:52Z'>
	<summary>tf.keras.initializers.glorot_normal is not consistent with its documentation</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
TensorFlow installed from (source or binary): Anaconda
TensorFlow version (use command below): 1.12.0
Python version: 3.5.0
Bazel version (if compiling from source): NA
GCC/Compiler version (if compiling from source): NA
CUDA/cuDNN version: NA
GPU model and memory: NA

You can collect some of this information using our environment capture &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"

reference: &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/initializers/glorot_normal&gt;https://www.tensorflow.org/api_docs/python/tf/keras/initializers/glorot_normal&lt;/denchmark-link&gt;

According to reference, it draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.
I generated them with fan_in = 100, fan_out = 100 and ploted their histogram. I printed out the smallest and largest from the sampe and I got -0.22728574 0.22735965. These should be 2!
Describe the expected behavior
I generated them with fan_in = 100, fan_out = 100 and ploted their histogram. I printed out the smallest and largest from the sampe and I got -0.22728574 0.22735965. These should be 2!
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
fan_in = 100
fan_out = 100
tf.random.set_random_seed(337)
W = tf.get_variable("W", 
shape=(fan_in, fan_out), 
initializer=tf.keras.initializers.glorot_normal)
sigma = np.sqrt(2 / (fan_in + fan_out))
with tf.Session() as sess:
tf.global_variables_initializer().run()
&lt;denchmark-code&gt;W_now = sess.run(W)

plt.hist(W_now.reshape((-1,)), bins=100)
plt.show()

print(2*sigma)
print(np.min(W_now.reshape((-1,))), np.max(W_now.reshape((-1,))))
&lt;/denchmark-code&gt;

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='SungchulLee' date='2019-02-07T02:23:01Z'>
		Hi, I am confused about what do you mean "2". If fan_in = 100, fan_out = 100, then stddev = sqrt(2 / (fan_in + fan_out)) = 0.1. So the samples are drawn from N(0.0, 0.1).
		</comment>
		<comment id='2' author='SungchulLee' date='2019-02-07T03:23:22Z'>
		If fan_in = 100, fan_out = 100, then stddev = sqrt(2 / (fan_in + fan_out)) = 0.1. So the samples are drawn from truncated_N(0.0, 0.1), not N(0.0, 0.1). See below ref2. If samples are from truncated_N(0.0, sigma) with sigma=0.1, then they are in [-2sigma,2sigma], i.e., [-0.2,0.2]. Oh, I mean 0.2, not 2. Sorry about the confusion.
Correction:
I generated them with fan_in = 100, fan_out = 100 and ploted their histogram. I printed out the smallest and largest from the sampe and I got -0.22728574 0.22735965. These two numbers should be approximately -0.2 and 0.2!
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;ref1: https://www.tensorflow.org/api_docs/python/tf/keras/initializers/glorot_normal
It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;ref2: https://www.tensorflow.org/api_docs/python/tf/random/truncated_normal
The generated values follow a normal distribution with specified mean and standard deviation, except that values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked.&lt;/denchmark-h&gt;

		</comment>
		<comment id='3' author='SungchulLee' date='2019-02-07T10:15:02Z'>
		The documentation is correct: it is a truncated normal distribution with that standard deviation. If it were what you expected, the standard deviation of the distribution would be lower (because of the truncated part). Eg. you can run:
&lt;denchmark-code&gt;&gt;&gt;&gt; from scipy import stats
&gt;&gt;&gt; stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)
0.8796256610342398
&lt;/denchmark-code&gt;

As you can see, the stddev of a truncated normal distribution (where the non-truncated normal distribution is a N(0, 1)) is lower than 1.0. So if you want a truncated normal distribution with stddev = 1, you need to use the corresponding non-truncated N(0, 1 / 0.8796256610342398). So you can get values higher than -2 and 2.
		</comment>
		<comment id='4' author='SungchulLee' date='2019-02-07T14:09:10Z'>
		Thank &lt;denchmark-link:https://github.com/mgaido91&gt;@mgaido91&lt;/denchmark-link&gt;
 for your detailed explanation. I'd like to add some details below:
glorot_normal is inherited from &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/initializers/VarianceScaling&gt;VarianceScaling&lt;/denchmark-link&gt;
. Its API explains:

With distribution="truncated_normal" or "untruncated_normal", samples are drawn from a truncated/untruncated normal distribution with a mean of zero and a standard deviation (after truncation, if used) stddev = sqrt(scale / n) where n is: - number of input units in the weight tensor, if mode = "fan_in" - number of output units, if mode = "fan_out" - average of the numbers of input and output units, if mode = "fan_avg"




tensorflow/tensorflow/python/ops/init_ops.py


        Lines 474 to 478
      in
      a6d8ffa






 if self.distribution == "normal" or self.distribution == "truncated_normal": 



 # constant taken from scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.) 



 stddev = math.sqrt(scale) / .87962566103423978 



 return random_ops.truncated_normal( 



 shape, 0.0, stddev, dtype, seed=self.seed) 





Hence in your example, the real stddev is corrected as 0.1 / 0.87962566103423978 = 0.11368472343385565, so the samples are expected in 2 * [-stddev, stddev] = [-0.2273694468677113, 0.2273694468677113], rather than [-0.2, 0.2].
Perhaps we need to also emphasize that stddev is used after truncation in the glorot_normal API ?
		</comment>
		<comment id='5' author='SungchulLee' date='2019-02-07T15:36:19Z'>
		&lt;denchmark-link:https://github.com/facaiy&gt;@facaiy&lt;/denchmark-link&gt;
 despite I think that a note to clarify that would be good, I haven't submitted a PR because it would mean that we have to make the same clarifications also in all the other initializers using a truncated normal distribution (eg. ) and I am not sure this approach is easy to maintain; moreover the description is accurate (despite the misunderstanding is easy since truncated normal distributions are not so widespread).
		</comment>
		<comment id='6' author='SungchulLee' date='2019-02-07T17:42:30Z'>
		Mant thanks &lt;denchmark-link:https://github.com/mgaido91&gt;@mgaido91&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/facaiy&gt;@facaiy&lt;/denchmark-link&gt;
. I understand what happens. However, documentations are still inconsistent or at least misleading. I think documentations should be updated. I have a simple suggestion below.
ref1: &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/initializers/glorot_normal&gt;https://www.tensorflow.org/api_docs/python/tf/keras/initializers/glorot_normal&lt;/denchmark-link&gt;

It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor. ---- This part is not consistent or at least misleading with definition of truncated normal in ref2. According to ref2 the key word stddev of tf.truncated_normal_initializer is refering the stddev of the original normal distribution, not the standard deviation of the truncated normal distribution. A simple fix I can think of: a truncated normal distribution centered on 0 with stddev = sqrt(2 / (fan_in + fan_out))  ------&gt; a truncated normal distribution centered on 0 whose standard deviation, not stddev of underling normal, is sqrt(2 / (fan_in + fan_out))
ref2: &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/random/truncated_normal&gt;https://www.tensorflow.org/api_docs/python/tf/random/truncated_normal&lt;/denchmark-link&gt;

The generated values follow a normal distribution with specified mean and standard deviation, except that values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked. ---- This part is ok with my simple simulation code using tf.truncated_normal_initializer:
I generate many samples from tf.truncated_normal_initializer(stddev=sigma) where sigma=0.1. In the code, print(-2sigma, 2sigma) produces -0.2 0.2 and print(np.min(W_now.reshape((-1,))), np.max(W_now.reshape((-1,)))) produces -0.19992638 0.19999139 and these two numbers are very close to -0.2 and 0.2. So, as ref2 said the generated values are from a normal distribution with specified mean and stddev, not stddev / .87962566103423978, except that values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked.
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
fan_in = 100
fan_out = 100
sigma = np.sqrt(2 / (fan_in + fan_out))
tf.random.set_random_seed(337)
W = tf.get_variable("W", 
shape=(fan_in, fan_out), 
initializer=tf.truncated_normal_initializer(stddev=sigma))
with tf.Session() as sess:
tf.global_variables_initializer().run()
&lt;denchmark-code&gt;W_now = sess.run(W)

plt.hist(W_now.reshape((-1,)), bins=100)
plt.show()

print(-2*sigma, 2*sigma)
print(np.min(W_now.reshape((-1,))), np.max(W_now.reshape((-1,))))
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='SungchulLee' date='2019-02-08T09:20:46Z'>
		&lt;denchmark-link:https://github.com/SungchulLee&gt;@SungchulLee&lt;/denchmark-link&gt;
 I still think that the documentation is correct and not inconsistent.
ref1 says: "It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / (fan_in + fan_out)) ..." which is true.
ref2 says: "The generated values follow a normal distribution with specified mean and standard deviation..." which is true again.
You are saying that the documentation is inconsistent because you are saying that these 2 methods are equivalent, but this is a connection you are making and it is not done by the documentation.
What I think could be done, which may help avoiding misunderstandings like this, IMHO, is to add a sentence in ref2 like: "The returned values, hence, follow a truncated normal distribution centered in mean with a stddev = stddev / 0.8.....". So pointing out that the stddev of a truncated normal distribution is different from the stddev of the original normal distribution it is taken from.
Do you think this may help?
		</comment>
		<comment id='8' author='SungchulLee' date='2019-02-08T14:59:04Z'>
		&lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ppwwyyxx&gt;@ppwwyyxx&lt;/denchmark-link&gt;
 can anyone shed some light on the discussion?
		</comment>
		<comment id='9' author='SungchulLee' date='2019-02-08T17:09:43Z'>
		&lt;denchmark-link:https://github.com/mgaido91&gt;@mgaido91&lt;/denchmark-link&gt;
 My point is that:
The key word stddev in the definition of tf.truncated_normal_initializer in ref2 is the standard deviation of the underlying normal distribution, not  the standard deviation of the truncated normal distribution itself (I did a simple experiment with above python code and ckecked this fact. &lt;denchmark-link:https://github.com/mgaido91&gt;@mgaido91&lt;/denchmark-link&gt;
 misunderstood this part) whereas
the key word stddev of tf.truncated_normal_initializer in the definition of tf.keras.initializers.glorot_normal of ref1 is, as &lt;denchmark-link:https://github.com/mgaido91&gt;@mgaido91&lt;/denchmark-link&gt;
 pointed out, the standard deviation of the truncated normal distribution itself.
That is why I was misunderstood initially.
		</comment>
		<comment id='10' author='SungchulLee' date='2019-02-08T17:22:34Z'>
		&lt;denchmark-link:https://github.com/SungchulLee&gt;@SungchulLee&lt;/denchmark-link&gt;
 I understood what you mean and why you got confused. The point is that in ref1 the documentation is talking about the stddev of the truncated normal distribution (I think it is quite clear from the snippet I copy-pasted above); while in ref2 it is referring to the stddev of the underlying normal distribution (again, I think it is clear from the snippet copied from there). I think the documentation states this, while it could be emphasized more in order to avoid confusions.
Maybe you mean that in the second case we should rename the parameter to underlying_stddev or something similar? That seems an overkill to me.
		</comment>
		<comment id='11' author='SungchulLee' date='2019-02-08T18:08:20Z'>
		The documentation looks correct to me as well. To make sure that such a confusion does not arise again, maybe we can edit the Returns part of ref2 to say. "A tensor of the specified shape filled with random values from a truncated normal distribution with trunc_mean = mean and trunc_stddev = stddev * 0.87962566103423978".
		</comment>
		<comment id='12' author='SungchulLee' date='2019-02-08T18:25:41Z'>
		How about changing ref1 as follows:
ref1: &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/initializers/glorot_normal&gt;https://www.tensorflow.org/api_docs/python/tf/keras/initializers/glorot_normal&lt;/denchmark-link&gt;

It draws samples from a truncated normal distribution centered on 0 with  sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.
This change is ok with me as long as stddev is not used since the stddev of tf.truncated_normal_initializer in ref2 or random_ops.truncated_normal below means the standard deviation of the underlying normal distribution, not the standard deviation of the truncated normal distribution itself.
&lt;denchmark-code&gt; if self.distribution == "normal" or self.distribution == "truncated_normal": 
   # constant taken from scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.) 
   stddev = math.sqrt(scale) / .87962566103423978 
   return random_ops.truncated_normal( 
       shape, 0.0, stddev, dtype, seed=self.seed)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='13' author='SungchulLee' date='2019-02-08T18:57:47Z'>
		&lt;denchmark-link:https://github.com/SungchulLee&gt;@SungchulLee&lt;/denchmark-link&gt;
, let's see what others think and we can change it the way you say, if more people agree to it. However, we have to keep in mind that if we are making any changes to ref1, then we should also make similar changes in the documentation of other initializers using a truncated normal distribution as well. On the other hand, changing the  part of ref2 would allow us to skip this extra work and would serve our purpose as well.
		</comment>
		<comment id='14' author='SungchulLee' date='2019-02-08T19:16:53Z'>
		@ Sudeepam97 I think you misunderstood the situation in two ways:


tf.random.truncated_normal is more fundamental than tf.keras.initializers.glorot_normal. tf.keras.initializers.glorot_normal depends on tf.random.truncated_normal.


In tf.random.truncated_normal, stddev means the standard deviation of the underlying normal distribution, not the standard deviation of the truncated normal distribution itself.


		</comment>
		<comment id='15' author='SungchulLee' date='2019-02-08T19:45:10Z'>
		&lt;denchmark-link:https://github.com/SungchulLee&gt;@SungchulLee&lt;/denchmark-link&gt;
, I'm confused. I'm not sure why you think that I misunderstand.
I agree with these two points and this is what I had been thinking. Am I wrong? If so, then can you please explain the actual situation to me? If I am not wrong then can you please point out why you thought that I think otherwise? I sense some miscommunication.
		</comment>
		<comment id='16' author='SungchulLee' date='2019-02-08T20:20:24Z'>
		In ref1, stddev refers to the standard deviation of truncated normal. There are other initializers as well (like lecun_normal) where stddev refers to the standard deviation of truncated normal. Going by your way, we should edit stddev to standard deviation for all of these initializers.
Now, all of these initializers depend on tf.random.truncated_normal and in it, stddev refers to the standard deviation of underlying normal, and this is why, standard deviation of truncated normal is divided by 0.8... for adjustment.
Now, If we mention, &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/random/truncated_normal#returns&gt;right here&lt;/denchmark-link&gt;
, in , that it...
"returns a tensor of the specified shape filled with random values from a truncated normal distribution with  and ".
We essentially convey that an initializer like tf.keras.initializers.glorot_normal draws samples from a truncated normal distribution centred on 0 with truncated normal standard deviation = underlying normal standard deviation * 0.87962566103423978. This is what I meant.
		</comment>
		<comment id='17' author='SungchulLee' date='2019-02-08T21:02:29Z'>
		&lt;denchmark-link:https://github.com/Sudeepam97&gt;@Sudeepam97&lt;/denchmark-link&gt;
 I think we are on the same page. Good.
I don't think it is wise to change any key word. What I propose is to change documentation instead.
Original:
ref1: &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/initializers/glorot_normal&gt;https://www.tensorflow.org/api_docs/python/tf/keras/initializers/glorot_normal&lt;/denchmark-link&gt;

It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.
Proposal:
ref1: &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/initializers/glorot_normal&gt;https://www.tensorflow.org/api_docs/python/tf/keras/initializers/glorot_normal&lt;/denchmark-link&gt;

It draws samples from a truncated normal distribution centered on 0 with standard deviation sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.
With proposed documentation I would figure out what's going on without unnecessary confusing.
		</comment>
		<comment id='18' author='SungchulLee' date='2019-02-08T21:51:00Z'>
		
@Sudeepam97 I think we are on the same page. Good.
I don't think it is wise to change any key word. What I propose is to change documentation instead.

Can you clear me up on what exactly do you mean by 'key word'? Even I propose to change the documentation, but I suppose that we should change &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/random/truncated_normal#returns&gt;this&lt;/denchmark-link&gt;
 section of ref2 to say, (I'll re-phrase) "returns a tensor of the specified shape, filled with random values from a truncated normal distribution with the mean equal to  and standard deviation equal to .
But we can go by your method as well, which is to change ref1 as follows...

Original:
ref1: https://www.tensorflow.org/api_docs/python/tf/keras/initializers/glorot_normal
It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.
Proposal:
ref1: https://www.tensorflow.org/api_docs/python/tf/keras/initializers/glorot_normal
It draws samples from a truncated normal distribution centered on 0 with standard deviation sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.
With proposed documentation I would figure out what's going on without unnecessary confusing.

Now that a few solutions have been discussed, let's see what the others think. A PR can be created accordingly.
		</comment>
		<comment id='19' author='SungchulLee' date='2019-02-08T22:37:58Z'>
		&lt;denchmark-link:https://github.com/Sudeepam97&gt;@Sudeepam97&lt;/denchmark-link&gt;
 I am new to github (this is my first writing in github), python (2 years in a computer language), and tensorflow (so, you can guess) and so my wording may be incorrect. Even, I don't know what is PR.
According to my shallow understanding of python language
&lt;denchmark-code&gt;tf.random.truncated_normal(
    shape, # &lt;----- positional argument
    mean=0.0, # &lt;----- keyword argument = default argument
    stddev=1.0, # &lt;----- keyword argument = default argument
    dtype=tf.float32, # &lt;----- keyword argument = default argument
    seed=None, # &lt;----- keyword argument = default argument
    name=None  # &lt;----- keyword argument = default argument
)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='20' author='SungchulLee' date='2019-02-09T00:49:18Z'>
		Don't worry, PR is short for pull request :-)
Thank everyone for your useful suggestion! Let's make it simple and keep consistent with VarianceScaling, I'm fine with:
It draws samples from a truncated normal distribution centered on 0 with standard deviation (after truncation)  stddev = sqrt(2 / (fan_in + fan_out))
Welcome to create a PR if anyone is interested in it. Please ping me,then I'll review and approve.
		</comment>
		<comment id='21' author='SungchulLee' date='2019-02-09T01:28:34Z'>
		&lt;denchmark-link:https://github.com/facaiy&gt;@facaiy&lt;/denchmark-link&gt;
  I am fine with your proposal.
		</comment>
		<comment id='22' author='SungchulLee' date='2019-02-09T08:25:34Z'>
		&lt;denchmark-link:https://github.com/facaiy&gt;@facaiy&lt;/denchmark-link&gt;
 I'll be happy to create a PR. I'll submit one by today. Should I edit the documentation for all the initializers that depend on  or should I edit  only?
		</comment>
		<comment id='23' author='SungchulLee' date='2019-02-09T10:46:05Z'>
		Thank you, Sudeepam. How about only glorot_normal at first?
		</comment>
		<comment id='24' author='SungchulLee' date='2019-02-09T10:51:08Z'>
		Alright, I'll start right away.
		</comment>
		<comment id='25' author='SungchulLee' date='2019-02-15T20:21:29Z'>
		Are you satisfied with the resolution of your issue? &lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=25564&gt;Yes&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=25564&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>