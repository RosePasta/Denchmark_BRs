<bug id='8504' author='zuoxingdong' open_date='2017-03-17T16:26:28Z' closed_time='2017-03-22T07:20:12Z'>
	<summary>Cannot reuse variables by tf.layers.conv2d</summary>
	<description>
I am trying to make two conv layers share the same weights, however, it seems the API does not work.
&lt;denchmark-code&gt;import tensorflow as tf

x = tf.random_normal(shape=[10, 32, 32, 3])

conv1 = tf.layers.conv2d(x, 3, [2, 2], padding='SAME', reuse=None, name='conv')
print(conv1.name)
conv2 = tf.layers.conv2d(x, 3, [2, 2], padding='SAME', reuse=True, name='conv')
print(conv2.name)
&lt;/denchmark-code&gt;

gives
&lt;denchmark-code&gt;conv/BiasAdd:0
conv_2/BiasAdd:0
&lt;/denchmark-code&gt;

--------------------------------Update-------------------------------
According to a &lt;denchmark-link:https://stackoverflow.com/questions/42862300/tensorflow-reuse-variable-with-tf-layers-conv2d&gt;post&lt;/denchmark-link&gt;
, the weights are already sharing, with different layer names, since the computation not sharing. However, is it feasible to consider a better naming strategy so that it is easier to see from names that different layers are sharing the same weights ?
	</description>
	<comments>
		<comment id='1' author='zuoxingdong' date='2017-03-22T07:52:18Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 Thanks for your PR. I have a short question
&lt;denchmark-code&gt;with tf.variable_scope('new_scope'):
    ker = tf.get_variable('kernel', [5, 6])
    fc1 = tf.layers.dense(inputs=tf.constant(np.random.randn(3, 4, 5)), units=3, reuse=None)
    fc2 = tf.layers.dense(inputs=tf.constant(np.random.randn(3, 4, 5)), units=3, reuse=True)
&lt;/denchmark-code&gt;

With the new behavior, will fc2 reuse the weights from fc1 or from ker ?
Because previously the weight sharing should be
&lt;denchmark-code&gt;with tf.variable_scope('new_scope'):
    fc1 = tf.layers.dense(inputs=tf.constant(np.random.randn(3, 4, 5)), units=3, reuse=None)
    fc2 = tf.layers.dense(inputs=tf.constant(np.random.randn(3, 4, 5)), units=3, reuse=True)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='zuoxingdong' date='2017-03-22T16:45:18Z'>
		I accidentally introduced a bug.  It *should* reuse the weights from fc1,
but i accidentally made it reuse the weights from ker.  i'm sending an
update to reverse this behavior.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Wed, Mar 22, 2017 at 12:52 AM, Xingdong Zuo ***@***.***&gt; wrote:
 @ebrevdo &lt;https://github.com/ebrevdo&gt; Thanks for your PR. I have a short
 question

 with tf.variable_scope('new_scope'):
     ker = tf.get_variable('kernel', [5, 6])
     fc1 = tf.layers.dense(inputs=tf.constant(np.random.randn(3, 4, 5)), units=3, **reuse=None**)
     fc2 = tf.layers.dense(inputs=tf.constant(np.random.randn(3, 4, 5)), units=3, **reuse=True**)

 With the new behavior, will fc2 reuse the weights from fc1 or from ker ?
 Because previously the weight sharing should be

 with tf.variable_scope('new_scope'):
     fc1 = tf.layers.dense(inputs=tf.constant(np.random.randn(3, 4, 5)), units=3, **reuse=None**)
     fc2 = tf.layers.dense(inputs=tf.constant(np.random.randn(3, 4, 5)), units=3, **reuse=True**)

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#8504 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtim9CC9jizTqiQkcxXS_47Cao8h9IIks5roNNGgaJpZM4MgzyA&gt;
 .



		</comment>
		<comment id='3' author='zuoxingdong' date='2017-03-23T21:34:44Z'>
		Fix should be in.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Wed, Mar 22, 2017 at 9:44 AM, Eugene Brevdo ***@***.***&gt; wrote:
 I accidentally introduced a bug.  It *should* reuse the weights from fc1,
 but i accidentally made it reuse the weights from ker.  i'm sending an
 update to reverse this behavior.

 On Wed, Mar 22, 2017 at 12:52 AM, Xingdong Zuo ***@***.***&gt;
 wrote:

&gt; @ebrevdo &lt;https://github.com/ebrevdo&gt; Thanks for your PR. I have a short
&gt; question
&gt;
&gt; with tf.variable_scope('new_scope'):
&gt;     ker = tf.get_variable('kernel', [5, 6])
&gt;     fc1 = tf.layers.dense(inputs=tf.constant(np.random.randn(3, 4, 5)), units=3, **reuse=None**)
&gt;     fc2 = tf.layers.dense(inputs=tf.constant(np.random.randn(3, 4, 5)), units=3, **reuse=True**)
&gt;
&gt; With the new behavior, will fc2 reuse the weights from fc1 or from ker ?
&gt; Because previously the weight sharing should be
&gt;
&gt; with tf.variable_scope('new_scope'):
&gt;     fc1 = tf.layers.dense(inputs=tf.constant(np.random.randn(3, 4, 5)), units=3, **reuse=None**)
&gt;     fc2 = tf.layers.dense(inputs=tf.constant(np.random.randn(3, 4, 5)), units=3, **reuse=True**)
&gt;
&gt; —
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly, view it on GitHub
&gt; &lt;#8504 (comment)&gt;,
&gt; or mute the thread
&gt; &lt;https://github.com/notifications/unsubscribe-auth/ABtim9CC9jizTqiQkcxXS_47Cao8h9IIks5roNNGgaJpZM4MgzyA&gt;
&gt; .
&gt;




		</comment>
	</comments>
</bug>