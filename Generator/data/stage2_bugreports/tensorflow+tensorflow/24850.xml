<bug id='24850' author='ernestum' open_date='2019-01-11T09:37:20Z' closed_time='2019-01-11T18:50:02Z'>
	<summary>Too many LSTM implementations (6)</summary>
	<description>
System information

TensorFlow version: v1.8
Doc Link: tf.contrib.rnn.LSTMCell, tf.contrib.rnn.LSTMBlockCell, tf.contrib.rnn.LSTMBlockFusedCell, tf.nn.rnn_cell.LSTMCell, tf.keras.layers.LSTM, tf.contrib.cudnn_rnn.CudnnLSTM, tf.contrib.cudnn_rnn.CudnnLSTMSaveable

Ignoring any fancy versions like Grid LSTM cells and LSTM cells somehow combined with convolutions there are 6! different LSTM implementations in tensorflow (see documentation links above). Some are better documented than others, some claim to be faster than others (there seems to be the order  tf.contrib.rnn.LSTMCell &lt; tf.contrib.rnn.LSTMBlockCell &lt; tf.contrib.rnn.LSTMBlockFusedCell but then why keep the slower implementations instead of just replacing the slower ones with faster implementations?), all have slightly different APIs.
What I am missing is a guide that tells me:

What is the fastest implementation on what hardware (I might be willing to deal with poor documentation if the result is at least fast, but having to deal with poor documentation just to find out that the performance is bad sucks).
What is the stable implementation which I can expect to be maintained for a longer time period.
What implementations are just included for historic reasons or to maintain backwards compatibility (and therefor should be avoided when starting a new project).
Maybe there are good reasons (that I am not aware of) to keep multiple implementations in parallel because they all have different tradeoffs. In this case I would like to know more what the differences are between the implementations. This can just be a table with pros and cons.

The state of affairs is probably similar for GRU cells and simple RNN cells. The situation with the LSTM cells is just exemplary ...
	</description>
	<comments>
		<comment id='1' author='ernestum' date='2019-01-11T10:27:06Z'>
		i found the code of tensorflow full of history, complex and vague
		</comment>
		<comment id='2' author='ernestum' date='2019-01-11T11:04:09Z'>
		Seems like the issue is considered by the roadmap for v2.0 (read it here in &lt;denchmark-link:https://www.tensorflow.org/community/roadmap&gt;short&lt;/denchmark-link&gt;
 or &lt;denchmark-link:https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/bgug1G6a89A&gt;long&lt;/denchmark-link&gt;
).
		</comment>
		<comment id='3' author='ernestum' date='2019-01-11T18:49:48Z'>
		Thanks for sharing the roadmap links @erniejunior. We can expect better structured tf.contrib module after TF 2.0 official release.
		</comment>
	</comments>
</bug>