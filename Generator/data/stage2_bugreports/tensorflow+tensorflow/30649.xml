<bug id='30649' author='CasiaFan' open_date='2019-07-12T13:21:12Z' closed_time='2020-06-09T08:45:23Z'>
	<summary>tf.distribute.MirroredStrategy incompatible with tf.estimator training when defining tf.train.Scaffold with saver</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information:

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.14.0
Python version: 3.7.3
CUDA/cuDNN version: 10.0/7.1
GPU model and memory: TitanXp 12G x 4


When I use  together with  for single worker multiple GPUs training, I meet the following error if I try to define  for  to configure the saver parameters. Everything works fine JUST I remove the scafflold and the multiple gpu training for estimator is referred this &lt;denchmark-link:https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/guide/distribute_strategy.ipynb#scrollTo=_098zB3vVhuV&gt;tutorial&lt;/denchmark-link&gt;
.
&lt;denchmark-code&gt;...
File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py", line 126, in _require_cross_replica_or_default_context_extended
    raise RuntimeError("Method requires being in cross-replica context, use "
RuntimeError: Method requires being in cross-replica context, use get_replica_context().merge_call()
&lt;/denchmark-code&gt;

Code to reproduce the issue
Here is my minimum snippet of code to reproduce this error.
import tensorflow as tf 
from tensorflow.python.keras.applications import MobileNetV2

l = tf.keras.layers

def input_fn(): 
    dataset = tf.data.Dataset.from_tensor_slices({"feature": tf.random_normal(shape=(1, 224, 224, 3), dtype=tf.float32),
                                                  "label": tf.random.uniform(shape=[1], minval=0, maxval=2, dtype=tf.int32)})
    dataset = dataset.repeat()
    dataset = dataset.batch(2)
    return dataset

def model_fn(features, labels, mode):
    input_tensor = features['feature']
    label = features['label']
    if mode == tf.estimator.ModeKeys.TRAIN:
        model = MobileNetV2(input_shape=(224, 224, 3), classes=2, weights=None)
        output = model(input_tensor)

        loss = tf.losses.sparse_softmax_cross_entropy(label, output)
        train_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss, global_step=tf.train.get_global_step())
        # define scaffold
        saver = tf.train.Saver(
            sharded=True,
            keep_checkpoint_every_n_hours=1,
            save_relative_paths=True)
        tf.add_to_collection(tf.GraphKeys.SAVERS, saver)
        scaffold = tf.train.Scaffold(saver=saver)
        # remove scaffold this code could work
        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op, scaffold=scaffold)

# multiple gpu configuration for estimator    
devices = ["/device:GPU:0", "/device:GPU:1"]
strategy = tf.distribute.MirroredStrategy(devices=devices)

config = tf.estimator.RunConfig(model_dir="test_multi_gpu",)
                                train_distribute=strategy)
estimator = tf.estimator.Estimator(model_fn=model_fn, config=config)
estimator.train(input_fn=input_fn, steps=1000)
Other info / logs
The total error log is here:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "test_multi_gpus.py", line 45, in &lt;module&gt;
    estimator.train(input_fn=input_fn, steps=1000)
  File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 367, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 1156, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 1219, in _train_model_distributed
    self._config._train_distribute, input_fn, hooks, saving_listeners)
  File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 1299, in _actual_train_model_distributed
    self.config))
  File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py", line 1555, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py", line 693, in _call_for_each_replica
    fn, args, kwargs)
  File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py", line 195, in _call_for_each_replica
    coord.join(threads)
  File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/six.py", line 693, in reraise
    raise value
  File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py", line 297, in stop_on_exception
    yield
  File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py", line 911, in run
    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
  File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 1146, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File "test_multi_gpus.py", line 34, in model_fn
    save_relative_paths=True)
  File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/saver.py", line 825, in __init__
    self.build()
  File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/saver.py", line 837, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/saver.py", line 875, in _build
    build_restore=build_restore)
  File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/saver.py", line 497, in _build_internal
    per_device = self._GroupByDevices(saveables)
  File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/saver.py", line 404, in _GroupByDevices
    pydev.canonical_name(spec.tensor.device) for spec in saveable.specs)
  File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/saver.py", line 404, in &lt;genexpr&gt;
    pydev.canonical_name(spec.tensor.device) for spec in saveable.specs)
  File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/saving/saveable_object.py", line 52, in tensor
    return self._tensor() if callable(self._tensor) else self._tensor
  File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/values.py", line 1358, in tensor
    return strategy.extended.read_var(sync_on_read_variable)
  File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py", line 768, in read_var
    return replica_local_var._get_cross_replica()  # pylint: disable=protected-access
  File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/values.py", line 1424, in _get_cross_replica
    axis=None)
  File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py", line 832, in reduce
    return super(StrategyV1, self).reduce(reduce_op, value, axis)
  File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py", line 552, in reduce
    _require_cross_replica_or_default_context_extended(self._extended)
  File "/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py", line 126, in _require_cross_replica_or_default_context_extended
    raise RuntimeError("Method requires being in cross-replica context, use "
RuntimeError: Method requires being in cross-replica context, use get_replica_context().merge_call()
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='CasiaFan' date='2019-07-15T09:27:43Z'>
		This error seems to be due to a redundant initialization of Saver when using MirroredStrategy for multi-gpu training since TF estimator would initialize Saver based on its RunConfig. In this case, commenting saver inside the EstimatorSpec could solve this problem. BUT if anyone has some ideas about the detailed mechanism of this conflict.
		</comment>
		<comment id='2' author='CasiaFan' date='2020-03-22T15:49:06Z'>
		I also meet this problem, have you had any idea of solving it? &lt;denchmark-link:https://github.com/CasiaFan&gt;@CasiaFan&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='CasiaFan' date='2020-03-28T07:20:55Z'>
		I fix this bug when training mobilenetv2_ssd in tensorflow object detection API with TF1.15, by add inplace_batchnorm_update: true in pipeline.config. I hope this can help anyone else.
		</comment>
		<comment id='4' author='CasiaFan' date='2020-05-13T09:20:43Z'>
		
This error seems to be due to a redundant initialization of Saver when using MirroredStrategy for multi-gpu training since TF estimator would initialize Saver based on its RunConfig. In this case, commenting saver inside the EstimatorSpec could solve this problem. BUT if anyone has some ideas about the detailed mechanism of this conflict.

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

hahaha kaobi
I also meet it
remove scaffold and saver in EstimatorSpec can fix it
it is caused by keras + estimator haha
		</comment>
		<comment id='5' author='CasiaFan' date='2020-05-29T02:32:06Z'>
		This &lt;denchmark-link:https://github.com/tensorflow/models/issues/5421#issuecomment-510821659&gt;thread&lt;/denchmark-link&gt;
 may help. I'm not sure whether you are meeting this error in tf models repo. &lt;denchmark-link:https://github.com/ZhuLingfeng1993&gt;@ZhuLingfeng1993&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='CasiaFan' date='2020-06-05T08:10:05Z'>
		"What's supported now?
In TF 2.0 release, there is limited support for training with Estimator using all strategies except TPUStrategy. Basic training and evaluation should work, but a number of advanced features such as scaffold do not yet work. There may also be a number of bugs in this integration."
Qutoed from link below:
&lt;denchmark-link:https://www.tensorflow.org/guide/distributed_training#whats_supported_now_3&gt;https://www.tensorflow.org/guide/distributed_training#whats_supported_now_3&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/CasiaFan&gt;@CasiaFan&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='CasiaFan' date='2020-06-09T08:44:33Z'>
		&lt;denchmark-link:https://github.com/wf-hit&gt;@wf-hit&lt;/denchmark-link&gt;
 Yep, maybe it's time to turn to the Keras type.
		</comment>
		<comment id='8' author='CasiaFan' date='2020-06-09T08:45:25Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30649&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30649&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>