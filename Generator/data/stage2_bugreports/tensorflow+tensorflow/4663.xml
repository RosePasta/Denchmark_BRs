<bug id='4663' author='eamartin' open_date='2016-09-29T23:54:40Z' closed_time='2017-06-16T22:48:54Z'>
	<summary>Control dependency on identity containing assign not working</summary>
	<description>
I'm running Tensorflow 0.10.
The following code
import tensorflow as tf

x = tf.Variable(0, dtype=tf.int32)

old_val = tf.identity(x)
with tf.control_dependencies([old_val]):
    new_val = tf.assign(x, x + 1)

with tf.Session() as sess:
    sess.run(tf.initialize_all_variables())

    for i in xrange(3):
        print sess.run([old_val, new_val, x])
outputs
&lt;denchmark-code&gt;[1, 1, 1]
[2, 2, 2]
[3, 3, 3]
&lt;/denchmark-code&gt;

From reading the docs on control_dependencies and identity as well as StackOverflow, I expected output
&lt;denchmark-code&gt;[0, 1, ?]
[1, 2, ?]
[2, 3, ?]
&lt;/denchmark-code&gt;

where ? indicates that the variable value is unspecified.
Is this a bug? If this is not a bug, what is the correct way to refer to the value of variable before and after assignment in a single graph?
	</description>
	<comments>
		<comment id='1' author='eamartin' date='2016-09-30T01:14:05Z'>
		Hm, that's weird, seems like some kind of optimization for tf.identity that
only shows up in the final fetch
If you add the line below, you'll see that identity op gets executed before
assignment
old_val = tf.Print(old_val, [old_val])
This means you can refer to "x" in computation,  and this will refer to old
value as long as control_dependency forces this computation to happen
before tf.assign.
Note that if you do another op, ie, old_val = tf.square, then this behavior
doesn't happen
On Thu, Sep 29, 2016 at 4:54 PM, Eric Martin &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;

wrote:

I'm running Tensorflow 0.10.
The following code
import tensorflow as tf
x = tf.Variable(0, dtype=tf.int32)
old_val = tf.identity(x)with tf.control_dependencies([old_val]):
new_val = tf.assign(x, x + 1)
with tf.Session() as sess:
sess.run(tf.initialize_all_variables())
for i in xrange(3):
    print sess.run([old_val, new_val, x])

outputs
[1, 1, 1]
[2, 2, 2]
[3, 3, 3]
From reading the docs on control_dependencies and identity as well as
StackOverflow, I expected output
[0, 1, ?]
[1, 2, ?]
[2, 3, ?]
where ? indicates that the variable value is unspecified.
Is this a bug? If this is not a bug, what is the correct way to refer to
the value of variable before and after assignment in a single graph?
—
You are receiving this because you are subscribed to this thread.
Reply to this email directly, view it on GitHub
#4663, or mute the thread
https://github.com/notifications/unsubscribe-auth/AABaHMmYr83smkIteubJqVOXJ-r1FLi7ks5qvE_DgaJpZM4KKma6
.

		</comment>
		<comment id='2' author='eamartin' date='2016-09-30T01:23:36Z'>
		I'm guessing identity has an optimization to not perform a copy if there is no device transfer. In this case, I do need a copy of x.
Really, what I want is a function that returns both the old value and the new value of a variable. As you noted with your tf.square example, applying a non-identity op to x seems to cause a copy, so I can likely hack around this bug with old_val = x + 0.
edit: I confirmed that replacing old_val = tf.identity(x) with old_val = x + 0 causes old_val to fetch as new_val - 1 (correct behavior) rather than new_val.
		</comment>
		<comment id='3' author='eamartin' date='2016-10-08T01:54:44Z'>
		This is intended behavior, unfortunately. The identity op returns a tensor that shares the same memory buffer as the variable, so any update to variable will get reflected in the output of the identity op. If you change the identity op to tf.add(x, 0), you should see the "old" value.
		</comment>
		<comment id='4' author='eamartin' date='2016-10-08T02:10:11Z'>
		It seems that identity never had a fully defined specification.
The docs read "Return a tensor with the same shape and contents as the input tensor or value", which seems to indicate a copy. When used across devices, it does cause a copy. I haven't tested it, but I believe if I located old_val on a GPU, then the output of sess.run([old_val, new_val]) would be [0, 1] rather than [1, 1]. Changing the device on which the op is located shouldn't change the semantics of the control dependency, so this seems like a bug. Additionally, if the official way to make a copy on the same device is to add 0, this seems like a bad spec.
If I may suggest a spec for identity:
"Identity returns the value of the tensor at the time of evaluation."
This could eventually be optimized to not perform a copy unless needed. For instance, if I did old_val = tf.identity(x); foo = 2 * old_val; and had a control dependency for assign on old_val, I could shift the control dependency to be on foo and not have to make an unnecessary copy of old_val.
		</comment>
		<comment id='5' author='eamartin' date='2016-10-08T05:51:49Z'>
		You are right that if the identity op is on a different device than the variable, its output is a copy. We had many discussions on this and even had some proposals that would give the option of stronger memory semantics.  I am hopeful that we will get this mess fixed soon.
		</comment>
		<comment id='6' author='eamartin' date='2016-10-18T18:44:57Z'>
		&lt;denchmark-link:https://github.com/yuanbyu&gt;@yuanbyu&lt;/denchmark-link&gt;
 : Should we update the documentation for the identity op as &lt;denchmark-link:https://github.com/lightcatcher&gt;@lightcatcher&lt;/denchmark-link&gt;
 suggests?
		</comment>
		<comment id='7' author='eamartin' date='2017-01-11T17:20:56Z'>
		Has there been any progress on this?
		</comment>
		<comment id='8' author='eamartin' date='2017-01-11T18:58:59Z'>
		 does some weird things (like removing ref-ness from its input), I think &lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 was working on sorting out semantics, &lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 will your resource containers work affect behavior of identity?
		</comment>
		<comment id='9' author='eamartin' date='2017-01-11T19:01:35Z'>
		tf.identity doesn't actually copy the value of the variable (if you want a copy something like +0 will do). For the new variables I'm working on there'll be a read op which does act like it makes a copy.
		</comment>
		<comment id='10' author='eamartin' date='2017-02-15T05:43:02Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 Any progress on the new variables? I see  has been introduced in 0.12 (or sometime before)
		</comment>
		<comment id='11' author='eamartin' date='2017-02-15T17:34:26Z'>
		If you want to try using the new variables wrap your code around a
variable_scope(..., use_resource=True) and use tf.get_variable() to create
the variables. Bear in mind that I don't expect it at this point to be
bug-free nor as performant in all use-cases as old variables but I'd
appreciate any bugs you find.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Feb 14, 2017 at 9:44 PM, Eric Martin ***@***.***&gt; wrote:
 @alextp &lt;https://github.com/alextp&gt; Any progress on the new variables? I
 see Variable.read_value() has been introduced in 0.12 (or sometime before)

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#4663 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAATxWKWx7u1rorX50feCr-jNXR8BTHYks5rcpCwgaJpZM4KKma6&gt;
 .


-- 
 - Alex

		</comment>
		<comment id='12' author='eamartin' date='2017-05-26T01:57:12Z'>
		Don't think this is a bug. tf.identity simply copied the shape and content of x, WHERE x IS UPDATED EVERYTIME BY tf.assign(x,x+1) though you have defined a new tensor new_val.
the definition of tf.assign is
"assign(
ref,
value,
validate_shape=None,
use_locking=None,
name=None
)
Update 'ref' by assigning 'value' to it.
This operation outputs "ref" after the assignment is done. This makes it easier to chain operations that need to use the reset value."
So x is constantly being updated and copied to new_val
		</comment>
		<comment id='13' author='eamartin' date='2017-06-16T22:48:54Z'>
		Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
		<comment id='14' author='eamartin' date='2017-10-06T23:49:19Z'>
		Can we please reopen this issue? The top two answers in &lt;denchmark-link:https://stackoverflow.com/a/34881060/6531137&gt;https://stackoverflow.com/a/34881060/6531137&lt;/denchmark-link&gt;
 implies that this behavior is not expected. The x+0 workaround is also very ugly.
		</comment>
		<comment id='15' author='eamartin' date='2017-10-07T00:27:15Z'>
		This issue is very old, we now have resource variables which have different semantics (don't use tf.identity, but rather var.read_value())
		</comment>
		<comment id='16' author='eamartin' date='2017-10-07T00:34:58Z'>
		real_value doesn't seem to make OP's code work. Am I doing something wrong here? The +0 workaround does work however.
import tensorflow as tf

x = tf.Variable(0, dtype=tf.int32)

old_val = x.read_value() # workaround: old_val = x+0
with tf.control_dependencies([old_val]):
    new_val = tf.assign(x, x + 1)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    for i in range(3):
        print(sess.run([old_val, new_val, x]))
		</comment>
		<comment id='17' author='eamartin' date='2017-10-07T02:21:20Z'>
		&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow.python.ops import resource_variable_ops as rr

x = rr.ResourceVariable(0, dtype=tf.int32)

old_val = x.read_value() # workaround: old_val = x+0
with tf.control_dependencies([old_val]):
    new_val = tf.assign(x, x.read_value() + 1)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    for i in range(3):
        print(sess.run([old_val, new_val, x.read_value()]))
&lt;/denchmark-code&gt;

This prints
&lt;denchmark-code&gt;[0, 1, 0]
[1, 2, 1]
[2, 3, 2]
&lt;/denchmark-code&gt;

The point is to always refer to "x.read_value()" instead of "x"
		</comment>
		<comment id='18' author='eamartin' date='2017-10-13T16:40:46Z'>
		I also came across this problem, when I tried to swap the values of two Variables. It seems that only +0 trick works, instead of read_value().
This works:
a = tf.Variable(1.)
b = tf.Variable(0.)
a_old = a + 0
b_old = b + 0
with tf.control_dependencies([a_old]):
    assign_a = a.assign(b_old)
with tf.control_dependencies([b_old]):
    assign_b = b.assign(a_old)
sess = tf.Session()
sess.run(tf.global_variables_initializer())
sess.run([a_old, b_old, assign_a, assign_b])
# =&gt; [1.0, 0.0, 0.0, 1.0]
This doesn't work:
a = tf.Variable(1.)
b = tf.Variable(0.)
a_old = a.read_value()
b_old = b.read_value()
with tf.control_dependencies([a_old]):
    assign_a = a.assign(b_old)
with tf.control_dependencies([b_old]):
    assign_b = b.assign(a_old)
sess = tf.Session()
sess.run(tf.global_variables_initializer())
sess.run([a_old, b_old, assign_a, assign_b])
# =&gt; [0.0, 0.0, 0.0, 0.0]
Why? I'm really confused.
		</comment>
		<comment id='19' author='eamartin' date='2017-10-13T16:45:42Z'>
		&lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 Seems that  is just implemented by .
  def read_value(self):
    """Returns the value of this variable, read in the current context.

    Can be different from value() if it's on another device, with control
    dependencies, etc.

    Returns:
      A `Tensor` containing the value of the variable.
    """
    return array_ops.identity(self._variable, name="read")
		</comment>
		<comment id='20' author='eamartin' date='2017-10-13T17:54:46Z'>
		&lt;denchmark-link:https://github.com/thjashin&gt;@thjashin&lt;/denchmark-link&gt;
 note that your example is not using read_value() on the RHS of your assign op, whereas my example is using it
		</comment>
		<comment id='21' author='eamartin' date='2017-10-14T01:41:18Z'>
		&lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 I tried but your code works because you use . When I changed the snippet to
a = rr.ResourceVariable(1.)
b = rr.ResourceVariable(0.)
a_old = a.read_value()
b_old = b.read_value()
with tf.control_dependencies([a_old]):
    assign_a = a.assign(b_old)
with tf.control_dependencies([b_old]):
    assign_b = b.assign(a_old)
sess = tf.Session()
sess.run(tf.global_variables_initializer())
sess.run([a_old, b_old, assign_a, assign_b])
It does work.
When I changed your code to
x = tf.Variable(0, dtype=tf.int32)

old_val = x.read_value() # workaround: old_val = x+0
with tf.control_dependencies([old_val]):
    new_val = tf.assign(x, x.read_value() + 1)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    for i in range(3):
        print(sess.run([old_val, new_val, x.read_value()]))
The result is
&lt;denchmark-code&gt;[1, 1, 1]
[2, 2, 2]
[3, 3, 3]
&lt;/denchmark-code&gt;

Pretty strange. Just wonder why control_dependencies only work when the variable is ResourceVariable.
		</comment>
		<comment id='22' author='eamartin' date='2017-10-14T04:40:40Z'>
		tf.Variable is semi broken, ResourceVariable is the replacement
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Oct 13, 2017 18:47, "Shi Jiaxin" ***@***.***&gt; wrote:
 @yaroslavvb &lt;https://github.com/yaroslavvb&gt; I tried but your code works
 because you use ResourceVariable. When I changed the snippet to

 a = rr.ResourceVariable(1.)
 b = rr.ResourceVariable(0.)
 a_old = a.read_value()
 b_old = b.read_value()with tf.control_dependencies([a_old]):
     assign_a = a.assign(b_old)with tf.control_dependencies([b_old]):
     assign_b = b.assign(a_old)
 sess = tf.Session()
 sess.run(tf.global_variables_initializer())
 sess.run([a_old, b_old, assign_a, assign_b])

 It does work.

 When I changed your code to

 x = tf.Variable(0, dtype=tf.int32)

 old_val = x.read_value() # workaround: old_val = x+0with tf.control_dependencies([old_val]):
     new_val = tf.assign(x, x.read_value() + 1)
 with tf.Session() as sess:
     sess.run(tf.global_variables_initializer())

     for i in range(3):
         print(sess.run([old_val, new_val, x.read_value()]))

 The result is

 [1, 1, 1]
 [2, 2, 2]
 [3, 3, 3]

 Pretty strange. Just wonder why control_dependencies only work when the
 variable is ResourceVariable.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#4663 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AABaHDPfLnH8-IHGWpF53pzzkqgZ37Gtks5ssBKsgaJpZM4KKma6&gt;
 .



		</comment>
		<comment id='23' author='eamartin' date='2017-10-14T10:37:12Z'>
		Oh I see. Thanks for clarifying this. Would be better if there is a warning about this in the doc string of Variable.
		</comment>
	</comments>
</bug>