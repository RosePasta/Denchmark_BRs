<bug id='8131' author='gabbard' open_date='2017-03-06T15:50:58Z' closed_time='2017-05-01T16:22:27Z'>
	<summary>Better document when to use tf.sparse_tensor_dense_matmul vs embedding lookup</summary>
	<description>
 has a regular (non-sparse) tensor gradient (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/sparse_grad.py#L153&gt;source&lt;/denchmark-link&gt;
):
  # gradient w.r.t. dense
  b_grad = sparse_ops.sparse_tensor_dense_matmul(sp_t, grad,
                                                 adjoint_a=not adj_a)
where sparse_ops.sparse_tensor_dense_matmul returns a regular tensor.
The &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/sparse_tensor_dense_matmul&gt;documentation&lt;/denchmark-link&gt;
 does not note this and it may be confusing for people coming from e.g. NLP where you frequently work with dense-sparse multiplications with sparse gradients.
In many cases where you might naively think you want to use  you actually want to use &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup_sparse&gt;tf.nn.embedding_lookup_sparse&lt;/denchmark-link&gt;
, even if your task has nothing to do with embeddings.  It would be helpful if there were a cross-reference in the documentation to guide users in the right direction.
	</description>
	<comments>
		<comment id='1' author='gabbard' date='2017-05-01T18:02:18Z'>
		Does that pr address the question of sparse updates when working with an
optimiser?
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On May 1, 2017 9:22 AM, "Vijay Vasudevan" ***@***.***&gt; wrote:
 Closed #8131 &lt;#8131&gt; via
 998baa0
 &lt;998baa0&gt;
 .

 —
 You are receiving this because you were assigned.
 Reply to this email directly, view it on GitHub
 &lt;#8131 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtim8Mbs9Vq7S7zW7KGCOs1fdDfpHMGks5r1gbagaJpZM4MUUvF&gt;
 .



		</comment>
	</comments>
</bug>