<bug id='45940' author='sseung0703' open_date='2020-12-23T13:40:17Z' closed_time='2021-01-14T11:36:04Z'>
	<summary>Problem about distributed training with XLA compiling.</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

custom layer and custom training step


OS Platform and Distribution (e.g., Linux Ubuntu 16.04):

I have tested on Windows 10, Ubuntu 16.04, and Ubuntu 18.04.


TensorFlow installed from (source or binary):

both


TensorFlow version (use command below):

I have tried TF 2.4, 2.5 distributed version and source installed 2.4


Python version:

3.7


Bazel version (if compiling from source):

3.5.0


GCC/Compiler version (if compiling from source):

7.5


CUDA/cuDNN version:

10.1 and 11.0


GPU model and memory:

1080ti x4



Describe the current behavior
When I train my model on multi-gpu with XLA compiling below error is occurred.
&lt;denchmark-code&gt;Training starts
Traceback (most recent call last):
  File "FFP_/train_w_pruning.py", line 76, in &lt;module&gt;
    train_step(*data)
  File "/home/cvip/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 787, in __call__
    result = self._call(*args, **kwds)
  File "/home/cvip/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 854, in _call
    filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access
  File "/home/cvip/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 1920, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/cvip/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 561, in call
    ctx=ctx)
  File "/home/cvip/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Trying to access resource ResNet/conv/kernel/replica_1_879 located in device /job:localhost/replica:0/task:0/device:GPU:0 [Op:__inference_train_step_dist_88943]
&lt;/denchmark-code&gt;

Describe the expected behavior
I want to compile my multi-gpu code but it seems unavailable.

&lt;denchmark-link:https://github.com/sseung0703/TF2-multi-gpu-training&gt;https://github.com/sseung0703/TF2-multi-gpu-training&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='sseung0703' date='2020-12-24T12:08:14Z'>
		&lt;denchmark-link:https://github.com/sseung0703&gt;@sseung0703&lt;/denchmark-link&gt;
,
The code provided in the GitHub repo is fairly complex hence it would be difficult for us to pinpoint the issue. Could you please get the example down to the simplest possible repro? That will allow us to determine the source of the issue easily. Thanks!
		</comment>
		<comment id='2' author='sseung0703' date='2020-12-24T12:33:12Z'>
		&lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;

Sorry for the complex example. I write a code as simple as possible as below.
It requires three codes in my repo, i.e., nets/ResNet.py, nets/tcl.py, and op_utils.py
It is still complex, but I think they should be checked because the problem may be in custom layers or a training step.
&lt;denchmark-code&gt;import os, argparse
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import tensorflow as tf
tf.debugging.set_log_device_placement(False)

import op_utils
from nets import ResNet

parser = argparse.ArgumentParser(description='')

parser.add_argument("--gpu_id", default= [0], type=int, nargs = '+')
parser.add_argument("--compile", default=True, action = 'store_true')
parser.add_argument("--learning_rate", default=1e-1, type=float)
parser.add_argument("--weight_decay", default=5e-4, type=float)
parser.add_argument("--batch_size", default=128, type=int)
args = parser.parse_args()

if __name__ == '__main__':
    gpus = tf.config.list_physical_devices('GPU')
    tf.config.set_visible_devices([tf.config.list_physical_devices('GPU')[i] for i in args.gpu_id], 'GPU')
    for gpu_id in args.gpu_id:
        tf.config.experimental.set_memory_growth(gpus[gpu_id], True)
    devices = ['/gpu:{}'.format(i) for i in args.gpu_id]
    strategy = tf.distribute.MirroredStrategy(devices, cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())

    with strategy.scope():
        options = tf.data.Options()
        options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA

        def inference(image, label):
            image = tf.cast(image, tf.float32)
            image = (image-np.array([113.9,123.0,125.3]))/np.array([66.7,62.1,63.0])
            return image, label

        from tensorflow.keras.datasets.cifar100 import load_data
        (train_images, train_labels), (test_images, test_labels) = load_data()

        test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))
        test_ds = test_ds.map(inference, num_parallel_calls=tf.data.experimental.AUTOTUNE)
        test_ds = test_ds.batch(args.batch_size)
        test_ds = test_ds.with_options(options)
        test_ds = test_ds.prefetch(tf.data.experimental.AUTOTUNE)

        model = ResNet.Model(num_layers = 56, num_class = 100, name = 'ResNet', trainable = True)

        train_step, train_loss, train_accuracy, optimizer = op_utils.Optimizer(args, model, strategy)

        for step, data in enumerate(test_ds):
            train_step(*data)
            print('check')
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='sseung0703' date='2020-12-30T17:46:31Z'>
		&lt;denchmark-link:https://github.com/sseung0703&gt;@sseung0703&lt;/denchmark-link&gt;
,
I was able to run the code without any issues on TF v2.4 and CUDA 11/cuDNN 8. Please check the below screenshot for reference.
&lt;denchmark-link:https://user-images.githubusercontent.com/57165142/103370963-3fee7580-4af4-11eb-8f4e-677b138bd360.png&gt;&lt;/denchmark-link&gt;

Could you please create a new virtual environment and check if you are facing the same error in that as well? Thanks!
		</comment>
		<comment id='4' author='sseung0703' date='2020-12-30T17:48:59Z'>
		&lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;

Did you check the above code on multi-GPU?
		</comment>
		<comment id='5' author='sseung0703' date='2021-01-04T16:22:39Z'>
		&lt;denchmark-link:https://github.com/sseung0703&gt;@sseung0703&lt;/denchmark-link&gt;
,
Yes, I ran the code on a machine with two Tesla V100s. Thanks!
		</comment>
		<comment id='6' author='sseung0703' date='2021-01-05T05:00:11Z'>
		&lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;

In the above code did you set the argument to use multi-GPU like "--gpu_id 0 1" ?
		</comment>
		<comment id='7' author='sseung0703' date='2021-01-06T14:19:06Z'>
		&lt;denchmark-link:https://github.com/sseung0703&gt;@sseung0703&lt;/denchmark-link&gt;
,
Thank you for the update. I had initially run the code without any arguments (i.e. with the default arguments).
On running the code with . I was able to reproduce the issue.
&lt;denchmark-link:https://user-images.githubusercontent.com/57165142/103778391-30bd7980-5058-11eb-99d1-51bc804037b4.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='sseung0703' date='2021-01-06T14:24:57Z'>
		&lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;

Yes, that is the same error as mine.
I think when the graph is compiled by XLA, parameters are not shareable.
How can I solve this?
		</comment>
		<comment id='9' author='sseung0703' date='2021-01-09T00:19:00Z'>
		Hi &lt;denchmark-link:https://github.com/sseung0703&gt;@sseung0703&lt;/denchmark-link&gt;
, to clarify, your code runs with  if  is not true. Is that correct?
		</comment>
		<comment id='10' author='sseung0703' date='2021-01-09T00:42:09Z'>
		&lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;
 yes, without compiling my code works well. You can handle that parameter by argument “—compile”.
		</comment>
		<comment id='11' author='sseung0703' date='2021-01-13T01:00:48Z'>
		I believe using custom training loops with tf.distribute.Strategy and XLA is not a supported feature. The error message indicates the code generates a computation graph that contains ops from multiple devices, and such computation graph is not compatible with XLA (within a XLA cluster, there must be only one device).
		</comment>
		<comment id='12' author='sseung0703' date='2021-01-13T01:18:30Z'>
		Hi &lt;denchmark-link:https://github.com/ckkuang&gt;@ckkuang&lt;/denchmark-link&gt;
, then is there any other option to use XLA on multi-gpu?
		</comment>
		<comment id='13' author='sseung0703' date='2021-01-13T02:43:25Z'>
		You can try &lt;denchmark-link:https://www.tensorflow.org/xla#auto-clustering&gt;auto-clustering&lt;/denchmark-link&gt;
 (although it seems to be an experimental feature). Don't forget to remove  the  knob from your tf.function.
		</comment>
		<comment id='14' author='sseung0703' date='2021-01-13T03:37:12Z'>
		&lt;denchmark-link:https://github.com/ckkuang&gt;@ckkuang&lt;/denchmark-link&gt;
 I have tried auto-clustering, but there is no performance improvement. Does it mean there is no compilable operation in my code due to sharable parameters?
		</comment>
		<comment id='15' author='sseung0703' date='2021-01-13T18:43:04Z'>
		&lt;denchmark-link:https://github.com/sseung0703&gt;@sseung0703&lt;/denchmark-link&gt;
 Your example does not show the whole code, since the part actually applying  is not in your example.
&lt;denchmark-link:https://github.com/nnigania&gt;@nnigania&lt;/denchmark-link&gt;
 is currently working on supporting collective ops under XLA:GPU compilation, so I think the best bet is to wait for this work to land.
Alternatively, you would have to compile the part of your model which is not using any collectives (as we did for MLPerf).
		</comment>
		<comment id='16' author='sseung0703' date='2021-01-13T18:55:25Z'>
		Hi &lt;denchmark-link:https://github.com/cheshire&gt;@cheshire&lt;/denchmark-link&gt;
, you can find the whole code in my first question, which contains experimental_compile.
Anyway, I'm glad to hear that someone is taking care of this issue. :)
I will wait for the next version.
		</comment>
		<comment id='17' author='sseung0703' date='2021-01-13T21:19:51Z'>
		'jit_compile' is the new alias for 'experimental_compile'
Current suggestion is to "jit_compile" only parts of training which are running independently on each replica(GPU). Any time any communication/synchronization is needed then jit_compile around that will fail. So a 'jit_compile' around entire strategy.run will fail, and a 'jit_compile' around the function containing 'optimizer.apply_gradients' will fail. Also any 'jit_compile' around functions which update metrics will fail. But 'jit_compile' around the function doing the main training should work.
We are looking to lower the all_reduce and resolved some of these issues. Soon folks can enable 'jit_compile' on the entire training step (ideally like the way the user has done in this case).
To fix their current code, user needs to change the train_step code in /main/op_utils.py from:
&lt;denchmark-code&gt;    @tf.function(experimental_compile = args.compile)
    def train_step(images, labels):
        with tf.GradientTape() as tape:
            pred = model(images, training = True)
            total_loss = loss_object(labels, pred)/args.batch_size
        gradients = tape.gradient(total_loss, model.trainable_variables)
        if args.weight_decay &gt; 0.:
            gradients = [g+v*args.weight_decay for g,v in zip(gradients, model.trainable_variables)]
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))

        train_loss.update_state(total_loss)
        train_accuracy.update_state(labels, pred)
   
    @tf.function(experimental_compile = args.compile)
    def train_step_dist(image, labels):
        strategy.run(train_step, args= (image, labels))
&lt;/denchmark-code&gt;

Code needs to changed to:
&lt;denchmark-code&gt;    @tf.function(jit_compile = True)
    def compiled_step(images, labels):
        with tf.GradientTape() as tape:
            pred = model(images, training = True)
            total_loss = loss_object(labels, pred)/args.batch_size
        gradients = tape.gradient(total_loss, model.trainable_variables)
        return total_loss, pred, gradients

    def train_step(images, labels):
        total_loss, pred, gradients = compiled_step(images, labels)
        if args.weight_decay &gt; 0.:
            gradients = [g+v*args.weight_decay for g,v in zip(gradients, model.trainable_variables)]

        optimizer.apply_gradients(zip(gradients, model.trainable_variables))

        train_loss.update_state(total_loss)
        train_accuracy.update_state(labels, pred)
   
    @tf.function()
    def train_step_dist(image, labels):
        strategy.run(train_step, args= (image, labels))
&lt;/denchmark-code&gt;

Please let me know if you still hit issues, and we will be happy to resolve them.
		</comment>
		<comment id='18' author='sseung0703' date='2021-01-14T04:55:28Z'>
		Thank you for your advice &lt;denchmark-link:https://github.com/nnigania&gt;@nnigania&lt;/denchmark-link&gt;
. :)
I upgraded my Tensorflow version to 2.5 due to jit_compile is not available on 2.4, and had to fix more codes to remove all the update operations in the compiled_step.
And after this work, my code works well with jit_compiling on multi-GPU, which gives 3 times faster iteration operation!!
Thank you very much again, and this issue is resolved :).
		</comment>
		<comment id='19' author='sseung0703' date='2021-01-14T11:36:05Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45940&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45940&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>