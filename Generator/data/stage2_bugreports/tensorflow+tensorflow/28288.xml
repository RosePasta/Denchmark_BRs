<bug id='28288' author='fhoering' open_date='2019-04-30T16:09:09Z' closed_time='2019-07-02T15:25:16Z'>
	<summary>Race condition with keras model_to_estimator in distributed mode</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos7
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.12.0
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

Describe the current behavior
We are using distributed tensorflow as described here with ParameterServerStrategy:
&lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate&gt;https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate&lt;/denchmark-link&gt;

Basically we are starting the tf server on every worker and then running train_and_evaluate on each worker. The estimator function is serialized, sent to each worker and then executed to create the estimator, create the graph and start the training.
This works with standard estimator models but doesn't when using keras models with model_to_estimator
(doing this still seems the advised way to do distributed learning with keras
&lt;denchmark-link:https://colab.research.google.com/github/lamberta/models/blob/keras-estimator-tutorial/samples/core/tutorials/estimators/keras_estimator.ipynb&gt;https://colab.research.google.com/github/lamberta/models/blob/keras-estimator-tutorial/samples/core/tutorials/estimators/keras_estimator.ipynb&lt;/denchmark-link&gt;
)
(we also tried new standalone mode without any success)
Some nodes are failing with an IO error when trying to save the first checkpoint concurrently
When creating the estimator on each worker it calls model_to_estimator on each worker which calls _save_first_checkpoint
l457
&lt;denchmark-link:https://github.com/tensorflow/estimator/blob/1d55f01d8af871a35ef83fc3354b9feaa671cbe1/tensorflow_estimator/python/estimator/keras.py&gt;https://github.com/tensorflow/estimator/blob/1d55f01d8af871a35ef83fc3354b9feaa671cbe1/tensorflow_estimator/python/estimator/keras.py&lt;/denchmark-link&gt;

Describe the expected behavior
Being able to train Keras model in distributed mode.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Basically we are executing this code on each worker:
&lt;denchmark-code&gt;def estimator_fn():
  estimator = tf.keras.estimator.model_to_estimator(model, config=config)
  return estimator

on each worker:
tf.estimator.train_and_evaluate(
    estimator_fn(),
    train_spec,
    eval_spec
)
&lt;/denchmark-code&gt;

Full code example is here:
&lt;denchmark-link:https://github.com/criteo/tf-yarn/blob/master/examples/keras_example.py&gt;https://github.com/criteo/tf-yarn/blob/master/examples/keras_example.py&lt;/denchmark-link&gt;

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Stacktrace of failure:
Traceback (most recent call last):                                                                                                                                                                                                             File "../_task_commons.py", line 59, in _get_experiment                experiment = dill.loads(client.kv.wait(KV_EXPERIMENT_FN))()                                                                                                                                                                                File "../init.py", line 233, in _new_experiment_fn                                                                                                                                                File "keras_example.py", line 76, in experiment_fn                                                                                                                                                                                           File "/tmp/347e8353-e113-45e6-bc3a-a89be4f9788a/install/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl.e0ceba8cc1b266d3356296be2708e12fb322668e/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl/tensorflow/python/estimator/keras.py", line 484, in model_to_estimator                                                                                                                                                                                                               config)                                                                                                                                                                                                                                    File "/tmp/347e8353-e113-45e6-bc3a-a89be4f9788a/install/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl.e0ceba8cc1b266d3356296be2708e12fb322668e/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl/tensorflow/python/estimator/keras.py", line 367, in _save_first_checkpoint                                                                                                                                                                                                           saver.save(sess, latest_path)                                                                                                                                                                                                              File "/tmp/347e8353-e113-45e6-bc3a-a89be4f9788a/install/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl.e0ceba8cc1b266d3356296be2708e12fb322668e/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl/tensorflow/python/training/saver.py", line 1441, in save                                                                                                                                                                                                                             {self.saver_def.filename_tensor_name: checkpoint_file})                                                                                                                                                                                    File "/tmp/347e8353-e113-45e6-bc3a-a89be4f9788a/install/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl.e0ceba8cc1b266d3356296be2708e12fb322668e/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl/tensorflow/python/client/session.py", line 929, in run                                                                                                                                                                                                                               run_metadata_ptr)                                                                                                                                                                                                                          File "/tmp/347e8353-e113-45e6-bc3a-a89be4f9788a/install/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl.e0ceba8cc1b266d3356296be2708e12fb322668e/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl/tensorflow/python/client/session.py", line 1152, in _run                                                                                                                                                                                                                             feed_dict_tensor, options, run_metadata)                                                                                                                                                                                                   File "/tmp/347e8353-e113-45e6-bc3a-a89be4f9788a/install/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl.e0ceba8cc1b266d3356296be2708e12fb322668e/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl/tensorflow/python/client/session.py", line 1328, in _do_run                                                                                                                                                                                                                          run_metadata)                                                                                                                                                                                                                              File "/tmp/347e8353-e113-45e6-bc3a-a89be4f9788a/install/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl.e0ceba8cc1b266d3356296be2708e12fb322668e/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl/tensorflow/python/client/session.py", line 1348, in _do_call                                                                                                                                                                                                                         raise type(e)(node_def, op, message)                                                                                                                                                                                                     tensorflow.python.framework.errors_impl.UnknownError: viewfs://root/../keras/keras_model.ckpt.index.tempstate16835974976294242898; Input/output error                                                      [[node save/SaveV2 (defined at keras_example.py:76)  = SaveV2[dtypes=[DT_FLOAT, DT_INT64, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device="/job:localhost/replica:0/task:0/device:CPU:0"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, SGD/decay/Read/ReadVariableOp, SGD/iterations/Read/ReadVariableOp, SGD/lr/Read/ReadVariableOp, SGD/momentum/Read/ReadVariableOp, dense/bias/Read/ReadVariableOp, dense/kernel/Read/ReadVariableOp, dense_1/bias/Read/ReadVariableOp, dense_1/kernel/Read/ReadVariableOp, dense_2/bias/Read/ReadVariableOp, dense_2/kernel/Read/ReadVariableOp, global_step, training/SGD/Variable/Read/ReadVariableOp, training/SGD/Variable_1/Read/ReadVariableOp, training/SGD/Variable_2/Read/ReadVariableOp, training/SGD/Variable_3/Read/ReadVariableOp, training/SGD/Variable_4/Read/ReadVariableOp, training/SGD/Variable_5/Read/ReadVariableOp)]]
	</description>
	<comments>
		<comment id='1' author='fhoering' date='2019-06-25T16:35:34Z'>
		omalleyt@ -- can you take a look?
		</comment>
		<comment id='2' author='fhoering' date='2019-06-25T17:02:18Z'>
		This code seems to do model_to_estimator in each worker, which will have the concurrent issue. Instead why not use model_to_estimator locally, and then train_and_evaluate using the right cluster config, i.e., let estimator do the right replication?
		</comment>
		<comment id='3' author='fhoering' date='2019-07-02T11:47:55Z'>
		&lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;

I didn't manage to make keras + model_to_estimator work with standalone mode (nor only keras).
Finally what I do is to call the model_to_estimator once locally to create the checkpoint and then start learning (&lt;denchmark-link:https://github.com/criteo/tf-yarn/blob/master/examples/keras_example.py#L93&gt;https://github.com/criteo/tf-yarn/blob/master/examples/keras_example.py#L93&lt;/denchmark-link&gt;
)
It works. Workaround is good enough. So you can close this as won't fix.
		</comment>
		<comment id='4' author='fhoering' date='2019-07-02T15:25:16Z'>
		Yeah that seems to be the right way to do it. In general we only allow one machine to write checkpoint to prevent race condition, that's probably true in synchronous training as well.
		</comment>
		<comment id='5' author='fhoering' date='2019-07-02T15:25:18Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=28288&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=28288&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>