<bug id='42037' author='zhilothebest' open_date='2020-08-04T18:06:15Z' closed_time='2020-08-06T17:06:39Z'>
	<summary>Random addition of channel during training when input data is all 1 channel</summary>
	<description>
System information
Have I written custom code [posted below]:
Done on google colab using TF 2.x
Currently during training, somehow my training data gets a surprising second channel. I think this has something to do with datatype changing that happens during the fit function, but is not resolved when forcing dtype prior to entering training. This messes with the concatenation of my Dense-Net like structures. Looking at the warning below, I don't understand how its being called with a shape of  512,512, 1, 1, as the overall shape of image_list is 1540, 512,512, 1. Where each image slice is 512, 512, 1. (HWC)
Traceback is:
&lt;denchmark-code&gt;WARNING:tensorflow:Model was constructed with shape (None, 512, 512, 1) for input Tensor("input_2:0", shape=(None, 512, 512, 1), dtype=float32), but it was called on an input with incompatible shape (512, 512, 1, 1).
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-17-b77dae276c25&gt; in &lt;module&gt;()
      1 
----&gt; 2 model.fit(dataset, epochs=1 )

10 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
    106   def _method_wrapper(self, *args, **kwargs):
    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
--&gt; 108       return method(self, *args, **kwargs)
    109 
    110     # Running inside `run_distribute_coordinator` already.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1096                 batch_size=batch_size):
   1097               callbacks.on_train_batch_begin(step)
-&gt; 1098               tmp_logs = train_function(iterator)
   1099               if data_handler.should_sync:
   1100                 context.async_wait()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    778       else:
    779         compiler = "nonXla"
--&gt; 780         result = self._call(*args, **kwds)
    781 
    782       new_tracing_count = self._get_tracing_count()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    821       # This is the first call of __call__, so we have to initialize.
    822       initializers = []
--&gt; 823       self._initialize(args, kwds, add_initializers_to=initializers)
    824     finally:
    825       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    695     self._concrete_stateful_fn = (
    696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--&gt; 697             *args, **kwds))
    698 
    699     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2853       args, kwargs = None, None
   2854     with self._lock:
-&gt; 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2856     return graph_function
   2857 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3211 
   3212       self._function_cache.missed.add(call_context_key)
-&gt; 3213       graph_function = self._create_graph_function(args, kwargs)
   3214       self._function_cache.primary[cache_key] = graph_function
   3215       return graph_function, args, kwargs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3073             arg_names=arg_names,
   3074             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 3075             capture_by_value=self._capture_by_value),
   3076         self._function_attributes,
   3077         function_spec=self.function_spec,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    984         _, original_func = tf_decorator.unwrap(python_func)
    985 
--&gt; 986       func_outputs = python_func(*func_args, **func_kwargs)
    987 
    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    599         # the function a weak reference to itself to avoid a reference cycle.
--&gt; 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    601     weak_wrapped_fn = weakref.ref(wrapped_fn)
    602 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    971           except Exception as e:  # pylint:disable=broad-except
    972             if hasattr(e, "ag_error_metadata"):
--&gt; 973               raise e.ag_error_metadata.to_exception(e)
    974             else:
    975               raise

ValueError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:789 run_step  **
        outputs = model.train_step(data)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:747 train_step
        y_pred = self(x, training=True)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:985 __call__
        outputs = call_fn(inputs, *args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:386 call
        inputs, training=training, mask=mask)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:508 _run_internal_graph
        outputs = node.layer(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:985 __call__
        outputs = call_fn(inputs, *args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/merge.py:183 call
        return self._merge_function(inputs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/merge.py:522 _merge_function
        return K.concatenate(inputs, axis=self.axis)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:2881 concatenate
        return array_ops.concat([to_dense(x) for x in tensors], axis)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:1654 concat
        return gen_array_ops.concat_v2(values=values, axis=axis, name=name)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py:1222 concat_v2
        "ConcatV2", values=values, axis=axis, name=name)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper
        attrs=attr_protos, op_def=op_def)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:593 _create_op_internal
        compute_device)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:3485 _create_op_internal
        op_def=op_def)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1975 __init__
        control_input_ops, op_def)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1815 _create_c_op
        raise ValueError(str(e))

    ValueError: Dimension 2 in both shapes must be equal, but are 2 and 1. Shapes are [512,32,2] and [512,32,1]. for '{{node functional_3/concatenate_60/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](functional_3/leaky_re_lu_84/LeakyRelu, functional_3/concatenate_55/concat, functional_3/concatenate_60/concat/axis)' with input shapes: [512,32,2,86], [512,32,1,172], [] and with computed input tensors: input[2] = &lt;3&gt;.
&lt;/denchmark-code&gt;

Link to colab: &lt;denchmark-link:https://colab.research.google.com/drive/1FwuS2Wa589CvbiqOgAznqqnbKruez9Nj?usp=sharing&gt;https://colab.research.google.com/drive/1FwuS2Wa589CvbiqOgAznqqnbKruez9Nj?usp=sharing&lt;/denchmark-link&gt;

link to sample dataset: 
	</description>
	<comments>
		<comment id='1' author='zhilothebest' date='2020-08-06T07:54:10Z'>
		&lt;denchmark-link:https://github.com/zhilothebest&gt;@zhilothebest&lt;/denchmark-link&gt;

I ran the code shared on colab for 2.2 as shared ans 2.3 and do not face any error, please find &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/1d5726ea6a790c644ebeb9cd9feefd5a/untitled328.ipynb&gt;gist here&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='zhilothebest' date='2020-08-06T08:04:14Z'>
		With respect to the error shared, please refer to below links:
&lt;denchmark-link:https://stackoverflow.com/questions/52365277/valueerror-dimension-2-in-both-shapes-must-be-equal-but-are-3-and-32&gt;link&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/titu1994/Image-Super-Resolution/issues/27&gt;link1&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='zhilothebest' date='2020-08-06T17:06:39Z'>
		&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 ahh I hadn't responded to this and closed it when I figured out the solution last night. My apologies. I was able to fix my problem by using: on my image list and mask list.
		</comment>
		<comment id='4' author='zhilothebest' date='2020-08-06T17:06:41Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42037&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42037&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='zhilothebest' date='2020-12-10T19:52:39Z'>
		
@Saduf2019 ahh I hadn't responded to this and closed it when I figured out the solution last night. My apologies. I was able to fix my problem by using:tf.convert_to_tensor() on my image list and mask list.

THIS IS A LIFE SAVER!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
		</comment>
	</comments>
</bug>