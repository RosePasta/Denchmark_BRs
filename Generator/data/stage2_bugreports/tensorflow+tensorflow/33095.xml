<bug id='33095' author='Nephalen' open_date='2019-10-07T01:07:39Z' closed_time='2020-02-28T19:05:24Z'>
	<summary>Tensorflow 2.0 stop_gradient cause ValueError has 'None' for gradient for tf.Optimizers.Adam</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04.2 LTS
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
pip
TensorFlow version (use command below):
v2.0.0-rc2-26-g64c3d38 2.0.0 / 1.14.0
Python version:
3.7.4
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
7.6.0
GPU model and memory:

Describe the current behavior
For Tensorflow 2.0, stop_gradient() doesn't seem to work properly.
For example, the code below update one model's weight using output from another model. In Tensorflow 2.0, it will try to compute gradient for the stop_gradient() model.
Describe the expected behavior
In Tensorflow 1.14.0 and 1.13.2, it works correctly. The model only updates for model1 without any checking for model2.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
&lt;denchmark-code&gt;import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_probability as tfp
import sys
import time

from tensorflow.keras.layers import Dense, Input, Add, Lambda
from tensorflow.keras import Model
from tensorflow.keras.models import load_model
from tensorflow.keras import optimizers

from scipy.io import loadmat
from scipy.spatial.distance import cdist

tf.compat.v1.disable_eager_execution()

x = np.random.rand(100).reshape(100, 1).astype(float)
y = np.array(x&gt;0.5).reshape(100, 1).astype(float)

input1 = Input(shape=(1, ), name='input1')
d1 = Dense(12, name='d11')(input1)
d1 = Dense(12, name='d12')(d1)
d1 = Dense(1, name='output1')(d1)

input2 = Input(shape=(1, ), name='input2')
d2 = Dense(12, name = 'd21')(input2)
d2 = Dense(12, name = 'd22')(d2)
d2 = Dense(1, name = 'output2')(d2)

label = Input(shape = (1, ), name = 'label')

model1 = Model(inputs = [input1, label], outputs = d1)
model2 = Model(inputs = input2, outputs = d2)

def cust_loss(xi, yi, yp):
    loss = tf.reduce_mean((yi - yp)**2) + tf.reduce_mean(tf.stop_gradient(model2(xi)))
    
    return loss

model1.add_loss(cust_loss(input1, label, d1))
optimizer = optimizers.Adam(lr = 3e-4)
model1.compile(optimizer, loss = None)

model1.fit({'input1': x, 'label': y}, None, epochs=100)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Other info / logs&lt;/denchmark-h&gt;

ValueError                                Traceback (most recent call last)
 in 
26 model1.compile(optimizer, loss = None)
27
---&gt; 28 model1.fit({'input1': x, 'label': y}, None, epochs=100)
~/miniconda3/envs/dl_ehr_rl/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
726         max_queue_size=max_queue_size,
727         workers=workers,
--&gt; 728         use_multiprocessing=use_multiprocessing)
729
730   def evaluate(self,
~/miniconda3/envs/dl_ehr_rl/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
672         validation_steps=validation_steps,
673         validation_freq=validation_freq,
--&gt; 674         steps_name='steps_per_epoch')
675
676   def evaluate(self,
~/miniconda3/envs/dl_ehr_rl/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
187   # function we recompile the metrics based on the updated
188   # sample_weight_mode value.
--&gt; 189   f = _make_execution_function(model, mode)
190
191   # Prepare validation data. Hold references to the iterator and the input list
~/miniconda3/envs/dl_ehr_rl/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py in _make_execution_function(model, mode)
563   if model._distribution_strategy:
564     return distributed_training_utils._make_execution_function(model, mode)
--&gt; 565   return model._make_execution_function(mode)
566
567
~/miniconda3/envs/dl_ehr_rl/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _make_execution_function(self, mode)
2182   def _make_execution_function(self, mode):
2183     if mode == ModeKeys.TRAIN:
-&gt; 2184       self._make_train_function()
2185       return self.train_function
2186     if mode == ModeKeys.TEST:
~/miniconda3/envs/dl_ehr_rl/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _make_train_function(self)
2114           # Training updates
2115           updates = self.optimizer.get_updates(
-&gt; 2116               params=self._collected_trainable_weights, loss=self.total_loss)
2117           # Unconditional updates
2118           updates += self.get_updates_for(None)
~/miniconda3/envs/dl_ehr_rl/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in get_updates(self, loss, params)
498
499   def get_updates(self, loss, params):
--&gt; 500     grads = self.get_gradients(loss, params)
501     grads_and_vars = list(zip(grads, params))
502     self._assert_valid_dtypes([
~/miniconda3/envs/dl_ehr_rl/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in get_gradients(self, loss, params)
396                            "gradient defined (i.e. are differentiable). "
397                            "Common ops without gradient: "
--&gt; 398                            "K.argmax, K.round, K.eval.".format(param))
399       if hasattr(self, "clipnorm"):
400         grads = [clip_ops.clip_by_norm(g, self.clipnorm) for g in grads]
ValueError: Variable &lt;tf.Variable 'd21/kernel:0' shape=(1, 12) dtype=float32&gt; has None for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.
	</description>
	<comments>
		<comment id='1' author='Nephalen' date='2019-10-10T03:37:56Z'>
		&lt;denchmark-link:https://github.com/Nephalen&gt;@Nephalen&lt;/denchmark-link&gt;
,
Can you please check &lt;denchmark-link:https://github.com/tensorflow/model-optimization/issues/3#issuecomment-497151388&gt;this link&lt;/denchmark-link&gt;
 and let us know if it helps. Thanks!
		</comment>
		<comment id='2' author='Nephalen' date='2019-10-10T04:40:52Z'>
		
@Nephalen,
Can you please check this link and let us know if it helps. Thanks!

&lt;denchmark-link:https://github.com/rmothukuru&gt;@rmothukuru&lt;/denchmark-link&gt;

I don't think that problem is the same as mine. The sample code that produces error in Tensorflow 2.0 runs on single machine. In addition, there is no part that can produce additional layers.
I also don't think it's the problem of keras' customized loss function. If I remove stop_gradient(), the code runs fine in Tensorflow 2.0.
Upon further testing, the sample code works in Tensorflow 1.14 and 1.13.2
However, it doesn't work in Tensorflow 1.15.0rc3 and 2.0.0.
To me, it looks like stop_gradient() is not working as intended since optimizer still tries to compute the gradient of part of graph that is blocked by stop_gradient().
		</comment>
		<comment id='3' author='Nephalen' date='2019-10-10T05:46:12Z'>
		Could reproduce the issue with Tensorflow Version 2.0. Here is the &lt;denchmark-link:https://colab.sandbox.google.com/gist/rmothukuru/5b15b64917818c18b24797b962e7f452/33095.ipynb&gt;Gist&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='4' author='Nephalen' date='2019-10-24T22:44:12Z'>
		&lt;denchmark-link:https://github.com/Nephalen&gt;@Nephalen&lt;/denchmark-link&gt;
 I think the original issue has been addressed by this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/26557#issuecomment-487133316&gt;comment&lt;/denchmark-link&gt;
 right? As mentioned stop_gradient does not work well and please follow the suggestion mentioned in the comment.
		</comment>
		<comment id='5' author='Nephalen' date='2019-10-24T23:25:39Z'>
		
@Nephalen I think the original issue has been addressed by this comment right? As mentioned stop_gradient does not work well and please follow the suggestion mentioned in the comment.

&lt;denchmark-link:https://github.com/gowthamkpr&gt;@gowthamkpr&lt;/denchmark-link&gt;
 I'm afraid the issue is not the same. If you look at the network structure of &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/26557#issue-419269658&gt;#26557&lt;/denchmark-link&gt;
 closely, you will find the model is completely disconnected from input because of stop_gradient(). In this case, TF 1.14 will also produce a ValueError saying that no gradient has been provided.
However, in the sample code I provided, the stop_gradient() only blocks the gradient of the second model which is not the model to be trained. In TF 1.14, it works correctly.
It is possible that it has something similar to do with &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33471#issue-508537917&gt;#33471&lt;/denchmark-link&gt;
. Because it is clear that weight of any additional model added in add_loss() function is incorrectly treated as update target in TF2.0/1.15. In this case, it actually makes sense that the code will raise ValueError in TF2.0/1.15 because there is no gradient for model2 completely. But this is only my guess.
		</comment>
		<comment id='6' author='Nephalen' date='2020-02-28T19:05:24Z'>
		&lt;denchmark-link:https://github.com/Nephalen&gt;@Nephalen&lt;/denchmark-link&gt;
 Thanks for the issue!
This looks fixed in the latest tf-nightly (w/ eager execution enabled). This code works for me:
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras import optimizers

x = np.random.rand(100).reshape(100, 1).astype(float)
y = np.array(x&gt;0.5).reshape(100, 1).astype(float)

input1 = Input(shape=(1, ), name='input1')
d1 = Dense(12, name='d11')(input1)
d1 = Dense(12, name='d12')(d1)
d1 = Dense(1, name='output1')(d1)

input2 = Input(shape=(1, ), name='input2')
d2 = Dense(12, name = 'd21')(input2)
d2 = Dense(12, name = 'd22')(d2)
d2 = Dense(1, name = 'output2')(d2)

label = Input(shape = (1, ), name = 'label')

model1 = Model(inputs = [input1, label], outputs = d1)
model2 = Model(inputs = input2, outputs = d2)

def cust_loss(xi, yi, yp):
    loss = tf.reduce_mean((yi - yp)**2) + tf.reduce_mean(tf.stop_gradient(model2(xi)))
    
    return loss

model1.add_loss(cust_loss(input1, label, d1))
optimizer = optimizers.Adam(lr = 3e-4)
model1.compile(optimizer, loss = None)

model1.fit({'input1': x, 'label': y}, None, epochs=100)
		</comment>
		<comment id='7' author='Nephalen' date='2020-02-28T19:05:26Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33095&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33095&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>