<bug id='36418' author='mjlbach' open_date='2020-02-02T06:01:50Z' closed_time='2020-02-07T21:53:00Z'>
	<summary>[Nightly] Distributed dataset/variable regression with dictionary data structures 2.2.0.dev20200130 -&amp;gt; 2.2.0.dev20200131</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): Wheel
TensorFlow version (use command below): 2.2.0.dev20200130, 2.2.0.dev20200131
Python version: 3.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.1/7.x
GPU model and memory: Titan XP
Exact command to reproduce:

You can collect some of this information using our environment capture script:
&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
When calling next on an iter made from a distributed dataset (a tf-agents replay buffer), I get the following error on the latest tf-nightly:
&lt;denchmark-code&gt;  File "/home/mjlbach/Repositories/box-physics/boxphysics/run/worker.py", line 238, in train
    inputs = next(self.iter)[0]
  File "/home/mjlbach/.virtualenvs/physics3.7/lib/python3.7/site-packages/tensorflow_core/python/distribute/input_lib.py", line 250, in __next__
    return self.get_next()
  File "/home/mjlbach/.virtualenvs/physics3.7/lib/python3.7/site-packages/tensorflow_core/python/distribute/input_lib.py", line 271, in get_next
    return values.regroup(replicas)
  File "/home/mjlbach/.virtualenvs/physics3.7/lib/python3.7/site-packages/tensorflow_core/python/distribute/values.py", line 1185, in regroup
    for i in range(len(v0)))
  File "/home/mjlbach/.virtualenvs/physics3.7/lib/python3.7/site-packages/tensorflow_core/python/distribute/values.py", line 1185, in &lt;genexpr&gt;
    for i in range(len(v0)))
  File "/home/mjlbach/.virtualenvs/physics3.7/lib/python3.7/site-packages/tensorflow_core/python/distribute/values.py", line 1202, in regroup
    for key in v0keys
TypeError: __init__() got an unexpected keyword argument 'flex_states'
&lt;/denchmark-code&gt;

I did a bisect and found the change was introduced from 20200130 to 20200131. If I had to guess, I would point towards this commit: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/4c4e3772ae95f17818e1e08cd7d6c33276a6b68f#diff-580627c9b7904095019167ef005a72de&gt;4c4e377#diff-580627c9b7904095019167ef005a72de&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
	</description>
	<comments>
		<comment id='1' author='mjlbach' date='2020-02-03T09:14:55Z'>
		&lt;denchmark-link:https://github.com/mjlbach&gt;@mjlbach&lt;/denchmark-link&gt;
.
In order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!
		</comment>
		<comment id='2' author='mjlbach' date='2020-02-03T21:07:28Z'>
		The following example fails on the latest nightly, but runs fine on 2.2.0dev20200130
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np

from tf_agents import specs
from tf_agents.replay_buffers import episodic_replay_buffer


class EpisodicReplayBuffer:
    def __init__(self, example_len, buffer_size):
        self.example_len = example_len
        self.buffer_size = buffer_size

        self.episodes_collected = 0

    def initialize(self, episode):
        spec = dict()
        for key, value in episode.items():
            dtype = str(value.dtype)
            if dtype == "int32":
                dtype = tf.int32
            elif dtype == "int64":
                dtype = tf.int64
            elif dtype == "float32":
                dtype = tf.float32
            elif dtype == "float64":
                dtype = tf.float64
            elif dtype == "bool":
                dtype = tf.bool
            else:
                print("Unknown dtype: {}".format(dtype))
                quit()
            shape = list(value.shape)[2:]
            spec[key] = specs.TensorSpec(shape, dtype, key)

        self.replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(
            spec,
            capacity=self.buffer_size,
            begin_episode_fn=lambda _: False,
            end_episode_fn=lambda _: True,
        )
        # self.add_episode(episode)

    def add_episode(self, episode):
        print("Episode collected: {}\n".format(self.episodes_collected))
        self.episodes_collected += 1
        episode_id = self.replay_buffer.create_episode_ids(num_episodes=0)
        episode = tf.nest.map_structure(lambda v: v[0], episode)
        self.replay_buffer.add_sequence(episode, episode_id)

    def as_dataset(self, batch_size):
        return self.replay_buffer._as_dataset(
            num_steps=self.example_len, sample_batch_size=batch_size
        )


def main():
    strategy = tf.distribute.MirroredStrategy()
    replay_buffer = EpisodicReplayBuffer(4, 4)
    episode = {}
    for key in ["tensor_1", "tensor_2", "tensor_3"]:
        value = np.ones(shape=[1, 4, 4, 4], dtype=np.float32)
        episode[key] = value

    replay_buffer.initialize(episode)
    with strategy.scope():
        replay_buffer.add_episode(episode)
        replay_buffer.add_episode(episode)
        replay_buffer.add_episode(episode)
        dataset = replay_buffer.as_dataset(4)
        distributed_dataset = strategy.experimental_distribute_dataset(dataset)
        iterator = iter(distributed_dataset)
        data = next(iterator)
        print(data)

if __name__ == "__main__":
    main()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='mjlbach' date='2020-02-07T19:45:51Z'>
		I believe this is fixed on the latest nightly.
		</comment>
		<comment id='4' author='mjlbach' date='2020-02-07T21:51:04Z'>
		&lt;denchmark-link:https://github.com/mjlbach&gt;@mjlbach&lt;/denchmark-link&gt;
 Yes. Looks like it was fixed in the . I could not see any error when I ran against . &lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/2df78b9f5a7eca4d661ca744088c365a/untitled813.ipynb&gt;Here&lt;/denchmark-link&gt;
 is the gist for y/our reference. Thanks!
		</comment>
		<comment id='5' author='mjlbach' date='2020-02-07T21:53:00Z'>
		Closing :)
		</comment>
		<comment id='6' author='mjlbach' date='2020-02-07T21:53:02Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36418&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36418&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>