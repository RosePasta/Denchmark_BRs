<bug id='23789' author='jchia' open_date='2018-11-16T07:04:29Z' closed_time='2019-02-18T03:16:42Z'>
	<summary>Dataset.map() with random_shuffle() and num_parallel_calls=1 has non-deterministic result</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 22
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): 1.12
Python version: 3.6.5
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: N/A
GPU model and memory: N/A

Describe the current behavior
tf.data.Dataset.range(100).batch(2).map(lambda x: tf.random_shuffle(x), num_parallel_calls=1) is non-deterministic. Different runs of the test program produce different output. It's as if the random_shuffle is ignoring the random seed. Although adding a seed argument to random_shuffle makes the problem go away, random_shuffle should still use the graph-level random seed when the seed argument is unspecified.
Describe the expected behavior
Two different runs should always have the same output.
Code to reproduce the issue
&lt;denchmark-code&gt;#!/usr/bin/env python3

import tensorflow as tf

tf.random.set_random_seed(0)
rds = tf.data.Dataset.range(100).batch(2).map(
    lambda x: tf.random_shuffle(x), num_parallel_calls=1)
r = rds.make_one_shot_iterator().get_next()
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(4):
        x, = sess.run([r])
        print(x)
&lt;/denchmark-code&gt;

Other info / logs
&lt;denchmark-code&gt;$ py3/rds.py
2018-11-16 15:11:02.272589: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[1 0]
[3 2]
[4 5]
[7 6]
$ py3/rds.py
2018-11-16 15:11:04.833767: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[0 1]
[2 3]
[5 4]
[6 7]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='jchia' date='2018-11-29T21:59:40Z'>
		It seems that both I and the OP are experiencing unexpected nondeterminism from tf.data.Dataset.map, although my usage scenario is slightly different.
I'm using  to sample from .  According to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/13932&gt;#13932&lt;/denchmark-link&gt;
, random ops that are nested in  should be deterministic as long as a seed has been set and . However this is not the case for me.
#!/usr/bin/env python
"""Test deterministic behavior of tf.data.Dataset.map when num_parallel_calls=1
"""
import tensorflow as tf


def test_0():
    """Use Dataset.map to sample from uniformly random distribution.
    """
    tf.reset_default_graph()
    tf.random.set_random_seed(9778)
    dset = tf.data.Dataset.range(1)
    dset = dset.map(lambda num: tf.random.uniform(()), num_parallel_calls=1)
    iterator = dset.make_one_shot_iterator()
    sample = iterator.get_next()
    with tf.train.MonitoredSession() as sess:
        print(sess.run(sample))


def test_1():
    """Sample from uniformly random distribution without Dataset.map.
    """
    tf.reset_default_graph()
    tf.random.set_random_seed(9778)
    sample = tf.random.uniform(())
    with tf.train.MonitoredSession() as sess:
        print(sess.run(sample))


if __name__ == '__main__':
    print("Using tf.data.Dataset.map with no parallelism:")
    test_0()
    test_0()
    print("Not using tf.data.Dataset.map:")
    test_1()
    test_1()
Output:
Using tf.data.Dataset.map with no parallelism:
2018-11-29 16:45:40.489347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-29 16:45:40.489702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1e.0
totalMemory: 11.17GiB freeMemory: 504.56MiB
2018-11-29 16:45:40.489728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-11-29 16:45:40.823917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-29 16:45:40.823978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0
2018-11-29 16:45:40.823995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N
2018-11-29 16:45:40.824164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 234 MB memory) -&gt; physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)
0.8847439
2018-11-29 16:45:40.876451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-11-29 16:45:40.876491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-29 16:45:40.876506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0
2018-11-29 16:45:40.876513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N
2018-11-29 16:45:40.876616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 234 MB memory) -&gt; physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)
0.66879976
Not using tf.data.Dataset.map:
2018-11-29 16:45:40.904989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-11-29 16:45:40.905024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-29 16:45:40.905038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0
2018-11-29 16:45:40.905059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N
2018-11-29 16:45:40.905165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 234 MB memory) -&gt; physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)
0.5011252
2018-11-29 16:45:40.935271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-11-29 16:45:40.935316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-29 16:45:40.935337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0
2018-11-29 16:45:40.935353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N
2018-11-29 16:45:40.951288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 234 MB memory) -&gt; physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)
0.5011252
My details:
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.12
Python version: 3.6
Bazel version (if compiling from source): 0.19.1
GCC/Compiler version (if compiling from source): 7.3.0
CUDA/cuDNN version: 10 / 7
GPU model and memory: Tesla K80 / 12GB

		</comment>
		<comment id='2' author='jchia' date='2018-12-03T15:19:49Z'>
		&lt;denchmark-link:https://github.com/jchia&gt;@jchia&lt;/denchmark-link&gt;
 I suggest the title be modified to "Dataset.map() with random ops are non-deterministic with num_parallel_calls=1 and user-defined seed"
		</comment>
		<comment id='3' author='jchia' date='2018-12-07T00:34:49Z'>
		&lt;denchmark-link:https://github.com/shivaniag&gt;@shivaniag&lt;/denchmark-link&gt;
 Assigning this to you, because I think the fix here will depend on switching to the new Python function implementation, and making sure that it respects seeds from the outer context. We should definitely add a test to confirm this.
		</comment>
		<comment id='4' author='jchia' date='2018-12-11T04:19:37Z'>
		This will mostly get fixed with switching to the new python function implementation, which we are expecting to be in by end of this month.  Thank you for the issue.
&lt;denchmark-link:https://github.com/feihugis&gt;@feihugis&lt;/denchmark-link&gt;
 I will keep this PR on hold for now, since the issue will get fixed with the changes I mentioned above. Thank you for the PR though.
		</comment>
		<comment id='5' author='jchia' date='2018-12-11T18:06:51Z'>
		&lt;denchmark-link:https://github.com/shivaniag&gt;@shivaniag&lt;/denchmark-link&gt;
 Thanks for your updates. Would you mind to share what is the new python function implementation about? So that I could avoid the contribution conflicts.
		</comment>
		<comment id='6' author='jchia' date='2019-02-18T01:03:00Z'>
		&lt;denchmark-link:https://github.com/shivaniag&gt;@shivaniag&lt;/denchmark-link&gt;
 Is the "new python function implementation" in yet? Will it make it into 1.13?
		</comment>
		<comment id='7' author='jchia' date='2019-02-18T03:16:42Z'>
		&lt;denchmark-link:https://github.com/jchia&gt;@jchia&lt;/denchmark-link&gt;
, The new python function implementation is in and this issue has been fixed with that. It will not make it into 1.13, but it is available with nightly build.
		</comment>
		<comment id='8' author='jchia' date='2019-05-02T03:08:44Z'>
		&lt;denchmark-link:https://github.com/shivaniag&gt;@shivaniag&lt;/denchmark-link&gt;
 do you know which nightly build it was fixed in?  having trouble running the most recent nightly build, so would like to try an exact one for this fix
		</comment>
	</comments>
</bug>