<bug id='42364' author='dakl' open_date='2020-08-14T12:52:54Z' closed_time='2020-09-01T21:55:44Z'>
	<summary>`tf.data.experimental.snapshot()` hangs when using GCS paths</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9.12 stretch
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.3
Python version: 3.5.7
Bazel version (if compiling from source): NA
GCC/Compiler version (if compiling from source): NA
CUDA/cuDNN version: CUDA 10.1
GPU model and memory: Nvidia Tesla T4

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"

Describe the current behavior
tf.data.experimental.snapshot() hangs when using a Google Storage path.
Describe the expected behavior
tf.data.experimental.snapshot() works.
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
&lt;denchmark-code&gt;import tensorflow as tf
for _ in tf.data.Dataset.range(10).apply(tf.data.experimental.snapshot('gs://my-bucket/my-path')): break
&lt;/denchmark-code&gt;

hangs. Using a local path
&lt;denchmark-code&gt;for _ in tf.data.Dataset.range(10).apply(tf.data.experimental.snapshot('gs://my-bucket/deleteme')): break
&lt;/denchmark-code&gt;

works as expected.
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='dakl' date='2020-08-14T12:53:28Z'>
		ping &lt;denchmark-link:https://github.com/piwell&gt;@piwell&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/carlthome&gt;@carlthome&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='dakl' date='2020-08-18T01:34:05Z'>
		Confirming that I was able to reproduce this in Colab and with a GCP deep learning vm instance. Although it was hanging, the metadata files did get written to the bucket. Did you see the same?
		</comment>
		<comment id='3' author='dakl' date='2020-08-21T12:28:35Z'>
		Yes, &lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;
, we saw the same behavior with files popping up on the bucket.
		</comment>
		<comment id='4' author='dakl' date='2020-08-21T12:29:55Z'>
		
Using a local path
for _ in tf.data.Dataset.range(10).apply(tf.data.experimental.snapshot('gs://my-bucket/deleteme')): break


Ping &lt;denchmark-link:https://github.com/dakl&gt;@dakl&lt;/denchmark-link&gt;
, the MCVE is not using a local path. Edit the OP?
		</comment>
		<comment id='5' author='dakl' date='2020-08-29T05:39:27Z'>
		Thanks for reporting this issue! I've investigated and the underlying cause is gcs_file_system.cc not handling calls to NewAppendableFile() correctly when the file does not yet exist.
The deadlock happens due to the following sequence of events:
The error is propagated all the way back to AsyncWriter::WriterThread, which then returns with the error code.
However, by this time, the entire writing process has finished and the main iterator thread calls SignalEOF from GetNextInternal. SignalEOF acquires the WriterThread's mu_ lock and then tries to clear() writers_ vector, containing all the AsyncWriters.
To clear the vector, C++ needs to call AsyncWriter's default destructor, which then blocks on the thread_ within AsyncWriter finishing.
Now, the AsyncWriter thread is still trying to call the done() function that was passed in, which tries to acquire the same mu_ lock that SignalEOF was already holding.
This results in a deadlock where the main thread calling writers_.clear() cannot proceed because the AsyncWriter thread has not terminated, but the AsyncWriter thread is blocked trying to acquire the mu_ lock held by the main thread.
Will fix the underlying problem and the deadlock in an upcoming commit.
		</comment>
		<comment id='6' author='dakl' date='2020-09-01T21:55:45Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42364&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42364&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>