<bug id='46325' author='wangwei-cmd' open_date='2021-01-11T04:18:09Z' closed_time='2021-01-13T16:46:01Z'>
	<summary>Can't create notrainable variables in the __ini__ function in tf.keras.layers.Layer.</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
TensorFlow installed from (source or binary): from conda
TensorFlow version (use command below):  2.3
Python version:3.7
Bazel version (if compiling from source): None
GCC/Compiler version (if compiling from source): None
CUDA/cuDNN version: 10.1.0/7.6.5
GPU model and memory: TITAN RTX/24576Mib

Describe the current behavior
I manually implement the batchnormalized layer and need to create two nontrainable variables to store the mean and variance. But when I embed the custom layer into tf.keras.Model, the two nontrainable variables are not cereated.
Describe the expected behavior
print(len(Model.variables))  should print 4 but 2.
Standalone code to reproduce the issue
&lt;denchmark-code&gt;class batchNormalization(tf.keras.layers.Layer):
    def __init__(self, shape, Trainable, **kwargs):
        super(batchNormalization, self).__init__(**kwargs)
        self.shape = shape
        self.Trainable = Trainable
        self.beta = tf.Variable(initial_value=tf.zeros(shape), trainable=Trainable)
        self.gamma = tf.Variable(initial_value=tf.ones(shape), trainable=Trainable)
        self.moving_mean = tf.Variable(initial_value=tf.zeros(self.shape), trainable=False)
        self.moving_var = tf.Variable(initial_value=tf.ones(self.shape), trainable=False)

    def update_var(self,inputs):
        wu, sigma = tf.nn.moments(inputs, axes=[0, 1, 2], shift=None, keepdims=False, name=None)
        var = tf.math.sqrt(sigma)
        self.moving_mean = self.moving_mean * 0.09 + wu * 0.01
        self.moving_var = self.moving_var * 0.09 + var * 0.01
        return wu,var

    def call(self, inputs):
        wu, var = self.update_var(inputs)
        return tf.nn.batch_normalization(inputs, wu, var, self.beta,
                                         self.gamma, variance_epsilon=0.001)


@tf.function
def train_step(model, inputs, label,optimizer):
    with tf.GradientTape(persistent=False) as tape:
        predictions = model(inputs, training=1)
        loss = tf.keras.losses.mean_squared_error(predictions,label)
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))


if __name__=='__main__':
    f=tf.ones([2,256,256,8])
    label=tf.ones([2,256,256,8])
    inputs = tf.keras.Input(shape=(256,256,8))
    outputs=batchNormalization([8],True)(inputs)
    Model = tf.keras.Model(inputs=inputs, outputs=outputs)
    Layer = batchNormalization([8],True)
    print(len(Model.variables))
    print(len(Model.trainable_variables))
    print(len(Layer.variables))
    print(len(Layer.trainable_variables))
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)
    for i in range(0,100):
        train_step(Layer, f, label,optimizer)
        # train_step(Model,f,label,optimizer)
&lt;/denchmark-code&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
When I trained the model, another error was raised,
TypeError: An op outside of the function building code is being passed a "Graph" tensor. It is possible to have Graph tensors
leak out of the function building context by including a tf.init_scope in your function building code.
When I comment the decorator '@tf.function' before the 'train_step' function, no error is raised. zbut I didn't know wheather it works like I want.
	</description>
	<comments>
		<comment id='1' author='wangwei-cmd' date='2021-01-11T12:51:18Z'>
		Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/4dcfb2028a9a52919d8b86f79b707b1b/46325-tf-nightly.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='wangwei-cmd' date='2021-01-13T01:33:54Z'>
		Hi &lt;denchmark-link:https://github.com/wangwei-cmd&gt;@wangwei-cmd&lt;/denchmark-link&gt;
, you can &lt;denchmark-link:https://www.tensorflow.org/guide/keras/custom_layers_and_models#layers_can_have_non-trainable_weights&gt;see in the docs here&lt;/denchmark-link&gt;
 that creating non trainable variables in  is permitted.
I ran your code and if I commented out the following lines, you do see  len(Model.variables) is 4
&lt;denchmark-code&gt;#self.moving_mean = self.moving_mean * 0.09 + wu * 0.01
#self.moving_var = self.moving_var * 0.09 + var * 0.01
&lt;/denchmark-code&gt;

I think the issue you're having is that you're not assigning the new value to the variable. Try the following:
&lt;denchmark-code&gt;self.moving_mean.assign(self.moving_mean * 0.09 + wu * 0.01)
self.moving_var.assign(self.moving_var * 0.09 + var * 0.01)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='wangwei-cmd' date='2021-01-13T01:50:59Z'>
		Oh, Yes, thank you so much.  I can train the network after using assign.
		</comment>
		<comment id='4' author='wangwei-cmd' date='2021-01-13T16:46:01Z'>
		Great! Closing this issue now since a solution was found.
		</comment>
		<comment id='5' author='wangwei-cmd' date='2021-01-13T16:46:03Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46325&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46325&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>