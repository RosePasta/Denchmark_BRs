<bug id='29027' author='jiarenyf' open_date='2019-05-25T12:33:42Z' closed_time='2019-06-19T13:13:29Z'>
	<summary>Cpu memory leak when using `tf.function` with gpu model.</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v1.12.1-2319-g81f2165 2.0.0-dev20190520
Python version: 3.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.1/7.1
GPU model and memory:

Describe the current behavior
When using tf.function, the cpu memory usage increase forever.
Describe the expected behavior
No cpu memory leak when training models on gpu device using 'tf.function'.
Code to reproduce the issue
A part of my code, since the whole source code is huge...
And the dataset_train in the following code is a tf.data.DataSet object.
&lt;denchmark-code&gt;    DATA_TRAIN_SPEC = [
        tf.TensorSpec(shape=(None, IMAGE_FIX_HEIGHT, None, 3), dtype=tf.float32),
        tf.TensorSpec(shape=None, dtype=tf.int32),
        tf.TensorSpec(shape=None, dtype=tf.int32),
        tf.TensorSpec(shape=None, dtype=tf.int32),
    ]

    # @tf.function(input_signature=DATA_TRAIN_SPEC)
    def train_step(*batch_data):
        imgs, labels, imgs_width, labels_len = batch_data
        with tf.GradientTape() as tape:
            logits = model(imgs, training=True)
            logits_len = tf.cast(tf.math.ceil(
                tf.divide(imgs_width, block_size)), tf.int32)
            loss, accuracy = loss_fn(
                labels, logits, labels_len, logits_len, CLASSES)
        grads = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(grads, model.trainable_variables))

        return loss, accuracy

    def train_epoch(epoch):
        print(f'Epoch {epoch} training start ...')
        for step, batch_data in enumerate(dataset_train.take(num)):
            loss, accuracy = train_step(*batch_data)

            if (step+1) % ARGS.show_per_iterations == 0:
                loss, accuracy = get_numpy((loss, accuracy))

                loss_mean = np.mean(loss)
                accuracy_mean = np.mean(accuracy)*100

                prefix = f'Epoch#{epoch}/Step#{step+1}/training'
                print(f'Loss of {prefix} is: {"%.6f"%loss_mean} ...')
                print(f'Accuracy of {prefix} is: {"%.3f"%accuracy_mean}% ...')
        print(f'Epoch {epoch} training finish ...\n')

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='jiarenyf' date='2019-05-25T12:39:02Z'>
		&lt;denchmark-link:https://github.com/muddham&gt;@muddham&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ry&gt;@ry&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jmhodges&gt;@jmhodges&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/eggie5&gt;@eggie5&lt;/denchmark-link&gt;
 Can anyone help me, please, thank you~
		</comment>
		<comment id='2' author='jiarenyf' date='2019-06-11T04:53:43Z'>
		&lt;denchmark-link:https://github.com/jiarenyf&gt;@jiarenyf&lt;/denchmark-link&gt;
 Can you check this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/26108&gt;issue&lt;/denchmark-link&gt;
 which is similar to yours and try it with TF.2.0.0-beta0 and let us know how it works. Thanks!
		</comment>
		<comment id='3' author='jiarenyf' date='2019-06-14T13:09:49Z'>
		Possibly similar to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29075&gt;#29075&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='4' author='jiarenyf' date='2019-06-16T02:43:32Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 Thank you for the suggestion.
However, the problem still occurs after .
I also tried , and it took no effect for me.
I have uploaded the code in &lt;denchmark-link:https://1drv.ms/u/s!Au2Wv1iKFA69hLpV3g9jjxsQa1AqtA?e=J24DMb&gt;the_codes&lt;/denchmark-link&gt;
, could you please help me to debug, thank you.
You could run the code by:
&lt;denchmark-code&gt;sh install.sh
unzip new.zip
unzip new_datas.zip
cd new
python train.py --config=./config/train/train001.json
&lt;/denchmark-code&gt;

I have refactor the code, so it look different from the one I mentioned 20 days ago, while the problem is the same:
When running without @tf.function the cpu_memory cost is as most 4.8GB and the gpu_util is 70%~100%, while running with @tf.function the cpu_memory keep increasing till over 10GB and  the gpu_util is always 0%.
Thank you.
		</comment>
		<comment id='5' author='jiarenyf' date='2019-06-19T13:13:23Z'>
		Since no one answers this question, I will delete the codes uploaded in one drive and switch my codes to use pytorch instead. Bye bye.
		</comment>
		<comment id='6' author='jiarenyf' date='2019-06-19T13:13:30Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29027&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29027&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='jiarenyf' date='2019-08-31T08:24:30Z'>
		Check if batch_data which in the train_step() function has changed its dimention(or some data in batch_data is accumulating) every epoch. Because if that @tf.function will create a new graph every epoch.
(Have been troubled by this problem for a long time, and finally solved with help of a classmate)
Have a good day.
&lt;denchmark-link:https://github.com/jiarenyf&gt;@jiarenyf&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='jiarenyf' date='2019-11-21T04:40:38Z'>
		Sir have you solved your problem? I also met a problem like this but some reshape is used in my programme but i have no idea how to fixed the leak of CPU memory.
		</comment>
		<comment id='9' author='jiarenyf' date='2019-11-21T07:34:59Z'>
		
Sir have you solved your problem? I also met a problem like this but some reshape is used in my programme but i have no idea how to fixed the leak of CPU memory.

Hi,
The follow @tf.function instruction in Tensorflow may provide some clues:

Note that unlike other TensorFlow operations, we don't convert python numerical inputs to tensors. Moreover, a new graph is generated for each distinct python numerical value, for example calling g(2) and g(3) will generate two new graphs (while only one is generated if you call g(tf.constant(2)) and g(tf.constant(3))). Therefore, python numerical inputs should be restricted to arguments that will have few distinct values, such as hyperparameters like the number of layers in a neural network.



More information about &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/function&gt;@tf.function&lt;/denchmark-link&gt;
.
So, check if have obey this, otherwise it will generate new graph each batch, and that will significantly raise your CPU memory.
Good luck.
		</comment>
		<comment id='10' author='jiarenyf' date='2020-06-08T14:28:27Z'>
		Thank you &lt;denchmark-link:https://github.com/Ain-Crad&gt;@Ain-Crad&lt;/denchmark-link&gt;
 . tf-function decorator is main culprit for increase of ram .
In my case

does lambda here cause a problem
		</comment>
	</comments>
</bug>