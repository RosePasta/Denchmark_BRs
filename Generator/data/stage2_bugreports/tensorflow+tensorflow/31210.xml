<bug id='31210' author='dsgupta' open_date='2019-07-31T20:03:01Z' closed_time='2019-09-07T03:51:57Z'>
	<summary>[TF 2.0] tf.function causes dataset iteration to crash in multi-GPU mode</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
TensorFlow version (use command below): tf-nightly-gpu-2.0-preview
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

Describe the current behavior
When using tf.function decorator for a function iterating over a dataset in Multi-GPU, the Colab notebook crashes and gives the following logs:
&lt;denchmark-code&gt;tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] function_optimizer failed: Invalid argument: Node 'while/body/_21/replica_1/StatefulPartitionedCall': Control dependencies must come after regular dependencies
terminate called after throwing an instance of 'std::out_of_range'
what(): vector::_M_range_check
KernelRestarter: restarting kernel (1/5), keep random ports
&lt;/denchmark-code&gt;

It works perfectly without tf.function decorator
Describe the expected behavior
Dataset should iterate as it does in eager mode without tf.function decorator.
Code to reproduce the issue
&lt;denchmark-code&gt;with strategy.scope():
    def _compute_loss(labels, predictions):
            per_example_loss = loss_object(labels, predictions)
            return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)


    def _train_step(cat_cols, num_cols, labels):

        with tf.GradientTape() as tape:
            inputs = [tf.unstack(cat_cols, axis=-1),num_cols]
            predictions = model(inputs=inputs, training=True)
            loss = _compute_loss(labels, predictions)

        gradients = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))
        predictions = tf.round(predictions)
        train_accuracy.update_state(labels, predictions)
        train_auc.update_state(labels, predictions)
        return loss

    @tf.function
    def _distributed_train_step(dist_train):
        total_loss = 0.0
        num_batches = 0.0
        for cat, num, label in dist_train:
            per_replica_losses = strategy.experimental_run_v2(_train_step,
                                       args=(cat, num, label,))
            total_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)

            num_batches += 1
        return total_loss/num_batches




    def train(dist_train):

        for epoch in range(num_epochs):
            print(f"Training epoch {epoch+1}.", end="")
            train_loss = _distributed_train_step(dist_train)
            template = 'Epoch {}, Loss: {:.3f}, Accuracy: {:.3f}, AUC: {:.3f}'
            print (template.format(epoch + 1, train_loss,
                                 train_accuracy.result() * 100, train_auc.result()))


            train_accuracy.reset_states()
            train_auc.reset_states()

train(dist_train)
&lt;/denchmark-code&gt;

Edit: Did some more digging, and I was able to narrow the issue down to the line gradients = tape.gradient(loss, model.trainable_variables)
	</description>
	<comments>
		<comment id='1' author='dsgupta' date='2019-08-01T05:38:32Z'>
		&lt;denchmark-link:https://github.com/dsgupta&gt;@dsgupta&lt;/denchmark-link&gt;
 , Looks like the code is incomplete please provide the complete code to reproduce the issue. Thanks!
		</comment>
		<comment id='2' author='dsgupta' date='2019-08-01T14:32:39Z'>
		Hi &lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
, can I email you the code?
		</comment>
		<comment id='3' author='dsgupta' date='2019-08-02T05:07:27Z'>
		&lt;denchmark-link:https://github.com/dsgupta&gt;@dsgupta&lt;/denchmark-link&gt;
, Will it be possible to provide the GitHub link or Colab gist. Thanks!
		</comment>
		<comment id='4' author='dsgupta' date='2019-08-02T16:08:04Z'>
		Hi, &lt;denchmark-link:https://colab.research.google.com/gist/dsgupta/7114aa6e13b4f33264ccf3758d169bab/test.ipynb&gt;here&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='dsgupta' date='2019-08-05T11:59:07Z'>
		I am able to reproduce the issue by running the provided colab link. Thanks!
		</comment>
		<comment id='6' author='dsgupta' date='2019-08-08T02:22:35Z'>
		Any updates on this issue?
		</comment>
		<comment id='7' author='dsgupta' date='2019-08-13T18:08:16Z'>
		&lt;denchmark-code&gt;Training epoch 2.Epoch 2, Loss: 0.705, Accuracy: 0.000, AUC: 0.000
Training epoch 3.Epoch 3, Loss: 0.705, Accuracy: 0.000, AUC: 0.000
Training epoch 4.Epoch 4, Loss: 0.705, Accuracy: 0.000, AUC: 0.000
Training epoch 5.Epoch 5, Loss: 0.705, Accuracy: 0.000, AUC: 0.000
Training epoch 6.Epoch 6, Loss: 0.705, Accuracy: 0.000, AUC: 0.000
Training epoch 7.Epoch 7, Loss: 0.705, Accuracy: 0.000, AUC: 0.000
Training epoch 8.Epoch 8, Loss: 0.705, Accuracy: 0.000, AUC: 0.000
Training epoch 9.Epoch 9, Loss: 0.705, Accuracy: 0.000, AUC: 0.000
Training epoch 10.Epoch 10, Loss: 0.705, Accuracy: 0.000, AUC: 0.000```

Colab seems to work for me, did you find a workaround? Or I'm missing something.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='dsgupta' date='2019-09-07T03:51:57Z'>
		Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
		<comment id='9' author='dsgupta' date='2019-09-07T03:51:58Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=31210&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=31210&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>