<bug id='16556' author='KevOBrien' open_date='2018-01-29T22:34:01Z' closed_time='2018-01-31T02:34:55Z'>
	<summary>tf.argmax appears to be functioning incorrectly on occasion</summary>
	<description>
EDIT
&lt;denchmark-link:https://drive.google.com/open?id=13Kice6p3IQvRVOKIlW0DSvD9wwJDL-YH&gt;Link to script and input/label data as pickle files to reproduce the error.&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;



Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
I have written my own code. My code base, data set and batch generating algorithm are quite large, so I am attempting to illustrate this as best as possible. If no mistake of mine can be seen in this post and the examples I have given, then I will provide further code/data. I have asked on StackOverflow, but have got not replies. If  I am missing something simple, then I'm sure it would have been pointed out on StackOverflow by now.


OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Manjaro 17.1.3 Kernel 4.14


TensorFlow installed from (source or binary):
python pip


TensorFlow version (use command below):
tensorflow-gpu 1.5.0


Python version:
3.6.4


CUDA/cuDNN version:
CUDA 9.0
cuDNN 7.0


GPU model and memory:
Nvidia GeForce GTX 1050 8GB


&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

tf.argmax seems to be occasionally producing incorrect results when used on the last axis of a 3-dimensional tensor.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

To debug this, I have printed out the following operations:
&lt;denchmark-code&gt;print(self.session.run(
    tf.equal(tf.argmax(self.predictions, axis=-1),
             tf.argmax(self.labelsUnrolled, axis=-1)),
    self.batchDict))
print("")
print(self.session.run(self.predictions, self.batchDict))
print("")
print(self.session.run(self.labelsUnrolled, self.batchDict))
print("\n********\n")
&lt;/denchmark-code&gt;

Which on two consecutive iterations output the following:
&lt;denchmark-code&gt;[[ True  True  True]
 [ True  True  True]]

[array([[0.06275553, 0.44493628, 0.42474008, 0.06756803],
        [0.06320112, 0.49631155, 0.4021484 , 0.03833894],
        [0.04378054, 0.59403986, 0.3236889 , 0.03849069]], dtype=float32), 
array([[8.1677590e-06, 9.9997127e-01, 2.0200867e-05, 3.6184446e-07],
        [4.3686719e-06, 9.9992716e-01, 6.6905603e-05, 1.5286902e-06],
        [1.3270236e-05, 9.9986196e-01, 1.1622251e-04, 8.5579613e-06]], dtype=float32)]

[array([[0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], dtype=float32),
 array([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], dtype=float32)]

********

[[False False  True]
 [ True  True  True]]

 [array([[0.0466171 , 0.53605616, 0.37778312, 0.03954368],
         [0.05007472, 0.4603508 , 0.44400516, 0.0455693 ],
         [0.06134444, 0.38073638, 0.504286  , 0.05363319]], dtype=float32),
 array([[9.6363285e-05, 9.9861979e-01, 1.2741127e-03, 9.7381862e-06],
         [1.6185455e-05, 9.9977034e-01, 2.0742408e-04, 6.0521238e-06],
         [2.9893983e-05, 9.9954152e-01, 4.2436572e-04, 4.2021661e-06]], dtype=float32)]

[array([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.]], dtype=float32), 
 array([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], dtype=float32)]

********
&lt;/denchmark-code&gt;

Isn't the very first True in the first iteration incorrect? There are many more examples being executed where these are wrong (although it is right the majority of the time). Am I going wrong somewhere with the tf.argmax function?
Printing out the same operations, but not inside the session gives the following shapes:
&lt;denchmark-code&gt;Tensor("Equal_175:0", shape=(2, ?), dtype=bool)

[&lt;tf.Tensor 'unstack_2:0' shape=(?, 4) dtype=float32&gt;, &lt;tf.Tensor 'unstack_2:1' shape=(?, 4) dtype=float32&gt;]

[&lt;tf.Tensor 'unstack_1:0' shape=(?, 4) dtype=float32&gt;, &lt;tf.Tensor 'unstack_1:1' shape=(?, 4) dtype=float32&gt;]
&lt;/denchmark-code&gt;

Is it a problem that the number associated with the tensor name "Equal_XXX:0" is incrementing each iteration?
I have also tried changing the axis argument in both argmax functions to axis=2, giving the "Equal" tensor a shape of (2, 3) again, but there are still similar errors.
Here is an example:
&lt;denchmark-code&gt;[[False False  True]
 [ True  True False]]

[array([[0.09075877, 0.41096467, 0.4460272 , 0.05224944],
        [0.04962843, 0.43777955, 0.46654516, 0.04604685],
        [0.07901238, 0.40768984, 0.46641603, 0.04688181]], dtype=float32),
 array([[0.04444276, 0.49195835, 0.42141557, 0.04218334],
        [0.02372498, 0.47147286, 0.4679979 , 0.03680426],
        [0.03707527, 0.435518  , 0.48937747, 0.03802926]], dtype=float32)]

[array([[0., 0., 1., 0.],
        [1., 0., 0., 0.],
        [0., 0., 1., 0.]], dtype=float32),
 array([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.]], dtype=float32)]
&lt;/denchmark-code&gt;

I would expect this to be:
&lt;denchmark-code&gt;[[ True False  True]
 [ True False False]]
&lt;/denchmark-code&gt;

I thought this may have been a problem with parallel computations on the GPU, but I tried the same execution on just my CPU and got the following, similar miscalculation, too:
&lt;denchmark-code&gt;[[False  True False]
 [False False False]]

[array([[0.07774187, 0.40993363, 0.47022063, 0.04210386],
        [0.04910654, 0.44086066, 0.46013904, 0.04989377],
        [0.06700655, 0.37128285, 0.51324743, 0.04846317]], dtype=float32), 
array([[0.07584244, 0.3863555 , 0.5090046 , 0.02879751],
        [0.06959026, 0.3221606 , 0.5715027 , 0.03674646],
        [0.09042579, 0.32515866, 0.5385905 , 0.04582503]], dtype=float32)]

[array([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.]], dtype=float32),
array([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], dtype=float32)]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='KevOBrien' date='2018-01-30T02:51:17Z'>
		Could you try to whittle that down to a simpler test case that's easy to repro? That would help. Thanks!
		</comment>
		<comment id='2' author='KevOBrien' date='2018-01-30T13:42:15Z'>
		&lt;denchmark-link:https://github.com/drpngx&gt;@drpngx&lt;/denchmark-link&gt;
 I have quickly generted a script which is a much simplified version of my project. I have also included two small pickle files to be fed into the  and  placeholders of the script. Depending on your OS, I think it should load the pickles and run as is, but you may need some tinkering otherwise.
&lt;denchmark-link:https://drive.google.com/open?id=13Kice6p3IQvRVOKIlW0DSvD9wwJDL-YH&gt;Here is a link to a zipped file containing the script, the inputs pickle file and the labels pickle file&lt;/denchmark-link&gt;
.
What I have done is run 100 iterations of training on the same input and labels batch. For each iteration, I compare tf.argmax with np.argmax and print the results if they are different. Usually, each time I run the script, I get at least one difference in the Numpy and TensorFlow results, as shown below. Each time there is an error, results in the following format are printed:
tf.equal(tf.argmax(predictions, axis=-1), tf.argmax(labels, axis=-1))
np.array_equal(np.argmax(predictions, axis=-1), np.argmax(labels, axis=-1))
predictions
labels
Sample output:
&lt;denchmark-code&gt;1
2
3
4
5
6
7

 [[False]
 [False]] 

 [[ True]
 [False]] 

 [array([[0.34652075, 0.34789905, 0.0830554 , 0.2225248 ]], dtype=float32), array([[0.31778762, 0.35688218, 0.09286535, 0.23246484]], dtype=float32)] 

 [array([[0., 1., 0., 0.]], dtype=float32), array([[0., 0., 1., 0.]], dtype=float32)] 

8
9
10
11
12
13
14
15
16
17
18
19
20
21
22

 [[ True]
 [False]] 

 [[ True]
 [ True]] 

 [array([[0.25822368, 0.38438326, 0.12220651, 0.23518656]], dtype=float32), array([[0.13360085, 0.28102002, 0.40999535, 0.17538387]], dtype=float32)] 

 [array([[0., 1., 0., 0.]], dtype=float32), array([[0., 0., 1., 0.]], dtype=float32)] 

23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='KevOBrien' date='2018-01-30T18:15:22Z'>
		Just double checking: could you run
tfCorrect, preds, labs = session.run([correctPredictions, preds, labs], feed_dict)? That makes it slightly easier to reason about.
		</comment>
		<comment id='4' author='KevOBrien' date='2018-01-30T18:39:31Z'>
		Ok, that seems to have fixed it. Could you explain why that is so? Is there some sort of parallelisation issues with when I run AdamOptimizer and then the three other operations in different session.run calls? I have always been calling all of my operations in separate session.run calls, and never knew that unexpectancies like this could come of it
		</comment>
		<comment id='5' author='KevOBrien' date='2018-01-30T18:42:23Z'>
		Also, is there a way to still call each operation in separate session.run calls, and avoid this problem? My entire model is written in an OOP fashion and so I retrieve the results of different operations through individual session.run calls in different, individual class methods
		</comment>
		<comment id='6' author='KevOBrien' date='2018-01-31T02:34:55Z'>
		I haven't looked at your code, but there are two possibilities:

requesting these variables changes global state, for instance, by assigning variables
similar to the first case, but specifically to random generators
the computation is non-deterministic, for instance, if you have parallel computation.

I would need to look at the code again to guess what it is, but in general it's both more efficient and more reliable to use a single run call.
There is partial_run but it's experimental and I wouldn't recommend using this. If it is really an issue for you, I think this is the kind of scenario that Tensorflow Eager Mode was built for.
I'll be closing this issue for admin purposes.
		</comment>
	</comments>
</bug>