<bug id='30513' author='ChuangLee' open_date='2019-07-09T04:18:15Z' closed_time='2019-07-24T04:24:08Z'>
	<summary>TF2-gpu: tf.distribute cause crash when using RNN model,</summary>
	<description>
System information

OS Platform and Distribution: Linux Ubuntu 16.04
TensorFlow installed from  binary:
TensorFlow version (use command below): v1.12.1-5670-g718503b075 2.0.0-dev20190707
Python version: 3.6.4
CUDA/cuDNN version: CUDA-10.0/ Cudnn7.6.1
GPU model and memory: 3 Titan XP

tf.distribute cause crash when using RNN model, work fine when using CNN.
if  "with mirrored_strategy.scope():"  removed, The code below can work well.
Code to reproduce the issue
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf

total_data_size = 10000
X = np.random.randint(100, size=(total_data_size, 100, 20)) / 100
X = X.astype(np.float32)
Y = np.random.randint(2, size=(total_data_size)).astype(
    np.int32)

dataset = tf.data.Dataset.from_tensor_slices((X, Y))
dataset = dataset.batch(12)

mirrored_strategy = tf.distribute.MirroredStrategy()
with mirrored_strategy.scope():
    model = tf.keras.Sequential([
        tf.keras.layers.LSTM(64),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(3, activation='sigmoid')
    ])
    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer=tf.keras.optimizers.Adam(),
                  metrics=['accuracy'])
    model.fit(dataset)
&lt;/denchmark-code&gt;

logs
Apply a constraint manually following the optimizer update step.
2019-07-09 12:01:15.720305: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] implementation_selector failed: Invalid argument: Invalid format of input node name: replica_1/sequential/lstm/StatefulPartitionedCall_replica_1/StatefulPartitionedCall_0 Expected: {forward_node_name}:{index}
2019-07-09 12:01:16.056015: W tensorflow/core/grappler/optimizers/implementation_selector.cc:199] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_8517_9019' and '__inference___backward_standard_lstm_8517_9019_specialized_for_replica_2_StatefulPartitionedCall_at___inference_distributed_function_9976' both implement 'lstm_e2ea6704-e320-4be8-b8e0-8ad71afc296b' but their signatures do not match.
2019-07-09 12:01:16.282647: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1558] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2019-07-09 12:01:16.325991: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-07-09 12:01:16.940501: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at partitioned_function_ops.cc:113 : Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:2 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal
2019-07-09 12:01:16.940501: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at partitioned_function_ops.cc:113 : Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:0 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal
2019-07-09 12:01:16.940560: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:0 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal
[[{{node sequential/lstm/StatefulPartitionedCall}}]]
2019-07-09 12:01:16.940597: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:2 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal
[[{{node replica_2/sequential/lstm/StatefulPartitionedCall}}]]
[[GroupCrossDeviceControlEdges_0/Adam/Adam/update_1_1/Const/_143]]
2019-07-09 12:01:16.940632: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:2 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal
[[{{node replica_2/sequential/lstm/StatefulPartitionedCall}}]]
[[metrics/accuracy/div_no_nan/ReadVariableOp_1/_110]]
2019-07-09 12:01:16.940810: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:2 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal
[[{{node replica_2/sequential/lstm/StatefulPartitionedCall}}]]
2019-07-09 12:01:16.943854: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at partitioned_function_ops.cc:113 : Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:1 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal
Traceback (most recent call last):
File "test_run.py", line 25, in 
model.fit(dataset)
File "/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 668, in fit
use_multiprocessing=use_multiprocessing)
File "/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 680, in fit
steps_name='steps_per_epoch')
File "/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py", line 294, in model_iteration
batch_outs = f(actual_inputs)
File "/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/distribute/distributed_training_utils.py", line 854, in execution_function
return [out.numpy() for out in distributed_function(input_fn)]
File "/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 429, in call
return self._stateless_fn(*args, **kwds)
File "/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 1662, in call
return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
File "/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 635, in _filtered_call
self.captured_inputs)
File "/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 733, in _call_flat
outputs = self._inference_function.call(ctx, args)
File "/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 459, in call
ctx=ctx)
File "/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py", line 67, in quick_execute
six.raise_from(core._status_to_exception(e.code, message), None)
File "", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
(0) Invalid argument:  Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:0 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal
[[node sequential/lstm/StatefulPartitionedCall (defined at /usr/local/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1654) ]]
(1) Invalid argument:  Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:2 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal
[[node replica_2/sequential/lstm/StatefulPartitionedCall (defined at /usr/local/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1654) ]]
0 successful operations.
2 derived errors ignored. [Op:__inference_distributed_function_9976]
Function call stack:
distributed_function -&gt; distributed_function
	</description>
	<comments>
		<comment id='1' author='ChuangLee' date='2019-07-10T21:21:23Z'>
		Note: this is being discussed in issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29189&gt;#29189&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='ChuangLee' date='2019-07-23T16:57:51Z'>
		The distribution strategy with RNN issue should be resolved now in the latest nightly. Please try to test again. Thanks.
		</comment>
		<comment id='3' author='ChuangLee' date='2019-07-24T04:24:09Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30513&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30513&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>