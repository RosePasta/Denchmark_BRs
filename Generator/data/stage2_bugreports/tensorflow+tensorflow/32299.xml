<bug id='32299' author='seandaug' open_date='2019-09-06T21:34:18Z' closed_time='2019-09-13T16:03:46Z'>
	<summary>RNN stateful is incompatible with initial_state</summary>
	<description>
System information

OS Platform and Distribution: Windows 10
TensorFlow installed from (source or binary): binary via conda (1.14.0) and pip (2.0.0-rc0)
TensorFlow version (use command below): 1.14.0 and 2.0.0-rc0
Python version: 3.6
CUDA/cuDNN version: 10.0
GPU model and memory: 1080 Ti

Describe the current behavior
When a stateful RNN is created and is given initial_state, it effectively resets the state to the initial state for every prediction. See the "NOT EXPECTED" example in the code below.
Describe the expected behavior
The stateful RNN should use the initial state the first time, and then update the state and use it for each following time.
Code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import backend as K
import numpy as np

INPUT_SIZE = 4
BATCH_SIZE = 1
OUTPUT_SIZE = 2

# Define a model without any state initialization
input_layer = keras.Input(shape=(None, INPUT_SIZE), batch_size=BATCH_SIZE)
rnn_layer = keras.layers.GRU(units=OUTPUT_SIZE, return_sequences=True, stateful=True)(input_layer)
model = keras.Model(input_layer, rnn_layer)
model.compile(optimizer=keras.optimizers.RMSprop(), loss='mse')

test = np.full((BATCH_SIZE,1,INPUT_SIZE), 0.5)

# This generates different predictions for the two time steps, as expected
model.reset_states()
print(model.predict(test))
print(model.predict(test))
# [[[0.15479106 0.3035699 ]]]
# [[[0.22833839 0.4250579 ]]]

# This generates the same prediction after resetting the state, as expected
model.reset_states()
print(model.predict(test))
model.reset_states()
print(model.predict(test))
# [[[0.15479106 0.3035699 ]]]
# [[[0.15479106 0.3035699 ]]]

# Define a model with a constant state initialization
input_layer = keras.Input(shape=(None, INPUT_SIZE), batch_size=BATCH_SIZE)
initial_state_layer = K.constant(np.ones(OUTPUT_SIZE), shape=(1, OUTPUT_SIZE))
rnn_layer = keras.layers.GRU(units=OUTPUT_SIZE, return_sequences=True, stateful=True)(input_layer, initial_state=initial_state_layer)
model = keras.Model(input_layer, rnn_layer)
model.compile(optimizer=keras.optimizers.RMSprop(), loss='mse')

# This generates the same prediction for the two time steps, NOT EXPECTED
model.reset_states()
print(model.predict(test))
print(model.predict(test))
# [[[0.23247536 0.8585994 ]]]
# [[[0.23247536 0.8585994 ]]]

# This generates the same prediction after resetting the state, as expected
model.reset_states()
print(model.predict(test))
model.reset_states()
print(model.predict(test))
# [[[0.23247536 0.8585994 ]]]
# [[[0.23247536 0.8585994 ]]]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='seandaug' date='2019-09-09T07:01:59Z'>
		I am able to reproduce the issue on Colab with Tensorflow 2.0.0.rc0. Please see the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/f2f267963a9047f5804fcd0892824762/untitled130.ipynb&gt;here&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='seandaug' date='2019-09-11T18:38:17Z'>
		Thanks for reporting the issue, I can reproduce it even with 1.13, which means this bug has been there for long time. Let me check what's the root cause there.
		</comment>
		<comment id='3' author='seandaug' date='2019-09-11T19:25:23Z'>
		Adding Francois since this probably impact the keras-team/keras as well. I think currently the initial_state is taking priority if specified, and I feel the state from previous batch should be respected if stateful=True.
		</comment>
		<comment id='4' author='seandaug' date='2019-09-11T22:47:39Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=32299&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=32299&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='seandaug' date='2019-09-12T16:12:15Z'>
		Thanks a lot for the quick followup, however I don't think the fix is correct. It seems that now the initial_state is being ignored.
Here's an experiment I ran after making the changes locally to  of 2.0.0-rc0 as in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/dccf1c7030867865189924989c857bc1d3454216&gt;dccf1c7&lt;/denchmark-link&gt;
:
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import backend as K
import numpy as np

INPUT_SIZE = 4
BATCH_SIZE = 1
OUTPUT_SIZE = 2

test_input = np.full((BATCH_SIZE,1,INPUT_SIZE), 0.5)

# Define some fixed weights for model consistency
# These were obtained from model.layers[1].get_weights() on the model created below
fixed_weights = [np.array([[ 0.43783283, -0.6397302 , -0.7594154 ,  0.2558545 ,  0.57092595,  0.35916996],
        [-0.30622163,  0.1416316 , -0.25233066,  0.53959405,  0.22741866,  0.03710318],
        [-0.3309809 ,  0.1201275 , -0.4858916 ,  0.4731754 ,  0.3292985 , -0.06378067],
        [-0.6220065 ,  0.27017498,  0.03370708,  0.00933939, -0.56660485,  0.33552015]], dtype=np.float32),
 np.array([[-0.5473411 ,  0.36915207, -0.14257154, -0.32368308,  0.43306705,  0.50149775],
        [ 0.33224842,  0.42010546, -0.77206314,  0.05650809, -0.30846536,  0.13673659]], dtype=np.float32),
 np.array([[0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0.]], dtype=np.float32)]

# Define a model with a constant state initialization to ONES
input_layer = keras.Input(shape=(None, INPUT_SIZE), batch_size=BATCH_SIZE)
initial_state_layer = K.constant(np.ones((1, OUTPUT_SIZE)))
rnn_layer = keras.layers.GRU(units=OUTPUT_SIZE, return_sequences=True, stateful=True)(input_layer, initial_state=initial_state_layer)
model = keras.Model(input_layer, rnn_layer)
model.compile(optimizer=keras.optimizers.RMSprop(), loss='mse')

# Load the fixed weights
model.layers[1].set_weights(fixed_weights)

# Verify that this generates different predictions for the two time steps, as expected
model.reset_states()
print(model.predict(test_input))
print(model.predict(test_input))
#[[[0.16437379 0.1653973 ]]]
#[[[0.23415121 0.2692895 ]]]

# Verify that different predictions are made after explicitly setting the state to ZEROS and ONES, as expected
# HOWEVER, the output from setting state to ZEROS matches the output above using the initial_state_layer of ONES
# AND, the output from setting state to ONES does NOT match the output above using the initial_state_layer of ONES
# This is NOT EXPECTED
model.layers[1].reset_states(states=np.zeros((1, OUTPUT_SIZE)))
print(model.predict(test_input))
model.layers[1].reset_states(states=np.ones((1, OUTPUT_SIZE)))
print(model.predict(test_input))
#[[[0.16437379 0.1653973 ]]]
#[[[0.5386554 0.8742118]]]

# Define a model with a constant state initialization to ZEROS
input_layer = keras.Input(shape=(None, INPUT_SIZE), batch_size=BATCH_SIZE)
initial_state_layer = K.constant(np.zeros((1, OUTPUT_SIZE)))
rnn_layer = keras.layers.GRU(units=OUTPUT_SIZE, return_sequences=True, stateful=True)(input_layer, initial_state=initial_state_layer)
model = keras.Model(input_layer, rnn_layer)
model.compile(optimizer=keras.optimizers.RMSprop(), loss='mse')

# Load the fixed weights
model.layers[1].set_weights(fixed_weights)

# Verify that this generates different predictions for the two time steps, as expected
# HOWEVER, the states output matches the model with initial_state_layer of ONES, which is NOT EXPECTED
model.reset_states()
print(model.predict(test_input))
print(model.predict(test_input))
#[[[0.16437379 0.1653973 ]]]
#[[[0.23415121 0.2692895 ]]]
&lt;/denchmark-code&gt;

The experiment suggests that the supplied initial_state is ignored and zeros initialization (the default for GRU, I think) is used in its place. When the model is built with a K.constant initial state of ones, it should generate the same output as when ones are supplied to model.layers[1].reset_states. Also, that output should be different than when the model is built with a K.constant initial state of zeros.
		</comment>
		<comment id='6' author='seandaug' date='2019-09-12T16:41:43Z'>
		I just edited my experiment code because I had a typo. The second version of the model was incorrectly set to ones, but now I fixed it to use zeros. Now the code illustrates the issue.
		</comment>
		<comment id='7' author='seandaug' date='2019-09-12T18:51:57Z'>
		This is probably a better test to illustrate the issue. What we want is for the stateful=True model to match the predictions of the stateful=False model when an initial_state is specified. This experiment creates a stateful=False model1 with non-zero initial state and uses it to predict 2 time steps. Then we build an equivalent stateful=True model2 and see that its predictions are different when they should match model1. Then model3 uses stateful=False and no initial state and it matches the output from model2 when it shouldn't (though it correctly differs from model1).
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import backend as K
import numpy as np

INPUT_SIZE = 4
BATCH_SIZE = 1
OUTPUT_SIZE = 2

test_input_one_step = np.full((BATCH_SIZE,1,INPUT_SIZE), 0.5)
test_input_two_steps = np.full((BATCH_SIZE,2,INPUT_SIZE), 0.5)

# Define some fixed weights for model consistency
# These were obtained from model.layers[1].get_weights() on the model created below
fixed_weights = [np.array([[ 0.43783283, -0.6397302 , -0.7594154 ,  0.2558545 ,  0.57092595,  0.35916996],
        [-0.30622163,  0.1416316 , -0.25233066,  0.53959405,  0.22741866,  0.03710318],
        [-0.3309809 ,  0.1201275 , -0.4858916 ,  0.4731754 ,  0.3292985 , -0.06378067],
        [-0.6220065 ,  0.27017498,  0.03370708,  0.00933939, -0.56660485,  0.33552015]], dtype=np.float32),
 np.array([[-0.5473411 ,  0.36915207, -0.14257154, -0.32368308,  0.43306705,  0.50149775],
        [ 0.33224842,  0.42010546, -0.77206314,  0.05650809, -0.30846536,  0.13673659]], dtype=np.float32),
 np.array([[0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0.]], dtype=np.float32)]

# model1: a stateful=False model with a non-zero constant state initialization
input_layer = keras.Input(shape=(None, INPUT_SIZE))
initial_state_layer = K.constant(np.ones((1, OUTPUT_SIZE)))
rnn_layer = keras.layers.GRU(units=OUTPUT_SIZE, return_sequences=True, stateful=False)(input_layer, initial_state=initial_state_layer)
model1 = keras.Model(input_layer, rnn_layer)
model1.compile(optimizer=keras.optimizers.RMSprop(), loss='mse')

# Load the fixed weights
model1.layers[1].set_weights(fixed_weights)

# Predict for 2 time steps
model1.reset_states()
print(model1.predict(test_input_two_steps))
#[[[0.5386554  0.8742118 ]
#  [0.3751272  0.74190784]]]

# model2: a stateful=True model with a non-zero constant state initialization
input_layer = keras.Input(shape=(None, INPUT_SIZE), batch_size=BATCH_SIZE)
initial_state_layer = K.constant(np.ones((1, OUTPUT_SIZE)))
rnn_layer = keras.layers.GRU(units=OUTPUT_SIZE, return_sequences=True, stateful=True)(input_layer, initial_state=initial_state_layer)
model2 = keras.Model(input_layer, rnn_layer)
model2.compile(optimizer=keras.optimizers.RMSprop(), loss='mse')

# Load the fixed weights
model2.layers[1].set_weights(fixed_weights)

# This generates different predictions from model1, which is NOT EXPECTED
model2.reset_states()
print(model2.predict(test_input_one_step))
print(model2.predict(test_input_one_step))
#[[[0.16437379 0.1653973 ]]]
#[[[0.23415121 0.2692895 ]]]

# model3: a stateful=False model with no state initialization (uses zeros by default)
input_layer = keras.Input(shape=(None, INPUT_SIZE))
rnn_layer = keras.layers.GRU(units=OUTPUT_SIZE, return_sequences=True, stateful=False)(input_layer)
model3 = keras.Model(input_layer, rnn_layer)
model3.compile(optimizer=keras.optimizers.RMSprop(), loss='mse')

# Load the fixed weights
model3.layers[1].set_weights(fixed_weights)

# Note that model2's predictions match model3's when they should instead match model1's.
model3.reset_states()
print(model3.predict(test_input_two_steps))
#[[[0.16437379 0.1653973 ]
#  [0.23415121 0.2692895 ]]]
&lt;/denchmark-code&gt;

I hope that helps.
		</comment>
		<comment id='8' author='seandaug' date='2019-09-12T19:19:19Z'>
		Thanks for catching this. Sending fix very soon.
		</comment>
		<comment id='9' author='seandaug' date='2019-09-13T16:03:47Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=32299&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=32299&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='seandaug' date='2019-09-13T17:30:48Z'>
		Thanks. I think this is very close, but now it seems that there is a weird corner case where using reset_states to set the state to zeros has no effect. See this:
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import backend as K
import numpy as np

INPUT_SIZE = 4
BATCH_SIZE = 1
OUTPUT_SIZE = 2

test_input = np.full((BATCH_SIZE,1,INPUT_SIZE), 0.5)

# Define some fixed weights for model consistency
# These were obtained from model.layers[1].get_weights() on the model created below
fixed_weights = [np.array([[ 0.43783283, -0.6397302 , -0.7594154 ,  0.2558545 ,  0.57092595,  0.35916996],
        [-0.30622163,  0.1416316 , -0.25233066,  0.53959405,  0.22741866,  0.03710318],
        [-0.3309809 ,  0.1201275 , -0.4858916 ,  0.4731754 ,  0.3292985 , -0.06378067],
        [-0.6220065 ,  0.27017498,  0.03370708,  0.00933939, -0.56660485,  0.33552015]], dtype=np.float32),
 np.array([[-0.5473411 ,  0.36915207, -0.14257154, -0.32368308,  0.43306705,  0.50149775],
        [ 0.33224842,  0.42010546, -0.77206314,  0.05650809, -0.30846536,  0.13673659]], dtype=np.float32),
 np.array([[0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0.]], dtype=np.float32)]

# Define a model with a constant state initialization to ONES
input_layer = keras.Input(shape=(None, INPUT_SIZE), batch_size=BATCH_SIZE)
initial_state_layer = K.constant(np.ones((1, OUTPUT_SIZE)))
rnn_layer = keras.layers.GRU(units=OUTPUT_SIZE, return_sequences=True, stateful=True)(input_layer, initial_state=initial_state_layer)
model = keras.Model(input_layer, rnn_layer)
model.compile(optimizer=keras.optimizers.RMSprop(), loss='mse')

# Load the fixed weights
model.layers[1].set_weights(fixed_weights)

# See what the model predicts when the initial state is ONES
model.reset_states()
print(model.predict(test_input), "expecting [[[0.5386554 0.8742118]]]")

# The same predictions are made after explicitly setting the state, which fails when setting to zeros
model.layers[1].reset_states(states=np.zeros((1, OUTPUT_SIZE)))
print(model.predict(test_input), "expecting [[[0.16437379 0.1653973 ]]]") # FAILS HERE
model.layers[1].reset_states(states=np.ones((1, OUTPUT_SIZE)))
print(model.predict(test_input), "expecting [[[0.5386554 0.8742118]]]") # correct
model.layers[1].reset_states(states=np.full((1, OUTPUT_SIZE), 0.5))
print(model.predict(test_input), "expecting [[[0.36637056 0.49489087]]]") # correct
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='seandaug' date='2019-09-17T17:03:20Z'>
		Thanks for pointing out the issue again. This corner case is actually hard to distinguish from the graph level. Since the initial_state param is always provided in the graph, there isn't an easy way to know whether the initial_state or the recorded state is the latest.
For now, I am inclined to leave it as is, since the most common use case is having a stateful RNN layer, and call model.reset_state().
Please let me know if you have other opinions.
Thanks.
		</comment>
	</comments>
</bug>