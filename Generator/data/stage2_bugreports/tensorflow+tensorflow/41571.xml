<bug id='41571' author='jackd' open_date='2020-07-21T01:30:22Z' closed_time='2020-09-10T19:53:38Z'>
	<summary>keras.Model.fit cannot handle variable epoch sizes</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v2.2.0-0-g2b96f3662b 2.2.0
Python version: 3.6.9

Describe the current behavior
keras.Model.fit with finite-length dataset inputs can run without steps_per_epoch. However, if the number of batches in the epoch changes between epochs, this can lead to:

truncation of epoch if subsequence epochs are longer than the initial epoch; and
truncation of training if subsequence epochsa re shorter than the initial epoch.

While not a common occurance, this happens e.g. in graph neural network applications where it makes more sense to batch according to some maximum number of edges/nodes in a graph as opposed to a fixed batch size. A fixed number of graphs in a dataset can result in a variable number of batches when these are shuffled.

Each epoch of  should iterate over a dataset in it's entirety when  is not provided, and run to the defined number of epochs (unless terminated prior with callbacks), even if the number of steps varies between epochs. Alternatively, a special  value (say, -1) should be able to be provided to specify a dynamic epoch size.

&lt;denchmark-link:https://colab.research.google.com/drive/1HWPtXa2N_EcSFjhcFDquxBrl8RlZvB9y?usp=sharing&gt;Colab&lt;/denchmark-link&gt;

Other info / logs
From colab above, when subsequence epochs are shorter than the first:
WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 30 batches). You may need to use the repeat() function when building your dataset.
	</description>
	<comments>
		<comment id='1' author='jackd' date='2020-07-21T12:15:17Z'>
		Was able to reproduce the issue with TF v2.2 and TF-nightly. Please find the gist of it &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/81bba42e7be6d5f5b31523988f902207/41571-tf-nightly.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='jackd' date='2020-09-10T19:53:36Z'>
		&lt;denchmark-link:https://github.com/jackd&gt;@jackd&lt;/denchmark-link&gt;
 Thanks for the issue! I'd recommend using a Dataset that contains every batch you want to train on, and setting steps_per_epoch as an arbitrary number, so that epoch boundaries are just a way to check on the progress of training in your case. If that doesn't work, then you can always write a custom training loop for your use case: &lt;denchmark-link:https://keras.io/guides/writing_a_training_loop_from_scratch/&gt;https://keras.io/guides/writing_a_training_loop_from_scratch/&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='jackd' date='2020-09-10T19:53:40Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41571&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41571&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>