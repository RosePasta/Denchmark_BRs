<bug id='28181' author='CJMenart' open_date='2019-04-26T12:53:00Z' closed_time='2019-05-09T18:06:41Z'>
	<summary>TF 2.0 tf.summary.scalar claiming it sees a whole tensor when fed scalar</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): My script is included below.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16
TensorFlow installed from (source or binary): conda environment with pip install tensorflow==2.0.0-alpha0
TensorFlow version (use command below): 2.0.0-alpha0. I also, just to check, ran pip install --upgrade tb-nightly I believe on the 25th.
Python version: 3.6
CUDA/cuDNN version: NO GPU USED

Describe the current behavior
When I run the simple MNIST script below (with the tf.summary.scalar line commented out) it runs fine. When I try to log a scalar inside Model.call() I get an error message that seems to indicate tf.summary.scalar is seeing the original tensor--somehow--as opposed to the numpy-summed result that I am passing to tf.summary.scalar.
Describe the expected behavior
According to &lt;denchmark-link:https://www.tensorflow.org/tensorboard/r2/scalars_and_keras&gt;https://www.tensorflow.org/tensorboard/r2/scalars_and_keras&lt;/denchmark-link&gt;
, this would appear to be the way to log scalar values in Tensorboard 2.0. Am I incorrect?
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Code:
`
&lt;denchmark-code&gt;"""
Learn the very basics by running MNIST in TF 2.0
"""
import tensorflow as tf
from tensorflow.keras import datasets, layers
import numpy as np
import datetime
L2_REG = 0.0000001
LEARN_RATE = 1e-5
NUM_LABELS = 10


class MyFirstConvnet(tf.keras.Model):
    def __init__(self):
        super(MyFirstConvnet, self).__init__()
        self.layer1 = layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(L2_REG))
        self.layer2 = layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(L2_REG))
        self.pool = layers.MaxPooling2D((2, 2))
        self.layer3 = layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(L2_REG))
        self.flatten = layers.Flatten()
        self.classifier = layers.Dense(NUM_LABELS, activation='softmax')
        self.batchnorm1 = layers.BatchNormalization(scale=False)
        self.batchnorm2 = layers.BatchNormalization(scale=False)

    def call(self, inputs):
        x = self.batchnorm1(self.layer1(inputs))
        x = self.layer2(x)
        tf.summary.scalar('layer_2_activation_sum', data=np.sum(x, axis=None))  # ERROR THROWN HERE
        x = self.batchnorm2(self.pool(x))
        x = self.layer3(x)
        x = self.flatten(x)
        return self.classifier(x)


if __name__ == '__main__':
    # load up MNIST
    (train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()
    train_images = train_images.reshape((60000, 28, 28, 1)).astype(np.float32)/255.0
    test_images = test_images.reshape((10000, 28, 28, 1)).astype(np.float32)/255.0

    # model must be 'compiled' which integrates information about training and stores it in the model structure
    model = MyFirstConvnet()
    optimizer = tf.keras.optimizers.Adam(lr=LEARN_RATE)
    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # set up tensorboard
    logdir = "logs/scalars/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    file_writer = tf.summary.create_file_writer(logdir)
    file_writer.set_as_default()
    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)

    # train
    model.fit(train_images, train_labels, epochs=10, batch_size=32, validation_split=0.05, shuffle=True,
              callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True),
                         tensorboard_callback])

    # test
    model.evaluate(test_images, test_labels, verbose=1)
&lt;/denchmark-code&gt;

`
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Error message:

2019-04-26 08:41:48.064844: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-04-26 08:41:48.089158: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394490000 Hz
2019-04-26 08:41:48.090339: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x55b1385baf40 executing computations on platform Host. Devices:
2019-04-26 08:41:48.090379: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): , 
Traceback (most recent call last):
File "hello_mnist.py", line 55, in 
tensorboard_callback])
File "/home/menarcj/OtherSoftware/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py", line 806, in fit
shuffle=shuffle)
File "/home/menarcj/OtherSoftware/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py", line 2503, in _standardize_user_data
self._set_inputs(cast_inputs)
File "/home/menarcj/OtherSoftware/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py", line 456, in _method_wrapper
result = method(self, *args, **kwargs)
File "/home/menarcj/OtherSoftware/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py", line 2775, in _set_inputs
outputs = self.call(inputs)
File "hello_mnist.py", line 28, in call
tf.summary.scalar('layer_2_activation_sum', data=np.sum(x, axis=None))  # ERROR THROWN HERE
File "/home/menarcj/OtherSoftware/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorboard/plugins/scalar/summary_v2.py", line 61, in scalar
tf.debugging.assert_scalar(data)
File "/home/menarcj/OtherSoftware/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py", line 1718, in assert_scalar_v2
assert_scalar(tensor=tensor, message=message, name=name)
File "/home/menarcj/OtherSoftware/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py", line 1751, in assert_scalar
% (message or '', tensor.name, shape))
ValueError: Expected scalar shape for conv2d_1/Relu:0, saw shape: (None, 24, 24, 64).

	</description>
	<comments>
		<comment id='1' author='CJMenart' date='2019-04-30T14:23:14Z'>
		&lt;denchmark-link:https://github.com/CJMenart&gt;@CJMenart&lt;/denchmark-link&gt;
 Able to reproduce the issue with the code provided, attached the log.
ValueError                                Traceback (most recent call last)
 in ()
50     model.fit(train_images, train_labels, epochs=10, batch_size=32, validation_split=0.05, shuffle=True,
51               callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True),
---&gt; 52                          tensorboard_callback])
53
54     # test
7 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/check_ops.py in assert_scalar(tensor, name, message)
1749       else:
1750         raise ValueError('%sExpected scalar shape for %s, saw shape: %s.'
-&gt; 1751                          % (message or '', tensor.name, shape))
1752     return tensor
1753
ValueError: Expected scalar shape for conv2d_1/Relu:0, saw shape: (None, 24, 24, 64).
Request you to reference stackoverflow similar issues &lt;denchmark-link:https://stackoverflow.com/questions/49057149/expected-conv2d-1-input-to-have-shape-28-28-1-but-got-array-with-shape-1-2&gt;link1&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://stackoverflow.com/questions/50860182/error-when-checking-input-expected-conv2d-1-input-to-have-shape-28-28-1-but/50860463&gt;link2&lt;/denchmark-link&gt;
. Hope these helps. This question is better asked on &lt;denchmark-link:http://stackoverflow.com/questions/tagged/tensorflow&gt;StackOverflow&lt;/denchmark-link&gt;
 since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/new/choose&gt;issue template&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='CJMenart' date='2019-04-30T15:03:59Z'>
		&lt;denchmark-link:https://github.com/muddham&gt;@muddham&lt;/denchmark-link&gt;
 There is nothing wrong with the size of the tensors invovled.
If I add the following print statements to call:
&lt;denchmark-code&gt;def call(self, inputs):
        x = self.batchnorm1(self.layer1(inputs))
        x = self.layer2(x)
        print(x)
        print(np.sum(x))
        tf.summary.scalar('layer_2_activation_sum', data=np.sum(x, axis=None))  # ERROR THROWN HERE
        x = self.batchnorm2(self.pool(x))
        x = self.layer3(x)
        x = self.flatten(x)
        return self.classifier(x)
&lt;/denchmark-code&gt;

The same thing is printed twice:

Tensor("conv2d_1/Relu:0", shape=(None, 24, 24, 64), dtype=float32)
Tensor("conv2d_1/Relu:0", shape=(None, 24, 24, 64), dtype=float32)

That should not be possible. np.sum() should not be passing back a Tensor object. The issue is that tf.summary.scalar is talking about tensors by name at all when, supposedly, all it is being given is a numpy scalar.
The following snippet appears to show the correct behavior:
&lt;denchmark-code&gt;const = tf.constant([3,3], dtype=tf.float32)
np.sum(const)
&lt;/denchmark-code&gt;


6.0

So my only guess is that something is going wrong involving Model.call().
		</comment>
		<comment id='3' author='CJMenart' date='2019-05-03T14:05:24Z'>
		&lt;denchmark-link:https://github.com/CJMenart&gt;@CJMenart&lt;/denchmark-link&gt;
 Could reproduce the issue.Log shows as below.
Tensor("conv2d_1/Relu:0", shape=(None, 24, 24, 64), dtype=float32)
Tensor("conv2d_1/Relu:0", shape=(None, 24, 24, 64), dtype=float32)
		</comment>
		<comment id='4' author='CJMenart' date='2019-05-09T18:06:34Z'>
		&lt;denchmark-link:https://github.com/CJMenart&gt;@CJMenart&lt;/denchmark-link&gt;
 is correct; the model.call is incorrect.
In this line:
&lt;denchmark-code&gt;        tf.summary.scalar('layer_2_activation_sum', data=np.sum(x, axis=None))  # ERROR THROWN HERE
&lt;/denchmark-code&gt;

x is a symbolic Tensor. When you call np.sum, it retrieves the Tensor __sum__ attr, which in the case of TensorFlow is the element wise sum. (And the sum of a single element is just an identity) And so tf.summary.scalar is correctly telling you that the value you passed is not a scalar. The function that you want is tf.reduce_sum.
		</comment>
		<comment id='5' author='CJMenart' date='2019-05-09T20:16:44Z'>
		So if I understand correctly what's happened here...model.call was supposed to do things in eager mode; that's what I initially assumed would be the case in 2.0. And in fact, it was supposed to do things in eager mode (in the default case, of course), but it was actually building a graph with symbolic tensors, a la the default method of model construction in 1.x. As such, the output which confused me came from the fact that np.sum is auto-cast to this Tensor attribute when called on a Tensor (which maybe should come with a warning message?). This would also explain why calls to print() only printed output once when my code snippet was run.
Is that roughly correct? Sorry if this comment re-opens an issue that shouldn't be.
		</comment>
		<comment id='6' author='CJMenart' date='2019-05-09T21:17:19Z'>
		This is intentional. Even though eager is the default execution mode in TF 2.0, keras defaults to symbolic computation because it is more efficient for the sorts of computation that keras is typically used for. (Namely, the same computation over different data.) You can enable eager execution inside of a layer's call method either on an individual basis (by passing dynamic=True to a Layer's constructor) or on a Model level by setting the run_eagerly attribute or passing run_eagerly to compile.
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf

tf.enable_v2_behavior()
print("We're in eager mode:   ", tf.random.uniform(shape=(4,)))

class LoggingIdentityLayer(tf.keras.layers.Layer):
  """A keras layer that prints some stuff."""
  
  def call(self, inputs):
    print("Inputs to {}: {}".format(self.name, tf.squeeze(inputs)))

    return tf.identity(inputs)
    # The base Layer is an identity layer
    return super(LoggingIdentityLayer, self).call(inputs=inputs)

  def compute_output_shape(self, input_shape):
    return input_shape  # This is an identity layer

x=np.random.random(size=(16, 1)),
y=np.random.random(size=(16, 1))

inp = tf.keras.layers.Input(shape=(1,))
dense_results = tf.keras.layers.Dense(1)(inp)
log_layer = LoggingIdentityLayer()(dense_results)
model = tf.keras.models.Model(inp, log_layer)

model.compile(loss="mse", optimizer="sgd")
model.fit(x=x, y=x, batch_size=4)
print()

# Run the entire model in eager mode:
model.compile(loss="mse", optimizer="sgd",
              run_eagerly=True)
model.fit(x=x, y=x, batch_size=4)
print()

# Only make a certain layer dynamic
dynamic_log_layer = LoggingIdentityLayer(dynamic=True)(dense_results)
model_prime = tf.keras.models.Model(inp, dynamic_log_layer)
model_prime.compile(loss="mse", optimizer="sgd")
model_prime.fit(x=x, y=x, batch_size=4)
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>