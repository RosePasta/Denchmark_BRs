<bug id='3748' author='florijanstamenkovic' open_date='2016-08-11T11:09:06Z' closed_time='2016-08-30T17:16:36Z'>
	<summary>unexpected names in `NamedOutputs` tuples when using `outputs_collections` with some layers</summary>
	<description>
We are getting unexpected names (they lack outer name scope) in NamedOutputs tuples added to collections using tf.contrib.layers outputs_collections. The following code demonstrates the issue:
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow.contrib import slim


with tf.name_scope("train"):
    with slim.arg_scope([slim.fully_connected, slim.flatten],
                        outputs_collections=tf.GraphKeys.ACTIVATIONS):
        ph = tf.placeholder(tf.float32, [2, 2])
        fc = slim.fully_connected(ph, 10)
        flat = slim.flatten(ph)

{print("name in tuple: ", no.name, ", tensor name:", no.outputs.name)
 for no in tf.get_collection(tf.GraphKeys.ACTIVATIONS)}
&lt;/denchmark-code&gt;

The output is:
&lt;denchmark-code&gt;name in tuple:  fully_connected , tensor name: train/fully_connected/Relu:0
name in tuple:  train/Flatten , tensor name: train/Flatten/Reshape:0
&lt;/denchmark-code&gt;

We have tracked the cause of this down in to tf.contrib.layers. For layers that use internal variables (fully_connected, conv2d, ...), final outputs are added to collections based on internal variable_scope name. Please see:
tensorflow/tensorflow/contrib/layers/python/layers/layers.py lines 758, 835 (version 0.10, commit &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/1df3fb0b4ae5915364f09e233496e98a99a4a886&gt;1df3fb0&lt;/denchmark-link&gt;
)
It is our understanding that activation names generally fall under name_scopes, which is consistent with actual op names in the output above.
This issue makes it impossible to retrieve items from collections filtered down with a name scope, an approach that we are trying to use for decoupling op creation and summarizing. It seems a valid use-case.
	</description>
	<comments>
		<comment id='1' author='florijanstamenkovic' date='2016-08-11T17:48:26Z'>
		Duplicate of &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/3721&gt;#3721&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='florijanstamenkovic' date='2016-08-11T18:22:35Z'>
		Never mind. Different issue.
		</comment>
		<comment id='3' author='florijanstamenkovic' date='2016-08-28T05:59:10Z'>
		&lt;denchmark-link:https://github.com/lukaszkaiser&gt;@lukaszkaiser&lt;/denchmark-link&gt;
 could you look at this? Is this a scope issue or something wrong with how we use the scopes in layers?
		</comment>
		<comment id='4' author='florijanstamenkovic' date='2016-08-28T09:21:41Z'>
		I'm not sure if It is the expected behaviour, but it might be. Are you printing variable names or just tensor names? Variable names are by design not affected by name_scope, only variable_scope. But is it this? Or is something else wrong? Could you clarify what you're printing and what you'd expect and why? Thanks!
		</comment>
		<comment id='5' author='florijanstamenkovic' date='2016-08-28T10:20:19Z'>
		&lt;denchmark-link:https://github.com/lukaszkaiser&gt;@lukaszkaiser&lt;/denchmark-link&gt;
, I am looking at tensor names, not variable names.
Refering to the example above. Two tensors are created with the train name_scope and added to the ACTIVATIONS collection. When looking at the contents of that collection it can be seen that the Flatten tensor's NamedOutput.name becomes train/Flatten. This obeys name scoping and matches the name attribute of the tensor itself (NamedOutput.outputs.name). However, the fully_connected tensor's NamedOutput.name is just fully_connected, instead of train/fully_connected. That does not obey name scoping, and differs from the actual tensor's name attribute which does obey name scoping.
I would expect that the NamedOutputs.name attribute for a fully_connected tensor respects name_scope and thus reflects the actual tensor's name attribute.
Tracking that down in TF source reference in the original description, it can be seen that tensors which use internal variables (such as fully_connect and conv2d) are using variable scopes when creating NamedOutputs, which seems a bug.
I have modified the original example where I inappropriately named some variables, it should be less confusing now. Sorry about that.
		</comment>
		<comment id='6' author='florijanstamenkovic' date='2016-08-29T15:53:53Z'>
		Great thanks for the explanation! It looks like a bug, probably not in scoping per se but, as you said, in assigning NamedOutputs.name based on variable_scope and not name_scope. Since you tracked it down, could you say where in TF code this appears? Would you like to contribute a fix? I assigned to Sergio who might know more, and in particular if there were any reasons to do it like this in layers.
Great thanks for pointing this out!
		</comment>
		<comment id='7' author='florijanstamenkovic' date='2016-08-29T15:59:08Z'>
		I think it was fixed by &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/3635&gt;#3635&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='florijanstamenkovic' date='2016-08-29T17:36:42Z'>
		&lt;denchmark-link:https://github.com/sguada&gt;@sguada&lt;/denchmark-link&gt;
: it seems fixed to me too, by looking at the source code of the fix and the tests.
		</comment>
	</comments>
</bug>