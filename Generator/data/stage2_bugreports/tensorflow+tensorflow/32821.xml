<bug id='32821' author='TimCapes' open_date='2019-09-25T20:29:47Z' closed_time='2020-04-29T01:06:41Z'>
	<summary>Optimizer Fails to Run in Compatibility Mode (tested for Adam and Gradient Descent)</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.12.6
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 2.0.0-rc1
Python version:3.7.1
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A (CPU)
GPU model and memory:N/A (CPU)

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
Optimizer increments the global step by 1 and stops without changing the network.
Describe the expected behavior
Optimizer runs metric to convergence
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
import tensorflow.compat.v1 as tf
import numpy as np
tf.disable_v2_behavior()
tf.disable_eager_execution()
input = tf.placeholder(dtype=tf.float32, shape=(None,28,28), name="input")
reshape = tf.reshape(input, [tf.shape(input)[0],784])
w1 = tf.Variable(tf.random_normal([128,784], dtype=tf.float32), name="w1")
b1 = tf.Variable(tf.random_normal([128], dtype=tf.float32), name="b1")
layer_one_unbiased = tf.matmul(w1,tf.transpose(reshape))
layer_one_biased = tf.add(tf.transpose(layer_one_unbiased),b1)
activated_layer_one = tf.nn.relu(layer_one_biased)
w2 = tf.Variable(tf.random_normal([10,128]), dtype=tf.float32, name="w2")
b2 = tf.Variable(tf.random_normal([10], dtype=tf.float32, name="b2"))
layer_two_unbiased = tf.matmul(w2,tf.transpose(activated_layer_one))
layer_two_biased = tf.add(tf.transpose(layer_two_unbiased), b2)
labels= tf.placeholder(dtype=tf.int32, shape=(None))
loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=layer_two_biased)
global_step = tf.Variable(0, name='global_step', trainable=False)
#optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)
optimizer = tf.train.GradientDescentOptimizer(0.1)
train = optimizer.minimize(loss, global_step=global_step, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))

predictions = tf.nn.softmax(layer_two_biased, name="final")
#label_acc = tf.one_hot(labels,10)
#accuracy = 1 - tf.norm(tf.subtract(predictions,label_acc), axis=1)/2
acc, acc_op = tf.metrics.accuracy(tf.argmax(predictions,1), labels)
#accuracy_averaged = tf.math.reduce_mean(accuracy)

fashion_mnist = tf.keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
train_images = train_images/255.0
test_images = test_images/255.0
print(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))

sess = tf.Session()
sess.run(tf.global_variables_initializer())
sess.run(tf.local_variables_initializer())
feed_dict = {input: train_images, labels: train_labels}
print(sess.run([loss,train,acc_op, global_step],feed_dict=feed_dict))

feed_dict2 = {input: test_images, labels: test_labels}
print("Test accuracy", sess.run(acc_op, feed_dict = feed_dict2))
print("Training accuracy",sess.run(acc_op, feed_dict = feed_dict))
saver =tf.train.Saver()
save_path = saver.save(sess, "model.ckpt")
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Code successfully runs so no error occurs but output of global step value is 1. Note that this network matches the fashion mnist network but is just using tf instead of tf.keras.layers to encode the layers.
	</description>
	<comments>
		<comment id='1' author='TimCapes' date='2019-09-25T20:30:45Z'>
		Note eager execution is disabled since under eager execution the optimizer is supposed to take a function that calculates loss and not a tensor.  With eager execution disabled according to the docs a tensor is acceptable input. Also tried without global step and with using var_list as just the defaults.
		</comment>
		<comment id='2' author='TimCapes' date='2019-09-26T11:34:47Z'>
		&lt;denchmark-link:https://github.com/TimCapes&gt;@TimCapes&lt;/denchmark-link&gt;
 ,
When tried executing the given code the following output was received, take a look at the &lt;denchmark-link:https://colab.sandbox.google.com/gist/oanush/c3b49526e45c415e33d707960caef47b/untitled10.ipynb&gt;gist&lt;/denchmark-link&gt;
 of colab.
Can you confirm if the same output is received?Thanks!
		</comment>
		<comment id='3' author='TimCapes' date='2019-09-26T14:44:57Z'>
		&lt;denchmark-link:https://github.com/oanush&gt;@oanush&lt;/denchmark-link&gt;
 Output looks to be the same as what I'm getting.  You'll notice it doesn't include any training of the network and that the accuracy is roughly random and the global_steps variable has value 1.
		</comment>
		<comment id='4' author='TimCapes' date='2019-09-30T16:16:25Z'>
		I am now using the following revised code which works better for TFLite conversion. Still have the optimizer issue.
import tensorflow.compat.v1 as tf
import numpy as np
tf.disable_v2_behavior()
tf.disable_eager_execution()
input = tf.placeholder(dtype=tf.float32, shape=(None,28,28), name="input")
reshape = tf.reshape(input, [tf.shape(input)[0],784])
w1 = tf.Variable(tf.random_normal([128,784], dtype=tf.float32), name="w1")
b1 = tf.Variable(tf.random_normal([128,1], dtype=tf.float32), name="b1")
layer_one_unbiased = tf.matmul(w1,tf.transpose(reshape))
layer_one_biased = tf.add(layer_one_unbiased,b1)
activated_layer_one = tf.nn.relu(layer_one_biased)
w2 = tf.Variable(tf.random_normal([10,128]), dtype=tf.float32, name="w2")
b2 = tf.Variable(tf.random_normal([10,1], dtype=tf.float32, name="b2"))
layer_two_unbiased = tf.matmul(w2,activated_layer_one)
layer_two_biased = tf.add(layer_two_unbiased, b2)
labels= tf.placeholder(dtype=tf.int32, shape=(None))
layer_two_biased_transpose = tf.transpose(layer_two_biased)
loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=layer_two_biased_transpose)
global_step = tf.Variable(0, name='global_step', trainable=False)
#optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)
optimizer = tf.train.GradientDescentOptimizer(0.1)
train = optimizer.minimize(loss, global_step=global_step, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))

predictions = tf.nn.softmax(layer_two_biased_transpose, name="final")
#label_acc = tf.one_hot(labels,10)
#accuracy = 1 - tf.norm(tf.subtract(predictions,label_acc), axis=1)/2
acc, acc_op = tf.metrics.accuracy(tf.argmax(predictions,1), labels)
#accuracy_averaged = tf.math.reduce_mean(accuracy)

fashion_mnist = tf.keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
train_images = train_images/255.0
test_images = test_images/255.0
print(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))

sess = tf.Session()
sess.run(tf.global_variables_initializer())
sess.run(tf.local_variables_initializer())
feed_dict = {input: train_images, labels: train_labels}
print(sess.run([loss,train,acc_op, global_step],feed_dict=feed_dict))
		</comment>
		<comment id='5' author='TimCapes' date='2020-04-15T00:06:59Z'>
		Apologies for the delay in response, Is this still an issue?
Can you please test with latest version of TensorFlow? Thanks!
		</comment>
		<comment id='6' author='TimCapes' date='2020-04-22T00:08:31Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='7' author='TimCapes' date='2020-04-29T01:06:40Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='8' author='TimCapes' date='2020-04-29T01:06:43Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32821&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32821&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='TimCapes' date='2020-04-29T02:06:38Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
	</comments>
</bug>