<bug id='32895' author='payne4handsome' open_date='2019-09-29T02:09:34Z' closed_time='2020-07-11T22:55:11Z'>
	<summary>Decorated the call methods of tf.keras.Model subclass with @tf.function or not , result is very different</summary>
	<description>
System information
OS Platform and Distribution
Mac os (10.14.6)
TensorFlow installed from (source or binary):
binary
TensorFlow version (use command below):
2.0.0rc1
Python version:
Python 3.6.4
I implement a  model with tensorflow keras just copy &lt;denchmark-link:https://www.tensorflow.org/tutorials/quickstart/advanced&gt;official tutorials&lt;/denchmark-link&gt;
. The code is shown below
&lt;denchmark-code&gt;import numpy as np
from tensorflow.keras.layers import Dense, Flatten, Conv2D
from tensorflow.keras import Model
import tensorflow as tf
import time
mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train, x_test
y_train, y_test = y_train , y_test
x_train = x_train[..., tf.newaxis].astype(np.float64)
x_test = x_test[..., tf.newaxis].astype(np.float64)
train_ds = tf.data.Dataset.from_tensor_slices(
(x_train, y_train)).shuffle(1000).batch(256)
test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(256)

class MyModel(Model):
    def __init__(self, *args, **kwargs):
        super(MyModel, self).__init__(args, kwargs)
        self.conv1 = Conv2D(32, 3, activation='relu')
        self.flatten = Flatten()
        self.d1 = Dense(128, activation='relu')
        self.d2 = Dense(10, activation='softmax')
        
    def call(self, x):
        x = self.conv1(x)
        x = self.flatten(x)
        x = self.d1(x)
        return self.d2(x)
model = MyModel()
model.build((512, 28, 28, 1))
loss_object = tf.keras.losses.SparseCategoricalCrossentropy()
optimizer = tf.keras.optimizers.Adam()
train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')

test_loss = tf.keras.metrics.Mean(name='test_loss')
test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')
@tf.function
def train_step(images, labels):
    with tf.GradientTape() as tape:
        predictions = model(images)
        loss = loss_object(labels, predictions)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    train_loss(loss)
    train_accuracy(labels, predictions)
@tf.function
def test_step(images, labels):
    predictions = model(images)
    t_loss = loss_object(labels, predictions)

    test_loss(t_loss)
    test_accuracy(labels, predictions)

EPOCHS = 100

for epoch in range(EPOCHS):
    start = time.time()
    for images, labels in train_ds:
        train_step(tf.cast(images, tf.float32), labels)

    model.reset_metrics()
    for test_images, test_labels in test_ds:
        test_step(tf.cast(test_images, tf.float32), test_labels)
    end = time.time()
    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}, cost:{}'
    print(
        template.format(epoch + 1, train_loss.result(),
                        train_accuracy.result() * 100, test_loss.result(),
                        test_accuracy.result() * 100, end -start))
&lt;/denchmark-code&gt;

The result show below
&lt;denchmark-code&gt;Epoch 1, Loss: 3.311713457107544, Accuracy: 89.99500274658203, Test Loss: 0.22570940852165222, Test Accuracy: 95.55000305175781, cost:14.249950170516968
Epoch 2, Loss: 1.7273597717285156, Accuracy: 93.3933334350586, Test Loss: 0.18705108761787415, Test Accuracy: 96.28499603271484, cost:13.976628065109253
Epoch 3, Loss: 1.1731806993484497, Accuracy: 95.00333404541016, Test Loss: 0.1706559658050537, Test Accuracy: 96.61333465576172, cost:14.125600099563599
Epoch 4, Loss: 0.8890712261199951, Accuracy: 95.9800033569336, Test Loss: 0.1615695059299469, Test Accuracy: 96.78250122070312, cost:14.998674154281616
Epoch 5, Loss: 0.7157263159751892, Accuracy: 96.64299774169922, Test Loss: 0.15943895280361176, Test Accuracy: 96.91000366210938, cost:14.496413230895996
Epoch 6, Loss: 0.5996174216270447, Accuracy: 97.10027313232422, Test Loss: 0.15636563301086426, Test Accuracy: 97.01667022705078, cost:12.977428197860718
Epoch 7, Loss: 0.5162842273712158, Accuracy: 97.44285583496094, Test Loss: 0.15558230876922607, Test Accuracy: 97.07571411132812, cost:13.014786005020142
Epoch 8, Loss: 0.4553721249103546, Accuracy: 97.66041564941406, Test Loss: 0.1616506278514862, Test Accuracy: 97.0574951171875, cost:14.170269966125488
Epoch 9, Loss: 0.4086601734161377, Accuracy: 97.81925964355469, Test Loss: 0.16302895545959473, Test Accuracy: 97.0955581665039, cost:13.114216804504395
Epoch 10, Loss: 0.3699580430984497, Accuracy: 97.98149871826172, Test Loss: 0.16444820165634155, Test Accuracy: 97.10900115966797, cost:12.783933162689209
Epoch 11, Loss: 0.3377893567085266, Accuracy: 98.12287902832031, Test Loss: 0.1649542897939682, Test Accuracy: 97.1427230834961, cost:12.907387256622314
Epoch 12, Loss: 0.31051450967788696, Accuracy: 98.2509765625, Test Loss: 0.16677971184253693, Test Accuracy: 97.17666625976562, cost:12.56903600692749
Epoch 13, Loss: 0.28760311007499695, Accuracy: 98.35563659667969, Test Loss: 0.168002188205719, Test Accuracy: 97.21846008300781, cost:13.040626049041748
Epoch 14, Loss: 0.26825159788131714, Accuracy: 98.44261932373047, Test Loss: 0.17205245792865753, Test Accuracy: 97.22142791748047, cost:13.130248785018921
Epoch 15, Loss: 0.2515304386615753, Accuracy: 98.51310729980469, Test Loss: 0.17627017199993134, Test Accuracy: 97.23332977294922, cost:12.66043996810913
Epoch 16, Loss: 0.2367033064365387, Accuracy: 98.58124542236328, Test Loss: 0.17778924107551575, Test Accuracy: 97.26062774658203, cost:12.836369037628174
Epoch 17, Loss: 0.2236185222864151, Accuracy: 98.6434326171875, Test Loss: 0.17880629003047943, Test Accuracy: 97.2752914428711, cost:12.845327854156494
Epoch 18, Loss: 0.21163013577461243, Accuracy: 98.706298828125, Test Loss: 0.18086740374565125, Test Accuracy: 97.29389190673828, cost:13.29944920539856
Epoch 19, Loss: 0.20079432427883148, Accuracy: 98.76526641845703, Test Loss: 0.18283464014530182, Test Accuracy: 97.31105041503906, cost:12.92448115348816
Epoch 20, Loss: 0.19131030142307281, Accuracy: 98.81291198730469, Test Loss: 0.18425647914409637, Test Accuracy: 97.32350158691406, cost:12.959301948547363
.....
&lt;/denchmark-code&gt;

But I find can  Decorated the call methods with call method from &lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/function#used_in_the_tutorials&gt;this&lt;/denchmark-link&gt;
.
so I add annotation @tf.function to call method with above code(The rest of the code remains unchanged), just like this
&lt;denchmark-code&gt;   @tf.function
    def call(self, x):
        x = self.conv1(x)
        x = self.flatten(x)
        x = self.d1(x)
        return self.d2(x)
&lt;/denchmark-code&gt;

The result show below
&lt;denchmark-code&gt;Epoch 1, Loss: 11.509495735168457, Accuracy: 28.50666618347168, Test Loss: 11.525819778442383, Test Accuracy: 28.400001525878906, cost:14.542086839675903
Epoch 2, Loss: 11.499279022216797, Accuracy: 28.60249900817871, Test Loss: 11.579718589782715, Test Accuracy: 28.064998626708984, cost:13.102718114852905
Epoch 3, Loss: 11.471456527709961, Accuracy: 28.78444480895996, Test Loss: 11.532645225524902, Test Accuracy: 28.369998931884766, cost:13.346691846847534
Epoch 4, Loss: 11.456918716430664, Accuracy: 28.886667251586914, Test Loss: 11.518753051757812, Test Accuracy: 28.497499465942383, cost:13.706462860107422
Epoch 5, Loss: 11.445549011230469, Accuracy: 28.961334228515625, Test Loss: 11.492920875549316, Test Accuracy: 28.6560001373291, cost:13.56296968460083
Epoch 6, Loss: 11.424226760864258, Accuracy: 29.0897216796875, Test Loss: 11.270418167114258, Test Accuracy: 30.030000686645508, cost:13.60814118385315
Epoch 7, Loss: 11.216507911682129, Accuracy: 30.37714385986328, Test Loss: 11.099614143371582, Test Accuracy: 31.09000015258789, cost:13.579961061477661
Epoch 8, Loss: 11.050827980041504, Accuracy: 31.407499313354492, Test Loss: 10.950920104980469, Test Accuracy: 32.01874923706055, cost:13.574744701385498
Epoch 9, Loss: 10.918838500976562, Accuracy: 32.227962493896484, Test Loss: 10.83558464050293, Test Accuracy: 32.7400016784668, cost:12.771528959274292
Epoch 10, Loss: 10.808745384216309, Accuracy: 32.91266632080078, Test Loss: 10.74200439453125, Test Accuracy: 33.32600021362305, cost:12.579218864440918
Epoch 11, Loss: 10.719698905944824, Accuracy: 33.46500015258789, Test Loss: 10.672236442565918, Test Accuracy: 33.759090423583984, cost:12.781628131866455
Epoch 12, Loss: 10.644211769104004, Accuracy: 33.93402862548828, Test Loss: 10.609025955200195, Test Accuracy: 34.154998779296875, cost:13.060226202011108
Epoch 13, Loss: 10.57997989654541, Accuracy: 34.33307647705078, Test Loss: 10.551304817199707, Test Accuracy: 34.51692199707031, cost:13.11635136604309
Epoch 14, Loss: 10.525317192077637, Accuracy: 34.671546936035156, Test Loss: 10.500534057617188, Test Accuracy: 34.834999084472656, cost:13.43892216682434
Epoch 15, Loss: 10.476855278015137, Accuracy: 34.972442626953125, Test Loss: 10.458395957946777, Test Accuracy: 35.099334716796875, cost:13.528913021087646
Epoch 16, Loss: 10.435961723327637, Accuracy: 35.22645950317383, Test Loss: 10.421695709228516, Test Accuracy: 35.329376220703125, cost:13.542529821395874
Epoch 17, Loss: 10.397627830505371, Accuracy: 35.46509552001953, Test Loss: 10.387943267822266, Test Accuracy: 35.541175842285156, cost:12.953328132629395
Epoch 18, Loss: 10.317543983459473, Accuracy: 35.95990753173828, Test Loss: 10.273320198059082, Test Accuracy: 36.2488899230957, cost:13.799224138259888
Epoch 19, Loss: 10.211501121520996, Accuracy: 36.61640167236328, Test Loss: 10.166360855102539, Test Accuracy: 36.910526275634766, cost:13.552597045898438
Epoch 20, Loss: 10.113394737243652, Accuracy: 37.22416687011719, Test Loss: 10.071305274963379, Test Accuracy: 37.49850082397461, cost:12.61729907989502
......
&lt;/denchmark-code&gt;

When you compare these two results, The first code is obviously better than the second. I've done many experiments, and the results are the same. The first get better acc and loss just after one epoch, but the second will iterated many time from a not so good result and finally can get a better result.
so my question are :
(1) What causes these two different results？ because I have added @tf.function annotation to the method train_step, so whether I add  @tf.function annotation to call method or not , the result should the same.
(2) tensorflow keras will run with graph model automatic，why add  @tf.function annotation to the call method in &lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/function#used_in_the_tutorials&gt;this api doc&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='payne4handsome' date='2019-09-30T08:59:06Z'>
		&lt;denchmark-link:https://github.com/payne4handsome&gt;@payne4handsome&lt;/denchmark-link&gt;
 ,
When tried executing the code in TF-rc2 i got the output as present in the &lt;denchmark-link:https://colab.sandbox.google.com/gist/oanush/d76aa20e3777359100449f80dc6d15ee/32895.ipynb&gt;gist&lt;/denchmark-link&gt;
 of colab, kindly take a look at that same.Thanks!
		</comment>
		<comment id='2' author='payne4handsome' date='2019-09-30T10:30:19Z'>
		&lt;denchmark-link:https://github.com/oanush&gt;@oanush&lt;/denchmark-link&gt;
 You didn't understand me and executed the wrong code.  Below code should replace call function that in class MyModel.
&lt;denchmark-code&gt;@tf.function
    def call(self, x):
        x = self.conv1(x)
        x = self.flatten(x)
        x = self.d1(x)
        return self.d2(x)
&lt;/denchmark-code&gt;

I mean you should execute code with two cases. one is not decorated call function with @tf.function, another is decorated call function with @tf.function.
first case
&lt;denchmark-code&gt;class MyModel(Model):
    def __init__(self, *args, **kwargs):
        super(MyModel, self).__init__(args, kwargs)
        self.conv1 = Conv2D(32, 3, activation='relu')
        self.flatten = Flatten()
        self.d1 = Dense(128, activation='relu')
        self.d2 = Dense(10, activation='softmax')
    
    def call(self, x):
        x = self.conv1(x)
        x = self.flatten(x)
        x = self.d1(x)
        return self.d2(x)
&lt;/denchmark-code&gt;

second case
&lt;denchmark-code&gt;class MyModel(Model):
    def __init__(self, *args, **kwargs):
        super(MyModel, self).__init__(args, kwargs)
        self.conv1 = Conv2D(32, 3, activation='relu')
        self.flatten = Flatten()
        self.d1 = Dense(128, activation='relu')
        self.d2 = Dense(10, activation='softmax')

    @tf.function
    def call(self, x):
        x = self.conv1(x)
        x = self.flatten(x)
        x = self.d1(x)
        return self.d2(x)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='payne4handsome' date='2019-10-01T10:52:27Z'>
		Please find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/oanush/e1ad3080cdd020c076f3bca11fae5b0f/32895.ipynb&gt;gist&lt;/denchmark-link&gt;
 of the colab replicating the issue for TF-2.0rc2.Thanks!
		</comment>
		<comment id='4' author='payne4handsome' date='2019-11-04T14:19:57Z'>
		
Please find the gist of the colab replicating the issue for TF-2.0rc2.Thanks!

This isn't correct. I am facing the same issue. Decorating call method with @tf.function doesn't helps and rather hurts the performance.
P.S. I was able to solve this by normalizing the data and few other changes. Please follow this link to check my solution &lt;denchmark-link:https://colab.research.google.com/gist/sourcecode369/daaef199cbf90a0f7491df31c01b364a/tensorflow-2-0-training-with-tf-function.ipynb&gt;https://colab.research.google.com/gist/sourcecode369/daaef199cbf90a0f7491df31c01b364a/tensorflow-2-0-training-with-tf-function.ipynb&lt;/denchmark-link&gt;

Thank you.
		</comment>
		<comment id='5' author='payne4handsome' date='2020-03-24T06:24:10Z'>
		&lt;denchmark-link:https://github.com/payne4handsome&gt;@payne4handsome&lt;/denchmark-link&gt;

please let us know if the issue still persist in nightly
		</comment>
		<comment id='6' author='payne4handsome' date='2020-03-31T11:55:49Z'>
		&lt;denchmark-link:https://github.com/payne4handsome&gt;@payne4handsome&lt;/denchmark-link&gt;

please update as per above comment
		</comment>
		<comment id='7' author='payne4handsome' date='2020-04-01T06:23:34Z'>
		&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;

Yes, the issue still persist in nightly.
Please follow this link to replicate this issue.
&lt;denchmark-link:https://colab.research.google.com/drive/19hYQVHxVK310e9KT4Wz1-PathzwkY9xC&gt;https://colab.research.google.com/drive/19hYQVHxVK310e9KT4Wz1-PathzwkY9xC&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='payne4handsome' date='2020-04-01T11:09:35Z'>
		&lt;denchmark-link:https://github.com/payne4handsome&gt;@payne4handsome&lt;/denchmark-link&gt;

i have run the gist shared by you on nightly and there are no errors, please find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/23705cc14b2ab78014af887d9d3ac1e0/32895.ipynb&gt;gist here&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='payne4handsome' date='2020-04-01T11:20:44Z'>
		&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;

Please compare the results that have great differences.  First result  accuracy is 97.86444091796875 after 15 epochs. Second result  accuracy is 47.27222442626953 after 15 epochs
		</comment>
		<comment id='10' author='payne4handsome' date='2020-04-16T05:23:29Z'>
		&lt;denchmark-link:https://github.com/payne4handsome&gt;@payne4handsome&lt;/denchmark-link&gt;
 In a subclassed model, decorating call method with a tf.function is fine if you are using model.fit method. I tried a subclass model with mnist data as you had used, but used model.fit and decorated call method with tf.function and didn't notice any drop in performance. Please check the &lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/2aafc0386bd723d8b3f3e417198d9211/mnist_subclassed.ipynb&gt;gist here&lt;/denchmark-link&gt;
.
However, decorating small code snippets using tf.function will lead to lot of performance overhead and is not recommended. If you are training using custom training loops, we recommend decorating the training step function with tf.function. When I tried your code with tf.function decorated call method, I noticed drop in performance. Please check the &lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/09d0fb4f1692b997b95281bd3df6064a/untitled92.ipynb&gt;gist here&lt;/denchmark-link&gt;
.
Can you please try model.fit method with subclass model or remove tf.function decoration on call method. Thanks!
		</comment>
		<comment id='11' author='payne4handsome' date='2020-04-16T05:49:51Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
  I know exactly how to solve this problem. I mean this is a bug.
		</comment>
		<comment id='12' author='payne4handsome' date='2020-04-16T10:08:33Z'>
		I tried it locally and I can also observe a difference in accuracy. I wonder where it comes from.
Regarding the performance of tf.function: Are you talking about additional compile time in the beginning or about training performance? If training performance suffers that would be good to know... Even in tf-addons there are some functions that are decorated with tf.function (see also the discussion in &lt;denchmark-link:https://github.com/tensorflow/addons/pull/28&gt;tensorflow/addons#28&lt;/denchmark-link&gt;
 ).
		</comment>
		<comment id='13' author='payne4handsome' date='2020-04-16T14:37:23Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 Can you please take a look at this issue? Any insights on the root-cause of the issue. Thanks!
		</comment>
		<comment id='14' author='payne4handsome' date='2020-04-16T18:13:05Z'>
		&lt;denchmark-link:https://github.com/omalleyt12&gt;@omalleyt12&lt;/denchmark-link&gt;
 and I spoke and we believe the difference is owed to the final Softmax layer.
In general, softmax with cross-entropy loss is prone to numerical instabilities, and the usual practice is to omit the softmax activation and use tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True). Normally, Keras will replace that automatically for you, but when you wrap the code with tf.function, it can't do it, so instead you need to do it explicitly in the code.
		</comment>
		<comment id='15' author='payne4handsome' date='2020-04-16T18:13:07Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32895&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32895&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='16' author='payne4handsome' date='2020-04-16T21:32:33Z'>
		With the modifications suggested by &lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 results are much better and are similar to the other approach (without @tf.function). I am getting accuracy around 97% in 15 epochs. Please check the &lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/d801056670534d90b75a899c56ec11f2/untitled92.ipynb&gt;gist here&lt;/denchmark-link&gt;
.
Please verify once and close the issue if this was resolved for you. Thanks!
		</comment>
		<comment id='17' author='payne4handsome' date='2020-06-25T17:52:35Z'>
		
Please check the gist here.

&lt;denchmark-link:https://github.com/payne4handsome&gt;@payne4handsome&lt;/denchmark-link&gt;
,
Could you please check &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
's comment above and let us know if it helps. Thanks!
		</comment>
		<comment id='18' author='payne4handsome' date='2020-07-02T18:19:26Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='19' author='payne4handsome' date='2020-07-11T22:55:10Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='20' author='payne4handsome' date='2020-07-11T22:55:13Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32895&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32895&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>