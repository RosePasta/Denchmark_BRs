<bug id='7' author='AvantiShri' open_date='2015-11-09T16:24:31Z' closed_time='2015-12-16T06:46:16Z'>
	<summary>API docs does not list RNNs</summary>
	<description>
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/nn.md&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/nn.md&lt;/denchmark-link&gt;

(It has convolutional layers listed, for instance, but does not show the RNNs. Actually, I don't quite see anything corresponding to a fully-connected layer either)
	</description>
	<comments>
		<comment id='1' author='AvantiShri' date='2015-11-09T16:44:27Z'>
		You should look at the RNN tutorial: &lt;denchmark-link:http://tensorflow.org/tutorials/recurrent/index.md&gt;http://tensorflow.org/tutorials/recurrent/index.md&lt;/denchmark-link&gt;
 .
		</comment>
		<comment id='2' author='AvantiShri' date='2015-11-09T18:07:38Z'>
		(Thanks, I have seen the tutorial but it is not a substitute for an API; I filed the issue after consulting with a friend at Google Brain)
		</comment>
		<comment id='3' author='AvantiShri' date='2015-11-09T18:28:46Z'>
		Hi Avanti -- internally we've been working on iterating the API for RNNs, and we were happy enough with the current API to use it in the tutorial, but we're making sure it's solid before promoting it to the public API, since we'd then have to support it indefinitely.  (Anything not in the public API is a work-in-progress :)
We'll keep this bug open in the meantime, and for now you can look at the source code documentation if you're interested in playing around: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/rnn.py#L9&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/rnn.py#L9&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='AvantiShri' date='2015-11-09T18:44:27Z'>
		Ah, got it, thanks for the explanation.
		</comment>
		<comment id='5' author='AvantiShri' date='2015-11-15T17:24:50Z'>
		The white paper of TensorFlow mentions looping control within the graph. Is it already available? If so, are there examples to show how it can be done?
The &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/rnn.py#L86&gt;RNN example&lt;/denchmark-link&gt;
 has a Python loop. Will TensorFlow treat that as a symbolic loop and compile it?
Also, the explanation of  &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/rnn.py#L30&gt;here&lt;/denchmark-link&gt;
 isn't clear to me. What does it mean by  calculations? When  is past , can it just break from the loop instead of continuing with  state? Returning  state is different from returning the state at , isn't it?
		</comment>
		<comment id='6' author='AvantiShri' date='2015-11-15T21:12:13Z'>
		On your first question, see &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/208&gt;#208&lt;/denchmark-link&gt;
.
On your second question: the core TF engine currently only sees the GraphDef produced by python, so the RNN example is an unrolled one today.
I'm not super familiar with that RNN example -- &lt;denchmark-link:https://github.com/lukaszkaiser&gt;@lukaszkaiser&lt;/denchmark-link&gt;
 or &lt;denchmark-link:https://github.com/ludimagister&gt;@ludimagister&lt;/denchmark-link&gt;
 might know better.
		</comment>
		<comment id='7' author='AvantiShri' date='2015-11-15T22:45:59Z'>
		I zer0n,
the current RNN is statically unrolled, there is no (not yet) dynamic unrolling based on the length of the sequence. Th dynamic calculation means the graph is unrolled up to the max_sequence_length, but if a sequence_length is provided the calculations on the unrolled graph are cut short once the sequence_length is reached, using a conditional op. Depending on the application this may result in shorter processing time.
		</comment>
		<comment id='8' author='AvantiShri' date='2015-11-16T17:45:56Z'>
		Yes, to add to what &lt;denchmark-link:https://github.com/ludimagister&gt;@ludimagister&lt;/denchmark-link&gt;
 says: the conditional op will plug in zeros to the output &amp; state past max(sequence_length), thus reducing the total amount of computation (if not memory).
I may actually modify this so that instead of plugging in zeros to the state, it just copies the state.  This way the final state will represent the "final" state at max(sequence_length).  However, I'm undecided on this.  If you want the final state at time sequence_length, you can concat the state vectors and use transpose() followed by gather() with sequence_length in order to pull out the states you care about.  That's probably what you would want to do, in fact, because if you have batch_size = 2 and sequence_length = [1, 2], then for the first minibatch entry, the state at max(sequence_length) will not equal the state at sequence_length[0].
An alternative solution is to right-align your inputs so that they always "end" on the final time step.  This breaks down the dynamic calculation performed when you pass sequence_length (because it assumes left-aligned inputs).  I may extend this by adding a bool flag like "right_aligned" to the rnn call, which assumes that calculation starts at len(inputs) - max(sequence_length), and copies the initial state through appropriately.  But that doesn't exist now.
		</comment>
		<comment id='9' author='AvantiShri' date='2015-11-16T18:18:27Z'>
		Thanks &lt;denchmark-link:https://github.com/vrv&gt;@vrv&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/ludimagister&gt;@ludimagister&lt;/denchmark-link&gt;
, and &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 for the answers. However, some details still confuse me.

@ludimagister, the code doesn't seem to statically unroll. It has a loop which depends on the length of the inputs. Plus, max_sequence_length is not a const; instead it's just the scalar of the sequence_length parameter, which can be and is None by default. So, by default, the unrolling is not truncated. Correct me if I misread the code.
@ebrevdo I understand the computational saving motivation. However, returning zeros is logically very different from returning the state at sequence_length (if provided). The former is just wrong. Again, please correct if I misread the code.

Thanks.
		</comment>
		<comment id='10' author='AvantiShri' date='2015-11-16T20:00:35Z'>
		&lt;denchmark-link:https://github.com/zer0n&gt;@zer0n&lt;/denchmark-link&gt;
 It depends on your task.
Returning zeros is fine if you only care about outputs (i.e., you're not hooking up to a decoder); and your loss function knows to ignore outputs past the sequence_length.
Returning the state from the end of the last time step might also be considered "wrong", but will generally always happen if you have inputs of different lengths (and aren't performing dynamic computation).  This is a typical approach to performing RNN with minibatches.  For this reason when performing encoding/decoding, people usually right-align with left-side padding instead, so the last input of any example always corresponds to the very last state.  This seems like the cleanest solution for now.
Anyway, this part of the API may change; not sure yet the best approach.
		</comment>
		<comment id='11' author='AvantiShri' date='2015-11-16T20:08:08Z'>
		(also, specifically returning the state at sequence_length for every entry is taxing both in terms of computation and in terms of memory, both in short supply with RNNs )
		</comment>
		<comment id='12' author='AvantiShri' date='2015-11-16T22:11:17Z'>
		OK, I did miss the line outputs.append(output). I originally thought that it returned the final state, not a sequence of states.
Anyway, this implementation still looks weird (I'm aware it's changing so I'm only discussing the current state). Usually, for truncated BPTT implementation, people pad eos for short sentences and truncate the sentences if the lengths are larger than max_length. This enables static unrolling and efficient mini-batching.
The RNN example seems doing the reverse. What I see is that it's doing dynamic unrolling (i.e. with dynamic output size), but padding zeros to the outputs past max_length.
		</comment>
		<comment id='13' author='AvantiShri' date='2015-11-16T22:25:33Z'>
		(This discussion is probably better off had on the discussion mailing list, rather than this bug about documentation)
		</comment>
		<comment id='14' author='AvantiShri' date='2015-11-17T21:48:30Z'>
		&lt;denchmark-link:https://github.com/vrv&gt;@vrv&lt;/denchmark-link&gt;
:

but we're making sure it's solid before promoting it to the public API, since we'd then have to support it indefinitely.

May we assume that TF is going to use semantic versioning for &lt;denchmark-link:/tensorflow/tensorflow/releases&gt;releases&lt;/denchmark-link&gt;
 (ie. major.minor.patch)?
Major releases can have backward-incompatible API changes, and minor releases can certainly add a new API (or extend an existing one in a backward-compatible way) especialy if it marks the old API as deprecated (and to be removed in the next major release).
Since TF has not yet had a major version release, (the current release is only &lt;denchmark-link:/tensorflow/tensorflow/releases/tag/0.5.0&gt;0.5.0&lt;/denchmark-link&gt;
), you have a lot of wiggle room between now and an eventual 1.0.0 release that then really would commit you to maintaining backward compatibility for quite a while.
		</comment>
		<comment id='15' author='AvantiShri' date='2015-12-16T06:46:16Z'>
		&lt;denchmark-link:https://github.com/webmaven&gt;@webmaven&lt;/denchmark-link&gt;
: We are going to be using semver, and we will publish exactly what we mean by that too. We'll try reasonably hard to maintain API stability even before 1.0.0, especially in the parts of the API that are official. For now, we have decided that something becomes "official" when it shows up on the API docs. You'll notice that many other functions are documented but not included in the docs, that means their interface is still in flux.
I'm closing this bug for now.
		</comment>
		<comment id='16' author='AvantiShri' date='2015-12-18T18:47:15Z'>
		&lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
:

We'll try reasonably hard to maintain API stability even before 1.0.0, especially in the parts of the API that are official. For now, we have decided that something becomes "official" when it shows up on the API docs.

Hmm. OK, but wouldn't putting APIs in the docs and marking them as 'draft' or 'unofficial' get you valuable external feedback while you're still iterating on an API? Or is external feedback only wanted/needed after an API becomes 'official'?
		</comment>
		<comment id='17' author='AvantiShri' date='2015-12-18T18:56:49Z'>
		Yeah, we're definitely considering adding something like this since it helps to get community feedback on some experimental APIs before they are fully ready.
		</comment>
		<comment id='18' author='AvantiShri' date='2015-12-20T16:45:03Z'>
		&lt;denchmark-link:https://github.com/vrv&gt;@vrv&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
, so getting back to the original topic, is the RNN API currently a good candidate for such 'draft' treatment, and does that mean this issue on documentation should be re-opened?
		</comment>
		<comment id='19' author='AvantiShri' date='2015-12-21T05:25:29Z'>
		We are reconsidering the api, deciding if we should make the current
rnncell implementations stateful, or whether to store some of the non
variable state in graph collections.  After that decision is made, we will
likely add documentation.
On Dec 20, 2015 6:45 AM, "Michael R. Bernstein" &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;

wrote:

@vrv https://github.com/vrv and @martinwicke
https://github.com/martinwicke, so getting back to the original topic,
is the RNN API currently a good candidate for such 'draft' treatment, and
does that mean this issue on documentation should be re-opened?
—
Reply to this email directly or view it on GitHub
#7 (comment)
.

		</comment>
		<comment id='20' author='AvantiShri' date='2015-12-26T22:50:05Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 what are the considerations surrounding this decision between stateful rnncell implementations vs. storing the state in the graph collections?
		</comment>
		<comment id='21' author='AvantiShri' date='2015-12-27T20:09:27Z'>
		The main consideration is in how calculations common across time are
cached.  Simpler caching is traded off against having RNNCell be a pure
logic object (non-stateful).
For example, if you access two Variables at each time step and then concat
them, using the result in your call calculation, then this is something
that should be cached beforehand because it creates redundant computation.
The two approaches to caching are:

RNNCell is stateful: create and cache this Tensor inside the RNNCell
object
RNNCell is non-stateful: call looks for the cached Tensor inside a
graph collection; if it doesn't exist, it creates it (similar to using
get_variable).

With a stateful RNNCell, Variables are created when the RNNCell is created;
and so that variable scope is used.  With a non-stateful RNNCell, Variables
are created / accessed during call and the variable scope used is
whatever it was when you ran rnn() or bidirectional_rnn() or whatever.
Because of this, moving from non-stateful RNNCell to a stateful one (and
modifying the associated implementations of LSTM, GRU, etc cells) would be
a breaking change.
I personally prefer stateful objects, because it's easier to understand and
debug them.  But there are arguments in both directions that have to be
considered.
On Sat, Dec 26, 2015 at 2:50 PM, Michael R. Bernstein &lt;
&lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;
&gt; wrote:

@ebrevdo https://github.com/ebrevdo what are the considerations
surrounding this decision between stateful rnncell implementations vs.
storing the state in the graph collections?
—
Reply to this email directly or view it on GitHub
#7 (comment)
.

		</comment>
		<comment id='22' author='AvantiShri' date='2015-12-28T22:55:35Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
:

[snip explanation]
But there are arguments in both directions that have to be considered.

What are those arguments?
		</comment>
		<comment id='23' author='AvantiShri' date='2015-12-29T21:13:17Z'>
		For example:
Moving to stateful objects now would break a bunch of external dependencies.  Of course, this is an undocumented API and therefore folks should expect it to break in their projects.  However, I'm afraid of breaking external projects in subtle ways that don't emit errors.  This indeed may happen with this change.  Especially for those who depth-stack RNNs on top of each other using the same instance.
In addition, there are those who argue that the RNNCell should continue to be a purely logical object with no state, so you can reuse the same instance of RNNCell across multiple RNNs without fear of reusing the same variable in multiple places (though get_variable's checks for over-sharing may ameliorate this somewhat).
EDIT: scratch that last sentence.  the get_variable would then be called only once in the RNNCell's initialization, and all those get_variable protections would go out the door :(.
		</comment>
		<comment id='24' author='AvantiShri' date='2015-12-29T23:30:40Z'>
		So, you can't actually reuse the same RNNCell instance across multiple RNNs in either case?
		</comment>
		<comment id='25' author='AvantiShri' date='2015-12-30T01:31:07Z'>
		Currently you can.  You can also use it with a shared name scope to tie the
parameters across multiple rnns.
On Dec 29, 2015 3:31 PM, "Michael R. Bernstein" &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;

wrote:

So, you can't actually reuse the same RNNCell across multiple RNNs in
either case?
—
Reply to this email directly or view it on GitHub
#7 (comment)
.

		</comment>
		<comment id='26' author='AvantiShri' date='2016-01-05T23:51:57Z'>
		OK, so you can currently reuse an instance of RNNCell across multiple RNNs without worrying about reusing the same variable in multiple places.
Does using a shared name scope with a single RNNCell instance across multiple RNNs gain you anything? Or is that really for reusing parameters across multiple RNNCell instances in multiple RNNs?
		</comment>
		<comment id='27' author='AvantiShri' date='2016-01-06T05:48:06Z'>
		Right. It gains you the ability to tie parameters not only within one lstm,
but also across multiple lstms.
On Jan 5, 2016 3:52 PM, "Michael R. Bernstein" &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;

wrote:

OK, so you can currently reuse an instance of RNNCell across multiple
RNNs without worrying about reusing the same variable in multiple places.
Does using a shared name scope with a single RNNCell instance across
multiple RNNs gain you anything? Or is that really for reusing parameters
across multiple RNNCell instances in multiple RNNs?
—
Reply to this email directly or view it on GitHub
#7 (comment)
.

		</comment>
	</comments>
</bug>