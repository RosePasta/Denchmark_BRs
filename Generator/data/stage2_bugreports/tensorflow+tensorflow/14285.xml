<bug id='14285' author='albertz' open_date='2017-11-06T10:33:10Z' closed_time='2017-11-07T13:05:02Z'>
	<summary>TF 1.4.0 on MacOSX: crash, object was probably modified after being freed</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOSX 10.13
TensorFlow installed from (source or binary): binary, via pip3.6 install tensorflow
TensorFlow version (use command below): v1.4.0-rc1-11-g130a514 1.4.0
Python version:  3.6.3, via Homebrew

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

TensorFlow crashes in some cases. This occurred only now with version TF 1.4.0. It is a test of my test suite (&lt;denchmark-link:https://github.com/rwth-i6/returnn/blob/4a69d0a1e74fb1ac7f76fc8c27694d906f9a8642/tests/test_TFEngine.py#L459&gt;this one&lt;/denchmark-link&gt;
). I can try to come up with a reduced test case but maybe the current information is already enough to identify the problem.
On the terminal, I see this:
&lt;denchmark-code&gt;Python(60770,0x70000fafb000) malloc: *** error for object 0x7fb861518b48: incorrect checksum for freed object - object was probably modified after being freed.
*** set a breakpoint in malloc_error_break to debug
fish: Job 1, 'python3 tests/test_TFEngine.py test_engine_train_simple_attention' terminated by signal SIGABRT (Abort)
&lt;/denchmark-code&gt;

The crashed thread stacktrace:
&lt;denchmark-code&gt;Thread 15 Crashed:
0   libsystem_kernel.dylib        	0x00007fff7c559fce __pthread_kill + 10
1   libsystem_pthread.dylib       	0x00007fff7c697150 pthread_kill + 333
2   libsystem_c.dylib             	0x00007fff7c4b632a abort + 127
3   libsystem_malloc.dylib        	0x00007fff7c5beb28 szone_error + 596
4   libsystem_malloc.dylib        	0x00007fff7c5c9ea5 tiny_free_no_lock + 2439
5   libsystem_malloc.dylib        	0x00007fff7c5ca254 free_tiny + 628
6   libtensorflow_framework.so    	0x00000001091927c2 tensorflow::Tensor::~Tensor() + 50
7   libtensorflow_framework.so    	0x00000001095d1b2e tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) + 5646
8   libtensorflow_framework.so    	0x00000001095d9d90 std::__1::__function::__func&lt;std::__1::__bind&lt;void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&amp;, long long&amp;&gt;, std::__1::allocator&lt;std::__1::__bind&lt;void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&amp;, long long&amp;&gt; &gt;, void ()&gt;::operator()() + 80
9   libtensorflow_framework.so    	0x0000000109277fc2 Eigen::NonBlockingThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WorkerLoop(int) + 1922
10  libtensorflow_framework.so    	0x0000000109277734 std::__1::__function::__func&lt;tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function&lt;void ()&gt;)::'lambda'(), std::__1::allocator&lt;tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function&lt;void ()&gt;)::'lambda'()&gt;, void ()&gt;::operator()() + 52
11  libtensorflow_framework.so    	0x000000010929a9c0 void* std::__1::__thread_proxy&lt;std::__1::tuple&lt;std::__1::function&lt;void ()&gt; &gt; &gt;(void*) + 96
12  libsystem_pthread.dylib       	0x00007fff7c6946c1 _pthread_body + 340
13  libsystem_pthread.dylib       	0x00007fff7c69456d _pthread_start + 377
14  libsystem_pthread.dylib       	0x00007fff7c693c5d thread_start + 13
&lt;/denchmark-code&gt;

Alternatively, I sometimes get this crashed thread stacktrace:
&lt;denchmark-code&gt;Thread 12 Crashed:
0   libtensorflow_framework.so    	0x000000010e9fd5b7 tensorflow::Tensor::CheckTypeAndIsAligned(tensorflow::DataType) const + 71
1   _pywrap_tensorflow_internal.so	0x0000000108b16932 tensorflow::(anonymous namespace)::CheckNumericsOp&lt;Eigen::ThreadPoolDevice, float&gt;::Compute(tensorflow::OpKernelContext*) + 98
2   libtensorflow_framework.so    	0x000000010ee6d88d tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) + 301
3   libtensorflow_framework.so    	0x000000010ee3c82b tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) + 4875
4   libtensorflow_framework.so    	0x000000010ee44d90 std::__1::__function::__func&lt;std::__1::__bind&lt;void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&amp;, long long&amp;&gt;, std::__1::allocator&lt;std::__1::__bind&lt;void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&amp;, long long&amp;&gt; &gt;, void ()&gt;::operator()() + 80
5   libtensorflow_framework.so    	0x000000010eae2fc2 Eigen::NonBlockingThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WorkerLoop(int) + 1922
6   libtensorflow_framework.so    	0x000000010eae2734 std::__1::__function::__func&lt;tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function&lt;void ()&gt;)::'lambda'(), std::__1::allocator&lt;tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function&lt;void ()&gt;)::'lambda'()&gt;, void ()&gt;::operator()() + 52
7   libtensorflow_framework.so    	0x000000010eb059c0 void* std::__1::__thread_proxy&lt;std::__1::tuple&lt;std::__1::function&lt;void ()&gt; &gt; &gt;(void*) + 96
8   libsystem_pthread.dylib       	0x00007fff7c6946c1 _pthread_body + 340
9   libsystem_pthread.dylib       	0x00007fff7c69456d _pthread_start + 377
10  libsystem_pthread.dylib       	0x00007fff7c693c5d thread_start + 13
&lt;/denchmark-code&gt;

Maybe the main thread stacktrace is also relevant:
&lt;denchmark-code&gt;Thread 0:: Dispatch queue: com.apple.main-thread
0   libsystem_kernel.dylib        	0x00007fff7c559e7e __psynch_cvwait + 10
1   libsystem_pthread.dylib       	0x00007fff7c695662 _pthread_cond_wait + 732
2   libc++.1.dylib                	0x00007fff7a449cb0 std::__1::condition_variable::wait(std::__1::unique_lock&lt;std::__1::mutex&gt;&amp;) + 18
3   _pywrap_tensorflow_internal.so	0x000000010dc2d23b nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) + 363
4   _pywrap_tensorflow_internal.so	0x000000010dc29c97 nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) + 423
5   _pywrap_tensorflow_internal.so	0x000000010dc2a3d1 nsync::nsync_cv_wait(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*) + 49
6   _pywrap_tensorflow_internal.so	0x000000010dc3771b tensorflow::DirectSession::WaitForNotification(tensorflow::Notification*, long long) + 107
7   _pywrap_tensorflow_internal.so	0x000000010dc332a6 tensorflow::DirectSession::WaitForNotification(tensorflow::DirectSession::RunState*, tensorflow::CancellationManager*, long long) + 38
8   _pywrap_tensorflow_internal.so	0x000000010dc2fe3e tensorflow::DirectSession::Run(tensorflow::RunOptions const&amp;, std::__1::vector&lt;std::__1::pair&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, tensorflow::Tensor&gt;, std::__1::allocator&lt;std::__1::pair&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, tensorflow::Tensor&gt; &gt; &gt; const&amp;, std::__1::vector&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, std::__1::allocator&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;, std::__1::vector&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, std::__1::allocator&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;, std::__1::vector&lt;tensorflow::Tensor, std::__1::allocator&lt;tensorflow::Tensor&gt; &gt;*, tensorflow::RunMetadata*) + 3438
9   _pywrap_tensorflow_internal.so	0x000000010bf0827e TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::__1::vector&lt;std::__1::pair&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, tensorflow::Tensor&gt;, std::__1::allocator&lt;std::__1::pair&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, tensorflow::Tensor&gt; &gt; &gt; const&amp;, std::__1::vector&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, std::__1::allocator&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;, TF_Tensor**, std::__1::vector&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, std::__1::allocator&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;, TF_Buffer*, TF_Status*) + 750
10  _pywrap_tensorflow_internal.so	0x000000010bf07eb6 TF_Run + 1286
11  _pywrap_tensorflow_internal.so	0x000000010bc990b3 tensorflow::TF_Run_wrapper_helper(TF_DeprecatedSession*, char const*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector&lt;char const*, 8&gt; const&amp;, tensorflow::gtl::InlinedVector&lt;char const*, 8&gt; const&amp;, TF_Status*, tensorflow::gtl::InlinedVector&lt;_object*, 8&gt;*, TF_Buffer*) + 1683
12  _pywrap_tensorflow_internal.so	0x000000010bc997e4 tensorflow::TF_Run_wrapper(TF_DeprecatedSession*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector&lt;char const*, 8&gt; const&amp;, tensorflow::gtl::InlinedVector&lt;char const*, 8&gt; const&amp;, TF_Status*, tensorflow::gtl::InlinedVector&lt;_object*, 8&gt;*, TF_Buffer*) + 52
13  _pywrap_tensorflow_internal.so	0x000000010bc5b885 _wrap_TF_Run(_object*, _object*) + 1861
14  org.python.python             	0x000000010797b5dd _PyCFunction_FastCallDict + 166
...
&lt;/denchmark-code&gt;

The full MacOSX crash report with the stacktrace of all threads can be seen &lt;denchmark-link:https://gist.github.com/albertz/6f92b691d6025f47f8ec3a738a8ba970&gt;here&lt;/denchmark-link&gt;
.
On another run, I also got this stacktrace:
&lt;denchmark-code&gt;Crashed Thread:        0  Dispatch queue: com.apple.main-thread

Exception Type:        EXC_BAD_ACCESS (SIGSEGV)
Exception Codes:       EXC_I386_GPFLT
Exception Note:        EXC_CORPSE_NOTIFY

Termination Signal:    Segmentation fault: 11
Termination Reason:    Namespace SIGNAL, Code 0xb
Terminating Process:   exc handler [0]

Thread 0 Crashed:: Dispatch queue: com.apple.main-thread
0   libtensorflow_framework.so    	0x0000000115d21a1e tensorflow::(anonymous namespace)::AddArgToSig(tensorflow::NodeDef const&amp;, tensorflow::OpDef_ArgDef const&amp;, tensorflow::gtl::InlinedVector&lt;tensorflow::DataType, 4&gt;*) + 78
1   libtensorflow_framework.so    	0x0000000115d21942 tensorflow::InOutTypesForNode(tensorflow::NodeDef const&amp;, tensorflow::OpDef const&amp;, tensorflow::gtl::InlinedVector&lt;tensorflow::DataType, 4&gt;*, tensorflow::gtl::InlinedVector&lt;tensorflow::DataType, 4&gt;*) + 82
2   libtensorflow_framework.so    	0x0000000115d22829 tensorflow::ValidateNodeDef(tensorflow::NodeDef const&amp;, tensorflow::OpDef const&amp;) + 1881
3   _pywrap_tensorflow_internal.so	0x0000000110f83e16 tensorflow::graph::ValidateGraphDef(tensorflow::GraphDef const&amp;, tensorflow::OpRegistryInterface const&amp;) + 134
4   _pywrap_tensorflow_internal.so	0x0000000110eef9c3 tensorflow::GraphExecutionState::Extend(tensorflow::GraphDef const&amp;, std::__1::unique_ptr&lt;tensorflow::GraphExecutionState, std::__1::default_delete&lt;tensorflow::GraphExecutionState&gt; &gt;*) const + 2147
5   _pywrap_tensorflow_internal.so	0x0000000110d18e03 tensorflow::DirectSession::ExtendLocked(tensorflow::GraphDef const&amp;) + 131
6   _pywrap_tensorflow_internal.so	0x0000000110d18eb3 tensorflow::DirectSession::Extend(tensorflow::GraphDef const&amp;) + 67
7   _pywrap_tensorflow_internal.so	0x000000010eff04f6 TF_ExtendGraph + 102
8   _pywrap_tensorflow_internal.so	0x000000010ed43b71 _wrap_TF_ExtendGraph(_object*, _object*) + 273
9   org.python.python             	0x000000010d57d5dd _PyCFunction_FastCallDict + 166
...
&lt;/denchmark-code&gt;

And yet another stacktrace:
&lt;denchmark-code&gt;Thread 13 Crashed:
0   _pywrap_tensorflow_internal.so	0x0000000107a4016a tensorflow::TTypes&lt;int, 3ul, long&gt;::ConstTensor tensorflow::Tensor::bit_casted_shaped&lt;int, 3ul&gt;(tensorflow::gtl::ArraySlice&lt;long long&gt;) const + 42
1   _pywrap_tensorflow_internal.so	0x0000000107a45a6e void tensorflow::HandleStridedSliceGradCase&lt;Eigen::ThreadPoolDevice, float, 3&gt;(tensorflow::OpKernelContext*, tensorflow::gtl::ArraySlice&lt;long long&gt; const&amp;, tensorflow::gtl::ArraySlice&lt;long long&gt; const&amp;, tensorflow::gtl::ArraySlice&lt;long long&gt; const&amp;, tensorflow::TensorShape const&amp;, bool, tensorflow::Tensor*) + 302
2   _pywrap_tensorflow_internal.so	0x00000001079ff7ac tensorflow::StridedSliceGradOp&lt;Eigen::ThreadPoolDevice, float&gt;::Compute(tensorflow::OpKernelContext*) + 2556
3   libtensorflow_framework.so    	0x000000010644788d tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) + 301
4   libtensorflow_framework.so    	0x000000010641682b tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) + 4875
5   libtensorflow_framework.so    	0x000000010641ed90 std::__1::__function::__func&lt;std::__1::__bind&lt;void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&amp;, long long&amp;&gt;, std::__1::allocator&lt;std::__1::__bind&lt;void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&amp;, long long&amp;&gt; &gt;, void ()&gt;::operator()() + 80
6   libtensorflow_framework.so    	0x00000001060bcfc2 Eigen::NonBlockingThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WorkerLoop(int) + 1922
7   libtensorflow_framework.so    	0x00000001060bc734 std::__1::__function::__func&lt;tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function&lt;void ()&gt;)::'lambda'(), std::__1::allocator&lt;tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function&lt;void ()&gt;)::'lambda'()&gt;, void ()&gt;::operator()() + 52
8   libtensorflow_framework.so    	0x00000001060df9c0 void* std::__1::__thread_proxy&lt;std::__1::tuple&lt;std::__1::function&lt;void ()&gt; &gt; &gt;(void*) + 96
9   libsystem_pthread.dylib       	0x00007fff7c6946c1 _pthread_body + 340
10  libsystem_pthread.dylib       	0x00007fff7c69456d _pthread_start + 377
11  libsystem_pthread.dylib       	0x00007fff7c693c5d thread_start + 13
&lt;/denchmark-code&gt;

Or this:
&lt;denchmark-code&gt;Thread 12 Crashed:
0   libsystem_kernel.dylib        	0x00007fff7c55a1ea __semwait_signal_nocancel + 10
1   libsystem_c.dylib             	0x00007fff7c460097 nanosleep$NOCANCEL + 188
2   libsystem_c.dylib             	0x00007fff7c488931 usleep$NOCANCEL + 53
3   libsystem_c.dylib             	0x00007fff7c4b6334 abort + 137
4   libsystem_malloc.dylib        	0x00007fff7c5beb28 szone_error + 596
5   libsystem_malloc.dylib        	0x00007fff7c5b3658 tiny_malloc_from_free_list + 1155
6   libsystem_malloc.dylib        	0x00007fff7c5b2403 szone_malloc_should_clear + 422
7   libsystem_malloc.dylib        	0x00007fff7c5b2201 malloc_zone_malloc + 103
8   libsystem_malloc.dylib        	0x00007fff7c5b150b malloc + 24
9   libc++abi.dylib               	0x00007fff7a49b628 operator new(unsigned long) + 40
10  _pywrap_tensorflow_internal.so	0x0000000108c9bc16 tensorflow::ApplyAdamOp&lt;Eigen::ThreadPoolDevice, float&gt;::Compute(tensorflow::OpKernelContext*) + 70
11  libtensorflow_framework.so    	0x000000010e45488d tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) + 301
12  libtensorflow_framework.so    	0x000000010e42382b tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) + 4875
13  libtensorflow_framework.so    	0x000000010e42bd90 std::__1::__function::__func&lt;std::__1::__bind&lt;void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&amp;, long long&amp;&gt;, std::__1::allocator&lt;std::__1::__bind&lt;void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&amp;, long long&amp;&gt; &gt;, void ()&gt;::operator()() + 80
14  libtensorflow_framework.so    	0x000000010e0c9fc2 Eigen::NonBlockingThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WorkerLoop(int) + 1922
15  libtensorflow_framework.so    	0x000000010e0c9734 std::__1::__function::__func&lt;tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function&lt;void ()&gt;)::'lambda'(), std::__1::allocator&lt;tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function&lt;void ()&gt;)::'lambda'()&gt;, void ()&gt;::operator()() + 52
16  libtensorflow_framework.so    	0x000000010e0ec9c0 void* std::__1::__thread_proxy&lt;std::__1::tuple&lt;std::__1::function&lt;void ()&gt; &gt; &gt;(void*) + 96
17  libsystem_pthread.dylib       	0x00007fff7c6946c1 _pthread_body + 340
18  libsystem_pthread.dylib       	0x00007fff7c69456d _pthread_start + 377
19  libsystem_pthread.dylib       	0x00007fff7c693c5d thread_start + 13
&lt;/denchmark-code&gt;

This might be related to our own C++ operation which has worked fine so far (we used it since TF 0.8), although of course this might be triggered only now by some race condition. Is there anything new I need to take care of? I think this NSync stuff is new?
	</description>
	<comments>
		<comment id='1' author='albertz' date='2017-11-06T10:45:58Z'>
		I have had a use-after-free bug before when some tensors were the wrong size. They were fed into an Eigen operation that only checks the size in debug mode. When I rebuilt in debug mode I got an Eigen assertion that pointed to the actual issue.
		</comment>
		<comment id='2' author='albertz' date='2017-11-06T22:08:47Z'>
		libsystem_malloc.dylib seems wrong; if you're using the pip package TensorFlow should be using jemalloc. Maybe &lt;denchmark-link:https://github.com/rwth-i6/returnn/blob/148d103e13ebecb83c0d40539ae84eadabc14873/NativeOp.cpp#L499&gt;this symbol&lt;/denchmark-link&gt;
 is leaking into TensorFlow since you &lt;denchmark-link:https://github.com/rwth-i6/returnn/blob/b53a0a5c1c6acd0235fde2adcfeb8fb20b213fa8/TFNativeOp.py#L88&gt;load the op with RTLD_GLOBAL&lt;/denchmark-link&gt;
? If so, you could just link the op to libtensorflow_framework.so and load with RTLD_LOCAL.
		</comment>
		<comment id='3' author='albertz' date='2017-11-07T09:20:49Z'>
		Thanks for the answers. &lt;denchmark-link:https://github.com/allenlavoie&gt;@allenlavoie&lt;/denchmark-link&gt;
  The symbol you refer to is inside  (a macro trick to make  from the outer scope available inside the functions there), so I don't think that symbol causes confusion.
However, it's good that you mention libsystem_malloc.dylib. So, you assume the malloc from libsystem_malloc overwrites the malloc from libtensorflow_framework?
Should it also use jemalloc for operator new?
Are you sure it should use jemalloc on MacOSX? When looking into libtensorflow_framework, I see lots of references to _malloc, which I think should resolve to the one in libsystem_malloc.
The RTLD_GLOBAL loaded lib is from tensorflow.contrib.rnn.python.ops and not my own. I wanted to access some symbols from that op (LSTMBlock utilities) inside my own op. Do you think that will cause the problem? But why exactly? Maybe I should link instead to it? otool -L contrib/rnn/python/ops/_lstm_ops.so gives me:
&lt;denchmark-code&gt;	@rpath/_lstm_ops.so (compatibility version 0.0.0, current version 0.0.0)
	@rpath/libtensorflow_framework.so (compatibility version 0.0.0, current version 0.0.0)
	/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1226.10.1)
&lt;/denchmark-code&gt;

The same for libtensorflow_framework.so gives me:
&lt;denchmark-code&gt;	@rpath/libtensorflow_framework.so (compatibility version 0.0.0, current version 0.0.0)
	/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1226.10.1)
	/System/Library/Frameworks/IOKit.framework/Versions/A/IOKit (compatibility version 1.0.0, current version 275.0.0)
&lt;/denchmark-code&gt;

And libSystem.B.dylib links to /usr/lib/system/libsystem_malloc.dylib.
Edit: I just removed that RTLD_GLOBAL lib loading as I didn't need it anyway for the CPU-only version of my op. It made no difference. So that is not related to the problem.
Edit 2: Also note, this whole workaround is there basically to solve this. Related is this issue.
Edit 3: The problem also happens when I remove any of the custom sgemm_ calls, which might have been related. But the crash still happens.
My own custom op, I load via  (&lt;denchmark-link:https://github.com/rwth-i6/returnn/blob/e06729e54131d4122d550bd0d576750b966a3f38/TFUtil.py#L2171&gt;here&lt;/denchmark-link&gt;
), so I guess it will use .
		</comment>
		<comment id='4' author='albertz' date='2017-11-07T13:05:02Z'>
		The commit I just did seems to fix it. It was actually not a bug in TensorFlow but in my code. I kept using the pointer tensor-&gt;shape().dim_sizes().data() which becomes invalid after the statement evaluation is finished. I wonder if that was valid before or not. Anyway, this seemed to be the problem and it doesn't crash anymore. So I'm closing this issue for now, unless it will happen again.
		</comment>
	</comments>
</bug>