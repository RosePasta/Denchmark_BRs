<bug id='38889' author='xuxiangsun' open_date='2020-04-25T13:42:25Z' closed_time='2020-06-19T06:53:03Z'>
	<summary>why does tensorflow2 use multiple Gpu but only one is used, the other always can not use</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
TensorFlow version (use command below):
Python version:
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"

Describe the current behavior
Describe the expected behavior
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
When I run my code to train, I found only one gpu is used while the others can not fully used. Exactly, for example, when the first gpu almost out of memory,  the code can not use the memory of other gpus. I don't know if it is a bug?
	</description>
	<comments>
		<comment id='1' author='xuxiangsun' date='2020-04-25T13:43:04Z'>
		&lt;denchmark-link:https://github.com/tensorflow&gt;@tensorflow&lt;/denchmark-link&gt;
 help!
		</comment>
		<comment id='2' author='xuxiangsun' date='2020-04-25T18:08:57Z'>
		By default, tensorflow uses the GPU:0 as the default GPU. I would suggest first check the available GPUs using tf.config.list_physical_devices to see the number of GPUs available.
If you have more than one GPU available then you need to do distributed GPU training.
This can be easily done by defining a  These scopes are given in &lt;denchmark-link:https://www.tensorflow.org/guide/distributed_training&gt;this&lt;/denchmark-link&gt;
 documentation link.
Please use a suitable strategy as per needs. In the simple case of having one machine with multiple GPUs, I guess the tf.distribute.MirroredStrategy will be useful. Please check the above docs for more info.
		</comment>
		<comment id='3' author='xuxiangsun' date='2020-04-28T12:29:21Z'>
		@KurtSunxx
Could you please update as per above comment.
		</comment>
		<comment id='4' author='xuxiangsun' date='2020-04-28T13:29:01Z'>
		Sorry.I forget release my setting.
CUDA:CUDA10.0
CUDNN:cudnn 7.6.5
Tensorflow:2.0
gcc:5.4
my code like the function: train_step() in &lt;denchmark-link:https://tensorflow.google.cn/tutorials/generative/pix2pix&gt;https://tensorflow.google.cn/tutorials/generative/pix2pix&lt;/denchmark-link&gt;

but in my train_step, I call a model(named classifier) in function A, while there still call the same model in function B of function A.So in short, the flow chart of my code is like the following:












The OOM happens when use func1 in train_step.The method to build a model like &lt;denchmark-link:https://tensorflow.google.cn/tutorials/generative/pix2pix&gt;https://tensorflow.google.cn/tutorials/generative/pix2pix&lt;/denchmark-link&gt;
 in which, I use a tf.keras.Model class.``
		</comment>
		<comment id='5' author='xuxiangsun' date='2020-04-28T13:31:47Z'>
		
@KurtSunxx
Could you please update as per above comment.

Sorry Sir. I dont know update what. Cause my english is realy poor. I just added some setting of my device and software environment. Is this information is helpfull?
		</comment>
		<comment id='6' author='xuxiangsun' date='2020-04-28T13:35:05Z'>
		
By default, tensorflow uses the GPU:0 as the default GPU. I would suggest first check the available GPUs using tf.config.list_physical_devices to see the number of GPUs available.
If you have more than one GPU available then you need to do distributed GPU training.
This can be easily done by defining a strategy.scope() These scopes are given in this documentation link.
Please use a suitable strategy as per needs. In the simple case of having one machine with multiple GPUs, I guess the tf.distribute.MirroredStrategy will be useful. Please check the above docs for more info.

Thanks Sir. I have tried this method. But it didn't work. By the way, Do you know if all the version of tensorflow(including tf1.x) also use one Gpu by default even if we set multiple GPUs to use? Cause I find that when I use tensorflow1.15, This also happens.
		</comment>
		<comment id='7' author='xuxiangsun' date='2020-04-28T14:07:51Z'>
		

By default, tensorflow uses the GPU:0 as the default GPU. I would suggest first check the available GPUs using tf.config.list_physical_devices to see the number of GPUs available.
If you have more than one GPU available then you need to do distributed GPU training.
This can be easily done by defining a strategy.scope() These scopes are given in this documentation link.
Please use a suitable strategy as per needs. In the simple case of having one machine with multiple GPUs, I guess the tf.distribute.MirroredStrategy will be useful. Please check the above docs for more info.

Thanks Sir. I have tried this method. But it didn't work. By the way, Do you know if all the version of tensorflow(including tf1.x) also use one Gpu by default even if we set multiple GPUs to use? Cause I find that when I use tensorflow1.15, This also happens.

Tensorflow does not do distributed computing by default. Even in v1 or in v2. By setting multiple GPU cards it won't mean that all will be used. I see that in your example you are writing a custom training loop instead of model.fit().
As per your system settings there is no issue with that part. Have a look at &lt;denchmark-link:https://tensorflow.google.cn/tutorials/distribute/custom_training&gt;this example &lt;/denchmark-link&gt;
 to use distributed training on custom loops.
		</comment>
		<comment id='8' author='xuxiangsun' date='2020-04-28T14:43:38Z'>
		


By default, tensorflow uses the GPU:0 as the default GPU. I would suggest first check the available GPUs using tf.config.list_physical_devices to see the number of GPUs available.
If you have more than one GPU available then you need to do distributed GPU training.
This can be easily done by defining a strategy.scope() These scopes are given in this documentation link.
Please use a suitable strategy as per needs. In the simple case of having one machine with multiple GPUs, I guess the tf.distribute.MirroredStrategy will be useful. Please check the above docs for more info.

Thanks Sir. I have tried this method. But it didn't work. By the way, Do you know if all the version of tensorflow(including tf1.x) also use one Gpu by default even if we set multiple GPUs to use? Cause I find that when I use tensorflow1.15, This also happens.

Tensorflow does not do distributed computing by default. Even in v1 or in v2. By setting multiple GPU cards it won't mean that all will be used. I see that in your example you are writing a custom training loop instead of model.fit().
As per your system settings there is no issue with that part. Have a look at this example  to use distributed training on custom loops.

Thanks Sir! I will try it soon! Best wishes to you.
		</comment>
		<comment id='9' author='xuxiangsun' date='2020-04-28T15:45:11Z'>
		


By default, tensorflow uses the GPU:0 as the default GPU. I would suggest first check the available GPUs using tf.config.list_physical_devices to see the number of GPUs available.
If you have more than one GPU available then you need to do distributed GPU training.
This can be easily done by defining a strategy.scope() These scopes are given in this documentation link.
Please use a suitable strategy as per needs. In the simple case of having one machine with multiple GPUs, I guess the tf.distribute.MirroredStrategy will be useful. Please check the above docs for more info.

Thanks Sir. I have tried this method. But it didn't work. By the way, Do you know if all the version of tensorflow(including tf1.x) also use one Gpu by default even if we set multiple GPUs to use? Cause I find that when I use tensorflow1.15, This also happens.

Tensorflow does not do distributed computing by default. Even in v1 or in v2. By setting multiple GPU cards it won't mean that all will be used. I see that in your example you are writing a custom training loop instead of model.fit().
As per your system settings there is no issue with that part. Have a look at this example  to use distributed training on custom loops.

Sorry Sir. maybe I can not make my idea clearly. I mean I had set 3 GPU to train, But when tensorflow need to allocate more memory, why it can not allocate the extra memory to other GPU cards I sepecified before? Is that can be solved by tf.distribute? For example, I have 3 GPUs whose memory is 11G, in fact, I found when I running my code, the first one uses almost 11G while the others usese 155M. And then tensorflow need allocate more memory, but the first one has on more memory to do. I think in this case, it should allocate memory to other GPUs, but in fact it just breakdown and return a OOM error. As far as I know, distribute training is like a parallel training while select a part of training procedure on one card and the other training procedure on the other cards. But it seems can not solve what I just asked above. I feel so puzzled.
		</comment>
		<comment id='10' author='xuxiangsun' date='2020-04-28T16:24:34Z'>
		Okay. So I get your problem. One of your GPUs is being used too much and you want your data to be shifted to other GPUs to avoid the OOM error.
Q. But when tensorflow need to allocate more memory, why it can not allocate the extra memory to other GPU cards I sepecified before?
A. Tensorflow does not allocate memory to other GPUs properly or synchronously if it is not in tf.distribute.
Q. Is that can be solved by tf.distribute?
A. Okay, tf.distribute provides several ways of dividing your data among various GPUs. It also supports synchronous distributed trining on GPUs.
tf.distribute will be able to handle synchronous distributed trining on GPUs which is required in your case. This will not be activated explicitly without specifying.
Getting OOM error is because you are unable to fit more data / model into your GPU currently.
Q. Will tf.distribute solve OOM ?
A. Maybe, it depends how huge your model is also how many operations can be parellized.
From the docs this is what MirroredStrategy will do
&lt;denchmark-code&gt;MirroredStrategy

tf.distribute.MirroredStrategy supports synchronous distributed training on multiple GPUs on one machine.

It creates one replica per GPU device. Each variable in the model is mirrored across all the replicas.

Together, these variables form a single conceptual variable called MirroredVariable. 

These variables are kept in sync with each other by applying identical updates.
&lt;/denchmark-code&gt;

please see the &lt;denchmark-link:https://tensorflow.google.cn/guide/distributed_training#types_of_strategies&gt;distributed training docs&lt;/denchmark-link&gt;
. It might solve your OOM problem but again it depends on your model size and training.
But I am sure this training strategy will be more efficient also significantly faster.
		</comment>
		<comment id='11' author='xuxiangsun' date='2020-04-29T01:50:33Z'>
		
Okay. So I get your problem. One of your GPUs is being used too much and you want your data to be shifted to other GPUs to avoid the OOM error.
Q. But when tensorflow need to allocate more memory, why it can not allocate the extra memory to other GPU cards I sepecified before?
A. Tensorflow does not allocate memory to other GPUs properly or synchronously if it is not in tf.distribute.
Q. Is that can be solved by tf.distribute?
A. Okay, tf.distribute provides several ways of dividing your data among various GPUs. It also supports synchronous distributed trining on GPUs.
tf.distribute will be able to handle synchronous distributed trining on GPUs which is required in your case. This will not be activated explicitly without specifying.
Getting OOM error is because you are unable to fit more data / model into your GPU currently.
Q. Will tf.distribute solve OOM ?
A. Maybe, it depends how huge your model is also how many operations can be parellized.
From the docs this is what MirroredStrategy will do
MirroredStrategy

tf.distribute.MirroredStrategy supports synchronous distributed training on multiple GPUs on one machine.

It creates one replica per GPU device. Each variable in the model is mirrored across all the replicas.

Together, these variables form a single conceptual variable called MirroredVariable. 

These variables are kept in sync with each other by applying identical updates.

please see the distributed training docs. It might solve your OOM problem but again it depends on your model size and training.
But I am sure this training strategy will be more efficient also significantly faster.

So nice you are! Thanks for your detailed answers! I will check and try it by myself!
		</comment>
		<comment id='12' author='xuxiangsun' date='2020-06-05T05:10:12Z'>
		@KurtSunxx
Please confirm if we may move this issue to closed status
		</comment>
		<comment id='13' author='xuxiangsun' date='2020-06-12T06:09:19Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='14' author='xuxiangsun' date='2020-06-19T06:52:53Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='15' author='xuxiangsun' date='2020-06-19T06:53:04Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38889&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38889&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>