<bug id='39753' author='Hacky-DH' open_date='2020-05-21T12:30:52Z' closed_time='2020-06-15T08:27:26Z'>
	<summary>MultiWorkerMirroredStrategy assign a wrong device</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  CentOS Linux release 7
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: nan
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0
Python version: 3.6.10
Bazel version (if compiling from source): nan
GCC/Compiler version (if compiling from source): nan
CUDA/cuDNN version: 10.1
GPU model and memory: nan

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"

Describe the current behavior
Use distribution strategy MultiWorkerMirroredStrategy in graph mode, throw an error
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation metrics/auc/Identity: {{node metrics/auc/Identity}} was explicitly assigned to /replica:0/task:0/device:CPU:0 but available devices are [ /job:worker/replica:0/task:1/device:CPU:0, /job:worker/replica:0/task:1/device:GPU:0, /job:worker/replica:0/task:1/device:XLA_CPU:0, /job:worker/replica:0/task:1/device:XLA_GPU:0 ]. Make sure the device specification refers to a valid device.
assign a wrong device which is /replica:0/task:0/device:CPU:0 on task1
Describe the expected behavior
train normly
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
# file demo.py
import sys
import numpy as np
import tensorflow as tf

strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
graph_context = tf.Graph().as_default()
strategy_context = strategy.scope()

with graph_context:
    with strategy_context:
        ip = tf.keras.layers.Input([2])
        h = tf.keras.layers.Dense(10, activation='relu', input_dim=2)(ip)
        out = tf.keras.layers.Dense(2, activation='softmax')(h)
        model = tf.keras.models.Model(inputs=[ip], outputs=[out])

        model.compile(optimizer='Adam',
                loss='categorical_crossentropy',
                metrics=[tf.keras.metrics.AUC(num_thresholds=100)])

    x = np.random.randn(100, 2)
    y = (x[:, 0] * x[:, 1]) &gt; 0
    model.fit(x, tf.keras.utils.to_categorical(y), epochs=1)
run above script command:
&lt;denchmark-code&gt;TF_CONFIG='{"cluster":{"worker":["localhost:2222","localhost:2223"]},"task":{"type":"worker","index":0}}' CUDA_VISIBLE_DEVICES=0 python demo.py
TF_CONFIG='{"cluster":{"worker":["localhost:2222","localhost:2223"]},"task":{"type":"worker","index":1}}' CUDA_VISIBLE_DEVICES=1 python demo.py
&lt;/denchmark-code&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1367, in _do_call
    return fn(*args)
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1350, in _run_fn
    self._extend_graph()
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1390, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation metrics/auc/Identity: {{node metrics/auc/Identity}} was explicitly assigned to /replica:0/task:0/device:CPU:0 but available devices are [ /job:worker/replica:0/task:1/device:CPU:0, /job:worker/replica:0/task:1/device:GPU:0, /job:worker/replica:0/task:1/device:XLA_CPU:0, /job:worker/replica:0/task:1/device:XLA_GPU:0 ]. Make sure the device specification refers to a valid device.
         [[metrics/auc/Identity]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "../x.py", line 22, in &lt;module&gt;
    model.fit(x, tf.keras.utils.to_categorical(y), epochs=1)
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 819, in fit
    use_multiprocessing=use_multiprocessing)
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 790, in fit
    *args, **kwargs)
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 777, in wrapper
    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py", line 853, in run_distribute_coordinator
    task_id, session_config, rpc_layer)
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py", line 360, in _run_single_worker
    return worker_fn(strategy)
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 772, in _worker_fn
    return method(model, **kwargs)
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 619, in fit
    epochs=epochs)
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 2200, in _distribution_standardize_user_data
    session = K.get_session()
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py", line 496, in get_session
    _initialize_variables(session)
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py", line 911, in _initialize_variables
    [variables_module.is_variable_initialized(v) for v in candidate_vars])
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 960, in run
    run_metadata_ptr)
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1183, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1361, in _do_run
    run_metadata)
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1386, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation metrics/auc/Identity: node metrics/auc/Identity (defined at ../x.py:18)  was explicitly assigned to /replica:0/task:0/device:CPU:0 but available devices are [ /job:worker/replica:0/task:1/device:CPU:0, /job:worker/replica:0/task:1/device:GPU:0, /job:worker/replica:0/task:1/device:XLA_CPU:0, /job:worker/replica:0/task:1/device:XLA_GPU:0 ]. Make sure the device specification refers to a valid device.
         [[metrics/auc/Identity]]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Hacky-DH' date='2020-05-22T10:17:38Z'>
		&lt;denchmark-link:https://github.com/Hacky-DH&gt;@Hacky-DH&lt;/denchmark-link&gt;

I ran your code on tf 2.1 and do  not face any error as reported by you, please refer to this &lt;denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/c590f7647c992bacefd61af1e474aa3c/untitled195.ipynb&gt;gist here&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='Hacky-DH' date='2020-05-22T13:17:54Z'>
		&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 use following commands to run the code
&lt;denchmark-code&gt;TF_CONFIG='{"cluster":{"worker":["localhost:2222","localhost:2223"]},"task":{"type":"worker","index":0}}' CUDA_VISIBLE_DEVICES=0 python demo.py
TF_CONFIG='{"cluster":{"worker":["localhost:2222","localhost:2223"]},"task":{"type":"worker","index":1}}' CUDA_VISIBLE_DEVICES=1 python demo.py
&lt;/denchmark-code&gt;

Just running python demo.py is one worker trainning, not multi worker
		</comment>
		<comment id='3' author='Hacky-DH' date='2020-06-08T07:24:41Z'>
		If you want to run with graph mode in TF2, please try the following and let us know if the issue is still there:
&lt;denchmark-code&gt;import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

with strategy.scope():
  model = .... # rest of your code as-is.

&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='Hacky-DH' date='2020-06-10T07:56:39Z'>
		&lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;

I modify code as you say, an error occurs:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "demo.py", line 14, in &lt;module&gt;
    h = tf.keras.layers.Dense(10, activation='relu', input_dim=2)(ip)
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 748, in __call__
    self._maybe_build(inputs)
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 2116, in _maybe_build
    self.build(input_shapes)
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/core.py", line 1113, in build
    trainable=True)
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 446, in add_weight
    caching_device=caching_device)
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py", line 744, in _add_variable_with_custom_getter
    **kwargs_for_getter)
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py", line 142, in make_variable
    shape=variable_shape if variable_shape else None)
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py", line 258, in __call__
    return cls._variable_v1_call(*args, **kwargs)
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py", line 219, in _variable_v1_call
    shape=shape)
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py", line 65, in getter
    return captured_getter(captured_previous, **kwargs)
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py", line 1319, in creator_with_resource_vars
    _require_strategy_scope_extended(self)
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py", line 253, in _require_strategy_scope_extended
    _wrong_strategy_scope(strategy, context)
  File "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py", line 218, in _wrong_strategy_scope
    (strategy,))
RuntimeError: Need to be inside "with strategy.scope()" for &lt;tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceStrategyV1 object at 0x7fcd8459c208&gt;
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='Hacky-DH' date='2020-06-10T08:13:28Z'>
		Can you please provide the complete code you tried? from the error message it seems that it thinks you're no longer in the scope of the strategy.
BTW do you need to use graph mode? Can you use TF2 without graph mode instead? We have done a lot more testing and improvements for MultiWorkerMirroredStrategy in TF2.
		</comment>
		<comment id='6' author='Hacky-DH' date='2020-06-12T09:11:28Z'>
		&lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
 here is full code that I modified
&lt;denchmark-code&gt;# file demo.py
import sys
import numpy as np
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
graph_context = tf.Graph().as_default()
strategy_context = strategy.scope()

with graph_context:
    with strategy_context:
        ip = tf.keras.layers.Input([2])
        h = tf.keras.layers.Dense(10, activation='relu', input_dim=2)(ip)
        out = tf.keras.layers.Dense(2, activation='softmax')(h)
        model = tf.keras.models.Model(inputs=[ip], outputs=[out])

        model.compile(optimizer='Adam',
                loss='categorical_crossentropy',
                metrics=[tf.keras.metrics.AUC(num_thresholds=100)])

    x = np.random.randn(100, 2)
    y = (x[:, 0] * x[:, 1]) &gt; 0
    model.fit(x, tf.keras.utils.to_categorical(y), epochs=1)
&lt;/denchmark-code&gt;

you can start with cmds:
&lt;denchmark-code&gt;TF_CONFIG='{"cluster":{"worker":["localhost:2222","localhost:2223"]},"task":{"type":"worker","index":0}}' CUDA_VISIBLE_DEVICES=0 python demo.py
TF_CONFIG='{"cluster":{"worker":["localhost:2222","localhost:2223"]},"task":{"type":"worker","index":1}}' CUDA_VISIBLE_DEVICES=1 python demo.py
&lt;/denchmark-code&gt;

I tried to run without graph context, it works, but a warning message occurs
&lt;denchmark-code&gt;W tensorflow/core/grappler/optimizers/data/auto_shard.cc:428] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Found an unshardable source dataset: name: "TensorSliceDataset/_2"
&lt;/denchmark-code&gt;

How to fix this?
BTW, what are differences between with graph and without graph context using distribute strategy e.g. MultiWorkerMirroredStrategy or MirroredStrategy?
Thanks
		</comment>
		<comment id='7' author='Hacky-DH' date='2020-06-14T01:35:14Z'>
		You don't need the graph context, I think make a separate graph is probably causing the problem you saw earlier. So running without that is the right thing to do.
Regarding that warning: when using model.fit with MultiWorkerMirroredStrategy, we try to shard your data across workers by sharding the files across them. but when the dataset doesn't read from files like in this case, we cannot do that, and that's all that warning means. I am guessing this is a toy example, you should not see it with a realistic dataset that reads from files.
You can read more about this:
&lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#dataset_sharding_and_batch_size&gt;https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#dataset_sharding_and_batch_size&lt;/denchmark-link&gt;

&lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy?version=nightly#experimental_distribute_dataset&gt;https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy?version=nightly#experimental_distribute_dataset&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='Hacky-DH' date='2020-06-15T03:19:41Z'>
		I had another question: is keras model run in eager mode without graph context in MultiWorkerMirroredStrategy scope? if not, is it run in tf.function?
Do I need pass run_eagerly=False to model.compile function?
		</comment>
		<comment id='9' author='Hacky-DH' date='2020-06-15T06:31:32Z'>
		In TF2, keras model.fit uses tf.function under the hood automatically. So you don't need to pass run_eagerly=False, it is False by default. run_eagerly is only needed if you actually need op by op execution.
High level question: why are you trying to use TF1 / legacy graph mode instead of just TF2 default?
		</comment>
		<comment id='10' author='Hacky-DH' date='2020-06-15T08:27:25Z'>
		Thank you very much!
I heard that it is much slower in eager mode than graph mode in TF2. So I put model.compile and dataset construction forcely in graph context.
As you said I will try remove the graph context in TF2.
		</comment>
		<comment id='11' author='Hacky-DH' date='2020-06-15T08:27:27Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39753&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39753&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>