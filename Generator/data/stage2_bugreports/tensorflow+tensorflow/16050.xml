<bug id='16050' author='frank-wei' open_date='2018-01-11T19:59:20Z' closed_time='2019-08-28T21:24:51Z'>
	<summary>Eigen assertion when running on GPU with debug enabled</summary>
	<description>
I used r1.5 release version to compile in debug mode. The build command is
&lt;denchmark-code&gt;bazel build -c opt --config cuda -c dbg --strip=never  //tensorflow/tools/pip_package:build_pip_package
&lt;/denchmark-code&gt;

I tested the tutorial/mnist/mnist_deep.py and it got assertion below. I searched the forum and it seems that there is no clear answer for it. Thanks.
===========================
Answer the questions below:
Have I written custom code:No
OS Platform and Distribution:ubuntu 16.04
TensorFlow installed from: official
TensorFlow version: r1.5
Bazel version: 0.8
CUDA/cuDNN version: 9.0 / 7.0
GPU model and memory: P100, 16GB
Exact command to reproduce: as above
===========================
&lt;denchmark-code&gt;Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz
Saving graph to: /tmp/tmpis6Bjq
2018-01-11 11:45:43.003071: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-01-11 11:45:43.377683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2018-01-11 11:45:43.737737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-01-11 11:45:43.738343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 1 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:84:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2018-01-11 11:45:43.738437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Device peer to peer matrix
2018-01-11 11:45:43.738512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1126] DMA: 0 1
2018-01-11 11:45:43.738527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1136] 0:   Y N
2018-01-11 11:45:43.738535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1136] 1:   N Y
2018-01-11 11:45:43.738573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
2018-01-11 11:45:43.738589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:1) -&gt; (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)
step 0, training accuracy 0.08
python: external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:262: static void Eigen::internal::TensorExecutor&lt;Expression, Eigen::GpuDevice, Vectorizable&gt;::run(const Expression&amp;, const Eigen::GpuDevice&amp;) [with Expression = const Eigen::TensorAssignOp&lt;Eigen::TensorMap&lt;Eigen::Tensor&lt;float, 1, 1, long int&gt;, 16, Eigen::MakePointer&gt;, const Eigen::TensorCwiseBinaryOp&lt;Eigen::internal::scalar_sum_op&lt;float, float&gt;, const Eigen::TensorMap&lt;Eigen::Tensor&lt;float, 1, 1, long int&gt;, 16, Eigen::MakePointer&gt;, const Eigen::TensorCwiseBinaryOp&lt;Eigen::internal::scalar_product_op&lt;float, float&gt;, const Eigen::TensorBroadcastingOp&lt;const Eigen::array&lt;long int, 1ul&gt;, const Eigen::TensorReshapingOp&lt;const Eigen::Sizes&lt;1l&gt;, const Eigen::TensorCwiseBinaryOp&lt;Eigen::internal::scalar_difference_op&lt;const float, const float&gt;, const Eigen::TensorCwiseNullaryOp&lt;Eigen::internal::scalar_constant_op&lt;const float&gt;, const Eigen::TensorMap&lt;Eigen::TensorFixedSize&lt;const float, Eigen::Sizes&lt;&gt;, 1, long int&gt;, 16, Eigen::MakePointer&gt; &gt;, const Eigen::TensorMap&lt;Eigen::TensorFixedSize&lt;const float, Eigen::Sizes&lt;&gt;, 1, long int&gt;, 16, Eigen::MakePointer&gt; &gt; &gt; &gt;, const Eigen::TensorCwiseBinaryOp&lt;Eigen::internal::scalar_difference_op&lt;const float, const float&gt;, const Eigen::TensorMap&lt;Eigen::Tensor&lt;const float, 1, 1, long int&gt;, 16, Eigen::MakePointer&gt;, const Eigen::TensorMap&lt;Eigen::Tensor&lt;float, 1, 1, long int&gt;, 16, Eigen::MakePointer&gt; &gt; &gt; &gt; &gt;; bool Vectorizable = true]: Assertion `**cudaGetLastError() == cudaSuccess'** failed.
Aborted (core dumped)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='frank-wei' date='2018-01-12T06:59:05Z'>
		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
Have I written custom code
OS Platform and Distribution
TensorFlow installed from
TensorFlow version
Bazel version
CUDA/cuDNN version
GPU model and memory
Exact command to reproduce
		</comment>
		<comment id='2' author='frank-wei' date='2018-01-12T18:55:33Z'>
		I have updated in the problem description. Thank.
		</comment>
		<comment id='3' author='frank-wei' date='2018-01-31T02:00:23Z'>
		Thank you for reporting that! It could be indicative of a bug in our code.
		</comment>
		<comment id='4' author='frank-wei' date='2018-01-31T02:00:35Z'>
		&lt;denchmark-link:https://github.com/rmlarsen&gt;@rmlarsen&lt;/denchmark-link&gt;
 WDYT?
		</comment>
		<comment id='5' author='frank-wei' date='2018-02-08T09:52:27Z'>
		I have the same issue
		</comment>
		<comment id='6' author='frank-wei' date='2018-02-21T08:30:54Z'>
		I have the same issue, too.
I use the r1.5 release and build TF from source with the debugging compilation mode.
The mnist_deep program fails at the same point with the same cuda assertion message.
The same problem runs successfully when I recompile TF with the default compilation mode.
		</comment>
		<comment id='7' author='frank-wei' date='2018-03-20T11:16:12Z'>
		I also have this problem on tf1.6.
		</comment>
		<comment id='8' author='frank-wei' date='2018-04-12T08:08:53Z'>
		i have same issue.
i use the r1.4 and bazel build code is "bazel build --config=opt --config=cuda -c dbg //tensorflow/tools/pip_package:build_pip_package".
		</comment>
		<comment id='9' author='frank-wei' date='2018-06-12T02:47:26Z'>
		This issue seems to be still present at the master
		</comment>
		<comment id='10' author='frank-wei' date='2018-09-27T22:54:20Z'>
		Repros on r1.9 Ubuntu 16.04 when trying to run an example built with:
bazel build -c opt --config=cuda -c dbg --strip=never tensorflow/examples/multibox_detector/...
		</comment>
		<comment id='11' author='frank-wei' date='2018-10-24T23:03:34Z'>
		I got a similar error using r1.11 on Ubuntu 16.04, using the C++ api to run a pre-trained model from this repo &lt;denchmark-link:https://github.com/tensorflow/models/tree/master/research/object_detection&gt;https://github.com/tensorflow/models/tree/master/research/object_detection&lt;/denchmark-link&gt;
 (got the same error with several different models)
built with
&lt;denchmark-code&gt;  bazel build \
    --config=opt \
    --config=cuda \
    --config=monolithic \
    //tensorflow:libtensorflow_cc.so \
    //tensorflow/tools/pip_package:build_pip_package
&lt;/denchmark-code&gt;

error:
&lt;denchmark-code&gt;external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:262: static void Eigen::internal::TensorExecutor&lt;Expression, Eigen::GpuDevice, Vectorizable&gt;::run(const Expression&amp;, const Eigen::GpuDevice&amp;) [with Expression = const Eigen::TensorAssignOp&lt;Eigen::TensorMap&lt;Eigen::Tensor&lt;float, 2, 1, int&gt;, 16, Eigen::MakePointer&gt;, const Eigen::TensorCwiseBinaryOp&lt;Eigen::internal::scalar_difference_op&lt;float, float&gt;, const Eigen::TensorMap&lt;Eigen::Tensor&lt;const float, 2, 1, int&gt;, 16, Eigen::MakePointer&gt;, const Eigen::TensorBroadcastingOp&lt;const Eigen::array&lt;long int, 2ul&gt;, const Eigen::TensorMap&lt;Eigen::Tensor&lt;const float, 2, 1, int&gt;, 16, Eigen::MakePointer&gt; &gt; &gt; &gt;; bool Vectorizable = true]: Assertion `cudaGetLastError() == cudaSuccess' failed.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='12' author='frank-wei' date='2019-08-28T21:24:51Z'>
		This is a stale issue. Please check the issue with latest TensorFlow. If the issue still persists in the newer version of TF, please feel free to reopen it by providing details about the issue and a standalone code to reproduce the issue. Thanks!
		</comment>
		<comment id='13' author='frank-wei' date='2019-08-28T21:24:53Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=16050&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=16050&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>