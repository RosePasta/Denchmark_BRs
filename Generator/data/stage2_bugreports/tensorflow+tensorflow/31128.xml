<bug id='31128' author='JiayiFu' open_date='2019-07-29T12:21:26Z' closed_time='2019-08-03T11:20:13Z'>
	<summary>What is the right way to use intra_op_parallelism_threads and inter_op_parallelism_threads?</summary>
	<description>
Hi,
I create a Session with tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1).
When I run the Session, I use the top command to observe the situations. But I found the program still use 1700% CPU. Why did this happen? What's the right way to control the number of cores/threads used by tensorflow?
thx!
	</description>
	<comments>
		<comment id='1' author='JiayiFu' date='2019-07-29T14:14:22Z'>
		Could you set:
&lt;denchmark-code&gt;export OMP_NUM_THREADS=1
&lt;/denchmark-code&gt;

What's your cpu utilization now?
		</comment>
		<comment id='2' author='JiayiFu' date='2019-07-31T02:29:20Z'>
		
Could you set:
export OMP_NUM_THREADS=1

What's your cpu utilization now?

I set this in the shell and then run the same python program. The CPU utilization is still more than 1000%.
		</comment>
		<comment id='3' author='JiayiFu' date='2019-07-31T07:13:20Z'>
		

Could you set:
export OMP_NUM_THREADS=1

What's your cpu utilization now?

I set this in the shell and then run the same python program. The CPU utilization is still more than 1000%.

Could you share some code snippet you used?
		</comment>
		<comment id='4' author='JiayiFu' date='2019-07-31T08:15:16Z'>
		


Could you set:
export OMP_NUM_THREADS=1

What's your cpu utilization now?

I set this in the shell and then run the same python program. The CPU utilization is still more than 1000%.

Could you share some code snippet you used?

Sure. I was doing a unit test in python with my model. The model is converted to TensorFlow from another framework through ONNX.
&lt;denchmark-code&gt;config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)
__sess = tf.Session(graph=graph, config=config)
input_node = "input:0"
ivector_node = "ivector:0"
inputs = np.loadtxt("inputs.txt", dtype=np.float32)
ivector = np.loadtxt("ivector.txt", dtype=np.float32)
__feed_dict = {input_node: [inputs], ivector_node: [ivector]}
output_node = "output.affine:0"
__out_tensor = __sess.graph.get_tensor_by_name(output_node)
output = __sess.run(__out_tensor, __feed_dict)
&lt;/denchmark-code&gt;

When I run this snippet, the CPU usage is about 160% when it goes into session.run.
When I run the session in a loop like:
&lt;denchmark-code&gt;for _ in range(50):    
   _ = __sess.run(__out_tensor, __feed_dict)
&lt;/denchmark-code&gt;

The CPU usage will rise to near 1600%.
Thanks for your relay!  If there is anything else that needs to be provided, just tell me.
		</comment>
		<comment id='5' author='JiayiFu' date='2019-07-31T13:18:17Z'>
		&lt;denchmark-link:https://github.com/JiayiFu&gt;@JiayiFu&lt;/denchmark-link&gt;

I try setting OMP_NUM_THREADS, intra_op_parallelism_threads and inter_op_parallelism_threads to 1.  And the cpu utilization is almost 100%.
Is it possible you have more than one sessions(async) are running, such as data_session which makes your CPU usage to 160%?
		</comment>
		<comment id='6' author='JiayiFu' date='2019-07-31T15:43:42Z'>
		&lt;denchmark-link:https://github.com/Leslie-Fang&gt;@Leslie-Fang&lt;/denchmark-link&gt;

I try to simplify the code and it just creates a session and runs once like the followed snippet. The CPU usage still raises to more than 100% and when I do session.run in a loop, the usage raised to more than 1000%
&lt;denchmark-code&gt;config = tf.ConfigProto(intra_op_parallelism_threads=1,
                        inter_op_parallelism_threads=1)
sess = tf.Session(graph=graph, config=config)
output_node = "output.affine:0"
out_tensor = sess.graph.get_tensor_by_name(output_node)
_ = sess.run(out_tensor, feed_dict)
sess.close()
&lt;/denchmark-code&gt;

In your case, what's kind of model you use? Have you tried to do the session.run in a loop? and is there any difference in the CPU usage?
Btw, what is data_session you mentioned?
		</comment>
		<comment id='7' author='JiayiFu' date='2019-08-01T08:53:59Z'>
		&lt;denchmark-link:https://github.com/JiayiFu&gt;@JiayiFu&lt;/denchmark-link&gt;
 ,
In order to expedite the trouble-shooting process, please provide complete code snippet to reproduce the issue reported here. Thanks!
		</comment>
		<comment id='8' author='JiayiFu' date='2019-08-02T02:46:31Z'>
		
@JiayiFu ,
In order to expedite the trouble-shooting process, please provide complete code snippet to reproduce the issue reported here. Thanks!

Thanks for your advice! This is the complete code snippet. It just loads a model and then runs many times. If you need the model, I would like to upload it. But I guess the model shouldn't be the reason that the CPU usage raised to more than 100%.
&lt;denchmark-code&gt;import os

import numpy as np
import tensorflow as tf

def __test_cnn(pb_path):
  graph_def = tf.GraphDef()
  with tf.gfile.GFile(pb_path, "rb") as f:
    graph_def.ParseFromString(f.read())
    tf.import_graph_def(graph_def, name='')
  
  os.environ["CUDA_VISIBLE_DEVICES"] = ""
  os.environ["OMP_NUM_THREADS"] = "1"
  config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)
  with tf.Session() as sess:
    input_node = "input:0"
    ivector_node = "ivector:0"
    output_name = "output.affine:0"
    output_tensor = sess.graph.get_tensor_by_name(output_name)
    input_value = np.random.rand(1,79, 43)
    ivector_value = np.random.rand(1,79, 100)
    feed_dict = {input_node: [input_value], ivector_node: [ivector_value]}
    _ = sess.run(out_tensor, feed_dict)
   
    for _ in range(50):
      _ = sess.run(out_tensor, feed_dict)
&lt;/denchmark-code&gt;

My OS version is Ubuntu 16.04. The TF is 1.14. The Python version is 3.5.2.
		</comment>
		<comment id='9' author='JiayiFu' date='2019-08-02T03:07:39Z'>
		Is it possible that the effects of inter and intra depend on the installation way? by pip or building from source?
		</comment>
		<comment id='10' author='JiayiFu' date='2019-08-03T11:20:13Z'>
		I tried these on another server and reinstalled the TensorFlow of it. It seems that everything is back to normal...Maybe there is something wrong in my envriment.
		</comment>
	</comments>
</bug>