<bug id='26792' author='ablanch5' open_date='2019-03-16T18:10:34Z' closed_time='2019-03-18T17:58:26Z'>
	<summary>Tensorflow Keras not adding trainable variable to Model in TF 1.13 (works in TF 1.12)</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.13
Python version: 3.6 (also tried 3.7)
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: GeForce 840M 8gigs ram

You can collect some of this information using our environment capture &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
Describe the current behavior
I am trying to add a trainable variable y to my model. The following code adds a trainable variable y to my model in TF 1.12. It does not work in TF 1.13
&lt;denchmark-code&gt;y = K.variable([0.0], dtype=tf.float32, name='y')
add_y = Lambda(lambda x: tf.math.add(x,y))
add_y.trainable_weights.append(y)
&lt;/denchmark-code&gt;

Describe the expected behavior
Model should be updated with the variable y as a trainable weight
Code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow.keras.backend as K
from tensorflow.keras import Model
from tensorflow.keras.layers import Input, Lambda
inputs = Input(shape=(1,))
y = K.variable([0.0], dtype=tf.float32, name='x')
add_y = Lambda(lambda x: tf.math.add(x,y))
add_y.trainable_weights.append(y)
outputs = add_y(inputs)
model = Model(inputs=inputs, outputs=outputs)
model.summary()
&lt;/denchmark-code&gt;

Model.summary() shows no trainable variables
Other info / logs
NONE
	</description>
	<comments>
		<comment id='1' author='ablanch5' date='2019-03-18T17:58:26Z'>
		Hi &lt;denchmark-link:https://github.com/ablanch5&gt;@ablanch5&lt;/denchmark-link&gt;
, that's bc in TF1.13  is a computed property.
I have a few ideas of what you could do here. The easiest is that, as of the latest nightly (but not as of TF1.13), TF Keras Lambda Layers can actually include variables, so you could do:
&lt;denchmark-code&gt;add_y = Lambda(lambda x: tf.math.add(x,K.variable([0.0], dtype=tf.float32, name='x')))
&lt;/denchmark-code&gt;

The most robust way to do this though (and the way that would work in all versions of Keras) is to create your own subclass Layer:
&lt;denchmark-code&gt;class Bias(tf.keras.layers.layer):
  def build(self, input_shape):
    self.bias = self.add_weight(shape=(), initializer='zeros', dtype=tf.float32, name='x')

  def call(self, inputs):
    return inputs + self.bias
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='ablanch5' date='2019-03-19T17:13:21Z'>
		Thanks so much for your answer
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, Mar 18, 2019, 2:04 PM omalleyt12 ***@***.***&gt; wrote:
 Closed #26792 &lt;#26792&gt;.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#26792 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AQe-tNpTQ95dht5OoBRKifIFIg-zElaVks5vX9U-gaJpZM4b38zy&gt;
 .



		</comment>
		<comment id='3' author='ablanch5' date='2019-04-15T11:10:44Z'>
		This is the only way I could find to get a single variable into a keras model in tensorflow 2.0. Seems a bit ridiculous, is there a better way than including a tensor only to multiple it by zero? What if we wanted to output something that was not aligned with batch size?
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.backend as K

class Bias(keras.layers.Layer):
    def __init__(self, output_dim=1, **kwargs):
        self.output_dim = output_dim
        super().__init__(**kwargs)

    def build(self, input_shape):
        self.bias = self.add_weight(shape=(1,), initializer='zeros', dtype=tf.float32, name='x')
        super().build(input_shape)

    def call(self, x):
        temp = tf.reduce_mean(x, axis=-1, keepdims=True)
        return temp * 0 + self.bias

    def compute_output_shape(self, input_shape):
        return (input_shape[0], self.output_dim)

x_input = keras.layers.Input(shape=(2,))
V = Bias()(x_input)
model = keras.models.Model(inputs=x_input, outputs=V)
model.compile(loss=keras.losses.MeanSquaredError(), optimizer=keras.optimizers.Adam())

X = np.random.randn(100,2)
y = np.random.randn(100, 1) + 1.2
print(model.predict(X).squeeze())
model.fit(X, y=y, epochs=10)
print(model.predict(X).squeeze())
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='ablanch5' date='2019-04-16T18:19:38Z'>
		&lt;denchmark-link:https://github.com/cottrell&gt;@cottrell&lt;/denchmark-link&gt;
 i.e. you want a Variable that is unconnected to the inputs of your Model? I would suggest using the subclassed API in that case. Note that in the case above, the inputs of your Model are completely ignored
		</comment>
	</comments>
</bug>