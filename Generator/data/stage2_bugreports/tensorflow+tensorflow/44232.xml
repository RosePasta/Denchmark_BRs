<bug id='44232' author='Arman-IMRSV' open_date='2020-10-22T15:19:14Z' closed_time='2020-11-03T13:43:06Z'>
	<summary>TFlite interpreter fails to load a saved tflite model when Dropout is used!</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): binary (pip)
TensorFlow version (or github SHA if from source): v 2.3.0

Provide the text output from tflite_convert
&lt;denchmark-code&gt;---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-74-3ae2f98ec211&gt; in &lt;module&gt;
----&gt; 1 interpreter = tf.lite.Interpreter(model_path=tflite_file)
      2 translate_tflite("este é o primeiro livro que eu fiz",tokenizer_pt,tokenizer_en,interpreter,MAX_LENGTH)

~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py in __init__(self, model_path, model_content, experimental_delegates, num_threads)
    196       self._interpreter = (
    197           _interpreter_wrapper.CreateWrapperFromFile(
--&gt; 198               model_path, self._custom_op_registerers))
    199       if not self._interpreter:
    200         raise ValueError('Failed to open {}'.format(model_path))

ValueError: Did not get operators, tensors, or buffers in subgraph 1.
&lt;/denchmark-code&gt;

Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

&lt;denchmark-link:https://www.tensorflow.org/tutorials/text/transformer&gt;https://www.tensorflow.org/tutorials/text/transformer&lt;/denchmark-link&gt;

The code for saving and loading the tflite model is as follows:
&lt;denchmark-code&gt;# Load TFLite model and allocate tensors.
def evaluate_tflite(inp_sentence,tokenizer_pt,tokenizer_en,interpreter,max_length):
  start_token = [tokenizer_pt.vocab_size]
  end_token = [tokenizer_pt.vocab_size + 1]
  
  # TODO: languague change [check for erros]
  # inp sentence is lev, hence adding the start and end token
  inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token
  encoder_input = tf.expand_dims(inp_sentence, 0)
  
  # as the target is english, the first word to the transformer should be the
  # english start token.
  decoder_input = [tokenizer_en.vocab_size]
  output = tf.expand_dims(decoder_input, 0)
  
  for i in range(max_length):
    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(
        encoder_input, output)
    
    # Use interpreter for inference
    # print (input_details)
#     interpreter = tf.lite.Interpreter(model_path=tflite_file)
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    interpreter.resize_tensor_input(input_details[0]['index'], encoder_input.shape)
    interpreter.resize_tensor_input(input_details[1]['index'], output.shape)
    interpreter.resize_tensor_input(input_details[3]['index'], enc_padding_mask.shape)
    interpreter.resize_tensor_input(input_details[4]['index'], combined_mask.shape)
    interpreter.resize_tensor_input(input_details[5]['index'], dec_padding_mask.shape)
    interpreter.allocate_tensors()

    # input_details = interpreter.get_input_details()
    # output_details = interpreter.get_output_details()
    interpreter.set_tensor(input_details[0]['index'], tf.cast(encoder_input, tf.float32))
    interpreter.set_tensor(input_details[1]['index'], tf.cast(output, tf.float32))
    interpreter.set_tensor(input_details[2]['index'], [False])
    interpreter.set_tensor(input_details[3]['index'], enc_padding_mask)
    interpreter.set_tensor(input_details[4]['index'], combined_mask)
    interpreter.set_tensor(input_details[5]['index'], dec_padding_mask)
    interpreter.invoke()

    # print("Inference worked!")
    

    # [print(d) for d in output_details]

    # The function `get_tensor()` returns a copy of the tensor data.
    # Use `tensor()` in order to get a pointer to the tensor.
    predictions = interpreter.get_tensor(output_details[0]['index'])
    attention_weights = interpreter.get_tensor(output_details[1]['index'])
    

    
    # select the last word from the seq_len dimension
    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)

    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)
    
    # return the result if the predicted_id is equal to the end token
    if predicted_id == tokenizer_en.vocab_size+1:
      return tf.squeeze(output, axis=0), attention_weights
    
    # concatentate the predicted_id to the output which is given to the decoder
    # as its input.
    output = tf.concat([output, predicted_id], axis=-1)

  return tf.squeeze(output, axis=0), attention_weights


def translate_tflite(sentence, tokenizer_pt,tokenizer_en, interpreter, max_length, plot=False):
  result, attention_weights = evaluate_tflite(sentence, tokenizer_pt,tokenizer_en, interpreter, max_length)
  
  predicted_sentence = tokenizer_en.decode([i for i in result 
                                            if i &lt; tokenizer_en.vocab_size])  

  print('Input: {}'.format(sentence))
  print('Predicted translation: {}'.format(predicted_sentence))
  
  if plot:
    plot_attention_weights(attention_weights, sentence, result, plot)
  
  return predicted_sentence


interpreter = tf.lite.Interpreter(model_path=tflite_file)
translate_tflite("este é o primeiro livro que eu fiz",tokenizer_pt,tokenizer_en,interpreter,MAX_LENGTH)
&lt;/denchmark-code&gt;

Also, please include a link to a GraphDef or the model if possible.
Any other info / logs
When I remove Dropout layers from, the interpreter works well and loads and interprets the tflite model well.
Do you have any idea how I can incorporate Dropout?
	</description>
	<comments>
		<comment id='1' author='Arman-IMRSV' date='2020-10-23T12:45:05Z'>
		Dropout is not supported by TFLite because usually it's of use during inference. Why you want to use Dropout during inference?
		</comment>
		<comment id='2' author='Arman-IMRSV' date='2020-10-23T12:54:12Z'>
		&lt;denchmark-link:https://github.com/freedomtan&gt;@freedomtan&lt;/denchmark-link&gt;

I do not want to use it during inference. I just want to use it for training. But the point is that when I add Dropout layer to the model architecture, and train the model, and save it using tflite compressor, It will not be loaded anymore for inference and raise the above exception.
		</comment>
		<comment id='3' author='Arman-IMRSV' date='2020-10-23T14:09:10Z'>
		when you save a model for inference, it should not contain Dropout :-)
		</comment>
		<comment id='4' author='Arman-IMRSV' date='2020-10-23T17:10:53Z'>
		It does not seem to be a straightforward step!
Do you have an easier solution other than the ones suggested here : (removing the node from the graph)
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/5867&gt;#5867&lt;/denchmark-link&gt;

thanks,
		</comment>
		<comment id='5' author='Arman-IMRSV' date='2020-11-03T13:43:02Z'>
		I solved it. In the call functions, put a condition: if it is on inference mode, do not call Dropout at all. (The training option that is passed as an argument to the Dropout class was not enough for me.)
		</comment>
		<comment id='6' author='Arman-IMRSV' date='2020-11-03T13:43:07Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44232&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44232&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>