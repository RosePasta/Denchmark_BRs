<bug id='36224' author='rodyt' open_date='2020-01-26T20:06:53Z' closed_time='2020-05-18T05:36:19Z'>
	<summary>tf.keras NaN loss when using multiple GPUs</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): Pip
TensorFlow version (use command below): 2.0
Python version: 3.7.3
Bazel version (if compiling from source): no
GCC/Compiler version (if compiling from source): no
CUDA/cuDNN version: 10
GPU model and memory: 2x NVIDIA GTX 1080 Ti 11GB
Driver Version: 440.33.01

I am currently using Tensorflow 2.0 (Python) and the tf.keras library to train a CNN. However, I am encountering an issue when I try to train my model by calling model.fit().
After I begin training, the loss is normal for 1 ~ 2 steps for the first epoch. But after that, it suddenly becomes NaN loss.
This issue only happens when using multiple GPUs. The code I'm using works perfectly fine on a single GPU. I have wrapped all of my code inside the scope of a tf.distribute.MirroredStrategy  using with strategy.scope():. I am feeding my network with data from a tf.data.Dataset (though this error occurs regardless of the data format).
I then ran some tests:


I tried to replace the data in my dataset with random number, but the loss still went to NaN.


I also tried feeding the numpy arrays directly to .fit(), but that didn't solve the issue.


I tried using different optimizers (Adam, RMSprop, SGD), batch sizes (4, 8, 16, 32), and learning rates, none of which helped to solve this problem.


I swapped out my network for a simple Multi-layer Perceptron, but the error persisted.


This doesn't appear to be an OOM issue, since the data is relatively small and running watch -n0.1 nvidia-smi reveals that memory usage never exceeds 30% on either of my GPUs. There doesn't seem to be any warning or error in the console output that might hint at the issue either.
Any help is appreciated
	</description>
	<comments>
		<comment id='1' author='rodyt' date='2020-01-27T05:07:12Z'>
		&lt;denchmark-link:https://github.com/rodyt&gt;@rodyt&lt;/denchmark-link&gt;
 Could you please provide us with simple standalone code to reproduce the issue in our environment, Thanks.
		</comment>
		<comment id='2' author='rodyt' date='2020-01-28T09:05:32Z'>
		Hi &lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 ,
Thanks for responding. I am able to reproduce the error with the sample MNIST code provided on the Tensorflow website:
&lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/keras&gt;https://www.tensorflow.org/tutorials/distribute/keras&lt;/denchmark-link&gt;

This is the sample code from the tutorial that I tried to run:
&lt;denchmark-code&gt;
import tensorflow_datasets as tfds
import tensorflow as tf
tfds.disable_progress_bar()
import os

datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)
mnist_train, mnist_test = datasets['train'], datasets['test']

num_train_examples = info.splits['train'].num_examples
num_test_examples = info.splits['test'].num_examples

BUFFER_SIZE = 10000

BATCH_SIZE_PER_REPLICA = 64
BATCH_SIZE = BATCH_SIZE_PER_REPLICA * 2

def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255

    return image, label

train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)

strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    
    model.compile(loss='sparse_categorical_crossentropy',
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['accuracy'])
    model.fit(
        train_dataset, 
        epochs=12
    )

&lt;/denchmark-code&gt;

This is the output when I run .fit using MirroredStrategy:

Epoch 1/12
INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
16/Unknown - 7s 423ms/step - loss: nan - accuracy: 0.0932

This is the output of the model when I run without MirroredStrategy:

Epoch 1/12
469/469 [==============================] - 2s 4ms/step - loss: 0.0480 - accuracy: 0.9853
Epoch 2/12
469/469 [==============================] - 2s 3ms/step - loss: 0.0361 - accuracy: 0.9892
Epoch 3/12
469/469 [==============================] - 2s 3ms/step - loss: 0.0302 - accuracy: 0.9908
Epoch 4/12
469/469 [==============================] - 2s 3ms/step - loss: 0.0249 - accuracy: 0.9922
Epoch 5/12
469/469 [==============================] - 2s 3ms/step - loss: 0.0197 - accuracy: 0.9944
Epoch 6/12
469/469 [==============================] - 2s 3ms/step - loss: 0.0163 - accuracy: 0.9950
Epoch 7/12
469/469 [==============================] - 1s 3ms/step - loss: 0.0122 - accuracy: 0.9966
Epoch 8/12
469/469 [==============================] - 2s 3ms/step - loss: 0.0102 - accuracy: 0.9970
Epoch 9/12
469/469 [==============================] - 1s 3ms/step - loss: 0.0082 - accuracy: 0.9977
Epoch 10/12
469/469 [==============================] - 1s 3ms/step - loss: 0.0087 - accuracy: 0.9975
Epoch 11/12
469/469 [==============================] - 2s 3ms/step - loss: 0.0060 - accuracy: 0.9983
Epoch 12/12
469/469 [==============================] - 1s 3ms/step - loss: 0.0051 - accuracy: 0.9984

		</comment>
		<comment id='3' author='rodyt' date='2020-01-30T06:56:11Z'>
		I am unable to repro the issue in a colab using the standalone snippet you provided.  Can you try using &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/debugging/enable_check_numerics&gt;tf.debugging.enabled_check_numerics&lt;/denchmark-link&gt;
 to help pinpoint the op that is causing this?
		</comment>
		<comment id='4' author='rodyt' date='2020-01-30T08:23:41Z'>
		I'm sorry, I don't have TF 2.1. What is the alternative to this?
		</comment>
		<comment id='5' author='rodyt' date='2020-01-30T09:22:46Z'>
		
I'm sorry, I don't have TF 2.1. What is the alternative to this?

Well, I can tell you that this problem have not been sloved at tensorflow 2.1....
		</comment>
		<comment id='6' author='rodyt' date='2020-01-30T18:03:51Z'>
		Same here with TF 2.1, have been using the same example and also get the NaN-Values while the training on 2 gpu's is much slower than on one
		</comment>
		<comment id='7' author='rodyt' date='2020-02-01T07:20:44Z'>
		Is there any other way I can provide debug information on TF 2.0? It's incredibly annoying to not be able to use all my GPUs. &lt;denchmark-link:https://github.com/anj-s&gt;@anj-s&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='rodyt' date='2020-02-06T09:36:20Z'>
		&lt;denchmark-link:https://github.com/rodyt&gt;@rodyt&lt;/denchmark-link&gt;
 I have got them same issue while using Keras in multiple GPUs. Have you solved your problem?
&lt;denchmark-link:https://user-images.githubusercontent.com/15938335/73924270-ca3b5300-48fe-11ea-8cdf-0f55fdc91137.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='rodyt' date='2020-04-17T06:24:22Z'>
		&lt;denchmark-link:https://github.com/rodyt&gt;@rodyt&lt;/denchmark-link&gt;
 we are unable to reproduce this problem with the code snippet you provided. Is it possible that there is a driver/hardware issue with one of the GPUs?
&lt;denchmark-link:https://github.com/VyBui&gt;@VyBui&lt;/denchmark-link&gt;
 please file a separate ticket for your issues - there could be many different reasons for getting a NaN. If you file a separate ticket with your code to repro, setup etc, that can help us look into that issue.
		</comment>
		<comment id='10' author='rodyt' date='2020-05-18T05:36:19Z'>
		&lt;denchmark-link:https://github.com/rodyt&gt;@rodyt&lt;/denchmark-link&gt;
  Closing this issue as we are unable to reproduce this problem with the code snippet you provided.Thanks!
		</comment>
		<comment id='11' author='rodyt' date='2020-05-18T05:36:21Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36224&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36224&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>