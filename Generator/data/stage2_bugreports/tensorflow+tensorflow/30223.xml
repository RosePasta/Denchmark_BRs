<bug id='30223' author='mudassirej' open_date='2019-06-28T12:54:20Z' closed_time='2019-07-23T22:22:52Z'>
	<summary>training code utilize more cpu memory than gpu memory</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source):
TensorFlow version (1.13.1):
Python version: 2.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:10
GPU model and memory:Nvidia RTx 2080

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

I am using tensorflow 1.13.1 with GPU RTx 2080. I am using reinforcement learning to train the robot in a gazebo platform using ROS. The issue is that it is running in CPU memory rather than GPU memory. It takes a lot of time to train a robot. I attached a screenshot of the nvidia smi which shows that it utilize 1.7Gb only of GPU while use 16Gb of RAM.

&lt;denchmark-link:https://user-images.githubusercontent.com/45818401/60343521-c188b300-99e6-11e9-8715-2b511e40b023.png&gt;&lt;/denchmark-link&gt;

Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='mudassirej' date='2019-07-01T08:55:27Z'>
		&lt;denchmark-link:https://github.com/mudassirej&gt;@mudassirej&lt;/denchmark-link&gt;
 Will it be possible us to provide the minimal code snippet which reproduces the reported issue here. Thanks!
		</comment>
		<comment id='2' author='mudassirej' date='2019-07-01T09:46:43Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 thank you for your consideration. Here is the snippet
from  import print_function
from GazeboWorld import GazeboWorld
import tensorflow as tf
import random
import numpy as np
import time
import rospy
from collections import deque
GAME = 'GazeboWorld'
ACTIONS = 7 # number of valid actions
SPEED = 2 # DoF of speed
GAMMA = 0.99 # decay rate of past observations
OBSERVE = 10. # timesteps to observe before training
EXPLORE = 20000. # frames over which to anneal epsilon
FINAL_EPSILON = 0.0001 # final value of epsilon
INITIAL_EPSILON = 0.1 # starting value of epsilon
REPLAY_MEMORY = 10000# number of previous transitions to remember
BATCH = 1 # size of minibatch
MAX_EPISODE = 20000
MAX_T = 200
DEPTH_IMAGE_WIDTH = 160
DEPTH_IMAGE_HEIGHT = 128
RGB_IMAGE_HEIGHT = 228
RGB_IMAGE_WIDTH = 304
CHANNEL = 3
TAU = 0.001 # Rate to update target network toward primary network
H_SIZE = 81064
IMAGE_HIST = 4
def variable_summaries(var):
"""Attach a lot of summaries to a Tensor (for TensorBoard visualization)."""
with tf.name_scope('summaries'):
mean = tf.reduce_mean(var)
tf.summary.scalar('mean', mean)
with tf.name_scope('stddev'):
stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
tf.summary.scalar('stddev', stddev)
tf.summary.scalar('max', tf.reduce_max(var))
tf.summary.scalar('min', tf.reduce_min(var))
tf.summary.histogram('histogram', var)
def weight_variable(shape):
initial = tf.truncated_normal(shape, stddev = 0.01)
return tf.Variable(initial, name="weights")
def bias_variable(shape):
initial = tf.constant(0., shape = shape)
return tf.Variable(initial, name="bias")
def conv2d(x, W, stride_h, stride_w):
return tf.nn.conv2d(x, W, strides = [1, stride_h, stride_w, 1], padding = "SAME")
class QNetwork(object):
"""docstring for ClassName"""
def init(self, sess):
# network weights
# input 128x160x1
with tf.name_scope("Conv1"):
W_conv1 = weight_variable([10, 14, IMAGE_HIST, 32])
variable_summaries(W_conv1)
b_conv1 = bias_variable([32])
# 16x20x32
with tf.name_scope("Conv2"):
W_conv2 = weight_variable([4, 4, 32, 64])
variable_summaries(W_conv2)
b_conv2 = bias_variable([64])
# 8x10x64
with tf.name_scope("Conv3"):
W_conv3 = weight_variable([3, 3, 64, 64])
variable_summaries(W_conv3)
b_conv3 = bias_variable([64])
# 8x10x64
with tf.name_scope("FCValue"):
W_value = weight_variable([H_SIZE, 512])
variable_summaries(W_value)
b_value = bias_variable([512])
# variable_summaries(b_ob_value)
&lt;denchmark-code&gt;	with tf.name_scope("FCAdv"):
		W_adv = weight_variable([H_SIZE, 512])
		variable_summaries(W_adv)
		b_adv = bias_variable([512])
		# variable_summaries(b_adv)

	with tf.name_scope("FCValueOut"):
		W_value_out = weight_variable([512, 1])
		variable_summaries(W_value_out)
		b_value_out = bias_variable([1])
		# variable_summaries(b_ob_value_out)

	with tf.name_scope("FCAdvOut"):
		W_adv_out = weight_variable([512, ACTIONS])
		variable_summaries(W_adv_out)
		b_adv_out = bias_variable([ACTIONS])
		# variable_summaries(b_ob_adv_out)	

	# input layer
	self.state = tf.placeholder("float", [None, DEPTH_IMAGE_HEIGHT, DEPTH_IMAGE_WIDTH, IMAGE_HIST])
	#print("State:", self.state.shape)
	# Conv1 layer
	h_conv1 = tf.nn.relu(conv2d(self.state, W_conv1, 8, 8) + b_conv1)
	#print("Hidden:", h_conv1)
	# Conv2 layer
	h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2, 2, 2) + b_conv2)
	# Conv2 layer
	h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 1, 1) + b_conv3)
	h_conv3_flat = tf.reshape(h_conv3, [-1, H_SIZE])

	# FC ob value layer
	h_fc_value = tf.nn.relu(tf.matmul(h_conv3_flat, W_value) + b_value)
	value = tf.matmul(h_fc_value, W_value_out) + b_value_out

	# FC ob adv layer
	h_fc_adv = tf.nn.relu(tf.matmul(h_conv3_flat, W_adv) + b_adv)		
	advantage = tf.matmul(h_fc_adv, W_adv_out) + b_adv_out
	
	# Q = value + (adv - advAvg)
	advAvg = tf.expand_dims(tf.reduce_mean(advantage, axis=1), axis=1)
	advIdentifiable = tf.subtract(advantage, advAvg)
	self.readout = tf.add(value, advIdentifiable)
	#print("readout:", self.readout.shape)

	# define the ob cost function
	self.a = tf.placeholder("float", [None, ACTIONS])
	self.y = tf.placeholder("float", [None])
	self.readout_action = tf.reduce_sum(tf.multiply(self.readout, self.a), axis=1)
	self.td_error = tf.square(self.y - self.readout_action)
	self.cost = tf.reduce_mean(self.td_error)
	self.train_step = tf.train.AdamOptimizer(1e-4).minimize(self.cost)
&lt;/denchmark-code&gt;

def updateTargetGraph(tfVars,tau):
total_vars = len(tfVars)
op_holder = []
for idx,var in enumerate(tfVars[0:total_vars/2]):
op_holder.append(tfVars[idx+total_vars/2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars/2].value())))
return op_holder
def updateTarget(op_holder,sess):
for op in op_holder:
sess.run(op)
def trainNetwork():
sess = tf.InteractiveSession()
with tf.name_scope("OnlineNetwork"):
online_net = QNetwork(sess)
with tf.name_scope("TargetNetwork"):
target_net = QNetwork(sess)
rospy.sleep(1.)
&lt;denchmark-code&gt;reward_var = tf.Variable(0., trainable=False)
reward_epi = tf.summary.scalar('reward', reward_var)
# define summary
merged_summary = tf.summary.merge_all()
summary_writer = tf.summary.FileWriter('./logs1', sess.graph)

# Initialize the World
env = GazeboWorld()
print('Environment initialized')

# Initialize the buffer
D = deque()

# get the first state 
#depth_img_t1 = env.GetDepthImageObservation()
#print("input image dimensrion:", depth_img_t1.shape)
#depth_imgs_t1 = np.stack((depth_img_t1, depth_img_t1, depth_img_t1, depth_img_t1), axis=2)
#print("depth images t1:", depth_imgs_t1.shape)
#terminal = False

# saving and loading networks
trainables = tf.trainable_variables()
trainable_saver = tf.train.Saver(trainables)
sess.run(tf.global_variables_initializer())
checkpoint = tf.train.get_checkpoint_state('saved_networks/Q')
print('checkpoint:', checkpoint)
if checkpoint and checkpoint.model_checkpoint_path:
	trainable_saver.restore(sess, checkpoint.model_checkpoint_path)
	print("Successfully loaded:", checkpoint.model_checkpoint_path)

else:
	print("Could not find old network weights")
	
# start training
episode = 0
epsilon = INITIAL_EPSILON
r_epi = 0.
t = 0
T = 0
rate = rospy.Rate(5)
print('Number of trainable variables:', len(trainables))
targetOps = updateTargetGraph(trainables,TAU)
loop_time = time.time()
last_loop_time = loop_time
while episode &lt; MAX_EPISODE and not rospy.is_shutdown():
	env.ResetWorld()
	t = 0
	r_epi = 0.
	terminal = False
	reset = False
	loop_time_buf = []
	action_index = 0
	while not reset and not rospy.is_shutdown():
		depth_img_t1 = env.GetDepthImageObservation()
		depth_imgs_t1 = np.stack((depth_img_t1, depth_img_t1, depth_img_t1, depth_img_t1), axis=2)
		print("input image dimensrion1:", depth_img_t1.shape)
		depth_img_t1 = np.reshape(depth_img_t1, (DEPTH_IMAGE_HEIGHT, DEPTH_IMAGE_WIDTH, 1))
		print("input image dimensrion after reshape:", depth_img_t1.shape)
		depth_imgs_t1 = np.append(depth_img_t1, depth_imgs_t1[:, :, :(IMAGE_HIST - 1)], axis=2)
		print("input image dimensrion after append1:",depth_imgs_t1.shape)
		reward, terminal, reset = env.GetRewardAndTerminate(t)
		v_t, theta_t = env.GetSelfOdomeSpeed(t)
		if t &gt; 0 :
			D.append((depth_imgs_t, a_t, reward, depth_imgs_t1, terminal))
			if len(D) &gt; REPLAY_MEMORY:
				D.popleft()
		depth_imgs_t = depth_imgs_t1

		# choose an action epsilon greedily
		a = sess.run(online_net.readout, feed_dict = {online_net.state : [depth_imgs_t1]})
		print("action:", a.shape)
		readout_t = a[0]
		a_t = np.zeros([ACTIONS])
		if episode &lt;= OBSERVE:
			action_index = random.randrange(ACTIONS)
			a_t[action_index] = 1
		else:
			if random.random() &lt;= epsilon:
				print("----------Random Action----------")
				action_index = random.randrange(ACTIONS)
				a_t[action_index] = 1
			else:
				action_index = np.argmax(readout_t)
				a_t[action_index] = 1
		# Control the agent
		env.Control(action_index)

		if episode &gt; OBSERVE :
			# # sample a minibatch to train on
			minibatch = random.sample(D, BATCH)
			y_batch = []
			# get the batch variables
			depth_imgs_t_batch = [d[0] for d in minibatch]
			a_batch = [d[1] for d in minibatch]
			r_batch = [d[2] for d in minibatch]
			depth_imgs_t1_batch = [d[3] for d in minibatch]
			Q1 = online_net.readout.eval(feed_dict = {online_net.state : depth_imgs_t1_batch})
			Q2 = target_net.readout.eval(feed_dict = {target_net.state : depth_imgs_t1_batch})
			for i in range(0, len(minibatch)):
				terminal_batch = minibatch[i][4]
				# if terminal, only equals reward
				if terminal_batch:
					y_batch.append(r_batch[i])
				else:
					y_batch.append(r_batch[i] + GAMMA * Q2[i, np.argmax(Q1[i])])

			#Update the network with our target values.
			online_net.train_step.run(feed_dict={online_net.y : y_batch,
												online_net.a : a_batch,
												online_net.state : depth_imgs_t_batch })
			updateTarget(targetOps, sess) # Set the target network to be equal to the primary network.

		r_epi = r_epi + reward
		t += 1
		T += 1
		last_loop_time = loop_time
		loop_time = time.time()
		loop_time_buf.append(loop_time - last_loop_time)
		rate.sleep()

		# scale down epsilon
		if epsilon &gt; FINAL_EPSILON and episode &gt; OBSERVE:
			epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE

	#  write summaries
	if episode &gt; OBSERVE:
		summary_str = sess.run(merged_summary, feed_dict={reward_var: r_epi})
		summary_writer.add_summary(summary_str, episode - OBSERVE)

	# save progress every 500 episodes
	if (episode+1) % 100 == 0 :
		trainable_saver.save(sess, 'saved_networks/' + GAME + '-dqn', global_step = episode)

	if len(loop_time_buf) == 0:
		print("EPISODE", episode, "/ REWARD", r_epi, "/ steps ", T, v_t, reward_t)
	else:
		print("EPISODE", episode, "/ REWARD", r_epi, "/ steps ", T,
			"/ LoopTime:", np.mean(loop_time_buf),"/ velocity ", v_t, "/ angle ", theta_t, "/ reward current ", reward)

	episode = episode + 1	
&lt;/denchmark-code&gt;

def main():
trainNetwork()
if name == "main":
main()
		</comment>
		<comment id='3' author='mudassirej' date='2019-07-16T20:07:23Z'>
		Apologies for the delay in response. It is difficult to debug lengthy scripts like these. Perhaps you can provide a minimal code snippet from your whole script which reflects the issue reported. Thanks!
		</comment>
		<comment id='4' author='mudassirej' date='2019-07-23T22:22:51Z'>
		Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
		<comment id='5' author='mudassirej' date='2019-07-23T22:22:53Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30223&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30223&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='mudassirej' date='2019-08-01T01:05:51Z'>
		sorry for the late response. I am attaching the snippet of the code and also the status of my gpu and system memory. I need a solution to this problem as I am not able to train my model.
&lt;denchmark-link:https://user-images.githubusercontent.com/45818401/62258113-74f82380-b43b-11e9-9093-8b0e6e55203e.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/45818401/62258114-7590ba00-b43b-11e9-9765-a7bc30631318.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/45818401/62258115-7590ba00-b43b-11e9-8ca1-382e366f421b.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/45818401/62258116-7590ba00-b43b-11e9-8b8f-029a8ef730a6.png&gt;&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>