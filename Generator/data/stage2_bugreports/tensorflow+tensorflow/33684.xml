<bug id='33684' author='georgealexandruvlad' open_date='2019-10-24T11:43:16Z' closed_time='2020-01-06T17:40:42Z'>
	<summary>[TF 2.0 Nightly] Tensorflow 2.0 nightly build doesn't work with colab TPUs</summary>
	<description>
TPU strategy can't be instantiated in colab. I am aware that this issue might be more suitable for colaboratory  github but there many issues are not getting a response very fast so I thought it will be better to ask you about a potential solution (if there is a bug related to the last build and not google colab environment) or whether it is planned that colab will have this support in the nearby future.
The following link demonstrates the issue: &lt;denchmark-link:https://colab.research.google.com/drive/1DsM_lsZXvBnlz3weymB8GHQ6cjoaxPgq&gt;https://colab.research.google.com/drive/1DsM_lsZXvBnlz3weymB8GHQ6cjoaxPgq&lt;/denchmark-link&gt;

TPU runtime was selected and the version of the installed tf2.0 is '2.1.0-dev20191024'. Thanks!
	</description>
	<comments>
		<comment id='1' author='georgealexandruvlad' date='2019-10-24T16:32:31Z'>
		Can you try with  2.1.0-dev20191020 instead? There is a failure in between 20 and 22 that I'm currently investigating, so just in case that's also the issue you're seeing, please try with 20.
		</comment>
		<comment id='2' author='georgealexandruvlad' date='2019-10-24T20:59:29Z'>
		&lt;denchmark-link:https://github.com/mihaimaruseac&gt;@mihaimaruseac&lt;/denchmark-link&gt;
 I tried it but it results in the same error.
		</comment>
		<comment id='3' author='georgealexandruvlad' date='2019-10-24T21:47:57Z'>
		Looking back, it doesn't seem related to what I was debugging
		</comment>
		<comment id='4' author='georgealexandruvlad' date='2019-10-25T06:02:14Z'>
		I have tried on colab with TF version 2.1.0-dev20191024 ,2.1.0-dev20191020, 2.1.0-dev20191021,2.1.0-dev20191022 ,2.1.0-dev20191019 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/fea33e547df4f4368a4e88518c4ed8c2/tpu_colab_not_working_with_tf2-0nightly.ipynb&gt;here.&lt;/denchmark-link&gt;
 Thanks!
		</comment>
		<comment id='5' author='georgealexandruvlad' date='2019-10-25T07:27:56Z'>
		Yes, this expected due to how TPUs and Colab work: You can upgrade the version of TensorFlow in Colab kernel but the version of TensorFlow on the remote TPU currently cannot be changed. We have ongoing work to fix this, which will hopefully land in the next few weeks. In the meantime, if you want to use TF2.0 and TPU and have access to Cloud TPU, you can allocate a one with the version 'nightly-2.x' and then follow the instructions &lt;denchmark-link:https://research.google.com/colaboratory/local-runtimes.html&gt;here&lt;/denchmark-link&gt;
 to setup and connect to a Colab runtime running on the Cloud TPU VM.
		</comment>
		<comment id='6' author='georgealexandruvlad' date='2019-10-28T09:12:16Z'>
		&lt;denchmark-link:https://github.com/bfontain&gt;@bfontain&lt;/denchmark-link&gt;
 Thank you! I will use your solution in the meantime. Is there any way I can follow the progress with colab's TPUs?
		</comment>
		<comment id='7' author='georgealexandruvlad' date='2019-10-28T16:44:28Z'>
		No problem. I'll update this bug once above mentioned work is finished.
		</comment>
		<comment id='8' author='georgealexandruvlad' date='2019-11-07T08:36:22Z'>
		&lt;denchmark-link:https://github.com/bfontain&gt;@bfontain&lt;/denchmark-link&gt;
 I've been trying your solution past couple of days but I couldn't figure out why I can't make it work.
I created a VM instance in google cloud and after that a TPU instance. Both are on the same zone and have the same name. For the TPU computing instance I've specified the tensorflow version by setting the --tf-version flag to "nightly-2.x". They were both created and I connected to the VM as described in guides. So far, so good. The problem appears when I'm trying to run the following python script to check connection to TPU.
&lt;denchmark-code&gt;import tensorflow as tf
print(tf.__version__)

tpu_addr = 'grpc://{ip}:8470'
cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu_addr)
tf.config.experimental_connect_to_cluster(cluster_resolver)
tf.tpu.experimental.initialize_tpu_system(cluster_resolver)
strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)

print(strategy)
&lt;/denchmark-code&gt;

The problem is that the process hangs with the following output:
&lt;denchmark-code&gt;georgealexandruvlad@tpu-bert:~$ python3 file.py
2019-11-06 22:19:16.116169: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory
2019-11-06 22:19:16.116213: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2.1.0-dev20191106
2019-11-06 22:19:17.457940: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2019-11-06 22:19:17.457982: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)
2019-11-06 22:19:17.458016: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tpu-bert): /proc/driver/nvidia/version does not exist
2019-11-06 22:19:17.458833: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-11-06 22:19:17.466620: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz
2019-11-06 22:19:17.466836: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556822a2a050 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-11-06 22:19:17.466937: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2019-11-06 22:19:17.473816: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; 10.240.1.2:8470}
2019-11-06 22:19:17.473853: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job localhost -&gt; {0 -&gt; localhost:32891}georgealexandruvlad@tpu-bert:~$ python3 file.py
2019-11-06 22:19:16.116169: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory
2019-11-06 22:19:16.116213: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2.1.0-dev20191106
2019-11-06 22:19:17.457940: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2019-11-06 22:19:17.457982: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)
2019-11-06 22:19:17.458016: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tpu-bert): /proc/driver/nvidia/version does not exist
2019-11-06 22:19:17.458833: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-11-06 22:19:17.466620: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz
2019-11-06 22:19:17.466836: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556822a2a050 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-11-06 22:19:17.466937: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2019-11-06 22:19:17.473816: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; 10.240.1.2:8470}
2019-11-06 22:19:17.473853: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job localhost -&gt; {0 -&gt; localhost:32891}
&lt;/denchmark-code&gt;

I installed on the vm the last tf-nightly version and checked that it is seen by the system. Both instances have owner permissions to rule out the possibility of a any restrictions of this kind.
Do you happen to know what might be the cause of this error? I've searched a lot past few days and I have no idea how to solve it. I'm trying to find out if there is a bug and I should stop searching or it's something faulty in my approach.
		</comment>
		<comment id='9' author='georgealexandruvlad' date='2019-11-07T09:14:29Z'>
		If you have the chance, can you try setting tpu_addr to just the name of the cloud tpu you allocated rather than the grpc://+ip address? TPUClusterResolver should be able to figure everything out from just that information, so long as the GCE instance is in the same zone/project as the TPU.
		</comment>
		<comment id='10' author='georgealexandruvlad' date='2019-11-07T09:39:30Z'>
		Already tried this. I've checked again to be sure and it has the following output:
&lt;denchmark-code&gt;2019-11-07 09:35:32.471945: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory
2019-11-07 09:35:32.471988: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2.1.0-dev20191106
2019-11-07 09:35:34.243429: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2019-11-07 09:35:34.243479: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)
2019-11-07 09:35:34.243516: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tpu-bert): /proc/driver/nvidia/version does not exist
2019-11-07 09:35:34.460615: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-11-07 09:35:34.709331: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000134999 Hz
2019-11-07 09:35:34.709561: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55abc59a2790 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-11-07 09:35:34.709587: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2019-11-07 09:35:34.861465: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; 10.240.1.2:8470}
2019-11-07 09:35:34.861517: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job localhost -&gt; {0 -&gt; localhost:31015}
&lt;/denchmark-code&gt;

As you said, based on the name it figures out the IP address and port to the TPU but unfortunately it hangs again.
		</comment>
		<comment id='11' author='georgealexandruvlad' date='2019-11-07T17:20:59Z'>
		OK, I just tried this with a new gce instance with 2.1.0-dev20191106 installed and a freshly created v3-8 TPU (running in europe-west4-a) with nightly-2.x and your code worked for me. In my case I created the GCE instance and TPU by hand in the cloud console and the only non-default settings I had to use for the GCE instance other than the name//region was to 'Allow full access to all Cloud APIs'. For the TPU, I picked the nightly-2.x version and a v3-8.
What your seeing is the same symptom I see if I pick an address/port that the GCE instance can't connect to. Was your TPU created in the same project as the GCE instance?
There is a slight possibility that you ended up with a bad combination TensorFlow versions and TPU/nightly-2.x version. Could you try deleting and recreating the TPU?
		</comment>
		<comment id='12' author='georgealexandruvlad' date='2019-11-07T19:31:22Z'>
		Yes, they were created in the same project otherwise the VM wouldn't have set the environment variable TPU_NAME, as I understood. After connecting to the VM I've checked for this. I deleted and created more than 10 VMs and TPUs past few days but I will give it a try again. It's important that you succeeded and now I know the problem is on my side. Thanks for that!
		</comment>
		<comment id='13' author='georgealexandruvlad' date='2019-11-07T22:08:06Z'>
		Thanks for being patient with this. I talked with one of my colleagues who is more familiar with Cloud TPU issues. Is there anyway that the TPU and GCE instance are on different networks? In my GCE instance has network 'default' under 'Network interfaces' and the TPU has network 'default' under its 'Network endpoints'.
If the network matches, is there any chance that you have custom firewall rules?
		</comment>
		<comment id='14' author='georgealexandruvlad' date='2019-11-07T23:34:09Z'>
		My colleague took a closer look at your issue and suggested that you file a bug here &lt;denchmark-link:https://issuetracker.google.com/issues/new?component=508028&gt;https://issuetracker.google.com/issues/new?component=508028&lt;/denchmark-link&gt;
 so they can help you directly.
		</comment>
		<comment id='15' author='georgealexandruvlad' date='2019-11-08T08:26:25Z'>
		I've checked the firewall rules and everything is on default. I've tried to make the instances directly from the console as you did and I left everything on default less the zone, the tf version for TPU (going for nightly-2.x) and the name which I made sure it's the same for both. I also checked the 'Allow full access to all Cloud APIs'. box. So I get a Debian VM and a TPU (I tried with v2-8 and v3-8). On the VM I installed python-3.7 for tensorflow 2.0 to work and tried to run the code I posted in here. Unfortunately, it resulted in the same problem. I will file an issue to the link you provided. Thanks a lot for your time!
EDIT: the code I've tried is the same I posted above with the mention that I provided the name of the TPU to the TPUClusterResolver instead of the address as you suggested
		</comment>
		<comment id='16' author='georgealexandruvlad' date='2019-12-04T22:15:46Z'>
		Hi, &lt;denchmark-link:https://github.com/bfontain&gt;@bfontain&lt;/denchmark-link&gt;
 ! Do you have any new information about the progress on the issue (colab TPU's compatibility with tf 2.0)?
		</comment>
		<comment id='17' author='georgealexandruvlad' date='2019-12-05T21:29:34Z'>
		Hi, right now you can try '%tensorflow_version 2.x' in a cell. That will bring you to TF 2.0 + a 2.0 TPU backend. Once 2.1 is released then I believe that will switch to 2.1. I'm currently working with the colab team to see if we can implement a '%tensorflow_version nightly' (this does not work right now).
		</comment>
		<comment id='18' author='georgealexandruvlad' date='2020-01-06T17:40:42Z'>
		Sorry for the delay, it looks like %tensorflow_version 2.x now brings in TF2.1-rc1 and works on TPUs. As an example of directly using TPUs in colab + TF 2.x, a colleague recently posted this colab notebook: &lt;denchmark-link:https://colab.research.google.com/github/znah/tpu4fun/blob/master/feature_viz.ipynb&gt;https://colab.research.google.com/github/znah/tpu4fun/blob/master/feature_viz.ipynb&lt;/denchmark-link&gt;

		</comment>
		<comment id='19' author='georgealexandruvlad' date='2020-01-06T17:40:44Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33684&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33684&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>