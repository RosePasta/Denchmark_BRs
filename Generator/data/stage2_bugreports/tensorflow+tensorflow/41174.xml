<bug id='41174' author='amaiya' open_date='2020-07-07T22:36:54Z' closed_time='2020-09-29T04:31:36Z'>
	<summary>setting model.stop_training=True in custom callbacks does not stop training in v2.2.0 (works in v2.1.0 though)</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0
Python version: 3.6.9

Describe the current behavior
Setting self.model.stop_training=True in on_train_batch_end does NOT stop training in v2.2.0.  It does work correctly in v2.1.0, though.
Describe the expected behavior
Setting self.model.stop_training=True in on_train_batch_end should correctly stop training.

Please see &lt;denchmark-link:https://colab.research.google.com/drive/1cDOIJTiVR0wFTdsTi0RaE-MVY71dYpD-&gt;this Google Colab link&lt;/denchmark-link&gt;
 to reproduce the problem.
The code to reproduce is also here:
import tensorflow as tf
from tensorflow import keras

# Define the Keras model to add callbacks to
def get_model():
    model = keras.Sequential()
    model.add(keras.layers.Dense(1, input_dim=784))
    model.compile(
        optimizer=keras.optimizers.RMSprop(learning_rate=0.1),
        loss="mean_squared_error",
        metrics=["mean_absolute_error"],
    )
    return model

# Load example MNIST data and pre-process it
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train = x_train.reshape(-1, 784).astype("float32") / 255.0
x_test = x_test.reshape(-1, 784).astype("float32") / 255.0

# Limit the data to 1000 samples
x_train = x_train[:1000]
y_train = y_train[:1000]
x_test = x_test[:1000]
y_test = y_test[:1000]

class CustomCallback(keras.callbacks.Callback):
    def on_train_batch_end(self, batch, logs=None):
        keys = list(logs.keys())
        print('value of model.stop_training: %s' % (self.model.stop_training))
        if batch == 1:
            print('stop training on batch %s' % (batch))
            self.model.stop_training = True
            return

model = get_model()
model.fit(
    x_train,
    y_train,
    batch_size=128,
    epochs=1,
    verbose=0,
    validation_split=0.5,
    callbacks=[CustomCallback()],
)
	</description>
	<comments>
		<comment id='1' author='amaiya' date='2020-07-08T14:48:46Z'>
		Was able to reproduce the issue with &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/0f297f0e2e8693608f0d99841d6f234b/41174-2-2.ipynb#scrollTo=PhCGba4zUtw9&gt;TF v2.2&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/766a327a49acf9a300c75aefb400639e/41174-tf-nightly.ipynb&gt;TF-nightly&lt;/denchmark-link&gt;
. Works as intended with &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/3f500ee38278657627c54c35402feda7/41174-2-1.ipynb&gt;TF v2.1&lt;/denchmark-link&gt;
. Please find the attached gist. Thanks!
		</comment>
		<comment id='2' author='amaiya' date='2020-07-09T20:16:46Z'>
		&lt;denchmark-link:https://github.com/amaiya&gt;@amaiya&lt;/denchmark-link&gt;
 we realized there was a regression between 2.1 and 2.2, and we're actively fixing this. We plan to release the fix in 2.4. In the meantime, would stopping at epoch boundaries work for you?
		</comment>
		<comment id='3' author='amaiya' date='2020-07-09T20:40:55Z'>
		&lt;denchmark-link:https://github.com/rchao&gt;@rchao&lt;/denchmark-link&gt;
 Thanks for including a fix in 2.4.
The  flag is being used in a &lt;denchmark-link:https://github.com/amaiya/ktrain&gt;learning-rate-finder&lt;/denchmark-link&gt;
 that estimates an optimal learning rate by gradually increasing the learning rate and stopping training when the loss increases and diverges (similar to the fastai/PyTorch implementation).  It is important to stop at the batch boundary rather than epoch boundary in such an application.
Given this and some other v2.2 issues, I'll try to pin to v2.1 until 2.4.0 is released.
Thanks.
		</comment>
		<comment id='4' author='amaiya' date='2020-07-13T17:07:04Z'>
		&lt;denchmark-link:https://github.com/amaiya&gt;@amaiya&lt;/denchmark-link&gt;
 thanks for the updates! Sorry for the inconvenience.
		</comment>
		<comment id='5' author='amaiya' date='2020-07-16T02:11:59Z'>
		Is there a recommended work-around for those of us that can't downgrade to 2.1 and can't rely on stopping at epoch boundaries? I am using a custom callback to stop and exit training gracefully if an ephemeral machine goes away don't always have time to wait until the epoch ends. Thanks!
		</comment>
		<comment id='6' author='amaiya' date='2020-07-19T09:06:01Z'>
		Setting self.model.stop_training=True in on_train_batch_end does NOT stop training in v2.2.0.
&lt;denchmark-link:https://github.com/rchao&gt;@rchao&lt;/denchmark-link&gt;

I suggest to add  setting in keras .    Now , many models  are  rather than   .
For example: Bert
		</comment>
		<comment id='7' author='amaiya' date='2020-08-20T14:15:37Z'>
		I report stop_training flag has no effect within on_train_batch_end in v2.3.0 too.
As a workaround, if you can extrapolate/deduce what will the batch number be when you need to interrupt your training, then you can achieve the same result by setting the steps_per_epoch par in .fit accordingly.
		</comment>
		<comment id='8' author='amaiya' date='2020-09-25T11:53:23Z'>
		&lt;denchmark-link:https://github.com/amaiya&gt;@amaiya&lt;/denchmark-link&gt;
,
Looks like the issue is resolved with the latest TF-nightly. I was able to run the code without any issues, please find the gist of it &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/f777e386d587d44b74b71f7690365547/41174-tf-nightly.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='9' author='amaiya' date='2020-09-28T17:06:03Z'>
		&lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;
 Very nice - thanks a lot.
		</comment>
		<comment id='10' author='amaiya' date='2020-09-29T04:31:36Z'>
		&lt;denchmark-link:https://github.com/amaiya&gt;@amaiya&lt;/denchmark-link&gt;
,
Thank you for the update. Marking this issue as closed, as it is resolved.
		</comment>
		<comment id='11' author='amaiya' date='2020-09-29T04:31:38Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41174&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41174&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>