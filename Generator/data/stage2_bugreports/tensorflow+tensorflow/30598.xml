<bug id='30598' author='yasushi-saito' open_date='2019-07-11T11:58:41Z' closed_time='2020-05-22T19:26:21Z'>
	<summary>gfile for S3 uploads corrupt data when called concurrently</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):

Ubuntu 16.04

Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):

Amazon DLAMI

TensorFlow version (use command below):
1.13.1
Python version:
3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
I have code that writes .npz files from multiple python processes. python code is single-threaded, and each process writes a separate .npz file. The files are sometimes corrupted on S3. I checked a sha256 checksum of the data in memory, and they differ from the checksum of the written data on S3.
Describe the expected behavior
the contents should be identical.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
I can create a repro if needed.
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
The reason seems to be that the gfile writer doesn't create a unique tempfile when it is invoked within a millisecond.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/s3/s3_file_system.cc#L213&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/s3/s3_file_system.cc#L213&lt;/denchmark-link&gt;

Here, the writer creates a tempfile. and the corresponding aws-sdk-cpp code is:
&lt;denchmark-link:https://github.com/aws/aws-sdk-cpp/blob/master/aws-cpp-sdk-core/source/platform/linux-shared/FileSystem.cpp#L262&gt;https://github.com/aws/aws-sdk-cpp/blob/master/aws-cpp-sdk-core/source/platform/linux-shared/FileSystem.cpp#L262&lt;/denchmark-link&gt;

The latter code only has a millisecond granularity, so if gfile is called within the same millisecond, it creates the same tempfile and they overwrite each other.
Also, the "XXXXXX" part in s3_file_system.c doesn't do anything, although it's not the cause of this problem.
I created a separate issue in
&lt;denchmark-link:https://github.com/aws/aws-sdk-cpp/issues/1192&gt;aws/aws-sdk-cpp#1192&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='yasushi-saito' date='2019-07-12T09:07:17Z'>
		&lt;denchmark-link:https://github.com/yasushi-saito&gt;@yasushi-saito&lt;/denchmark-link&gt;
 Provide us the full minimal code snippet which replicates mentioned issue. It will indeed help us to move faster.Thanks!
		</comment>
		<comment id='2' author='yasushi-saito' date='2019-07-12T17:31:14Z'>
		Try the attached file. You need to change test_path. With CONCURRENCY &gt; 1, I consistency get file corruption. With CONCURRENCY=1, I don't see an error.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3387404/gfilebug.txt&gt;gfilebug.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='yasushi-saito' date='2019-11-11T13:16:50Z'>
		Any news on this issue? It seems that the aws-sdk fixed the issue on version 1.7.153
&lt;denchmark-link:https://github.com/aws/aws-sdk-cpp/issues/1192#issuecomment-518383782&gt;aws/aws-sdk-cpp#1192 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='yasushi-saito' date='2020-05-08T16:56:31Z'>
		As now, almost a year has passed since this issue was opened. &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 do you know if this issue fixed on a newer tf release?
		</comment>
		<comment id='5' author='yasushi-saito' date='2020-05-08T17:08:44Z'>
		&lt;denchmark-link:https://github.com/tomasferraro&gt;@tomasferraro&lt;/denchmark-link&gt;
 Sorry for missing this issue. Can you please check the issue with recent versions like  and . Thanks!
		</comment>
		<comment id='6' author='yasushi-saito' date='2020-05-15T18:44:21Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='7' author='yasushi-saito' date='2020-05-22T19:26:20Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='8' author='yasushi-saito' date='2020-05-22T19:26:23Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30598&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30598&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>