<bug id='17720' author='mbrio' open_date='2018-03-14T20:05:55Z' closed_time='2018-03-16T12:39:52Z'>
	<summary>map_and_batch tensor shape does not match value of tensor in the same way that calling map and batch individually does</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Happens with stock code
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu Server 17.10.1
TensorFlow installed from (source or binary):
Source
TensorFlow version (use command below):
b'v1.6.0-0-gd2e24b6039' 1.6.0
Python version:
3.6
Bazel version (if compiling from source):
0.11.0
GCC/Compiler version (if compiling from source):
gcc (Ubuntu 6.4.0-8ubuntu1) 6.4.0 20171010
CUDA/cuDNN version:
9.1/7.0.5
GPU model and memory:
not applicable
Exact command to reproduce:
not applicable

When I create a tf.data.Dataset from tfrecord files that utilizes a call to map to parse the tfrecord file and a call to batch to batch the dataset I am able to filter out the last small batch utilizing a straight forward call to filter. This same function does not work correctly when utilizing the combined map_and_batch function. The filter function in question is:
&lt;denchmark-code&gt;dataset = dataset.filter(
    lambda *args: tf.equal(tf.shape(args[0])[0], batch_size)
)
&lt;/denchmark-code&gt;

The reason this does not work is because every tensor passed through tf.shape when utilizing map_and_batch has the same shape even though the contents of the tensor does not. This is not the case when executing map and batch separately, the last batch has a shape returned from tf.shape that correctly matches the shape of the value.
My real world example has 7153 batches in 1 epoch, using map and batch separately I am returned 7152 batches that have a shape returned from tf.shape of 128 and the final 1 batch returned as 70, in this case the filter function works correctly. When I swap out this configuration with map_and_batch I am returned 7153 batches with shape returned from tf.shape of 128, in this case my filter function does not work and my estimator throws an exception because it receives a batch of 70 (when it was expecting a batch of 128).
I would very much like to use map_and_batch because it takes 1/3 the time of map and batch separately.
Here is an example script:
# example.py
import tensorflow as tf

flags = tf.app.flags

flags.DEFINE_boolean('use_broken_map_and_batch', False,
                     'Directory to write the model and ')
flags.DEFINE_string('train_data', 'data.tfrecord', 'Training tfrecord file. ')

FLAGS = flags.FLAGS

def parse_fn(example):
    example_fmt = {
        'src': tf.FixedLenFeature([], tf.int64),
        'dst': tf.FixedLenFeature([], tf.int64),
    }

    features = tf.parse_single_example(
        example,
        features=example_fmt
    )

    return tuple(features[k] for k in example_fmt)


graph = tf.Graph()

with graph.as_default():
    files = tf.data.Dataset.list_files(FLAGS.train_data)
    dataset = files.interleave(tf.data.TFRecordDataset, cycle_length=4)

    if FLAGS.use_broken_map_and_batch:
        dataset = dataset.apply(
            tf.contrib.data.map_and_batch(
                map_func=parse_fn,
                batch_size=128,
                num_parallel_batches=28
            )
        )
    else:
        dataset = dataset.map(parse_fn, num_parallel_calls=28)
        dataset = dataset.batch(128)

    dataset = dataset.filter(
        lambda *args: tf.equal(tf.shape(args[0])[0], 128)
    )

    iterator = dataset.make_one_shot_iterator()
    src, dst = iterator.get_next()

    train_op = src * dst

    init_op = tf.group(tf.global_variables_initializer(),
                       tf.local_variables_initializer())

with tf.Session(graph=graph) as sess:
    sess.run(init_op)

    shapes = []

    try:
        while True:
            val = sess.run(train_op)
            shapes.append(val.shape[0])
    except tf.errors.OutOfRangeError:
        print(shapes[-10:])
When executed with the following parameters you should see the output:
&lt;denchmark-code&gt;$ python example.py --train_data data.tfrecord

[128, 128, 128, 128, 128, 128, 128]

$ python example.py --train_data data.tfrecord  --use_broken_map_and_batch

[128, 128, 128, 128, 128, 128, 128, 104]
&lt;/denchmark-code&gt;

To be clear the execution with the   --use_broken_map_and_batch shows a 104 at the end, this is because that batch was not filtered out, you can recreate this by using the following code to generate a tfrecord file:
&lt;denchmark-code&gt;# data_generation.py
import numpy as np
import tensorflow as tf

flags = tf.app.flags

flags.DEFINE_string('train_data', 'data.tfrecord', 'Training tfrecord file. ')

FLAGS = flags.FLAGS

vals = np.random.randint(1, 1000, (1000, 2))

with tf.python_io.TFRecordWriter(FLAGS.train_data) as writer:
    for src, dst in vals:
        example = tf.train.Example(
            features=tf.train.Features(
                feature={
                    'src': tf.train.Feature(
                        int64_list=tf.train.Int64List(value=[src])),
                    'dst': tf.train.Feature(
                        int64_list=tf.train.Int64List(value=[dst])),
                }
            )
        )

        serialized = example.SerializeToString()
        writer.write(serialized)
&lt;/denchmark-code&gt;

And you can execute in this way:
&lt;denchmark-code&gt;$ python data_generation.py --train_data data.tfrecord
&lt;/denchmark-code&gt;

Any help on this issue is greatly appreciated.
	</description>
	<comments>
		<comment id='1' author='mbrio' date='2018-03-14T22:08:20Z'>
		It looks like this has been broken since &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/8693bf519399495cedd91293ec82b492ea401f6f&gt;8693bf5&lt;/denchmark-link&gt;
, which did not update the Python shape inference logic when adding the ability for  to return a smaller final batch. Sorry for the inconvenience, and we'll have a fix for this soon. In the meantime, you can fix by using  to inhibit the shape inference:
&lt;denchmark-code&gt;     dataset = dataset.apply(
        tf.contrib.data.map_and_batch(
            map_func=parse_fn,
            batch_size=tf.placeholder_with_default(tf.constant(128, dtype=tf.int64)),
            num_parallel_batches=28
        )
     )
&lt;/denchmark-code&gt;

/cc &lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='mbrio' date='2018-03-15T00:40:40Z'>
		To clarify, it should be:
dataset = dataset.apply(
    tf.contrib.data.map_and_batch(
        map_func=parse_fn,
        batch_size=tf.placeholder_with_default(tf.constant(128, dtype=tf.int64), ()),
        num_parallel_batches=28
     )
)
tf.placeholder_with_default needs shape passed to it, is it correct to pass that in as () in this situation? I don't get a problem, and it fixes the issue, thank you!
		</comment>
		<comment id='3' author='mbrio' date='2018-03-15T00:45:36Z'>
		Right, that should work.
		</comment>
	</comments>
</bug>