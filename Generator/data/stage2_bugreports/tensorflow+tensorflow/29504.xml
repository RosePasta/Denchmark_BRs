<bug id='29504' author='jackshi0912' open_date='2019-06-06T17:34:38Z' closed_time='2019-08-09T19:10:30Z'>
	<summary>tf.function fails to parse for-loop in some circumstances</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): gLinux
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.0
Python version: 3.6
Bazel version (if compiling from source): n/a
GCC/Compiler version (if compiling from source): n/a
CUDA/cuDNN version: n/a
GPU model and memory: n/a

Describe the current behavior
@tf.function
def tf_function_with_loop(num_iter):
  digit_list = []
  for i in tf.range(num_iter):
    digit_list.append(i)
  return tf.add_n(digit_list)

tf_function_with_loop(5)
fails with log posted in log section.

def function_with_loop(num_iter):
  digit_list = []
  for i in tf.range(num_iter):
    digit_list.append(i)
  return tf.add_n(digit_list)

function_with_loop(5)  # samething without @tf.function
returns tf.Tensor(10, shape=(), dtype=int32)
Describe the expected behavior
tf_function_with_loop(5) should return tf.Tensor(10, shape=(), dtype=int32)
Code to reproduce the issue
written above.
Other info / logs
ValueError: Trying to capture a tensor from an inner function. This can be caused by accessing a tensor defined inside a loop or conditional body, or a subfunction, from a calling function, without going through the proper return value mechanism. Consider using TensorFlow mechanisms such as TensorArrays to return tensors from inner functions or loop / conditional bodies. Tensor: Tensor("TensorArrayV2Read/TensorListGetItem:0", shape=(), dtype=int32); tensor graph: FuncGraph(name=while_body_40, id=139946430861008); this graph: FuncGraph(name=tf_function_with_loop, id=139946437179024)
	</description>
	<comments>
		<comment id='1' author='jackshi0912' date='2019-06-14T09:54:20Z'>
		I am able to reproduce the issue on colab with Tf 2.0.0.beta0. Thanks!
		</comment>
		<comment id='2' author='jackshi0912' date='2019-06-14T15:26:32Z'>
		Found this in a autograph example. It seems that you need TensorArray for this. Feel free to close it. I'm not sure if supporting python list was intended or not (since Autograph otherwise support parsing python list).
&lt;denchmark-code&gt;@tf.function
def square_if_positive_naive(x):
  result = tf.TensorArray(tf.int32, size=x.shape[0])
  for i in tf.range(x.shape[0]):
    if x[i] &gt; 0:
      result = result.write(i, x[i] ** 2)
    else:
      result = result.write(i, x[i])
  return result.stack()
&lt;/denchmark-code&gt;

square_if_positive_naive(tf.range(-5, 5))
		</comment>
		<comment id='3' author='jackshi0912' date='2019-06-15T18:05:30Z'>
		Closing this issue since the error message helps resolving it.  Feel free to reopen if have further questions. Thanks!
		</comment>
		<comment id='4' author='jackshi0912' date='2019-06-15T18:05:32Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29504&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29504&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='jackshi0912' date='2019-08-06T19:01:35Z'>
		I think this deserves another look. Especially since the solution provided by the poster stops gradients from flowing back in graph mode. See:
&lt;denchmark-code&gt;@tf.function
def tf_function(x):
  arr = tf.TensorArray(tf.float32, size=5)
  with tf.GradientTape() as tape:
    for i in range(5):
      arr.write(i, i)
    result = tf.reduce_sum(x * arr.stack())
  grad = tape.gradient(result, x)
  return grad

def py_function(x):
  arr = tf.TensorArray(tf.float32, size=5)
  with tf.GradientTape() as tape:
    for i in range(5):
      arr.write(i, i)
    result = tf.reduce_sum(x * arr.stack())
  grad = tape.gradient(result, x)
  return grad

x = tf.Variable(tf.constant(1.))
g = tf_function(x)
tf.print('return from tf_function: ', g)
g = py_function(x)
tf.print('return from py_function: ', g)
&lt;/denchmark-code&gt;

Returns:
&lt;denchmark-code&gt;return from tf_function:  0
return from py_function:  10
&lt;/denchmark-code&gt;

See this colab notebook: &lt;denchmark-link:https://colab.research.google.com/drive/1d5E9Xs58t6lJuKFfnXwcnaxsE_fGHj7Z&gt;https://colab.research.google.com/drive/1d5E9Xs58t6lJuKFfnXwcnaxsE_fGHj7Z&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='jackshi0912' date='2019-08-09T19:10:30Z'>
		&lt;denchmark-link:https://github.com/rhezab&gt;@rhezab&lt;/denchmark-link&gt;
 Can you please post another issue to report your problem? Thanks!
		</comment>
		<comment id='7' author='jackshi0912' date='2019-08-12T16:31:27Z'>
		&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
  So I realized this was a stupid mistake where I should've been doing
arr = arr.write(i,i)
Apparently this is required in graph mode - perhaps a side effect problem. Is this the same problem with using list appends? Why does it sometimes work and sometimes not?
I guess some explanation of the above would be nice... then again maybe I need to read the AutoGraph documentation more carefully.
Thanks!
		</comment>
	</comments>
</bug>