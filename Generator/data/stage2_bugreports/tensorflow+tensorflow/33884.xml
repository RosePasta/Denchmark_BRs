<bug id='33884' author='jvit' open_date='2019-10-31T14:58:18Z' closed_time='2019-11-12T20:37:01Z'>
	<summary>GRU layer fails on single GPU when using MirroredStrategy</summary>
	<description>
System information

Have I written custom code: yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows and linux
TensorFlow installed from: binary
TensorFlow version: 2.0
Python version: 3.7

Describe the current behavior
Using single GPU and tf.keras.layers.GRU layer and model.fit with MirroredStrategy fails on error
Invalid argument:  var and delta do not have the same shape
tf.keras.layers.LSTM works fine on one GPU.
On machine with more GPU, the GRU layers works fine.
I know it does not make much sense to run Mirrored Strategy on only one GPU but it is good for testing your code before uploading it to multi gpu machine.
It works in TF 1.5.
Code to reproduce the issue
On Google Colab, use GPU runtime and run this code
&lt;denchmark-code&gt;%tensorflow_version 2.x
# %tensorflow_version 1.x &lt;- here it works

import tensorflow as tf

print(f'TensorFlow ver. {tf.__version__}')
print(f"Num GPUs Available: {len(tf.config.experimental.list_physical_devices('GPU'))}")

ds = tf.data.Dataset.from_tensor_slices({
  'input': tf.zeros([64, 4]),
  'target': tf.zeros([64, 5])
})
ds = ds.batch(3)

strategy = tf.distribute.MirroredStrategy()

with strategy.scope():

  p_input = tf.keras.Input(shape=[4], name='input')
  p_target = tf.keras.Input(shape=[5], name='target')

  # gru = tf.keras.layers.LSTM(8) # &lt;- this works
  gru = tf.keras.layers.GRU(8)

  x = p_input
  x = tf.expand_dims(x, axis=-1)
  x = gru(x)
  x = tf.keras.layers.Dense(5, activation='tanh')(x)

  model = tf.keras.Model([p_input, p_target], x)
  model.add_loss(tf.keras.losses.MSE(p_target, x))

  model.compile(optimizer=tf.keras.optimizers.SGD())
  model.fit(ds)
&lt;/denchmark-code&gt;

Other info / logs
&lt;denchmark-code&gt;TensorFlow ver. 2.0.0
Num GPUs Available: 1
WARNING:tensorflow:Output dense_9 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_9.
      1/Unknown - 3s 3s/step
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
&lt;ipython-input-10-e5114f02da9a&gt; in &lt;module&gt;()
     32 
     33   model.compile(optimizer=tf.keras.optimizers.SGD())
---&gt; 34   model.fit(ds)

11 frames
/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  var and delta do not have the same shape[8,24] [2,24]
	 [[node SGD/SGD/update_1/update_0/ResourceApplyGradientDescent (defined at /tensorflow-2.0.0/python3.6/tensorflow_core/python/framework/ops.py:1751) ]]
  (1) Invalid argument:  var and delta do not have the same shape[8,24] [2,24]
	 [[node SGD/SGD/update_1/update_0/ResourceApplyGradientDescent (defined at /tensorflow-2.0.0/python3.6/tensorflow_core/python/framework/ops.py:1751) ]]
	 [[ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int64_Cast_3/_18]]
0 successful operations.
0 derived errors ignored. [Op:__inference_distributed_function_24603]

Function call stack:
distributed_function -&gt; distributed_function
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='jvit' date='2019-11-04T19:29:51Z'>
		I could reproduce the issue with . &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/4331c4a13bf6983eb0359d81cec85f19/untitled617.ipynb&gt;Here&lt;/denchmark-link&gt;
 is the gist. Thanks!
		</comment>
		<comment id='2' author='jvit' date='2019-11-07T23:38:53Z'>
		Thanks for reporting the issue. We also found a similar issue internally, it is caused by the graph rewrite between cpu kernel and gpu kernel.
For now, if you want to walk around the issue with some performance loss, you can add "gru.could_use_cudnn = False" to disable the cudnn path, which will allow the model to run, but just not use the fused cudnn GRU kernel.
We will fix the graph rewrite in future release.
		</comment>
		<comment id='3' author='jvit' date='2019-11-07T23:39:06Z'>
		&lt;denchmark-link:https://github.com/reedwm&gt;@reedwm&lt;/denchmark-link&gt;
 for visibility.
		</comment>
		<comment id='4' author='jvit' date='2019-11-08T08:26:18Z'>
		Thanks for the workaround.
Question:
Considering the fact that it does work on multiple gpu, does it mean that the cuDNN path is not used  when more than one gpu is used?
		</comment>
		<comment id='5' author='jvit' date='2019-11-12T20:37:01Z'>
		In the colab, TF 2.0 is still being used instead of the nightly, despite installing tf-nightly-gpu. I am not familiar with colab so I am not sure why this is happening. The colab prints the version is 2.0.0, but the nightly version will look similar to to 2.1.0-dev20191023. I confirmed the issue does not occur in the TF nightlies by running outside colab.
The fix will be in TF 2.1, so you might want to just wait until that is out. Closing this issue since this is fixed in the nightlies.
		</comment>
		<comment id='6' author='jvit' date='2019-11-12T20:37:03Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33884&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33884&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='jvit' date='2019-11-12T20:37:55Z'>
		
Considering the fact that it does work on multiple gpu, does it mean that the cuDNN path is not used when more than one gpu is used?

I am not sure, but since this will be fixed in 2.1, I do not think it's worth investigating.
		</comment>
	</comments>
</bug>