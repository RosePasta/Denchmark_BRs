<bug id='11236' author='fferroni' open_date='2017-07-03T07:58:49Z' closed_time='2017-07-19T11:55:55Z'>
	<summary>Cannot use AdamOptimizer in C++ when running graph defined from Python</summary>
	<description>
Running Ubuntu 14.04, with r1.2 Tensorflow
I have defined a graph in Python
&lt;denchmark-code&gt;features = 784
output_dim = 10
graph_name = "graph.pb"
graph_folder = "/home/fran/Repositories/tensorflow/bazel-bin/tensorflow/test"
input_node_name = "input"
output_node_name = "output"

with tf.Session() as session:
    x = tf.placeholder(tf.float32, [None, features], name="input")
    W = tf.Variable(tf.zeros([features, output_dim]), dtype=tf.float32, name="Weight")
    b = tf.Variable(tf.zeros([output_dim]), dtype=tf.float32, name="bias")
    y_pred = tf.nn.softmax(tf.matmul(x, W)+b, name=output_node_name)

    y_true = tf.placeholder(tf.float32, [None, 10], name="y_true")
    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(y_pred), reduction_indices=[1]), name="loss")
    optimizer = tf.train.AdamOptimizer(0.00001)
    train_step = optimizer.minimize(cross_entropy, name="train")
    init = tf.variables_initializer(tf.global_variables(), name='init_all_vars_op')
    saver = tf.train.Saver()
    tf.train.write_graph(session.graph_def, graph_folder, graph_name, as_text=False)
&lt;/denchmark-code&gt;

If I run this graph in a C++ program, where I load the graph, initialize the variables and feed X and Y tensors to "train", I get the following error (works fine with other optimizers like GradientDescent or RMSPropOptimizer):
&lt;denchmark-code&gt;Training.
Step: 0; Loss: 9.25913
2017-07-03 09:54:43.264264: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'use_nesterov' not in Op&lt;name=ApplyAdam; signature=var:Ref(T), m:Ref(T), v:Ref(T), beta1_power:T, beta2_power:T, lr:T, beta1:T, beta2:T, epsilon:T, grad:T -&gt; out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT64, DT_INT32, DT_UINT8, DT_UINT16, DT_INT16, DT_INT8, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT32, DT_HALF]; attr=use_locking:bool,default=false&gt;; NodeDef: train/update_Weight/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=["loc:@Weight"], use_locking=false, use_nesterov=false, _device="/job:localhost/replica:0/task:0/cpu:0"](Weight, Weight/Adam, Weight/Adam_1, beta1_power/read, beta2_power/read, train/learning_rate, train/beta1, train/beta2, train/epsilon, gradients/MatMul_grad/tuple/control_dependency_1). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
	 [[Node: train/update_Weight/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=["loc:@Weight"], use_locking=false, use_nesterov=false, _device="/job:localhost/replica:0/task:0/cpu:0"](Weight, Weight/Adam, Weight/Adam_1, beta1_power/read, beta2_power/read, train/learning_rate, train/beta1, train/beta2, train/epsilon, gradients/MatMul_grad/tuple/control_dependency_1)]]
2017-07-03 09:54:43.264468: F tensorflow/driving_school/training.cc:102] Non-OK-status: session-&gt;Run(inputs, {}, {"train"}, nullptr) status: Invalid argument: NodeDef mentions attr 'use_nesterov' not in Op&lt;name=ApplyAdam; signature=var:Ref(T), m:Ref(T), v:Ref(T), beta1_power:T, beta2_power:T, lr:T, beta1:T, beta2:T, epsilon:T, grad:T -&gt; out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT64, DT_INT32, DT_UINT8, DT_UINT16, DT_INT16, DT_INT8, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT32, DT_HALF]; attr=use_locking:bool,default=false&gt;; NodeDef: train/update_Weight/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=["loc:@Weight"], use_locking=false, use_nesterov=false, _device="/job:localhost/replica:0/task:0/cpu:0"](Weight, Weight/Adam, Weight/Adam_1, beta1_power/read, beta2_power/read, train/learning_rate, train/beta1, train/beta2, train/epsilon, gradients/MatMul_grad/tuple/control_dependency_1). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
	 [[Node: train/update_Weight/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=["loc:@Weight"], use_locking=false, use_nesterov=false, _device="/job:localhost/replica:0/task:0/cpu:0"](Weight, Weight/Adam, Weight/Adam_1, beta1_power/read, beta2_power/read, train/learning_rate, train/beta1, train/beta2, train/epsilon, gradients/MatMul_grad/tuple/control_dependency_1)]]
Aborted (core dumped)
&lt;/denchmark-code&gt;

Is this a bug?
For reference, the code I use to load the graph definition and train is the following:
&lt;denchmark-code&gt;#include "tensorflow/core/public/session.h"
#include "tensorflow/core/platform/env.h"

using namespace tensorflow;

int main(int argc, char* argv[]) {

    Session* session;
    std::cout &lt;&lt; "Initializing Tensorflow session." &lt;&lt; std::endl;
    TF_CHECK_OK(NewSession(SessionOptions(), &amp;session));

    // Read in the protobuf graph we previously exported
    // (The path is relative to the cwd. Keep this in mind
    // when using `bazel run` since the cwd isn't where you call
    // `bazel run` but from inside a temp folder.)
    GraphDef graph_def;
    std::cout &lt;&lt; "Reading protobuf graph." &lt;&lt; std::endl;
    TF_CHECK_OK(ReadBinaryProto(Env::Default(), "graph.pb", &amp;graph_def));

    std::cout &lt;&lt; "Loading graph in Tensorflow session." &lt;&lt; std::endl;
    TF_CHECK_OK(session-&gt;Create(graph_def));

    int batch_size = 64;
    int input_feature_len = 784;
    int output_feature_len = 10;
    int training_steps = 1000;
    int save_interval = 100;
    std::string save_path = "/home/fran/Repositories/tensorflow/tensorflow/test/";

    std::cout &lt;&lt; "Initializing Tensorflow I/O tensors." &lt;&lt; std::endl;
    Tensor X(DT_FLOAT, TensorShape({batch_size, input_feature_len}));
    Tensor Y(DT_FLOAT, TensorShape({batch_size, output_feature_len}));

    // Initialize our variables for training
    std::cout &lt;&lt; "Initializing Tensorflow variables." &lt;&lt; std::endl;
    TF_CHECK_OK(session-&gt;Run({}, {}, {"init_all_vars_op"}, nullptr));

    auto _XTensor = X.matrix&lt;float&gt;();
    auto _YTensor = Y.matrix&lt;float&gt;();
    _XTensor.setRandom();
    _YTensor.setRandom();

    // Assuming graph contains single input X, with node name "input"
    // and single ground truth placeholder, with node name "y_true"
    std::vector&lt;std::pair&lt;string, Tensor&gt;&gt; inputs = {
            { "input", X },
            { "y_true", Y }
    };

    // for checkpoint generation
    Tensor model_string(DT_STRING, TensorShape( { 1, 1 } ) );

    // Initialize output pointer for loss metric.
    // Assuming graph contains single loss, with node name "loss"
    // and single training node, with node name "train"
    std::cout &lt;&lt; "Training." &lt;&lt; std::endl;
    std::vector&lt;Tensor&gt; output;
    for (int i=0; i&lt;training_steps; ++i) {
        TF_CHECK_OK(session-&gt;Run(inputs, {"loss"}, {}, &amp;output)); // Get loss (for monitoring)
        float loss = output[0].scalar&lt;float&gt;()(0);
        std::cout &lt;&lt; "Step: " &lt;&lt; i &lt;&lt; "; Loss: " &lt;&lt; loss &lt;&lt; std::endl;
        TF_CHECK_OK(session-&gt;Run(inputs, {}, {"train"}, nullptr)); // Train
        output.clear();

        // to save checkpoint feed name of checkpoint file as saver_def.filename_tensor_name
        // and fetch the value of saver_def.save_tensor_name, as per graph definition script.
        if (i % save_interval == 0){
            model_string.matrix&lt; std::string &gt;()( 0, 0 ) = save_path + "model_checkpoint_step" + std::to_string(i) + ".ckpt";
            TF_CHECK_OK(session-&gt;Run({{"save/Const:0", model_string}}, {}, {"save/control_dependency"}, nullptr));
        };
    };
    std::cout &lt;&lt; "Training complete." &lt;&lt; std::endl;
    
    std::cout &lt;&lt; "Closing Tensorflow session." &lt;&lt; std::endl;
    session-&gt;Close();
    delete session;
    return 0;
}
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='fferroni' date='2017-07-03T09:51:21Z'>
		Maybe you used a different TF version for the C++ code? In &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/0c58158903ea9a3af612ae428c6cb3dede4abf16/tensorflow/core/ops/training_ops.cc#L1162&gt;here&lt;/denchmark-link&gt;
 you see that  is part of the  op. That was introduced in commit &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/53cb26d05a5c2080d8022124178b1cc43a30ffe5&gt;53cb26d&lt;/denchmark-link&gt;
, which is part of TF 1.2.0.
		</comment>
		<comment id='2' author='fferroni' date='2017-11-07T08:57:38Z'>
		&lt;denchmark-link:https://github.com/fferroni&gt;@fferroni&lt;/denchmark-link&gt;
 Can I please know how to create model_string.meta . The below command creates only  model_string.index and model_string.data-00000-of-00001 files. .meta file is needed to do prediction as described in &lt;denchmark-link:https://stackoverflow.com/questions/35508866/tensorflow-different-ways-to-export-and-run-graph-in-c&gt;https://stackoverflow.com/questions/35508866/tensorflow-different-ways-to-export-and-run-graph-in-c&lt;/denchmark-link&gt;

TF_CHECK_OK(session-&gt;Run({{"save/Const:0", model_string}}, {}, {"save/control_dependency"}, nullptr));
Is there any other way to do prediction in c++ without help of .meta file ?
		</comment>
		<comment id='3' author='fferroni' date='2019-03-05T03:13:53Z'>
		&lt;denchmark-link:https://github.com/fferroni&gt;@fferroni&lt;/denchmark-link&gt;
 Is the problem solved? Is it the TF version that matters? Very interested in it.
		</comment>
	</comments>
</bug>