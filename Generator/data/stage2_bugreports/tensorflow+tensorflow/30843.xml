<bug id='30843' author='rishabhsahrawat' open_date='2019-07-18T14:38:28Z' closed_time='2019-07-27T00:47:48Z'>
	<summary>Unable to train model on multiple GPUs using MirroredStrategy in TF2.0</summary>
	<description>
I have two Tesla T4 GPUs from Google. I am using this &lt;denchmark-link:https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches&gt;example&lt;/denchmark-link&gt;
 in order to train it on multiple GPUs with defined the  inside  as described &lt;denchmark-link:https://www.tensorflow.org/beta/guide/distribute_strategy#using_tfdistributestrategy_with_keras&gt;here&lt;/denchmark-link&gt;
. The model trains without  on single GPU normally but not when I want to train it on multiple GPUs. You can find the error log below.
Another dummy example program in the docs for using  is &lt;denchmark-link:https://www.tensorflow.org/beta/guide/distribute_strategy#using_tfdistributestrategy_with_keras&gt;here&lt;/denchmark-link&gt;
. On executing this, it trains and utilises both GPUs. Between these two examples, I noticed one thing which is the use of  and . It works fine in the latter case. I also have a model of mine where I am using . This model also can not be trained and although the error there is a bit different.


Train on None steps
Epoch 1/10
W0718 14:21:33.186410 139786698037056 cross_device_ops.py:764] Efficient allreduce is not supported for 1 IndexedSlices
W0718 14:21:38.304410 139786698037056 cross_device_ops.py:764] Efficient allreduce is not supported for 1 IndexedSlices
2019-07-18 14:21:39.456127: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] implementation_selector failed: Invalid argument: Invalid format of input node name: replica_1/sequential/bidirectional/StatefulPartitionedCall_replica_1/StatefulPartitionedCall_8 Expected: {forward_node_name}:{index}
2019-07-18 14:21:40.179202: W tensorflow/core/grappler/optimizers/implementation_selector.cc:199] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_356864_357366' and '__inference___backward_standard_lstm_356864_357366_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_360209' both implement 'lstm_3542e53d-8ce9-47ba-b422-dc9202236064' but their signatures do not match.
2019-07-18 14:21:40.669817: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1483] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2019-07-18 14:21:40.752732: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-18 14:21:51.150641: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:111] Filling up shuffle buffer (this may take a while): 17050 of 50000
2019-07-18 14:22:00.643784: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:162] Shuffle buffer filled.
2019-07-18 14:22:00.664615: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at partitioned_function_ops.cc:113 : Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:1 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_102 , and the dst node is while_2_RetVal
2019-07-18 14:22:00.664706: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:1 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_102 , and the dst node is while_2_RetVal
[[{{node replica_1/sequential/bidirectional/StatefulPartitionedCall}}]]
[[metrics/accuracy/div_no_nan/ReadVariableOp_1/_50]]
2019-07-18 14:22:00.664771: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:1 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_102 , and the dst node is while_2_RetVal
[[{{node replica_1/sequential/bidirectional/StatefulPartitionedCall}}]]
[[replica_1/metrics/accuracy/AssignAddVariableOp_1/_41]]
2019-07-18 14:22:00.665117: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at partitioned_function_ops.cc:113 : Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:1 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal
2019-07-18 14:22:00.665167: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:1 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_102 , and the dst node is while_2_RetVal
[[{{node replica_1/sequential/bidirectional/StatefulPartitionedCall}}]]
2019-07-18 14:22:00.666533: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at partitioned_function_ops.cc:113 : Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:0 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_101 , and the dst node is while_1_RetVal
2019-07-18 14:22:00.679274: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at partitioned_function_ops.cc:113 : Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:0 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal
Traceback (most recent call last):
File "colab.py", line 94, in 
model.fit(train_data, epochs=10, validation_data=test_data)
File "/home/rishabh/.local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.py", line 643, in fit
use_multiprocessing=use_multiprocessing)
File "/home/rishabh/.local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training_distributed.py", line 681, in fit
steps_name='steps_per_epoch')
File "/home/rishabh/.local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training_arrays.py", line 294, in model_iteration
batch_outs = f(actual_inputs)
File "/home/rishabh/.local/lib/python2.7/site-packages/tensorflow/python/keras/distribute/distributed_training_utils.py", line 813, in execution_function
return [out.numpy() for out in distributed_function(input_fn)]
File "/home/rishabh/.local/lib/python2.7/site-packages/tensorflow/python/eager/def_function.py", line 428, in call
return self._stateless_fn(*args, **kwds)
File "/home/rishabh/.local/lib/python2.7/site-packages/tensorflow/python/eager/function.py", line 1335, in call
return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
File "/home/rishabh/.local/lib/python2.7/site-packages/tensorflow/python/eager/function.py", line 589, in _filtered_call
(t for t in nest.flatten((args, kwargs), expand_composites=True)
File "/home/rishabh/.local/lib/python2.7/site-packages/tensorflow/python/eager/function.py", line 671, in _call_flat
outputs = self._inference_function.call(ctx, args)
File "/home/rishabh/.local/lib/python2.7/site-packages/tensorflow/python/eager/function.py", line 445, in call
ctx=ctx)
File "/home/rishabh/.local/lib/python2.7/site-packages/tensorflow/python/eager/execute.py", line 67, in quick_execute
six.raise_from(core._status_to_exception(e.code, message), None)
File "/home/rishabh/.local/lib/python2.7/site-packages/six.py", line 737, in raise_from
raise value
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
(0) Invalid argument:  Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:1 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_102 , and the dst node is while_2_RetVal
[[node replica_1/sequential/bidirectional/StatefulPartitionedCall (defined at usr/lib/python2.7/threading.py:801) ]]
[[metrics/accuracy/div_no_nan/ReadVariableOp_1/_50]]
(1) Invalid argument:  Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:1 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_102 , and the dst node is while_2_RetVal
[[node replica_1/sequential/bidirectional/StatefulPartitionedCall (defined at usr/lib/python2.7/threading.py:801) ]]
0 successful operations.
1 derived errors ignored. [Op:__inference_distributed_function_360209]
Function call stack:
distributed_function -&gt; distributed_function

	</description>
	<comments>
		<comment id='1' author='rishabhsahrawat' date='2019-07-19T08:12:20Z'>
		&lt;denchmark-link:https://github.com/rishabhsahrawat&gt;@rishabhsahrawat&lt;/denchmark-link&gt;
 ,
In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here and also mention the TF version being used. Thanks!
		</comment>
		<comment id='2' author='rishabhsahrawat' date='2019-07-19T08:54:17Z'>
		@anush-o thank you for your questions.
I am using TF2.0 as I mentioned it in the Title.
I am using the exact same code as this &lt;denchmark-link:https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches&gt;example&lt;/denchmark-link&gt;
 provided by TF except before the model definition I added 2 lines for the multiple GPU support. The lines are


This is my code file with in txt format (could not upload .py file here.)
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3410258/colab.txt&gt;colab.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='rishabhsahrawat' date='2019-07-24T01:15:37Z'>
		This has been fixed in the latest nightly, can you try with 2.0 nightly and see if it's still failing? (same as &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/30513&gt;#30513&lt;/denchmark-link&gt;
)
		</comment>
		<comment id='4' author='rishabhsahrawat' date='2019-07-24T08:30:02Z'>
		&lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
 Hi Priya, thank you for your helpful response. After updating TF to latest 2.0 nightly from &lt;denchmark-link:https://pypi.org/project/tf-nightly-gpu-2.0-preview/#history&gt;here&lt;/denchmark-link&gt;
, it starts training on both GPUs however once the first epoch is finished it throws following error on providing (consisting test dataset) to  argument in . However, no error when  is defined  in -

File "/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training.py", line 710, in fit
use_multiprocessing=use_multiprocessing)
File "/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 680, in fit
steps_name='steps_per_epoch')
File "/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py", line 436, in model_iteration
steps_name='validation_steps')
File "/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py", line 174, in model_iteration
steps_per_epoch)
File "/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py", line 490, in _get_num_samples_or_steps
'steps_per_epoch')
File "/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py", line 425, in check_num_samples
if hasattr(ins[0], 'shape'):
TypeError: 'function' object has no attribute 'getitem'

It seems like a bug to me. Please give your suggestions. I also tried training another program of my own, which also arises the same behaviour.
		</comment>
		<comment id='5' author='rishabhsahrawat' date='2019-07-25T08:21:35Z'>
		Can you try setting validation_steps in fit and see if that helps? This is still a bug and we will look into it.
		</comment>
		<comment id='6' author='rishabhsahrawat' date='2019-07-25T10:22:57Z'>
		thank you for your reply. I tried your suggestion and now validation data can also be used in model.fit function. Since, validation_steps require number of batches, and lets' say length of test data is (5000) not completely divisible by the batch size (64) so in this case it will skip the last data (of length 8) that doesn't make any batch. Now my question is if there is a way to also include that or should I wait for the bug to be fixed. Sorry, if the question is too naive.
		</comment>
		<comment id='7' author='rishabhsahrawat' date='2019-07-26T02:06:15Z'>
		Hi - great to know it worked for you. Once we do fix the bug, you should be able to run the validation until the end. In the meantime, perhaps you can try rounding up instead of rounding down when calculating the number of steps and see if that works?
BTW, would you mind filing a separate bug for the error you saw in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/30843#issuecomment-514534349&gt;#30843 (comment)&lt;/denchmark-link&gt;
? Will help us track it. Thanks
		</comment>
		<comment id='8' author='rishabhsahrawat' date='2019-07-26T02:10:25Z'>
		Actually ignore the new bug part, i found a similar bug here : &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/28797&gt;#28797&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='rishabhsahrawat' date='2019-07-26T07:49:32Z'>
		Hi Priya, &lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
 thank you for your response again. I tried using the rounded up value and it worked. I thought it only takes integer values as input. :D
		</comment>
		<comment id='10' author='rishabhsahrawat' date='2019-07-27T00:47:48Z'>
		Glad to know it worked! Thanks for testing out 2.0!
		</comment>
		<comment id='11' author='rishabhsahrawat' date='2019-07-27T00:47:49Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30843&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30843&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>