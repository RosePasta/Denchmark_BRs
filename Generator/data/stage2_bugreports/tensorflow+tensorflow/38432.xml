<bug id='38432' author='adfayed' open_date='2020-04-10T13:20:14Z' closed_time='2020-04-15T17:09:08Z'>
	<summary>Converting retrained ssd_resnet50_v1 object detection model to TFLite for use on Coral USB Accelerator</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): tf-nightly installed through pip
TensorFlow version (or github SHA if from source): 2.2.0.dev20200401
Nvidia GeForce GTX 1050 Ti Driver Version: 440.64.00
Cuda compilation tools, release 9.0, V9.0.176
CUDNN_MAJOR 7 CUDNN_MINOR 0 CUDNN_PATCHLEVEL 5
Edge TPU Compiler version 2.1.302470888

Provide the text output from tflite_convert
&lt;denchmark-code&gt;TensorFlow Lite currently doesn't support control flow ops: Merge, Switch. 
We are working on supporting control flow ops, 
please see github issue at https://github.com/tensorflow/tensorflow/issues/28485. 
Some of the operators in the model are not supported by the 
standard TensorFlow Lite runtime and are not recognized by TensorFlow. 
If you have a custom implementation 
for them you can disable this error with --allow_custom_ops, or by setting 
allow_custom_ops=True when calling tf.lite.TFLiteConverter(). 
Here is a list of builtin operators you 
are using: ADD, CAST, CONCATENATION, CONV_2D, EQUAL, EXP, EXPAND_DIMS, FILL, 
GATHER, GREATER, GREATER_EQUAL,
LESS, LOGICAL_OR, LOGISTIC, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, PACK, PAD, 
RANGE, RESHAPE, RESIZE_BILINEAR, SELECT, 
SHAPE, SLICE, SPLIT, SQUEEZE, STRIDED_SLICE, SUB, SUM, TOPK_V2, TRANSPOSE, UNPACK, 
WHERE.Here is a list of operators for which you will need custom implementations: 
DecodeGif, DecodeJpeg, DecodePng, DecodeRaw, NonMaxSuppressionV5, Substr.
&lt;/denchmark-code&gt;

Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
&lt;denchmark-code&gt;import tensorflow as tf

BATCH_SIZE = 32
IMG_HEIGHT = 320
IMG_WIDTH = 320
physical_devices = tf.config.list_physical_devices('GPU')
tf.config.set_logical_device_configuration(physical_devices[0],[tf.config.LogicalDeviceConfiguration(memory_limit=500)])
logical_devices = tf.config.list_logical_devices('GPU')

list_ds_Dianthus = tf.data.Dataset.list_files(str('/home/afayed/coral/detection_ag/ag/Dianthus''*/*/*'))
list_ds_DustyMiller = tf.data.Dataset.list_files(str('/home/afayed/coral/detection_ag/ag/DustyMiller''*/*/*'))
list_ds_Pansy = tf.data.Dataset.list_files(str('/home/afayed/coral/detection_ag/ag/Pansy''*/*/*'))
list_all = list_ds_Dianthus.concatenate(list_ds_DustyMiller).concatenate(list_ds_Pansy)
#len(list(list_all))

def get_label(file_path):
  parts = tf.strings.split(file_path, '/')
  class_name = tf.strings.split(parts[-3], '1')
  class_name = tf.strings.split(class_name, '2')
  class_name = tf.strings.split(class_name, '3')
  class_name = tf.strings.split(class_name, '4')
  return class_name

def decode_img(img):
	img = tf.image.decode_jpeg(img, channels=3) #color images
	img = tf.image.convert_image_dtype(img, tf.float32) 
	#convert unit8 tensor to floats in the [0,1]range
	return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT]) 

def process_path(file_path):
  label = get_label(file_path)
  img = tf.io.read_file(file_path)
  img = decode_img(img)
  return img, label

num_calibration_steps= 1000

def representative_dataset_gen():
	for _ in range(num_calibration_steps):
	# Get sample input data as a numpy array in a method of your choosing.
		image = list_all.take(1)
		yield [decode_img(image)]

saved_model_obj = tf.saved_model.load(export_dir='/home/afayed/coral/detection_ag/train/export/Servo/1586490086/')
concrete_func = saved_model_obj.signatures['serving_default']
concrete_func.inputs[0].set_shape([])
converter =  tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
converter.experimental_new_converter = False
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                                       tf.lite.OpsSet.SELECT_TF_OPS]
converter.optimizations =  [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen
tflite_quant_model = converter.convert()

open("/home/afayed/Desktop/output_tflite_graph.tflite", "wb").write(tflite_quant_model)
&lt;/denchmark-code&gt;

Also, please include a link to a GraphDef or the model if possible.
&lt;denchmark-h:h4&gt;Model ckpt 5 used here as a .tar.&lt;/denchmark-h&gt;

&lt;denchmark-h:h4&gt;Frozen graph generated here&lt;/denchmark-h&gt;

&lt;denchmark-h:h4&gt;saved_model.pb used here&lt;/denchmark-h&gt;

Any other info / logs
&lt;denchmark-h:h1&gt;Detailed explanation&lt;/denchmark-h&gt;

I am trying to use custom training data to retrain 'ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03' (model obtained from model zoo here ) following Google's Retrain an object detection model tutorial
I retrained it on custom training data using TensorFlow with GPU devel version 1.12.0-rc2 on a docker container built with this Dockerfile.
I also tried retraining with TensorFlow with GPU version 1.15.2. The pipeline config file I am using is available here.
I have made sure I used a fixed_shape_resizer and included "max_number_of_boxes: 200" in train_input_reader and eval_input_reader and all of that.
My image dimensions as found in pipeline_v2.config are 320,320.
I found the new Experimental Converter here
&lt;denchmark-h:h2&gt;So, my process is as such:&lt;/denchmark-h&gt;

I start the docker container and start retraining from ckpt 0 that is given by downloading the model from the model zoo:
&lt;denchmark-code&gt;sudo docker run --gpus all --name edgetpu-detect \
--rm -it --privileged -p 6006:6006 \
--mount type=bind,src=${DETECT_DIR},dst=/tensorflow/models/research/learn_pet \
detect-tutorial
&lt;/denchmark-code&gt;

For the sake of testing lets only retrain 5 steps (I've tried up to 20,000 giving the same results):
&lt;denchmark-code&gt;./retrain_detection_model.sh --num_training_steps 5 --num_eval_steps 100
&lt;/denchmark-code&gt;

retrain_detection_model.sh found &lt;denchmark-link:https://drive.google.com/open?id=1mMErPY2j68cNGDS_pmh9gbdAKOAvWmLB&gt;here&lt;/denchmark-link&gt;

(&lt;denchmark-link:https://drive.google.com/open?id=1nDNSpEyjExg-_L_wpaDJaNlbVrRvfbp8&gt;https://drive.google.com/open?id=1nDNSpEyjExg-_L_wpaDJaNlbVrRvfbp8&lt;/denchmark-link&gt;
):
&lt;denchmark-code&gt;python /tensorflow/tensorflow/python/tools/saved_model_cli.py show 
--dir /tensorflow/models/research/learn_pet/train/export/Servo/1586336368/ 
--all &gt; /tensorflow/models/research/learn_pet/train/saved_model_cli_output.txt
&lt;/denchmark-code&gt;

I then export a frozen graph using export_tflite_ssd_graph.py (unedited) both through 1.12.0-rc2 and 1.15.2 code shown here is using TF 1.15.2:
&lt;denchmark-code&gt;python object_detection/export_tflite_ssd_graph.py 
--pipeline_config_path="/tensorflow/models/research/learn_pet/ckpt/pipeline_v2.config" 
--trained_checkpoint_prefix="/tensorflow/models/research/learn_pet/train/model.ckpt-5" 
--output_directory="/tensorflow/models/research/learn_pet/models" 
&lt;/denchmark-code&gt;

And then I use the converter invocation (stated up above + its output) that causes this issue.
I limit memory growth using
physical_devices = tf.config.list_physical_devices('GPU') tf.config.set_logical_device_configuration(physical_devices[0],[tf.config.LogicalDeviceConfiguration(memory_limit=500)]) logical_devices = tf.config.list_logical_devices('GPU')
Please help, I am also trying to do this with faster_rcnn_inception_resnet_v2_atrous_coco_2018_01_28 and that is where I found about the Experimental Converter working for RCNNs. My goal is to run either of these models on the Coral USB Accelerator after retraining with my own custom training data. I did not edit any scripts generating either of these models. Thanks in advanced
	</description>
	<comments>
		<comment id='1' author='adfayed' date='2020-04-15T17:09:08Z'>
		SSD models don't convert directly to TFLite. You need an intermediate step to export a graph that uses our custom NMS op to speed up post-processing. Please take a look at &lt;denchmark-link:https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md&gt;these instructions&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='adfayed' date='2020-04-15T17:09:10Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38432&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38432&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='adfayed' date='2020-04-16T07:26:20Z'>
		&lt;denchmark-link:https://github.com/srjoglekar246&gt;@srjoglekar246&lt;/denchmark-link&gt;
 Apologies formatting typo in my original comment caused a step of my process to not show. I have updated it, but I am restating it here for your convenience 

&lt;denchmark-code&gt;python object_detection/export_tflite_ssd_graph.py 
--pipeline_config_path="/tensorflow/models/research/learn_pet/ckpt/pipeline_v2.config" 
--trained_checkpoint_prefix="/tensorflow/models/research/learn_pet/train/model.ckpt-5" 
--output_directory="/tensorflow/models/research/learn_pet/models" 
&lt;/denchmark-code&gt;

And then I use the generated  as an input graph_def_file to TFLite converter following the Coral retrain object detection &lt;denchmark-link:https://coral.ai/docs/edgetpu/retrain-detection/&gt;tutorial&lt;/denchmark-link&gt;
:
&lt;denchmark-code&gt;tflite_convert  --output_file="/tensorflow/models/research/learn_pet/models/output_tflite_graph.tflite"   
--graph_def_file="/tensorflow/models/research/learn_pet/models/tflite_graph.pb"   
--inference_type=QUANTIZED_UINT8  --input_arrays='normalized_input_image_tensor'   
--output_arrays="TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3" --mean_values=128 --std_dev_values=128 
--input_shapes=1,320,320,3  --change_concat_input_ranges=false 
--allow_nudging_weights_to_use_fast_gemm_kernel=true --allow_custom_ops 
&lt;/denchmark-code&gt;

At which point I get error:
&lt;denchmark-code&gt;F tensorflow/lite/toco/tooling_util.cc:1728] Array FeatureExtractor/resnet_v1_50/fpn/top_down/nearest_neighbor_upsampling/stack, 
which is an input to the Pack operator producing the output array 
FeatureExtractor/resnet_v1_50/fpn/top_down/nearest_neighbor_upsampling/stack_1, 
is lacking min/max data, which is necessary for quantization. If accuracy matters, 
either target a non-quantized output format, or run quantized training with your model 
from a floating point checkpoint to change the input graph to contain min/max information. 
If you don't care about accuracy, you can pass --default_ranges_min= and 
--default_ranges_max= for easy experimentation.
Fatal Python error: Aborted
&lt;/denchmark-code&gt;

If I add the following to the above tflite_convert command:
&lt;denchmark-code&gt;--default_ranges_min=0 --default_ranges_max=6
&lt;/denchmark-code&gt;

It works but then I get this error when using the Edgetpu Compiler for the final step to run inference on the Coral:
&lt;denchmark-code&gt;Edge TPU Compiler version 2.1.302470888
ERROR: :79 input-&gt;params.zero_point != output-&gt;params.zero_point (110 != 0)
ERROR: Node number 77 (PACK) failed to prepare.

Internal compiler error. Aborting!
&lt;/denchmark-code&gt;

And that is the point where I wrote the above script (stated as standalone code to reproduce issue in original comment)
		</comment>
		<comment id='4' author='adfayed' date='2020-04-16T07:27:02Z'>
		By the way, while following the &lt;denchmark-link:https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md&gt;link&lt;/denchmark-link&gt;
 you shared, running this command 
&lt;denchmark-code&gt;root@d97ce87cb12f:/tensorflow/models/research# bazel run -c opt //tensorflow/tensorflow/contrib/lite/toco:toco -- --input_file=$OUTPUT_DIR/tflite_graph.pb --output_file=$OUTPUT_DIR/output_tflite_graph.tflite --input_shapes=1,320,320,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops
&lt;/denchmark-code&gt;

Weirdly gives me this error:
&lt;denchmark-code&gt;ERROR: Skipping '//tensorflow/tensorflow/contrib/lite/toco:toco': no such package 
'tensorflow/tensorflow/contrib/lite/toco': BUILD file not found on package path
&lt;/denchmark-code&gt;

Which is strange since tflite_converter is working on the same docker image. I believe tflite_convert uses toco in the backend doesn't it?
		</comment>
		<comment id='5' author='adfayed' date='2020-04-16T13:50:15Z'>
		&lt;denchmark-link:https://github.com/adfayed&gt;@adfayed&lt;/denchmark-link&gt;
 Are you using  or ?. There is no  in . mixing  and  will throw errors as there is a compatibility issue. Thanks!
		</comment>
		<comment id='6' author='adfayed' date='2020-04-16T15:46:33Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 No no, I am using TF 1.15.2 GPU for all of this (retraining, generating frozen graph, and attempting conversion to TFLite). I tried the latest TF nightly for the new experimental TF Lite converter added functionality for RCNNs but with no luck.
Can you please reply to my previous comment before that? That is the important one :)
Running tflite_convert as per the detect tutorial on TF1.15.2 on an SSD network (albeit ResNet instead of Mobilenet) gives me that error. I do not have min/max data even though my config file (linked above) includes the graph_rewrite options. After you answer that question please, my second question would be, can I use Faster RCNN with the new experimental TFlite converter? If so how?
		</comment>
		<comment id='7' author='adfayed' date='2020-04-16T16:34:50Z'>
		It doesn't seem to me that the model is quantized. This might be why the convertor is complaining about min/max data etc.
For float models, the params should look something like this:
&lt;denchmark-code&gt;--input_file=$OUTPUT_DIR/tflite_graph.pb \
--output_file=$OUTPUT_DIR/detect.tflite \
--input_shapes=1,300,300,3 \
--input_arrays=normalized_input_image_tensor \
--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  \
--inference_type=FLOAT \
--allow_custom_ops
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='adfayed' date='2020-04-17T16:38:48Z'>
		&lt;denchmark-link:https://github.com/srjoglekar246&gt;@srjoglekar246&lt;/denchmark-link&gt;
 Yes! If I run tflite_convert with the above command (i.e inference_type FLOAT instead of QUANTIZED_INT8) it will convert, but then the EdgeTPU Compiler errs as such:
&lt;denchmark-code&gt;Edge TPU Compiler version 2.1.302470888
Invalid model: /home/afayed/coral/detection_ag/models/output_tflite_graph.tflite
Model not quantized
&lt;/denchmark-code&gt;

I am trying to make a quantized model to run on the Coral. Isn't quantized re-training done by adding this snippet to the pipeline.config file?
&lt;denchmark-code&gt;graph_rewriter {
  quantization {
    delay: 0
    weight_bits: 8
    activation_bits: 8
  }
}
&lt;/denchmark-code&gt;

My pipeline.config looks like this:
&lt;denchmark-code&gt;# SSD Resnet 50
# Configured for MSCOCO Dataset.
# Users should configure the fine_tune_checkpoint field in the train config as
# well as the label_map_path and input_path fields in the train_input_reader and
# eval_input_reader. Search for "PATH_TO_BE_CONFIGURED" to find the fields that
# should be configured.

model {
  ssd {
    num_classes: 3
    image_resizer {
      fixed_shape_resizer {
        height: 320
        width: 320
      }
    }
    feature_extractor {
      type: "ssd_resnet50_v1_fpn"
      depth_multiplier: 1.0
      min_depth: 16
      conv_hyperparams {
        regularizer {
          l2_regularizer {
            weight: 0.000399999989895
          }
        }
        initializer {
          truncated_normal_initializer {
            mean: 0.0
            stddev: 0.0299999993294
          }
        }
        activation: RELU_6
        batch_norm {
          decay: 0.996999979019
          scale: true
          epsilon: 0.0010000000475
        }
      }
      override_base_feature_extractor_hyperparams: true
    }
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
        use_matmul_gather: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    box_predictor {
      weight_shared_convolutional_box_predictor {
        conv_hyperparams {
          regularizer {
            l2_regularizer {
              weight: 0.000399999989895
            }
          }
          initializer {
            random_normal_initializer {
              mean: 0.0
              stddev: 0.00999999977648
            }
          }
          activation: RELU_6
          batch_norm {
            decay: 0.996999979019
            scale: true
            epsilon: 0.0010000000475
          }
        }
        depth: 256
        num_layers_before_predictor: 4
        kernel_size: 3
        class_prediction_bias_init: -4.59999990463
      }
    }
    anchor_generator {
      multiscale_anchor_generator {
        min_level: 3
        max_level: 7
        anchor_scale: 4.0
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        scales_per_octave: 2
      }
    }
    post_processing {
      batch_non_max_suppression {
        score_threshold: 0.300000011921
        iou_threshold: 0.600000023842
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SIGMOID
    }
    normalize_loss_by_num_matches: true
    loss {
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      classification_loss {
        weighted_sigmoid_focal {
          gamma: 2.0
          alpha: 0.25
        }
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
    encode_background_as_zeros: true
    normalize_loc_loss_by_codesize: true
    inplace_batchnorm_update: true
    freeze_batchnorm: false
  }
}
train_config {
  batch_size: 1
  data_augmentation_options {
    random_horizontal_flip {
    }
    random_pixel_value_scale {
    }
    random_vertical_flip {
    }
    random_patch_gaussian {
    }
    random_jitter_boxes {
    }
    random_adjust_brightness {
    }
    random_adjust_contrast {
    }
    random_adjust_hue {
    }
    random_adjust_saturation {
    }
  }
  sync_replicas: true
  optimizer {
    momentum_optimizer: {
      learning_rate: {
        manual_step_learning_rate {
          initial_learning_rate: 0.003
          schedule {
            step: 900000
            learning_rate: 0.00003
          }
          schedule {
            step: 1200000
            learning_rate: 0.000003
          }      
        }
      }
      momentum_optimizer_value: 0.9
    }
    use_moving_average: false
  }
  gradient_clipping_by_norm: 10.0
  fine_tune_checkpoint: "/tensorflow/models/research/learn_pet/ckpt/model.ckpt"

  # Note: The below line limits the training process to 200K steps, which we
  # empirically found to be sufficient enough to train the COCO dataset. This
  # effectively bypasses the learning rate schedule (the learning rate will
  # never decay). Remove the below line to train indefinitely.

  num_steps: 25000
  from_detection_checkpoint: true
  load_all_detection_checkpoint_vars: true
  startup_delay_steps: 0.0
  replicas_to_aggregate: 8
  max_number_of_boxes: 100
  unpad_groundtruth_tensors: false
}
train_input_reader {
  label_map_path: "/tensorflow/models/research/learn_pet/ag/ag_label_map.pbtxt"
  tf_record_input_reader {
    input_path: "/tensorflow/models/research/learn_pet/ag/train_dataset.record"
  }
  max_number_of_boxes: 200
}
eval_config {
  metrics_set: "coco_detection_metrics"
  num_examples: 400
}
eval_input_reader {
  label_map_path: "/tensorflow/models/research/learn_pet/ag/ag_label_map.pbtxt"
  shuffle: true
  num_readers: 1
  tf_record_input_reader {
    input_path: "/tensorflow/models/research/learn_pet/ag/validate_dataset.record"
  }
  max_number_of_boxes: 200
}
graph_rewriter {
  quantization {
    delay: 0
    weight_bits: 8
    activation_bits: 8
  }
}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='adfayed' date='2020-04-28T15:01:04Z'>
		&lt;denchmark-link:https://github.com/srjoglekar246&gt;@srjoglekar246&lt;/denchmark-link&gt;
 Hello? Can you please reply? I believe my model .
		</comment>
		<comment id='10' author='adfayed' date='2020-04-28T16:19:44Z'>
		Does the floating point model run on CPU, maybe with our &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark&gt;benchmark_model tool&lt;/denchmark-link&gt;
? (EdgeTPU may not support floating-point inference, but atleast CPU should give a sanity check)
TBH, I am not very familiar with the object detection config APIs. But I am not sure that you can re-train with quantization only for fine-tuning.
Could you try quantized re-training with an already-quantized model from the detection zoo, such as ssd_mobilenet_v2_quantized_coco?
		</comment>
		<comment id='11' author='adfayed' date='2020-05-06T18:43:06Z'>
		Any update about this issue? Same problem here.
		</comment>
		<comment id='12' author='adfayed' date='2020-05-10T14:59:16Z'>
		Also, looking for some help on this and retraining object detection for the Coral Dev Board.
		</comment>
		<comment id='13' author='adfayed' date='2020-06-19T09:32:21Z'>
		I believe this problems occurs when training with SIGMOID as a score converter for classes &gt; 2. Im facing a similar issue, but it can be circumvented by using SOFTMAX.
&lt;denchmark-link:https://github.com/srjoglekar246&gt;@srjoglekar246&lt;/denchmark-link&gt;


Could you try quantized re-training with an already-quantized model from the detection zoo, such as ssd_mobilenet_v2_quantized_coco?

I did that (Or i used the ssd_mobilenet_edgetpu_coco) and thats when i faced this error when i tried to run inference witht the .tflite model. I was training with SIGMOID as the score converter for classes &gt; 2.

RuntimeError: tensorflow/lite/kernels/kernel_util.cc:129 std::abs(input_product_scale - bias_scale) &lt;= 1e-6 * std::min(input_product_scale, bias_scale) was not true.Node number 108 (CONV_2D) failed to prepare.

So there must be something with the post-processing op i imagine
		</comment>
		<comment id='14' author='adfayed' date='2020-06-19T16:00:19Z'>
		&lt;denchmark-link:https://github.com/liufengdb&gt;@liufengdb&lt;/denchmark-link&gt;
 I vaguely remember us encountering the issue &lt;denchmark-link:https://github.com/NicholaiStaalung&gt;@NicholaiStaalung&lt;/denchmark-link&gt;
 mentions above. Do you know what the issue is?
		</comment>
		<comment id='15' author='adfayed' date='2020-11-03T07:56:07Z'>
		I was facing the same issue:
&lt;denchmark-code&gt;Array FeatureExtractor/MobilenetV1/fpn/top_down/nearest_neighbor_upsampling/stack, which is an input to the Pack operator producing the output array
FeatureExtractor/MobilenetV1/fpn/top_down/nearest_neighbor_upsampling/stack_1, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantize
d training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy
experimentation.
Aborted (core dumped)
&lt;/denchmark-code&gt;

In my case the edge device didn't support nearest neighbor upsampling so I changed to bilinear upsampling and it worked fine. Made the change here:
&lt;denchmark-link:https://github.com/tensorflow/models/blob/master/research/object_detection/models/feature_map_generators.py#L716&gt;https://github.com/tensorflow/models/blob/master/research/object_detection/models/feature_map_generators.py#L716&lt;/denchmark-link&gt;

changed tf.image.resize_nearest_neighbor to tf.image.resize_bilinear and it worked just fine.
		</comment>
	</comments>
</bug>