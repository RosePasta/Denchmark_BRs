<bug id='20021' author='DHZS' open_date='2018-06-14T14:04:51Z' closed_time='2018-06-21T18:55:24Z'>
	<summary>[BUG] TensorFlow crashed when using tf.cast() in dataset's map() function</summary>
	<description>
Please go to Stack Overflow for help and support:
&lt;denchmark-link:https://stackoverflow.com/questions/tagged/tensorflow&gt;https://stackoverflow.com/questions/tagged/tensorflow&lt;/denchmark-link&gt;

If you open a GitHub issue, here is our policy:

It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
The form below must be filled out.
It shouldn't be a TensorBoard issue. Those go here.

Here's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
TensorFlow installed from (source or binary):
pip
TensorFlow version (use command below):
1.8.0 GPU
Python version:
3.6.4
Bazel version (if compiling from source):
No
GCC/Compiler version (if compiling from source):
No
CUDA/cuDNN version:
9.0/7
GPU model and memory:
GPU: GPU 0: GeForce GTX 1080 Ti x2
RAM: 16GB
Exact command to reproduce:
No

You can collect some of this information using our environment capture script:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&lt;/denchmark-link&gt;

You can obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

When I using TensorFlow eager execution with gpu, I want to read some data, so I use tf.data.Dataset. Then I want to pre-process the data before training, then I use dataset.map() and tfe.py_func(). Also, I use dataset.prefetch(1) to read data with pipelining,
Everything is fine so far. But when I using tf.cast() in map function, the program will crash.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

import tensorflow as tf
from tensorflow.contrib.eager.python import tfe
tf.enable_eager_execution()


def main():
    data = tf.range(0, 100, dtype=tf.float32)

    def map_fn(n):
        def fn(nn):
            int_tensor = tf.constant(1, tf.int32)
            float_tensor = tf.cast(int_tensor, tf.float32)
            return float_tensor
        return tfe.py_func(fn, [n], [tf.float32])

    dataset = tf.data.Dataset.from_tensor_slices(data)
    dataset = dataset.map(map_fn).prefetch(1)

    for i in dataset:
        print(i)


if __name__ == '__main__':
    main()
run the code:
$ python xxx.py
output:

2018-06-14 13:29:48.784627: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2018-06-14 13:29:48.925618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:17:00.0
totalMemory: 10.92GiB freeMemory: 10.45GiB
2018-06-14 13:29:49.063957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 1 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:65:00.0
totalMemory: 10.92GiB freeMemory: 10.44GiB
2018-06-14 13:29:49.064088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1
2018-06-14 13:29:49.449500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-06-14 13:29:49.449552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1
2018-06-14 13:29:49.449562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y
2018-06-14 13:29:49.449570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N
2018-06-14 13:29:49.449834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10108 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)
2018-06-14 13:29:49.543343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10107 MB memory) -&gt; physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)
[1]    84561 segmentation fault (core dumped)  python dataset_eager_bug.py

	</description>
	<comments>
		<comment id='1' author='DHZS' date='2018-06-14T17:20:46Z'>
		Akshay: I've run a few versions of this, and think it could be a bug in tfe.py_func(), so could you please take a look? Here are some notes:

The crash only happens (1) when a GPU is available, and (2) when Dataset.prefetch() is used.
The crash doesn't seem to depend on using tf.cast(). Replacing fn with lambda _: tf.constant([1., 2., 3.]) + tf.constant([4., 5., 6.]) gives the same failure.
The crashing stack suggests that we are treating an EagerTensor that is actually in GPU memory as if it were in host memory:

&lt;denchmark-code&gt;    @     ...
    @     0x55e3661fb60c        400  tensorflow::TF_TensorToPyArray()
    @     0x55e3661fd2e3         96  tensorflow::TensorToNdarray()
    @     0x55e3661c30ea         96  EagerTensor_numpy()
    @     0x55e36295a4f3        320  PyEval_EvalFrameEx
    @     ...
&lt;/denchmark-code&gt;


When Dataset.prefetch() is used, the tf.cast() kernel runs on GPU. When the Dataset.prefetch() is removed, the tf.cast() kernel runs on CPU. The main difference between these two  cases is that when Dataset.prefetch() is used, the body of the py_func will run on a background thread, so maybe some thread-local state isn't configured?
Even though the tf.cast() kernel runs on GPU (when Dataset.prefetch() is used), the TensorHandle for its result has nullptr for its device, which appears to be why EagerTensor_numpy() thinks it is safe to copy.

/CC &lt;denchmark-link:https://github.com/asimshankar&gt;@asimshankar&lt;/denchmark-link&gt;
, since the  method has comments with TODOs against his name, and so he might know where the skeletons are :).
		</comment>
		<comment id='2' author='DHZS' date='2018-06-14T17:23:47Z'>
		Thanks for the notes, Derek. I'll dig in further.
		</comment>
		<comment id='3' author='DHZS' date='2018-06-15T00:38:51Z'>
		Thanks for the report &lt;denchmark-link:https://github.com/DHZS&gt;@DHZS&lt;/denchmark-link&gt;
 and investigation &lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 .
While we dig into an appropriate fix, a quick workaround would be to explicitly include a  in your py func, or just add a  to the outputs to ensure that the returned tensors are in CPU. So something like:
def map_fn(n):
    def fn(nn):
        int_tensor = tf.constant(1, tf.int32)
        float_tensor = tf.cast(int_tensor, tf.float32)
        return float_tensor.cpu()  # Notice the .cpu()
    return tfe.py_func(fn, [n], [tf.float32])
		</comment>
		<comment id='4' author='DHZS' date='2018-06-15T03:45:53Z'>
		Thank you &lt;denchmark-link:https://github.com/akshayka&gt;@akshayka&lt;/denchmark-link&gt;
, it works for me.
By the way, I have a related issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/19945&gt;#19945&lt;/denchmark-link&gt;
, maybe it's helpful for you to find the bug.
		</comment>
	</comments>
</bug>