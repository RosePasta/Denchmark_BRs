<bug id='31510' author='sdll' open_date='2019-08-10T15:45:17Z' closed_time='2019-08-21T00:26:20Z'>
	<summary>TF 1.14 on AI Platform: MirroredStrategy fails on 2 GPUs with RuntimeError</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): AI Platform 1.14
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): 1.14
Python version: 3.5
Bazel version (if compiling from source): n/a
GCC/Compiler version (if compiling from source): n/a
CUDA/cuDNN version: AI Platform
GPU model and memory: V100 16GB

Describe the current behavior
I have written a custom estimator and wanted to train it on 2 GPUs using , submitting &lt;denchmark-link:https://github.com/sdll/psenet/blob/cc3041da4565da03f5ebe206962ee424c1f36bbb/train.sh#L4&gt;this job&lt;/denchmark-link&gt;
 to the AI Platform. Unfortunately, after ~900 steps, the training fails with with
Describe the expected behavior
The model should train on two GPUs asynchronously.
Code to reproduce the issue
The estimator definition starts &lt;denchmark-link:https://github.com/sdll/psenet/blob/cc3041da4565da03f5ebe206962ee424c1f36bbb/psenet/train.py#L193&gt;here&lt;/denchmark-link&gt;
.
Other info / logs

the entire AI platform log
another curious detail is that the estimator seems to be executed only on a single GPU.

Turning on the placement logging with a similar set-up confirmed that training just used the first GPU before failing.
Here is the usage graph, where you can see a short spike before the failure:
&lt;denchmark-link:https://user-images.githubusercontent.com/17913919/62823852-b0fb5900-bb9e-11e9-8cd2-d4eacd73cfeb.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='sdll' date='2019-08-10T23:29:05Z'>
		You can use MirroredStrategy with one GPU; perhaps try that to suss out other errors?
You haven't by chance tried this on a local or AWS instance, have you?  I have recently seen Google Cloud instances give me multiple GPUs but one or more of them showed 100% utilization in nvidia-smi even though I had no processes running in my VM at the time.  It's possible that bugs in Google Cloud are confounding the problem.
		</comment>
		<comment id='2' author='sdll' date='2019-08-13T11:31:18Z'>
		&lt;denchmark-link:https://github.com/pwais&gt;@pwais&lt;/denchmark-link&gt;
, thanks for your reply! I have switched to using 4 K80 Teslas instead of 2 V100 GPUs and also disabled evaluation by replacing  with , and the training started working fine. One difference between training and evaluation is that during evaluation I skip cropping the image for data augmentation, which results in a larger input. Wonder whether this is a latent OOM error.
I do not have a local GPU and have not tried training on AWS.
		</comment>
		<comment id='3' author='sdll' date='2019-08-15T17:38:56Z'>
		I think that variable scope error isn't the root cause, rather this is:
&lt;denchmark-code&gt;InternalError: File contents are inconsistent for file: gs://gsoc-tfjs/weights/psenet/custom/psenet_rc47/checkpoint @ 0.
gs://gsoc-tfjs/weights/psenet/custom/psenet_rc47/checkpoint: Checkpoint ignored
Estimator is not trained yet. Will start an evaluation when a checkpoint is ready.
&lt;/denchmark-code&gt;

Is this failure deterministic?   Does it happen every time you run it with 2 GPUs?
		</comment>
		<comment id='4' author='sdll' date='2019-08-15T21:23:37Z'>
		&lt;denchmark-link:https://github.com/isaprykin&gt;@isaprykin&lt;/denchmark-link&gt;
, yes, this seems to happen only during the evaluation phase. I have managed to revert to  for a multi-worker multi-gpu setup and avoid this problem by setting a humongous .
		</comment>
		<comment id='5' author='sdll' date='2019-08-16T19:37:41Z'>
		Does it happen every time during the evaluation phase when using train_and_evaluate?
		</comment>
		<comment id='6' author='sdll' date='2019-08-16T19:51:15Z'>
		It failed two times in a row at about the time evaluation must have started, after which I have switched to training without evaluation, so I assume it is reproducible.
		</comment>
		<comment id='7' author='sdll' date='2019-08-16T23:56:58Z'>
		Is it possible that multiple jobs are using the same directory for their checkpoint in parallel?
		</comment>
		<comment id='8' author='sdll' date='2019-08-17T06:52:19Z'>
		I make sure that each job creates its own folder with weights in the bucket.
Wonder whether setting MirroredStrategy is a bad idea as eval_strategy, or maybe keep_checkpoint_every_n_hours does not play well with save_checkpoint_secs.
		</comment>
		<comment id='9' author='sdll' date='2019-08-19T17:44:47Z'>
		Do you mind running the code with some extra logging turned on?
os.environ["TF_CPP_VMODULE"] = "gcs_file_system=2"
You might need to remove this line too:
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
		</comment>
		<comment id='10' author='sdll' date='2019-08-21T00:26:20Z'>
		I tried running the code again, reducing the time until the first evaluation, and the job failed with a simple OOM. Closing the issue, since it is not reproducible.
		</comment>
		<comment id='11' author='sdll' date='2019-08-21T00:26:22Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=31510&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=31510&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>