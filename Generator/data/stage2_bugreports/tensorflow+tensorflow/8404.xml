<bug id='8404' author='Androbin' open_date='2017-03-14T18:25:12Z' closed_time='2017-03-18T13:38:48Z'>
	<summary>Wrong order of dependencies after running freeze_graph and/or optimize_for_inference</summary>
	<description>
I haven't found any mention of this anywhere online.
It makes the graph serializations completely useless for inference.
Steps to reproduce:

create graph that contains tf.contrib.layers.batch_norm with tf.bool tensor as is_training argument (to force use of Switch node
run freeze_graph.freeze_graph and optimize_for_inference_lib.optimize_for_inference
load resulting graph on Android via TensorFlowInferenceInterface

What happened:
ADB Logcat shows error message
E/TensorFlowInferenceInterface: Failed to load model from 'file:///android_asset/optimized_model.pb': java.io.IOException: Not a valid TensorFlow Graph serialization: Node 'conv1/bn1/BatchNorm/cond/AssignMovingAvg/BatchNorm/moving_mean/sub_1/x': Control dependencies must come after regular dependencies
Why did this happen:
I found out that the order of dependencies was inconsistent after the processing.
Dependencies before processing:
&lt;denchmark-code&gt;input: "^conv1/bn1/BatchNorm/cond/AssignMovingAvg/BatchNorm/moving_mean/BatchNorm/BatchNorm/moving_mean"
input: "^conv1/bn1/BatchNorm/cond/AssignMovingAvg/BatchNorm/moving_mean/AssignAdd"
input: "^conv1/bn1/BatchNorm/cond/switch_t"
&lt;/denchmark-code&gt;

Dependencies after processing:
&lt;denchmark-code&gt;input: "^conv1/bn1/BatchNorm/cond/AssignMovingAvg/BatchNorm/moving_mean/BatchNorm/BatchNorm/moving_mean"
input: "^conv1/bn1/BatchNorm/cond/AssignMovingAvg/BatchNorm/moving_mean/AssignAdd"
input: "conv1/bn1/BatchNorm/cond/Switch:1"
&lt;/denchmark-code&gt;

What is wrong:
The control dependencies (starting with '^') should be after the regular dependencies.
Expected behaviour:
Reordering of dependencies to ensure ordering consistency.
Expected order of dependencies:
&lt;denchmark-code&gt;input: "conv1/bn1/BatchNorm/cond/Switch:1"
input: "^conv1/bn1/BatchNorm/cond/AssignMovingAvg/BatchNorm/moving_mean/BatchNorm/BatchNorm/moving_mean"
input: "^conv1/bn1/BatchNorm/cond/AssignMovingAvg/BatchNorm/moving_mean/AssignAdd"
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Androbin' date='2017-03-15T01:10:48Z'>
		&lt;denchmark-link:https://github.com/petewarden&gt;@petewarden&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='Androbin' date='2017-03-15T15:29:08Z'>
		Can you try using the new Graph Transform Tool approach to optimizing for inference?
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms/#optimizing-for-deployment&gt;https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms/#optimizing-for-deployment&lt;/denchmark-link&gt;

I'm hoping to deprecate the old optimize_for_inference Python script soon, so it would be helpful to know if this works better.
		</comment>
		<comment id='3' author='Androbin' date='2017-03-17T20:16:37Z'>
		&lt;denchmark-link:https://github.com/petewarden&gt;@petewarden&lt;/denchmark-link&gt;

Okay, I have used the new Graph Transform Tool with arguments:
--inputs='x' --outputs='y_conv'  --transforms='strip_unused_nodes(type=float, shape="1,49,257,1") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms'
and all of the previous error messages are gone.
The serialization works just fine now, but when trying to run the model:
&lt;denchmark-code&gt;Inference exception: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Switch' with these attrs.  Registered devices: [CPU], Registered kernels:
        device='CPU'; T in [DT_FLOAT]
        device='CPU'; T in [DT_INT32]
        device='GPU'; T in [DT_STRING]
        device='GPU'; T in [DT_BOOL]
        device='GPU'; T in [DT_INT32]
        device='GPU'; T in [DT_FLOAT]
        
        [[Node: drop/cond/Switch = Switch[T=DT_BOOL](is_training/_0__cf__0, is_training/_0__cf__0)]]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='Androbin' date='2017-03-18T13:38:37Z'>
		I had to manually turn Switch ops into Identity ops.
Effectively,  is now permanently .
Seems to be issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/6124&gt;#6124&lt;/denchmark-link&gt;
, maybe related to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/5919&gt;#5919&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='Androbin' date='2017-04-26T16:24:13Z'>
		&lt;denchmark-link:https://github.com/Androbin&gt;@Androbin&lt;/denchmark-link&gt;
 I have the exact same issue - would you be so kind as to explain how to convert the Switch ops into Identity ops?
Thanks a lot!
		</comment>
		<comment id='6' author='Androbin' date='2017-04-26T16:37:49Z'>
		&lt;denchmark-link:https://github.com/ronny3050&gt;@ronny3050&lt;/denchmark-link&gt;

This arguably simple python script should work for you:
&lt;denchmark-code&gt;import tensorflow as tf
from google.protobuf import text_format

gd = tf.GraphDef()

with tf.gfile.FastGFile("model.pb", "r") as f:
    text_format.Merge(f.read(), gd)

for node in gd.node:
    if node.op == "Switch":
        node.op = "Identity"
        del node.input[1]

tf.train.write_graph(gd, ".", "fixed_model.pb")
&lt;/denchmark-code&gt;

Feel free to ask if there are further issues!
		</comment>
		<comment id='7' author='Androbin' date='2017-04-26T20:12:36Z'>
		&lt;denchmark-link:https://github.com/Androbin&gt;@Androbin&lt;/denchmark-link&gt;
 thanks a ton for this code, it's an excellent starting point. Apologies for further questions:

del node.input[2] fails in scenarios where I have node.input = ['phase_train','phase_train']. Why is this important?
If I comment out del node.input[2], your code converts Switch ops to Identity ops and removes the second redundant input. However, I end up with errors such as Input 0 of node Bottleneck/BatchNorm/cond/Switch_2 was passed bool from Bottleneck/BatchNorm/cond/pred_id:0 incompatible with expected float.

Any help is greatly appreciated!
		</comment>
		<comment id='8' author='Androbin' date='2017-04-27T04:30:09Z'>
		&lt;denchmark-link:https://github.com/Androbin&gt;@Androbin&lt;/denchmark-link&gt;
 just wanted to thank you again for your help. I got it by freezing the graph without phase_train. 
		</comment>
		<comment id='9' author='Androbin' date='2017-04-27T16:18:21Z'>
		&lt;denchmark-link:https://github.com/ronny3050&gt;@ronny3050&lt;/denchmark-link&gt;
 I am sorry. I got confused because of a special case of mine. I have already updated the script. For reference: &lt;denchmark-link:https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/switch&gt;Switch&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/identity&gt;Identity&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='Androbin' date='2017-05-28T15:01:55Z'>
		I happened to encounter the exact same issue (with DT_BOOL) crashing my app at run time.
Been trying to work around that for 2 days now (trying to remove keras_learning_phase Switch node branch)... this is frustrating
		</comment>
		<comment id='11' author='Androbin' date='2017-05-28T15:07:02Z'>
		&lt;denchmark-link:https://github.com/Lakedaemon&gt;@Lakedaemon&lt;/denchmark-link&gt;
 Do any of the provided solutions work for you?
		</comment>
		<comment id='12' author='Androbin' date='2017-05-28T19:37:12Z'>
		I'm investigating. I don't have a definitive answer yet.
I have setup tensorflow through the virutal env method and python, so some solutions are harder to try than others (I had to get a look into the optimize_for_inference_lib.py file to find how to use it with python as I couldn't/didn't know how to use the optimize_for_inference.py app...I don't have any binary for that)
I have tried some variants of your  solution that modifies the graphdef nodes  and then I applied optimize_for_inference but it corrupts it (maybee I'm doing something wrong).
I'm going to pruduce an image to visualize my graph next, to see what nodes I should prune.
I'm also going to try to use the transform graph api (when I find out how, I'll probably have to build tensorflow from source).
I'll tell you if I succeed and ask for help if I really can't make it work
(I can also build the android lib with more ops but I would rather avoid that)
		</comment>
		<comment id='13' author='Androbin' date='2017-05-28T19:41:29Z'>
		I tried tinkering with this quite a bit and made progress until I had to account for a ton of edge cases. At the end, I just retrained the network with the Boolean.
		</comment>
		<comment id='14' author='Androbin' date='2017-05-28T21:34:49Z'>
		Where is the tensorflow/tensorflow/tools/graph_transforms/ stuff in the virtual env installation ?
Is it shipped with the tenserflow release ? or not ?
I searched on my ubuntu, but couldn't find anything
(and I see in the source code for tensorflow 1.1 that there should be a python wrapper for the c code)
		</comment>
		<comment id='15' author='Androbin' date='2017-05-29T10:16:51Z'>
		I tried replacing a PlaceHolder node holding a boolean, namely
node { name: "dropout_1/keras_learning_phase" op: "Placeholder" attr { key: "dtype" value { type: DT_BOOL } } attr { key: "shape" value { shape { } } } }
with this node (not sure if it is correct)
node { name: "dropout_1/keras_learning_phase" op: "Const" attr { key: "dtype" value { type: DT_BOOL } } attr { key: "value" value { tensor { dtype: DT_BOOL tensor_shape { } bool_val: false } } } }
and then running it through
`gd = tf.GraphDef()
from google.protobuf import text_format
with tf.gfile.FastGFile(self.exportPath+self.modelName+"Constant.pbtxt", "r") as f:
text_format.Merge(f.read(), gd)
&lt;denchmark-code&gt;  tf.train.write_graph(gd,  self.exportPath,  self.modelName + "Test.pbtxt") 
  optimized_graph_def = optimize_for_inference(input_graph_def= gd,
                     input_node_names="conv2d_1_input".split(","),# \
                    output_node_names="activation_6/Softmax".split(","),
                        placeholder_type_enum=dtypes.float32.as_datatype_enum)
  optimizedGraphPath = self.exportPath +  self.modelName + "ConstantOptimized.pbtxt"
  tf.train.write_graph(optimized_graph_def,  self.exportPath,  self.modelName + "ConstantOptimized.pbtxt") 
  self.freeze(self.modelName + "ConstantOptimized", self.modelName + "-"+str(self.step))`
&lt;/denchmark-code&gt;

But, when freezing, I ended up with
ValueError: graph_def is invalid at node u'dropout_1/cond/mul/y': More inputs specified ('dropout_1/cond/Switch:1') than the op expects..
was my attempt correct ?
Next I'm going to try to build tenserflow with basel...I really need those graph transform tools
		</comment>
		<comment id='16' author='Androbin' date='2017-05-29T14:31:51Z'>
		ok, built tensorflow from sources (30 minutes O.O)
retrained my model (took mostly as long for 1 epoch as the prebuilt tensorflow binary, despite the added cpu instructions ? I didn't add any optimisation flag (there is no documentation for that anyway))
built summarize_graph (it takes sooooooooooooooooooooo long to compile C++ :/, like 8 minutes for a simple utility)
Also, tried this command bazel build tensorflow/tools/graph_transforms:transform_graph bazel-bin/tensorflow/tools/graph_transforms/transform_graph \ --in_graph=tensorflow_inception_graph.pb \ --out_graph=optimized_inception_graph.pb \ --inputs='Mul' \ --outputs='softmax' \ --transforms=' strip_unused_nodes(type=float, shape="1,299,299,3") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms'
loaded the .pb file, exported as .pbtext to see the differences (went from 370kb to 29kb, nice).
There should have been quite a lot of optimizations in there...
Yet, the DT_BOOL &amp; keras leraning-phase stuff is still in there (obviously, because the keras_learning_phase placeholder hasn't been replaced by a constant op)
And... looking at the doc for the transform_graph tools, I don't see any transformation that allows one to replace a placeholder op (with a single bool) by a const op... :/
sigh.... Am I supposed to write a custom transform function for that ?
And if I do, will the switch op disappear with an opmtimising phase ?
/me begins to think that he'll throw the towel and just build tensorflow for android with CPU:BOOL kernel
(why is there support for GPU:BOOL and not for CPU:BOOL anyway ?)
		</comment>
		<comment id='17' author='Androbin' date='2017-05-29T14:41:32Z'>
		manually replaced the placeholder with a constant op (potentially in a bad way), rerun through transform_graph -&gt; no change to the Switch ops that are fed with a constant false.
That's it... let's build CPU:DT_BOOL for android.. :/
		</comment>
		<comment id='18' author='Androbin' date='2017-05-29T17:03:18Z'>
		&lt;denchmark-link:https://github.com/Lakedaemon&gt;@Lakedaemon&lt;/denchmark-link&gt;
 Totally agree on Android lacking default support for important -.- stuff &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/10254&gt;#10254&lt;/denchmark-link&gt;

		</comment>
		<comment id='19' author='Androbin' date='2017-05-29T21:17:59Z'>
		Well, tried compiling the android demo example with basel (on a fresh git clone of the repo, after having nuked the .cache for bazel on my box) : fail
ERROR: /home/lakedaemon/.cache/bazel/_bazel_lakedaemon/6532aa38fb8c8ce96c07312dcb04db38/external/protobuf/BUILD:113:1: C++ compilation of rule '@protobuf//:protobuf' failed: false failed: error executing command
(cd /home/lakedaemon/.cache/bazel/_bazel_lakedaemon/6532aa38fb8c8ce96c07312dcb04db38/execroot/tensorflow &amp;&amp; 
exec env - 
PWD=/proc/self/cwd 
/bin/false -MD -MF bazel-out/stub_armeabi-v7a-opt/bin/external/protobuf/_objs/protobuf/external/protobuf/src/google/protobuf/util/internal/type_info_test_helper.pic.d '-frandom-seed=bazel-out/stub_armeabi-v7a-opt/bin/external/protobuf/_objs/protobuf/external/protobuf/src/google/protobuf/util/internal/type_info_test_helper.pic.o' -fPIC -iquote external/protobuf -iquote bazel-out/stub_armeabi-v7a-opt/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/stub_armeabi-v7a-opt/genfiles/external/bazel_tools -isystem external/protobuf/src -isystem bazel-out/stub_armeabi-v7a-opt/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -c external/protobuf/src/google/protobuf/util/internal/type_info_test_helper.cc -o bazel-out/stub_armeabi-v7a-opt/bin/external/protobuf/_objs/protobuf/external/protobuf/src/google/protobuf/util/internal/type_info_test_helper.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
Target //tensorflow/examples/android:tensorflow_native_libs failed to build
____Elapsed time: 67.998s, Critical Path: 7.19s
FAILED
So, I managed to build tenserflow natively for my linux box with basel.... but it won't crosse compile for the android demo app with the 'basel' build tool....  :/
		</comment>
		<comment id='20' author='Androbin' date='2017-05-29T21:58:18Z'>
		mmh managed to build the android app with make  and found why it wouldn't build with basel (used a wrong git clone command missing recurse subprojects).
Will now try to build the android app with Bazel and then with custom ops (selective) and kernels
		</comment>
		<comment id='21' author='Androbin' date='2017-05-30T20:04:27Z'>
		I managed to crosscompile tensorFlow with Basel and to build an android app with the native library.
Of course, the app crashed at runtime as it was still missing types (DT_BOOL) and probably ops too.
Today, I have been trying to cross-compile libtensorflow_inference.so  for android with more types/ops without success :
I created a "ops_to_register.h" header file that I put in tenserflow/core/framework with the print_selective_registration_header.py utility
But I could not find the bazel command I should use or the files I should modify to build  libtensorflow_inference.so with selective_registration/ SUPPORT_SELECTIVE_REGISTRATION or with ANDROID_FULL_TYPE
The documentations is severely lacking concerning this. Lost one more day looking for a solution (losing 10/30 minutes each time I was trying a bazel build)
Could someone please tell me how to use selective_registration, pretty please ?
see issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/10299&gt;#10299&lt;/denchmark-link&gt;

		</comment>
		<comment id='22' author='Androbin' date='2017-05-31T12:51:20Z'>
		&lt;denchmark-link:https://github.com/Lakedaemon&gt;@Lakedaemon&lt;/denchmark-link&gt;
 The following should work:

		</comment>
		<comment id='23' author='Androbin' date='2017-06-01T13:28:00Z'>
		I have tried a few commands :

bazel build -c opt --copt="-DSELECTIVE_REGISTRATION" tensorflow/contrib/android:libtensorflow_inference.so --config=android_arm

-&gt; I get a 37 MB libtensorflow_inference.
When compiling in an android apk, I get
:app:transformNativeLibsWithStripDebugSymbolForDebug/home/lakedaemon/Android/Sdk/ndk-bundle/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-strip:/home/lakedaemon/IdeaProjects/Handwriter/app/build/intermediates/transforms/mergeJniLibs/debug/folders/2000/1f/main/lib/armeabi-v7a/libtensorflow_inference.so: File format not recognized
So, the file format might be wrong
When trying my app on an arm device, I get
dlopen("/data/app-lib/org.lakedaemon.handwriter-5/libtensorflow_inference.so") failed: dlopen failed: "/data/app-lib/org.lakedaemon.handwriter-5/libtensorflow_inference.so" not 32-bit: 2
and then a java.lang.RuntimeException: Native TF methods not found; check that the correct native libraries are present in the APK

bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --config=android_arm

-&gt; I get a 91.2 MB libtensorflow_inference. Which means that the "-DSELECTIVE_REGISTRATION" looks like it is working.
But I get the same errors on android...
investigating further
		</comment>
		<comment id='24' author='Androbin' date='2017-06-01T13:53:01Z'>
		
bazel build -c opt --copt="-DSELECTIVE_REGISTRATION" //tensorflow/contrib/android:libtensorflow_inference.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a

-&gt; I get a 3.7 MB libtensorflow_inference.
no android error at compile time/when loading the .so code in Java but the infamous
java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Switch' with these attrs.  Registered devices: [CPU], Registered kernels:
device='CPU'; T in [DT_FLOAT]
device='CPU'; T in [DT_INT32]
device='GPU'; T in [DT_STRING]
device='GPU'; T in [DT_BOOL]
device='GPU'; T in [DT_INT32]
device='GPU'; T in [DT_FLOAT]
&lt;denchmark-code&gt;                                                                       	 [[Node: dropout_1/cond/Switch = Switch[T=DT_BOOL](dropout_1/keras_learning_phase, dropout_1/keras_learning_phase)]]
&lt;/denchmark-code&gt;

so, either selective_registration doesn't work or
it doesn't load the DT_BOOL type which might be possible since register.types.h uses this condition
#if !defined(IS_MOBILE_PLATFORM) || defined(SUPPORT_SELECTIVE_REGISTRATION)
and I couldn't find any place where SUPPORT_SELECTIVE_REGISTRATION was defined
so, next I'm trying to define it also in the copt arguments
		</comment>
		<comment id='25' author='Androbin' date='2017-06-01T14:33:08Z'>
		and the winner is.....
bazel build -c opt --copt="-DSELECTIVE_REGISTRATION" --copt="-DSUPPORT_SELECTIVE_REGISTRATION" //tensorflow/contrib/android:libtensorflow_inference.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a
I get a 3.9 MB libtensorflow_inference.so that doesn't crash the app because of tensorflow code
(well, it crashes later on, but because of a bug in MY code, which is much less frustrating as I'm going to be able to fix it quick)
So, either the tenserflow developers intended to have 2 different flags :
SELECTIVE_REGISTRATION for ops and SUPPORT_SELECTIVE_REGISTRATION for types and this is not documented either it is a bug in the tenserflow code
Please fix ! &lt;denchmark-link:https://github.com/petewarden&gt;@petewarden&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/andrewharp&gt;@andrewharp&lt;/denchmark-link&gt;

		</comment>
		<comment id='26' author='Androbin' date='2017-06-01T15:10:21Z'>
		&lt;denchmark-link:https://github.com/Lakedaemon&gt;@Lakedaemon&lt;/denchmark-link&gt;
 Look what I found in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/BUILD#L1004&gt;tensorflow/core/BUILD#L1004&lt;/denchmark-link&gt;

One more question: What is  about?
		</comment>
		<comment id='27' author='Androbin' date='2017-06-01T15:26:44Z'>
		Seems that SUPPORT_SELECTIVE_REGISTRATION is used in core and all BUILD files while SELECTIVE_REGISTRATION appears further away from core (python tools). These may be outdated references:
&lt;denchmark-link:../blob/master/tensorflow/python/tools/selective_registration_header_lib.py&gt;tensorflow/python/tools/selective_registration_header_lib.py&lt;/denchmark-link&gt;

&lt;denchmark-link:../blob/master/tensorflow/python/tools/print_selective_registration_header.py&gt;tensorflow/python/tools/print_selective_registration_header.py&lt;/denchmark-link&gt;

&lt;denchmark-link:../blob/master/tensorflow/core/framework/selective_registration.h&gt;tensorflow/core/framework/selective_registration.h&lt;/denchmark-link&gt;

		</comment>
		<comment id='28' author='Androbin' date='2017-06-01T15:39:51Z'>
		yes, I read that stuff in tensorflow/core/BUILD#L1004 while greping for SELECTIVE_REGISTRATION but I could not find a way to build lib_tensorflow_inference.so with this target.
Bazel is really new to me (and to any non-googler I guess), learned a lot of things about it while trying to find a solution to my issue this past feaw days (the BUILD language, the if_android "pythonic"-functions/macro, ..)
found about android_arm while greping for it, but surprisingly, it doesn't produce valid android lib for me. probably because it doesn't set --host_crosstool_top=@bazel_tools//tools/cpp:toolchain
a complete guess : proto_rtti might be a version of the protocol buffer lib without support for run type information
		</comment>
		<comment id='29' author='Androbin' date='2017-06-01T15:49:02Z'>
		&lt;denchmark-link:https://github.com/Lakedaemon&gt;@Lakedaemon&lt;/denchmark-link&gt;
 I share your disorientation.
Interestingly, wherever  or its expansion is used, it adds  to it.
Maybe &lt;denchmark-link:https://github.com/andrewharp&gt;@andrewharp&lt;/denchmark-link&gt;
 can help clarifying things here.
		</comment>
		<comment id='30' author='Androbin' date='2017-07-25T00:44:05Z'>
		Hey guys! I've been having the same problem trying to make this work on android and I have used some of the solutions from above but none seem to work...here are some of the errors I get back from the various methods of trying to solve this problem...Any Help would be appreciated!
The models work when I run it on python but doesn't when I run them on android.
Im using the SSD_Mobilenet model from tensorflow object detection api.
This is from the optimize for inference function:
NodeDef expected inputs '' do not match 1 inputs specified; Op&lt;name=Const; signature= -&gt; output:dtype; attr=value:tensor; attr=dtype:type&gt;; NodeDef: Preprocessor/map/while/add/y = Constdtype=DT_INT32, value=Tensor&lt;type: int32 shape: [] values: 1&gt;
This is from renaming Switch to Identify:
Not a valid TensorFlow Graph serialization: Node 'Preprocessor/map/while/add/y': Connecting to invalid output 1 of source node Preprocessor/map/while/Switch which has 1 outputs
This last one loads the model but crashes when processing the inputs:
java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Switch' with these attrs. Registered devices: [CPU], Registered kernels:
device='GPU'; T in [DT_STRING]
device='GPU'; T in [DT_BOOL]
device='GPU'; T in [DT_INT32]
device='GPU'; T in [DT_FLOAT]
device='CPU'; T in [DT_FLOAT]
device='CPU'; T in [DT_INT32]
&lt;denchmark-code&gt;                                                          	 [[Node: Postprocessor/BatchMultiClassNonMaxSuppression/PadOrClipBoxList/cond/Switch = Switch[T=DT_BOOL](Postprocessor/BatchMultiClassNonMaxSuppression/PadOrClipBoxList/Greater, Postprocessor/BatchMultiClassNonMaxSuppression/PadOrClipBoxList/Greater)]]
&lt;/denchmark-code&gt;

Any ideas guys? I've also tried to recompile tensorflow with the changes from &lt;denchmark-link:url&gt;https://stackoverflow.com/questions/40855271/no-opkernel-was-registered-to-support-op-switch-with-these-attrs-on-ios/43627334#43627334&lt;/denchmark-link&gt;
 but bazel crashes while compiling :(
		</comment>
		<comment id='31' author='Androbin' date='2017-07-25T07:06:28Z'>
		If you are using keras, don't rename node to identity to strip switches from learning to inference.
In keras, you train your model, you save weigths and when you want to export your frozen model to tenserflow, you :

launch a new python script FROM SCRATCH that

K.set_learning_phase(0) inputShape = (32, 32, 1) model = self.buildModel(inputShape, self.nbClasses)# I'm building the model from scratch here model.load_weights('weights-improvement-12-0.96.hdf5') saver = tf.train.Saver() saver.save(K.get_session(), self.exportPath + self.modelName, global_step=self.step) self.frozenOptimizedQuantizied()# I optimize things here 

load your weigths (with learning phase set to false) -&gt; the useless switches will be stripped
then you save your model to tenserflow
after that, you apply a python method to freeze your graph
afterwards, you use the transfrom tools (you need bazel) to optimize your model for android
then it works really great
I have a 5MB model that classify 3000 japanese characters on android blazingly fast :
The guys who developped NN and tensorflow are geniuses. This is one of the best tools ever...

		</comment>
		<comment id='32' author='Androbin' date='2017-10-16T13:08:48Z'>
		I am trying to do something exactly similar. I was able to save the model to a graph.pb file and use it in android studio.
The nodes are as such:
[print(n.name) for n in tf.get_default_graph().as_graph_def().node]
&gt;&gt;&gt;[print(n.name) for n in tf.get_default_graph().as_graph_def().node]
keras_learning_phase
Sigmoid
[None, None]
I am trying to read the input and output node but it doesnt recognize keras_learning_phase. I tried using the summarize_graph to read the summary of the nodes but it throws an error.
(tensorflow) azainab:tensorflow-master_2 azain$ bazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=graph.pb
2017-10-16 16:07:48.147635: E tensorflow/tools/graph_transforms/summarize_graph_main.cc:283] Loading graph 'graph.pb' failed with graph.pb
(for file graph.pb)
2017-10-16 16:07:48.148816: E tensorflow/tools/graph_transforms/summarize_graph_main.cc:285] usage: bazel-bin/tensorflow/tools/graph_transforms/summarize_graph
Flags:
--in_graph=""                    	string	input graph file name
--print_structure=false          	bool	whether to print the network connections of the graph
Can somebody help please
&lt;denchmark-link:https://github.com/Lakedaemon&gt;@Lakedaemon&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/andrewharp&gt;@andrewharp&lt;/denchmark-link&gt;

		</comment>
		<comment id='33' author='Androbin' date='2017-10-16T16:30:42Z'>
		In my experience, if you still have keras_learning_phase node in your model after you have trained it and frozen/optimized it in python for use in java/android, you have been doing something wrong when using the keras api.
You have to reload your model after training and before optimizing in a completely new python process (i;e. you launch your python file on another line of your console)
		</comment>
		<comment id='34' author='Androbin' date='2017-10-16T16:32:50Z'>
		Also, there aren't many nodes in your model. This feel weirds (my model had like 60+ nodes before optimizing and maybee half after optimizing, but I still had like 20 or 30 nodes....)
		</comment>
	</comments>
</bug>