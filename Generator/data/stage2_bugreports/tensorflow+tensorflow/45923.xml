<bug id='45923' author='Flamefire' open_date='2020-12-22T16:11:46Z' closed_time='2021-01-13T16:47:29Z'>
	<summary>Loss reported by fit vs custom_loop</summary>
	<description>
&lt;denchmark-h:h2&gt;URL(s) with the issue:&lt;/denchmark-h&gt;

&lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/custom_training&gt;https://www.tensorflow.org/tutorials/distribute/custom_training&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Description of issue (what needs changing):&lt;/denchmark-h&gt;

I wanted to compare the  performance vs a "custom loop" with a  strategy according to the tutorials at &lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/keras&gt;https://www.tensorflow.org/tutorials/distribute/keras&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/custom_training&gt;https://www.tensorflow.org/tutorials/distribute/custom_training&lt;/denchmark-link&gt;

However I'm seeing differences in the loss reported. Examples for an MNIST dataset with a Lenet:

fit: Eval loss: 32.09, Eval Accuracy: 0.9307
custom loop: Eval loss: 1.742, Eval Accuracy: 0.965

Also the loss reported in the logs member of the callbacks seems to be higher by some factor compared to the custom_loop.
&lt;denchmark-h:h3&gt;Clear description&lt;/denchmark-h&gt;

At &lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/custom_training#define_the_loss_function&gt;https://www.tensorflow.org/tutorials/distribute/custom_training#define_the_loss_function&lt;/denchmark-link&gt;
 I found the following:


Using tf.reduce_mean is not recommended. Doing so divides the loss by actual per replica batch size which may vary step to step.
This reduction and scaling is done automatically in keras model.compile and model.fit
... SUM_OVER_BATCH_SIZE is disallowed because currently it would only divide by per replica batch size, and leave the dividing by number of replicas to the user, which might be easy to miss. So instead we ask the user do the reduction themselves explicitly.


So question here which isn't clear to me from reading the docs/tutorials: How is the loss calculated by model.compile and model.fit and is that expected to be the same as the one shown in the tutorial for custom_loops?
What I recon is that it should be the average loss per sample over the whole batch (on all replicas). So the sum of all per_example_loss divided by the number of examples.
The custom_loop tutorial divides by the global batch size, i.e. the local batch size times the number of replicas. This would be wrong for the mentioned case where the "per replica batch size [...] may vary step to step." So this sounds like tf.reduce_mean would be useful with an additional step of averaging the per_replica_losses instead of summing them after the strategy.run part. Why is that not recommended?
	</description>
	<comments>
		<comment id='1' author='Flamefire' date='2020-12-23T08:08:41Z'>
		&lt;denchmark-link:https://github.com/Flamefire&gt;@Flamefire&lt;/denchmark-link&gt;

Can you please share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!
		</comment>
		<comment id='2' author='Flamefire' date='2020-12-31T05:55:37Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='3' author='Flamefire' date='2021-01-04T08:08:20Z'>
		&lt;denchmark-link:https://github.com/Flamefire&gt;@Flamefire&lt;/denchmark-link&gt;
,
Can you please respond to the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/45923#issuecomment-749998924&gt;above comment&lt;/denchmark-link&gt;
? Thanks!
		</comment>
		<comment id='4' author='Flamefire' date='2021-01-05T15:36:21Z'>
		I created a minified example which trains the same model 2 times: once with the fit function and once with the custom loop. Both variants follow very closely the tutorial code and should train the same network on the same dataset for the same number of batches resulting in the same losses.
However the loss for the custom loop at the end is significantly lower than that from , see &lt;denchmark-link:https://colab.research.google.com/drive/1G_r2pU9usKgB5U_MAEkBZ0eJyK48YMta?usp=sharing&gt;https://colab.research.google.com/drive/1G_r2pU9usKgB5U_MAEkBZ0eJyK48YMta?usp=sharing&lt;/denchmark-link&gt;

When using multiple devices (may require GPUs, set num_gpus = 2) then an issue becomes visible: Batch 0 has a loss of 4.609 (custom) vs 2.304 (fit) which makes me wonder if the scaling is indeed correct.
		</comment>
		<comment id='5' author='Flamefire' date='2021-01-07T13:03:22Z'>
		Ok I found the issue(s):

I was missing a strategy.experimental_distribute_dataset. Not sure if this is new as I started writing the code for TF 2.0
fit reports the loss via the Mean metric, so the loss is (likely) always higher then what I manually had which reported the loss for the current batch(es)

It might be worth mentioning that in the tutorial for custom loops
		</comment>
		<comment id='6' author='Flamefire' date='2021-01-12T19:48:21Z'>
		Hi &lt;denchmark-link:https://github.com/Flamefire&gt;@Flamefire&lt;/denchmark-link&gt;
,  should be mentioned everywhere custom training loops + distribution strategy are discussed. However, if you find somewhere it's missing let me know.
As for updating the custom training loops tutorial, can you elaborate on what you think should be added and where?
		</comment>
		<comment id='7' author='Flamefire' date='2021-01-13T10:32:25Z'>
		
As for updating the custom training loops tutorial, can you elaborate on what you think should be added and where?

The missing link for me was that  reports the loss as the Mean, also in callbacks. &lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/custom_training#training_loop&gt;https://www.tensorflow.org/tutorials/distribute/custom_training#training_loop&lt;/denchmark-link&gt;
 has  so I guess that is similar enough, but seems like I missed it. Maybe using the  metric here would be a good thing to show so users can output the current mean loss inbetween batches when starting with the tutorial code?
Anyway at the current state nothing seems to be really missing, so I'd also be fine to close this now. Thanks a lot!
		</comment>
		<comment id='8' author='Flamefire' date='2021-01-13T16:47:29Z'>
		Thanks! I will close this issue now.
		</comment>
	</comments>
</bug>