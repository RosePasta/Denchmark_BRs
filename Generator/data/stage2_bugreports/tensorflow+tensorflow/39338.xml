<bug id='39338' author='Frank-Dz' open_date='2020-05-09T08:29:01Z' closed_time='2020-05-12T12:49:55Z'>
	<summary>TypeError: 'DatasetV1Adapter' object is not subscriptable</summary>
	<description>
System information

OS Platform and Distribution: Linux Ubuntu 16.04
TensorFlow version (use command below):1.15.0 (I have to use tf 1.x rather than tf 2.x)
Python version: python3.7
CUDA/cuDNN version: 10.0
GPU model and memory: Nvidia 8G
Error

&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/home/frank/PycharmProjects/reconstruction_NN/my_test.py", line 77, in &lt;module&gt;
    train_script(model,example,opt)
  File "/home/frank/PycharmProjects/reconstruction_NN/my_test.py", line 54, in train_script
    output = model(example['my_x'])
TypeError: 'DatasetV1Adapter' object is not subscriptable
&lt;/denchmark-code&gt;

Full Code
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow.keras.layers import Input, Flatten, Dense, Lambda, Conv2D, Reshape, MaxPool2D, Average, Dropout, Concatenate, \
    Add, Maximum, Layer, Activation, Conv1D, TimeDistributed, GlobalAvgPool2D
import numpy as np
tf.compat.v1.enable_eager_execution()
print(tf.__version__)
print(tf.executing_eagerly())


class Test(tf.keras.Model):
    def __init__(self,attention_sz,dropout_rt, name=None):
        super(Test, self).__init__(name=name)
        # here we define the layer:
        self.fc = Dense(attention_sz,input_dim = attention_sz ,activation='relu')
        self.fc2 = Dense(attention_sz, activation='relu')
        self.fc3 = Dense(1, activation='sigmoid')

        self.dp = Dropout(dropout_rt,input_shape=(attention_sz,))
        self.dp2 = Dropout(dropout_rt,input_shape=(attention_sz,))


    def call(self, inp):
        # here we get the segmentation and pose
        with tf.device('/gpu:0'):
            print("~~~~~~~~~~~")
            x = self.fc(inp)
            print(x.shape)
            z = self.dp(x)
            print(z.shape)
            x = self.fc2(z)
            print(x.shape)
            z = self.dp2(x)
            print(z.shape)
            y = self.fc3(z)
            print(y.shape)
        return y # here z is the weight.


    # here we overwrite the method.
    def save(self, checkpoint_path):
        print("Saving model...")
        self.save_weights(checkpoint_path)
        print("Model saved")
    def load(self, checkpoint_path):
        print("Loading model checkpoint {} ...\n".format(checkpoint_path))
        self.load_weights(checkpoint_path)
        print("Model loaded")

checkpoint_path = "saved_model/"


def train_script(model, example, optimizer):
    with tf.GradientTape() as tape:
        output = model(example['my_x'])
        loss = tf.reduce_mean(tf.abs(output - example['my_y']))
    variables = model.trainable_variables
    gradients = tape.gradient(loss, variables)
    optimizer.apply_gradients(zip(gradients, variables))
    return loss

if __name__ == '__main__':
    model = Test(1024, 0.05)

    x = np.round(np.random.normal(1.75, 0.2, size=(10000, 1024)), 2)
    x2 = np.round(np.random.normal(100.75, 0.2, size=(10000, 1024)), 2)
    labels = np.zeros((10000, 1))
    labels2 = np.ones((10000, 1))
    x_t = np.row_stack((x, x2))
    labels = np.row_stack((labels, labels2))
    x_t = tf.convert_to_tensor(x_t)
    labels = tf.convert_to_tensor(labels)

    example = tf.data.Dataset.from_tensor_slices(
        dict(my_x=x_t, my_y=labels)).repeat().batch(2)

    opt = tf.keras.optimizers.Adam(0.1)
    train_script(model,example,opt)

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Frank-Dz' date='2020-05-09T08:30:13Z'>
		Please note that eager mode is enabled.
&lt;denchmark-code&gt;tf.compat.v1.enable_eager_execution()
print(tf.__version__)
print(tf.executing_eagerly())
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='Frank-Dz' date='2020-05-11T07:39:57Z'>
		I have tried in colab with TF version 1.15 and was able to reproduce the issue.However i tried with TF version 2.2-rc4 and was seeing different error message.().Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/d9cdedde6cfd26f7d33c545550077064/untitled872.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='3' author='Frank-Dz' date='2020-05-11T23:11:28Z'>
		&lt;denchmark-link:https://github.com/Frank-Dz&gt;@Frank-Dz&lt;/denchmark-link&gt;
 Your code snippet fails irrespective of eager execution enabled/disabled in TF 1.15
However you should try passing example object as iterable as suggested by the error message.
You can achieve this by setting:
def train_script(model, example, optimizer):
    with tf.GradientTape() as tape:
        example = list(example.as_numpy_iterator()) # add this line
        output = model(example['my_x'])
        loss = tf.reduce_mean(tf.abs(output - example['my_y']))
       ... 
See &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly#from_tensor_slices&gt;https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly#from_tensor_slices&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='Frank-Dz' date='2020-05-12T12:49:55Z'>
		Thanks! Well solved!
		</comment>
		<comment id='5' author='Frank-Dz' date='2020-05-12T12:49:57Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39338&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39338&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>