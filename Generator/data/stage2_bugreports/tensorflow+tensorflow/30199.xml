<bug id='30199' author='Jarboui' open_date='2019-06-27T11:15:58Z' closed_time='2019-07-23T19:53:47Z'>
	<summary>Gradient returns "Nan" values when using triplet loss with similar batches</summary>
	<description>
Hi,
I am using tf 1.12.0.
My issue is related to this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/783&gt;one&lt;/denchmark-link&gt;
.
It seems that it was closed judging it the benefit are marginal with respect to the effort.
I stumbled on this problem as I was working on my use case. I am not sure how common this is, but I think it deserves a second look.
I am working on sequence representation learning, and when generating &lt;denchmark-link:https://arxiv.org/abs/1703.07737&gt;my triplet loss batches&lt;/denchmark-link&gt;
, I end up sometime generating positives that are similar to my anchor.
This causes the whole gradient to become "Nan".
&lt;denchmark-link:https://user-images.githubusercontent.com/38140485/60261705-d547f280-98dc-11e9-9e5d-e7b84348ff0a.png&gt;&lt;/denchmark-link&gt;

The suggested work around of replacing nan values with zeros post-computation isn't sufficient in this case because in reality not the whole batch gradient is null.
PS: I am generating the sequences randomly, however with relatively small vocabulary size and horizon, generating similar sequences does happen.
For those having the same issue, in my use case, the work around I opted for is to either condition my samples or add a small random noise.
However I think implementing the discussed "zero" solution would be really helpful for future work on sequence representation learning.
Thanks :)
	</description>
	<comments>
		<comment id='1' author='Jarboui' date='2019-06-28T10:09:10Z'>
		&lt;denchmark-link:https://github.com/Jarboui&gt;@Jarboui&lt;/denchmark-link&gt;
 Will it be possible to provide us the ipynb file. It will indeed help us to move faster. Thanks!
		</comment>
		<comment id='2' author='Jarboui' date='2019-06-28T16:46:15Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 I can't upload the ipynb in the comment section.
I will past the code bellow for your convenience :)
import tensorflow as tf
from keras.layers import *
from keras.models import Model
import numpy as np

# I define the representation model
inp_dim = 5
outp_dim = 5
inp = Input(shape=( inp_dim,), name="input")
outp = Dense(outp_dim, activation='tanh', name="simple_layer")(inp)
model = Model(inputs=inp, outputs=outp) 

# I define over here part of my overal loss (note that defining the full loss doesn't change the output.)
anchor = tf.placeholder(tf.float32, [None, inp_dim], name="representation_shift")
positive = tf.placeholder(tf.float32, [None, inp_dim], name="representation_shift")
outp_anchor = model(anchor)
outp_positive = model(positive)
u = tf.math.subtract(outp_anchor, outp_positive)
u_norm = tf.linalg.norm(u, axis=-1) #u_norm is the loss am trying to minimize in this example

#I define here my gradient operations
_OPTIMIZER = tf.train.AdamOptimizer()
grads_and_vars = _OPTIMIZER.compute_gradients(u_norm)

sess = tf.Session()
init_op = tf.global_variables_initializer()
sess.run(init_op)

# I generate a dummy example
batch_size = 10
random_input1 = np.random.rand(batch_size, inp_dim)
random_input2 = np.random.rand(batch_size, inp_dim)
# And I force it to the case where parts of the data is similar (juste the first line actually)
random_input2[0] = random_input1[0] 

def replace_none_with_zero(l):
  return [0 if i==None else i for i in l]
grads= replace_none_with_zero(grads_and_vars)

sess.run(grads, feed_dict={
    anchor: random_input1,
    positive: random_input2, 
})[0][0]
# This outputs "nan" erroneously 
I hope this could be helpful :)
		</comment>
		<comment id='3' author='Jarboui' date='2019-07-01T04:15:28Z'>
		I could reproduce the issue on colab with Tensorflow 1.12.0. by adding following lines of code
&lt;denchmark-code&gt;def replace_none_with_zero(l):
  return [0 if i==None else i for i in l]
grads= replace_none_with_zero(grads_and_vars)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='Jarboui' date='2019-07-23T19:53:47Z'>
		The issue here is that u_norm will be zero and the gradient of a zero norm in TF is NaN. You can use the double-where trick to replace zeros (or small values in general) with something valid, and then things should work:
&lt;denchmark-code&gt;u_safe = tf.where(tf.abs(u) &gt; 0.001, u, tf.ones_like(u))
u_norm = tf.linalg.norm(u_safe, axis=-1) #u_norm is the loss am trying to minimize in this example
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='Jarboui' date='2019-07-23T19:53:48Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30199&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30199&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>