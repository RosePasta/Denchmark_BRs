<bug id='39702' author='chuanli11' open_date='2020-05-20T06:14:06Z' closed_time='2020-05-20T22:00:02Z'>
	<summary>Customized loss function requires eager tensor, but symbolic tensor is passed</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): tensorflow-gpu==2.2.0
Python version:3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.1/7.6.5
GPU model and memory: Quadro RTX 8000 / 48GB

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)" v2.2.0-rc4-8-g2b96f3662b 2.2.0

Describe the current behavior
I need to apply a binary mask to the model output for computing loss. My current implementation uses a model that takes two inputs (the data and the mask), and use function closure to implement the customized loss.
However, this raises the error "tensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors".
Apparently, the mask input is treated as symbolic tensor.
Describe the expected behavior
This only happens in the eager mode. Apply disable_eager_execution() will eliminate the problem. However, I want to know if there is any way to make this work in the eager mode.
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
This is the &lt;denchmark-link:https://colab.research.google.com/drive/1D_GitVtknoYk33hURTXzC0murA60UBH0?usp=sharing&gt;gist&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np

import tensorflow.keras.backend as K

from tensorflow.keras.layers import Input, Flatten, Dense
from tensorflow.keras import Model

x_data = np.zeros((32, 28, 28))
x_mask = np.zeros((32, 10))
y = np.zeros((32, 10))

input_data = Input(shape=(28, 28))
input_mask = Input(shape=(10,))

output= Flatten()(input_data)
output = Dense(64, activation='relu')(output)
output = Dense(10)(output)
model = Model(inputs=[input_data, input_mask], outputs=output)


def custom_loss():
    def loss(y_true, y_pred):
        # This line causes the error
        return K.mean(K.square(y_true - y_pred * model.inputs[1]), axis=-1)   

        # This line doesn't cause the error
        # return K.mean(K.square(y_true - y_pred), axis=-1)   
    return loss


model.compile(
    optimizer=tf.keras.optimizers.SGD(),
    loss=custom_loss(),
    metrics=['accuracy'])

for i in range (2):
    print(i)
    model.train_on_batch([x_data, x_mask], y)
&lt;/denchmark-code&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='chuanli11' date='2020-05-20T09:44:41Z'>
		I have tried in colab with TF version 2.2 ,nightly version(2.3.0-dev20200519) and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/fb1237bf883632c27c2c617d100fbf44/untitled913.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='2' author='chuanli11' date='2020-05-20T22:00:02Z'>
		&lt;denchmark-link:https://github.com/chuanli11&gt;@chuanli11&lt;/denchmark-link&gt;
 Thanks for the issue!
Yes, the model.inputs are symbolic Tensors. For Functional Models, these Tensors are used to build the Model with a static graph, but in eager mode the Model is then executed with a tf.function. This means that model.inputs can't be used in losses, metrics, etc.
Instead, here's how I'd recommend achieving your use case. Essentially, sample_weight will handle this, rather than trying to access the mask an a keras.Input:
import tensorflow as tf
import numpy as np

from tensorflow.keras.layers import Input, Flatten, Dense
from tensorflow.keras import Model

x_data = np.zeros((32, 28, 28))
x_mask = np.zeros((32, 10))
y = np.zeros((32, 10))

input_data = Input(shape=(28, 28))
output= Flatten()(input_data)
output = Dense(64, activation='relu')(output)
output = Dense(10)(output)
model = Model(inputs=input_data, outputs=output)


class MyLoss(tf.keras.losses.Loss):
  def call(self, y_true, y_pred):
      return (y_true - y_pred) ** 2  


model.compile(
    optimizer=tf.keras.optimizers.SGD(),
    loss=MyLoss(name='loss'),
    metrics=['accuracy'])

for i in range (2):
    print(i)
    loss = model.train_on_batch(x_data, y, sample_weight=x_mask)
Hope that helps!
Closing out as this is intended behavior
		</comment>
		<comment id='3' author='chuanli11' date='2020-05-20T22:00:04Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39702&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39702&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='chuanli11' date='2020-05-21T23:10:56Z'>
		&lt;denchmark-link:https://github.com/omalleyt12&gt;@omalleyt12&lt;/denchmark-link&gt;
 Thank you. This is really helpful.
		</comment>
		<comment id='5' author='chuanli11' date='2020-08-13T01:25:35Z'>
		&lt;denchmark-link:https://github.com/omalleyt12&gt;@omalleyt12&lt;/denchmark-link&gt;
 I'm in a similar situation but I can not convert my code to not use model.inputs. Is there any other workaround for this?
In v2.0.1, I was using experimental_run_tf_function=False in model.compile() to hack around this issue. But doing run_eagerly=True on v.2.3.0 doesn't seem to work.
I have custom losses and metrics both using inputs. I am also using TFRecordDataset.
Using run_eagerly=True, I get this -&gt; AttributeError: 'Tensor' object has no attribute 'numpy'.
Here is the &lt;denchmark-link:https://colab.research.google.com/drive/1wpL8WNT7gtOdL6V9_tK5AQ0M6mBPVkNI?usp=sharing&gt;gist&lt;/denchmark-link&gt;

And as above, I have verified that it works fine without custom losses and metrics.
Any help is appreciated.
		</comment>
		<comment id='6' author='chuanli11' date='2020-08-13T02:58:44Z'>
		Tagging &lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 as you had tagged this solution in other issue threads.
		</comment>
		<comment id='7' author='chuanli11' date='2020-08-13T19:12:00Z'>
		&lt;denchmark-link:https://github.com/lastmansleeping&gt;@lastmansleeping&lt;/denchmark-link&gt;
 Can you please create a new issue with the standalone code you provided above? Thanks!
		</comment>
	</comments>
</bug>