<bug id='33607' author='bhavitvyamalik' open_date='2019-10-22T12:33:05Z' closed_time='2019-11-07T04:15:04Z'>
	<summary>Coral Dev Board runtime error during prediction</summary>
	<description>
I'm working on a classification problem. I used Inception v3 model in Keras for classification. I did post training quantization and converted my hdf5 model to TFLite. Further, I compiled my TFLite model to use it on TPU instead of CPU on dev board. It's giving runtime error when I run this code during prediction on dev board
Describe the current behavior
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "inference_try.py", line 25, in &lt;module&gt;
    interpreter.allocate_tensors()
  File "/home/mendel/.local/lib/python3.5/site-packages/tflite_runtime/interpreter.py", line 244, in allocate_tensors
    return self._interpreter.AllocateTensors()
  File "/home/mendel/.local/lib/python3.5/site-packages/tflite_runtime/interpreter_wrapper.py", line 114, in AllocateTensors
    return _interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
RuntimeError: Internal: :71 tf_lite_type != kTfLiteUInt8 (9 != 3)Node number 5 (EdgeTpuDelegateForCustomOp) failed to `prepare.`
&lt;/denchmark-code&gt;

Describe the expected behavior
Should predict the outcomes successfully and not give error on interpreter.allocate_tensors() statement.
Code to reproduce the issue
&lt;denchmark-code&gt;from tflite_runtime.interpreter import Interpreter
from tflite_runtime.interpreter import load_delegate

interpreter = Interpreter(
      model_path="/home/mendel/classification/inceptionV3Q_edgetpu.tflite",
      experimental_delegates=[load_delegate('libedgetpu.so.1.0')])

interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

interpreter.set_tensor(input_details[0]['index'], img)
interpreter.invoke()
output_details1 = interpreter.get_output_details()[0]
answer= np.squeeze(interpreter.get_tensor(output_details1['index']))
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='bhavitvyamalik' date='2019-10-22T16:20:17Z'>
		See also &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33558&gt;#33558&lt;/denchmark-link&gt;
, and my comment there. &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33558#issuecomment-545041049&gt;#33558 (comment)&lt;/denchmark-link&gt;
. Appears to crash when model or input tensor is large, despite the fact that my model easily fits into TPU on-chip memory.
		</comment>
		<comment id='2' author='bhavitvyamalik' date='2019-10-22T17:54:23Z'>
		The model you mentioned i.e., model.tflite, was that working fine or still giving error while executing? Looks like I'll need to reduce my model size to a huge extent so as to make it run successfully.
&lt;denchmark-code&gt;Edge TPU Compiler version 2.0.267685300

Model compiled successfully in 2804 ms.

Input model: inceptionV3Q.tflite
Input size: 21.30MiB
Output model: inceptionV3Q_edgetpu.tflite
Output size: 22.49MiB
On-chip memory available for caching model parameters: 4.75MiB
On-chip memory used for caching model parameters: 4.74MiB
Off-chip memory used for streaming uncached model parameters: 16.91MiB
Number of Edge TPU subgraphs: 1
Total number of operations: 165
Operation log: inceptionV3Q_edgetpu.log

Model successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.
Number of operations that will run on Edge TPU: 161
Number of operations that will run on CPU: 4
See the operation log file for individual operation details.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='bhavitvyamalik' date='2019-10-22T18:38:50Z'>
		My model compiled and executed on the TPU without runtime error after I reduced model size (though I don't see why size reduction is/was needed, given the modest size even of my larger model). I did not retrain the smaller model, just used random weights as proof of concept.
Yes, &lt;denchmark-link:https://github.com/bhavitvyamalik&gt;@bhavitvyamalik&lt;/denchmark-link&gt;
, your model looks very large. I have no idea what the delays would be like when using "off-chip memory used for streaming uncached model parameters."
		</comment>
		<comment id='4' author='bhavitvyamalik' date='2019-10-23T08:12:55Z'>
		&lt;denchmark-link:https://github.com/bhavitvyamalik&gt;@bhavitvyamalik&lt;/denchmark-link&gt;
, Can you try the &lt;denchmark-link:https://github.com/mattroos&gt;@mattroos&lt;/denchmark-link&gt;
's suggestion and let us know how it progresses. Thanks!
		</comment>
		<comment id='5' author='bhavitvyamalik' date='2019-10-24T15:46:57Z'>
		Since I was using InceptionV3 as my base model so even after reducing image size to 100x100 my model size was around 85MB which as &lt;denchmark-link:https://github.com/mattroos&gt;@mattroos&lt;/denchmark-link&gt;
 mentioned was too large. Eventually I had to change my base model to MobileNetV2 to bring model size under 3MB and then it worked like a charm!
&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 I'm still working on running InceptionV3 model on Edge TPU. Even though official documentation says inceptionV3 is supported still it doesn't work. Please look into if possible. Thanks!
		</comment>
		<comment id='6' author='bhavitvyamalik' date='2019-11-07T04:15:04Z'>
		I was able to run InceptionV3 model on EdgeTPU. The issue was not with size of the model but with the quantization process. Here is the code on how to quantize and convert the code to TFLite:
&lt;denchmark-code&gt;def representative_data_gen():
  for input_value in mnist_data[:100]:      //mnist_data list contains images
    data = np.array([input_value])
    yield [data]

opt = tf.lite.Optimize.DEFAULT
ops = tf.lite.OpsSet.TFLITE_BUILTINS_INT8
dtype = tf.uint8

converter = tf.lite.TFLiteConverter.from_keras_model_file(model_path)

converter.optimizations = [opt]
converter.representative_dataset = representative_data_gen
converter.target_spec.supported_ops = [ops]
converter.inference_input_type = dtype
converter.inference_output_type = dtype

tflite_quant_model = converter.convert()
open("model_quantised.tflite", "wb").write(tflite_quant_model)
&lt;/denchmark-code&gt;

Tensorflow version used was 1.15.0. If you use TF2.0 and use from_keras_model instead, it will give the same error.
		</comment>
		<comment id='7' author='bhavitvyamalik' date='2019-11-07T04:15:06Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33607&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33607&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>