<bug id='32127' author='51616' open_date='2019-08-31T05:46:32Z' closed_time='2020-01-11T23:51:45Z'>
	<summary>Tensorflow 2.0 : Combining model.add_loss and keras losses function in training doesn't work</summary>
	<description>
I read about TensorFlow 2.0 &lt;denchmark-link:https://www.tensorflow.org/beta/guide/keras/custom_layers_and_models&gt;tutorial&lt;/denchmark-link&gt;
 in VAE section. I follow the tutorial but the model doesn't work as expected despite running the notebook directly from given &lt;denchmark-link:https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/r2/guide/keras/custom_layers_and_models.ipynb&gt;Google Colab&lt;/denchmark-link&gt;
. The result actually is the same as in the tutorial (i.e. loss value is very similar) but if you look at the output you'll see that the model can't reconstruct the input at all (i.e. output the same image for all inputs). This seems to be a mistake from the tutorial itself when combining  and .
&lt;denchmark-link:https://camo.githubusercontent.com/feb09ba7e621b56be7cd23e0907be1a41e0d30c4b4cab53588a3448ad0c48115/68747470733a2f2f696d6775722e636f6d2f337254414f725a2e706e67&gt;&lt;/denchmark-link&gt;

I changed MSE loss to BinaryCrossentropy but the result is still the same.
Later I tried compute the BinaryCrossentropy loss explicitly in my forward pass then use model.add_loss() in addition with the KL-divergence loss
&lt;denchmark-link:https://camo.githubusercontent.com/0130180c64cacf3da4b109966e298a31480bfa0cd3d9bf686158718aed4d6dda/68747470733a2f2f696d6775722e636f6d2f56564f524f43492e706e67&gt;&lt;/denchmark-link&gt;

This way the model can actually learn the data and the output seems good enough.
So I have a question about model.add_loss() and losses as a function that takes (y_true, y_pred) (i.e. keras.losses). The updated code works only if it can calculate losses in forward pass (e.g. kl-divergence or reconstruction loss), how can I combine model.add_loss() and keras.losses correctly in the case where the model need ground truth of the output (e.g. denoise VAE).
	</description>
	<comments>
		<comment id='1' author='51616' date='2019-09-05T19:45:32Z'>
		Thank you for the issue &lt;denchmark-link:https://github.com/51616&gt;@51616&lt;/denchmark-link&gt;
. I tried the colab and got different results for MSE and BCE losses in custom training loop:
MSE loss:
&lt;denchmark-code&gt;Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11493376/11490434 [==============================] - 0s 0us/step
Start of epoch 0
step 0: mean loss = tf.Tensor(0.35074496, shape=(), dtype=float32)
step 100: mean loss = tf.Tensor(0.12501891, shape=(), dtype=float32)
step 200: mean loss = tf.Tensor(0.09896767, shape=(), dtype=float32)
step 300: mean loss = tf.Tensor(0.088992655, shape=(), dtype=float32)
step 400: mean loss = tf.Tensor(0.084115215, shape=(), dtype=float32)
step 500: mean loss = tf.Tensor(0.08080357, shape=(), dtype=float32)
step 600: mean loss = tf.Tensor(0.078655444, shape=(), dtype=float32)
step 700: mean loss = tf.Tensor(0.07708102, shape=(), dtype=float32)
step 800: mean loss = tf.Tensor(0.07592539, shape=(), dtype=float32)
step 900: mean loss = tf.Tensor(0.074898444, shape=(), dtype=float32)
Start of epoch 1
step 0: mean loss = tf.Tensor(0.07461691, shape=(), dtype=float32)
step 100: mean loss = tf.Tensor(0.073966034, shape=(), dtype=float32)
step 200: mean loss = tf.Tensor(0.073460035, shape=(), dtype=float32)
step 300: mean loss = tf.Tensor(0.07299255, shape=(), dtype=float32)
step 400: mean loss = tf.Tensor(0.07266195, shape=(), dtype=float32)
step 500: mean loss = tf.Tensor(0.07227474, shape=(), dtype=float32)
step 600: mean loss = tf.Tensor(0.07198363, shape=(), dtype=float32)
step 700: mean loss = tf.Tensor(0.071675226, shape=(), dtype=float32)
step 800: mean loss = tf.Tensor(0.07145154, shape=(), dtype=float32)
step 900: mean loss = tf.Tensor(0.07118706, shape=(), dtype=float32)
Start of epoch 2
step 0: mean loss = tf.Tensor(0.07111354, shape=(), dtype=float32)
step 100: mean loss = tf.Tensor(0.07093662, shape=(), dtype=float32)
step 200: mean loss = tf.Tensor(0.07080722, shape=(), dtype=float32)
step 300: mean loss = tf.Tensor(0.07065797, shape=(), dtype=float32)
step 400: mean loss = tf.Tensor(0.07056295, shape=(), dtype=float32)
step 500: mean loss = tf.Tensor(0.07040719, shape=(), dtype=float32)
step 600: mean loss = tf.Tensor(0.07030004, shape=(), dtype=float32)
step 700: mean loss = tf.Tensor(0.07016859, shape=(), dtype=float32)
step 800: mean loss = tf.Tensor(0.07007885, shape=(), dtype=float32)
step 900: mean loss = tf.Tensor(0.06994936, shape=(), dtype=float32)
&lt;/denchmark-code&gt;

BCE loss:
&lt;denchmark-code&gt;Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11493376/11490434 [==============================] - 0s 0us/step
Start of epoch 0
WARNING:tensorflow:From /tensorflow-2.0.0-rc0/python3.6/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
step 0: mean loss = tf.Tensor(0.7803986, shape=(), dtype=float32)
step 100: mean loss = tf.Tensor(0.4406416, shape=(), dtype=float32)
step 200: mean loss = tf.Tensor(0.3644295, shape=(), dtype=float32)
step 300: mean loss = tf.Tensor(0.33377928, shape=(), dtype=float32)
step 400: mean loss = tf.Tensor(0.31776747, shape=(), dtype=float32)
step 500: mean loss = tf.Tensor(0.30728954, shape=(), dtype=float32)
step 600: mean loss = tf.Tensor(0.30034995, shape=(), dtype=float32)
step 700: mean loss = tf.Tensor(0.2952541, shape=(), dtype=float32)
step 800: mean loss = tf.Tensor(0.29153773, shape=(), dtype=float32)
step 900: mean loss = tf.Tensor(0.28829667, shape=(), dtype=float32)
Start of epoch 1
step 0: mean loss = tf.Tensor(0.28734177, shape=(), dtype=float32)
step 100: mean loss = tf.Tensor(0.28517666, shape=(), dtype=float32)
step 200: mean loss = tf.Tensor(0.28343785, shape=(), dtype=float32)
step 300: mean loss = tf.Tensor(0.2818749, shape=(), dtype=float32)
step 400: mean loss = tf.Tensor(0.28066754, shape=(), dtype=float32)
step 500: mean loss = tf.Tensor(0.27942434, shape=(), dtype=float32)
step 600: mean loss = tf.Tensor(0.27840894, shape=(), dtype=float32)
step 700: mean loss = tf.Tensor(0.2775054, shape=(), dtype=float32)
step 800: mean loss = tf.Tensor(0.27676252, shape=(), dtype=float32)
step 900: mean loss = tf.Tensor(0.27591947, shape=(), dtype=float32)
Start of epoch 2
step 0: mean loss = tf.Tensor(0.27567974, shape=(), dtype=float32)
step 100: mean loss = tf.Tensor(0.27511278, shape=(), dtype=float32)
step 200: mean loss = tf.Tensor(0.2745993, shape=(), dtype=float32)
step 300: mean loss = tf.Tensor(0.27408925, shape=(), dtype=float32)
step 400: mean loss = tf.Tensor(0.2737239, shape=(), dtype=float32)
step 500: mean loss = tf.Tensor(0.2732339, shape=(), dtype=float32)
step 600: mean loss = tf.Tensor(0.2728653, shape=(), dtype=float32)
step 700: mean loss = tf.Tensor(0.27246934, shape=(), dtype=float32)
step 800: mean loss = tf.Tensor(0.27214858, shape=(), dtype=float32)
step 900: mean loss = tf.Tensor(0.27174425, shape=(), dtype=float32)
&lt;/denchmark-code&gt;

Are you observing something else?
		</comment>
		<comment id='2' author='51616' date='2020-01-11T23:51:45Z'>
		Closing due to lack of activity, please re-open if you see this issue again.
		</comment>
		<comment id='3' author='51616' date='2020-01-11T23:51:47Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32127&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32127&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='51616' date='2020-05-16T02:34:44Z'>
		Dear pavithrasv,
Yes, the loss changes, but still the model does not learn anything. The KL-Divergence should give much higher errors than those presented.
Best Regards,
Ant√≥nio
		</comment>
		<comment id='5' author='51616' date='2020-06-07T19:22:51Z'>
		Is this possibly related to : &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/32058&gt;#32058&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>