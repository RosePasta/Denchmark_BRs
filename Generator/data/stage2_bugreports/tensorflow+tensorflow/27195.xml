<bug id='27195' author='meyerjo' open_date='2019-03-27T10:23:01Z' closed_time='2019-09-08T12:23:05Z'>
	<summary>[TF2.0alpha] Eagerness/Distribute Strategies performance issues</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04/18.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.0.0-dev20190327
Python version: 3.6
CUDA/cuDNN version: 10.0
GPU model and memory: TITAN X 12 GB

Describe the current behavior
I have some code, where to the best of my knowledge not everything can be wrapped with the tf.function. Thus, in the function calling strategy.experimental_run I removed the @tf.function wrapper. However, this tremendously decreases the overall performance and falls way behind the 'normal' eager execution.
For the toy example in this issue, these are the performance differences:
&lt;denchmark-code&gt;Computing for 1000-steps, no strategy: Time: 7.228003740310669 sec
Computing for 1000-steps, strategy with tf-function: Time: 2.6209354400634766
Computing for 1000-steps, strategy with no tf-function: Time: 63.60745906829834
&lt;/denchmark-code&gt;

Describe the expected behavior
I would expect that without tf.function at least the usual performance of eagerness is achieved.
Code to reproduce the issue
import time

import tensorflow as tf


class DebugModel(tf.keras.Model):
  def __init__(self):
    super(DebugModel, self).__init__()
    self.dense = tf.keras.layers.Dense(20, activation='relu', input_shape=(5,))

  def __call__(self, input, *args, **kwargs):
    x = self.dense(input)
    return x


def train_step(input):
  with tf.GradientTape() as tape:
    out = my_model(input)
    my_loss = -tf.reduce_mean(out)
  gradients = tape.gradient(my_loss, my_model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, my_model.trainable_variables))
  return my_loss


def distributed_train():
  return strategy.experimental_run(train_step, train_iterator)

@tf.function
def distributed_train_tf_function():
  return strategy.experimental_run(train_step, train_iterator)


if __name__ == '__main__':
  dataset = tf.data.Dataset.from_tensor_slices(
    (tf.random.uniform([3000, 16, 5])))
  N_steps = 1000

  # Plain computation
  my_model = DebugModel()
  optimizer = tf.keras.optimizers.Adam(1e-2)
  _s = time.time()
  i = 0
  for data in dataset:
    if i &gt;= N_steps:
      break
    train_step(data)
    i += 1
  print('Computing for {}-steps, no strategy: Time: {} sec'.format(
      N_steps, time.time() - _s))


  # Mirrored strategy with @tf.function
  dataset = tf.data.Dataset.from_tensor_slices(
    (tf.random.uniform([3000, 16, 5])))
  strategy = tf.distribute.MirroredStrategy()

  with strategy.scope():
    train_iterator = strategy.make_dataset_iterator(dataset)
    train_iterator.initialize()
    optimizer = tf.keras.optimizers.Adam(1e-2)
    my_model = DebugModel()

    _s = time.time()
    for s in range(N_steps):
      distributed_train_tf_function()
    print('Computing for {}-steps, strategy with tf-function: Time: {}'.format(
      N_steps, time.time() - _s))


  # Mirrored strategy without @tf.function decorator
  dataset = tf.data.Dataset.from_tensor_slices(
    (tf.random.uniform([3000, 16, 5])))
  strategy = tf.distribute.MirroredStrategy()
  with strategy.scope():
    train_iterator = strategy.make_dataset_iterator(dataset)
    train_iterator.initialize()
    optimizer = tf.keras.optimizers.Adam(1e-2)
    my_model = DebugModel()

    _s = time.time()
    for s in range(N_steps):
      distributed_train()
    print('Computing for {}-steps, strategy with no tf-function: Time: {}'.format(
      N_steps,
      time.time() - _s))
	</description>
	<comments>
		<comment id='1' author='meyerjo' date='2019-04-01T08:47:51Z'>
		&lt;denchmark-link:https://github.com/goldiegadde&gt;@goldiegadde&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/dynamicwebpaige&gt;@dynamicwebpaige&lt;/denchmark-link&gt;
 Would be nice to see some progress on this issue
		</comment>
		<comment id='2' author='meyerjo' date='2019-04-01T23:52:47Z'>
		Assigning to &lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 for triage.
I might be wrong, but at a glance it might be a caching issue. You might want to wrap train_step into a tf.function.
		</comment>
		<comment id='3' author='meyerjo' date='2019-04-08T05:09:22Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 can you please guide me, I want to work on this issue.
		</comment>
		<comment id='4' author='meyerjo' date='2019-04-08T16:06:38Z'>
		&lt;denchmark-link:https://github.com/shashvatshahi1998&gt;@shashvatshahi1998&lt;/denchmark-link&gt;
 the first step is to try to reproduce the issue, and then try to follow &lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 's advice to wrap train_step in tf.function and see whether the issue goes away.
If it doesn't, running a profiler like python's cprofiler on the code to find out what is happening and triaging that to a separate issue is the next step.
		</comment>
		<comment id='5' author='meyerjo' date='2019-04-08T16:18:18Z'>
		Wrapping train_step in tf.function does not work. It raises the following error: RuntimeError: `merge_call` called while defining a new graph. [...].
		</comment>
		<comment id='6' author='meyerjo' date='2019-04-08T16:24:07Z'>
		&lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
 is this a known issue internally?
		</comment>
		<comment id='7' author='meyerjo' date='2019-04-08T18:57:44Z'>
		There are couple of things mentioned here, let me clarify:


Performance without tf.function when using distribution strategy is terrible - this is known issue and we do not recommend doing this right now, and it is not something we are prioritizing yet either. The recommended approach is to wrap experimental_run inside a tf.function. Fixing this requires a significant amount of work inside MirroredStrategy.


Wrapping just the train_step into a tf.function: The error seen there is also known, and I am working on fixing that. However, that will not address the performance issue from #1. For anyone who can wrap their train_step in a tf.function, they should be able to wrap strategy.experimental_run inside the tf.function as well - AND get the performance benefit. So there is not a great reason yet to wrap just the train_step into a tf.function but not strategy.run. So while I am working on fixing that issue, it will still not help with performance.


Hope this helps. Please let me know if there are more questions.
		</comment>
		<comment id='8' author='meyerjo' date='2019-04-08T20:03:31Z'>
		&lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
 Regarding 1.), this is rather confusing to me. My main problem causing this issue was my dependance on eager execution during the forward/backward pass.
However, wrapping my experimental_run with tf.function straps me from the eager execution. The new TF policy should not be eager-execution is the default except you want to use multiple GPUs for training, should it?
		</comment>
		<comment id='9' author='meyerjo' date='2019-04-08T20:14:24Z'>
		&lt;denchmark-link:https://github.com/meyerjo&gt;@meyerjo&lt;/denchmark-link&gt;
 Ideally eager execution is available and performant for all use cases on day 1, but we have limited resources and have to prioritize what gets done first and then what gets fast first. I agree with &lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
's priorities outlined above as being the best to move the existing ecosystem forward, and I'm sure that once performance is good for the tf.function use cases we'll start to improve it for pure-eager multi-gpu.
		</comment>
		<comment id='10' author='meyerjo' date='2019-09-08T12:23:06Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27195&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27195&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='meyerjo' date='2019-09-08T12:23:12Z'>
		We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!
		</comment>
	</comments>
</bug>