<bug id='43789' author='kalosisz' open_date='2020-10-05T13:27:16Z' closed_time='2020-10-25T13:02:14Z'>
	<summary>Google Cloud Storage cannot be used in tf.keras.callbacks.experimental.BackupAndRestore as a back-up dir</summary>
	<description>
System information

Run in official container: tensorflow/tensorflow:2.3.1
Host OS:  Debian GNU/Linux 10 (Cloud shell)

Describe the current behavior
On successful training, the checkpoints produced by tf.keras.callbacks.experimental.BackupAndRestore should be removed. However, if one provided a Cloud Storage location, the cleanup fails. This is not the case for specifying a local backup location.
Describe the expected behavior
tf.keras.callbacks.experimental.BackupAndRestore should be able to remove the checkpoint upon successful training.

&lt;denchmark-link:https://colab.research.google.com/drive/1XocuDL8I8tivEa-DCgH6iWBcpV5WiJRJ?usp=sharing&gt;Colab Notebook&lt;/denchmark-link&gt;


Other info / logs
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/usr/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/trainig/trainer/task.py", line 69, in &lt;module&gt;
    main(args)
  File "/trainig/trainer/task.py", line 41, in main
    callbacks=callbacks
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py", line 1145, in fit
    callbacks.on_train_end(logs=training_logs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py", line 514, in on_train_end
    callback.on_train_end(logs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py", line 1547, in on_train_end
    self._training_state.delete_backup()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/distribute/worker_training_state.py", line 124, in delete_backup
    file_io.delete_recursively(pathname)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py", line 586, in delete_recursively
    delete_recursively_v2(dirname)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py", line 599, in delete_recursively_v2
    _pywrap_file_io.DeleteRecursively(compat.as_bytes(path))
tensorflow.python.framework.errors_impl.NotFoundError: gs://[bucket]/training/backup/ckpt-2.data-00000-of-00001 doesn't exist or not a directory.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='kalosisz' date='2020-10-05T13:43:56Z'>
		I have also looked in the codebase and believe the culprit is that even though  is used, it is used for all the files found the the looked up directories, &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/ac4e209f8cd33aab482d5414b38043e03fc72ef5/tensorflow/python/keras/distribute/worker_training_state.py#L115-L120&gt;code snippet&lt;/denchmark-link&gt;
. , however, can remove the content recursively. This recursive call is not compatible with Cloud Storage when calling on files and not directories.
The preceding checkpoints are deleted on epoch ends and that is implemented using file_io.delete_file. This works well with Cloud Storage.
A possible solution is to not to call file_io.delete_recursively_v2 on looked up files, but on the directories itself. For instance,
file_io.delete_recursively_v2(self.write_checkpoint_manager._prefix)
file_io.delete_recursively_v2(
  os.path.join(self.write_checkpoint_manager.directory, 'checkpoint'))
		</comment>
		<comment id='2' author='kalosisz' date='2020-10-25T13:02:14Z'>
		&lt;denchmark-link:https://github.com/kalosisz&gt;@kalosisz&lt;/denchmark-link&gt;
 Closing the issue since the necessary changes are implemented.Please feel free to re-open the issue if you have any concern.Thanks!
		</comment>
		<comment id='3' author='kalosisz' date='2020-10-25T13:02:16Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43789&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43789&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='kalosisz' date='2020-10-25T13:44:48Z'>
		&lt;denchmark-link:https://github.com/saikumarchalla&gt;@saikumarchalla&lt;/denchmark-link&gt;
 Thank you for the fix!
		</comment>
	</comments>
</bug>