<bug id='39344' author='shahaneshantanu' open_date='2020-05-09T13:02:45Z' closed_time='2020-05-14T18:40:31Z'>
	<summary>Gradient of outputs with respect to inputs inside a custom loss function</summary>
	<description>
I want to write a custom loss function for a Multilayer Perceptron network in Keras. The loss has two components: first is the regular 'mse' and the second is element wise gradients of output with respect to input features. Let x be the input with 2 features (size: number of samples X 2) and y the output with single output (size: number of samples X 1). I am denoting derivative of each output sample with first feature of each sample as $\frac{dy[:]}{dx[:,0]}$
Similarly, I want to compute the following expression inside the loss function:
$$r[:] = y[:] \frac{dy[:]}{dx[:,0]} - x[:,1] \frac{d^2y[:]}{dx[:,0]^2}$$
and take the mean square of the r vector. The total loss being the sum of the regular 'mse' and mean square of r vector.
This is a minimal, reproducible example of the code I tried:
&lt;denchmark-code&gt;def custom_loss_envelop(model_inputs, model_outputs):
    def custom_loss(y_true,y_pred):
        mse_loss = keras.losses.mean_squared_error(y_true, y_pred)
        print()
        print(model_inputs); print()
        print(model_outputs); print()
        dy_dx = keras.backend.gradients(model_outputs, tf.gather(model_inputs, [0], axis=1))
        print(dy_dx); print()
        d2y_dx2 = keras.backend.gradients(dy_dx, tf.gather(model_inputs, [0], axis=1))
        print(d2y_dx2); print()

        r = tf.multiply(model_outputs, tf.gather(dy_dx, [0], axis=1)) - tf.multiply(tf.gather(model_inputs, [1], axis=1), tf.gather(d2y_dx2, [0], axis=1)) # y*dy_dx[0] - x[1]*d2y_dx[0]2

        r = keras.backend.mean(keras.backend.square(r))
        loss = mse_loss + r
        return loss
    return custom_loss

nx=100;
inputs_train=np.random.uniform(0,1,(nx,2)); outputs_train=np.random.uniform(0,1,(nx,1))
inputs_val=np.random.uniform(0,1,(int(nx/2),2)); outputs_val=np.random.uniform(0,1,(int(nx/2),1))
n_hidden_units=50; l2_reg_lambda=0; learning_rate=0.001; dropout_factor=0.0; epochs=3

model = keras.Sequential();
model.add(keras.layers.Dense(n_hidden_units, activation='relu', input_shape=(inputs_train.shape[1],), kernel_regularizer=keras.regularizers.l2(l2_reg_lambda))); #first hidden layer
model.add(keras.layers.Dropout(dropout_factor)); model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(n_hidden_units, activation='relu', kernel_regularizer = keras.regularizers.l2(l2_reg_lambda)));
model.add(keras.layers.Dropout(dropout_factor)); model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(n_hidden_units, activation='relu', kernel_regularizer = keras.regularizers.l2(l2_reg_lambda)));
model.add(keras.layers.Dropout(dropout_factor)); model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(outputs_train.shape[1], activation='linear'));
optimizer1 = keras.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)

model.compile(loss=custom_loss_envelop(model.inputs, model.outputs), optimizer=optimizer1, metrics=['mse'])

model.fit(inputs_train, outputs_train, batch_size=100, epochs=epochs, shuffle=True, validation_data=(inputs_val,outputs_val), verbose=1)
&lt;/denchmark-code&gt;

Here, I have generated training and validation samples randomly. I am getting the tensor shapes as follows: model_inputs: [&lt;tf.Tensor 'dense_input:0' shape=(None, 2) dtype=float32&gt;], model_outputs: [&lt;tf.Tensor 'dense_3/Identity:0' shape=(None, 1) dtype=float32&gt;] and dy_dx: [None]. The first 2 are as expected but the derivative should also be of shape (None, 1) which it is not. Hence, I get AttributeError: 'NoneType' object has no attribute 'op' error in the line d2y_dx2 = keras.backend.gradients(dy_dx, tf.gather(model_inputs, [0], axis=1))
Any help is appreciated either to fix this issue or with alternate solution.
	</description>
	<comments>
		<comment id='1' author='shahaneshantanu' date='2020-05-11T09:22:51Z'>
		I have tried in colab with TF 2.2-rc4 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/02487344c5e408d7f44a841096dd3f2c/untitled874.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='2' author='shahaneshantanu' date='2020-05-14T18:40:31Z'>
		Thanks &lt;denchmark-link:https://github.com/shahaneshantanu&gt;@shahaneshantanu&lt;/denchmark-link&gt;
 . There is likely an issue in the custom code above. You can review the &lt;denchmark-link:https://keras.io/guides/customizing_what_happens_in_fit/&gt;guide&lt;/denchmark-link&gt;
 on overriding train_step for some help, and also reach out on Stack Overflow, where there is a larger community to help you debug. If there is a concrete bug that you find, please file a new issue describing it.
		</comment>
		<comment id='3' author='shahaneshantanu' date='2020-05-14T18:40:33Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39344&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39344&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='shahaneshantanu' date='2020-11-25T08:21:54Z'>
		&lt;denchmark-link:https://github.com/shahaneshantanu&gt;@shahaneshantanu&lt;/denchmark-link&gt;
  - have you solved this problem?
		</comment>
	</comments>
</bug>