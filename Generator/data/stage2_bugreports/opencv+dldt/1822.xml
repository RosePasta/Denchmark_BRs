<bug id='1822' author='leoll2' open_date='2020-08-17T17:59:30Z' closed_time='2020-11-09T19:36:02Z'>
	<summary>Huge loss of precision on VPU with HW optimizations enabled</summary>
	<description>
&lt;denchmark-h:h5&gt;System information (version)&lt;/denchmark-h&gt;


OpenVINO=&gt; 2020.4
Operating System / Platform =&gt; Ubuntu 18.04
Compiler =&gt; g++
Problem classification =&gt; VPU - Wrong output

&lt;denchmark-h:h5&gt;Detailed description&lt;/denchmark-h&gt;

Myriad (NCS2) suffers from heavy precision loss when VPU_HW_STAGES_OPTIMIZATION is enabled.
Let's consider this extremely simple network, made of a single convolution block:
&lt;denchmark-link:https://camo.githubusercontent.com/38521df090856d3cc33967f1f519b1950fbfe0e9307e30bcf05c68db0aa5043b/68747470733a2f2f692e696d6775722e636f6d2f43466c627165792e706e67&gt;&lt;/denchmark-link&gt;

After running inference (with hw optimizations enabled) on the same image on CPU and MYRIAD respectively, I printed (part of) the output:
&lt;denchmark-link:https://camo.githubusercontent.com/92143cf3aac9ccb7552c285e54023a48eac60625ba4999dbcbcf2fc6d073f1f6/68747470733a2f2f692e696d6775722e636f6d2f4f6b63774d65552e706e67&gt;&lt;/denchmark-link&gt;

This CPU/VPU output comparison shows that some values are already inaccurate to the second decimal place, after just one layer. One could deem this acceptable, however any realistic network is composed of many more layers, and of course the problem gets worse as the inaccuracies stack as we go deeper in the model.
For instance, this is what the output deviation looks like after 10 conv blocks like that above:
&lt;denchmark-link:https://camo.githubusercontent.com/f27de2f634e9aa23994b89879318d51a450ef97103ae30772590843de7ce5850/68747470733a2f2f692e696d6775722e636f6d2f474561797150782e706e67&gt;&lt;/denchmark-link&gt;

Some numbers are now quite different (e.g. 0.21 vs 0.31, -0.01 vs -0.08).
At the end of my network, about 50 blocks later, the situation becomes embarassing, and the Myriad output is unusable in practice:
&lt;denchmark-link:https://camo.githubusercontent.com/84878500108b8a3951eb779d36e6c6b93d34e16ee4deac61fcbf0396c3c64500/68747470733a2f2f696d6775722e636f6d2f45326f436c66552e706e67&gt;&lt;/denchmark-link&gt;

I know many people recommend to disable VPU_HW_STAGES_OPTIMIZATION, but this also causes a significant performance drop in terms of latency and throughput. Honestly, I don't see why the effect of any properly designed optimization would affect the network accuracy so negatively.
Is the VPU accuracy supposed to be so low?
I am curious to know more details about what hardware optimizations get activated by that flag, and whether there is any way to turn them on/off individually.
Thanks in advance.
&lt;denchmark-h:h5&gt;Steps to reproduce&lt;/denchmark-h&gt;

I uploaded &lt;denchmark-link:https://easyupload.io/d5lvz7&gt;here&lt;/denchmark-link&gt;
 the chunks of my model that I have used for inference, plus the sample image. Then you can easily print any output value you prefer, the loss of precision is everywhere.
&lt;denchmark-h:h5&gt;Issue submission checklist&lt;/denchmark-h&gt;


 I report the issue, it's not a question


 I checked the problem with documentation, FAQ, open issues, Stack Overflow, etc and have not found solution


 There is reproducer code and related data files: images, videos, models, etc.



	</description>
	<comments>
		<comment id='1' author='leoll2' date='2020-08-18T06:31:36Z'>
		Hi!
May I ask to try an experiment with [0..255] input range for network? With and without VPU_HW_STAGES_OPTIMIZATION .
		</comment>
		<comment id='2' author='leoll2' date='2020-08-18T12:08:07Z'>
		&lt;denchmark-link:https://github.com/dkurt&gt;@dkurt&lt;/denchmark-link&gt;

The relative error looks smaller if I provide an input in the [0..255] range. Is it all about the working point?
By the way, it's curious how the magnitude of VPU values seems to be always lower than CPU ones by 2-10%, never greater: maybe the VPU introduces a bias when rounding internally?



[0..1] no optimization
[0..1] with optimization












[0..255] no optimization
[0..255] with optimization









		</comment>
		<comment id='3' author='leoll2' date='2020-08-18T12:20:23Z'>
		Actually, it's interesting to see the difference between [0..1] inputs with and w/o HW optimization and [0..255] input but with convolutions weights divided by 255. So first convolution layer will perform this normalization. Can you try please?
		</comment>
		<comment id='4' author='leoll2' date='2020-08-18T15:53:33Z'>
		This is the output right after the first conv. Left is standard (input in range [0..1]), right is rescaled (input in [0..255], weights of first conv layer divided by 255). I display one more decimal digit to show better the differences.
Optimizations are enabled in both experiments.



[0..1] standard
[0..255] weights rescaled









As you can see, rescaling does not look better.
As for the final network output (after many layers), the values look very different between 'standard' and 'rescaled', even on CPU, so I don't even plot them.
		</comment>
		<comment id='5' author='leoll2' date='2020-09-01T21:16:37Z'>
		Hi &lt;denchmark-link:https://github.com/dkurt&gt;@dkurt&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/jgespino&gt;@jgespino&lt;/denchmark-link&gt;
 !
Any update on this?
		</comment>
		<comment id='6' author='leoll2' date='2020-10-13T23:29:49Z'>
		&lt;denchmark-link:https://github.com/jgespino&gt;@jgespino&lt;/denchmark-link&gt;
 any updates on this?
		</comment>
		<comment id='7' author='leoll2' date='2020-10-14T13:17:31Z'>
		Hi &lt;denchmark-link:https://github.com/mhkabir&gt;@mhkabir&lt;/denchmark-link&gt;

Please share the model and image once more, I am unable to access the easyupload link in the original post.
Regards,
Jesus
		</comment>
		<comment id='8' author='leoll2' date='2020-10-14T20:13:01Z'>
		Hi &lt;denchmark-link:https://github.com/jgespino&gt;@jgespino&lt;/denchmark-link&gt;
,
here is are the models and image:
&lt;denchmark-link:https://drive.google.com/file/d/1Qv9YFjRTlnOFHyoARf3pismmhaSGgeYa/view?usp=sharing&gt;https://drive.google.com/file/d/1Qv9YFjRTlnOFHyoARf3pismmhaSGgeYa/view?usp=sharing&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='leoll2' date='2020-10-15T16:56:53Z'>
		&lt;denchmark-link:https://github.com/leoll2&gt;@leoll2&lt;/denchmark-link&gt;
 Could you also share this model in ONNX format and the model optimizer command used? Feel free to send it privately via email if needed.
		</comment>
		<comment id='10' author='leoll2' date='2020-11-09T18:43:43Z'>
		Hi &lt;denchmark-link:https://github.com/leoll2&gt;@leoll2&lt;/denchmark-link&gt;

Please let me know if we can proceed to close this issue. As I mentioned via email, using --scale 255 improved the results of Myriad when comparing to CPU.
Regards,
Jesus
		</comment>
		<comment id='11' author='leoll2' date='2020-11-09T18:52:41Z'>
		Thanks for the support.
As a last note, I'd like to know the reason behind the issue, i.e. why --scaled works better. Is it a hardware bug, or supposed to behave like this by design? In any case, it may be worth mentioning it somewhere in the documentation, just in case other users have the same problem.
Feel free to close this issue.
		</comment>
		<comment id='12' author='leoll2' date='2020-11-09T19:35:52Z'>
		Hi &lt;denchmark-link:https://github.com/leoll2&gt;@leoll2&lt;/denchmark-link&gt;

Thank you for confirming, we will need to update our documentation to reflect this. The issue happens because there is a half precision overflow on MatMul layer for the MYRIAD plugin. Adding --scale will have the data values and weights analyzed by the MYRIAD plugin correctly and there will be no overflow.
Regards,
Jesus
		</comment>
	</comments>
</bug>