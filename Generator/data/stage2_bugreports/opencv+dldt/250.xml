<bug id='250' author='laggui' open_date='2019-09-16T17:38:54Z' closed_time='2020-08-12T22:06:02Z'>
	<summary>[VPU] Unsupported precision I32</summary>
	<description>
Hi all,
I successfully converted my SSDLite models (without any post-processing) from ONNX to OpenVINO IR to use with the Inference Engine. I have one FP32 model that I managed to benchmark on my Intel CPU, and another model in half precision (FP16) to evaluate the performance on the NCS2.
The model optimizer successfully generates the IR model for my FP16 model, but when I try to infer an input image I get the following error:
terminate called after throwing an instance of 'InferenceEngine::details::InferenceEngineException'
  what():  [VPU] Unsupported precision I32for data 7648_const
Aborted (core dumped)
This constant is associated to a reshape operation on the outputs, and I'm not sure why the Model Optimizer successfully generated the IR for the specified data type (FP16) without converting that integer constant to a supported type for the VPU.
I haven't been able to find much on this, any help is appreciated!
Thanks :)
	</description>
	<comments>
		<comment id='1' author='laggui' date='2019-09-16T17:43:50Z'>
		Dear &lt;denchmark-link:https://github.com/laggui&gt;@laggui&lt;/denchmark-link&gt;

Please attach your  IR and your inference script as a *.zip so that we can reproduce it. Also are your SSDLite models public or did you custom build them ? Having access to the SSDLite original models as well as the mo command you used to generate IR would also be helpful.
Thanks,
Shubha
		</comment>
		<comment id='2' author='laggui' date='2019-09-16T18:30:05Z'>
		&lt;denchmark-link:https://github.com/shubha-ramani&gt;@shubha-ramani&lt;/denchmark-link&gt;
 the models are based on our PyTorch implementation, but the models I am trying to benchmark with have dummy weights so I don't mind releasing the ONNX models exported from PyTorch. You can download the necessary files &lt;denchmark-link:https://drive.google.com/file/d/1Dzx3nb4T8P88R8dCKi8haGFzz0aZkItf/view?usp=sharing&gt;here&lt;/denchmark-link&gt;
.
Model Optimizer command
$ python mo.py -m SSDLite_fp16.onnx -n ssdlite_fp16 --output_dir ssdlite_fp16 -i [1,3,300,300] --data_type FP16
Inference script reference
I included the C++ project file as well as the CMake file to build the example. From within the bin/intel64/Release dir, run the following command to execute the sample on your NCS2:
$ ./openvino_inference_example &lt;ssdlite_fp16.xml_path&gt; &lt;input_image&gt; MYRIAD -pc
If you want to test the FP32 model on your CPU, you can use the following command:
$ ./openvino_inference_example &lt;ssdlite.xml_path&gt; &lt;input_image&gt; CPU -pc
Let me know if you need anything else!
		</comment>
		<comment id='3' author='laggui' date='2019-09-17T17:14:45Z'>
		Dear &lt;denchmark-link:https://github.com/laggui&gt;@laggui&lt;/denchmark-link&gt;
 ,
I downloaded your tar.gz. And thanks for the additional information above. I will definitely debug this and take a look !
Shubha
		</comment>
		<comment id='4' author='laggui' date='2019-09-18T20:56:03Z'>
		Dearest &lt;denchmark-link:https://github.com/laggui&gt;@laggui&lt;/denchmark-link&gt;

Thanks for all your debugging materials ! You'll be happy to know that this is surely a bug and I have filed it on your behalf. I can't thank you enough for your cooperation and patience. I will let you know once we have a root cause and fix.
Thanks again !
Shubha
		</comment>
		<comment id='5' author='laggui' date='2019-09-18T21:19:05Z'>
		&lt;denchmark-link:https://github.com/shubha-ramani&gt;@shubha-ramani&lt;/denchmark-link&gt;
 you're welcome :)
Thanks for the help, I'll be following this closely.
		</comment>
		<comment id='6' author='laggui' date='2020-08-12T22:06:02Z'>
		&lt;denchmark-link:https://github.com/laggui&gt;@laggui&lt;/denchmark-link&gt;
 this bug should have been already addressed, please checkout the latest version of OpenVINO available.
Closing this, feel free to reopen and ask additional questions related to this topic.
Ref. 22525
		</comment>
	</comments>
</bug>