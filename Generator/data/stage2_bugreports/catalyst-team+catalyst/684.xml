<bug id='684' author='CrafterKolyan' open_date='2020-03-03T09:35:54Z' closed_time='2020-03-21T15:38:10Z'>
	<summary>SupervisedWandbRunner logs wrong number of epochs to WandB</summary>
	<description>
Describe the bug
Catalyst 20.02.4
WandbRunner is logging wrong number of epochs to WandB
To Reproduce
Steps to reproduce the behavior:
&lt;denchmark-code&gt;from catalyst import dl
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

class Projector(nn.Module):
    def __init__(self, input_size):
        super().__init__()
        self.linear = nn.Linear(input_size, 1)
        
    def forward(self, X):
        return self.linear(X).squeeze(-1)

X = torch.rand(16, 10)
y = torch.rand(X.shape[0])
model = Projector(X.shape[1])
dataset = TensorDataset(X, y)
loader = DataLoader(dataset, batch_size=8)
runner = dl.SupervisedWandbRunner()

runner.train(
    model=model,
    loaders={
        "train": loader,
        "valid": loader
    },
    criterion=nn.MSELoss(),
    optimizer=optim.Adam(model.parameters()),
    logdir="log_xxx_000",
    monitoring_params={
        "project": "wandb_wrong_epochs"
    },
    num_epochs=10
)
&lt;/denchmark-code&gt;

Expected behavior
In WandB I see two plots with MSELoss with exactly 10 epochs
Actual behaviour
In WandB I see two plots with MSELoss with 20 epochs (but each has exactly 10 dots)

Look on number of steps. It has 20 steps. But should have 10.
&lt;denchmark-link:https://user-images.githubusercontent.com/9883873/75762200-564c6900-5d4b-11ea-87e3-20ef0e840725.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='CrafterKolyan' date='2020-05-02T14:50:27Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>