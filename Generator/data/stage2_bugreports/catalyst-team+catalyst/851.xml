<bug id='851' author='lokeshkvn' open_date='2020-06-22T17:46:12Z' closed_time='2020-07-06T04:09:19Z'>
	<summary>OneCycleLRWithWarmup does not starts with init_lr</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug Report&lt;/denchmark-h&gt;

OneCycleLRWithWarmup starts ahead of initial LR (does not start with init_lr)
If the number of steps is 6 and warmup_fraction = 0.5 and init_lr = 1e-3 and lr_range=(2e-3,1e-3),
The lr starts with 1.36-3 excutes two steps and starts cooling down from 2e-3
&lt;denchmark-h:h3&gt;How To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Go to https://colab.research.google.com/github/catalyst-team/catalyst/blob/master/examples/notebooks/segmentation-tutorial.ipynb#scrollTo=OG6_pgmkxk_b


&lt;denchmark-code&gt;batch_size = 8
loaders = get_loaders(
    images=ALL_IMAGES[:200],
    masks=ALL_MASKS[:200],
    random_state=SEED,
    train_transforms_fn=train_transforms,
    valid_transforms_fn=valid_transforms,
    batch_size=batch_size
) 
&lt;/denchmark-code&gt;


set epochs to 10
replace scheduler with

&lt;denchmark-code&gt;scheduler = OneCycleLRWithWarmup(optimizer, num_steps = 6,lr_range=(2e-3,1e-3), init_lr = 1e-3, warmup_fraction = 0.5)
&lt;/denchmark-code&gt;


Run

&lt;denchmark-h:h4&gt;Screenshots&lt;/denchmark-h&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/16884938/85318756-20e2ab80-b4f3-11ea-8589-40039d7a056f.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

OneCycleLRWithWarmup should start with lr = init_lr
(Pardon me if I am wrong, please advise me if this is the correct behavior of OneCycleLRWithWarmup as I am new to DL)
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

Google Colab
	</description>
	<comments>
		<comment id='1' author='lokeshkvn' date='2020-06-22T17:46:48Z'>
		Hi! Thank you for your contribution! Great first issue!
		</comment>
		<comment id='2' author='lokeshkvn' date='2020-06-22T18:26:11Z'>
		hi &lt;denchmark-link:https://github.com/lokeshkvn&gt;@lokeshkvn&lt;/denchmark-link&gt;
 , thanks for the issue!
Due to a bunch of current tasks (we are working on huge codestyle and docs update),
could you please help us and provide full-featured example for reproduce the issue?
here is the template:
import torch
from torch.utils.data import DataLoader, TensorDataset
from catalyst.dl import SupervisedRunner

# data
num_samples, num_features = int(1e4), int(1e1)
X, y = torch.rand(num_samples, num_features), torch.rand(num_samples)
dataset = TensorDataset(X, y)
loader = DataLoader(dataset, batch_size=32, num_workers=1)
loaders = {"train": loader, "valid": loader}

# model, criterion, optimizer, scheduler
model = torch.nn.Linear(num_features, 1)
criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters())
scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [3, 6])

# model training
runner = SupervisedRunner()
runner.train(
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    scheduler=scheduler,
    loaders=loaders,
    logdir="./logdir",
    num_epochs=8,
    verbose=True,
)
		</comment>
		<comment id='3' author='lokeshkvn' date='2020-06-22T18:42:32Z'>
		Sure &lt;denchmark-link:https://github.com/Scitator&gt;@Scitator&lt;/denchmark-link&gt;
 I will update you with the full-feature example.
		</comment>
		<comment id='4' author='lokeshkvn' date='2020-06-22T18:57:53Z'>
		Hi &lt;denchmark-link:https://github.com/Scitator&gt;@Scitator&lt;/denchmark-link&gt;

Here is the code example to reproduce:
import torch
from torch.utils.data import DataLoader, TensorDataset
from catalyst.dl import SupervisedRunner
from catalyst.contrib.nn.schedulers.onecycle import OneCycleLRWithWarmup

# data
num_samples, num_features = int(1e4), int(1e1)
X, y = torch.rand(num_samples, num_features), torch.rand(num_samples)
dataset = TensorDataset(X, y)
loader = DataLoader(dataset, batch_size=32, num_workers=1)
loaders = {"train": loader, "valid": loader}

# model, criterion, optimizer, scheduler
model = torch.nn.Linear(num_features, 1)
criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters())
scheduler = OneCycleLRWithWarmup(optimizer, num_steps = 6,lr_range=(2e-3,1e-3), init_lr = 1e-3, warmup_fraction = 0.5)

# model training
runner = SupervisedRunner()
runner.train(
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    scheduler=scheduler,
    loaders=loaders,
    logdir="./logdir",
    num_epochs=8,
    verbose=True,
)
Then run tensorboard
&lt;denchmark-code&gt;load_ext tensorboard 
tensorboard --logdir 'logdir/
&lt;/denchmark-code&gt;

Below is the lr plot I get using tensorboard:
&lt;denchmark-link:https://user-images.githubusercontent.com/16884938/85324867-34930f80-b4fd-11ea-94fb-da62c801df94.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='lokeshkvn' date='2020-06-23T20:34:13Z'>
		Dear &lt;denchmark-link:https://github.com/lokeshkvn&gt;@lokeshkvn&lt;/denchmark-link&gt;
 ,
looks like I found the solution,
You need to change this line
&lt;denchmark-link:https://github.com/catalyst-team/catalyst/blob/master/catalyst/core/callbacks/scheduler.py#L121&gt;https://github.com/catalyst-team/catalyst/blob/master/catalyst/core/callbacks/scheduler.py#L121&lt;/denchmark-link&gt;

to
self._scheduler.recalculate(
    loader_len=runner.loader_len, current_step=runner.epoch - 1
)
Would you like to make a PR with this fix?
		</comment>
		<comment id='6' author='lokeshkvn' date='2020-06-24T02:58:24Z'>
		Thanks I will make a PR soon with the above fix. üòÉ
		</comment>
	</comments>
</bug>