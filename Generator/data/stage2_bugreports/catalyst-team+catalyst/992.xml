<bug id='992' author='and-kul' open_date='2020-11-08T17:54:28Z' closed_time='2021-01-21T21:11:38Z'>
	<summary>AMPOptimizerCallback doesn't work with DataParallel</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug Report&lt;/denchmark-h&gt;

Actually, I'm not sure, whether it should be treated as a bug in Catalyst, but just to prevent Catalyst users from hours of complaints like:

"Why AMPOptimizerCallback is not working?!"

By default, if you have multiple GPUs on one machine, catalyst will automatically convert your model(s) to DataParallel. But, as was mentioned in &lt;denchmark-link:https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-multiple-gpus&gt;Automatic Mixed Precision docs&lt;/denchmark-link&gt;
  doesn't simply work with DataParallel. And you have to change your model's  method to fix this.
I don't see an easy way to fix this problem, but at least it would be helpful to warn a user about this "feature" (for example, mention this is the docstring of AMPOptimizerCallback)
&lt;denchmark-h:h3&gt;How To Reproduce&lt;/denchmark-h&gt;

Just core components:

Run script with multiple GPUs enabled (for example via os.environ["CUDA_VISIBLE_DEVICES"] = "0,1")
Add AMPOptimizerCallback to your runner (to activate native fp16 AMP)
Catalyst will transform your model to DataParallel
AMP will not work:

Outputs of your model will still be in float32
You won't see neither consumed GPU memory reduction nor performance speedup



&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;Catalyst version: 20.10.1
PyTorch version: 1.6.0
Python version: 3.8.3
CUDA runtime version: 10.2
cuDNN version: 7.6.5
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='and-kul' date='2021-01-07T20:57:37Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>