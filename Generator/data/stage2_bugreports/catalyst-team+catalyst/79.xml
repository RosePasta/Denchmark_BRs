<bug id='79' author='ivbelkin' open_date='2019-01-23T18:14:25Z' closed_time='2019-01-23T19:50:29Z'>
	<summary>[solved] ResnetEncoder 'frozen' argument doesn't work</summary>
	<description>
Passing frozen=True to catalyst.contrib.models.ResnetEncoder does not actually make encoder not trainable. Simple code to check that tensor value is changing during training:
import torch
from catalyst.contrib.models import ResnetEncoder

class Net(torch.nn.Module):
    
    def __init__(self):
        super().__init__()
        self.enc = ResnetEncoder(
            arch="resnet18", 
            pooling="GlobalAvgPool2d"
        )
        self.logits = torch.nn.Linear(
            self.enc.out_features, 1)
    
    def forward(self, x):
        return self.logits(self.enc(x))
    
    
model = Net()

old_value = model.enc.feature_net[0].weight[0,0,0,0].item()

inputs = torch.randn(8, 3, 224, 224)
targets = torch.ones(8, 1)
optimizer = torch.optim.Adam(model.parameters())
criterion = torch.nn.BCEWithLogitsLoss()

optimizer.zero_grad()
loss = criterion(model(inputs), targets)
loss.backward()
optimizer.step()

new_value = model.enc.feature_net[0].weight[0,0,0,0].item()

print(old_value, new_value)
It will print different values. It is so because nn.Module has no attribute 'requires_grad', only nn.Parameter has it. I fix it in my fork, but I don't see 'dev' branch to open PR.
	</description>
	<comments>
		<comment id='1' author='ivbelkin' date='2019-01-23T19:13:04Z'>
		Hi,
Checked your example, you are right. PR would be welcome.
There is no dev branch currently, so just PR it to master.
PS. contribution.md fix would be also welcome
		</comment>
		<comment id='2' author='ivbelkin' date='2019-01-23T19:50:29Z'>
		PR: &lt;denchmark-link:https://github.com/catalyst-team/catalyst/pull/81&gt;#81&lt;/denchmark-link&gt;

Thanks!
		</comment>
	</comments>
</bug>