<bug id='765' author='balakhonoff' open_date='2020-04-22T10:43:25Z' closed_time='2020-04-22T12:37:48Z'>
	<summary>Distributed mode Fail</summary>
	<description>
I am trying to use distributed mode in a .py script file converted from .ipynb notebook. When I run the notebook everything is ok with distributed=False, but when I change it to True and launch the script I get an error:
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by (1) passing the keyword argument find_unused_parameters=True to torch.nn.parallel.DistributedDataParallel; (2) making sure all forward function outputs participate in calculating loss. If you already have done the above two steps, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's forward function. Please include the loss function and the structure of the return value of forward of your module when reporting this issue (e.g. list, dict, iterable). (prepare_for_backward at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:514)
frame #0: c10::Error::Error(c10::SourceLocation, std::string const&amp;) + 0x33 (0x7f21ce104193 in /home/kb/.local/lib/python3.7/site-packages/torch/lib/libc10.so)
frame &lt;denchmark-link:https://github.com/catalyst-team/catalyst/pull/1&gt;#1&lt;/denchmark-link&gt;
: c10d::Reducer::prepare_for_backward(std::vector&lt;at::Tensor, std::allocatorat::Tensor &gt; const&amp;) + 0x731 (0x7f22198c46f1 in /home/kb/.local/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
It seems to be we need to transfer find_unused_parameters=True when wrapping a model into torch.nn.parallel.DistributedDataParallel
	</description>
	<comments>
		<comment id='1' author='balakhonoff' date='2020-04-22T12:37:39Z'>
		&lt;denchmark-link:https://github.com/balakhonoff&gt;@balakhonoff&lt;/denchmark-link&gt;
 please provide an issue report in our template format. Thank you!
		</comment>
		<comment id='2' author='balakhonoff' date='2020-04-22T12:38:24Z'>
		you can use this one as an example,
&lt;denchmark-link:https://github.com/catalyst-team/catalyst/issues/764&gt;#764&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>