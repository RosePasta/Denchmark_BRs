<bug id='713' author='litvinich' open_date='2020-03-19T13:12:22Z' closed_time='2020-04-22T12:49:30Z'>
	<summary>Freezing after training ends, Catalyst 20.03+ version</summary>
	<description>
Describe the bug
After training end I see next picture forever:
&lt;denchmark-code&gt;Top best models:
logs/4classes/version3_noleak_simpleface_effb0_cosine_ls_e01/112_really_hard_augs/200319.130339.SEybo2/checkpoints/stage1.1.pth  0.6985
&lt;/denchmark-code&gt;

It just freezes. This happened with my pipeline after upgrading 20.02 -&gt; 20.03.
To Reproduce
I have 3 GPU server with NVIDIA 1080Ti and Threadripper, if needed I can give detailed hardware. Training is in DataParallel mode, not DDP.
My config:
&lt;denchmark-code&gt;model_params:
  model: MixFace
  encoder_name: efficientnet_b0
  encoder_pretrained: True
  encoder_library: timm
  neck_transform: linear
  embeddings_size: 512
  classes: 4
  head_s: 12
  head_m1: 0.2
  head_m2: 0


runner_params:
  input_key: ["x", "labels"]


args:
  expdir: "./experiments/4classes"
  baselogdir: "./logs/4classes/version3_mixface_s12_effb0_cosine_ls_e01"
  seed: 8
  deterministic: True
  benchmark: True


stages:

  stage1:

    data_params:
      batch_size: 170
      num_workers: 12
      per_gpu_scaling: True
      drop_last: False
      shuffle: True
      loaders_params:
        valid:
          batch_size: 170

      train_dataset_params:
        image_folder_path: "/usr/local/FileStorage/data_training/glasses_classification/train"
        image_file_path: "/usr/local/FileStorage/data_training/glasses_classification/version_3/train.txt"

      valid_dataset_params:
        image_folder_path: "/usr/local/FileStorage/data_training/glasses_classification/valid"
        image_file_path: "/usr/local/FileStorage/data_training/glasses_classification/version_3/valid.txt"


    state_params:
      num_epochs: 70
      main_metric: &amp;main_metric f1
      minimize_metric: False
      valid_loader: valid


    criterion_params:
      criterion: CrossEntropyLossLabelSmoothing
      reduction: mean
      smooth_eps: 0.1


    scheduler_params:
      scheduler: CosineAnnealingWarmRestarts
      T_0: 900
      T_mult: 2
      eta_min: 0


    optimizer_params:
      optimizer: SGD
      lr: 0.1
      weight_decay: 0.001
      nesterov: True
      momentum: 0.9


    callbacks_params:
      loss:
        callback: CriterionCallback

      optimizer:
        callback: OptimizerCallback

      accuracy:
        callback: AccuracyCallback
        accuracy_args: [1, 2]

      f1_score:
        callback: F1ScoreMetricCallback
        n_classes: 4
        class_mapping: ['face_in_optical_glasses', 'face_in_protective_glasses', 'face_in_sunglasses', 'face_without_glasses']


      scheduler:
        callback: SchedulerCallback
        mode: batch

      logger:
        callback: AlchemyLogger
        token: "484b162171bce2dc5371f9acc09d9fd2"
        project: "glasses_classification"
        experiment: "version3_mixface_s12_effb0_cosine_ls_e01"
        group: "validation_version_2"
&lt;/denchmark-code&gt;

Additional context
After Ctrl + C I see next picture:
&lt;denchmark-code&gt;raceback (most recent call last):
  File "/home/andrii/anaconda3/envs/face/lib/python3.7/threading.py", line 1273, in _shutdown
    t.join()
  File "/home/andrii/anaconda3/envs/face/lib/python3.7/threading.py", line 1032, in join
    self._wait_for_tstate_lock()
  File "/home/andrii/anaconda3/envs/face/lib/python3.7/threading.py", line 1048, in _wait_for_tstate_lock
    elif lock.acquire(block, timeout):
KeyboardInterrupt
&lt;/denchmark-code&gt;

Obviously it's problem with multithreading. I can suggest, that threads are not daemons and they have something like forever loop or there are deadlocks
	</description>
	<comments>
		<comment id='1' author='litvinich' date='2020-03-20T11:30:26Z'>
		I turned Alchemy-Catalyst off and bug is fixed. So it's problem with Alchemy.
		</comment>
		<comment id='2' author='litvinich' date='2020-03-24T08:23:09Z'>
		Could not reproduce the issue, do you have a minimal example for that?
		</comment>
		<comment id='3' author='litvinich' date='2020-03-24T09:30:20Z'>
		I'm sorry, but I don't know how to give you an working example, because I cannot share my data, only my hardware and configs. But I was able to fix it - may be it will give you a suggestion why it's happening. It was fixed by next actions:
&lt;denchmark-link:https://github.com/catalyst-team/alchemy/blob/master/alchemy/logger.py#L52&gt;https://github.com/catalyst-team/alchemy/blob/master/alchemy/logger.py#L52&lt;/denchmark-link&gt;
 change this line to
&lt;denchmark-code&gt;self._thread = threading.Thread(target=self._run_worker, daemon=True)
&lt;/denchmark-code&gt;

and commenting this line &lt;denchmark-link:https://github.com/catalyst-team/alchemy/blob/master/alchemy/logger.py#L84&gt;https://github.com/catalyst-team/alchemy/blob/master/alchemy/logger.py#L84&lt;/denchmark-link&gt;

I thought my problem was caused my forever loop, so I made an thread daemon and deleted join of this thread, when logger is closed, so main thread is not waiting for this thread to join anymore, and this helped.
		</comment>
		<comment id='4' author='litvinich' date='2020-04-22T12:49:30Z'>
		could you please transfer this issue to Alchemy?
closing here
		</comment>
	</comments>
</bug>