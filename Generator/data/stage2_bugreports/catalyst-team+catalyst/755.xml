<bug id='755' author='YaLTeR' open_date='2020-04-14T11:28:06Z' closed_time='2020-05-24T13:07:52Z'>
	<summary>Tensorboard logs start at a non-zero step</summary>
	<description>
Describe the bug
Tensorboard logs start at a non-zero step.
To Reproduce
Just run one of the showcase colab notebooks, e.g. &lt;denchmark-link:https://colab.research.google.com/github/catalyst-team/catalyst/blob/master/examples/notebooks/classification-tutorial.ipynb&gt;https://colab.research.google.com/github/catalyst-team/catalyst/blob/master/examples/notebooks/classification-tutorial.ipynb&lt;/denchmark-link&gt;
 (you will need to replace multiple  invocations in the second cell with a single one to fix issues with dependencies: ).
Expected behavior
The step starts at 0.

&lt;denchmark-link:https://user-images.githubusercontent.com/1794388/79219556-2cfb1e80-7e5b-11ea-8de5-2407c14faebb.png&gt;&lt;/denchmark-link&gt;

Additional context
I believe the issue is in the following code:



catalyst/catalyst/core/runner.py


        Lines 391 to 394
      in
      2ede535






 self.state.global_step = ( 



 self.state.global_step 



 or self.state.global_epoch * len(loader) * self.state.batch_size 



 ) 





When global_step is supposed to be zero at the very start it is instead set to the value it would have after 1 epoch (since global_epoch starts at 1).
	</description>
	<comments>
		<comment id='1' author='YaLTeR' date='2020-04-22T12:57:07Z'>
		We are using one global step for both train and validation loaders. By this way, your validation plots shifts a bit. Nevertheless this allows to to check your batch metrics plots easily.
		</comment>
		<comment id='2' author='YaLTeR' date='2020-05-08T22:48:29Z'>
		Sorry, &lt;denchmark-link:https://github.com/Scitator&gt;@Scitator&lt;/denchmark-link&gt;
, it seems you don't quite understand the issue.
The value of state.global_step equals the length of the training dataset before the beginning of the first epoch. (After the first epoch it equals doubled length of the training dataset and so on).
The initial value of state.global_step is 0 and the initial value of state.global_epoch is 1, as a result, the code mentioned in the issue has this effect.
This has nothing to do with the validation loader and the fact that you have a single step counter for both loaders.
		</comment>
		<comment id='3' author='YaLTeR' date='2020-05-08T23:15:57Z'>
		I have a couple of somehow related questions.
First of all, just curious, what the piece of code mentioned in the issue was supposed to do?
Also, in documentation is said that state.global_step is

numerical indicator, counter for all batches

and I agree, "number of steps" most often means "number of butches". But in reality, it counts the number of individual samples, not batches. IMHO, it's worth changing the current behavior to counting batches, don't you think so? (Probably it's even better to have separate counters for counting samples and batches).
I can make a PR with these changes.
P.S.The current counting of samples is incorrect in any case because the size of batches can vary. (at least for the last batch in the loader)
		</comment>
		<comment id='4' author='YaLTeR' date='2020-05-10T08:26:34Z'>
		&lt;denchmark-link:https://github.com/yhn112&gt;@yhn112&lt;/denchmark-link&gt;
 Issue confirmed, thanks for explanation.
I would be glad, if you could make a PR with required fixes.
Nevertheless, sample-based logging used to have an opportunity to compare pipelines with different batch_size correctly. In this way you directly measure the number of  passed through the network.
		</comment>
	</comments>
</bug>