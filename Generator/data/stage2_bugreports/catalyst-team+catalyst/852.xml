<bug id='852' author='Ditwoo' open_date='2020-06-23T21:25:01Z' closed_time='2020-06-25T08:46:19Z'>
	<summary>Wrong global_epoch in Runner</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug Report&lt;/denchmark-h&gt;

With 20.6 release (State -&gt; Runner) global_epoch has the same value as epoch on a new stage.
Expected that global_epoch will continue to increase.
For example if experiment has two stages - first with 10 epochs and second with 5 epochs, then value of global_epoch at the end of experiment should be a 15 (not 5).
&lt;denchmark-h:h3&gt;How To Reproduce&lt;/denchmark-h&gt;

To reproduce the issue please compare outputs in 20.5 and 20.6 versions:
import torch
from torch.utils.data import DataLoader, TensorDataset
from catalyst.dl import (
    SupervisedRunner, Callback, CallbackOrder,
    CriterionCallback,
)

class PrinterCallback(Callback):
    def __init__(self):
        super().__init__(CallbackOrder.Internal)

    def on_epoch_start(self, runner):
        print(f"&gt;&gt; epoch - {runner.epoch}, global epoch - {runner.global_epoch}")

# experiment_setup
logdir = './remove_me_logs'
num_epochs = 5

# data
num_samples, num_features = int(1e4), int(1e1)
X = torch.rand(num_samples, num_features)
y = torch.randint(0, 5, size=[num_samples])
dataset = TensorDataset(X, y)
loader = DataLoader(dataset, batch_size=32, num_workers=1)
loaders = {
    'train': loader,
    'valid': loader,
}

# model, criterion, optimizer, scheduler
model = torch.nn.Linear(num_features, 5)
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())
runner = SupervisedRunner()


# first stage
runner.train(
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    loaders=loaders,
    logdir=logdir,
    num_epochs=num_epochs,
    verbose=False,
    callbacks=[PrinterCallback()]
)

# second stage
runner.train(
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    loaders=loaders,
    logdir=logdir,
    num_epochs=num_epochs,
    verbose=False,
    callbacks=[PrinterCallback()]
)
Output for 20.5:
&lt;denchmark-code&gt;&gt;&gt; epoch - 1, global epoch - 1
[2020-06-24 00:13:10,981] 
1/5 * Epoch 1 (_base): lr=0.0010 | momentum=0.9000
1/5 * Epoch 1 (train): loss=1.6276
1/5 * Epoch 1 (valid): loss=1.6183
&gt;&gt; epoch - 2, global epoch - 2
[2020-06-24 00:13:13,073] 
2/5 * Epoch 2 (_base): lr=0.0010 | momentum=0.9000
2/5 * Epoch 2 (train): loss=1.6179
2/5 * Epoch 2 (valid): loss=1.6159
&gt;&gt; epoch - 3, global epoch - 3
[2020-06-24 00:13:15,219] 
3/5 * Epoch 3 (_base): lr=0.0010 | momentum=0.9000
3/5 * Epoch 3 (train): loss=1.6158
3/5 * Epoch 3 (valid): loss=1.6140
&gt;&gt; epoch - 4, global epoch - 4
[2020-06-24 00:13:17,328] 
4/5 * Epoch 4 (_base): lr=0.0010 | momentum=0.9000
4/5 * Epoch 4 (train): loss=1.6141
4/5 * Epoch 4 (valid): loss=1.6126
&gt;&gt; epoch - 5, global epoch - 5
[2020-06-24 00:13:19,462] 
5/5 * Epoch 5 (_base): lr=0.0010 | momentum=0.9000
5/5 * Epoch 5 (train): loss=1.6129
5/5 * Epoch 5 (valid): loss=1.6115
Top best models:
remove_me_logs/checkpoints/train.5.pth  1.6115
&gt;&gt; epoch - 1, global epoch - 6
[2020-06-24 00:13:21,456] 
1/5 * Epoch 6 (_base): lr=0.0010 | momentum=0.9000
1/5 * Epoch 6 (train): loss=1.6119
1/5 * Epoch 6 (valid): loss=1.6106
&gt;&gt; epoch - 2, global epoch - 7
[2020-06-24 00:13:23,499] 
2/5 * Epoch 7 (_base): lr=0.0010 | momentum=0.9000
2/5 * Epoch 7 (train): loss=1.6111
2/5 * Epoch 7 (valid): loss=1.6099
&gt;&gt; epoch - 3, global epoch - 8
[2020-06-24 00:13:25,619] 
3/5 * Epoch 8 (_base): lr=0.0010 | momentum=0.9000
3/5 * Epoch 8 (train): loss=1.6105
3/5 * Epoch 8 (valid): loss=1.6094
&gt;&gt; epoch - 4, global epoch - 9
[2020-06-24 00:13:27,727] 
4/5 * Epoch 9 (_base): lr=0.0010 | momentum=0.9000
4/5 * Epoch 9 (train): loss=1.6101
4/5 * Epoch 9 (valid): loss=1.6090
&gt;&gt; epoch - 5, global epoch - 10
[2020-06-24 00:13:29,856] 
5/5 * Epoch 10 (_base): lr=0.0010 | momentum=0.9000
5/5 * Epoch 10 (train): loss=1.6097
5/5 * Epoch 10 (valid): loss=1.6087
Top best models:
remove_me_logs/checkpoints/train.5.pth  1.6087
&lt;/denchmark-code&gt;

Output for 20.6:
&lt;denchmark-code&gt;&gt;&gt; epoch - 1, global epoch - 1
[2020-06-24 00:09:35,632] 
1/5 * Epoch 1 (_base): lr=0.0010 | momentum=0.9000
1/5 * Epoch 1 (train): loss=1.6223
1/5 * Epoch 1 (valid): loss=1.6174
&gt;&gt; epoch - 2, global epoch - 2
[2020-06-24 00:09:36,722] 
2/5 * Epoch 2 (_base): lr=0.0010 | momentum=0.9000
2/5 * Epoch 2 (train): loss=1.6170
2/5 * Epoch 2 (valid): loss=1.6151
&gt;&gt; epoch - 3, global epoch - 3
[2020-06-24 00:09:37,793] 
3/5 * Epoch 3 (_base): lr=0.0010 | momentum=0.9000
3/5 * Epoch 3 (train): loss=1.6150
3/5 * Epoch 3 (valid): loss=1.6133
&gt;&gt; epoch - 4, global epoch - 4
[2020-06-24 00:09:38,842] 
4/5 * Epoch 4 (_base): lr=0.0010 | momentum=0.9000
4/5 * Epoch 4 (train): loss=1.6134
4/5 * Epoch 4 (valid): loss=1.6120
&gt;&gt; epoch - 5, global epoch - 5
[2020-06-24 00:09:39,907] 
5/5 * Epoch 5 (_base): lr=0.0010 | momentum=0.9000
5/5 * Epoch 5 (train): loss=1.6122
5/5 * Epoch 5 (valid): loss=1.6109
Top best models:
remove_me_logs/checkpoints/train.5.pth  1.6109
&gt;&gt; epoch - 1, global epoch - 1
[2020-06-24 00:09:40,967] 
1/5 * Epoch 1 (_base): lr=0.0010 | momentum=0.9000
1/5 * Epoch 1 (train): loss=1.6112
1/5 * Epoch 1 (valid): loss=1.6100
&gt;&gt; epoch - 2, global epoch - 2
[2020-06-24 00:09:42,000] 
2/5 * Epoch 2 (_base): lr=0.0010 | momentum=0.9000
2/5 * Epoch 2 (train): loss=1.6104
2/5 * Epoch 2 (valid): loss=1.6094
&gt;&gt; epoch - 3, global epoch - 3
[2020-06-24 00:09:43,091] 
3/5 * Epoch 3 (_base): lr=0.0010 | momentum=0.9000
3/5 * Epoch 3 (train): loss=1.6099
3/5 * Epoch 3 (valid): loss=1.6089
&gt;&gt; epoch - 4, global epoch - 4
[2020-06-24 00:09:44,131] 
4/5 * Epoch 4 (_base): lr=0.0010 | momentum=0.9000
4/5 * Epoch 4 (train): loss=1.6094
4/5 * Epoch 4 (valid): loss=1.6085
&gt;&gt; epoch - 5, global epoch - 5
[2020-06-24 00:09:45,171] 
5/5 * Epoch 5 (_base): lr=0.0010 | momentum=0.9000
5/5 * Epoch 5 (train): loss=1.6090
5/5 * Epoch 5 (valid): loss=1.6081
Top best models:
remove_me_logs/checkpoints/train.5.pth  1.6081
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Expected that after first stage global epoch counter will increase.
	</description>
	<comments>
		<comment id='1' author='Ditwoo' date='2020-06-25T08:46:16Z'>
		Fixed in &lt;denchmark-link:https://github.com/catalyst-team/catalyst/pull/858&gt;#858&lt;/denchmark-link&gt;
.
		</comment>
	</comments>
</bug>