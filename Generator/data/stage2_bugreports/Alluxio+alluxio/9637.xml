<bug id='9637' author='zrss' open_date='2019-08-05T08:18:05Z' closed_time='2019-08-07T23:01:00Z'>
	<summary>Alluxio worker keep output this stream error warning</summary>
	<description>
Alluxio Version:
What version of Alluxio are you using?
tag v2.0.0
Describe the bug
we launch a python multi process app for reading data from alluxio-fuse, it works at this way
&lt;denchmark-code&gt;python dataset_read_raw.py --raw_data_url=/mnt/alluxio-fuse --num_gpus=1 --num_epochs=30 --raw_reader_threads=16
&lt;/denchmark-code&gt;

and the param of raw_reader_threads controll the number of processes for reading (data is the index file, like train-data-000, train-data-001, and each process read part of them)
kubectl logs alluxio-worker-d6lnd alluxio-worker
&lt;denchmark-code&gt;Aug 05, 2019 8:16:54 AM io.grpc.netty.NettyServerHandler onStreamError
WARNING: Stream Error
io.netty.handler.codec.http2.Http2Exception$StreamException: Received DATA frame for an unknown stream 11769
	at io.netty.handler.codec.http2.Http2Exception.streamError(Http2Exception.java:129)
	at io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder$FrameReadListener.shouldIgnoreHeadersOrDataFrame(DefaultHttp2ConnectionDecoder.java:531)
	at io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder$FrameReadListener.onDataRead(DefaultHttp2ConnectionDecoder.java:183)
	at io.netty.handler.codec.http2.Http2InboundFrameLogger$1.onDataRead(Http2InboundFrameLogger.java:48)
	at io.netty.handler.codec.http2.DefaultHttp2FrameReader.readDataFrame(DefaultHttp2FrameReader.java:421)
	at io.netty.handler.codec.http2.DefaultHttp2FrameReader.processPayloadState(DefaultHttp2FrameReader.java:251)
	at io.netty.handler.codec.http2.DefaultHttp2FrameReader.readFrame(DefaultHttp2FrameReader.java:160)
	at io.netty.handler.codec.http2.Http2InboundFrameLogger.readFrame(Http2InboundFrameLogger.java:41)
	at io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder.decodeFrame(DefaultHttp2ConnectionDecoder.java:118)
	at io.netty.handler.codec.http2.Http2ConnectionHandler$FrameDecoder.decode(Http2ConnectionHandler.java:390)
	at io.netty.handler.codec.http2.Http2ConnectionHandler.decode(Http2ConnectionHandler.java:450)
	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:502)
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:441)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:278)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1434)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:965)
	at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:799)
	at io.netty.channel.epoll.EpollDomainSocketChannel$EpollDomainUnsafe.epollInReady(EpollDomainSocketChannel.java:138)
	at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:421)
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:321)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
	at java.lang.Thread.run(Thread.java:748)
&lt;/denchmark-code&gt;

and the CPU usage of fuse java process is around 300%
&lt;denchmark-code&gt;top - 20:14:26 up 13 days,  4:05,  3 users,  load average: 1.37, 2.73, 3.33
Tasks:   1 total,   0 running,   1 sleeping,   0 stopped,   0 zombie
%Cpu(s): 24.8 us, 16.2 sy,  0.0 ni, 57.7 id,  0.0 wa,  0.0 hi,  1.2 si,  0.0 st
KiB Mem : 13184770+total, 11139716+free,  5116756 used, 15333776 buff/cache
KiB Swap:        0 total,        0 free,        0 used. 11355702+avail Mem 

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                          
18151 root      20   0 10.567g 1.247g  15476 S 443.3  1.0 163:56.81 java                                                                             
&lt;/denchmark-code&gt;

the performance output by our python app
&lt;denchmark-code&gt;INFO:tensorflow:Read raw image from raw_data_url /mnt/alluxio-fuse (3670 files, 221.684 MB)
INFO:tensorflow:Epoch=1, time=2.607, 1407.866 images/sec, 85.041 MB/sec
INFO:tensorflow:Epoch=2, time=2.784, 1318.240 images/sec, 79.627 MB/sec
INFO:tensorflow:Epoch=3, time=2.047, 1793.023 images/sec, 108.306 MB/sec
INFO:tensorflow:Epoch=4, time=1.388, 2644.803 images/sec, 159.757 MB/sec
INFO:tensorflow:Epoch=5, time=1.328, 2763.994 images/sec, 166.957 MB/sec
INFO:tensorflow:Epoch=6, time=1.172, 3130.983 images/sec, 189.125 MB/sec
INFO:tensorflow:Epoch=7, time=1.399, 2623.301 images/sec, 158.459 MB/sec
INFO:tensorflow:Epoch=8, time=2.498, 1469.237 images/sec, 88.748 MB/sec
INFO:tensorflow:Epoch=9, time=2.959, 1240.401 images/sec, 74.926 MB/sec
INFO:tensorflow:Epoch=10, time=1.960, 1872.500 images/sec, 113.107 MB/sec
INFO:tensorflow:Epoch=11, time=1.372, 2674.114 images/sec, 161.528 MB/sec
INFO:tensorflow:Epoch=12, time=1.257, 2920.619 images/sec, 176.418 MB/sec
INFO:tensorflow:Epoch=13, time=1.409, 2604.536 images/sec, 157.325 MB/sec
INFO:tensorflow:Epoch=14, time=1.543, 2378.217 images/sec, 143.654 MB/sec
INFO:tensorflow:Epoch=15, time=2.159, 1699.490 images/sec, 102.656 MB/sec
INFO:tensorflow:Epoch=16, time=3.073, 1194.418 images/sec, 72.148 MB/sec
INFO:tensorflow:Epoch=17, time=2.472, 1484.765 images/sec, 89.686 MB/sec
INFO:tensorflow:Epoch=18, time=1.596, 2299.553 images/sec, 138.903 MB/sec
INFO:tensorflow:Epoch=19, time=1.993, 1841.103 images/sec, 111.210 MB/sec
INFO:tensorflow:Epoch=20, time=3.291, 1115.039 images/sec, 67.353 MB/sec
INFO:tensorflow:Epoch=21, time=3.300, 1112.132 images/sec, 67.177 MB/sec
INFO:tensorflow:Epoch=22, time=2.710, 1354.343 images/sec, 81.808 MB/sec
INFO:tensorflow:Epoch=23, time=3.233, 1135.041 images/sec, 68.561 MB/sec
INFO:tensorflow:Epoch=24, time=3.158, 1162.271 images/sec, 70.206 MB/sec
INFO:tensorflow:Epoch=25, time=2.598, 1412.641 images/sec, 85.330 MB/sec
INFO:tensorflow:Epoch=26, time=1.654, 2218.405 images/sec, 134.001 MB/sec
INFO:tensorflow:Epoch=27, time=1.478, 2483.514 images/sec, 150.015 MB/sec
INFO:tensorflow:Epoch=28, time=1.305, 2812.763 images/sec, 169.903 MB/sec
INFO:tensorflow:Epoch=29, time=2.850, 1287.927 images/sec, 77.796 MB/sec
INFO:tensorflow:Epoch=30, time=1.602, 2290.286 images/sec, 138.343 MB/sec
&lt;/denchmark-code&gt;

To Reproduce
Steps to reproduce the behavior (as minimally and precisely as possible)
Expected behavior
A clear and concise description of what you expected to happen.
Urgency
Describe the impact and urgency of the bug.
Additional context
Add any other context about the problem here.
	</description>
	<comments>
		<comment id='1' author='zrss' date='2019-08-05T17:27:11Z'>
		&lt;denchmark-link:https://github.com/ggezer&gt;@ggezer&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/bf8086&gt;@bf8086&lt;/denchmark-link&gt;
 can you take a look at the error message and get an opinion if this error message is something fatal or benign?
		</comment>
		<comment id='2' author='zrss' date='2019-08-05T18:19:06Z'>
		This is common when gRPC server receives messages belongs to a closed channel due to stream cancellation. It can be safely ignored.
		</comment>
		<comment id='3' author='zrss' date='2019-08-05T20:00:35Z'>
		This is a common issue with gRPC being a bit too noisy with its logs. Relevant issue related on Github:
&lt;denchmark-link:https://github.com/grpc/grpc-java/issues/4651&gt;grpc/grpc-java#4651&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='zrss' date='2019-08-06T00:18:34Z'>
		It seems that we have already disabled the logging messages from 
&lt;denchmark-link:https://github.com/Alluxio/alluxio/blob/master/conf/log4j.properties#L174&gt;log4j.logger.io.grpc.netty.NettyServerHandler=OFF&lt;/denchmark-link&gt;
,
&lt;denchmark-link:https://github.com/zrss&gt;@zrss&lt;/denchmark-link&gt;
 do you see above line from your worker container at ?
		</comment>
		<comment id='5' author='zrss' date='2019-08-06T00:41:15Z'>
		thx for the reply. i do see this line in log4j.properties in the last line of worker's log conf ...
&lt;denchmark-code&gt;# Appender for Fuse
log4j.appender.FUSE_LOGGER=org.apache.log4j.RollingFileAppender
log4j.appender.FUSE_LOGGER.File=${alluxio.logs.dir}/fuse.log
log4j.appender.FUSE_LOGGER.MaxFileSize=10MB
log4j.appender.FUSE_LOGGER.MaxBackupIndex=10
log4j.appender.FUSE_LOGGER.layout=org.apache.log4j.PatternLayout
log4j.appender.FUSE_LOGGER.layout.ConversionPattern=%d{ISO8601} %-5p %c{1} - %m%n

# Disable noisy DEBUG logs
log4j.logger.io.grpc.netty.NettyServerHandler=OFF
&lt;/denchmark-code&gt;

actually, i doubt why the fuse have to use so much CPU (~ 300%), can there is a way to find that out ?
		</comment>
		<comment id='6' author='zrss' date='2019-08-07T23:01:00Z'>
		Resolved, this message is from gRPC lib
		</comment>
		<comment id='7' author='zrss' date='2019-09-21T16:16:47Z'>
		I also see three are a lot of those errors in the worker's log. It's difficult to find the useful info in the logs. Is there any way to fix it?
		</comment>
		<comment id='8' author='zrss' date='2019-10-09T06:24:51Z'>
		&lt;denchmark-link:https://github.com/cheyang&gt;@cheyang&lt;/denchmark-link&gt;
 if you have an easy way to reproduce the "onStreamError" message, please create an issue as errors like this are supposed to be suppressed by  set in 
		</comment>
	</comments>
</bug>