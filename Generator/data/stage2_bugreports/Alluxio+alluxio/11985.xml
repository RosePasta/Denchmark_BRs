<bug id='11985' author='marrycheck' open_date='2020-08-14T03:21:58Z' closed_time='2020-09-22T08:03:01Z'>
	<summary>When installing alluxio through helm, UnknownHostException appeared</summary>
	<description>
Alluxio Version:
alluxio:2.3.0

Following the guide &lt;denchmark-link:https://docs.alluxio.io/os/user/stable/en/deploy/Running-Alluxio-On-Kubernetes.html&gt;https://docs.alluxio.io/os/user/stable/en/deploy/Running-Alluxio-On-Kubernetes.html&lt;/denchmark-link&gt;
 to install alluxio.Error s occurd. I had set multi master3.
Pod status is :
&lt;denchmark-code&gt;[root@k8s-master01 ~]# kubectl -n spark get pods
NAME                                        READY   STATUS             RESTARTS   AGE
alluxio-fuse-j8rrm                          1/1     Running            0          10s
alluxio-fuse-rrcrn                          1/1     Running            0          10s
alluxio-fuse-wfrkr                          1/1     Running            0          10s
alluxio-master-0                            0/2     Running            0          10s
alluxio-worker-4kmbq                        0/2     CrashLoopBackOff   1          10s
alluxio-worker-5ng5f                        0/2     CrashLoopBackOff   1          10s
alluxio-worker-8rvbz                        0/2     CrashLoopBackOff   1          10s

&lt;/denchmark-code&gt;

Then i got endpoints:
&lt;denchmark-code&gt;[root@k8s-master01 ~]# kubectl -n spark describe  endpoints alluxio-master-0 
Name:         alluxio-master-0
Namespace:    spark
Labels:       app=alluxio
              chart=alluxio-0.6.5
              heritage=Helm
              release=alluxio-1597374253
              role=alluxio-master
              service.kubernetes.io/headless=
Annotations:  endpoints.kubernetes.io/last-change-trigger-time: 2020-08-14T03:04:36Z
Subsets:
  Addresses:          10.244.5.96
  NotReadyAddresses:  &lt;none&gt;
  Ports:
    Name          Port   Protocol
    ----          ----   --------
    embedded      19200  TCP
    rpc           19998  TCP
    job-rpc       20001  TCP
    web           19999  TCP
    job-web       20002  TCP
    job-embedded  20003  TCP

Events:  &lt;none&gt;

&lt;/denchmark-code&gt;

Logs for master :
&lt;denchmark-code&gt;[root@k8s-master01 ~]# kubectl -n spark logs alluxio-master-0 -c alluxio-master
2020-08-14 03:04:20,037 INFO  BlockMasterFactory - Creating alluxio.master.block.BlockMaster 
2020-08-14 03:04:20,037 INFO  FileSystemMasterFactory - Creating alluxio.master.file.FileSystemMaster 
2020-08-14 03:04:20,037 INFO  MetricsMasterFactory - Creating alluxio.master.metrics.MetricsMaster 
2020-08-14 03:04:20,037 INFO  MetaMasterFactory - Creating alluxio.master.meta.MetaMaster 
2020-08-14 03:04:20,037 INFO  TableMasterFactory - Creating alluxio.master.table.TableMaster 
2020-08-14 03:04:20,113 INFO  TieredIdentityFactory - Initialized tiered identity TieredIdentity(node=alluxio-master-0, rack=null)
2020-08-14 03:04:20,114 INFO  UnderDatabaseRegistry - Loading udb jars from /opt/alluxio-2.3.0/lib
2020-08-14 03:04:20,137 INFO  UnderDatabaseRegistry - Registered UDBs: hive,glue
2020-08-14 03:04:20,139 INFO  LayoutRegistry - Registered Table Layouts: hive
2020-08-14 03:04:20,146 INFO  ExtensionFactoryRegistry - Loading core jars from /opt/alluxio-2.3.0/lib
2020-08-14 03:04:20,174 INFO  ExtensionFactoryRegistry - Loading extension jars from /opt/alluxio-2.3.0/extensions
2020-08-14 03:04:20,942 INFO  ProcessUtils - Starting Alluxio master @alluxio-master-0:19998.
2020-08-14 03:04:20,943 INFO  RaftJournalSystem - Initializing Raft Journal System
2020-08-14 03:04:20,951 INFO  JournalStateMachine - Initialized new journal state machine
2020-08-14 03:04:21,062 INFO  AbstractPrimarySelector - Primary selector transitioning to SECONDARY
2020-08-14 03:04:21,063 INFO  RaftJournalSystem - Starting Raft journal system. Cluster addresses: [alluxio-master-0:19200, alluxio-master-1:19200, alluxio-master-2:19200]. Local address: alluxio-master-0:19200
2020-08-14 03:04:21,288 INFO  GrpcMessagingServer - Successfully started messaging server at: alluxio-master-0:19200
2020-08-14 03:04:21,299 INFO  ServerContext - alluxio-master-0:19200 - Transitioning to FOLLOWER
2020-08-14 03:04:21,307 INFO  AbstractPrimarySelector - Primary selector transitioning to SECONDARY
2020-08-14 03:04:21,333 INFO  NettyUtils - EPOLL_MODE is available
Aug 14, 2020 3:04:21 AM io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
WARNING: [Channel&lt;4&gt;: (alluxio-master-2:19200)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host alluxio-master-2, cause=java.lang.RuntimeException: java.net.UnknownHostException: alluxio-master-2: Name does not resolve
	at io.grpc.internal.DnsNameResolver.resolveAll(DnsNameResolver.java:436)
	at io.grpc.internal.DnsNameResolver$Resolve.resolveInternal(DnsNameResolver.java:272)
	at io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:228)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.UnknownHostException: alluxio-master-2: Name does not resolve
	at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)
	at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929)
	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324)
	at java.net.InetAddress.getAllByName0(InetAddress.java:1277)
	at java.net.InetAddress.getAllByName(InetAddress.java:1193)
	at java.net.InetAddress.getAllByName(InetAddress.java:1127)
	at io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:646)
	at io.grpc.internal.DnsNameResolver.resolveAll(DnsNameResolver.java:404)
	... 5 more
}
Aug 14, 2020 3:04:21 AM io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
WARNING: [Channel&lt;6&gt;: (alluxio-master-1:19200)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host alluxio-master-1, cause=java.lang.RuntimeException: java.net.UnknownHostException: alluxio-master-1: Name does not resolve
	at io.grpc.internal.DnsNameResolver.resolveAll(DnsNameResolver.java:436)
	at io.grpc.internal.DnsNameResolver$Resolve.resolveInternal(DnsNameResolver.java:272)
	at io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:228)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.UnknownHostException: alluxio-master-1: Name does not resolve
	at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)
	at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929)
	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324)
	at java.net.InetAddress.getAllByName0(InetAddress.java:1277)
	at java.net.InetAddress.getAllByName(InetAddress.java:1193)
	at java.net.InetAddress.getAllByName(InetAddress.java:1127)
	at io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:646)
	at io.grpc.internal.DnsNameResolver.resolveAll(DnsNameResolver.java:404)
	... 5 more
}
2020-08-14 03:04:31,458 INFO  FollowerState - alluxio-master-0:19200 - Polling members [ServerMember[type=ACTIVE, status=AVAILABLE, serverAddress=alluxio-master-2:19200, clientAddress=null], ServerMember[type=ACTIVE, status=AVAILABLE, serverAddress=alluxio-master-1:19200, clientAddress=null]]
Aug 14, 2020 3:04:31 AM io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
WARNING: [Channel&lt;10&gt;: (alluxio-master-1:19200)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host alluxio-master-1, cause=java.lang.RuntimeException: java.net.UnknownHostException: alluxio-master-1
	at io.grpc.internal.DnsNameResolver.resolveAll(DnsNameResolver.java:436)
	at io.grpc.internal.DnsNameResolver$Resolve.resolveInternal(DnsNameResolver.java:272)
	at io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:228)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.UnknownHostException: alluxio-master-1
	at java.net.InetAddress.getAllByName0(InetAddress.java:1281)
	at java.net.InetAddress.getAllByName(InetAddress.java:1193)
	at java.net.InetAddress.getAllByName(InetAddress.java:1127)
	at io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:646)
	at io.grpc.internal.DnsNameResolver.resolveAll(DnsNameResolver.java:404)
	... 5 more
}

&lt;/denchmark-code&gt;

logs for worker:
&lt;denchmark-code&gt;[root@k8s-master01 ~]# kubectl -n spark logs alluxio-worker-55n5h  -c alluxio-worker
2020-08-14 05:18:17,159 INFO  NettyUtils - EPOLL_MODE is available
Aug 14, 2020 5:18:22 AM io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
WARNING: [Channel&lt;1&gt;: (alluxio-master-0:19998)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host alluxio-master-0, cause=java.lang.RuntimeException: java.net.UnknownHostException: alluxio-master-0: Try again
	at io.grpc.internal.DnsNameResolver.resolveAll(DnsNameResolver.java:436)
	at io.grpc.internal.DnsNameResolver$Resolve.resolveInternal(DnsNameResolver.java:272)
	at io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:228)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.UnknownHostException: alluxio-master-0: Try again
	at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)
	at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929)
	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324)
	at java.net.InetAddress.getAllByName0(InetAddress.java:1277)
	at java.net.InetAddress.getAllByName(InetAddress.java:1193)
	at java.net.InetAddress.getAllByName(InetAddress.java:1127)
	at io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:646)
	at io.grpc.internal.DnsNameResolver.resolveAll(DnsNameResolver.java:404)
	... 5 more
}
2020-08-14 05:18:22,395 WARN  RetryUtils - Failed to load cluster default configuration with master (attempt 1): alluxio.exception.status.UnavailableException: Failed to handshake with master alluxio-master-0:19998 to load cluster default configuration values: UNAVAILABLE: Unable to resolve host alluxio-master-0
Aug 14, 2020 5:18:22 AM io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
WARNING: [Channel&lt;3&gt;: (alluxio-master-0:19998)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host alluxio-master-0, cause=java.lang.RuntimeException: java.net.UnknownHostException: alluxio-master-0
	at io.grpc.internal.DnsNameResolver.resolveAll(DnsNameResolver.java:436)
	at io.grpc.internal.DnsNameResolver$Resolve.resolveInternal(DnsNameResolver.java:272)
	at io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:228)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.UnknownHostException: alluxio-master-0
	at java.net.InetAddress.getAllByName0(InetAddress.java:1281)
	at java.net.InetAddress.getAllByName(InetAddress.java:1193)
	at java.net.InetAddress.getAllByName(InetAddress.java:1127)
	at io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:646)
	at io.grpc.internal.DnsNameResolver.resolveAll(DnsNameResolver.java:404)
	... 5 more
}

&lt;/denchmark-code&gt;

Logs for fuse:
&lt;denchmark-code&gt;[root@k8s-master01 ~]# kubectl -n spark logs alluxio-fuse-pmr58
umount: /mnt/alluxio-fuse: not mounted
Starting alluxio-fuse on local host.
2020-08-14 05:18:17,532 INFO  TieredIdentityFactory - Initialized tiered identity TieredIdentity(node=192.168.37.162, rack=null)
2020-08-14 05:19:13,220 INFO  NettyUtils - EPOLL_MODE is available
Aug 14, 2020 5:19:33 AM io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
WARNING: [Channel&lt;1&gt;: (alluxio-master-0:19998)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host alluxio-master-0, cause=java.lang.RuntimeException: java.net.UnknownHostException: alluxio-master-0: Temporary failure in name resolution
	at io.grpc.internal.DnsNameResolver.resolveAll(DnsNameResolver.java:436)
	at io.grpc.internal.DnsNameResolver$Resolve.resolveInternal(DnsNameResolver.java:272)
	at io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:228)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.UnknownHostException: alluxio-master-0: Temporary failure in name resolution
	at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)
	at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929)
	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324)
	at java.net.InetAddress.getAllByName0(InetAddress.java:1277)
	at java.net.InetAddress.getAllByName(InetAddress.java:1193)
	at java.net.InetAddress.getAllByName(InetAddress.java:1127)
	at io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:646)
	at io.grpc.internal.DnsNameResolver.resolveAll(DnsNameResolver.java:404)
	... 5 more
}
Aug 14, 2020 5:19:33 AM io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
WARNING: [Channel&lt;3&gt;: (alluxio-master-0:19998)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host alluxio-master-0, cause=java.lang.RuntimeException: java.net.UnknownHostException: alluxio-master-0
	at io.grpc.internal.DnsNameResolver.resolveAll(DnsNameResolver.java:436)
	at io.grpc.internal.DnsNameResolver$Resolve.resolveInternal(DnsNameResolver.java:272)
	at io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:228)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.UnknownHostException: alluxio-master-0
	at java.net.InetAddress.getAllByName0(InetAddress.java:1281)
	at java.net.InetAddress.getAllByName(InetAddress.java:1193)
	at java.net.InetAddress.getAllByName(InetAddress.java:1127)
	at io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:646)
	at io.grpc.internal.DnsNameResolver.resolveAll(DnsNameResolver.java:404)
	... 5 more
}

&lt;/denchmark-code&gt;

To Reproduce
Steps to reproduce the behavior (as minimally and precisely as possible)
Expected behavior
A clear and concise description of what you expected to happen.
Urgency
Describe the impact and urgency of the bug.
Additional context
Add any other context about the problem here.
	</description>
	<comments>
		<comment id='1' author='marrycheck' date='2020-08-14T03:27:19Z'>
		And my Configuration file is :
&lt;denchmark-code&gt;#
# The Alluxio Open Foundation licenses this work under the Apache License, version 2.0
# (the "License"). You may not use this work except in compliance with the License, which is
# available at www.apache.org/licenses/LICENSE-2.0
#
# This software is distributed on an "AS IS" basis, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
# either express or implied, as more fully set forth in the License.
#
# See the NOTICE file distributed with this work for information regarding copyright ownership.
#

# This should not be modified in the usual case.
fullnameOverride: alluxio


## Common ##

# Docker Image
image: alluxio/alluxio
imageTag: 2.3.0
imagePullPolicy: IfNotPresent

# Security Context
user: 1000
group: 1000
fsGroup: 1000

# Site properties for all the components
properties:
  # alluxio.user.metrics.collection.enabled: 'true'
#  alluxio.security.stale.channel.purge.interval: 365d
  alluxio.master.mount.table.root.ufs: "s3://xxxx"
  alluxio.master.mount.table.root.option.aws.accessKeyId: "xxx"
  alluxio.master.mount.table.root.option.aws.secretKey: "xxx"
  alluxio.master.mount.table.root.option.alluxio.underfs.s3.endpoint: "xxx"
  # worker props
  alluxio.worker.data.server.domain.socket.address: /opt/domain
  alluxio.worker.data.server.domain.socket.as.uuid: true
  # master props
  alluxio.master.metastore.dir: "/mnt/data/alluxio-k8s/metastore"


# Recommended JVM Heap options for running in Docker
# Ref: https://developers.redhat.com/blog/2017/03/14/java-inside-docker/
# These JVM options are common to all Alluxio services
# jvmOptions:
#   - "-XX:+UnlockExperimentalVMOptions"
#   - "-XX:+UseCGroupMemoryLimitForHeap"
#   - "-XX:MaxRAMFraction=2"

# Mount Persistent Volumes to all components
# mounts:
# - name: &lt;persistentVolume claimName&gt;
#   path: &lt;mountPath&gt;

# Use labels to run Alluxio on a subset of the K8s nodes
nodeSelector: {"alluxio": "true"}

## Master ##

master:
  count: 3 # Controls the number of StatefulSets. For multiMaster mode increase this to &gt;1.
  replicas: 1 # Controls #replicas in a StatefulSet and should not be modified in the usual case.
  args: # Arguments to Docker entrypoint
    - master-only
    - --no-format
  # Properties for the master component
  properties:
    # Example: use ROCKS DB instead of Heap
    # alluxio.master.metastore: ROCKS
    # alluxio.master.metastore.dir: /metastore
  resources:
    # The default xmx is 8G
    limits:
      cpu: "4"
      memory: "8G"
    requests:
      cpu: "1"
      memory: "1G"
  ports:
    embedded: 19200
    rpc: 19998
    web: 19999
  hostPID: false
  hostNetwork: false
  # dnsPolicy will be ClusterFirstWithHostNet if hostNetwork: true
  # and ClusterFirst if hostNetwork: false
  # You can specify dnsPolicy here to override this inference
  # dnsPolicy: ClusterFirst
  # JVM options specific to the master container
  jvmOptions:
  nodeSelector: {}

jobMaster:
  args:
    - job-master
  # Properties for the jobMaster component
  properties:
  resources:
    limits:
      cpu: "4"
      memory: "8G"
    requests:
      cpu: "1"
      memory: "1G"
  ports:
    embedded: 20003
    rpc: 20001
    web: 20002
  # JVM options specific to the jobMaster container
  jvmOptions:

# Alluxio supports journal type of UFS and EMBEDDED
# UFS journal with HDFS example
# journal:
#   type: "UFS"
#   folder: "hdfs://{$hostname}:{$hostport}/journal"
# EMBEDDED journal to /journal example
# journal:
#   type: "EMBEDDED"
#   folder: "/journal"
journal:
  type: "EMBEDDED" # "UFS" or "EMBEDDED"
  ufsType: "local" # Ignored if type is "EMBEDDED". "local" or "HDFS"
  folder: "/journal" # Master journal folder
  # volumeType controls the type of journal volume.
  # It can be "persistentVolumeClaim" or "emptyDir"
  volumeType: emptyDir
  size: 1Gi
  # Attributes to use when the journal is persistentVolumeClaim
  storageClass: "standard"
  accessModes:
    - ReadWriteOnce
  # Attributes to use when the journal is emptyDir
  medium: ""
  # Configuration for journal formatting job
  format:
    runFormat: true # Change to true to format journal


# You can enable metastore to use ROCKS DB instead of Heap
# metastore:
#   volumeType: persistentVolumeClaim # Options: "persistentVolumeClaim" or "emptyDir"
#   size: 1Gi
#   mountPath: /metastore
# # Attributes to use when the metastore is persistentVolumeClaim
#   storageClass: "standard"
#   accessModes:
#    - ReadWriteOnce
# # Attributes to use when the metastore is emptyDir
#   medium: ""


## Worker ##

worker:
  args:
    - worker-only
    - --no-format
  # Properties for the worker component
  properties:
  resources:
    limits:
      cpu: "4"
      memory: "4G"
    requests:
      cpu: "1"
      memory: "2G"
  ports:
    rpc: 29999
    web: 30000
  # hostPID requires escalated privileges
  hostPID: false
  hostNetwork: true
  # dnsPolicy will be ClusterFirstWithHostNet if hostNetwork: true
  # and ClusterFirst if hostNetwork: false
  # You can specify dnsPolicy here to override this inference
  dnsPolicy: ClusterFirstWithHostNet
  # JVM options specific to the worker container
  jvmOptions:
  nodeSelector: {}

jobWorker:
  args:
    - job-worker
  # Properties for the jobWorker component
  properties:
  resources:
    limits:
      cpu: "4"
      memory: "4G"
    requests:
      cpu: "1"
      memory: "1G"
  ports:
    rpc: 30001
    data: 30002
    web: 30003
  # JVM options specific to the jobWorker container
  jvmOptions:

# Tiered Storage
# emptyDir example
#  - level: 0
#    alias: MEM
#    mediumtype: MEM
#    path: /dev/shm
#    type: emptyDir
#    quota: 1G
#
# hostPath example
#  - level: 0
#    alias: MEM
#    mediumtype: MEM
#    path: /dev/shm
#    type: hostPath
#    quota: 1G
#
# persistentVolumeClaim example
#  - level: 1
#    alias: SSD
#    mediumtype: SSD
#    type: persistentVolumeClaim
#    name: alluxio-ssd
#    path: /dev/ssd
#    quota: 10G
#
# multi-part mediumtype example
#  - level: 1
#    alias: SSD,HDD
#    mediumtype: SSD,HDD
#    type: persistentVolumeClaim
#    name: alluxio-ssd,alluxio-hdd
#    path: /dev/ssd,/dev/hdd
#    quota: 10G,10G
tieredstore:
  levels:
    - level: 0
      alias: MEM
      mediumtype: MEM
      path: /mnt/data/alluxio-k8s/ramdisk
      type: hostPath
      quota: 10G
      high: 0.9
      low: 0.7
    - level: 1
      alias: HDD
      mediumtype: HDD
      path: /mnt/data/alluxio-k8s/hdd-disk1
      type: hostPath
      quota: 100G
      high: 0.9
      low: 0.7

# Short circuit related properties
shortCircuit:
  enabled: true
  # The policy for short circuit can be "local" or "uuid",
  # local means the cache directory is in the same mount namespace,
  # uuid means interact with domain socket
  policy: uuid
  # volumeType controls the type of shortCircuit volume.
  # It can be "persistentVolumeClaim" or "hostPath"
  volumeType: hostPath
  size: 1Mi
  # Attributes to use if the domain socket volume is PVC
  pvcName: alluxio-worker-domain-socket
  accessModes:
    - ReadWriteOnce
  storageClass: standard
  # Attributes to use if the domain socket volume is hostPath
  hostPath: "/tmp/alluxio-domain" # The hostPath directory to use


## FUSE ##

fuse:
  image: alluxio/alluxio-fuse
  imageTag: 2.3.0
  imagePullPolicy: IfNotPresent
  # Change both to true to deploy FUSE
  enabled: true
  clientEnabled: false
  # Properties for the jobWorker component
  properties:
  # Customize the MaxDirectMemorySize
  # These options are specific to the FUSE daemon
  jvmOptions:
    - "-XX:MaxDirectMemorySize=2g"
  hostNetwork: true
  # hostPID requires escalated privileges
  hostPID: true
  dnsPolicy: ClusterFirstWithHostNet
  user: 0
  group: 0
  fsGroup: 0
  args:
    - fuse
    - --fuse-opts=allow_other
  # Mount path in the host
  mountPath: /mnt/alluxio-fuse
  resources:
    requests:
      cpu: "0.5"
      memory: "1G"
    limits:
      cpu: "1"
      memory: "1G"


##  Secrets ##

# Format: (&lt;name&gt;:&lt;mount path under /secrets/&gt;):
# secrets:
#   master: # Shared by master and jobMaster containers
#     alluxio-hdfs-config: hdfsConfig
#   worker: # Shared by worker and jobWorker containers
#     alluxio-hdfs-config: hdfsConfig

&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='marrycheck' date='2020-08-14T09:55:53Z'>
		Do you use the Alluxio helm repo or extract the helm package yourself from the 2.3.0 tarball?
Alluxio master pods are deployed using StatefulSet, meaning it is created one by one. Once alluxio-master-0 is Running and is Ready 2/2, alluxio-master-1 is started. You see alluxio-master-2 is not resolvable because the pod is not started yet. Normally alluxio-master-0 will start first and pass the readiness probe, although it will not find the peers for the quorum, it's gonna retry for a while and not crash. In this period alluxio-master-1 and 2 will start the same way and they will finally create the quorum.
I think the error you see in the master log is just the symptom, not the root cause. Can you please do a kubectl get pods --watch after you do helm install and add what you see here? I'd like to see if the master pods ever pass the readiness check and alluxio-master-1 gets created. You should also do kubectl describe pod alluxio-master-&lt;index&gt; do look for anormalies.
		</comment>
		<comment id='3' author='marrycheck' date='2020-08-24T01:26:19Z'>
		Thanks for your replay.I extract the helm package by using the commands
&lt;denchmark-code&gt;!/bin/sh
id=$(docker create alluxio/alluxio:2.3.0)
docker cp $id:/opt/alluxio/integration/kubernetes/ - &gt; ./kubernetes.tar
docker rm -v $id 1&gt;/dev/null  
&lt;/denchmark-code&gt;

Logs for the command ''' kubectl -n spark get pods --watch```
&lt;denchmark-code&gt;[root@k8s-master01 ~]# kubectl -n spark get pods --watch
NAME                                        READY   STATUS    RESTARTS   AGE
sparkoperator-1596693252-6fc5457db9-5zgqp   1/1     Running   4          12d
alluxio-fuse-client-2gx7v                   0/1     Pending   0          0s
alluxio-worker-4s6kw                        0/2     Pending   0          0s
alluxio-master-0                            0/2     Pending   0          0s
alluxio-fuse-client-qv77h                   0/1     Pending   0          0s
alluxio-fuse-client-wmw7z                   0/1     Pending   0          0s
alluxio-fuse-client-2gx7v                   0/1     Pending   0          0s
alluxio-worker-4s6kw                        0/2     Pending   0          0s
alluxio-master-0                            0/2     Pending   0          0s
alluxio-worker-hzppj                        0/2     Pending   0          0s
alluxio-fuse-client-qv77h                   0/1     Pending   0          0s
alluxio-worker-kvmnf                        0/2     Pending   0          0s
alluxio-fuse-client-wmw7z                   0/1     Pending   0          0s
alluxio-fuse-client-kg5m6                   0/1     Pending   0          0s
alluxio-fuse-client-tm7dx                   0/1     Pending   0          0s
alluxio-worker-hzppj                        0/2     Pending   0          0s
alluxio-fuse-client-2gx7v                   0/1     ContainerCreating   0          0s
alluxio-fuse-client-kg5m6                   0/1     Pending             0          0s
alluxio-worker-kvmnf                        0/2     Pending             0          0s
alluxio-worker-4s6kw                        0/2     ContainerCreating   0          0s
alluxio-fuse-client-tm7dx                   0/1     Pending             0          0s
alluxio-fuse-client-wmw7z                   0/1     ContainerCreating   0          0s
alluxio-master-0                            0/2     Init:0/1            0          0s
alluxio-fuse-6hhx6                          0/1     Pending             0          0s
alluxio-worker-kvmnf                        0/2     ContainerCreating   0          0s
alluxio-fuse-client-qv77h                   0/1     ContainerCreating   0          0s
alluxio-fuse-client-tm7dx                   0/1     ContainerCreating   0          0s
alluxio-fuse-6hhx6                          0/1     Pending             0          0s
alluxio-fuse-jzqtw                          0/1     Pending             0          0s
alluxio-fuse-8phv4                          0/1     Pending             0          0s
alluxio-fuse-jzqtw                          0/1     Pending             0          0s
alluxio-fuse-client-kg5m6                   0/1     ContainerCreating   0          0s
alluxio-fuse-8phv4                          0/1     Pending             0          0s
alluxio-worker-hzppj                        0/2     ContainerCreating   0          0s
alluxio-fuse-6hhx6                          0/1     ContainerCreating   0          0s
alluxio-fuse-8phv4                          0/1     ContainerCreating   0          0s
alluxio-fuse-jzqtw                          0/1     ContainerCreating   0          1s
alluxio-worker-hzppj                        0/2     Running             0          1s
alluxio-worker-4s6kw                        0/2     Running             0          2s
alluxio-fuse-6hhx6                          1/1     Running             0          2s
alluxio-worker-kvmnf                        0/2     Running             0          2s
alluxio-fuse-8phv4                          1/1     Running             0          2s
alluxio-master-0                            0/2     Init:0/1            0          2s
alluxio-fuse-jzqtw                          1/1     Running             0          3s
alluxio-fuse-client-tm7dx                   1/1     Running             0          3s
alluxio-master-0                            0/2     PodInitializing     0          3s
alluxio-fuse-client-2gx7v                   1/1     Running             0          4s
alluxio-master-0                            0/2     Running             0          4s
alluxio-fuse-client-qv77h                   1/1     Running             0          5s
alluxio-master-0                            1/2     Running             0          18s
alluxio-master-0                            2/2     Running             0          21s
alluxio-master-1                            0/2     Pending             0          0s
alluxio-master-1                            0/2     Pending             0          0s
alluxio-master-1                            0/2     Init:0/1            0          0s
alluxio-master-1                            0/2     PodInitializing     0          1s
alluxio-master-1                            0/2     Running             0          2s
alluxio-master-1                            1/2     Running             0          12s
alluxio-master-1                            2/2     Running             0          19s
alluxio-master-2                            0/2     Pending             0          0s
alluxio-master-2                            0/2     Pending             0          0s
alluxio-master-2                            0/2     Init:0/1            0          1s
alluxio-master-2                            0/2     Init:0/1            0          1s
alluxio-master-2                            0/2     PodInitializing     0          2s
alluxio-master-2                            0/2     Running             0          3s
alluxio-master-2                            1/2     Running             0          14s
alluxio-master-2                            2/2     Running             0          18s
alluxio-fuse-client-kg5m6                   1/1     Running             0          103s
alluxio-fuse-client-wmw7z                   1/1     Running             0          108s
alluxio-worker-kvmnf                        0/2     Running             1          2m21s
alluxio-worker-kvmnf                        0/2     Running             2          2m23s
alluxio-worker-hzppj                        0/2     Running             1          2m32s
alluxio-worker-4s6kw                        0/2     Running             1          2m36s
alluxio-worker-hzppj                        0/2     Running             2          2m36s
alluxio-worker-4s6kw                        0/2     Running             2          2m39s
alluxio-worker-kvmnf                        0/2     Running             3          4m51s
alluxio-worker-kvmnf                        0/2     Running             4          4m53s
alluxio-worker-hzppj                        0/2     Running             3          5m1s
alluxio-worker-4s6kw                        0/2     Running             3          5m6s
alluxio-worker-hzppj                        0/2     Running             4          5m6s
alluxio-worker-4s6kw                        0/2     Running             4          5m9s
alluxio-worker-kvmnf                        0/2     Running             5          7m21s
alluxio-worker-kvmnf                        0/2     Running             6          7m23s
alluxio-worker-hzppj                        0/2     Running             5          7m31s
alluxio-worker-4s6kw                        0/2     Running             5          7m36s
alluxio-worker-hzppj                        0/2     Running             6          7m36s
alluxio-worker-4s6kw                        0/2     Running             6          7m39s
&lt;/denchmark-code&gt;

And logs for the command  kubectl -n spark describe pod alluxio-master-0:
&lt;denchmark-code&gt;[root@k8s-master01 ~]# kubectl -n spark describe pod alluxio-master-0
Name:         alluxio-master-0
Namespace:    spark
Priority:     0
Node:         k8s-bignode03/192.168.37.163
Start Time:   Mon, 24 Aug 2020 09:10:18 +0800
Labels:       app=alluxio
              chart=alluxio-0.6.5
              controller-revision-hash=alluxio-master-96c6758c
              heritage=Helm
              name=alluxio-master
              release=alluxio-1598231414
              role=alluxio-master
              statefulset.kubernetes.io/pod-name=alluxio-master-0
Annotations:  &lt;none&gt;
Status:       Running
IP:           10.244.5.110
IPs:
  IP:           10.244.5.110
Controlled By:  StatefulSet/alluxio-master
Init Containers:
  journal-format:
    Container ID:  docker://6e2ae4d6de07e7d763f419d92f6d34c94d54f28940b5d982acd8b29de6d28c34
    Image:         alluxio/alluxio:2.3.0
    Image ID:      docker-pullable://alluxio/alluxio@sha256:caf1f0aaa8529082d7562e8b9e4bcd2d845e376a9261b44cf96799e88f4bea3a
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      alluxio
      formatJournal
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Mon, 24 Aug 2020 09:10:19 +0800
      Finished:     Mon, 24 Aug 2020 09:10:20 +0800
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /journal from alluxio-journal (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-2lqr6 (ro)
Containers:
  alluxio-master:
    Container ID:  docker://157548c56fb8a611e7c2844e53319e5ec9e11c8fcfec304c6494eb5322bb91bd
    Image:         alluxio/alluxio:2.3.0
    Image ID:      docker-pullable://alluxio/alluxio@sha256:caf1f0aaa8529082d7562e8b9e4bcd2d845e376a9261b44cf96799e88f4bea3a
    Ports:         19998/TCP, 19999/TCP, 19200/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Command:
      /entrypoint.sh
    Args:
      master-only
      --no-format
    State:          Running
      Started:      Mon, 24 Aug 2020 09:10:21 +0800
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     4
      memory:  8G
    Requests:
      cpu:      1
      memory:   1G
    Liveness:   exec [alluxio-monitor.sh master] delay=15s timeout=5s period=30s #success=1 #failure=2
    Readiness:  exec [alluxio-monitor.sh master] delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment Variables from:
      alluxio-config  ConfigMap  Optional: false
    Environment:
      ALLUXIO_MASTER_HOSTNAME:  alluxio-master-0 (v1:metadata.name)
    Mounts:
      /journal from alluxio-journal (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-2lqr6 (ro)
  alluxio-job-master:
    Container ID:  docker://f575b264dd75f5ec8d7a24630dbdcdc69667655f7140d4959334b68e742572f8
    Image:         alluxio/alluxio:2.3.0
    Image ID:      docker-pullable://alluxio/alluxio@sha256:caf1f0aaa8529082d7562e8b9e4bcd2d845e376a9261b44cf96799e88f4bea3a
    Ports:         20001/TCP, 20002/TCP, 20003/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Command:
      /entrypoint.sh
    Args:
      job-master
    State:          Running
      Started:      Mon, 24 Aug 2020 09:10:21 +0800
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     4
      memory:  8G
    Requests:
      cpu:      1
      memory:   1G
    Liveness:   exec [alluxio-monitor.sh job_master] delay=15s timeout=5s period=30s #success=1 #failure=2
    Readiness:  exec [alluxio-monitor.sh job_master] delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment Variables from:
      alluxio-config  ConfigMap  Optional: false
    Environment:
      ALLUXIO_MASTER_HOSTNAME:  alluxio-master-0 (v1:metadata.name)
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-2lqr6 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  alluxio-journal:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  1Gi
  default-token-2lqr6:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-2lqr6
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  alluxio=true
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age        From                    Message
  ----    ------     ----       ----                    -------
  Normal  Scheduled  &lt;unknown&gt;  default-scheduler       Successfully assigned spark/alluxio-master-0 to k8s-bignode03
  Normal  Pulled     11m        kubelet, k8s-bignode03  Container image "alluxio/alluxio:2.3.0" already present on machine
  Normal  Created    11m        kubelet, k8s-bignode03  Created container journal-format
  Normal  Started    11m        kubelet, k8s-bignode03  Started container journal-format
  Normal  Pulled     11m        kubelet, k8s-bignode03  Container image "alluxio/alluxio:2.3.0" already present on machine
  Normal  Created    11m        kubelet, k8s-bignode03  Created container alluxio-master
  Normal  Started    11m        kubelet, k8s-bignode03  Started container alluxio-master
  Normal  Pulled     11m        kubelet, k8s-bignode03  Container image "alluxio/alluxio:2.3.0" already present on machine
  Normal  Created    11m        kubelet, k8s-bignode03  Created container alluxio-job-master
  Normal  Started    11m        kubelet, k8s-bignode03  Started container alluxio-job-master

&lt;/denchmark-code&gt;

The master pods seem to be started,but logs for the master are the same.Can you reproduce the issue in your env?
		</comment>
		<comment id='4' author='marrycheck' date='2020-08-25T09:56:26Z'>
		
Do you use the Alluxio helm repo or extract the helm package yourself from the 2.3.0 tarball?
Alluxio master pods are deployed using StatefulSet, meaning it is created one by one. Once alluxio-master-0 is Running and is Ready 2/2, alluxio-master-1 is started. You see alluxio-master-2 is not resolvable because the pod is not started yet. Normally alluxio-master-0 will start first and pass the readiness probe, although it will not find the peers for the quorum, it's gonna retry for a while and not crash. In this period alluxio-master-1 and 2 will start the same way and they will finally create the quorum.
I think the error you see in the master log is just the symptom, not the root cause. Can you please do a kubectl get pods --watch after you do helm install and add what you see here? I'd like to see if the master pods ever pass the readiness check and alluxio-master-1 gets created. You should also do kubectl describe pod alluxio-master-&lt;index&gt; do look for anormalies.

I have added some info here.Please take a look at your convenience.
		</comment>
		<comment id='5' author='marrycheck' date='2020-08-26T06:05:59Z'>
		I use the command helm install alluxio -f ./config.yaml alluxio-charts/alluxio -n spark  to install alluxio,errors are the same.I find that if i set hostNetwork to true in worker props,alluxio can not work,but when i set it to false ,allxuio can deploy success.
And in both cases, alluxio does not work properly when i test with spark which read data from alluxio.
		</comment>
		<comment id='6' author='marrycheck' date='2020-08-31T15:19:39Z'>
		Thanks for the additional information!
A quick test to run is to try 2.4.0-SNAPSHOT solves your problem, there has been a few improvements related to fuse.
From your information, it seems your workers are never READY for serving. I would check if you can ping or curl to the master service names (alluxio-master-0) from the master/worker pods. If that does not work, check what is wrong with the services by kubectl describe svc alluxio-master-x.
I will try to reproduce your case and let you know too.
		</comment>
		<comment id='7' author='marrycheck' date='2020-09-01T10:01:28Z'>
		Thanks for your replay!
When i set hostNetwork to true in worker props ,i can ping to the master service names (alluxio-master-0) from the master pods,but can not ping to alluxio-master-0 from worker pods . And i use the command  kubectl -n spark describe svc alluxio-master-o to get infos :
&lt;denchmark-code&gt;[root@k8s-master01 ~]# kubectl -n spark describe svc  alluxio-master-0
Name:              alluxio-master-0
Namespace:         spark
Labels:            app=alluxio
                   chart=alluxio-0.6.11
                   heritage=Helm
                   release=alluxio-1598952492
                   role=alluxio-master
Annotations:       &lt;none&gt;
Selector:          app=alluxio,release=alluxio-1598952492,role=alluxio-master,statefulset.kubernetes.io/pod-name=alluxio-master-0
Type:              ClusterIP
IP:                None
Port:              rpc  19998/TCP
TargetPort:        19998/TCP
Endpoints:         10.244.5.184:19998
Port:              web  19999/TCP
TargetPort:        19999/TCP
Endpoints:         10.244.5.184:19999
Port:              job-rpc  20001/TCP
TargetPort:        20001/TCP
Endpoints:         10.244.5.184:20001
Port:              job-web  20002/TCP
TargetPort:        20002/TCP
Endpoints:         10.244.5.184:20002
Port:              embedded  19200/TCP
TargetPort:        19200/TCP
Endpoints:         10.244.5.184:19200
Port:              job-embedded  20003/TCP
TargetPort:        20003/TCP
Endpoints:         10.244.5.184:20003
Session Affinity:  None
Events:            &lt;none&gt;

&lt;/denchmark-code&gt;

When i change the version of alluxio to 2.4.0-SNAPSHOT,errors are same.Any suggestions would be appreciate!
		</comment>
		<comment id='8' author='marrycheck' date='2020-09-07T03:24:22Z'>
		&lt;denchmark-link:https://github.com/marrycheck&gt;@marrycheck&lt;/denchmark-link&gt;
 I tried both &lt;denchmark-link:2.3.0&gt;https://alluxio-charts.storage.googleapis.com/openSource/2.3.0&lt;/denchmark-link&gt;
 and &lt;denchmark-link:2.4.0-SNAPSHOT&gt;https://alluxio-charts.storage.googleapis.com/openSource/2.4.0-SNAPSHOT&lt;/denchmark-link&gt;
 but did not reproduce your issue. I'm installing with your config (almost the same, changed a few properties like aws key). I'm running with master not using  and workers using . The masters and workers can find each other without issue and  work.
&lt;denchmark-code&gt;NAME                   READY   STATUS    RESTARTS   AGE     IP              NODE            NOMINATED NODE   READINESS GATES
alluxio-fuse-29crc     1/1     Running   0          4m21s   172.x.x.x   172.x.x.x   &lt;none&gt;           &lt;none&gt;
alluxio-fuse-mp8r8     1/1     Running   0          4m21s   172.x.x.x   172.x.x.x   &lt;none&gt;           &lt;none&gt;
alluxio-master-0       2/2     Running   0          4m21s   10.38.0.1       172.x.x.x   &lt;none&gt;           &lt;none&gt;
alluxio-master-1       2/2     Running   0          4m3s    10.40.0.1       172.x.x.x   &lt;none&gt;           &lt;none&gt;
alluxio-master-2       2/2     Running   0          3m44s   10.38.0.2       172.x.x.x   &lt;none&gt;           &lt;none&gt;
alluxio-worker-cm9wt   2/2     Running   0          4m21s   172.x.x.x   172.x.x.x   &lt;none&gt;           &lt;none&gt;
alluxio-worker-jdnl7   2/2     Running   0          4m21s   172.x.x.x   172.x.x.x   &lt;none&gt;           &lt;none&gt;
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='marrycheck' date='2020-09-07T03:35:14Z'>
		Unfortunately the checks on the easy perspectives do not solve your issue. Based on the information i cannot think of a root cause for you issue, but I would suggest maybe checking the following. They should at least give us a bit direction on what to look next:

What if your k8s network setup? For example, iptables or ipvs? Do you have any specific setting in your environment like PodSecurityConext?
What CNI implementation do you use? For example, in our test environment we are using weave. Are there logs in the CNI provider pods that you can check?

Some other tests that I can think of:

Create pods that are not relevant to alluxio, for example nginx in your environment. Try if you can make it ping alluxio-master-x services successfully with and without host network. And can the ngnix pods ping IPs of alluxio worker/master pods? Note that if your masters are using HA, only the primary master will respond to your curl requests.
What other services are using host network in your environment, and how are they set up?

		</comment>
		<comment id='10' author='marrycheck' date='2020-09-16T08:03:32Z'>
		Thanks for your suggestions.
i use ipvs and flannel, neither have no any specific  settings for  PodSecurityConext.
I test with nginx by using hostNetwork,and can not ping alluio-master-x.Then i change to another k8s env,alluxio seems to be ok.Em,,,i do not know why.Thanks again!
		</comment>
		<comment id='11' author='marrycheck' date='2020-09-28T02:48:16Z'>
		Shutdown flannel checksum  by using ethtool -K flannel.1 tx-checksum-ip-generic off seem to solve this issue.
		</comment>
		<comment id='12' author='marrycheck' date='2020-11-24T09:46:13Z'>
		same issue with you.
		</comment>
	</comments>
</bug>