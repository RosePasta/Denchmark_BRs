<bug id='10996' author='jiacheliu3' open_date='2020-02-24T03:50:21Z' closed_time='2020-03-02T10:15:56Z'>
	<summary>K8s Alluxio master restart loses journal</summary>
	<description>
Alluxio Version:
2.2.0-SNAPSHOT
Describe the bug
Restarting the master StatefulSet results in a smaller journal file size. MUST_CACHE files are lost in the restart.
This error doesn't happen every time the master is restarted. Only sometimes the files are lost. The journal is in a PV so the node of master pod should be irrelevant.
MUST_CACHE and ASYNCH_THROUGH files are lost.
&lt;denchmark-code&gt;bash-4.4$ alluxio fs ls -R /
drwx------  jiacheng       jiacheng                    12       PERSISTED 02-21-2020 16:05:59:725  DIR /default_tests_files
-rwx------  jiacheng       jiacheng                    80       PERSISTED 02-21-2020 16:05:22:000   0% /default_tests_files/BASIC_CACHE_CACHE_THROUGH
-rwx------  jiacheng       jiacheng                    80       PERSISTED 02-21-2020 16:05:19:000   0% /default_tests_files/BASIC_CACHE_PROMOTE_CACHE_THROUGH
-rwx------  jiacheng       jiacheng                    80       PERSISTED 02-21-2020 16:05:20:000   0% /default_tests_files/BASIC_CACHE_PROMOTE_THROUGH
-rwx------  jiacheng       jiacheng                    80       PERSISTED 02-21-2020 16:05:22:000   0% /default_tests_files/BASIC_CACHE_THROUGH
-rwx------  jiacheng       jiacheng                    84       PERSISTED 02-21-2020 16:05:22:000   0% /default_tests_files/BASIC_NON_BYTE_BUFFER_CACHE_CACHE_THROUGH
-rwx------  jiacheng       jiacheng                    84       PERSISTED 02-21-2020 16:05:19:000   0% /default_tests_files/BASIC_NON_BYTE_BUFFER_CACHE_PROMOTE_CACHE_THROUGH
-rwx------  jiacheng       jiacheng                    84       PERSISTED 02-21-2020 16:05:21:000   0% /default_tests_files/BASIC_NON_BYTE_BUFFER_CACHE_PROMOTE_THROUGH
-rwx------  jiacheng       jiacheng                    84       PERSISTED 02-21-2020 16:05:22:000   0% /default_tests_files/BASIC_NON_BYTE_BUFFER_CACHE_THROUGH
-rwx------  jiacheng       jiacheng                    84       PERSISTED 02-21-2020 16:05:23:000   0% /default_tests_files/BASIC_NON_BYTE_BUFFER_NO_CACHE_CACHE_THROUGH
-rwx------  jiacheng       jiacheng                    84       PERSISTED 02-21-2020 16:05:23:000   0% /default_tests_files/BASIC_NON_BYTE_BUFFER_NO_CACHE_THROUGH
-rwx------  jiacheng       jiacheng                    80       PERSISTED 02-21-2020 16:05:23:000   0% /default_tests_files/BASIC_NO_CACHE_CACHE_THROUGH
-rwx------  jiacheng       jiacheng                    80       PERSISTED 02-21-2020 16:05:23:000   0% /default_tests_files/BASIC_NO_CACHE_THROUGH
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;# Before restart
/tmp/kubernetes/logs/journals/initial_test/alluxio-master-0/BlockMaster/v1/logs:
total 12
drwxr-xr-x. 2 ec2-user ec2-user 4096 Feb 21 16:05 .
drwxr-xr-x. 3 ec2-user ec2-user 4096 Feb 21 16:05 ..
-rw-rw-r--. 1 ec2-user ec2-user  305 Feb 21 16:05 0x0-0x7fffffffffffffff
/tmp/kubernetes/logs/journals/initial_test/alluxio-master-0/FileSystemMaster/v1/logs:
total 20
drwxr-xr-x. 2 ec2-user ec2-user  4096 Feb 21 16:05 .
drwxr-xr-x. 3 ec2-user ec2-user  4096 Feb 21 16:05 ..
-rw-rw-r--. 1 ec2-user ec2-user 12214 Feb 21 16:05 0x0-0x7fffffffffffffff
/tmp/kubernetes/logs/journals/initial_test/alluxio-master-0/MetaMaster/v1/logs:
total 12
drwxr-xr-x. 2 ec2-user ec2-user 4096 Feb 21 16:05 .
drwxr-xr-x. 3 ec2-user ec2-user 4096 Feb 21 16:05 ..
-rw-rw-r--. 1 ec2-user ec2-user   44 Feb 21 16:05 0x0-0x7fffffffffffffff
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;# After restart
/tmp/kubernetes/logs/journals/verify_must_cache/alluxio-master-0/BlockMaster/v1/logs:
total 12
drwxr-xr-x. 2 ec2-user ec2-user 4096 Feb 21 16:11 .
drwxr-xr-x. 3 ec2-user ec2-user 4096 Feb 21 16:11 ..
-rw-rw-r--. 1 ec2-user ec2-user  152 Feb 21 16:11 0x0-0x7fffffffffffffff
/tmp/kubernetes/logs/journals/verify_must_cache/alluxio-master-0/FileSystemMaster/v1/logs:
total 16
drwxr-xr-x. 2 ec2-user ec2-user 4096 Feb 21 16:11 .
drwxr-xr-x. 3 ec2-user ec2-user 4096 Feb 21 16:11 ..
-rw-rw-r--. 1 ec2-user ec2-user 6014 Feb 21 16:11 0x0-0x7fffffffffffffff
/tmp/kubernetes/logs/journals/verify_must_cache/alluxio-master-0/MetaMaster/v1/logs:
total 12
drwxr-xr-x. 2 ec2-user ec2-user 4096 Feb 21 16:11 .
drwxr-xr-x. 3 ec2-user ec2-user 4096 Feb 21 16:11 ..
-rw-rw-r--. 1 ec2-user ec2-user   44 Feb 21 16:11 0x0-0x7fffffffffffffff
&lt;/denchmark-code&gt;

To Reproduce
Based on a single-master Alluxio cluster defined in alluxio/integration/kubernetes/singleMaster-localJournal.
&lt;denchmark-code&gt;# Restart the single master
$ kubectl delete -f master/alluxio-master-service.yaml.template
$ kubectl delete -f master/alluxio-master-statefulset.yaml.template
$ kubectl create -f master/alluxio-master-statefulset.yaml.template
$ kubectl create -f master/alluxio-master-service.yaml.template
&lt;/denchmark-code&gt;

Expected behavior
The MUST_CACHE files should be consistently persistent, always recovered from journal after restarts.
Urgency
MEDIUM
Additional context
Using UFS journal provided from PV mounted on /journal.
&lt;denchmark-code&gt;# Irrelevant JAVA_OPTS omitted
ALLUXIO_WORKER_JAVA_OPTS:
----
-Dalluxio.worker.bind.host=0.0.0.0  -Dalluxio.worker.data.server.domain.socket.address=/opt/domain -Dalluxio.worker.data.server.domain.socket.as.uuid=true -Dalluxio.worker.hostname=${ALLUXIO_WORKER_HOSTNAME} -Dalluxio.worker.tieredstore.levels=1 -Dalluxio.worker.tieredstore.level0.dirs.mediumtype=MEM -Dalluxio.worker.tieredstore.level0.dirs.path=/dev/shm

ALLUXIO_WORKER_TIEREDSTORE_LEVEL0_DIRS_PATH:
----
/dev/shm 

ALLUXIO_JAVA_OPTS:
----
-Dalluxio.worker.tieredstore.level0.alias=MEM -Dalluxio.master.hostname=alluxio-master-0 -Dalluxio.master.journal.folder=/journal -Dalluxio.master.mount.table.root.ufs=s3a://xxx/autobots-test -Dalluxio.worker.tieredstore.level0.dirs.path=/dev/shm -Dalluxio.master.journal.type=UFS 

ALLUXIO_MASTER_JAVA_OPTS:
----
-Dalluxio.master.hostname=${ALLUXIO_MASTER_HOSTNAME} -Dalluxio.master.web.bind.host=0.0.0.0 
&lt;/denchmark-code&gt;

Some more context regarding the timestamp in logs:
&lt;denchmark-code&gt;# Time of master shutdown
[2020-02-21 16:05:24.729613222 +0000 UTC m=+12.509683421] Pod status: NAME                   READY   STATUS        RESTARTS   AGE
alluxio-master-0       2/2     Terminating   0          79s
alluxio-worker-4547k   2/2     Running       0          75s
alluxio-worker-scpkz   2/2     Running       0          75s
# The master restart is complete
[2020-02-21 16:05:54.969465746 +0000 UTC m=+42.749536005] Pod status: NAME                   READY   STATUS    RESTARTS   AGE
alluxio-master-0       2/2     Running   0          27s
alluxio-worker-4547k   2/2     Running   0          105s
alluxio-worker-scpkz   2/2     Running   0          105s
&lt;/denchmark-code&gt;

Seems the master deemed some blocks are invalid and should be removed
&lt;denchmark-code&gt;2020-02-21 16:05:50,753 INFO  AlluxioMasterProcess - Started Alluxio master gRPC server on address alluxio-master-0:19998
2020-02-21 16:05:57,983 WARN  DefaultBlockMaster - Could not find worker id: 2817240564275639650 for heartbeat.
2020-02-21 16:05:57,983 WARN  DefaultBlockMaster - Could not find worker id: 8075367430880972319 for heartbeat.
2020-02-21 16:05:58,173 INFO  DefaultBlockMaster - getWorkerId(): WorkerNetAddress: WorkerNetAddress{host=172.31.83.20, rpcPort=29999, dataPort=29999, webPort=30000, domainSocketPath=/opt/domain/53b23890-b4dd-4682-9980-488dabf0a9a0, tieredIdentity=TieredIdentity(node=172.31.83.20, rack=null)} id: 3241698298930175994
2020-02-21 16:05:58,173 INFO  DefaultBlockMaster - getWorkerId(): WorkerNetAddress: WorkerNetAddress{host=172.31.84.226, rpcPort=29999, dataPort=29999, webPort=30000, domainSocketPath=/opt/domain/4f5fc3df-2a3a-413f-8a87-c89e34e6602d, tieredIdentity=TieredIdentity(node=172.31.84.226, rack=null)} id: 7521277311275201049
2020-02-21 16:05:58,424 WARN  DefaultBlockMaster - Invalid block: 83886080 from worker 172.31.83.20.
2020-02-21 16:05:58,424 WARN  DefaultBlockMaster - Invalid block: 16777216 from worker 172.31.84.226.
2020-02-21 16:05:58,467 WARN  DefaultBlockMaster - Invalid block: 33554432 from worker 172.31.83.20.
2020-02-21 16:05:58,467 WARN  DefaultBlockMaster - Invalid block: 100663296 from worker 172.31.84.226.
2020-02-21 16:05:58,467 WARN  DefaultBlockMaster - Invalid block: 50331648 from worker 172.31.83.20.
2020-02-21 16:05:58,467 WARN  DefaultBlockMaster - Invalid block: 234881024 from worker 172.31.84.226.
2020-02-21 16:05:58,467 WARN  DefaultBlockMaster - Invalid block: 67108864 from worker 172.31.83.20.
2020-02-21 16:05:58,467 WARN  DefaultBlockMaster - Invalid block: 251658240 from worker 172.31.84.226.
2020-02-21 16:05:58,467 WARN  DefaultBlockMaster - Invalid block: 117440512 from worker 172.31.83.20.
2020-02-21 16:05:58,467 WARN  DefaultBlockMaster - Invalid block: 301989888 from worker 172.31.84.226.
2020-02-21 16:05:58,468 WARN  DefaultBlockMaster - Invalid block: 134217728 from worker 172.31.83.20.
2020-02-21 16:05:58,468 WARN  DefaultBlockMaster - Invalid block: 150994944 from worker 172.31.83.20.
2020-02-21 16:05:58,468 INFO  DefaultBlockMaster - Requesting delete for orphaned block: 16777216 from worker 172.31.84.226.
2020-02-21 16:05:58,468 WARN  DefaultBlockMaster - Invalid block: 167772160 from worker 172.31.83.20.
2020-02-21 16:05:58,468 WARN  DefaultBlockMaster - Invalid block: 184549376 from worker 172.31.83.20.
2020-02-21 16:05:58,468 INFO  DefaultBlockMaster - Requesting delete for orphaned block: 100663296 from worker 172.31.84.226.
2020-02-21 16:05:58,468 WARN  DefaultBlockMaster - Invalid block: 201326592 from worker 172.31.83.20.
2020-02-21 16:05:58,468 INFO  DefaultBlockMaster - Requesting delete for orphaned block: 234881024 from worker 172.31.84.226.
2020-02-21 16:05:58,468 WARN  DefaultBlockMaster - Invalid block: 218103808 from worker 172.31.83.20.
2020-02-21 16:05:58,468 INFO  DefaultBlockMaster - Requesting delete for orphaned block: 251658240 from worker 172.31.84.226.
2020-02-21 16:05:58,468 WARN  DefaultBlockMaster - Invalid block: 268435456 from worker 172.31.83.20.
2020-02-21 16:05:58,468 INFO  DefaultBlockMaster - Requesting delete for orphaned block: 301989888 from worker 172.31.84.226.
2020-02-21 16:05:58,468 WARN  DefaultBlockMaster - Invalid block: 285212672 from worker 172.31.83.20.
2020-02-21 16:05:58,469 WARN  DefaultBlockMaster - Invalid block: 318767104 from worker 172.31.83.20.
2020-02-21 16:05:58,469 WARN  DefaultBlockMaster - Invalid block: 335544320 from worker 172.31.83.20.
2020-02-21 16:05:58,469 WARN  DefaultBlockMaster - Invalid block: 385875968 from worker 172.31.83.20.
2020-02-21 16:05:58,469 WARN  DefaultBlockMaster - Invalid block: 402653184 from worker 172.31.83.20.
2020-02-21 16:05:58,469 INFO  DefaultBlockMaster - Requesting delete for orphaned block: 83886080 from worker 172.31.83.20.
2020-02-21 16:05:58,469 INFO  DefaultBlockMaster - Requesting delete for orphaned block: 33554432 from worker 172.31.83.20.
2020-02-21 16:05:58,469 INFO  DefaultBlockMaster - Requesting delete for orphaned block: 50331648 from worker 172.31.83.20.
2020-02-21 16:05:58,469 INFO  DefaultBlockMaster - Requesting delete for orphaned block: 67108864 from worker 172.31.83.20.
2020-02-21 16:05:58,469 INFO  DefaultBlockMaster - Requesting delete for orphaned block: 117440512 from worker 172.31.83.20.
2020-02-21 16:05:58,469 INFO  DefaultBlockMaster - Requesting delete for orphaned block: 134217728 from worker 172.31.83.20.
2020-02-21 16:05:58,469 INFO  DefaultBlockMaster - Requesting delete for orphaned block: 150994944 from worker 172.31.83.20.
2020-02-21 16:05:58,470 INFO  DefaultBlockMaster - Requesting delete for orphaned block: 167772160 from worker 172.31.83.20.
2020-02-21 16:05:58,470 INFO  DefaultBlockMaster - Requesting delete for orphaned block: 184549376 from worker 172.31.83.20.
2020-02-21 16:05:58,470 INFO  DefaultBlockMaster - Requesting delete for orphaned block: 201326592 from worker 172.31.83.20.
2020-02-21 16:05:58,470 INFO  DefaultBlockMaster - Requesting delete for orphaned block: 218103808 from worker 172.31.83.20.
2020-02-21 16:05:58,470 INFO  DefaultBlockMaster - Requesting delete for orphaned block: 268435456 from worker 172.31.83.20.
2020-02-21 16:05:58,470 INFO  DefaultBlockMaster - Requesting delete for orphaned block: 285212672 from worker 172.31.83.20.
2020-02-21 16:05:58,470 INFO  DefaultBlockMaster - Requesting delete for orphaned block: 318767104 from worker 172.31.83.20.
2020-02-21 16:05:58,470 INFO  DefaultBlockMaster - Requesting delete for orphaned block: 335544320 from worker 172.31.83.20.
2020-02-21 16:05:58,470 INFO  DefaultBlockMaster - Requesting delete for orphaned block: 385875968 from worker 172.31.83.20.
2020-02-21 16:05:58,470 INFO  DefaultBlockMaster - Requesting delete for orphaned block: 402653184 from worker 172.31.83.20.
2020-02-21 16:05:58,471 INFO  DefaultBlockMaster - registerWorker(): MasterWorkerInfo{id=7521277311275201049, workerAddress=WorkerNetAddress{host=172.31.84.226, rpcPort=29999, dataPort=29999, webPort=30000, domainSocketPath=/opt/domain/4f5fc3df-2a3a-413f-8a87-c89e34e6602d, tieredIdentity=TieredIdentity(node=172.31.84.226, rack=null)}, capacityBytes=10737418240, usedBytes=412, lastUpdatedTimeMs=1582301158470, blocks=[16777216, 100663296, 234881024, 251658240, 301989888], lostStorage={}}
2020-02-21 16:05:58,471 INFO  DefaultBlockMaster - registerWorker(): MasterWorkerInfo{id=3241698298930175994, workerAddress=WorkerNetAddress{host=172.31.83.20, rpcPort=29999, dataPort=29999, webPort=30000, domainSocketPath=/opt/domain/53b23890-b4dd-4682-9980-488dabf0a9a0, tieredIdentity=TieredIdentity(node=172.31.83.20, rack=null)}, capacityBytes=10737418240, usedBytes=1392, lastUpdatedTimeMs=1582301158471, blocks=[83886080, 33554432, 50331648, 67108864, 117440512, 134217728, 150994944, 167772160, 184549376, 201326592, 218103808, 268435456, 285212672, 318767104, 335544320, 385875968, 402653184], lostStorage={}}
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/Alluxio/alluxio/files/4243044/logs.zip&gt;logs.zip&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='jiacheliu3' date='2020-03-02T10:15:46Z'>
		Confirmed this is because I was using a hostPath volume for the journal. When the master is restarted it schedules to a different path and the hostPath volume gets re-created.
		</comment>
		<comment id='2' author='jiacheliu3' date='2020-03-02T19:14:41Z'>
		Thanks &lt;denchmark-link:https://github.com/jiacheliu3&gt;@jiacheliu3&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>