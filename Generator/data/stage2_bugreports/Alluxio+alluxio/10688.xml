<bug id='10688' author='cheyang' open_date='2019-12-31T09:19:12Z' closed_time='2020-05-01T23:03:43Z'>
	<summary>Performance Overhead using read type as CACHE_PROMOTE v.s NO_CACHE</summary>
	<description>
Alluxio Version:
2.1.1
Describe the bug
Deploy alluxio in kubernetes with CACHE_PROMOTE and NO_CACHE, and I found the NO_CAHCE performance is much better than CACHE_PROMOTE. The file is about 130MB, and 1154 files in all.
To Reproduce

Deploy Alluxio with helm chart, the values file uses NO_CACHE

&lt;denchmark-code&gt;image: alluxio/alluxio
imageTag: "2.1.1"
imagePullPolicy: Always

properties:
    alluxio.user.metrics.collection.enabled: false
    alluxio.user.file.passive.cache.enabled: false
    alluxio.user.ufs.block.read.location.policy: alluxio.client.block.policy.LocalFirstAvoidEvictionPolicy
    # It should be great than (120)*0.01 = 2GB
    alluxio.user.block.avoid.eviction.policy.reserved.size.bytes: 2GB
    alluxio.fuse.debug.enabled: "false"
    alluxio.user.file.writetype.default: MUST_CACHE
    alluxio.user.block.write.location.policy.class: alluxio.client.block.policy.LocalFirstAvoidEvictionPolicy
    alluxio.worker.allocator.class: alluxio.worker.block.allocator.GreedyAllocator
    alluxio.user.block.size.bytes.default: 32MB
    fs.oss.endpoint: oss-cn-shanghai-internal.aliyuncs.com
    # set OSS endpoint
    fs.oss.accessKeyId: xxx
    fs.oss.accessKeySecret: yyy
    alluxio.user.file.readtype.default: NO_CACHE
    alluxio.master.mount.table.root.ufs: oss://imagenet/
    #alluxio.user.file.master.client.threads: "128"
    #alluxio.user.block.master.client.threads: "128"
    # alluxio.underfs.oss.socket.timeout: 3000sec
    alluxio.user.network.reader.chunk.size.bytes: 32MB
    alluxio.worker.file.buffer.size: 320MB
    alluxio.security.stale.channel.purge.interval: 365d
    alluxio.user.metadata.cache.enabled: "false"
    alluxio.user.metadata.cache.max.size: "1000000"
    alluxio.user.direct.memory.io.enabled: "true"
    alluxio.fuse.cached.paths.max: "1000000"
    alluxio.job.worker.threadpool.size: "64"
    alluxio.worker.block.master.client.pool.size: 128


tieredstore:
  levels:
  - alias: MEM
    level: 0
    path: /dev/shm
    type: hostPath
    # quota: 120GB
    high: 0.99
    low: 0.7

fuse:
  image: alluxio/alluxio-fuse
  imageTag: "2.1.1"
  imagePullPolicy: Always
  args:
    - fuse
    - --fuse-opts=direct_io
&lt;/denchmark-code&gt;


Run the training jobs without any cache
Performance result is :

&lt;denchmark-code&gt;10000	images/sec: 897.3 +/- 2.1 (jitter = 37.6)	4.466
----------------------------------------------------------------
total images/sec: 7171.94
----------------------------------------------------------------
&lt;/denchmark-code&gt;


Deploy the alluxio without setting alluxio.user.file.readtype.default

&lt;denchmark-code&gt;image: alluxio/alluxio
imageTag: "2.1.1"
imagePullPolicy: Always

properties:
    alluxio.user.metrics.collection.enabled: false
    alluxio.user.file.passive.cache.enabled: false
    alluxio.user.ufs.block.read.location.policy: alluxio.client.block.policy.LocalFirstAvoidEvictionPolicy
    # It should be great than (120)*0.01 = 2GB
    alluxio.user.block.avoid.eviction.policy.reserved.size.bytes: 2GB
    alluxio.fuse.debug.enabled: "false"
    alluxio.user.file.writetype.default: MUST_CACHE
    alluxio.user.block.write.location.policy.class: alluxio.client.block.policy.LocalFirstAvoidEvictionPolicy
    alluxio.worker.allocator.class: alluxio.worker.block.allocator.GreedyAllocator
    alluxio.user.block.size.bytes.default: 32MB
    fs.oss.endpoint: oss-cn-shanghai-internal.aliyuncs.com
    # set OSS endpoint
    fs.oss.accessKeyId: xxx
    fs.oss.accessKeySecret: yyy
    alluxio.master.mount.table.root.ufs: oss://imagenet/
    #alluxio.user.file.master.client.threads: "128"
    #alluxio.user.block.master.client.threads: "128"
    # alluxio.underfs.oss.socket.timeout: 3000sec
    alluxio.user.network.reader.chunk.size.bytes: 32MB
    alluxio.worker.file.buffer.size: 320MB
    alluxio.security.stale.channel.purge.interval: 365d
    alluxio.user.metadata.cache.enabled: "false"
    alluxio.user.metadata.cache.max.size: "1000000"
    alluxio.user.direct.memory.io.enabled: "true"
    alluxio.fuse.cached.paths.max: "1000000"
    alluxio.job.worker.threadpool.size: "64"
    alluxio.worker.block.master.client.pool.size: 128


tieredstore:
  levels:
  - alias: MEM
    level: 0
    path: /dev/shm
    type: hostPath
    # quota: 120GB
    high: 0.99
    low: 0.7

fuse:
  image: alluxio/alluxio-fuse
  imageTag: "2.1.1"
  imagePullPolicy: Always
  args:
    - fuse
    - --fuse-opts=direct_io
&lt;/denchmark-code&gt;


Load all the data into allxuio
Run the training job
Get the performance result:

&lt;denchmark-code&gt;----------------------------------------------------------------
1000	images/sec: 620.6 +/- 7.8 (jitter = 264.4)	4.721
----------------------------------------------------------------
total images/sec: 4963.33
----------------------------------------------------------------
&lt;/denchmark-code&gt;

Expected behavior
A clear and concise description of what you expected to happen.
Urgency
Describe the impact and urgency of the bug.
Additional context
Add any other context about the problem here.
	</description>
	<comments>
		<comment id='1' author='cheyang' date='2020-01-03T03:32:13Z'>
		&lt;denchmark-link:https://github.com/ggezer&gt;@ggezer&lt;/denchmark-link&gt;
 as we talked offline, do you see this issue is consistent with what you found too or the problem you mentioned is different?
		</comment>
		<comment id='2' author='cheyang' date='2020-01-06T19:17:25Z'>
		&lt;denchmark-link:https://github.com/apc999&gt;@apc999&lt;/denchmark-link&gt;
, It does not seem so looking at cluster configuration.
&lt;denchmark-link:https://github.com/cheyang&gt;@cheyang&lt;/denchmark-link&gt;
, Do you see anything interesting in worker logs during CACHE_PROMOTE scenario?
		</comment>
		<comment id='3' author='cheyang' date='2020-01-07T06:00:21Z'>
		The additional information is updated here:
Alluxio status:

1 master, 1 worker, 1 alluxio-fuse, all launched in the same machine through helm kubernetes
only 1 tier of alluxio worker memory large enough to hold the whole dataset

machine status:

Cpus: 82
Memory：336G
GPU：8 * NVIDIA V100
Network: 20GiB/s

dataset:

a dir holding a large number of photos (more than 1000)
about 144GB

training job:

deep learning training job cnn
will launch multiple processors (e.g. 8 processor for 8 GPU in this test)
first list dir then read photos

The tests:
&lt;denchmark-code&gt;| readtype | data cached| training speed |
| NO_CACHE | False | about 990 images/s |
| NO_CACHE | True | about 1040 images/s |
| CACHE | True | about 1040 images/s |
| CACHE_PROMOTE | True |  about 630 images/s | 
&lt;/denchmark-code&gt;

Observations:

This is a powerful machine, it has strong computation ability (Cpus: 82,GPU：8 * NVIDIA V100 )
The network interaction with alluxio UFS is fast (The network receive data amount is about 1500MB/s during training)
CACHE and NO_CACHE have similar performance when data is cached in Alluxio, but CACHE_PROMOTE adds additional overhead even when the Alluxio worker has only 1 tier
NO_CACHE nearly used all the GPU, while CACHE_PROMOTE didn't fully utilize the GPU （used about 70%). NO_CACHE nearly used about 5 to 6 cores while CACHE_PROMOTE can only use about 3 cores

According to &lt;denchmark-link:https://github.com/cheyang&gt;@cheyang&lt;/denchmark-link&gt;
, the CACHE_PROMOTE performance is worse than NO_CACHE only occurs when the machine has strong computation ability (strong GPU). When the machine is not strong enough, this issue doesn't occur.
We guess maybe CACHE_PROMOTE has some concurrency issues or some overheads that make the training speed slower.
		</comment>
		<comment id='4' author='cheyang' date='2020-01-07T23:09:43Z'>
		Would you mind pasting the full worker property configuration instead of summarizing it.
		</comment>
		<comment id='5' author='cheyang' date='2020-01-07T23:15:14Z'>
		Also if you haven't already spotted anything interesting in worker logs, is it possible running the cache-promote scenario with debug traces for alluxio.worker.block.TieredBlockStore class?
		</comment>
	</comments>
</bug>