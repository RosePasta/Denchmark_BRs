<bug id='9958' author='liuhongtong' open_date='2019-09-24T12:53:58Z' closed_time='2020-01-29T21:59:40Z'>
	<summary>distributedLoad failed</summary>
	<description>
Alluxio Version:
master
Describe the bug
Command:
hadp$ alluxio fs distributedLoad /ns8 -thread 1024
Error log:
alluxio-2.0.1/logs/user/user_hadp.log
&lt;denchmark-code&gt;2019-09-24 20:15:54,986 WARN  JobGrpcClientUtils - Job 1569322498851 failed to complete: Task execution failed: Failed to request 8388608 bytes after 10,000ms to create blockId 9207587799040 (LocalFileDataWriter{request=block_id: 9207587799040
tier: 0
space_to_reserve: 8388608
medium_type: ""
pin_on_create: false
, address=WorkerNetAddress{host=x.x.x.x, rpcPort=29999, dataPort=29999, webPort=30000, domainSocketPath=, tieredIdentity=TieredIdentity(node=x.x.x.x, rack=null)}})
&lt;/denchmark-code&gt;

x.x.x.x
alluxio-2.0.1/logs/worker.log
&lt;denchmark-code&gt;2019-09-24 20:15:53,509 WARN  ShortCircuitBlockWriteHandler - Exit(stream) (Error): ReserveSpace: Session=1862586475912655049, Request=block_id: 9207587799040
tier: 0
space_to_reserve: 8388608
only_reserve_space: true
medium_type: ""
pin_on_create: false
, Error=alluxio.exception.WorkerOutOfSpaceException: Failed to request 8388608 bytes after 10,000ms to create blockId 9207587799040
2019-09-24 20:15:53,516 WARN  TieredBlockStore - Clean up expired temporary block 9207587799040 from session 1862586475912655049.
&lt;/denchmark-code&gt;

Worker state:



State
Workers Capacity
Space Used




In Service
105.37TB
3334.48GB



To Reproduce
N/A
Expected behavior
N/A
Urgency
N/A
Additional context
Add any other context about the problem here.
	</description>
	<comments>
		<comment id='1' author='liuhongtong' date='2020-01-17T23:14:31Z'>
		hey &lt;denchmark-link:https://github.com/liuhongtong&gt;@liuhongtong&lt;/denchmark-link&gt;
 we have recently updated  command in the latest 2.1.x releases. Have you tried it and see if this still repros?
		</comment>
		<comment id='2' author='liuhongtong' date='2020-01-29T21:59:40Z'>
		There are couple of fixes made in upcoming 2.1.2 patch. If you still see the issue, re-open the case.
		</comment>
	</comments>
</bug>