<bug id='10318' author='cheyang' open_date='2019-10-28T04:10:25Z' closed_time='2020-05-01T23:03:43Z'>
	<summary>Failed to distributed load data when the size of alluxio is smaller than size of data</summary>
	<description>
Alluxio Version:
2.1.0-SNAPSHOT-ec12adc44298d5887697b517177d1b86963e7662
Describe the bug

Failed to distribute load the data when the size of alluxio cache is smaller than underlay filesystem(OSS in this case). The size of alluxio is 120GB, the size of data in OSS is 140GB.

&lt;denchmark-code&gt;#/opt/alluxio/bin/alluxio fs distributedLoad --thread 54 /images
/images/train-00852-of-01024 is loaded
/images/train-00991-of-01024 loading
/images/train-00922-of-01024 is loaded
/images/train-00072-of-01024 loading
/images/train-00622-of-01024 is loaded
/images/train-00556-of-01024 loading
java.lang.RuntimeException: Failed to successfully complete the job.'
&lt;/denchmark-code&gt;


Expected about 88GB(each worker should use 22GB) should be used, but only 51.06GB is used.

&lt;denchmark-code&gt;# alluxio fsadmin report capacity
Capacity information for all workers:
    Total Capacity: 120.00GB
        Tier: MEM  Size: 120.00GB
    Used Capacity: 51.06GB
        Tier: MEM  Size: 51.06GB
    Used Percentage: 42%
    Free Percentage: 58%

Worker Name      Last Heartbeat   Storage       MEM
192.168.0.117    0                capacity      30.00GB
                                  used          13.23GB (44%)
192.168.0.118    0                capacity      30.00GB
                                  used          12.59GB (41%)
192.168.0.119    0                capacity      30.00GB
                                  used          13.26GB (44%)
192.168.0.120    0                capacity      30.00GB
                                  used          11.98GB (39%)
&lt;/denchmark-code&gt;

To Reproduce

Make MEM and SDD into a single tired store, and there are 4 workers for it. There are 120 GB in Alluxio

&lt;denchmark-code&gt;# alluxio getConf |grep level0
alluxio.master.tieredstore.global.level0.alias=MEM
alluxio.worker.tieredstore.level0.alias=MEM
alluxio.worker.tieredstore.level0.dirs.mediumtype=MEM,SSD
alluxio.worker.tieredstore.level0.dirs.path=/dev/shm,/var/lib/docker/alluxio-ssd
alluxio.worker.tieredstore.level0.dirs.quota=10GB,20GB
alluxio.worker.tieredstore.level0.watermark.high.ratio=0.95
alluxio.worker.tieredstore.level0.watermark.low.ratio=0.7
alluxio.user.file.passive.cache.enabled: false
alluxio.user.ufs.block.read.location.policy: alluxio.client.block.policy.LocalFirstAvoidEvictionPolicy
alluxio.user.block.avoid.eviction.policy.reserved.size.bytes: 8GB
alluxio.worker.allocator.class: alluxio.worker.block.allocator.GreedyAllocator
&lt;/denchmark-code&gt;


Run distributed loadï¼Œ but it's failed.

&lt;denchmark-code&gt;#/opt/alluxio/bin/alluxio fs distributedLoad --thread 54 /images
java.lang.RuntimeException: Failed to successfully complete the job.'
&lt;/denchmark-code&gt;


All the logs are here
diagnose_alluxio_1572233368.tar.gz

Expected behavior
A clear and concise description of what you expected to happen.
Urgency
Describe the impact and urgency of the bug.
Additional context
Add any other context about the problem here.
	</description>
	<comments>
		<comment id='1' author='cheyang' date='2019-10-28T05:48:53Z'>
		/ping &lt;denchmark-link:https://github.com/apc999&gt;@apc999&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ZacBlanco&gt;@ZacBlanco&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='cheyang' date='2019-10-28T08:02:53Z'>
		I can see the following error:
&lt;denchmark-code&gt;{
  "result" : null,
  "status" : "FAILED",
  "jobId" : 1572179984298,
  "errorMessage" : "Task execution failed: Failed to request 1048576 bytes after 10,000ms to create blockId 8891924483 (GrpcDataWriter{request=type: ALLUXIO_BLOCK\nid: 8891924483\ntier: 0\nmedium_type: \"\"\npin_on_create: false\n, address=WorkerNetAddress{host=192.168.0.119, rpcPort=29999, dataPort=29999, webPort=30000, domainSocketPath=, tieredIdentity=TieredIdentity(node=192.168.0.119, rack=null)}})",
  "lastStatusChangeMs" : 1572180603098,
  "jobConfig" : {
    "@type" : "alluxio.job.load.LoadConfig",
    "filePath" : "/images/train-00529-of-01024",
    "replication" : 1,
    "name" : "Load"
  },
  "taskInfoList" : [ {
    "result" : null,
    "status" : "FAILED",
    "jobId" : 1572179984298,
    "errorMessage" : "Failed to request 1048576 bytes after 10,000ms to create blockId 8891924480 (GrpcDataWriter{request=type: ALLUXIO_BLOCK\nid: 8891924480\ntier: 0\nmedium_type: \"\"\npin_on_create: false\n, address=WorkerNetAddress{host=192.168.0.120, rpcPort=29999, dataPort=29999, webPort=30000, domainSocketPath=, tieredIdentity=TieredIdentity(node=192.168.0.120, rack=null)}})",
    "taskId" : 0
  }, {
    "result" : null,
    "status" : "FAILED",
    "jobId" : 1572179984298,
    "errorMessage" : "Failed to request 1048576 bytes after 10,000ms to create blockId 8891924482 (GrpcDataWriter{request=type: ALLUXIO_BLOCK\nid: 8891924482\ntier: 0\nmedium_type: \"\"\npin_on_create: false\n, address=WorkerNetAddress{host=192.168.0.118, rpcPort=29999, dataPort=29999, webPort=30000, domainSocketPath=, tieredIdentity=TieredIdentity(node=192.168.0.118, rack=null)}})",
    "taskId" : 1
  }, {
    "result" : null,
    "status" : "FAILED",
    "jobId" : 1572179984298,
    "errorMessage" : "Failed to request 1048576 bytes after 10,000ms to create blockId 8891924483 (GrpcDataWriter{request=type: ALLUXIO_BLOCK\nid: 8891924483\ntier: 0\nmedium_type: \"\"\npin_on_create: false\n, address=WorkerNetAddress{host=192.168.0.119, rpcPort=29999, dataPort=29999, webPort=30000, domainSocketPath=, tieredIdentity=TieredIdentity(node=192.168.0.119, rack=null)}})",
    "taskId" : 2
  } ]
}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='cheyang' date='2019-11-01T05:40:52Z'>
		Discussed with &lt;denchmark-link:https://github.com/cheyang&gt;@cheyang&lt;/denchmark-link&gt;
 ,
FYI &lt;denchmark-link:https://github.com/calvinjia&gt;@calvinjia&lt;/denchmark-link&gt;
 , this is the same issue we talked before when using  where creating an initial temp block is successful while when the caching block size grows, the worker fails and always fails (as it is constantly trying the same storage dir)

Workaround: alluxio.worker.block.allocator.RoundRobinAllocator to avoid the error
TODO(binfan): make worker smarter to create initial (caching) block according to the block size

		</comment>
		<comment id='4' author='cheyang' date='2019-11-02T03:39:25Z'>
		
Change the following properties

&lt;denchmark-code&gt;bash-4.4# alluxio getConf | grep -i alloc
alluxio.worker.allocator.class=alluxio.worker.block.allocator.RoundRobinAllocator

bash-4.4# alluxio getConf | grep level0
alluxio.master.tieredstore.global.level0.alias=MEM
alluxio.worker.tieredstore.level0.alias=MEM
alluxio.worker.tieredstore.level0.dirs.mediumtype=MEM,SSD
alluxio.worker.tieredstore.level0.dirs.path=/dev/shm,/var/lib/docker/alluxio-ssd
alluxio.worker.tieredstore.level0.dirs.quota=10GB,20GB
alluxio.worker.tieredstore.level0.watermark.high.ratio=0.99
alluxio.worker.tieredstore.level0.watermark.low.ratio=0.7

bash-4.4# alluxio getConf | grep -i evict
alluxio.master.metastore.inode.cache.evict.batch.size=1000
alluxio.user.block.avoid.eviction.policy.reserved.size.bytes=2GB
alluxio.user.ufs.block.read.location.policy=alluxio.client.block.policy.LocalFirstAvoidEvictionPolicy
alluxio.worker.evictor.class=alluxio.worker.block.evictor.LRUEvictor
alluxio.worker.evictor.lrfu.attenuation.factor=2.0
alluxio.worker.evictor.lrfu.step.factor=0.25
&lt;/denchmark-code&gt;


I tried to run distributed load, it's still failed

&lt;denchmark-code&gt;#/opt/alluxio/bin/alluxio fs distributedLoad --thread 54 /images
/images/train-00852-of-01024 is loaded
/images/train-00991-of-01024 loading
/images/train-00922-of-01024 is loaded
/images/train-00072-of-01024 loading
/images/train-00622-of-01024 is loaded
/images/train-00556-of-01024 loading
java.lang.RuntimeException: Failed to successfully complete the job.'
&lt;/denchmark-code&gt;


Check the capacity, the cache used percentage is up to 90%. But I think it should the usage should be up to (120G-8G)=112G

&lt;denchmark-code&gt;alluxio fsadmin report capacity
Capacity information for all workers:
    Total Capacity: 120.00GB
        Tier: MEM  Size: 120.00GB
    Used Capacity: 108.35GB
        Tier: MEM  Size: 108.35GB
    Used Percentage: 90%
    Free Percentage: 10%

Worker Name      Last Heartbeat   Storage       MEM
192.168.0.195    0                capacity      30.00GB
                                  used          27.22GB (90%)
192.168.0.117    0                capacity      30.00GB
                                  used          26.57GB (88%)
192.168.0.118    0                capacity      30.00GB
                                  used          27.82GB (92%)
192.168.0.194    0                capacity      30.00GB
                                  used          26.74GB (89%)
&lt;/denchmark-code&gt;


I can see the workerOutOfSpaceException in worker logs:

&lt;denchmark-code&gt;2019-11-01 14:13:37,136 ERROR AbstractWriteHandler - Failed to write data for request BlockWriteRequest{id=1526726659, sessionId=1231062401983204444, tier=0, createUfsBlockOptions=null}
alluxio.exception.WorkerOutOfSpaceException: Failed to request 1048576 bytes after 10,000ms to create blockId 1526726659
        at alluxio.worker.block.TieredBlockStore.requestSpace(TieredBlockStore.java:305)
        at alluxio.worker.block.DefaultBlockWorker.requestSpace(DefaultBlockWorker.java:507)
        at alluxio.worker.grpc.BlockWriteHandler.writeBuf(BlockWriteHandler.java:126)
        at alluxio.worker.grpc.BlockWriteHandler.writeBuf(BlockWriteHandler.java:38)
        at alluxio.worker.grpc.AbstractWriteHandler.writeData(AbstractWriteHandler.java:260)
        at alluxio.worker.grpc.AbstractWriteHandler.lambda$writeDataMessage$1(AbstractWriteHandler.java:163)
        at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
        at alluxio.worker.grpc.GrpcExecutors$ImpersonateThreadPoolExecutor.lambda$execute$0(GrpcExecutors.java:82)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
2019-11-01 14:13:37,136 WARN  TieredBlockStore - Clean up expired temporary block 1526726659 from session 1231062401983204444.
&lt;/denchmark-code&gt;


All the logs are attached:

&lt;denchmark-link:https://github.com/Alluxio/alluxio/files/3800180/diagnose_alluxio_1572619563.tar.gz&gt;diagnose_alluxio_1572619563.tar.gz&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='cheyang' date='2020-01-17T22:47:45Z'>
		&lt;denchmark-link:https://github.com/cheyang&gt;@cheyang&lt;/denchmark-link&gt;
 Have you found a solution to this issue?
		</comment>
		<comment id='6' author='cheyang' date='2020-01-24T03:00:40Z'>
		Not yet. Any suggestions on it.
		</comment>
		<comment id='7' author='cheyang' date='2020-01-29T22:50:35Z'>
		&lt;denchmark-link:https://github.com/ggezer&gt;@ggezer&lt;/denchmark-link&gt;
  - can you take a look and see if this is relevant to the eviction items being worked upon.
		</comment>
		<comment id='8' author='cheyang' date='2020-01-29T23:19:03Z'>
		There is a planned feature for failures during requestSpace at worker. So initialized block-out-stream to a worker will always find a space in that worker.
However, complete solution would need client level retries when a block-out-stream can't be opened to a selected worker.
		</comment>
	</comments>
</bug>