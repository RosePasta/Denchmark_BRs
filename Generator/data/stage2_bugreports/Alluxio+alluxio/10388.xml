<bug id='10388' author='chentao' open_date='2019-11-01T09:02:55Z' closed_time='2019-11-14T08:39:28Z'>
	<summary>AlluxioMaster doesn't respond to any client operations (forever)</summary>
	<description>
Alluxio Version:
1.8.1

Alluxio Cluster Info:
2 Masters + 42 workers with zk enabled (3 zk nodes)
2 tiered store, each worker 220GB MEM + 6TB HDD
alluxio-site:
&lt;denchmark-link:https://github.com/Alluxio/alluxio/files/3797179/alluxio-site.properties.zip&gt;alluxio-site.properties.zip&lt;/denchmark-link&gt;

UFS Info:
A Hadoop 2.6.0-cdh5.14.4 cluster with 274 DataNodes
We mount the HDFS in Read-Only mode as we only need to read from alluxio that have the UFS(HDFS) in another IDC with a 200GB fiber network. We use Hive and Spark with alluxio client to read and compute data.
We found that when the Master.PathsTotal increase faster than usual, the master process is NOT respond to any new client, eg. a alluxio fs ls / is not respond and no message produced, freezing until Ctrl+C. There are no logs produced in master.log.
The Hive or Spark job failed with error log like "Failed to connect to FileSystemMasterClient @ master address and port" or "java.net.SocketTimeoutException: Read timed out".
Even More Serious:
After a period of time (20 minutes  to several hours) from the Master start, the Master is going into a fake death.

No client can talk to it, but the process does not exit
the CPU/MEM/IO looks good
the WEB is normal
the master.log have no message

finally after a long time, the Master can't recovery from this status, until we manually restart the Master, even we stop all the hive/spark jobs and make sure no client was active. But same thing will happen soon.
To Reproduce
Running a big job with hive or spark, reading lots of fresh data (most of path not in alluxio metadata)
Expected behavior
Alluxio Master working continuously and alluxio client access was success.

As this picture, the big increase of PathsTotal result in the others' client not functional:
&lt;denchmark-link:https://user-images.githubusercontent.com/101755/68013180-33d80f80-fcc7-11e9-8071-f32b687a2135.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='chentao' date='2019-11-01T17:31:02Z'>
		&lt;denchmark-link:https://github.com/chentao&gt;@chentao&lt;/denchmark-link&gt;
 I checked your alluxio-site.properties and I don't see a root UFS. Is the root mount cluster the same as this one hdfs://hadoop01/? If the journal hdfs is in READ only mode, then Alluxio won't be able to perform operations since it is unable to flush any edit logs.
		</comment>
		<comment id='2' author='chentao' date='2019-11-04T01:32:29Z'>
		&lt;denchmark-link:https://github.com/ns1123&gt;@ns1123&lt;/denchmark-link&gt;
 The root UFS is mounted through a mount command after cluster startup, and the target is not the same as journal hdfs, it's hdfs://hadoop04/.
		</comment>
		<comment id='3' author='chentao' date='2019-11-08T08:22:50Z'>
		&lt;denchmark-link:https://github.com/ns1123&gt;@ns1123&lt;/denchmark-link&gt;
 We found that:

It's tightly related to the under HDFS's performance, the average RPC processing time of HDFS increase result this problem more serious, we solved the HDFS's performance issue and the alluxio master became more healthy.
And, the big amount of small files of HDFS directory reading by client is a big cause of the increasing of metadata of alluxio master.

		</comment>
		<comment id='4' author='chentao' date='2019-11-14T08:39:27Z'>
		as the reason above, close the issue.
		</comment>
	</comments>
</bug>