<bug id='10683' author='XiaochenCui' open_date='2019-12-30T09:11:04Z' closed_time='2020-01-07T09:35:11Z'>
	<summary>UnavailableException will test spark with alluxio</summary>
	<description>
Alluxio Version:
2.1.1

I have followed the docs &lt;denchmark-link:https://docs.alluxio.io/os/user/stable/en/compute/Spark.html&gt;https://docs.alluxio.io/os/user/stable/en/compute/Spark.html&lt;/denchmark-link&gt;
 to run spark on alluxio, but when I run 
It failed with UnavailableException
To Reproduce
It can be reproduce in my company's cluster, I will be there if you need any extra information.
Expected behavior
Execution success
Urgency
We are going to build alluxio before the 2020 comes.
Additional context
Here is the all log:
&lt;denchmark-code&gt;scala&gt; double.saveAsTextFile("alluxio://localhost:19998/Output_HDFS")
[Stage 0:&gt;                                                          (0 + 2) / 2]19/12/30 17:00:25 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, cdh2, executor 1): alluxio.exception.status.UnavailableException: Failed to connect to FileSystemMast
erClient @ localhost:19998 after 44 attempts
        at alluxio.AbstractClient.connect(AbstractClient.java:269)
        at alluxio.client.file.BaseFileSystem.rpc(BaseFileSystem.java:613)
        at alluxio.client.file.BaseFileSystem.getStatus(BaseFileSystem.java:314)
        at alluxio.client.file.BaseFileSystem.openFile(BaseFileSystem.java:415)
        at alluxio.client.file.BaseFileSystem.openFile(BaseFileSystem.java:406)
        at alluxio.hadoop.HdfsFileInputStream.&lt;init&gt;(HdfsFileInputStream.java:60)
        at alluxio.hadoop.AbstractFileSystem.open(AbstractFileSystem.java:633)
        at alluxio.hadoop.FileSystem.open(FileSystem.java:41)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
        at org.apache.hadoop.mapred.LineRecordReader.&lt;init&gt;(LineRecordReader.java:109)
        at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
        at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:257)
        at org.apache.spark.rdd.HadoopRDD$$anon$1.&lt;init&gt;(HadoopRDD.scala:256)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:214)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
        at org.apache.spark.scheduler.Task.run(Task.scala:109)
&lt;/denchmark-code&gt;

It's too long to attach
	</description>
	<comments>
		<comment id='1' author='XiaochenCui' date='2020-01-03T18:14:08Z'>
		&lt;denchmark-link:https://github.com/XiaochenCui&gt;@XiaochenCui&lt;/denchmark-link&gt;
 this issue usually indicates some issue from the YARN side. Likely due to impersonation. Can you see if the Alluxio Master logs show any information about failed attempts to impersonate? If so, please attach those logs here. If you're running Spark on YARN with Alluxio, you should have this line (or something similar) in  your alluxio-site.properties.
alluxio.master.security.impersonation.yarn.users=*
For more detail on the format of the property, look at this link:
&lt;denchmark-link:https://docs.alluxio.io/os/user/stable/en/operation/Security.html#client-side-hadoop-impersonation&gt;https://docs.alluxio.io/os/user/stable/en/operation/Security.html#client-side-hadoop-impersonation&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='XiaochenCui' date='2020-01-04T05:59:22Z'>
		also please double check if your master is running at address localhost:19998
		</comment>
		<comment id='3' author='XiaochenCui' date='2020-01-06T02:02:27Z'>
		
also please double check if your master is running at address localhost:19998

The localhost is one of master, but not always the leader.
The master list is server1, server2, server3. so as the worker.
And the spark-shell in running on server1.
The leader is server2 when I do the experiments.
But what's the function of alluxio proxy? I have thought it to be like 'kubelet proxy' to proxy traffix so I can access leader thought the proxy endpoint localhost:39999, but it dose not works either.
Btw, the alluxio.master.rpc.addresses config in spark does not works good.
I set it to server1, server2, server3 and the current leader is server2.
It always throw errors shows that server1 and server3 refuse connection, so does it has the ability to find the leader? It it has, how it works and how to config it?
Thanks
		</comment>
		<comment id='4' author='XiaochenCui' date='2020-01-07T09:35:11Z'>
		problem solved, it's caused by wrong config
		</comment>
	</comments>
</bug>