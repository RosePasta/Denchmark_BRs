<bug id='10539' author='Shmilyqjj' open_date='2019-12-02T10:59:29Z' closed_time='2019-12-10T13:17:40Z'>
	<summary>Timed out waiting for file to be persisted</summary>
	<description>
Alluxio Version:
2.1.0
Describe the bug
I can not free or persist a file because of the file  which is not persisted.
To Reproduce

writeType:ASYNC_THROUGH
readType:CACHE_PROMOTE
A 30GB table with some small files
spark version 2.1.0
53 Worker Alive and 2 Worker Lost. 50GB MEM each Worker.

df = spark.table(table_name)
df.write.parquet('alluxio:///data/cdbp')
Because there are not enough resourcesï¼ŒSpark tasks failed with the following stack information:
19/12/02 16:55:41 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container killed by YARN for exceeding memory limits. 34.8 GB of 33 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
19/12/02 16:55:41 ERROR cluster.YarnScheduler: Lost executor 1 on host_name: Container killed by YARN for exceeding memory limits. 34.8 GB of 33 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
19/12/02 16:55:41 WARN scheduler.TaskSetManager: Lost task 229.1 in stage 5.0 (TID 484, data_node_addr, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 34.8 GB of 33 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
19/12/02 16:55:44 WARN server.TransportChannelHandler: Exception in connection from /host_name:42504
java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
        at sun.nio.ch.IOUtil.read(IOUtil.java:192)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
        at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:221)
        at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:899)
        at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:275)
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)
        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
        at java.lang.Thread.run(Thread.java:748)
19/12/02 16:55:44 ERROR cluster.YarnScheduler: Lost an executor 1 (already removed): Pending loss reason.
[Stage 5:======================================================&gt;(238 + 1) / 239]19/12/02 16:56:41 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container killed by YARN for exceeding memory limits. 34.9 GB of 33 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
19/12/02 16:56:41 ERROR cluster.YarnScheduler: Lost executor 5 on host_name: Container killed by YARN for exceeding memory limits. 34.9 GB of 33 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
19/12/02 16:56:41 WARN scheduler.TaskSetManager: Lost task 229.2 in stage 5.0 (TID 485, host_name, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 34.9 GB of 33 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
19/12/02 16:56:43 WARN server.TransportChannelHandler: Exception in connection from /host_name:44284
java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
        at sun.nio.ch.IOUtil.read(IOUtil.java:192)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
        at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:221)
        at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:899)
        at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:275)
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)
        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
        at java.lang.Thread.run(Thread.java:748)
19/12/02 16:56:43 ERROR cluster.YarnScheduler: Lost an executor 5 (already removed): Pending loss reason.

19/12/02 16:58:40 WARN hadoop.AbstractFileSystem: delete failed: Path "/data/cdbp/_temporary-cd3f6e79-7b73-4c02-86c7-938151149ac4" does not exist.
This kind of mistake is very common. So I  want to free the path "/data/cdbp" on Alluxio.
But I failed.
alluxio fs free /data/cdbp/
then it return this:
Cannot free file /data/cdbp/part-00035-cd3f6e79-7b73-4c02-86c7-938151149ac4.snappy.parquet which is not persisted
I try to use 'free -f' ,still the same.
then I try this command:
alluxio fs persist  /data/cdbp/part-00035-cd3f6e79-7b73-4c02-86c7-938151149ac4.snappy.parquet
I wait it for a long time.Finally, it prompts this:
Timed out waiting for file to be persisted: /data/cdbp/part-00035-cd3f6e79-7b73-4c02-86c7-938151149ac4.snappy.parquet
user.log
2019-12-02 18:24:45,487 ERROR AbstractShell - Error running free /data/cdbp/
java.io.IOException: Cannot free file /data/cdbp/part-00035-cd3f6e79-7b73-4c02-86c7-938151149ac4.snappy.parquet which is not persisted
        at alluxio.cli.fs.command.AbstractFileSystemCommand.runWildCardCmd(AbstractFileSystemCommand.java:99)
        at alluxio.cli.fs.command.FreeCommand.run(FreeCommand.java:116)
        at alluxio.cli.AbstractShell.run(AbstractShell.java:135)
        at alluxio.cli.fs.FileSystemShell.main(FileSystemShell.java:66)
java.util.concurrent.TimeoutException: Timed out waiting for /data/cdbp/part-00035-cd3f6e79-7b73-4c02-86c7-938151149ac4.snappy.parquet to be persisted options: WaitForOptions{interval=1000, timeout=1200000}
        at alluxio.util.CommonUtils.waitForResult(CommonUtils.java:345)
        at alluxio.util.CommonUtils.waitFor(CommonUtils.java:324)
        at alluxio.client.file.FileSystemUtils.persistAndWait(FileSystemUtils.java:158)
        at alluxio.cli.fs.command.PersistCommand$PersistCallable.call(PersistCommand.java:216)
        at alluxio.cli.fs.command.PersistCommand$PersistCallable.call(PersistCommand.java:193)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
master.log
2019-12-02 18:24:45,472 WARN  FileSystemMasterClientServiceHandler - Exit (Error): Free: request=path: "/data/cdbp"
options {
  recursive: true
  forced: false
  commonOptions {
    syncIntervalMs: -1
    ttl: -1
    ttlAction: DELETE
  }
}
, Error=alluxio.exception.UnexpectedAlluxioException: Cannot free file /data/cdbp/part-00035-cd3f6e79-7b73-4c02-86c7-938151149ac4.snappy.parquet which is not persisted
2019-12-02 18:25:08,774 WARN  DefaultFileSystemMaster - Failed to persist file /data/cdbp/part-00228-cd3f6e79-7b73-4c02-86c7-938151149ac4.snappy.parquet, will retry later: alluxio.exception.status.Unavailabl
eException: Failed to determine address for JobMasterClient after 11 attempts
2019-12-02 18:25:28,126 INFO  SupportedHdfsActiveSyncProvider - HDFS generated 5444 events in 60071 ms, at a rate of 90.63 rps
2019-12-02 18:25:28,127 INFO  SupportedHdfsActiveSyncProvider - processed 5444 events in 60071 ms, at a rate of 90.63 rps
2019-12-02 18:25:28,127 INFO  SupportedHdfsActiveSyncProvider - Currently TxidsBehindEstimate by 0
2019-12-02 18:26:28,211 INFO  SupportedHdfsActiveSyncProvider - HDFS generated 8492 events in 60085 ms, at a rate of 141.33 rps
2019-12-02 18:26:28,211 INFO  SupportedHdfsActiveSyncProvider - processed 8492 events in 60085 ms, at a rate of 141.33 rps
2019-12-02 18:26:28,211 INFO  SupportedHdfsActiveSyncProvider - Currently TxidsBehindEstimate by 0
2019-12-02 18:27:15,026 WARN  DefaultFileSystemMaster - Failed to persist file /data/cdbp/part-00217-cd3f6e79-7b73-4c02-86c7-938151149ac4.snappy.parquet, will retry later: alluxio.exception.status.Unavailabl
eException: Failed to determine address for JobMasterClient after 11 attempts
2019-12-02 18:27:28,270 INFO  SupportedHdfsActiveSyncProvider - HDFS generated 8985 events in 60059 ms, at a rate of 149.60 rps
2019-12-02 18:27:28,270 INFO  SupportedHdfsActiveSyncProvider - processed 8985 events in 60059 ms, at a rate of 149.60 rps
2019-12-02 18:27:28,270 INFO  SupportedHdfsActiveSyncProvider - Currently TxidsBehindEstimate by 0
2019-12-02 18:28:28,421 INFO  SupportedHdfsActiveSyncProvider - HDFS generated 23177 events in 60151 ms, at a rate of 385.31 rps
2019-12-02 18:28:28,422 INFO  SupportedHdfsActiveSyncProvider - processed 23177 events in 60151 ms, at a rate of 385.31 rps
2019-12-02 18:28:28,422 INFO  SupportedHdfsActiveSyncProvider - Currently TxidsBehindEstimate by 0
2019-12-02 18:29:02,922 WARN  SleepingTimer - Master Metrics Time Series last execution took 2108 ms. Longer than the interval 1000
2019-12-02 18:29:20,060 WARN  SleepingTimer - Master Metrics Time Series last execution took 17138 ms. Longer than the interval 1000
2019-12-02 18:29:20,957 WARN  DefaultFileSystemMaster - Failed to persist file /data/cdbp/part-00195-cd3f6e79-7b73-4c02-86c7-938151149ac4.snappy.parquet, will retry later: alluxio.exception.status.Unavailabl
eException: Failed to determine address for JobMasterClient after 11 attempts
no worker logs were found at that time.
now,I found more new files can not be persisted ,but no WARN/ERROR logs.
Expected behavior
I expected I can free the file successfully.Or,give me an ERROR message why I can not persisted/free the file.
Urgency
normal
	</description>
	<comments>
		<comment id='1' author='Shmilyqjj' date='2019-12-02T18:04:30Z'>
		&lt;denchmark-link:https://github.com/Shmilyqjj&gt;@Shmilyqjj&lt;/denchmark-link&gt;
 If possible, can you provide Alluxio master, worker, and/or client logs during the time when the above error occurs?
		</comment>
		<comment id='2' author='Shmilyqjj' date='2019-12-03T06:55:08Z'>
		
@Shmilyqjj If possible, can you provide Alluxio master, worker, and/or client logs during the time when the above error occurs?
I updated the comment.

		</comment>
		<comment id='3' author='Shmilyqjj' date='2019-12-09T20:38:55Z'>
		I noticed in the last line of your master log
&lt;denchmark-code&gt;DefaultFileSystemMaster - Failed to persist file /data/cdbp/part-00195-cd3f6e79-7b73-4c02-86c7-938151149ac4.snappy.parquet, will retry later: alluxio.exception.status.Unavailabl
eException: Failed to determine address for JobMasterClient after 11 attempts
&lt;/denchmark-code&gt;

The persisting of files is handled by the Alluxio job service which requires running the Job master process (./bin/alluxio-start.sh job_master) and job workers (./bin/alluxio-start.sh job_worker). It looks like either your job master/worker isn't running or it can't determine the address.
Can you provide your alluxio-site.properties?
		</comment>
		<comment id='4' author='Shmilyqjj' date='2019-12-10T13:17:40Z'>
		
I noticed in the last line of your master log
DefaultFileSystemMaster - Failed to persist file /data/cdbp/part-00195-cd3f6e79-7b73-4c02-86c7-938151149ac4.snappy.parquet, will retry later: alluxio.exception.status.Unavailabl
eException: Failed to determine address for JobMasterClient after 11 attempts

The persisting of files is handled by the Alluxio job service which requires running the Job master process (./bin/alluxio-start.sh job_master) and job workers (./bin/alluxio-start.sh job_worker). It looks like either your job master/worker isn't running or it can't determine the address.
Can you provide your alluxio-site.properties?

I did not make ssh, so I did not use the start-all script. I only started up the master and worker on the corresponding nodes and did not start the job_master and job_worker components. However,  it is running normally after starting job_master and job_worker.
		</comment>
		<comment id='5' author='Shmilyqjj' date='2019-12-10T13:19:16Z'>
		
I noticed in the last line of your master log
DefaultFileSystemMaster - Failed to persist file /data/cdbp/part-00195-cd3f6e79-7b73-4c02-86c7-938151149ac4.snappy.parquet, will retry later: alluxio.exception.status.Unavailabl
eException: Failed to determine address for JobMasterClient after 11 attempts

The persisting of files is handled by the Alluxio job service which requires running the Job master process (./bin/alluxio-start.sh job_master) and job workers (./bin/alluxio-start.sh job_worker). It looks like either your job master/worker isn't running or it can't determine the address.
Can you provide your alluxio-site.properties?

I did not make ssh, so I did not use the start-all script. I only started up the master and worker on the corresponding nodes and did not start the job_master and job_worker components. However,  it is running normally after starting job_master and job_worker.


I noticed in the last line of your master log
DefaultFileSystemMaster - Failed to persist file /data/cdbp/part-00195-cd3f6e79-7b73-4c02-86c7-938151149ac4.snappy.parquet, will retry later: alluxio.exception.status.Unavailabl
eException: Failed to determine address for JobMasterClient after 11 attempts

The persisting of files is handled by the Alluxio job service which requires running the Job master process (./bin/alluxio-start.sh job_master) and job workers (./bin/alluxio-start.sh job_worker). It looks like either your job master/worker isn't running or it can't determine the address.
Can you provide your alluxio-site.properties?

I did not make ssh, so I did not use the start-all script. I only started up the master and worker on the corresponding nodes and did not start the job_master and job_worker components. However, it is running normally after starting job_master and job_worker.I still hope that the official documentation can introduce these two components in detail. If these two components are not started, I hope that the error messages will be shown immediately in the persist timed out situation.

		</comment>
	</comments>
</bug>