<bug id='12517' author='cheyang' open_date='2020-11-17T09:34:31Z' closed_time='2020-11-23T11:47:26Z'>
	<summary>Failed to distributedLoad data on dataset with many small files</summary>
	<description>
Alluxio Version:
What version of Alluxio are you using?
alluxio version -r
2.3.1-SNAPSHOT-2c412267e0e749ced63262d53043cf753cc11927
&lt;denchmark-link:https://github.com/Alluxio/alluxio/commit/2c412267e0e749ced63262d53043cf753cc11927&gt;2c41226&lt;/denchmark-link&gt;

Describe the bug
A clear and concise description of what the bug is.
Alluxio cluster consists of 1 master node and 1 worker nodes. The under storage is local, and data size is 84.29GB.
&lt;denchmark-code&gt;alluxio fsadmin report capacity
Capacity information for all workers:
    Total Capacity: 200.00GB
        Tier: SSD  Size: 200.00GB
    Used Capacity: 36.50GB
        Tier: SSD  Size: 36.50GB
    Used Percentage: 0%
    Free Percentage: 100%

Worker Name      Last Heartbeat   Storage       SSD
192.168.5.109    0                capacity      200.00GB
                                  used          0GB (0%)
&lt;/denchmark-code&gt;

When I distributedLoad the data, it succeed and say:
&lt;denchmark-code&gt;time  alluxio fs distributedLoad  --replication 1 /
Successfully loaded path /insightface/images_2/329390.png after 1 attempts
Successfully loaded path /insightface/images_2/3199053.png after 1 attempts
Successfully loaded path /insightface/images_2/3579696.png after 1 attempts
Successfully loaded path /insightface/images_2/1401421.png after 1 attempts
Successfully loaded path /insightface/images_2/1245001.png after 1 attempts
Successfully loaded path /insightface/images_2/1625644.png after 1 attempts
Successfully loaded path /insightface/images_2/3788067.png after 1 attempts
Successfully loaded path /insightface/images_2/452706.png after 1 attempts

real    83m36.100s
user    21m31.902s
sys     13m36.277s
&lt;/denchmark-code&gt;

But When I checked the result:
&lt;denchmark-code&gt;bash-4.4# alluxio fsadmin report capacity
Capacity information for all workers:
    Total Capacity: 200.00GB
        Tier: SSD  Size: 200.00GB
    Used Capacity: 36.50GB
        Tier: SSD  Size: 36.50GB
    Used Percentage: 18%
    Free Percentage: 82%

Worker Name      Last Heartbeat   Storage       SSD
192.168.5.109    0                capacity      200.00GB
                                  used          36.50GB (18%)
bash-4.4# alluxio fs du -sh /
File Size     In Alluxio       Path
84.29GB       36.50GB (43%)    /
&lt;/denchmark-code&gt;

To Reproduce
Steps to reproduce the behavior (as minimally and precisely as possible)
Expected behavior
A clear and concise description of what you expected to happen.

The distributed time took too long
only no more than half data is loaded

Urgency
Describe the impact and urgency of the bug.
Additional context
Add any other context about the problem here.
	</description>
	<comments>
		<comment id='1' author='cheyang' date='2020-11-17T09:35:58Z'>
		&lt;denchmark-link:https://github.com/Alluxio/alluxio/files/5552509/diagnose_fluid_1605581604.tar.gz&gt;diagnose_fluid_1605581604.tar.gz&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='cheyang' date='2020-11-18T19:51:55Z'>
		&lt;denchmark-link:https://github.com/bradyoo&gt;@bradyoo&lt;/denchmark-link&gt;
  can you take a look ?
		</comment>
		<comment id='3' author='cheyang' date='2020-11-18T19:59:34Z'>
		More context for &lt;denchmark-link:https://github.com/bradyoo&gt;@bradyoo&lt;/denchmark-link&gt;
 : the previous related issue  (&lt;denchmark-link:https://github.com/Alluxio/alluxio/issues/12116&gt;#12116&lt;/denchmark-link&gt;
) was fixed by increasing RPC timeout as explained inside the issue. After applying the fix, &lt;denchmark-link:https://github.com/cheyang&gt;@cheyang&lt;/denchmark-link&gt;
 was able to complete distributedLoad for bigger files (e.g., 1k files of 100+MB each). With the same fix, however, distributedLoad still cannot complete for his datasets with millions of smaller files. The original issue is closed and now let's iterate on this issue
		</comment>
		<comment id='4' author='cheyang' date='2020-11-18T20:27:07Z'>
		Are you sure this is the correct set of logs?
For instance, all the logs in JobMaster logs are:
&lt;denchmark-code&gt;2020-11-16 16:49:30,098 INFO  TieredIdentityFactory - Initialized tiered identity TieredIdentity(node=192.168.5.109, rack=null)
2020-11-16 16:49:30,123 INFO  ExtensionFactoryRegistry - Loading core jars from /opt/alluxio-2.3.0-SNAPSHOT/lib
2020-11-16 16:49:30,161 INFO  ExtensionFactoryRegistry - Loading extension jars from /opt/alluxio-2.3.0-SNAPSHOT/extensions
2020-11-16 16:49:30,187 INFO  ProcessUtils - Starting Alluxio job master @ 192.168.5.109:20004.
2020-11-16 16:49:30,189 INFO  UfsJournalCheckpointThread - JobMaster: Journal checkpoint thread started.
2020-11-16 16:49:30,191 INFO  UfsJournalCheckpointThread - JobMaster: Journal checkpointer shutdown has been initiated.
2020-11-16 16:49:35,201 INFO  UfsJournalCheckpointThread - JobMaster: Journal checkpoint thread has been shutdown. No new logs have been found during the quiet period.
2020-11-16 16:49:35,201 INFO  UfsJournalCheckpointThread - JobMaster: Journal shutdown complete
2020-11-16 16:49:35,214 INFO  UfsJournal - JobMaster: journal switched to primary mode. location: /journal/JobJournal/JobMaster/v1
2020-11-16 16:49:35,248 INFO  AbstractMaster - JobMaster: Starting primary master.
2020-11-16 16:49:35,250 INFO  AlluxioJobMasterProcess - Alluxio job master web server version 2.3.1-SNAPSHOT starting. webAddress=/0.0.0.0:20005
2020-11-16 16:49:35,257 INFO  log - Logging initialized @5818ms
2020-11-16 16:49:35,342 INFO  WebServer - Alluxio Job Manager Master Web service starting @ /0.0.0.0:20005
2020-11-16 16:49:35,344 INFO  Server - jetty-9.2.z-SNAPSHOT
2020-11-16 16:49:35,363 INFO  ContextHandler - Started o.e.j.s.ServletContextHandler@3943a2be{/metrics/json,null,AVAILABLE}
2020-11-16 16:49:35,364 WARN  SecurityHandler - ServletContext@o.e.j.s.ServletContextHandler@3cc1435c{/,null,STARTING} has uncovered http methods for path: /
2020-11-16 16:49:41,000 INFO  ContextHandler - Started o.e.j.s.ServletContextHandler@3cc1435c{/,null,AVAILABLE}
2020-11-16 16:49:41,006 INFO  ServerConnector - Started ServerConnector@64beebb7{HTTP/1.1}{0.0.0.0:20005}
2020-11-16 16:49:41,006 INFO  Server - Started @11566ms
2020-11-16 16:49:41,006 INFO  WebServer - Alluxio Job Manager Master Web service started @ /0.0.0.0:20005
2020-11-16 16:49:41,006 INFO  AlluxioJobMasterProcess - Alluxio job master version 2.3.1-SNAPSHOT started. bindAddress=/0.0.0.0:20004, connectAddress=192.168.5.109:20004, webAddress=/0.0.0.0:20005
2020-11-16 16:49:41,006 INFO  AlluxioJobMasterProcess - Starting Alluxio job master gRPC server on address /0.0.0.0:20004
2020-11-16 16:49:41,151 INFO  MasterProcess - registered service JOB_MASTER_WORKER_SERVICE
2020-11-16 16:49:41,151 INFO  MasterProcess - registered service JOB_MASTER_CLIENT_SERVICE
2020-11-16 16:49:41,254 INFO  AlluxioJobMasterProcess - Started Alluxio job master gRPC server on address 192.168.5.109:20004
2020-11-16 16:49:56,928 INFO  JobMaster - registerWorker(): WorkerNetAddress: WorkerNetAddress{host=192.168.5.109, containerHost=, rpcPort=20006, dataPort=20008, webPort=20007, domainSocketPath=, tieredIdentity=TieredIdentity(node=192.168.5.109)} id: 1605545370184
&lt;/denchmark-code&gt;

Perhaps there was a previous instance that got killed in some way?
		</comment>
		<comment id='5' author='cheyang' date='2020-11-19T01:35:21Z'>
		&lt;denchmark-link:https://github.com/cheyang&gt;@cheyang&lt;/denchmark-link&gt;
  could you help Bradley to double check ? thanks
		</comment>
		<comment id='6' author='cheyang' date='2020-11-19T03:28:35Z'>
		Sorry for that.
Here is the new logs, please check:
&lt;denchmark-link:https://github.com/Alluxio/alluxio/files/5564154/diagnose_fluid_1605755296.tar.gz&gt;diagnose_fluid_1605755296.tar.gz&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='cheyang' date='2020-11-19T19:03:20Z'>
		Ah yes. This issue was discovered and fixed quite recently here: &lt;denchmark-link:https://github.com/Alluxio/alluxio/pull/12468&gt;#12468&lt;/denchmark-link&gt;
 which modifies a default of a setting of a configuration that causes this issue.
But for now, I recommend changing the configuration
alluxio.job.master.finished.job.retention.time
(See &lt;denchmark-link:https://docs.alluxio.io/os/user/2.3/en/reference/Properties-List.html&gt;https://docs.alluxio.io/os/user/2.3/en/reference/Properties-List.html&lt;/denchmark-link&gt;
)
to a much lower value like 60 seconds or perhaps even less (like 30 seconds).
Alternatively (or with it), you can also increase "alluxio.job.master.job.capacity" to a higher value.
		</comment>
		<comment id='8' author='cheyang' date='2020-11-19T23:33:03Z'>
		&lt;denchmark-link:https://github.com/cheyang&gt;@cheyang&lt;/denchmark-link&gt;
  I updated branch-2.3-fuse by cherry-pick &lt;denchmark-link:https://github.com/Alluxio/alluxio/pull/12468&gt;#12468&lt;/denchmark-link&gt;
 . Can you make another try?
		</comment>
		<comment id='9' author='cheyang' date='2020-11-20T02:29:29Z'>
		Is there any scientific way to set this value? Is there a way to calculate the  alluxio.job.master.finished.job.retention.time based on the file number or the size of the file?  Or is it fine to set alluxio.job.master.finished.job.retention.time=1 or 0?
		</comment>
		<comment id='10' author='cheyang' date='2020-11-20T02:37:47Z'>
		And can we test with the following way:
&lt;denchmark-code&gt;time  alluxio fs distributedLoad -Dalluxio.job.master.finished.job.retention.time=5  --replication 1 /
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='cheyang' date='2020-11-20T18:35:26Z'>
		No. You can not set it in that way. The setting is a global setting for the job service.
This is the behavior of the JobMaster when the JobMaster is at capacity. Lowering this value so much has the potential behavior where the client that needs to check that the job is complete misses it because it was removed. But even then, 30 seconds is plenty high enough (compared to 5 minutes) to not run into this problem.
		</comment>
		<comment id='12' author='cheyang' date='2020-11-21T05:48:55Z'>
		May I ask if there is any impact when I decrease the default value from 60 seconds to 30 seconds?
		</comment>
		<comment id='13' author='cheyang' date='2020-11-21T12:51:39Z'>
		Thank you, I've verified. It works.
		</comment>
	</comments>
</bug>