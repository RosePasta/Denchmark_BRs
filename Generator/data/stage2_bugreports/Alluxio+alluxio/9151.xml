<bug id='9151' author='witgo' open_date='2019-05-28T13:33:25Z' closed_time='2019-07-02T01:44:17Z'>
	<summary>Block XXX   is expected to be XXX  bytes, but only XXX bytes are available.</summary>
	<description>
Alluxio Version:
1.8.1
Describe the bug
Errors in reading alluxio cache files are as follows
&lt;denchmark-code&gt;Block 278143139053568 is expected to be 25971804 bytes, but only 25971796 bytes are available. Please ensure its metadata is consistent between Alluxio and UFS.
&lt;/denchmark-code&gt;

To Reproduce
I don't know how to reproduce it.
Expected behavior
NA
Urgency
NA
Additional context
NA
	</description>
	<comments>
		<comment id='1' author='witgo' date='2019-05-29T18:32:53Z'>
		&lt;denchmark-link:https://github.com/witgo&gt;@witgo&lt;/denchmark-link&gt;
 Can you maybe describe what is your cluster configuration? What is your under file system and what was the compute task you ran that result in this error?
		</comment>
		<comment id='2' author='witgo' date='2019-06-04T12:01:10Z'>
		&lt;denchmark-link:https://github.com/bf8086&gt;@bf8086&lt;/denchmark-link&gt;

TPC-H(10T) on SparkSQL (4 node(64 vcore 512G MEM).
&lt;denchmark-code&gt;alluxio.underfs.address=s3a://XX/XX
alluxio.worker.memory.size=128G
alluxio.worker.tieredstore.levels=2
alluxio.worker.tieredstore.level0.alias=MEM
alluxio.worker.tieredstore.level0.dirs.path=/opt/data1/alluxio/ramdisk
alluxio.worker.tieredstore.level0.dirs.quota=128G
alluxio.worker.tieredstore.level1.alias=SSD
alluxio.worker.tieredstore.level1.dirs.path=/opt/data2
alluxio.worker.tieredstore.level1.dirs.quota=256G
alluxio.worker.data.folder.permissions=rwxrwx---
alluxio.worker.ufs.instream.cache.enabled=false
alluxio.worker.ufs.instream.cache.max.size=64
alluxio.worker.data.server.domain.socket.address=/opt/data1/alluxio
alluxio.worker.data.server.domain.socket.as.uuid=true

# User properties
alluxio.user.short.circuit.enabled=true
alluxio.user.metrics.collection.enabled=false
alluxio.user.network.netty.writer.close.timeout=10min
alluxio.user.network.socket.timeout=10min
alluxio.user.block.master.client.threads=5
alluxio.user.file.master.client.threads=5
alluxio.user.file.delete.unchecked=true
alluxio.user.file.metadata.load.type=Always
alluxio.user.file.metadata.sync.interval=-1
alluxio.user.file.readtype.default=CACHE_PROMOTE  
alluxio.user.file.writetype.default=CACHE_THROUGH
alluxio.user.file.write.tier.default=0
```
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='witgo' date='2019-06-05T01:21:27Z'>
		&lt;denchmark-link:https://github.com/witgo&gt;@witgo&lt;/denchmark-link&gt;
 Thanks! I have a couple questions to understand your scenario better:
Which version of Alluxio are you running?
Does the error happen during the data generation stage or when you are running a query?
Was the data generated directly on S3 or through Alluxio?
		</comment>
		<comment id='4' author='witgo' date='2019-06-05T02:12:15Z'>
		&lt;denchmark-link:https://github.com/bf8086&gt;@bf8086&lt;/denchmark-link&gt;
  This error occurred during in a query. the data generated through Alluxio.
		</comment>
		<comment id='5' author='witgo' date='2019-06-07T04:33:26Z'>
		&lt;denchmark-link:https://github.com/witgo&gt;@witgo&lt;/denchmark-link&gt;
 This might be due to file size inconsistency between Alluxio and UFS. Does it still repro on your cluster? If so, can you free the data from Alluxio storage (assuming you already write all data to S3) and reload them (or just rerun the query and let the query read load the data automatically). Would you still see the error after the cache refresh?
I will check with the team to see if this is a known issue in 1.8.
		</comment>
		<comment id='6' author='witgo' date='2019-06-07T05:54:22Z'>
		&lt;denchmark-link:https://github.com/bf8086&gt;@bf8086&lt;/denchmark-link&gt;
 After the cache refresh, the error no longer appears.
		</comment>
		<comment id='7' author='witgo' date='2019-06-10T20:30:52Z'>
		Glad to hear it no longer show up after the refresh.
Unfortunately we were not able to reproduce this issue on our end. Please let us know if you see this error again or find a way to reproduce it consistently, so we can investigate the root cause.
		</comment>
		<comment id='8' author='witgo' date='2019-07-02T01:44:17Z'>
		&lt;denchmark-link:https://github.com/witgo&gt;@witgo&lt;/denchmark-link&gt;
  - closing the issue, please reopen if you have a way to reproduce it.
		</comment>
		<comment id='9' author='witgo' date='2019-07-11T22:19:16Z'>
		&lt;denchmark-link:https://github.com/r7raul1984&gt;@r7raul1984&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/bf8086&gt;@bf8086&lt;/denchmark-link&gt;
   I have encountered this bug again, and there is no error log output..
		</comment>
		<comment id='10' author='witgo' date='2019-07-16T01:30:35Z'>
		ping &lt;denchmark-link:https://github.com/rastogiasr&gt;@rastogiasr&lt;/denchmark-link&gt;
  This issue does not seem to be reopened.
		</comment>
		<comment id='11' author='witgo' date='2019-07-17T14:29:53Z'>
		ping
		</comment>
		<comment id='12' author='witgo' date='2019-07-17T16:40:42Z'>
		&lt;denchmark-link:https://github.com/witgo&gt;@witgo&lt;/denchmark-link&gt;
 Can you explain a little bit how you repro'ed it this time? Are you running a query? Is your data accessed outside of Alluxio? Were there any warning or unusual log entries before the issue happened?
		</comment>
		<comment id='13' author='witgo' date='2019-07-18T02:45:01Z'>
		Once this problem arises, it causes Spark SQL to misreport any sql job that reads to this file. Data is not accessible outside of alluxio. The data files of the underlying hdfs are not a problem.
The logs that appear frequently are as follows:
&lt;denchmark-code&gt;java.net.SocketException: Socket closed
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:118)
        at java.net.SocketOutputStream.write(SocketOutputStream.java:155)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
        at java.io.FilterOutputStream.close(FilterOutputStream.java:158)
        at org.apache.thrift.transport.TIOStreamTransport.close(TIOStreamTransport.java:110)
        at org.apache.thrift.transport.TSocket.close(TSocket.java:235)
        at org.apache.thrift.transport.TSaslTransport.close(TSaslTransport.java:402)
        at org.apache.thrift.transport.TSaslClientTransport.close(TSaslClientTransport.java:37)
        at alluxio.AbstractClient.disconnect(AbstractClient.java:335)
        at alluxio.AbstractClient.retryRPCInternal(AbstractClient.java:440)
        at alluxio.AbstractClient.retryRPC(AbstractClient.java:389)
        at alluxio.worker.block.BlockMasterClient.commitBlock(BlockMasterClient.java:87)
        at alluxio.worker.block.DefaultBlockWorker.commitBlock(DefaultBlockWorker.java:314)
        at alluxio.worker.netty.BlockWriteHandler$BlockPacketWriter.completeRequest(BlockWriteHandler.java:131)
        at alluxio.worker.netty.BlockWriteHandler$BlockPacketWriter.completeRequest(BlockWriteHandler.java:109)
        at alluxio.worker.netty.AbstractWriteHandler$PacketWriter.runInternal(AbstractWriteHandler.java:318)
        at alluxio.worker.netty.AbstractWriteHandler$PacketWriter.run(AbstractWriteHandler.java:250)
        at alluxio.worker.netty.BlockWriteHandler$BlockPacketWriter.run(BlockWriteHandler.java:109)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

&lt;/denchmark-code&gt;

		</comment>
		<comment id='14' author='witgo' date='2019-07-18T22:16:29Z'>
		Thanks for the logs. I have a couple questions:

Does it happen if you delete and regenerate the data then rerun the benchmark?
Are you seeing it with S3 as underfs or HDFS or both?
Can you provide a list of steps with commands or queries you ran to generate test data and run the benchmark?

		</comment>
		<comment id='15' author='witgo' date='2019-07-19T00:57:12Z'>
		
The production system has multiple jobs running and is not sure how much time to trigger this log.
yes
insert into table , select table, performed once an hour.

		</comment>
		<comment id='16' author='witgo' date='2019-07-19T21:47:55Z'>
		Does it repro every time after you generate the data or only sometimes?
For the block size mismatch exception, do you know which file it was trying to read when the error occurred? If you do, can you run alluxio fs stat &lt;file_name&gt; and post the output here?
For the SocketException, can you check Alluxio master log to see if there is any error at the corresponding time?
		</comment>
		<comment id='17' author='witgo' date='2019-07-21T08:42:00Z'>
		
Only sometimes.
The cached data has been cleaned up and only appeared twice, it seems that the last 8 bytes of data are lost.
No other related errors.

		</comment>
	</comments>
</bug>