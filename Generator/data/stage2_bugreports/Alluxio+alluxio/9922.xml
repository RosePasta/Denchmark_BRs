<bug id='9922' author='hustnn' open_date='2019-09-20T13:10:34Z' closed_time='2020-01-10T20:36:40Z'>
	<summary>Client hang for Presto query</summary>
	<description>
Alluxio Version:
2.0.1
Describe the bug
We repeatedly run a query and find it hang when running for several days.
The query is
select count(1) from a table. 
This table we already load in ahead from hdfs to alluxio
During the running period, some alluxio workers may be down and some lost data may need to read directly from hdfs.
From jstack, find 30 threads (as we set presto concurrency to 30) stuck at
&lt;denchmark-code&gt;"20190919_081107_00969_nmmwc.1.7-19-4862" #4862 prio=5 os_prio=0 tid=0x00007f9618009dd0 nid=0xa86a2 waiting on condition [0x00007f925c2f1000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  &lt;0x00007f9bf8c94008&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at alluxio.resource.ResourcePool.acquire(ResourcePool.java:132)
        at alluxio.resource.ResourcePool.acquire(ResourcePool.java:77)
        at alluxio.client.file.FileSystemContext.acquireBlockMasterClient(FileSystemContext.java:421)
        at alluxio.client.file.FileSystemContext.acquireBlockMasterClientResource(FileSystemContext.java:441)
        at alluxio.client.block.AlluxioBlockStore.getInStream(AlluxioBlockStore.java:169)
        at alluxio.client.file.FileInStream.positionedReadInternal(FileInStream.java:252)
        at alluxio.client.file.FileInStream.positionedRead(FileInStream.java:232)
        at alluxio.hadoop.HdfsFileInputStream.read(HdfsFileInputStream.java:128)
        at alluxio.hadoop.HdfsFileInputStream.readFully(HdfsFileInputStream.java:145)
        at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
        at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:120)
        at io.prestosql.parquet.reader.MetadataReader.readFully(MetadataReader.java:313)
        at io.prestosql.parquet.reader.MetadataReader.readFooter(MetadataReader.java:92)
        at io.prestosql.plugin.hive.parquet.ParquetPageSourceFactory.createParquetPageSource(ParquetPageSourceFactory.java:161)
        at io.prestosql.plugin.hive.parquet.ParquetPageSourceFactory.createPageSource(ParquetPageSourceFactory.java:120)
        at io.prestosql.plugin.hive.HivePageSourceProvider.createHivePageSource(HivePageSourceProvider.java:162)
        at io.prestosql.plugin.hive.HivePageSourceProvider.createPageSource(HivePageSourceProvider.java:96)
        at io.prestosql.spi.connector.classloader.ClassLoaderSafeConnectorPageSourceProvider.createPageSource(ClassLoaderSafeConnectorPageSourceProvider.java:53)
        at io.prestosql.split.PageSourceManager.createPageSource(PageSourceManager.java:56)
        at io.prestosql.operator.TableScanOperator.getOutput(TableScanOperator.java:271)
        at io.prestosql.operator.Driver.processInternal(Driver.java:379)
        at io.prestosql.operator.Driver.lambda$processFor$8(Driver.java:283)
        at io.prestosql.operator.Driver$$Lambda$2810/201986911.get(Unknown Source)
        at io.prestosql.operator.Driver.tryWithLock(Driver.java:675)
        at io.prestosql.operator.Driver.processFor(Driver.java:276)
        at io.prestosql.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:1075)
        at io.prestosql.execution.executor.PrioritizedSplitRunner.process(PrioritizedSplitRunner.java:163)
        at io.prestosql.execution.executor.TaskExecutor$TaskRunner.run(TaskExecutor.java:484)
        at io.prestosql.$gen.Presto_shopee_101_dirty__shopee102____20190918_090137_1.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
&lt;/denchmark-code&gt;

To Reproduce
Currently, not clear.
Expected behavior
No hanging
Urgency
High
Additional context
	</description>
	<comments>
		<comment id='1' author='hustnn' date='2019-09-21T10:44:36Z'>
		Take some time to go through related code and find one suspect place may cause this issue.
&lt;denchmark-code&gt;  private void releaseBlockMasterClient(BlockMasterClient client) {
    try (ReinitBlockerResource r = blockReinit()) {
      if (!client.isClosed()) {
        // The client might have been closed during reinitialization.
        mBlockMasterClientPool.release(client);
      }
    }
  }
&lt;/denchmark-code&gt;

Is it possible that  throws exception, then cause the resource leak here? &lt;denchmark-link:https://github.com/apc999&gt;@apc999&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='hustnn' date='2019-09-23T21:47:47Z'>
		&lt;denchmark-link:https://github.com/hustnn&gt;@hustnn&lt;/denchmark-link&gt;
 thanks for identify the suspicious code snippet, we will investigate this issue.
		</comment>
		<comment id='3' author='hustnn' date='2019-09-26T06:12:53Z'>
		Hang at  in ResourcePool.java. &lt;denchmark-link:https://github.com/apc999&gt;@apc999&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='hustnn' date='2019-12-11T16:25:01Z'>
		is there any update on the same ?
		</comment>
		<comment id='5' author='hustnn' date='2020-01-10T00:08:48Z'>
		&lt;denchmark-link:https://github.com/cabhi&gt;@cabhi&lt;/denchmark-link&gt;
 we will patch the system with  &lt;denchmark-link:https://github.com/Alluxio/alluxio/pull/10719&gt;#10719&lt;/denchmark-link&gt;
 for this issue
		</comment>
	</comments>
</bug>