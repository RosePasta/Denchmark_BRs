<bug id='12616' author='ljk-1993' open_date='2020-12-08T09:20:02Z' closed_time='2020-12-08T09:57:26Z'>
	<summary>The task committed to yarn has no short-circuit read</summary>
	<description>
Alluxio Version:
v2.3.0
Describe the bug
All tasks that are committed to Yarn are remote Alluxio Read,no short-circuit read.
To Reproduce
Our cluster deployment is as follows：



IP
hostname
role
service




192.168.1.3
manager.bigdata
管理节点
Namenode,resourcemanager,spark client,alluxio master


192.168.1.4
worker1.bigdata
计算节点
nodemanager,alluxio worker


192.168.1.5
worker2.bigdata
计算节点
nodemanager,alluxio worker


192.168.1.6
worker3.bigdata
计算节点
nodemanager,alluxio worker


192.168.1.7
worker4.bigdata
计算节点
nodemanager,alluxio worker


192.168.1.8
worker5.bigdata
存储节点
datanode


192.168.1.9
worker6.bigdata
存储节点
datanode


192.168.1.10
worker7.bigdata
存储节点
datanode



We run a Spark SQL task many times，there is no no short-circuit read.We added some log information in alluxio.client.block.AlluxioBlockStore.getInStream method.When we read the file using the ‘bin/alluxio fs cat’, we can see that the log has the log output we want.However, after we submitted the Spark SQL task, we did not see the relevant logging information.
Expected behavior
Which piece of code should I debug
Urgency
Describe the impact and urgency of the bug.
Additional context
Add any other context about the problem here.
	</description>
	<comments>
		<comment id='1' author='ljk-1993' date='2020-12-08T09:24:08Z'>
		Both YARN and Alluxio are configured with domain names。



Alluxio nodes
yarn nodes




master.bigdata
master.bigdata


worker1.bigdata
worker1.bigdata


worker2.bigdata
worker2.bigdata


worker3.bigdata
worker3.bigdata



		</comment>
	</comments>
</bug>