<bug id='11744' author='Bing-ok' open_date='2020-07-10T09:08:38Z' closed_time='2020-08-21T08:53:24Z'>
	<summary>“NoSuchMethodErrors” due to multiple versions of com.google.guava:guava:jar</summary>
	<description>
Hi, there are multiple versions of com.google.guava:guava in alluxio-2.3.0\examples. As shown in the following dependency tree, according to Maven's “nearest wins” strategy, only com.google.guava:guava:27.1-jre can be loaded, com.google.guava:guava:11.0.2 and com.google.guava:guava:28.1-android will be shadowed.
As com.google.guava:guava:11.0.2 has not been loaded during the building process, several methods are missing. However, the missing methods:
1. com.google.common.util.concurrent.Futures: void addCallback(com.google.common.util.concurrent.ListenableFuture,com.google.common.util.concurrent.FutureCallback)

Check for details of invocation
paths------
&lt;alluxio.examples.Performance$HdfsWorker: void init (int,int,int,java.nio.ByteBuffer,boolean,java.lang.String)&gt; alluxio-2.3.0\examples\target\classes
&lt;org.apache.hadoop.conf.Configuration: void clinit ()&gt; Repositories\org\apache\hadoop\hadoop-common\2.7.3\hadoop-common-2.7.3.jar
&lt;org.apache.hadoop.util.ApplicationClassLoader: java.net.URL getResource(java.lang.String)&gt; Repositories\org\apache\hadoop\hadoop-common\2.7.3\hadoop-common-2.7.3.jar
&lt;org.apache.hadoop.util.ApplicationClassLoader: void clinit ()&gt; Repositories\org\apache\hadoop\hadoop-common\2.7.3\hadoop-common-2.7.3.jar
&lt;org.apache.hadoop.hdfs.RemoteBlockReader: void close()&gt; Repositories\org\apache\hadoop\hadoop-hdfs\2.7.3\hadoop-hdfs-2.7.3.jar
&lt;org.apache.hadoop.hdfs.PeerCache: void put(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.net.Peer)&gt; Repositories\org\apache\hadoop\hadoop-hdfs\2.7.3\hadoop-hdfs-2.7.3.jar
&lt;org.apache.hadoop.hdfs.PeerCache: void putInternal(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.net.Peer)&gt; Repositories\org\apache\hadoop\hadoop-hdfs\2.7.3\hadoop-hdfs-2.7.3.jar
&lt;org.apache.hadoop.hdfs.PeerCache: void startExpiryDaemon()&gt; Repositories\org\apache\hadoop\hadoop-hdfs\2.7.3\hadoop-hdfs-2.7.3.jar
&lt;org.apache.hadoop.hdfs.server.namenode.Checkpointer: void run()&gt; Repositories\org\apache\hadoop\hadoop-hdfs\2.7.3\hadoop-hdfs-2.7.3.jar
&lt;org.apache.hadoop.hdfs.server.namenode.Checkpointer: void doCheckpoint()&gt; Repositories\org\apache\hadoop\hadoop-hdfs\2.7.3\hadoop-hdfs-2.7.3.jar
&lt;org.apache.hadoop.hdfs.server.namenode.FSImage: void saveFSImageInAllDirs(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,long)&gt; Repositories\org\apache\hadoop\hadoop-hdfs\2.7.3\hadoop-hdfs-2.7.3.jar
&lt;org.apache.hadoop.hdfs.server.namenode.FSImage: void saveFSImageInAllDirs(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,long,org.apache.hadoop.hdfs.util.Canceler)&gt; Repositories\org\apache\hadoop\hadoop-hdfs\2.7.3\hadoop-hdfs-2.7.3.jar
&lt;org.apache.hadoop.util.ShutdownHookManager$1: void run()&gt; Repositories\org\apache\hadoop\hadoop-common\2.7.3\hadoop-common-2.7.3.jar
&lt;com.google.common.util.concurrent.Futures$6: void run()&gt; 
&lt;org.apache.hadoop.mapred.LocatedFileStatusFetcher$ProcessInitialInputPathCallback: void onSuccess(java.lang.Object)&gt; Repositories\org\apache\hadoop\hadoop-mapreduce-client-core\2.7.3\hadoop-mapreduce-client-core-2.7.3.jar
&lt;org.apache.hadoop.mapred.LocatedFileStatusFetcher$ProcessInitialInputPathCallback: void onSuccess(org.apache.hadoop.mapred.LocatedFileStatusFetcher$ProcessInitialInputPathCallable$Result)&gt; Repositories\org\apache\hadoop\hadoop-mapreduce-client-core\2.7.3\hadoop-mapreduce-client-core-2.7.3.jar
&lt;com.google.common.util.concurrent.Futures: void addCallback(com.google.common.util.concurrent.ListenableFuture,com.google.common.util.concurrent.FutureCallback)&gt; 

2. com.google.common.base.Objects: java.lang.Object firstNonNull(java.lang.Object,java.lang.Object)

Check for details of invocation
paths------
&lt;alluxio.examples.Performance$HdfsWorker: void init (int,int,int,java.nio.ByteBuffer,boolean,java.lang.String)&gt; alluxio-2.3.0\examples\target\classes
&lt;org.apache.hadoop.conf.Configuration: void clinit ()&gt; Repositories\org\apache\hadoop\hadoop-common\2.7.3\hadoop-common-2.7.3.jar
&lt;org.apache.hadoop.util.ApplicationClassLoader: java.net.URL getResource(java.lang.String)&gt; Repositories\org\apache\hadoop\hadoop-common\2.7.3\hadoop-common-2.7.3.jar
&lt;org.apache.hadoop.util.ApplicationClassLoader: void clinit ()&gt;&gt; Repositories\org\apache\hadoop\hadoop-common\2.7.3\hadoop-common-2.7.3.jar
&lt;org.apache.hadoop.hdfs.RemoteBlockReader: void close()&gt; Repositories\org\apache\hadoop\hadoop-hdfs\2.7.3\hadoop-hdfs-2.7.3.jar
&lt;org.apache.hadoop.hdfs.PeerCache: void put(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.net.Peer)&gt; Repositories\org\apache\hadoop\hadoop-hdfs\2.7.3\hadoop-hdfs-2.7.3.jar
&lt;org.apache.hadoop.hdfs.PeerCache: void putInternal(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.net.Peer)&gt; Repositories\org\apache\hadoop\hadoop-hdfs\2.7.3\hadoop-hdfs-2.7.3.jar
&lt;org.apache.hadoop.hdfs.PeerCache: void startExpiryDaemon()&gt; Repositories\org\apache\hadoop\hadoop-hdfs\2.7.3\hadoop-hdfs-2.7.3.jar
&lt;org.apache.hadoop.hdfs.server.namenode.Checkpointer: void run()&gt; Repositories\org\apache\hadoop\hadoop-hdfs\2.7.3\hadoop-hdfs-2.7.3.jar
&lt;org.apache.hadoop.hdfs.server.namenode.Checkpointer: void doCheckpoint()&gt; Repositories\org\apache\hadoop\hadoop-hdfs\2.7.3\hadoop-hdfs-2.7.3.jar
&lt;org.apache.hadoop.hdfs.server.namenode.Checkpointer: void rollForwardByApplyingLogs(org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest,org.apache.hadoop.hdfs.server.namenode.FSImage,org.apache.hadoop.hdfs.server.namenode.FSNamesystem)&gt; Repositories\org\apache\hadoop\hadoop-hdfs\2.7.3\hadoop-hdfs-2.7.3.jar
&lt;org.apache.hadoop.hdfs.server.namenode.FSImage: long loadEdits(java.lang.Iterable,org.apache.hadoop.hdfs.server.namenode.FSNamesystem)&gt; Repositories\org\apache\hadoop\hadoop-hdfs\2.7.3\hadoop-hdfs-2.7.3.jar
&lt;org.apache.hadoop.hdfs.server.namenode.FSImage: long loadEdits(java.lang.Iterable,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)&gt; Repositories\org\apache\hadoop\hadoop-hdfs\2.7.3\hadoop-hdfs-2.7.3.jar
&lt;org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadFSEdits(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)&gt; Repositories\org\apache\hadoop\hadoop-hdfs\2.7.3\hadoop-hdfs-2.7.3.jar
&lt;org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)&gt; Repositories\org\apache\hadoop\hadoop-hdfs\2.7.3\hadoop-hdfs-2.7.3.jar
&lt;org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long applyEditLogOp(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp,org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int,long)&gt; Repositories\org\apache\hadoop\hadoop-hdfs\2.7.3\hadoop-hdfs-2.7.3.jar
&lt;org.apache.hadoop.hdfs.server.namenode.FSDirAclOp: java.util.List unprotectedSetAcl(org.apache.hadoop.hdfs.server.namenode.FSDirectory,java.lang.String,java.util.List,boolean)&gt; Repositories\org\apache\hadoop\hadoop-hdfs\2.7.3\hadoop-hdfs-2.7.3.jar
&lt;org.apache.hadoop.hdfs.server.namenode.AclTransformation: java.util.List replaceAclEntries(java.util.List,java.util.List)&gt; Repositories\org\apache\hadoop\hadoop-hdfs\2.7.3\hadoop-hdfs-2.7.3.jar
&lt;org.apache.hadoop.hdfs.server.namenode.AclTransformation: void calculateMasks(java.util.List,java.util.EnumMap,java.util.EnumSet,java.util.EnumSet)&gt; Repositories\org\apache\hadoop\hadoop-hdfs\2.7.3\hadoop-hdfs-2.7.3.jar
&lt;com.google.common.base.Objects: java.lang.Object firstNonNull(java.lang.Object,java.lang.Object)&gt;

The above missing methods are actually referenced by alluxio-2.3.0\examples, which will cause “NoSuchMethodErrors” at runtime.
Suggested fixing solutions:

Upgrade dependency org.apache.hadoop:hadoop-client from 2.7.3 to 3.2.1. Because one conflicting library version com.google.guava:guava:11.0.2 is transitively introduced by org.apache.hadoop:hadoop-client:2.7.3. Upgrading dependency org.apache.hadoop:hadoop-client from 2.7.3 to 3.2.1 can solve this dependency conflict.

Thank you very much for your attention.
Best regards,
Dependency tree----

click for details
[INFO] org.alluxio:alluxio-examples:jar:2.3.0
[INFO] +- com.google.guava:guava:jar:27.0.1-jre:compile
[INFO] +- org.apache.hadoop:hadoop-client:jar:2.7.3:compile
[INFO] |  +- org.apache.hadoop:hadoop-common:jar:2.7.3:compile
[INFO] |  |  \- (com.google.guava:guava:jar:27.0.1-jre:compile - version managed from 11.0.2; omitted for duplicate)
[INFO] |  +- org.apache.hadoop:hadoop-hdfs:jar:2.7.3:compile
[INFO] |  |  \- (com.google.guava:guava:jar:27.0.1-jre:compile - version managed from 11.0.2; omitted for duplicate)
[INFO] |  +- org.apache.hadoop:hadoop-mapreduce-client-app:jar:2.7.3:compile
[INFO] |  |  \- org.apache.hadoop:hadoop-mapreduce-client-common:jar:2.7.3:compile
[INFO] |  |     +- org.apache.hadoop:hadoop-yarn-client:jar:2.7.3:compile
[INFO] |  |     |  \- (com.google.guava:guava:jar:27.0.1-jre:compile - version managed from 11.0.2; omitted for duplicate)
[INFO] |  |     \- org.apache.hadoop:hadoop-yarn-server-common:jar:2.7.3:compile
[INFO] |  |        \- (com.google.guava:guava:jar:27.0.1-jre:compile - version managed from 11.0.2; omitted for duplicate)
[INFO] |  +- org.apache.hadoop:hadoop-yarn-api:jar:2.7.3:compile
[INFO] |  |  \- (com.google.guava:guava:jar:27.0.1-jre:compile - version managed from 11.0.2; omitted for duplicate)
[INFO] |  \- org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.7.3:compile
[INFO] |     \- org.apache.hadoop:hadoop-yarn-common:jar:2.7.3:compile
[INFO] |        \- (com.google.guava:guava:jar:27.0.1-jre:compile - version managed from 11.0.2; omitted for duplicate)
[INFO] +- org.alluxio:alluxio-core-client-fs:jar:2.3.0:compile
[INFO] |  +- (com.google.guava:guava:jar:27.0.1-jre:compile - version managed from 11.0.2; omitted for duplicate)
[INFO] |  +- io.grpc:grpc-api:jar:1.28.1:compile
[INFO] |  |  \- (com.google.guava:guava:jar:27.0.1-jre:compile - version managed from 28.1-android; omitted for duplicate)
[INFO] |  +- org.alluxio:alluxio-core-base:jar:2.3.0:compile
[INFO] |  |  \- (com.google.guava:guava:jar:27.0.1-jre:compile - version managed from 28.1-android; omitted for duplicate)
[INFO] |  \- org.alluxio:alluxio-core-transport:jar:2.3.0:compile
[INFO] |     +- (com.google.guava:guava:jar:27.0.1-jre:compile - version managed from 28.1-android; omitted for duplicate)
[INFO] |     \- io.grpc:grpc-protobuf:jar:1.28.1:compile
[INFO] |        +- (com.google.guava:guava:jar:27.0.1-jre:compile - version managed from 28.1-android; omitted for duplicate)
[INFO] |        \- io.grpc:grpc-protobuf-lite:jar:1.28.1:compile
[INFO] |           \- (com.google.guava:guava:jar:27.0.1-jre:compile - version managed from 28.1-android; omitted for duplicate)
[INFO] +- org.alluxio:alluxio-core-common:jar:2.3.0:compile
[INFO] |  +- (com.google.guava:guava:jar:27.0.1-jre:compile - version managed from 28.1-android; omitted for duplicate)
[INFO] |  \- org.apache.curator:curator-client:jar:4.2.0:compile
[INFO] |     \- (com.google.guava:guava:jar:27.0.1-jre:compile - version managed from 28.1-android; omitted for duplicate)
[INFO] \- com.google.guava:guava-testlib:jar:27.0.1-jre:test
[INFO]    \- (com.google.guava:guava:jar:27.0.1-jre:test - version managed from 28.1-android; omitted for duplicate)

	</description>
	<comments>
		<comment id='1' author='Bing-ok' date='2020-07-10T09:17:48Z'>
		Could you help me review this issue?  Thanks! &lt;denchmark-link:https://github.com/ZacBlanco&gt;@ZacBlanco&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='Bing-ok' date='2020-07-10T22:58:22Z'>
		thanks for reporting this issue &lt;denchmark-link:https://github.com/Bing-ok&gt;@Bing-ok&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='Bing-ok' date='2020-07-11T06:38:42Z'>
		&lt;denchmark-link:https://github.com/apc999&gt;@apc999&lt;/denchmark-link&gt;
  Do you agree with the solution? Could I pull a request to fix it?
		</comment>
		<comment id='4' author='Bing-ok' date='2020-08-10T06:44:07Z'>
		&lt;denchmark-link:https://github.com/Bing-ok&gt;@Bing-ok&lt;/denchmark-link&gt;
 we recently upgraded hdfs client version, please let me know if there is more to be done about this ticket? otherwise i will close it in a week.
		</comment>
		<comment id='5' author='Bing-ok' date='2020-08-12T01:44:17Z'>
		Thanks for your support!
		</comment>
	</comments>
</bug>