<bug id='9194' author='liuzx32' open_date='2019-06-03T06:40:42Z' closed_time='2019-06-17T06:45:15Z'>
	<summary>SparkSQL can't access alluxio by spark-sql shell (not spark-shell).</summary>
	<description>
Alluxio Version:

alluxio-1.8.1

Describe the bug
1.SparkSQL can't access alluxio by sparksql shell (not spark-shell).
2.Refer the Additional context below.
To Reproduce

bin/spark-sql enter shell env
alter table tableName add partition(...) location alluxio:///path

Expected behavior
SparkSQL can't access alluxio by sparksql shell (not spark-shell).
Additional context
19/06/03 14:32:18 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
spark-sql&gt;
&gt;
&gt;
&gt; alter table  dwh_kylin.dmp_user_info_daily add partition(date_str='20190513') location 'alluxio:///mob-emr-test/kylin/olap/dmp_user_info_daily/20190513';
[GC (Allocation Failure) [ParNew: 71723K-&gt;6897K(72512K), 0.0160423 secs] 134997K-&gt;72858K(233664K), 0.0160907 secs] [Times: user=0.03 sys=0.01, real=0.02 secs]
[GC (Allocation Failure) [ParNew: 71409K-&gt;4777K(72512K), 0.0120276 secs] 137370K-&gt;72923K(233664K), 0.0120703 secs] [Times: user=0.05 sys=0.00, real=0.01 secs]
[GC (Allocation Failure) [ParNew: 69289K-&gt;3944K(72512K), 0.0038203 secs] 137435K-&gt;72090K(233664K), 0.0038633 secs] [Times: user=0.01 sys=0.00, real=0.01 secs]
19/06/03 14:32:24 ERROR metadata.Hive: MetaException(message:java.io.IOException: Got exception: org.apache.hadoop.fs.UnsupportedFileSystemException No FileSystem for scheme "alluxio")
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$add_partitions_req_result$add_partitions_req_resultStandardScheme.read(ThriftHiveMetastore.java:47626)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$add_partitions_req_result$add_partitions_req_resultStandardScheme.read(ThriftHiveMetastore.java:47585)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$add_partitions_req_result.read(ThriftHiveMetastore.java:47508)
at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_add_partitions_req(ThriftHiveMetastore.java:1563)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.add_partitions_req(ThriftHiveMetastore.java:1550)
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.add_partitions(HiveMetaStoreClient.java:568)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
at com.sun.proxy.$Proxy5.add_partitions(Unknown Source)
at org.apache.hadoop.hive.ql.metadata.Hive.createPartitions(Hive.java:1710)
at org.apache.spark.sql.hive.client.Shim_v0_13.createPartitions(HiveShim.scala:503)
at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createPartitions$1.apply$mcV$sp(HiveClientImpl.scala:534)
at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createPartitions$1.apply(HiveClientImpl.scala:534)
at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createPartitions$1.apply(HiveClientImpl.scala:534)
at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:278)
at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:216)
at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:215)
at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:261)
at org.apache.spark.sql.hive.client.HiveClientImpl.createPartitions(HiveClientImpl.scala:533)
at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createPartitions$1.apply$mcV$sp(HiveExternalCatalog.scala:945)
at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createPartitions$1.apply(HiveExternalCatalog.scala:928)
at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createPartitions$1.apply(HiveExternalCatalog.scala:928)
at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
at org.apache.spark.sql.hive.HiveExternalCatalog.createPartitions(HiveExternalCatalog.scala:928)
at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createPartitions(SessionCatalog.scala:798)
at org.apache.spark.sql.execution.command.AlterTableAddPartitionCommand.run(ddl.scala:448)
at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:190)
at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:190)
at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)
at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)
at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)
at org.apache.spark.sql.Dataset.(Dataset.scala:190)
at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:75)
at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:62)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:364)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:272)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
	</description>
	<comments>
		<comment id='1' author='liuzx32' date='2019-06-17T06:45:15Z'>
		Mybe the case is from the environment.
		</comment>
	</comments>
</bug>