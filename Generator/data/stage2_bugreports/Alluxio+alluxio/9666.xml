<bug id='9666' author='cheyang' open_date='2019-08-08T00:46:21Z' closed_time='2019-10-01T19:34:43Z'>
	<summary>worker crash when copying the data to local</summary>
	<description>
Alluxio Version:
2.1.0-SNAPSHOT
Describe the bug
Creating a large file which is more than than size of cache layer.Setting the file in alluxio as pin, then run copyToLocal. The worker is crash
To Reproduce
Steps to reproduce the behavior (as minimally and precisely as possible)

Create the master and worker with docker

&lt;denchmark-code&gt;$ docker network create alluxio_nw
$ docker volume create ufs
$ docker volume create journal
$ docker run -d \
  -p 19999:19999 \
  --net=alluxio_nw \
  --name=alluxio-master \
  -e ALLUXIO_JAVA_OPTS="-Dalluxio.master.hostname=alluxio-master  " \
  -v journal:/opt/alluxio/journal \
  -v ufs:/opt/alluxio/underFSStorage \
  alluxio/alluxio:2.1.0-SNAPSHOT master
$ docker run -d \
  --net=alluxio_nw \
  --name=alluxio-worker \
  --shm-size=1G \
  -v ufs:/opt/alluxio/underFSStorage \
  -v journal:/opt/alluxio/journal \
  -e ALLUXIO_JAVA_OPTS="-Dalluxio.worker.memory.size=1G -Dalluxio.master.hostname=alluxio-master" \
  alluxio/alluxio:2.1.0-SNAPSHOT worker
&lt;/denchmark-code&gt;


jump into the worker, create a file which is 2048M

&lt;denchmark-code&gt;docker exec -it alluxio-worker bash
cd /opt/alluxio/underFSStorage/
dd if=/dev/zero of=hello.txt bs=2048M count=1
&lt;/denchmark-code&gt;

3.change to the master, sync the data
&lt;denchmark-code&gt;/opt/alluxio/bin/alluxio fs ls   -Dalluxio.user.file.metadata.sync.interval=0 /
-rw-r--r--  root           root                2147479552       PERSISTED 08-07-2019 23:41:21:315   0% /hello.txt
&lt;/denchmark-code&gt;

4.pin the file /hello.txt
&lt;denchmark-code&gt;/opt/alluxio/bin/alluxio fs pin /hello.txt
File '/hello.txt' was successfully pinned.
&lt;/denchmark-code&gt;


run copyToLocal inside the master

&lt;denchmark-code&gt; /opt/alluxio/bin/alluxio fs copyToLocal /hello.txt /
Channel authentication failed. ChannelId: 9f4990e4-9e7c-4b03-a59f-5484db296d48, AuthType: SIMPLE, Target: 694fc63e18f3:29999, Error: alluxio.exception.status.UnavailableException: io exception
&lt;/denchmark-code&gt;


Check the workers log:

&lt;denchmark-code&gt;2019-08-07 23:42:18,400 WARN  UnderFileSystemBlockReader - Failed to cache data read from UFS (on transferTo()): Failed to request 1048576 bytes after 10,000ms to create blockId 50331650
Aug 07, 2019 11:42:18 PM io.grpc.netty.NettyServerHandler onStreamError
WARNING: Stream Error
io.netty.handler.codec.http2.Http2Exception$StreamException: Received DATA frame for an unknown stream 9
	at io.netty.handler.codec.http2.Http2Exception.streamError(Http2Exception.java:129)
	at io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder$FrameReadListener.shouldIgnoreHeadersOrDataFrame(DefaultHttp2ConnectionDecoder.java:531)
	at io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder$FrameReadListener.onDataRead(DefaultHttp2ConnectionDecoder.java:183)
	at io.netty.handler.codec.http2.Http2InboundFrameLogger$1.onDataRead(Http2InboundFrameLogger.java:48)
	at io.netty.handler.codec.http2.DefaultHttp2FrameReader.readDataFrame(DefaultHttp2FrameReader.java:421)
	at io.netty.handler.codec.http2.DefaultHttp2FrameReader.processPayloadState(DefaultHttp2FrameReader.java:251)
	at io.netty.handler.codec.http2.DefaultHttp2FrameReader.readFrame(DefaultHttp2FrameReader.java:160)
	at io.netty.handler.codec.http2.Http2InboundFrameLogger.readFrame(Http2InboundFrameLogger.java:41)
	at io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder.decodeFrame(DefaultHttp2ConnectionDecoder.java:118)
	at io.netty.handler.codec.http2.Http2ConnectionHandler$FrameDecoder.decode(Http2ConnectionHandler.java:390)
	at io.netty.handler.codec.http2.Http2ConnectionHandler.decode(Http2ConnectionHandler.java:450)
	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:502)
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:441)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:278)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1434)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:965)
	at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:799)
	at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:421)
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:321)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
	at java.lang.Thread.run(Thread.java:748)

#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGBUS (0x7) at pc=0x00007ff3fb79c045, pid=253, tid=0x00007ff390e73ae8
#
# JRE version: OpenJDK Runtime Environment (8.0_171-b11) (build 1.8.0_171-b11)
# Java VM: OpenJDK 64-Bit Server VM (25.171-b11 mixed mode linux-amd64 compressed oops)
# Derivative: IcedTea 3.8.0
# Distribution: Custom build (Wed Jun 13 18:28:11 UTC 2018)
# Problematic frame:
# V  [libjvm.so+0x4e0045]  JVM_FindSignal+0x6c77c
#
# Core dump written. Default location: /opt/alluxio-2.1.0-SNAPSHOT/core or core.253
#
# An error report file with more information is saved as:
# /opt/alluxio-2.1.0-SNAPSHOT/hs_err_pid253.log
integration/docker/bin/alluxio-worker.sh: line 34:   253 Segmentation fault      (core dumped) ${JAVA} -cp ${ALLUXIO_SERVER_CLASSPATH} ${ALLUXIO_WORKER_JAVA_OPTS} alluxio.worker.AlluxioWorker
&lt;/denchmark-code&gt;

The core dump file is big. Let me know where I can upload it. Thanks.
Expected behavior
A clear and concise description of what you expected to happen.
Urgency
Describe the impact and urgency of the bug.
Additional context
Add any other context about the problem here.
	</description>
	<comments>
		<comment id='1' author='cheyang' date='2019-08-08T03:00:03Z'>
		&lt;denchmark-link:https://github.com/cheyang&gt;@cheyang&lt;/denchmark-link&gt;
 Thanks for reporting the issue! Can you please also post the contents of ?
		</comment>
		<comment id='2' author='cheyang' date='2019-08-08T05:50:22Z'>
		&lt;denchmark-link:https://github.com/Alluxio/alluxio/files/3480022/hs_err_pid253.log&gt;hs_err_pid253.log&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='cheyang' date='2019-08-08T06:38:26Z'>
		
Creating a large file which is more than than size of cache layer.

currently this is not supported, but the expected behavior should be write failure, rather than worker crash. We should definitely debug and fix the crash issue
		</comment>
		<comment id='4' author='cheyang' date='2019-08-08T08:17:28Z'>
		The dump file is about 36MB, if you let me know where I can put the core dump. I can help to upload.
		</comment>
	</comments>
</bug>