<bug id='151' author='hanpum' open_date='2016-02-23T06:44:04Z' closed_time='2017-07-06T09:20:03Z'>
	<summary>exception while training non-class::    LM Function: SetValue  -&amp;gt; Feature Not  Implemented</summary>
	<description>
hi, all, I'm train a non-class based LM with lstm, network is describe with NDL, run in CentOS Linux 7.1,  gcc version is 4.8.5, belows are my configuration and ndl:
&lt;denchmark-link:https://github.com/wxm71/attaches_for_issues/blob/master/lstm/rnn.config&gt;configuration file&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/wxm71/attaches_for_issues/blob/master/lstm/lstmNDL.txt&gt;ndl description file&lt;/denchmark-link&gt;

belows are my analysis
while running, output log is:
&lt;denchmark-code&gt;Inside File: Source/Math/Matrix.cpp  Line: 1238  Function: SetValue  -&gt; Feature Not Implemented.
About to throw exception 'Inside File: Source/Math/Matrix.cpp  Line: 1238  Function: SetValue  -&gt; Feature Not Implemented.
&lt;/denchmark-code&gt;

As I have add some print in code, so the line number is incorrect, the correction postion can located as blow:


file:  Source/Readers/LMSequenceReader/SequenceReader.cpp


function:  bool BatchSequenceReader::GetMinibatch(std::map&lt;std::wstring, Matrix*&gt;&amp; matrices)


code:
  for (size_t j = 0; j &lt; actualmbsize; ++j) // note: this is a loop over matrix columns, not time steps or parallel sequences
    {
        // vector of feature data goes into matrix column
        size_t idx = (size_t) m_featureData[j]; // one-hot index of the word, indexed by column (i.e. already interleaved)

        features.SetValue(idx, j, (ElemType) 1);

        // actual time position
        // size_t timeIdx = (size_t)j / mToProcess.size();
        // size_t uttIdx = (size_t)fmod(j, mToProcess.size()); // parallel-sequence index
    }

and the problem line is features.SetValue(idx, j, (ElemType) 1);


[CALL STACK] section print in log shows SetValue is refer to
Microsoft::MSR::CNTK::Matrix::SetValue(unsigned long, unsigned long, double)
which define as below:
&lt;denchmark-code&gt;template &lt;class ElemType&gt;
void Matrix&lt;ElemType&gt;::SetValue(const size_t rIdx, const size_t cIdx, ElemType val) 
{
    DISPATCH_MATRIX_ON_FLAG_USECPU_4BOTH(this,
                                         this,
                                         (*m_CPUMatrix)(rIdx, cIdx) = val, 
                                         NOT_IMPLEMENTED,
                                         m_CPUSparseMatrix-&gt;SetValue(rIdx, cIdx, val),
                                         NOT_IMPLEMENTED);
}
&lt;/denchmark-code&gt;

after extent this macro(defined in Source/Math/Matrix.cpp, line 31),  I guees the exeception is raise by NOT_IMPLEMENTED, namely, the 4'th and 6'th parameter of DISPATCH_MATRIX_ON_FLAG_USECPU_4BOTH, which is a block of expression execute while the feature's  dataLocation is CurrentDataLocation::GPU or CurrentDataLocation::BOTH
In my configure, deviceId="0", which mean using GPU instead of CPU, so, why execute this code that think GPU version SetValue is not implemented??? btw, what's mean of feature's datalocation and when it's value been set?
I'm a newbie of CNTK, Thanks for any help and suggestion,
	</description>
	<comments>
		<comment id='1' author='hanpum' date='2016-03-14T23:59:25Z'>
		Can you please share your log? It could be that CNTK tried to use GPU and then failed over to the CPU.
		</comment>
		<comment id='2' author='hanpum' date='2016-06-08T12:17:57Z'>
		I have the same Problem with SimpleNetworkBuilder (rnnType = SIMPLERNN)
I'm using CNTKTextFormatReader for my training data...
Here is the Log:
CNTKCommandTrainBegin: train
LockDevice: Locked GPU 0 to test availability.
LockDevice: Unlocked GPU 0 after testing.
LockDevice: Locked GPU 0 for exclusive use.
SimpleNetworkBuilder Using GPU 0
Creating virgin network.
Microsoft::MSR::CNTK::GPUMatrix::SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==8
Post-processing network...
5 roots:
AutoName0 = Mean()
AutoName1 = InvStdDev()
criterion = CrossEntropyWithSoftmax()
eval = ErrorPrediction()
outputs = Times()
Loop[0] --&gt; Loop_AutoName7 -&gt; 4 nodes
&lt;denchmark-code&gt;    AutoName3       AutoName4       AutoName6
    AutoName7
&lt;/denchmark-code&gt;

Validating network. 17 nodes to process in pass 1.
Validating --&gt; features = SparseInputValue() :  -&gt; [12 x *]
Validating --&gt; AutoName0 = Mean (features) : [12 x *] -&gt; [12]
Validating --&gt; AutoName1 = InvStdDev (features) : [12 x *] -&gt; [12]
Validating --&gt; labels = InputValue() :  -&gt; [5 x *]
Validating --&gt; W1 = LearnableParameter() :  -&gt; [5 x 20]
Validating --&gt; U0 = LearnableParameter() :  -&gt; [20 x 12]
Validating --&gt; AutoName2 = PerDimMeanVarNormalization (features, AutoName0, AutoName1) : [12 x *], [12], [12] -&gt; [12 x *]
Validating --&gt; AutoName5 = Times (U0, AutoName2) : [20 x 12], [12 x *] -&gt; [20 x *]
Validating --&gt; W0 = LearnableParameter() :  -&gt; [20 x 20]
Validating --&gt; AutoName4 = Times (W0, AutoName3) : [20 x 20], [20] -&gt; [20]
Validating --&gt; AutoName6 = Plus (AutoName5, AutoName4) : [20 x *], [20] -&gt; [20 x *]
Validating --&gt; AutoName7 = Sigmoid (AutoName6) : [20 x *] -&gt; [20 x *]
Validating --&gt; AutoName8 = Times (W1, AutoName7) : [5 x 20], [20 x *] -&gt; [5 x *]
Validating --&gt; criterion = CrossEntropyWithSoftmax (labels, AutoName8) : [5 x *], [5 x *] -&gt; [1]
Validating --&gt; eval = ErrorPrediction (labels, AutoName8) : [5 x *], [5 x *] -&gt; [1]
Validating --&gt; outputs = Times (W1, AutoName7) : [5 x 20], [20 x *] -&gt; [5 x *]
Validating network. 12 nodes to process in pass 2.
Validating --&gt; AutoName3 = PastValue (AutoName7) : [20 x *] -&gt; [20 x *]
Validating --&gt; AutoName4 = Times (W0, AutoName3) : [20 x 20], [20 x *] -&gt; [20 x *]
Validating network. 2 nodes to process in pass 3.
Validating network, final pass.
7 out of 17 nodes do not share the minibatch layout with the input data.
Post-processing network complete.
Created model with 17 nodes on GPU 0.
Training criterion node(s):
criterion = CrossEntropyWithSoftmax
Evaluation criterion node(s):
&lt;denchmark-code&gt;    eval = ErrorPrediction
&lt;/denchmark-code&gt;

Allocating matrices for forward and/or backward propagation.
Memory Sharing Structure:
0000000000000000: {[AutoName0 Gradient[12]] [AutoName1 Gradient[12]] [AutoName2 Gradient[12 x *]] [eval Gradient[1]] [features Gradient[12 x *]] [labels Gradient[5 x *]] [outputs Gradient[5 x *]] }
0000002E9947A340: {[AutoName6 Value[20 x *]] }
0000002E9947A3E0: {[AutoName7 Value[20 x *]] }
0000002E9947A520: {[criterion Gradient[1]] }
0000002E9947A660: {[AutoName7 Gradient[20 x *]] }
0000002E9947B100: {[AutoName6 Gradient[20 x *]] [AutoName8 Value[5 x *]] }
0000002E9947B920: {[AutoName3 Gradient[20 x *]] }
0000002E9947BB00: {[AutoName5 Gradient[20 x *]] [AutoName8 Gradient[5 x *]] }
0000002E9947BBA0: {[W0 Gradient[20 x 20]] }
0000002E9947BCE0: {[W1 Gradient[5 x 20]] }
0000002E9947BD80: {[AutoName4 Gradient[20 x *]] }
0000002EFE91B7E0: {[features Value[12 x *]] }
0000002EFF216530: {[AutoName5 Value[20 x *]] }
0000002EFF2165D0: {[AutoName4 Value[20 x *]] }
0000002EFF2168F0: {[W1 Value[5 x 20]] }
0000002EFF216D50: {[U0 Value[20 x 12]] }
0000002EFF216FD0: {[W0 Value[20 x 20]] }
0000002EFF217110: {[AutoName3 Value[20 x *]] [U0 Gradient[20 x 12]] }
0000002EFF217390: {[AutoName1 Value[12]] }
0000002EFF217750: {[labels Value[5 x *]] }
0000002EFF217ED0: {[AutoName0 Value[12]] }
0000002EFF2180B0: {[eval Value[1]] }
0000002EFF218150: {[criterion Value[1]] }
0000002EFF2181F0: {[outputs Value[5 x *]] }
0000002EFF218290: {[AutoName2 Value[12 x *]] }
Precomputing --&gt; 2 PreCompute nodes found.
&lt;denchmark-code&gt;    AutoName0 = Mean()
    AutoName1 = InvStdDev()
&lt;/denchmark-code&gt;

Inside File: Matrix.cpp  Line: 1270  Function: Microsoft::MSR::CNTK::Matrix::SetValue  -&gt; Feature Not Implemented.
[CALL STACK]
&gt; Microsoft::MSR::CNTK::ReaderShim::  FillMatrixFromStream
- Microsoft::MSR::CNTK::ReaderShim::  GetMinibatch
- Microsoft::MSR::CNTK::DataReader::  GetMinibatch
- Microsoft::MSR::CNTK::DataReaderHelpers::GetMinibatchIntoNetwork
- Microsoft::MSR::CNTK::SGD::  PreCompute
- Microsoft::MSR::CNTK::SGD::  TrainOrAdaptModel
- Microsoft::MSR::CNTK::SGD::  Train
- DoTrainMicrosoft::MSR::CNTK::ConfigParameters,double
- DoCommands
- wmainOldCNTKConfig
- wmain1
- wmain
- __tmainCRTStartup
- BaseThreadInitThunk
- RtlUserThreadStart
EXCEPTION occurred: Inside File: Matrix.cpp  Line: 1270  Function: Microsoft::MSR::CNTK::Matrix::SetValue  -&gt; Feature Not Implemented.
Changing precision from double to float doesn't change anything. Also by using CPU (-1) instead of GPU(auto) the same Exception.
Any advice apreciated! Thanks!
EDIT: The configFile: &lt;denchmark-link:https://github.com/Microsoft/CNTK/files/304850/RNN.txt&gt;RNN.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='hanpum' date='2016-06-17T07:03:42Z'>
		I got the same error with the SIMPLERNN RnnType. Did you already solve that issue &lt;denchmark-link:https://github.com/madingo87&gt;@madingo87&lt;/denchmark-link&gt;
 ?
		</comment>
		<comment id='4' author='hanpum' date='2016-06-17T17:31:49Z'>
		Yes and no... i wrote my own rnn in BrainScript! This seems to work, at least it trains as expected!
I thiks it's a bug in the SimpleNetworkBuilder...
Here's the Script (.cntk), hope this helps!
&lt;denchmark-link:https://github.com/Microsoft/CNTK/files/320991/RNN.txt&gt;RNN.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='hanpum' date='2016-06-20T07:21:10Z'>
		great, thank you. i highly appriciate it.
		</comment>
		<comment id='6' author='hanpum' date='2017-04-12T09:22:22Z'>
		this is  a bug  ,you can fix it by yourself   as follows:
SimpleNetworkBuilder.cpp:174
ComputationNetworkPtr SimpleNetworkBuilder::BuildRNNFromDescription()
{
ComputationNetworkBuilder builder(*m_net);
if (m_net-&gt;GetTotalNumberOfNodes() &lt; 1) // not built yet
{
unsigned long randomSeed = 1;
&lt;denchmark-code&gt;    size_t numHiddenLayers = m_layerSizes.size() - 2;

    size_t numRecurrentLayers = m_recurrentLayers.size();

    ComputationNodePtr input, w, b, u, pastValue, output, label, prior;

	//add by ljh
	if (m_sparse_input)
		input = builder.CreateSparseInputNode(L"features", m_layerSizes[0]);
	else
		input = builder.CreateInputNode(L"features", m_layerSizes[0]);

    //input = builder.CreateSparseInputNode(L"features", m_layerSizes[0]);
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='hanpum' date='2017-07-06T09:20:03Z'>
		A lot of improvements made since then. Please, re-open if it's still an issue
		</comment>
	</comments>
</bug>