<bug id='127' author='BorisJineman' open_date='2016-02-17T04:15:04Z' closed_time='2016-03-16T16:20:04Z'>
	<summary>Some change between cuDNN 4.0 rc and cnDNN 4.0 prod will cause the build fault.</summary>
	<description>
Hi Sir,
The NVIDIA just release the cuDNN 4.0 prod version. One of the function was changed in the last version.
cudnnBatchNormalizationBackward
In the last version of the cuDNN, NVIDIA's developer add two new parameters into this function. And that will cause our build fault.
The currently define of this function is as bellow:
/* Performs backward pass of Batch Normalization layer. Returns x gradient,
* bnScale gradient and bnBias gradient */
cudnnStatus_t CUDNNWINAPI cudnnBatchNormalizationBackward(
                                cudnnHandle_t                       handle,
                                cudnnBatchNormMode_t                mode,
                                const void                         *alphaDataDiff,
                                const void                         *betaDataDiff,
                                const void                         *alphaParamDiff,
                                const void                         *betaParamDiff,
                                const cudnnTensorDescriptor_t       xDesc, // same desc for x, dx, dy
                                const void                         *x,
                                const cudnnTensorDescriptor_t       dyDesc,
                                const void                         *dy,
                                const cudnnTensorDescriptor_t       dxDesc,
                                void                               *dx,
                                /* Shared tensor desc for the 4 tensors below */
                                const cudnnTensorDescriptor_t       dBnScaleBiasDesc,
                                const void                         *bnScale, // bnBias doesn't affect backpropagation
                                /* scale and bias diff are not backpropagated below this layer */
                                void                               *dBnScaleResult,
                                void                               *dBnBiasResult,
                                /* Same epsilon as forward pass */
                                double                              epsilon,

                                /* Optionally cached intermediate results from
                                   forward pass */
                                const void                         *savedMean,
                                const void                         *savedInvVariance );

Here is the release note,

UPDATES SINCE RELEASE CANDIDATE
The API of cudnnBatchNormalizationBackward has been changed to include an additional set of scaling parameters (alphaParamsDiff and betaParamsDiff) applied to the dBnScaleResult and dBnBiasResult outputs of the function.
The prior restriction of batch size 512 in all Batch Normalization routines has been removed.
Numerical stability and performance of cudnnBatchNormalizationBackward in some cases has been improved.
Performance of cudnnConvolutionBackwardFilter when using Algo 1 has been improved for some cases. This code path now also requires a workspace.

So here comes the question, could i just set the new parameters as "&amp;C::One"?
Before:
            CUDNN_CALL(cudnnBatchNormalizationBackward(m_cudnn, mode, &amp;C::One, &amp;C::One, t(inT), ptr(in), t(inT), ptr(srcGrad), t(inT), ptr(grad),
                                                       t(scaleBiasT), ptr(scale), ptr(scaleGrad), ptr(biasGrad), CUDNN_BN_MIN_EPSILON, ptr(saveMean), ptr(saveInvStdDev)));
After:
            CUDNN_CALL(cudnnBatchNormalizationBackward(m_cudnn, mode, &amp;C::One, &amp;C::One, &amp;C::One, &amp;C::One, t(inT), ptr(in), t(inT), ptr(srcGrad), t(inT), ptr(grad),
                                                       t(scaleBiasT), ptr(scale), ptr(scaleGrad), ptr(biasGrad), CUDNN_BN_MIN_EPSILON, ptr(saveMean), ptr(saveInvStdDev)));
Peace and Long Life,
Boris
	</description>
	<comments>
		<comment id='1' author='BorisJineman' date='2016-02-17T06:29:47Z'>
		Thus, Should I submit a pull request about this issue?
		</comment>
		<comment id='2' author='BorisJineman' date='2016-02-17T11:52:34Z'>
		&lt;denchmark-link:https://github.com/BorisJineman&gt;@BorisJineman&lt;/denchmark-link&gt;
: please do submit a pull request. We will need to figure out when to do the switch from RC to production (there's documentation changes and test setup changes that need to be done), so it probably won't be merged right away. Anyway it will be good to have this ready.
Thanks, Mark
		</comment>
		<comment id='3' author='BorisJineman' date='2016-02-17T12:55:14Z'>
		Hi Sir,
I just submit a pull request, about this cuDNN changed. If there is any needing of me, please feel free to let me know.
&lt;denchmark-link:https://github.com/microsoft/CNTK/pull/130&gt;#130&lt;/denchmark-link&gt;

Peace and Long Life,
Boris
		</comment>
		<comment id='4' author='BorisJineman' date='2016-02-17T17:08:50Z'>
		Note that CNTK currently has two batch normalization implementations: cuDNN-based (which uses RC bits) and its own which does not have limitations of cuDNN RC (e.g. minibatch size) and other issues. CNTK implementation is currently a default implementation, but you can switch to cuDNN if you set engine attribute of BN node to "cudnn", e.g.:
bn = BatchNormalization(t, sc, b, m, isd, eval = false, spatial = false, expAvgFactor = expAvg, engine="cudnn")
		</comment>
		<comment id='5' author='BorisJineman' date='2016-03-16T16:20:04Z'>
		Latest master version should work both with RC and prod versions of cuDNN 4.0
		</comment>
	</comments>
</bug>