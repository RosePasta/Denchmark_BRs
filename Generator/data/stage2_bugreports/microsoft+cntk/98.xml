<bug id='98' author='rold2007' open_date='2016-02-07T19:23:39Z' closed_time='2016-03-14T23:17:15Z'>
	<summary>Implement all CPU logic</summary>
	<description>
I tried running the MNIST example with "02_Convolution.cntk" and "03_ConvBatchNorm.cntk" on a CPU-only machine with the CPU binaries (running with "01_OneHidden.cntk" works fine). I hit a RunTime error in CNTK/Source/Math/ConvolutionEngine.cpp (line 301) saying "Not yet implemented.".
So:
1- It seems like the CPU version only support very basic models (no BatchNormalize). Do you plan to fully support CPU soon/at all ?
2- I've seen in many other parts of the code the use of the NOT_IMPLEMENTED macro. Would it be better to use this macro instead of the RuntimeError in ConvolutionEngine.cpp ?
	</description>
	<comments>
		<comment id='1' author='rold2007' date='2016-02-07T20:28:13Z'>
		Hello,
Yes, we plan to implement all functionality on CPU very soon. In fact, the only missing feature is batch norm. For the convolution, we have CPU implementation for non-cuDNN engine (aka legacy) so you can try building the code without cuDNN and see if that works. Or just wait a little bit until we complete our CPU implementation.
		</comment>
		<comment id='2' author='rold2007' date='2016-02-08T08:27:27Z'>
		&lt;denchmark-link:https://github.com/Alexey-Kamenev&gt;@Alexey-Kamenev&lt;/denchmark-link&gt;
 : Thanks for the workaround. Unfortunately I wasn't able to apply it. I tried many things to run without cuDNN but I still hit RuntimeError("Not yet implemented.").
I tried compiling without the CUDNN_PATH (USE_CUDNN is now undefined).
In the .cntk file I set: deviceId = -1 and imageLayout = "legacy"
Is there something I'm missing ? I couldn't see any logic in the code which would skip the NormalizeBatch() call.
		</comment>
		<comment id='3' author='rold2007' date='2016-02-22T22:27:49Z'>
		I'm able to get MNIST example with "02_Convolution.cntk" to run with the CPU version - in "macros.ndl" there are BatchNormalization versions of some layers (DnnBNReLULayer, ConvBNLayer) which are used in the 03_ConvBatchNorm.ndl, but not in 02_Convolution.cntk.  For me 02_Convolution.cntk doesn't create any BatchNormalizationNode and runs.
However, although it runs without any errors, it doesn't learn - the error rate stays around 90% through all 15 epochs.  When I run MNIST example 01_OneHidden.cntk on CPU it quickly reaches the 2.3% error mentioned in the docs.  Any idea what could be causing that, or where to start looking?  (I've tried both the published binaries, and building from source on 2 different windows PCs - same issue)
		</comment>
		<comment id='4' author='rold2007' date='2016-02-24T21:51:50Z'>
		Changing learningRatesPerMB from 0.5 to 0.05 fixed 02_Convolution.cntk on CPU for me, it now converges.
		</comment>
		<comment id='5' author='rold2007' date='2016-03-14T23:17:15Z'>
		It seems like this issue has been resolved. Please re-open if the problem persists.
		</comment>
	</comments>
</bug>