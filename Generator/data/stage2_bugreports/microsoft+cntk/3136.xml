<bug id='3136' author='kaituoxu' open_date='2018-04-18T02:49:45Z' closed_time='2018-08-02T07:06:15Z'>
	<summary>[Bug Report] CNTK training bug of multi-dimensional LSTM (e.g. TF-LSTM)</summary>
	<description>
When I used CNTK to implement Time Frequency LSTM (TF-LSTM [1], a 2-D LSTM), I noticed that there maybe exist a training bug under NDL or BrainScript as this issue says (&lt;denchmark-link:https://github.com/microsoft/CNTK/issues/738&gt;#738&lt;/denchmark-link&gt;
)
And I found a solution for this bug in this commit: &lt;denchmark-link:https://github.com/microsoft/CNTK/commit/f34b7d78f0f9fbfe889934cf68909cc12fe9526c&gt;f34b7d7&lt;/denchmark-link&gt;

The bug looks like: when training TF-LSTM, the weight update is executed multiple times on the same node in one mini-batch training.
The new bug I found is:
When I use this commit to solve the repetitive update problem and train TF-LSTM models, the loss curve of single-GPU training looks fine (loss decrease steadily), but the loss curve of BMUF multi-GPU training has a lot of jitter.
My partial experiment also shows that the loss curve of ASGD multi-GPU training looks fine (loss decrease steadily).
I'm working on fixing this TF-LSTM multi-GPU training problem between BMUF and ASGD, and I'm reading the source code to confirm the existence of this repetitive update problem.
I think it's important to confirm the existence of this repetitive update problem and merge the solution into master branch, or we can not correctly train any multi-dimensional LSTM using CNTK NDL or BrainScript, such as F-LSTM[2], TF-LSTM, and Grid-LSTM[3].
[1] &lt;denchmark-link:https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/TFLSTM.pdf&gt;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/TFLSTM.pdf&lt;/denchmark-link&gt;

[2] &lt;denchmark-link:https://www.microsoft.com/en-us/research/wp-content/uploads/2016/04/spectral_LSTM.pdf&gt;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/04/spectral_LSTM.pdf&lt;/denchmark-link&gt;

[3] &lt;denchmark-link:https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45401.pdf&gt;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45401.pdf&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='kaituoxu' date='2018-04-18T04:51:21Z'>
		The reason I didn't merge that fix is that learnable parameters should not duplicate in the learnableNodes in the first place. I suspect the bug really comes from &lt;denchmark-link:https://github.com/Microsoft/CNTK/blob/master/Source/ComputationNetworkLib/ComputationNetwork.cpp#L549-L568&gt;here&lt;/denchmark-link&gt;
, that when nodes being pushed to learnable parameters it’s not pushed in visited set.
Python should not have the problem though as it passed a dictionary of Parameter to gradient, where the uniqueness of dict key should prevent it from being updated twice. Otherwise, we’ll have all sorts of problems in parameter sharing cases.
		</comment>
		<comment id='2' author='kaituoxu' date='2018-06-01T03:33:35Z'>
		&lt;denchmark-link:https://github.com/KeDengMS&gt;@KeDengMS&lt;/denchmark-link&gt;
 Thanks for your help.
I tried to fix this bug in this commit &lt;denchmark-link:https://github.com/kaituoxu/CNTK/commit/cc387453174e47add2bea96e9b991994f9227c44&gt;https://github.com/kaituoxu/CNTK/commit/cc387453174e47add2bea96e9b991994f9227c44&lt;/denchmark-link&gt;

After this commit, the training process of TF-LSTM works fine.
And I write a bug solution report here:
&lt;denchmark-link:https://user-images.githubusercontent.com/14189886/40819760-41bd5fa4-658f-11e8-9500-f51999bd061e.png&gt;&lt;/denchmark-link&gt;

[6] &lt;denchmark-link:https://github.com/microsoft/CNTK/commit/5a0dd3e3b57e97649651bb3d60662a33af820611&gt;5a0dd3e&lt;/denchmark-link&gt;

[7] &lt;denchmark-link:https://github.com/Microsoft/CNTK/blob/master/Source/ComputationNetworkLib/ComputationNetwork.cpp#L549-L568&gt;https://github.com/Microsoft/CNTK/blob/master/Source/ComputationNetworkLib/ComputationNetwork.cpp#L549-L568&lt;/denchmark-link&gt;

[8] &lt;denchmark-link:https://github.com/Microsoft/CNTK/blob/binbzha/tf-lstm-refine/Source/ComputationNetworkLib/LinearAlgebraNodes.h#L743-L746&gt;https://github.com/Microsoft/CNTK/blob/binbzha/tf-lstm-refine/Source/ComputationNetworkLib/LinearAlgebraNodes.h#L743-L746&lt;/denchmark-link&gt;

[9] &lt;denchmark-link:https://github.com/Microsoft/CNTK/blob/binbzha/tf-lstm-refine/Source/ComputationNetworkLib/LinearAlgebraNodes.h#L762-L765&gt;https://github.com/Microsoft/CNTK/blob/binbzha/tf-lstm-refine/Source/ComputationNetworkLib/LinearAlgebraNodes.h#L762-L765&lt;/denchmark-link&gt;

[10] &lt;denchmark-link:https://github.com/Microsoft/CNTK/blob/binbzha/tf-lstm-refine/Source/ComputationNetworkLib/ComputationNetworkEvaluation.cpp#L1106-L1107&gt;https://github.com/Microsoft/CNTK/blob/binbzha/tf-lstm-refine/Source/ComputationNetworkLib/ComputationNetworkEvaluation.cpp#L1106-L1107&lt;/denchmark-link&gt;

[11] &lt;denchmark-link:https://github.com/microsoft/CNTK/commit/f34b7d78f0f9fbfe889934cf68909cc12fe9526c&gt;f34b7d7&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='kaituoxu' date='2018-08-02T07:27:28Z'>
		Thanks for the detailed report. Could you send a PR for this fix?
		</comment>
	</comments>
</bug>