<bug id='797' author='online-translation' open_date='2018-08-23T02:29:33Z' closed_time='2019-05-13T15:22:51Z'>
	<summary>convolution bug</summary>
	<description>
I am using the latest tensorlayer in linux system.
There are two bugs that I want to report:
(1) binary conv2d can not work!
(2) deform convolution can be used to train, but when evaluate, weights of offset parts were not included, which makes it impossible to load weight to evaluate.
	</description>
	<comments>
		<comment id='1' author='online-translation' date='2018-08-23T08:51:08Z'>
		Wow... This seems like a very huge bug.
Could you please give us a short snippet to test both cases. We will release a bug fix as soon as possible.
Thanks a lot for this bug report. Really appreciated ;)
		</comment>
		<comment id='2' author='online-translation' date='2018-08-24T02:48:18Z'>
		(1) Binary
Some code like this:
import tensorlayer as tl
import tensorflow as tf
from tensorlayer.layers import *

t_image = tf.placeholder(tf.float32, [None,128,128,1])
h_image = tf.placeholder(tf.float32, [None,128,128,1])
net = InputLayer(t_image)
net = BinaryConv2d(net)
sess = tf.InteractiveSession()
train_op = tf.train.AdamOptimizer(learning_rate=LR).minimize(cost, var_list=model.all_params)
tl.layers.initialize_global_variables(sess)
cost = tl.cost.mean_squared_error(net.outputs, h_image)
for i in range(Max_iter):
    tr, hr = dataloader.get_next_train_batch()
    cost = sess.run([train_op, net.outputs], feed_dict = {t_image:tr, h_image:hr})
This will give you some error like: tensor can not be gradient.
(2) for the second issue, I try to add code at line 148 in deformable_conv.py. And it works:
        self._add_layers(self.outputs)
        new_variables = get_collection_trainable(offset_layer.name)
        self._add_params(new_variables)
        
        if b_init:
            self._add_params([W, b])
        else:
            self._add_params(W)
		</comment>
		<comment id='3' author='online-translation' date='2018-08-24T05:48:17Z'>
		For the case 1), could you give the complete example code?
Some variables are not defined in the above sample code.
		</comment>
		<comment id='4' author='online-translation' date='2018-08-24T05:56:13Z'>
		Sorry for that. &lt;denchmark-link:https://github.com/lgarithm&gt;@lgarithm&lt;/denchmark-link&gt;
. Following is the test code.
import tensorlayer as tl
import tensorflow as tf
from tensorlayer.layers import *
import numpy as np
t_image = tf.placeholder(tf.float32, [None,128,128,1])
h_image = tf.placeholder(tf.float32, [None,128,128,1])
net = InputLayer(t_image)
net = BinaryConv2d(net)
sess = tf.InteractiveSession()
cost = tl.cost.mean_squared_error(net.outputs, h_image)
train_op = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost, var_list=net.all_params)
tl.layers.initialize_global_variables(sess)

for i in range(1):
    tr = np.random.randn(32,128,128,1)
    hr = np.random.randn(32,128,128,1)
    cost = sess.run([train_op, net.outputs], feed_dict = {t_image:tr, h_image:hr})
These codes will throw error like:

NotImplementedError                       Traceback (most recent call last)
 in ()
9 sess = tf.InteractiveSession()
10 cost = tl.cost.mean_squared_error(net.outputs, h_image)
---&gt; 11 train_op = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost, var_list=net.all_params)
12 tl.layers.initialize_global_variables(sess)
13
~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)
408
409     return self.apply_gradients(grads_and_vars, global_step=global_step,
--&gt; 410                                 name=name)
411
412   def compute_gradients(self, loss, var_list=None,
~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py in apply_gradients(self, grads_and_vars, global_step, name)
603           scope_name = var.op.name
604         with ops.name_scope("update_" + scope_name), ops.colocate_with(var):
--&gt; 605           update_ops.append(processor.update_op(self, grad))
606       if global_step is None:
607         apply_updates = self._finish(update_ops, name)
~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py in update_op(self, optimizer, g)
187
188   def update_op(self, optimizer, g):
--&gt; 189     raise NotImplementedError("Trying to update a Tensor ", self._v)
190
191
NotImplementedError: ('Trying to update a Tensor ', &lt;tf.Tensor 'binary_cnn2d/Sign:0' shape=(3, 3, 1, 32) dtype=float32&gt;)

		</comment>
		<comment id='5' author='online-translation' date='2018-08-24T06:06:12Z'>
		BinaryConv2d can't do backward, &lt;denchmark-link:https://github.com/XJTUWYD&gt;@XJTUWYD&lt;/denchmark-link&gt;
 is this by design?
		</comment>
		<comment id='6' author='online-translation' date='2018-08-24T09:08:33Z'>
		This is due to quantize op



tensorlayer/tensorlayer/layers/utils.py


         Line 345
      in
      69f3f36






 def quantize(x): 





def quantize(x):
    # ref: https://github.com/AngusG/tensorflow-xnor-bnn/blob/master/models/binary_net.py#L70
    #  https://github.com/itayhubara/BinaryNet.tf/blob/master/nnUtils.py
    with tf.get_default_graph().gradient_override_map({"Sign": "TL_Sign_QuantizeGrad"}):
        return tf.sign(x)
		</comment>
		<comment id='7' author='online-translation' date='2018-08-25T08:51:04Z'>
		The link of binary connect network is:&lt;denchmark-link:url&gt;https://arxiv.org/pdf/1511.00363.pdf&lt;/denchmark-link&gt;

In this paper, author only use binary weight in forward and backward propagation but when it comes to update author update the gradient to original weights.
def binarize(x):
g = tf.get_default_graph()
with ops.name_scope("Binarized") as name:
with g.gradient_override_map({"Sign": "Identity"}):
x=tf.clip_by_value(x,-1,1)
return tf.sign(x)
It is my implement, so is "TL_Sign_QuantizeGrad" same as  "Identity"?
&lt;denchmark-link:https://github.com/zsdonghao&gt;@zsdonghao&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='online-translation' date='2019-05-13T15:22:51Z'>
		fixed by 2.0.0 version
		</comment>
	</comments>
</bug>