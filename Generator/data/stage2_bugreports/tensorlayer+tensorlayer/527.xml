<bug id='527' author='DEKHTIARJonathan' open_date='2018-04-25T08:22:07Z' closed_time='2018-04-26T13:23:14Z'>
	<summary>Tensorlayer Conv Variables Name Changed: Checkpoint Backward Compatibility Break</summary>
	<description>
&lt;denchmark-h:h3&gt;Issue Description&lt;/denchmark-h&gt;

I have noticed that networks containing the layer: tl.layers.Conv2d have a comptibility break between TL 1.8.3 and 1.8.4.
Indeed, the variable names between TL 1.8.3 and TL 1.8.4 are not consistent and change with lead to a Variable not found error:
&lt;denchmark-code&gt;NotFoundError: Key convnet/h1/conv2d/bias not found in checkpoint
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Reproducible Code&lt;/denchmark-h&gt;

Alright, let's go for a very simple vanilla code:
a = tf.placeholder(tf.float32, [None, 64, 64, 3])
in_layer = tl.layers.InputLayer(a)

h1_layer = tl.layers.Conv2d(in_layer, 16)
h1_layer.print_params(details = False)
&lt;denchmark-h:h4&gt;Output with TL 1.8.4 and more&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;[TL] InputLayer  input: (?, 64, 64, 3)
[TL] Conv2d conv2d: n_filter:16 filter_size:(3, 3) strides:(1, 1) pad:SAME act:identity
[TL]   param   0: conv2d/kernel:0      (3, 3, 3, 16)      float32_ref
[TL]   param   1: conv2d/bias:0        (16,)              float32_ref
[TL]   num of params: 448
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Output with TL 1.8.3 and before&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;[TL] InputLayer  input: (?, 64, 64, 3)
[TL] Conv2dLayer conv2d: shape:(3, 3, 3, 16) strides:(1, 1, 1, 1) pad:SAME act:identity
[TL]   param   0: conv2d/W_conv2d:0    (3, 3, 3, 16)      float32_ref
[TL]   param   1: conv2d/b_conv2d:0    (16,)              float32_ref
[TL]   num of params: 448
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Conclusion:&lt;/denchmark-h&gt;

As you can clearly see, the names do not match and thus it has become impossible to load any checkpoint saved with TL &lt;= 1.8.3 with newer version of TL and this is very sad from my point of view.
This must come from something we have modified, with the same version of TF we can produce both situation by just making TL version vary.
Is it something that we consider to be an error and want to rollback the change?
&lt;denchmark-h:h3&gt;How to prevent future similar incompatibilities.&lt;/denchmark-h&gt;

We should design a model including all possible layers saving it. Then during the unittest we try to load it.
if it works we are sure that nothing have broken.
	</description>
	<comments>
		<comment id='1' author='DEKHTIARJonathan' date='2018-04-25T08:52:28Z'>
		I just took a quick look, this is caused by
&lt;denchmark-link:https://github.com/tensorlayer/tensorlayer/blob/master/tensorlayer/layers/convolution.py#L1547&gt;https://github.com/tensorlayer/tensorlayer/blob/master/tensorlayer/layers/convolution.py#L1547&lt;/denchmark-link&gt;

it will be triggered when
&lt;denchmark-code&gt;        if tf.__version__ &gt; '1.5':
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/tensorlayer/tensorlayer/blob/master/tensorlayer/layers/convolution.py#L1541&gt;https://github.com/tensorlayer/tensorlayer/blob/master/tensorlayer/layers/convolution.py#L1541&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='DEKHTIARJonathan' date='2018-04-25T08:58:46Z'>
		Is it something that we should address ? It basically deprecated all the training I have ever did. And the work of everyone who used Convolutions...
I can relaunch my work. Maybe not everyone is willing to do so...
		</comment>
		<comment id='3' author='DEKHTIARJonathan' date='2018-04-26T12:50:54Z'>
		it is caused by using tf.layers ..
the easy way to do it is to save the network to TL .npz, and restore it in new version and save to TF .ckpt again ...
		</comment>
		<comment id='4' author='DEKHTIARJonathan' date='2018-04-26T12:56:45Z'>
		&lt;denchmark-link:https://github.com/zsdonghao&gt;@zsdonghao&lt;/denchmark-link&gt;
 I was just wondering if it was a volontary move. Because it completely breaks the backward compatibbility of all convnets.
If you think this should not be solved, you can close the issue.
		</comment>
		<comment id='5' author='DEKHTIARJonathan' date='2018-04-26T13:14:49Z'>
		&lt;denchmark-link:http://tensorlayer.readthedocs.io/en/latest/_modules/tensorlayer/layers/convolution.html#Conv2d&gt;http://tensorlayer.readthedocs.io/en/latest/_modules/tensorlayer/layers/convolution.html#Conv2d&lt;/denchmark-link&gt;

Is there any way to modify the name of the variables created by tf.layers ?..
		</comment>
		<comment id='6' author='DEKHTIARJonathan' date='2018-04-26T13:23:04Z'>
		I just had a look. It seems that no.
Wait in TF they have:

tf.nn.convolution
tf.nn.conv2d
tf.layers.conv2d (func)
tf.layers.Conv2D (class)
tf.contrib.layers.conv2d
tf.keras.layers.Conv2D

Why using one, when you can have six of them !!!
		</comment>
		<comment id='7' author='DEKHTIARJonathan' date='2018-04-26T13:57:35Z'>
		It can be solved with the following script:
import tensorflow as tf


def rename(checkpoint_dir, replace_list, dry_run):
    
    checkpoint = tf.train.get_checkpoint_state(checkpoint_dir)
    
    with tf.Session() as sess:
        for var_name, _ in tf.contrib.framework.list_variables(checkpoint_dir):
            
            is_renamed = False
            new_name = var_name
            
            for rename_var in replace_list:
                if rename_var[0] in var_name:
                    new_name = var_name.replace(rename_var[0], rename_var[1], 1)
                    is_renamed = True
            
            # Load the variable
            var = tf.contrib.framework.load_variable(checkpoint_dir, var_name)
            
            if is_renamed:
                if dry_run:
                    print('%s would be renamed to %s.' % (var_name, new_name))
                else:
                    print('Renaming %s to %s.' % (var_name, new_name))
                    # Rename the variable
            
            var = tf.Variable(var, name=new_name)

        if not dry_run:
            # Save the variables
            print("Saving Checkpoint...")
            saver = tf.train.Saver()
            sess.run(tf.global_variables_initializer())
            saver.save(sess, checkpoint.model_checkpoint_path)
            print("Checkpoint Saved!")


if __name__ == '__main__':
    replace_list = [
        ('W_conv2d', 'kernel'),
        ('b_conv2d', 'bias'),
    ]
        
    checkpoint_dir = "the/folder/relative_path/where/your/ckptfile/is_saved"
    dry_run = False # set to True to run a simulation and do not modify your checkpoint file

    rename(checkpoint_dir, replace_list, dry_run)
		</comment>
		<comment id='8' author='DEKHTIARJonathan' date='2018-04-26T13:59:13Z'>
		&lt;denchmark-link:https://github.com/lgarithm&gt;@lgarithm&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/luomai&gt;@luomai&lt;/denchmark-link&gt;
 Hi, is there some standard to put this kind of API into the library?  e.g.  , ?
		</comment>
		<comment id='9' author='DEKHTIARJonathan' date='2018-04-26T14:04:04Z'>
		I think the tl command is a good place to go.
Users can do
&lt;denchmark-code&gt;tl convert xxx.ckpt
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='DEKHTIARJonathan' date='2018-04-26T14:06:18Z'>
		I have created a batch version
import os

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

import tensorflow as tf

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}
tf.logging.set_verbosity(tf.logging.ERROR)

def rename(checkpoint_dir, replace_list, dry_run):
    
    checkpoint = tf.train.get_checkpoint_state(checkpoint_dir)
    
    renamed_vars = 0
    
    with tf.Session() as sess:
        for var_name, _ in tf.contrib.framework.list_variables(checkpoint_dir):
            
            is_renamed = False
            new_name = var_name
            
            for rename_var in replace_list:
                if rename_var[0] in var_name:
                    new_name = var_name.replace(rename_var[0], rename_var[1], 1)
                    is_renamed = True
                    renamed_vars += 1
            
            # Load the variable
            var = tf.contrib.framework.load_variable(checkpoint_dir, var_name)
            
            if is_renamed:
                if dry_run:
                    print('%s would be renamed to %s.' % (var_name, new_name))
                else:
                    print('Renaming %s to %s.' % (var_name, new_name))
                    # Rename the variable
            
            var = tf.Variable(var, name=new_name)

        if not dry_run and renamed_vars:
            # Save the variables
            print("Saving Checkpoint (%s)..." % checkpoint_dir)
            saver = tf.train.Saver()
            sess.run(tf.global_variables_initializer())
            saver.save(sess, checkpoint.model_checkpoint_path)
            print("Checkpoint Saved!")
            
        elif not renamed_vars:
            print("Checkpoint (%s) is already in correct format - Not Modified" % checkpoint_dir) 


if __name__ == '__main__':
    replace_list = [
        ('W_conv2d', 'kernel'),
        ('b_conv2d', 'bias'),
    ]
        
    checkpoint_dirs = [ckpt_dir for ckpt_dir in os.listdir() if os.path.isdir(ckpt_dir)]

    dry_run = False # set to True to run a simulation and do not modify your checkpoint file
    
    for ckpt_dir in checkpoint_dirs:
        rename(ckpt_dir, replace_list, dry_run)
		</comment>
		<comment id='11' author='DEKHTIARJonathan' date='2018-04-26T15:14:15Z'>
		&lt;denchmark-link:https://github.com/lgarithm&gt;@lgarithm&lt;/denchmark-link&gt;
 is correct. Any helper script shall go into the "tl cmd" module. "tl train" is a simple example how to add a new helper script.
		</comment>
	</comments>
</bug>