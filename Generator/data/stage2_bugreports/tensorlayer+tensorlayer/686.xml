<bug id='686' author='DEKHTIARJonathan' open_date='2018-06-07T16:39:36Z' closed_time='2018-06-12T08:24:42Z'>
	<summary>[BUG] - All tf.layers return empty parameters when they reuse parameters.</summary>
	<description>
This issue affect all tl.Layers relying on a tf.layers implementation.
The following Layers are concerned:

 tl.layers.Conv1d
 tl.layers.Conv2d
 tl.layers.DeConv2d
 tl.layers.DeConv3d
 tl.layers.SeparableConv1d
 tl.layers.SeparableConv2d
 tl.layers.MaxPool1d
 tl.layers.MeanPool1d
 tl.layers.MaxPool2d
 tl.layers.MeanPool2d
 tl.layers.MaxPool3d
 tl.layers.MeanPool3d

The following code is perfectly in TL==1.8.3 and start breaking with TL&gt;=1.84:
import tensorflow as tf
import tensorlayer as tl

def get_network_1d(inputs, reuse=False):

    with tf.variable_scope("my_bloody_1D_network", reuse=reuse):
        net = tl.layers.InputLayer(inputs)

        net = tl.layers.Conv1d(net, name="conv1d")  # 2 params
        if reuse: print("IS_PROBLEM FIXED (%s):" % net.name, len(net.all_params) == 2)
        net = tl.layers.SeparableConv1d(net, name="SeparableConv1d")  # 3 params
        if reuse: print("IS_PROBLEM FIXED (%s):" % net.name, len(net.all_params) == 5)
        net = tl.layers.MaxPool1d(net, 1, name="MaxPool1d")  # 0 params
        if reuse: print("IS_PROBLEM FIXED (%s):" % net.name, len(net.all_params) == 5)
        net = tl.layers.MeanPool1d(net, 1, name="MeanPool1d")  # 0 params
        if reuse: print("IS_PROBLEM FIXED (%s):" % net.name, len(net.all_params) == 5)

        return net


def get_network_2d(inputs, reuse=False):

    with tf.variable_scope("my_bloody_2D_network", reuse=reuse):
        net = tl.layers.InputLayer(inputs)

        net = tl.layers.Conv2d(net, name="conv2d")  # 2 params
        if reuse: print("IS_PROBLEM FIXED (%s):" % net.name, len(net.all_params) == 2)
        net = tl.layers.DeConv2d(net, name="deconv2d")  # 2 params
        if reuse: print("IS_PROBLEM FIXED (%s):" % net.name, len(net.all_params) == 4)
        net = tl.layers.SeparableConv2d(net, name="separableconv2d")  # 3 params
        if reuse: print("IS_PROBLEM FIXED (%s):" % net.name, len(net.all_params) == 7)
        net = tl.layers.MaxPool2d(net, (1, 1), name="MaxPool2d")  # 0 params
        if reuse: print("IS_PROBLEM FIXED (%s):" % net.name, len(net.all_params) == 7)
        net = tl.layers.MeanPool2d(net, (1, 1), name="MeanPool2d")  # 0 params
        if reuse: print("IS_PROBLEM FIXED (%s):" % net.name, len(net.all_params) == 7)

        return net

def get_network_3d(inputs, reuse=False):

    with tf.variable_scope("my_bloody_3D_network", reuse=reuse):
        net = tl.layers.InputLayer(inputs)

        net = tl.layers.Conv3dLayer(net, shape=(2, 2, 2, 3, 32), strides=(1, 2, 2, 2, 1), name="conv3d")  # 2 params
        if reuse: print("IS_PROBLEM FIXED (%s):" % net.name, len(net.all_params) == 2)
        net = tl.layers.DeConv3d(net, name="DeConv3d")  # 2 params
        if reuse: print("IS_PROBLEM FIXED (%s):" % net.name, len(net.all_params) == 4)
        net = tl.layers.MaxPool3d(net, (1, 1, 1), name="MaxPool3d")  # 0 params
        if reuse: print("IS_PROBLEM FIXED (%s):" % net.name, len(net.all_params) == 4)
        net = tl.layers.MeanPool3d(net, (1, 1, 1), name="MeanPool3d")  # 0 params
        if reuse: print("IS_PROBLEM FIXED (%s):" % net.name, len(net.all_params) == 4)

        return net

if __name__ == "__main__":

    tf.logging.set_verbosity(tf.logging.WARN)

    if tl.__version__ &gt;= "1.8.6":
        tl.logging.set_verbosity(tl.logging.WARN)

    print("\n" + "#"*50)
    print("#"*19 + "  1D Layers  " + "#"*18)
    print("#"*50 + "\n")

    input_pl_train = tf.placeholder(tf.float32, [None, 32, 3])
    input_plh_test = tf.placeholder(tf.float32, [None, 32, 3])

    network_1 = get_network_1d(input_pl_train, reuse=False)
    network_2 = get_network_1d(input_plh_test, reuse=True)

    print("\n####################### network_1 - 1D: ##########################\n", network_1.all_params)
    print("\n####################### network_2 - 1D: ##########################\n", network_2.all_params)

    print("\n" + "#" * 50)
    print("#"*19 + "  2D Layers  " + "#"*18)
    print("#"*50 + "\n")

    input_pl_train = tf.placeholder(tf.float32, [None, 32, 32, 3])
    input_plh_test = tf.placeholder(tf.float32, [None, 32, 32, 3])

    network_1 = get_network_2d(input_pl_train, reuse=False)
    network_2 = get_network_2d(input_plh_test, reuse=True)

    print("\n####################### network_1 - 2D: ##########################\n", network_1.all_params)
    print("\n####################### network_2 - 2D: ##########################\n", network_2.all_params)

    print("\n" + "#" * 50)
    print("#"*19 + "  3D Layers  " + "#"*18)
    print("#"*50 + "\n")

    input_pl_train = tf.placeholder(tf.float32, [None, 32, 32, 32, 3])
    input_plh_test = tf.placeholder(tf.float32, [None, 32, 32, 32, 3])

    network_1 = get_network_3d(input_pl_train, reuse=False)
    network_2 = get_network_3d(input_plh_test, reuse=True)

    print("\n####################### network_1 - 3D: ##########################\n", network_1.all_params)
    print("\n####################### network_2 - 3D: ##########################\n", network_2.all_params)
	</description>
	<comments>
	</comments>
</bug>