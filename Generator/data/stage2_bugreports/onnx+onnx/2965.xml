<bug id='2965' author='przybyszewskiw' open_date='2020-08-19T11:40:19Z' closed_time='2020-08-31T22:59:04Z'>
	<summary>Shape inference on the Torchvisionâ€™s Mask R-CNN causes a segmentation fault</summary>
	<description>
&lt;denchmark-h:h1&gt;Bug Report&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Is the issue related to model conversion?&lt;/denchmark-h&gt;

Probably not.
&lt;denchmark-h:h3&gt;Describe the bug&lt;/denchmark-h&gt;

When I try to run shape inference on the Torchvision's Mask R-CNN it causes a segfault. However, check_model doesn't return any warning/exception.
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g. Linux Ubuntu 16.04): Linux Ubuntu 18.04.4
ONNX version (e.g. 1.7):  1.6.0
Python version: 3.6.10
PyTorch version: 1.6.0a0+9907a3e
Torchvision version: 0.7.0a0
GCC/Compiler version (if compiling from source): N/A
CMake version: 3.14.0
Protobuf version: 3.12.2
Visual Studio version (if applicable): N/A

&lt;denchmark-h:h3&gt;Reproduction instructions&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import torch
import torchvision
import onnx

model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)
model.eval()
x = [torch.rand(3, 800, 800)]
torch.onnx.export(model, x, "mask_rcnn.onnx", opset_version = 11)
onnx_model = onnx.load("mask_rcnn.onnx")
onnx.checker.check_model(onnx_model)
onnx.shape_inference.infer_shapes(onnx_model)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Shape inference doesn't cause any errors.
	</description>
	<comments>
		<comment id='1' author='przybyszewskiw' date='2020-08-19T18:15:48Z'>
		Hi &lt;denchmark-link:https://github.com/przybyszewskiw&gt;@przybyszewskiw&lt;/denchmark-link&gt;
,
Sorry that current shape inference does not catch the exception properly.
This segfault bug will be fixed in this PR: &lt;denchmark-link:https://github.com/onnx/onnx/pull/2783&gt;#2783&lt;/denchmark-link&gt;
.
After applying this PR, I tried to check this model with onnx.checker.check_model('mask_rcnn.onnx', full_check=True) and got the error as follows:
&lt;denchmark-code&gt;Invalid tensor data type.
&lt;/denchmark-code&gt;

For detailed error messages, I "shape_inference" the model and got the error as follows:
&lt;denchmark-code&gt;Shape inference error(s): (op_type:Gather, name:Gather_2359): [TypeInferenceError] Input 0 expected to have tensor type
(op_type:Gather, name:Gather_2360): [TypeInferenceError] Input 0 expected to have tensor type
(op_type:Slice, name:Slice_2389): [TypeInferenceError] Input 0 expected to have tensor type
(op_type:Slice, name:Slice_2398): [TypeInferenceError] Input 0 expected to have tensor type

&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='przybyszewskiw' date='2020-08-20T10:18:16Z'>
		Hi &lt;denchmark-link:https://github.com/jcwchen&gt;@jcwchen&lt;/denchmark-link&gt;
,
Thank you for your reply. I extended my script, so now it performs an inference in the  as well:
&lt;denchmark-code&gt;import torch
import torchvision
import onnxruntime
import numpy as np
import requests
from io import BytesIO
from PIL import Image

def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()

def get_image(url):
    response = requests.get(url)
    pil_image = Image.open(BytesIO(response.content)).convert("RGB")
    return torch.tensor(np.array(pil_image)).permute(2, 1, 0).to(dtype=torch.float)/255

model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)
model.eval()

url = "http://farm3.staticflickr.com/2469/3915380994_2e611b1779_z.jpg"
x = get_image(url)

torch_out = model([x])
torch.onnx.export(model, [x], "mask_rcnn.onnx", opset_version=11, input_names = ['x'])

ort_session = onnxruntime.InferenceSession("mask_rcnn.onnx")
ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}
ort_outs = ort_session.run(None, ort_inputs)

for i, key in enumerate(torch_out[0].keys()):
    np.testing.assert_allclose(to_numpy(torch_out[0][key]), ort_outs[i], rtol=1e-03, atol=2e-05)

print("Exported model passed the test!")
&lt;/denchmark-code&gt;

It seems like the exported model does the inference correctly, even though the type inference fails. Could you explain this inconsistency?
		</comment>
		<comment id='3' author='przybyszewskiw' date='2020-08-20T15:59:03Z'>
		Can you specify what the type inference failure is? As far I know, type inference of onnxruntime must finish successfully so it should print error messages if any.
		</comment>
		<comment id='4' author='przybyszewskiw' date='2020-08-20T17:19:55Z'>
		There is no error when I run the script above. I mentioned the type inference failure you wrote above. How is it that I can run inference on my model without any problem, but when I run shape_inference it segfaults because of TypeInferenceErrors?
		</comment>
		<comment id='5' author='przybyszewskiw' date='2020-08-20T17:39:05Z'>
		I think the reason is onnxruntime has its own shape_inference and it only uses part of onnx.shape_inference (node-level). If you use shape inference from onnxruntime successfully, the type inference of your model should be done.
		</comment>
		<comment id='6' author='przybyszewskiw' date='2020-08-21T11:10:26Z'>
		And what about shape and type inferences used in C api? Do they differ from Python ones?
I wrote a C++ program that tries to run the onnx model I exported before:
&lt;denchmark-code&gt;#include &lt;assert.h&gt;
#include &lt;onnxruntime_c_api.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;stdio.h&gt;


const OrtApi* g_ort = OrtGetApiBase()-&gt;GetApi(ORT_API_VERSION);

void CheckStatus(OrtStatus* status)
{
    if (status != NULL) {
      const char* msg = g_ort-&gt;GetErrorMessage(status);
      fprintf(stderr, "%s\n", msg);
      g_ort-&gt;ReleaseStatus(status);
      exit(1);
    }
}

int main(int argc, char* argv[]) {
  // initialize  enviroment
  OrtEnv* env;
  CheckStatus(g_ort-&gt;CreateEnv(ORT_LOGGING_LEVEL_WARNING, "test", &amp;env));

  // initialize session options if needed
  OrtSessionOptions* session_options;
  CheckStatus(g_ort-&gt;CreateSessionOptions(&amp;session_options));
  g_ort-&gt;SetIntraOpNumThreads(session_options, 1);

  // Sets graph optimization level
  g_ort-&gt;SetSessionGraphOptimizationLevel(session_options, ORT_ENABLE_BASIC);

  // create session and load model into memory
  OrtSession* session;
  const char* model_path = "/workspace/mask_rcnn.onnx";
  CheckStatus(g_ort-&gt;CreateSession(env, model_path, session_options, &amp;session));

  printf("done\n");
  return 0;
}
&lt;/denchmark-code&gt;

and it fails:
&lt;denchmark-code&gt;Node (ConstantOfShape_1888) Op (ConstantOfShape) [ShapeInferenceError] Invalid shape value: 0
&lt;/denchmark-code&gt;

Do you know why is that? Is there any way to run my ONNX model using C++, knowing that I can run it using Python?
		</comment>
		<comment id='7' author='przybyszewskiw' date='2020-08-24T23:56:32Z'>
		This is a bit strange. As far as I know, the inference used by onnxruntime is the same regardless of whether you use the python or C/C++ API. There is a slightly different implementation of the inference in onnxruntime and ONNX. But even here, if the behavior is different, then one of them needs to be fixed. This seems worth investigating.
		</comment>
		<comment id='8' author='przybyszewskiw' date='2020-08-31T22:59:04Z'>
		Thanks &lt;denchmark-link:https://github.com/gramalingam&gt;@gramalingam&lt;/denchmark-link&gt;
 for providing a PR to fix the type inference! This issue should be resolved by that PR. &lt;denchmark-link:https://github.com/przybyszewskiw&gt;@przybyszewskiw&lt;/denchmark-link&gt;
 if you still encounter the same error, please reopen it. Thank you.
		</comment>
		<comment id='9' author='przybyszewskiw' date='2020-09-01T04:50:03Z'>
		(This might be an onnxruntime issue)
&lt;denchmark-link:https://github.com/przybyszewskiw&gt;@przybyszewskiw&lt;/denchmark-link&gt;
 One more thing: the gap between Python API and C API for onnxruntime. This is not supposed to happen.

Could you try the following code on your end? Let's see whether it will encounter the same error as C API.

&lt;denchmark-code&gt;import onnxruntime as ort
so = onnxrt.SessionOptions()
so.graph_optimization_level = onnxrt.GraphOptimizationLevel.ORT_ENABLE_BASIC
onnxrt.InferenceSession("mask_rcnn.onnx") 
&lt;/denchmark-code&gt;


Did you build C API and Python API with the same source code? What is your onnxruntime and onnx version?

Thank you for the help.
		</comment>
		<comment id='10' author='przybyszewskiw' date='2020-09-02T06:18:39Z'>
		&lt;denchmark-link:https://github.com/jcwchen&gt;@jcwchen&lt;/denchmark-link&gt;
 after the PR I don't have any problems with the  function in Python API. Thanks for that!
When it comes to this gap between APIs:

I don't encounter any bug while running your code - modifying SessionOptions doesn't change anything;
Yes, my C and Python APIs are built with the same source code. My onnx version is 1.6.0 and onnxruntime version is 1.3.0.

Can you reproduce that bug? Should I start a new issue for it?
		</comment>
		<comment id='11' author='przybyszewskiw' date='2020-09-02T06:40:15Z'>
		Thanks for the answer. My previous testing versions of onnxruntime and onnx are different from yours and I will try to reproduce it. Could you try to test the same model with  newer onnxruntime 1.4.0 and onnx 1.7.0 if possible?
If the Python API and C API still behave differently, feel free to open an issue under onnxruntime. Let's track this problem there. Thanks!
		</comment>
		<comment id='12' author='przybyszewskiw' date='2020-09-02T07:24:01Z'>
		It looks like when I use onnx 1.7.0 and onnxruntime 1.4.0, model loads correctly in both APIs. Thanks for your help!
		</comment>
	</comments>
</bug>