<bug id='3128' author='AKAlilghost' open_date='2020-11-26T03:52:40Z' closed_time='2020-11-26T06:11:42Z'>
	<summary>Got different inference time of one net struct with different weights</summary>
	<description>
&lt;denchmark-h:h1&gt;Got different inference time of one net struct with different weights&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Is the issue related to model conversion?&lt;/denchmark-h&gt;

pytorch
&lt;denchmark-h:h3&gt;Describe the bug&lt;/denchmark-h&gt;


Firstly I save two pytorch model of same net struct with different weights.
Then I convert the two pytorch model to onnx .
I try to test inference time of two onnx model, but find there is twice the speed difference.

&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (Linux Ubuntu 16.04):
ONNX version (1.1):
Python version: 3.6

&lt;denchmark-h:h3&gt;Reproduction instructions&lt;/denchmark-h&gt;


Describe the code to reproduce the behavior.

&lt;denchmark-code&gt;######   convert pytorch model to onxx   #######
import sys
sys.path.append('../')
import os
import torch
import argparse
import torch.onnx as torch_onnx
from torch.autograd import Variable
import numpy as np

from model_design.m_siamese_multibrahch_onnx_2input import SiameseNet
from checkpoint_mgr.checkpoint_mgr import CheckpointMgr

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--pretrained_model', default='output', type=str)
    args = parser.parse_args()
    return args

args = parse_args()
pretrained_model = args.pretrained_model

model = SiameseNet(n_cls=6)
checkpoint_op = CheckpointMgr(ckpt_dir=os.path.dirname(pretrained_model))
checkpoint_op.load_checkpoint(model=model, warm_load=True)
model.eval()

model_onnx_path = 'onnxs_2input/'+os.path.basename(pretrained_model)[:-4]+'.onnx'
dummy_input = (Variable(torch.randn(1,256,2,2)), Variable(torch.randn(1,3,64,64)))
output = torch_onnx.export(model,
                          dummy_input,
                          model_onnx_path,
                          verbose=False,
                          opset_version=11)
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;######   test inference time   #######
import sys
sys.path.append('../')
import os
os.environ['OMP_NUM_THREADS'] = "1"
import time
import torch
import argparse
import onnxruntime
import numpy as np

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--pretrained_model', default='output', type=str)
    args = parser.parse_args()
    return args

def infer(model, img):
    inputs = {model.get_inputs()[0].name: img[0], model.get_inputs()[1].name: img[1]}
    output = model.run(None, inputs)
    return output

if __name__ == "__main__":
    args = parse_args()
    pretrained_model = args.pretrained_model

    batch_size, iters = 1, 3000
    opts = onnxruntime.SessionOptions()
    opts.intra_op_num_threads = 1
    opts.inter_op_num_threads = 1

    model = onnxruntime.InferenceSession(pretrained_model, sess_options=opts)
    img = (np.random.rand(batch_size,256,2,2).astype('float32'),
           np.random.rand(batch_size,3,64,64).astype('float32'))
    s = time.time()
    for i in range(iters):
        res = infer(model, img)
    end = time.time()
    print('spend time is {}'.format((end-s)/iters))

&lt;/denchmark-code&gt;

My questions is:
Inference time of ONNX will be different with different model weights?
	</description>
	<comments>
		<comment id='1' author='AKAlilghost' date='2020-11-26T06:11:38Z'>
		I find how to sovle this problem!
&lt;denchmark-link:https://github.com/microsoft/onnxruntime/issues/5472&gt;PyTorch model 10x slower on CPU provider when exported with pre-trained weights&lt;/denchmark-link&gt;

firstly I find torch.set_flush_denormal(True) can make time consuming less, but can not get same inference time of Randomly initialized weights. So I try to set floating numbers that smaller than 1e-32 to zero, now the pretrained model is running at the expected inference speed.
		</comment>
	</comments>
</bug>