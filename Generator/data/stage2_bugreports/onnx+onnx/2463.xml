<bug id='2463' author='ccbrown' open_date='2019-11-18T18:20:50Z' closed_time='2020-08-11T16:53:22Z'>
	<summary>Optimizer and shape inference throw / segfault on model exported from PyTorch</summary>
	<description>
I'm exporting a Mask-RCNN model from PyTorch and running into a few Onnx issues.
Here's a full repro:
#!/usr/bin/env python3
import argparse
import onnx
import onnx.optimizer
import onnx.shape_inference
import sys
import torch
import torchvision
from PIL import Image
from torchvision.transforms import functional as F

parser = argparse.ArgumentParser(description='tests pytorch onnx functionality')
parser.add_argument('--image', type=str, help='an image path')
parser.add_argument('--dest', type=str, help='the destination .onnx path')
args = parser.parse_args()

model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)
model.eval()

img = Image.open(args.image).convert('RGB')
width, height = img.width, img.height
img_tensor = F.to_tensor(img)

# As a sanity check, let's run the image through the model and print out detected objects.
def sanity_check():
    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
    with torch.no_grad():
        prediction = model([img_tensor.to(device)])[0]

    class_names = ['unlabeled', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'street sign', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'hat', 'backpack', 'umbrella', 'shoe', 'eye glasses', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'plate', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'mirror', 'dining table', 'window', 'desk', 'toilet', 'door', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'blender', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'hair brush', 'banner', 'blanket', 'branch', 'bridge', 'building-other', 'bush', 'cabinet', 'cage', 'cardboard', 'carpet', 'ceiling-other', 'ceiling-tile', 'cloth', 'clothes', 'clouds', 'counter', 'cupboard', 'curtain', 'desk-stuff', 'dirt', 'door-stuff', 'fence', 'floor-marble', 'floor-other', 'floor-stone', 'floor-tile', 'floor-wood', 'flower', 'fog', 'food-other', 'fruit', 'furniture-other', 'grass', 'gravel', 'ground-other', 'hill', 'house', 'leaves', 'light', 'mat', 'metal', 'mirror-stuff', 'moss', 'mountain', 'mud', 'napkin', 'net', 'paper', 'pavement', 'pillow', 'plant-other', 'plastic', 'platform', 'playingfield', 'railing', 'railroad', 'river', 'road', 'rock', 'roof', 'rug', 'salad', 'sand', 'sea', 'shelf', 'sky-other', 'skyscraper', 'snow', 'solid-other', 'stairs', 'stone', 'straw', 'structural-other', 'table', 'tent', 'textile-other', 'towel', 'tree', 'vegetable', 'wall-brick', 'wall-concrete', 'wall-other', 'wall-panel', 'wall-stone', 'wall-tile', 'wall-wood', 'water-other', 'waterdrops', 'window-blind', 'window-other', 'wood']

    detections = []
    for i in range(len(prediction['scores'])):
        score = prediction['scores'][i].numpy()
        label = prediction['labels'][i].numpy()
        if score &gt;= 0.5 and label &gt; 0:
            detections.append('{} ({:.02f}%)'.format(class_names[label], score * 100.0))
    print('sanity check detections: {}'.format(', '.join(detections)))

sanity_check()

# Export the model. This seemingly works.
torch.onnx.export(
    model,
    img_tensor.unsqueeze(0),
    args.dest,
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names = ['input'],
    output_names = ['boxes', 'labels', 'scores', 'masks'],
)
print('model exported to {}'.format(args.dest))

# Load and check the model. This also seemingly works.
onnx_model = onnx.load(args.dest)
onnx.checker.check_model(onnx_model)
print('model checked')

# IndexError: unordered_map::at: key not found
try:
    onnx.optimizer.optimize(onnx_model)
except Exception as e:
    print(e)

# Segfault!
onnx.shape_inference.infer_shapes(onnx_model)
My environment in macOS 10.15.1, Python 3.7. The packages used are the latest dev PyTorch and Torchvision packages, and Onnx 1.6.0. PyTorch and Torchvision were installed via:
&lt;denchmark-code&gt;pip3 install --user --upgrade --pre torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html
pip3 install --user --upgrade git+git://github.com/pytorch/vision.git#egg=torchvision
&lt;/denchmark-code&gt;

When run, the optimizer throws an exception: "unordered_map::at: key not found".
And the shape inference segfaults:
&lt;denchmark-code&gt;Process 35364 stopped
* thread #2, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x28)
    frame #0: 0x000000010407d6bf onnx_cpp2py_export.cpython-37m-darwin.so`std::__1::__function::__func&lt;ONNX_REL_1_6::OpSchema ONNX_REL_1_6::GetOpSchema&lt;ONNX_REL_1_6::ConstantOfShape_Onnx_ver9&gt;()::$_1, std::__1::allocator&lt;ONNX_REL_1_6::OpSchema ONNX_REL_1_6::GetOpSchema&lt;ONNX_REL_1_6::ConstantOfShape_Onnx_ver9&gt;()::$_1&gt;, void (ONNX_REL_1_6::InferenceContext&amp;)&gt;::operator()(ONNX_REL_1_6::InferenceContext&amp;) + 271
onnx_cpp2py_export.cpython-37m-darwin.so`std::__1::__function::__func&lt;ONNX_REL_1_6::OpSchema ONNX_REL_1_6::GetOpSchema&lt;ONNX_REL_1_6::ConstantOfShape_Onnx_ver9&gt;()::$_1, std::__1::allocator&lt;ONNX_REL_1_6::OpSchema ONNX_REL_1_6::GetOpSchema&lt;ONNX_REL_1_6::ConstantOfShape_Onnx_ver9&gt;()::$_1&gt;, void (ONNX_REL_1_6::InferenceContext&amp;)&gt;::operator():
-&gt;  0x10407d6bf &lt;+271&gt;: cmpl   $0x1, 0x28(%rax)
    0x10407d6c3 &lt;+275&gt;: jne    0x10407d6ea               ; &lt;+314&gt;
    0x10407d6c5 &lt;+277&gt;: movq   0x20(%rax), %rax
    0x10407d6c9 &lt;+281&gt;: testb  $0x1, 0x10(%rax)
Target 0: (Python) stopped.
(lldb) bt
* thread #2, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x28)
  * frame #0: 0x000000010407d6bf onnx_cpp2py_export.cpython-37m-darwin.so`std::__1::__function::__func&lt;ONNX_REL_1_6::OpSchema ONNX_REL_1_6::GetOpSchema&lt;ONNX_REL_1_6::ConstantOfShape_Onnx_ver9&gt;()::$_1, std::__1::allocator&lt;ONNX_REL_1_6::OpSchema ONNX_REL_1_6::GetOpSchema&lt;ONNX_REL_1_6::ConstantOfShape_Onnx_ver9&gt;()::$_1&gt;, void (ONNX_REL_1_6::InferenceContext&amp;)&gt;::operator()(ONNX_REL_1_6::InferenceContext&amp;) + 271
    frame #1: 0x00000001041399a0 onnx_cpp2py_export.cpython-37m-darwin.so`ONNX_REL_1_6::shape_inference::InferShapesImpl(ONNX_REL_1_6::GraphProto*, std::__1::unordered_map&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, ONNX_REL_1_6::TypeProto*, std::__1::hash&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt;, std::__1::equal_to&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt;, std::__1::allocator&lt;std::__1::pair&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const, ONNX_REL_1_6::TypeProto*&gt; &gt; &gt; const&amp;, std::__1::unordered_map&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, int, std::__1::hash&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt;, std::__1::equal_to&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt;, std::__1::allocator&lt;std::__1::pair&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const, int&gt; &gt; &gt; const&amp;, ONNX_REL_1_6::ISchemaRegistry const*) + 1248
    frame #2: 0x000000010413a130 onnx_cpp2py_export.cpython-37m-darwin.so`ONNX_REL_1_6::shape_inference::InferShapes(ONNX_REL_1_6::ModelProto&amp;, ONNX_REL_1_6::ISchemaRegistry const*) + 192
    frame #3: 0x0000000104032b83 onnx_cpp2py_export.cpython-37m-darwin.so`void pybind11::cpp_function::initialize&lt;ONNX_REL_1_6::pybind11_init_onnx_cpp2py_export(pybind11::module&amp;)::$_20, pybind11::bytes, pybind11::bytes const&amp;, pybind11::name, pybind11::scope, pybind11::sibling&gt;(ONNX_REL_1_6::pybind11_init_onnx_cpp2py_export(pybind11::module&amp;)::$_20&amp;&amp;, pybind11::bytes (*)(pybind11::bytes const&amp;), pybind11::name const&amp;, pybind11::scope const&amp;, pybind11::sibling const&amp;)::'lambda'(pybind11::detail::function_call&amp;)::__invoke(pybind11::detail::function_call&amp;) + 179
    frame #4: 0x000000010400eaa2 onnx_cpp2py_export.cpython-37m-darwin.so`pybind11::cpp_function::dispatcher(_object*, _object*, _object*) + 3442
    frame #5: 0x00000001001234bd Python`_PyMethodDef_RawFastCallKeywords + 537
    frame #6: 0x0000000100122a18 Python`_PyCFunction_FastCallKeywords + 41
    frame #7: 0x00000001001b7350 Python`call_function + 628
    frame #8: 0x00000001001b032b Python`_PyEval_EvalFrameDefault + 6767
    frame #9: 0x0000000100122dec Python`function_code_fastcall + 106
    frame #10: 0x00000001001b73bd Python`call_function + 737
    frame #11: 0x00000001001b032b Python`_PyEval_EvalFrameDefault + 6767
    frame #12: 0x00000001001b7bb1 Python`_PyEval_EvalCodeWithName + 1698
    frame #13: 0x00000001001ae819 Python`PyEval_EvalCode + 51
    frame #14: 0x00000001001dcc2c Python`run_mod + 54
    frame #15: 0x00000001001dbc5f Python`PyRun_FileExFlags + 160
    frame #16: 0x00000001001db316 Python`PyRun_SimpleFileExFlags + 270
    frame #17: 0x00000001001f3b26 Python`pymain_main + 5445
    frame #18: 0x00000001001f4194 Python`_Py_UnixMain + 56
    frame #19: 0x00007fff6a2712e5 libdyld.dylib`start + 1
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='ccbrown' date='2019-11-19T03:40:22Z'>
		It is due to &lt;denchmark-link:https://github.com/onnx/onnx/issues/2417&gt;#2417&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='ccbrown' date='2020-08-11T16:53:22Z'>
		This is a known issue. Optimizers have bugs right now. They are going to be moved to a separate optimizer repo and the bug fixes will be planned there. Closing this issue now. Until this work is complete please don't use the optimizers. You can check the optimizers in onnxruntime repo : &lt;denchmark-link:https://github.com/microsoft/onnxruntime/blob/master/docs/ONNX_Runtime_Graph_Optimizations.md&gt;https://github.com/microsoft/onnxruntime/blob/master/docs/ONNX_Runtime_Graph_Optimizations.md&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>