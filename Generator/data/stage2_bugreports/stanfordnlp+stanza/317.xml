<bug id='317' author='BramVanroy' open_date='2020-05-17T12:33:44Z' closed_time='2020-05-18T04:19:59Z'>
	<summary>Memory usage almost doubles after ~6479 sentences: terminal bug</summary>
	<description>
Describe the bug
I was trying to optimize batch size and particularly how many sentences we can feed to nlp()  before running into out of memory issues. However, I found that for some reason the model can run just fine for ~6479 sentences in total (not batch sizes), but after that the memory usage suddenly doubles which - depending on the batch size - crashes the parser. It seems that this doubling only happens ones at 6479 sentences but does not continue to increase after that.
This seems to be particularly related to the dependency parser, though it also happens (less noticeably) with the other processors. The increase in memory always happens around the 6479 sentences mark.

tokenize,mwt,pos: memory usage increase from 2.7 to 2.9GB
tokenize,mwt,pos,lemma: memory usage increase from 2.7 to 2.9GB
tokenize,mwt,pos,lemma,depparse: memory usage increase from 6.6GB to OOM

The same behaviour is seen when disabling tokenize_pretokenized, though surprisingly the script uses a lot less memory in general if you set tokenize_pretokenized=False. This may lead to the origin of this growing memory issue.
To Reproduce
Find a 10k sentences file and run the following code. On a 11GB GPU this will crash after around 6479 sentences. You can for instance use the WMT news datasets: &lt;denchmark-link:http://data.statmt.org/news-crawl/en/&gt;http://data.statmt.org/news-crawl/en/&lt;/denchmark-link&gt;

import stanza
from tqdm import tqdm

nlp = stanza.Pipeline("en", processors="tokenize,mwt,pos,lemma,depparse", tokenize_pretokenized=True)


with open('your-large-file.txt', encoding='utf-8') as fhin:
    lines = []
    # change 'total' to actual number of lines
    for line_idx, line in tqdm(enumerate(fhin, 1), total=10000):
        lines.append(line.rstrip())

        if line_idx % 40 == 0:
            doc = nlp('\n'.join(lines) + '\n')
            lines = []
Expected behavior
The memory usage should not increase so dramatically. This seems like a grave bug.
Environment (please complete the following information):

OS: Windows; Ubuntu
Python version: vanilla 3.8.2
Stanza version: most recent pip

	</description>
	<comments>
		<comment id='1' author='BramVanroy' date='2020-05-17T22:23:16Z'>
		This seems super weird and I doubt there is a very long sentence in your document, can you provide the file you used with ~10000 lines?
You can attach files by dragging &amp; dropping, selecting or pasting them.
		</comment>
		<comment id='2' author='BramVanroy' date='2020-05-18T03:06:43Z'>
		&lt;denchmark-link:https://github.com/BramVanroy&gt;@BramVanroy&lt;/denchmark-link&gt;
 to collect more data here: does it happen at the same place if you shuffled your text file? Trying to figure out whether it's related to a particular sentence/span of text, or just generally the case after the model's processed MAGIC_NUMBER many sentences.
		</comment>
		<comment id='3' author='BramVanroy' date='2020-05-18T03:46:00Z'>
		Minor point: mwt for English is not necessary or available
		</comment>
		<comment id='4' author='BramVanroy' date='2020-05-18T03:51:44Z'>
		Another less minor point: what's going on with your demo program?
Appreciate the attempt, but please note that it never actually does
anything, since N % 1 == 0 for all values of N.
		</comment>
		<comment id='5' author='BramVanroy' date='2020-05-18T04:19:40Z'>
		I ran the following:

```
with open('45000.txt', encoding='utf-8') as fin:
    lines = fin.readlines()[0:20000]
    for idx in range(0, len(lines), 20):
        if idx % 100 == 0:
            # don't hate me for being lazy
            mem_usage = subprocess.run("nvidia-smi",
stdout=subprocess.PIPE).stdout.decode("utf-8").split("\n")[8][36:54]
            print(idx, mem_usage)
        chunk = lines[idx:idx+20]
        doc = nlp('\n'.join(chunk))
```

It did in fact jump at 6200, which I thought was a little uncanny.

```
6100 2040MiB / 11019MiB
6200 2320MiB / 11019MiB
```

However, I then ran it starting at 5000 instead of 0, and now it jumped at
1200, and it didn't later jump.  I strongly believe you have some gigantic
text (11MB??) somewhere around line 6400 which is causing your problems.
		</comment>
		<comment id='6' author='BramVanroy' date='2020-05-18T07:18:30Z'>
		You're all right. I assumed that the news dataset that I linked was clean (as it is recommended for things like WMT) but apparently that has not been done yet. So the culprit was this "sentence" of 636 white-spaced tokens.

FNB CORPORATION AND SUBSIDIARIES (in thousands, except percent and per share data) 2007 2006 Change % Change Quarter Ended September 30 Net income $3,285 $4,664 (1,379) (29.6) Net interest income 13,342 13,840 (498) (3.6) Net interest income (FTE) (1) 13,441 13,896 (455) (3.3) Noninterest income 3,498 3,815 (317) (8.3) Noninterest expense 10,350 10,254 96 0.9 Provision for loan losses 1,623 354 1,269 358.5 Per Share Data EPS basic $0.45 $0.64 (0.19) (29.7) EPS fully diluted 0.44 0.63 (0.19) (30.2) Dividends declared 0.21 0.21 0.00 0.0 Book value 24.52 23.22 1.30 5.6 Weighted average shares outstanding basic 7,369 7,343 26 0.4 Weighted average shares outstanding fully diluted 7,440 7,425 15 0.2 Shares outstanding quarter end (net of unearned) 7,374 7,345 29 0.4 Financial Ratios Return on average assets 0.85% 1.23% Return on average shareholders' equity 7.30 11.25 Net interest margin (1) 3.78 3.96 Equity to assets 11.93 11.31 Allowance for loan losses to loans, net of unearned income 1.04 1.24 Selected Balances at September 30 Total assets $1,515,714 $1,508,901 6,813 0.5 Loans, net of unearned income 1,119,726 1,173,424 (53,698) (4.6) Allowance for loan losses 11,634 14,589 (2,955) (20.3) Securities 217,217 198,616 18,601 9.4 Deposits 1,260,205 1,254,678 5,527 0.4 Other interest-bearing funds 66,008 75,826 (9,818) (12.9) Shareholders' equity 180,817 170,592 10,225 6.0 Nine Months Ended September 30 Net income $11,798 $13,712 (1,914) (14.0) EPS basic 1.60 1.87 (0.27) (14.4) EPS fully diluted 1.59 1.85 (0.26) (14.1) Dividends declared 0.63 0.61 0.02 3.3 Weighted average shares outstanding basic 7,361 7,331 30 0.4 Weighted average shares outstanding fully diluted 7,441 7,410 31 0.4 Return on average assets 1.03% 1.22% (0.19) NM Return on average shareholders' equity 8.90 11.16 (2.26) NM Net interest margin (1) 3.86 3.98 (0.12) NM Asset Quality % of % of Loans Loans 2007 &amp; ORE 2006 &amp; ORE Nonperforming Assets Nonaccrual loans $13,759 1.23 $5,393 0.46 Other real estate 973 0.09 573 0.05 Loans past due 90 days or more 567 0.05 593 0.05 Total nonperforming assets $15,299 1.37 $6,559 0.56 Net charge off ratio 0.54% 0.13% (1) Fully taxable equivalent NM - Not meaningful FNB CORPORATION AND SUBSIDIARIES (in thousands, except percent and per share data) 2007 2006 Change % Change Alternative Performance Measures for Quarter Ended September 30 (2) Net income $3,285 $4,664 (1,379) (29.6) Plus amortization of core deposit intangibles 146 173 (27) (15.6) Equals cash basis operating earnings (2) 3,431 4,837 (1,406) (29.1) QTD average assets 1,531,722 1,519,867 11,855 0.8 Less QTD intangible assets 46,917 47,878 (961) (2.0) Equals QTD average tangible assets (2) 1,484,805 1,471,989 12,816 0.9 QTD average equity 178,490 165,833 12,657 7.6 Less intangible assets equals QTD average tangible equity (2) 131,573 117,955 13,618 11.5 Cash basis EPS basic (2) 0.47 0.66 (0.19) (28.8) Cash basis EPS fully diluted (2) 0.46 0.65 (0.19) (29.2) Return on average tangible assets (2) 0.92% 1.31% (0.39) (29.8) Return on average tangible equity (2) 10.35 16.40 (6.05) (36.9) Alternative Performance Measures for Nine Months Ended September 30 (2) Net income $11,798 $13,712 (1,914) (14.0) Plus amortization of core deposit intangibles 439 518 (79) (15.3) Equals cash basis operating earnings (2) 12,237 14,230 (1,993) (14.0) YTD average assets 1,529,939 1,498,584 31,355 2.1 Less YTD intangible assets 47,139 48,144 (1,005) (2.1) Equals YTD average tangible assets (2) 1,482,800 1,450,440 32,360 2.2 YTD average equity 177,174 163,829 13,345 8.1 Less intangible assets equals YTD average tangible equity (2) 130,035 115,685 14,350 12.4 Cash basis EPS basic (2) 1.66 1.94 (0.28) (14.4) Cash basis EPS fully diluted (2) 1.64 1.92 (0.28) (14.6) Return on average tangible assets (2) 1.10% 1.31% (0.21) (16.0) Return on average tangible equity (2) 12.58 16.40 (3.82) (23.3) (1) Fully taxable equivalent NM - Not meaningful (2) As a supplement to Generally Accepted Accounting Principles ("GAAP"), management also reviews operating performance based on its "cash basis earnings" to fully analyze its core businesses.

Lesson learned - always check your data. Thanks for the feedback everyone!
&lt;denchmark-link:https://github.com/AngledLuffa&gt;@AngledLuffa&lt;/denchmark-link&gt;
 Loading mwt is a habit since I work with a variety of languages. Even if it is not available, it just throws a warning so that is fine by me. I made a typo here on GitHub in the snippet, fixed now.
Just a note: even though I greatly appreciate the help, it's a bit odd to close an issue before you have certainty that the poster's problem is fixed.
		</comment>
		<comment id='7' author='BramVanroy' date='2020-05-18T07:30:21Z'>
		I see your point.  I'm just used to people not cleaning up after themselves
recently :)

Your issue was already unique over the last couple weeks in that not only
did you fully explain how to reproduce it, but you even included a code
snippet with it to make reproducing it easier.  Thanks!
		</comment>
		<comment id='8' author='BramVanroy' date='2020-05-18T22:53:41Z'>
		For future reference: depparse was the likely culprit here for the memory consumption, because its memory footprint is O(bn^2) where n is the length of the longest sentence in a b-sentence minibatch (most other parts of the pipeline are O(bn)). Super long sentences are really bad for graph-based parsers...
		</comment>
	</comments>
</bug>