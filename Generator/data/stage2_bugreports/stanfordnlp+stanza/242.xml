<bug id='242' author='pasinit' open_date='2020-04-09T21:30:50Z' closed_time='2020-04-10T08:31:32Z'>
	<summary>Italian Tokenizer skips Arbitrary tokens</summary>
	<description>
While using stanza I noticed that some tokens were skipped for no apparent reason. For example, consider the following code:
import stanza
example = "cascare al suolo."
p = stanza.Pipeline(processors="tokenize", lang="it")
p(example)
It outputs
[
  [
    {
      "id": "1",
      "text": "cascare",
      "misc": "start_char=0|end_char=7"
    },
    {
      "id": "3",
      "text": "suolo",
      "misc": "start_char=11|end_char=16"
    },
    {
      "id": "4",
      "text": ".",
      "misc": "start_char=16|end_char=17"
    }
  ]
]
where, as you may notice, the token "al" is missing. The sentence translates as follow:
falling (cascare) al (to) the ground (suolo).
Similarly this other code:
import stanza
example = "andargli incontro."
p = stanza.Pipeline(processors="tokenize", lang="it")
p(example)
outputs the following:
[
  [
    {
      "id": "2",
      "text": "incontro",
      "misc": "start_char=9|end_char=17"
    },
    {
      "id": "3",
      "text": ".",
      "misc": "start_char=17|end_char=18"
    }
  ]
]
where the first word (andargli - going to him) is missing.
This looks weird to me but I haven't found any other open issue nor documented behaviour.
Do you guys have any thought about this or are aware on any possible workaround?
Thanks
	</description>
	<comments>
		<comment id='1' author='pasinit' date='2020-04-09T22:18:50Z'>
		&lt;denchmark-link:https://github.com/pasinit&gt;@pasinit&lt;/denchmark-link&gt;
 thanks for reporting!
This is actually an issue with how the data is presented through , and is a similar issue to that reported in &lt;denchmark-link:https://github.com/stanfordnlp/stanza/issues/210&gt;#210&lt;/denchmark-link&gt;
 . You can actually still access token text through
import stanza
example = "andargli incontro."
p = stanza.Pipeline(processors="tokenize", lang="it")
doc=p(example)
print([token.text for sentence in doc.sentences for token in sentence.tokens])
# prints ['andargli', 'incontro', '.']
The reason this is happening is because our tokenizer predicted "andargli" to be a multi-word token, and that triggered this issue with printing it to command-line. The data is still there, though!
		</comment>
		<comment id='2' author='pasinit' date='2020-04-10T08:31:25Z'>
		I see, thanks for your click response :) Do you think this is the best default behaviour by the way? For the first example I did "cascare al suolo" the token for "al", even if I activate lemma and pos processors, is empty, i.e., no annotation is provided for that token. I think this should be somehow reported (a warning?) or a parameter could be exposed so as to provide control on this behaviour.
Thanks again for your help.
		</comment>
		<comment id='3' author='pasinit' date='2020-04-10T21:20:42Z'>
		&lt;denchmark-link:https://github.com/pasinit&gt;@pasinit&lt;/denchmark-link&gt;
 good point! Italian is a language that requires multi-word token expansion so you won't get sensible results for lemmatizaiton/POS tagging without . We will continue to think about how to improve the error/warning messages here -- the tricky part is we need to be flexible enough to serve languages that don't require . :)
		</comment>
	</comments>
</bug>