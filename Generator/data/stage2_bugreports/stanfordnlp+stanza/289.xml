<bug id='289' author='RogerDavidClarke' open_date='2020-05-03T16:07:56Z' closed_time='2020-05-03T17:55:32Z'>
	<summary>Chinese Simplified text is incorrectly segmented</summary>
	<description>
Specific words in Chinese (Simp) text are incorrectly segmented into separate characters or pairs and separate characters. This entirly changes the mean when the tokens are used for more than character counting.
To Reproduce
Steps to reproduce the behavior:
import stanza
nlp = stanza.download('zh-hans', processors='tokenize,pos')
nlp = stanza.Pipeline(**{'processors': 'tokenize', 'lang': 'zh-hans',
'tokenize_no_ssplit': True})
doc = nlp("中国官方媒体《环球时报》呼应了赵的观点。该报尽管强调，赵立坚以个人身份发表了这些言论，他的言论引起“中国公众的类似质疑”。全球研究是加拿大全球化研究中心（Centre for Research on Globalization）的网站，该中心2001年在加拿大成立。美国独立事实核查网站PolitiFact称，该中心在911、疫苗和全球变暖等话题上都提出了似是而非的阴谋论。赵立坚在推特上发布的文章出自固定撰稿人罗曼诺夫（Larry Romanoff）之笔，他后来重申了自己先前文章（现已删除）的结论，即该病毒并非源自中国。")
[[w.text for w in s.words] for s in doc.sentences]

I had the output reviewed by a native speaker and she highlighted issues using a gray background.
&lt;denchmark-link:https://user-images.githubusercontent.com/30701731/80919109-2b3cc080-8d60-11ea-9b4b-84964f07d2bd.png&gt;&lt;/denchmark-link&gt;

Environment (please complete the following information):

OS: Window10 X64
Python version: 3.7
Stanza version: latest as of 3/May/2020

Additional context
It is proving very difficult to find a decent Chinese tokenizer. At first I tried StanfordNlp only to find the only model is Chinese (Trad). I then discovered Stanza and was pleasantly suprised to see support for Chinese Simp and Trad witha distinct difference in the outputs using Chinese Simp text.
	</description>
	<comments>
		<comment id='1' author='RogerDavidClarke' date='2020-05-03T17:55:32Z'>
		Thank you for your interest in using Stanza. This is a common issue, especially given the fact that Chinese token segmentation is much more difficult than languages with space as word boundary:

Model predictions are wrong on some of my examples, is this normal?


This is absolutely normal, as all models in Stanza (yes, even tokenization!) are statistical. Although they are quite accurate, it does not mean these models are perfect. Therefore, it’s quite likely that you’ll find cases where the model prediction clearly doesn’t make sense, but statistically speaking, it shouldn’t be too far off on a large collection of text from the performance we report as long as the genre of your text is similar to what the models are trained on.

You can try other packages in Chinese but you will probably find that no one is perfect. We support jieba as Chinese tokenizer for our latest release, you can try in this way:
&lt;denchmark-code&gt;pip install jieba

nlp = stanza.Pipeline(**{'processors': {'tokenize':'jieba'}, 'package': None, 'lang': 'zh-hans', 'tokenize_no_ssplit': True})
doc = nlp("中国官方媒体《环球时报》呼应了赵的观点。该报尽管强调，赵立坚以个人身份发表了这些言论，他的言论引起“中国公众的类似质疑”。全球研究是加拿大全球化研究中心（Centre for Research on Globalization）的网站，该中心2001年在加拿大成立。美国独立事实核查网站PolitiFact 称，该中心在911、疫苗和全球变暖等话题上都提出了似是而非的阴谋论。赵立坚在推特上发布的文章出自固定撰稿人罗曼诺夫（Larry Romanoff）之笔，他后来重申了自己先前文章（现已删除）的结论，即该病毒并非源自中国。")
print([[w.text for w in s.words] for s in doc.sentences])
&lt;/denchmark-code&gt;

Not surprising, the output is not perfect as well:
&lt;denchmark-code&gt;[['中国', '官方', '媒体', '《', '环球时报', '》', '呼应', '了', '赵', '的', '观点', '。'], ['该报', '尽管', '强调', '，', '赵立', '坚以', '个人身份', '发表', '了', '这些', '言论', '，', '他', '的', '言论', '引起', '“', '中国', '公众', '的', '类似', '质疑', '”', '。'], ['全球', '研究', '是', '加拿大', '全球化', '研究', '中心', '（', 'Centre', 'for', 'Research', 'on', 'Globalization', '）', '的', '网站', '，', '该', '中心', '2001', '年', '在', '加拿大', '成立', '。'], ['美国', '独立', '事实', '核查', '网站', 'PolitiFact', '称', '，', '该', '中心', '在', '911', '、', '疫苗', '和', '全球', '变暖', '等', '话题', '上', '都', '提出', '了', '似是而非', '的', '阴谋论', '。'], ['赵立', '坚在', '推特上', '发布', '的', '文章', '出自', '固定', '撰稿人', '罗曼诺夫', '（', 'Larry', 'Romanoff', '）', '之笔', '，', '他', '后来', '重申', '了', '自己', '先前', '文章', '（', '现已', '删除', '）', '的', '结论', '，', '即该', '病毒', '并非', '源自', '中国', '。']]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='RogerDavidClarke' date='2020-05-03T18:12:21Z'>
		One note here is that the UD data we're using separates surnames from given names so 赵 | 立坚 is expected.
		</comment>
		<comment id='3' author='RogerDavidClarke' date='2020-05-03T23:29:59Z'>
		Many thanks for the fast response and suggestion. Much appreciated.

From: Yuhui Zhang &lt;notifications@github.com&gt;
Sent: Sunday, May 3, 2020 6:56 PM
To: stanfordnlp/stanza &lt;stanza@noreply.github.com&gt;
Cc: Roger Clarke &lt;rogercl@microsoft.com&gt;; Author &lt;author@noreply.github.com&gt;
Subject: Re: [stanfordnlp/stanza] Chinese Simplified text is incorrectly segmented (&lt;denchmark-link:https://github.com/stanfordnlp/stanza/issues/289&gt;#289&lt;/denchmark-link&gt;
)


Thank you for your interest in using Stanza. This is a common issue, especially given the fact that Chinese token segmentation is much more difficult than languages with space as word boundary:

Model predictions are wrong on some of my examples, is this normal?

This is absolutely normal, as all models in Stanza (yes, even tokenization!) are statistical. Although they are quite accurate, it does not mean these models are perfect. Therefore, it’s quite likely that you’ll find cases where the model prediction clearly doesn’t make sense, but statistically speaking, it shouldn’t be too far off on a large collection of text from the performance we report as long as the genre of your text is similar to what the models are trained on.

You can try other packages in Chinese but you will probably find that no one is perfect. We support jieba as Chinese tokenizer for our latest release, you can try in this way:

pip install jieba



nlp = stanza.Pipeline(**{'processors': {'tokenize':'jieba'}, 'package': None, 'lang': 'zh-hans', 'tokenize_no_ssplit': True})

doc = nlp("中国官方媒体《环球时报》呼应了赵的观点。该报尽管强调，赵立坚以个人身份发表了这些言论，他的言论引起“中国公众的类似质疑”。全球研究是加拿大全球化研究中心（Centre for Research on Globalization）的网站，该中心2001年在加拿大成立。美国独立事实核查网站PolitiFact 称，该中心在911、疫苗和全球变暖等话题上都提出了似是而非的阴谋论。赵立坚在推特上发布的文章出自固定撰稿人罗曼诺夫（Larry Romanoff）之笔，他后来重申了自己先前文章（现已删除）的结论，即该病毒并非源自中国。")

print([[w.text for w in s.words] for s in doc.sentences])

The output is not perfect as well:

[['中国', '官方', '媒体', '《', '环球时报', '》', '呼应', '了', '赵', '的', '观点', '。'], ['该报', '尽管', '强调', '，', '赵立', '坚以', '个人身份', '发表', '了', '这些', '言论', '，', '他', '的', '言论', '引起', '“', '中国', '公众', '的', '类似', '质疑', '”', '。'], ['全球', '研究', '是', '加拿大', '全球化', '研究', '中心', '（', 'Centre', 'for', 'Research', 'on', 'Globalization', '）', '的', '网站', '，', '该', '中心', '2001', '年', '在', '加拿大', '成立', '。'], ['美国', '独立', '事实', '核查', '网站', 'PolitiFact', '称', '，', '该', '中心', '在', '911', '、', '疫苗', '和', '全球', '变暖', '等', '话题', '上', '都', '提出', '了', '似是而非', '的', '阴谋论', '。'], ['赵立', '坚在', '推特上', '发布', '的', '文章', '出自', '固定', '撰稿人', '罗曼诺夫', '（', 'Larry', 'Romanoff', '）', '之笔', '，', '他', '后来', '重申', '了', '自己', '先前', '文章', '（', '现已', '删除', '）', '的', '结论', '，', '即该', '病毒', '并非', '源自', '中国', '。']]

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub&lt;&lt;denchmark-link:https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fstanfordnlp%2Fstanza%2Fissues%2F289%23issuecomment-623152653&amp;data=02%7C01%7Crogercl%40microsoft.com%7Ca22cb137b14340f750f408d7ef8b3399%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637241253465345816&amp;sdata=TWMDSBDDp68d%2FsMILjxAO4UGeRoPLoD50krQNtOqYc0%3D&amp;reserved=0&gt;https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fstanfordnlp%2Fstanza%2Fissues%2F289%23issuecomment-623152653&amp;data=02%7C01%7Crogercl%40microsoft.com%7Ca22cb137b14340f750f408d7ef8b3399%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637241253465345816&amp;sdata=TWMDSBDDp68d%2FsMILjxAO4UGeRoPLoD50krQNtOqYc0%3D&amp;reserved=0&lt;/denchmark-link&gt;
&gt;, or unsubscribe&lt;&lt;denchmark-link:https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAHKHRI3ZL22E4R5CVGZMWMDRPWVZ7ANCNFSM4MYF52XQ&amp;data=02%7C01%7Crogercl%40microsoft.com%7Ca22cb137b14340f750f408d7ef8b3399%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637241253465355810&amp;sdata=%2FR0R5Ln4HaIIreGDVZMREYEEA4upyIVm5naS2wsfRjU%3D&amp;reserved=0&gt;https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAHKHRI3ZL22E4R5CVGZMWMDRPWVZ7ANCNFSM4MYF52XQ&amp;data=02%7C01%7Crogercl%40microsoft.com%7Ca22cb137b14340f750f408d7ef8b3399%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637241253465355810&amp;sdata=%2FR0R5Ln4HaIIreGDVZMREYEEA4upyIVm5naS2wsfRjU%3D&amp;reserved=0&lt;/denchmark-link&gt;
&gt;.
		</comment>
		<comment id='4' author='RogerDavidClarke' date='2020-05-17T14:59:47Z'>
		Hello. As a non-Chinese speaker, which tokenisation method is preferable for best quality results? Default or jieba? Thanks. :-)
		</comment>
		<comment id='5' author='RogerDavidClarke' date='2020-05-18T03:08:33Z'>
		&lt;denchmark-link:https://github.com/hobodrifterdavid&gt;@hobodrifterdavid&lt;/denchmark-link&gt;
 I suspect in the short term jieba is gonna be a bit more accurate. Though we're actively working on making Stanza's neural tokenizer more robust so Stanza's tokenizer would probably win out eventually.
		</comment>
	</comments>
</bug>