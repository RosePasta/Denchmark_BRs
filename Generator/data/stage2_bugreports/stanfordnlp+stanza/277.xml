<bug id='277' author='valerio65xz' open_date='2020-04-26T17:19:18Z' closed_time='2020-04-26T17:46:35Z'>
	<summary>Possible bug when run simple annotation</summary>
	<description>
Describe the bug
Here there is a simple code ran over two different sentences (I've already downloaded the english model)
&lt;denchmark-code&gt;import stanza

nlp = stanza.Pipeline('en') 
string = "Barack Obama, the president of United States of America, was born in Hawaii.  He was elected president in 2008. "
# string = "Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44."
doc = nlp(string)  
doc.sentences[0].print_dependencies()
&lt;/denchmark-code&gt;

So I make a run with the first sentence, then I comment it and I make a second run with the second sentence)
For the first sentences, I get this:
('Barack', '13', 'nsubj:pass') ('Obama', '1', 'flat') (',', '1', 'punct') ('the', '5', 'det') ('president', '1', 'appos') ('of', '8', 'case') ('United', '8', 'compound') ('States', '5', 'nmod') ('of', '10', 'case') ('America', '8', 'nmod') (',', '13', 'punct') ('was', '13', 'aux:pass') ('born', '0', 'root') ('in', '15', 'case') ('Hawaii', '13', 'obl') ('.', '13', 'punct')
For the first sentences, I get just this:
('Dana', '0', 'root')
That's really an anomalous behavior
Environment:
I've tested in two different enviroments:

OS: [Windows, CentOS]
Python version: [Windows: Python 3.8, CentOS: Python 3.6.2 from Anaconda]
Stanza version: [1.0.0]

	</description>
	<comments>
		<comment id='1' author='valerio65xz' date='2020-04-26T17:46:34Z'>
		Thank you for pointing out! It seems the model wrongly segment the sentence Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44. as ['Dana', 'Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44.'], so doc.sentences[0] only contains 1 token.
As all models in Stanza are statistical, although they are quite accurate, it does not mean these models are perfect. Therefore, it’s quite likely that you’ll find cases where the model prediction clearly doesn’t make sense, but statistically speaking, it shouldn’t be too far off on a large collection of text from the performance we report as long as the genre of your text is similar to what the models are trained on.
&lt;denchmark-link:https://stanfordnlp.github.io/stanza/faq.html#model-predictions-are-wrong-on-some-of-my-examples-is-this-normal&gt;https://stanfordnlp.github.io/stanza/faq.html#model-predictions-are-wrong-on-some-of-my-examples-is-this-normal&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>