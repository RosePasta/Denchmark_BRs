<bug id='381' author='awentzonline' open_date='2020-07-16T18:36:31Z' closed_time='2020-09-17T21:13:30Z'>
	<summary>English tokenizer splits single sentence into two</summary>
	<description>
The English language tokenizer splits the following sentence into two:
While at work today Cooper realized he didn't have his laptop.
It appears to have problems with the uppercased name because the issue goes away when "Cooper" is lowercased.
To reproduce
&lt;denchmark-code&gt;p = stanza.Pipeline(lang='en', processors='tokenize')
doc = p("While at work today Cooper realized he didn't have his laptop.")
assert len(doc.sentences) == 1
&lt;/denchmark-code&gt;

Expected behavior
The document should contain a single sentence.
Environment (please complete the following information):

MacOS 10.14.4 with Python 3.7.5
and
Linux 6ec5ff8bcbe3 4.19.76-linuxkit #1 SMP x86_64 GNU/Linux with Python 3.7.7 (from pytorch/pytorch:1.5.1-cuda10.1-cudnn7-runtime docker image)
Both using Stanza version: 1.0.1

	</description>
	<comments>
		<comment id='1' author='awentzonline' date='2020-07-16T19:35:21Z'>
		In my case, I'm able to work around with the tokenize_no_ssplit=True kwarg to Pipeline, since I already have the sentence segmentation. Not sure if this still qualifies as a bug?
		</comment>
		<comment id='2' author='awentzonline' date='2020-07-16T22:45:40Z'>
		&lt;denchmark-link:https://github.com/awentzonline&gt;@awentzonline&lt;/denchmark-link&gt;
 Thanks for reporting! This is a statistical "bug" that we're actively looking for a solution for.
In short, because everything in Stanza's neural pipeline is data-driven, it could overfit to some of the spurious statistical patterns in the training data.
		</comment>
		<comment id='3' author='awentzonline' date='2020-07-16T23:58:51Z'>
		Perhaps you've already considered this but I wonder if augmenting your training data with some "case noise" would help here?
		</comment>
		<comment id='4' author='awentzonline' date='2020-07-18T00:20:48Z'>
		&lt;denchmark-link:https://github.com/awentzonline&gt;@awentzonline&lt;/denchmark-link&gt;
 thanks for your suggestion! Yeah we have been considering a few ways to augment data, including literally adding more data. Unfortunately more of the main contributors are a bit bandwidth-strapped at the moment. We'll definitely improve the robustness of the models (esp the tokenizers) in the near future, and some efforts are already in the works!
		</comment>
		<comment id='5' author='awentzonline' date='2020-09-17T21:13:30Z'>
		Closing now as there is no further update, free free to reopen if you have any other questions.
		</comment>
	</comments>
</bug>