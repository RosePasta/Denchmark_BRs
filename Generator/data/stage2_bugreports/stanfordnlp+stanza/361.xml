<bug id='361' author='greasystrangler' open_date='2020-06-22T11:04:24Z' closed_time='2020-06-30T18:26:50Z'>
	<summary>The to_dict function does not include NER fields for tokens</summary>
	<description>
When using the to_dict function I would expect the resulting list of dicts to include NER information but they do not. Therefore when a stanza Document is created using the output from to_dict all NER data is lost from the reconstituted document.
I think the issue is in the  token.to_dict() function of the Document class 


stanza/stanza/models/common/doc.py


        Lines 549 to 562
      in
      ebb374b






 def to_dict(self, fields=[ID, TEXT, NER, MISC]): 



 """ Dumps the token into a list of dictionary for this token with its extended words 



         if the token is a multi-word token. 



         """ 



 ret = [] 



 if multi_word_token_id.match(self.id): 



 token_dict = {} 



 for field in fields: 



 if getattr(self, field) is not None: 



 token_dict[field] = getattr(self, field) 



 ret.append(token_dict) 



 for word in self.words: 



 ret.append(word.to_dict()) 



 return ret 





Unless the token is MWT (which is never true for English) then token.to_dict() simply calls word.to_dict() which does not include NER information.
To Reproduce
The following short code will quickly reproduce the behaviour:
import stanza
text = '''Here is a small sample showing that Stanza, a great NLP tool, does not retain NER information in the data generated using "to_dict()"'''
nlp = stanza.Pipeline('en')

doc1 = nlp(text)
print ('Doc1 ents:',', '.join(e.text for e in doc1.entities))

doc2 = stanza.Document(doc1.to_dict())  # Export doc1 and create doc2 from the imported list of dicts
print ('Doc2 ents:',', '.join(e.text for e in doc2.entities))
Output from above script:
&lt;denchmark-code&gt;2020-06-22 11:59:57 INFO: Loading these models for language: en (English):
=========================
| Processor | Package   |
-------------------------
| tokenize  | ewt       |
| pos       | ewt       |
| lemma     | ewt       |
| depparse  | ewt       |
| ner       | ontonotes |
=========================

2020-06-22 11:59:57 INFO: Use device: cpu
2020-06-22 11:59:57 INFO: Loading: tokenize
2020-06-22 11:59:57 INFO: Loading: pos
2020-06-22 11:59:57 INFO: Loading: lemma
2020-06-22 11:59:57 INFO: Loading: depparse
2020-06-22 11:59:58 INFO: Loading: ner
2020-06-22 11:59:59 INFO: Done loading processors!
Doc1 ents: Stanza, NLP, NER
Doc2 ents: 

Process finished with exit code 0
&lt;/denchmark-code&gt;

Expected behaviour
I would expect the output of doc.to_dict() and token.to_dict() to include NER data.
Environment:

OS: Linux
Python version:3.6.10
Stanza version: 1.0.1 (git)

	</description>
	<comments>
		<comment id='1' author='greasystrangler' date='2020-06-22T15:00:18Z'>
		OK I have had a go at fixing this.
If you add the 'ner' field to the Token dict returned in Token.to_dict() then it will work but you must call Document.build_ents().
However Document.build_ents() requires that the original Document.text is set to generate the entities but again, this text is 'lost' when one relies on Document.to_dict().
So I may be doing something wrong but as it stands I don't think you can run a pipeline, save the document with Document.to_dict and then simply load the document again.
As a result I have added simple serialiser/deserialiser functions and also modified Token.to_dict() to include the token 'ner' field. The serialiser outputs a bytestring (like the stanza CoreNLP document). The deserialiser takes a bytestring and returns the re-created document.
&lt;denchmark-link:https://github.com/greasystrangler/stanza/commit/8226009364f43fa1ef01828d0ad6d402552d6663&gt;greasystrangler@8226009&lt;/denchmark-link&gt;

If you think it may be useful then I can create a pull request.
thanks
GS
		</comment>
		<comment id='2' author='greasystrangler' date='2020-06-26T23:21:26Z'>
		If I understand you correctly, there seem to be too different (but related) issues here:


the Document.to_dict() function does not include the NER field in the output, unless for a multi-word token, which is never the case in languages such as English or Chinese;


Due to issue 1, and that Document.text is also not included in the output of to_dict() function, one cannot recover all annotations in a document from deserializing the dumped dictionary string.


And your take above is meant to fix both these issues?
		</comment>
		<comment id='3' author='greasystrangler' date='2020-06-28T23:44:46Z'>
		Hello
Yes I suppose there are two issues that combine to cause my particular issue. The commit does fix issue 1 (as best I can tell) and definitely address issue 2.
My objective is to batch process documents using Stanza, save the annotations and then, at some later point in a different place, process those annotations. I also use CoreNLP this way too (now via the Stanza client) and it works quite well
		</comment>
		<comment id='4' author='greasystrangler' date='2020-06-29T21:43:23Z'>
		Hi &lt;denchmark-link:https://github.com/greasystrangler&gt;@greasystrangler&lt;/denchmark-link&gt;
, thank you for reporting this!
I slightly adapted your codes and made a pull request &lt;denchmark-link:https://github.com/stanfordnlp/stanza/pull/366&gt;#366&lt;/denchmark-link&gt;
. The difference is that I take multi-word token into consideration and only propagate the NER label from Token to Word if the token is a single-word token.  We will include this in the future release.
You can learn more about multi-word token by running:
&lt;denchmark-code&gt;import stanza
nlp = stanza.Pipeline('fr')
doc = nlp('Nous avons atteint la fin du sentier.')
print(doc)
&lt;/denchmark-code&gt;

And you will see for the multi-word token du, the NER tag should not be propagated to its words de and le.
		</comment>
		<comment id='5' author='greasystrangler' date='2020-06-29T21:48:10Z'>
		Besides, I use pickle for string serialization, as it is stated in the document that:
&lt;denchmark-code&gt;Python has a more primitive serialization module called marshal, but in general pickle should always be the preferred way to serialize Python objects. marshal exists primarily to support Python's .pyc files.
&lt;/denchmark-code&gt;

Thanks again for your contribution!
		</comment>
		<comment id='6' author='greasystrangler' date='2020-06-29T21:55:03Z'>
		
Besides, I use pickle for string serialization, as it is stated in the document that:
Python has a more primitive serialization module called marshal, but in general pickle should always be the preferred way to serialize Python objects. marshal exists primarily to support Python's .pyc files.

Thanks again for your contribution!

Hi
Yes pickle works, I find that marshal is a little quicker sometimes but more importantly it is somewhat less vulnerable to data manipulation attacks whereby an attacker modifies your pickled data to introduce malformed or malicious Python objects.
I don't think that is necessarily an issue here but worth remembering that importing data via Pickle does have some potential risk.
Anyway, I'm glad it was useful.
thanks
GS
		</comment>
		<comment id='7' author='greasystrangler' date='2020-06-29T22:42:19Z'>
		Hi &lt;denchmark-link:https://github.com/greasystrangler&gt;@greasystrangler&lt;/denchmark-link&gt;
, we totally understand the concerns about malicious data manipulation via pickle, etc., but we feel that these functions are mainly meant for users to serialize/deserialize documents for their own usage, and not for sharing with others, therefore we went for the more generally endorsed  library. In fact, libraries such as  also &lt;denchmark-link:https://pytorch.org/tutorials/beginner/saving_loading_models.html&gt;uses pickle for serializing models and data&lt;/denchmark-link&gt;
, etc.
In addition, we had a more general concern about marshal's compatibility across different Python versions, as quoted from the &lt;denchmark-link:https://docs.python.org/3/library/pickle.html#comparison-with-marshal&gt;official Python documentation&lt;/denchmark-link&gt;
:

The marshal serialization format is not guaranteed to be portable across Python versions. Because its primary job in life is to support .pyc files, the Python implementers reserve the right to change the serialization format in non-backwards compatible ways should the need arise. The pickle serialization format is guaranteed to be backwards compatible across Python releases provided a compatible pickle protocol is chosen.

With all that said, we really appreciate that you have pointed out this issue for us and proposed a solution that helped us improve its usability.
		</comment>
	</comments>
</bug>