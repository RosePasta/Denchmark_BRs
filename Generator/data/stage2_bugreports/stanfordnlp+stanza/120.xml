<bug id='120' author='ButteredGroove' open_date='2019-08-15T01:56:58Z' closed_time='2020-03-17T22:47:02Z'>
	<summary>models.parser --no_pretrain tries to load embeddings</summary>
	<description>
When training a model, if you replace --wordvec_dir &lt;WORDVEC_DIR&gt; with --no_pretrain, stanfordnlp.models.parser still tries to load embeddings:
&lt;denchmark-code&gt;python3 -m stanfordnlp.models.parser --train_file /gpfs-volume/stanfordnlp/intermediates_no_embeddings/depparse/id_gsd.train.in.conllu --eval_file /gpfs-volume/stanfordnlp/intermediates_no_embeddings/depparse/id_gsd.dev.in.conllu --output_file /gpfs-volume/stanfordnlp/intermediates_no_embeddings/depparse/id_gsd.dev.pred.conllu --gold_file /gpfs-volume/stanfordnlp/intermediates_no_embeddings/depparse/id_gsd.dev.gold.conllu --lang id --shorthand id_gsd --batch_size 10000 --mode train --no_pretrain
Running parser in train mode
Loading data with batch size 10000...
Reading pretrained vectors from extern_data/word2vec/Indonesian/id.vectors.xz...
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='ButteredGroove' date='2019-08-15T02:17:33Z'>
		This was indeed a mistake. The training code never checked for no_pretrainand would try to load the embedding file regardless. Thanks for finding this for us!
		</comment>
		<comment id='2' author='ButteredGroove' date='2019-09-27T03:40:08Z'>
		So have you fixed the bug?
		</comment>
		<comment id='3' author='ButteredGroove' date='2020-03-17T22:47:02Z'>
		This is resolved in the new v1.0.0 release: &lt;denchmark-link:https://github.com/stanfordnlp/stanza/releases/tag/v1.0.0&gt;https://github.com/stanfordnlp/stanza/releases/tag/v1.0.0&lt;/denchmark-link&gt;

Closing now!
		</comment>
	</comments>
</bug>