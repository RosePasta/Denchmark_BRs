<bug id='621' author='yuangu' open_date='2020-10-27T02:18:50Z' closed_time='2020-10-29T04:47:58Z'>
	<summary>pth and onnx export different results on pytorch</summary>
	<description>
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.onnx
 
import netron
 
class model(nn.Module):
    def __init__(self):
        super(model, self).__init__()
        self.block1 = nn.Sequential(
            nn.Conv2d(64, 64, 3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 32, 1, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 64, 3, padding=1, bias=False),
            nn.BatchNorm2d(64)
        )
 
        self.conv1 = nn.Conv2d(3, 64, 3, padding=1, bias=False)
        self.output = nn.Sequential(
            nn.Conv2d(64, 1, 3, padding=1, bias=True),
            nn.Sigmoid()
        )
 
    def forward(self, x):
        x = self.conv1(x)
        identity = x
        x = F.relu(self.block1(x) + identity)
        x = self.output(x)
        return x
 
 
d = torch.rand(1, 3, 416, 416)
m = model()
o = m(d)
 
# onnx_path = "onnx_model_name.onnx"
# torch.onnx.export(m, d, onnx_path)
# netron.start(onnx_path)

modelPath = './model_para.pth'
torch.save(m, modelPath)
netron.start(modelPath)`
here is this code . The result of PTH lacks an addition operatorã€‚
pth data:
&lt;denchmark-link:https://user-images.githubusercontent.com/1897438/97248664-379c6480-183d-11eb-9dbb-5cb7c9179b3b.png&gt;&lt;/denchmark-link&gt;

onnx data:
&lt;denchmark-link:https://user-images.githubusercontent.com/1897438/97248750-67e40300-183d-11eb-8916-4689eff53abe.png&gt;&lt;/denchmark-link&gt;

win10 + Python 3.8.5 + Pytorch 1.6.0 + netron 4.5.9
	</description>
	<comments>
		<comment id='1' author='yuangu' date='2020-10-29T04:47:57Z'>
		torch.save will not save the full model graph. The connections and order can only be guessed or not rendered. The full graph is included in ONNX or TorchScript (as code).
		</comment>
	</comments>
</bug>