<bug id='370' author='JeffreyCA' open_date='2020-05-10T18:31:31Z' closed_time='2020-05-12T13:55:24Z'>
	<summary>[Bug] Key not found in checkpoint when separating multiple times using librosa backend</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

When calling separator.separate() multiple times consecutively, it gives me errors about keys not found in checkpoint. This only occurs when using a Separator using librosa backend. tensorflow backend does not have this problem.
&lt;denchmark-h:h2&gt;Step to reproduce&lt;/denchmark-h&gt;


pip install spleeter. I have the latest version: v1.5.1
Run the following snippet, with sample file test.mp3 in the same directory

from spleeter import *
from spleeter.separator import Separator
from spleeter.utils import *
from spleeter.audio.adapter import get_default_audio_adapter

def func():
    separator = Separator('spleeter:4stems', stft_backend='librosa', multiprocess=False)
    audio_adapter = get_default_audio_adapter()
    waveform, _ = audio_adapter.load('test.mp3', sample_rate=44100)
    separator.separate(waveform)

for i in range(2):
    func()
&lt;denchmark-h:h2&gt;Output&lt;/denchmark-h&gt;

jeffrey@MacBook-Pro:~/Dev/spleeter-bug$ python main.py
/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.
Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.
  from numba.decorators import jit as optional_jit
/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.
Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.
  from numba.decorators import jit as optional_jit
WARNING:tensorflow:From /Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /Users/jeffrey/Library/Python/3.7/lib/python/site-packages/spleeter/separator.py:167: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING:tensorflow:From /Users/jeffrey/Library/Python/3.7/lib/python/site-packages/spleeter/separator.py:168: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2020-05-10 14:19:26.228251: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-05-10 14:19:26.267446: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fca34494fa0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-10 14:19:26.267467: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-05-10 14:20:02.117266: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key batch_normalization_48/beta not found in checkpoint
Traceback (most recent call last):
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/client/session.py", line 1365, in _do_call
    return fn(*args)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/client/session.py", line 1350, in _run_fn
    target_list, run_metadata)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/client/session.py", line 1443, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.NotFoundError: Key batch_normalization_48/beta not found in checkpoint
         [[{{node save_1/RestoreV2}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/training/saver.py", line 1290, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/client/session.py", line 956, in run
    run_metadata_ptr)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/client/session.py", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/client/session.py", line 1359, in _do_run
    run_metadata)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/client/session.py", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Key batch_normalization_48/beta not found in checkpoint
         [[node save_1/RestoreV2 (defined at /Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]

Original stack trace for 'save_1/RestoreV2':
  File "main.py", line 13, in &lt;module&gt;
    func()
  File "main.py", line 10, in func
    separator.separate(waveform)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/spleeter/separator.py", line 184, in separate
    return self._separate_librosa(waveform, audio_descriptor)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/spleeter/separator.py", line 167, in _separate_librosa
    saver = tf.train.Saver()
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/training/saver.py", line 828, in __init__
    self.build()
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/training/saver.py", line 840, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/training/saver.py", line 878, in _build
    build_restore=build_restore)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/training/saver.py", line 508, in _build_internal
    restore_sequentially, reshape)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/training/saver.py", line 328, in _AddRestoreOps
    restore_sequentially)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/training/saver.py", line 575, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/ops/gen_io_ops.py", line 1696, in restore_v2
    name=name)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/framework/op_def_library.py", line 794, in _apply_op_helper
    op_def=op_def)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/framework/ops.py", line 3357, in create_op
    attrs, op_def, compute_device)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/framework/ops.py", line 3426, in _create_op_internal
    op_def=op_def)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/framework/ops.py", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/training/saver.py", line 1300, in restore
    names_to_keys = object_graph_key_mapping(save_path)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/training/saver.py", line 1618, in object_graph_key_mapping
    object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py", line 915, in get_tensor
    return CheckpointReader_GetTensor(self, compat.as_bytes(tensor_str))
tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main.py", line 13, in &lt;module&gt;
    func()
  File "main.py", line 10, in func
    separator.separate(waveform)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/spleeter/separator.py", line 184, in separate
    return self._separate_librosa(waveform, audio_descriptor)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/spleeter/separator.py", line 169, in _separate_librosa
    saver.restore(sess, latest_checkpoint)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/training/saver.py", line 1306, in restore
    err, "a Variable name or other graph key that is missing")
tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Key batch_normalization_48/beta not found in checkpoint
         [[node save_1/RestoreV2 (defined at /Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]

Original stack trace for 'save_1/RestoreV2':
  File "main.py", line 13, in &lt;module&gt;
    func()
  File "main.py", line 10, in func
    separator.separate(waveform)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/spleeter/separator.py", line 184, in separate
    return self._separate_librosa(waveform, audio_descriptor)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/spleeter/separator.py", line 167, in _separate_librosa
    saver = tf.train.Saver()
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/training/saver.py", line 828, in __init__
    self.build()
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/training/saver.py", line 840, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/training/saver.py", line 878, in _build
    build_restore=build_restore)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/training/saver.py", line 508, in _build_internal
    restore_sequentially, reshape)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/training/saver.py", line 328, in _AddRestoreOps
    restore_sequentially)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/training/saver.py", line 575, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/ops/gen_io_ops.py", line 1696, in restore_v2
    name=name)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/framework/op_def_library.py", line 794, in _apply_op_helper
    op_def=op_def)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/framework/ops.py", line 3357, in create_op
    attrs, op_def, compute_device)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/framework/ops.py", line 3426, in _create_op_internal
    op_def=op_def)
  File "/Users/jeffrey/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/framework/ops.py", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;










OS
macOS 10.15.5


Installation type
pip


RAM available
16 GB


Hardware spec
2.2 GHz Quad-Core Intel Core i7



&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

Similar to &lt;denchmark-link:https://github.com/deezer/spleeter/issues/311&gt;#311&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='JeffreyCA' date='2020-05-12T13:55:23Z'>
		Hi &lt;denchmark-link:https://github.com/JeffreyCA&gt;@JeffreyCA&lt;/denchmark-link&gt;
 ,
This is a know issue that comes from the current design of the  object.
Actually, the  object was not designed to be instanciated several times: if you need to perform several separations with the same config, you only need a single  and run several times the  method on it.
Then, instantiating it only once should solve your issue:
from spleeter import *
from spleeter.separator import Separator
from spleeter.utils import *
from spleeter.audio.adapter import get_default_audio_adapter

def func(separator):
    audio_adapter = get_default_audio_adapter()
    waveform, _ = audio_adapter.load('test.mp3', sample_rate=44100)
    separator.separate(waveform)

separator = Separator('spleeter:4stems', stft_backend='librosa', multiprocess=False)
for i in range(2):
    func(separator)
		</comment>
		<comment id='2' author='JeffreyCA' date='2020-05-12T13:59:05Z'>
		Thanks for the update. So I wouldn't be able to use it in parallel? E.g. multiple separations using same config at the same time on different threads?
		</comment>
		<comment id='3' author='JeffreyCA' date='2020-05-13T09:59:21Z'>
		I think that trying to do multi-threading on top of the tensorflow session won't work properly and will results in deadlocks.
If the goal of having separation in parallel is to be faster, be aware that tensorflow already does a fairly good job at using multiple core (and this is not controlled by the  parameter which only deals with parallelizing audio export).
I realized that the model is actually reloaded at each separation (when using the librosa backend, not with the tensorflow one) which is sub-optimal.
I made a fix in &lt;denchmark-link:https://github.com/deezer/spleeter/tree/avoid_multiple_restore_librosa&gt;this branch&lt;/denchmark-link&gt;
 that avoid reloading the model and should make multiple separation faster (with the librosa backend).
It also makes it possible to instanciate several Separator objects (while it is not recommended to do so as it will add a lot of overhead compared to calling several time the  method of the same object).
		</comment>
		<comment id='4' author='JeffreyCA' date='2020-05-13T15:08:56Z'>
		Thanks, the fix is a good improvement!
		</comment>
		<comment id='5' author='JeffreyCA' date='2020-05-15T14:31:49Z'>
		I'm running into this same Key batch_normalization_48/beta not found in checkpoint error, but I don't believe I'm able to use a single instance of Separator because I'm using different models each time based on user selection (2stems, 4stems, 5stems).  Is it possible to update the config to use a different separation model once Separator is instantiated?
EDIT: I should mention that I'm using the librosa backend
		</comment>
	</comments>
</bug>