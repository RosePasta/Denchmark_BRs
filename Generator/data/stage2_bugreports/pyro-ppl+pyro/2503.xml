<bug id='2503' author='ciaobladoo' open_date='2020-05-25T00:47:12Z' closed_time='2020-05-25T20:23:56Z'>
	<summary>Possible bug in affine_autoregressive</summary>
	<description>
I think line &lt;denchmark-link:https://github.com/pyro-ppl/pyro/blob/c00ff3b8f20a28585dcbf565c645352ed2758df4/pyro/distributions/transforms/affine_autoregressive.py#L220&gt;220&lt;/denchmark-link&gt;
 of affine_autoregressive.py might be wrong. Shouldn't it be like line &lt;denchmark-link:https://github.com/pyro-ppl/pyro/blob/c00ff3b8f20a28585dcbf565c645352ed2758df4/pyro/distributions/transforms/affine_autoregressive.py#L196&gt;196&lt;/denchmark-link&gt;
. Otherwise it is not the same as in the naive version (line &lt;denchmark-link:https://github.com/pyro-ppl/pyro/blob/c00ff3b8f20a28585dcbf565c645352ed2758df4/pyro/distributions/transforms/affine_autoregressive.py#L162&gt;162&lt;/denchmark-link&gt;
).
In addition, I think the clamp should be applied for the inverse of stable version too since the inverse of sigmoid could also easily blow up.
	</description>
	<comments>
		<comment id='1' author='ciaobladoo' date='2020-05-25T01:00:05Z'>
		cc &lt;denchmark-link:https://github.com/stefanwebb&gt;@stefanwebb&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='ciaobladoo' date='2020-05-25T19:10:33Z'>
		&lt;denchmark-link:https://github.com/ciaobladoo&gt;@ciaobladoo&lt;/denchmark-link&gt;
 thanks for catching this bug! I've added a fix and an additional test to catch for this in the future (&lt;denchmark-link:https://github.com/pyro-ppl/pyro/pull/2504&gt;#2504&lt;/denchmark-link&gt;
)
I think the jury is still out on whether clamping does more good than harm in general, although happy to change it if it can be demonstrated that clamping improves optimization with some specific experiments in this case... The torch.nn.functional.logsigmoid should take care of some stability issues
		</comment>
	</comments>
</bug>