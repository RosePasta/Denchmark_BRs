<bug id='1778' author='cfperez' open_date='2019-02-28T21:47:14Z' closed_time='2019-03-06T19:23:40Z'>
	<summary>[discussion] Pyro sets all Parameters to requires_grad=True</summary>
	<description>
&lt;denchmark-h:h3&gt;Issue Description&lt;/denchmark-h&gt;

pyro.module() set requires_grad=True fo all nn.Parameters in a module, even if they were created with requires_grad=False.
Using register_buffer() solves this problem, but requires changing code that otherwise works in PyTorch.
It is hard to see how this would not be considered unexpected behavior. The question is how important is it, and if it is worth changing the behavior (and risk breaking other code?) or simply logging a warning + adding documentation.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


OSX, Python 3.6
PyTorch 1.0
Pyro 0.3.1

&lt;denchmark-h:h3&gt;Code Snippet&lt;/denchmark-h&gt;

import torch
import pyro

class Module(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.with_grad = torch.nn.Parameter(torch.zeros(1))
        self.no_grad = torch.nn.Parameter(torch.zeros(1), requires_grad=False)

m = Module()
assert m.with_grad.requires_grad
assert not m.no_grad.requires_grad

pyro_m = pyro.module('module', m)
assert pyro_m.with_grad.requires_grad
assert not pyro_m.no_grad.requires_grad
	</description>
	<comments>
		<comment id='1' author='cfperez' date='2019-02-28T23:07:52Z'>
		Thanks for the report &lt;denchmark-link:https://github.com/cfperez&gt;@cfperez&lt;/denchmark-link&gt;
 . Can you clarify your expected/desired behavior? When a PyTorch param has  does the parameter simply not get updated during training? If that is the case, I guess we could ignore such params in  and possible emit a warning recommending use of  (so as to maintain plausible backwards compatibility with previous versions of Pyro).
		</comment>
		<comment id='2' author='cfperez' date='2019-03-01T02:06:33Z'>
		
does the parameter simply not get updated during training

yes, from &lt;denchmark-link:https://pytorch.org/docs/stable/notes/autograd.html#excluding-subgraphs&gt;the pytorch docs&lt;/denchmark-link&gt;


we could ignore such params in pyro.module and possible emit a warning recommending use of register_buffer()

i can make this fix; i think this might have been a later pytorch feature we didnt account for - i seem to remember that when module was implemented in pyro (~pytorch 0.2), Parameters were defined to be Variables with requires_grad = True.
		</comment>
		<comment id='3' author='cfperez' date='2019-03-01T03:41:21Z'>
		&lt;denchmark-link:https://github.com/jpchen&gt;@jpchen&lt;/denchmark-link&gt;
 Great. Just to ensure we're in agreement: I believe the correct behavior is to  calling  inside  whenever the param has . Specifically, I believe changes should be limited to , and there should be no changes required to . Let me know if you disagree. Otherwise, thanks for implementing this!
		</comment>
		<comment id='4' author='cfperez' date='2019-03-01T08:56:44Z'>
		&lt;denchmark-link:https://github.com/cfperez&gt;@cfperez&lt;/denchmark-link&gt;
 what is your use case of non-grad parameters of a nn?  if you have fixed weights or non-learnable params can't you just not wrap it in ?
		</comment>
	</comments>
</bug>