<bug id='70' author='OptimusLime' open_date='2017-08-11T01:51:38Z' closed_time='2017-08-16T07:40:30Z'>
	<summary>Improve non-default Tensor type support, remove setting global flag</summary>
	<description>
Currently, to support GPU (or non-float) tensors, there is a global set_cuda method for pyro. This sets a global default tensor type for torch, when we should be careful to call type_as when constructing autograd Variables.
This is especially problematic in the distributions library where there are no type_as calls.
Here is an example demonstrating what I mean:
import torch
from torch.autograd import Variable
from pyro.distributions import DiagNormal

z = Variable(torch.randn(4))
mu = Variable(torch.randn(4))
sigma = torch.exp(Variable(torch.randn(4))

# Succeeds
DiagNormal().batch_log_pdf(z, mu=mu, sigma=sigma)

# Fails
DiagNormal().batch_log_pdf(z.cuda(), mu=mu.cuda(), sigma=sigma.cuda())

# Fails
DiagNormal().batch_log_pdf(z.double(), mu=mu.double(), sigma=sigma.double()) 
I know there are global flags:
# global tensor constructors become all cuda 
pyro.set_cuda()

# global float 
pyro.set_cpu()
But it's not very good practice to force users to set global torch tensor types just to get non-float tensor support in pyro distributions. Before wider release, I'd like to correct this behavior.
	</description>
	<comments>
		<comment id='1' author='OptimusLime' date='2017-08-11T15:29:36Z'>
		is the suggestion that distributions should detect the type of params and type_as appropriately? if so, then that seems reasonable. though there could be case where a distribution has no tensor args and so must rely on some global default to choose tensor output type?
		</comment>
		<comment id='2' author='OptimusLime' date='2017-08-11T20:31:16Z'>
		Basically, any time a Variable is called somewhere in pyro, it should be followed by type_as. This happens to occur in almost all the distribution classes.
Here is an example from DiagNormal that explains above failure case (&lt;denchmark-link:https://github.com/uber/pyro/blob/dev/pyro/distributions/diag_normal.py#L70&gt;https://github.com/uber/pyro/blob/dev/pyro/distributions/diag_normal.py#L70&lt;/denchmark-link&gt;
)
   def batch_log_pdf(self, x, mu=None, sigma=None, batch_size=1, *args, **kwargs):
        """
        Diagonal Normal log-likelihood
        """
        # expand to patch size of input
        _mu, _sigma = self._sanitize_input(mu, sigma)
        if x.dim() == 1 and _mu.dim() == 1 and batch_size == 1:
            return self.log_pdf(x, _mu, _sigma)
        elif x.dim() == 1:
            x = x.expand(batch_size, x.size(0))
        log_pxs = -1 * torch.add(torch.add(torch.log(_sigma),
                                 0.5 * torch.log(2.0 * np.pi *
                                 # This Variable creation causes the error. 
                                 # Should have .type_as(_sigma)
                                 Variable(torch.ones(_sigma.size())))),
                                 0.5 * torch.pow(((x - _mu) / _sigma), 2))
        return torch.sum(log_pxs, 1)
		</comment>
	</comments>
</bug>