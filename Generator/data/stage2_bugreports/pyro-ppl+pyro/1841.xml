<bug id='1841' author='curtis999' open_date='2019-04-30T03:16:49Z' closed_time='2019-05-08T05:55:21Z'>
	<summary>LDA tutorial: incorrect tensor shape when document size is larger than the vocabulary size</summary>
	<description>



pyro/examples/lda.py


         Line 101
      in
      56c0617






 counts.scatter_add_(0, data[:, ind], torch.tensor(1.).expand(counts.shape)) 





The filler array of ones should have expanded into the size of the sliced data. This is problematic when the number of words per document is larger than the vocabulary size
Try
counts.scatter_add_(0, data[:, ind], torch.tensor(1.).expand(data[:,ind].shape))
instead
	</description>
	<comments>
		<comment id='1' author='curtis999' date='2019-04-30T17:02:23Z'>
		Thanks &lt;denchmark-link:https://github.com/curtis999&gt;@curtis999&lt;/denchmark-link&gt;
 for the report and fix! Do you have a simple example we can add as a regression test? It can be totally random data.
		</comment>
		<comment id='2' author='curtis999' date='2019-04-30T21:36:15Z'>
		&lt;denchmark-link:https://github.com/fritzo&gt;@fritzo&lt;/denchmark-link&gt;
 the error can be replicated simply by passing  or whatever that sets the vocabulary size to be smaller than the total number of words in a document.
In the example, the data is from the generative model, or we can explicatly generate it with
&lt;denchmark-code&gt;theta = dist.Dirichlet(torch.zeros([args.num_docs, args.num_topics]) + 0.2).sample()
phi = dist.Dirichlet(torch.zeros([args.num_topics, args.num_words]) + 0.1).sample()
z = dist.Categorical(theta).expand([args.num_words_per_doc, args.num_docs]).sample()
data = dist.Categorical(phi[z]).sample()
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>