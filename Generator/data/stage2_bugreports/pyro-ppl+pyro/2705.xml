<bug id='2705' author='fritzo' open_date='2020-12-03T13:27:00Z' closed_time='2020-12-04T04:37:33Z'>
	<summary>PyroModule.requires_grad_(False).__call__() issues unwanted warning</summary>
	<description>
After training a PyroModule model, I'd like to detach it and use it elsewhere in a "frozen" state. One obvious way to do so would be to call my_module.requires_grad_(False) to detach all parameters. Doing so results in the desired behavior, but has the side effect of triggering the following warning (many many times):
&lt;denchmark-code&gt;/Users/fobermey/github/pyro-ppl/pyro/pyro/primitives.py:361: UserWarning: linear.bias was not registered in the param store because requires_grad=False
  " requires_grad=False")
&lt;/denchmark-code&gt;

It is unclear to me how to resolve this. Should we provide a detach_() method for modules that converts all PyroParams to Tensors? Should we simply remove that warning if it occurs in a PyroModule? Should we be calling some other method like my_module.train(False)?
	</description>
	<comments>
	</comments>
</bug>