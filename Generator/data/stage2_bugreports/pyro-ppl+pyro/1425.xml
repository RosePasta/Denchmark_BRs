<bug id='1425' author='neerajprad' open_date='2018-09-30T20:04:34Z' closed_time='2018-10-02T03:36:58Z'>
	<summary>test_mixture_of_diag_normals gives wrong loss on linux</summary>
	<description>
The following tests fail on linux, because the hand_loss (~85) is very different from the auto_loss (~20). I am only observing this issue with PyTorch master on linux, and not Mac OSX.
&lt;denchmark-code&gt;tests/infer/test_enum.py::test_mixture_of_diag_normals[MixtureOfDiagNormals-scale0] 
tests/infer/test_enum.py::test_mixture_of_diag_normals[MixtureOfDiagNormalsSharedCovariance-scale1]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='neerajprad' date='2018-09-30T20:04:50Z'>
		cc. &lt;denchmark-link:https://github.com/martinjankowiak&gt;@martinjankowiak&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='neerajprad' date='2018-10-01T19:28:40Z'>
		This is related to &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/12230&gt;pytorch/pytorch#12230&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='3' author='neerajprad' date='2018-10-01T20:45:13Z'>
		We're only using torch.masked_fill_() in three places in Pyro. I think it's reasonable to work around this via a helper
# workaround for https://github.com/pytorch/pytorch/issues/12230
def torch_masked_fill(destin, mask, value):
    destin.masked_fill_(mask.contiguous(), value)
		</comment>
		<comment id='4' author='neerajprad' date='2018-10-01T21:26:03Z'>
		Sounds good. I'll use this helper in the pytorch-0.5.0 branch. We can keep this issue open as a reminder to remove it once the upstream issue is resolved.
		</comment>
		<comment id='5' author='neerajprad' date='2018-10-02T03:36:56Z'>
		Since we only require the destination tensor to be contiguous, there was only a single place in the code where we needed to make a change (in pytorch-0.5.0). Closing this issue, we can directly track the upstream issue instead.
		</comment>
	</comments>
</bug>