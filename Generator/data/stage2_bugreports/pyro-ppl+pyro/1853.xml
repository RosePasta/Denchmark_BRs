<bug id='1853' author='ajrcampbell' open_date='2019-05-07T11:07:28Z' closed_time='2019-05-08T16:20:48Z'>
	<summary>Memory leak using Dirichlet distribution in guide</summary>
	<description>
&lt;denchmark-h:h3&gt;Issue Description&lt;/denchmark-h&gt;

Memory leak encountered on CPU when sampling from a Dirichlet distribution in a guide over many training iterations. The leak only occurs when the parameters of the Dirichlet distribution are defined using param.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

OS: Ubuntu 18.04.2
Python: 3.6.7
PyTorch: 1.1.0
Pyro: 0.3.3
&lt;denchmark-h:h3&gt;Code Snippet&lt;/denchmark-h&gt;

import torch
from torch.distributions import constraints

import pyro
from pyro.infer import SVI, Trace_ELBO
from pyro.optim import Adam
import pyro.distributions as dist


a = 20
b = 100000

def model(x):
    pass

def guide(x):
    alpha = pyro.param('alpha', torch.ones(a, b), constraint=constraints.positive)                  
    with pyro.plate('sample_x', a):
        pyro.sample('x', dist.Dirichlet(alpha))

elbo = Trace_ELBO(max_plate_nesting=1, vectorize_particles=True)
opt = Adam({'lr': 0.0001})
svi = SVI(model, guide, opt, loss=elbo)

for epoch in range(500):
    svi.step(1)
Changing alpha in the guide to
&lt;denchmark-code&gt;alpha = torch.ones(a, b) / b 
&lt;/denchmark-code&gt;

stops the leak from occurring (ignore errors).
	</description>
	<comments>
		<comment id='1' author='ajrcampbell' date='2019-05-07T16:43:15Z'>
		&lt;denchmark-link:https://github.com/ajrcampbell&gt;@ajrcampbell&lt;/denchmark-link&gt;
 Thanks for the clear bug report! I'll try to reproduce.
		</comment>
		<comment id='2' author='ajrcampbell' date='2019-05-07T17:41:31Z'>
		This appears to be a PyTorch issue. I'll first try fixing torch._dirichlet_grad() upstream and then try to push a workaround patch to Pyro if possible. To reproduce the PyTorch issue it suffices to:
import resource, torch
concentration = torch.ones(20, 100000)
total = concentration.sum(-1, True).expand_as(concentration)
x = concentration / total
for epoch in range(500):
    torch._dirichlet_grad(x, concentration, total)
    print('maxrss = {}'.format(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss))
		</comment>
		<comment id='3' author='ajrcampbell' date='2019-05-08T15:00:33Z'>
		I believe this can be closed now.
		</comment>
		<comment id='4' author='ajrcampbell' date='2019-05-08T15:42:07Z'>
		How can I use the fix?
		</comment>
		<comment id='5' author='ajrcampbell' date='2019-05-08T15:44:12Z'>
		You can try installing PyTorch from source using the current master branch, or use the nightly builds published everyday (in which case you will have to wait until tomorrow I suppose).
		</comment>
	</comments>
</bug>