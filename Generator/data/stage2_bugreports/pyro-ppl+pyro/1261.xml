<bug id='1261' author='windj007' open_date='2018-07-24T13:17:51Z' closed_time='2018-08-06T12:48:31Z'>
	<summary>Importance sampling on cuda</summary>
	<description>
I have a classification model with latent variables and I would like to empirically calculate p(y|x) = p(y|x,z) p(z|x) over many sampled z values (make a prediction).
I'm trying to do this by:
def predict(self, features, samples_n=100):
     posterior = infer.Importance(self.model, guide=self.guide, num_samples=samples_n)
     trace = posterior.run(features, labels=None)
     marginal = infer.EmpiricalMarginal(trace)
     return marginal()
My model is on CUDA and I get the following exception:
&lt;denchmark-code&gt;Traceback (most recent call last):
...
  File "/usr/local/miniconda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/miniconda/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 115, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/usr/local/miniconda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "...", line 131, in forward
    trace = posterior.run(video, audio, None)
  File "/usr/local/miniconda/lib/python3.6/site-packages/pyro/infer/abstract_infer.py", line 83, in run
    for tr, logit in poutine.block(self._traces)(*args, **kwargs):
  File "/usr/local/miniconda/lib/python3.6/site-packages/pyro/infer/importance.py", line 45, in _traces
    log_weight = model_trace.log_prob_sum() - guide_trace.log_prob_sum()
  File "/usr/local/miniconda/lib/python3.6/site-packages/pyro/poutine/trace_struct.py", line 236, in log_prob_sum
    log_p += site_log_p
RuntimeError: Expected object of type torch.FloatTensor but found type torch.cuda.FloatTensor for argument #4 'other'
&lt;/denchmark-code&gt;

Something wrong here? Is this a proper way to achieve an ensemble-like predictions?
	</description>
	<comments>
		<comment id='1' author='windj007' date='2018-07-24T17:52:56Z'>
		Hi, are you sure all of your model and guide tensors (data, priors, parameters) are of the right type?  You might also need to manually type some scalars by making them size-0 tensors.
		</comment>
		<comment id='2' author='windj007' date='2018-07-25T08:00:11Z'>
		Hi, yes. I'm training this model using SVI. Earlier I predicted without sampling (just from mean hidden values), but now I want to try sample it. This is what my model looks like:
class Simple1(nn.Module):
    def __init__(self, input_size, labels_n, hidden_size=1000, y_decoder_kwargs={}, encoder_kwargs={}):
        super().__init__()
        self._hidden_size = hidden_size
        self._y_decoder = FCN(input_size + hidden_size, labels_n, out_activation=nn.Sigmoid, **y_decoder_kwargs)
        self._encoder = FCN(input_size, hidden_size*2, **encoder_kwargs)

    def model(self, inputs, labels):
        pyro.module('y_decoder', self._y_decoder)

        batch_size = inputs.size(0)

        with pyro.iarange('data', batch_size):
            hidden_mean = inputs.new_zeros(batch_size, self._hidden_size)
            hidden_std = inputs.new_ones(batch_size, self._hidden_size)
            hidden = pyro.sample('hidden', dist.Normal(hidden_mean, hidden_std).independent(1))

            return pyro.sample('labels',
                               dist.Bernoulli(self._y_decoder.forward(torch.cat([inputs, hidden], dim=1))).independent(1),
                               obs=labels)

    def guide(self, inputs, labels):
        pyro.module('encoder', self._encoder)
        with pyro.iarange('data', inputs.size(0)):
            hidden_params = self._encoder.forward(inputs)
            hidden_mean = hidden_params[:, :hidden_params.size(1) // 2]
            hidden_std = torch.exp(hidden_params[:, hidden_params.size(1) // 2:])
            pyro.sample('hidden',
                        dist.Normal(hidden_mean, hidden_std).independent(1))

    def forward(self, inputs, samples_n=100):
        posterior = infer.Importance(self.model, guide=self.guide, num_samples=samples_n)
        trace = posterior.run(inputs, None)  # here we get exception
        marginal = infer.EmpiricalMarginal(trace)
        return marginal()

    # predict without sampling - it's working
    def forward2(self, video, audio):
        inputs = torch.cat([video, audio], dim=1)
        hidden_params = self._encoder.forward(torch.cat([video, audio], dim=1))
        hidden_mean = hidden_params[:, :hidden_params.size(1) // 2]
        return self._y_decoder.forward(torch.cat([inputs, hidden_mean], dim=1))
FCN is a multi-layer perceptron. This component is used in many parts of project, so it's well-tested.
Also, if I train on CPU, sampling works. Copying model to CPU after each batch to evaluate domain metrics is not OK :).
		</comment>
		<comment id='3' author='windj007' date='2018-07-25T17:01:21Z'>
		Hi &lt;denchmark-link:https://github.com/windj007&gt;@windj007&lt;/denchmark-link&gt;
, you may need to pass  to  (see &lt;denchmark-link:http://docs.pyro.ai/en/0.2.1-release/primitives.html#pyro.iarange&gt;docs&lt;/denchmark-link&gt;
).
(jp: edited to change link to point to 0.2.1 docs)
		</comment>
		<comment id='4' author='windj007' date='2018-08-06T12:48:31Z'>
		It worked, thank you!
		</comment>
	</comments>
</bug>