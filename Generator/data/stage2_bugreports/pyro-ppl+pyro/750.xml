<bug id='750' author='jinserk' open_date='2018-02-07T22:11:31Z' closed_time='2018-02-23T19:58:25Z'>
	<summary>New RuntimeError of ss_vae_M2.py with -cuda option</summary>
	<description>
Hello. With the latest pytorch update (0.4.0a0+bf60329) and pyro (53bf515), I've got a new error as:
&lt;denchmark-code&gt;$ python ss_vae_M2.py -cuda
THCudaCheck FAIL file=/home/jbaik/setup/pytorch/pytorch/aten/src/THC/generated/../generic/THCTensorMathPointwise.cu line=410 error=59 : device-side assert triggered
Traceback (most recent call last):
  File "ss_vae_M2.py", line 450, in &lt;module&gt;
    run_inference_ss_vae(args)
  File "ss_vae_M2.py", line 357, in run_inference_ss_vae
    run_inference_for_epoch(data_loaders, losses, periodic_interval_batches)
  File "ss_vae_M2.py", line 254, in run_inference_for_epoch
    new_loss = losses[loss_id].step(xs)
  File "/home/jbaik/.pyenv/versions/3.6.4/lib/python3.6/site-packages/pyro_ppl-0.1.2-py3.6.egg/pyro/infer/svi.py", line 98, in step
    loss = self.loss_and_grads(self.model, self.guide, *args, **kwargs)
  File "/home/jbaik/.pyenv/versions/3.6.4/lib/python3.6/site-packages/pyro_ppl-0.1.2-py3.6.egg/pyro/infer/elbo.py", line 65, in loss_and_grads
    return self.which_elbo.loss_and_grads(model, guide, *args, **kwargs)
  File "/home/jbaik/.pyenv/versions/3.6.4/lib/python3.6/site-packages/pyro_ppl-0.1.2-py3.6.egg/pyro/infer/trace_elbo.py", line 146, in loss_and_grads
    for weight, model_trace, guide_trace, log_r in self._get_traces(model, guide, *args, **kwargs):
  File "/home/jbaik/.pyenv/versions/3.6.4/lib/python3.6/site-packages/pyro_ppl-0.1.2-py3.6.egg/pyro/infer/trace_elbo.py", line 87, in _get_traces
    guide_trace = poutine.trace(guide).get_trace(*args, **kwargs)
  File "/home/jbaik/.pyenv/versions/3.6.4/lib/python3.6/site-packages/pyro_ppl-0.1.2-py3.6.egg/pyro/poutine/trace_poutine.py", line 250, in get_trace
    self(*args, **kwargs)
  File "/home/jbaik/.pyenv/versions/3.6.4/lib/python3.6/site-packages/pyro_ppl-0.1.2-py3.6.egg/pyro/poutine/trace_poutine.py", line 238, in __call__
    ret = super(TracePoutine, self).__call__(*args, **kwargs)
  File "/home/jbaik/.pyenv/versions/3.6.4/lib/python3.6/site-packages/pyro_ppl-0.1.2-py3.6.egg/pyro/poutine/poutine.py", line 147, in __call__
    return self.fn(*args, **kwargs)
  File "ss_vae_M2.py", line 154, in guide
    pyro.sample("z", dist.normal, mu, sigma, extra_event_dims=1)
  File "/home/jbaik/.pyenv/versions/3.6.4/lib/python3.6/site-packages/pyro_ppl-0.1.2-py3.6.egg/pyro/__init__.py", line 87, in sample
    apply_stack(msg)
  File "/home/jbaik/.pyenv/versions/3.6.4/lib/python3.6/site-packages/pyro_ppl-0.1.2-py3.6.egg/pyro/util.py", line 243, in apply_stack
    default_process_message(msg)
  File "/home/jbaik/.pyenv/versions/3.6.4/lib/python3.6/site-packages/pyro_ppl-0.1.2-py3.6.egg/pyro/util.py", line 53, in default_process_message
    val = fn(*args, **kwargs)
  File "/home/jbaik/.pyenv/versions/3.6.4/lib/python3.6/site-packages/pyro_ppl-0.1.2-py3.6.egg/pyro/distributions/random_primitive.py", line 46, in sample
    return self.dist_class(*args, **kwargs).sample(sample_shape)
  File "/home/jbaik/.pyenv/versions/3.6.4/lib/python3.6/site-packages/pyro_ppl-0.1.2-py3.6.egg/pyro/distributions/torch_wrapper.py", line 39, in sample
    return self.torch_dist.rsample(sample_shape)
  File "/home/jbaik/.pyenv/versions/3.6.4/lib/python3.6/site-packages/torch/distributions/normal.py", line 59, in rsample
    return self.loc + eps * self.scale
RuntimeError: cuda runtime error (59) : device-side assert triggered at /home/jbaik/setup/pytorch/pytorch/aten/src/THC/generated/../generic/THCTensorMathPointwise.cu:410
/home/jbaik/setup/pytorch/pytorch/aten/src/THC/THCTensorRandom.cuh:193: void sampleMultinomialOnce(long *, long, int, T *, T *, int, int) [with T = float, AccT = float]: block: [43,0,0], thread: [0,0,0] Assertion `THCNumerics&lt;AccT&gt;::gt(su
m, accZero)` failed.
/home/jbaik/setup/pytorch/pytorch/aten/src/THC/THCTensorRandom.cuh:193: void sampleMultinomialOnce(long *, long, int, T *, T *, int, int) [with T = float, AccT = float]: block: [21,0,0], thread: [0,0,0] Assertion `THCNumerics&lt;AccT&gt;::gt(su
m, accZero)` failed.
/home/jbaik/setup/pytorch/pytorch/aten/src/THC/THCTensorRandom.cuh:193: void sampleMultinomialOnce(long *, long, int, T *, T *, int, int) [with T = float, AccT = float]: block: [36,0,0], thread: [0,0,0] Assertion `THCNumerics&lt;AccT&gt;::gt(su
m, accZero)` failed.
/home/jbaik/setup/pytorch/pytorch/aten/src/THC/THCTensorRandom.cuh:193: void sampleMultinomialOnce(long *, long, int, T *, T *, int, int) [with T = float, AccT = float]: block: [52,0,0], thread: [0,0,0] Assertion `THCNumerics&lt;AccT&gt;::gt(su
m, accZero)` failed.
/home/jbaik/setup/pytorch/pytorch/aten/src/THC/THCTensorRandom.cuh:193: void sampleMultinomialOnce(long *, long, int, T *, T *, int, int) [with T = float, AccT = float]: block: [56,0,0], thread: [0,0,0] Assertion `THCNumerics&lt;AccT&gt;::gt(su
m, accZero)` failed.
/home/jbaik/setup/pytorch/pytorch/aten/src/THC/THCTensorRandom.cuh:193: void sampleMultinomialOnce(long *, long, int, T *, T *, int, int) [with T = float, AccT = float]: block: [7,0,0], thread: [0,0,0] Assertion `THCNumerics&lt;AccT&gt;::gt(sum
, accZero)` failed.
/home/jbaik/setup/pytorch/pytorch/aten/src/THC/THCTensorRandom.cuh:193: void sampleMultinomialOnce(long *, long, int, T *, T *, int, int) [with T = float, AccT = float]: block: [11,0,0], thread: [0,0,0] Assertion `THCNumerics&lt;AccT&gt;::gt(su
m, accZero)` failed.
/home/jbaik/setup/pytorch/pytorch/aten/src/THC/THCTensorRandom.cuh:193: void sampleMultinomialOnce(long *, long, int, T *, T *, int, int) [with T = float, AccT = float]: block: [59,0,0], thread: [0,0,0] Assertion `THCNumerics&lt;AccT&gt;::gt(su
m, accZero)` failed.
/home/jbaik/setup/pytorch/pytorch/aten/src/THC/THCTensorRandom.cuh:193: void sampleMultinomialOnce(long *, long, int, T *, T *, int, int) [with T = float, AccT = float]: block: [14,0,0], thread: [0,0,0] Assertion `THCNumerics&lt;AccT&gt;::gt(su
m, accZero)` failed.
/home/jbaik/setup/pytorch/pytorch/aten/src/THC/THCTensorRandom.cuh:193: void sampleMultinomialOnce(long *, long, int, T *, T *, int, int) [with T = float, AccT = float]: block: [8,0,0], thread: [0,0,0] Assertion `THCNumerics&lt;AccT&gt;::gt(sum
, accZero)` failed.
/home/jbaik/setup/pytorch/pytorch/aten/src/THC/THCTensorRandom.cuh:193: void sampleMultinomialOnce(long *, long, int, T *, T *, int, int) [with T = float, AccT = float]: block: [23,0,0], thread: [0,0,0] Assertion `THCNumerics&lt;AccT&gt;::gt(su
m, accZero)` failed.
/home/jbaik/setup/pytorch/pytorch/aten/src/THC/THCTensorRandom.cuh:193: void sampleMultinomialOnce(long *, long, int, T *, T *, int, int) [with T = float, AccT = float]: block: [25,0,0], thread: [0,0,0] Assertion `THCNumerics&lt;AccT&gt;::gt(su
m, accZero)` failed.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='jinserk' date='2018-02-08T02:09:08Z'>
		Hi, I suspect this issue has the same underlying cause as the one in &lt;denchmark-link:https://forum.pyro.ai/t/ss-vae-unsupervised-loss-problems/103&gt;this Pyro forum thread&lt;/denchmark-link&gt;
.  There was a recent change (&lt;denchmark-link:https://github.com/pyro-ppl/pyro/pull/695&gt;#695&lt;/denchmark-link&gt;
 ) that added drawing a single sample internally before enumerating to infer the correct shape of the enumerated values, and this error seems to be caused by invalid weights in that sampling step.  The Python stack trace seems incorrect given the source of the error in THC.
cc &lt;denchmark-link:https://github.com/fritzo&gt;@fritzo&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='jinserk' date='2018-02-08T02:24:47Z'>
		Sounds like we should instead call site["fn"].shape() and and deprecate RandomizedPrimitive.
		</comment>
		<comment id='3' author='jinserk' date='2018-02-08T02:46:21Z'>
		
Sounds like we should instead call site["fn"].shape() and and deprecate RandomizedPrimitive

Agreed, but that doesn't address the underlying cause of this issue and the one on the forum, which is the imbalanced categorical weights.  We should figure out how to make this example more robust.
		</comment>
		<comment id='4' author='jinserk' date='2018-02-08T22:43:38Z'>
		Could you let me know if there is any workaround for this? I have also zero loss issue for unsupervised samples as described in the &lt;denchmark-link:https://forum.pyro.ai/t/ss-vae-unsupervised-loss-problems/103&gt;Pyro forum thread&lt;/denchmark-link&gt;
, as &lt;denchmark-link:https://github.com/eb8680&gt;@eb8680&lt;/denchmark-link&gt;
 said.
		</comment>
		<comment id='5' author='jinserk' date='2018-02-09T05:53:37Z'>
		Sorry &lt;denchmark-link:https://github.com/jinserk&gt;@jinserk&lt;/denchmark-link&gt;
 I haven't had time to diagnose the underlying cause. I think your best options are to either use Pyro 0.1.2 release or wait a few weeks until Pyro dev branch stabilizes. We're currently in the middle of a big refactoring to support PyTorch distributions, distribution broadcasting, multiple batch dimensions, nested s, and parallel .
If you have time to help us diagnose this, it would be really helpful to grab the logits or probs arg to the Multinomial or Categorical sampler that is causing the cuda failure. If we can narrow this down to a reproducible test case, I could submit numerical stability fixes upstream to PyTorch.
		</comment>
		<comment id='6' author='jinserk' date='2018-02-22T15:07:35Z'>
		&lt;denchmark-link:https://github.com/jinserk&gt;@jinserk&lt;/denchmark-link&gt;
 How is it going? I use pytorch with commit  as suggested at &lt;denchmark-link:https://github.com/uber/pyro/blob/dev/README.md&gt;https://github.com/uber/pyro/blob/dev/README.md&lt;/denchmark-link&gt;
 and the current dev branch. The  option works for me and I did not see the zero loss issue for unsupervised examples.
		</comment>
		<comment id='7' author='jinserk' date='2018-02-22T21:12:14Z'>
		Thanks &lt;denchmark-link:https://github.com/fehiepsi&gt;@fehiepsi&lt;/denchmark-link&gt;
 ! I figured out the latest update of pyro changed the example ss_vae code, but I have a running training so couldn't update it. Will check it soon!
		</comment>
		<comment id='8' author='jinserk' date='2018-02-23T15:33:59Z'>
		I've update pytorch and pyro and tested ss_vae_M2.py with -cuda option. It seems works! Thanks for the great job!
		</comment>
	</comments>
</bug>