<bug id='619' author='rstebbing' open_date='2017-12-11T00:08:36Z' closed_time='2018-02-06T22:45:15Z'>
	<summary>Default guide for `Importance` gives non-zero `log_weight`</summary>
	<description>
The default guide in Importance blocks all sites (and not just those observed). This leads to the guide_trace being empty (except for the input and return values). As a result, no samples are reused when replayed on the model and the guide_trace.log_pdf() evaluates to 0.0. The log_weight is then equal to the model_trace.log_pdf() (which is also evaluated on different samples), which I believe is unintended and incorrect.
The program below illustrates this for a simple univariate Gaussian, where my proposal and target distribution are identical and I would expect the log of the weights to be 0.0. The latter is only the case when the sites are explicitly exposed.
import torch
from torch.autograd import Variable

import pyro
from pyro import distributions as dist
from pyro import infer
from pyro import poutine


def gaussian():
    return pyro.sample('x', dist.normal,
                       Variable(torch.Tensor([0.0])),
                       Variable(torch.Tensor([1.0])))


# Using `Importance` with the default `guide`, the `log_weight` is equal to the
# `model_trace.log_pdf()`. That is, the `guide_trace.log_pdf()` (evaluated
# internally) is incorrectly `0.0`.
print('importance_default_guide:')
importance_default_guide = infer.Importance(gaussian, num_samples=10)
for model_trace, log_weight in importance_default_guide._traces():
    model_trace_log_pdf = model_trace.log_pdf()
    are_equal = log_weight.data[0] == model_trace_log_pdf.data[0]
    print(log_weight.data[0], are_equal)

# However, setting the `guide` to expose `x` ensures that it is replayed so
# that the `log_weight` is exactly zero for each sample.
print('importance_exposed_guide:')
importance_exposed_guide = infer.Importance(
    gaussian,
    guide=poutine.block(gaussian, expose=['x']),
    num_samples=10)
for model_trace, log_weight in importance_exposed_guide._traces():
    print(log_weight.data[0])
&lt;denchmark-code&gt;importance_default_guide:
-0.9368391633033752 True
-1.3421428203582764 True
-0.9189755320549011 True
-2.1423826217651367 True
-2.301940679550171 True
-1.142196774482727 True
-0.9449963569641113 True
-2.7146053314208984 True
-3.420013904571533 True
-1.7994171380996704 True
importance_exposed_guide:
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='rstebbing' date='2017-12-12T20:29:30Z'>
		&lt;denchmark-link:https://github.com/rstebbing&gt;@rstebbing&lt;/denchmark-link&gt;
 Thanks for the clear reproducible example!
&lt;denchmark-link:https://github.com/eb8680&gt;@eb8680&lt;/denchmark-link&gt;
 Can you confirm that this is a bug and not intended behavior?
		</comment>
		<comment id='2' author='rstebbing' date='2017-12-12T22:12:55Z'>
		Thanks &lt;denchmark-link:https://github.com/rstebbing&gt;@rstebbing&lt;/denchmark-link&gt;
, this seems like an issue with .  I'll push a fix shortly.
		</comment>
		<comment id='3' author='rstebbing' date='2018-01-10T18:33:34Z'>
		&lt;denchmark-link:https://github.com/eb8680&gt;@eb8680&lt;/denchmark-link&gt;
 did you push a fix for this?
		</comment>
	</comments>
</bug>