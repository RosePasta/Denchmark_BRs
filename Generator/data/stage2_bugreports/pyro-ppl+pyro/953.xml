<bug id='953' author='neerajprad' open_date='2018-03-29T19:34:25Z' closed_time='2018-04-05T17:29:47Z'>
	<summary>Tests failing on PyTorch master</summary>
	<description>
The following tests (list may increase) fail on PyTorch master, which seem unrelated to tensor casting issue in &lt;denchmark-link:https://github.com/pyro-ppl/pyro/issues/944&gt;#944&lt;/denchmark-link&gt;
. I encountered these while fixing the tensor casting issues in &lt;denchmark-link:https://github.com/pyro-ppl/pyro/pull/952&gt;#952&lt;/denchmark-link&gt;
. Easy way to replicate this would be to run  with commit  --&gt;  in README.md*. I might skip these for the time being.

 bayesian_regression.py - RuntimeError: you can only change requires_grad flags of leaf variables. cc. @jpchen
 Tests using ADVIMultivariateNormal in test_advi.py - advi covariance off. cc. @fritzo
 Tests in gp/test_models.py - test_inference, test_inference_svgp, test_inference_deepGP. Lapack Error in potrf : the leading minor of order 2 is not positive definite, and assertion failure on variance. cc. @fehiepsi
 Tests in gp/test_likelihoods - same as above.
 gp.ipynb - Non-deterministic travis failures due to constraint checking in multivariate normal. See discussion below.

EDIT - After removing the mvn wrapper, all but bayesian_regression.py and gp.ipynb need to be fixed.
* Separately I will modify the script to take in an optional arg for build commit.
	</description>
	<comments>
		<comment id='1' author='neerajprad' date='2018-03-30T06:27:11Z'>
		I think these lines trigger the "requires_grad flags" issue:
&lt;denchmark-link:https://github.com/uber/pyro/blob/de50a8ee5854c4f00f37cbd851b536b8fbfe2151/examples/bayesian_regression.py#L76&gt;https://github.com/uber/pyro/blob/de50a8ee5854c4f00f37cbd851b536b8fbfe2151/examples/bayesian_regression.py#L76&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/uber/pyro/blob/de50a8ee5854c4f00f37cbd851b536b8fbfe2151/examples/bayesian_regression.py#L79&gt;https://github.com/uber/pyro/blob/de50a8ee5854c4f00f37cbd851b536b8fbfe2151/examples/bayesian_regression.py#L79&lt;/denchmark-link&gt;

The issue is that w_mu is that the type_as comes after the requires_grad assignment, so w_mu is no longer a leaf variable. The existing code is equivalent to:
w_mu_hidden = torch.randn(1, p, requires_grad=True)  # leaf variable (optimizable)
w_mu = w_mu_hidden.type_as(data)  # backprops to of w_mu_hidden via backward type conversion
A fix is:
w_mu = torch.randn(1, p).type_as(data)
w_mu.requires_grad = True
(We need a better way to avoid or detect these sorts of issues in PyTorch)
		</comment>
		<comment id='2' author='neerajprad' date='2018-03-30T06:30:54Z'>
		I've been seeing spurrious failures in gp.ipynb. Are the random seeds being fixed? &lt;denchmark-link:https://github.com/fehiepsi&gt;@fehiepsi&lt;/denchmark-link&gt;
 any ideas? &lt;denchmark-link:https://travis-ci.org/uber/pyro/jobs/360155682&gt;https://travis-ci.org/uber/pyro/jobs/360155682&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='neerajprad' date='2018-03-30T06:43:28Z'>
		
I've been seeing spurrious failures in gp.ipynb. Are the random seeds being fixed? @fehiepsi any ideas? https://travis-ci.org/uber/pyro/jobs/360155682

I restarted the run, but I noticed it too. The issue is that the constraint checking complains that the precision matrix has invalid values, but the constructor was called without the precision matrix in the first place. I think it may have to do with the precision_matrix being lazy loading. The other thing is that validate_args is reversing all the gains by actually eagerly initializing all of these properties.
		</comment>
		<comment id='4' author='neerajprad' date='2018-03-30T06:45:30Z'>
		
The issue is that w_mu is that the type_as comes after the requires_grad assignment, so w_mu is no longer a leaf variable. The existing code is equivalent to:

Thanks for explaining, &lt;denchmark-link:https://github.com/colesbury&gt;@colesbury&lt;/denchmark-link&gt;
. That makes it super clear what's happening! cc. &lt;denchmark-link:https://github.com/jpchen&gt;@jpchen&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='5' author='neerajprad' date='2018-03-30T06:52:47Z'>
		&lt;denchmark-link:https://github.com/fritzo&gt;@fritzo&lt;/denchmark-link&gt;
 - &lt;denchmark-link:https://github.com/pyro-ppl/pyro/pull/956&gt;#956&lt;/denchmark-link&gt;
 temporarily disables smoke test on the gp tutorial, until we investigate and fix that upstream.
		</comment>
		<comment id='6' author='neerajprad' date='2018-03-30T10:02:34Z'>
		&lt;denchmark-link:https://github.com/fritzo&gt;@fritzo&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/neerajprad&gt;@neerajprad&lt;/denchmark-link&gt;
 I run the notebook locally and was able to reproduce the issue. That covariance matrix is 100x100 dimensional and positive definite (after checking with ) but its inverse (precision matrix) is not positive definite. Its smallest eigenvalue is about -0.2, all other eigenvalues are positive. I made a notebook to observe the behavior here: &lt;denchmark-link:https://gist.github.com/fehiepsi/e33d8a0ffbb52ca3a8f7d589c9dcb9ab&gt;https://gist.github.com/fehiepsi/e33d8a0ffbb52ca3a8f7d589c9dcb9ab&lt;/denchmark-link&gt;

I don't know if it is better to use  version instead of . &lt;denchmark-link:https://github.com/SsnL&gt;@SsnL&lt;/denchmark-link&gt;
 what do you think about calculating the lazy precision_matrix based on scale_tril instead of covariance_matrix?
Edit: I think that it is better to use scale_tril version. In future, if we have grad for potri, it is also easier to change. So I made a pull request in pytorch.
		</comment>
	</comments>
</bug>