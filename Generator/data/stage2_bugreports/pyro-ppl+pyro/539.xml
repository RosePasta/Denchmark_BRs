<bug id='539' author='shami-nisimov' open_date='2017-11-09T09:40:41Z' closed_time='2018-04-24T04:01:43Z'>
	<summary>reproduce vae sample results</summary>
	<description>
in the documentation there is a figure showing the ELBO for the test dataset.
after 50 epochs, the ELBO reaches below -80.
when running the vae sample, after 200 epochs the ELBO reaches -100.
is it a bug in the sample code ?
	</description>
	<comments>
		<comment id='1' author='shami-nisimov' date='2017-11-09T15:58:30Z'>
		Thanks for commenting. We ran exactly the sample code to produce results
and were aiming for complete transparency. The elbo is a bit inflated
because we did not binarize images across all our examples, but the
discrepancy you describe is too far from acceptable. I will investigate,
this should be 100% replicable.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Nov 9, 2017 1:40 AM, "shami-nisimov" ***@***.***&gt; wrote:
 in the documentation there is a figure showing the ELBO for the test
 dataset.
 after 50 epochs, the ELBO reaches below -80.

 when running the vae sample, after 200 epochs the ELBO reaches -100.

 is it a bug in the sample code ?

 —
 You are receiving this because you are subscribed to this thread.
 Reply to this email directly, view it on GitHub
 &lt;#539&gt;, or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABVhL5alYSsiIq1BKk2CfoN-1IfHHn3rks5s0siagaJpZM4QXrcj&gt;
 .



		</comment>
		<comment id='2' author='shami-nisimov' date='2017-11-10T21:44:44Z'>
		Small update:
I ran our code a few times to test what happens and reconstructed your observation. Interestingly, that model code was not changed since the plots were made for the tutorial, but the numbers you observed and I verified upon running it again a few times are consistent with the original VAE paper by Durk Kingma and Max Welling (ICLR 2013).
I will search through the many moving parts of pyro, the plotting scripts and my own system installation that may have changed since release to find what caused this discrepancy and once I have some more clarity can also update the plots in the tutorial to reflect the correct numbers. I currently do not see any bug in the example code for vae, I think this is all fine, so I am curious to find out what happened there.
Thank you again for catching this, the intention with all our models is to be 100% reproducible and match the published state of the art and comments such as yours help us keep them honest.
		</comment>
		<comment id='3' author='shami-nisimov' date='2017-11-10T22:39:20Z'>
		We can replicate this independently with the vae_comparison.py example. Bisecting the commits, the results for commit - d2252b241e63e3f79716aadeeb8361e7855ca547.
&lt;denchmark-code&gt;Running Pyro VAE implementation
====&gt; Epoch: 0 
Training loss: 0.0016
Test set loss: 0.0013
====&gt; Epoch: 1 
Training loss: 0.0012
Test set loss: 0.0012
====&gt; Epoch: 2 
Training loss: 0.0011
Test set loss: 0.0011
====&gt; Epoch: 3 
Training loss: 0.0011
Test set loss: 0.0011
====&gt; Epoch: 4 
Training loss: 0.0011
Test set loss: 0.0011
====&gt; Epoch: 5 
Training loss: 0.0011
Test set loss: 0.0011
&lt;/denchmark-code&gt;

and for the next commit - 6f01edb5743a393b47deeca879545bab60366f27 are quite different, and repeatable:
&lt;denchmark-code&gt;Running Pyro VAE implementation
====&gt; Epoch: 0 
Training loss: 0.0015
Test set loss: 0.0011
====&gt; Epoch: 1 
Training loss: 0.0010
Test set loss: 0.0010
====&gt; Epoch: 2 
Training loss: 0.0010
Test set loss: 0.0009
====&gt; Epoch: 3 
Training loss: 0.0009
Test set loss: 0.0009
====&gt; Epoch: 4 
Training loss: 0.0009
Test set loss: 0.0009
====&gt; Epoch: 5 
Training loss: 0.0009
Test set loss: 0.0009
&lt;/denchmark-code&gt;

I am still not sure why the changes in &lt;denchmark-link:https://github.com/pyro-ppl/pyro/pull/494&gt;#494&lt;/denchmark-link&gt;
 would result in this difference, and I haven't debugged any further.
		</comment>
		<comment id='4' author='shami-nisimov' date='2017-11-10T22:47:42Z'>
		OK, this is actually it. Thank you &lt;denchmark-link:https://github.com/neerajprad&gt;@neerajprad&lt;/denchmark-link&gt;
 .
In short: the vae example did not change, but our Bernoulli distribution used in this example changed in that PR post-release, which explains the difference.
We can now interpret the core of this issue as looking into why the old Bernoulli behaved so awkwardly compared to the new one. My guess is the new one is improved and the previous one buggy, but we will look at our tests more to validate exactly.
		</comment>
		<comment id='5' author='shami-nisimov' date='2017-11-11T00:56:01Z'>
		I can confirm that our current implementation is correct, or at least consistent with other libraries like tensorflow. Both would return the same result for discrete 0, 1 values, but different when data is continuous. Our prior calculation (while it worked on continuous values), would actually calculate the following (assume only a single datum):
&lt;denchmark-code&gt; log_pdf(x) = log(p*x + (1-p) * (1-x))
&lt;/denchmark-code&gt;

This does not blow up at 0 or 1, and I suppose that was the reason why we had this. The fact that it did not error for continuous values was accidental.
The current implementation calculates binary cross entropy, and is consistent with the tf implementation:
&lt;denchmark-code&gt;log_pdf(x) = log (p^x * (1-p)^(1-x)) = x log p + (1-x) log (1-p)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='shami-nisimov' date='2017-11-11T01:04:00Z'>
		I reopened this, as we should update some of our tutorials. We can track that as part of this task.
		</comment>
		<comment id='7' author='shami-nisimov' date='2017-12-16T22:36:20Z'>
		&lt;denchmark-link:https://github.com/neerajprad&gt;@neerajprad&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/eb8680&gt;@eb8680&lt;/denchmark-link&gt;
  are tutorial updates still pending?
		</comment>
		<comment id='8' author='shami-nisimov' date='2017-12-16T23:59:27Z'>
		I reran this and have new plots with the updated likelihood, will push this
weekend.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Dec 16, 2017 2:36 PM, "ngoodman" ***@***.***&gt; wrote:
 @neerajprad &lt;https://github.com/neerajprad&gt; @eb8680
 &lt;https://github.com/eb8680&gt; are tutorial updates still pending?

 —
 You are receiving this because you were assigned.
 Reply to this email directly, view it on GitHub
 &lt;#539 (comment)&gt;, or mute
 the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABVhL9xvCMNli_axWTp45t-c7LZn1NXaks5tBEXmgaJpZM4QXrcj&gt;
 .



		</comment>
		<comment id='9' author='shami-nisimov' date='2017-12-18T03:04:03Z'>
		&lt;denchmark-link:https://github.com/karalets&gt;@karalets&lt;/denchmark-link&gt;
 please add me to the PR so i know what to push to the site
		</comment>
		<comment id='10' author='shami-nisimov' date='2018-01-10T18:34:42Z'>
		&lt;denchmark-link:https://github.com/karalets&gt;@karalets&lt;/denchmark-link&gt;
 did you update the plots?
		</comment>
		<comment id='11' author='shami-nisimov' date='2018-04-24T03:57:15Z'>
		&lt;denchmark-link:https://github.com/jpchen&gt;@jpchen&lt;/denchmark-link&gt;
 what's the status of this after &lt;denchmark-link:https://github.com/pyro-ppl/pyro/issues/930&gt;#930&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='12' author='shami-nisimov' date='2018-04-24T04:01:43Z'>
		&lt;denchmark-link:https://github.com/neerajprad&gt;@neerajprad&lt;/denchmark-link&gt;
 has been working on vae, and if im not mistaken the plots and numbers in the tutorials have been freshly generated &lt;denchmark-link:https://github.com/pyro-ppl/pyro/pull/1050&gt;#1050&lt;/denchmark-link&gt;
 (comment). feel free to reopen if i am mistaken
		</comment>
	</comments>
</bug>