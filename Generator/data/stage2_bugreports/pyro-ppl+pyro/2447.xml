<bug id='2447' author='PaperclipBadger' open_date='2020-04-27T17:33:52Z' closed_time='2020-05-06T10:01:26Z'>
	<summary>[bug] pyro.poutine.mask with nan observations results in nan gradient for Trace_ELBO</summary>
	<description>
&lt;denchmark-h:h3&gt;Issue Description&lt;/denchmark-h&gt;

I expect to be able to use pyro.poutine.mask to mask out invalid (nan) values in my training data, but though the computed loss is not nan when the model is masked, the gradient of the loss w.r.t. the parameters is.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

Ubuntu 16.04, torch==1.4.0, pyro-api==0.1.1, pyro-ppl==1.3.1
&lt;denchmark-h:h3&gt;Code Snippet&lt;/denchmark-h&gt;

A minimal example:
import pyro
import torch


def model(x, y=None):
    """x = y + e, e ~ N(loc, 1.0), loc ~ N(0.0, 1.0)"""
    prior_loc = 0.0
    prior_scale = 1.0

    loc = pyro.sample(
        "loc",
        pyro.distributions.Normal(
            loc=prior_loc,
            scale=prior_scale,
        ),
    )

    with pyro.plate("n", size=len(x)):
        return x + pyro.sample(
            "epsilon",
            pyro.distributions.Normal(
                loc=loc,
                scale=1.0,
            ),
            obs=y - x if y is not None else None,
        )
        
mask = torch.tensor([False, True])
x = torch.tensor([float("nan"), 1.0])
y = torch.tensor([float("nan"), 0.0])

masked_model = pyro.poutine.mask(model, mask)
guide = pyro.infer.autoguide.AutoDelta(masked_model)

loss = pyro.infer.Trace_ELBO().loss_and_grads(masked_model, guide, x, y)
print(pyro.param("AutoDelta.loc"))
print("loss:", loss)
print("grad:", pyro.param("AutoDelta.loc").grad.data)
Output:
&lt;denchmark-code&gt;Parameter containing:
tensor(0.0117, requires_grad=True)
loss: 2.349758565425873
grad: tensor(nan)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='PaperclipBadger' date='2020-05-06T10:01:26Z'>
		This seems to be a recurring issue ("expected behaviour") with Torch:

pytorch/pytorch#23395
pytorch/pytorch#36923
pytorch/pytorch#23156

It seems like the solution is to use some value other than nan for missing data.
		</comment>
		<comment id='2' author='PaperclipBadger' date='2020-05-06T14:29:13Z'>
		&lt;denchmark-link:https://github.com/PaperclipBadger&gt;@PaperclipBadger&lt;/denchmark-link&gt;
 this sounds like a real usability issue. I think it is fine if  has different semantics than  if it improves user experience.
One option would be to implement a  function with custom , similar to our &lt;denchmark-link:https://github.com/pyro-ppl/pyro/blob/3eda097def2f02b01c978e2b1e4e35e02efafac9/pyro/ops/tensor_utils.py#L9&gt;safe_log()&lt;/denchmark-link&gt;
 helper  that avoids nan grads. A downside is that custom autograd functions are not compatible with jit script (but neither are poutines).
		</comment>
	</comments>
</bug>