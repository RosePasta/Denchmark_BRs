<bug id='2366' author='PaperclipBadger' open_date='2020-03-13T17:53:57Z' closed_time='2020-03-13T20:54:43Z'>
	<summary>[bug] pyro.distributions.InverseGamma.sample does not work on GPU</summary>
	<description>
&lt;denchmark-h:h3&gt;Issue Description&lt;/denchmark-h&gt;

pyro.distributions.InverseGamma.sample fails with the following error when its arguments are on the GPU:
&lt;denchmark-code&gt;RuntimeError: iter.device(arg).is_cuda() INTERNAL ASSERT FAILED at /pytorch/aten/src/ATen/native/cuda/Loops.cuh:197, please report a bug to PyTorch.
&lt;/denchmark-code&gt;

I think the problem is in __init__:
    def __init__(self, concentration, rate, validate_args=None):
        base_dist = Gamma(concentration, rate)
        super().__init__(base_dist, PowerTransform(-1.0), validate_args=validate_args)
The argument to PowerTransform should probably be something like -torch.ones_like(rate).
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

Ubuntu 16.04, Python 3.7, PyTorch 1.4.0, Pyro 1.3.0
&lt;denchmark-h:h3&gt;Code Snippet&lt;/denchmark-h&gt;

&gt;&gt;&gt; import torch
&gt;&gt;&gt; from pyro.distributions import Gamma, InverseGamma, TransformedDistribution
&gt;&gt;&gt; from pyro.distributions.transforms import PowerTransform
&gt;&gt;&gt;
&gt;&gt;&gt; concentration = torch.tensor(1.0).to("cuda")
&gt;&gt;&gt; rate = torch.tensor(1.0).to("cuda")
&gt;&gt;&gt; 
&gt;&gt;&gt; # InverseGamma.sample fails with an error
&gt;&gt;&gt; InverseGamma(concentration, rate).sample()
RuntimeError: iter.device(arg).is_cuda() INTERNAL ASSERT FAILED at /pytorch/aten/src/ATen/native/cuda/Loops.cuh:197, please report a bug to PyTorch.
&gt;&gt;&gt; 
&gt;&gt;&gt; # The equivalent TransformedDistribution is fine
&gt;&gt;&gt; TransformedDistribution(
...     Gamma(concentration, rate),
...     PowerTransform(torch.tensor(-1.0).to("cuda")),
... ).sample()
tensor(0.5707, device='cuda:0')
	</description>
	<comments>
		<comment id='1' author='PaperclipBadger' date='2020-03-13T18:40:54Z'>
		Good catch, you can probably work around via torch.set_default_tensor_type("torch.cuda.FloatTensor") for now.
		</comment>
	</comments>
</bug>