<bug id='668' author='1Reinier' open_date='2018-01-08T16:28:21Z' closed_time='2018-01-12T23:58:25Z'>
	<summary>`enum_discrete` Tensor location bug</summary>
	<description>
Hi,
When using enum_discrete = True to reduce variance in my SVI, Pyro complains that some internal tensors are not in the same location (GPU/CPU), whereas turning it off makes it work fine:
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-31-93fd337a0d5c&gt; in &lt;module&gt;()
----&gt; 1 train()

&lt;ipython-input-30-98110a9a9720&gt; in train()
     33 
     34             # Train and score
---&gt; 35             train_loss += svi.step(input_batch, target_batch)
     36 
     37         # Store history

/home/reinier/anaconda2/lib/python2.7/site-packages/pyro/infer/svi.pyc in step(self, *args, **kwargs)
     96         """
     97         # get loss and compute gradients
---&gt; 98         loss = self.loss_and_grads(self.model, self.guide, *args, **kwargs)
     99 
    100         # get active params

/home/reinier/anaconda2/lib/python2.7/site-packages/pyro/infer/elbo.pyc in loss_and_grads(self, model, guide, *args, **kwargs)
     63         :rtype: float
     64         """
---&gt; 65         return self.which_elbo.loss_and_grads(model, guide, *args, **kwargs)

/home/reinier/anaconda2/lib/python2.7/site-packages/pyro/infer/trace_elbo.pyc in loss_and_grads(self, model, guide, *args, **kwargs)
    149                         guide_site = guide_trace.nodes[name]
    150                         lp_lq = model_site[log_pdf] - guide_site[log_pdf]
--&gt; 151                         elbo_particle += lp_lq
    152                         if guide_site["fn"].reparameterized:
    153                             surrogate_elbo_particle += lp_lq

RuntimeError: Expected object of type Variable[torch.FloatTensor] but found type Variable[torch.cuda.FloatTensor] for argument #1 'other'
I suspect this is a bug. Any tips?
-Reinier
PS The bug arises here:
        for inputs, targets in train_loader:

            # Put batch on GPU
            input_batch = Variable(inputs.cuda(device=CUDA_ID, async=use_async))  # Pinned memory with async transfer
            target_batch = Variable(targets.cuda(device=CUDA_ID, async=use_async))

            # Train and score
            train_loss += svi.step(input_batch, target_batch)

        # Store history
        train_losses += [train_loss / n_train_samples]
Where CUDA_ID is some device id and async is currently False.
	</description>
	<comments>
		<comment id='1' author='1Reinier' date='2018-01-08T16:36:17Z'>
		Thanks for pinpointing this. I think we have a ton of these placement bugs. I'll take a look.
		</comment>
	</comments>
</bug>