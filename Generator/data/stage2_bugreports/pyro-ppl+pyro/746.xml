<bug id='746' author='sameerkhurana10' open_date='2018-02-07T13:58:23Z' closed_time='2018-11-27T21:34:38Z'>
	<summary>Bayesian GMM</summary>
	<description>
Trying to implement Bayesian Gaussian Mixture Model. Below is the model and the guide:
import torch
from torch.autograd import Variable
import pyro
from pyro.distributions import dirichlet, normal, categorical, gamma, beta, bernoulli
import numpy as np
from  pyro.optim import Adam
from pyro.infer import SVI

data = Variable(torch.Tensor([0, 1, 10, 11, 12]))

# number of mixture components, fixed for now
K=3

def model(data):
    """
    """
    alpha0 = Variable(torch.Tensor([1, 1, 1]))
    mean0 = Variable(torch.zeros(K))
    sigma0 = Variable(torch.ones(K))
    
    pi = pyro.sample('pi', dirichlet, alpha0)
    
    mean = pyro.sample('mu', normal, mean0, sigma0)
    #mean = Variable(torch.zeros(K))
    sigma = Variable(torch.ones(K))
    
    for n in range(len(data)):
        z_n = pyro.sample('z_{}'.format(n), categorical, pi, one_hot=False).squeeze(-1)
        mean_z_n = mean.index_select(0, z_n)
        sigma_z_n = sigma.index_select(0, z_n)
        pyro.sample('x_{}'.format(n), normal, mean_z_n, sigma_z_n, obs=data[n])
    
def guide(data):
    """
    """
    # want alpha_q to be positive and hence the log domain
    log_alpha_q = pyro.param('log_alpha_q', Variable(torch.Tensor(np.log([10, 10, 10])), requires_grad=True))
    alpha_q = torch.exp(log_alpha_q)
    mean_q = pyro.param('mean_q', Variable(torch.zeros(K), requires_grad=True))
    log_sigma_q = pyro.param('sigma_q', Variable(np.log(torch.ones(K)), requires_grad=True))
    sigma_q = torch.exp(log_sigma_q)
    
    # global Random Variable
    pi = pyro.sample('pi', dirichlet, alpha_q)
    pyro.sample('mu', normal, mean_q, sigma_q)
    
    for n in range(len(data)):
        # local random variables
        # local variational paramters
        #p = softmax(pyro.param('unconstrained_p',
        #                       Variable(torch.zeros(K), requires_grad=True)))
        pyro.sample('z_{}'.format(n), categorical, pi, one_hot=False)
        
adam_params = {"lr": 0.0005, "betas": (0.90, 0.999)}
optimizer = Adam(adam_params)

svi = SVI(model, guide, optimizer, loss="ELBO", num_particles=7, enum_discrete=True)

n_steps = 400
# do gradient steps
for step in range(n_steps):
    svi.step(data)
    if step % 100 == 0:
        print('.', end='')
Getting the error:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-11-66f78c41fb82&gt; in &lt;module&gt;()
     60 # do gradient steps
     61 for step in range(n_steps):
---&gt; 62     svi.step(data)
     63     if step % 100 == 0:
     64         print('.', end='')

/data/sls/u/sameerk/anaconda3/envs/nova-cpu/lib/python3.5/site-packages/pyro/infer/svi.py in step(self, *args, **kwargs)
     96         """
     97         # get loss and compute gradients
---&gt; 98         loss = self.loss_and_grads(self.model, self.guide, *args, **kwargs)
     99 
    100         # get active params

/data/sls/u/sameerk/anaconda3/envs/nova-cpu/lib/python3.5/site-packages/pyro/infer/elbo.py in loss_and_grads(self, model, guide, *args, **kwargs)
     63         :rtype: float
     64         """
---&gt; 65         return self.which_elbo.loss_and_grads(model, guide, *args, **kwargs)

/data/sls/u/sameerk/anaconda3/envs/nova-cpu/lib/python3.5/site-packages/pyro/infer/trace_elbo.py in loss_and_grads(self, model, guide, *args, **kwargs)
    173             if trainable_params:
    174                 surrogate_loss_particle = -surrogate_elbo_particle
--&gt; 175                 torch_backward(surrogate_loss_particle)
    176                 pyro.get_param_store().mark_params_active(trainable_params)
    177 

/data/sls/u/sameerk/anaconda3/envs/nova-cpu/lib/python3.5/site-packages/pyro/infer/util.py in torch_backward(x)
     32     """
     33     if isinstance(x, torch.autograd.Variable):
---&gt; 34         x.backward()

/data/sls/u/sameerk/anaconda3/envs/nova-cpu/lib/python3.5/site-packages/torch/autograd/variable.py in backward(self, gradient, retain_graph, create_graph, retain_variables)
    165                 Variable.
    166         """
--&gt; 167         torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
    168 
    169     def register_hook(self, hook):

/data/sls/u/sameerk/anaconda3/envs/nova-cpu/lib/python3.5/site-packages/torch/autograd/__init__.py in backward(variables, grad_variables, retain_graph, create_graph, retain_variables)
     97 
     98     Variable._execution_engine.run_backward(
---&gt; 99         variables, grad_variables, retain_graph)
    100 
    101 

RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.


&lt;/denchmark-code&gt;

Any comments?
	</description>
	<comments>
		<comment id='1' author='sameerkhurana10' date='2018-02-07T18:47:33Z'>
		I'm not sure where that could be happening. Could you

provide your Pyro version and PyTorch version?
try pyro.clear_param_store() and see if you still get the error?
try with num_particles=1 and see if you still get the error?
try with enum_discrete=False and see if you still get the error?

Also FYI  is exponentially expensive when your discrete variables are in a for loop. To reduce this to linear cost, you can use  and vectorize your guide and model, as in  our &lt;denchmark-link:https://github.com/uber/pyro/blob/dev/tutorial/source/gmm.ipynb&gt;Gaussian Mixture Model Tutorial&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='sameerkhurana10' date='2018-02-07T21:28:57Z'>
		

Pyro: 0.1.2 &amp; Pytorch: 0.3.0.post4


pyro.clear_param_store(), still the same error


num_particles=1, same error


enum_discrete=False, no error anymore. But is this what we would ideally want to do in case of discrete random variables?


yes, i will try pyro.iarange once i get this to work properly
		</comment>
		<comment id='3' author='sameerkhurana10' date='2018-11-27T04:39:56Z'>
		&lt;denchmark-link:https://github.com/fritzo&gt;@fritzo&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/neerajprad&gt;@neerajprad&lt;/denchmark-link&gt;
 can this be closed?
		</comment>
		<comment id='4' author='sameerkhurana10' date='2018-11-27T21:34:38Z'>
		Closing as obsolete
		</comment>
	</comments>
</bug>