<bug id='1242' author='jpchen' open_date='2018-07-17T23:41:11Z' closed_time='2018-07-19T01:08:39Z'>
	<summary>lr scheduler not optimizing params</summary>
	<description>
 is not updating parameters with each optimizer.  see &lt;denchmark-link:https://github.com/pyro-ppl/pyro/pull/1241&gt;#1241&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='jpchen' date='2018-07-17T23:55:41Z'>
		It would also help to have a more fleshed-out Example: section in the docstring and a more end-to-end test. While looking at the PR I had trouble determining how the LRSchedulers were intended to be used.
		</comment>
		<comment id='2' author='jpchen' date='2018-07-18T00:09:11Z'>
		ahh the pytorch scheduler only updates the scheduler and does not take an optimization step. this was my mistake - i had assumed pytorch was documented correctly and only tested the hyperparam updates not the optimization. sorry for the inconvenience.
		</comment>
	</comments>
</bug>