<bug id='152' author='fritzo' open_date='2017-09-25T23:56:48Z' closed_time='2017-09-26T00:35:21Z'>
	<summary>Flaky test: tests/integration/test_tracegraph_klqp.py::PoissonGammaTests::test_elbo_nonreparameterized</summary>
	<description>
Test fails on an unrelated PR that only modifies README.md. See e.g. this travis log:
&lt;denchmark-link:https://travis-ci.com/uber/pyro/jobs/91797872#L988&gt;https://travis-ci.com/uber/pyro/jobs/91797872#L988&lt;/denchmark-link&gt;

I recommend we deterministically fix seeds on all tests. This will reduce friction to making non-functional changes like documentation.
	</description>
	<comments>
		<comment id='1' author='fritzo' date='2017-09-26T00:35:21Z'>
		 &lt;denchmark-link:https://github.com/pyro-ppl/pyro/issues/114&gt;#114&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='fritzo' date='2017-09-26T00:36:45Z'>
		This should be fixed with &lt;denchmark-link:https://github.com/pyro-ppl/pyro/pull/151&gt;#151&lt;/denchmark-link&gt;
 merged. Basically, the integration tests are still non-deterministic, as we are not setting the rng seed for these. Instead, we are using the  marker on flaky integration tests. In &lt;denchmark-link:https://github.com/pyro-ppl/pyro/pull/151&gt;#151&lt;/denchmark-link&gt;
, I flagged off this flaky test and increased the tolerance limit as well. If we rebuild this, I think it should pass.
		</comment>
		<comment id='3' author='fritzo' date='2017-09-26T01:18:22Z'>
		OK, glad to hear this will be fixed soon. FWIW I think pinning seeds is a healthier solution than marking xfail.
		</comment>
		<comment id='4' author='fritzo' date='2017-09-26T03:39:23Z'>
		I am fine with either approach. The patterns of failure, i.e. which tests fail and how often, are pretty predictable. So I was going with the approach of increasing the tolerance, marking them with xfail, and later removing the marker if the tests consistently pass with the higher tolerance. By fixing the seed, we wouldn't have to go through this process, though we will lose this additional information of how variable the test results are (either through the tolerance parameter or the xfail marker). That said, I am not sure how important that information is, or even if this is something we would like our integration tests to capture.
p.s. - the above is only for integration tests; for unit tests, the seed is initialized.
		</comment>
		<comment id='5' author='fritzo' date='2017-09-26T03:53:44Z'>
		
... lose information of how variable the test results are ...

Well said. While agree that it's good to be able to run arbitrarily strong tests to search for possible failures, I'm concerned that we're currently mixing concerns: we're tying statistical strength to github PRs and this has the unintended effect of disincentivizing contributors from writing documentation. Maybe we could run a nightly cron job?
		</comment>
		<comment id='6' author='fritzo' date='2017-09-26T04:06:26Z'>
		Agreed, we should probably have these 1 hr+ integration test suite run as a daily cron job. That's one of the pending tasks as part of our infra haul, and I will try to take a look at it either this week or the next one. That way, the PRs can be validated within a short time-frame (20 mins), and all our battery of statistical tests can be run as cron jobs. In the meantime, hopefully, we wouldn't be facing this flakiness issue on the existing tests.
		</comment>
	</comments>
</bug>