<bug id='675' author='dori-reichmann' open_date='2018-01-11T18:51:11Z' closed_time='2018-01-18T19:15:13Z'>
	<summary>error with dist.multinomial.log_pdf</summary>
	<description>
A simple attempt to sample, and calculate the pdf:
import torch
from torch.autograd import Variable
import pyro
import pyro.distributions as dist

theta = Variable(torch.Tensor([0.2,0.3,0.5]))
n = Variable(torch.Tensor([100]))
clicks = pyro.sample("measurement", dist.multinomial, theta,n)
log_pdf_clicks = dist.multinomial.log_pdf(clicks,theta,n)
And here it the error:


RuntimeError                              Traceback (most recent call last)
 in ()
7 n = Variable(torch.Tensor([100]))
8 clicks = pyro.sample("measurement", dist.multinomial, theta,n)
----&gt; 9 log_pdf_clicks = dist.multinomial.log_pdf(clicks,theta,n)
/opt/miniconda3/lib/python3.6/site-packages/pyro/distributions/random_primitive.py in log_pdf(self, x, *args, **kwargs)
40
41     def log_pdf(self, x, *args, **kwargs):
---&gt; 42         return self.dist_class(*args, **kwargs).log_pdf(x)
43
44     def batch_log_pdf(self, x, *args, **kwargs):
/opt/miniconda3/lib/python3.6/site-packages/pyro/distributions/distribution.py in log_pdf(self, x, *args, **kwargs)
183         :rtype: torch.autograd.Variable
184         """
--&gt; 185         return torch.sum(self.batch_log_pdf(x, *args, **kwargs))
186
187     @AbstractMethod
/opt/miniconda3/lib/python3.6/site-packages/pyro/distributions/multinomial.py in batch_log_pdf(self, x)
84         """
85         batch_log_pdf_shape = self.batch_shape(x) + (1,)
---&gt; 86         log_factorial_n = log_gamma(x.sum(-1) + 1)
87         log_factorial_xs = log_gamma(x + 1).sum(-1)
88         log_powers = (x * torch.log(self.ps)).sum(-1)
/opt/miniconda3/lib/python3.6/site-packages/pyro/distributions/util.py in log_gamma(xx)
22     x = xx - 1.0
23     t = x + 5.5
---&gt; 24     t = t - (x + 0.5) * torch.log(t)
25     ser = Variable(torch.ones(x.size()).type(ttype)) * magic1
26     for c in gamma_coeff:
RuntimeError: log is not implemented for type torch.LongTensor

I am running with
(python) sys.version = '3.6.4 |Anaconda, Inc.| (default, Dec 21 2017, 21:42:08) \n[GCC 7.2.0]'
torch.version = '0.3.0.post4'
pyro.version = '0.1.2'
	</description>
	<comments>
		<comment id='1' author='dori-reichmann' date='2018-01-11T19:22:22Z'>
		Thanks for the detailed bur report. Pyro will be switching to &lt;denchmark-link:http://pytorch.org/docs/master/distributions.html&gt;PyTorch distributions&lt;/denchmark-link&gt;
 in a few days. When we do, we'll ensure that this bug is fixed. I'll leave this issue open until we migrate.
		</comment>
		<comment id='2' author='dori-reichmann' date='2018-01-18T15:41:41Z'>
		I ran into this too. As a workaround you can cast Tensor n to .float().
&lt;denchmark-link:https://github.com/fritzo&gt;@fritzo&lt;/denchmark-link&gt;
 Note that in the Pytroch distributions Pyro intends to switch to, multinomial does not accept Tensors for  (just numbers). It would be nice if we can batch that operation too in Pytorch.
		</comment>
		<comment id='3' author='dori-reichmann' date='2018-01-18T17:15:54Z'>
		&lt;denchmark-link:https://github.com/reinier&gt;@reinier&lt;/denchmark-link&gt;
, what are your use cases for heterogeneous  in ? You're correct that the PyTorch version accepts only a single . However that  is not required if the multinomial is used only for  calls. Therefoer the PyTorch  should support heterogeneous  if it is only used in Pyro observe statements e.g. observing word counts in a batch of documents where each document has a different length.
		</comment>
		<comment id='4' author='dori-reichmann' date='2018-01-18T18:20:42Z'>
		Yeah exactly, I'm using it in an observe statement.
		</comment>
		<comment id='5' author='dori-reichmann' date='2018-01-18T19:15:11Z'>
		Fixed. Pyro dev branch is now using PyTorch distributions, and I've verified that the above code works.
		</comment>
		<comment id='6' author='dori-reichmann' date='2018-01-21T08:18:10Z'>
		great, thanks
		</comment>
		<comment id='7' author='dori-reichmann' date='2018-04-17T15:06:59Z'>
		&lt;denchmark-link:https://github.com/fritzo&gt;@fritzo&lt;/denchmark-link&gt;
 quick question (don't want to reopen this) about broadcasting the PyTorch Multinomial distribution over : if I leave the argument default (meaning == 1), the observe statement will work as expected (even though the counts are different per row?). Thanks!
		</comment>
		<comment id='8' author='dori-reichmann' date='2018-04-17T16:27:31Z'>
		&lt;denchmark-link:https://github.com/1Reinier&gt;@1Reinier&lt;/denchmark-link&gt;
 Yes  should support heterogeneous counts. We simply haven't implemented a heterogeneous sampler. cc &lt;denchmark-link:https://github.com/neerajprad&gt;@neerajprad&lt;/denchmark-link&gt;
 this might be useful since you can use heterogeneous  in observe statements and in HMC.
		</comment>
		<comment id='9' author='dori-reichmann' date='2018-04-17T22:44:20Z'>
		&lt;denchmark-link:https://github.com/fritzo&gt;@fritzo&lt;/denchmark-link&gt;
 - Thanks for pointing this out. This works for the example but is quite a bit slower than using Binomial, unfortunately.
		</comment>
		<comment id='10' author='dori-reichmann' date='2020-07-20T18:00:10Z'>
		Hi, &lt;denchmark-link:https://github.com/fritzo&gt;@fritzo&lt;/denchmark-link&gt;
 I can't get the Multinomial to work with heterogenous counts like you said it could above. Here is a basic example:
import torch
import torch.distributions as tdist

x = torch.tensor([[3.,0.,0.], [0.,2.,0.], [1.,2.,1.]])
x2 = torch.tensor([[1.,0.,0.], [0.,1.,0.], [0.,0.,1.]])
p = torch.tensor([[0.99,0.,0.01], [0.95,0.05,0.], [1./3.,1./3.,1./3.]])
The following works, where the total counts are all 1:
tdist.Multinomial(probs=p).log_prob(x2)
However, this yields an error:
tdist.Multinomial(probs=p).log_prob(x)
&lt;denchmark-code&gt;---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-61-e6d65e933790&gt; in &lt;module&gt;
      4 p2 = torch.ones([3]) * (1.0/3.0)
      5 print(tdist.Multinomial(probs=p).log_prob(x2))
----&gt; 6 tdist.Multinomial(probs=p).log_prob(x)

~/.local/lib/python3.7/site-packages/torch/distributions/multinomial.py in log_prob(self, value)
    101     def log_prob(self, value):
    102         if self._validate_args:
--&gt; 103             self._validate_sample(value)
    104         logits, value = broadcast_all(self.logits.clone(memory_format=torch.contiguous_format), value)
    105         log_factorial_n = torch.lgamma(value.sum(-1) + 1)

~/.local/lib/python3.7/site-packages/torch/distributions/distribution.py in _validate_sample(self, value)
    251 
    252         if not self.support.check(value).all():
--&gt; 253             raise ValueError('The value argument must be within the support')
    254 
    255     def _get_checked_instance(self, cls, _instance=None):

ValueError: The value argument must be within the support
&lt;/denchmark-code&gt;

And if I try:
torch.multinomial(input=p).log_prob(x)
I get:
&lt;denchmark-code&gt;TypeError                                 Traceback (most recent call last)
&lt;ipython-input-70-65340bff0fc1&gt; in &lt;module&gt;
      3 p = torch.tensor([[0.99,0.,0.01],[0.95,0.05,0.],[1./3.,1./3.,1./3.]])
      4 print(tdist.Multinomial(probs=p).log_prob(x2))
----&gt; 5 torch.multinomial(input=p).log_prob(x)

TypeError: multinomial() missing 1 required positional arguments: "num_samples"
&lt;/denchmark-code&gt;

I have an N x P data matrix that is multinomial, but each document has a different total word count. I'm at a loss as to how to get this to work in a model observation statement. Any help would be GREATLY GREATLY appreciated!! Thank you!
		</comment>
		<comment id='11' author='dori-reichmann' date='2020-07-20T19:29:15Z'>
		&lt;denchmark-link:https://github.com/gewirtz&gt;@gewirtz&lt;/denchmark-link&gt;
 you can either set  to an upper bound on the number of counts via , or disable validation via .
		</comment>
		<comment id='12' author='dori-reichmann' date='2020-07-22T21:38:21Z'>
		Thank you so much!!
		</comment>
	</comments>
</bug>