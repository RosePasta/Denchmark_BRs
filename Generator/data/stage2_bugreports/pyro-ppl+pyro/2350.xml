<bug id='2350' author='fehiepsi' open_date='2020-03-07T23:19:08Z' closed_time='2020-03-09T17:05:07Z'>
	<summary>Jitting HMC does not work for models with `param` statements</summary>
	<description>
The following repro script shows the issue
import torch
import pyro
import pyro.distributions as dist
from pyro.infer import MCMC, NUTS

def model():
    x = pyro.param("x", torch.tensor(1.))
    pyro.sample("y", dist.Normal(x, 1))

mcmc = MCMC(NUTS(model, jit_compile=True), 10)
mcmc.run()
mcmc.summary()
raises the error RuntimeError: Cannot insert a Tensor that requires grad as a constant. Consider making it a parameter or input, or detaching the gradient.
	</description>
	<comments>
		<comment id='1' author='fehiepsi' date='2020-03-07T23:22:52Z'>
		An easy workaround is to hide the constant by a lambda:
import torch

import pyro
import pyro.distributions as dist
from pyro.infer import MCMC, NUTS

def model():
    x = pyro.param("x", lambda: torch.tensor(1.))
    pyro.sample("y", dist.Normal(x, 1))

model()  # &lt;------ initialize param store

mcmc = MCMC(NUTS(model, jit_compile=True), 10)
mcmc.run()
mcmc.summary()
		</comment>
		<comment id='2' author='fehiepsi' date='2020-03-07T23:25:10Z'>
		Hmm, I still get the issue. Are you using PyTorch nightly?
		</comment>
		<comment id='3' author='fehiepsi' date='2020-03-08T00:14:43Z'>
		I'm using PyTorch release. Did you make sure to initialize the param store before inference?
model()  # initialize param store
		</comment>
		<comment id='4' author='fehiepsi' date='2020-03-08T02:05:52Z'>
		Yes, I used your code and still observed the issue. I think this is the same issue as &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/17583&gt;this one&lt;/denchmark-link&gt;
 and PyTorch won't support it. IIUC we can resolve the issue by replaying  sites by constant values (using  e.g.) in the computation of .
		</comment>
		<comment id='5' author='fehiepsi' date='2020-03-09T00:27:58Z'>
		The JIT tracer expects any constants in the trace to have requires_grad=False as expected. Some of our examples and tests for HMC use model.requires_grad_(False) if the model is a PyroModule or nn.Module.
I think we could provide a param_store._requires_grad(requires_grad=True) method which will allow us to use the param store for simple key value lookup that will be useful in such situations (i.e. when model is not a PyroModule).
def requires_grad_(self, requires_grad=True):
    for v in self._params.values():
        v.requires_grad_(requires_grad)
		</comment>
		<comment id='6' author='fehiepsi' date='2020-03-09T02:02:29Z'>
		&lt;denchmark-link:https://github.com/neerajprad&gt;@neerajprad&lt;/denchmark-link&gt;
's solution works like a charm!! :D (works with multi-chains too) I think we can call this method automatically at the beginning and the ending of  method.
		</comment>
	</comments>
</bug>