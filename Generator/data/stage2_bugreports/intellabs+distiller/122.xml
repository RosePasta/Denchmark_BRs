<bug id='122' author='kuanzi' open_date='2019-01-10T02:53:20Z' closed_time='2020-04-30T07:23:10Z'>
	<summary>no T*T in distillation loss</summary>
	<description>
Why  there was no T*T posed on the soft loss in the final distillation loss ? Actually, in [Hinton, 2015], it  is important to utilize T**2 to balance the tradeoff.
[1] Hinton, Geoffrey, Oriol Vinyals, and Jeffrey Dean. "Distilling the knowledge in a neural network." (2015).
	</description>
	<comments>
		<comment id='1' author='kuanzi' date='2019-01-15T09:00:54Z'>
		&lt;denchmark-link:https://github.com/kuanzi&gt;@kuanzi&lt;/denchmark-link&gt;
 you're correct. Not sure how I neglected it during the implementation. Will fix. Thanks.
		</comment>
	</comments>
</bug>