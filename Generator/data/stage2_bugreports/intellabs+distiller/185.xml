<bug id='185' author='aab60526' open_date='2019-03-11T10:08:27Z' closed_time='2020-04-16T12:14:02Z'>
	<summary>Quantization aware training resume with checkpoint</summary>
	<description>
Hello, when I training a resuming model with checkpoint,
It raise an error
&lt;denchmark-link:https://user-images.githubusercontent.com/31931066/54115821-52cc1d80-4428-11e9-824d-0826a3818d90.png&gt;&lt;/denchmark-link&gt;

And this is my command line
&lt;denchmark-link:https://user-images.githubusercontent.com/31931066/54115930-96268c00-4428-11e9-8111-8b0a612e7460.png&gt;&lt;/denchmark-link&gt;

Where is the error come from?
	</description>
	<comments>
		<comment id='1' author='aab60526' date='2019-03-12T08:39:45Z'>
		It's not 100% clear from the command line, but I assume you're trying to resume  from a previous quantization-aware training session. Unfortunately, that's not supported at the moment (see comment &lt;denchmark-link:https://github.com/NervanaSystems/distiller/issues/21#issuecomment-403293755&gt;here&lt;/denchmark-link&gt;
).
(In case you want to load a quantization-aware training checkpoint just for , you can use the workaround detailed &lt;denchmark-link:https://github.com/NervanaSystems/distiller/issues/59#issuecomment-432546165&gt;here&lt;/denchmark-link&gt;
)
We're currently working on fixing some other issues related to loading checkpoints (see &lt;denchmark-link:https://github.com/IntelLabs/distiller/pull/182&gt;#182&lt;/denchmark-link&gt;
), once that's finalized I'll try to get quantization-aware training resume soon after.
		</comment>
		<comment id='2' author='aab60526' date='2019-04-03T03:36:03Z'>
		Maybe, you could try is:


Follow the steps here to hack the distiller framework.


Create two model


&lt;denchmark-code&gt;resume_model = create_model(&lt;your params&gt;)
new_model = create_model(&lt;your params same as resume model&gt;)
&lt;/denchmark-code&gt;


Resume quantized weights to resume model

&lt;denchmark-code&gt;    if args.resume:
        resume_model, _, _ = apputils.load_checkpoint(resume_model, chkpt_file=args.resume)
        resume_model.to(args.device)
&lt;/denchmark-code&gt;


Quantize the new model

&lt;denchmark-code&gt;    if args.compress:
        compression_scheduler = distiller.file_config(new_model, optimizer, args.compress, compression_scheduler)

&lt;/denchmark-code&gt;

Now, you have a resume model which contains quantized trained weights, and a new model contains random initial quantized weights.

Load weights

&lt;denchmark-code&gt;new_model.load_state_dict(copy.deepcopy(resume_model.state_dict()))
del(resume_model)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='aab60526' date='2020-08-14T04:19:12Z'>
		Hi, does anyone figure out any quick way to fix this bug? I really need to run evaluation on quantized model
		</comment>
	</comments>
</bug>