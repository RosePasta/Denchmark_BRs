<bug id='213' author='dabadee' open_date='2019-04-02T16:06:19Z' closed_time='2020-04-27T19:48:10Z'>
	<summary>Network Thinner Error</summary>
	<description>
I am getting the following error when I use network thinner, how can I resolve it ?
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "compress_classifier.py", line 754, in &lt;module&gt;
    main()
  File "compress_classifier.py", line 256, in main
    loggers=[tflogger, pylogger], args=args)
  File "compress_classifier.py", line 333, in train
    output = model(inputs)
  File "/home/daba/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/daba/anaconda3/lib/python3.7/site-packages/torchvision/models/alexnet.py", line 44, in forward
    x = x.view(x.size(0), 256 * 6 * 6)
RuntimeError: shape '[32, 9216]' is invalid for input of size 266112
&lt;/denchmark-code&gt;

I use the command
&lt;denchmark-code&gt;python3 compress_classifier.py -a=alexnet --lr=0.000005 -p=50 /media/daba/0C5235005234F056/ImageNet/ -j 8 --epochs 1 --pretrained --compress=/home/daba/schedules/alexnetOneShot.yaml -b 32
&lt;/denchmark-code&gt;

I am using the following schedule
&lt;denchmark-code&gt;version: 1
pruners:
  fc1_pruner:
    class: 'L1RankedStructureParameterPruner'
    group_type: Rows
    desired_sparsity: 0.33
    weights: ['classifier.1.weight']

  fc2_pruner:
    class: 'L1RankedStructureParameterPruner'
    group_type: Rows
    desired_sparsity: 0.3
    weights: ['classifier.4.weight']

  fc3_pruner:
    class: 'L1RankedStructureParameterPruner'
    group_type: Rows
    desired_sparsity: 0.2
    weights: ['classifier.6.weight']

  conv1_pruner:
    class: 'L1RankedStructureParameterPruner'
    group_type: Filters
    desired_sparsity: 0.07
    weights: ['features.module.0.weight']

  conv2_pruner:
    class: 'L1RankedStructureParameterPruner'
    group_type: Filters
    desired_sparsity: 0.1
    weights: ['features.module.3.weight']

  conv3_pruner:
    class: 'L1RankedStructureParameterPruner'
    group_type: Filters
    desired_sparsity: 0.12
    weights: ['features.module.6.weight']

  conv4_pruner:
    class: 'L1RankedStructureParameterPruner'
    group_type: Filters
    desired_sparsity: 0.1
    weights: ['features.module.8.weight']

  conv5_pruner:
    class: 'L1RankedStructureParameterPruner'
    group_type: Filters
    desired_sparsity: 0.1
    weights: ['features.module.10.weight']

extensions:
  net_thinner:
      class: 'FilterRemover'
      thinning_func_str: remove_filters
      arch: 'alexnet'
      dataset: 'imagenet'


lr_schedulers:
  # Learning rate decay scheduler
   pruning_lr:
     class: ExponentialLR
     gamma: 0.9


policies:

  - pruner:
      instance_name : 'conv1_pruner'
    epochs : [0]

  - pruner:
      instance_name : 'conv2_pruner'
    epochs : [0]

  - pruner:
      instance_name : 'conv3_pruner'
    epochs : [0]

  - pruner:
      instance_name : 'conv4_pruner'
    epochs : [0]

  - pruner:
      instance_name : 'conv5_pruner'
    epochs : [0]

  - pruner:
      instance_name : 'fc1_pruner'
    epochs : [0]

  - pruner:
      instance_name: 'fc2_pruner'
    epochs : [0]

  - pruner:
      instance_name: 'fc3_pruner'
    epochs : [0]

  - extension:
      instance_name: 'net_thinner'
    epochs : [0]

  - lr_scheduler:
      instance_name: pruning_lr
    starting_epoch: 24
    ending_epoch: 200
    frequency: 1
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='dabadee' date='2019-04-03T23:23:02Z'>
		Hi &lt;denchmark-link:https://github.com/dabadee&gt;@dabadee&lt;/denchmark-link&gt;
,
Thanks for the detailed description of the issue - it made it very easy to reproduce the problem.
There are two problems here:

The number of channels in the view created between the features and the classifier is hard-coded for no real reason:
https://github.com/pytorch/vision/blob/f566fac80e3182a8b3c0219a88ae00ed1b81d7c7/torchvision/models/alexnet.py#L46

In Alexnet change:

to

The thinning code could take care of this, but I don't think adding this now is very important.
2. The &lt;denchmark-link:https://github.com/pytorch/vision/blob/f566fac80e3182a8b3c0219a88ae00ed1b81d7c7/torchvision/models/alexnet.py#L34&gt;Dropout layers&lt;/denchmark-link&gt;
 remain when traversing the pytorch graph in evaluation mode and this is confusing .
I will fix this.  Meanwhile you can disable .  Alternatively you can remark the problematic Dropout layer, but note that if you do that you won't be able to load the pretrained model from TorchVision.
Cheers
Neta
		</comment>
		<comment id='2' author='dabadee' date='2019-04-08T09:29:08Z'>
		Hi &lt;denchmark-link:https://github.com/dabadee&gt;@dabadee&lt;/denchmark-link&gt;
,
I merged the fix, but please note that there is another bug: pruning rows of a Linear layer following a convolutional layer that had its filters thinned throws a RuntimeError (wrong tensor sizes).
I will file an issue.
Cheers
Neta
		</comment>
	</comments>
</bug>