<bug id='4567' author='dirkgr' open_date='2020-08-17T16:40:11Z' closed_time='2020-10-20T16:13:54Z'>
	<summary>`batches_per_epoch` should not affect evaluation</summary>
	<description>
&lt;denchmark-link:https://github.com/HarshTrivedi&gt;@HarshTrivedi&lt;/denchmark-link&gt;
 noticed that  affects evaluation, in that it only evaluates on the first  batches when this is set. While it's easy to see why this happens, that's almost always wrong. We should ignore that parameter for evaluations. If you want to restrict the dataset size for evaluations, you can do it with the  parameter.
	</description>
	<comments>
		<comment id='1' author='dirkgr' date='2020-08-17T17:01:27Z'>
		Btw, this could also be a problem for validation evaluation happening after every checkpoint during the training.
For fixing only the post-training evaluation, my current hack is just to add the following line &lt;denchmark-link:https://github.com/allenai/allennlp/blob/master/allennlp/commands/evaluate.py#L148&gt;here&lt;/denchmark-link&gt;
.
&lt;denchmark-code&gt;data_loader_params.pop("batches_per_epoch")
&lt;/denchmark-code&gt;

This doesn't fix the training phase evaluations though.
		</comment>
		<comment id='2' author='dirkgr' date='2020-08-17T17:09:57Z'>
		We talked about this during issue review.

We don't want to take away the ability to set batches_per_epoch. The difference to max_instances is that with batches_per_epoch, you will get different instances every time, whereas with max_instances you always get the first n.
We don't want to make the validation data loader act differently when it's specified in jsonnet vs. when we default-fallback to the train dataloader.
We want to be able to silence the warning if we print one.

We might not be able to meet all of these goals.
		</comment>
		<comment id='3' author='dirkgr' date='2020-08-17T18:13:27Z'>
		&lt;denchmark-link:https://github.com/HarshTrivedi&gt;@HarshTrivedi&lt;/denchmark-link&gt;
, the proper way to fix this right now is to set the  in the config. You can use jsonnet to set it to a copy of , minus the  parameter. It would look similar to this: &lt;denchmark-link:https://github.com/allenai/allennlp-models/blob/master/training_config/classification/stanford_sentiment_treebank_roberta.jsonnet#L22&gt;https://github.com/allenai/allennlp-models/blob/master/training_config/classification/stanford_sentiment_treebank_roberta.jsonnet#L22&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='dirkgr' date='2020-08-17T19:23:45Z'>
		Yep, thanks. I'm already doing that for training new models. However, several of my models are already trained without the validation_data_loader being set. For config change to take affect, I'd have to do unarchive model.tar.gz, tweak config, rearchive for every trained model and only then use it. Other option is to pass validation_data_loader  via overrides for every evaluate command I run. I figured, for my purpose at least, hardcoding to pop off batches_per_epoch  from validation dataloader would be easier. : )
		</comment>
		<comment id='5' author='dirkgr' date='2020-10-15T21:39:44Z'>
		The way we want to fix this is this:
When params for the validation data loader are not explicitly set, and therefore they are inherited from the training data loader, and the training data loader specifies batches_per_epoch, then we show a warning.
That is all.
		</comment>
	</comments>
</bug>