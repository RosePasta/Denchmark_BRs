<bug id='4354' author='dirkgr' open_date='2020-06-11T17:19:28Z' closed_time='2020-07-17T08:44:54Z'>
	<summary>Reported loss is confusing</summary>
	<description>
The loss we write into tensorboard is the average loss over the epoch. So the first number we get is not averaged at all, while the last number in the epoch is averaged over the entire epoch. Then we start the second epoch, and the next value reported is just the loss of the first batch of the second epoch. As a result, we always see a big drop in loss when the epoch changes. This makes no sense.
	</description>
	<comments>
		<comment id='1' author='dirkgr' date='2020-06-12T16:13:04Z'>
		We discussed and decided we should add a batch loss to our tensorboard metrics.  Here's a link to a relevant method: &lt;denchmark-link:https://github.com/allenai/allennlp/blob/master/allennlp/training/trainer.py#L495&gt;https://github.com/allenai/allennlp/blob/master/allennlp/training/trainer.py#L495&lt;/denchmark-link&gt;

This code is likely highly relevant: &lt;denchmark-link:https://github.com/allenai/allennlp/blob/master/allennlp/training/trainer.py#L621&gt;https://github.com/allenai/allennlp/blob/master/allennlp/training/trainer.py#L621&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='dirkgr' date='2020-07-16T08:05:44Z'>
		&lt;denchmark-link:https://github.com/HarshTrivedi&gt;@HarshTrivedi&lt;/denchmark-link&gt;
 discovered that &lt;denchmark-link:https://github.com/allenai/allennlp/pull/4477&gt;#4477&lt;/denchmark-link&gt;
 didn't really fix this. We want to log the actual loss of each individual batch, not the sum of the losses.
		</comment>
	</comments>
</bug>