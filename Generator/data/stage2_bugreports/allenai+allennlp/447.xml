<bug id='447' author='schmmd' open_date='2017-10-26T21:05:11Z' closed_time='2018-01-05T21:33:28Z'>
	<summary>Overriding environment variables causes them to be interpreted as a string</summary>
	<description>
From &lt;denchmark-link:https://github.com/tusharkhot&gt;@tusharkhot&lt;/denchmark-link&gt;

Michael Schmitz So there is one issue with using environment variables as fallback in HOCON. These environment variables are assumed to be strings by default which lead to incompatible type errors in AllenNLP. E.g. when setting cuda_device using environment variables
&lt;denchmark-code&gt;  File "/usr/local/lib/python3.6/site-packages/allennlp/training/trainer.py", line 515, in from_params
    if cuda_device &gt;= 0:
TypeError: '&gt;=' not supported between instances of 'str' and 'int'
&lt;/denchmark-code&gt;

I can submit a PR to allennlp to convert every parameter to the right type (or at least the interesting ones) or maybe there is a fancy way to ensure environment variables have the right type ?
	</description>
	<comments>
		<comment id='1' author='schmmd' date='2017-10-26T21:05:21Z'>
		&lt;denchmark-link:https://github.com/joelgrus&gt;@joelgrus&lt;/denchmark-link&gt;
 just curious, do you have any ideas here?
		</comment>
		<comment id='2' author='schmmd' date='2017-10-27T00:33:59Z'>
		probably the "best" thing to do is to add some more pop methods to Params e.g. pop_int that do something like
&lt;denchmark-code&gt;def pop_int(self, key: str, default: Any = DEFAULT):
    value = self.pop(key, default)
    if isinstance(value, str):
        value = int(value)
    return value
&lt;/denchmark-code&gt;

and then change all of the Params.pop() calls that we expect to be ints to pop_int(), e.g.
&lt;denchmark-code&gt;        cuda_device = params.pop_int("cuda_device", -1)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='schmmd' date='2017-10-27T00:35:24Z'>
		although I think we're probably a little bit loose about using dicts where we say we're using Params, so this might break that (although we should fix those)
		</comment>
		<comment id='4' author='schmmd' date='2017-10-27T15:32:14Z'>
		it's also possible this is a bug in pyhocon
		</comment>
		<comment id='5' author='schmmd' date='2017-10-27T15:52:42Z'>
		&lt;denchmark-link:https://github.com/joelgrus&gt;@joelgrus&lt;/denchmark-link&gt;
, where are we loose with using  where we say we're using ?  Like this: ?  Those get converted to  objects by the  method, so it's not actually being loose, it's just allowing the calling code to be less verbose.
And yeah, I'm not incredibly happy about having to add pop_int and such, but I think you're right that it's the best solution to this issue.
		</comment>
		<comment id='6' author='schmmd' date='2017-10-27T17:06:43Z'>
		oops, it's possible that's what I was thinking of
		</comment>
		<comment id='7' author='schmmd' date='2017-10-27T17:07:13Z'>
		in re my "bug" comment, the hocon spec says

A substitution is replaced with any value type (number, object, string, array, true, false, null). If the substitution is the only part of a value, then the type is preserved. Otherwise, it is value-concatenated to form a string.

which suggests that it is a bug
		</comment>
		<comment id='8' author='schmmd' date='2017-10-27T17:10:26Z'>
		It has the same behavior in typesafe config

environment variables always become a string value, though if an app asks for another type automatic type conversion would kick in

&lt;denchmark-link:https://github.com/lightbend/config/blob/master/HOCON.md#substitution-fallback-to-environment-variables&gt;https://github.com/lightbend/config/blob/master/HOCON.md#substitution-fallback-to-environment-variables&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='schmmd' date='2017-10-27T17:14:34Z'>
		ok, then probably something like pop_int() is best
		</comment>
		<comment id='10' author='schmmd' date='2017-11-02T23:03:57Z'>
		A related item: Oyvind noted is that the --overrides flag provides the ability to set a particular field while also setting the type.
		</comment>
		<comment id='11' author='schmmd' date='2017-11-02T23:21:57Z'>
		So it seems Pyhocon is using the type information from the original config file to correctly cast the overriden value ?  It doesn't have this information if you use an environment variable in the config file directly (e.g. "cuda_device": ${cuda} in the config json).
So rather than making changes to the entire codebase, it seems easier to use the --overrides flag (will have to enable it for other commands too). This is also easier to use than defining environment variables in my beaker script and then modifying the config to use these environment variables.
		</comment>
		<comment id='12' author='schmmd' date='2017-11-03T19:01:26Z'>
		Actually, I still think that pop_int, etc., is the way to go here.  Presumably the primary use case for environment variables in config files is beaker, and using --overrides doesn't actually solve any problems there, because you still have to figure out how to get the environment variables into the --overrides argument.  You'd just have two ugly hacks to get this to work, where doing pop_int is a little bit of work, but improves our codebase (by improving type safety) instead of making it more hacky.
		</comment>
		<comment id='13' author='schmmd' date='2017-11-08T18:54:50Z'>
		I agree that pop_int is the cleanest way to go. I was looking at the path of least resistance in my comment especially since it seemed like a Beaker-only feature request. :)
I was not considering creating a new environment variable for overrides. Assuming we replicate the same --overrides flag from evaluate command to the other commands, we can then add the overrides to the beaker command directly. E.g.
&lt;denchmark-code&gt;beaker experiment run \
   ...
  python skidls/run.py train /config.json -s /output \
  --overrides "trainer.cuda_device=0,..."
&lt;/denchmark-code&gt;

This is definitely an ugly hack but has its advantages. I can add new configuration flags to the overrides without changing the config json or defining new environment variables. Also this seemed to be a beaker-only feature request, so I was not sure if you want to change every file in your code-base for this.
		</comment>
		<comment id='14' author='schmmd' date='2017-11-27T16:41:53Z'>
		&lt;denchmark-link:https://github.com/matt-gardner&gt;@matt-gardner&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tusharkhot&gt;@tusharkhot&lt;/denchmark-link&gt;
 PyHocon interprets environment variables as strings.  However,  simply allows you to write PyHocon, so you can control the type of what you are overriding.  If you write something like  then it will be a numeric value as it's unquoted (whereas  would be a string).
Note I'm not advocated for the above--just articulating how it works.
I'm not terribly excited about methods like pop_int to interpret environment variables, as we would end up writing custom parsing code and we could make assumptions that might not always be true (what if the users wants a string with an integer value?).
&lt;denchmark-link:https://github.com/tusharkhot&gt;@tusharkhot&lt;/denchmark-link&gt;
 how much is this a blocking issue for you presently?  I'd like to make sure we have something that works here for you, but you may have found a work-around already.
		</comment>
		<comment id='15' author='schmmd' date='2017-11-27T16:59:33Z'>
		Methods like pop_int would be used in all of our from_params methods where the code is explicitly expecting an int.  They actually improve our type safety and code readability, they are just annoying to implement, because we have a lot of code already written.  It's not an issue of the user wanting a string with an integer value.
Another snag in this approach is if you want to have the learning rate be specified by an environment variable (which, if you're doing a grid search with beaker, is definitely something you would want to do).  Those parameters get passed through to pytorch without a pop at all - we'd have to write wrappers for all of the optimizers and whatever else, which is even more annoying.
		</comment>
		<comment id='16' author='schmmd' date='2017-11-27T17:04:01Z'>
		&lt;denchmark-link:https://github.com/matt-gardner&gt;@matt-gardner&lt;/denchmark-link&gt;
 thanks for clarifying re: --that makes sense.
		</comment>
		<comment id='17' author='schmmd' date='2017-11-27T19:53:07Z'>
		I have a script that makes it easy to start experiments with new configuration files and so I have just been starting runs with different hyper-parameter settings. I haven't been running that many tuning experiments, so this wasn't too cumbersome to use. Definitely not a long-term solution. I think &lt;denchmark-link:https://github.com/bhavanadalvi&gt;@bhavanadalvi&lt;/denchmark-link&gt;
 was also looking at running hyper-parameter tuning with AllenNLP models. Maybe she has a workaround.
I was considering creating a PR based on &lt;denchmark-link:https://github.com/matt-gardner&gt;@matt-gardner&lt;/denchmark-link&gt;
 's suggestion, but the issue with optimizer  parameters makes this a bit more complicated. I can't think of a simple and clean solution for it. Maybe interpreting the variable type would be okay in this case ?
		</comment>
		<comment id='18' author='schmmd' date='2017-11-27T20:19:34Z'>
		Yeah, the optimizers are a pretty big sticking point.  That problem might be messy enough that adding --overrides to train is a less messy solution.
		</comment>
		<comment id='19' author='schmmd' date='2017-11-28T19:58:33Z'>
		I spoke with &lt;denchmark-link:https://github.com/tusharkhot&gt;@tusharkhot&lt;/denchmark-link&gt;
 about this a bit yesterday.  He's not blocked on this presently and he thinks Beaker may have a future that will solve this issue another way.
		</comment>
		<comment id='20' author='schmmd' date='2017-12-21T22:37:39Z'>
		Here's the code that relates to Tushar's comment about optimizers: &lt;denchmark-link:https://github.com/allenai/allennlp/blob/master/allennlp/training/optimizers.py#L35&gt;https://github.com/allenai/allennlp/blob/master/allennlp/training/optimizers.py#L35&lt;/denchmark-link&gt;

		</comment>
		<comment id='21' author='schmmd' date='2017-12-21T22:41:15Z'>
		a couple of ideas (if we want to proceed this way):

add a guess_types flag to Params.as_dict() that converts int-looking strings to ints, float-looking strings to floats, etc...
add a type_overrides parameter to Params.as_dict() that can say things like {'learning_rate': float}. then you could pass that into Optimizer.from_params? not sure, this might get messy.

		</comment>
		<comment id='22' author='schmmd' date='2017-12-22T19:06:50Z'>
		Hmm...  I think I've come around to just using --overrides for the train command to solve this problem. The other solutions are way more messy and/or error-prone, and this is just a non-issue if you use --overrides, right?
		</comment>
		<comment id='23' author='schmmd' date='2017-12-22T19:23:36Z'>
		Yeah. It can lead to possible extremely long and hard to read input to the --overrides flag. We can wrap the call to allennlp/run in a shell script and avoid having to directly edit this long argument. And I think its highly unlikely to hit the maximum command length either.
		</comment>
		<comment id='24' author='schmmd' date='2017-12-22T19:35:51Z'>
		I re-implemented --overrides and created a PR this morning as it's a quick fix for this issue.  I also have most of the code done to add pop_int and pop_float where possible--of course this misses certain cases.
		</comment>
		<comment id='25' author='schmmd' date='2017-12-22T19:38:04Z'>
		Yeah, I guess you can reduce the need for --overrides by using pop_int where it's possible, and --overrides only for things that we don't explicitly pop, like learning rates.  Then the command doesn't get so out of control.
		</comment>
		<comment id='26' author='schmmd' date='2018-01-05T21:33:28Z'>
		Closing this for now, as we've mitigated it with two PRs.
		</comment>
	</comments>
</bug>