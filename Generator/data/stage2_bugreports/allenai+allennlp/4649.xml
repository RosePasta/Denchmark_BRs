<bug id='4649' author='ianupright' open_date='2020-09-18T01:26:02Z' closed_time='2020-11-11T00:44:06Z'>
	<summary>Problem with PretrainedTransformerEmbedder and models such as T5</summary>
	<description>
In  PretrainedTransformerEmbedder and PretrainedTransformerTokenizer, there is the line:
&lt;denchmark-code&gt;    self._num_added_end_tokens = len(tokenizer.single_sequence_end_tokens)
&lt;/denchmark-code&gt;

In the case of models such as T5, the _num_added_tokens results in zero.
but then there is code like:
&lt;denchmark-code&gt;    embeddings = embeddings[
        :, :, self._num_added_start_tokens : -(self._num_added_end_tokens), :
    ]  # truncate segment-level start/end tokens
&lt;/denchmark-code&gt;

which results in an empty tensor, because I think it should be a -1 to get the last element in the array, instead of -(self._num_added_end_tokens), which results in 0?
Nevertheless, a T5 model doesn't seem to work with the PretrainedTransformerEmbedder/PretrainedTransformerTokenizer
	</description>
	<comments>
	</comments>
</bug>