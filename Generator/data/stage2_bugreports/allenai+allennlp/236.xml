<bug id='236' author='matt-gardner' open_date='2017-08-31T16:23:40Z' closed_time='2017-09-15T04:17:07Z'>
	<summary>Figure out how to get spacy to tokenize wiki text correctly</summary>
	<description>
SQuAD has plenty of paragraphs that have wiki notes, formatting like "This was a protest.[note 4]".  Spacy for some reason does not tokenize these strings correctly, giving "protest.[note" as a single token.  We should be able to improve performance on SQuAD at least a little bit by fixing these issues, as it affects a fair number of our training examples, and some of the dev set.
A test that currently fails, but should pass (goes in &lt;denchmark-link:https://github.com/allenai/allennlp/blob/master/tests/data/tokenizers/word_splitter_test.py&gt;word_splitter_test.py&lt;/denchmark-link&gt;
):
 def test_tokenize_handles_wiki_notes(self):
     passage = "McWhorter writes of Lee, \"for a white person from the South to write a " +\
             "book like this in the late 1950s is really unusual\u2014by its very existence " +\
             "an act of protest.\"[note 4] Author James McBride calls Lee brilliant but " +\
             "stops short of calling her brave: \"I think by calling Harper Lee brave you " +\
             "kind of absolve yourself of your own racism.\""
     tokens, offsets = self.word_splitter.split_words(passage)
     assert "protest" in tokens
	</description>
	<comments>
		<comment id='1' author='matt-gardner' date='2017-09-15T04:17:07Z'>
		This is largely fixed by &lt;denchmark-link:https://github.com/allenai/allennlp/pull/317&gt;#317&lt;/denchmark-link&gt;
, which just uses a simple regex to split words.
		</comment>
	</comments>
</bug>