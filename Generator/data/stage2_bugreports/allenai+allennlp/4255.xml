<bug id='4255' author='epwalsh' open_date='2020-05-19T02:12:52Z' closed_time='2020-05-21T22:07:52Z'>
	<summary>Distributed training bug</summary>
	<description>
Currently the train command does the following WRT the vocabulary in the distributed setting:

initializes the vocab from the main process and serializes it to file,
spawns the _train_worker command for each worker, which themselves re-initialize the vocab from file,
then the master worker re-serializes the vocab to file after initializing the model in case the model has modified the vocab in some way.

But this is potentially really bad because a non-master worker could be initializing their vocab from the files that are currently being written to from the master worker. In fact, I believe this what caused this CI run to fail: &lt;denchmark-link:https://github.com/allenai/allennlp/pull/4253/checks?check_run_id=687029330&gt;https://github.com/allenai/allennlp/pull/4253/checks?check_run_id=687029330&lt;/denchmark-link&gt;
.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Potential solutions:


Only serialize the vocab once, and do it before initializing the model.
This has a quick fix. See #4256 for a draft.
The downside here is that model side-effects on the vocab won't be saved to the vocab files. Is this really a downside? I feel like models should not modify the vocab, and I only know of one that does: CopyNet. But it's really not necessary. See allenai/allennlp-models#55.


Refactor the train command so that even in the distributed setting we only serialize the vocab once, but do it after initializing the model. This will definitely take more work.


	</description>
	<comments>
	</comments>
</bug>