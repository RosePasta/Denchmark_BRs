<bug id='4805' author='wlhgtc' open_date='2020-11-20T12:38:27Z' closed_time='2020-12-08T15:46:46Z'>
	<summary>CUDA OOV when use beam_search with amp</summary>
	<description>
Hi, there.
Thanks for your great job that add multiple samplers and reconstruct Beam Search.
But recently, I find a bug when use beam_search with amp.
I use the config in allennlp-models: &lt;denchmark-link:https://github.com/allenai/allennlp-models/blob/master/training_config/generation/bart_cnn_dm.jsonnet&gt;config&lt;/denchmark-link&gt;


All things goes well until I add .
The cuda memory is as follows( I use a 32G v100):




use amp
no amp




train
13G
15G


valid
32+G
15G



It's strange and I wonder why beam search in amp takes twice memory ?
&lt;denchmark-link:https://github.com/epwalsh&gt;@epwalsh&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jvstokes&gt;@jvstokes&lt;/denchmark-link&gt;
 Hope you could help!
	</description>
	<comments>
		<comment id='1' author='wlhgtc' date='2020-11-20T16:00:06Z'>
		I will try to reproduce this when I get a chance.
		</comment>
		<comment id='2' author='wlhgtc' date='2020-11-20T17:11:52Z'>
		Probably related to &lt;denchmark-link:https://github.com/allenai/allennlp/issues/4689&gt;#4689&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='3' author='wlhgtc' date='2020-12-07T16:37:37Z'>
		&lt;denchmark-link:https://github.com/epwalsh&gt;@epwalsh&lt;/denchmark-link&gt;
 this is just a friendly ping to make sure you haven't forgotten about this issue 
		</comment>
		<comment id='4' author='wlhgtc' date='2020-12-08T11:42:03Z'>
		&lt;denchmark-link:https://github.com/epwalsh&gt;@epwalsh&lt;/denchmark-link&gt;
 This bug was fixed in &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/48049&gt;pytorch&lt;/denchmark-link&gt;
. Seems like memory leak.
		</comment>
		<comment id='5' author='wlhgtc' date='2020-12-08T15:46:46Z'>
		That's great to hear! Thanks &lt;denchmark-link:https://github.com/wlhgtc&gt;@wlhgtc&lt;/denchmark-link&gt;
!
		</comment>
	</comments>
</bug>