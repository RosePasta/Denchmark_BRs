<bug id='4687' author='vikigenius' open_date='2020-09-30T17:49:56Z' closed_time='2020-09-30T19:18:43Z'>
	<summary>allennlp-models Seq2Seq dataset reader does not work with pretrained transformer tokenizers</summary>
	<description>
&lt;denchmark-h:h2&gt;Checklist&lt;/denchmark-h&gt;


 I have verified that the issue exists against the master branch of AllenNLP.
 I have read the relevant section in the contribution guide on reporting bugs.
 I have checked the issues list for similar or identical bug reports.
 I have checked the pull requests list for existing proposed fixes.
 I have checked the CHANGELOG and the commit log to find out if the bug was already fixed in the master branch.
 I have included in the "Description" section below a traceback from any exceptions related to this bug.
 I have included in the "Related issues or possible duplicates" section beloew all related issues and possible duplicate issues (If there are none, check this box anyway).
 I have included in the "Environment" section below the name of the operating system and Python version that I was using when I discovered this bug.
 I have included in the "Environment" section below the output of pip freeze.
 I have included in the "Steps to reproduce" section below a minimally reproducible example.

&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

The Seq2Seq dataset reader from allennlp-models does not work with pretrained transformer tokenizers.

Python traceback:

Traceback (most recent call last):
  File "/home/void/.miniconda3/envs/sqgen/bin/allennlp", line 8, in &lt;module&gt;
    sys.exit(run())
  File "/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/__main__.py", line 34, in run
    main(prog="allennlp")
  File "/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/commands/__init__.py", line 92, in main
    args.func(args)
  File "/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/commands/train.py", line 109, in train_model_from_args
    train_model_from_file(
  File "/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/commands/train.py", line 169, in train_model_from_file
    return train_model(
  File "/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/commands/train.py", line 232, in train_model
    model = _train_worker(
  File "/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/commands/train.py", line 430, in _train_worker
    train_loop = TrainModel.from_params(
  File "/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/common/from_params.py", line 591, in from_params
    return retyped_subclass.from_params(
  File "/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/common/from_params.py", line 622, in from_params
    kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)
  File "/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/common/from_params.py", line 192, in create_kwargs
    constructed_arg = pop_and_construct_arg(
  File "/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/common/from_params.py", line 302, in pop_and_construct_arg
    return construct_arg(class_name, name, popped_params, annotation, default, **extras)
  File "/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/common/from_params.py", line 336, in construct_arg
    return annotation.from_params(params=popped_params, **subextras)
  File "/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/common/from_params.py", line 591, in from_params
    return retyped_subclass.from_params(
  File "/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp/common/from_params.py", line 624, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File "/home/void/.miniconda3/envs/sqgen/lib/python3.8/site-packages/allennlp_models/generation/dataset_readers/seq2seq.py", line 85, in __init__
    self._start_token, self._end_token = self._source_tokenizer.tokenize(
ValueError: too many values to unpack (expected 2)



&lt;denchmark-h:h2&gt;Related issues or possible duplicates&lt;/denchmark-h&gt;


None

&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

OS: GNU/LINUX 5.8.9_1
Python version: 3.8.5

Output of pip freeze:

alabaster==0.7.12
alembic==1.4.3
allennlp==1.1.0
allennlp-models==1.1.0
apache-airflow==1.10.12
apispec==1.3.3
argcomplete==1.12.0
argon2-cffi==20.1.0
astor==0.8.1
async-generator==1.10
attrs==19.3.0
Babel==2.8.0
backcall==0.2.0
bandit==1.6.2
bleach==3.2.1
blis==0.4.1
boto3==1.15.4
botocore==1.18.4
cached-property==1.5.2
catalogue==1.0.0
cattrs==1.0.0
certifi==2020.4.5.2
cffi==1.14.3
chardet==3.0.4
click==7.1.2
colorama==0.4.3
colorlog==4.0.2
configparser==3.5.3
conllu==4.1
coverage==5.1
croniter==0.3.34
cymem==2.0.3
darglint==1.4.1
datasets==1.0.1
decorator==4.4.2
defusedxml==0.6.0
dictdiffer==0.8.1
dill==0.3.2
dnspython==2.0.0
doc8==0.8.1
docutils==0.16
dparse==0.5.1
email-validator==1.1.1
entrypoints==0.3
eradicate==1.0
filelock==3.0.12
flake8==3.8.3
flake8-bandit==2.1.2
flake8-broken-line==0.2.0
flake8-bugbear==19.8.0
flake8-commas==2.0.0
flake8-comprehensions==3.2.3
flake8-debugger==3.2.1
flake8-docstrings==1.5.0
flake8-eradicate==0.3.0
flake8-isort==3.0.1
flake8-plugin-utils==1.3.1
flake8-polyfill==1.0.2
flake8-pytest-style==1.3.0
flake8-quotes==2.1.2
flake8-rst-docstrings==0.0.12
flake8-string-format==0.2.3
Flask==1.1.2
Flask-Admin==1.5.4
Flask-AppBuilder==2.3.0
Flask-Babel==1.0.0
Flask-Caching==1.3.3
Flask-JWT-Extended==3.24.1
Flask-Login==0.4.1
Flask-OpenID==1.2.5
Flask-SQLAlchemy==2.4.4
flask-swagger==0.2.14
Flask-WTF==0.14.3
ftfy==5.8
funcsigs==1.0.2
future==0.18.2
gitdb==4.0.5
GitPython==3.1.3
graphviz==0.14.1
gunicorn==20.0.4
h5py==2.10.0
idna==2.9
imagesize==1.2.0
importlib-metadata==1.6.1
ipykernel==5.3.4
ipython==7.16.1
ipython-genutils==0.2.0
ipywidgets==7.5.1
iso8601==0.1.13
isort==4.3.21
itsdangerous==1.1.0
jedi==0.17.2
Jinja2==2.11.2
jmespath==0.10.0
joblib==0.16.0
json-merge-patch==0.2
jsonnet==0.16.0
jsonpickle==1.4.1
jsonschema==3.2.0
jupyter==1.0.0
jupyter-client==6.1.7
jupyter-console==6.2.0
jupyter-core==4.6.3
jupyterlab-pygments==0.1.1
lazy-object-proxy==1.5.1
lockfile==0.12.2
m2r==0.2.1
Mako==1.1.3
Markdown==2.6.11
MarkupSafe==1.1.1
marshmallow==3.6.1
marshmallow-enum==1.5.1
marshmallow-polyfield==5.9
marshmallow-sqlalchemy==0.23.1
mccabe==0.6.1
mistune==0.8.4
more-itertools==8.4.0
murmurhash==1.0.2
mypy==0.782
mypy-extensions==0.4.3
natsort==7.0.1
nbclient==0.5.0
nbconvert==6.0.3
nbformat==5.0.7
nest-asyncio==1.4.0
nitpick==0.22.2
nltk==3.5
notebook==6.1.4
numpy==1.19.2
overrides==3.1.0
packaging==20.4
pandas==0.25.3
pandocfilters==1.4.2
parso==0.7.1
pbr==5.4.5
pendulum==1.4.4
pep8-naming==0.9.1
pexpect==4.8.0
pickleshare==0.7.5
plac==1.1.3
pluggy==0.13.1
preshed==3.0.2
prison==0.1.3
prometheus-client==0.8.0
prompt-toolkit==3.0.3
protobuf==3.13.0
psutil==5.7.2
ptyprocess==0.6.0
py==1.8.1
py-rouge==1.1
py4j==0.10.9
pyarrow==1.0.1
pycodestyle==2.6.0
pycparser==2.20
pydocstyle==5.0.2
pyflakes==2.2.0
Pygments==2.6.1
PyJWT==1.7.1
pyparsing==2.4.7
pyrsistent==0.17.3
pyspark==3.0.1
pytest==5.4.3
pytest-cov==2.10.1
pytest-randomly==3.4.1
python-daemon==2.2.4
python-dateutil==2.8.1
python-editor==1.0.4
python-nvd3==0.15.0
python-openid==2.2.5
python-slugify==4.0.0
pytz==2020.1
pytzdata==2020.1
PyYAML==5.3.1
pyzmq==19.0.2
qtconsole==4.7.7
QtPy==1.9.0
regex==2020.9.27
requests==2.23.0
restructuredtext-lint==1.3.1
ruamel.yaml==0.16.10
ruamel.yaml.clib==0.2.0
s3transfer==0.3.3
sacremoses==0.0.43
safety==1.9.0
scikit-learn==0.23.2
scipy==1.5.2
Send2Trash==1.5.0
sentencepiece==0.1.91
setproctitle==1.1.10
six==1.15.0
smmap==3.0.4
snowballstemmer==2.0.0
sortedcontainers==2.2.2
spacy==2.3.2
Sphinx==2.4.4
sphinx-autodoc-typehints==1.10.3
sphinxcontrib-applehelp==1.0.2
sphinxcontrib-devhelp==1.0.2
sphinxcontrib-htmlhelp==1.0.3
sphinxcontrib-jsmath==1.0.1
sphinxcontrib-qthelp==1.0.3
sphinxcontrib-serializinghtml==1.1.4
SQLAlchemy==1.3.19
SQLAlchemy-JSONField==0.9.0
SQLAlchemy-Utils==0.36.8
srsly==1.0.2
stevedore==2.0.0
tabulate==0.8.7
tenacity==4.12.0
tensorboardX==2.1
terminado==0.8.3
testfixtures==6.14.1
testpath==0.4.4
text-unidecode==1.3
thinc==7.4.1
threadpoolctl==2.1.0
thrift==0.13.0
tokenizers==0.8.1rc1
toml==0.10.0
tomlkit==0.7.0
torch==1.6.0
tornado==6.0.4
tqdm==4.49.0
traitlets==4.3.3
transformers==3.0.2
typed-ast==1.4.1
typing-extensions==3.7.4.2
tzlocal==1.5.1
unicodecsv==0.14.1
urllib3==1.25.9
wasabi==0.8.0
wcwidth==0.2.4
webencodings==0.5.1
wemake-python-styleguide==0.14.1
Werkzeug==0.16.1
widgetsnbextension==3.5.1
word2number==1.1
WTForms==2.3.3
xxhash==2.0.0
zipp==3.1.0
zope.deprecation==4.4.0




&lt;denchmark-h:h2&gt;Steps to reproduce&lt;/denchmark-h&gt;

Use any Seq2Seq dataset with the bart model:

Example source:

local model_name = "sshleifer/distilbart-xsum-12-6";
local data_base_url = "data/pubmedqa/processed/";

{
  "train_data_path": data_base_url + "train.tsv",
  "validation_data_path": data_base_url + "test.tsv",
  "dataset_reader": {
    "type": "seq2seq",
    "source_tokenizer": {
      "type": "pretrained_transformer",
      "model_name": model_name
    },
    "source_token_indexers": {
      "tokens": {
        "type": "pretrained_transformer",
        "model_name": model_name,
        "namespace": "tokens"
      }
    },
    "source_add_start_token": false,
    "source_add_end_token": false,
    "target_add_start_token": false,
    "target_add_end_token": false,
    "source_max_tokens": 512,
    "target_max_tokens": 40,
  },
  "model": {
    "type": "bart",
    "model_name": model_name
  },
  "data_loader": {
    "batch_size": 2,
    "shuffle": true
  },
  "trainer": {
    "num_epochs": 1,
    "optimizer": {
      "type": "huggingface_adamw",
      "lr": 3e-5,
      "betas": [0.9, 0.999],
      "eps": 1e-8,
      "correct_bias": true
    },
    "learning_rate_scheduler": {
      "type": "polynomial_decay",
    },
    "grad_norm": 1.0,
  }
}



	</description>
	<comments>
		<comment id='1' author='vikigenius' date='2020-09-30T19:18:43Z'>
		Okay, I isolated the issue, you have to specify the correct start symbols and end symbols in the config for the pretrained transformer tokenizer, which are &lt;s&gt; and &lt;/s&gt; respectively. Closing, since it's not a bug.
		</comment>
		<comment id='2' author='vikigenius' date='2020-09-30T19:24:24Z'>
		I would still consider this a bug. Here's a fix: &lt;denchmark-link:https://github.com/allenai/allennlp-models/pull/139&gt;allenai/allennlp-models#139&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>