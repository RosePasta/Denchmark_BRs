<bug id='4580' author='wj-Mcat' open_date='2020-08-20T03:22:44Z' closed_time='2020-09-03T16:21:59Z'>
	<summary>a small pretrained model occurs error with `out of memory `</summary>
	<description>
When I run bert+crf model to complete sequence tagging task, it occurs RuntimeError:
RuntimeError: CUDA out of memory. Tried to allocate 148.00 MiB (GPU 2; 10.92 GiB total capacity; 7.35 GiB already allocated; 124.50 MiB free; 7.60 GiB reserved in total by PyTorch)
Configuration file is:
{
  "dataset_reader": {
    "type": "sequence_tagging",
    "token_indexers": {
      "tokens": {
        "type": "pretrained_transformer_mismatched",
        "model_name": "bert-base-chinese"
      }
    }
  },
  "train_data_path": "./data/train_base_phrase.txt",
  "data_loader": {
    "batch_size": 16,
    "shuffle": true
  },
  "model": {
    "type": "crf_tagger",
    "text_field_embedder": {
      "token_embedders": {
        "tokens": {
          "type": "pretrained_transformer_mismatched",
          "model_name": "bert-base-chinese"
        }
      }
    },
    "encoder": {
      "type": "pass_through",
      "input_dim":768
    },
    "calculate_span_f1": true,
    "label_encoding": "BIO"
  },
  "trainer": {
    "optimizer": "adagrad",
    "patience": 10,
    "cuda_device": 2
  }
}
My dataset is 2.3M. So this error make me confused.
refer to : &lt;denchmark-link:https://discourse.allennlp.org/t/pretrained-model-gpu-memory-consumption/356&gt;pretrained-model-gpu-memory-consumption&lt;/denchmark-link&gt;
 , how to make pre-trained model run well for me ?
	</description>
	<comments>
		<comment id='1' author='wj-Mcat' date='2020-08-20T03:28:53Z'>
		Customized Trainer can give me a hand ?
		</comment>
		<comment id='2' author='wj-Mcat' date='2020-08-20T16:39:38Z'>
		16 is a rather large batch size for a BERT model on a GPU with only 10-11 GB of memory. I would try a batch size of 4 or 8 instead.
		</comment>
		<comment id='3' author='wj-Mcat' date='2020-08-23T05:38:53Z'>
		&lt;denchmark-link:https://github.com/epwalsh&gt;@epwalsh&lt;/denchmark-link&gt;
 but, 4 or 8 batch_size is too small for trainning.  Is there any solution to resolve it ?
		</comment>
		<comment id='4' author='wj-Mcat' date='2020-08-24T15:49:38Z'>
		&lt;denchmark-link:https://github.com/wj-Mcat&gt;@wj-Mcat&lt;/denchmark-link&gt;
, there are a couple alternatives:

Use gradient checkpointing, i.e. set gradient_checkpointing to True in your pretrained_transformer_mismatched embedder config.
Use gradient accumulation, i.e. set num_gradient_accumulation_steps to an integer larger than 1 in your trainer config. For example, setting batch_size to 8 and num_gradient_accumulation_steps to 2 gives you an effective batch size of 16.

		</comment>
		<comment id='5' author='wj-Mcat' date='2020-09-03T16:21:57Z'>
		This issue is being closed due to lack of activity. If you think it still needs to be addressed, please comment on this thread ðŸ‘‡
		</comment>
	</comments>
</bug>