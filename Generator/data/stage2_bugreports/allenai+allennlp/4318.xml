<bug id='4318' author='epwalsh' open_date='2020-06-03T20:43:11Z' closed_time='2020-06-04T22:50:03Z'>
	<summary>Peak CPU memory not reported correctly in distributed training</summary>
	<description>
I noticed this bug while benchmarking some distributed experiments on beaker. I think allennlp.common.util.peak_memory_mb() is only called from the master process, and it only reports the memory usage for the master process. This should really report the combined memory used across all workers.
	</description>
	<comments>
	</comments>
</bug>