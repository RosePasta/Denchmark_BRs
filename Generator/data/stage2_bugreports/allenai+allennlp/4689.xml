<bug id='4689' author='vikigenius' open_date='2020-10-01T05:09:11Z' closed_time='2020-12-02T16:41:48Z'>
	<summary>OOM with BART during validation even with low batch size and truncation</summary>
	<description>
I am training BART on a custom Seq2Seq dataset.
I am getting OOM errors during validation even when the batch size is 1.
Currently my training batch size is 3, with source_max_tokens=1022 and target_max_tokens=54.
My validation batch size is 1, with source_max_tokens=512 and target_max_tokens=54.
I am training on the 16GB Tesla V100.
Even with reduced source_max_tokens I am getting an OOM during validation. How is it possible that training does not cause OOM but validation does?

Configuration:

local model_name = "sshleifer/distilbart-xsum-12-6";
local data_base_url = "data/hqa_canlii/processed/";
local num_gpus = 8;

{
  "train_data_path": data_base_url + "train.tsv",
  "validation_data_path": data_base_url + "test.tsv",
  "dataset_reader": {
    "type": "seq2seq",
    "source_tokenizer": {
      "type": "pretrained_transformer",
      "model_name": model_name,
    },
    "source_token_indexers": {
      "tokens": {
        "type": "pretrained_transformer",
        "model_name": model_name,
        "namespace": "tokens"
      }
    },
    "start_symbol": "",
    "end_symbol": "",
    "source_add_start_token": false,
    "source_add_end_token": false,
    "target_add_start_token": false,
    "target_add_end_token": false,
    "source_max_tokens": 1022,
    "target_max_tokens": 54,
  },
  "validation_dataset_reader": {
    "type": "seq2seq",
    "source_tokenizer": {
      "type": "pretrained_transformer",
      "model_name": model_name,
    },
    "source_token_indexers": {
      "tokens": {
        "type": "pretrained_transformer",
        "model_name": model_name,
        "namespace": "tokens"
      }
    },
    "start_symbol": "",
    "end_symbol": "",
    "source_add_start_token": false,
    "source_add_end_token": false,
    "target_add_start_token": false,
    "target_add_end_token": false,
    "source_max_tokens": 512,
    "target_max_tokens": 40,
  },
  "model": {
    "type": "bartqgen",
    "model_name": model_name,
    "max_decoding_steps": 40,
    "beam_size": 4,
  },
  "data_loader": {
    "batch_size": 3,
    "shuffle": true
  },
  "validation_data_loader": {
    "batch_size": 1,
    "shuffle": true
  },
  "distributed": {
    "cuda_devices": if num_gpus &gt; 1 then std.range(0, num_gpus - 1) else 0,
  },
  "trainer": {
    "num_epochs": 6,
    "optimizer": {
      "type": "huggingface_adamw",
      "lr": 3e-5,
      "betas": [0.9, 0.999],
      "eps": 1e-8,
      "correct_bias": true
    },
    "learning_rate_scheduler": {
      "type": "polynomial_decay",
    },
    "use_amp": true,
    "grad_norm": 1.0,
  }
}




Traceback:

Traceback (most recent call last):
  File "/home/ec2-user/miniconda3/envs/sqgendebug/bin/allennlp", line 8, in &lt;module&gt;
    sys.exit(run())
  File "/home/ec2-user/miniconda3/envs/sqgendebug/lib/python3.8/site-packages/allennlp/__main__.py", line 34, in run
    main(prog="allennlp")
  File "/home/ec2-user/miniconda3/envs/sqgendebug/lib/python3.8/site-packages/allennlp/commands/__init__.py", line 92, in main
    args.func(args)
  File "/home/ec2-user/miniconda3/envs/sqgendebug/lib/python3.8/site-packages/allennlp/commands/train.py", line 109, in train_model_from_args
    train_model_from_file(
  File "/home/ec2-user/miniconda3/envs/sqgendebug/lib/python3.8/site-packages/allennlp/commands/train.py", line 169, in train_model_from_file
    return train_model(
  File "/home/ec2-user/miniconda3/envs/sqgendebug/lib/python3.8/site-packages/allennlp/commands/train.py", line 232, in train_model
    model = _train_worker(
  File "/home/ec2-user/miniconda3/envs/sqgendebug/lib/python3.8/site-packages/allennlp/commands/train.py", line 443, in _train_worker
    metrics = train_loop.run()
  File "/home/ec2-user/miniconda3/envs/sqgendebug/lib/python3.8/site-packages/allennlp/commands/train.py", line 505, in run
    return self.trainer.train()
  File "/home/ec2-user/miniconda3/envs/sqgendebug/lib/python3.8/site-packages/allennlp/training/trainer.py", line 879, in train
    val_loss, val_reg_loss, num_batches = self._validation_loss(epoch)
  File "/home/ec2-user/miniconda3/envs/sqgendebug/lib/python3.8/site-packages/allennlp/training/trainer.py", line 776, in _validation_loss
    batch_outputs = self.batch_outputs(batch, for_training=False)
  File "/home/ec2-user/miniconda3/envs/sqgendebug/lib/python3.8/site-packages/allennlp/training/trainer.py", line 479, in batch_outputs
    output_dict = self._pytorch_model(**batch)
  File "/home/ec2-user/miniconda3/envs/sqgendebug/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ec2-user/miniconda3/envs/sqgendebug/lib/python3.8/site-packages/allennlp_models/generation/models/bart.py", line 243, in forward
    beam_result = self._beam_search.search(
  File "/home/ec2-user/miniconda3/envs/sqgendebug/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 15, in decorate_context
    return func(*args, **kwargs)
  File "/home/ec2-user/miniconda3/envs/sqgendebug/lib/python3.8/site-packages/allennlp/nn/beam_search.py", line 233, in search
    class_log_probabilities, state = step(last_predictions, state, timestep + 1)
  File "/home/ec2-user/miniconda3/envs/sqgendebug/lib/python3.8/site-packages/allennlp_models/generation/models/bart.py", line 342, in take_step
    outputs = self.bart(
  File "/home/ec2-user/miniconda3/envs/sqgendebug/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ec2-user/miniconda3/envs/sqgendebug/lib/python3.8/site-packages/transformers/modeling_bart.py", line 1005, in forward
    lm_logits = F.linear(outputs[0], self.model.shared.weight, bias=self.final_logits_bias)
  File "/home/ec2-user/miniconda3/envs/sqgendebug/lib/python3.8/site-packages/torch/nn/functional.py", line 1676, in linear
    output = input.matmul(weight.t())
RuntimeError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 15.78 GiB total capacity; 14.45 GiB already allocated; 13.75 MiB free; 14.60 GiB reserved in total by PyTorch)




Just for verification I tried to train on the validation set to see if I get OOM during training, but I don't so maybe the issue lies with beam search?
	</description>
	<comments>
		<comment id='1' author='vikigenius' date='2020-10-01T16:30:54Z'>
		On further investigation and debugging on the beam search code. I found that state['input_ids'] is occupying storage of size 2048 = beam_size*512, even though it should be just 512. I am not sure why this is happening since unsqueeze and expand should preserve memory.
More specifically the storage size jumps from 512 to 2048 for input_ids, specifically in the following snippet:
&lt;denchmark-link:https://github.com/allenai/allennlp/blob/master/allennlp/nn/beam_search.py#L357-L368&gt;https://github.com/allenai/allennlp/blob/master/allennlp/nn/beam_search.py#L357-L368&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;                    _, *last_dims = state_tensor.size()
                    # shape: (batch_size, beam_size, *)
                    expanded_backpointer = backpointer.view(
                        batch_size, self.beam_size, *([1] * len(last_dims))
                    ).expand(batch_size, self.beam_size, *last_dims)

                    # shape: (batch_size * beam_size, *)
                    state[key] = (
                        state_tensor.reshape(batch_size, self.beam_size, *last_dims)
                        .gather(1, expanded_backpointer)
                        .reshape(batch_size * self.beam_size, *last_dims)
                    )
&lt;/denchmark-code&gt;

Even though the shape remains the same.
		</comment>
		<comment id='2' author='vikigenius' date='2020-10-07T00:07:56Z'>
		Hi there,
I’m not able to reproduce this issue. It looks like the size of state stays consistent within that snippet (in the timestep loop). It’s expected that the state would be updated to consider each beam, not just a singular beam.
		</comment>
		<comment id='3' author='vikigenius' date='2020-10-07T00:31:52Z'>
		&lt;denchmark-link:https://github.com/jvstokes&gt;@jvstokes&lt;/denchmark-link&gt;
  What do you mean size of the state? That's a dictionary right? I am talking specifically about the GPU storage of 
Evaluate state['input_ids'].storage() before and after the gather step in line 364-368, it doubles.
Which at least for that specific key should not happen because state[input_ids] holds input_ids which should never change no matter what the beam is right? Or am i misunderstanding something?
		</comment>
		<comment id='4' author='vikigenius' date='2020-10-07T18:21:15Z'>
		Are you having the same issue with any other tensors within the state or just for that key? Can you provide a repro example?
		</comment>
		<comment id='5' author='vikigenius' date='2020-10-07T20:06:01Z'>
		&lt;denchmark-link:https://github.com/jvstokes&gt;@jvstokes&lt;/denchmark-link&gt;

Use the following configuration copied from your test_fixtures with slight modification.

Configuration:

local model_name = "sshleifer/bart-tiny-random";
local data_base_url = "test_fixtures/generation/bart/data/";

{
    "train_data_path": data_base_url + "/url_lists/all_train.txt",
    "validation_data_path": data_base_url + "/url_lists/all_val.txt",
    "dataset_reader": {
        "type": "cnn_dm",
        "source_tokenizer": {
            "type": "pretrained_transformer",
            "model_name": model_name
        },
        "source_token_indexers": {
            "tokens": {
                "type": "pretrained_transformer",
                "model_name": model_name,
                "namespace": "tokens"
            }
        },
        "source_max_tokens": 512,
        "target_max_tokens": 54,
    },
    "model": {
        "type": "bart",
        "model_name": model_name,
	"beam_size": 4,
	"max_decoding_steps": 40,
    },
    "data_loader": {
        "batch_size": 2,
        "shuffle": true
    },
    "validation_data_loader": {
        "batch_size": 1,
        "shuffle": true
    },
    "trainer": {
        "num_epochs": 1,
        "optimizer": {
            "type": "huggingface_adamw",
            "lr": 3e-5,
            "betas": [0.9, 0.999],
            "eps": 1e-8,
            "correct_bias": true
        },
        "learning_rate_scheduler": {
            "type": "polynomial_decay",
        },
	"cuda_device": 0,
	"use_amp": true,
        "grad_norm": 1.0,
    }
}



Now if you evaluate the following snippet: 


allennlp/allennlp/nn/beam_search.py


        Lines 364 to 368
      in
      ae7cf85






 state[key] = ( 



 state_tensor.reshape(batch_size, self.beam_size, *last_dims) 



     .gather(1, expanded_backpointer) 



     .reshape(batch_size * self.beam_size, *last_dims) 



 ) 





Just add 2 variables to track the storage size
&lt;denchmark-code&gt;ssb = state['input_ids'].storage().size()
state[key] = (
    state_tensor.reshape(batch_size, self.beam_size, *last_dims)
    .gather(1, expanded_backpointer)
    .reshape(batch_size * self.beam_size, *last_dims)
)
ssa = state['input_ids'].storage().size()
assert ssa == ssb, 'Oh No! memory storage of input_ids has increased. But value and shape still same"
&lt;/denchmark-code&gt;

Afaik input_ids are never going to change,
So for example with batch_size=1 and beam_size=4, somewhere in the code expand or repeat must have been called to get shape from [1, 512] -&gt; [4, 512]. But no additional memory allocated because it is just a different view  i.e memory should still have only 512 and that's true if you check value of ssb.
But after the gather call, even though the shape of the tensor is still [4, 512] memory becomes 4x512. And because input_ids is never going to change this is a waste of memory.
		</comment>
		<comment id='6' author='vikigenius' date='2020-10-07T20:14:37Z'>
		Ideally the reshape-&gt;gather-&gt;reshape call should not happen on tensors that we know are never going to change. Because why would you have 4 explicit copies of the same input_ids? Just an expanded tensor of shape [beam_size, seq_len] but occupying the memory of just seq_len should be enough.
The same idea would be generalized to batch_size &gt; 1.
The same could be said of many other tensors that are part of the state, but are never going to change, for eg: input_mask.
Now I am operating under the assumption that these tensors are never going to change irrespective of the beam under consideration, and that's what I am observing while debugging. Please correct me if my assumption is wrong.
If my assumption is right, a quick, hacky solution would be to have a blacklist of keys in the state for which the reshape-&gt;gather-&gt;reshape logic should not be applied.
		</comment>
		<comment id='7' author='vikigenius' date='2020-10-07T20:26:28Z'>
		&lt;denchmark-link:https://github.com/vikigenius&gt;@vikigenius&lt;/denchmark-link&gt;
 the work-around to avoid the unnecessary  /  calls on  would be to leave  of the . You'd just have to modify the step function in BART (well, the step function would probably have to  be created dynamically as a closure, but that wouldn't be too hard).
By the way, are you sure this is the main reason you're getting OOM errors? It seems like this would only account for a very small amount of memory.
		</comment>
		<comment id='8' author='vikigenius' date='2020-10-07T20:32:45Z'>
		&lt;denchmark-link:https://github.com/epwalsh&gt;@epwalsh&lt;/denchmark-link&gt;

I am not sure this is the main reason I have OOM errors, but this could be a reason. Because reducing the max_decoding_steps and the beam_size helps me out.
Also, this storage increase does not happen when the batch_size &gt; 1. So I think the whole thing could be avoided by expanding the state tensors properly.
When the batch_size &gt; 1 the storage increases even before the gather step, it increases because of the reshape call here:



allennlp/allennlp/nn/beam_search.py


        Lines 242 to 246
      in
      ae7cf85






 state[key] = ( 



 state_tensor.unsqueeze(1) 



     .expand(batch_size, self.beam_size, *last_dims) 



     .reshape(batch_size * self.beam_size, *last_dims) 



 ) 





		</comment>
		<comment id='9' author='vikigenius' date='2020-10-07T21:03:38Z'>
		Ultimately for my particular problem reducing the max_decoding_steps from 40 to 30 has helped prevent OOM errors. But after setting the validation batch_size to 1, I expected to be able to do at least a 40 step decoding.
		</comment>
		<comment id='10' author='vikigenius' date='2020-10-22T16:34:04Z'>
		&lt;denchmark-link:https://github.com/jvstokes&gt;@jvstokes&lt;/denchmark-link&gt;
 this is just a friendly ping to make sure you haven't forgotten about this issue 
		</comment>
		<comment id='11' author='vikigenius' date='2020-11-05T16:38:05Z'>
		&lt;denchmark-link:https://github.com/jvstokes&gt;@jvstokes&lt;/denchmark-link&gt;
 this is just a friendly ping to make sure you haven't forgotten about this issue 
		</comment>
		<comment id='12' author='vikigenius' date='2020-11-20T16:31:27Z'>
		&lt;denchmark-link:https://github.com/jvstokes&gt;@jvstokes&lt;/denchmark-link&gt;
 this is just a friendly ping to make sure you haven't forgotten about this issue 
		</comment>
		<comment id='13' author='vikigenius' date='2020-11-21T01:55:15Z'>
		&lt;denchmark-link:https://github.com/vikigenius&gt;@vikigenius&lt;/denchmark-link&gt;
  I got a nearly same bug in &lt;denchmark-link:https://github.com/allenai/allennlp/issues/4805&gt;#4805&lt;/denchmark-link&gt;
. I wonder if you use the param .
And it's fine when I remove this param.
		</comment>
		<comment id='14' author='vikigenius' date='2020-11-23T22:38:39Z'>
		&lt;denchmark-link:https://github.com/wlhgtc&gt;@wlhgtc&lt;/denchmark-link&gt;
 Yes I do use use_amp: true
		</comment>
		<comment id='15' author='vikigenius' date='2020-12-02T16:41:47Z'>
		This issue is being closed due to lack of activity. If you think it still needs to be addressed, please comment on this thread 👇
		</comment>
	</comments>
</bug>