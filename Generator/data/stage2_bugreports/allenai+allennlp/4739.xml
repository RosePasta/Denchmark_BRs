<bug id='4739' author='MichalMalyska' open_date='2020-10-19T23:30:31Z' closed_time='2020-11-05T23:50:04Z'>
	<summary>Potential bug: The maxpool in cnn_encoder can be triggered by pad tokens.</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

When using a text_field_embedder -&gt; cnn_encoder (without seq2seq_encoder), the output of the embedder (and mask) get fed directly into the cnn_encoder. The pad tokens will get masked (set to 0), but it's still possible that after applying the mask followed by the CNN, the PAD tokens are those with highest activations. This could lead to the same exact datapoint getting different predictions if's part of a batch vs single prediction.
&lt;denchmark-h:h2&gt;Related issues or possible duplicates&lt;/denchmark-h&gt;


None

&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

OS:  NA
Python version: NA
&lt;denchmark-h:h2&gt;Steps to reproduce&lt;/denchmark-h&gt;

This can be reproduced by replacing



allennlp/allennlp/modules/seq2vec_encoders/cnn_encoder.py


         Line 113
      in
      00bb6c5






 filter_outputs.append(self._activation(convolution_layer(tokens)).max(dim=2)[0]) 





&lt;denchmark-code&gt;filter_outputs.append(self._activation(convolution_layer(tokens)).max(dim=2)[0])
&lt;/denchmark-code&gt;

with
&lt;denchmark-code&gt;activated_outputs, max_indices = self._activation(convolution_layer(tokens)).max(dim=2)
&lt;/denchmark-code&gt;

and checking the indices for the same example inside of a batch vs unpadded.
&lt;denchmark-h:h2&gt;Possible solution:&lt;/denchmark-h&gt;

We could resolve this by adding a large negative value to all CNN outputs for masked tokens, similarly to what they do in the transformers library (&lt;denchmark-link:https://github.com/huggingface/transformers/issues/542&gt;huggingface/transformers#542&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/huggingface/transformers/blob/c912ba5f69a47396244c64deada5c2b8a258e2b8/src/transformers/modeling_bert.py#L262&gt;https://github.com/huggingface/transformers/blob/c912ba5f69a47396244c64deada5c2b8a258e2b8/src/transformers/modeling_bert.py#L262&lt;/denchmark-link&gt;
), but I have not been able to figure out how to do this efficiently.
	</description>
	<comments>
		<comment id='1' author='MichalMalyska' date='2020-10-21T14:24:54Z'>
		I created a google collab notebook that showcases how the problem arises and potential solution:
&lt;denchmark-link:https://colab.research.google.com/drive/1i73ZCEdPGRjS_hKNlrqHBklITME2hGKL?usp=sharing&gt;https://colab.research.google.com/drive/1i73ZCEdPGRjS_hKNlrqHBklITME2hGKL?usp=sharing&lt;/denchmark-link&gt;

The solution I created creates an additive mask with large negative values for all filter activations that involved a pad token anywhere. It is a bit clunky so I would appreciate any ideas on how to make this more readable / efficient before creating a PR to fix the bug.
		</comment>
		<comment id='2' author='MichalMalyska' date='2020-10-21T18:05:28Z'>
		Thanks for the detailed report &lt;denchmark-link:https://github.com/MichalMalyska&gt;@MichalMalyska&lt;/denchmark-link&gt;
. I'll take a look at this soon.
		</comment>
		<comment id='3' author='MichalMalyska' date='2020-10-21T18:25:47Z'>
		&lt;denchmark-link:https://github.com/MichalMalyska&gt;@MichalMalyska&lt;/denchmark-link&gt;
 It seems like you have a reasonable solution, would you mind opening up a PR with your fix?
By the way, you can utilize allennlp.nn.util.min_value_of_dtype() to get an appropriate large negative number for the masked values instead of always using 10e-5.
		</comment>
		<comment id='4' author='MichalMalyska' date='2020-10-21T19:04:29Z'>
		Yeah, I'll make a PR + some tests on the new seq2vec to ensure the batched and non-batched predictions have the same results. Similar tests are probably in order for other seq2vec encoders that don't use the pack_padded_sequence and pad_packed_sequence methods, and for potentially seq2seq just to ensure masking is actually working as intended.
There is one design decision that needs to be made:
Since the approach requires a mask to be not none, we can either:

create a new one of all ones at the start if the original one is None
Wrap all of this in an if statement that runs the old version of code if mask is None and the new one otherwise.

I am partial to just creating a mask tensor of all True, since it seems much more elegant and doesn't increase the code complexity but it adds memory overhead.
		</comment>
		<comment id='5' author='MichalMalyska' date='2020-10-21T20:20:12Z'>
		If creating a mask of all ones significantly reduces the complexity, then I'm for it. It should be rare that mask is actually None, so I'm not too worried about the performance penalty.
		</comment>
		<comment id='6' author='MichalMalyska' date='2020-11-05T16:38:03Z'>
		&lt;denchmark-link:https://github.com/epwalsh&gt;@epwalsh&lt;/denchmark-link&gt;
 this is just a friendly ping to make sure you haven't forgotten about this issue 
		</comment>
	</comments>
</bug>