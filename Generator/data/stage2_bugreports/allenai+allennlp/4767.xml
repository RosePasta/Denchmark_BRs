<bug id='4767' author='Jazzssmine' open_date='2020-11-03T00:35:51Z' closed_time='2020-11-06T08:26:54Z'>
	<summary>No super class method found for "decode"</summary>
	<description>
&lt;denchmark-h:h2&gt;Checklist&lt;/denchmark-h&gt;


 I have verified that the issue exists against the master branch of AllenNLP.
 I have read the relevant section in the contribution guide on reporting bugs.
 I have checked the issues list for similar or identical bug reports.
 I have checked the pull requests list for existing proposed fixes.
 I have checked the CHANGELOG and the commit log to find out if the bug was already fixed in the master branch.
 I have included in the "Description" section below a traceback from any exceptions related to this bug.
 I have included in the "Related issues or possible duplicates" section beloew all related issues and possible duplicate issues (If there are none, check this box anyway).
 I have included in the "Environment" section below the name of the operating system and Python version that I was using when I discovered this bug.
 I have included in the "Environment" section below the output of pip freeze.
 I have included in the "Steps to reproduce" section below a minimally reproducible example.

&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

I'm running classifier and got the following error:

Python traceback:

Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.8/bin/allennlp", line 8, in &lt;module&gt;
    sys.exit(run())
  File "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/allennlp/__main__.py", line 34, in run
    main(prog="allennlp")
  File "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/allennlp/commands/__init__.py", line 117, in main
    import_module_and_submodules(package_name)
  File "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/allennlp/common/util.py", line 351, in import_module_and_submodules
    import_module_and_submodules(subpackage)
  File "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/allennlp/common/util.py", line 340, in import_module_and_submodules
    module = importlib.import_module(package_name)
  File "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "&lt;frozen importlib._bootstrap&gt;", line 1014, in _gcd_import
  File "&lt;frozen importlib._bootstrap&gt;", line 991, in _find_and_load
  File "&lt;frozen importlib._bootstrap&gt;", line 975, in _find_and_load_unlocked
  File "&lt;frozen importlib._bootstrap&gt;", line 671, in _load_unlocked
  File "&lt;frozen importlib._bootstrap_external&gt;", line 783, in exec_module
  File "&lt;frozen importlib._bootstrap&gt;", line 219, in _call_with_frames_removed
  File "/Users/wangyian/Desktop/Jasmine/EPFL/semester_project_I/ALLEN/allen_tweet/dont_stop_pretraining/models/__init__.py", line 1, in &lt;module&gt;
    from dont_stop_pretraining.models.basic_classifier_with_f1 import BasicClassifierWithF1
  File "/Users/wangyian/Desktop/Jasmine/EPFL/semester_project_I/ALLEN/allen_tweet/dont_stop_pretraining/models/basic_classifier_with_f1.py", line 15, in &lt;module&gt;
    class BasicClassifierWithF1(Model):
  File "/Users/wangyian/Desktop/Jasmine/EPFL/semester_project_I/ALLEN/allen_tweet/dont_stop_pretraining/models/basic_classifier_with_f1.py", line 149, in BasicClassifierWithF1
    def decode(self, output_dict: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:
  File "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/overrides/overrides.py", line 67, in overrides
    raise AssertionError('No super class method found for "%s"' % method.__name__)
AssertionError: No super class method found for "decode"



&lt;denchmark-h:h2&gt;Related issues or possible duplicates&lt;/denchmark-h&gt;


code of basic_classifier_with_f1

`@Model.register("basic_classifier_with_f1")
class BasicClassifierWithF1(Model):
    """
    This ``Model`` implements a basic text classifier. After embedding the text into
    a text field, we will optionally encode the embeddings with a ``Seq2SeqEncoder``. The
    resulting sequence is pooled using a ``Seq2VecEncoder`` and then passed to
    a linear classification layer, which projects into the label space. If a
    ``Seq2SeqEncoder`` is not provided, we will pass the embedded text directly to the
    ``Seq2VecEncoder``.
This model additionally provides F1 measure for classification.

Parameters
----------
vocab : ``Vocabulary``
text_field_embedder : ``TextFieldEmbedder``
    Used to embed the input text into a ``TextField``
seq2seq_encoder : ``Seq2SeqEncoder``, optional (default=``None``)
    Optional Seq2Seq encoder layer for the input text.
seq2vec_encoder : ``Seq2VecEncoder``
    Required Seq2Vec encoder layer. If `seq2seq_encoder` is provided, this encoder
    will pool its output. Otherwise, this encoder will operate directly on the output
    of the `text_field_embedder`.
dropout : ``float``, optional (default = ``None``)
    Dropout percentage to use.
num_labels: ``int``, optional (default = ``None``)
    Number of labels to project to in classification layer. By default, the classification layer will
    project to the size of the vocabulary namespace corresponding to labels.
label_namespace: ``str``, optional (default = "labels")
    Vocabulary namespace corresponding to labels. By default, we use the "labels" namespace.
initializer : ``InitializerApplicator``, optional (default=``InitializerApplicator()``)
    If provided, will be used to initialize the model parameters.
regularizer : ``RegularizerApplicator``, optional (default=``None``)
    If provided, will be used to calculate the regularization penalty during training.
"""

def __init__(
    self,
    vocab: Vocabulary,
    text_field_embedder: TextFieldEmbedder,
    seq2vec_encoder: Seq2VecEncoder,
    feedforward_layer: FeedForward,
    seq2seq_encoder: Seq2SeqEncoder = None,
    dropout: float = None,
    num_labels: int = None,
    label_namespace: str = "labels",
    initializer: InitializerApplicator = InitializerApplicator(),
    regularizer: Optional[RegularizerApplicator] = None,
) -&gt; None:

    super().__init__(vocab, regularizer)
    self._text_field_embedder = text_field_embedder

    if seq2seq_encoder:
        self._seq2seq_encoder = seq2seq_encoder
    else:
        self._seq2seq_encoder = None

    self._seq2vec_encoder = seq2vec_encoder
    self._classifier_input_dim = self._seq2vec_encoder.get_output_dim()

    if dropout:
        self._dropout = torch.nn.Dropout(dropout)
    else:
        self._dropout = None

    self._label_namespace = label_namespace

    if num_labels:
        self._num_labels = num_labels
    else:
        self._num_labels = vocab.get_vocab_size(namespace=self._label_namespace)
    self._feedforward_layer = feedforward_layer
    self._classification_layer = torch.nn.Linear(self._classifier_input_dim, self._num_labels)
    self._accuracy = CategoricalAccuracy()
    self._label_f1_metrics: Dict[str, F1Measure] = {}
    for i in range(self._num_labels):
        self._label_f1_metrics[vocab.get_token_from_index(index=i, namespace="labels")] = F1Measure(positive_label=i)
    self._loss = torch.nn.CrossEntropyLoss()
    initializer(self)

def forward(  # type: ignore
    self, tokens: Dict[str, torch.LongTensor], label: torch.IntTensor = None
) -&gt; Dict[str, torch.Tensor]:

    """
    Parameters
    ----------
    tokens : Dict[str, torch.LongTensor]
        From a ``TextField``
    label : torch.IntTensor, optional (default = None)
        From a ``LabelField``

    Returns
    -------
    An output dictionary consisting of:

    logits : torch.FloatTensor
        A tensor of shape ``(batch_size, num_labels)`` representing
        unnormalized log probabilities of the label.
    probs : torch.FloatTensor
        A tensor of shape ``(batch_size, num_labels)`` representing
        probabilities of the label.
    loss : torch.FloatTensor, optional
        A scalar loss to be optimised.
    """
    embedded_text = self._text_field_embedder(tokens)
    mask = get_text_field_mask(tokens).float()

    if self._seq2seq_encoder:
        embedded_text = self._seq2seq_encoder(embedded_text, mask=mask)

    embedded_text = self._seq2vec_encoder(embedded_text, mask=mask)

    if self._dropout:
        embedded_text = self._dropout(embedded_text)

    feedforward_output = self._feedforward_layer(embedded_text)

    logits = self._classification_layer(feedforward_output)
    probs = torch.nn.functional.softmax(logits, dim=-1)

    output_dict = {"logits": logits, "probs": probs}

    if label is not None:
        loss = self._loss(logits, label.long().view(-1))
        output_dict["loss"] = loss
        for i in range(self._num_labels):
            metric = self._label_f1_metrics[self.vocab.get_token_from_index(index=i, namespace="labels")]
            metric(probs, label)
        self._accuracy(logits, label)

    return output_dict

@overrides
def decode(self, output_dict: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:
    """
    Does a simple argmax over the probabilities, converts index to string label, and
    add ``"label"`` key to the dictionary with the result.
    """
    predictions = output_dict["probs"]
    if predictions.dim() == 2:
        predictions_list = [predictions[i] for i in range(predictions.shape[0])]
    else:
        predictions_list = [predictions]
    classes = []
    for prediction in predictions_list:
        label_idx = prediction.argmax(dim=-1).item()
        '''label_str = self.vocab.get_index_to_token_vocabulary(self._label_namespace).get(
            label_idx, str(label_idx)
        )'''
        label_str = self.vocab.get_token_from_index(label_idx, namespace="labels")
        classes.append(label_str)
    output_dict["label"] = classes
    return output_dict

def get_metrics(self, reset: bool = False) -&gt; Dict[str, float]:
    metric_dict = {}
    sum_f1 = 0.0
    for name, metric in self._label_f1_metrics.items():
        metric_val = metric.get_metric(reset)
        sum_f1 += metric_val[2]
    names = list(self._label_f1_metrics.keys())
    total_len = len(names)
    average_f1 = sum_f1 / total_len
    metric_dict['f1'] = average_f1
    metric_dict['accuracy'] = self._accuracy.get_metric(reset)
    return metric_dict

`


&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

OS: Linux
Python version: 3.8.5

Output of pip freeze:


absl-py==0.11.0
altair==4.1.0
appnope==0.1.0
argon2-cffi==20.1.0
astor==0.8.1
async-generator==1.10
attrs==20.2.0
backcall==0.2.0
base58==2.0.1
bleach==3.2.1
blinker==1.4
boto3==1.16.4
botocore==1.19.4
cachetools==4.1.1
certifi==2020.6.20
cffi==1.14.3
chardet==3.0.4
click==7.1.2
decorator==4.4.2
defusedxml==0.6.0
entrypoints==0.3
enum-compat==0.0.3
gitdb==4.0.5
GitPython==3.1.11
google-auth==1.23.0
google-auth-oauthlib==0.4.2
grpcio==1.33.2
idna==2.10
ipykernel==5.3.4
ipython==7.18.1
ipython-genutils==0.2.0
ipywidgets==7.5.1
jedi==0.17.2
Jinja2==2.11.2
jmespath==0.10.0
joblib==0.17.0
jsonschema==3.2.0
jupyter-client==6.1.7
jupyter-core==4.6.3
jupyterlab-pygments==0.1.2
Markdown==3.3.3
MarkupSafe==1.1.1
mistune==0.8.4
nbclient==0.5.1
nbconvert==6.0.7
nbformat==5.0.8
nest-asyncio==1.4.2
notebook==6.1.4
numpy==1.19.2
oauthlib==3.1.0
packaging==20.4
pandas==1.1.3
pandocfilters==1.4.3
parso==0.7.1
pathtools==0.1.2
pexpect==4.8.0
pickleshare==0.7.5
Pillow==8.0.1
prometheus-client==0.8.0
prompt-toolkit==3.0.8
protobuf==3.13.0
ptyprocess==0.6.0
pyarrow==2.0.0
pyasn1==0.4.8
pyasn1-modules==0.2.8
pycparser==2.20
pydeck==0.5.0
Pygments==2.7.2
pyparsing==2.4.7
pyrsistent==0.17.3
python-dateutil==2.8.1
pytz==2020.1
pyzmq==19.0.2
requests==2.24.0
requests-oauthlib==1.3.0
rsa==4.6
s3transfer==0.3.3
scikit-learn==0.23.2
scipy==1.5.3
Send2Trash==1.5.0
seqeval==1.2.2
six==1.15.0
smmap==3.0.4
tensorboard==2.3.0
tensorboard-plugin-wit==1.7.0
tensorboardX==2.1
terminado==0.9.1
testpath==0.4.4
threadpoolctl==2.1.0
tokenizers==0.9.2
toml==0.10.1
toolz==0.11.1
tornado==6.0.4
tqdm==4.51.0
traitlets==5.0.5
tzlocal==2.1
urllib3==1.25.11
validators==0.18.1
wcwidth==0.2.5
webencodings==0.5.1
Werkzeug==1.0.1
widgetsnbextension==3.5.1


&lt;denchmark-h:h2&gt;Steps to reproduce&lt;/denchmark-h&gt;


Example source:




	</description>
	<comments>
		<comment id='1' author='Jazzssmine' date='2020-11-03T19:42:19Z'>
		Hi &lt;denchmark-link:https://github.com/Jazzssmine&gt;@Jazzssmine&lt;/denchmark-link&gt;
 Can you include the output of your  and a code snippet? Particularly, the code in .
		</comment>
		<comment id='2' author='Jazzssmine' date='2020-11-03T20:00:36Z'>
		
Hi @Jazzssmine Can you include the output of your pip freeze and a code snippet? Particularly, the code in basic_classifier_with_f1.py.

Hi, I've updated the code and requirements. : )
		</comment>
		<comment id='3' author='Jazzssmine' date='2020-11-06T08:26:53Z'>
		Hi &lt;denchmark-link:https://github.com/Jazzssmine&gt;@Jazzssmine&lt;/denchmark-link&gt;
 , I believe it's because you are using the  decorator for the  function. The  class, which is the parent class for your , does not have a  method, so there is nothing to override. You can read about overrides &lt;denchmark-link:https://pypi.org/project/overrides/&gt;here&lt;/denchmark-link&gt;
. Removing the decorator should remove the error. Hope this helps!
		</comment>
		<comment id='4' author='Jazzssmine' date='2020-11-06T11:08:58Z'>
		
Hi @Jazzssmine , I believe it's because you are using the @overrides decorator for the decode function. The Model class, which is the parent class for your basic_classifier_with_f1, does not have a decode method, so there is nothing to override. You can read about overrides here. Removing the decorator should remove the error. Hope this helps!

ok, I'll try it, thx!
		</comment>
	</comments>
</bug>