<bug id='463' author='lucylw' open_date='2017-11-04T00:11:01Z' closed_time='2018-03-23T16:21:47Z'>
	<summary>guard against using unspecified GPUs</summary>
	<description>
When transferring data onto GPU, it would be helpful if the library would guard against using unspecified GPUs based on the specified GPU in the config file, i.e.
with torch.cuda.device(cuda_device):
   # all cuda data transfers
otherwise, some data can wind up being transferred onto an unspecified device (default GPU), which could be used by another process at the same time.
I experienced some related issues, e.g. GPU0 was being used, I specified GPU5, some of the model was transferred onto GPU0, and there was an out of memory error.
Related, it would also be nice if the library could check whether the specified GPU is available before loading data rather than after.
	</description>
	<comments>
		<comment id='1' author='lucylw' date='2017-11-05T18:43:55Z'>
		Do you know where in particular the wrong GPU was being used?
		</comment>
		<comment id='2' author='lucylw' date='2017-11-06T19:40:57Z'>
		In pytorch, when torch.nn.Module.cuda(cuda_device_number) is called, some context gets generated on the default CUDA device (first visible device, usually GPU0), in addition to the device specified by cuda_device_number.
It would be nice if allennlp could restrict the context creation to whatever GPU is specified in the config_file (using either a with construct or by setting the CUDA_VISIBLE_DEVICES environmental variable).
		</comment>
		<comment id='3' author='lucylw' date='2017-11-06T19:57:39Z'>
		Right, I'm wondering if you know where in AllenNLP code we're doing the wrong thing.  We can search for it, but if you've already pinpointed some spots, that'd be helpful.
		</comment>
		<comment id='4' author='lucylw' date='2017-11-06T21:10:29Z'>
		&lt;denchmark-link:https://github.com/matt-gardner&gt;@matt-gardner&lt;/denchmark-link&gt;
 Ah, right. The only place I've noticed it so far is :
if cuda_device &gt;= 0:
    model = model.cuda(cuda_device)
I would expect this to only put context on the specified cuda_device. When cuda_device does not equal 0, then at the end of this, there will be some memory used on the GPU specified by cuda_device and some memory used on GPU0. You could put a with torch.cuda.device(cuda_device) around this, or maybe set the cuda_device earlier on, i.e. torch.cuda.set_device(devID)?
		</comment>
		<comment id='5' author='lucylw' date='2017-11-06T22:08:25Z'>
		Ok, I'm guessing there is some spot where we're doing something like tensor.cuda() without specifying a device, and we need to dig through the code to figure out where it is.  If you can give us a reliable failure case that we can reproduce, that'd be helpful in tracking this down.
		</comment>
		<comment id='6' author='lucylw' date='2017-11-28T19:54:17Z'>
		Apparently, many things in pytorch can lead to context initialization on an unspecified GPU. It happens when moving the model onto GPU, e.g.
import torch
import torch.nn as nn

features = 50
hidden_size = 50
num_layers = 2
cuda_device = 1    # on a machine with multiple GPUs (&gt;0)

model = nn.Module()
model.rnn = nn.RNN(input_size=features, hidden_size=hidden_size, num_layers=num_layers)
model.cuda(cuda_device)
After model.cuda(cuda_device), there will be memory used on the specified GPU (in this case, cuda_device=1), as well as the default GPU (which is usually device 0).
seq_len = 10
batch_size = 32

X_train = torch.randn(seq_len, batch_size, features)
X_train = Variable(X_train).cuda(cuda_device)
The same problem doesn't seem to occur when moving tensors onto GPU (as above).
We could try to make sure this doesn't happen by adding a with construct around the model.cuda line in AllenNLP, i.e.:
with torch.cuda.device(cuda_device):
    model.cuda(cuda_device)
Or more robustly, using torch.cuda.set_device(cuda_device) when creating the model to set the device id before moving anything onto GPU.
		</comment>
	</comments>
</bug>