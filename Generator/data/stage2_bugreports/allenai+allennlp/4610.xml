<bug id='4610' author='uahmad235' open_date='2020-08-27T12:57:25Z' closed_time='2020-08-28T16:45:17Z'>
	<summary>Demo results for Sentiment Analysis model does not yields same results as locally downloaded model</summary>
	<description>
&lt;denchmark-h:h2&gt;Checklist&lt;/denchmark-h&gt;


 I have verified that the issue exists against the master branch of AllenNLP.
 I have read the relevant section in the contribution guide on reporting bugs.
 I have checked the issues list for similar or identical bug reports.
 I have checked the pull requests list for existing proposed fixes.
 I have checked the CHANGELOG and the commit log to find out if the bug was already fixed in the master branch.
 I have included in the "Description" section below a traceback from any exceptions related to this bug.
 I have included in the "Related issues or possible duplicates" section beloew all related issues and possible duplicate issues (If there are none, check this box anyway).
 I have included in the "Environment" section below the name of the operating system and Python version that I was using when I discovered this bug.
 I have included in the "Environment" section below the output of pip freeze.
 I have included in the "Steps to reproduce" section below a minimally reproducible example.

&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

After using the demo for sentiment Analysis (GloVe-LSTM) on the URL :&lt;denchmark-link:https://demo.allennlp.org/sentiment-analysis/MjI2NDEzOQ==&gt;https://demo.allennlp.org/sentiment-analysis/MjI2NDEzOQ==&lt;/denchmark-link&gt;
 I tried to replicate the results on the local system by following the instructions on the same URL. After downloading model on the local machine, i tried to calculate the sentiment of some generic text and its very strange that the results are different on the demo than those on my local machine.

Python traceback:




&lt;denchmark-h:h2&gt;Related issues or possible duplicates&lt;/denchmark-h&gt;


None

&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

OS: Linux
Python version: 3.6.9

Output of pip freeze:

allennlp==1.0.0
allennlp-models==1.0.0
argon2-cffi==20.1.0
attrs==20.1.0
backcall==0.2.0
bleach==3.1.5
blis==0.4.1
boto3==1.14.48
botocore==1.17.48
catalogue==1.0.0
certifi==2020.6.20
cffi==1.14.2
chardet==3.0.4
click==7.1.2
conllu==3.0
cymem==2.0.3
dataclasses==0.7
decorator==4.4.2
defusedxml==0.6.0
docutils==0.15.2
en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz
entrypoints==0.3
filelock==3.0.12
future==0.18.2
h5py==2.10.0
idna==2.10
importlib-metadata==1.7.0
iniconfig==1.0.1
ipykernel==5.3.4
ipython==7.16.1
ipython-genutils==0.2.0
ipywidgets==7.5.1
jedi==0.17.2
Jinja2==2.11.2
jmespath==0.10.0
joblib==0.16.0
jsonnet==0.16.0
jsonpickle==1.4.1
jsonschema==3.2.0
jupyter-client==6.1.7
jupyter-core==4.6.3
MarkupSafe==1.1.1
mistune==0.8.4
more-itertools==8.4.0
murmurhash==1.0.2
nbconvert==5.6.1
nbformat==5.0.7
nltk==3.5
notebook==6.1.3
numpy==1.19.1
overrides==3.0.0
packaging==20.4
pandas==1.1.1
pandocfilters==1.4.2
parso==0.7.1
pexpect==4.8.0
pickleshare==0.7.5
plac==1.1.3
pluggy==0.13.1
preshed==3.0.2
prometheus-client==0.8.0
prompt-toolkit==3.0.6
protobuf==3.13.0
ptyprocess==0.6.0
py==1.9.0
py-rouge==1.1
pycparser==2.20
Pygments==2.6.1
pyparsing==2.4.7
pyrsistent==0.16.0
pytest==6.0.1
python-dateutil==2.8.1
pytz==2020.1
pyzmq==19.0.2
regex==2020.7.14
requests==2.24.0
s3transfer==0.3.3
sacremoses==0.0.43
scikit-learn==0.23.2
scipy==1.5.2
Send2Trash==1.5.0
sentencepiece==0.1.91
six==1.15.0
spacy==2.2.4
srsly==1.0.2
tensorboardX==2.1
terminado==0.8.3
testpath==0.4.4
thinc==7.4.0
threadpoolctl==2.1.0
tokenizers==0.7.0
toml==0.10.1
torch==1.5.1
tornado==6.0.4
tqdm==4.48.2
traitlets==4.3.3
transformers==2.11.0
urllib3==1.25.10
wasabi==0.8.0
wcwidth==0.2.5
webencodings==0.5.1
widgetsnbextension==3.5.1
word2number==1.1
zipp==3.1.0




&lt;denchmark-h:h2&gt;Steps to reproduce&lt;/denchmark-h&gt;


Example source: 

from allennlp.predictors.predictor import Predictor
import allennlp_models.classification
predictor = Predictor.from_path("https://storage.googleapis.com/allennlp-public-models/basic_stanford_sentiment_treebank-2020.06.09.tar.gz")
predictor.predict(
  sentence="The movie was overall good but not very excellent."
)

output:
{'logits': [0.28388914465904236, -0.3324323296546936],
 'probs': [0.6493814587593079, 0.35061854124069214],
 'token_ids': [24, 20, 106, 965, 45, 22, 28, 72, 473, 7],
 'label': '1',
 'tokens': ['The',
  'movie',
  'was',
  'overall',
  'good',
  'but',
  'not',
  'very',
  'excellent',
  '.']}

Note: If we input the same text into the Demo, we get the following output:
The model is quite sure the sentence is Negative. (98.2%)


	</description>
	<comments>
		<comment id='1' author='uahmad235' date='2020-08-27T16:12:10Z'>
		I downloaded the model and got the same result that you see locally, so I can confirm this issue.  I even checked out the same version of the code that the demo is running, and it gives the same result.  I don't have any good ideas about what could be the cause, though; my best guess is that something is up with our permalink storage.
		</comment>
		<comment id='2' author='uahmad235' date='2020-08-28T09:59:35Z'>
		&lt;denchmark-link:https://github.com/matt-gardner&gt;@matt-gardner&lt;/denchmark-link&gt;
 So how long until it is fixed, as the best results I can get is the one on Demo.
		</comment>
		<comment id='3' author='uahmad235' date='2020-08-28T16:45:17Z'>
		I'm not sure what the difference is between the model you used and the model in the demo, but I found the model that's actually backing the demo right now: "https://s3-us-west-2.amazonaws.com/allennlp/models/sst-2-basic-classifier-glove-2019.06.27.tar.gz".  It gives the same output on your example query as the demo does.  It's possible that the one you linked to is using ELMo, which would easily explain the difference.  The one with the google storage link actually appears to have better performance, as the sentence you're querying has positive sentiment, while the demo is very confident that it is negative.
You can see the difference between these two models by untarring the model files and looking at their configs.
		</comment>
	</comments>
</bug>