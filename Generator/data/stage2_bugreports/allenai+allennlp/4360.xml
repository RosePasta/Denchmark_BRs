<bug id='4360' author='koren-v' open_date='2020-06-15T14:10:01Z' closed_time='2020-11-02T22:21:23Z'>
	<summary>Can't get correct output using .saliency_interpret_from_json() from Interpret</summary>
	<description>
Hi, I'm trying to get the importance of each word in a sentence while sentiment classification but always get the same result for different inputs. The Code:
&lt;denchmark-code&gt;from allennlp.predictors.predictor import Predictor
import allennlp_models.classification
from allennlp.interpret.saliency_interpreters import (
    SaliencyInterpreter,
    SimpleGradient,
    SmoothGradient,
    IntegratedGradient,
)

predictor = Predictor.from_path("https://storage.googleapis.com/allennlp-public-models/sst-roberta-large-2020.06.08.tar.gz")
simple_grad = SimpleGradient(predictor)

input_json = {'sentence' : "a very well-made, funny and entertaining picture."}

simple_grad.saliency_interpret_from_json(input_json)
&lt;/denchmark-code&gt;

gives:
&lt;denchmark-code&gt;{'instance_1': {'grad_input_1': [1.0,
   0.0,
   0.0,
   0.0,
   0.0,
   0.0,
   0.0,
   0.0,
   0.0,
   0.0,
   0.0]}}
&lt;/denchmark-code&gt;

So what can be the reason of this problem?
	</description>
	<comments>
		<comment id='1' author='koren-v' date='2020-06-15T16:00:04Z'>
		No idea what's causing the issue yet, but I have confirmed that this is a bug.  I get the correct output with rc3, but it's broken with rc6.
		</comment>
		<comment id='2' author='koren-v' date='2020-06-15T17:55:44Z'>
		&lt;denchmark-link:https://github.com/matt-gardner&gt;@matt-gardner&lt;/denchmark-link&gt;
 Thanks for your answer.
		</comment>
		<comment id='3' author='koren-v' date='2020-06-15T19:37:32Z'>
		&lt;denchmark-link:https://github.com/matt-gardner&gt;@matt-gardner&lt;/denchmark-link&gt;
 Is it possible way to load this model in rc3? Because using the same code with this version gives:
&lt;denchmark-code&gt;ConfigurationError: sst_tokens not in acceptable choices for validation_dataset_reader.type: ['conll2003', 'interleaving', 'sequence_tagging', 'sharded', 'babi', 'text_classification_json']. You should either use the --include-package flag to make sure the correct module is loaded, or use a fully qualified class name in your config file like {"model": "my_module.models.MyModel"} to have it imported automatically.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='koren-v' date='2020-06-15T19:59:58Z'>
		Yeah, just include this line in your script: from allennlp_models import sentiment.
		</comment>
		<comment id='5' author='koren-v' date='2020-06-30T21:54:05Z'>
		Possibly related: &lt;denchmark-link:https://github.com/allenai/allennlp-models/pull/85&gt;allenai/allennlp-models#85&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='koren-v' date='2020-08-18T16:18:34Z'>
		&lt;denchmark-link:https://github.com/matt-gardner&gt;@matt-gardner&lt;/denchmark-link&gt;
 this is just a friendly ping to make sure you haven't forgotten about this issue 
		</comment>
		<comment id='7' author='koren-v' date='2020-08-18T19:42:51Z'>
		I thought the issue might have been resolved with recent changes to the SST model / tokenization stuff.  But I just ran the model on master (both this and the models repo), and had the same issue.  This is still not resolved.
We're giving a tutorial at EMNLP on interpreting predictions, and this should definitely be fixed before then.  I'm putting this into the 1.2 milestone, but only so we don't forget about it; it really should be in a 1.3 or a 2.1 milestone, or something, but those don't exist yet.
		</comment>
		<comment id='8' author='koren-v' date='2020-10-09T23:10:37Z'>
		Just because it hasn't been mentioned yet: The problem is that it takes the gradients at the top of the transformer, not the bottom, when using the mismatched embedder. Since the pooler takes only the first token ([CLS]), all the gradients are there.
		</comment>
		<comment id='9' author='koren-v' date='2020-10-15T21:40:37Z'>
		&lt;denchmark-link:https://github.com/matt-gardner&gt;@matt-gardner&lt;/denchmark-link&gt;
, do you have an idea of when you will get to this?
		</comment>
		<comment id='10' author='koren-v' date='2020-10-15T22:15:59Z'>
		I have some thoughts on how to fix this.  If I'm lucky, I can maybe get to it tomorrow; if not, I'm not sure, but I might be able to make it happen next week.
		</comment>
		<comment id='11' author='koren-v' date='2020-11-02T23:19:09Z'>
		I couldn't get the demo to work end-to-end, but I have confirmed that the example from in here works now as expected.
		</comment>
		<comment id='12' author='koren-v' date='2020-11-07T00:01:14Z'>
		&lt;denchmark-link:https://github.com/dirkgr&gt;@dirkgr&lt;/denchmark-link&gt;
 can you share which example works?  i am still having trouble getting this working.  i tried 1.0.0rc3 (as suggested way up above) and see this error:  
this still seems broken.
i tried 1.1.0, 1.2.0, and master and i see the incorrect array [1.0, 0.0, 0.0, ...].
here is my code:  &lt;denchmark-link:https://github.com/data-science-on-aws/workshop/blob/1958164/07_train/wip/99_AllenNLP_RoBERTa_Prediction.ipynb&gt;https://github.com/data-science-on-aws/workshop/blob/1958164/07_train/wip/99_AllenNLP_RoBERTa_Prediction.ipynb&lt;/denchmark-link&gt;

any help would be appreciated!  hoping to get a working demo of this soon.
should we re-open this?
		</comment>
		<comment id='13' author='koren-v' date='2020-11-07T01:08:07Z'>
		You have to install  from both allennlp and allennlp-models. Then copy-and-paste the code from the description above. When I do this, I see reasonable numbers. I also made a test for this here: &lt;denchmark-link:https://github.com/allenai/allennlp-models/pull/163&gt;allenai/allennlp-models#163&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>