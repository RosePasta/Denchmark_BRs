<bug id='4317' author='ud2195' open_date='2020-06-03T17:45:27Z' closed_time='2020-06-03T21:13:29Z'>
	<summary>facing  AssertionError when running my textual entailment training using roBERTa</summary>
	<description>
Hi , I am facing assertion error which didnt used to happen earlier while starting my training using roBERTa for textual entailment.
Please find the full traceback below:-
&lt;denchmark-code&gt;File "/usr/local/bin/allennlp", line 8, in &lt;module&gt;
    sys.exit(run())
  File "/usr/local/lib/python3.6/dist-packages/allennlp/__main__.py", line 19, in run
    main(prog="allennlp")
  File "/usr/local/lib/python3.6/dist-packages/allennlp/commands/__init__.py", line 92, in main
    args.func(args)
  File "/usr/local/lib/python3.6/dist-packages/allennlp/commands/train.py", line 112, in train_model_from_args
    dry_run=args.dry_run,
  File "/usr/local/lib/python3.6/dist-packages/allennlp/commands/train.py", line 171, in train_model_from_file
    dry_run=dry_run,
  File "/usr/local/lib/python3.6/dist-packages/allennlp/commands/train.py", line 230, in train_model
    dry_run=dry_run,
  File "/usr/local/lib/python3.6/dist-packages/allennlp/commands/train.py", line 418, in _train_worker
    params=params, serialization_dir=serialization_dir, local_rank=process_rank,
  File "/usr/local/lib/python3.6/dist-packages/allennlp/common/from_params.py", line 580, in from_params
    **extras,
  File "/usr/local/lib/python3.6/dist-packages/allennlp/common/from_params.py", line 609, in from_params
    kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)
  File "/usr/local/lib/python3.6/dist-packages/allennlp/common/from_params.py", line 181, in create_kwargs
    cls.__name__, param_name, annotation, param.default, params, **extras
  File "/usr/local/lib/python3.6/dist-packages/allennlp/common/from_params.py", line 287, in pop_and_construct_arg
    return construct_arg(class_name, name, popped_params, annotation, default, **extras)
  File "/usr/local/lib/python3.6/dist-packages/allennlp/common/from_params.py", line 321, in construct_arg
    return annotation.from_params(params=popped_params, **subextras)
  File "/usr/local/lib/python3.6/dist-packages/allennlp/common/from_params.py", line 580, in from_params
    **extras,
  File "/usr/local/lib/python3.6/dist-packages/allennlp/common/from_params.py", line 611, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File "/usr/local/lib/python3.6/dist-packages/allennlp_models/pair_classification/dataset_readers/snli.py", line 51, in __init__
    assert not self._tokenizer._add_special_tokens
AssertionError
&lt;/denchmark-code&gt;

my config file:-
&lt;denchmark-code&gt;local transformer_model = "roberta-large";
local transformer_dim = 1024;
local cls_is_last_token = false;

{
  "dataset_reader":{
    "type": "snli",
    "tokenizer": {
      "type": "pretrained_transformer",
      "model_name": transformer_model,
      
    },
    "token_indexers": {
      "tokens": {
        "type": "pretrained_transformer",
        "model_name": transformer_model,
        "max_length": 512
      }
    }
  },
  "train_data_path": "/content/cnli_train_5L.jsonl",
  "validation_data_path": "/content/cnli_val_5L.jsonl",
  
  "model": {
    "type": "basic_classifier",
    "text_field_embedder": {
      "token_embedders": {
        "tokens": {
          "type": "pretrained_transformer",
          "model_name": transformer_model,
          "max_length": 512
        }
      }
    },
    "seq2vec_encoder": {
       "type": "cls_pooler",
       "embedding_dim": transformer_dim,
       "cls_is_last_token": cls_is_last_token
    },
    "feedforward": {
      "input_dim": transformer_dim,
      "num_layers": 1,
      "hidden_dims": transformer_dim,
      "activations": "tanh"
    },
    "dropout": 0.1,
    "namespace": "tags"
  },
  "data_loader": {
    "batch_sampler": {
      "type": "bucket",
      "batch_size" : 16
    }
  },
  "trainer": {
    "num_epochs": 10,
    "cuda_device" : -1,
    "validation_metric": "+accuracy",
    "learning_rate_scheduler": {
      "type": "slanted_triangular",
      "cut_frac": 0.06
    },
    "optimizer": {
      "type": "huggingface_adamw",
      "lr": 2e-5,
      "weight_decay": 0.1,
    }
  }
}
&lt;/denchmark-code&gt;

this is the same config.jsonnet given for roBERTa for training on mnli dataset except the fact that i removed
"add_special_tokens": False from tokenizer field in jsonnet. as it gave me a static error
surprising thing is i didnt get the error mentioned above few hours back while initiating training.
steps to reproduce:-
&lt;denchmark-code&gt;1) !pip install --pre allennlp-models
2) Training data was already there in jsonl
3) !allennlp train /content/mnliconfig.jsonnet -s /content/allenmodel

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='ud2195' date='2020-06-03T18:10:30Z'>
		same code is working when i use:-
allennlp-models==1.0.0rc3
		</comment>
		<comment id='2' author='ud2195' date='2020-06-03T21:13:29Z'>
		That error is because you need to set the flag that you removed ("add_special_tokens": False) in order for the MNLI data and model code to work properly.  If that flag is not set, you would get messed up special tokens in your model.  You didn't need that flag with 1.0.0rc3, but you do with rc5, due to some changes made in between those two releases.
		</comment>
		<comment id='3' author='ud2195' date='2020-06-04T06:14:50Z'>
		&lt;denchmark-link:https://github.com/matt-gardner&gt;@matt-gardner&lt;/denchmark-link&gt;
 Hi matt, thanks for your prompt response but i am still getting  when i include the flag.
		</comment>
		<comment id='4' author='ud2195' date='2020-06-04T06:46:45Z'>
		&lt;denchmark-link:https://github.com/matt-gardner&gt;@matt-gardner&lt;/denchmark-link&gt;
  Matt, the flag "add_special_tokens": should be set to 'false' instead of 'False'.Hence, was throwing the error. i.e.
 instead of  mentioned in the github repository here:-
&lt;denchmark-link:url&gt;https://github.com/allenai/allennlp-models/blob/master/training_config/pair_classification/snli_roberta.jsonnet&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='ud2195' date='2020-06-04T16:35:41Z'>
		Thanks for catching that!  PR opened to fix it: &lt;denchmark-link:https://github.com/allenai/allennlp-models/pull/70&gt;allenai/allennlp-models#70&lt;/denchmark-link&gt;
.
		</comment>
	</comments>
</bug>