<bug id='292' author='OyvindTafjord' open_date='2017-09-11T22:05:47Z' closed_time='2017-09-14T18:59:25Z'>
	<summary>Inconsistent epoch numbering</summary>
	<description>
It's a bit confusing that we display epoch + 1 in the logger during training, while epoch is the number saved in the model_state_epoch_xx.th files. More problematic is that when restoring from a checkpoint, the epoch gets set to the number of the loaded file, which then is promptly overwritten at the end of the first epoch of training, which seems like a bug.
	</description>
	<comments>
	</comments>
</bug>