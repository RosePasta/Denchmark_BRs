<bug id='4496' author='mateuszpieniak' open_date='2020-07-20T21:08:13Z' closed_time='2020-08-04T10:24:38Z'>
	<summary>Padding error for two ListFields in Instance object</summary>
	<description>
&lt;denchmark-h:h2&gt;Checklist&lt;/denchmark-h&gt;


 I have verified that the issue exists against the master branch of AllenNLP.
 I have read the relevant section in the contribution guide on reporting bugs.
 I have checked the issues list for similar or identical bug reports.
 I have checked the pull requests list for existing proposed fixes.
 I have checked the CHANGELOG and the commit log to find out if the bug was already fixed in the master branch.
 I have included in the "Description" section below a traceback from any exceptions related to this bug.
 I have included in the "Related issues or possible duplicates" section below all related issues and possible duplicate issues (If there are none, check this box anyway).
 I have included in the "Environment" section below the name of the operating system and Python version that I was using when I discovered this bug.
 I have included in the "Environment" section below the output of pip freeze.
 I have included in the "Steps to reproduce" section below a minimally reproducible example.

&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

In my custom DatasetReader I read pairs of text, where each text is processed with ListField. It results in the exception for batch_size &gt; 1. It works for batch_size == 1. It suggests that the issue is with the padding logic.

Python traceback:

  0%|          | 0/100 [00:00&lt;?, ?it/s]
2020-07-20 22:48:57,813 - CRITICAL - root - Uncaught exception
Traceback (most recent call last):
  File "/home/pi3ni0/.venv/dev/lib/python3.6/site-packages/allennlp/__main__.py", line 38, in &lt;module&gt;
    run()
  File "/home/pi3ni0/.venv/dev/lib/python3.6/site-packages/allennlp/__main__.py", line 34, in run
    main(prog="allennlp")
  File "/home/pi3ni0/.venv/dev/lib/python3.6/site-packages/allennlp/commands/__init__.py", line 92, in main
    args.func(args)
  File "/home/pi3ni0/.venv/dev/lib/python3.6/site-packages/allennlp/commands/train.py", line 105, in train_model_from_args
    dry_run=args.dry_run,
  File "/home/pi3ni0/.venv/dev/lib/python3.6/site-packages/allennlp/commands/train.py", line 159, in train_model_from_file
    dry_run=dry_run,
  File "/home/pi3ni0/.venv/dev/lib/python3.6/site-packages/allennlp/commands/train.py", line 213, in train_model
    dry_run=dry_run,
  File "/home/pi3ni0/.venv/dev/lib/python3.6/site-packages/allennlp/commands/train.py", line 407, in _train_worker
    metrics = train_loop.run()
  File "/home/pi3ni0/.venv/dev/lib/python3.6/site-packages/allennlp/commands/train.py", line 469, in run
    return self.trainer.train()
  File "/home/pi3ni0/.venv/dev/lib/python3.6/site-packages/allennlp/training/trainer.py", line 848, in train
    train_metrics = self._train_epoch(epoch)
  File "/home/pi3ni0/.venv/dev/lib/python3.6/site-packages/allennlp/training/trainer.py", line 554, in _train_epoch
    for batch_group in batch_group_generator_tqdm:
  File "/home/pi3ni0/.venv/dev/lib/python3.6/site-packages/tqdm/std.py", line 1129, in __iter__
    for obj in iterable:
  File "/home/pi3ni0/.venv/dev/lib/python3.6/site-packages/allennlp/common/util.py", line 135, in lazy_groups_of
    s = list(islice(iterator, group_size))
  File "/home/pi3ni0/.venv/dev/lib/python3.6/site-packages/allennlp/data/dataloader.py", line 119, in __iter__
    yield next(self._data_generator)
  File "/home/pi3ni0/.venv/dev/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 345, in __next__
    data = self._next_data()
  File "/home/pi3ni0/.venv/dev/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 385, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/pi3ni0/.venv/dev/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py", line 35, in fetch
    return self.collate_fn(data)
  File "/home/pi3ni0/.venv/dev/lib/python3.6/site-packages/allennlp/data/dataloader.py", line 18, in allennlp_collate
    return batch.as_tensor_dict(batch.get_padding_lengths())
  File "/home/pi3ni0/.venv/dev/lib/python3.6/site-packages/allennlp/data/batch.py", line 141, in as_tensor_dict
    for field, tensors in instance.as_tensor_dict(lengths_to_use).items():
  File "/home/pi3ni0/.venv/dev/lib/python3.6/site-packages/allennlp/data/instance.py", line 101, in as_tensor_dict
    tensors[field_name] = field.as_tensor(padding_lengths[field_name])
  File "/home/pi3ni0/.venv/dev/lib/python3.6/site-packages/allennlp/data/fields/list_field.py", line 99, in as_tensor
    return self.field_list[0].batch_tensors(padded_fields)
  File "/home/pi3ni0/.venv/dev/lib/python3.6/site-packages/allennlp/data/fields/text_field.py", line 132, in batch_tensors
    for indexer_name, indexer_outputs in indexer_lists.items()
  File "/home/pi3ni0/.venv/dev/lib/python3.6/site-packages/allennlp/data/fields/text_field.py", line 132, in &lt;dictcomp&gt;
    for indexer_name, indexer_outputs in indexer_lists.items()
  File "/home/pi3ni0/.venv/dev/lib/python3.6/site-packages/allennlp/nn/util.py", line 99, in batch_tensor_dicts
    batched_tensor = torch.stack(tensor_list)
RuntimeError: Expected object of scalar type bool but got scalar type long int for sequence element 16.
reading instances: 3it [00:00,  8.25it/s]

Process finished with exit code 1




&lt;denchmark-h:h2&gt;Related issues or possible duplicates&lt;/denchmark-h&gt;

Not a duplicate, but a bit related (address only a single ListField in the returned Instance object)

#2839

&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

OS: Ubuntu 18.04.4 LTS
Python version: 3.6.9

Output of pip freeze:

absl-py==0.9.0
alabaster==0.7.12
alembic==1.4.2
allennlp @ git+https://github.com/allenai/allennlp.git@478bf46cb676524ee9b74fb271ec0a592d1c4a48
allennlp-models==1.0.0
-e git+https://github.com/allenai/allennlp-server@4901cd9c93b77949d1877e18c6b019615004a6b5#egg=allennlp_server
altgraph==0.17
appdirs==1.4.3
attrs==19.3.0
awscli==1.18.34
Babel==2.8.0
backcall==0.1.0
beautifulsoup4==4.8.2
black==19.10b0
bleach==3.1.4
blis==0.4.1
boto3==1.14.17
botocore==1.17.19
bravado==10.6.0
bravado-core==5.17.0
bs4==0.0.1
cachetools==4.0.0
catalogue==1.0.0
certifi==2020.6.20
chardet==3.0.4
click==7.1.2
cliff==3.3.0
cloudpickle==1.3.0
cmaes==0.5.1
cmd2==1.1.0
colorama==0.4.3
colorlog==4.1.0
ConfigArgParse==1.2.3
configparser==5.0.0
conllu==3.0
cycler==0.10.0
cymem==2.0.3
dask==2.14.0
databricks-cli==0.10.0
dataclasses==0.7
decorator==4.4.2
defusedxml==0.6.0
dill==0.3.1.1
docker==4.2.0
docutils==0.15.2
editdistance==0.5.3
en-core-sci-sm @ https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_sm-0.2.4.tar.gz
entrypoints==0.3
face-alignment==1.0.0
fasttext==0.9.1
filelock==3.0.12
flaky==3.6.1
Flask==1.1.1
Flask-Cors==3.0.8
ftfy==5.7
future==0.18.2
gensim==3.8.1
gevent==1.4.0
gitdb==4.0.4
GitPython==3.1.1
google-api-core==1.16.0
google-auth==1.12.0
google-auth-oauthlib==0.4.1
google-cloud-core==1.3.0
google-cloud-storage==1.26.0
google-resumable-media==0.5.0
googleapis-common-protos==1.51.0
gorilla==0.3.0
greenlet==0.4.15
grpcio==1.28.1
gunicorn==20.0.4
h5py==2.10.0
hydra-core==0.11.3
icalendar==4.0.4
idna==2.10
imageio==2.8.0
imagesize==1.2.0
importlib-metadata==1.7.0
ipykernel==5.2.0
ipython==7.13.0
ipython-genutils==0.2.0
ipywidgets==7.5.1
itsdangerous==1.1.0
jedi==0.16.0
Jinja2==2.11.1
jmespath==0.10.0
joblib==0.16.0
json-lines==0.5.0
jsonlines==1.2.0
jsonnet==0.16.0
jsonpickle==1.4.1
jsonpointer==2.0
jsonref==0.2
jsonschema==3.2.0
jupyter==1.0.0
jupyter-client==6.1.2
jupyter-console==6.1.0
jupyter-core==4.6.3
kiwisolver==1.1.0
Mako==1.1.2
Markdown==3.2.1
MarkupSafe==1.1.1
matplotlib==3.2.1
mistune==0.8.4
mlflow==1.8.0
monotonic==1.5
more-itertools==8.4.0
msgpack==1.0.0
msgpack-python==0.5.6
murmurhash==1.0.2
nbconvert==5.6.1
nbformat==5.0.4
neptune-client==0.4.113
networkx==2.4
nltk==3.5
nmslib==2.0.5
notebook==6.0.3
numpy==1.19.0
numpydoc==0.9.2
oauthlib==3.1.0
omegaconf==1.4.1
opencv-python==4.2.0.34
overrides==3.1.0
packaging==20.4
pandas==1.0.1
pandocfilters==1.4.2
parsimonious==0.8.1
parso==0.6.2
pathspec==0.8.0
pbr==5.4.5
pexpect==4.8.0
pickleshare==0.7.5
Pillow==7.0.0
pkg-resources==0.0.0
plac==1.1.3
plotly==4.5.4
pluggy==0.13.1
preshed==3.0.2
prettytable==0.7.2
prometheus-client==0.7.1
prometheus-flask-exporter==0.13.0
prompt-toolkit==3.0.5
protobuf==3.12.2
psutil==5.7.0
ptyprocess==0.6.0
py==1.9.0
py-rouge==1.1
py3nvml==0.2.6
pyasn1==0.4.8
pyasn1-modules==0.2.8
pybind11==2.5.0
pycparser==2.20
pyfakewebcam==0.1.0
pygit==0.1
Pygments==2.6.1
PyInstaller==3.6
PyJWT==1.7.1
pyparsing==2.4.7
pyperclip==1.8.0
pyrsistent==0.16.0
pysbd==0.2.3
pyspellchecker==0.5.4
pytest==5.4.3
python-dateutil==2.8.1
python-editor==1.0.4
python-Levenshtein==0.12.0
pytorch-lightning==0.7.5
pytorch-pretrained-bert==0.6.2
pytorch-transformers==1.1.0
pytz==2019.3
PyWavelets==1.1.1
PyYAML==5.3.1
pyzmq==19.0.0
qtconsole==4.7.2
QtPy==1.9.0
querystring-parser==1.2.4
regex==2020.6.8
requests==2.24.0
requests-oauthlib==1.3.0
responses==0.10.12
retrying==1.3.3
rfc3987==1.3.8
rsa==3.4.2
s3transfer==0.3.3
sacremoses==0.0.43
SciencePlots==1.0.3
scikit-image==0.16.2
scikit-learn==0.23.1
scipy==1.5.1
seaborn==0.10.0
semantic-version==2.8.5
Send2Trash==1.5.0
sentencepiece==0.1.91
simplejson==3.17.0
six==1.15.0
sklearn==0.0
smart-open==1.10.0
smmap==3.0.2
snowballstemmer==2.0.0
soupsieve==2.0
spacy==2.2.4
Sphinx==2.4.4
sphinxcontrib-applehelp==1.0.2
sphinxcontrib-devhelp==1.0.2
sphinxcontrib-htmlhelp==1.0.3
sphinxcontrib-jsmath==1.0.1
sphinxcontrib-qthelp==1.0.3
sphinxcontrib-serializinghtml==1.1.4
SQLAlchemy==1.3.13
sqlparse==0.3.1
srsly==1.0.2
stevedore==2.0.1
strict-rfc3339==0.7
swagger-spec-validator==2.5.0
tabulate==0.8.7
tensorboard==2.2.1
tensorboard-plugin-wit==1.6.0.post3
tensorboardX==2.1
terminado==0.8.3
test-tube==0.7.5
testpath==0.4.4
thinc==7.4.0
threadpoolctl==2.1.0
tokenizers==0.8.1rc1
toml==0.10.0
toolz==0.10.0
torch==1.5.1
torchtext==0.6.0
torchvision==0.6.0
tornado==6.0.4
tqdm==4.47.0
traitlets==4.3.3
transformers==3.0.2
typed-ast==1.4.1
typing-extensions==3.7.4.2
Unidecode==1.1.1
urllib3==1.25.9
wasabi==0.7.0
wcwidth==0.2.5
webcolors==1.11.1
webencodings==0.5.1
websocket-client==0.57.0
Werkzeug==1.0.0
widgetsnbextension==3.5.1
word2number==1.1
wordsegment==1.3.1
xmltodict==0.12.0
zipp==3.1.0



&lt;denchmark-h:h2&gt;Steps to reproduce&lt;/denchmark-h&gt;


Example source:

Reader
import json
from typing import Dict, Iterable, Optional, List

from allennlp.data.dataset_readers.dataset_reader import DatasetReader
from allennlp.data.fields import Field, TextField, LabelField, ListField
from allennlp.data.instance import Instance
from allennlp.data.token_indexers import TokenIndexer
from allennlp.data.tokenizers import Tokenizer
from allennlp.data.tokenizers.sentence_splitter import SpacySentenceSplitter
from overrides import overrides


@DatasetReader.register("custom_reader")
class CustomReader(DatasetReader):
    def __init__(
        self,
        tokenizer: Tokenizer,
        token_indexers: Dict[str, TokenIndexer],
        segment_sentences: bool,
        **kwargs,
    ) -&gt; None:
        super().__init__(**kwargs)
        self._tokenizer = tokenizer
        self._token_indexers = token_indexers
        self._segment_sentences = segment_sentences
        if self._segment_sentences:
            self._sentence_segmenter = SpacySentenceSplitter(language="en_core_sci_sm")

    @overrides
    def _read(self, file_path: str) -&gt; Iterable[Instance]:
        with open(file_path, "r") as file:
            for line in file:
                example = json.loads(line)

                yield self.text_to_instance(
                    text1=example["text1"],
                    text2=example["text2"],
                    label=example.get("label"),
                )

    def __get_text_fields(self, text: str, prefix_name: str) -&gt; Dict[str, Field]:
        fields: Dict[str, Field] = {}

        if not self._segment_sentences:
            tokens = self._tokenizer.tokenize(text)
            fields[f"{prefix_name}_tokens"] = TextField(tokens, self._token_indexers)
            return fields

        sentences: List[Field] = []
        sentence_splits = self._sentence_segmenter.split_sentences(text)
        for sentence in sentence_splits:
            tokens = self._tokenizer.tokenize(sentence)
            sentences.append(TextField(tokens, self._token_indexers))

        fields[f"{prefix_name}_tokens"] = ListField(sentences)
        return fields

    @overrides
    def text_to_instance(
        self, text1: str, text2: str, label: Optional[int] = None
    ) -&gt; Instance:
        text1_fields = self.__get_text_fields(text1 prefix_name="text1")
        text2_fields = self.__get_text_fields(text2, prefix_name="text2")
        fields = {**text1_fields, **text2_fields}

        if label is not None:
            fields["labels"] = LabelField(label, skip_indexing=True)

        return Instance(fields)

Related Config
  dataset_reader: {
    type: 'custom_reader',
    segment_sentences: true,
    tokenizer: {
      type: 'pretrained_transformer',
      model_name: 'allenai/scibert_scivocab_cased',
    },
    token_indexers: {
      tokens: {
        type: 'pretrained_transformer',
        model_name: 'allenai/scibert_scivocab_cased',
        max_length: 512,
      },
    },

    // DatasetReader's fields
    max_instances: 10,
    lazy: true,
  },
  data_loader: {
    batch_size: 2
  },



	</description>
	<comments>
		<comment id='1' author='mateuszpieniak' date='2020-07-21T15:45:51Z'>
		Thanks for the detail! I'll take a look at this today
		</comment>
		<comment id='2' author='mateuszpieniak' date='2020-07-23T21:22:29Z'>
		&lt;denchmark-link:https://github.com/mateuszpieniak&gt;@mateuszpieniak&lt;/denchmark-link&gt;
 I suspect the issue comes from the logic here: &lt;denchmark-link:https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/pretrained_transformer_indexer.py#L207-L216&gt;https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/pretrained_transformer_indexer.py#L207-L216&lt;/denchmark-link&gt;

But I haven't been able to reproduce the error. Could provide example inputs / instances that lead to the error?
		</comment>
		<comment id='3' author='mateuszpieniak' date='2020-08-04T10:24:37Z'>
		&lt;denchmark-link:https://github.com/epwalsh&gt;@epwalsh&lt;/denchmark-link&gt;
 Sorry for the trouble. I don't really know what happened, but I cannot reproduce the error I had as well 
		</comment>
	</comments>
</bug>