<bug id='4506' author='erncnerky' open_date='2020-07-24T10:06:21Z' closed_time='2020-07-31T16:06:55Z'>
	<summary>Coreference resolution out of memory error</summary>
	<description>
Hi,
I am using coreference resolution model (&lt;denchmark-link:https://demo.allennlp.org/coreference-resolution&gt;https://demo.allennlp.org/coreference-resolution&lt;/denchmark-link&gt;
)
class CoreferenceResolution():

    def __init__(self):
        self.predictor = Predictor.from_path("https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz", cuda_device=0)

    def predict(self, text):
        prediction = self.predictor.predict(document=text)
        return prediction
After a certain period of time, I get CUDA out of memory error.
&lt;denchmark-code&gt;ERROR:api:Exception on /api/v1/coreference-resolution [POST]
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/flask/app.py", line 2446, in wsgi_app
    response = self.full_dispatch_request()
  File "/usr/local/lib/python3.6/dist-packages/flask/app.py", line 1951, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/usr/local/lib/python3.6/dist-packages/flask/app.py", line 1820, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File "/usr/local/lib/python3.6/dist-packages/flask/_compat.py", line 39, in reraise
    raise value
  File "/usr/local/lib/python3.6/dist-packages/flask/app.py", line 1949, in full_dispatch_request
    rv = self.dispatch_request()
  File "/usr/local/lib/python3.6/dist-packages/flask/app.py", line 1935, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File "/app/api.py", line 43, in coreference_resolution_f
    coreference_prediction = coreference_resolution.predict(text)
  File "/app/src/coreference.py", line 12, in predict
    prediction = self.predictor.predict(document=text)
  File "/usr/local/lib/python3.6/dist-packages/allennlp_models/coref/predictors/coref.py", line 65, in predict
    return self.predict_json({"document": document})
  File "/usr/local/lib/python3.6/dist-packages/allennlp/predictors/predictor.py", line 48, in predict_json
    return self.predict_instance(instance)
  File "/usr/local/lib/python3.6/dist-packages/allennlp/predictors/predictor.py", line 171, in predict_instance
    outputs = self._model.forward_on_instance(instance)
  File "/usr/local/lib/python3.6/dist-packages/allennlp/models/model.py", line 146, in forward_on_instance
    return self.forward_on_instances([instance])[0]
  File "/usr/local/lib/python3.6/dist-packages/allennlp/models/model.py", line 172, in forward_on_instances
    outputs = self.make_output_human_readable(self(**model_input))
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/allennlp_models/coref/models/coref.py", line 206, in forward
    attended_span_embeddings = self._attentive_span_extractor(text_embeddings, spans)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/allennlp/modules/span_extractors/self_attentive_span_extractor.py", line 73, in forward
    attended_text_embeddings = util.weighted_sum(span_embeddings, span_attention_weights)
  File "/usr/local/lib/python3.6/dist-packages/allennlp/nn/util.py", line 692, in weighted_sum
    intermediate = attention.unsqueeze(-1).expand_as(matrix) * matrix
RuntimeError: CUDA out of memory. Tried to allocate 1.69 GiB (GPU 0; 11.17 GiB total capacity; 3.18 GiB already allocated; 1.14 GiB free; 3.19 GiB reserved in total by PyTorch)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='erncnerky' date='2020-07-24T15:42:21Z'>
		Based on this:
GPU 0; 11.17 GiB total capacity; 3.18 GiB already allocated; 1.14 GiB free; 3.19 GiB reserved in total by PyTorch
It seems like there is another process running on your GPU that is taking up a lot of memory
		</comment>
		<comment id='2' author='erncnerky' date='2020-07-24T15:54:55Z'>
		
Based on this:
GPU 0; 11.17 GiB total capacity; 3.18 GiB already allocated; 1.14 GiB free; 3.19 GiB reserved in total by PyTorch
It seems like there is another process running on your GPU that is taking up a lot of memory

I am aware of the other process. The problem is that gpu memory consumption is gradually increasing as I call prediction function.
		</comment>
		<comment id='3' author='erncnerky' date='2020-07-24T15:56:32Z'>
		That's pretty typical for PyTorch. At some point it's memory will level-out, provided the model doesn't get any huge inputs.
		</comment>
		<comment id='4' author='erncnerky' date='2020-07-24T16:18:37Z'>
		If you can repro this with 16GB of GPU memory and no other processes, then I think we have a real problem we should investigate. Like this, it's not enough evidence to investigate. 12GB is not that much to start with for this model.
		</comment>
		<comment id='5' author='erncnerky' date='2020-07-31T16:06:55Z'>
		Closing this due to lack of activity.  Feel free to follow up if there's still an issue.
		</comment>
	</comments>
</bug>