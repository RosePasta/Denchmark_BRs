<bug id='4584' author='calusbr' open_date='2020-08-20T20:54:10Z' closed_time='2020-09-30T16:24:37Z'>
	<summary>ELMo RuntimeError: CUDA out of memory.</summary>
	<description>
Hello I have a problem when running a Corpus NER with size 25 mb.
See my .jsonnet attachment
&lt;denchmark-code&gt;{
  "dataset_reader": {
    "type": "conll2003",
    "tag_label": "ner",
    "coding_scheme": "BIOUL",
    "token_indexers": {
      "tokens": {
        "type": "single_id",
        "lowercase_tokens": true
      },
      "token_characters": {
        "type": "characters",
        "min_padding_length": 3
      },
      "elmo": {
        "type": "elmo_characters"
     }
    }
  },
  "train_data_path": "/home/xxx/datasets/ner/wikiner/train.txt",
  "validation_data_path": "/home/xxx/datasets/ner/wikiner/dev.txt",
  "test_data_path": "/home/xxx/datasets/ner/wikiner/test.txt",
  "model": {
    "type": "crf_tagger",
    "label_encoding": "BIOUL",
    "calculate_span_f1": true,
    "dropout": 0.5,
    "include_start_end_transitions": false,
    "text_field_embedder": {
      "token_embedders": {
        "tokens": {
            "type": "embedding",
            "embedding_dim": 300,
            "pretrained_file": "/home/xxx/model/glove/glove_s300.zip",
            "trainable": true
        },
        "elmo":{
          "type": "elmo_token_embedder",
          "options_file": "/home/lucasrodrigues/model/elmo/options.json",
          "weight_file": "/home/lucasrodrigues/model/elmo/elmo_pt_weights_dgx1.hdf5",
          "do_layer_norm": false,
          "dropout": 0.0
        },
        "token_characters": {
            "type": "character_encoding",
            "embedding": {
            "embedding_dim": 16
            },
            "encoder": {
            "type": "cnn",
            "embedding_dim": 16,
            "num_filters": 128,
            "ngram_filter_sizes": [3],
            "conv_layer_activation": "relu"
            }
        }
      }
    },
    "encoder": {
      "type": "lstm",
      "input_size": 1452,
      "hidden_size": 200,
      "num_layers": 2,
      "dropout": 0.5,
      "bidirectional": true
    },
    "verbose_metrics": true,
    "regularizer": [
      [
        "scalar_parameters",
        {
          "type": "l2",
          "alpha": 0.1
        }
      ]
    ]
  },
  "iterator": {
    "type": "basic",
    "batch_size":2
  },
  "trainer": {
    "optimizer": {
        "type": "adam",
        "lr": 0.001
    },
    "validation_metric": "+f1-measure-overall",
    "num_serialized_models_to_keep": 3,
    "num_epochs": 10,
    "grad_norm": 5.0,
    "patience": 25,
    "cuda_device":[0] 
  },
}
&lt;/denchmark-code&gt;

Log Error:
&lt;denchmark-code&gt;File "/home/xxx/.conda/envs/allennlp09/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 212, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA out of memory. Tried to allocate 12.25 GiB (GPU 0; 10.92 GiB total capacity; 1.96 GiB already allocated; 1.92 GiB free; 8.40 GiB reserved in total by PyTorch)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='calusbr' date='2020-08-21T16:14:07Z'>
		Hey &lt;denchmark-link:https://github.com/calusbr&gt;@calusbr&lt;/denchmark-link&gt;
, could you provide the whole stack trace? You could also run the train command with  which might help us debug it. I'm guessing you have a very, very long sequence in one of your inputs.
		</comment>
		<comment id='2' author='calusbr' date='2020-09-04T16:21:25Z'>
		This issue is being closed due to lack of activity. If you think it still needs to be addressed, please comment on this thread ðŸ‘‡
		</comment>
		<comment id='3' author='calusbr' date='2020-09-19T20:58:53Z'>
		(I'm going to reopen this issue, because I'm also weirdly seeing it):
We're training some SQuAD models with ELMo (just the vanilla version from AllenNLP). To debug, we're training on a subsample of SQuAD 1.1 (1K examples).
We're getting the following error:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/usr/local/bin/allennlp", line 8, in &lt;module&gt;
    sys.exit(run())
  File "/usr/local/lib/python3.6/dist-packages/allennlp/run.py", line 18, in run
    main(prog="allennlp")
  File "/usr/local/lib/python3.6/dist-packages/allennlp/commands/__init__.py", line 117, in main
    args.func(args)
  File "/usr/local/lib/python3.6/dist-packages/allennlp/commands/train.py", line 163, in train_model_from_args
    args.include_package,
  File "/usr/local/lib/python3.6/dist-packages/allennlp/commands/train.py", line 220, in train_model_from_file
    include_package,
  File "/usr/local/lib/python3.6/dist-packages/allennlp/commands/train.py", line 283, in train_model
    include_package=include_package,
  File "/usr/local/lib/python3.6/dist-packages/allennlp/commands/train.py", line 491, in _train_worker
    metrics = trainer.train()
  File "/usr/local/lib/python3.6/dist-packages/allennlp/training/trainer.py", line 549, in train
    train_metrics = self._train_epoch(epoch)
  File "/usr/local/lib/python3.6/dist-packages/allennlp/training/trainer.py", line 369, in _train_epoch
    loss = self.batch_loss(batch, for_training=True)
  File "/usr/local/lib/python3.6/dist-packages/allennlp/training/trainer.py", line 300, in batch_loss
    output_dict = self._pytorch_model(**batch)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "./bidaf_optimized/models/bidaf_optimized.py", line 196, in forward
    embedded_passage = self._highway_layer(self._text_field_embedder(passage))
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/allennlp/modules/text_field_embedders/basic_text_field_embedder.py", line 139, in forward
    token_vectors = embedder(*tensors, **forward_params_values)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/allennlp/modules/token_embedders/elmo_token_embedder.py", line 98, in forward
    elmo_output = self._elmo(inputs, word_inputs)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/allennlp/modules/elmo.py", line 184, in forward
    bilm_output = self._elmo_lstm(reshaped_inputs, reshaped_word_inputs)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/allennlp/modules/elmo.py", line 622, in forward
    token_embedding = self._token_embedder(inputs)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/allennlp/modules/elmo.py", line 389, in forward
    convolved = conv(character_embedding)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py", line 200, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA out of memory. Tried to allocate 5.45 GiB (GPU 0; 11.78 GiB total capacity; 1.06 GiB already allocated; 4.06 GiB free; 5.53 GiB cached)
&lt;/denchmark-code&gt;

What's weird, though, is that this happens on the second epoch of training. I'd expect such a large memory allocation to happen on the first epoch / to hit the OOM on the first epoch, if the issue was a paticularly long sequence.
Here's the stdout: &lt;denchmark-link:https://gist.github.com/nelson-liu/f5be8bcdecafccc670097687c1339775&gt;https://gist.github.com/nelson-liu/f5be8bcdecafccc670097687c1339775&lt;/denchmark-link&gt;

here's the stderr: &lt;denchmark-link:https://gist.github.com/nelson-liu/a4a7c138ca84dd3bae515cf3c6b8116b&gt;https://gist.github.com/nelson-liu/a4a7c138ca84dd3bae515cf3c6b8116b&lt;/denchmark-link&gt;

Any idea what might be going on here?
		</comment>
		<comment id='4' author='calusbr' date='2020-09-19T21:26:25Z'>
		Also: it looks like this happens only on Titan V. If we rerun on another GPU, things seem ok...
		</comment>
		<comment id='5' author='calusbr' date='2020-09-21T22:36:43Z'>
		
Also: it looks like this happens only on Titan V. If we rerun on another GPU, things seem ok...

I've also seen this on Tesla V100. Reducing batch size is only workaround I know of.
		</comment>
		<comment id='6' author='calusbr' date='2020-09-30T16:24:35Z'>
		This issue is being closed due to lack of activity. If you think it still needs to be addressed, please comment on this thread ðŸ‘‡
		</comment>
	</comments>
</bug>