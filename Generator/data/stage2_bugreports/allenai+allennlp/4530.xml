<bug id='4530' author='entslscheia' open_date='2020-08-04T02:55:11Z' closed_time='2020-08-04T18:01:50Z'>
	<summary>Is this a bug of using BERT in version 0.9.0?</summary>
	<description>
First of all, I know the usage of BERT has been changed drastically since 0.9.0. But since I am still working on my old project, I am still sticking to 0.9.0 (updating to 1.0 will make many of my codes totally unusable...).
In version 0.9.0, it seems like padding is not handled at all when using PretrainedTransformerEmbedder:
class PretrainedTransformerEmbedder(TokenEmbedder):
    """
    Uses a pretrained model from ``pytorch-transformers`` as a ``TokenEmbedder``.
    """
    def __init__(self, model_name: str) -&gt; None:
        super().__init__()
        self.transformer_model = AutoModel.from_pretrained(model_name)
        # I'm not sure if this works for all models; open an issue on github if you find a case
        # where it doesn't work.
        self.output_dim = self.transformer_model.config.hidden_size

    @overrides
    def get_output_dim(self):
        return self.output_dim

    def forward(self, token_ids: torch.LongTensor) -&gt; torch.Tensor:  # type: ignore
        # pylint: disable=arguments-differ
        return self.transformer_model(token_ids)[0]
You can see in forward method, there is only one argument token_ids.
Also, in forward of self.transformer_model, which I use BERT here, padding is not handled either:
    def forward(self, input_ids, token_type_ids=None, attention_mask=None, position_ids=None, head_mask=None):
        if attention_mask is None:
            attention_mask = torch.ones_like(input_ids)
        if token_type_ids is None:
            token_type_ids = torch.zeros_like(input_ids)
        ...
You can see that attention_mask is required as an explicit input instead of generating from input_ids, and since PretrainedTransformerEmbedder does not accept an input of attention_mask at all, it indicates that paddings input to BERT are not ignored. I didn't notice this issue till this afternoon, which is kinda surprising to me. I expected this kind of stuff to be handled by AllenNLP and made transparent to users, but now it seems I probably need to implement my own PretrainedTransformerEmbedder to take care of this.
	</description>
	<comments>
		<comment id='1' author='entslscheia' date='2020-08-04T18:01:50Z'>
		The PretrainedTransformerEmbedder wasn't fully implemented until version 1.0.  In 0.9, we only really had the BertIndexer and embedder classes (which aren't compatible with current transformers).  Your best bet is probably to copy our current Pretrained classes into your code.
I'm closing this issue, though, as it's not a problem that we can fix, seeing as how it's about old code.
		</comment>
	</comments>
</bug>