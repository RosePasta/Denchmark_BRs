<bug id='4558' author='tanmayag78' open_date='2020-08-14T01:34:00Z' closed_time='2020-08-24T17:44:05Z'>
	<summary>Bulk predict for coreference with constant memory usage</summary>
	<description>

So I want to use coreference resolution on more than 2000 stories. But every time I run the predict method the memory increases. Any way to use this with constant memory usage. I am using CPU for now. I have tried clearing the cache and running garbage collector. Please have a lot at the pseudo-code.
And also is there a way to reduce the prediction for large data.
I am also thinking of adding NER and other NLP functionality like relation extraction. Is there a way to create a pipeline in Allennlp.

&lt;denchmark-code&gt;def _get_coref():
    if hasattr(_get_coref, "COREF"):
        return _get_coref.COREF

    from allennlp.predictors.predictor import Predictor
    import allennlp_models.coref

    _get_coref.COREF = Predictor.from_path(
        os.path.join(
            ROOT_DIR,
            'coref-spanbert-large-2020.02.27.tar.gz'
        )
    )
    return _get_coref.COREF

predictor = _get_coref()
for story in stories:
    story = entity_info['whole']
    clusters = predictor.predict(document=text).get('clusters')

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='tanmayag78' date='2020-08-14T09:17:24Z'>
		That's an interesting way of getting a "static" variable in a function. I might steal this in the future.
There is no way to create a "pipeline" in AllenNLP. By construction, our models for the various tasks don't work that way, though you could probably write your own model that does all the tasks you need, and pre-train the components separately.
I have checked your code for obvious problems with the memory consumption, but nothing comes to mind. I will look into this more seriously.
		</comment>
		<comment id='2' author='tanmayag78' date='2020-08-18T13:42:05Z'>
		I simplified your code down to this:
from allennlp.predictors.predictor import Predictor
predictor = Predictor.from_path("https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz")
s = "The woman reading a newspaper sat on the bench with her dog. The weather was nice, even though it had rained the night before. The roads were still wet, but they dried with every passing minute. Suddenly, the dog jumped, and took the woman by surprise. What had startled him? A cat, peacfully licking its paws, had appeared in the window of a passing car. Why was the cat in the car? We will never know because this example is over."
while True:
     predictor.predict(document=s)
My memory usage is constant when I do this.
Can you try running this, and report back? If you see a problem, can you paste the output of pip freeze?
		</comment>
		<comment id='3' author='tanmayag78' date='2020-09-28T07:43:35Z'>
		I am also facing the same issue. The problem appears when using different stories, not the same story. Memory usage is increasing when applying predictor.predict(story) in a while loop.
		</comment>
		<comment id='4' author='tanmayag78' date='2020-09-28T21:31:10Z'>
		from allennlp.predictors.predictor import Predictor
predictor = Predictor.from_path("https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz")
s = "The woman reading a newspaper sat on the bench with her dog. The weather was nice, even though it had rained the night before. The roads were still wet, but they dried with every passing minute. Suddenly, the dog jumped, and took the woman by surprise. What had startled him? A cat, peacfully licking its paws, had appeared in the window of a passing car. Why was the cat in the car? We will never know because this example is over."
words = list(set(s.split(" ")))
import random
def story():
    return " ".join(random.choices(words, k=100))
while True:
     predictor.predict(document=story())
This sends a different story each time. I am still not seeing any memory problems. Can you try running this and tell me what you see?
		</comment>
	</comments>
</bug>