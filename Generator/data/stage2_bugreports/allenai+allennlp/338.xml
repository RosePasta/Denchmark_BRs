<bug id='338' author='OyvindTafjord' open_date='2017-09-20T07:21:34Z' closed_time='2017-09-21T00:45:23Z'>
	<summary>Vocabulary token roundtrip bug</summary>
	<description>
On a new dataset, I ran into the tokens '\x0b' (vertical tab) and, weirdly, '\x0b ' (with a space behind) which don't roundtrip well when read back from tokens.txt (each get read from file as two separate tokens, messing up the indexing).
	</description>
	<comments>
		<comment id='1' author='OyvindTafjord' date='2017-09-20T22:56:29Z'>
		I think we could handle this similarly to how we handle newlines: 


allennlp/allennlp/data/vocabulary.py


         Line 261
      in
      c6bc7fd






 token = line[:-1].replace('@@NEWLINE@@', '\n') 





We'd probably want to generalize this into some sanitize and desanitize methods, with a list of characters to replace and things to replace them with.  Seems pretty easy.
		</comment>
		<comment id='2' author='OyvindTafjord' date='2017-09-20T23:24:02Z'>
		FWIW, here's the list of character codes (&lt;= 65535) which don't roundtrip properly with the current code: 11, 12, 28, 29, 30, 133, 8232, 8233 (in addition to the 10 already "handled"). Kinda seems like an unexpected "feature" of .readlines() (or upstream .splitlines()) that this happens?
&lt;denchmark-code&gt;&gt;&gt;&gt; "foo\nbar\x0bxx\x0cyy".splitlines()
['foo', 'bar', 'xx', 'yy']
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='OyvindTafjord' date='2017-09-20T23:26:51Z'>
		Simpler fix might be to just read the whole file and do .split('\n') only.
		</comment>
		<comment id='4' author='OyvindTafjord' date='2017-09-20T23:44:46Z'>
		How are these tokens with whitespace passing through the tokenizer anyway?  Most tokenizers will remove them.
		</comment>
		<comment id='5' author='OyvindTafjord' date='2017-09-20T23:52:29Z'>
		Yes, spacy seems to have some interesting interpretations of these (I'm particularly intrigued by the '\x0b ' one with a space in it!):
&lt;denchmark-code&gt;&gt;&gt;&gt; import spacy
&gt;&gt;&gt; spacy_model = spacy.load("en")
&gt;&gt;&gt; [x.text for x in spacy_model("foo bar aa\x0bbb cc\x0b dd")]
['foo', 'bar', 'aa', '\x0b', 'bb', 'cc', '\x0b ', 'dd']
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='OyvindTafjord' date='2017-09-21T00:00:44Z'>
		Ah yes -- this is a hidden "feature" of spacy.  In order to reconstruct the character offsets (I think) it will include tokens that are all spaces.  There is an attribute on the token object is_space I usually use to eliminate them.
For example:
&lt;denchmark-code&gt;tokens = [token.text for token in spacy_model(text) if not token.is_space]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='OyvindTafjord' date='2017-09-21T00:08:09Z'>
		Interesting, I see "  " (two spaces) tokenizes as " " (one space) etc, not that you can reliably reconstruct from just these, as, say, "foo/bar" tokenizes to ["foo","/","bar"] (no negative spaces...), so the utility is a bit unclear to me. We should probably add that .is_space filter though.
		</comment>
		<comment id='8' author='OyvindTafjord' date='2017-09-21T00:45:23Z'>
		Resolved in PR &lt;denchmark-link:https://github.com/allenai/allennlp/pull/344&gt;#344&lt;/denchmark-link&gt;
.
		</comment>
	</comments>
</bug>