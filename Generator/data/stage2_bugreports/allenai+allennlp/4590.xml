<bug id='4590' author='wj-Mcat' open_date='2020-08-23T08:01:54Z' closed_time='2020-09-11T16:21:37Z'>
	<summary>how to set segment_id in pretrained_transformer model</summary>
	<description>
I train my sequence tagging model, here is my configuration file.
{
  "dataset_reader": {
    "type": "ccks-sequence_tagging",
    "token_indexers": {
      "tokens": {
        "type": "pretrained_transformer_mismatched",
        "model_name": "bert-base-chinese"
      }
    }
  },
  "train_data_path": "./data/train_base_phrase_clear.txt",
  "data_loader": {
    "batch_size": 4,
    "shuffle": true
  },
  "model": {
    "type": "crf_tagger",
    "text_field_embedder": {
      "token_embedders": {
        "tokens": {
          "type": "pretrained_transformer_mismatched",
          "model_name": "bert-base-chinese"
        }
      }
    },
    "encoder": {
      "type": "pass_through",
      "input_dim":768
    },
    "calculate_span_f1": true,
    "label_encoding": "BIO"
  },
  "trainer": {
    "optimizer": "adam",
    "patience": 10,
    "cuda_device": 2,
    "num_epochs": 2
  }
}
And the data in train_base_phrase_clear.txt file is like: word###label word###label [SEP]###O sentence-question-word###O sentence-question-word###O.
My main problem is that the result from token_embedders doesn't contains type_ids info, which is all 0 value. I have set [SEP] word label in my trainning file, So I think there should be different type_ids.
So, I finnaly check that there is no type_id info from SequenceTaggingDatasetReader class. Then I custom this reader and add init PretrainedTransformerTokenizer instance to this reader, and add type_ids to Instance. B-U-T, when the data flow from dataset_reader to crf_tagger module, there is no type_id info.
This make me confused. where does type_id info go away ? And how to tokenize sentence with segment_id information in allennlp ?
This one is urgent. Waiting for your reply! Thanks for your later help.
	</description>
	<comments>
		<comment id='1' author='wj-Mcat' date='2020-08-23T08:24:57Z'>
		I check the huggingface transformer model, there is build_inputs_with_special_tokens method to get special type_id.  How to use it in Allennlp ?
		</comment>
		<comment id='2' author='wj-Mcat' date='2020-08-23T08:32:04Z'>
		I finnaly use the intra_word_tokenize_sentence_pair method in PretrainedTransformerTokenizer instance  to complete this function.
I will close this issue later.
		</comment>
		<comment id='3' author='wj-Mcat' date='2020-08-23T11:44:57Z'>
		I find that type_id info doesn't exist in tokens which is the input of crf_tagger.forward method.
&lt;denchmark-link:https://user-images.githubusercontent.com/10242208/90977607-39b22080-e579-11ea-95c2-f8c3645b5eaf.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='wj-Mcat' date='2020-08-24T17:09:15Z'>
		&lt;denchmark-link:https://github.com/dirkgr&gt;@dirkgr&lt;/denchmark-link&gt;
 can correct me if I'm wrong, but I believe the right thing to do is to call  on your .  You'll have to manually split on the  token.  If you just call , we call huggingface's , which does not detect  tokens to set type ids.  We used to have code that detected  tokens and incremented type ids, but that got lost somewhere in the upgrade from our  to what we have now.
		</comment>
		<comment id='5' author='wj-Mcat' date='2020-08-24T17:25:00Z'>
		If you don't have pre-tokenized text, then calling  like &lt;denchmark-link:https://github.com/matt-gardner&gt;@matt-gardner&lt;/denchmark-link&gt;
 said is correct. But that will not do the right thing when your text is already tokenized.
In fact, when you use the , we ignore the type ids that the reader might have set on the tokens, so there is no way to get the type ids to the right place. I think we should fix &lt;denchmark-link:https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/pretrained_transformer_mismatched_indexer.py#L74&gt;this line&lt;/denchmark-link&gt;
 so it uses the type id from the token instead of from the word pieces. Then your reader can set type ids, and they will be respected by the mismatched indexer. &lt;denchmark-link:https://github.com/wj-Mcat&gt;@wj-Mcat&lt;/denchmark-link&gt;
, do you think that will work?
		</comment>
		<comment id='6' author='wj-Mcat' date='2020-08-25T11:40:02Z'>
		I have looked at this a little bit closer, and found it's not that easy. The mismatched indexer also adds special tokens, but it has no way of knowing anything about sequence pairs. So even if we create a way to pass token type ids through to the mismatched indexer, we can't guarantee that it'll create the correct special tokens. &lt;denchmark-link:https://github.com/wj-Mcat&gt;@wj-Mcat&lt;/denchmark-link&gt;
 is "solving" the problem by setting the middle special token himself, but relying on the mismatched indexer to place the special token at the front and back of the sequence. That's very brittle, and only happens to work for a few models, not in general.
I don't think you can use the mismatched indexer for this. It's designed to make old-school datasets work with transformers. If you already have transformer-specific input data, it won't be very nice to use.
As a workaround, I recommend you use the regular pretrained transformer indexer and embedder, and  keep track of the offsets yourself, as part of the instances that the reader produces. You can probably steal a lot of the code from mismatched indexer/embedder, and you can use intra_word_tokenize of course.
		</comment>
		<comment id='7' author='wj-Mcat' date='2020-08-26T08:56:41Z'>
		&lt;denchmark-link:https://github.com/dirkgr&gt;@dirkgr&lt;/denchmark-link&gt;
  I need to tokenize the sentence at character level, but  will tokenize the  data to ['2019', '.', '08', '23'] data. And sometimes, it will ads  prefix to the token text. These features will make me hard to set character-level tag.  So, I tokenize the text before training.
		</comment>
		<comment id='8' author='wj-Mcat' date='2020-08-26T09:36:32Z'>
		To use the pretrained transformer, you have to tokenize exactly the way it was tokenized when BERT was trained. Otherwise the pre-trained BERT weights don't work. That is what PretrainedTransformerTokenizer does, and that is what PretrainedTransformerIndexer expects. This is why the date gets split up like that, and that's why you see the "##" prefix.
I understand that your tags use a different tokenization. You said "character-level", but I think you mean "word-level"? Either way, I think you need to tokenize the input text with PretrainedTransformerTokenizer, maybe with intra_word_tokenize, and keep track of the mapping between BERT word pieces and words yourself. That way you can set token IDs, add special tokens, and do other stuff as you need to. We have some functions that help with that.
It is strange to see that your input data on the one hand uses a tokenization that doesn't match with BERT tokenization, but on the other hand it uses [SEP] tokens that are specific to BERT. It does not make a lot of sense to have data this way. This is why the "mismatched" group of classes doesn't help you.
		</comment>
		<comment id='9' author='wj-Mcat' date='2020-08-30T08:20:45Z'>
		When I review the transformers source code, I found that it support segment feature is supported.
    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)
    def encode_plus(
        self,
        text: Union[TextInput, PreTokenizedInput, EncodedInput],
        text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] = None,
        add_special_tokens: bool = True,
        padding: Union[bool, str] = False,
        truncation: Union[bool, str] = False,
        max_length: Optional[int] = None,
        stride: int = 0,
        is_pretokenized: bool = False,
        pad_to_multiple_of: Optional[int] = None,
        return_tensors: Optional[Union[str, TensorType]] = None,
        return_token_type_ids: Optional[bool] = None,
        return_attention_mask: Optional[bool] = None,
        return_overflowing_tokens: bool = False,
        return_special_tokens_mask: bool = False,
        return_offsets_mapping: bool = False,
        return_length: bool = False,
        verbose: bool = True,
        **kwargs
    ) -&gt; BatchEncoding:
I think text_pair is the mainly supported feature for segment_id, which is the token_type_id in allennlp.
But you haven't make the sentence pair tokenized by pair mode.
Then, I should add text-pair feature to tokenized method in PretrainedTransformerTokenizer class.
    @overrides
    def tokenize(self, text: str) -&gt; List[Token]:
        """
        This method only handles a single sentence (or sequence) of text.
        """
        text_pair = None
        if '[SEP]' in text:
            sep_index = text.index('[SEP]')
            text_pair = text[sep_index+5:]
            text = text[:sep_index]

        encoded_tokens = self.tokenizer.encode_plus(
            text=text,
            text_pair=text_pair,
            add_special_tokens=False,
            max_length=self._max_length,
            stride=self._stride,
            truncation=self._truncation_strategy if self._max_length is not None else False,
            return_tensors=None,
            return_offsets_mapping=self.tokenizer.is_fast,
            return_attention_mask=False,
            return_token_type_ids=True,
        )
Am I right ?
		</comment>
		<comment id='10' author='wj-Mcat' date='2020-09-02T16:43:52Z'>
		You could also do this, yes.
		</comment>
		<comment id='11' author='wj-Mcat' date='2020-09-11T16:21:36Z'>
		This issue is being closed due to lack of activity. If you think it still needs to be addressed, please comment on this thread ðŸ‘‡
		</comment>
	</comments>
</bug>