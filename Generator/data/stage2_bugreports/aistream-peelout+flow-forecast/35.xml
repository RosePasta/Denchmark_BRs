<bug id='35' author='isaacmg' open_date='2020-01-03T06:50:38Z' closed_time='2020-01-05T04:49:54Z'>
	<summary>Wandb bug when running train long</summary>
	<description>
When running this code &lt;denchmark-link:https://github.com/AIStream-Peelout/flow-forecast/commit/1f67ac4844859e5d60a0f5dba2dbbe8f4c5dbc30&gt;1f67ac4&lt;/denchmark-link&gt;
 from a colab notebook Wandb views the entire thing as one training session and continue gradient steps indefinitely. Training session should be forced to end when that model stops training not when the meta training loop finishes. Should only be 28 training steps not 80.
&lt;denchmark-link:https://user-images.githubusercontent.com/3865062/71710653-65567f80-2dcb-11ea-8558-0f3280c4ab7b.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='isaacmg' date='2020-01-05T01:52:52Z'>
		Currently working on this should be fixed. Just need to test it
		</comment>
		<comment id='2' author='isaacmg' date='2020-01-05T04:25:33Z'>
		So it seems to not be doing steps anymore on the same model run chart, but the runs are still not terminating until the loop ends. Don't really know if this is a problem or not.
		</comment>
		<comment id='3' author='isaacmg' date='2020-01-05T04:49:54Z'>
		Going to close this for now Wandb supports up to 50 concurrent runs. So as long as aren't training on more than 50 rivers at a time this shouldn't be an issue. If it becomes one will revert to the subprocess thing but don't want otherwise as with that it doesn't log debugging.
		</comment>
	</comments>
</bug>