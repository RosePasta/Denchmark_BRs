<bug id='821' author='Hazelgnash' open_date='2019-08-01T18:31:46Z' closed_time='2019-08-05T13:51:17Z'>
	<summary>Werid memory behavior/crash with AMD Vega multi-gpu "shared GPU memory" and Ping Pong</summary>
	<description>
Describe the bug
System has 2x AMD Vega 64 cards, each with 8GB VRAM and 16GB system ram. Crossfire is enabled and when training a model with Ping Pong then every time it switches sides more and more memory is stored in GPU2 under Shared GPU memory (for GPU1). GPU2 is not actually doing anything. When memory in GPU2 is full the trainer crashes
The first 100 iterations will run with,
1-100 iterations 7.5GB GPU1, 1.8GB GPU2, 13GB/16GB RAM, GPU1 compute 25-50% load and GPU1 copy 49% load.
101-200 iterations  7.5GB GPU1, 1.5-3.5GB GPU2, 11.6GB ram, compute 80%-90%'s, copy 30%
201-300 iterations 4GB GPU2, 15.5GB RAM,  96% compute, 0% copy usage
When switching at 301 iterations GPU2 memory increases to 6GB, 7.5GB once it maxes out to 8GB, training crashes with "[WinError 2] The system cannot find the file specified".
&lt;denchmark-link:https://i.gyazo.com/d17a58808c68a93ec5b31bd6ce4e238b.png&gt;https://i.gyazo.com/d17a58808c68a93ec5b31bd6ce4e238b.png&lt;/denchmark-link&gt;

Screenshot has task manager GPU VRAM usage during crash.
When reloading training continues again normally and usage is 7.5GB GPU1, 13GB RAM, 1.8GB GPU2, compute 25-50% and copy 49% load. This only happens with Ping Pong enabled, without Ping Pong shared GPU memory stays at 0.5GB-0.9GB and is does something... but it will run fine for 10,000+ iterations.
To Reproduce
Steps to reproduce the behavior:
1- 2x AMD Vega GPUs in crossfire
2. High VRAM trainer ( I think normal RAM has to be getting closed to maxed out)
3. Ping Pong and AMD enabled.
4. Let it run and when switching sides memory will randomly get dumped into GPU2 until VRAM usage in GPU2 reaches 100% and the trainer crashes.
Expected behavior
I know AMD multi-gpu is not supported but I had a spare Vega card and wanted to see if it would help at all. With normal settings there was no big difference, I think instead of running at max 7.9GB/8GB the card would only use 7.4GB and 0.5GB of shared GPU memory. I performance was slightly higher.
With Ping Pong I think it was unknowingly dumping stuff from RAM into GPU2 as a page file when switching sides. The amount would still decrease and increase during iterations and the weird part is that after switching sides 2 times performance was much higher than when just starting up again from a reload. Compute load was almost maxed out in the 95%'s and it wasn't doing any copying, compared to swinging between 25% to 50% compute and copy load sitting at 50% when restarting. This was great until VRAM on GPU2 maxed out and it crashed.
I don't know if this is related to HBM2 and Vega/Radeon VII cards having a HBCC but it seems possible. If it's possible to get it aware of shared GPU memory and to always use shared GPU memory over RAM that would probably boost performance a lot when using multiple AMD cards and big models.
Desktop (please complete the following information):
In crash report.
&lt;denchmark-link:https://github.com/deepfakes/faceswap/files/3458195/faceswap.log&gt;faceswap.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/deepfakes/faceswap/files/3458196/crash_report.2019.08.01.190600332073.log&gt;crash_report.2019.08.01.190600332073.log&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='Hazelgnash' date='2019-08-01T18:35:19Z'>
		&lt;denchmark-link:https://github.com/kilroythethird&gt;@kilroythethird&lt;/denchmark-link&gt;
 Will need to help look into this....
Bottom line is AMD support is secondary, I'm afraid, so I can't promise that we will be able to solve this. Ping Pong was built as a way to load an unload Tensorflow-GPU sessions from VRAM. As AMD does not use TF-GPU (rather it uses PlaidML as a backend), its release mechanism won't be the same.
I would welcome patches to address this. Unfortunately I run with an Nvidia card, so it isn't something I can patch. I will leave this as an open issue though.
In the meantime, sadly, your only option is "don't use ping-pong".
		</comment>
		<comment id='2' author='Hazelgnash' date='2019-08-01T18:50:24Z'>
		Yeah AMD support is rough I noticed, however everything still works fine and preferably I don't even want to use ping-pong. With this niche setup ping-pong I just accidentally tapped into a 3x performance boost with that specific setting because actual compute load is going from 30% to 95% when stuff is stored on GPU2 instead of RAM. If it's is doing it by accident then maybe it's relatively easy enough to implement it in default AMD settings, while also not having it crash.
It could also be a bug with PlaidML, but I really have no idea how any of this works, just noticed this weird behavior.
		</comment>
		<comment id='3' author='Hazelgnash' date='2019-08-05T13:51:17Z'>
		After discussions with our resident AMD guy, we have decided to just disable ping-pong for AMD users.
Unfortunately (as you noted) plaidML does not currently support multi GPU, so any performance boost (albeit briefly) is entirely accidental.
		</comment>
	</comments>
</bug>