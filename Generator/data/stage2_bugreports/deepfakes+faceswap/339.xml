<bug id='339' author='andenixa' open_date='2018-04-10T12:14:31Z' closed_time='2019-01-16T23:25:47Z'>
	<summary>GAN128 gets worse and develops moiré patter</summary>
	<description>
When training with GAN128 after a certain number of epochs (about 15hours of training on modern GPU)

result becomes worse: faces more disfigured than at the first stages of training (both discriminator and generator).
as GAN very quickly learns to swap eyes and mouth extremely accurately it  gradually losses the ability as more epochs pass instead of improving further.
develops horrible moire pattern at both  mask and the output (even raw) non existent at the data-sets
(in fact data starts to look 64x64 printed at 128x128 tiles)

That happens irrespective of training patterns which are taken from various source for both dataA and dataB. I tried tv recoding, movie flics, 3d cartoons, my own footage of me and friends. The behaviors is always the same.
Mask moire pattern
&lt;denchmark-link:https://user-images.githubusercontent.com/37909402/38556588-ddb87e74-3cd2-11e8-87a2-634319c2ff3e.png&gt;&lt;/denchmark-link&gt;

Disfigured with moire. Despite A and B are super similar, that's why I choose those to test if GAN128 works.
&lt;denchmark-link:https://user-images.githubusercontent.com/37909402/38556769-7e61d97e-3cd3-11e8-8942-04b0eaeebc7e.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='andenixa' date='2018-04-17T02:18:43Z'>
		I get the same behavior. The results are much cleaner if I drop the mask:
new_image = numpy.copy( image[:,:,:3] )
masked_fake_output = rgb
masked_fake_output = rgb
fake = fake_rgb
Sorry for the poor quality diff, the formatting gets bad if I paste it with the +/-.
		</comment>
		<comment id='2' author='andenixa' date='2018-04-18T00:56:30Z'>
		Missed a spot in my ugly hacking. Need to also delete this line to drop the mask during conversion:
new_face = mask * new_face + (1 - mask) * normalized_face
		</comment>
		<comment id='3' author='andenixa' date='2018-04-25T15:26:09Z'>
		Thanks a lot a I shall report with my findings
		</comment>
		<comment id='4' author='andenixa' date='2018-04-25T19:16:43Z'>
		When the size of the kernel and stride don't play well with each other it can lead to checkerboard-like convolution artifacts in the resulting image. &lt;denchmark-link:https://distill.pub/2016/deconv-checkerboard/&gt;This article&lt;/denchmark-link&gt;
 can give some insight into why this happens.
I am thinking that perhaps the line of code below is why we are seeing the moire patterns.

Line 66 of plugins/Model_GAN128/Model.py
		</comment>
		<comment id='5' author='andenixa' date='2018-04-29T23:11:56Z'>
		Thanks everyone for you feedback.
I realized while I was "tweaking" the Original model that GAN encoder doesn't have enough memory/neurons to hold the data (encoded_dim). GAN is also one Conv2d layer big to be trainable. My experiments shown that these kind of architecture can converge relatively well with 5 Conv2D layers which is 1 more than Original and one less than GAN. Also my unproven speculation that initial Conv2D layers should have a bigger windows which should span horizontal (something like (5,6) they are inverted). I haven't tested this yet, but I can see results are too generalized on horizontal features, but that yet to be proven.
PS: My current Original model tweak is times faster than GAN and gives a better result. It converges even with different versions of face such as 3D, photos, sketches, and drawings mixed together as target_B. My Original model tweak also plays nice with one-to-many approach. Only problem I found it gives too much preference to colors in favor of the shape (ie it would try to match, say, yellowish targets with photos taken in sunlight). If someone knows how to modify this part that would be very welcome. In general its OK, but the emphasys is a little to big.
PSS: I am aware that isn't a chat. The latest findings also that batch size should not be a power of 2 it has to be even (though not every model requires that neither) and it has to be smaller than number of samples (which can be overcame with a few lines of code). In fact decreasing-train-restoring back batch size can make net trainable again as it re-arranges its clusters positions.
		</comment>
		<comment id='6' author='andenixa' date='2018-05-14T17:00:02Z'>
		&lt;denchmark-link:https://github.com/andenixa&gt;@andenixa&lt;/denchmark-link&gt;
 Are you planning on doing a pull request of your OriginalHighRes model?
		</comment>
		<comment id='7' author='andenixa' date='2018-05-14T17:34:00Z'>
		He already has. You can test it by checking out the staging branch &lt;denchmark-link:https://github.com/deepfakes/faceswap/tree/staging&gt;https://github.com/deepfakes/faceswap/tree/staging&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='andenixa' date='2018-05-17T06:09:01Z'>
		Going to post a model update in a day or so with an improvement that should alleviate or solve this issue by implementing ICNR initialization of the convolution kernels.
See this paper by the same authors of Pixel Shuffle for an overview
&lt;denchmark-link:https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf&gt;https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='andenixa' date='2018-05-17T14:42:36Z'>
		Been playing with your model and works pretty well. I'll test your update
as soon as you release it.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, May 17, 2018, 1:09 AM kvrooman ***@***.***&gt; wrote:
 Going to post a model update in a day or so with an improvement that
 should alleviate or solve this issue by implementing ICNR initialization of
 the convolution kernels.

 See this paper by the same authors of Pixel Shuffle for an overview
 https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf

 —
 You are receiving this because you commented.
 Reply to this email directly, view it on GitHub
 &lt;#339 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAAGgmTBFadZl08-NiiH1WLJI4o0zwsjks5tzRQKgaJpZM4TOI7X&gt;
 .



		</comment>
		<comment id='10' author='andenixa' date='2018-05-18T23:38:33Z'>
		&lt;denchmark-link:https://github.com/kvrooman&gt;@kvrooman&lt;/denchmark-link&gt;
 any news? I'm burning for a test 'cause GAN works best with 2 of my facesets until moiré kicks in
		</comment>
		<comment id='11' author='andenixa' date='2018-05-20T13:27:37Z'>
		I noticed lr=1e-04 sometimes contributes to long lasting moire patterns (beats me why) (instead of 5e-05 in faceswap) and since GAN has additional mask layer it reinforces that pattern and sometimes embeds it as a "valid" outcome.
I can observe the same pattern with Adam lr=1e-4 at my mask-less Model though it tends to vanish with time.
		</comment>
		<comment id='12' author='andenixa' date='2018-06-07T11:44:33Z'>
		I tried OriginalHighRes and it takes a lot longer to train, but it's expected.
But it seems to develop a moire pattern too. I'm around 0.03x loss though, so I'll see if it improves when loss goes down.
		</comment>
		<comment id='13' author='andenixa' date='2018-06-07T21:34:19Z'>
		&lt;denchmark-link:https://github.com/Kirin-kun&gt;@Kirin-kun&lt;/denchmark-link&gt;
 I am going to make it slightly faster. I wouldn't watch for loss as its not really an objective measurement of completion. If you can see the predictor output (the rightmost image in the 1st and second triplet cols) to have eyes with clearly visible pupil (better if you see eyelashes), and facial expression follows the original during convert then the training is finished. As for moire I shall try to get rid of it.
Once I trained a net at 50k epochs I had all blurry silhouette of teeth and eyes with 0.04 loss, at 200k epochs I had crisp eyes and every tooth visible with clear tongue and.. well loss was still 0.04
		</comment>
		<comment id='14' author='andenixa' date='2018-06-25T23:15:21Z'>
		GAN v2.2 release
2018-06-25 Model update: faceswap-GAN v2.2 has been released. The main improvements of v2.2 model are its capability of generating realistic and consistent eyeballs (results are shown below, or Ctrl+F for eyeballs), as well as its higher quality of video results with face alignment.
&lt;denchmark-link:https://github.com/shaoanlu/faceswap-GAN&gt;Faceswap-GAN&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='andenixa' date='2018-06-26T11:18:47Z'>
		&lt;denchmark-link:https://github.com/tjess78&gt;@tjess78&lt;/denchmark-link&gt;
 in my experience any 128x model is able to generate realistic consistent eyeballs.
To achieve that all that is needed to:

raise Dense layer dimensions by about 50%
add additional Conv2d layer before to the last one.
train your Net at -bs 16 or higher for ~120k epochs

This &lt;denchmark-link:https://www.youtube.com/watch?v=j7KMep1PC2U&gt;video&lt;/denchmark-link&gt;
 was made by me using a Non-GAN autoencoder model using the instructions above. The video is SFW.
Prerequisites for data set -B is that it shall:

consist of crystal crisp frames with no blur or smudges
should not contain pics with partially obscured faces
have even lighting conditions, i.e. no blinds shadows overlapping the face
have approximately same number of frames for every angle with very similar shots removed to expedite the training

		</comment>
		<comment id='16' author='andenixa' date='2018-06-26T12:25:57Z'>
		Dear &lt;denchmark-link:https://github.com/andenixa&gt;@andenixa&lt;/denchmark-link&gt;
, your video looks incredible.
How did you achieve this nearly seamless face replacement?
No matter which model I use, the superimposed mask looks clearly above the background body.
Did you use any smoothing algorithm?
		</comment>
		<comment id='17' author='andenixa' date='2018-06-26T14:30:26Z'>
		&lt;denchmark-link:https://github.com/agilebean&gt;@agilebean&lt;/denchmark-link&gt;
 its regular seamless mode on the left and masked with slight smoothing at the right side.
Both modes use histogram matching. I use a slight mask erosion (about 10-15%). That's pretty much everything.
I suggest to not use any sharpening parameter to avoid the mask to be visible.
		</comment>
		<comment id='18' author='andenixa' date='2018-06-27T12:09:42Z'>
		&lt;denchmark-link:https://github.com/andenixa&gt;@andenixa&lt;/denchmark-link&gt;
 thanks a lot - forgive me to ask again, how did you

enable histogram matching
mask erosion
smoothing
with the faceswap.py command? I didn't see these options, the closest was perceptual loss...

		</comment>
		<comment id='19' author='andenixa' date='2018-06-27T12:32:05Z'>
		Those are options during the conversion, not the training. -mh activates histogram matching and with -e you can set the erosion kernel size.
		</comment>
		<comment id='20' author='andenixa' date='2018-07-05T12:29:09Z'>
		Hello &lt;denchmark-link:https://github.com/andenixa&gt;@andenixa&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/HelpSeeker&gt;@HelpSeeker&lt;/denchmark-link&gt;
,
thanks so much for your tips.
The histogram matching and seamless options made a big difference.
As for mask erosion with the -e option, how do you specify 15%?
or  or else?
		</comment>
		<comment id='21' author='andenixa' date='2018-07-06T14:15:58Z'>
		Hello &lt;denchmark-link:https://github.com/agilebean&gt;@agilebean&lt;/denchmark-link&gt;
,
I apologize for confusion there is no actual way at the moment to to supply parameters in percent.
As of now you could only supply erosion kernel in pixels. What I meant is an approximate amount obtained by visual comparison of the results. I make in approximately about 15% of the overall face dimensions (20% in some difficult cases) by eye comparison.
Perhaps its would be a good feature request though I am not sure if its fair because &lt;denchmark-link:https://github.com/torzdf&gt;@torzdf&lt;/denchmark-link&gt;
 is the one who does converter re-factoring. I am not sure if they have time for it.
		</comment>
		<comment id='22' author='andenixa' date='2018-07-07T14:43:30Z'>
		So the moire pattern problem is still not solved right?
Cause I've witnessed this after 60k iterations with 32 batch
&lt;denchmark-link:https://user-images.githubusercontent.com/36047765/42411891-7e037b0c-823f-11e8-8913-0fe0e3ba77f2.png&gt;&lt;/denchmark-link&gt;

Or am I using the wrong repository??
I used the GAN128 on the master branch
		</comment>
		<comment id='23' author='andenixa' date='2018-07-07T15:10:29Z'>
		&lt;denchmark-link:https://github.com/tjdwo13579&gt;@tjdwo13579&lt;/denchmark-link&gt;
 I don't want to overshadow Shaonlu's work in any way. But as far as I know at FaceSwap none ever got satisfactory results with GAN2.1 (64x nor 128x). I trained it for a week with no conclusive results. If you really want to see whats going on perhaps it would be wise to train it for 150-200k epochs since it has lots of layers.
		</comment>
		<comment id='24' author='andenixa' date='2018-07-07T15:13:47Z'>
		&lt;denchmark-link:https://github.com/andenixa&gt;@andenixa&lt;/denchmark-link&gt;
 Ah I see. Thanks for the reply.
I thought this was solved since there was a 3 month period.
So the result you showed on the Donald trump -&gt; Cage video is using Shaonlu's repository?
		</comment>
		<comment id='25' author='andenixa' date='2018-07-07T15:15:09Z'>
		&lt;denchmark-link:https://github.com/tjdwo13579&gt;@tjdwo13579&lt;/denchmark-link&gt;
 no, it was DeepFaker (a regular autoencoder, no GAN involved).
		</comment>
		<comment id='26' author='andenixa' date='2018-07-07T15:16:39Z'>
		&lt;denchmark-link:https://github.com/andenixa&gt;@andenixa&lt;/denchmark-link&gt;
  Oops sorry I didn't read your post thoroughly.
		</comment>
		<comment id='27' author='andenixa' date='2018-07-07T15:20:03Z'>
		&lt;denchmark-link:https://github.com/tjdwo13579&gt;@tjdwo13579&lt;/denchmark-link&gt;
 I think the moire pattern is due to initial initialization with random pattern which propagates all layers and mask "thinks" this pattern is actual a part of the deal since Shaonlu mixes mask with the results and there is no separate (as far as I know) loss function for it. You could make different weights initializations as a temporary fix. I could play with it actually, but not sure if PR would ever get accepted.
		</comment>
		<comment id='28' author='andenixa' date='2018-07-07T15:25:47Z'>
		&lt;denchmark-link:https://github.com/andenixa&gt;@andenixa&lt;/denchmark-link&gt;
 "due to initial initialization with random pattern which propagates all layers and mask "thinks" this pattern is actual a part of the deal" -&gt; That sounds quite right.
I'm not an expert on this field so there are limitations on what I can do now. I'm still learning.
Good luck with whatever you are doing now. I've noticed that you've made a lot of contributions to this repo. Thanks!
		</comment>
		<comment id='29' author='andenixa' date='2018-07-08T12:40:33Z'>
		&lt;denchmark-link:https://github.com/andenixa&gt;@andenixa&lt;/denchmark-link&gt;
 sorry could your clarify your answer:

Hello @agilebean,
I apologize for confusion there is no actual way at the moment to to supply parameters in percent.
As of now you could only supply erosion kernel in pixels. What I meant is an approximate amount &gt; &gt; obtained by visual comparison of the results. I make in approximately about 15% of the overall face &gt; dimensions (20% in some difficult cases) by eye comparison.

So concretely, how do you specify the erosion kernel in pixels?
Do you specify a matrix, and if yes, in which format? If not, is is absolute amount of pixels?
Sorry to bother again, but there is nothing in the online help that specifies exactly how you specify each option.
		</comment>
		<comment id='30' author='andenixa' date='2018-07-08T13:30:55Z'>
		&lt;denchmark-link:https://github.com/agilebean&gt;@agilebean&lt;/denchmark-link&gt;

I just specify erosion and smooth via cmdline option and look at the merged results until borders take approx. 15% of mask size. The actual absolute values vary depending on the image size and Model used.
		</comment>
		<comment id='31' author='andenixa' date='2018-07-08T13:40:57Z'>
		&lt;denchmark-link:https://github.com/andenixa&gt;@andenixa&lt;/denchmark-link&gt;

thanks. now i got you. the value specified with option -e seems to be the number of pixels from the outer edge of the mask. now i understand that what you meant is the border width which is removed from the mask outer edge - and that's what is meant by erosion.
Thanks again!
Just one wish for the future:
Can you provide these specifications in the --help ?
		</comment>
		<comment id='32' author='andenixa' date='2019-01-16T23:25:47Z'>
		GAN is being dropped from the repo. It may be re-added in future. Either way, it will be a new implementation,
		</comment>
	</comments>
</bug>