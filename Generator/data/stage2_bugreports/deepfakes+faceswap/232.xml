<bug id='232' author='Aardwolf2018' open_date='2018-03-01T18:51:45Z' closed_time='2018-06-15T09:18:14Z'>
	<summary>Extract with multiple processes fails</summary>
	<description>
&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

python faceswap.py extract -i srcFolder -o outputFolder -D cnn -j 4
This command should start and use 4 threads/processes
&lt;denchmark-h:h2&gt;Actual behavior&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;C:\Users\aard\PycharmProjects\faceswap&gt;python faceswap.py extract -i imgagesPeople\testPerson -o output\testPerson -D cnn -j 4
C:\Users\aard\AppData\Local\Programs\Python\Python36\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Input Directory: C:\Users\aard\PycharmProjects\faceswap\imgagesPeople\testPerson
Output Directory: C:\Users\aard\PycharmProjects\faceswap\output\testPerson
Using json serializer
Starting, this may take a while...
Loading Extract from Extract_Align plugin...
  0%|                                                                                                                                                                                                              | 0/1209 [00:00&lt;?, ?it/s]C:\Users\aard\AppData\Local\Programs\Python\Python36\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
C:\Users\aard\AppData\Local\Programs\Python\Python36\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
C:\Users\aard\AppData\Local\Programs\Python\Python36\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
C:\Users\aard\AppData\Local\Programs\Python\Python36\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Using TensorFlow backend.
Using TensorFlow backend.
Using TensorFlow backend.
Writing alignments to: C:\Users\aard\PycharmProjects\faceswap\imgagesPeople\testPerson\alignments.json
multiprocessing.pool.RemoteTraceback:
"""
Traceback (most recent call last):
  File "C:\Users\aard\AppData\Local\Programs\Python\Python36\lib\multiprocessing\pool.py", line 119, in worker
    result = (True, func(*args, **kwds))
  File "C:\Users\aard\PycharmProjects\faceswap\lib\multithreading.py", line 16, in runner
    return method(item)
TypeError: 'NoneType' object is not callable
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "faceswap.py", line 29, in &lt;module&gt;
    arguments.func(arguments)
  File "C:\Users\aard\PycharmProjects\faceswap\lib\cli.py", line 62, in process_arguments
    self.process()
  File "C:\Users\aard\PycharmProjects\faceswap\scripts\extract.py", line 49, in process
    for filename, faces in tqdm(pool_process(self.processFiles, files, processes=processes), total = len(files)):
  File "C:\Users\aard\AppData\Local\Programs\Python\Python36\lib\site-packages\tqdm\_tqdm.py", line 959, in __iter__
    for obj in iterable:
  File "C:\Users\aard\PycharmProjects\faceswap\lib\multithreading.py", line 12, in pool_process
    for i in pool.imap_unordered(runner, data):
  File "C:\Users\aard\AppData\Local\Programs\Python\Python36\lib\multiprocessing\pool.py", line 735, in next
    raise value
TypeError: 'NoneType' object is not callable
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Steps to reproduce&lt;/denchmark-h&gt;

Just try to run the command mentioned on top in the plain windows command line.
For me nothing changed by using a different detector (HOG)
&lt;denchmark-h:h2&gt;Other relevant information&lt;/denchmark-h&gt;


Operating system and version: Windows 10
Python version: 3.6.4
Faceswap version: 232d931
Faceswap method: CPU/GPU
**Tensorflow:**1.5.0
**GPU:**GTX970

	</description>
	<comments>
		<comment id='1' author='Aardwolf2018' date='2018-03-01T23:28:54Z'>
		Again, this is an issue I hit when running the pytorch version of face-alignment. Either upstream doesn't support multi-processing, or some code needs to change here to better support it. Either way, it seems to hit my GPU pretty hard when extracting (GTX 1080) , so if you're going down the GPU route, it probably isn't necessary to increase the cores.
		</comment>
		<comment id='2' author='Aardwolf2018' date='2018-03-02T13:21:14Z'>
		I suppose multiprocess work only with CPU version of dlib and tf.
in GPU version there is nothing to be "multiprocessed", because dlib and tf eats all vram.
		</comment>
		<comment id='3' author='Aardwolf2018' date='2018-03-03T10:09:12Z'>
		Seems to be a problem with the "method" global variable in multithreading.py. It's value isn't preserved in the forked Pool processes. I made a hacky workaround which seems to do the trick. Just be warned that this doesn't work with the latest commits (last compatible commit is &lt;denchmark-link:https://github.com/deepfakes/faceswap/commit/6f2d260591b830b4230bcdc3aa20bb3623883172&gt;6f2d260&lt;/denchmark-link&gt;
) because if you try using it with them, it tries to initialize multiple instances of TensorFlow.
multithreading.py
import multiprocessing as mp

method = None

def pool_process(method_to_run, data, processes=None):
    global method
    if processes is None:
        processes = mp.cpu_count()
    method = method_to_run
    pool = mp.Pool(processes=processes)

    for i in pool.imap_unordered(runner, map(lambda x: (method_to_run, x), data)):
        yield i if i is not None else 0
    
def runner(item):
    return item[0](item[1])
extract.py
    def process(self):
        extractor_name = "Align" # TODO Pass as argument
        self.extractor = PluginLoader.get_extractor(extractor_name)()
        processes = self.arguments.processes
        try:
            if processes != 1:
                files = list(self.read_directory())
                self.parser = None # &lt;--- Add this line, since multiprocessing can't pickle the parser
                for filename, faces in tqdm(pool_process(self.processFiles, files, processes=processes), total = len(files)):
                    self.num_faces_detected += 1
                    self.faces_detected[os.path.basename(filename)] = faces
            else:
                try:
                    for filename in tqdm(self.read_directory()):
                        image = cv2.imread(filename)
                        self.faces_detected[os.path.basename(filename)] = self.handleImage(image, filename)
                except Exception as e:
                    print('Failed to extract from image: {}. Reason: {}'.format(filename, e))
        finally:
            self.write_alignments()
		</comment>
		<comment id='4' author='Aardwolf2018' date='2018-03-03T10:27:09Z'>
		Thnaks &lt;denchmark-link:https://github.com/master131&gt;@master131&lt;/denchmark-link&gt;
 . The problem of the global was known on Python3.6, that's why multi process was not made a default. Can you di a Pull Request so that people can test and give feedback? Thanks
		</comment>
		<comment id='5' author='Aardwolf2018' date='2018-03-03T10:38:25Z'>
		&lt;denchmark-link:https://github.com/Clorr&gt;@Clorr&lt;/denchmark-link&gt;
 I'd love to do a PR, but unfortunately it doesn't work with the &lt;denchmark-link:https://github.com/deepfakes/faceswap/commit/232d9313afabc075e28c1a42047b22e0568200e6&gt;232d931&lt;/denchmark-link&gt;
 commit. I'm not too fond of the keras stuff so if someone else could look into it that'd be great.
		</comment>
		<comment id='6' author='Aardwolf2018' date='2018-03-04T14:30:30Z'>
		Hey, i keep getting issues when i try to convert the images, when i start the process all i get is a square box on top of the converted person face, also after 20 images i get "no alignment found for {}" the image file name, any ideas ?
		</comment>
		<comment id='7' author='Aardwolf2018' date='2018-03-06T18:40:08Z'>
		I've observed similar thing and mentioned it on playground repo.
Is it actually using all resources, you could try running parallel scripts on different sets of data.
I've splitted some files into different sets and executed extract scripts in parallel. It didn't affect processing time that much, so it makes everything 3 times faster, though it is not convenient.
So if executing script in parallel makes speeds up processing then I guess something is not optimized.
		</comment>
		<comment id='8' author='Aardwolf2018' date='2018-03-08T23:41:46Z'>
		I've been doing some tests on this issue, I think it is caused by Dlib being compiled with cuda enabled.
Basically, if dlib is compiled with cuda flags, there's no way to disable cuda at runtime; hence the initialization error.
I've reinstalled Dlib without cuda in my virtualenv and I was able to use multiprocessing, although the Extract script crashed with a MemoryError on 8GB RAM.
If anybody can confirm this. I think the -j parameter would be redundant for GPU enabled environments.
		</comment>
		<comment id='9' author='Aardwolf2018' date='2018-03-09T16:49:27Z'>
		Note that there is a dlib.DLIB_USE_CUDA flag in Python, so we can rely on this to show a warning or to disable the multiprocess if someone uses CUDA in dlib
		</comment>
		<comment id='10' author='Aardwolf2018' date='2018-03-09T16:50:52Z'>
		Ok. I'll take a look into that and make a PR if possible.
		</comment>
		<comment id='11' author='Aardwolf2018' date='2018-03-11T04:57:59Z'>
		I managed to recompile Dlib and OpenCV to use CUDA on Win 10 x64, hoping to speed up the extract/align process.  It boosted GPU use during training from about 50% to 65%.  But as stated above, with Dlib CUDA-enabled, adding the -j parameter to the command line crashes facewap.py, but the extract script runs fine if -j parameter is omitted, on a GTX 1070 with 8 Gb.  But there is only 1% or 2% GPU use and less than 20% CPU use during the extract/align process on a Ryzen 1700X.
		</comment>
		<comment id='12' author='Aardwolf2018' date='2018-03-12T20:18:06Z'>
		After further inspection, I think this issue is related to Tensorflow using CUDA instead of Dlib. I have just tested the -j flag in an environment with Tensorflow compitled with CUDA and Dlib without it, and it works. It just crashes on with Out of Memory error.
		</comment>
		<comment id='13' author='Aardwolf2018' date='2018-03-12T20:34:45Z'>
		I can confirm that it works without CUDA on the dlib.  I use this myself on a server for most of my extraction tasks.
		</comment>
		<comment id='14' author='Aardwolf2018' date='2018-03-12T20:45:50Z'>
		I've added some logic to test if tensorflow has GPUs available in the extract script. If the user uses the -j argument and GPU are visible for TF, the program exits with a warning stating there's a way to force TF to use CPU only by using the CUDA_VISIBLE_DEVICES=-1 environment variable.
Disable GPU with environment variable:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/2175&gt;tensorflow/tensorflow#2175&lt;/denchmark-link&gt;

Link to the commit on my fork:
&lt;denchmark-link:https://github.com/ppmdo/faceswap/commit/eb928c24212deed620ba0326387055a7631543a0&gt;ppmdo@eb928c2&lt;/denchmark-link&gt;

I don't know if disabling CUDA for TF by exporting the environment variable within any of the scripts is worth looking at. That could allow the user to disable GPU processing within faceswap.py:
&lt;denchmark-link:https://stackoverflow.com/questions/37893755/tensorflow-set-cuda-visible-devices-within-jupyter&gt;https://stackoverflow.com/questions/37893755/tensorflow-set-cuda-visible-devices-within-jupyter&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='Aardwolf2018' date='2018-03-12T21:00:54Z'>
		Okay, I investigated the problem.  Here is the basic cause: when you use -j it actually just starts multiple copies of the extract script.  The extract scripts then ALL try to use the GPU.  But any one extract script uses all the ram on the GPU.  This makes all the others throw exceptions that they are out of memory.  Even if they don't throw the OOM exception, they can't use the GPU and fail, creating the black box problem.
This means that -j necessarily needs to be limited to CPU only.  Doing it from a environment variable is not the best way.  I'll work on this and create a patch that runs on the CPU only when -j is invoked.  That said, GPU will be MUCH faster than CPU only, especially if you're using CNN.  This is not yet multi gpu capable, but even a single GPU is significantly faster than a CPU (and an order of magnitude better an any NN based solutions).
So for now: don't use -j if you have GPUs it wont be faster than the GPU anyway.  If you do want to use -j on a GPU system, wait for the patch.  If you have a CPU only system, you can use -j but you will need to ensure you don't install CUDA or your dlib will try to access a nonexistant GPU (if you .already installed CUDA on a GPU less system, uninstall it then uninstall/reinstall dlib through pip to get a version compiled for your current layout).
		</comment>
		<comment id='16' author='Aardwolf2018' date='2018-03-12T21:03:38Z'>
		Maybe use dlib.DLIB_USE_CUDA to detect dlib using GPU and invalidate -j option. (I said "maybe" because I'm not sure of it but this could be a quick patch)
		</comment>
		<comment id='17' author='Aardwolf2018' date='2018-06-15T09:18:14Z'>
		I added a warning to the help text in &lt;denchmark-link:https://github.com/deepfakes/faceswap/commit/c3a047559b516fae6a7f26535b1131d991ab2398&gt;c3a0475&lt;/denchmark-link&gt;
. In the absence of a better solution, I'm closing this issue.
		</comment>
	</comments>
</bug>