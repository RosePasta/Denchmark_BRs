<bug id='286' author='koenhelwegen' open_date='2019-10-11T14:04:21Z' closed_time='2019-10-14T17:55:36Z'>
	<summary>Bop not working with TF2 on MultiGPU</summary>
	<description>
&lt;denchmark-h:h3&gt;Describe the bug&lt;/denchmark-h&gt;

In a multi-gpu environment, using Bop generates the following error:
&lt;denchmark-code&gt;2019-10-11 13:45:47 UTC -- Epoch 1/150
2019-10-11 13:45:50 UTC -- Traceback (most recent call last):
2019-10-11 13:45:50 UTC --   File "/usr/local/bin/nf", line 11, in &lt;module&gt;
2019-10-11 13:45:50 UTC --     load_entry_point('project-final', 'console_scripts', 'nf')()
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/click/core.py", line 764, in __call__
2019-10-11 13:45:50 UTC --     return self.main(*args, **kwargs)
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/click/core.py", line 717, in main
2019-10-11 13:45:50 UTC --     rv = self.invoke(ctx)
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/click/core.py", line 1137, in invoke
2019-10-11 13:45:50 UTC --     return _process_result(sub_ctx.command.invoke(sub_ctx))
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/click/core.py", line 956, in invoke
2019-10-11 13:45:50 UTC --     return ctx.invoke(self.callback, **ctx.params)
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/click/core.py", line 555, in invoke
2019-10-11 13:45:50 UTC --     return callback(*args, **kwargs)
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/zookeeper/cli.py", line 114, in train
2019-10-11 13:45:50 UTC --     function(build_model, dataset, hparams, output_dir, **kwargs)
2019-10-11 13:45:50 UTC --   File "/code/project_final/train.py", line 110, in train
2019-10-11 13:45:50 UTC --     callbacks=callbacks,
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training.py", line 728, in fit
2019-10-11 13:45:50 UTC --     use_multiprocessing=use_multiprocessing)
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.py", line 324, in fit
2019-10-11 13:45:50 UTC --     total_epochs=epochs)
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.py", line 123, in run_one_epoch
2019-10-11 13:45:50 UTC --     batch_outs = execution_function(iterator)
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", line 86, in execution_function
2019-10-11 13:45:50 UTC --     distributed_function(input_fn))
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py", line 457, in __call__
2019-10-11 13:45:50 UTC --     result = self._call(*args, **kwds)
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py", line 503, in _call
2019-10-11 13:45:50 UTC --     self._initialize(args, kwds, add_initializers_to=initializer_map)
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py", line 408, in _initialize
2019-10-11 13:45:50 UTC --     *args, **kwds))
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py", line 1848, in _get_concrete_function_internal_garbage_collected
2019-10-11 13:45:50 UTC --     graph_function, _, _ = self._maybe_define_function(args, kwargs)
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py", line 2150, in _maybe_define_function
2019-10-11 13:45:50 UTC --     graph_function = self._create_graph_function(args, kwargs)
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py", line 2041, in _create_graph_function
2019-10-11 13:45:50 UTC --     capture_by_value=self._capture_by_value),
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/func_graph.py", line 915, in func_graph_from_py_func
2019-10-11 13:45:50 UTC --     func_outputs = python_func(*func_args, **func_kwargs)
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py", line 358, in wrapped_fn
2019-10-11 13:45:50 UTC --     return weak_wrapped_fn().__wrapped__(*args, **kwds)
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", line 73, in distributed_function
2019-10-11 13:45:50 UTC --     per_replica_function, args=(model, x, y, sample_weights))
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_lib.py", line 760, in experimental_run_v2
2019-10-11 13:45:50 UTC --     return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_lib.py", line 1787, in call_for_each_replica
2019-10-11 13:45:50 UTC --     return self._call_for_each_replica(fn, args, kwargs)
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py", line 661, in _call_for_each_replica
2019-10-11 13:45:50 UTC --     fn, args, kwargs)
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py", line 196, in _call_for_each_replica
2019-10-11 13:45:50 UTC --     coord.join(threads)
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/coordinator.py", line 389, in join
2019-10-11 13:45:50 UTC --     six.reraise(*self._exc_info_to_raise)
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/six.py", line 693, in reraise
2019-10-11 13:45:50 UTC --     raise value
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/coordinator.py", line 297, in stop_on_exception
2019-10-11 13:45:50 UTC --     yield
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py", line 190, in _call_for_each_replica
2019-10-11 13:45:50 UTC --     **merge_kwargs)
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py", line 446, in _distributed_apply
2019-10-11 13:45:50 UTC --     ds_reduce_util.ReduceOp.SUM, grads_and_vars)
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_lib.py", line 1481, in batch_reduce_to
2019-10-11 13:45:50 UTC --     return self._batch_reduce_to(reduce_op, value_destination_pairs)
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py", line 707, in _batch_reduce_to
2019-10-11 13:45:50 UTC --     value_destination_pairs)
2019-10-11 13:45:50 UTC --   File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 317, in batch_reduce
2019-10-11 13:45:50 UTC --     value_destination_pairs[0][0].values) == 1:
2019-10-11 13:45:50 UTC -- IndexError: list index out of range
&lt;/denchmark-code&gt;

The code runs fine if I use tf.keras.optimizers.Adam() instead of Bop, or if I run it on a single GPU.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;with tf.distribute.MirroredStrategy().scope():
    model = build_model()
    model.compile(
        optimizer=lq.optimizers.Bop(tf.keras.optimizers.Adam()),
        loss="categorical_crossentropy",
        metrics=["categorical_accuracy", "top_k_categorical_accuracy"],
        )
model.fit(train_data)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

TensorFlow version: 2.0.0
Larq version:  0.7.3
	</description>
	<comments>
		<comment id='1' author='koenhelwegen' date='2019-10-14T14:53:30Z'>
		Turns out the problem is twofold:

The direct cause for the error is that fp_grads_and_vars is empty, which apparently crashes in a distributed environments.
However, fp_grads_and_vars shouldn't be empty (you have biases and bn vars); the problem is that the original grads_and_vars passed by the optimizer is a zip object which is empty after the first loop here.

		</comment>
		<comment id='2' author='koenhelwegen' date='2019-10-14T14:55:31Z'>
		This should likely be solved together with &lt;denchmark-link:https://github.com/larq/larq/issues/183&gt;#183&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='koenhelwegen' date='2019-10-14T15:10:03Z'>
		I don't think we have a good solution for the API yet, while this a serious bug, so I would merge this fix asap. But yeah, I we should take on the API as well.
		</comment>
		<comment id='4' author='koenhelwegen' date='2019-10-14T16:11:09Z'>
		Checked in TF1.14 and there the passed grads_and_vars is a list, so this worked as expected before TF2.0.
		</comment>
	</comments>
</bug>