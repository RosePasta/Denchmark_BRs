<bug id='574' author='AlexRux' open_date='2020-09-25T10:01:44Z' closed_time='2020-09-25T14:50:07Z'>
	<summary>Causal Padding</summary>
	<description>
&lt;denchmark-h:h3&gt;Describe the bug&lt;/denchmark-h&gt;

"causal" padding with QuantConv1D doesn't work. It reduces the dimension.
I got this summary:
&lt;denchmark-code&gt;+model stats--------------------------------------------------------------------------------------+
| Layer                     Input prec.           Outputs  # 4-bit  # 32-bit  Memory  32-bit MACs |
|                                 (bit)                        x 1       x 1    (kB)              |
+-------------------------------------------------------------------------------------------------+
| input_1                             -  ((None, 29, 1),)        0         0       0            ? |
| quant_conv1d                        -      (-1, 28, 12)       24         0    0.01            ? |
| activation                          -      (-1, 28, 12)        0         0       0            ? |
| quant_conv1d_1                      -      (-1, 26, 16)      384         0    0.19            ? |
| activation_1                        -      (-1, 26, 16)        0         0       0            ? |
| quant_conv1d_2                      -      (-1, 22, 20)      640         0    0.31            ? |
| activation_2                        -      (-1, 22, 20)        0         0       0            ? |
| quant_conv1d_3                      -      (-1, 21, 24)      960         0    0.47            ? |
| activation_3                        -      (-1, 21, 24)        0         0       0            ? |
| global_average_pooling1d            -          (-1, 24)        0         0       0            ? |
| flatten                             -          (-1, 24)        0         0       0            0 |
| quant_dense                         -           (-1, 4)       96         4    0.06           96 |
| activation_4                        -           (-1, 4)        0         0       0            ? |
+-------------------------------------------------------------------------------------------------+
| Total                                                       2104         4    1.04           96 |
+-------------------------------------------------------------------------------------------------+
&lt;/denchmark-code&gt;

Described by this:
&lt;denchmark-code&gt;kwargs = dict(
                  kernel_quantizer=kernel_quantizer,
                  kernel_constraint=kernel_constraint,
                  padding="causal",
                  use_bias = False,
    )

    x = tf.keras.Input(shape=(Time_series_length, 1))

    ConvL1 = lq.layers.QuantConv1D(filters=12, kernel_size=2, dilation_rate=1, **kwargs)(x)
    ConvL1 = tf.keras.layers.Activation(activation)(ConvL1)

    ConvL2 = lq.layers.QuantConv1D(filters=16, kernel_size=2, dilation_rate=2, **kwargs)(ConvL1)
    ConvL2 = tf.keras.layers.Activation(activation)(ConvL2)

    ConvL3 = lq.layers.QuantConv1D(filters=20, kernel_size=2, dilation_rate=4, **kwargs)(ConvL2)
    ConvL3 = tf.keras.layers.Activation(activation)(ConvL3)

    ConvL4 = lq.layers.QuantConv1D(filters=24, kernel_size=2, dilation_rate=1, **kwargs)(ConvL3)
    ConvL4 = tf.keras.layers.Activation(activation)(ConvL4)

    final = tf.keras.layers.GlobalAveragePooling1D()(ConvL4)
    final = tf.keras.layers.Flatten()(final)
    final = lq.layers.QuantDense(classi, kernel_quantizer=kernel_quantizer, kernel_constraint=kernel_constraint)(
        final)
    final = tf.keras.layers.Activation("softmax")(final)
&lt;/denchmark-code&gt;

But using tf.keras.layers.Conv1D instead of lq.layers.QuantConv1D I got this:
&lt;denchmark-code&gt;+model stats-----------------------------------------------------------------------------+
| Layer                     Input prec.           Outputs  # 32-bit  Memory  32-bit MACs |
|                                 (bit)                         x 1    (kB)              |
+----------------------------------------------------------------------------------------+
| input_1                             -  ((None, 29, 1),)         0       0            ? |
| conv1d                              -      (-1, 29, 12)        24    0.09            ? |
| activation                          -      (-1, 29, 12)         0       0            ? |
| conv1d_1                            -      (-1, 29, 16)       384    1.50            ? |
| activation_1                        -      (-1, 29, 16)         0       0            ? |
| conv1d_2                            -      (-1, 29, 20)       640    2.50            ? |
| activation_2                        -      (-1, 29, 20)         0       0            ? |
| conv1d_3                            -      (-1, 29, 24)       960    3.75            ? |
| activation_3                        -      (-1, 29, 24)         0       0            ? |
| global_average_pooling1d            -          (-1, 24)         0       0            ? |
| flatten                             -          (-1, 24)         0       0            0 |
| quant_dense                         -           (-1, 4)        96    0.38           96 |
| activation_4                        -           (-1, 4)         0       0            ? |
+----------------------------------------------------------------------------------------+
| Total                                                        2104    8.22           96 |
+----------------------------------------------------------------------------------------+
&lt;/denchmark-code&gt;

They should be the same. What can I do to fix it? I need quantizations and causal padding.
TensorFlow version: 2.1
Larq version: 10.0
	</description>
	<comments>
		<comment id='1' author='AlexRux' date='2020-09-25T12:07:56Z'>
		I was able to reproduce your issue.
This is actually caused by the fact that TensorFlow  &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/25fba035f3e453d94490932096282c7b0624bbb3/tensorflow/python/keras/layers/convolutional.py#L204&gt;explicitely checks for the class name in the Conv1D layer&lt;/denchmark-link&gt;
 which prevent subclasses of  to use causal padding.
This bug has been fixed in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/17b63987e58a08ecaa950c2668a5870a399cfaba&gt;tensorflow/tensorflow@17b6398&lt;/denchmark-link&gt;
, so upgrading to TensorFlow 2.3 should solve your issue.
I've personally never used causal padding in Keras so please let us know if this fixes your issue.
		</comment>
		<comment id='2' author='AlexRux' date='2020-09-25T14:34:02Z'>
		Fixed, thank you.
		</comment>
	</comments>
</bug>