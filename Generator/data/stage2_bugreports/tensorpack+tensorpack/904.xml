<bug id='904' author='thuzhf' open_date='2018-09-21T10:04:49Z' closed_time='2018-09-21T20:14:52Z'>
	<summary>AttributeError: 'TrainLoop' object has no attribute 'starting_epoch'</summary>
	<description>
The problem occurred when L318 below is executed if stats is not None (e.g. when you restore training from a log dir). The full error message is attached in the end.



tensorpack/tensorpack/callbacks/monitor.py


        Lines 309 to 318
      in
      36b05bb






 def _setup_graph(self): 



 stats = JSONWriter.load_existing_json() 



 self._fname = os.path.join(logger.get_logger_dir(), JSONWriter.FILENAME) 



 if stats is not None: 



 try: 



 epoch = stats[-1]['epoch_num'] + 1 



 except Exception: 



 epoch = None 



 



 starting_epoch = self.trainer.loop.starting_epoch 





It can be seen self.starting_epoch is defined in L48 below.



tensorpack/tensorpack/train/base.py


        Lines 44 to 48
      in
      36b05bb






 def config(self, steps_per_epoch, starting_epoch, max_epoch): 



 """ 



         Configure the loop given the settings. 



         """ 



 self.starting_epoch = int(starting_epoch) 





And config() is called in L241 below in the main_loop() function.



tensorpack/tensorpack/train/base.py


        Lines 233 to 241
      in
      36b05bb






 def main_loop(self, steps_per_epoch, starting_epoch, max_epoch): 



 """ 



         Run the main training loop. 



  



         Args: 



             steps_per_epoch, starting_epoch, max_epoch (int): 



         """ 



 with self.sess.as_default(): 



 self.loop.config(steps_per_epoch, starting_epoch, max_epoch) 





And setup_callbacks() (L289 below) is executed before main_loop() (L291 below).



tensorpack/tensorpack/train/base.py


        Lines 274 to 291
      in
      36b05bb






 def train(self, 



 callbacks, monitors, 



 session_creator, session_init, 



 steps_per_epoch, starting_epoch=1, max_epoch=9999999): 



 """ 



         Implemented by three lines: 



  



         .. code-block:: python 



  



             self.setup_callbacks(callbacks, monitors) 



             self.initialize(session_creator, session_init) 



             self.main_loop(steps_per_epoch, starting_epoch, max_epoch) 



  



         You can call those methods by yourself to have better control on details if needed. 



         """ 



 self.setup_callbacks(callbacks, monitors) 



 self.initialize(session_creator, session_init) 



 self.main_loop(steps_per_epoch, starting_epoch, max_epoch) 





And L318 in the first file (monitor.py) will be finally executed in the calling of setup_callbacks() above. Actually you already have load_existing_epoch_number  in L298 below.



tensorpack/tensorpack/callbacks/monitor.py


         Line 298
      in
      36b05bb






 def load_existing_epoch_number(): 





And the above function is only called in the following class:



tensorpack/tensorpack/train/config.py


        Lines 177 to 240
      in
      36b05bb






 class AutoResumeTrainConfig(TrainConfig): 



 """ 



     Same as :class:`TrainConfig`, but does the following to automatically 



     resume from training: 



  



     1. If a checkpoint was found in :meth:`logger.get_logger_dir()`, set 



        `session_init` option to load it. 



     2. If a JSON history was found in :meth:`logger.get_logger_dir()`, try to 



        load the epoch number from it and set the `starting_epoch` option to 



        continue training. 



  



     You can choose to let the above two option to either overwrite or 



     not overwrite user-provided arguments, as explained below. 



     """ 



 def __init__(self, always_resume=True, **kwargs): 



 """ 



         Args: 



             always_resume (bool): If False, user-provided arguments 



                 `session_init` and `starting_epoch` will take priority. 



                 Otherwise, resume will take priority. 



             kwargs: same as in :class:`TrainConfig`. 



  



         Note: 



             The main goal of this class is to let a training job to resume 



             without changing any line of code or command line arguments. 



             So it's useful to let resume take priority over user-provided arguments sometimes: 



  



             If your training starts from a pre-trained model, 



             you would want it to use user-provided model loader at the 



             beginning, but a "resume" model loader when the job was 



             interrupted and restarted. 



         """ 



 if always_resume or 'session_init' not in kwargs: 



 sessinit = self._get_sessinit_resume() 



 if sessinit is not None: 



 path = sessinit.path 



 if 'session_init' in kwargs: 



 logger.info("Found checkpoint at {}. " 



 "session_init arguments will be overwritten.".format(path)) 



 else: 



 logger.info("Will load checkpoint at {}.".format(path)) 



 kwargs['session_init'] = sessinit 



 



 if always_resume or 'starting_epoch' not in kwargs: 



 last_epoch = self._get_last_epoch() 



 if last_epoch is not None: 



 now_epoch = last_epoch + 1 



 logger.info("Found history statistics from JSON. " 



 "Overwrite the starting epoch to epoch #{}.".format(now_epoch)) 



 kwargs['starting_epoch'] = now_epoch 



 



 super(AutoResumeTrainConfig, self).__init__(**kwargs) 



 



 def _get_sessinit_resume(self): 



 logdir = logger.get_logger_dir() 



 if not logdir: 



 return None 



 path = os.path.join(logdir, 'checkpoint') 



 if not tf.gfile.Exists(path): 



 return None 



 return SaverRestore(path) 



 



 def _get_last_epoch(self): 



 return JSONWriter.load_existing_epoch_number() 





Even if I changed TrainConfig() to AutoResumeTrainConfig(), the same problem still occurred due to the same reason. Although self.starting_epoch is also defined below, it is the config's attribute instead of the train.loop's:



tensorpack/tensorpack/train/config.py


         Line 146
      in
      7b8728f






 self.starting_epoch = int(starting_epoch) 





I think some easy workarounds will be:

Defining starting_epoch earlier as the train.loop's attribute (that way AutoResumeTrainConfig() will probably work).
Changing L318 in the first file (monitor.py) to use function load_existing_epoch_number() (that way maybe will make AutoResumeTrainConfig() useless, but this is handy because users can always use TrainConfig()).

However, to be consistent with the logic of your Trainer.train() function (including 3 functions), it seems it needs more consideration on how to fix this.
The following is the full error message:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "main.py", line 307, in &lt;module&gt;
    main()
  File "main.py", line 303, in main
    launch_train_with_config(config, trainer)
  File "/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/train/interface.py", line 95, in launch_train_with_config
    extra_callbacks=config.extra_callbacks)
  File "/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/train/base.py", line 319, in train_with_defaults
    steps_per_epoch, starting_epoch, max_epoch)
  File "/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/train/base.py", line 289, in train
    self.setup_callbacks(callbacks, monitors)
  File "/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/utils/argtools.py", line 181, in wrapper
    return func(*args, **kwargs)
  File "/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/train/base.py", line 189, in setup_callbacks
    self._callbacks.setup_graph(weakref.proxy(self))
  File "/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/callbacks/base.py", line 52, in setup_graph
    self._setup_graph()
  File "/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/callbacks/group.py", line 70, in _setup_graph
    cb.setup_graph(self.trainer)
  File "/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/callbacks/monitor.py", line 55, in setup_graph
    self._setup_graph()
  File "/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/callbacks/monitor.py", line 318, in _setup_graph
    starting_epoch = self.trainer.loop.starting_epoch
AttributeError: 'TrainLoop' object has no attribute 'starting_epoch'
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='thuzhf' date='2018-09-21T15:07:13Z'>
		This is an issue I fixed before but forgot to take a note about it. Unfortunately it was introduced again lately..
I will think about how to properly address this.
		</comment>
	</comments>
</bug>