<bug id='431' author='chunfuchen' open_date='2017-09-26T17:46:06Z' closed_time='2017-10-04T00:26:45Z'>
	<summary>Distributed Leaning on ResNet (follow up)</summary>
	<description>
Thanks for your rapid response on &lt;denchmark-link:https://github.com/tensorpack/tensorpack/issues/430&gt;#430&lt;/denchmark-link&gt;
 .
After I pull changes, I got another error about learning rate setup after graph is built.
Thanks.
Here is the error log:
&lt;denchmark-code&gt;[0926 13:42:19 @base.py:167] Setup callbacks graph ...
Traceback (most recent call last):
  File "examples/ResNet/imagenet-resnet-dist.py", line 169, in &lt;module&gt;
    DistributedTrainerReplicated(config, server).train()
  File "/mnt/nvme1/chenrich/Developer/tensorpack-0.5.0/tensorpack/train/base.py", line 135, in train
    self.setup()
  File "/mnt/nvme1/chenrich/Developer/tensorpack-0.5.0/tensorpack/train/base.py", line 169, in setup
    self._callbacks.setup_graph(weakref.proxy(self))
  File "/mnt/nvme1/chenrich/Developer/tensorpack-0.5.0/tensorpack/callbacks/base.py", line 54, in setup_graph
    self._setup_graph()
  File "/mnt/nvme1/chenrich/Developer/tensorpack-0.5.0/tensorpack/callbacks/group.py", line 64, in _setup_graph
    cb.setup_graph(self.trainer)
  File "/mnt/nvme1/chenrich/Developer/tensorpack-0.5.0/tensorpack/callbacks/base.py", line 54, in setup_graph
    self._setup_graph()
  File "/mnt/nvme1/chenrich/Developer/tensorpack-0.5.0/tensorpack/callbacks/param.py", line 129, in _setup_graph
    self.param.setup_graph()
  File "/mnt/nvme1/chenrich/Developer/tensorpack-0.5.0/tensorpack/callbacks/param.py", line 75, in setup_graph
    raise ValueError("{} is not a GLOBAL_VARIABLE in the graph!".format(self.var_name))
ValueError: learning_rate:0 is not a GLOBAL_VARIABLE in the graph!
ERROR:tensorflow:==================================
Object was never used (type &lt;class 'tensorflow.python.framework.ops.Tensor'&gt;):
&lt;tf.Tensor 'report_uninitialized_variables/boolean_mask/Gather:0' shape=(?,) dtype=string&gt;
If you want to mark it as used call its "mark_used()" method.
It was originally created here:
['File "examples/ResNet/imagenet-resnet-dist.py", line 169, in &lt;module&gt;\n    DistributedTrainerReplicated(config, server).train()', 'File "/mnt/nvme1/chenrich/Developer/tensorpack-0.5.0/tensorpack/train/distributed.py", line 91, in __init__\n    super(DistributedTrainerReplicated, self).__init__(config)', 'File "/mnt/nvme1/chenrich/Developer/tensorpack-0.5.0/tensorpack/train/base.py", line 93, in __init__\n    self._setup()   # subclass will setup the graph and InputSource', 'File "/mnt/nvme1/chenrich/Developer/tensorpack-0.5.0/tensorpack/train/distributed.py", line 242, in _setup\n    self._set_session_creator()', 'File "/mnt/nvme1/chenrich/Developer/tensorpack-0.5.0/tensorpack/train/distributed.py", line 254, in _set_session_creator\n    ready_op = tf.report_uninitialized_variables()', 'File "/home/chenrich/.tensorflowEnvPy3Alt/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 175, in wrapped\n    return _add_should_use_warning(fn(*args, **kwargs))', 'File "/home/chenrich/.tensorflowEnvPy3Alt/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 144, in _add_should_use_warning\n    wrapped = TFShouldUseWarningWrapper(x)', 'File "/home/chenrich/.tensorflowEnvPy3Alt/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 101, in __init__\n    stack = [s.strip() for s in traceback.format_stack()]']
==================================
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='chunfuchen' date='2017-09-26T17:53:49Z'>
		Could you try again?
		</comment>
		<comment id='2' author='chunfuchen' date='2017-09-26T18:12:14Z'>
		Thanks. I do not encounter the error but I get out of memory issue if I start param server and worker at the same gpu. (launch order: param server --&gt; worker)
The param server will occupy all memory as tensorflow did by default.
So, in practice, I need to set up the upper bound memory utilzation ratio for parameter server, right?
Or for distributed learning, the parameter server is allocated at cpu?
		</comment>
		<comment id='3' author='chunfuchen' date='2017-09-26T18:13:08Z'>
		Yeah. TF by default allocate all GPU memory although a PS may not need GPU. You can start the worker first as a workaround.
		</comment>
		<comment id='4' author='chunfuchen' date='2017-09-26T18:19:20Z'>
		okay, thanks.
Keep getting:
&lt;denchmark-code&gt;2017-09-26 14:18:27.912036: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session 1dc3a76f13e926d3 with config: intra_op_parallelism_threads: 1 gpu_options { per_process_gpu_memory_fraction: 0.99 allocator_type: "BFC" allow_growth: true force_gpu_compatible: true } allow_soft_placement: true
&lt;/denchmark-code&gt;

I think it should be my fault in setting up cluster spec. Let me take a look first.
Thanks for your help.
		</comment>
		<comment id='5' author='chunfuchen' date='2017-09-26T18:21:03Z'>
		I remember TF will print some duplicated logs when starting up distributed training. Can't recall if this is the one.
		</comment>
		<comment id='6' author='chunfuchen' date='2017-09-26T19:29:44Z'>
		:( still can not train the model,
My clusterSpec is
    cluster_spec = tf.train.ClusterSpec({
            'ps': ['localhost:2222', 'localhost:2232'],
            'worker': ['localhost:2223', 'localhost:2233']
        })
I start 4 processes: 2 param servers, 2 workers (start 2 workers first, and then start 2 param sersers.)
After 4 processes are started, I do see 2 param servers show the message:
&lt;denchmark-code&gt;[0926 15:04:30 @distributed.py:72] My role in the cluster: job=ps, task=0
[0926 15:04:30 @distributed.py:196] Running ps 0
[0926 15:04:30 @distributed.py:197] Kill me with 'kill 29729'
&lt;/denchmark-code&gt;

and
&lt;denchmark-code&gt;[0926 15:03:41 @distributed.py:72] My role in the cluster: job=ps, task=1
[0926 15:03:41 @distributed.py:196] Running ps 1
[0926 15:03:41 @distributed.py:197] Kill me with 'kill 29473'
&lt;/denchmark-code&gt;

One of workers displayed already
&lt;denchmark-code&gt;0926 15:05:34 @concurrency.py:36] Starting EnqueueThread QueueInput/input_queue ...
[0926 15:05:35 @base.py:228] Start Epoch 1 ...
0%|                                                         |0/5000[00:00&lt;?,?it/s]
&lt;/denchmark-code&gt;

however, the other worker keep displaying  (I assume that this worker should show the same message to the other worker but it did not, so I guess there is something wrong.)
&lt;denchmark-code&gt;Start master session 6c3a06a21ffc4486 with config: intra_op_parallelism_threads: 1 gpu_options { per_process_gpu_memory_fraction: 0.99 allocator_type: "BFC" allow_growth: true force_gpu_compatible: true } allow_soft_placement: true
2017-09-26 15:24:24.040698: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session a27bc07052114c4c with config: intra_op_parallelism_threads: 1 gpu_options { per_process_gpu_memory_fraction: 0.99 allocator_type: "BFC" allow_growth: true force_gpu_compatible: true } allow_soft_placement: true
2017-09-26 15:24:54.368830: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session ede2e745ee062fe4 with config: intra_op_parallelism_threads: 1 gpu_options { per_process_gpu_memory_fraction: 0.99 allocator_type: "BFC" allow_growth: true force_gpu_compatible: true } allow_soft_placement: true
2017-09-26 15:25:24.735236: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session f5989ba1e0196365 with config: intra_op_parallelism_threads: 1 gpu_options { per_process_gpu_memory_fraction: 0.99 allocator_type: "BFC" allow_growth: true force_gpu_compatible: true } allow_soft_placement: true
&lt;/denchmark-code&gt;

May I use single machine to mimic distributed learning? I am testing my script and then I will deploy on real cluster.
Thanks.
		</comment>
		<comment id='7' author='chunfuchen' date='2017-09-26T19:47:01Z'>
		I can use single machine to run 2ps 2 workers using your cluster_spec. Not sure what's going on at your side. I did notice that I cannot run distributed training inside a chroot-based container but not sure that's related.
		</comment>
		<comment id='8' author='chunfuchen' date='2017-09-26T20:15:03Z'>
		okay, I need to ask admin about the machine configuration, I will try to find another machine to test it. Thanks.
		</comment>
		<comment id='9' author='chunfuchen' date='2017-09-27T15:16:24Z'>
		No luck, by using my laptop as a single server, I still keep getting the above message... (the non-chief worker can not start working.)
May I know that by using your example codes, cifar10-resnet.py, I think I only need to replace original SyncMultiGPUTrainerParameterServer to DistributedTrainerReplicated and setup tf.train.ClusterSpec and tf.train.Server, right? Do I need to change Optimizer or model definition?
Here is codes examples [cifar10-resnet-dist.py]. (I modify the codes to run on CPU, replace AvgPooling with Stride Conv.) (&lt;denchmark-link:https://gist.github.com/chunfuchen/158434a1d86f44d77666ec685f57e501&gt;https://gist.github.com/chunfuchen/158434a1d86f44d77666ec685f57e501&lt;/denchmark-link&gt;
)
Any suggestion would be appreciated.
		</comment>
		<comment id='10' author='chunfuchen' date='2017-09-27T17:12:14Z'>
		Seems like I can reproduce the problem. I'll look into it.
		</comment>
		<comment id='11' author='chunfuchen' date='2017-09-27T18:54:57Z'>
		Should've been fixed now. Sorry for these bugs -- I never really used distributed trainer for anything serious so a lot of functionalities are not well tested.
		</comment>
		<comment id='12' author='chunfuchen' date='2017-09-27T19:18:19Z'>
		Thanks for your great help, it is really helpful for me. I can train it now.
Btw, I saw a warning message as below:
&lt;denchmark-code&gt;WARNING:tensorflow:From ~/tensorpack/callbacks/param.py:69: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Please use tf.global_variables instead.
&lt;/denchmark-code&gt;

Is it okay? I think you made this change for distributed trainer yesterday.(&lt;denchmark-link:https://github.com/tensorpack/tensorpack/commit/584e9cd4cb2812c722b65a79309cc78ee9d5b552&gt;584e9cd&lt;/denchmark-link&gt;
)
		</comment>
		<comment id='13' author='chunfuchen' date='2017-09-27T19:37:03Z'>
		The use of all_variables was removed today.
		</comment>
		<comment id='14' author='chunfuchen' date='2017-09-27T20:21:19Z'>
		Thanks. Will close this issue after I validate on the cluster.
		</comment>
		<comment id='15' author='chunfuchen' date='2017-10-04T00:26:45Z'>
		Everything works well on a cluster, I have tested on the following configurations:

4 nodes, each with one gpus
2 nodes, each with two gpus

Thanks for your help.
		</comment>
	</comments>
</bug>