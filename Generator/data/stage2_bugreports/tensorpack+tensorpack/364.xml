<bug id='364' author='chunfuchen' open_date='2017-08-07T16:09:54Z' closed_time='2017-08-07T16:26:56Z'>
	<summary>global_step number is incorrect after resume</summary>
	<description>
After resuming a model trained at global_step=475000, the next global_step jumps to 955000 directly. Here is the log:
&lt;denchmark-code&gt;[0807 10:55:08 @steps.py:71] Start training with global_step=475000
[0807 10:55:11 @param.py:144] learning_rate at epoch 96 will change to 0.00001000
[0807 10:55:12 @concurrency.py:36] Starting EnqueueThread ...
[0807 10:55:12 @input_source.py:418] Pre-filling staging area ...
[0807 10:55:12 @base.py:200] Start Epoch 96 ...
100%|############################################################################################|5000/5000[51:14&lt;00:00, 0.66it/s]
[0807 11:46:26 @base.py:210] Epoch 96 (global_step 955000) finished, time:3074.13 sec.
&lt;/denchmark-code&gt;

It seems that the global_step is messed after resuming. (955000 = 475000 + 480000, in my training, 5000 iters = 1 epoch); but the learning rate scheduler still wokrs fine and the epoch num is correct.
Do I need to maintain the global_step by myself?
Here is the command of my execution:
&lt;denchmark-code&gt;python3 examples/ResNet/imagenet-resnet.py -d 50 --gpu 0,1,2,3 --load $SOMEWHERE
&lt;/denchmark-code&gt;

Thanks.
	</description>
	<comments>
		<comment id='1' author='chunfuchen' date='2018-04-03T02:57:43Z'>
		Actually, I think the number is correct here. Since you are resuming the interrupted training process, not retraining a pretrained model, the epoch number should follow previous process. As in your case, the model is stopped at No. 475000 iteration (475000/5000 = 95 epoch)ï¼Œ the resumed epoch number should be 96. &lt;denchmark-link:https://github.com/chunfuchen&gt;@chunfuchen&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>