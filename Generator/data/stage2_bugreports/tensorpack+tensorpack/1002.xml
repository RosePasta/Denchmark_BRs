<bug id='1002' author='clampert' open_date='2018-12-06T15:45:25Z' closed_time='2018-12-06T16:57:43Z'>
	<summary>Assert fails on machine with more than 8 GPUs</summary>
	<description>
I'm describing a problem of version 0.9.0.1-24-gef334e31, hash 'ef334e31c34bab909d2c7469d3905347a0d5137a' on machines with more than 8 GPUs.
I ran the imagenet-resnet.py example with unmodified source on a 10 GPU machine:
python3 mygit/tensorpack/examples/ResNet/imagenet-resnet.py --data /localhome/chl/ILSVRC2012/ -d 50 --batch 320
The code fails, and with good reason:
The class SyncMultiGPUTrainerReplicated will activate hierarchical mode for 8 GPUs or more:
train/trainers.py:176      mode = 'hierarchical' if len(gpus) &gt;= 8 else 'nccl'
Afterwards, in the routine allreduce_grads_hierarchical will fail because an assert allows only exactly 8 GPUs:
graph_builder/utils.py:187     assert num_gpu == 8, num_gpu
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;log of /dev/stderr&lt;/denchmark-h&gt;

Traceback (most recent call last):
File "mygit/tensorpack/examples/ResNet/imagenet-resnet.py", line 148, in 
launch_train_with_config(config, trainer)
File "/nfs/scistore12/chlgrp/chl/.local/lib/python3.5/site-packages/tensorpack/train/interface.py", line 87, in launch_train_with_config
model._build_graph_get_cost, model.get_optimizer)
File "/nfs/scistore12/chlgrp/chl/.local/lib/python3.5/site-packages/tensorpack/utils/argtools.py", line 176, in wrapper
return func(*args, **kwargs)
File "/nfs/scistore12/chlgrp/chl/.local/lib/python3.5/site-packages/tensorpack/train/tower.py", line 204, in setup_graph
train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)
File "/nfs/scistore12/chlgrp/chl/.local/lib/python3.5/site-packages/tensorpack/train/trainers.py", line 186, in _setup_graph
self._make_get_grad_fn(input, get_cost_fn, get_opt_fn), get_opt_fn)
File "/nfs/scistore12/chlgrp/chl/.local/lib/python3.5/site-packages/tensorpack/graph_builder/training.py", line 251, in build
packed_grads, raw_devices, average=self._average)
File "/nfs/scistore12/chlgrp/chl/.local/lib/python3.5/site-packages/tensorpack/tfutils/scope_utils.py", line 94, in wrapper
return func(*args, **kwargs)
File "/nfs/scistore12/chlgrp/chl/.local/lib/python3.5/site-packages/tensorpack/graph_builder/utils.py", line 187, in allreduce_grads_hierarchical
assert num_gpu == 8, num_gpu
AssertionError: 10
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;log of /dev/stdout&lt;/denchmark-h&gt;

�[32m[1206 16:21:16 @logger.py:85]�[0m Argv: mygit/tensorpack/examples/ResNet/imagenet-resnet.py --data /localhome/chl/ILSVRC2012/ -d 50 --batch 320
�[32m[1206 16:21:16 @imagenet-resnet.py:64]�[0m Running on 10 towers. Batch size per tower: 32
�[32m[1206 16:21:16 @fs.py:100]�[0m �[5m�[31mWRN�[0m Env var $TENSORPACK_DATASET not set, using /nfs/scistore12/chlgrp/chl/tensorpack_data for datasets.
�[32m[1206 16:21:17 @imagenet_utils.py:106]�[0m �[5m�[31mWRN�[0m DataFlow may become the bottleneck when too few processes are used.
�[32m[1206 16:21:17 @parallel.py:313]�[0m [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
�[32m[1206 16:21:17 @ilsvrc.py:128]�[0m [ILSVRC12] Assuming directory /localhome/chl/ILSVRC2012/val has 'original' structure.
�[32m[1206 16:21:18 @training.py:52]�[0m [DataParallel] Training a model of 10 towers.
�[32m[1206 16:21:18 @interface.py:46]�[0m Automatically applying StagingInput on the DataFlow.
�[32m[1206 16:21:18 @input_source.py:219]�[0m Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...
�[32m[1206 16:21:18 @training.py:112]�[0m Building graph for training tower 0 on device /gpu:0 ...
�[32m[1206 16:21:18 @registry.py:121]�[0m conv0 input: [None, 3, 224, 224]
�[32m[1206 16:21:18 @registry.py:129]�[0m conv0 output: [None, 64, 112, 112]
�[32m[1206 16:21:18 @registry.py:121]�[0m pool0 input: [None, 64, 112, 112]
�[32m[1206 16:21:18 @registry.py:129]�[0m pool0 output: [None, 64, 56, 56]
�[32m[1206 16:21:18 @registry.py:121]�[0m group0/block0/conv1 input: [None, 64, 56, 56]
�[32m[1206 16:21:18 @registry.py:129]�[0m group0/block0/conv1 output: [None, 64, 56, 56]
�[32m[1206 16:21:18 @registry.py:121]�[0m group0/block0/conv2 input: [None, 64, 56, 56]
�[32m[1206 16:21:18 @registry.py:129]�[0m group0/block0/conv2 output: [None, 64, 56, 56]
�[32m[1206 16:21:18 @registry.py:121]�[0m group0/block0/conv3 input: [None, 64, 56, 56]
�[32m[1206 16:21:18 @registry.py:129]�[0m group0/block0/conv3 output: [None, 256, 56, 56]
�[32m[1206 16:21:18 @registry.py:121]�[0m group0/block0/convshortcut input: [None, 64, 56, 56]
�[32m[1206 16:21:18 @registry.py:129]�[0m group0/block0/convshortcut output: [None, 256, 56, 56]
�[32m[1206 16:21:18 @registry.py:121]�[0m group0/block1/conv1 input: [None, 256, 56, 56]
�[32m[1206 16:21:18 @registry.py:129]�[0m group0/block1/conv1 output: [None, 64, 56, 56]
�[32m[1206 16:21:18 @registry.py:121]�[0m group0/block1/conv2 input: [None, 64, 56, 56]
�[32m[1206 16:21:18 @registry.py:129]�[0m group0/block1/conv2 output: [None, 64, 56, 56]
�[32m[1206 16:21:18 @registry.py:121]�[0m group0/block1/conv3 input: [None, 64, 56, 56]
�[32m[1206 16:21:18 @registry.py:129]�[0m group0/block1/conv3 output: [None, 256, 56, 56]
�[32m[1206 16:21:18 @registry.py:121]�[0m group0/block2/conv1 input: [None, 256, 56, 56]
�[32m[1206 16:21:18 @registry.py:129]�[0m group0/block2/conv1 output: [None, 64, 56, 56]
�[32m[1206 16:21:18 @registry.py:121]�[0m group0/block2/conv2 input: [None, 64, 56, 56]
�[32m[1206 16:21:18 @registry.py:129]�[0m group0/block2/conv2 output: [None, 64, 56, 56]
�[32m[1206 16:21:18 @registry.py:121]�[0m group0/block2/conv3 input: [None, 64, 56, 56]
�[32m[1206 16:21:18 @registry.py:129]�[0m group0/block2/conv3 output: [None, 256, 56, 56]
�[32m[1206 16:21:18 @registry.py:121]�[0m group1/block0/conv1 input: [None, 256, 56, 56]
�[32m[1206 16:21:18 @registry.py:129]�[0m group1/block0/conv1 output: [None, 128, 56, 56]
�[32m[1206 16:21:18 @registry.py:121]�[0m group1/block0/conv2 input: [None, 128, 56, 56]
�[32m[1206 16:21:18 @registry.py:129]�[0m group1/block0/conv2 output: [None, 128, 28, 28]
�[32m[1206 16:21:18 @registry.py:121]�[0m group1/block0/conv3 input: [None, 128, 28, 28]
�[32m[1206 16:21:18 @registry.py:129]�[0m group1/block0/conv3 output: [None, 512, 28, 28]
�[32m[1206 16:21:18 @registry.py:121]�[0m group1/block0/convshortcut input: [None, 256, 56, 56]
�[32m[1206 16:21:18 @registry.py:129]�[0m group1/block0/convshortcut output: [None, 512, 28, 28]
�[32m[1206 16:21:18 @registry.py:121]�[0m group1/block1/conv1 input: [None, 512, 28, 28]
�[32m[1206 16:21:18 @registry.py:129]�[0m group1/block1/conv1 output: [None, 128, 28, 28]
�[32m[1206 16:21:18 @registry.py:121]�[0m group1/block1/conv2 input: [None, 128, 28, 28]
�[32m[1206 16:21:18 @registry.py:129]�[0m group1/block1/conv2 output: [None, 128, 28, 28]
�[32m[1206 16:21:18 @registry.py:121]�[0m group1/block1/conv3 input: [None, 128, 28, 28]
�[32m[1206 16:21:18 @registry.py:129]�[0m group1/block1/conv3 output: [None, 512, 28, 28]
�[32m[1206 16:21:18 @registry.py:121]�[0m group1/block2/conv1 input: [None, 512, 28, 28]
�[32m[1206 16:21:18 @registry.py:129]�[0m group1/block2/conv1 output: [None, 128, 28, 28]
�[32m[1206 16:21:18 @registry.py:121]�[0m group1/block2/conv2 input: [None, 128, 28, 28]
�[32m[1206 16:21:18 @registry.py:129]�[0m group1/block2/conv2 output: [None, 128, 28, 28]
�[32m[1206 16:21:18 @registry.py:121]�[0m group1/block2/conv3 input: [None, 128, 28, 28]
�[32m[1206 16:21:18 @registry.py:129]�[0m group1/block2/conv3 output: [None, 512, 28, 28]
�[32m[1206 16:21:18 @registry.py:121]�[0m group1/block3/conv1 input: [None, 512, 28, 28]
�[32m[1206 16:21:18 @registry.py:129]�[0m group1/block3/conv1 output: [None, 128, 28, 28]
�[32m[1206 16:21:18 @registry.py:121]�[0m group1/block3/conv2 input: [None, 128, 28, 28]
�[32m[1206 16:21:18 @registry.py:129]�[0m group1/block3/conv2 output: [None, 128, 28, 28]
�[32m[1206 16:21:18 @registry.py:121]�[0m group1/block3/conv3 input: [None, 128, 28, 28]
�[32m[1206 16:21:18 @registry.py:129]�[0m group1/block3/conv3 output: [None, 512, 28, 28]
�[32m[1206 16:21:18 @registry.py:121]�[0m group2/block0/conv1 input: [None, 512, 28, 28]
�[32m[1206 16:21:18 @registry.py:129]�[0m group2/block0/conv1 output: [None, 256, 28, 28]
�[32m[1206 16:21:18 @registry.py:121]�[0m group2/block0/conv2 input: [None, 256, 28, 28]
�[32m[1206 16:21:18 @registry.py:129]�[0m group2/block0/conv2 output: [None, 256, 14, 14]
�[32m[1206 16:21:18 @registry.py:121]�[0m group2/block0/conv3 input: [None, 256, 14, 14]
�[32m[1206 16:21:18 @registry.py:129]�[0m group2/block0/conv3 output: [None, 1024, 14, 14]
�[32m[1206 16:21:18 @registry.py:121]�[0m group2/block0/convshortcut input: [None, 512, 28, 28]
�[32m[1206 16:21:18 @registry.py:129]�[0m group2/block0/convshortcut output: [None, 1024, 14, 14]
�[32m[1206 16:21:18 @registry.py:121]�[0m group2/block1/conv1 input: [None, 1024, 14, 14]
�[32m[1206 16:21:18 @registry.py:129]�[0m group2/block1/conv1 output: [None, 256, 14, 14]
�[32m[1206 16:21:18 @registry.py:121]�[0m group2/block1/conv2 input: [None, 256, 14, 14]
�[32m[1206 16:21:18 @registry.py:129]�[0m group2/block1/conv2 output: [None, 256, 14, 14]
�[32m[1206 16:21:18 @registry.py:121]�[0m group2/block1/conv3 input: [None, 256, 14, 14]
�[32m[1206 16:21:18 @registry.py:129]�[0m group2/block1/conv3 output: [None, 1024, 14, 14]
�[32m[1206 16:21:18 @registry.py:121]�[0m group2/block2/conv1 input: [None, 1024, 14, 14]
�[32m[1206 16:21:18 @registry.py:129]�[0m group2/block2/conv1 output: [None, 256, 14, 14]
�[32m[1206 16:21:18 @registry.py:121]�[0m group2/block2/conv2 input: [None, 256, 14, 14]
�[32m[1206 16:21:18 @registry.py:129]�[0m group2/block2/conv2 output: [None, 256, 14, 14]
�[32m[1206 16:21:18 @registry.py:121]�[0m group2/block2/conv3 input: [None, 256, 14, 14]
�[32m[1206 16:21:18 @registry.py:129]�[0m group2/block2/conv3 output: [None, 1024, 14, 14]
�[32m[1206 16:21:18 @registry.py:121]�[0m group2/block3/conv1 input: [None, 1024, 14, 14]
�[32m[1206 16:21:18 @registry.py:129]�[0m group2/block3/conv1 output: [None, 256, 14, 14]
�[32m[1206 16:21:18 @registry.py:121]�[0m group2/block3/conv2 input: [None, 256, 14, 14]
�[32m[1206 16:21:18 @registry.py:129]�[0m group2/block3/conv2 output: [None, 256, 14, 14]
�[32m[1206 16:21:18 @registry.py:121]�[0m group2/block3/conv3 input: [None, 256, 14, 14]
�[32m[1206 16:21:18 @registry.py:129]�[0m group2/block3/conv3 output: [None, 1024, 14, 14]
�[32m[1206 16:21:18 @registry.py:121]�[0m group2/block4/conv1 input: [None, 1024, 14, 14]
�[32m[1206 16:21:19 @registry.py:129]�[0m group2/block4/conv1 output: [None, 256, 14, 14]
�[32m[1206 16:21:19 @registry.py:121]�[0m group2/block4/conv2 input: [None, 256, 14, 14]
�[32m[1206 16:21:19 @registry.py:129]�[0m group2/block4/conv2 output: [None, 256, 14, 14]
�[32m[1206 16:21:19 @registry.py:121]�[0m group2/block4/conv3 input: [None, 256, 14, 14]
�[32m[1206 16:21:19 @registry.py:129]�[0m group2/block4/conv3 output: [None, 1024, 14, 14]
�[32m[1206 16:21:19 @registry.py:121]�[0m group2/block5/conv1 input: [None, 1024, 14, 14]
�[32m[1206 16:21:19 @registry.py:129]�[0m group2/block5/conv1 output: [None, 256, 14, 14]
�[32m[1206 16:21:19 @registry.py:121]�[0m group2/block5/conv2 input: [None, 256, 14, 14]
�[32m[1206 16:21:19 @registry.py:129]�[0m group2/block5/conv2 output: [None, 256, 14, 14]
�[32m[1206 16:21:19 @registry.py:121]�[0m group2/block5/conv3 input: [None, 256, 14, 14]
�[32m[1206 16:21:19 @registry.py:129]�[0m group2/block5/conv3 output: [None, 1024, 14, 14]
�[32m[1206 16:21:19 @registry.py:121]�[0m group3/block0/conv1 input: [None, 1024, 14, 14]
�[32m[1206 16:21:19 @registry.py:129]�[0m group3/block0/conv1 output: [None, 512, 14, 14]
�[32m[1206 16:21:19 @registry.py:121]�[0m group3/block0/conv2 input: [None, 512, 14, 14]
�[32m[1206 16:21:19 @registry.py:129]�[0m group3/block0/conv2 output: [None, 512, 7, 7]
�[32m[1206 16:21:19 @registry.py:121]�[0m group3/block0/conv3 input: [None, 512, 7, 7]
�[32m[1206 16:21:19 @registry.py:129]�[0m group3/block0/conv3 output: [None, 2048, 7, 7]
�[32m[1206 16:21:19 @registry.py:121]�[0m group3/block0/convshortcut input: [None, 1024, 14, 14]
�[32m[1206 16:21:19 @registry.py:129]�[0m group3/block0/convshortcut output: [None, 2048, 7, 7]
�[32m[1206 16:21:19 @registry.py:121]�[0m group3/block1/conv1 input: [None, 2048, 7, 7]
�[32m[1206 16:21:19 @registry.py:129]�[0m group3/block1/conv1 output: [None, 512, 7, 7]
�[32m[1206 16:21:19 @registry.py:121]�[0m group3/block1/conv2 input: [None, 512, 7, 7]
�[32m[1206 16:21:19 @registry.py:129]�[0m group3/block1/conv2 output: [None, 512, 7, 7]
�[32m[1206 16:21:19 @registry.py:121]�[0m group3/block1/conv3 input: [None, 512, 7, 7]
�[32m[1206 16:21:19 @registry.py:129]�[0m group3/block1/conv3 output: [None, 2048, 7, 7]
�[32m[1206 16:21:19 @registry.py:121]�[0m group3/block2/conv1 input: [None, 2048, 7, 7]
�[32m[1206 16:21:19 @registry.py:129]�[0m group3/block2/conv1 output: [None, 512, 7, 7]
�[32m[1206 16:21:19 @registry.py:121]�[0m group3/block2/conv2 input: [None, 512, 7, 7]
�[32m[1206 16:21:19 @registry.py:129]�[0m group3/block2/conv2 output: [None, 512, 7, 7]
�[32m[1206 16:21:19 @registry.py:121]�[0m group3/block2/conv3 input: [None, 512, 7, 7]
�[32m[1206 16:21:19 @registry.py:129]�[0m group3/block2/conv3 output: [None, 2048, 7, 7]
�[32m[1206 16:21:19 @registry.py:121]�[0m gap input: [None, 2048, 7, 7]
�[32m[1206 16:21:19 @registry.py:129]�[0m gap output: [None, 2048]
�[32m[1206 16:21:19 @registry.py:121]�[0m linear input: [None, 2048]
�[32m[1206 16:21:19 @registry.py:129]�[0m linear output: [None, 1000]
�[32m[1206 16:21:19 @regularize.py:90]�[0m regularize_cost() found 54 variables to regularize.
�[32m[1206 16:21:19 @regularize.py:19]�[0m The following tensors will be regularized: conv0/W:0, group0/block0/conv1/W:0, group0/block0/conv2/W:0, group0/block0/conv3/W:0, group0/block0/convshortcut/W:0, group0/block1/conv1/W:0, group0/block1/conv2/W:0, group0/block1/conv3/W:0, group0/block2/conv1/W:0, group0/block2/conv2/W:0, group0/block2/conv3/W:0, group1/block0/conv1/W:0, group1/block0/conv2/W:0, group1/block0/conv3/W:0, group1/block0/convshortcut/W:0, group1/block1/conv1/W:0, group1/block1/conv2/W:0, group1/block1/conv3/W:0, group1/block2/conv1/W:0, group1/block2/conv2/W:0, group1/block2/conv3/W:0, group1/block3/conv1/W:0, group1/block3/conv2/W:0, group1/block3/conv3/W:0, group2/block0/conv1/W:0, group2/block0/conv2/W:0, group2/block0/conv3/W:0, group2/block0/convshortcut/W:0, group2/block1/conv1/W:0, group2/block1/conv2/W:0, group2/block1/conv3/W:0, group2/block2/conv1/W:0, group2/block2/conv2/W:0, group2/block2/conv3/W:0, group2/block3/conv1/W:0, group2/block3/conv2/W:0, group2/block3/conv3/W:0, group2/block4/conv1/W:0, group2/block4/conv2/W:0, group2/block4/conv3/W:0, group2/block5/conv1/W:0, group2/block5/conv2/W:0, group2/block5/conv3/W:0, group3/block0/conv1/W:0, group3/block0/conv2/W:0, group3/block0/conv3/W:0, group3/block0/convshortcut/W:0, group3/block1/conv1/W:0, group3/block1/conv2/W:0, group3/block1/conv3/W:0, group3/block2/conv1/W:0, group3/block2/conv2/W:0, group3/block2/conv3/W:0, linear/W:0
�[32m[1206 16:21:20 @training.py:112]�[0m Building graph for training tower 1 on device /gpu:1 ...
�[32m[1206 16:21:21 @regularize.py:90]�[0m regularize_cost() found 54 variables to regularize.
�[32m[1206 16:21:22 @training.py:112]�[0m Building graph for training tower 2 on device /gpu:2 ...
�[32m[1206 16:21:23 @regularize.py:90]�[0m regularize_cost() found 54 variables to regularize.
�[32m[1206 16:21:24 @training.py:112]�[0m Building graph for training tower 3 on device /gpu:3 ...
�[32m[1206 16:21:25 @regularize.py:90]�[0m regularize_cost() found 54 variables to regularize.
�[32m[1206 16:21:26 @training.py:112]�[0m Building graph for training tower 4 on device /gpu:4 ...
�[32m[1206 16:21:27 @regularize.py:90]�[0m regularize_cost() found 54 variables to regularize.
�[32m[1206 16:21:28 @training.py:112]�[0m Building graph for training tower 5 on device /gpu:5 ...
�[32m[1206 16:21:29 @regularize.py:90]�[0m regularize_cost() found 54 variables to regularize.
�[32m[1206 16:21:30 @training.py:112]�[0m Building graph for training tower 6 on device /gpu:6 ...
�[32m[1206 16:21:31 @regularize.py:90]�[0m regularize_cost() found 54 variables to regularize.
�[32m[1206 16:21:32 @training.py:112]�[0m Building graph for training tower 7 on device /gpu:7 ...
�[32m[1206 16:21:33 @regularize.py:90]�[0m regularize_cost() found 54 variables to regularize.
�[32m[1206 16:21:34 @training.py:112]�[0m Building graph for training tower 8 on device /gpu:8 ...
�[32m[1206 16:21:35 @regularize.py:90]�[0m regularize_cost() found 54 variables to regularize.
�[32m[1206 16:21:36 @training.py:112]�[0m Building graph for training tower 9 on device /gpu:9 ...
�[32m[1206 16:21:37 @regularize.py:90]�[0m regularize_cost() found 54 variables to regularize.
�[32m[1206 16:21:38 @utils.py:360]�[0m Will pack 161 gradients of total dimension=25557032 into 10 splits.
	</description>
	<comments>
		<comment id='1' author='clampert' date='2018-12-06T16:58:19Z'>
		The mode was now changed to "nccl".
Actually I don't have such machine to test if "nccl" works for 10 GPUs. Let me know if anything goes wrong.
		</comment>
	</comments>
</bug>