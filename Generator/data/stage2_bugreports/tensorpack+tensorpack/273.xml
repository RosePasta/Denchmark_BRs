<bug id='273' author='eyaler' open_date='2017-05-18T08:38:15Z' closed_time='2017-05-19T00:09:18Z'>
	<summary>error when running training multiple times</summary>
	<description>
i want to run an example multiple times in a loop, each time from scratch.
to recreate my issue take InfoGAN-mnist.py and after
GANTrainer(config).train()
add:
tf.reset_default_graph(); config = get_config(); GANTrainer(config).train()
on the second training, i get:
ValueError: Tensor("EMA/decay:0", shape=(), dtype=float32) must be from the same graph as Tensor("truediv:0", shape=(), dtype=float32).
	</description>
	<comments>
		<comment id='1' author='eyaler' date='2017-05-18T16:44:38Z'>
		wow! how did you pinpoint the problem? I was debugging for hours...
for completeness, in my usecase i also have to do between runs:
import logging; logging.shutdown()
		</comment>
		<comment id='2' author='eyaler' date='2017-05-18T17:28:10Z'>
		What's the issue with logging?
I do find that you may need to set_logger_dir to a different place otherwise the second training would see the old logs and thinks the training has finished already.
		</comment>
		<comment id='3' author='eyaler' date='2017-05-18T18:22:34Z'>
		In windows i get error 32 file in use
		</comment>
		<comment id='4' author='eyaler' date='2017-05-18T18:28:11Z'>
		Even when you've set logger_dir to a different place? Do you know which file it is referring to?
I did find that I should've unload the existing file handler when logger_dir was changed. But this seems unrelated.
		</comment>
		<comment id='5' author='eyaler' date='2017-05-18T19:01:26Z'>
		this is when trying to write to the same log file with set_logger_dir(action='d')
		</comment>
		<comment id='6' author='eyaler' date='2017-05-18T19:03:35Z'>
		Oh I see. It attempts to delete the old log file while it is open. It is allowed in linux.
I'll see if I can fix it.
		</comment>
		<comment id='7' author='eyaler' date='2017-05-18T23:07:14Z'>
		Don't have a windows to test, but from strace log it looks like now all files are closed before performing the delete action.
&lt;denchmark-code&gt;[pid 14405] open("train_log/mnist-convnet/stat.json.tmp", O_WRONLY|O_CREAT|O_TRUNC|O_CLOEXEC, 0666) = 11&lt;/home/wyx/Work/projects/tensorpack/examples/train_log/mnist-convnet/stat.json.tmp&gt;                                             
[pid 14405] close(11&lt;/home/wyx/Work/projects/tensorpack/examples/train_log/mnist-convnet/stat.json.tmp&gt;) = 0                                                                                                                            
[0518 16:02:39 @monitor.py:294] accuracy: 0.29745                                                                                                                                                                                       
[0518 16:02:39 @monitor.py:294] cross_entropy_loss: 1.9657                                                                                                                                                                              
[0518 16:02:39 @monitor.py:294] lr: 0.001                                                                                                                                                                                               
[0518 16:02:39 @monitor.py:294] param-summary/conv0/W-rms: 0.46391                                                                                                                                                                      
[0518 16:02:39 @monitor.py:294] param-summary/conv1/W-rms: 0.083028                                                                                                                                                                     
[0518 16:02:39 @monitor.py:294] param-summary/conv2/W-rms: 0.084169                                                                                                                                                                     
[0518 16:02:39 @monitor.py:294] param-summary/conv3/W-rms: 0.083865                                                                                                                                                                     
[0518 16:02:39 @monitor.py:294] param-summary/fc0/W-rms: 0.034486                                                                                                                                                                       
[0518 16:02:39 @monitor.py:294] param-summary/fc1/W-rms: 0.062386                                                                                                                                                                       
[0518 16:02:39 @monitor.py:294] regularize_loss: 0.0049347                                                                                                                                                                              
[0518 16:02:39 @monitor.py:294] total_cost: 1.9706                                                                                                                                                                                      
[0518 16:02:39 @monitor.py:294] train_error: 0.70253                                                                                                                                                                                    
[pid 14405] close(10&lt;/home/wyx/Work/projects/tensorpack/examples/train_log/mnist-convnet/events.out.tfevents.1495148557.KeepLearning&gt;) = 0                                                                                              
[pid 14478] +++ exited with 0 +++                                                                                                                                                                                                       
[pid 14405] close(9&lt;/home/wyx/Work/projects/tensorpack/examples/train_log/mnist-convnet/log.log&gt;) = 0                                                                                                                                   
[0518 16:02:39 @logger.py:92] WRN Directory train_log/mnist-convnet exists! Please either backup/delete it, or use a new directory.                                                                                                     
[0518 16:02:39 @logger.py:94] WRN If you're resuming from a previous run you can choose to keep it.                                                                                                                                     
[0518 16:02:39 @logger.py:95] Select Action: k (keep) / b (backup) / d (delete) / n (new):   
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='eyaler' date='2017-05-19T00:09:18Z'>
		i can confirm this fixed the logging issue.
		</comment>
		<comment id='9' author='eyaler' date='2017-05-21T21:09:29Z'>
		Seems to have caused a new issue. &lt;denchmark-link:https://github.com/tensorpack/tensorpack/issues/276&gt;#276&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>