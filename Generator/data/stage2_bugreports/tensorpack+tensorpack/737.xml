<bug id='737' author='deeplearningmachine' open_date='2018-04-25T19:50:44Z' closed_time='2018-04-28T14:40:45Z'>
	<summary>error message from running cifar10-resnet.py</summary>
	<description>
I tried to run cifar10-resnet.py, but got the following message:
File "cifar10-resnet.py", line 174, in 
launch_train_with_config(config, SyncMultiGPUTrainerParameterServer(nr_gpu))
...
tf.v1.4.0/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py", line 98, in init
raise TypeError('Keyword argument not understood:', kwarg)
TypeError: ('Keyword argument not understood:', 'virtual_batch_size')
	</description>
	<comments>
		<comment id='1' author='deeplearningmachine' date='2018-04-25T20:36:39Z'>
		Thanks for reporting. Looks like I was using a feature unavailable in TF 1.4. I'll update the code soon.
		</comment>
		<comment id='2' author='deeplearningmachine' date='2018-04-25T21:41:44Z'>
		virtual_batch_size is only available after TF 1.5. Now the code won't touch this option if running under TF1.4.
		</comment>
		<comment id='3' author='deeplearningmachine' date='2018-04-26T14:22:42Z'>
		Thanks! but it seems I still got the same error message. Also, on another machine I got a different message: /lib/python2.7/site-packages/tensorpack/dataflow/common.py", line 142, in _aggregate_batch
raise TypeError("Unsupported type to batch: {}".format(type(dt)))
TypeError: Unsupported type to batch: &lt;type 'long'&gt;
I use TF1.4+Python2.7
		</comment>
		<comment id='4' author='deeplearningmachine' date='2018-04-26T16:46:25Z'>
		The "long" issue is a python2 specific issue that should be fixed in the last commit.
Could you post your error log?
		</comment>
		<comment id='5' author='deeplearningmachine' date='2018-04-26T18:37:10Z'>
		ESC[32m[0426 10:25:23 @logger.py:74]ESC[0m Argv: imagenet-resnet.py --data ../../../imageNet2012/ --gpu 2,3 -d 18
ESC[32m[0426 10:25:23 @fs.py:89]ESC[0m ESC[5mESC[31mWRNESC[0m Env var $TENSORPACK_DATASET not set, using /home/testing/tensorpack_data for datasets.
ESC[32m[0426 10:25:24 @parallel.py:290]ESC[0m [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
ESC[32m[0426 10:25:24 @param.py:195]ESC[0m Use train_log/imagenet-resnet/hyper.txt to set hyperparam: 'learning_rate'.
ESC[32m[0426 10:25:24 @config.py:166]ESC[0m ESC[5mESC[31mWRNESC[0m TrainConfig.nr_tower was deprecated! Set the number of GPUs on the trainer instead!
ESC[32m[0426 10:25:24 @config.py:167]ESC[0m ESC[5mESC[31mWRNESC[0m See https://github.com/&lt;denchmark-link:https://github.com/tensorpack/tensorpack/issues/458&gt;/issues/458&lt;/denchmark-link&gt;
 for more information.
ESC[32m[0426 10:25:24 @base.py:345]ESC[0m ESC[5mESC[31mWRNESC[0m You're calling new trainers with old trainer API!
ESC[32m[0426 10:25:24 @base.py:346]ESC[0m ESC[5mESC[31mWRNESC[0m Now it returns the old trainer for you, please switch to use new trainers soon!
ESC[32m[0426 10:25:24 @base.py:347]ESC[0m ESC[5mESC[31mWRNESC[0m See https://github.com/&lt;denchmark-link:https://github.com/tensorpack/tensorpack/issues/458&gt;/issues/458&lt;/denchmark-link&gt;
 for more information.
ESC[32m[0426 10:25:25 @input_source.py:194]ESC[0m Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...
ESC[32m[0426 10:25:25 @training.py:42]ESC[0m [DataParallel] Training a model of 2 towers.
ESC[32m[0426 10:25:25 @training.py:102]ESC[0m Building graph for training tower 0 ...
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m conv0 input: [None, 3, 448, 224]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m conv0 output: [None, 64, 224, 112]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m pool0 input: [None, 64, 224, 112]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m pool0 output: [None, 64, 112, 56]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group0/block0/conv1 input: [None, 64, 112, 56]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group0/block0/conv1 output: [None, 64, 112, 56]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group0/block0/conv2 input: [None, 64, 112, 56]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group0/block0/conv2 output: [None, 64, 112, 56]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group0/block1/conv1 input: [None, 64, 112, 56]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group0/block1/conv1 output: [None, 64, 112, 56]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group0/block1/conv2 input: [None, 64, 112, 56]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group0/block1/conv2 output: [None, 64, 112, 56]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group1/block0/conv1 input: [None, 64, 112, 56]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group1/block0/conv1 output: [None, 128, 56, 28]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group1/block0/conv2 input: [None, 128, 56, 28]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group1/block0/conv2 output: [None, 128, 56, 28]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group1/block0/convshortcut input: [None, 64, 112, 56]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group1/block0/convshortcut output: [None, 128, 56, 28]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group1/block1/conv1 input: [None, 128, 56, 28]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group1/block1/conv1 output: [None, 128, 56, 28]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group1/block1/conv2 input: [None, 128, 56, 28]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group1/block1/conv2 output: [None, 128, 56, 28]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group2/block0/conv1 input: [None, 128, 56, 28]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group2/block0/conv1 output: [None, 256, 28, 14]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group2/block0/conv2 input: [None, 256, 28, 14]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group2/block0/conv2 output: [None, 256, 28, 14]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group2/block0/convshortcut input: [None, 128, 56, 28]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group2/block0/convshortcut output: [None, 256, 28, 14]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group2/block1/conv1 input: [None, 256, 28, 14]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group2/block1/conv1 output: [None, 256, 28, 14]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group2/block1/conv2 input: [None, 256, 28, 14]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group2/block1/conv2 output: [None, 256, 28, 14]
ESC[32m[0426 10:25:26 @registry.py:121]ESC[0m group3/block0/conv1 input: [None, 256, 28, 14]
ESC[32m[0426 10:25:26 @registry.py:129]ESC[0m group3/block0/conv1 output: [None, 512, 14, 7]
ESC[32m[0426 10:25:26 @registry.py:121]ESC[0m group3/block0/conv2 input: [None, 512, 14, 7]
ESC[32m[0426 10:25:26 @registry.py:129]ESC[0m group3/block0/conv2 output: [None, 512, 14, 7]
ESC[32m[0426 10:25:26 @registry.py:121]ESC[0m group3/block0/convshortcut input: [None, 256, 28, 14]
ESC[32m[0426 10:25:26 @registry.py:129]ESC[0m group3/block0/convshortcut output: [None, 512, 14, 7]
ESC[32m[0426 10:25:26 @registry.py:121]ESC[0m group3/block1/conv1 input: [None, 512, 14, 7]
ESC[32m[0426 10:25:26 @registry.py:129]ESC[0m group3/block1/conv1 output: [None, 512, 14, 7]
ESC[32m[0426 10:25:26 @registry.py:121]ESC[0m group3/block1/conv2 input: [None, 512, 14, 7]
ESC[32m[0426 10:25:26 @registry.py:129]ESC[0m group3/block1/conv2 output: [None, 512, 14, 7]
ESC[32m[0426 10:25:26 @registry.py:121]ESC[0m gap input: [None, 512, 14, 7]
ESC[32m[0426 10:25:26 @registry.py:129]ESC[0m gap output: [None, 512]
ESC[32m[0426 10:25:26 @registry.py:121]ESC[0m linear input: [None, 512]
ESC[32m[0426 10:25:26 @registry.py:129]ESC[0m linear output: [None, 1000]
ESC[32m[0426 10:25:26 @registry.py:121]ESC[0m linear_2 input: [None, 512]
ESC[32m[0426 10:25:26 @registry.py:129]ESC[0m linear_2 output: [None, 1000]
ESC[32m[0426 10:25:26 @regularize.py:88]ESC[0m regularize_cost() found 22 variables to regularize.
ESC[32m[0426 10:25:26 @regularize.py:19]ESC[0m The following tensors will be regularized: conv0/W:0, group0/block0/conv1/W:0, group0/block0/conv2/W:0, group0/block1/conv1/W:0, group0/block1/conv2/W:0, group1/block0/conv1/W:0, group1/bloc
k0/conv2/W:0, group1/block0/convshortcut/W:0, group1/block1/conv1/W:0, group1/block1/conv2/W:0, group2/block0/conv1/W:0, group2/block0/conv2/W:0, group2/block0/convshortcut/W:0, group2/block1/conv1/W:0, group2/block1/conv2/W:0, group3/bl
ock0/conv1/W:0, group3/block0/conv2/W:0, group3/block0/convshortcut/W:0, group3/block1/conv1/W:0, group3/block1/conv2/W:0, linear/W:0, linear_2/W:0
ESC[32m[0426 10:25:26 @training.py:102]ESC[0m Building graph for training tower 1 ...
group3/block0/conv2/W:0          [3, 3, 512, 512]  2359296
group3/block0/convshortcut/W:0   [1, 1, 256, 512]   131072
group3/block1/preact/bn/gamma:0  [512]                 512
group3/block1/preact/bn/beta:0   [512]                 512
group3/block1/conv1/W:0          [3, 3, 512, 512]  2359296
group3/block1/conv1/bn/gamma:0   [512]                 512
group3/block1/conv1/bn/beta:0    [512]                 512
group3/block1/conv2/W:0          [3, 3, 512, 512]  2359296
bnlast/bn/gamma:0                [512]                 512
bnlast/bn/beta:0                 [512]                 512
linear/W:0                       [512, 1000]        512000
linear/b:0                       [1000]               1000
linear_2/W:0                     [512, 1000]        512000
linear_2/b:0                     [1000]               1000ESC[36m
Total #vars=58, #params=12200720, size=46.54MBESC[0m
ESC[32m[0426 10:25:27 @base.py:142]ESC[0m Setup callbacks graph ...
ESC[32m[0426 10:25:27 @predict.py:42]ESC[0m Building predictor tower 'InferenceTower' on device /gpu:0 ...
ESC[32m[0426 10:25:27 @collection.py:165]ESC[0m These collections were modified but restored in InferenceTower: (tf.GraphKeys.SUMMARIES: 7-&gt;8)
ESC[32m[0426 10:25:27 @summary.py:39]ESC[0m Maintain moving average summary of 4 tensors in collection MOVING_SUMMARY_OPS.
ESC[32m[0426 10:25:27 @summary.py:76]ESC[0m Summarizing collection 'summaries' of size 7.
ESC[32m[0426 10:25:27 @graph.py:92]ESC[0m Applying collection UPDATE_OPS of 34 ops.
ESC[32m[0426 10:25:32 @base.py:147]ESC[0m Creating the session ...
ESC[32m[0426 10:25:39 @base.py:151]ESC[0m Initializing the session ...
ESC[32m[0426 10:25:39 @base.py:158]ESC[0m Graph Finalized.
ESC[32m[0426 10:25:40 @inference_runner.py:100]ESC[0m InferenceRunner will eval 782 iterations
ESC[32m[0426 10:25:40 @concurrency.py:38]ESC[0m Starting EnqueueThread QueueInput/input_queue ...
ESC[32m[0426 10:25:40 @base.py:192]ESC[0m Start Epoch 1 ...
ESC[32m[0426 10:25:40 @input_source.py:502]ESC[0m Pre-filling StagingArea ...
ESC[32m[0426 10:25:40 @input_source.py:143]ESC[0m ESC[4mESC[5mESC[31mERRESC[0m Exception in EnqueueThread QueueInput/input_queue:
Traceback (most recent call last):
File "/home/testing/condaEnv2/lib/python2.7/site-packages/tensorpack/input_source/input_source.py", line 133, in run
dp = next(self._itr)
File "/home/testing/condaEnv2/lib/python2.7/site-packages/tensorpack/dataflow/common.py", line 339, in get_data
for dp in self.ds.get_data():
File "/home/testing/condaEnv2/lib/python2.7/site-packages/tensorpack/dataflow/common.py", line 274, in get_data
for dp in self.ds.get_data():
File "/home/testing/condaEnv2/lib/python2.7/site-packages/tensorpack/dataflow/common.py", line 119, in get_data
yield BatchData._aggregate_batch(holder, self.use_list)
File "/home/testing/condaEnv2/lib/python2.7/site-packages/tensorpack/dataflow/common.py", line 142, in _aggregate_batch
raise TypeError("Unsupported type to batch: {}".format(type(dt)))
TypeError: Unsupported type to batch: &lt;type 'long'&gt;
ESC[32m[0426 10:25:40 @input_source.py:149]ESC[0m EnqueueThread QueueInput/input_queue Exited.
ESC[32m[0426 10:25:40 @base.py:208]ESC[0m Training was stopped.
		</comment>
		<comment id='6' author='deeplearningmachine' date='2018-04-26T19:24:41Z'>
		The long type problem should be fixed by the early commit. I was asking about other issues (if any).
		</comment>
		<comment id='7' author='deeplearningmachine' date='2018-04-28T14:40:31Z'>
		Sorry, I used some old codes. It works perfectly now.
		</comment>
	</comments>
</bug>