<bug id='124' author='ppwwyyxx' open_date='2017-01-28T16:38:54Z' closed_time='2017-02-03T01:25:10Z'>
	<summary>MultiGPU ResNet training not working with TF after 12.1</summary>
	<description>
With TF nightly, imagenet-resnet.py on 4 TitanX gives 95% top1 error after the first epoch, but the number was about 75% with TF 12.1 release.
The final performance is also 10% worse than before.
Most of the APIs in tensorpack was already adopted to TF 1.0, so it's kind of awkward now. If you need multi-GPU training you can try TF 12.1, but maybe you'll need to check out an older version of tensorpack as well if there is an API issue.
Very likely this is related to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/7038&gt;tensorflow/tensorflow#7038&lt;/denchmark-link&gt;
. Will do some more tests to figure out.
	</description>
	<comments>
		<comment id='1' author='ppwwyyxx' date='2017-01-28T18:29:00Z'>
		Tried training with one GPU of batch_size 64. Get 95% top1 error after the first epoch, with TF 12.1.
So it's probably the upstream dequeue issue which makes multiple GPUs behave the same as one.
		</comment>
		<comment id='2' author='ppwwyyxx' date='2017-02-02T06:57:58Z'>
		Is this still a problem or you have resolved this? If you do want to checkout an earlier version of resnet for TF 12.1 do you recollect which git commit number to checkout?
Thanks
		</comment>
		<comment id='3' author='ppwwyyxx' date='2017-02-02T08:41:41Z'>
		It's a TF bug that seems to be fixed now. But I haven't got time to test it.
		</comment>
		<comment id='4' author='ppwwyyxx' date='2017-02-02T16:07:23Z'>
		It seems to be fixed.
With the earlier version of TF, I was also getting 95% error after 1st epoch before:
&lt;denchmark-code&gt;[0202 02:56:58 @stats.py:113] input_queue_size: 50
[0202 02:56:58 @stats.py:113] l2_regularize_loss: 1.2286
[0202 02:56:58 @stats.py:113] learning_rate: 0.1
[0202 02:56:58 @stats.py:113] train-error-top1: 0.96814
[0202 02:56:58 @stats.py:113] train-error-top5: 0.8822
[0202 02:56:58 @stats.py:113] val-error-top1: 0.9553
[0202 02:56:58 @stats.py:113] val-error-top5: 0.86338
[0202 02:56:58 @stats.py:113] xentropy-loss: 5.7036
&lt;/denchmark-code&gt;

But with today's build, I get after 1st epoch:
&lt;denchmark-code&gt;[0202 11:03:23 @stats.py:113] input_queue_size: 48.341
[0202 11:03:23 @stats.py:113] l2_regularize_loss: 1.2029
[0202 11:03:23 @stats.py:113] learning_rate: 0.1
[0202 11:03:23 @stats.py:113] train-error-top1: 0.77943
[0202 11:03:23 @stats.py:113] train-error-top5: 0.55875
[0202 11:03:23 @stats.py:113] val-error-top1: 0.80062
[0202 11:03:23 @stats.py:113] val-error-top5: 0.58646
[0202 11:03:23 @stats.py:113] xentropy-loss: 3.9533
&lt;/denchmark-code&gt;

Thanks for figuring this, &lt;denchmark-link:https://github.com/ppwwyyxx&gt;@ppwwyyxx&lt;/denchmark-link&gt;
 !
		</comment>
		<comment id='5' author='ppwwyyxx' date='2017-02-03T01:25:10Z'>
		I've also started a job to train it from scratch. At epoch 60, resnet-18 trained with 4 GPUs is getting 35% top1 val error. This is similar to the number I had before.
Although it haven't finished I guess the issue was resolved.
		</comment>
		<comment id='6' author='ppwwyyxx' date='2017-02-03T01:27:35Z'>
		Also, just curious, &lt;denchmark-link:https://github.com/rohitgirdhar&gt;@rohitgirdhar&lt;/denchmark-link&gt;
 From the log you seem to have very good hard disk, to be able to train without data loading overhead. Is it a RAID or SSD or anything? If it is a RAID could you tell me the configuration?  I don't have resources to test which setting is enough for the current implementation.
		</comment>
		<comment id='7' author='ppwwyyxx' date='2017-02-03T03:34:27Z'>
		I think it's just a standard SSD, I don't think we have any RAID configured on it. Though I'm far from an expert on this. PM me if you need specific model details, and I can try to figure that out.
		</comment>
		<comment id='8' author='ppwwyyxx' date='2017-02-03T06:14:23Z'>
		Sorry for polluting this thread. But I am curios: What's the speed (it/sec or sec/iter) when training ResNet on your machine on imagenet (and what's the batch size, num gpus)? I get a very slow pre-fetching on my hardware here.
&lt;denchmark-link:https://github.com/rohitgirdhar&gt;@rohitgirdhar&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ppwwyyxx&gt;@ppwwyyxx&lt;/denchmark-link&gt;

And you are really reading from all these cluttered single imagefiles with their original size from the tar files? I just currently try to figure out the most efficient way.
		</comment>
		<comment id='9' author='ppwwyyxx' date='2017-02-03T07:37:53Z'>
		So my images are resized to 256px, and stored as individual JPEGs on the disk (not in a tarfile). With 256 batch size over 4 GPUs, I was getting around 1 it/sec.
		</comment>
		<comment id='10' author='ppwwyyxx' date='2017-02-03T09:39:22Z'>
		I was using the original ResNet18 script, the original ILSVRC12 dataset.
Hardware: 4 TitanX, normal hard disk (cannot fill the queue).
Speed: 2.3~2.5 it/s.
It will be faster if you 1. use a single file such as lmdb and 2. don't globally shuffle all training data, but maintain a pool to locally shuffle the data.
I think I'll put a page in docs about this when I have time to test the numbers.
		</comment>
		<comment id='11' author='ppwwyyxx' date='2017-02-03T09:49:17Z'>
		I converted the entire tar file into lmdb. This gives 788GB for the training (without cropping) and is slow as hell when reading. Even sequential reading from tar is very slow (reads 17 images per second from ssd). That's why I was asking.
Thank you both for providing some concrete numbers.
		</comment>
		<comment id='12' author='ppwwyyxx' date='2017-02-03T10:16:39Z'>
		I think you should store jpeg string instead of the raw array. The decompression is faster than reading.
I've converted to lmdb before and the whole db should be less than 150G.
		</comment>
		<comment id='13' author='ppwwyyxx' date='2017-02-03T10:31:45Z'>
		I unpacked all JPEGS to SSD without cropping and now I can read 422 images per second.
How wrong I was thinking a single file would be faster. Is this roughly the same you get?
from tensorpack import *

ds = dataset.ILSVRC12('/scratch/imagenet', 'train')
ds = MapData(ds, lambda dp: [dp[0][:, :, ::-1]])
ds = TestDataSpeed(ds)
ds.start_test()
		</comment>
		<comment id='14' author='ppwwyyxx' date='2017-02-03T10:35:22Z'>
		Single lmdb file with jpeg string may still be faster. But since you already have an SSD the benefit could be very small.
		</comment>
		<comment id='15' author='ppwwyyxx' date='2017-02-03T10:36:36Z'>
		I used:
&lt;denchmark-link:https://gist.github.com/PatWie/9537acc0e1f24687613856b60dba7eab&gt;https://gist.github.com/PatWie/9537acc0e1f24687613856b60dba7eab&lt;/denchmark-link&gt;

The important part is Line 140.
		</comment>
		<comment id='16' author='ppwwyyxx' date='2017-02-03T10:40:39Z'>
		The lmdb format caffe used is very inefficient. It stores a serialized raw array without image compression.
		</comment>
		<comment id='17' author='ppwwyyxx' date='2017-03-02T09:14:23Z'>
		This problem doesnt seem to be fixed. I am working with TF 1.0.0rc1 and HEAD version of this repo.
Here is what I am getting after first epoch:
&lt;denchmark-code&gt;236 ^[[32m[0227 18:27:56 @base.py:160]^[[0m Start Epoch 1 ...
 237 ^[[32m[0227 18:48:56 @base.py:168]^[[0m Epoch 1 (global_step 5000) finished, time:1259.80 sec.
 238 ^[[32m[0227 18:48:57 @saver.py:65]^[[0m Model saved to resnet34BaselineTrain/model-5000.
 239 ^[[32m[0227 18:54:41 @group.py:43]^[[0m Callbacks took 345.025 sec in total. InferenceRunner: 343.555sec
 240 ^[[32m[0227 18:54:41 @monitor.py:233]^[[0m input_queue_size: 50
 241 ^[[32m[0227 18:54:41 @monitor.py:233]^[[0m l2_regularize_loss: 0.48794
 242 ^[[32m[0227 18:54:41 @monitor.py:233]^[[0m learning_rate: 0.1
 243 ^[[32m[0227 18:54:41 @monitor.py:233]^[[0m train-error-top1: 0.96621
 244 ^[[32m[0227 18:54:41 @monitor.py:233]^[[0m train-error-top5: 0.87891
 245 ^[[32m[0227 18:54:41 @monitor.py:233]^[[0m val-error-top1: 0.96124
 246 ^[[32m[0227 18:54:41 @monitor.py:233]^[[0m val-error-top5: 0.87464
 247 ^[[32m[0227 18:54:41 @monitor.py:233]^[[0m xentropy-loss: 5.6556
&lt;/denchmark-code&gt;

The final error is also off:
&lt;denchmark-code&gt;1548 ^[[32m[0301 18:02:54 @base.py:160]^[[0m Start Epoch 110 ...
1549 ^[[32m[0301 18:23:36 @base.py:168]^[[0m Epoch 110 (global_step 550000) finished, time:1241.72 sec.
1550 ^[[32m[0301 18:23:37 @saver.py:65]^[[0m Model saved to resnet34BaselineTrain/model-550000.
1551 ^[[32m[0301 18:29:02 @group.py:43]^[[0m Callbacks took 326.282 sec in total. InferenceRunner: 325.909sec
1552 ^[[32m[0301 18:29:02 @monitor.py:233]^[[0m input_queue_size: 50
1553 ^[[32m[0301 18:29:02 @monitor.py:233]^[[0m l2_regularize_loss: 0.55047
1554 ^[[32m[0301 18:29:02 @monitor.py:233]^[[0m learning_rate: 1e-05
1555 ^[[32m[0301 18:29:02 @monitor.py:233]^[[0m train-error-top1: 0.33629
1556 ^[[32m[0301 18:29:02 @monitor.py:233]^[[0m train-error-top5: 0.1454
1557 ^[[32m[0301 18:29:02 @monitor.py:233]^[[0m val-error-top1: 0.3068
1558 ^[[32m[0301 18:29:02 @monitor.py:233]^[[0m val-error-top5: 0.11092
1559 ^[[32m[0301 18:29:02 @monitor.py:233]^[[0m xentropy-loss: 1.414
1560 ^[[32m[0301 18:29:02 @input_data.py:124]^[[0m EnqueueThread Exited.
&lt;/denchmark-code&gt;

When I had done this run a while back using TF 0.12 this is what I was getting after the first epoch:
&lt;denchmark-code&gt;254 ^[[32m[1227 12:08:31 @timer.py:46]^[[0m Epoch 1 (global_step 5000) finished, time:1432.42sec.
 255 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m cost: 4.6151
 256 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m input_queue_size: 48.462
 257 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m l2_regularize_loss: 0.62589
 258 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m learning_rate: 0.1
 259 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m train-error-top1: 0.80657
 260 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m train-error-top5: 0.55757
 261 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m val-error-top1: 0.76442
 262 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m val-error-top5: 0.5148
 263 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m xentropy-loss: 3.9892
&lt;/denchmark-code&gt;

		</comment>
		<comment id='18' author='ppwwyyxx' date='2017-03-02T09:39:50Z'>
		rc1 is old. You can use the 1.0 release version.
		</comment>
		<comment id='19' author='ppwwyyxx' date='2017-03-03T01:23:04Z'>
		Using tensorflow-gpu (1.0.0) and still getting val-error-top1: 0.98914 after Epoch 1.
		</comment>
		<comment id='20' author='ppwwyyxx' date='2017-03-03T05:05:15Z'>
		Sorry, looking at the comment in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/7038&gt;tensorflow/tensorflow#7038&lt;/denchmark-link&gt;
, it looks like 1.0 is still not new enough. Maybe you have to use the nightly version or a custom build.
		</comment>
		<comment id='21' author='ppwwyyxx' date='2017-03-03T05:40:47Z'>
		I see. What's the solution for now? Use TF 0.12?
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


 On Mar 2, 2017, at 9:05 PM, Yuxin Wu ***@***.***&gt; wrote:

 Sorry, looking at the comment in tensorflow/tensorflow#7038, it looks like 1.0 is still not new enough.

 —
 You are receiving this because you commented.
 Reply to this email directly, view it on GitHub, or mute the thread.



		</comment>
		<comment id='22' author='ppwwyyxx' date='2017-03-03T05:56:50Z'>
		Use the nightly tensorflow bulid, or build yourself.
		</comment>
	</comments>
</bug>