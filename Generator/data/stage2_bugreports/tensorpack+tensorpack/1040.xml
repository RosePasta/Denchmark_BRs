<bug id='1040' author='elmirador' open_date='2019-01-09T10:57:57Z' closed_time='2019-01-10T07:05:48Z'>
	<summary>Bug Report: BilinearUpSample() only works when channels=1</summary>
	<description>
&lt;denchmark-h:h3&gt;1. What you did:&lt;/denchmark-h&gt;

I wrote a piece of code to test the behavior of BilinearUpSample() layer:
&lt;denchmark-link:https://gist.github.com/elmirador/4816836ca5b35481a43b07109f6dae63&gt;https://gist.github.com/elmirador/4816836ca5b35481a43b07109f6dae63&lt;/denchmark-link&gt;

The layer's code is directly copied from models/pool.py from 


tensorpack/tensorpack/models/pool.py


        Lines 145 to 196
      in
      7bf5581






 def BilinearUpSample(x, shape): 



 """ 



     Deterministic bilinearly-upsample the input images. 



     It is implemented by deconvolution with "BilinearFiller" in Caffe. 



     It is aimed to mimic caffe behavior. 



  



     Args: 



         x (tf.Tensor): a NHWC tensor 



         shape (int): the upsample factor 



  



     Returns: 



         tf.Tensor: a NHWC tensor. 



     """ 



 log_deprecated("BilinearUpsample", "Please implement it in your own code instead!", "2019-03-01") 



 inp_shape = x.shape.as_list() 



 ch = inp_shape[3] 



 assert ch is not None 



 



 shape = int(shape) 



 filter_shape = 2 * shape 



 



 def bilinear_conv_filler(s): 



 """ 



         s: width, height of the conv filter 



         https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/include/caffe/filler.hpp#L219-L268 



         """ 



 f = np.ceil(float(s) / 2) 



 c = float(2 * f - 1 - f % 2) / (2 * f) 



 ret = np.zeros((s, s), dtype='float32') 



 for x in range(s): 



 for y in range(s): 



 ret[x, y] = (1 - abs(x / f - c)) * (1 - abs(y / f - c)) 



 return ret 



 w = bilinear_conv_filler(filter_shape) 



 w = np.repeat(w, ch * ch).reshape((filter_shape, filter_shape, ch, ch)) 



 



 weight_var = tf.constant(w, tf.float32, 



 shape=(filter_shape, filter_shape, ch, ch), 



 name='bilinear_upsample_filter') 



 x = tf.pad(x, [[0, 0], [shape - 1, shape - 1], [shape - 1, shape - 1], [0, 0]], mode='SYMMETRIC') 



 out_shape = tf.shape(x) * tf.constant([1, shape, shape, 1], tf.int32) 



 deconv = tf.nn.conv2d_transpose(x, weight_var, out_shape, 



                                     [1, shape, shape, 1], 'SAME') 



 edge = shape * (shape - 1) 



 deconv = deconv[:, edge:-edge, edge:-edge, :] 



 



 if inp_shape[1]: 



 inp_shape[1] *= shape 



 if inp_shape[2]: 



 inp_shape[2] *= shape 



 deconv.set_shape(inp_shape) 



 return deconv 





&lt;denchmark-h:h3&gt;2. What you observed:&lt;/denchmark-h&gt;

Original Image:
&lt;denchmark-link:https://user-images.githubusercontent.com/3897907/50889932-2437c380-1434-11e9-92ff-25babf24f31d.png&gt;&lt;/denchmark-link&gt;

Upsampled Image (in color):
&lt;denchmark-link:https://user-images.githubusercontent.com/3897907/50890247-c6f04200-1434-11e9-983c-5db985f61fdd.png&gt;&lt;/denchmark-link&gt;

Read the original image as grayscale, then upsample:
&lt;denchmark-link:https://user-images.githubusercontent.com/3897907/50890657-9230ba80-1435-11e9-870d-ce8c59c4b9f0.png&gt;&lt;/denchmark-link&gt;

The reason for this behavior is that conv2d_tranpose() is a convolution. It convolutes on all channels instead of one single channel while bilinear upsampling is a channel-wise operation. To fix it just apply the convolution channel-wise.
I do think it's a good idea to use Caffe's version of bilinear upsampling instead of the one in TF. Sad to see it's going to be deprecated.
The CaffeBilinearUpSample() in the HED example should also be corrected.
&lt;denchmark-h:h3&gt;3. Your environment:&lt;/denchmark-h&gt;


Python version: 3.5
TF version: 1.4.0
Tensorpack version: 0.9.0.1-master (ac9ac2a)

	</description>
	<comments>
		<comment id='1' author='elmirador' date='2019-01-09T17:49:41Z'>
		Thanks a lot for finding this!
A great example of what I wrote here: &lt;denchmark-link:https://tensorpack.readthedocs.io/tutorial/symbolic.html#use-other-symbolic-libraries&gt;https://tensorpack.readthedocs.io/tutorial/symbolic.html#use-other-symbolic-libraries&lt;/denchmark-link&gt;


It’s best to not trust others’ layers!
For non-standard layers that’s not included in TensorFlow or Tensorpack, it’s best to implement them yourself. Non-standard layers often do not have a mathematical definition that people all agree on, and different people can implement it differently. Also, deep learning models on github often have bugs, especially when there is no reproduced experiments with the code.
For your own good, it’s best to implement the layers yourself. This is also why Tensorpack does not contain non-standard layers.

 was made un-public and removed from documentation (&lt;denchmark-link:https://tensorpack.readthedocs.io/modules/models.html&gt;https://tensorpack.readthedocs.io/modules/models.html&lt;/denchmark-link&gt;
) a while ago because it's a non-standard layer and we should not provide an implementation for it. But it's good to know the bug!
		</comment>
		<comment id='2' author='elmirador' date='2019-01-09T18:13:33Z'>
		New implementation:
def BilinearUpSample(x, shape):
    """
    Deterministic bilinearly-upsample the input images.
    It is implemented by deconvolution with "BilinearFiller" in Caffe.
    It is aimed to mimic caffe behavior.
    Args:
        x (tf.Tensor): a NHWC tensor
        shape (int): the upsample factor
    Returns:
        tf.Tensor: a NHWC tensor.
    """
    #log_deprecated("BilinearUpsample", "Please implement it in your own code instead!", "2019-03-01")
    inp_shape = x.shape.as_list()
    ch = inp_shape[3]
    assert ch is not None

    shape = int(shape)
    filter_shape = 2 * shape

    def bilinear_conv_filler(s):
        """
        s: width, height of the conv filter
        https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/include/caffe/filler.hpp#L219-L268
        """
        f = np.ceil(float(s) / 2)
        c = float(2 * f - 1 - f % 2) / (2 * f)
        ret = np.zeros((s, s), dtype='float32')
        for x in range(s):
            for y in range(s):
                ret[x, y] = (1 - abs(x / f - c)) * (1 - abs(y / f - c))
        return ret
    w = bilinear_conv_filler(filter_shape)
    w = np.repeat(w, ch * 1).reshape((filter_shape, filter_shape, ch, 1))

    weight_var = tf.constant(w, tf.float32,
                             shape=(filter_shape, filter_shape, ch, 1),
                             name='bilinear_upsample_filter')
    x = tf.pad(x, [[0, 0], [shape - 1, shape - 1], [shape - 1, shape - 1], [0, 0]], mode='SYMMETRIC')
    out_shape = tf.shape(x) * tf.constant([1, shape, shape, 1], tf.int32)

    @tf.custom_gradient
    def depthwise_deconv(x):
        ret = tf.nn.depthwise_conv2d_native_backprop_input(
            out_shape, weight_var, x, [1, shape, shape, 1], padding='SAME')
        def grad(dy):
            return tf.nn.depthwise_conv2d(dy, weight_var, [1, shape, shape, 1], padding='SAME')
        return ret, grad

    deconv = depthwise_deconv(x)

    edge = shape * (shape - 1)
    deconv = deconv[:, edge:-edge, edge:-edge, :]

    if inp_shape[1]:
        inp_shape[1] *= shape
    if inp_shape[2]:
        inp_shape[2] *= shape
    deconv.set_shape(inp_shape)
    return deconv
I haven't verified the backward is correct, but forward seems right now.
		</comment>
		<comment id='3' author='elmirador' date='2019-01-10T06:16:33Z'>
		
Thanks a lot for finding this!
A great example of what I wrote here: https://tensorpack.readthedocs.io/tutorial/symbolic.html#use-other-symbolic-libraries

It’s best to not trust others’ layers!
For non-standard layers that’s not included in TensorFlow or Tensorpack, it’s best to implement them yourself. Non-standard layers often do not have a mathematical definition that people all agree on, and different people can implement it differently. Also, deep learning models on github often have bugs, especially when there is no reproduced experiments with the code.
For your own good, it’s best to implement the layers yourself. This is also why Tensorpack does not contain non-standard layers.

BilinearUpSample was made un-public and removed from documentation (https://tensorpack.readthedocs.io/modules/models.html) a while ago because it's a non-standard layer and we should not provide an implementation for it. But it's good to know the bug!

I see, but some of the non-standard layers like upsampling are pretty common. A separate non-standard layer codebase might be helpful though.
The reason I hesitate to use TF's implementation is that the image.resize_* functions are messy (at least in resize_area) and might cause some weird things to happen. tf.image.resize_bilinear works fine though.
		</comment>
		<comment id='4' author='elmirador' date='2019-01-10T06:27:01Z'>
		
A separate non-standard layer codebase might be helpful though.

Might be helpful as a reference. But it will have the same issue: personally I would not trust any non-standard layers written by others.
tf.image.resize_bilinear has its own issues as well and it certainly is not equivalent to this layer in tensorpack during upsampling. That's why they're called non-standard.
		</comment>
		<comment id='5' author='elmirador' date='2019-01-10T07:03:28Z'>
		
Might be helpful as a reference. But it will have the same issue: personally I would not trust any non-standard layers written by others.

I agree. Someone should at least read the code before using it.

tf.image.resize_bilinear has its own issues as well and it certainly is not equivalent to this layer in tensorpack during upsampling. That's why they're called non-standard.

Got it. I'll dig harder on TF's code.
All in all, pretty happy to contribute. I love tensorpack much more than keras personally lol
		</comment>
		<comment id='6' author='elmirador' date='2019-01-10T07:05:47Z'>
		Also, just found that the HED example uses the layer with channel=1 only, so there is nothing to fix. But I'll add some notes to the code.
Closing as resolved. Thanks again!
		</comment>
	</comments>
</bug>