<bug id='969' author='sytham' open_date='2018-11-05T17:17:53Z' closed_time='2018-11-06T09:48:05Z'>
	<summary>Clean up parallel Dataflow (PrefetchDataZMQ)</summary>
	<description>
Problem
Processes spawned by PrefetchDataZMQ stay alive. I'm doing a hyperparameter search, and since the dataflow is not reentrant, my understanding is I need to create a new one for each hyperparam evaluation. This leads to a very large amount of live processes, eventually causing my machine to run out of memory.
I create my dataflow as:
&lt;denchmark-code&gt;def create_dataflow(input_data, target_data):
    ds = DataFromList(zip(input_data, target_data), shuffle=True)
    ds = BatchData(ds, BATCH_SIZE)
    ds = PrefetchDataZMQ(ds, nr_proc=NUM_PARALLEL_FETCHERS)
    return ds
&lt;/denchmark-code&gt;

What have I tried

reuse the same dataflow. This leads to the reentrant eror.
in each hyperparam evaluation, do the following:

&lt;denchmark-code&gt;dataflow_train = create_dataflow(train_inp, train_targ)
&lt;do stuff&gt;
del dataflow_train
del model # holds a reference to dataflow_train
gc.collect()
&lt;/denchmark-code&gt;

Since it seems the _MultiProcessZMQDataFlow.__del__ method should do the cleanup. However, I never see the "successfully cleaned-up" message that this method prints if successful. It seems this could either be because __del__ isn't called, or because _MultiProcessZMQDataFlow._reset_done = False (I wouldn't understand why).
In any case, is there a better way to do this?
	</description>
	<comments>
		<comment id='1' author='sytham' date='2018-11-05T17:23:16Z'>
		del does free the dataflow. And you can see "successfully cleaned-up" if you run the following code:
from tensorpack import *

def create():
    x = FakeData([[3,3,3,3], [2]], 1000)
    x = BatchData(x, 3)
    x = PrefetchDataZMQ(x, 3)
    return x

x = create()
x.reset_state()
for idx, dp in enumerate(x):
    print(idx)
del x

import time; time.sleep(10)
So I assume there are other objects in your code that holds a reference to the dataflow so it never gets cleaned.
		</comment>
		<comment id='2' author='sytham' date='2018-11-06T07:50:16Z'>
		Thanks, I do indeed see the "successfully cleaned-up" message in your example.
However, expanding your example to the minimal one that represents my use case, I don't see it:
import gc
from tensorpack import *
from tensorpack.contrib.keras import KerasModel
import tensorflow as tf
from tensorflow import keras
KL = keras.layers

def create_dataflow():
    x = FakeData([[3], [1]], 1000)
    x = BatchData(x, 3)
    x = PrefetchDataZMQ(x, 3)
    return x

def model_func(x):
    M = keras.models.Sequential()
    M.add(KL.InputLayer(input_tensor=x))
    M.add(KL.Dense(1))
    return M

x = create_dataflow()
Q = QueueInput(x)
M = KerasModel(
        model_func,
        inputs_desc=[InputDesc(tf.float32, [None, 3], 'inputs')],
        targets_desc=[InputDesc(tf.float32, [None, 1], 'labels')],
        input=Q)
M.compile(optimizer=tf.train.AdamOptimizer(), loss='binary_crossentropy')
M.fit(steps_per_epoch=len(x), max_epoch=2)

del M
del Q
del x
gc.collect()

import time; time.sleep(10)
I'm not sure what else I should delete here?
		</comment>
		<comment id='3' author='sytham' date='2018-11-06T08:07:19Z'>
		Thanks for the short repro! There might be a bug somewhere that mistakenly keeps reference to the dataflow. I'll take a look at that!
		</comment>
		<comment id='4' author='sytham' date='2018-11-06T08:38:23Z'>
		I think I've found the problem. I'm running everything from a Jupyter notebook. When I run the above minimal example from script, I do see the "successfully cleaned-up" message.
I understand this has now basically ceased to be your problem :) but do you have any suggestions for alternative solutions that don't involve not using a notebook? Could there be another way of shutting down the processes without using del? I can move large parts of code to library/script, but our users mostly use notebooks to set up training runs, especially for collecting the right dataset and configuring the model.
		</comment>
		<comment id='5' author='sytham' date='2018-11-06T09:20:26Z'>
		Do you see the message before sleep or after sleep? It should appear before the sleep, otherwise it's still a bug.
		</comment>
		<comment id='6' author='sytham' date='2018-11-06T09:27:01Z'>
		Ah, I see. You're saying if it appears after sleep it's just because the whole kernel is shutting down.
It appears after sleep.
		</comment>
		<comment id='7' author='sytham' date='2018-11-06T09:49:45Z'>
		After the above fix, your code can now print the "successfully cleaned-up" message before the sleep finished.
		</comment>
		<comment id='8' author='sytham' date='2018-11-06T13:09:04Z'>
		Thanks for the quick fix! Works for me now
		</comment>
		<comment id='9' author='sytham' date='2018-11-06T13:23:41Z'>
		There is a bug in the first commit that was fixed by a later commit.
		</comment>
	</comments>
</bug>