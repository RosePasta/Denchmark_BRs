<bug id='505' author='llidev' open_date='2017-11-22T08:56:58Z' closed_time='2017-12-01T18:51:27Z'>
	<summary>Distributed training failed</summary>
	<description>
Hi,
I am trying to start a POC on distributed training, but get stuck at beginning:
trainer = DistributedTrainerReplicated(os.getenv('CUDA_VISIBLE_DEVICES'), server)
launch_train_with_config(config, trainer)
resulted in
AttributeError: 'TrainConfig' object has no attribute 'session_config'
Also tried to start training without using launch_train_with_config(...), but got
AttributeError: 'DistributedTrainerReplicated' object has no attribute '_callbacks'
Please help... I am using with tensorflow 1.4.0 docker image.
	</description>
	<comments>
		<comment id='1' author='llidev' date='2017-11-22T09:54:09Z'>
		
The first argument (gpus) needs to be a int or a list of int. Somehow it's missing from the documentation.
Please post full error when you report bugs.

		</comment>
		<comment id='2' author='llidev' date='2017-11-23T00:03:36Z'>
		Thanks.
We switched to v1 and that works. Will test v2 trainer later on.
		</comment>
		<comment id='3' author='llidev' date='2017-12-01T18:51:27Z'>
		Closing due to lack of activity. Feel free to reopen and include details if you still encounter error.
		</comment>
		<comment id='4' author='llidev' date='2017-12-06T06:34:36Z'>
		hi,&lt;denchmark-link:https://github.com/ppwwyyxx&gt;@ppwwyyxx&lt;/denchmark-link&gt;
  I have the same question,could you give me any advice?
I read the docs &lt;denchmark-link:http://tensorpack.readthedocs.io/en/latest/modules/train.html?highlight=DistributedTrainerReplicated#tensorpack.train.DistributedTrainerReplicated&gt;http://tensorpack.readthedocs.io/en/latest/modules/train.html?highlight=DistributedTrainerReplicated#tensorpack.train.DistributedTrainerReplicated&lt;/denchmark-link&gt;

and  I modified last lines of tensorpack/examples/ResNet/cifar10-resnet.py
&lt;denchmark-code&gt;166    config = TrainConfig(
167         model=Model(n=NUM_UNITS),
168         dataflow=dataset_train,
169         callbacks=  [
170             ModelSaver(),
171             InferenceRunner(dataset_test,
172                             [ScalarStats('cost'), ClassificationError('wrong_vector')]),
173              ScheduledHyperParamSetter('learning_rate',
174                                       [(1, 0.1), (82, 0.01), (123, 0.001), (300, 0.0002)]),
175        ],
176         max_epoch=164,
177         session_init=SaverRestore(args.load) if args.load else None,
178 
179     )
180     config.session_config=None
181     print(config.data,config.model)
182     nr_gpu = max(get_nr_gpu(), 1)
183 
184     hosts = ['gpu2', 'gpu3']
185     cluster_spec = tf.train.ClusterSpec({
186             'ps': [h + ':2222' for h in hosts],
187             'worker': [h + ':2223' for h in hosts]
188          })
189 
190     server = tf.train.Server(
191                 cluster_spec, job_name=args.job, task_index=args.task,
192                     config = get_default_sess_config()  )
193 
194     #DistributedTrainerReplicated(config, server).train()
195 
196     #launch_train_with_config(config, SyncMultiGPUTrainerParameterServer(nr_gpu))
197     #launch_train_with_config(config, SyncMultiGPUTrainer(nr_gpu))
198     launch_train_with_config(config, DistributedTrainerReplicated(nr_gpu, server))
&lt;/denchmark-code&gt;

and I launch it like this:
(tf-py27) [chongyang@gpu2 ResNet]$ python cifar10-resnet.py --job worker --task 0
and part of the error message
&lt;denchmark-code&gt;[1206 14:16:28 @training.py:90] Building graph for training tower 1 on device /job:worker/task:0/gpu:1...
2017-12-06 14:16:28.115375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:06:00.0, compute capability: 6.0)
2017-12-06 14:16:28.115407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -&gt; (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)
[1206 14:16:47 @trainers.py:193] WRN For efficiency, local MODEL_VARIABLES are only synced to PS once every epoch. Be careful if you save the model more frequently than this.
Traceback (most recent call last):
  File "cifar10-resnet.py", line 198, in &lt;module&gt;
    launch_train_with_config(config, DistributedTrainerReplicated(nr_gpu, server))
  File "/home/chongyang/.local/lib/python2.7/site-packages/tensorpack/train/interface.py", line 88, in launch_train_with_config
    model._build_graph_get_cost, model.get_optimizer)
  File "/home/chongyang/.local/lib/python2.7/site-packages/tensorpack/utils/argtools.py", line 171, in wrapper
    return func(*args, **kwargs)
  File "/home/chongyang/.local/lib/python2.7/site-packages/tensorpack/train/tower.py", line 148, in setup_graph
    self.register_callback(input_callbacks + train_callbacks)
  File "/home/chongyang/.local/lib/python2.7/site-packages/tensorpack/train/base.py", line 146, in _register_callback
    self._register_callback(x)
  File "/home/chongyang/.local/lib/python2.7/site-packages/tensorpack/train/base.py", line 149, in _register_callback
    assert not isinstance(self._callbacks, Callbacks), \
AttributeError: 'DistributedTrainerReplicated' object has no attribute '_callbacks'
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='llidev' date='2017-12-06T14:51:48Z'>
		There are some bugs in distributed trainer. Should be fixed now.
		</comment>
	</comments>
</bug>