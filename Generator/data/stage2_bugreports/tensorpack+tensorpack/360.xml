<bug id='360' author='waleedka' open_date='2017-08-03T06:44:21Z' closed_time='2017-08-03T08:10:36Z'>
	<summary>Incorrect implementation of Batch Renorm?</summary>
	<description>
I've been checking your implementation of Batch Re-Normalization, and I think there is an error that you might want to look into.
To set a baseline, this is how the original paper defines batch renormalization:
normed_x = (x - batch_mean) / batch_std    # standard BN
normed_x = normed_x * r + d                # The batch renorm correction
normed_x = normed_x * gamma + beta         # final scale and bias
In other words, apply BN first, then apply the r/d correction, then apply gamma/beta last. But, in batch_norm.py, it applies BN, then the gamma/beta (both operations are done using tf.nn.fused_batch_norm in this line):
&lt;denchmark-link:https://github.com/ppwwyyxx/tensorpack/blob/master/tensorpack/models/batch_norm.py#L209&gt;https://github.com/ppwwyyxx/tensorpack/blob/master/tensorpack/models/batch_norm.py#L209&lt;/denchmark-link&gt;

Then it applies the r/d correction last in:
&lt;denchmark-link:https://github.com/ppwwyyxx/tensorpack/blob/master/tensorpack/models/batch_norm.py#L220&gt;https://github.com/ppwwyyxx/tensorpack/blob/master/tensorpack/models/batch_norm.py#L220&lt;/denchmark-link&gt;

It's possible that I'm missing something here or misunderstood the paper. If so, please do correct me.
	</description>
	<comments>
		<comment id='1' author='waleedka' date='2017-08-03T06:58:00Z'>
		You're right.
		</comment>
		<comment id='2' author='waleedka' date='2017-08-03T07:29:16Z'>
		Btw I'm going to fix it by just letting it call tf.layers.batch_normalization with renorm option, since wrapping symbolic functions to layers is not a focus of this project. You can also just call tf.layers instead of tensorpack layers.
The two implementation are checkpoint-incompatible because they have different number of variables. I would just trust the TF version since they wrote the paper.
		</comment>
		<comment id='3' author='waleedka' date='2017-08-03T08:18:38Z'>
		Thanks a lot!
		</comment>
	</comments>
</bug>