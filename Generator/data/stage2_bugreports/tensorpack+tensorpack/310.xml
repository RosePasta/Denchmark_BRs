<bug id='310' author='xingyul' open_date='2017-06-23T02:26:11Z' closed_time='2017-06-23T02:33:59Z'>
	<summary>Multi-GPU training when #GPU&amp;gt;10</summary>
	<description>
When using multi-GPU training and the number of GPU &gt; 10, I encounter problems of variable sharing. When building 11th tower, I have the following error:
[0623 02:20:52 @multigpu.py:89] Building graph for training tower 10... ... ValueError: Variable XXX/W already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at: 
This error won't happen when #GPU &lt;= 10.
I was using SyncMultiGPUTrainer(config).train()
	</description>
	<comments>
		<comment id='1' author='xingyul' date='2017-06-23T02:28:34Z'>
		Oh I don't know such machine exists. I'll try to fix.
		</comment>
	</comments>
</bug>