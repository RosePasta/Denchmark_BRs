<bug id='673' author='vfdev-5' open_date='2018-02-25T22:58:10Z' closed_time='2018-03-02T08:03:23Z'>
	<summary>Infinite loop with MultiProcessMapDataZMQ</summary>
	<description>
There is an infinite loop with the following code:
&lt;denchmark-code&gt;from tensorpack.dataflow import MultiProcessMapDataZMQ, FakeData

ds = FakeData(((50, 50, 3), (1,)))

import numpy as np

def proc(dp):
    img = np.random.randint(0, 255, size=(1012, 1012, 3))
    img[img &lt; 120] = 0
    return dp

mp_ds = MultiProcessMapDataZMQ(ds, map_func=proc, nr_proc=10, strict=True)

mp_ds.reset_state()

for i, (_, _) in enumerate(mp_ds.get_data()):
    if i % 100 == 0:
        print(i, end=" . ", flush=True)
&lt;/denchmark-code&gt;

Important part is  and small size of . If change size to (512, 512, 3), then loop ends properly. Probably, there is something with buffer filling and dequeing...
Execution stucks &lt;denchmark-link:https://github.com/ppwwyyxx/tensorpack/blob/master/tensorpack/dataflow/parallel_map.py#L81&gt;here&lt;/denchmark-link&gt;
.
	</description>
	<comments>
		<comment id='1' author='vfdev-5' date='2018-02-25T23:01:22Z'>
		Cannot reproduce. Please update
		</comment>
		<comment id='2' author='vfdev-5' date='2018-02-25T23:11:44Z'>
		Actually, I have already updated to the master. Try a smaller size, for example (10, 10, 3) in FakeData
		</comment>
		<comment id='3' author='vfdev-5' date='2018-02-26T07:29:27Z'>
		&lt;denchmark-link:https://github.com/ppwwyyxx&gt;@ppwwyyxx&lt;/denchmark-link&gt;
 could you reproduce the issue ?
		</comment>
		<comment id='4' author='vfdev-5' date='2018-02-26T07:47:00Z'>
		I can not. Maybe upgrade/reinstall msgpack?
		</comment>
		<comment id='5' author='vfdev-5' date='2018-02-26T08:01:23Z'>
		Could you try please this code too:
from tensorpack.dataflow import MultiProcessMapDataZMQ, RNGDataFlow, MultiThreadMapData


dataset = [("file_%i" % i, i % 3) for i in range(500)]

class TestDataset(RNGDataFlow):
    
    def __init__(self, dataset):
        super(TestDataset, self).__init__()
        self.dataset = dataset
        
    def size(self):
        return len(self.dataset)
        
    def get_data(self):
        for k in range(len(self.dataset)):
            yield self.dataset[k]            

test_dataset = TestDataset(dataset)
test_dataset.size()

def data_transform(dp):
    return dp

mp_test_dataset = MultiProcessMapDataZMQ(test_dataset, nr_proc=10, map_func=data_transform, strict=True)

mp_test_dataset.reset_state()


for i, (fp, _) in enumerate(mp_test_dataset.get_data()):
    if i % 100 == 0:
        print(i, end=" . ", flush=True)
Okay, I'll try to upgrade msgpack, which version do you use ?
Version of msgpack I use is msgpack (0.5.6)
		</comment>
		<comment id='6' author='vfdev-5' date='2018-02-26T08:03:17Z'>
		The code successfully exits. I use the latest version of msgpack &amp; msgpack-numpy. Maybe also try upgrading pyzmq.
		</comment>
		<comment id='7' author='vfdev-5' date='2018-02-26T08:08:12Z'>
		Strange configuration I have. Here is a list of versions of some of packages I have:
&lt;denchmark-code&gt;msgpack (0.5.6)
msgpack-numpy (0.4.3)
msgpack-python (0.5.5)
numpy (1.14.1)
pickleshare (0.7.4)
pip (9.0.1)
pyzmq (17.0.0)
tensorpack (0.8.1)
&lt;/denchmark-code&gt;

on Ubuntu 16.04
		</comment>
		<comment id='8' author='vfdev-5' date='2018-02-26T08:13:39Z'>
		&lt;denchmark-link:https://github.com/msgpack/msgpack-python#pypi-package-name&gt;https://github.com/msgpack/msgpack-python#pypi-package-name&lt;/denchmark-link&gt;

Maybe related.
		</comment>
		<comment id='9' author='vfdev-5' date='2018-02-26T09:29:29Z'>
		I could reproduce the issue using this docker file. Could you try it please?
&lt;denchmark-link:https://github.com/ppwwyyxx/tensorpack/files/1757877/Dockerfile.txt&gt;Dockerfile.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ppwwyyxx/tensorpack/files/1757878/test_tensorpack_inf_loop.txt&gt;test_tensorpack_inf_loop.txt&lt;/denchmark-link&gt;

Please remove .txt extensions from Dockerfile and replace it for  by . Github does not allow include py and dockerfile as is.
		</comment>
		<comment id='10' author='vfdev-5' date='2018-02-27T00:43:23Z'>
		I can reproduce the issue in docker. Looks like I'm using zmq socket in a wrong way where messages can get lost if the sockets aren't created with a desired order.
		</comment>
		<comment id='11' author='vfdev-5' date='2018-03-02T09:58:30Z'>
		Awesome ! ðŸŽ‰
		</comment>
	</comments>
</bug>