<bug id='122' author='PatWie' open_date='2017-01-28T13:43:22Z' closed_time='2017-01-28T17:23:34Z'>
	<summary>custom layers</summary>
	<description>
I am currently testing whether I should switch from tfSlim to your layers. There something I cannot figure out. The minimal example is:
from tensorpack import *
import tensorflow as tf

tf.reset_default_graph()

@layer_register()
def revReLU(x):
    return -1 * tf.nn.relu(x)

a = tf.Variable(tf.random_normal([8, 224, 224, 3], stddev=0.35), name="a")
b = (LinearWrap(a).Conv2D('b', 8, 7, stride=2, nl=tf.identity))()
c = (LinearWrap(a).Conv2D('c', 8, 7, stride=2, nl=tf.identity).revReLU())()

sess = tf.InteractiveSession()
sess.run(tf.global_variables_initializer())
print sess.run(b).shape
print sess.run(c).shape
When I put "revReLU" into the modules directory everything works as exected. But when using layer_register in the actual script LinearWrap cannot use it.
revReLU is just for demonstration.
	</description>
	<comments>
		<comment id='1' author='PatWie' date='2017-01-28T15:59:41Z'>
		I didn't make linearwrap to work with custom layers. Will try to fix that.
		</comment>
		<comment id='2' author='PatWie' date='2017-01-28T17:28:41Z'>
		Note that by default a layer should be under some variable scope as the first argument, just like Conv2D. So you should call it with: revReLU('relu', x). And in LinearWrap this becomes .revReLU('relu'). With the latest commit this would work.
You can register it with @layer_register(use_scope=False) to disable the scope, then your original code would work.
Also, in your case you don't have to register it as a layer to use LinearWrap. You can use apply.
def revReLU(x):
    return -1 * tf.nn.relu(x)
#....
c = (LinearWrap(a).Conv2D('c', 8, 7, stride=2, nl=tf.identity).apply(revReLU))()
		</comment>
	</comments>
</bug>