<bug id='1078' author='jiang1st' open_date='2019-02-12T08:13:58Z' closed_time='2019-02-13T03:30:55Z'>
	<summary>training gets stuck with sync bn</summary>
	<description>
Hi, I am replacing standard batch norm with sync batch norm in my network. I am using 8 P100 on centos 7.0.  I have installed nccl 2.4.2 + cuda9.0. The backbone is mobilenet v2. Here is my problem:
When I set:   with argscope(BatchNorm, momentum=0.997, epsilon=0.001, training=True, sync_statistics='nccl') , after building the graph, the training phase gets stuck at:
0%|                                                                                                                              |0/300[00:00&lt;?,?it/s][0212 15:59:37 @input_source.py:551] Pre-filling StagingArea ...
[0212 15:59:39 @input_source.py:555] 1 element was put into StagingArea on each tower.
But when I set with argscope(BatchNorm, momentum=0.997, epsilon=0.001, training=True) , then the training phase goes successfully.
I guess there is something wrong with my sync batch norm. What could be the reason? Thank you.
	</description>
	<comments>
		<comment id='1' author='jiang1st' date='2019-02-12T08:18:07Z'>
		&lt;denchmark-link:https://tensorpack.readthedocs.io/modules/models.html#tensorpack.models.BatchNorm&gt;https://tensorpack.readthedocs.io/modules/models.html#tensorpack.models.BatchNorm&lt;/denchmark-link&gt;
 has mentioned the common reasons why it may hang.
If you cannot figure out the reason, please try running the built-in example with syncbn first -- if that fails, please fill the issue template.
		</comment>
		<comment id='2' author='jiang1st' date='2019-02-13T03:33:15Z'>
		I tried one built-in example with sync bn, and it works fine. It should be my problem when replacing slim.batch_norm with tensorpack's batch norm. Will check it.
		</comment>
		<comment id='3' author='jiang1st' date='2019-02-13T08:05:26Z'>
		Another possibility:
In every iteration, either all GPUs execute the BatchNorm once, or no GPUs execute the BatchNorm. If only a subset of GPUs execute the BatchNorm, or if the BatchNorm is executed more than once, this layer may hang because there is nothing to sync.
		</comment>
		<comment id='4' author='jiang1st' date='2019-02-13T11:30:00Z'>
		Thank you for your suggestion. I think the operations in each gpu are the same. I used SyncMultiGPUTrainerReplicated from tensorpack for multi-gpu training.
When would the BatchNorm be executed more than once? I tried to print the tensor name of the input at the beginning of BatchNorm, it looks like each BatchNorm is called only once.
		</comment>
		<comment id='5' author='jiang1st' date='2019-02-13T16:58:34Z'>
		
When would the BatchNorm be executed more than once?

For example when it is executed in a while loop
		</comment>
		<comment id='6' author='jiang1st' date='2019-02-13T16:59:49Z'>
		You may also want to remove some custom callbacks to see if they affect this. Callbacks in general should not use hooked_sess and it may cause issues like this if you use hooked_sess.
		</comment>
		<comment id='7' author='jiang1st' date='2019-02-14T07:53:45Z'>
		Thank you for the suggestions. I think I have put the batch_norm in a  loop (&lt;denchmark-link:https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py#L248&gt; slim mobilenet v2&lt;/denchmark-link&gt;
 ). If a put batch_norm outside the  loop, then it works well.
What I don't understand is that, in the for loop, layers all have different names (including batch_norm). Why would batch_norm hang? Thank you.
		</comment>
		<comment id='8' author='jiang1st' date='2019-02-14T07:57:00Z'>
		
I have put the batch_norm in a for loop ( slim mobilenet v2 )

This is unrelated. What I meant is a symbolic loop (like tf.while_loop) which will execute the same layer in each iteration.
A loop in python side like this will create a new layer in each iteration.
		</comment>
		<comment id='9' author='jiang1st' date='2019-02-14T08:01:41Z'>
		
If a put batch_norm outside the for loop, then it works well.

That sounds like you may have used the same name for different batchnorm layers.
		</comment>
		<comment id='10' author='jiang1st' date='2019-02-14T08:09:37Z'>
		You can print(shared_name) at 


tensorpack/tensorpack/models/batch_norm.py


         Line 248
      in
      4967559






 shared_name = re.sub('tower[0-9]+/', '', tf.get_variable_scope().name) 





and see if there are any duplications within one GPU, and if different GPUs produce the same name.
		</comment>
		<comment id='11' author='jiang1st' date='2019-02-15T05:14:31Z'>
		Thank you for the suggestions. I think I have found the reason: I only used the first few layers of mobilenet backbone for training. The last several layers are not used for training, but were created by default when loading mobilenet_base(). After removing the batch_norm in these layers, the training works.
		</comment>
		<comment id='12' author='jiang1st' date='2019-02-15T05:24:03Z'>
		Thanks a lot for finding this out!
Indeed, unused batchnorm will also cause this. I'll fix this or add this note to documentation.
		</comment>
		<comment id='13' author='jiang1st' date='2019-02-15T05:24:53Z'>
		Btw, it will not cause this issue if you use the internal_update=True option in BatchNorm.
		</comment>
		<comment id='14' author='jiang1st' date='2019-02-15T05:48:45Z'>
		Should've been fixed after &lt;denchmark-link:https://github.com/tensorpack/tensorpack/commit/c366eebf04f5e53022b1b266cc0f940bf6789f61&gt;c366eeb&lt;/denchmark-link&gt;

Docs (&lt;denchmark-link:https://tensorpack.readthedocs.io/modules/models.html#tensorpack.models.BatchNorm&gt;https://tensorpack.readthedocs.io/modules/models.html#tensorpack.models.BatchNorm&lt;/denchmark-link&gt;
) was also updated.
In general, the model will run faster with internal_update=True when some batchnorm are unused. This was also now mentioned in the docs.
The default was kept as internal_update=False to be consistent with tf.layers.
		</comment>
		<comment id='15' author='jiang1st' date='2019-02-15T07:08:32Z'>
		Thanks for the update!
		</comment>
	</comments>
</bug>