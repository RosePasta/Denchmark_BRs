<bug id='351' author='haamoon' open_date='2017-07-30T22:51:27Z' closed_time='2017-07-30T23:56:44Z'>
	<summary>Having BNReLU activation inside freeze_variables</summary>
	<description>
Apparently, tensorpack does not like to see BNReLU activation when using varreplace.freeze_variables(). Here is a minimal code to reproduce the error:
&lt;denchmark-code&gt;import tensorflow as tf
from tensorpack import *

class Model(ModelDesc):
    def __init__(self, data_format):
        self.data_format = data_format
    def _get_inputs(self):
        return [InputDesc(tf.float32, [1, 224, 224, 3], 'im')]
    def _build_graph(self, inputs):
        image = inputs[0]
        if self.data_format == 'NCHW':
            image = tf.transpose(image, [0, 3, 1, 2])
        with argscope([Conv2D, BatchNorm], data_format=self.data_format):
            with varreplace.freeze_variables():
                fea = Conv2D('conv0', image, 64, 7, stride=2, nl=BNReLU)
    def _get_optimizer(self):
        pass

if __name__ == '__main__':
    model = Model('NCHW');
    inputs = [tf.placeholder(dtype=inp.type, shape=inp.shape, name=inp.name) for inp in model._get_inputs()]
    with TowerContext('', is_training=True):
          model._build_graph(inputs)
&lt;/denchmark-code&gt;

without nl=BNReLU or freeze_variables it runs without any problem. Any Idea how to fix it?
	</description>
	<comments>
	</comments>
</bug>