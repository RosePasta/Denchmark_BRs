<bug id='651' author='deliahu' open_date='2019-12-12T03:47:59Z' closed_time='2020-03-25T18:49:19Z'>
	<summary>Improve request queue fairness during scaling</summary>
	<description>
&lt;denchmark-h:h3&gt;Description&lt;/denchmark-h&gt;

Currently, when scaling at a time when API replicas have lengthy queues, some requests that come in later are fulfilled before other requests that came in earlier.
One theory: when the first request came in, all of the available replicas already had long queues, and this request was added to the back of one of them. In the mean time, a new replica came online with no queue, and the next few requests that came in after that went to the new replica and didn't have to wait. But since the previous request was already assigned a replica and was sitting in a waitress queue, the scheduler couldn't re-route it to the new replica. If this is what's happening, then a possible fix would be to shrink the waitress queue (possibly to 0) and create a queue upstream of all of the replicas.
Some data (p2.xlarge TensorFlow GPT-2): with multiple instances and many concurrent requests, there is a large spread of queue wait times (8s-60s that all return with a few seconds of each other)
	</description>
	<comments>
		<comment id='1' author='deliahu' date='2020-03-25T18:49:19Z'>
		 &lt;denchmark-link:https://github.com/cortexlabs/cortex/issues/839&gt;#839&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>