<bug id='1596' author='RobertLucian' open_date='2020-11-24T18:31:14Z' closed_time='2020-12-08T20:09:24Z'>
	<summary>No available models when multiple APIs are deployed in one go</summary>
	<description>
&lt;denchmark-h:h4&gt;Version&lt;/denchmark-h&gt;

&gt;= 0.22
&lt;denchmark-h:h4&gt;Description&lt;/denchmark-h&gt;

When multiple MMC/LR Python/ONNX/TensorFlow APIs are deployed in one go, only one API from the spec list is gonna work successfully, whereas all the rest will not work because they won't detect the specified model(s). Subsequent cortex get &lt;api-name&gt; calls will show the rest of the APIs not detecting any model.
&lt;denchmark-h:h4&gt;Steps to reproduce&lt;/denchmark-h&gt;

As reproduced in &lt;denchmark-link:https://github.com/cortexlabs/cortex/tree/0.22/examples/traffic-splitter&gt;https://github.com/cortexlabs/cortex/tree/0.22/examples/traffic-splitter&lt;/denchmark-link&gt;
.
&lt;denchmark-h:h4&gt;Additional context&lt;/denchmark-h&gt;

A culprit of this is the CuratedModelResources field from our stack.
&lt;denchmark-h:h4&gt;Suggested solution&lt;/denchmark-h&gt;

A list of model metadata (CuratedModelResources) is collected during validations. This list is currently attached to the API spec. The list doesn't need to be attached to the API spec because the model metadata is regenerated by each replica. The list can also become very large depending on the number of models and model versions in the directory.
	</description>
	<comments>
	</comments>
</bug>