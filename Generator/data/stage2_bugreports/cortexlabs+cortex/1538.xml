<bug id='1538' author='lapaniku' open_date='2020-11-06T11:46:20Z' closed_time='2020-11-27T14:11:37Z'>
	<summary>Hard to retrieve logs from cluster &amp;gt; 100 instances</summary>
	<description>
&lt;denchmark-h:h4&gt;Version&lt;/denchmark-h&gt;

0.21.0
&lt;denchmark-h:h4&gt;Description&lt;/denchmark-h&gt;

I started experimenting with big clusters of spot instances and from time to time got failed workers in job status.
&lt;denchmark-code&gt;job id             status                    progress   failed   start time                 duration
69bb1905c592545e   running                   529/1000   17       06 Nov 2020 11:15:29 UTC   12m5s
&lt;/denchmark-code&gt;

Which is ok, but when I try to get logs it became quite challenging, as I get either:
&lt;denchmark-code&gt;error encountered while fetching logs from cloudwatch: InvalidParameterException: 1 validation error detected: Value '[69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-smpqz_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-txzhz_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-vkbr8_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-x7c2b_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-b92mh_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-nr5fv_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-rfnpb_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-jkk7p_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-zpbvx_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-w2pfx_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-4h6c2_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-m9kbt_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-q4x9z_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-klpgx_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-w8qkr_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-zkpkj_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-b6k8r_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-cx96k_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-gr6j2_api, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-4b96b_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-6hc2b_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-hq5v6_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-s57bk_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-xnlfr_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-6r8lb_api, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-bbfsq_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-hfss2_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-ckdhv_downloader, 69bb50b67e78ddea_imagesearch-batch-69bb50b67e78ddea-ljcdb_downloader, 69bb50b67e78ddea_imagesearch-ba ...' at 'logStreamNames' failed to satisfy constraint: Member must have length less than or equal to 100
&lt;/denchmark-code&gt;

or
&lt;denchmark-code&gt;error encountered while searching for log streams: ThrottlingException: Rate exceeded
	status code: 400, request id: adc71b03-a1c5-4091-8402-52cc006efeff
&lt;/denchmark-code&gt;

It's clear that retrieving logs for such an amount of workers is challenging, that's why I wonder if it has some workaround. Or what is the best way to discover issues with some workers in such cases.
Note: I created a tiny instance to run cortex commands (create cluster and deploy) in the same region and account to reduce any network issues.
&lt;denchmark-h:h4&gt;Configuration&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;cluster_name: cortex

# AWS region
region: us-east-1

# instance type
instance_type: t3.small

min_instances: 1
max_instances: 1000

# whether to use spot instances in the cluster; spot instances are not guaranteed to be available so please take that into account for production clusters (default: false)
spot: true

spot_config:
  on_demand_base_capacity: 0
  on_demand_percentage_above_base_capacity: 0
  on_demand_backup: true
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Steps to reproduce&lt;/denchmark-h&gt;


Start a cluster with more than 100 spot instances
Start a job with the same amount of workers as spot instances (&gt; 100)
Try to fetch the corresponding job logs

&lt;denchmark-h:h4&gt;Expected behavior&lt;/denchmark-h&gt;

Got some actual logs
&lt;denchmark-h:h4&gt;Actual behavior&lt;/denchmark-h&gt;

Errors described above
&lt;denchmark-h:h4&gt;Suggested solution&lt;/denchmark-h&gt;

It might make sense to filter out logs somehow and show only errors if possible, as now I get pip install ... requirements logs which are not quite useful.
	</description>
	<comments>
		<comment id='1' author='lapaniku' date='2020-11-06T14:47:37Z'>
		We have filed an issue recently &lt;denchmark-link:https://github.com/cortexlabs/cortex/issues/1484&gt;#1484&lt;/denchmark-link&gt;
 to provide the ability to control log levels.
In the meantime, all workers logs are streamed to Cloudwatch. If I were in your situation, here is how I would attempt to debug a big job with a large number of workers:

Navigate to your AWS cloudwatch console and select your cluster's region
Find your API's log group which should take the form &lt;cortex cluster name&gt;/&lt;api name&gt;
You should notice that there are multiple log streams that take the form: api-&lt;api name&gt;-&lt;job-id&gt;-&lt;internal id&gt;_[api|downloader]
Click on search all streams

&lt;denchmark-link:https://user-images.githubusercontent.com/4365343/98377679-201a6400-2013-11eb-9e3e-b20e85fc1638.png&gt;&lt;/denchmark-link&gt;


5. Click on view as text and search for keywords such as Exception or error

&lt;denchmark-link:https://user-images.githubusercontent.com/4365343/98377801-42ac7d00-2013-11eb-93d3-892657e305be.png&gt;&lt;/denchmark-link&gt;

Alternatively, if you are attempting to debug errors in a job and you are okay with stopping the job, then you can stop the job with cortex delete &lt;api_name&gt; &lt;job_id&gt; and then run cortex logs &lt;api_name&gt; &lt;job_id&gt;. Getting the logs for a completed job should be less error prone. You can search for keywords like exception or error.
		</comment>
		<comment id='2' author='lapaniku' date='2020-11-27T14:11:37Z'>
		Closing this issue for now. Specifying log levels is being tracked here &lt;denchmark-link:https://github.com/cortexlabs/cortex/issues/1484&gt;#1484&lt;/denchmark-link&gt;
.
Feel free to follow up on this issue.
		</comment>
	</comments>
</bug>