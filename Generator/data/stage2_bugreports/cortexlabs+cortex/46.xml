<bug id='46' author='vishalbollu' open_date='2019-03-28T14:13:16Z' closed_time='2019-08-21T21:45:00Z'>
	<summary>Aggregate still in "aggregating" status even when logs say OOM</summary>
	<description>
&lt;denchmark-h:h3&gt;Description&lt;/denchmark-h&gt;

Aggregate says it is still in progress even though logs say out of memory
&lt;denchmark-h:h3&gt;Application Configuration&lt;/denchmark-h&gt;

Added a sum_distinct_float aggregate operation to the normalize template in fraud
&lt;denchmark-h:h3&gt;Actual Behavior&lt;/denchmark-h&gt;

Aggregate remains in "aggregating" status
&lt;denchmark-h:h3&gt;Expected Behavior&lt;/denchmark-h&gt;

Expected aggregate to "fail" given that we are summing distinctly on a float column with many different values.
&lt;denchmark-h:h3&gt;Stack Trace&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;19/03/26 14:07:12 SPARK:WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
19/03/26 14:09:22 SPARK:WARN TaskSetManager: Lost task 0.0 in stage 8.0 (TID 12, 192.168.56.198, executor 1): java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.&lt;init&gt;(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
	at org.apache.spark.io.ReadAheadInputStream.&lt;init&gt;(ReadAheadInputStream.java:105)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.&lt;init&gt;(UnsafeSorterSpillReader.java:82)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:156)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:477)
	at org.apache.spark.sql.execution.UnsafeKVExternalSorter.sortedIterator(UnsafeKVExternalSorter.java:204)
	at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.switchToSortBasedAggregation(TungstenAggregationIterator.scala:261)
	at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:221)
	at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.&lt;init&gt;(TungstenAggregationIterator.scala:360)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$doExecute$1$$anonfun$4.apply(HashAggregateExec.scala:112)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$doExecute$1$$anonfun$4.apply(HashAggregateExec.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

19/03/26 14:09:24 SPARK:ERROR TaskSchedulerImpl: Lost executor 1 on 192.168.56.198: 
The executor with id 1 exited with exit code 52.
The API gave the following brief reason: null
The API gave the following message: null
The API gave the following container statuses:

ContainerStatus(containerID=docker://b5b1b2dc6ad8c04e8162db0737f3f42a0092d67ea0b4c072abb4b311c07dca2e, image=969758392368.dkr.ecr.us-west-2.amazonaws.com/cortexlabs/spark:latest, imageID=docker-pullable://969758392368.dkr.ecr.us-west-2.amazonaws.com/cortexlabs/spark@sha256:a0fc3551548c7f4dd1b5a27b52c4b0a96a52b5f63e8bb0a25899234a9cbe61df, lastState=ContainerState(running=null, terminated=null, waiting=null, additionalProperties={}), name=executor, ready=false, restartCount=0, state=ContainerState(running=null, terminated=ContainerStateTerminated(containerID=docker://b5b1b2dc6ad8c04e8162db0737f3f42a0092d67ea0b4c072abb4b311c07dca2e, exitCode=52, finishedAt=Time(time=2019-03-26T14:09:22Z, additionalProperties={}), message=null, reason=Error, signal=null, startedAt=Time(time=2019-03-26T14:06:12Z, additionalProperties={}), additionalProperties={}), waiting=null, additionalProperties={}), additionalProperties={})
      
19/03/26 14:09:24 SPARK:WARN TaskSetManager: Lost task 1.0 in stage 8.0 (TID 13, 192.168.56.198, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: 
The executor with id 1 exited with exit code 52.
The API gave the following brief reason: null
The API gave the following message: null
The API gave the following container statuses:

ContainerStatus(containerID=docker://b5b1b2dc6ad8c04e8162db0737f3f42a0092d67ea0b4c072abb4b311c07dca2e, image=969758392368.dkr.ecr.us-west-2.amazonaws.com/cortexlabs/spark:latest, imageID=docker-pullable://969758392368.dkr.ecr.us-west-2.amazonaws.com/cortexlabs/spark@sha256:a0fc3551548c7f4dd1b5a27b52c4b0a96a52b5f63e8bb0a25899234a9cbe61df, lastState=ContainerState(running=null, terminated=null, waiting=null, additionalProperties={}), name=executor, ready=false, restartCount=0, state=ContainerState(running=null, terminated=ContainerStateTerminated(containerID=docker://b5b1b2dc6ad8c04e8162db0737f3f42a0092d67ea0b4c072abb4b311c07dca2e, exitCode=52, finishedAt=Time(time=2019-03-26T14:09:22Z, additionalProperties={}), message=null, reason=Error, signal=null, startedAt=Time(time=2019-03-26T14:06:12Z, additionalProperties={}), additionalProperties={}), waiting=null, additionalProperties={}), additionalProperties={})
      
19/03/26 14:11:06 SPARK:WARN TaskSetManager: Lost task 1.1 in stage 8.0 (TID 14, 192.168.53.180, executor 2): java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.&lt;init&gt;(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
	at org.apache.spark.io.ReadAheadInputStream.&lt;init&gt;(ReadAheadInputStream.java:105)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.&lt;init&gt;(UnsafeSorterSpillReader.java:82)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:156)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:477)
	at org.apache.spark.sql.execution.UnsafeKVExternalSorter.sortedIterator(UnsafeKVExternalSorter.java:204)

	at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.switchToSortBasedAggregation(TungstenAggregationIterator.scala:261)
	at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:221)
	at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.&lt;init&gt;(TungstenAggregationIterator.scala:360)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$doExecute$1$$anonfun$4.apply(HashAggregateExec.scala:112)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$doExecute$1$$anonfun$4.apply(HashAggregateExec.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

19/03/26 14:11:08 SPARK:ERROR TaskSchedulerImpl: Lost executor 2 on 192.168.53.180: 
The executor with id 2 exited with exit code 52.
The API gave the following brief reason: null
The API gave the following message: null
The API gave the following container statuses:

ContainerStatus(containerID=docker://394490c15a9d6b0f904574e7aba6af5aac5c4add56256d728009ebb5ea355d43, image=969758392368.dkr.ecr.us-west-2.amazonaws.com/cortexlabs/spark:latest, imageID=docker-pullable://969758392368.dkr.ecr.us-west-2.amazonaws.com/cortexlabs/spark@sha256:a0fc3551548c7f4dd1b5a27b52c4b0a96a52b5f63e8bb0a25899234a9cbe61df, lastState=ContainerState(running=null, terminated=null, waiting=null, additionalProperties={}), name=executor, ready=false, restartCount=0, state=ContainerState(running=null, terminated=ContainerStateTerminated(containerID=docker://394490c15a9d6b0f904574e7aba6af5aac5c4add56256d728009ebb5ea355d43, exitCode=52, finishedAt=Time(time=2019-03-26T14:11:07Z, additionalProperties={}), message=null, reason=Error, signal=null, startedAt=Time(time=2019-03-26T14:09:24Z, additionalProperties={}), additionalProperties={}), waiting=null, additionalProperties={}), additionalProperties={})
      
19/03/26 14:11:08 SPARK:WARN TaskSetManager: Lost task 0.1 in stage 8.0 (TID 15, 192.168.53.180, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: 
The executor with id 2 exited with exit code 52.
The API gave the following brief reason: null
The API gave the following message: null
The API gave the following container statuses:

ContainerStatus(containerID=docker://394490c15a9d6b0f904574e7aba6af5aac5c4add56256d728009ebb5ea355d43, image=969758392368.dkr.ecr.us-west-2.amazonaws.com/cortexlabs/spark:latest, imageID=docker-pullable://969758392368.dkr.ecr.us-west-2.amazonaws.com/cortexlabs/spark@sha256:a0fc3551548c7f4dd1b5a27b52c4b0a96a52b5f63e8bb0a25899234a9cbe61df, lastState=ContainerState(running=null, terminated=null, waiting=null, additionalProperties={}), name=executor, ready=false, restartCount=0, state=ContainerState(running=null, terminated=ContainerStateTerminated(containerID=docker://394490c15a9d6b0f904574e7aba6af5aac5c4add56256d728009ebb5ea355d43, exitCode=52, finishedAt=Time(time=2019-03-26T14:11:07Z, additionalProperties={}), message=null, reason=Error, signal=null, startedAt=Time(time=2019-03-26T14:09:24Z, additionalProperties={}), additionalProperties={}), waiting=null, additionalProperties={}), additionalProperties={})
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Version&lt;/denchmark-h&gt;

master
	</description>
	<comments>
		<comment id='1' author='vishalbollu' date='2019-03-28T22:13:09Z'>
		Additional Context:
&lt;denchmark-link:https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/api.md&gt;https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/api.md&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/pkg/apis/sparkoperator.k8s.io/v1beta1/types.go&gt;https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/pkg/apis/sparkoperator.k8s.io/v1beta1/types.go&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>