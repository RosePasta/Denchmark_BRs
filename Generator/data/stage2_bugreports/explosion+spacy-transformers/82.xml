<bug id='82' author='mmy12580' open_date='2019-10-08T20:57:24Z' closed_time='2019-10-28T23:21:26Z'>
	<summary>Problems with newest Spacy-Transformers</summary>
	<description>
I am trying yesterday updated version of spacy transformers, and I would like to apply transfer learning on the distilled version of BERT. I tried the train_textcat.py for IMDB sentiment analysis. However, it gives me error like this. The format of texts and annotations are exactly from the same the textcat tutorial scripts. spacy_pytorch_transfomers do not have this problem, while spacy_transformers has this problem. May I feel it is good to let you know.
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "train.py", line 207, in &lt;module&gt;
    trainer.train(Config)
  File "train.py", line 49, in train
    self.train_by_batch(config)
  File "train.py", line 105, in train_by_batch
    nlp.update(texts, annotations, sgd=optimizer, drop=0.1, losses=losses)
  File "/home/darktower/.local/lib/python3.6/site-packages/spacy_transformers/language.py", line 98, in update
    tok2vec.set_annotations(docs, outputs)
  File "/home/darktower/.local/lib/python3.6/site-packages/spacy_transformers/pipeline/tok2vec.py", line 198, in set_annotations
    doc.tensor[i] = wp_weighted[word_piece_slice].sum(0)
  File "cupy/core/core.pyx", line 1185, in cupy.core.core.ndarray.__getitem__
  File "cupy/core/_routines_indexing.pyx", line 31, in cupy.core._routines_indexing._ndarray_getitem
  File "cupy/core/_routines_indexing.pyx", line 204, in cupy.core._routines_indexing._prepare_slice_list
IndexError: too many indices for array
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='mmy12580' date='2019-10-08T22:55:43Z'>
		Thanks, we'll look into this. It looks like the problem was introduced here: &lt;denchmark-link:https://github.com/explosion/spacy-transformers/pull/67&gt;#67&lt;/denchmark-link&gt;

I think it probably is just missing a check if word_piece_slice.
		</comment>
		<comment id='2' author='mmy12580' date='2019-10-09T15:28:30Z'>
		Thanks for your response sincerely. However, it is not solving the case as you suggested. As you can see the problem is IndexError for code
&lt;denchmark-code&gt;doc.tensor[i] = wp_weighted[word_piece_slice].sum(0)
&lt;/denchmark-code&gt;

 is a cupy array with shape, for example, (236, 768). If  is a general list for example [1,3,7] or even None, slicing from cupy array should be all good, but the error is too many indices for array. I am not sure if it is dependency problem from copy or not. Therefore, I leveraged Google Colab which I assume it has quite empty environment to do the same thing, and it came with same errors. If you are interested, the sharable link I posted is at &lt;denchmark-link:https://colab.research.google.com/drive/1GbSflIMPFsijR67Mni_OneckvxUgR0Ws&gt;https://colab.research.google.com/drive/1GbSflIMPFsijR67Mni_OneckvxUgR0Ws&lt;/denchmark-link&gt;
. Again, thanks for your help.
		</comment>
		<comment id='3' author='mmy12580' date='2019-10-09T17:51:58Z'>
		
Thanks for your response sincerely. However, it is not solving the case as you suggested. As you can see the problem is IndexError for code
doc.tensor[i] = wp_weighted[word_piece_slice].sum(0)

wp_weighted is a cupy array with shape, for example, (236, 768). If word_piece_slice is a general list for example [1,3,7] or even None, slicing from cupy array should be all good, but the error is too many indices for array. I am not sure if it is dependency problem from copy or not. Therefore, I leveraged Google Colab which I assume it has quite empty environment to do the same thing, and it came with same errors. If you are interested, the sharable link I posted is at https://colab.research.google.com/drive/1GbSflIMPFsijR67Mni_OneckvxUgR0Ws. Again, thanks for your help.

I added a comma to the index of wp_weighted to show that we are indexing the rows over all columns, and it seemed to fix the IndexError.
doc.tensor[i] = wp_weighted[word_piece_slice, ].sum(0)
		</comment>
		<comment id='4' author='mmy12580' date='2019-10-10T07:20:00Z'>
		Hi,
I can confirm that &lt;denchmark-link:https://github.com/tylerjthomas9&gt;@tylerjthomas9&lt;/denchmark-link&gt;
 's hotfix works for me too.
Regards
		</comment>
		<comment id='5' author='mmy12580' date='2019-10-11T16:14:09Z'>
		I can also confirm it works for me üëç
		</comment>
		<comment id='6' author='mmy12580' date='2019-10-28T14:36:28Z'>
		I also recieved this error!
Are there any updates to this or do we have to adapt code in "tok2vec.py" according to &lt;denchmark-link:https://github.com/tylerjthomas9&gt;@tylerjthomas9&lt;/denchmark-link&gt;
?
Cheers Frederik
		</comment>
		<comment id='7' author='mmy12580' date='2019-10-28T22:58:16Z'>
		Sorry for letting this slip. We were travelling to PyCon India when this was reported, and I didn't realise we hadn't pushed out the fix yet :(. It should be up today.
		</comment>
		<comment id='8' author='mmy12580' date='2019-10-28T23:21:26Z'>
		Okay, should be up now.
		</comment>
	</comments>
</bug>