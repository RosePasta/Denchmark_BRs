<bug id='169' author='ZhuoruLin' open_date='2020-04-16T17:40:02Z' closed_time='2020-04-21T10:48:54Z'>
	<summary>Alignment truncation causes error in tok2vec</summary>
	<description>
Hi &lt;denchmark-link:https://github.com/adrianeboyd&gt;@adrianeboyd&lt;/denchmark-link&gt;
 ,
I noticed that the current master branch, which contains the changes that you made to move word pieces truncation to TransformersWordpiecer pipeline, would cause index error in TransformersTok2vec pipeline. Please refer to the mini example below:
&lt;denchmark-code&gt;import spacy_transformers
import spacy

nlp = spacy_transformers.language.TransformersLanguage(trf_name='test', meta={'lang':'en'})

text = "We need to fix alignment!" * 100

doc = nlp.make_doc(text)

trf_wordpiecer = spacy_transformers.pipeline.wordpiecer.TransformersWordPiecer.from_pretrained(
    nlp.vocab, 
    trf_name='distilbert-base-uncased'
)

doc = trf_wordpiecer(doc)

trf_tok2vec = spacy_transformers.pipeline.tok2vec.TransformersTok2Vec.from_pretrained(
    nlp.vocab,
    name='distilbert-base-uncased'
)

trf_tok2vec(doc)
&lt;/denchmark-code&gt;

causing IndexError
&lt;denchmark-code&gt;---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
&lt;ipython-input-1-cc2b353b58db&gt; in &lt;module&gt;
     22 )
     23 
---&gt; 24 trf_tok2vec(doc)

~/miniconda3/envs/spacy_dev/lib/python3.6/site-packages/spacy_transformers/pipeline/tok2vec.py in __call__(self, doc)
    105         self.require_model()
    106         outputs = self.predict([doc])
--&gt; 107         self.set_annotations([doc], outputs)
    108         return doc
    109 

~/miniconda3/envs/spacy_dev/lib/python3.6/site-packages/spacy_transformers/pipeline/tok2vec.py in set_annotations(self, docs, activations)
    192             # a weighted sum, so that we can make sure doc.tensor.sum()
    193             # equals wp_tensor.sum(). Do this with sensitivity to boundary tokens
--&gt; 194             wp_rows, align_sizes = _get_boundary_sensitive_alignment(doc)
    195             wp_weighted = wp_tensor / xp.array(align_sizes, dtype="f").reshape((-1, 1))
    196             # TODO: Obviously incrementing the rows individually is bad. How

~/miniconda3/envs/spacy_dev/lib/python3.6/site-packages/spacy_transformers/pipeline/tok2vec.py in _get_boundary_sensitive_alignment(doc)
    212         wp_rows.append(list(word_piece_slice))
    213         for i in word_piece_slice:
--&gt; 214             align_sizes[i] += 1
    215     # To make this weighting work, we "align" the boundary tokens against
    216     # every token in their sentence. The boundary tokens are otherwise

IndexError: list index out of range
&lt;/denchmark-code&gt;

I think this is due to the fact that in



spacy-transformers/spacy_transformers/pipeline/wordpiecer.py


        Lines 101 to 103
      in
      bb80344






 for idx in range(max_seq_length, len(seg_align)): 



 seg_align[idx] = [] 



 assert len(segment) == len(seg_align) 





alignments that is after poistion 511 are set to []. But this still allows the element inside alignment to be larger than 511.
We have a bugfix branch in &lt;denchmark-link:https://github.com/bomboradata/spacy-transformers/tree/bugfix/wordpiecer_alignment_truncate&gt;https://github.com/bomboradata/spacy-transformers/tree/bugfix/wordpiecer_alignment_truncate&lt;/denchmark-link&gt;
 that solves this issue.
Can you take a look and tell us your opinion?
Thanks in advance!
	</description>
	<comments>
		<comment id='1' author='ZhuoruLin' date='2020-04-17T15:36:39Z'>
		That does look like the right fix, thanks!
		</comment>
	</comments>
</bug>