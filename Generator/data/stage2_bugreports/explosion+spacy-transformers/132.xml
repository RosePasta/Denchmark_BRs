<bug id='132' author='adrianeboyd' open_date='2020-02-04T11:41:31Z' closed_time='2020-02-25T13:39:59Z'>
	<summary>Batching leads to different tensors</summary>
	<description>
Batching leads to different tensors with transformers models on both CPU and GPU.
Previously discussed here: &lt;denchmark-link:https://github.com/explosion/spaCy/issues/4766&gt;explosion/spaCy#4766&lt;/denchmark-link&gt;

Here's an example script to reproduce this issue (from &lt;denchmark-link:https://github.com/explosion/spaCy/issues/4766#issuecomment-577870808&gt;explosion/spaCy#4766 (comment)&lt;/denchmark-link&gt;
):
&lt;denchmark-code&gt;import spacy
import numpy as np

nlp = spacy.load('en_trf_bertbaseuncased_lg')

messages = ["Hello my name is daniel", "I would like to participate in interesting events",
            "something is very strange with SpaCy", "The vectors aren't similar",
            "let us try to round both vectors to 5 decimals"]

docs_by_batch = nlp.pipe(messages, batch_size=2)
docs_standard = [nlp(message) for message in messages]

for doc_by_batch, doc_standard in zip(docs_by_batch, docs_standard):
    if np.array_equal(np.around(doc_by_batch.tensor, 2), np.around(doc_standard.tensor, 2)):
        print("Identical Vectors")
    else:
        print("Different Vectors")
&lt;/denchmark-code&gt;

In some quick tests, I saw differences with all spacy-transformers models and I couldn't see an immediate pattern in terms of batch size or the position of a document within a batch.
	</description>
	<comments>
		<comment id='1' author='adrianeboyd' date='2020-02-18T20:17:12Z'>
		Hi &lt;denchmark-link:https://github.com/adrianeboyd&gt;@adrianeboyd&lt;/denchmark-link&gt;
, I think we might bump into the same issue with . I would be grateful if you can take a look at issue &lt;denchmark-link:https://github.com/explosion/spacy-transformers/issues/135&gt;#135&lt;/denchmark-link&gt;
 and tell me what you think.
		</comment>
		<comment id='2' author='adrianeboyd' date='2020-02-25T13:39:59Z'>
		I think this should be fixed by &lt;denchmark-link:https://github.com/explosion/spacy-transformers/pull/136&gt;#136&lt;/denchmark-link&gt;
.
See the script in &lt;denchmark-link:https://github.com/explosion/spacy-transformers/pull/136&gt;#136&lt;/denchmark-link&gt;
 and note that  is more suitable than  for testing.
		</comment>
	</comments>
</bug>