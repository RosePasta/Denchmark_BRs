<bug id='135' author='ZhuoruLin' open_date='2020-02-18T20:12:30Z' closed_time='2020-03-05T10:22:26Z'>
	<summary>trf_tok2vec pipeline produces different output for different batch sizes</summary>
	<description>
Hi,
I noticed that trf_tok2vec pipeline produce different 'last_hidden_state`. This is not always the case but I have the following example showing different output:
&lt;denchmark-code&gt;import spacy_transformers
from spacy_transformers.pipeline.wordpiecer import TransformersWordPiecer
from spacy_transformers.pipeline.tok2vec import TransformersTok2Vec
from spacy.util import minibatch

# test_texts is a list of length 1 while test_texts 2 is a list of length 24
# Each element in the list is a piece of text
assert test_texts[0] == test_texts2[0] # This assertion passed

nlp = spacy_transformers.language.TransformersLanguage(trf_name='issue')

# wordpiecer pipe
distilbert_worpiecer_pipe = TransformersWordPiecer.from_pretrained(nlp.vocab, 'distilbert-base-uncased')
# tok2vec pipe
distilbert_tok2vec_pipe = TransformersTok2Vec.from_pretrained(nlp.vocab, 'distilbert-base-uncased')

nlp.add_pipe(distilbert_worpiecer_pipe)
nlp.add_pipe(distilbert_tok2vec_pipe)

outputs1 = list(nlp.pipe(test_texts))
outputs2 = list(nlp.pipe(test_texts2))

assert (outputs1[0]._.trf_last_hidden_state == outputs2[0]._.trf_last_hidden_state).all()
&lt;/denchmark-code&gt;

The last assertion would not pass. If you need these test cases I am glad to provide.
After examining the source code of spacy_transformers.wrapper, I noticed some potential problems with the current get_model_kwargs method:


The truncate in



spacy-transformers/spacy_transformers/wrapper.py


         Line 184
      in
      88814f5






 padded = padded[:, : self.max_length] 





would produce a different output from huggingface's tokenizer.encode method. Since trf_wordpiecer does not truncate the word pieces (because it is used to calculate the alignment), the simple truncate now would effectively making the word pieces not ending with [SEP] token.


In 


spacy-transformers/spacy_transformers/wrapper.py


         Line 186
      in
      88814f5






 neg_idx = ids &lt; 0 







I might have miss something here but I don't see how would any element of ids being negative. Since neg_idx is used to create the attention_mask of model input, it would essentially results in a mask of all ones, which would make the model pay attention to padded values.
I am currently working with spacy-transformers and I would be happy to submit pull-request to fix the problem. If it is needed. :)
	</description>
	<comments>
		<comment id='1' author='ZhuoruLin' date='2020-02-21T12:07:11Z'>
		Thanks for taking a look at this, it looks like a very serious bug. (And bear with me a bit because I'm not very familiar with any of the transformers code, so please feel free to correct me if I get any details wrong.)
You're right that the attention_mask isn't being created correctly. If I fix the attention_mask (by modifying to_padded() to fill with the provided value and providing value=-1), it seems to fix the problems with your example where you manually construct the trf_tok2vec pipeline from pretrained models, at least for non-truncated texts.
In tests using the existing  models, I still see inconsistencies with batching for every model except  (using the example in &lt;denchmark-link:https://github.com/explosion/spacy-transformers/issues/132&gt;#132&lt;/denchmark-link&gt;
), but it's such a minimal test that it probably just isn't catching the problem. I don't think that any of these details are serialized with the models, but it's possible I'm overlooking something and that's why my local changes aren't making a difference when I load the  models.
We need to figure out whether it's possible to do model-specific truncation in get_model_kwargs() or whether this should be moved into the tokenizer to be more like transformers. predict() doesn't seem to handle the truncated texts correctly, either, because it doesn't seem to modify lengths correspondingly when it calls make_activations(). I strongly suspect this should all be handled by the tokenizer and not here.
		</comment>
		<comment id='2' author='ZhuoruLin' date='2020-02-25T13:40:53Z'>
		I think these issues should be fixed by &lt;denchmark-link:https://github.com/explosion/spacy-transformers/pull/136&gt;#136&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/explosion/spacy-transformers/pull/137&gt;#137&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='3' author='ZhuoruLin' date='2020-02-25T15:22:53Z'>
		Thanks!!
		</comment>
	</comments>
</bug>