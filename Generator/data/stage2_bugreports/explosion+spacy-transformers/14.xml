<bug id='14' author='Neuronys(ghost)' open_date='2019-08-05T12:33:08Z' closed_time='2019-08-12T08:16:20Z'>
	<summary>potential misalignment with punctuation (only with xlnet) ?</summary>
	<description>
Congrats for this brand new cool extension to spacy !
I'm playing with spacy-pytorch-transformer to understand the bits and bytes, especially the token alignment. And I notice something I found not logical (I may be wrong of course) only when using the XLNET model:
alignment between spacy &amp; xlnet
[[1], [2], [3], [4], [5], [6, 7], [8, 9], [10], [11], [12], [13, 14], [], [17], [18], [19], [20], [21], [22], [23], [24, 25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43, 44], [], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57, 58], [], [61], [62], [63], [64], [65], [66], [67], [68, 69, 70, 71], [], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84, 85], [], [85, 86, 87], []]
I found [13, 14], [], [17] looking like a strange pattern because of the empty []
So I digged a little bit (lines with number are from spacy tokenizer, line(s) below from XLNET tokenizer)
9 = transfer
‚ñÅtransfer
10 = learning
‚ñÅlearning
.
11 = .
12 = You
‚ñÅYou
the punctuation token of spacy is not rattached to the punctuation found by xlnet
'‚ñÅtransfer', '‚ñÅlearning', '.', 'CLS', 'SEP', '‚ñÅYou',
The token '.' from XLNET is rattached to the token 'learning' of spacy.
The token alignment is perfect when using BERT model ('.' linked to '.'):
9 = transfer
transfer
10 = learning
learning
11 = .
.
12 = You
you
see [https://gist.github.com/Neuronys/559192d4cd8992affe21feae19b8ffb9] to easily spot the issue.
Cheers
	</description>
	<comments>
		<comment id='1' author='Neuronys(ghost)' date='2019-08-05T13:00:15Z'>
		Thanks for the report and test case. I wonder if this is related to a problem with how the special tokens are handled ü§î
One detail that's not officially documented yet are the different steps of the alignment strategy (also see our &lt;denchmark-link:https://github.com/explosion/spacy-pytorch-transformers/blob/master/examples/test_wordpiece_alignment.py&gt;alignment test script&lt;/denchmark-link&gt;
):

Align normally.
Retry alignment: If alignment fails after cleaning and normalizing both sets of tokens, try again with a more aggressive strategy that strips out all characters that are not uppercase/lowercase letters.
Force alignment: If alignment still fails, run the word-piece tokenizer on the individual spaCy tokens, so that alignment is trivial. This should always work.

By disabling the retry and force settings, I could at least confirm that the misalignment is not caused by the more aggressive alignment strategy and also occurs.
import spacy                                                                              

nlp = spacy.load("en_pytt_xlnetbasecased_lg")                                                       
text = "The main use case for pretrained transformer models is transfer learning. You load in a large generic model pretrained on lots of text, and start training on your smaller dataset with labels specific to your problem. This package has custom pipeline components that make this especially easy. We provide an example component for text categorization. Development of analogous components for other tasks should be quite straight-forward."

wp = nlp.get_pipe("pytt_wordpiecer")
wp.cfg["retry_alignment"] = False 
wp.cfg["force_alignment"] = False
assert wp.alignment_strategy == (False, False)

doc = nlp(text)
print(doc[10:12]._.pytt_alignment) # [[13, 14], []]
print(doc[37:39]._.pytt_alignment) # [[43, 44], []]
So this needs some further investigation.
		</comment>
		<comment id='2' author='Neuronys(ghost)' date='2019-08-12T08:16:20Z'>
		Fix should be merged into master and released today. Thanks for your patience!
		</comment>
	</comments>
</bug>