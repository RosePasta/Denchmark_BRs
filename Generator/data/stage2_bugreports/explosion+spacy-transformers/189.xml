<bug id='189' author='dixonhsiao' open_date='2020-05-29T03:21:06Z' closed_time='2020-06-05T18:01:21Z'>
	<summary>set self.model.max_len back to max_len before raising the error so that the model can be used continuously without restarting program/reloading model ?</summary>
	<description>
in line 165 of pipeline/wordpiecer.py , it seems that if the input to the model exceeds the maximum length of that model, then it will raise that error, but the value of self.model.max_len will remain at 1e12. Should it be set back to max_len before raising that error so that the model can be used continuously without restarting program/reloading model ? Just a suggestion and I'm not sure of the correctness of the program flow...
	</description>
	<comments>
		<comment id='1' author='dixonhsiao' date='2020-05-29T11:57:45Z'>
		Hi, thanks for the report, that does look like a bug. We're working on a complete rewrite that will avoid a lot of the current fiddly alignment code and don't want to spend too much time fixing bugs in the old code at this point, but this is simple enough that I think we can include it in the next minor release before the rewrite.
		</comment>
		<comment id='2' author='dixonhsiao' date='2020-05-29T12:04:20Z'>
		You can also save/restore the max_len without reloading the model by accessing:
&lt;denchmark-code&gt;nlp.get_pipe("trf_wordpiecer").model.max_len
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>