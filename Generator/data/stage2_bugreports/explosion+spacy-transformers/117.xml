<bug id='117' author='JulianGerhard21' open_date='2019-12-24T11:23:14Z' closed_time='2019-12-27T11:24:55Z'>
	<summary>Transformers-Compatibility Issue with SerializableDistilBertTokenizer</summary>
	<description>
spacy-transformers: 0.5.1
transformers: 2.3.0
Hi,
today I wanted to follow &lt;denchmark-link:https://github.com/huggingface/transformers/tree/master/examples/distillation&gt;HuggingFace's DistilBERT&lt;/denchmark-link&gt;
 for the german pretrained model. Following &lt;denchmark-link:https://github.com/explosion/spacy-transformers/issues/81&gt;this reply from @ines &lt;/denchmark-link&gt;
 I downloaded the ,  and the , executed the  and got the following result:
&lt;denchmark-code&gt;(ea_env) C:\Users\xx\PycharmProjects\word_embeddings\pretrained_models&gt;python init_model.py --lang de --name distilbert-base-german-cased distilbert-base-german-cased
ℹ Creating model for 'distilbert-base-german-cased' (de)
✔ Initialized the model pipeline
✔ Saved 'distilbert-base-german-cased' (de)
Pipeline: ['sentencizer', 'trf_wordpiecer', 'trf_tok2vec']
Location: distilbert-base-german-cased
✔ Model loads!
&lt;/denchmark-code&gt;

which looks good. Loading the model in spacy by providing the absolute path to spacy.load("..."), no error is thrown. However if I want to parse a sentence with Spacy, the following error occurs:
&lt;denchmark-code&gt;&gt;&gt;&gt; parsed_sentence = nlp(sentence)
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "C:\Users\xx\Envs\eva_env\lib\site-packages\spacy\language.py", line 435, in __call__
    doc = proc(doc, **component_cfg.get(name, {}))
  File "C:\Users\xx\Envs\eva_env\lib\site-packages\spacy_transformers\pipeline\wordpiecer.py", line 50, in __call__
    pieces = self.predict([doc])
  File "C:\Users\xx\Envs\eva_env\lib\site-packages\spacy_transformers\pipeline\wordpiecer.py", line 93, in predict
    seg_words = self.model.tokenize(segment.text)
  File "C:\Users\xx\Envs\eva_env\lib\site-packages\transformers\tokenization_utils.py", line 656, in tokenize
    if self.init_kwargs.get('do_lower_case', False):
AttributeError: 'SerializableDistilBertTokenizer' object has no attribute 'init_kwargs'
&lt;/denchmark-code&gt;

As far as I have read, DistilBERT is natively supported by the transformers library thus it should be by spacy-transformers or am I wrong? Now I have seen, that a requirement for spacy-tranformers is transformers &lt;2.1.0. Do you think the error is related to a version conflict?
Regards and merry christmas
Julian
	</description>
	<comments>
		<comment id='1' author='JulianGerhard21' date='2019-12-26T14:59:21Z'>
		Have you tried running the exact same code with e.g. transformers==2.0.0 ?
		</comment>
		<comment id='2' author='JulianGerhard21' date='2019-12-26T23:09:05Z'>
		Ok I could confirm the bug is present also with transformers==2.0.0.
It looks like a few crucial fields are not being serialized by the SerializableDistilBertTokenizer - submitting a PR with a fix.
		</comment>
		<comment id='3' author='JulianGerhard21' date='2019-12-27T09:37:50Z'>
		Hi &lt;denchmark-link:https://github.com/svlandeg&gt;@svlandeg&lt;/denchmark-link&gt;
 ,
thanks for testing it and submitting a solution here aswell as in NER branch. Ill test both today and give you Feedback!
Regards
Julian
		</comment>
		<comment id='4' author='JulianGerhard21' date='2019-12-27T11:13:17Z'>
		Hi &lt;denchmark-link:https://github.com/svlandeg&gt;@svlandeg&lt;/denchmark-link&gt;
,
I can confirm that your &lt;denchmark-link:https://github.com/explosion/spacy-transformers/pull/121&gt;PR&lt;/denchmark-link&gt;
 fixes the problem, however, another one occurs:
&lt;denchmark-code&gt;  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "C:\Users\\Envs\eva_env\lib\site-packages\spacy\language.py", line 435, in __call__
    doc = proc(doc, **component_cfg.get(name, {}))
  File "C:\Users\\Envs\eva_env\lib\site-packages\spacy_transformers\pipeline\tok2vec.py", line 105, in __call__
    outputs = self.predict([doc])
  File "C:\Users\\Envs\eva_env\lib\site-packages\spacy_transformers\pipeline\tok2vec.py", line 160, in predict
    return self.model.predict(docs)
  File "C:\Users\\Envs\eva_env\lib\site-packages\thinc\neural\_classes\model.py", line 133, in predict
    y, _ = self.begin_update(X, drop=None)
  File "C:\Users\\Envs\eva_env\lib\site-packages\spacy_transformers\model_registry.py", line 408, in sentence_fwd
    acts, bp_acts = layer.begin_update(sents, drop=drop)
  File "C:\Users\\Envs\eva_env\lib\site-packages\thinc\neural\_classes\feed_forward.py", line 46, in begin_update
    X, inc_layer_grad = layer.begin_update(X, drop=drop)
  File "C:\Users\\Envs\eva_env\lib\site-packages\spacy_transformers\model_registry.py", line 310, in get_features_forward
    sent_seg_ids = get_segment_ids(transformers_name, *seg_lengths)
  File "C:\Users\\Envs\eva_env\lib\site-packages\spacy_transformers\util.py", line 286, in get_segment_ids
    raise ValueError(f"Unexpected model name: {name}")
ValueError: Unexpected model name: distilbert-base-german-cased
&lt;/denchmark-code&gt;

After investigating this, I observed that unlike roberta, xlnet and others, there is no get_distilbert_segment_ids method in the util.py of the library. I added this likewise the corresponding BERT one - now everything works.
I assumed that the segment ID computation in BERT format is the same for DistilBERT.
I am now able to package and install the model in a spaCy-esque way.
		</comment>
		<comment id='5' author='JulianGerhard21' date='2019-12-27T11:14:48Z'>
		Ah, yes! We recently fixed that issue with the segment IDs: &lt;denchmark-link:https://github.com/explosion/spacy-transformers/pull/119&gt;#119&lt;/denchmark-link&gt;

I was working of the main branch (from source) so already had that fix ;-)
		</comment>
	</comments>
</bug>