<bug id='193' author='ab3llini' open_date='2020-06-09T15:16:21Z' closed_time='2020-06-16T19:19:24Z'>
	<summary>Error calculating word pieces for sentences.</summary>
	<description>
Hi,
While trying to use this amazing library with an Italian pre-trained instance of  (&lt;denchmark-link:https://huggingface.co/dbmdz/bert-base-italian-xxl-cased&gt;dbmdz/bert-base-italian-xxl-uncased&lt;/denchmark-link&gt;
) we experienced a . We believe this error applies to , no matter the language because it is bound to the model capacity (512 in this case).
After further investigation, we discovered that it is due to the length of the spacy document. In our case we were working with a document totaling 2209 tokens. After the TransformersWordPiecer call, these tokens get converted to 2555 word-piece tokens ready for BERT input.
It looks like after the maximum model sequence length capacity is reached (512 for BERT) the word-piece token alignments are not being computed at all, resulting in an Exception being raised in wordpiecer.py,  within set_annotations().
You can replicate this issue by taking any Italian document long enough (i.e. with its word-piece representation longer than 512 - 2 tokens) and running the following snippet:
&lt;denchmark-code&gt;from spacy_transformers import TransformersLanguage, TransformersWordPiecer, TransformersTok2Vec

doc = open('document.txt').read()

name = "dbmdz/bert-base-italian-xxl-uncased"

nlp = TransformersLanguage(trf_name=name, meta={"lang": "it"})
nlp.add_pipe(nlp.create_pipe("sentencizer"))
nlp.add_pipe(TransformersWordPiecer.from_pretrained(nlp.vocab, name))
nlp.add_pipe(TransformersTok2Vec.from_pretrained(nlp.vocab, name))

nlp(doc)  # Crashes if doc word pieces are more than 510 (512 - 2)

&lt;/denchmark-code&gt;

Please note that if the document is shorter, everything works fine.
For the time being, we found a workaround by manually chopping the document into smaller pieces and calling the pipeline manually, but we would really love to have this fantastic library account for longer documents on its own.
Thank you, you are amazing.
	</description>
	<comments>
		<comment id='1' author='ab3llini' date='2020-06-10T16:04:27Z'>
		Hi, thanks for the report! It looks like moving the truncation to the wordpiecer wasn't taking into account the fact that the truncation should be per sentence rather than per document. I think I have a fix...
		</comment>
		<comment id='2' author='ab3llini' date='2020-06-12T08:01:45Z'>
		If you're interested, you can try installing from source to test the fix (this will be released as v0.5.3 soon):
&lt;denchmark-code&gt;pip install https://github.com/explosion/spacy-transformers/archive/v0.5.x.zip
&lt;/denchmark-code&gt;

Feedback is appreciated if you run into any errors!
		</comment>
		<comment id='3' author='ab3llini' date='2020-06-22T15:36:57Z'>
		Hi &lt;denchmark-link:https://github.com/adrianeboyd&gt;@adrianeboyd&lt;/denchmark-link&gt;
, I just wanted to inform you that 0.5.3 forces me to use transformers 2.0.0, but I need the latest one (2.11.0). Can you fix that too? By the way, we tested even 6.0.0 but it lacks your changes and keeps on crashing.
		</comment>
		<comment id='4' author='ab3llini' date='2020-06-22T15:39:57Z'>
		Then, there is a further issue with model naming:
Within the lib you switch config depending on the name of the model.
We are using an official model from hugging face repo named dbmdz/bert-base-italian-xxl-uncased
Apparently, that dbmdz/ prefix breaks the logic within the lib
		</comment>
		<comment id='5' author='ab3llini' date='2020-06-22T15:55:48Z'>
		If you upgrade to spacy-transformers v0.6.1, it should support "dbmdz/bert-base-italian-xxl-uncased", which isn't supported in v0.5.x. Unfortunately due to changes in transformers, spacy-transformers v0.6.1 only supports up to transformers v2.8.0.
spacy-transformers v0.6.1 should include the same changes related to truncation as v0.5.3. (Please don't use v0.6.0, it has some major bugs.)
		</comment>
	</comments>
</bug>