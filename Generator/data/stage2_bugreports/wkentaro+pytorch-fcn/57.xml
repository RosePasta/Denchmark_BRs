<bug id='57' author='WeiQijie' open_date='2017-10-25T09:24:00Z' closed_time='2017-11-17T08:02:27Z'>
	<summary>A little bug in trainer.py</summary>
	<description>
Hello, wkentaro. Do you find that the training loss will suddenly become larger when a new epoch starts? I've noticed that and think it is because of the incorrect use of the training and evaluating mode of the model.
In train_epoch(), line 171, when you finish validation, I think you should change the model back into training mode.
	</description>
	<comments>
		<comment id='1' author='WeiQijie' date='2017-10-25T13:28:09Z'>
		Thank you I will check it.
		</comment>
		<comment id='2' author='WeiQijie' date='2017-10-27T06:21:09Z'>
		My pull request resolves this issue. &lt;denchmark-link:https://github.com/wkentaro/pytorch-fcn/pull/56&gt;#56&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='WeiQijie' date='2017-11-15T14:07:08Z'>
		I was getting unpredictable results and I just realized that the reason was this issue.
Came here to post it and saw it here!
		</comment>
		<comment id='4' author='WeiQijie' date='2017-11-15T17:53:53Z'>
		Sorry, currently I have no GPU left to try this. Almost all GPUs at my lab are used for other experiments.
Maybe it takes more a few days to solve this.
		</comment>
		<comment id='5' author='WeiQijie' date='2017-11-15T21:48:57Z'>
		&lt;denchmark-link:https://github.com/wkentaro&gt;@wkentaro&lt;/denchmark-link&gt;
 Thanks. I believe one only needs to set  again right after , as &lt;denchmark-link:https://github.com/WeiQijie&gt;@WeiQijie&lt;/denchmark-link&gt;
 remarked.
		</comment>
		<comment id='6' author='WeiQijie' date='2017-11-17T08:02:26Z'>
		&lt;denchmark-link:https://github.com/nolsigan&gt;@nolsigan&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/monaj07&gt;@monaj07&lt;/denchmark-link&gt;
 thanks! &lt;denchmark-link:https://github.com/wkentaro/pytorch-fcn/commit/d10dd3062824195fe5cc9d196b4af4081247bb3f&gt;d10dd30&lt;/denchmark-link&gt;
 should fix this.
		</comment>
	</comments>
</bug>