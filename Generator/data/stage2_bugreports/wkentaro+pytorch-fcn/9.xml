<bug id='9' author='Lyken17' open_date='2017-03-19T00:14:22Z' closed_time='2017-03-19T05:49:44Z'>
	<summary>A bug in loss computation</summary>
	<description>
Hi author
First I would like to thank for your sharing. During training, I notice the loss (around 30000) is obviously larger than fcn experiment on caffe(0.4 ~ 3.0). After reading your code, I find the cause. In your code &lt;denchmark-link:https://github.com/wkentaro/pytorch-fcn/blob/master/torchfcn/trainer.py#L29&gt;trainer.py&lt;/denchmark-link&gt;
,
    loss = F.nll_loss(log_p, target, weight=weight, size_average=False)
    if size_average:
        loss /= mask.sum().data[0]
Note here mask is a torch.ByteTensor, which means the data is stored in a byte, value from 0~255 (8 bits). So when you sum it up, it will easily lead to overflow (because answer is also stored in torch.ByteTensor).
For example
test1 = Variable(torch.ones(1, 256,256).type('torch.ByteTensor'))
print(test1, test1.sum())

'''
Variable containing:
( 0 ,.,.) = 
   1   1   1  ...    1   1   1
   1   1   1  ...    1   1   1
   1   1   1  ...    1   1   1
     ...       ⋱       ...    
   1   1   1  ...    1   1   1
   1   1   1  ...    1   1   1
   1   1   1  ...    1   1   1
[torch.ByteTensor of size 1x256x256]
 Variable containing:
 0
[torch.ByteTensor of size 1]
'''
The correct way should be
test1 = Variable(torch.ones(1, 256,256).type('torch.ByteTensor'))
print(test1, test1.data.sum())

'''
Variable containing:
( 0 ,.,.) = 
   1   1   1  ...    1   1   1
   1   1   1  ...    1   1   1
   1   1   1  ...    1   1   1
     ...       ⋱       ...    
   1   1   1  ...    1   1   1
   1   1   1  ...    1   1   1
   1   1   1  ...    1   1   1
[torch.ByteTensor of size 1x256x256]
 65536
'''
So to compute the loss properly, you need to change loss /= mask.sum().data[0] to loss /= mask.data.sum().
I have created &lt;denchmark-link:https://github.com/wkentaro/pytorch-fcn/pull/8&gt;a pull request&lt;/denchmark-link&gt;
 to fix this.
	</description>
	<comments>
		<comment id='1' author='Lyken17' date='2017-03-19T00:19:47Z'>
		The experiment result after fix
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/7783214/24077093/109ec344-0bff-11e7-92a1-1ce8e936978b.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='Lyken17' date='2017-03-19T00:26:17Z'>
		Thanks, but the result should not be affected because I use  in trainer.py in actual.
And I think it is same configuration with the original project: &lt;denchmark-link:https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/voc-fcn32s/net.py#L64&gt;https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/voc-fcn32s/net.py#L64&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='Lyken17' date='2017-03-19T05:49:44Z'>
		Oops, careless me. I forget I have turned on the size_average. But thanks to it, a potential bug is avoided. XD
		</comment>
		<comment id='4' author='Lyken17' date='2017-03-19T08:03:48Z'>
		Yeah, Thanks!
		</comment>
	</comments>
</bug>