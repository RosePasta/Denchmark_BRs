<bug id='410' author='ekinakyurek' open_date='2018-12-07T16:39:42Z' closed_time='2019-01-05T17:20:30Z'>
	<summary>cannot take derrivative of a scalar function</summary>
	<description>
I have strange issue, not sure it is a bug. I am sure that loss is a scalar function.
using Knet,LinearAlgebra
h(x) = exp(-x); h‚Ä≤(x,y) = -y
ùìÅ(x,y) = sum(abs2,x-y)/2
function neural_net(mparams, input; h=h, h‚Ä≤=h‚Ä≤, N=length(mparams))
    Œ¥ = [];
    X = Any[input];
    for i=1:N
        x = sum(mparams[i] .* [X[i],1])
        y = h.(x) 
        push!(Œ¥, h‚Ä≤.(x,y))
        push!(X,y)
    end
    return X,Œ¥
end
mparams =[[randn(),randn()] for i=1:3]
P = Param(mparams)
loss(P,x,y)= ùìÅ(neural_net(P,x)[1][end],y)
x,y=randn(),randn()
J = @diff loss(P,x,y)
julia&gt; J = @diff loss(P,x,y)
ERROR: Only scalar valued functions supported.
Stacktrace:
 [1] error(::String) at ./error.jl:33
 [2] #differentiate#1(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function) at /kuacc/users/eakyurek13/.julia/packages/AutoGrad/eAmjh/src/core.jl:61
 [3] differentiate(::Function) at /kuacc/users/eakyurek13/.julia/packages/AutoGrad/eAmjh/src/core.jl:45
 [4] top-level scope at none:0
julia&gt; loss(P,x,y)
0.0036112795041667906
	</description>
	<comments>
		<comment id='1' author='ekinakyurek' date='2018-12-12T20:44:52Z'>
		In a @diff context, the return value seems to have multiple embedded Results:
R(R(R(8.982629056671427e32)))
and value(ans) does not strip all of them, therefore is not recognized as a Number. Need to see why this is happening, whether it is a valid problem and if so what is the best way to solve it.
		</comment>
		<comment id='2' author='ekinakyurek' date='2019-01-05T17:20:30Z'>
		Since this is an AutoGrad problem, I am moving it to &lt;denchmark-link:https://github.com/denizyuret/AutoGrad.jl/issues/106&gt;denizyuret/AutoGrad.jl#106&lt;/denchmark-link&gt;
  and closing here.
		</comment>
	</comments>
</bug>