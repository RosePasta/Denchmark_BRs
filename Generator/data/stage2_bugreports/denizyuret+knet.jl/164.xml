<bug id='164' author='ilkerkesen' open_date='2017-09-23T11:56:19Z' closed_time='2017-09-27T07:39:37Z'>
	<summary>Extreme slowness while using dropout layer with Julia v0.6</summary>
	<description>
&lt;denchmark-code&gt;# without dropout
julia&gt; for ijk=1:10; @time this_loss, this_words = train!(w, s, images, captions, masks, opts, o); end
  0.531813 seconds (202.65 k allocations: 11.514 MiB)
  0.378499 seconds (21.04 k allocations: 1.026 MiB)
  0.459622 seconds (21.86 k allocations: 1.057 MiB)
  0.483142 seconds (22.62 k allocations: 1.068 MiB)
  0.503123 seconds (22.72 k allocations: 1.070 MiB)
  0.497833 seconds (31.63 k allocations: 1.265 MiB, 2.75% gc time)
  0.114891 seconds (18.63 k allocations: 955.672 KiB)
  0.108818 seconds (18.63 k allocations: 955.672 KiB)
  0.104394 seconds (18.63 k allocations: 955.672 KiB)
  0.107155 seconds (18.66 k allocations: 957.422 KiB)

# with dropout
julia&gt; for ijk=1:10;@time this_loss, this_words = train!(w, s, images, captions, masks, opts, o);end
109.294758 seconds (10.05 M allocations: 914.646 MiB, 0.18% gc time)
108.792262 seconds (9.94 M allocations: 909.116 MiB, 0.17% gc time)
# I interrupted operation after two lossgradient/update! call

# example, how I use dropout
x = dropout(x, vembdrop)
&lt;/denchmark-code&gt;

Using current master branches (both Knet/AutoGrad).
	</description>
	<comments>
		<comment id='1' author='ilkerkesen' date='2017-09-25T10:29:04Z'>
		I had the same slowness issue in julia 0.5.2, I benchmarked Knet versions of 0.8.3 and 0.8.4. The latest version of Knet was 25 times slower when dropout used but no speed difference otherwise.
		</comment>
		<comment id='2' author='ilkerkesen' date='2017-09-27T06:51:14Z'>
		Fixed in latest master, please check:
&lt;denchmark-code&gt;25f61f2 2017-09-27 Fixed dropout efficiency bug introduced in 0.8.4
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='ilkerkesen' date='2017-09-27T07:39:37Z'>
		It seems working, I'm closing this issue.
		</comment>
	</comments>
</bug>