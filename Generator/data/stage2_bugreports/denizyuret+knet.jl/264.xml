<bug id='264' author='CarloLucibello' open_date='2018-01-31T10:12:11Z' closed_time='2018-09-12T19:18:05Z'>
	<summary>ERROR: curand.curandGenerateNormal error 102</summary>
	<description>
occasionally when calling randn!(similar(a))) when a is a KnetArray
	</description>
	<comments>
		<comment id='1' author='CarloLucibello' date='2018-01-31T14:37:02Z'>
		I cannot seem to replicate the error. Could you provide a self-contained snippet that exhibits the problem?
		</comment>
		<comment id='2' author='CarloLucibello' date='2018-01-31T18:48:14Z'>
		You may also look at curand.h to see what error 102 is.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Wed, Jan 31, 2018 at 17:37 Ulaş Sert ***@***.***&gt; wrote:
 I cannot seem to replicate the error. Could you provide a self-contained
 snippet that exhibits the problem?

 —
 You are receiving this because you are subscribed to this thread.
 Reply to this email directly, view it on GitHub
 &lt;#264 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABvNpmls2PLqzEYaJEIde-USgFWE9uwSks5tQHqPgaJpZM4RzuKn&gt;
 .



		</comment>
		<comment id='3' author='CarloLucibello' date='2018-01-31T18:53:46Z'>
		As far as I've gathered it is CURAND_STATUS_ALLOCATION_FAILED. I've tried calling the code with a sufficiently big KnetArray but I've failed to allocate a KnetArray due to limited memory before getting the curand error.
		</comment>
		<comment id='4' author='CarloLucibello' date='2018-02-01T11:25:30Z'>
		It could be CURAND itself does not have enough memory to initialized.  This
may happen if all of GPU memory is filled with KnetArrays before the first
CURAND call is made.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Wed, Jan 31, 2018 at 9:53 PM Ulaş Sert ***@***.***&gt; wrote:
 As far as I've gathered it is CURAND_STATUS_ALLOCATION_FAILED. I've tried
 calling the code with a sufficiently big KnetArray but I've failed to
 allocate a KnetArray due to limited memory before getting the curand error.

 —
 You are receiving this because you commented.


 Reply to this email directly, view it on GitHub
 &lt;#264 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABvNpi-vdBLbDERTxrZsl7P4laRvxx5sks5tQLa7gaJpZM4RzuKn&gt;
 .



		</comment>
		<comment id='5' author='CarloLucibello' date='2018-02-01T12:44:34Z'>
		Hi, I'm sorry I cannot come with a reduced example, but essentially I encounter the problem when I run a certain script for the second time  from the REPL, something like:
julia&gt; include("scipt.jl")

julia&gt; main()
## many calls to randn! but ok here

julia&gt; main()
....
ERROR: curand.curandGenerateNormal error 102 
...
I tried to call knetgc() at the begin of main but doesn't seem to fix the issue
		</comment>
		<comment id='6' author='CarloLucibello' date='2018-02-02T12:37:05Z'>
		I have managed to replicate the error after successfully calling randn! once. Are you calling setseed inside the script? If so that will regenerate the RNG which can lead to 102 error if there is not enough memory to actually regenerate it.
Calling knetgc() should free the KnetPtrs that had been reclaimed by julia gc, but since that didn't work perhaps your objects in the main() aren't reclaimed by julia gc.
		</comment>
		<comment id='7' author='CarloLucibello' date='2018-02-02T14:15:17Z'>
		I was using 'setseed'  indeed, thanks &lt;denchmark-link:https://github.com/Sophylax&gt;@Sophylax&lt;/denchmark-link&gt;
. Maybe  i was doing setseed before knetgc(), I don't know, it was some rapidly changing research code. If 'setseed' is the problem, should we change it to call 'knetgc()' internally?
		</comment>
		<comment id='8' author='CarloLucibello' date='2018-02-02T15:49:14Z'>
		knetgc is really slow, not everybody may want the cost.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Fri, Feb 2, 2018 at 17:15 Carlo Lucibello ***@***.***&gt; wrote:
 I was using 'setseed' indeed, thanks @Sophylax
 &lt;https://github.com/sophylax&gt;. Maybe i was doing setseed before knetgc(),
 I don't know, it was some rapidly changing research code. If 'setseed' is
 the problem, should we change it to call 'knetgc()' internally?

 —
 You are receiving this because you commented.


 Reply to this email directly, view it on GitHub
 &lt;#264 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABvNppWzw4IzRz6QKXOSnzQXqClzw3o6ks5tQxh2gaJpZM4RzuKn&gt;
 .



		</comment>
		<comment id='9' author='CarloLucibello' date='2018-02-03T16:32:03Z'>
		but setseed is called rarely, so maybe the impact would not be relevant. Otherwise, some way to point the user to the source of the error should be thought of.
		</comment>
		<comment id='10' author='CarloLucibello' date='2018-02-25T15:50:19Z'>
		My experiments with calls to setseed, rand! and randn! monitored using Knet.gpuinfo() indicate that the setseed in fact does not allocate extra space (possibly because I do a curandInit during initialization) but the first call to rand! or randn! still does. Need to understand exactly why rand! or randn! is allocating to solve this problem.
		</comment>
		<comment id='11' author='CarloLucibello' date='2018-04-07T05:01:00Z'>
		I came across this error today, triggered by the  call in . One of the workarounds in &lt;denchmark-link:https://github.com/denizyuret/Knet.jl/issues/181&gt;#181&lt;/denchmark-link&gt;
 was to run something like  immediately after initializing the GPU, and that also worked in my case to prevent the error.
A minimal example to reproduce this behavior seems like it might be hardware dependent.  In the code I'm working with, the accuracy of a newly initialized network is calculated on the train/test sets (during which the dropout layer doesn't call rand!), and then the error occurs on the first training minibatch.  I'm using a K80, and immediately before that first training batch, the nvidia-smi shows that the memory usage is 11421 / 11441 MB.
The error occurs on the second call to dropout in training mode. This is a Lenet-ish model with dropout applied to the input data, and then to the output of the first convolution layer, and so on; the error occurs when calling dropout on the output of the first convolution.
The top of the error message is below:
&lt;denchmark-code&gt;(:epoch, 0, :training, 0.4371035940803383, :validation, 0.4514666666666667)
ERROR: LoadError: curand.curandGenerateUniform error 102
Stacktrace:
 [1] macro expansion at ~/.julia/v0.6/Knet/src/gpu.jl:18 [inlined]
 [2] rand!(::Knet.KnetArray{Float32,4}) at ~/.julia/v0.6/Knet/src/random.jl:4
 [3] dropout!(::Float64, ::Knet.KnetArray{Float32,4}, ::Knet.KnetArray{Float32,4}) at ~/.julia/v0.6/Knet/src/dropout.jl:53
 [4] #dropout#566(::Int64, ::Bool, ::Function, ::Knet.KnetArray{Float32,4}, ::Float64) at ~/.julia/v0.6/Knet/src/dropout.jl:20
 [5] (::Knet.#kw##dropout)(::Array{Any,1}, ::Knet.#dropout, ::Knet.KnetArray{Float32,4}, ::Float64) at ./&lt;missing&gt;:0
 [6] (::AutoGrad.##rfun#7#10{Knet.#dropout})(::Array{Any,1}, ::Function, ::AutoGrad.Rec{Knet.KnetArray{Float32,4}}, ::Vararg{Any,N} where N) at ~/.julia/v0.6/AutoGrad/src/core.jl:124
 [7] (::AutoGrad.#kw##rfun#9)(::Array{Any,1}, ::AutoGrad.#rfun#9, ::AutoGrad.Rec{Knet.KnetArray{Float32,4}}, ::Vararg{Any,N} where N) at ./&lt;missing&gt;:0
 [8] dropout(::AutoGrad.Rec{Knet.KnetArray{Float32,4}}, ::Float64) at ~/.julia/v0.6/Knet/src/dropout.jl:44
 [9] #predict#13(::Float64, ::Function, ::AutoGrad.Rec{Array{Knet.KnetArray{Float32,N} where N,1}}, ::Knet.KnetArray{Float32,4}) at ~/src/knet-research/knet-cnn-coda.jl:288
&lt;/denchmark-code&gt;

		</comment>
		<comment id='12' author='CarloLucibello' date='2018-09-02T03:19:53Z'>
		I have not seen this problem since I started running curandInit() in gpu.jl every time a gpu is initialized (which generates some random numbers so initialization is taken care of). A more general solution is to prevent Knet from grabbing all available GPU memory by default, which is in 1.1 todo list. Can anybody still replicate this problem with newer Knet versions?
		</comment>
	</comments>
</bug>