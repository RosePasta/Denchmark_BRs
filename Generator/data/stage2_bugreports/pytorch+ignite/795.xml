<bug id='795' author='vfdev-5' open_date='2020-02-19T11:11:52Z' closed_time='2020-04-28T13:42:35Z'>
	<summary>Dataflow randomness problem with infinite data iterators</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug description&lt;/denchmark-h&gt;

import torch
from ignite.engine import Engine, Events

def args_printing_wrapper(fn, prefix=""):    
    def wrapper(*args, **kwargs):
        print(prefix, *args, **kwargs)
        fn(*args, **kwargs)
    return wrapper

def random_train_data_generator():
    while True:
        yield torch.randint(0, 100, size=(4, ))

def random_val_data_generator():
    while True:
        yield torch.randint(0, 100, size=(4, )) + 100

def print_train_data(engine, batch):
    i = engine.state.iteration
    e = engine.state.epoch
    print("train", e, i, batch)

def print_val_data(engine, batch):
    i = engine.state.iteration
    print("val", i, batch)
    
trainer = Engine(print_train_data)
evaluator = Engine(print_val_data)

trainer._manual_seed = args_printing_wrapper(trainer._manual_seed, prefix="trainer")
evaluator._manual_seed = args_printing_wrapper(evaluator._manual_seed, prefix="evaluator")

@trainer.on(Events.EPOCH_COMPLETED)
def run_evaluation(_):
    evaluator.run(random_val_data_generator(), epoch_length=4)

trainer.run(random_train_data_generator(), max_epochs=4, epoch_length=6);
gives
&lt;denchmark-code&gt;trainer 12 0
train 1 1 tensor([63,  3, 22, 77])
train 1 2 tensor([93, 86, 19, 83])
train 1 3 tensor([88, 20, 74, 93])
train 1 4 tensor([52, 25, 49, 41])
train 1 5 tensor([82, 87, 58, 18])
train 1 6 tensor([72,  9, 32, 64])
evaluator 12 0
val 1 tensor([163, 103, 122, 177])
val 2 tensor([193, 186, 119, 183])
val 3 tensor([188, 120, 174, 193])
val 4 tensor([152, 125, 149, 141])
train 2 7 tensor([82, 87, 58, 18])
train 2 8 tensor([72,  9, 32, 64])
train 2 9 tensor([77, 98, 57, 51])
train 2 10 tensor([58, 76, 39, 45])
train 2 11 tensor([82, 39, 27, 60])
train 2 12 tensor([58, 90,  2, 92])
evaluator 12 0
val 1 tensor([163, 103, 122, 177])
val 2 tensor([193, 186, 119, 183])
val 3 tensor([188, 120, 174, 193])
val 4 tensor([152, 125, 149, 141])
train 3 13 tensor([82, 87, 58, 18])
train 3 14 tensor([72,  9, 32, 64])
train 3 15 tensor([77, 98, 57, 51])
train 3 16 tensor([58, 76, 39, 45])
train 3 17 tensor([82, 39, 27, 60])
train 3 18 tensor([58, 90,  2, 92])
evaluator 12 0
val 1 tensor([163, 103, 122, 177])
val 2 tensor([193, 186, 119, 183])
val 3 tensor([188, 120, 174, 193])
val 4 tensor([152, 125, 149, 141])
train 4 19 tensor([82, 87, 58, 18])
train 4 20 tensor([72,  9, 32, 64])
train 4 21 tensor([77, 98, 57, 51])
train 4 22 tensor([58, 76, 39, 45])
train 4 23 tensor([82, 39, 27, 60])
train 4 24 tensor([58, 90,  2, 92])
evaluator 12 0
val 1 tensor([163, 103, 122, 177])
val 2 tensor([193, 186, 119, 183])
val 3 tensor([188, 120, 174, 193])
val 4 tensor([152, 125, 149, 141])
&lt;/denchmark-code&gt;

The problem is that _manual_seed is not called in this case and randomness of the data is configured by evaluator. Thus we have the same data all other epochs.
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version (e.g., 1.4): 1.4
Ignite Version (e.g., 0.3.0): 0.4.0.dev20200206
OS (e.g., Linux): Linux
How you installed Ignite (conda, pip, source): pip
Python version: 3.7
Any other relevant information:

	</description>
	<comments>
		<comment id='1' author='vfdev-5' date='2020-02-21T11:05:12Z'>
		WIth &lt;denchmark-link:https://github.com/pytorch/ignite/pull/799&gt;#799&lt;/denchmark-link&gt;
 we do not have the same data all other epochs anymore as evaluator's seed is now random. But if we provide a fixed seed , then training dataflow will be the same after each evaluation part.
    size = 1
    def random_train_data_generator():
        while True:
            yield torch.randint(0, 100, size=(size,))

    def random_val_data_generator():
        while True:
            yield torch.randint(0, 100, size=(size,)) + 100

    train_only_batches = []
    def train_fn(engine, batch):
        train_only_batches.append(batch[0].item())

    trainer = Engine(train_fn)
    trainer.run(random_train_data_generator(), max_epochs=4, epoch_length=6, seed=1)

    def val_fn(engine, batch):
        pass

    evaluator = Engine(val_fn)
    train_batches = []
    def train_fn2(engine, batch):
        train_batches.append(batch[0].item())

    trainer = Engine(train_fn2)

    @trainer.on(Events.EPOCH_COMPLETED)
    def run_evaluation(_):
        evaluator.run(random_val_data_generator(), epoch_length=4, seed=2)

    trainer.run(random_train_data_generator(), max_epochs=4, epoch_length=6, seed=1)
    print("train_only_batches:\n", train_only_batches)
    print("train_batches:\n", train_batches)
and output
&lt;denchmark-code&gt;train_only_batches:
 [45, 39, 24, 68, 63, 13, 91, 41, 59, 32, 48, 49, 16, 43, 13, 40, 2, 21, 42, 16, 50, 93, 46, 4]
train_batches:
 [45, 39, 24, 68, 63, 13, 58, 79, 42, 15, 60, 11, 58, 79, 42, 15, 60, 11, 58, 79, 42, 15, 60, 11]
&lt;/denchmark-code&gt;

The problem is about if trainer's random dataflow should / should not be impacted by the randomness of attached handlers.
To deal with that we in fully general case should syncronize the random state on each iteration, such that fetched batch is determined (perf overhead?).
Sketch on what happens while dealing with inf iterators:
&lt;denchmark-code&gt;set seed
|
v        ep-len
|-training-|           
           |-evalution =&gt; set seed-|
                                   |-training-no-set-seed-|
                                                          |-evalution =&gt; set seed-|
&lt;/denchmark-code&gt;

EDIT:
The fact that Engine calls torch.manual_seed can be problematic for all use-cases. To make it more lean, a solution could be to set it as an option (when False dataflow is no longer controlled by the Engine).
		</comment>
	</comments>
</bug>