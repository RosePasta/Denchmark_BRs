<bug id='541' author='vfdev-5' open_date='2019-06-02T00:47:55Z' closed_time='2019-09-21T08:22:00Z'>
	<summary>Possibly a bug when try to simulate lr scheduling values</summary>
	<description>
The following code fails:
lr_scheduler = create_lr_scheduler_with_warmup(
    StepLR(optimizer, step_size=2.4, gamma=0.97),
    warmup_start_value=0.0,
    warmup_end_value=scaled_lr,
    warmup_duration=5,
    output_simulated_values=[None] * 50
)
with the error
&lt;denchmark-code&gt;ignite/ignite/contrib/handlers/param_scheduler.py in _replicate_lr_scheduler(lr_scheduler, new_optimizer_param_groups)
    489         t = torch.zeros([1], requires_grad=True)
    490         dummy_optimizer = optimizer_cls([t], lr=0.1)
--&gt; 491         dummy_optimizer.load_state_dict(lr_scheduler.optimizer.state_dict())
    492         if new_optimizer_param_groups is not None:
    493             dummy_optimizer.param_groups = new_optimizer_param_groups

/opt/conda/lib/python3.7/site-packages/torch/optim/optimizer.py in load_state_dict(self, state_dict)
    113         saved_lens = (len(g['params']) for g in saved_groups)
    114         if any(p_len != s_len for p_len, s_len in zip(param_lens, saved_lens)):
--&gt; 115             raise ValueError("loaded state dict contains a parameter group "
    116                              "that doesn't match the size of optimizer's group")
    117 

ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
&lt;/denchmark-code&gt;

In this case optimizer has a real model inside.
	</description>
	<comments>
		<comment id='1' author='vfdev-5' date='2019-06-02T11:20:33Z'>
		Another observation: we need to multiply by 1.25 warmup_end_value in order to end the warmup with warmup_end_value value. This should be fixed.
		</comment>
		<comment id='2' author='vfdev-5' date='2019-07-30T01:03:25Z'>
		&lt;denchmark-link:https://github.com/vfdev-5&gt;@vfdev-5&lt;/denchmark-link&gt;
 could you provide some more code to reproduce this issue?
		</comment>
		<comment id='3' author='vfdev-5' date='2019-07-30T08:49:05Z'>
		&lt;denchmark-link:https://github.com/anmolsjoshi&gt;@anmolsjoshi&lt;/denchmark-link&gt;
 following snippet should reproduce the error:
import torch
from torch.optim.lr_scheduler import StepLR

from ignite.contrib.handlers import create_lr_scheduler_with_warmup

from torchvision.models.resnet import resnet18

model = resnet18()

optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

output_simulated_values = [None] * 50

lr_scheduler = create_lr_scheduler_with_warmup(
    StepLR(optimizer, step_size=2.4, gamma=0.97),
    warmup_start_value=0.0,
    warmup_end_value=0.01,
    warmup_duration=5,
    output_simulated_values=output_simulated_values
)
		</comment>
		<comment id='4' author='vfdev-5' date='2019-07-31T01:34:04Z'>
		&lt;denchmark-link:https://github.com/vfdev-5&gt;@vfdev-5&lt;/denchmark-link&gt;
 updating &lt;denchmark-link:https://github.com/pytorch/ignite/blob/master/ignite/contrib/handlers/param_scheduler.py#L490&gt;this line of code&lt;/denchmark-link&gt;
 to the code below seems to resolve the issue and the tests are still passing.
Do you know if this correctly resolves the issue? I don't want things to silently fail.
dummy_optimizer = optimizer_cls(lr_scheduler.optimizer.param_groups[0]["params"], lr=0.1)
		</comment>
		<comment id='5' author='vfdev-5' date='2019-07-31T01:35:56Z'>
		Could you clarify &lt;denchmark-link:https://github.com/pytorch/ignite/issues/541#issuecomment-498022062&gt;this comment&lt;/denchmark-link&gt;
 further?
		</comment>
		<comment id='6' author='vfdev-5' date='2019-07-31T07:00:56Z'>
		&lt;denchmark-link:https://github.com/anmolsjoshi&gt;@anmolsjoshi&lt;/denchmark-link&gt;
, the only problem I feel with this fix
dummy_optimizer = optimizer_cls(lr_scheduler.optimizer.param_groups[0]["params"], lr=0.1)
is that we pass the real model's parameters to the replicated optimizer and lr scheduler. Maybe the thing wont work if there more than 1 param_groups ?
I was thinking about whether the following line is really necessary :



ignite/ignite/contrib/handlers/param_scheduler.py


         Line 491
      in
      c5dda81






 dummy_optimizer.load_state_dict(lr_scheduler.optimizer.state_dict()) 





maybe can omit it ?
Concerning &lt;denchmark-link:https://github.com/pytorch/ignite/issues/541#issuecomment-498022062&gt;#541 (comment)&lt;/denchmark-link&gt;
, I think I fixed this &lt;denchmark-link:https://github.com/pytorch/ignite/pull/556&gt;here&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='vfdev-5' date='2019-08-02T03:56:01Z'>
		&lt;denchmark-link:https://github.com/vfdev-5&gt;@vfdev-5&lt;/denchmark-link&gt;
 commenting that line out caused other tests to fail. &lt;denchmark-link:https://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py#L24-L25&gt;This error&lt;/denchmark-link&gt;
 is being raised.
Could you explain &lt;denchmark-link:https://github.com/pytorch/ignite/blob/master/tests/ignite/contrib/handlers/test_param_scheduler.py#L338-L339&gt;this test&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='8' author='vfdev-5' date='2019-08-02T07:04:41Z'>
		&lt;denchmark-link:https://github.com/anmolsjoshi&gt;@anmolsjoshi&lt;/denchmark-link&gt;
 this should check this condition:


I agree that it would be better to write it as :
with pytest.raises(ValueError, match="Optimizer passed to lr_scheduler should have a single param group"):
        LRScheduler.simulate_values(num_events=100, lr_scheduler=lr_scheduler)

commenting that line out caused other tests to fail. This error is being raised.

we can possibly set manually initial_lr into dummy optimizer ?
		</comment>
		<comment id='9' author='vfdev-5' date='2019-08-23T06:48:05Z'>
		&lt;denchmark-link:https://github.com/anmolsjoshi&gt;@anmolsjoshi&lt;/denchmark-link&gt;
 any updates on this ?
		</comment>
	</comments>
</bug>