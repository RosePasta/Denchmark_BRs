<bug id='935' author='amatsukawa' open_date='2020-04-18T03:45:52Z' closed_time='2020-04-28T13:42:35Z'>
	<summary>Improvements around ReproducibleBatchSampler.</summary>
	<description>
&lt;denchmark-h:h2&gt;ðŸš€ Feature&lt;/denchmark-h&gt;

IMHO an attempt should be made to not wrap or change the data pipeline objects. Despite the name, this ReproducibleBatchSampler seems to be about making datasets resumable from the middle of an epoch by skipping the first few examples, rather than being reproducible since it doesn't appear to set any seeds on the samplers.
(If this is not the case, I think the point is even stronger. I'd prefer not to have seeds and "reproducibility" introduced into my data pipeline in the background. They are all good things, but not when it happens without my knowledge, or without an option to disable it).
Silently wrapping objects or creating new instances is can introduce unexpected issues, eg:

point brought up in #812, about side effects.
ReproducibleBatchSampler seems to assume it's wrapping the pytorch BatchSampler. For instance, it assumes the BatchSampler has a sampler instance variable. This this the case for the pytorch class, but not required in general. BatchSampler is just a sampler where the __iter__ returns a batch of indexes.
ReproducibleBatchSampler samples all the indexes first. This isn't necessarily a trivial amount of time or memory to hold l those ints if the dataset is large.

First and foremost, I believe this new behavior should be very prominently noted in the documentation, change notes, a warning in the logs, etc. I realized after the fact this is a "note" in the run method engine,  but as a user who is upgrading from 0.2.1, I would have had no idea this was happening if it wasn't for my implementation of batch sampler trivially not being compatible.
Perhaps the behavior should be changed to: if the data loader stack is not holding an instance of ReproducibleBatchSampler, then engine simply loads and does nothing with the batches that need to be skipped upon resume. Users who wish to have the ReproducibleBatchSampler option can explicitly use this class when constructing DataLoader.
Luckily, ignite is a small library and its code is very readable (awesome, very happy user, thanks!). Upon reading the code, I see that I can simply implement my own ReproducibleBatchSampler to bypass the three concerns I pointed out above. For the case where batches are simply sampled iid with replacement out of the data, it's sufficient to simply shorten the number of elements of the first call to __iter__ when the dataset is resumed.
So this is more of a suggestion about default behavior, which I found to be somewhat unexpected.
Thanks for your work on this library!
	</description>
	<comments>
		<comment id='1' author='amatsukawa' date='2020-04-18T07:35:01Z'>
		&lt;denchmark-link:https://github.com/amatsukawa&gt;@amatsukawa&lt;/denchmark-link&gt;
 thanks for the report. Yes, we also agree on that and since v0.4.0 it will be optional. See &lt;denchmark-link:https://github.com/pytorch/ignite/pull/895&gt;#895&lt;/denchmark-link&gt;
 . Default behaviour will be to run as is without patching the dataloader and without dataflow sync.
Please, take a look at &lt;denchmark-link:https://github.com/pytorch/ignite/pull/895&gt;#895&lt;/denchmark-link&gt;
 and tell me what do you think about it ?
		</comment>
		<comment id='2' author='amatsukawa' date='2020-04-18T13:12:12Z'>
		&lt;denchmark-link:https://github.com/vfdev-5&gt;@vfdev-5&lt;/denchmark-link&gt;
 thanks for the response. Glad to see this is already being addressed. The solution in &lt;denchmark-link:https://github.com/pytorch/ignite/pull/895&gt;#895&lt;/denchmark-link&gt;
 seems good to me.
		</comment>
		<comment id='3' author='amatsukawa' date='2020-04-18T20:57:34Z'>
		I see you've merged in &lt;denchmark-link:https://github.com/pytorch/ignite/pull/938&gt;#938&lt;/denchmark-link&gt;
, thanks! I was wondering, would you consider doing a 0.3.1 release since it seems this is blocking the workflow of &lt;denchmark-link:https://github.com/pytorch/ignite/issues/795&gt;#795&lt;/denchmark-link&gt;
? After some exploration, I realized my workflow is blocked also.
It appears this currently does not work: &lt;denchmark-link:https://pytorch.org/docs/stable/data.html#disable-automatic-batching&gt;https://pytorch.org/docs/stable/data.html#disable-automatic-batching&lt;/denchmark-link&gt;
. When automatic batching is disabled, a sampler that samples a batch of indexes is passed in as the  argument, not . I think solving this is simpler, and simply involves some extra checks to see if  is  in , in which case perhaps it should expect a  as . But anyhow, this issue can be side stepped with the new changes to master if we don't need determinism.
I know I could install from a local clone, but it would be much easier to be able to install from pypi :)
EDIT: I realized you had merged that PR into a branch that is not master..
		</comment>
		<comment id='4' author='amatsukawa' date='2020-04-18T21:13:23Z'>
		
I see you've merged in #938, thanks!

&lt;denchmark-link:https://github.com/amatsukawa&gt;@amatsukawa&lt;/denchmark-link&gt;
 it is not yet merged to master due to missing updates on examples. It is merged to  branch.
Once we merge new_engine to master, you can install nightly release using pypi as
&lt;denchmark-code&gt;pip install --pre pytorch-ignite
&lt;/denchmark-code&gt;

If you are blocked by &lt;denchmark-link:https://github.com/pytorch/ignite/issues/795&gt;#795&lt;/denchmark-link&gt;
, please use the following temporary solution to keep random states
import torch
import random

def _get_rng_states():
    output = [random.getstate(), torch.get_rng_state()]
    try:
        import numpy as np

        output.append(np.random.get_state())
    except ImportError:
        pass

    return output

def _set_rng_states(rng_states):
    random.setstate(rng_states[0])
    torch.set_rng_state(rng_states[1])
    try:
        import numpy as np

        np.random.set_state(rng_states[2])
    except ImportError:
        pass

def keep_random_state(func: Callable):
    """Helper decorator to keep random state of torch, numpy and random intact
    while executing a function. For more details on usage, please see
    `"Concepts/Dataflow synchronization" &lt;https://pytorch.org/ignite/concepts.html#dataflow-synchronization&gt;`_.
    Args:
        func (callable): function to decorate
    """

    @wraps(func)
    def wrapper(*args, **kwargs):
        rng_states = _get_rng_states()
        func(*args, **kwargs)
        _set_rng_states(rng_states)

    return wrapper
Thus, you can decorate evaluation handler with keep_random_state and after the handler the training will get back its own random states.
And thanks for mentioning about this feature "automatic batching disable" (forgot about it). Yes, we need to check this too for the deterministic engine...
		</comment>
		<comment id='5' author='amatsukawa' date='2020-04-18T21:22:18Z'>
		Thanks for the response! I'm actually blocked by not being able to disable automatic batching. Do you have a suggested workaround for that? I'd love to be able to upgrade to take advantage of the new checkpointing features.
Perhaps I could inherit from Engine and just intercept the function here with a different one? 


ignite/ignite/engine/engine.py


         Line 716
      in
      012cf38






 batch_sampler = self.state.dataloader.batch_sampler 





		</comment>
		<comment id='6' author='amatsukawa' date='2020-04-18T21:27:22Z'>
		&lt;denchmark-link:https://github.com/amatsukawa&gt;@amatsukawa&lt;/denchmark-link&gt;
 we are very sorry for that ! We'll update the library soon with more stable v0.4.0 release.
I think what can be temporary done is to convert torch DataLoader into an iterator and specify epoch_length in the run:
trainer.run(map(lambda x: x, data_loader), epoch_length=len(data_loader))
Based on &lt;denchmark-link:https://github.com/pytorch/ignite/issues/714&gt;#714&lt;/denchmark-link&gt;
 (so, probably this work only on nightly release)
		</comment>
	</comments>
</bug>