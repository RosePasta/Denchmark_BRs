<bug id='615' author='miguelvr' open_date='2019-09-08T18:11:30Z' closed_time='2019-09-08T20:00:57Z'>
	<summary>MNIST Distributed example is incorrect</summary>
	<description>
The MNIST distributed example does not reduce the loss tensors, and therefore is incorrect.
By not reducing the loss, backpropagation is currently being computed on each process separately, leading to different sets of weights.
To fix this, a custom process function with a call to all_reduce needs to be passed to the engine.
Same applies to the metrics computed. They will all be unreliable since none of them is taking into account the distributed scenario.
I would say that this needs to be fixed ASAP, because it is extremely misleading for new users.
	</description>
	<comments>
		<comment id='1' author='miguelvr' date='2019-09-08T19:13:28Z'>
		&lt;denchmark-link:https://github.com/miguelvr&gt;@miguelvr&lt;/denchmark-link&gt;
 yes &lt;denchmark-link:https://github.com/pytorch/ignite/blob/master/examples/mnist/mnist_dist.py&gt;mnist_dist.py&lt;/denchmark-link&gt;
 needs to be reworked or even to be removed. For distributed example I would suggest &lt;denchmark-link:https://github.com/pytorch/ignite/tree/distrib/examples/contrib/cifar10&gt;cifar10&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='miguelvr' date='2019-09-08T19:16:09Z'>
		doesn't seem like the loss is being reduced in that example either
		</comment>
		<comment id='3' author='miguelvr' date='2019-09-08T19:18:52Z'>
		Each process has its batch loss and we log only batch loss of the master. IMO synchronization and copying at each iteration can cost (to check).
		</comment>
		<comment id='4' author='miguelvr' date='2019-09-08T19:22:05Z'>
		
By not reducing the loss, backpropagation is currently being computed on each process separately, leading to different sets of weights.

I'm not really sure about this. If we take pytorch/examples/imagenet as example of correct distributed training, there is no reduction of loss across batches : &lt;denchmark-link:https://github.com/pytorch/examples/blob/master/imagenet/main.py#L293&gt;https://github.com/pytorch/examples/blob/master/imagenet/main.py#L293&lt;/denchmark-link&gt;

Am I missing something ?
		</comment>
		<comment id='5' author='miguelvr' date='2019-09-08T19:23:50Z'>
		correct me if i'm wrong but:
if you don't call loss = torch.distributed.all_reduce(loss) before the backward call, you are computing different gradients in each process, leading to different model parameter version per process.
I've checked &lt;denchmark-link:https://github.com/facebookresearch/maskrcnn-benchmark/blob/24c8c90efdb7cc51381af5ce0205b23567c3cd21/maskrcnn_benchmark/engine/trainer.py#L76-L85&gt;maskrcnn_benchmark code&lt;/denchmark-link&gt;
 and they seem to reduce the loss before the  call.
I'm not sure if pytorch handles this "automagically" or not.
		</comment>
		<comment id='6' author='miguelvr' date='2019-09-08T19:27:41Z'>
		I would say, that
&lt;denchmark-code&gt;        loss_dict_reduced = reduce_loss_dict(loss_dict)
        losses_reduced = sum(loss for loss in loss_dict_reduced.values())
        meters.update(loss=losses_reduced, **loss_dict_reduced)
&lt;/denchmark-code&gt;

is only for correct logging and not the training.
Yes, pytorch handles this "automagically" :) See &lt;denchmark-link:https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel&lt;/denchmark-link&gt;


This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension. The module is replicated on each machine and each device, and each such replica handles a portion of the input. During the backwards pass, gradients from each node are averaged.

		</comment>
		<comment id='7' author='miguelvr' date='2019-09-08T19:29:01Z'>
		ah cool :) üëç thanks for the clarification
		</comment>
	</comments>
</bug>