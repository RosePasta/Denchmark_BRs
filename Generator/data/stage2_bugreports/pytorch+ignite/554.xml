<bug id='554' author='leomao' open_date='2019-07-15T07:58:08Z' closed_time='2019-10-07T18:36:36Z'>
	<summary>`type_as` will not move Tensor to the same device</summary>
	<description>
In some metrics, we use type_as to change the data type of tensors. But type_as does not move the tensor to the same device.



ignite/ignite/metrics/confusion_matrix.py


        Lines 73 to 90
      in
      c92838c






 def update(self, output): 



 y_pred, y = self._check_shape(output) 



 



 if y_pred.shape != y.shape: 



 y_ohe = to_onehot(y.reshape(-1), self.num_classes) 



 y_ohe_t = y_ohe.transpose(0, 1).float() 



 else: 



 y_ohe_t = y.transpose(0, 1).reshape(y.shape[1], -1).float() 



 



 indices = torch.argmax(y_pred, dim=1) 



 y_pred_ohe = to_onehot(indices.reshape(-1), self.num_classes) 



 y_pred_ohe = y_pred_ohe.float() 



 



 if self.confusion_matrix.type() != y_ohe_t.type(): 



 self.confusion_matrix = self.confusion_matrix.type_as(y_ohe_t) 



 



 self.confusion_matrix += torch.matmul(y_ohe_t, y_pred_ohe).float() 



 self._num_examples += y_pred.shape[0] 





In the code above,  if y is on a non-default cuda device, then confusion_matrix will be move to the  default cuda device at line 87. This makes line 89 fail because two tensors are not on the same device.  I think replacing type_as with to can resolve this issue.
BTW, the .float() in line 89 seems unnecessary.
	</description>
	<comments>
		<comment id='1' author='leomao' date='2019-07-15T10:17:53Z'>
		&lt;denchmark-link:https://github.com/leomao&gt;@leomao&lt;/denchmark-link&gt;
 thanks for the feedback. Yes, we need to reconsider ingite metrics for distributed configuration. Concerning confusion metrics implementation, probably we need to follow the code similar as in torchvision, see &lt;denchmark-link:https://github.com/pytorch/ignite/issues/543&gt;#543&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='leomao' date='2019-08-02T07:23:34Z'>
		&lt;denchmark-link:https://github.com/leomao&gt;@leomao&lt;/denchmark-link&gt;
 I'm working on adapting all metrics to handle distributed computations when val/test data is sampled with pytorch DistributedSampler. Could you please provide some more info in which case you have  to be on a non-default cuda device ? Thanks
		</comment>
		<comment id='3' author='leomao' date='2019-08-04T04:23:46Z'>
		Actually, it was my friends who encountered this problem.  AFAIK, they have computers with multiple GPUs and they manually use a device id like 'cuda:1' to specify which GPU they want to use.  I am not sure if they are trying to use multiple GPUs at the same time.  But I think this problem is still a valid issue.
		</comment>
		<comment id='4' author='leomao' date='2019-08-04T08:03:04Z'>
		&lt;denchmark-link:https://github.com/leomao&gt;@leomao&lt;/denchmark-link&gt;
 okay, I see, thank you. Yes, you  are  right, we need to handle properly the devices. I pushed &lt;denchmark-link:https://github.com/pytorch/ignite/pull/572&gt;#572&lt;/denchmark-link&gt;
 PR which should fix this. And more generally there is work to adapt metrcs to distributed settings, &lt;denchmark-link:https://github.com/pytorch/ignite/pull/573&gt;#573&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='leomao' date='2019-10-07T18:36:36Z'>
		Probably we can close this issue as solved as today Confusion Matrix was reimplemented in &lt;denchmark-link:https://github.com/pytorch/ignite/pull/572&gt;#572&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>