<bug id='136' author='LordZorgoth' open_date='2019-09-12T00:38:54Z' closed_time='2019-09-20T15:15:00Z'>
	<summary>Pathological memory usage in training, scaling with number of examples</summary>
	<description>
I'm using TF_KERAS=1 and running on CPU using tensorflow 1.14.
For every example I fit on, the memory usage is increasing by somewhere from 400-600MB. It seems likely that it is duplicating the weights in memory for each training example. This is to say, if I train it on 10 examples, it uses 6.5GB of memory. It I train it on 20, it uses 12GB. If I train it on 30, it uses 17GB. This obviously prevents me from using the model for any practical purpose, since I want to train it on thousands of examples.
I'm loading and fine-tuning a pretrained model from a checkpoint using
&lt;denchmark-code&gt;os.environ['TF_KERAS'] = '1'
import keras_bert
import numpy as np
import tensorflow.keras as keras
from tensorflow.keras.layers import Input, Dense, Dropout, Lambda
from tensorflow.keras.models import Model
def get_model(config, checkpoint):
    model = keras_bert.load_trained_model_from_checkpoint(config, checkpoint, trainable=True, seq_len=256)
    pooling_layer = Lambda(lambda X: X[:, 0, :],
                                        output_shape=(768,),
                                        input_shape=(256, 768))
    inputs = [Input(256), Input(256)]
    X = model(inputs)
    X = pooling_layer(X)
    X = Dropout(0.1)(X)
    X = Dense(50, activation='relu')(X)
    X = Dropout(0.1)(X)
    X = Dense(1, activation='sigmoid')(X)
    model = Model(inputs=inputs, outputs=X)    
    return model
model = get_model(config, checkpoint)
model.compile(optimizer='adam', loss='binary_crossentropy')
model.fit([input_tokens, np.zeros_like(input_tokens)], labels)
&lt;/denchmark-code&gt;

where input_tokens are token IDs and labels are binary labels. In my use case, the config and checkpoint I am using are from v1.1 of https://github.com/naver/biobert-pretrained, in case that is relevant.
	</description>
	<comments>
		<comment id='1' author='LordZorgoth' date='2019-09-12T01:30:31Z'>
		I just confirmed that the same bug is present with TF_KERAS=0 and base Keras (as in, import keras, from keras.layers import ..., etc)
		</comment>
		<comment id='2' author='LordZorgoth' date='2019-09-12T01:55:43Z'>
		The bug also exists if I remove the layers I add at the end and label with an N*256*768 array where the first dimension is always equal to the values in labels.
		</comment>
		<comment id='3' author='LordZorgoth' date='2019-09-12T03:11:36Z'>
		Perhaps you can use fit_generator instead.
		</comment>
		<comment id='4' author='LordZorgoth' date='2019-09-12T04:29:47Z'>
		Thank you for the workaround! fit_generator is also bugged, as memory usage does still scale with batch size and gets rather large (to use a batch of 16, I need 10GB of memory, even though the pretrained weights are only 400MB on disk; I do think that the weights are being unnecessarily duplicated in memory, just because the amount of extra memory per example ends up being so close to the size of the weights).
		</comment>
		<comment id='5' author='LordZorgoth' date='2019-09-12T20:59:11Z'>
		Memory usage is supposed to scale with batch size.
&lt;denchmark-link:https://github.com/google-research/bert#out-of-memory-issues&gt;https://github.com/google-research/bert#out-of-memory-issues&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='LordZorgoth' date='2019-09-13T14:07:26Z'>
		Ah, my bad. In that case, there is no bug with fit-generator, but fit is behaving oddly, as in that case, memory appears to scale with number of examples, not with batch size. Of course, creating an iterator to imitate model.fit is a trivial exercise.
		</comment>
		<comment id='7' author='LordZorgoth' date='2019-09-18T14:47:39Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>