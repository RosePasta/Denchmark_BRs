<bug id='213' author='reck1ezz' open_date='2020-12-29T05:48:12Z' closed_time='2020-12-29T12:08:13Z'>
	<summary>The method get_model to get the bert model is right?</summary>
	<description>
Describe the Bug
A clear and concise description of what the bug is.
I think the method get_encoders to get transformed feature is not right. The method get_encoders calls  get_encoder_component, but the attention_layer in get_encoder_component doesn't use mask. The attention_layer only use the parameter history_only to control the mask, and it's not right for the self-attention encoder which should not operate on the invalid padding words.
Version Info

 I'm using the latest version

Minimal Codes To Reproduce
import keras_bert

pass
	</description>
	<comments>
		<comment id='1' author='reck1ezz' date='2020-12-29T06:08:50Z'>
		The attention module uses both the history_only parameter and the given mask.
&lt;denchmark-link:https://github.com/CyberZHG/keras-self-attention/blob/f3341547271068243866b1c0ff2512baddf92068/keras_self_attention/scaled_dot_attention.py#L69-L70&gt;https://github.com/CyberZHG/keras-self-attention/blob/f3341547271068243866b1c0ff2512baddf92068/keras_self_attention/scaled_dot_attention.py#L69-L70&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='reck1ezz' date='2020-12-29T09:20:51Z'>
		Yes, the attention module in the keras_self_attention has parameter mask. But the _attention_builder returned by attention_builder only receives one parameters x (i.e. the parameter inputs in ScaledDotProductAttention) and the parameter mask is None. So only the history_only parameter controls the self-attention mask in transformer encoder.
Below is the code in keras-transformer/keras_transformer/transformer.py
def attention_builder(name,
head_num,
activation,
history_only,
trainable=True):
"""Get multi-head self-attention builder.
:param name: Prefix of names for internal layers.
:param head_num: Number of heads in multi-head self-attention.
:param activation: Activation for multi-head self-attention.
:param history_only: Only use history data.
:param trainable: Whether the layer is trainable.
:return:
"""
def _attention_builder(x):
return MultiHeadAttention(
head_num=head_num,
activation=activation,
history_only=history_only,
trainable=trainable,
name=name,
)(x)
return _attention_builder
		</comment>
		<comment id='3' author='reck1ezz' date='2020-12-29T09:28:32Z'>
		Note that the second () in MultiHeadAttention(...)(x) calls __call__(...) but not call(...).
		</comment>
		<comment id='4' author='reck1ezz' date='2020-12-29T11:24:02Z'>
		Oh, god, thand you very much. I have read the &lt;Understanding masking &amp; padding&gt; in the keras docs, and understand the _keras_mask. Thank you again taking time out to answer my questions.
		</comment>
	</comments>
</bug>