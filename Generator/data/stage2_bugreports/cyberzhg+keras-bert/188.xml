<bug id='188' author='showerage' open_date='2020-04-18T15:31:59Z' closed_time='2020-04-18T15:57:37Z'>
	<summary>The loss of MLM seems wrong</summary>
	<description>
Describe the Bug
In keras-bert, the loss of MLM is the sum of log loss of all the tokens in the sequence. However, the loss of MLM is the sum of   log loss  of the masked tokens in the original implementation of Google. Therefore, the implementation of keras-bert seems wrong.
Version Info

 I'm using the latest version

Minimal Codes To Reproduce
# bert.py line 136
masked_layer = Masked(name='MLM')([mlm_pred_layer, inputs[-1]])
# Masked.call()
def call(self, inputs, mask=None, **kwargs):
        output = inputs[0] + 0
        if self.return_masked:
            return [output, K.cast(self.compute_mask(inputs, mask)[0], K.floatx())]
        return output
# The function return all tokens rather than masked tokens.  It seems a bug.
# The same operation of google is get_masked_lm_output  in run_pretraining.py.
	</description>
	<comments>
	</comments>
</bug>