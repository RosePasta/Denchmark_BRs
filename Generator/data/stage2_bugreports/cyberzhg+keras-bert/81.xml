<bug id='81' author='ericpts' open_date='2019-06-18T08:24:39Z' closed_time='2019-06-22T03:33:50Z'>
	<summary>BaseCollectiveExecutor::StartAbort Invalid argument</summary>
	<description>
I am trying to fine-tune the bert model for sentiment analysis, and I get some internal tensorflow error:
(with TF_KERAS=1)
I am using tensorflow 2.0.0-beta1, nightly.
This is the error:
&lt;denchmark-code&gt;2019-06-18 10:20:31.766003: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1483] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2019-06-18 10:20:32.810988: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-06-18 10:20:33.302915: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: indices[27,26] = 30523 is not in [0, 30522)
	 [[{{node bert_embeddings/Embedding-Token/embedding_lookup}}]]
	 [[bert_embeddings/Encoder-12-FeedForward-Dropout/cond/then/_264/OptionalFromValue/_810]]
2019-06-18 10:20:33.304593: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: indices[27,26] = 30523 is not in [0, 30522)
	 [[{{node bert_embeddings/Embedding-Token/embedding_lookup}}]]
/cluster/home/ericst/.local/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:414: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Epoch 1/50
Traceback (most recent call last):
  File "model_bert.py", line 167, in &lt;module&gt;
    main()
  File "model_bert.py", line 155, in main
    steps_per_epoch=STEPS_PER_EPOCH,
  File "/cluster/home/ericst/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py", line 643, in fit
    use_multiprocessing=use_multiprocessing)
  File "/cluster/home/ericst/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py", line 694, in fit
    steps_name='steps_per_epoch')
  File "/cluster/home/ericst/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py", line 264, in model_iteration
    batch_outs = batch_function(*batch_data)
  File "/cluster/home/ericst/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py", line 918, in train_on_batch
    outputs = self.train_function(ins)  # pylint: disable=not-callable
  File "/cluster/home/ericst/.local/lib/python3.7/site-packages/tensorflow/python/keras/backend.py", line 3510, in __call__
    outputs = self._graph_fn(*converted_inputs)
  File "/cluster/home/ericst/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 572, in __call__
    return self._call_flat(args)
  File "/cluster/home/ericst/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 671, in _call_flat
    outputs = self._inference_function.call(ctx, args)
  File "/cluster/home/ericst/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 445, in call
    ctx=ctx)
  File "/cluster/home/ericst/.local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "&lt;string&gt;", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  indices[27,26] = 30523 is not in [0, 30522)
	 [[node bert_embeddings/Embedding-Token/embedding_lookup (defined at model_bert.py:155) ]]
	 [[bert_embeddings/Encoder-12-FeedForward-Dropout/cond/then/_264/OptionalFromValue/_810]]
  (1) Invalid argument:  indices[27,26] = 30523 is not in [0, 30522)
	 [[node bert_embeddings/Embedding-Token/embedding_lookup (defined at model_bert.py:155) ]]
0 successful operations.
0 derived errors ignored. [Op:__inference_keras_scratch_graph_52670]

Function call stack:
keras_scratch_graph -&gt; keras_scratch_graph
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='ericpts' date='2019-06-18T08:38:50Z'>
		Invalid index of token.
		</comment>
		<comment id='2' author='ericpts' date='2019-06-18T11:59:01Z'>
		When I generate the training data, I make sure that all of the tokens are in the token_dict.
Furthermore, this error only seems to happen with 2.0.0-beta1.
If I downgrade to 2.0.0-alpha, it does not occur anymore.
		</comment>
		<comment id='3' author='ericpts' date='2019-06-19T17:58:41Z'>
		I think I have an idea: in this demo &lt;denchmark-link:https://github.com/CyberZHG/keras-bert/blob/master/demo/tune/keras_bert_classification_tpu.ipynb&gt;https://github.com/CyberZHG/keras-bert/blob/master/demo/tune/keras_bert_classification_tpu.ipynb&lt;/denchmark-link&gt;

the  is composed entirely of the vocabulary that the model comes with.
In my code, I was adding extra stuff to the token dict before building the tokenizer:
token_dict['&lt;user&gt;'] = len(token_dict)
token_dict['&lt;url&gt;'] = len(token_dict)
I suppose that this is the cause of my problems, that it was trying to access an ID bigger than the vocabulary of the model.
What is then the recommended way of adding my words to the vocabulary of the model?
		</comment>
		<comment id='4' author='ericpts' date='2019-06-20T02:29:50Z'>
		You can use [unused1] to [unused99].
		</comment>
	</comments>
</bug>