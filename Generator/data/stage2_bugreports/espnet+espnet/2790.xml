<bug id='2790' author='DTDwind' open_date='2020-12-20T02:51:30Z' closed_time='2021-01-03T12:16:27Z'>
	<summary>Code-mixing Multi-GPU training: "Segmentation fault"</summary>
	<description>
&lt;denchmark-h:h3&gt;Code-mixing Multi-GPU training: "Segmentation fault"&lt;/denchmark-h&gt;

Hi, I don't know what happened, but something went wrong.
Maybe the normalize of gradient?
(asr:235) INFO: grad norm
Anybody can help with that?
Thanks!
Error logs
&lt;denchmark-code&gt;bash: line 1: 21180 Segmentation fault      (core dumped) ( asr_train.py --config conf/train_635_storng_baseline.yaml --ngpu 2 --backend pytorch --outdir exp/train_635_sp_re_pytorch_rnn_635_strong_baseline_small_test/results --tensorboard-dir tensorboard/exp/train_635_sp_re_pytorch_rnn_635_strong_baseline_small_test --debugmode 1 --dict data/lang_1char/train_635_sp_re_units.txt --debugdir exp/train_635_sp_re_pytorch_rnn_635_strong_baseline_small_test --minibatches 0 --verbose 1 --resume --train-json dump/train_635_sp_re/deltafalse/data.json --valid-json dump/dev_635_re/deltafalse/data.json --dropout-rate-decoder 0.2 ) 2&gt;&gt; exp/train_635_sp_re_pytorch_rnn_635_strong_baseline_small_test/train.log &gt;&gt; exp/train_635_sp_re_pytorch_rnn_635_strong_baseline_small_test/train.log
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/22238266/102703835-1ee58380-42af-11eb-8d83-1112b5ba4a32.png&gt;&lt;/denchmark-link&gt;


OS information: Linux 4.15.0-123-generic #126~16.04.1-Ubuntu SMP Wed Oct 21 13:48:05 UTC 2020 x86_64
python version: Python 3.7.7
espnet version: espnet 0.9.6
pytorch version pytorch 1.4.0
Git hash: 747c46d
Commit date: Tue Dec 1 12:09:32 2020 +0000

OS: Ubuntu 16.04.7 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
CMake version: version 3.14.0
Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration:
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti
Nvidia driver version: 410.48
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5
Versions of relevant libraries:
[pip3] numpy==1.19.4
[pip3] pytorch-ranger==0.1.1
[pip3] pytorch-wpe==0.0.0
[pip3] torch==1.4.0+cu100
[pip3] torch-complex==0.2.0
[pip3] torch-optimizer==0.0.1a17
[pip3] torchaudio==0.4.0
[pip3] warpctc-pytorch==0.2.1
[pip3] warprnnt-pytorch==0.1
[conda] Could not collect
Task information:

Task: ASR
ESPnet1

	</description>
	<comments>
		<comment id='1' author='DTDwind' date='2020-12-20T11:54:11Z'>
		In this situation, you could use faulthandler to catch the error.
You could follow these instructions (&lt;denchmark-link:https://stackoverflow.com/questions/16731115/how-to-debug-a-python-segmentation-fault&gt;https://stackoverflow.com/questions/16731115/how-to-debug-a-python-segmentation-fault&lt;/denchmark-link&gt;
) or use the environmental variable  to catch the error.
Most of the cases could be due to OOM. Just in case, could you test with  to see if the error is showed on CPU mode.
		</comment>
		<comment id='2' author='DTDwind' date='2020-12-20T13:00:16Z'>
		&lt;denchmark-link:https://github.com/Fhrozen&gt;@Fhrozen&lt;/denchmark-link&gt;
 Thanks for your answer, I will try and report the results.
		</comment>
		<comment id='3' author='DTDwind' date='2020-12-22T15:02:49Z'>
		
In this situation, you could use faulthandler to catch the error.
You could follow these instructions (https://stackoverflow.com/questions/16731115/how-to-debug-a-python-segmentation-fault) or use the environmental variable PYTHONFAULTHANDLER=1 to catch the error.
Most of the cases could be due to OOM. Just in case, could you test with ngpu=0 to see if the error is showed on CPU mode.

&lt;denchmark-link:https://user-images.githubusercontent.com/22238266/102901005-0d1b0080-44a8-11eb-9c59-fb17b1af1235.png&gt;&lt;/denchmark-link&gt;

This is my error log. (ngpu=2, ESPnet Releases 0.9.6)
When I test with ngpu=1, the error did not occur.
When I test with ngpu=2 and ESPnet Releases 0.9,  the error did not occur.
But I still don't know what happened...
Thanks.
		</comment>
		<comment id='4' author='DTDwind' date='2020-12-29T08:24:59Z'>
		Have you tried to decrease the batch size? Since it may be a OOM problem
		</comment>
		<comment id='5' author='DTDwind' date='2021-01-03T12:16:23Z'>
		&lt;denchmark-link:https://github.com/lyqlola&gt;@lyqlola&lt;/denchmark-link&gt;
 I tried half of barch size and same, but after I reinstall ESPnet and update my python from 3.7.7 to 3.7.9 the problem is solved, thanks.
		</comment>
	</comments>
</bug>