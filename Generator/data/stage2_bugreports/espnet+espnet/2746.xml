<bug id='2746' author='picheny-nyu' open_date='2020-12-03T02:08:58Z' closed_time='2021-01-04T01:13:53Z'>
	<summary>Problem with AMI EGS2 Recipe</summary>
	<description>
I am trying to run the egs2/ami recipe (for comparison with my MALACH run) and seem to have a problem with short utterances. I am a bit surprised because the length of the input segment in this case is 3200 samples which should be 20 frames, but the input length is reported as 5 frames (see attached). Do I need to set a minimum input segment length?
Thanks
Michael
&lt;denchmark-code&gt;2020-12-02 20:14:54,628 (beam_search:358) INFO: decoder input length: 5
2020-12-02 20:14:54,628 (beam_search:359) INFO: max output length: 5
2020-12-02 20:14:54,628 (beam_search:360) INFO: min output length: 0
2020-12-02 20:14:54,828 (batch_beam_search:321) INFO: adding &lt;eos&gt; in the last position in the loop
Traceback (most recent call last):
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/site-packages/espnet2/bin/asr_inference.py", line 447, in &lt;module&gt;
    main()
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/site-packages/espnet2/bin/asr_inference.py", line 443, in main
    inference(**kwargs)
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/site-packages/espnet2/bin/asr_inference.py", line 312, in inference
    results = speech2text(**batch)
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 15, in decorate_context
    return func(*args, **kwargs)
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/site-packages/espnet2/bin/asr_inference.py", line 194, in __call__
    x=enc[0], maxlenratio=self.maxlenratio, minlenratio=self.minlenratio
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/site-packages/espnet/nets/beam_search.py", line 369, in forward
    running_hyps = self.post_process(i, maxlen, maxlenratio, best, ended_hyps)
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/site-packages/espnet/nets/batch_beam_search.py", line 341, in post_process
    running_hyps.yseq[torch.arange(n_batch), running_hyps.length - 1]
IndexError: index 9 is out of bounds for dimension 1 with size 7
# Accounting: time=48 threads=1
# Ended (code 1) at Wed Dec  2 20:14:55 EST 2020, elapsed time 48 seconds
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='picheny-nyu' date='2020-12-03T02:11:50Z'>
		I think this is already fixed.
&lt;denchmark-link:https://github.com/espnet/espnet/issues/2483&gt;#2483&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/espnet/espnet/pull/2484&gt;#2484&lt;/denchmark-link&gt;

Did you reproduce this error with the current master?
		</comment>
		<comment id='2' author='picheny-nyu' date='2020-12-03T13:51:11Z'>
		Applying the above fixed most of the problems but two of the 10 decoding jobs failed with the following message. Again, it looks like too short an utterance. The key is:
AMI_EN2002c_H03_MEE073_0221142_0221146 dump/raw/ihm_eval/data/format.3/AMI_EN2002c_H03_MEE073_0221142_0221146.wav which looks like a 5-frame utterance, correct? Should I specify a minimum length?
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/site-packages/espnet2/bin/asr_inference.py", line 447, in &lt;module&gt;
    main()
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/site-packages/espnet2/bin/asr_inference.py", line 443, in main
    inference(**kwargs)
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/site-packages/espnet2/bin/asr_inference.py", line 312, in inference
    results = speech2text(**batch)
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 15, in decorate_context
    return func(*args, **kwargs)
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/site-packages/espnet2/bin/asr_inference.py", line 189, in __call__
    enc, _ = self.asr_model.encode(**batch)
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/site-packages/espnet2/asr/espnet_model.py", line 202, in encode
    encoder_out, encoder_out_lens, _ = self.encoder(feats, feats_lengths)
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/site-packages/espnet2/asr/encoder/transformer_encoder.py", line 167, in forward
    xs_pad, masks = self.embed(xs_pad, masks)
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/site-packages/espnet/nets/pytorch_backend/transformer/subsampling.py", line 48, in forward
    x = self.conv(x)
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/site-packages/torch/nn/modules/conv.py", line 419, in forward
    return self._conv_forward(input, self.weight)
  File "/misc/vlgscratch4/PichenyGroup/picheny/anaconda3/envs/espnet-sep032020/lib/python3.7/site-packages/torch/nn/modules/conv.py", line 416, in _conv_forward
    self.padding, self.dilation, self.groups)
RuntimeError: Calculated padded input size per channel: (2 x 39). Kernel size: (3 x 3). Kernel size can't be greater than actual input size
# Accounting: time=5844 threads=1
# Ended (code 1) at Thu Dec  3 00:29:20 EST 2020, elapsed time 5844 seconds
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='picheny-nyu' date='2020-12-04T11:55:09Z'>
		Hmm. The utterance seems to be too short.
There are several choices:

Introduce padding in the CNN of the encoder to deal with the short utterance
Change the encodoer input layer not to perform subsampling
Remove the utterance

		</comment>
		<comment id='4' author='picheny-nyu' date='2020-12-04T13:01:03Z'>
		Does this mean this recipe was not checked before on AMI? I just want to make sure I am not doing something wrong.
Also is there some way of filtering out short utterances automatically (rather than me changing the input file lists?). Or are you saying I can add padding to the encoder just for decoding (rather than retraining).
I also thought subsampling was needed to make the input lengths more similar to the output lengths for good seq2seq modeling?
Thanks
Michael
		</comment>
		<comment id='5' author='picheny-nyu' date='2020-12-04T13:42:23Z'>
		Iâ€™ve not maintained this recipe.
Do you have comments? &lt;denchmark-link:https://github.com/sw005320&gt;@sw005320&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/kamo-naoyuki&gt;@kamo-naoyuki&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='picheny-nyu' date='2020-12-04T21:17:53Z'>
		It is not maintained since it was made in 2020 March.
It was originally working but we had a couple of changes in the main espnet2 side, and this might affect it.
I can check it if I could make time at the weekend, or want &lt;denchmark-link:https://github.com/ftshijt&gt;@ftshijt&lt;/denchmark-link&gt;
 to check it
		</comment>
		<comment id='7' author='picheny-nyu' date='2020-12-07T13:23:00Z'>
		I got this working by deleting two of the test files, but the results are terrible - like a 50% error rate compared to the egs (not egs2) recipe I ran earlier in the year. I will rerun the egs recipe alone again to make sure nothing else has changed.
		</comment>
		<comment id='8' author='picheny-nyu' date='2020-12-07T16:43:22Z'>
		I also encountered the same issue during decoding, which did not happen in the previous AMI experimental trials, and it should be fixed.
It happens for AMI_TS3003d_H01_MTD011UID_0238796_0238800.
It only has a 40 ms duration.
&lt;denchmark-code&gt;$ soxi dump/raw/ihm_eval/data/format.32/AMI_TS3003d_H01_MTD011UID_0238796_0238800.flac

Input File     : 'dump/raw/ihm_eval/data/format.32/AMI_TS3003d_H01_MTD011UID_0238796_0238800.flac'
Channels       : 1
Sample Rate    : 16000
Precision      : 16-bit
Duration       : 00:00:00.04 = 640 samples ~ 3 CDDA sectors
File Size      : 499
Bit Rate       : 99.8k
Sample Encoding: 16-bit FLAC
&lt;/denchmark-code&gt;

Then, the second CNN seems to be failed due to a too-short length.
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/expscratch/swatanabe/202007espnet/espnet_v2/tools/venv/envs/espnet/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/expscratch/swatanabe/202007espnet/espnet_v2/tools/venv/envs/espnet/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/expscratch/swatanabe/202007espnet/espnet_v2/espnet2/bin/asr_inference.py", line 447, in &lt;module&gt;
    main()
  File "/expscratch/swatanabe/202007espnet/espnet_v2/espnet2/bin/asr_inference.py", line 443, in main
    inference(**kwargs)
  File "/expscratch/swatanabe/202007espnet/espnet_v2/espnet2/bin/asr_inference.py", line 312, in inference
    results = speech2text(**batch)
  File "/expscratch/swatanabe/202007espnet/espnet_v2/tools/venv/envs/espnet/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 49, in decorate_no_grad
    return func(*args, **kwargs)
  File "/expscratch/swatanabe/202007espnet/espnet_v2/espnet2/bin/asr_inference.py", line 189, in __call__
    enc, _ = self.asr_model.encode(**batch)
  File "/expscratch/swatanabe/202007espnet/espnet_v2/espnet2/asr/espnet_model.py", line 205, in encode
    encoder_out, encoder_out_lens, _ = self.encoder(feats, feats_lengths)
  File "/expscratch/swatanabe/202007espnet/espnet_v2/tools/venv/envs/espnet/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/expscratch/swatanabe/202007espnet/espnet_v2/espnet2/asr/encoder/transformer_encoder.py", line 167, in forward
    xs_pad, masks = self.embed(xs_pad, masks)
  File "/expscratch/swatanabe/202007espnet/espnet_v2/tools/venv/envs/espnet/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/expscratch/swatanabe/202007espnet/espnet_v2/espnet/nets/pytorch_backend/transformer/subsampling.py", line 54, in forward
    x = self.conv(x)
  File "/expscratch/swatanabe/202007espnet/espnet_v2/tools/venv/envs/espnet/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/expscratch/swatanabe/202007espnet/espnet_v2/tools/venv/envs/espnet/lib/python3.7/site-packages/torch/nn/modules/container.py", line 100, in forward
    input = module(input)
  File "/expscratch/swatanabe/202007espnet/espnet_v2/tools/venv/envs/espnet/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/expscratch/swatanabe/202007espnet/espnet_v2/tools/venv/envs/espnet/lib/python3.7/site-packages/torch/nn/modules/conv.py", line 345, in forward
    return self.conv2d_forward(input, self.weight)
  File "/expscratch/swatanabe/202007espnet/espnet_v2/tools/venv/envs/espnet/lib/python3.7/site-packages/torch/nn/modules/conv.py", line 342, in conv2d_forward
    self.padding, self.dilation, self.groups)
RuntimeError: Calculated padded input size per channel: (2 x 39). Kernel size: (3 x 3). Kernel size can't be greater than actual input size
&lt;/denchmark-code&gt;

Of course, if the data preparation is correct, this would be due to an irregular annotation (0.04 second is too short as one utterance), and it would be better to be considered by the AMI recipe design (that's why we did not encounter this issue in the other recipes).
However, at the same time, we should accept any utterances and correctly exit the program without having the above error...
I'm checking why it did not happen previously (or it happened but that was ignored?).
&lt;denchmark-link:https://github.com/ftshijt&gt;@ftshijt&lt;/denchmark-link&gt;
, could you check this issue when you created this recipe?
		</comment>
		<comment id='9' author='picheny-nyu' date='2020-12-07T19:35:10Z'>
		I did not remember to have this issue for decoding while creating the recipe. However, I believe we did not have any procedures regarding the issue, so it might be ignored in the previous settings. The sad thing is that I am not able to get back to previous experiments in AMI.
Given the current circumstance, I think one solution is to add a checker to see if the coming utterance is workable for sub-sampling in the inference scripts (generate an empty hypothesis if not). I could shortly update those checks if you think it is fair.
		</comment>
		<comment id='10' author='picheny-nyu' date='2020-12-07T19:37:20Z'>
		
Given the current circumstance, I think one solution is to add a checker to see if the coming utterance is workable for sub-sampling in the inference scripts (generate an empty hypothesis if not). I could shortly update those checks if you think it is fair.

Year, I think this is a good solution.
It would be great if you work on it.
		</comment>
		<comment id='11' author='picheny-nyu' date='2020-12-08T10:32:54Z'>
		In the early recipe, too short utterances in test sets were removed, but now test sets are not changed even if it has illegal  utterances.
		</comment>
		<comment id='12' author='picheny-nyu' date='2020-12-19T22:42:48Z'>
		OK so I finally reran the egs version (it took two weeks because the distributed version does not run on my version of pytorch :-( ) and still get the same good results (while for egs2 the results are horrible...)
I looked through the parameters and noticed that in the egs recipe the noam optimizer was used but in egs2 the Adam optimizer is used, but with the noamlr learning rate profile. Before I start a run with a single gpu (as a sanity check,  to parallel the egs run) is this a potential problem?
Is another problem that a wordlm cannot be used in egs2 yet?
Thanks
Michael
		</comment>
	</comments>
</bug>