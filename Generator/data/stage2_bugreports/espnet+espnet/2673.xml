<bug id='2673' author='Piteryo' open_date='2020-11-11T16:42:06Z' closed_time='2020-11-16T15:57:38Z'>
	<summary>"Operation timed out" in torch.distributed.all_reduce</summary>
	<description>
Describe the bug
On a custom dataset which is slightly bigger than some default datasets from this repo, there is a problem: when starting new epoch torch.distributed.all_reduce(iterator_stop, ReduceOp.SUM):372 in espnet2/train/trainer.py throws an exception. Restarting the script from the checkpoint of previous epoch works well for one epoch. What can cause this problem?
Basic environments:
I'm using 5 Nvidia Tesla V100 GPUs on one node.
Error logs
&lt;denchmark-code&gt;Process SpawnProcess-3:                                                                                                                               
Traceback (most recent call last):                                                                                                                    
  File "/opt/conda/envs/espnet_env/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap                                                 
    self.run()                                                                                                                                        
  File "/opt/conda/envs/espnet_env/lib/python3.6/multiprocessing/process.py", line 93, in run                                                         
    self._target(*self._args, **self._kwargs)                                                                                                         
  File "/workspace/espnet_october/espnet2/tasks/abs_task.py", line 1234, in main_worker                                                               
    distributed_option=distributed_option,                                                                                                            
  File "/workspace/espnet_october/espnet2/train/trainer.py", line 210, in run                                                                         
    options=trainer_options,                                                                                                                          
  File "/workspace/espnet_october/espnet2/train/trainer.py", line 372, in train_one_epoch                                                             
    torch.distributed.all_reduce(iterator_stop, ReduceOp.SUM)                                                                                         
  File "/opt/conda/envs/espnet_env/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 911, in all_reduce                        
    work.wait()                                                                                                                                       
RuntimeError: Operation timed out!                                                                                                                    
Process SpawnProcess-4:                                                                                                                               
Traceback (most recent call last):                                                                                                                    
  File "/opt/conda/envs/espnet_env/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap                                                 
    self.run()                                                                                                                                        
  File "/opt/conda/envs/espnet_env/lib/python3.6/multiprocessing/process.py", line 93, in run                                                         
    self._target(*self._args, **self._kwargs)                                                                                                         
  File "/workspace/espnet_october/espnet2/tasks/abs_task.py", line 1234, in main_worker                                                               
    distributed_option=distributed_option,                                                                                                            
  File "/workspace/espnet_october/espnet2/train/trainer.py", line 210, in run                                                                         
    options=trainer_options,                                                                                                                          
  File "/workspace/espnet_october/espnet2/train/trainer.py", line 372, in train_one_epoch                                                             
    torch.distributed.all_reduce(iterator_stop, ReduceOp.SUM)                                                                                         
  File "/opt/conda/envs/espnet_env/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 911, in all_reduce                        
    work.wait()                                                                                                                                       
RuntimeError: Operation timed out!
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Piteryo' date='2020-11-12T02:16:44Z'>
		Several reasons are possible. Bug or connection error. If it's connection error, it's very hard to help for me.
Please show more log. Need all errors from all processes to detect the reason.
		</comment>
		<comment id='2' author='Piteryo' date='2020-11-16T15:57:38Z'>
		It seems that setting multiprocessing_distributed to false fixes this issue.
		</comment>
		<comment id='3' author='Piteryo' date='2020-11-16T16:09:12Z'>
		You haven't resolved nothing.
multiprocessing_distributed=false disables distributed training for multi gpus training. It decrease the training speed.
		</comment>
		<comment id='4' author='Piteryo' date='2020-11-16T16:12:26Z'>
		Show all error log.
		</comment>
	</comments>
</bug>