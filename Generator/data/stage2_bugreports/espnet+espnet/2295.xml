<bug id='2295' author='gafsd' open_date='2020-08-12T06:43:59Z' closed_time='2020-08-15T03:48:31Z'>
	<summary>Vertical attention during training</summary>
	<description>
I am using ESPNet 1 ljspeech-like dataset for pretrained weights and am getting some attention graphs like this in training which I haven't seen before:
&lt;denchmark-link:https://user-images.githubusercontent.com/62999745/89983349-f8349200-dca9-11ea-8f46-2ae8b7300890.png&gt;&lt;/denchmark-link&gt;

I recently updated to latest commit and I know there are lots of ESPNet2 changes so could this be version related?
I init from pretrained tacotron v4 with BCE-pos-weight 2.0
	</description>
	<comments>
		<comment id='1' author='gafsd' date='2020-08-12T15:37:55Z'>
		Basically such changes would not cause this issue, but it might be.
It would be great if you try it with several different versions (e.g., v08, v07), and identify the issue due to the version or your ljspeech-like dataset-specific problems.
		</comment>
		<comment id='2' author='gafsd' date='2020-08-12T15:57:40Z'>
		&lt;denchmark-link:https://github.com/sw005320&gt;@sw005320&lt;/denchmark-link&gt;
 I just try on v0.8.0 and the attention graphs look normal.
		</comment>
		<comment id='3' author='gafsd' date='2020-08-12T16:07:03Z'>
		Thanks. Then, it would be a bug. &lt;denchmark-link:https://github.com/kan-bayashi&gt;@kan-bayashi&lt;/denchmark-link&gt;
, could you check this issue?
		</comment>
		<comment id='4' author='gafsd' date='2020-08-12T16:23:50Z'>
		How did you get this attention plot?
Is this a plot during training?
		</comment>
		<comment id='5' author='gafsd' date='2020-08-12T16:26:46Z'>
		Yes, from images tab in tensorboard, even after 11k steps.
I use torch 1.4.0 espnet1 commit &lt;denchmark-link:https://github.com/espnet/espnet/commit/b2d1d44668d44885d8a0ac496dd59136a8d1d1cc&gt;b2d1d44&lt;/denchmark-link&gt;
 and two GPU with v4 tacotron and pretrained model
		</comment>
		<comment id='6' author='gafsd' date='2020-08-12T17:00:43Z'>
		Thanks. Could you attach the normal attention plot for the same data in v0.8.0?
(I checked diff between v0.8.0 and v0.9.0 but I could not find the change in espnet1 tts and Tacotron2 part.)
		</comment>
		<comment id='7' author='gafsd' date='2020-08-12T17:12:13Z'>
		Sorry I did not save exact same data but here is a 0.8.0 graph after some small steps. I find these graphs helpful to track down utts with bad attention (there is sadly a limit to num-save-attention I think but I have set to 32 here).
&lt;denchmark-link:https://user-images.githubusercontent.com/62999745/90045556-c9e2a100-dd01-11ea-800d-04da2ff2d1aa.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='gafsd' date='2020-08-12T17:17:37Z'>
		Thanks.
Could you try to remove [::-1] here?



espnet/espnet/asr/asr_utils.py


         Line 115
      in
      5410797






 self.data = copy.deepcopy(data)[::-1] 





		</comment>
		<comment id='9' author='gafsd' date='2020-08-12T17:45:06Z'>
		Yes that seems to have fixed it.
&lt;denchmark-link:https://user-images.githubusercontent.com/62999745/90048820-96564580-dd06-11ea-9b24-c84d3230001a.png&gt;&lt;/denchmark-link&gt;

(different data but was wrong before change)
&lt;denchmark-link:https://user-images.githubusercontent.com/62999745/90048827-98200900-dd06-11ea-9273-30f4fcd27dd6.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='gafsd' date='2020-08-12T17:49:26Z'>
		Thank you for your checking.
It really helps us!
&lt;denchmark-link:https://github.com/hirofumi0810&gt;@hirofumi0810&lt;/denchmark-link&gt;
 Is this change for ASR or ST?
&lt;denchmark-link:https://github.com/espnet/espnet/commit/8eae4ff81f2708dc3552ae863a1b97b40a782a9a#diff-405f988e9d432ff8a43112abd8094233&gt;8eae4ff#diff-405f988e9d432ff8a43112abd8094233&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='gafsd' date='2020-08-12T17:55:32Z'>
		Thanks. If I could suggest it would be helpful to be able to see all attention plots in tensorboard if I set num-save-attention high. It is nice to check problematic input files.
		</comment>
		<comment id='12' author='gafsd' date='2020-08-15T02:53:40Z'>
		Thank you for your suggestions.
In decoding, we support the attention plot of all of the data.
It might help you.
		</comment>
	</comments>
</bug>