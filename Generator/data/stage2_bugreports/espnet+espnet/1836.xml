<bug id='1836' author='SeanNaren' open_date='2020-04-14T10:45:50Z' closed_time='2020-08-08T01:07:49Z'>
	<summary>CTC loss and feature extractor in espnet model with mixed precision</summary>
	<description>
Hey guys! When I use --train_dtype O1 as a parameter for espnet2.bin.asr_train I run into the below error message.
&lt;denchmark-code&gt;RuntimeError: "ctc_loss_cuda" not implemented for 'Half'
&lt;/denchmark-code&gt;

In deepspeech.pytorch this is handled by converting the loss to float32 as seen &lt;denchmark-link:https://github.com/SeanNaren/deepspeech.pytorch/blob/master/train.py#L253&gt;here&lt;/denchmark-link&gt;
. We've tested this internally on a variety of scales and this works well for us.
Is this something acceptable for you guys? If so I don't mind making the change via a PR!
	</description>
	<comments>
		<comment id='1' author='SeanNaren' date='2020-04-14T10:49:53Z'>
		Thanks. I know it. It should be fixed as you said.
		</comment>
		<comment id='2' author='SeanNaren' date='2020-04-14T11:01:18Z'>
		&lt;denchmark-link:https://github.com/kamo-naoyuki&gt;@kamo-naoyuki&lt;/denchmark-link&gt;
 will you be handling this? I'm more than happy to work on this if it will help you out!
		</comment>
		<comment id='3' author='SeanNaren' date='2020-04-14T11:08:03Z'>
		Thanks. If you send PR, it's helpful.
The reason I didn't is just laziness. There are too many things to do about espnet2.
		</comment>
		<comment id='4' author='SeanNaren' date='2020-04-14T11:51:59Z'>
		haha 100%, I'll make a PR today
		</comment>
		<comment id='5' author='SeanNaren' date='2020-04-14T14:46:12Z'>
		Hey &lt;denchmark-link:https://github.com/kamo-naoyuki&gt;@kamo-naoyuki&lt;/denchmark-link&gt;
 just wanted to confirm something, the  wraps everything including the extraction of the features. Does this mean if we convert the model via Apex, that the input speech tensor will be converted to FP16? This I think is causing me issues
		</comment>
		<comment id='6' author='SeanNaren' date='2020-04-14T15:12:16Z'>
		Are you concerned about O2/O3 case? Hmm, in my understanding, yes.
I don't know the detail of apex API. How should we do it if we exclude the extraction from apex?
These APIs are appropriate?
&lt;denchmark-link:https://github.com/NVIDIA/apex/tree/master/apex/amp&gt;https://github.com/NVIDIA/apex/tree/master/apex/amp&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='SeanNaren' date='2020-04-14T17:43:56Z'>
		I was running into this issue using O1, should I not be running into this issue?
Let's see if I can figure out how to exclude this from Apex so it doesn't do any conversions.
		</comment>
		<comment id='8' author='SeanNaren' date='2020-04-15T02:14:46Z'>
		I think CTC loss has reduce operation which is not suitable for mix precision training.
		</comment>
		<comment id='9' author='SeanNaren' date='2020-04-15T10:22:13Z'>
		After further inspection this is going to be a difficult problem to solve and may require modifying the pipeline.
Using Apex, there are wrappers added to the forward methods to automatically convert specific modules into half, this is done to reduce the amount of boilerplate by users.
Since feature extraction is a part of the converted model and a lot of the functions are defined as torch modules they are being wrapped as well, leading to improper feature extraction. The two solutions I see is convert the feature extractions into non torch modules, or remove feature extraction as a part of the model but I don't have a clear view of the rest of the package to decide the path, &lt;denchmark-link:https://github.com/kamo-naoyuki&gt;@kamo-naoyuki&lt;/denchmark-link&gt;
 any feedback?
		</comment>
		<comment id='10' author='SeanNaren' date='2020-04-15T12:01:20Z'>
		&lt;denchmark-link:https://github.com/SeanNaren&gt;@SeanNaren&lt;/denchmark-link&gt;
 Thank you for clarification.
Can't we use  or  for the feature extraction?
		</comment>
		<comment id='11' author='SeanNaren' date='2020-04-15T15:00:00Z'>
		I tried wrapping &lt;denchmark-link:https://github.com/espnet/espnet/blob/master/espnet2/asr/espnet_model.py#L204&gt;extract_feats&lt;/denchmark-link&gt;
 with the  as well as using the  before we convert the model via amp initialize &lt;denchmark-link:https://github.com/espnet/espnet/blob/master/espnet2/tasks/abs_task.py#L1038&gt;here&lt;/denchmark-link&gt;
 but both seemed to not work, it seems like the internal functions such as  called in  are still returning half tensors...
		</comment>
		<comment id='12' author='SeanNaren' date='2020-04-15T15:24:38Z'>
		Thanks. Hmm. I'll read the apex code for detail and think about it.
		</comment>
		<comment id='13' author='SeanNaren' date='2020-04-15T16:08:15Z'>
		Cheers if I find out anything more will let you know!
		</comment>
		<comment id='14' author='SeanNaren' date='2020-04-17T16:51:39Z'>
		We can't solve this problem in any way!
I concluded that apex.amp is non sense idea because it patches all torch functions globally regardless whether the model uses them or not.
model = Net()
model.cuda()
optimizer = torch.optim.Adam(model.parameters())

amp.initialize(model, optimizer, opt_level="O1")

x = torch.rand(2, 10).cuda()
w = torch.rand(10, 10).cuda()
print(x.matmul(w).dtype)  # torch.half
It means that any torch functions can't coexist at non training places with apex.amp.  This is crazy. They ought to implement some function to limit the scope.
I gave up to support apex.amp from now and I'll consider to use torch native amp only.
&lt;denchmark-link:https://pytorch.org/docs/master/notes/amp_examples.html&gt;https://pytorch.org/docs/master/notes/amp_examples.html&lt;/denchmark-link&gt;

The API looks sane unlike apex.amp. Maybe torch1.5 comes soon.
		</comment>
		<comment id='15' author='SeanNaren' date='2020-04-18T11:39:55Z'>
		This is unfortunate, let me make an issue on the Apex repo about this and see what the authors have to say!
		</comment>
		<comment id='16' author='SeanNaren' date='2020-04-22T02:02:33Z'>
		&lt;denchmark-link:https://github.com/SeanNaren&gt;@SeanNaren&lt;/denchmark-link&gt;
 PyTorch1.5 is out now! We can consider to move apex.amp to torch.cuda.amp to solve this issue.
		</comment>
		<comment id='17' author='SeanNaren' date='2020-04-22T14:50:27Z'>
		Awesome timing however speaking to &lt;denchmark-link:https://github.com/mcarilli&gt;@mcarilli&lt;/denchmark-link&gt;
 it seems a lot of the implementation for native AMP didn't make it into the PyTorch 1.5 release :( we should see if we can get it working regardless but if we run into a blocker it may be because of this!
		</comment>
		<comment id='18' author='SeanNaren' date='2020-04-22T23:32:27Z'>
		Do you know what implementation has not been completed yet? The official documentation is not clear for me. Should we wait until pytorch1.6?
BTW, I got an idea. The patching can be avoided by copying torch functions, so we can solve this problem if the feature extractor uses copied functions, but this way is too dirty ?
import torch
import copy
from apex import amp

model = torch.nn.Linear(10, 10)
model.cuda()
optimizer = torch.optim.Adam(model.parameters())

matmul = copy.copy(torch.matmul)
amp.initialize(model, optimizer, opt_level="O1")

x = torch.rand(2, 10).cuda()
w = torch.rand(10, 10).cuda()
print(torch.matmul(x, w).dtype)  # torch.float16
print(matmul(x, w).dtype)   # float32
Another thought, I have never tested amp actually. Does this problem affect the result so much?
		</comment>
		<comment id='19' author='SeanNaren' date='2020-04-22T23:49:08Z'>
		Apex Amp has many known pain points (extension builds, forward/backward compatibility across Pytorch versions, DataParallel support, flaky checkpointing, global enablement has silent consequences, i donâ€™t even know if it can be hacked to handle double backward/gradient penalty).
Rather than sink more time into Apex, I fixed these in the native integration.  is fully-featured in master and nightly pip/conda binaries:
&lt;denchmark-link:https://pytorch.org/docs/master/amp.html&gt;https://pytorch.org/docs/master/amp.html&lt;/denchmark-link&gt;

&lt;denchmark-link:https://pytorch.org/docs/master/notes/amp_examples.html&gt;https://pytorch.org/docs/master/notes/amp_examples.html&lt;/denchmark-link&gt;

The autocast context manager (which automatically selects precision in enabled regions) didn't make 1.5, unfortunately.
		</comment>
		<comment id='20' author='SeanNaren' date='2020-04-22T23:57:44Z'>
		&lt;denchmark-link:https://github.com/mcarilli&gt;@mcarilli&lt;/denchmark-link&gt;
 Oh, thank you for replying for here and also considering the native integration!
I see. I'm looking forward to the completion.
		</comment>
		<comment id='21' author='SeanNaren' date='2020-07-29T16:43:24Z'>
		&lt;denchmark-link:https://github.com/SeanNaren&gt;@SeanNaren&lt;/denchmark-link&gt;
 PyTorch 1.6.0 is out and amp becomes stable! &lt;denchmark-link:https://github.com/pytorch/pytorch/releases&gt;https://github.com/pytorch/pytorch/releases&lt;/denchmark-link&gt;

Now we can solve this problem, so I added amp to my TODO list.
		</comment>
		<comment id='22' author='SeanNaren' date='2020-07-30T11:00:43Z'>
		Thanks &lt;denchmark-link:https://github.com/kamo-naoyuki&gt;@kamo-naoyuki&lt;/denchmark-link&gt;
 just to warn you (might not impact you) but torch native AMP is broken for nn.LSTM, see issue here: &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/36428&gt;pytorch/pytorch#36428&lt;/denchmark-link&gt;

		</comment>
		<comment id='23' author='SeanNaren' date='2020-07-30T11:20:13Z'>
		Hmm, thanks, I didn't know it.
		</comment>
	</comments>
</bug>