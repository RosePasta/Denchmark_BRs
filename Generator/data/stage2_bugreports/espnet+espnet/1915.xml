<bug id='1915' author='lilian0118' open_date='2020-05-12T02:32:16Z' closed_time='2020-08-28T21:23:59Z'>
	<summary>Transducer multi-gpu error with status code 139</summary>
	<description>
When I use espnet/egs/mini_an4/asr1/conf/train_transducer.yaml or espnet/egs/vivos/asr1/conf/tuning/transducer/train_transducer.yaml config to train a RNN-T model, I meet a strange problem:
If ngpu is larger than one, the train shell script will  exit with status 139 at hundreds of iterations. But if ngpu = 1, the train will success. I also changed the "etype" in "encoder" to "lstmp" and "blstm", if ngpu &gt; 1, the train will exit with status 139.
When I changed the train config to espnet/egs/vivos/asr1/conf/tuning/transducer/train_transformer-rnn_transducer.yaml, the train will success no matter ngpu &gt; 1 or ngpu = 1.
My experiment environments is below:

python version: 3.6.4 
espnet version: espnet 0.7.0
chainer version: chainer 6.0.0
pytorch version: pytorch 1.4.0+cu100
Train Datasets: About 1000H Mandarin corpus

Has anyone ever encountered this question? Thanks.
	</description>
	<comments>
		<comment id='1' author='lilian0118' date='2020-05-12T07:31:29Z'>
		Hi,
Could you share the training log please?
It could be related to &lt;denchmark-link:https://github.com/espnet/espnet/issues/1860&gt;#1860&lt;/denchmark-link&gt;
 with multi-gpu behaviour though, as code status 139 in Unix means a SIGSEV (segmentation violation / illegal memory access) was received.
		</comment>
		<comment id='2' author='lilian0118' date='2020-05-12T11:46:26Z'>
		&lt;denchmark-link:https://github.com/b-flo&gt;@b-flo&lt;/denchmark-link&gt;
 Thanks for your reply.
I use this config(espnet/egs/mini_an4/asr1/conf/train_transducer.yaml) re-run the train script. I also set "verbose=1", but from the log I can't find any information about crash.
&lt;denchmark-link:https://github.com/espnet/espnet/files/4615275/train.log&gt;train.log&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='lilian0118' date='2020-05-12T11:58:23Z'>
		Interesting. Some gdb usage may be needed to pinpoint the problem.
Could you try to re-build warp-transducer as suggested in &lt;denchmark-link:https://github.com/espnet/espnet/issues/1860&gt;#1860&lt;/denchmark-link&gt;
 though? It may be unrelated but there was an illegal memory problem in warp-transducer and I'm not sure how it behave in multi-gpu setup.
&lt;denchmark-link:https://github.com/ShigekiKarita&gt;@ShigekiKarita&lt;/denchmark-link&gt;
 maybe you have some comments on that?
		</comment>
		<comment id='4' author='lilian0118' date='2020-05-14T05:48:20Z'>
		&lt;denchmark-link:https://github.com/b-flo&gt;@b-flo&lt;/denchmark-link&gt;
 I followed you your suggestion and re-build warp-transducer, but the problem is still occur.
It's very strange that the problem only occur when train RNN-transducer, if I train transformer-rnn-transducer or transfomer-transducer, the train process on multi-gpu is ok.
		</comment>
		<comment id='5' author='lilian0118' date='2020-05-14T07:56:40Z'>
		I did some experiment with RNN-transducer and multi-GPU in the same environment you provided, unfortunately I couldn't reproduce the error..
If you are familiar with gdb or any debugging tools, some logs/traces would be useful to pinpoint the issue.
		</comment>
		<comment id='6' author='lilian0118' date='2020-05-14T08:00:54Z'>
		&lt;denchmark-link:https://github.com/b-flo&gt;@b-flo&lt;/denchmark-link&gt;
 Thank you very much. I'll try to dump some core file.
		</comment>
	</comments>
</bug>