<bug id='2137' author='BuaaAlban' open_date='2020-07-02T08:45:14Z' closed_time='2020-07-07T07:09:04Z'>
	<summary>ASR transformer decode on librispeech using GPU  bug</summary>
	<description>
the training config is default.
For stage 5 decoding , I have set ngpu=1
For nj =32 or 16, it will meet CUDA memory error.
But when I reduce nj to 4, anothor  error occurs.
`
Traceback (most recent call last):
File "/scratch/workspace/bokangz/project/espnet-master/egs/librispeech/asr1/../../../espnet/bin/asr_recog.py", line 296, in 
main(sys.argv[1:])
File "/scratch/workspace/bokangz/project/espnet-master/egs/librispeech/asr1/../../../espnet/bin/asr_recog.py", line 274, in main
recog(args)
File "/scratch/workspace/bokangz/project/espnet-master/espnet/asr/pytorch_backend/asr.py", line 960, in recog
feat, args, train_args.char_list, rnnlm
File "/scratch/workspace/bokangz/project/espnet-master/espnet/nets/pytorch_backend/e2e_asr_transformer.py", line 321, in recognize
enc_output = self.encode(x).unsqueeze(0)
File "/scratch/workspace/bokangz/project/espnet-master/espnet/nets/pytorch_backend/e2e_asr_transformer.py", line 308, in encode
enc_output, _ = self.encoder(x, None)
File "/proj/xcdhdstaff1/bokangz/anaconda3/envs/espnet/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in call
result = self.forward(*input, **kwargs)
File "/scratch/workspace/bokangz/project/espnet-master/espnet/nets/pytorch_backend/transformer/encoder.py", line 160, in forward
xs, masks = self.embed(xs, masks)
File "/proj/xcdhdstaff1/bokangz/anaconda3/envs/espnet/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in call
result = self.forward(*input, **kwargs)
File "/scratch/workspace/bokangz/project/espnet-master/espnet/nets/pytorch_backend/transformer/subsampling.py", line 46, in forward
x = self.conv(x)
File "/proj/xcdhdstaff1/bokangz/anaconda3/envs/espnet/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in call
result = self.forward(*input, **kwargs)
File "/proj/xcdhdstaff1/bokangz/anaconda3/envs/espnet/lib/python3.6/site-packages/torch/nn/modules/container.py", line 92, in forward
input = module(input)
File "/proj/xcdhdstaff1/bokangz/anaconda3/envs/espnet/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in call
result = self.forward(*input, **kwargs)
File "/proj/xcdhdstaff1/bokangz/anaconda3/envs/espnet/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 320, in forward
self.padding, self.dilation, self.groups)
RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same
`
	</description>
	<comments>
		<comment id='1' author='BuaaAlban' date='2020-07-02T09:52:39Z'>
		I have sucessfully run the decoding by debugging by the log, however it is not that fast using nj=3, it takes maybe 1-2 days.
		</comment>
		<comment id='2' author='BuaaAlban' date='2020-07-06T17:00:01Z'>
		Could you check  GPU-Util in the nvidia-smi?
If the GPU usage is low, it may be a bug.
		</comment>
		<comment id='3' author='BuaaAlban' date='2020-07-07T01:30:36Z'>
		
Could you check  GPU-Util in the nvidia-smi?
If the GPU usage is low, it may be a bug.

As s matter of fact, it has been running for 3 days with nj=2 for decoding, and there is no log for decoding part, the usage is always 100% .
Every 0.1s: nvidia-smi                                                                                                                                               Tue Jul  7 09:29:06 2020
Tue Jul  7 09:29:07 2020
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla P100-PCIE...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   34C    P0    47W / 250W |  16264MiB / 16280MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla P100-PCIE...  On   | 00000000:5E:00.0 Off |                    0 |
| N/A   29C    P0    25W / 250W |     10MiB / 16280MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla P100-PCIE...  On   | 00000000:86:00.0 Off |                    0 |
| N/A   27C    P0    25W / 250W |     10MiB / 16280MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla P100-PCIE...  On   | 00000000:AF:00.0 Off |                    0 |
| N/A   28C    P0    26W / 250W |     10MiB / 16280MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      8245      C   python3                                     1851MiB |
|    0      8257      C   python3                                     1877MiB |
|    0      8262      C   python3                                     1759MiB |
|    0      8263      C   python3                                     1903MiB |
|    0      8266      C   python3                                     1929MiB |
|    0      8267      C   python3                                     1869MiB |
|    0      8268      C   python3                                     2173MiB |
|    0      8271      C   python3                                     2891MiB |
		</comment>
		<comment id='4' author='BuaaAlban' date='2020-07-07T07:06:15Z'>
		
Could you check  GPU-Util in the nvidia-smi?
If the GPU usage is low, it may be a bug.

It was caused by the default decoding beam size, 60 is too big and too slow.
I set it to 5 to speed up the decoding
		</comment>
		<comment id='5' author='BuaaAlban' date='2020-07-07T07:09:04Z'>
		
Could you check  GPU-Util in the nvidia-smi?
If the GPU usage is low, it may be a bug.

BTW, the slow decoding is not a bug,  but the previous unmatched tensor type (cuda) might be a bug, I will close the issue and If you have time you can check this bug for librispeech transformer decoding.
		</comment>
		<comment id='6' author='BuaaAlban' date='2020-07-08T21:10:53Z'>
		Thanks.
We'll have some follow up.
		</comment>
	</comments>
</bug>