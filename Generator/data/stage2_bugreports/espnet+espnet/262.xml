<bug id='262' author='arendu' open_date='2018-07-02T02:58:17Z' closed_time='2018-08-28T03:45:58Z'>
	<summary>setting `mtlalpha` and `ctc_weight` to 0 causes large increase in CER/WER</summary>
	<description>
I ran the latest code from the master branch and notice something odd. When mtlalpha and ctc_weight is 0.0 performance drops catastrophically. I tested this on voxforge italian. See numbers below:



mtlalpha
ctc_weight
CER




0.5
0.3
14.5, 14.6


0.0
0.0
81.1, 81.6


0.5
0.0
18.6, 19.4



Interestingly, a model trained with mtlalpha=0.5 but decoded with ctc_weight=0.5 does ok. My understanding is that the bottom 2 rows should have very similar performance.
	</description>
	<comments>
		<comment id='1' author='arendu' date='2018-07-03T17:02:07Z'>
		More info, I compared the performance with an older version of the code and I see a large change in performance.
Voxforge Italian:



version
mtlalpha
ctc_weight
CER




current
0.0
0.0
81.1, 81.6


older version (may 22)
0.0
0.0
18.2, 19.3



Voxforge French:



version
mtlalpha
ctc_weight
CER




current
0.0
0.0
76.6, 76.0.6


older version (may 22)
0.0
0.0
42.5, 44.8



		</comment>
		<comment id='2' author='arendu' date='2018-07-03T21:17:39Z'>
		I faced the same issue...
aw=attention weight, cw=ctc weight
atype | (1-mtl) | mtl | cer   | wer   | cer   | wer  | ctcwt
noatt  |  1   | 0    | 68.7 | 85.1 | 67.7 | 85.1 | 0
noatt  | 0.5 | 0.5 | 26.7 | 44.8 | 28.2 | 48.2 | 0.5
noatt  | 0    | 1    | 31.1 | 61.4 | 32.8 | 67.1 | 1
I guess, this should be primary issue of both attention and ctc. While attention suffers from aligning, CTC suffers from less context info.
		</comment>
		<comment id='3' author='arendu' date='2018-07-05T04:48:50Z'>
		I definitely have this problem as well.
Backend: pytorch
elayers: 4
etype: blstmp
default for all other parameters.
&lt;denchmark-h:h2&gt;CER&lt;/denchmark-h&gt;

w/o ctc alpha = 0: 113.4, 111.9
w ctc alpha = 0.5: 14.7, 14.5
I'm running with mtlalpha= 0.0001. I suspect it's small bug in dealing with the mtlalpha=0 case which I noticed was recently added and was not in my prior versions.
		</comment>
		<comment id='4' author='arendu' date='2018-07-05T11:50:14Z'>
		Update...
I ran with mtlalpha = 0.0001 and as suspected the numbers look good.
w/o ctc alpha = 0: 113.4, 111.9
w ctc alpha = 0.0001: 14.2, 14.2
w ctc alpha = 0.5: 14.7, 14.5
So someone should fix the mtlalpha=0 bug.
		</comment>
		<comment id='5' author='arendu' date='2018-07-05T12:06:38Z'>
		Thabks. That's good to proceed.
On Thu, 5 Jul 2018 at 07:50, m-wiesner ***@***.***&gt; wrote:
 Update...

 I ran with mtlalpha = 0.0001 and as suspected the numbers look good.

 w/o ctc alpha = 0: 113.4, 111.9

 w ctc alpha = 0.0001: 14.2, 14.2


 w ctc alpha = 0.5: 14.7, 14.5

 So someone should fix the mtlalpha=0 bug.

 â€”
 You are receiving this because you commented.
 Reply to this email directly, view it on GitHub
 &lt;&lt;denchmark-link:https://github.com/espnet/espnet/issues/262#issuecomment-402697036&gt;#262 (comment)&lt;/denchmark-link&gt;
&gt;, or mute
 the thread
 &lt;&lt;denchmark-link:https://github.com/notifications/unsubscribe-auth/AHJUVUT0iaWXv-W3tBoJDSo0paMchAF4ks5uDf13gaJpZM4U-nQH&gt;https://github.com/notifications/unsubscribe-auth/AHJUVUT0iaWXv-W3tBoJDSo0paMchAF4ks5uDf13gaJpZM4U-nQH&lt;/denchmark-link&gt;
&gt;
 .

-- 
Peace,
Murali Karthick Baskar
		</comment>
		<comment id='6' author='arendu' date='2018-07-05T17:14:22Z'>
		&lt;denchmark-link:https://github.com/arendu&gt;@arendu&lt;/denchmark-link&gt;
, can you test whether the same thing happen in the chainer backend? Now, I'm starting a debug.
		</comment>
		<comment id='7' author='arendu' date='2018-07-05T17:28:31Z'>
		&lt;denchmark-link:https://github.com/sw005320&gt;@sw005320&lt;/denchmark-link&gt;
 ok will run them in chainer and update you.
		</comment>
		<comment id='8' author='arendu' date='2018-07-11T17:55:42Z'>
		In my experiments, when mtlalpha=0 and system is trained without CTC, the model is trained properly according to validation accuracy.
In order to get a reasonable decoding results, first set ctcweight=0, and tune the maxlenratio and minlenratio to regularize the decoding length. I suggest to start with (0.8, 0.2).
		</comment>
		<comment id='9' author='arendu' date='2018-07-13T19:11:52Z'>
		&lt;denchmark-link:https://github.com/sw005320&gt;@sw005320&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/takaaki-hori&gt;@takaaki-hori&lt;/denchmark-link&gt;

The observations with only using attention with an4
Baseline with mtlalpha 0.5 on test set:
%CER    17.1
%CER   130.5 (maxlenratio=0.8, minlenratio=0.3)
The directory you can find these files are in:
/home/hltmbask/espnet/egs/an4/asr1/exp{train_nodev_attention_pure,train_nodev_baseline} 
		</comment>
		<comment id='10' author='arendu' date='2018-07-18T17:24:24Z'>
		Your issue might be a consequence of this issue in warp_ctc &lt;denchmark-link:https://github.com/SeanNaren/deepspeech.pytorch/issues/250&gt;SeanNaren/deepspeech.pytorch#250&lt;/denchmark-link&gt;
 - the gradients wrt the CTC loss are not scaled by the hyperparameter.
		</comment>
		<comment id='11' author='arendu' date='2018-07-19T15:13:43Z'>
		Further update on pure attention based models on voxforge Italian, bigger data compared to an4:
The training accuracy varies as follows:
&lt;denchmark-link:https://github.com/espnet/espnet/files/2210501/train_loss.txt&gt;train_loss.txt&lt;/denchmark-link&gt;

%CER on eval set for all combinations of minlenratio and maxlenratio:
&lt;denchmark-link:https://github.com/espnet/espnet/files/2210508/test_cer.txt&gt;test_cer.txt&lt;/denchmark-link&gt;

Observations: The train loss does not look bad and it is converging 
		</comment>
		<comment id='12' author='arendu' date='2018-07-19T17:55:36Z'>
		
Your issue might be a consequence of this issue in warp_ctc SeanNaren/deepspeech.pytorch#250 - the gradients wrt the CTC loss are not scaled by the hyperparameter.

This is super critical for our MTL. Now, I found a lot of reasoning of our wired results. We'll work on fix it.
&lt;denchmark-link:https://github.com/jheymann85&gt;@jheymann85&lt;/denchmark-link&gt;
, just make sure. Your chainer warp CTC does not have such problems, right?
		</comment>
		<comment id='13' author='arendu' date='2018-07-19T19:41:48Z'>
		Ok. We've confirmed that this is at least partially a warp_ctc issue. The gradient is not scaled by mtlalpha and hence mtl_alpha = 0.0000000000001 results in the same gradient as mtl_alpha 1.0. Only the attention weight is being changed.
This means that there is probably another bug in the attention network.
		</comment>
		<comment id='14' author='arendu' date='2018-07-27T19:47:24Z'>
		After updating my branch with the warp_ctc issue
I trained a network with mtlalpha=0.0 and decoded with ctcweight0.0 and [minlen, maxlen] = [0.2, 0.8]
baseline model (CER %) = 4.5
train_mtlalpha0.0, decode ctcweight=0.0 without [minlen, maxlen] constraint (CER %) = 23.6
train_mtlalpha0.0, decode with ctcweight=0.0 [minlen=0.2, maxlen=0.8]  (CER %) = 5.8
So my take is, if you want to trained attention only network with mtlalpha=0.0,
use "--ctc_weight 0.0 --minlenratio 0.2 --maxlenratio 0.8" options for decoding
		</comment>
		<comment id='15' author='arendu' date='2018-08-21T13:58:16Z'>
		
This is super critical for our MTL. Now, I found a lot of reasoning of our wired results. We'll work on fix it.
@jheymann85, just make sure. Your chainer warp CTC does not have such problems, right?

Should be fine for Chainer. The gradient is scaled &lt;denchmark-link:https://github.com/jheymann85/chainer_ctc/blob/master/chainer_ctc/warpctc.py#L83&gt;here&lt;/denchmark-link&gt;
 with the incoming (scalar) gradient as well as with the number of examples since we output the mean batch loss.
		</comment>
		<comment id='16' author='arendu' date='2018-08-28T03:45:58Z'>
		Now we use &lt;denchmark-link:https://github.com/jnishi&gt;@jnishi&lt;/denchmark-link&gt;
 patched warp-ctc (&lt;denchmark-link:https://github.com/jnishi/warp-ctc&gt;https://github.com/jnishi/warp-ctc&lt;/denchmark-link&gt;
).
The problem of gradient scaling in warp-ctc is fixed.
And if we use only attention network, we should set appropriate minlenratio, maxlenratio, and penalty.
You can check these settings in our article.
[2] Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R. Hershey and Tomoki Hayashi, "Hybrid CTC/Attention Architecture for End-to-End Speech Recognition," IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240-1253, Dec. 2017
		</comment>
	</comments>
</bug>