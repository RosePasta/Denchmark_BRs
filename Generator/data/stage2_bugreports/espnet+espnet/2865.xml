<bug id='2865' author='lnalborczyk' open_date='2021-01-11T10:16:10Z' closed_time='2021-01-19T01:56:25Z'>
	<summary>Stage 1 error in egs/ljspeech/asr1</summary>
	<description>
Describe the bug
Hi, I am trying to run the LJspeech ASR example, but I get the following error:
&lt;denchmark-code&gt;stage 1: Feature Generation
steps/make_fbank_pitch.sh --cmd run.pl --nj 8 --write_utt2num_frames true data/char_eval exp/make_fbank/char_eval fbank
steps/make_fbank_pitch.sh: no such file data/char_eval/wav.scp
&lt;/denchmark-code&gt;

Basic environments:

OS information: OS information: Linux 4.15.0-101-generic #102~16.04.1-Ubuntu SMP Mon May 11 11:38:16 UTC 2020 x86_64
python version: Python 3.7.9 (default, Aug 18 2020, 06:24:24) [GCC 5.4.0 20160609]
espnet version: 0.9.6
Git hash [e.g. b88e89f]
Commit date [e.g. Tue Sep 1 09:32:54 2020 -0400]
pytorch version pytorch 1.4.0

Environments from torch.utils.collect_env:
&lt;denchmark-code&gt;Collecting environment information...
PyTorch version: 1.4.0
Is debug build: No
CUDA used to build PyTorch: 10.1

OS: Ubuntu 16.04.7 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.1.243
GPU models and configuration: GPU 0: Tesla P4
Nvidia driver version: 455.23.05
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.4
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.3
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.3
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.3
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.3
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.3
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.3
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.3

Versions of relevant libraries:
[pip3] numpy==1.19.5
[pip3] pytorch-ranger==0.1.1
[pip3] pytorch-wpe==0.0.0
[pip3] torch==1.4.0
[pip3] torch-complex==0.2.0
[pip3] torch-optimizer==0.1.0
[pip3] torchaudio==0.4.0
[pip3] warpctc-pytorch==0.2.1
[conda] Could not collect
&lt;/denchmark-code&gt;

Task information:

Task: ASR
Recipe: LJspeech
ESPnet1

To Reproduce
Steps to reproduce the behavior:

move to a recipe directory, e.g., cd egs/ljspeech/asr1
execute ./run.sh
specify the error log, e.g., exp/xxx/yyy.log

Error logs
No logs created...
	</description>
	<comments>
		<comment id='1' author='lnalborczyk' date='2021-01-12T12:07:17Z'>
		Could you move



espnet/egs/ljspeech/asr1/run.sh


        Lines 86 to 90
      in
      e962a3c






 for x in ${trans_type}_eval ${trans_type}_train; do 



     steps/make_fbank_pitch.sh --cmd "$train_cmd" --nj 8 --write_utt2num_frames true \ 



         data/${x} exp/make_fbank/${x} ${fbankdir} 



     utils/fix_data_dir.sh data/${x} 



 done 





after



espnet/egs/ljspeech/asr1/run.sh


         Line 96
      in
      e962a3c






 utils/subset_data_dir.sh --first data/${trans_type}_train ${n} data/${train_set} 





I think that the code is inverted, and you first need to subdivide the train set into dev and eval set
or u can just only remove ${trans_type}_eval, from the for 


espnet/egs/ljspeech/asr1/run.sh


         Line 86
      in
      e962a3c






 for x in ${trans_type}_eval ${trans_type}_train; do 





		</comment>
		<comment id='2' author='lnalborczyk' date='2021-01-12T14:12:11Z'>
		Thank you for your prompt answer! I have moved the for loop after the dev/eval split. I now get the following error when executing run.sh:
&lt;denchmark-code&gt;stage -1: Data Download
already exists. skipped.
stage 0: Data preparation
finished making wav.scp, utt2spk, spk2utt.
finished making text.
utils/validate_data_dir.sh: WARNING: you have only one speaker.  This probably a bad idea.
   Search for the word 'bold' in http://kaldi-asr.org/doc/data_prep.html
   for more information.
utils/validate_data_dir.sh: Successfully validated data-directory data/char_train
stage 1: Feature Generation
utils/subset_data_dir.sh: reducing #utt from 13100 to 500
utils/subset_data_dir.sh: reducing #utt from 500 to 250
utils/subset_data_dir.sh: reducing #utt from 500 to 250
utils/subset_data_dir.sh: reducing #utt from 13100 to 12600
steps/make_fbank_pitch.sh --cmd run.pl --nj 8 --write_utt2num_frames true data/char_eval exp/make_fbank/char_eval fbank
utils/validate_data_dir.sh: WARNING: you have only one speaker.  This probably a bad idea.
   Search for the word 'bold' in http://kaldi-asr.org/doc/data_prep.html
   for more information.
utils/validate_data_dir.sh: Successfully validated data-directory data/char_eval
steps/make_fbank_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
steps/make_fbank_pitch.sh: It seems not all of the feature files were successfully procesed (243 != 250); consider using utils/fix_data_dir.sh data/char_eval
steps/make_fbank_pitch.sh: Succeeded creating filterbank and pitch features for char_eval
fix_data_dir.sh: kept 243 utterances out of 250
fix_data_dir.sh: old files are kept in data/char_eval/.backup
steps/make_fbank_pitch.sh --cmd run.pl --nj 8 --write_utt2num_frames true data/char_train exp/make_fbank/char_train fbank
utils/validate_data_dir.sh: WARNING: you have only one speaker.  This probably a bad idea.
   Search for the word 'bold' in http://kaldi-asr.org/doc/data_prep.html
   for more information.
utils/validate_data_dir.sh: Successfully validated data-directory data/char_train
steps/make_fbank_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
steps/make_fbank_pitch.sh: It seems not all of the feature files were successfully procesed (12804 != 13100); consider using utils/fix_data_dir.sh data/char_train
steps/make_fbank_pitch.sh: Succeeded creating filterbank and pitch features for char_train
fix_data_dir.sh: kept 12804 utterances out of 13100
fix_data_dir.sh: old files are kept in data/char_train/.backup
compute-cmvn-stats scp:data/char_train_no_dev/feats.scp data/char_train_no_dev/cmvn.ark 
WARNING (compute-cmvn-stats[5.5.0~1527-32d9]:Open():util/kaldi-table-inl.h:106) Failed to open script file data/char_train_no_dev/feats.scp
ERROR (compute-cmvn-stats[5.5.0~1527-32d9]:SequentialTableReader():util/kaldi-table-inl.h:860) Error constructing TableReader: rspecifier is scp:data/char_train_no_dev/feats.scp

[ Stack-Trace: ]
/research/crissp/espnet/tools/kaldi/src/lib/libkaldi-base.so(kaldi::MessageLogger::LogMessage() const+0x82c) [0x7fb0a39c730a]
compute-cmvn-stats(kaldi::MessageLogger::LogAndThrow::operator=(kaldi::MessageLogger const&amp;)+0x21) [0x41582d]
compute-cmvn-stats(kaldi::SequentialTableReader&lt;kaldi::KaldiObjectHolder&lt;kaldi::Matrix&lt;float&gt; &gt; &gt;::SequentialTableReader(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)+0xa1) [0x41d483]
compute-cmvn-stats(main+0xb63) [0x413b3f]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0) [0x7fb0a2e62840]
compute-cmvn-stats(_start+0x29) [0x412d19]

kaldi::KaldiFatalError
&lt;/denchmark-code&gt;

Any insight about this issue?
		</comment>
		<comment id='3' author='lnalborczyk' date='2021-01-12T14:16:35Z'>
		ok, then instead of moving the for loop, just only remove ${trans_type}_eval, from the for loop 


espnet/egs/ljspeech/asr1/run.sh


         Line 86
      in
      e962a3c






 for x in ${trans_type}_eval ${trans_type}_train; do 





This will create feats.scp for the subdir
		</comment>
		<comment id='4' author='lnalborczyk' date='2021-01-12T14:23:55Z'>
		Thanks! I now get the following error...
&lt;denchmark-code&gt;stage -1: Data Download
already exists. skipped.
stage 0: Data preparation
finished making wav.scp, utt2spk, spk2utt.
finished making text.
utils/validate_data_dir.sh: WARNING: you have only one speaker.  This probably a bad idea.
   Search for the word 'bold' in http://kaldi-asr.org/doc/data_prep.html
   for more information.
utils/validate_data_dir.sh: Error: in data/char_train, utterance-ids extracted from utt2spk and features
utils/validate_data_dir.sh: differ, partial diff is:
--- /tmp/kaldi.ywHi/utts	2021-01-12 15:20:37.198583309 +0100
+++ /tmp/kaldi.ywHi/utts.feats	2021-01-12 15:20:37.586583309 +0100
@@ -26,3 +26,2 @@
 LJ001-0026
-LJ001-0027
 LJ001-0028
...
-LJ050-0234
 LJ050-0235
@@ -13088,3 +12793,2 @@
 LJ050-0266
-LJ050-0267
 LJ050-0268
[Lengths are /tmp/kaldi.ywHi/utts=13100 versus /tmp/kaldi.ywHi/utts.feats=12804]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='lnalborczyk' date='2021-01-12T14:58:54Z'>
		Delete the data folder and try again. That should correct the problem
		</comment>
		<comment id='6' author='lnalborczyk' date='2021-01-12T15:12:02Z'>
		Thanks! I now get the following error...
&lt;denchmark-code&gt;stage -1: Data Download
already exists. skipped.
stage 0: Data preparation
finished making wav.scp, utt2spk, spk2utt.
finished making text.
utils/validate_data_dir.sh: WARNING: you have only one speaker.  This probably a bad idea.
   Search for the word 'bold' in http://kaldi-asr.org/doc/data_prep.html
   for more information.
utils/validate_data_dir.sh: Successfully validated data-directory data/char_train
stage 1: Feature Generation
steps/make_fbank_pitch.sh --cmd run.pl --nj 8 --write_utt2num_frames true data/char_train exp/make_fbank/char_train fbank
utils/validate_data_dir.sh: WARNING: you have only one speaker.  This probably a bad idea.
   Search for the word 'bold' in http://kaldi-asr.org/doc/data_prep.html
   for more information.
utils/validate_data_dir.sh: Successfully validated data-directory data/char_train
steps/make_fbank_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
steps/make_fbank_pitch.sh: It seems not all of the feature files were successfully procesed (12804 != 13100); consider using utils/fix_data_dir.sh data/char_train
steps/make_fbank_pitch.sh: Succeeded creating filterbank and pitch features for char_train
fix_data_dir.sh: kept 12804 utterances out of 13100
fix_data_dir.sh: old files are kept in data/char_train/.backup
utils/subset_data_dir.sh: reducing #utt from 12804 to 500
utils/subset_data_dir.sh: reducing #utt from 500 to 250
utils/subset_data_dir.sh: reducing #utt from 500 to 250
utils/subset_data_dir.sh: reducing #utt from 12804 to 12304
compute-cmvn-stats scp:data/char_train_no_dev/feats.scp data/char_train_no_dev/cmvn.ark 
LOG (compute-cmvn-stats[5.5.0~1527-32d9]:main():compute-cmvn-stats.cc:168) Wrote global CMVN stats to data/char_train_no_dev/cmvn.ark
LOG (compute-cmvn-stats[5.5.0~1527-32d9]:main():compute-cmvn-stats.cc:171) Done accumulating CMVN stats for 12304 utterances; 0 had errors.
/research/crissp/espnet/egs/ljspeech/asr1/../../../utils/dump.sh --cmd run.pl --nj 32 --do_delta false data/char_train_no_dev/feats.scp data/char_train_no_dev/cmvn.ark exp/dump_feats/char_train dump/char_train_no_dev
/research/crissp/espnet/egs/ljspeech/asr1/../../../utils/dump.sh --cmd run.pl --nj 32 --do_delta false data/char_dev/feats.scp data/char_train_no_dev/cmvn.ark exp/dump_feats/char_dev dump/char_dev
/research/crissp/espnet/egs/ljspeech/asr1/../../../utils/dump.sh --cmd run.pl --nj 32 --do_delta false data/char_eval/feats.scp data/char_train_no_dev/cmvn.ark exp/dump_feats/char_eval dump/char_eval
/research/crissp/espnet/egs/ljspeech/asr1/../../../utils/dump.sh --cmd run.pl --nj 8 --do_delta false data/char_dev/feats.scp data/char_train_no_dev/cmvn.ark exp/dump_feats/recog/char_dev dump/char_dev/deltafalse
/research/crissp/espnet/egs/ljspeech/asr1/../../../utils/dump.sh --cmd run.pl --nj 8 --do_delta false data/char_eval/feats.scp data/char_train_no_dev/cmvn.ark exp/dump_feats/recog/char_eval dump/char_eval/deltafalse
stage 2: Dictionary and Json Data Preparation
33 data/lang_1char/char_train_no_dev_units.txt
/research/crissp/espnet/egs/ljspeech/asr1/../../../utils/data2json.sh --feat dump/char_train_no_dev/feats.scp --trans_type char data/char_train_no_dev data/lang_1char/char_train_no_dev_units.txt
/research/crissp/espnet/egs/ljspeech/asr1/../../../utils/feat_to_shape.sh --cmd run.pl --nj 1 --filetype  --preprocess-conf  --verbose 0 dump/char_train_no_dev/feats.scp data/char_train_no_dev/tmp-NokK2/input_1/shape.scp
/research/crissp/espnet/egs/ljspeech/asr1/../../../utils/data2json.sh --feat dump/char_dev/feats.scp --trans_type char data/char_dev data/lang_1char/char_train_no_dev_units.txt
/research/crissp/espnet/egs/ljspeech/asr1/../../../utils/feat_to_shape.sh --cmd run.pl --nj 1 --filetype  --preprocess-conf  --verbose 0 dump/char_dev/feats.scp data/char_dev/tmp-2OEAN/input_1/shape.scp
/research/crissp/espnet/egs/ljspeech/asr1/../../../utils/data2json.sh --feat dump/char_eval/feats.scp --trans_type char data/char_eval data/lang_1char/char_train_no_dev_units.txt
/research/crissp/espnet/egs/ljspeech/asr1/../../../utils/feat_to_shape.sh --cmd run.pl --nj 1 --filetype  --preprocess-conf  --verbose 0 dump/char_eval/feats.scp data/char_eval/tmp-VSusM/input_1/shape.scp
stage 3: LM Preparation
WARNING:root:OOV rate = 50.75 %
run.pl: job failed, log is in exp/train_rnnlm_pytorch_lm_word100/train.log
&lt;/denchmark-code&gt;

Here is the log file
&lt;denchmark-code&gt;# lm_train.py --config conf/lm.yaml --ngpu 1 --backend pytorch --verbose 1 --outdir exp/train_rnnlm_pytorch_lm_word100 --tensorboard-dir tensorboard/train_rnnlm_pytorch_lm_word100 --train-label data/local/wordlm_train/train.txt --valid-label data/local/wordlm_train/valid.txt --test-label data/local/wordlm_train/test.txt --resume --dict data/local/wordlm_train/wordlist_100.txt 
# Started at Tue Jan 12 16:05:33 CET 2021
#
2021-01-12 16:05:48,680 (lm_train:254) INFO: ngpu: 1
2021-01-12 16:05:48,681 (lm_train:257) INFO: python path = (None)
2021-01-12 16:05:48,687 (lm_train:274) INFO: backend = pytorch
2021-01-12 16:05:49,746 (lm:222) INFO: torch version = 1.4.0
2021-01-12 16:05:49,791 (deterministic_utils:26) INFO: torch type check is disabled
2021-01-12 16:05:49,840 (lm_utils:44) INFO: skip dump/load HDF5 because the output dir is not specified
2021-01-12 16:05:49,840 (lm_utils:45) INFO: reading text dataset: data/local/wordlm_train/valid.txt
250it [00:00, 90990.63it/s]
2021-01-12 16:05:49,847 (lm_utils:44) INFO: skip dump/load HDF5 because the output dir is not specified
2021-01-12 16:05:49,847 (lm_utils:45) INFO: reading text dataset: data/local/wordlm_train/train.txt
12304it [00:00, 183750.57it/s]
2021-01-12 16:05:49,934 (lm:240) INFO: #vocab = 103
2021-01-12 16:05:49,934 (lm:241) INFO: #sentences in the training data = 12304
2021-01-12 16:05:49,934 (lm:242) INFO: #tokens in the training data = 208991
2021-01-12 16:05:49,934 (lm:245) INFO: oov rate in the training data = 50.75 %
2021-01-12 16:05:49,934 (lm:247) INFO: #sentences in the validation data = 250
2021-01-12 16:05:49,934 (lm:248) INFO: #tokens in the validation data = 4200
2021-01-12 16:05:49,934 (lm:250) INFO: oov rate in the validation data = 50.36 %
2021-01-12 16:05:49,939 (lm:273) INFO: #iterations per epoch = 42
2021-01-12 16:05:49,939 (lm:274) INFO: #total iterations = 840
Traceback (most recent call last):
  File "/research/crissp/espnet/egs/ljspeech/asr1/../../../espnet/bin/lm_train.py", line 288, in &lt;module&gt;
    main(sys.argv[1:])
  File "/research/crissp/espnet/egs/ljspeech/asr1/../../../espnet/bin/lm_train.py", line 282, in main
    train(args)
  File "/research/crissp/espnet/espnet/lm/pytorch_backend/lm.py", line 282, in train
    model.to("cuda")
  File "/research/crissp/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 425, in to
    return self._apply(convert)
  File "/research/crissp/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 201, in _apply
    module._apply(fn)
  File "/research/crissp/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 201, in _apply
    module._apply(fn)
  File "/research/crissp/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 201, in _apply
    module._apply(fn)
  File "/research/crissp/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 223, in _apply
    param_applied = fn(param)
  File "/research/crissp/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 423, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: out of memory
# Accounting: time=26 threads=1
# Ended (code 1) at Tue Jan 12 16:05:59 CET 2021, elapsed time 26 seconds

&lt;/denchmark-code&gt;

I guess I am running out of memory on the GPU, right?
		</comment>
		<comment id='7' author='lnalborczyk' date='2021-01-13T01:22:28Z'>
		yes, you will need to reduce the number of the batch size:



espnet/egs/ljspeech/asr1/conf/lm.yaml


         Line 5
      in
      3437dbb






 batchsize: 300 # 1024 for character LMs 





from 300 -&gt;150
This error may also prompt on the Training stage, so you will need to reduce the batch size also:



espnet/egs/ljspeech/asr1/conf/train_mtlalpha0.5.yaml


         Line 2
      in
      3437dbb






 batch-size: 30 





		</comment>
		<comment id='8' author='lnalborczyk' date='2021-01-13T13:21:39Z'>
		Hi, thank you again for your answer! Reducing the batch size in lm.yaml (to 10 or 20!) stills gives the same Stage 3 CUDA memory error...
&lt;denchmark-code&gt;stage -1: Data Download
already exists. skipped.
stage 0: Data preparation
finished making wav.scp, utt2spk, spk2utt.
finished making text.
utils/validate_data_dir.sh: WARNING: you have only one speaker.  This probably a bad idea.
   Search for the word 'bold' in http://kaldi-asr.org/doc/data_prep.html
   for more information.
utils/validate_data_dir.sh: Successfully validated data-directory data/char_train
stage 1: Feature Generation
steps/make_fbank_pitch.sh --cmd run.pl --nj 8 --write_utt2num_frames true data/char_train exp/make_fbank/char_train fbank
utils/validate_data_dir.sh: WARNING: you have only one speaker.  This probably a bad idea.
   Search for the word 'bold' in http://kaldi-asr.org/doc/data_prep.html
   for more information.
utils/validate_data_dir.sh: Successfully validated data-directory data/char_train
steps/make_fbank_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
steps/make_fbank_pitch.sh: It seems not all of the feature files were successfully procesed (12804 != 13100); consider using utils/fix_data_dir.sh data/char_train
steps/make_fbank_pitch.sh: Succeeded creating filterbank and pitch features for char_train
fix_data_dir.sh: kept 12804 utterances out of 13100
fix_data_dir.sh: old files are kept in data/char_train/.backup
utils/subset_data_dir.sh: reducing #utt from 12804 to 500
utils/subset_data_dir.sh: reducing #utt from 500 to 250
utils/subset_data_dir.sh: reducing #utt from 500 to 250
utils/subset_data_dir.sh: reducing #utt from 12804 to 12304
compute-cmvn-stats scp:data/char_train_no_dev/feats.scp data/char_train_no_dev/cmvn.ark 
LOG (compute-cmvn-stats[5.5.0~1527-32d9]:main():compute-cmvn-stats.cc:168) Wrote global CMVN stats to data/char_train_no_dev/cmvn.ark
LOG (compute-cmvn-stats[5.5.0~1527-32d9]:main():compute-cmvn-stats.cc:171) Done accumulating CMVN stats for 12304 utterances; 0 had errors.
/research/crissp/espnet/egs/ljspeech/asr1/../../../utils/dump.sh --cmd run.pl --nj 32 --do_delta false data/char_train_no_dev/feats.scp data/char_train_no_dev/cmvn.ark exp/dump_feats/char_train dump/char_train_no_dev
/research/crissp/espnet/egs/ljspeech/asr1/../../../utils/dump.sh --cmd run.pl --nj 32 --do_delta false data/char_dev/feats.scp data/char_train_no_dev/cmvn.ark exp/dump_feats/char_dev dump/char_dev
/research/crissp/espnet/egs/ljspeech/asr1/../../../utils/dump.sh --cmd run.pl --nj 32 --do_delta false data/char_eval/feats.scp data/char_train_no_dev/cmvn.ark exp/dump_feats/char_eval dump/char_eval
/research/crissp/espnet/egs/ljspeech/asr1/../../../utils/dump.sh --cmd run.pl --nj 8 --do_delta false data/char_dev/feats.scp data/char_train_no_dev/cmvn.ark exp/dump_feats/recog/char_dev dump/char_dev/deltafalse
/research/crissp/espnet/egs/ljspeech/asr1/../../../utils/dump.sh --cmd run.pl --nj 8 --do_delta false data/char_eval/feats.scp data/char_train_no_dev/cmvn.ark exp/dump_feats/recog/char_eval dump/char_eval/deltafalse
stage 2: Dictionary and Json Data Preparation
33 data/lang_1char/char_train_no_dev_units.txt
/research/crissp/espnet/egs/ljspeech/asr1/../../../utils/data2json.sh --feat dump/char_train_no_dev/feats.scp --trans_type char data/char_train_no_dev data/lang_1char/char_train_no_dev_units.txt
/research/crissp/espnet/egs/ljspeech/asr1/../../../utils/feat_to_shape.sh --cmd run.pl --nj 1 --filetype  --preprocess-conf  --verbose 0 dump/char_train_no_dev/feats.scp data/char_train_no_dev/tmp-vI69J/input_1/shape.scp
/research/crissp/espnet/egs/ljspeech/asr1/../../../utils/data2json.sh --feat dump/char_dev/feats.scp --trans_type char data/char_dev data/lang_1char/char_train_no_dev_units.txt
/research/crissp/espnet/egs/ljspeech/asr1/../../../utils/feat_to_shape.sh --cmd run.pl --nj 1 --filetype  --preprocess-conf  --verbose 0 dump/char_dev/feats.scp data/char_dev/tmp-Sapr8/input_1/shape.scp
/research/crissp/espnet/egs/ljspeech/asr1/../../../utils/data2json.sh --feat dump/char_eval/feats.scp --trans_type char data/char_eval data/lang_1char/char_train_no_dev_units.txt
/research/crissp/espnet/egs/ljspeech/asr1/../../../utils/feat_to_shape.sh --cmd run.pl --nj 1 --filetype  --preprocess-conf  --verbose 0 dump/char_eval/feats.scp data/char_eval/tmp-ed4Da/input_1/shape.scp
stage 3: LM Preparation
WARNING:root:OOV rate = 50.75 %
run.pl: job failed, log is in exp/train_rnnlm_pytorch_lm_word100/train.log
&lt;/denchmark-code&gt;

Here is the log...
&lt;denchmark-code&gt;# lm_train.py --config conf/lm.yaml --ngpu 1 --backend pytorch --verbose 1 --outdir exp/train_rnnlm_pytorch_lm_word100 --tensorboard-dir tensorboard/train_rnnlm_pytorch_lm_word100 --train-label data/local/wordlm_train/train.txt --valid-label data/local/wordlm_train/valid.txt --test-label data/local/wordlm_train/test.txt --resume --dict data/local/wordlm_train/wordlist_100.txt 
# Started at Wed Jan 13 12:26:22 CET 2021
#
2021-01-13 12:26:27,041 (lm_train:254) INFO: ngpu: 1
2021-01-13 12:26:27,042 (lm_train:257) INFO: python path = (None)
2021-01-13 12:26:27,043 (lm_train:274) INFO: backend = pytorch
2021-01-13 12:26:27,151 (lm:222) INFO: torch version = 1.4.0
2021-01-13 12:26:27,155 (deterministic_utils:26) INFO: torch type check is disabled
2021-01-13 12:26:27,188 (lm_utils:44) INFO: skip dump/load HDF5 because the output dir is not specified
2021-01-13 12:26:27,188 (lm_utils:45) INFO: reading text dataset: data/local/wordlm_train/valid.txt
250it [00:00, 130728.84it/s]
2021-01-13 12:26:27,194 (lm_utils:44) INFO: skip dump/load HDF5 because the output dir is not specified
2021-01-13 12:26:27,194 (lm_utils:45) INFO: reading text dataset: data/local/wordlm_train/train.txt
12304it [00:00, 189878.57it/s]
2021-01-13 12:26:27,279 (lm:240) INFO: #vocab = 103
2021-01-13 12:26:27,279 (lm:241) INFO: #sentences in the training data = 12304
2021-01-13 12:26:27,279 (lm:242) INFO: #tokens in the training data = 208991
2021-01-13 12:26:27,279 (lm:245) INFO: oov rate in the training data = 50.75 %
2021-01-13 12:26:27,279 (lm:247) INFO: #sentences in the validation data = 250
2021-01-13 12:26:27,279 (lm:248) INFO: #tokens in the validation data = 4200
2021-01-13 12:26:27,279 (lm:250) INFO: oov rate in the validation data = 50.36 %
2021-01-13 12:26:27,285 (lm:273) INFO: #iterations per epoch = 616
2021-01-13 12:26:27,285 (lm:274) INFO: #total iterations = 12320
Traceback (most recent call last):
  File "/research/crissp/espnet/egs/ljspeech/asr1/../../../espnet/bin/lm_train.py", line 288, in &lt;module&gt;
    main(sys.argv[1:])
  File "/research/crissp/espnet/egs/ljspeech/asr1/../../../espnet/bin/lm_train.py", line 282, in main
    train(args)
  File "/research/crissp/espnet/espnet/lm/pytorch_backend/lm.py", line 282, in train
    model.to("cuda")
  File "/research/crissp/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 425, in to
    return self._apply(convert)
  File "/research/crissp/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 201, in _apply
    module._apply(fn)
  File "/research/crissp/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 201, in _apply
    module._apply(fn)
  File "/research/crissp/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 201, in _apply
    module._apply(fn)
  File "/research/crissp/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 223, in _apply
    param_applied = fn(param)
  File "/research/crissp/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 423, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: out of memory
# Accounting: time=7 threads=1
# Ended (code 1) at Wed Jan 13 12:26:29 CET 2021, elapsed time 7 seconds

&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='lnalborczyk' date='2021-01-13T13:35:34Z'>
		The problem could be related to the length of the sequence. You could check some similar issues: &lt;denchmark-link:https://github.com/espnet/espnet/issues/2741&gt;#2741&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/espnet/espnet/issues/2798&gt;#2798&lt;/denchmark-link&gt;

BTW, you do not need to run from stage -1 every time. if the program fails in some part you can just re-run with  ./run.sh --stage &lt;stage&gt;, in this case &lt;stage&gt;=3
		</comment>
		<comment id='10' author='lnalborczyk' date='2021-01-13T13:43:04Z'>
		Thanks. Is there a parameter somewhere allowing to remove sequences that are longer than X, or should I do it manually?
		</comment>
		<comment id='11' author='lnalborczyk' date='2021-01-14T01:36:38Z'>
		YOu can use:



espnet/egs/chime6/asr1/run.sh


        Lines 56 to 57
      in
      0866492






 echo "stage 1: trimming and speed pertrubation for training data" 



 remove_longshortdata.sh --maxframes 2000 --maxchars 200 data/${train_set} data/${train_set}_trim 





But, just in case, check if the text for training is correctly split or if the data is loaded in a single utterance.
		</comment>
		<comment id='12' author='lnalborczyk' date='2021-01-18T10:59:40Z'>
		Thank you for the tips. It seems to work fine now. I think the issue can be closed.
		</comment>
		<comment id='13' author='lnalborczyk' date='2021-01-20T09:44:02Z'>
		Hi, it's more a question than an issue but it's still related to the LJSpeech example so I am asking here (please redirect me if you think it would be better to ask this question somewhere else).
I am trying to fit the same model with a smaller dataset. Let's say for instance I only want to use 1% of the LJSpeech corpus. The LJspeech corpus originally contains 13100 audio clips, so I only want 131 of them. I have therefore randomly picked 131 lines in espnet/egs/ljspeech/asr1/downloads/LJSpeech-1.1/metadata.csv as well as the corresponding 131 audio (.wav) files in espnet/egs/ljspeech/asr1/downloads/LJSpeech-1.1/wavs/.
When I execute ./run.sh, I now get the following error at Stage 1.
&lt;denchmark-code&gt;stage -1: Data Download
already exists. skipped.
stage 0: Data preparation
finished making wav.scp, utt2spk, spk2utt.
finished making text.
utils/validate_data_dir.sh: WARNING: you have only one speaker.  This probably a bad idea.
   Search for the word 'bold' in http://kaldi-asr.org/doc/data_prep.html
   for more information.
utils/validate_data_dir.sh: Successfully validated data-directory data/char_train
stage 1: Feature Generation
steps/make_fbank_pitch.sh --cmd run.pl --nj 8 --write_utt2num_frames true data/char_train exp/make_fbank/char_train fbank
utils/validate_data_dir.sh: WARNING: you have only one speaker.  This probably a bad idea.
   Search for the word 'bold' in http://kaldi-asr.org/doc/data_prep.html
   for more information.
utils/validate_data_dir.sh: Successfully validated data-directory data/char_train
steps/make_fbank_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
steps/make_fbank_pitch.sh: It seems not all of the feature files were successfully procesed (127 != 131); consider using utils/fix_data_dir.sh data/char_train
steps/make_fbank_pitch.sh: Succeeded creating filterbank and pitch features for char_train
fix_data_dir.sh: kept 127 utterances out of 131
fix_data_dir.sh: old files are kept in data/char_train/.backup
utils/subset_data_dir.sh: cannot subset to more utterances than you originally had.
&lt;/denchmark-code&gt;

Any idea why this occurs and how to fix it?
		</comment>
		<comment id='14' author='lnalborczyk' date='2021-01-20T10:41:44Z'>
		The problem is because you are trying to subset a dataset with 127 into 500:



espnet/egs/ljspeech/asr1/run.sh


        Lines 91 to 93
      in
      a1a537c






 utils/subset_data_dir.sh --last data/${trans_type}_train 500 data/${trans_type}_deveval 



 utils/subset_data_dir.sh --last data/${trans_type}_deveval 250 data/${eval_set} 



 utils/subset_data_dir.sh --first data/${trans_type}_deveval 250 data/${dev_set} 





You could change the size. Or it will be better to use the main dir to get the dev/test dirs
		</comment>
		<comment id='15' author='lnalborczyk' date='2021-01-20T11:38:02Z'>
		Thank you! What do you mean by "using the main dir to get the dev/test dirs"? How can I do that?
		</comment>
		<comment id='16' author='lnalborczyk' date='2021-01-20T13:27:55Z'>
		you could replace line 


espnet/egs/ljspeech/asr1/run.sh


         Line 91
      in
      a1a537c






 utils/subset_data_dir.sh --last data/${trans_type}_train 500 data/${trans_type}_deveval 





with the following:
&lt;denchmark-code&gt;utils/copy_data_dir.sh data/${trans_type}_train data/${trans_type}_train_full
rm -rf data/${trans_type}_train
utils/subset_data_dir.sh --first data/${trans_type}_train_full 131 data/${trans_type}_train
utils/subset_data_dir.sh --last data/${trans_type}_train_full 500 data/${trans_type}_deveval 
&lt;/denchmark-code&gt;

Best
		</comment>
		<comment id='17' author='lnalborczyk' date='2021-01-20T17:42:40Z'>
		Thanks! I have replaced line 91 by the four lines you suggested but I still get the same error... my run.sh file looks like:
&lt;denchmark-code&gt;# make a dev set
utils/copy_data_dir.sh data/${trans_type}_train data/${trans_type}_train_full
rm -rf data/${trans_type}_train
utils/subset_data_dir.sh --first data/${trans_type}_train_full 131 data/${trans_type}_train
utils/subset_data_dir.sh --last data/${trans_type}_train_full 500 data/${trans_type}_deveval 
utils/subset_data_dir.sh --last data/${trans_type}_deveval 250 data/${eval_set}
utils/subset_data_dir.sh --first data/${trans_type}_deveval 250 data/${dev_set}
n=$(( $(wc -l &lt; data/${trans_type}_train/wav.scp) - 500 ))
utils/subset_data_dir.sh --first data/${trans_type}_train ${n} data/${train_set}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='18' author='lnalborczyk' date='2021-01-21T01:28:11Z'>
		Just to confirm.

I have therefore randomly picked 131 lines in espnet/egs/ljspeech/asr1/downloads/LJSpeech-1.1/metadata.csv as well as the corresponding 131 audio (.wav) files in espnet/egs/ljspeech/asr1/downloads/LJSpeech-1.1/wavs/.

Did you undo this last operation? and keep the 13000 files?
did you delete data folder and executed from stage 0?
		</comment>
		<comment id='19' author='lnalborczyk' date='2021-01-21T09:36:54Z'>
		I did not... Now that I have kept the 13100 original files, removed the data folder, and executed run.sh from stage 0, I get the following error:
&lt;denchmark-code&gt;stage 0: Data preparation
finished making wav.scp, utt2spk, spk2utt.
finished making text.
utils/validate_data_dir.sh: WARNING: you have only one speaker.  This probably a bad idea.
   Search for the word 'bold' in http://kaldi-asr.org/doc/data_prep.html
   for more information.
utils/validate_data_dir.sh: Successfully validated data-directory data/char_train
stage 1: Feature Generation
steps/make_fbank_pitch.sh --cmd run.pl --nj 8 --write_utt2num_frames true data/char_train exp/make_fbank/char_train fbank
utils/validate_data_dir.sh: WARNING: you have only one speaker.  This probably a bad idea.
   Search for the word 'bold' in http://kaldi-asr.org/doc/data_prep.html
   for more information.
utils/validate_data_dir.sh: Successfully validated data-directory data/char_train
steps/make_fbank_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
steps/make_fbank_pitch.sh: It seems not all of the feature files were successfully procesed (12804 != 13100); consider using utils/fix_data_dir.sh data/char_train
steps/make_fbank_pitch.sh: Succeeded creating filterbank and pitch features for char_train
fix_data_dir.sh: kept 12804 utterances out of 13100
fix_data_dir.sh: old files are kept in data/char_train/.backup
utils/copy_data_dir.sh: copied data from data/char_train to data/char_train_full
utils/validate_data_dir.sh: WARNING: you have only one speaker.  This probably a bad idea.
   Search for the word 'bold' in http://kaldi-asr.org/doc/data_prep.html
   for more information.
utils/validate_data_dir.sh: Successfully validated data-directory data/char_train_full
utils/subset_data_dir.sh: reducing #utt from 12804 to 131
utils/subset_data_dir.sh: reducing #utt from 12804 to 500
utils/subset_data_dir.sh: reducing #utt from 500 to 250
utils/subset_data_dir.sh: reducing #utt from 500 to 250
utils/subset_data_dir.sh: reducing #utt from 131 to 0
compute-cmvn-stats scp:data/char_train_no_dev/feats.scp data/char_train_no_dev/cmvn.ark 
LOG (compute-cmvn-stats[5.5.0~1527-32d9]:main():compute-cmvn-stats.cc:168) Wrote global CMVN stats to data/char_train_no_dev/cmvn.ark
LOG (compute-cmvn-stats[5.5.0~1527-32d9]:main():compute-cmvn-stats.cc:171) Done accumulating CMVN stats for 0 utterances; 0 had errors.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='20' author='lnalborczyk' date='2021-01-21T10:22:51Z'>
		You modified the metadata, right ?
Please use the original one.
		</comment>
		<comment id='21' author='lnalborczyk' date='2021-01-21T11:11:48Z'>
		If you refer to the espnet/egs/ljspeech/asr1/downloads/LJSpeech-1.1/metadata.csv file, it is the original one (I have just re-downloaded the data to be sure).
		</comment>
		<comment id='22' author='lnalborczyk' date='2021-01-21T11:35:29Z'>
		Remember to comment out 


espnet/egs/ljspeech/asr1/run.sh


        Lines 94 to 95
      in
      a1a537c






 n=$(( $(wc -l &lt; data/${trans_type}_train/wav.scp) - 500 )) 



 utils/subset_data_dir.sh --first data/${trans_type}_train ${n} data/${train_set} 




.
The lines remove the data employed from dev/eval from the original train.
		</comment>
		<comment id='23' author='lnalborczyk' date='2021-01-21T11:37:13Z'>
		and instead of
utils/subset_data_dir.sh --first data/${trans_type}_train_full 131 data/${trans_type}_train
, use
utils/subset_data_dir.sh --first data/${trans_type}_train_full 131 data/${train_set}
So, the code should be smth like:
&lt;denchmark-code&gt;# make a dev set
utils/copy_data_dir.sh data/${trans_type}_train data/${trans_type}_train_full
rm -rf data/${trans_type}_train
utils/subset_data_dir.sh --first data/${trans_type}_train_full 131 data/${train_set}
utils/subset_data_dir.sh --last data/${trans_type}_train_full 500 data/${trans_type}_deveval 
utils/subset_data_dir.sh --last data/${trans_type}_deveval 250 data/${eval_set}
utils/subset_data_dir.sh --first data/${trans_type}_deveval 250 data/${dev_set}
# n=$(( $(wc -l &lt; data/${trans_type}_train/wav.scp) - 500 ))
# utils/subset_data_dir.sh --first data/${trans_type}_train ${n} data/${train_set}

&lt;/denchmark-code&gt;

		</comment>
		<comment id='24' author='lnalborczyk' date='2021-01-21T14:54:24Z'>
		Thanks! It works perfectly now! 🙏
		</comment>
		<comment id='25' author='lnalborczyk' date='2021-01-25T11:01:55Z'>
		Hi, I am sorry to get back to you again but I still have another newbie question. I am training a similar (to the default LJSpeech example) ASR model with a custom dataset in which I have only 150 pairs of audio + text. I would like to be sure of how the train/test split is done.
Out of the 150 sentences in my corpus, let's say I would like to use 120 for training and 30 for testing. Out of the 120 sentences used for training, I would like to use 20 for validation. Here is the code I would use to achieve this.
&lt;denchmark-code&gt;# make a dev set
utils/copy_data_dir.sh data/${trans_type}_train data/${trans_type}_train_full
rm -rf data/${trans_type}_train
utils/subset_data_dir.sh --first data/${trans_type}_train_full 100 data/${train_set}
utils/subset_data_dir.sh --last data/${trans_type}_train_full 50 data/${trans_type}_deveval 
utils/subset_data_dir.sh --last data/${trans_type}_deveval 30 data/${eval_set}
utils/subset_data_dir.sh --first data/${trans_type}_deveval 20 data/${dev_set}
&lt;/denchmark-code&gt;

Am I correct?
Also, in the manual it says that executing ./run.sh with --verbose 0 gives the following information during network training (see screenshot below), but nothing is happening on my machine during training, despite the --verbose 1 in my run.sh file. Any idea why?
&lt;denchmark-link:https://user-images.githubusercontent.com/10812999/105695252-2d543a00-5f02-11eb-817f-9b2a0f93992c.png&gt;&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>