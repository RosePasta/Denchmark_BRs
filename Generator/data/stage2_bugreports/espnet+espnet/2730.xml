<bug id='2730' author='jaesong' open_date='2020-11-30T17:21:27Z' closed_time='2020-12-02T20:46:01Z'>
	<summary>librispeech asr1 recipe emits error at stage 2</summary>
	<description>
&lt;denchmark-code&gt;$ cd &lt;espnet&gt;/egs/librispeech/asr1
$ bash ./run.sh --dumpdir ... --datadir ... --stop-stage 1
...
$ bash ./run.sh --dumpdir ... --datadir ... --stop-stage 2 --stage 2
dictionary: data/lang_char/train_960_unigram5000_units.txt
stage 2: Dictionary and Json Data Preparation
cut: data/train_960/text: No such file or directory
&lt;/denchmark-code&gt;

Stage 1 produces train_960_org, train_sp_org (with speed perturbation) and train_sp (via remove_longshortdata.sh), but it does not produce train_960.
However, stage 2 tries to read data/${train_set}/text whereas train_set=train_960.
I guess it is related to recent changes regarding speed perturbation (&lt;denchmark-link:https://github.com/espnet/espnet/pull/2617&gt;#2617&lt;/denchmark-link&gt;
).
	</description>
	<comments>
		<comment id='1' author='jaesong' date='2020-11-30T18:02:44Z'>
		Thanks, &lt;denchmark-link:https://github.com/jaesong&gt;@jaesong&lt;/denchmark-link&gt;
!
Year, I think you're right. This is a bug.
&lt;denchmark-link:https://github.com/yuekaizhang&gt;@yuekaizhang&lt;/denchmark-link&gt;
, could you fix it?
We can just use  (before pruning too long/short utterances) at stage 2.
The same thing happens in stage 3.
If your pre-trained model is based on the text data after pruning too long/short utterances, it might be better to prepare data/${train_set}/text to avoid the mismatch.
		</comment>
		<comment id='2' author='jaesong' date='2020-12-01T02:15:12Z'>
		Sure, I would fix it. Thank you jaeson. Sorry, I didn't care too much about LM training since I used previous LM.
		</comment>
	</comments>
</bug>