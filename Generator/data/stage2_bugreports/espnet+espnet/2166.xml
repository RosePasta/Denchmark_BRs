<bug id='2166' author='okkteam' open_date='2020-07-11T10:04:17Z' closed_time='2020-07-15T08:57:59Z'>
	<summary>Transformer encoder with rnn attention ecoder is not supported yet</summary>
	<description>
May I ask the reason for ESPnet does not supported Transformer encoder in rnnt-att is the attention type will include both MultiHeadedAttention and attention types in espnet/nets/pytorch_backend/rnn/attentions ?
And an error occured when I run a etype=Transformer, dtype=Transformer, rnnt_mode=rnnt transducer model and returned:
&lt;denchmark-code&gt;File "/home/oshindo/espnet/espnet/asr/asr_utils.py", line 231, in get_attention_weight  
    if len(att_w.shape) == 3:  
AttributeError: 'str' object hs no attribute 'shape'  
&lt;/denchmark-code&gt;

I don't understand why the att_w is a 'str' type.
	</description>
	<comments>
		<comment id='1' author='okkteam' date='2020-07-11T10:18:49Z'>
		It seems that the return from e2e_asr_transducer.calculate_all_attentions is a dict, and when att_w is emumberated from att_ws, it becomes a 'str' which is a layer name.
		</comment>
		<comment id='2' author='okkteam' date='2020-07-11T12:06:03Z'>
		I already fixed this by pointing out that att_ws is a dict in /home/oshindo/espnet/espnet/asr/asr_utils.py. Maybe it's a bug.
		</comment>
		<comment id='3' author='okkteam' date='2020-07-11T13:44:38Z'>
		
May I ask the reason for ESPnet does not supported Transformer encoder in rnnt-att is the attention type will include both MultiHeadedAttention and attention types in espnet/nets/pytorch_backend/rnn/attentions ?

I don't recall the specific reason but it was due to some incompatibility between MultiheadedAttention and other attention used in RNN decoder usage. I may enable it in the future (didn't have time to work on it and it's not a priority)

And an error occured when I run a etype=Transformer, dtype=Transformer, rnnt_mode=rnnt transducer model and returned:

&lt;denchmark-code&gt;File "/home/oshindo/espnet/espnet/asr/asr_utils.py", line 231, in get_attention_weight  
    if len(att_w.shape) == 3:  
AttributeError: 'str' object hs no attribute 'shape'  
&lt;/denchmark-code&gt;

In master branch? That is weird, the only time I met this error was... well yesterday evening in an integration test when I pushed a fix related to our discussion in &lt;denchmark-link:https://github.com/espnet/espnet/issues/2162&gt;#2162&lt;/denchmark-link&gt;
. But it's not merged and the problem is on Transformer-Transducer-Att because a method from the wrong class is called.
The problem starts within , I'm currently working on it.
		</comment>
		<comment id='4' author='okkteam' date='2020-07-11T14:14:28Z'>
		Really thanks for your awsome work!

I don't recall the specific reason but it was due to some incompatibility between MultiheadedAttention and other attention used in RNN decoder usage. I may enable it in the future (didn't have time to work on it and it's not a priority)

What is the meaning of incompatibility? I thought it will be easier to find an attention-type like AttAdd in rnn/attentions with MultiheadedAttention together in e2e_asr_transducer.calculate_all_attentions.

In master branch? That is weird, the only time I met this error was... well yesterday evening in an integration test when I pushed a fix related to our discussion in #2162. But it's not merged and the problem is on Transformer-Transducer-Att because a method from the wrong class is called.
The problem starts within asr.py, I'm currently working on it.

Sorry, I don't really know how to get a fixed project. The branch I used is download in a couple days ago by git clone. Emm, actually it's not a big deal. I just add a sentence like att_w_npy = att_w[key] in asr/asr_utils.PlotAttentionReport.get_attention_weights. Then the project works again. Do you mean that the problem starts with asr.py is att_vis_fn = model.calculate_all_attentions?
		</comment>
		<comment id='5' author='okkteam' date='2020-07-12T10:42:33Z'>
		Sorry, I had other things to deal with.

What is the meaning of incompatibility? I thought it will be easier to find an attention-type like AttAdd in rnn/attentions with MultiheadedAttention together in e2e_asr_transducer.calculate_all_attentions.

What I mean by "incompatibility" is that some issues occurred due to the use of MultiheadedAttention coupled to other attentions, but I can't recall. I may investigate after my current PR, to be honest it wasn't a priority at that time.

Sorry, I don't really know how to get a fixed project. The branch I used is download in a couple days ago by git clone. Emm, actually it's not a big deal. I just add a sentence like att_w_npy = att_w[key] in asr/asr_utils.PlotAttentionReport.get_attention_weights. Then the project works again. Do you mean that the problem starts with asr.py is att_vis_fn = model.calculate_all_attentions?

In , these conditions seems wrong to me: &lt;denchmark-link:https://github.com/espnet/espnet/blob/master/espnet/asr/pytorch_backend/asr.py#L662-L664&gt;https://github.com/espnet/espnet/blob/master/espnet/asr/pytorch_backend/asr.py#L662-L664&lt;/denchmark-link&gt;

It disable attention weights saving for all cases because  was not set to "transducer" as you mentionned in &lt;denchmark-link:https://github.com/espnet/espnet/issues/2162&gt;#2162&lt;/denchmark-link&gt;
. However modifying the conditions shows another problem, when I unified E2E classes for RNN and Transformer, I discarded a decorator used only for transformer: &lt;denchmark-link:https://github.com/espnet/espnet/blob/master/espnet/nets/pytorch_backend/e2e_asr_transformer.py#L204-L207&gt;https://github.com/espnet/espnet/blob/master/espnet/nets/pytorch_backend/e2e_asr_transformer.py#L204-L207&lt;/denchmark-link&gt;
.
It's fixed now but it's kind of an hack though.
		</comment>
	</comments>
</bug>