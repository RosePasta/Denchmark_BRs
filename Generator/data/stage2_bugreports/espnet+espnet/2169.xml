<bug id='2169' author='unilight' open_date='2020-07-12T17:47:32Z' closed_time='2020-09-17T13:35:48Z'>
	<summary>Multi-GPU training failure with PyTorch 1.5.1</summary>
	<description>
E.g. in egs/libritts/tts1, $ CUDA_VISIBLE_DEVICES=0,1 ./run.sh --ngpu 2 --trans_type phn will result in the following error in the log file,
&lt;denchmark-code&gt;Exception in main training loop: Caught StopIteration in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/huang18/VC/Experiments/espnet/tools/venv/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 60, in _worker
    output = module(*input, **kwargs)
  File "/home/huang18/VC/Experiments/espnet/tools/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/nas01/internal/wenchin-h/VC/Experiments/espnet/espnet/nets/pytorch_backend/e2e_tts_tacotron2.py", line 746, in forward
    after_outs, before_outs, logits, att_ws = self.dec(hs, hlens, ys)
  File "/home/huang18/VC/Experiments/espnet/tools/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/nas01/internal/wenchin-h/VC/Experiments/espnet/espnet/nets/pytorch_backend/tacotron2/decoder.py", line 436, in forward
    att_c, att_w = self.att(hs, hlens, z_list[0], prev_att_w)                                                                                                                                           File "/home/huang18/VC/Experiments/espnet/tools/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)                                                                                                                                                             File "/nas01/internal/wenchin-h/VC/Experiments/espnet/espnet/nets/pytorch_backend/rnn/attentions.py", line 365, in forward
    self.mask = to_device(self, make_pad_mask(enc_hs_len))
  File "/nas01/internal/wenchin-h/VC/Experiments/espnet/espnet/nets/pytorch_backend/nets_utils.py", line 24, in to_device
    device = next(m.parameters()).device
StopIteration
Traceback (most recent call last):                                                                                                                                                                      File "/home/huang18/VC/Experiments/espnet/tools/venv/lib/python3.6/site-packages/chainer/training/trainer.py", line 316, in run
    update()
  File "/nas01/internal/wenchin-h/VC/Experiments/espnet/espnet/tts/pytorch_backend/tts.py", line 171, in update                                                                                           self.update_core()
  File "/nas01/internal/wenchin-h/VC/Experiments/espnet/espnet/tts/pytorch_backend/tts.py", line 151, in update_core
    loss = self.model(**x).mean() / self.accum_grad                                                                                                                                                     File "/home/huang18/VC/Experiments/espnet/tools/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/huang18/VC/Experiments/espnet/tools/venv/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 155, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/huang18/VC/Experiments/espnet/tools/venv/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 165, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/huang18/VC/Experiments/espnet/tools/venv/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 85, in parallel_apply
    output.reraise()
  File "/home/huang18/VC/Experiments/espnet/tools/venv/lib/python3.6/site-packages/torch/_utils.py", line 395, in reraise
    raise self.exc_type(msg)
Will finalize trainer extensions and updater before reraising the exception.
Traceback (most recent call last):
  File "/home/huang18/VC/Experiments/espnet/egs/ljspeech/tts1/../../../espnet/bin/tts_train.py", line 355, in &lt;module&gt;
    main(sys.argv[1:])
  File "/home/huang18/VC/Experiments/espnet/egs/ljspeech/tts1/../../../espnet/bin/tts_train.py", line 349, in main
    train(args)
  File "/nas01/internal/wenchin-h/VC/Experiments/espnet/espnet/tts/pytorch_backend/tts.py", line 563, in train
    trainer.run()
  File "/home/huang18/VC/Experiments/espnet/tools/venv/lib/python3.6/site-packages/chainer/training/trainer.py", line 349, in run
    six.reraise(*exc_info)
  File "/home/huang18/VC/Experiments/espnet/tools/venv/lib/python3.6/site-packages/six.py", line 693, in reraise
    raise value
  File "/home/huang18/VC/Experiments/espnet/tools/venv/lib/python3.6/site-packages/chainer/training/trainer.py", line 316, in run
    update()
  File "/nas01/internal/wenchin-h/VC/Experiments/espnet/espnet/tts/pytorch_backend/tts.py", line 171, in update
    self.update_core()
  File "/nas01/internal/wenchin-h/VC/Experiments/espnet/espnet/tts/pytorch_backend/tts.py", line 151, in update_core
    loss = self.model(**x).mean() / self.accum_grad
  File "/home/huang18/VC/Experiments/espnet/tools/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/huang18/VC/Experiments/espnet/tools/venv/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 155, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/huang18/VC/Experiments/espnet/tools/venv/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 165, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/huang18/VC/Experiments/espnet/tools/venv/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 85, in parallel_apply
    output.reraise()
  File "/home/huang18/VC/Experiments/espnet/tools/venv/lib/python3.6/site-packages/torch/_utils.py", line 395, in reraise
    raise self.exc_type(msg)
StopIteration: Caught StopIteration in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/huang18/VC/Experiments/espnet/tools/venv/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 60, in _worker
    output = module(*input, **kwargs)
  File "/home/huang18/VC/Experiments/espnet/tools/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/nas01/internal/wenchin-h/VC/Experiments/espnet/espnet/nets/pytorch_backend/e2e_tts_tacotron2.py", line 746, in forward
    after_outs, before_outs, logits, att_ws = self.dec(hs, hlens, ys)
  File "/home/huang18/VC/Experiments/espnet/tools/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/nas01/internal/wenchin-h/VC/Experiments/espnet/espnet/nets/pytorch_backend/tacotron2/decoder.py", line 436, in forward
    att_c, att_w = self.att(hs, hlens, z_list[0], prev_att_w)
  File "/home/huang18/VC/Experiments/espnet/tools/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/nas01/internal/wenchin-h/VC/Experiments/espnet/espnet/nets/pytorch_backend/rnn/attentions.py", line 365, in forward
    self.mask = to_device(self, make_pad_mask(enc_hs_len))
  File "/nas01/internal/wenchin-h/VC/Experiments/espnet/espnet/nets/pytorch_backend/nets_utils.py", line 24, in to_device
    device = next(m.parameters()).device
StopIteration
&lt;/denchmark-code&gt;

According to this issue: &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/40457&gt;pytorch/pytorch#40457&lt;/denchmark-link&gt;
 , a quick fix is to use PyTorch 1.4.0.
But as the answer states, &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/40457#issuecomment-648396469&gt;pytorch/pytorch#40457 (comment)&lt;/denchmark-link&gt;
 , accessing model parameters with   is the root cause.
For example, in
&lt;denchmark-link:https://github.com/espnet/espnet/blob/master/espnet/nets/pytorch_backend/nets_utils.py#L24&gt;https://github.com/espnet/espnet/blob/master/espnet/nets/pytorch_backend/nets_utils.py#L24&lt;/denchmark-link&gt;
 .
I am not sure if there exists a small, fast solution to this issue that does not touch this seemingly fundamental, highly used function... Just reporting it in case someone else is also having this problem.
	</description>
	<comments>
		<comment id='1' author='unilight' date='2020-07-13T18:32:50Z'>
		Thanks for the report, &lt;denchmark-link:https://github.com/unilight&gt;@unilight&lt;/denchmark-link&gt;
.
I think this would not be a serious issue in the near future once we start to use espnet2 (espnet2 &lt;denchmark-link:https://github.com/espnet/espnet/blob/536d163870e29ea52bbaa4cc0b03cfc407df0c0b/espnet2/torch_utils/device_funcs.py#L8&gt;to_device&lt;/denchmark-link&gt;
 does not use the ).
So, I'll leave it as it is for now.
		</comment>
		<comment id='2' author='unilight' date='2020-09-07T08:25:08Z'>
		still happen in pytorch 1.6
		</comment>
		<comment id='3' author='unilight' date='2020-09-09T08:34:33Z'>
		Please fix if you need. This is easy bug.
		</comment>
	</comments>
</bug>