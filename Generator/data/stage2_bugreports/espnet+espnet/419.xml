<bug id='419' author='chenzhehuai' open_date='2018-09-23T06:28:02Z' closed_time='2018-11-11T18:24:47Z'>
	<summary>error when using ngpu=8</summary>
	<description>
version:
&lt;denchmark-link:https://github.com/espnet/espnet/commit/84f72a4950a23ee92e3a554ccc4960e565e5939e&gt;84f72a4&lt;/denchmark-link&gt;

command (in librispeech):
asr_train.py --ngpu 8 --backend pytorch --outdir exp/train_100_blstmp_e8_subsample1_2_2_1_1_unit320_proj320_d1_unit300_location_aconvc10_aconvf100_mtlalpha0.5_adadelta_bs20_mli800_mlo150.2c/results --debugmode 1 --dict data/lang_1char/train_100_units.txt --debugdir exp/train_100_blstmp_e8_subsample1_2_2_1_1_unit320_proj320_d1_unit300_location_aconvc10_aconvf100_mtlalpha0.5_adadelta_bs20_mli800_mlo150.2c --minibatches 0 --verbose 5 --resume --train-json dump/train_100/deltafalse/data.json --valid-json dump/dev/deltafalse/data.json --etype blstmp --elayers 8 --eunits 320 --eprojs 320 --subsample 1_2_2_1_1 --dlayers 1 --dunits 300 --atype location --aconv-chans 10 --aconv-filts 100 --mtlalpha 0.5 --batch-size 5 --maxlen-in 800 --maxlen-out 150 --opt adadelta --epochs 15
error log:
Exception in main training loop: invalid gradient at index 0 - expected shape [7] but got [8]
Traceback (most recent call last):
File "/mnt/homedir/chenzhehuai/works/e2e/codes/espnet-git/tools/venv/lib/python3.7/site-packages/chainer/training/trainer.py", line 306, in run
update()
File "/mnt/homedir/chenzhehuai/works/e2e/codes/espnet-git/tools/venv/lib/python3.7/site-packages/chainer/training/updaters/standard_updater.py", line 149, in update
self.update_core()
File "/mnt/homedir/chenzhehuai/works/e2e/codes/espnet-git/src/asr/asr_pytorch.py", line 123, in update_core
loss.backward(loss.new_ones(self.ngpu))  # Backprop
File "/mnt/homedir/chenzhehuai/works/e2e/codes/espnet-git/tools/venv/lib/python3.7/site-packages/torch/tensor.py", line 93, in backward
torch.autograd.backward(self, gradient, retain_graph, create_graph)
File "/mnt/homedir/chenzhehuai/works/e2e/codes/espnet-git/tools/venv/lib/python3.7/site-packages/torch/autograd/init.py", line 90, in backward
allow_unreachable=True)  # allow_unreachable flag
Will finalize trainer extensions and updater before reraising the exception.
Traceback (most recent call last):
File "/mnt/homedir/chenzhehuai/works/e2e/egs/partlibrispeech/asr1/../../../codes/espnet-work//src/bin/asr_train.py", line 193, in 
main()
File "/mnt/homedir/chenzhehuai/works/e2e/egs/partlibrispeech/asr1/../../../codes/espnet-work//src/bin/asr_train.py", line 187, in main
train(args)
File "/mnt/homedir/chenzhehuai/works/e2e/codes/espnet-git/src/asr/asr_pytorch.py", line 353, in train
trainer.run()
File "/mnt/homedir/chenzhehuai/works/e2e/codes/espnet-git/tools/venv/lib/python3.7/site-packages/chainer/training/trainer.py", line 320, in run
six.reraise(*sys.exc_info())
File "/mnt/homedir/chenzhehuai/works/e2e/codes/espnet-git/tools/venv/lib/python3.7/site-packages/six.py", line 693, in reraise
raise value
File "/mnt/homedir/chenzhehuai/works/e2e/codes/espnet-git/tools/venv/lib/python3.7/site-packages/chainer/training/trainer.py", line 306, in run
update()
File "/mnt/homedir/chenzhehuai/works/e2e/codes/espnet-git/tools/venv/lib/python3.7/site-packages/chainer/training/updaters/standard_updater.py", line 149, in update
self.update_core()
File "/mnt/homedir/chenzhehuai/works/e2e/codes/espnet-git/src/asr/asr_pytorch.py", line 123, in update_core
loss.backward(loss.new_ones(self.ngpu))  # Backprop
File "/mnt/homedir/chenzhehuai/works/e2e/codes/espnet-git/tools/venv/lib/python3.7/site-packages/torch/tensor.py", line 93, in backward
torch.autograd.backward(self, gradient, retain_graph, create_graph)
File "/mnt/homedir/chenzhehuai/works/e2e/codes/espnet-git/tools/venv/lib/python3.7/site-packages/torch/autograd/init.py", line 90, in backward
allow_unreachable=True)  # allow_unreachable flag
RuntimeError: invalid gradient at index 0 - expected shape [7] but got [8]
note:
it always exists. However, using different sizes of batch will change the number of iterations before coming into this error
	</description>
	<comments>
		<comment id='1' author='chenzhehuai' date='2018-09-23T23:27:42Z'>
		I'm not sure, but maybe due to the small number of batchsize.
We change batchsize with respect to output / input length so batchsize will be less than #gpus.
You can use bigger batchsize or bigger maxlen-out / maxlen-in to avoid this problem.
		</comment>
		<comment id='2' author='chenzhehuai' date='2018-09-23T23:44:16Z'>
		why shall we make batchsize less than #gpus? Moreover, I notice that real batchsize = batchsize * #gpu. Then why shall we use bigger batchsize?
		</comment>
		<comment id='3' author='chenzhehuai' date='2018-09-24T00:09:41Z'>
		
why shall we make batchsize less than #gpus?
I notice that real batchsize = batchsize * #gpu

Actually, we do not assume the use of small batchsize with a lot of gpus.
As you mentioned, real batchsize is batchsize * #gpu.
But we dynamically change batchsize because there are quite long samples in such as librispeech dataset.
You can see this procedure in



espnet/src/asr/asr_utils.py


        Lines 33 to 58
      in
      d53fff9






 def make_batchset(data, batch_size, max_length_in, max_length_out, num_batches=0): 



 # sort it by input lengths (long to short) 



 sorted_data = sorted(data.items(), key=lambda data: int( 



 data[1]['input'][0]['shape'][0]), reverse=True) 



 logging.info('# utts: ' + str(len(sorted_data))) 



 # change batchsize depending on the input and output length 



 minibatch = [] 



 start = 0 



 while True: 



 ilen = int(sorted_data[start][1]['input'][0]['shape'][0]) 



 olen = int(sorted_data[start][1]['output'][0]['shape'][0]) 



 factor = max(int(ilen / max_length_in), int(olen / max_length_out)) 



 # if ilen = 1000 and max_length_in = 800 



 # then b = batchsize / 2 



 # and max(1, .) avoids batchsize = 0 



 b = max(1, int(batch_size / (1 + factor))) 



 end = min(len(sorted_data), start + b) 



 minibatch.append(sorted_data[start:end]) 



 if end == len(sorted_data): 



 break 



 start = end 



 if num_batches &gt; 0: 



 minibatch = minibatch[:num_batches] 



 logging.info('# minibatches: ' + str(len(minibatch))) 



 



 return minibatch 





In your case, real batchsize = 5 * 8 = 40, but if the factor is more than 5, batchsize will be less than 8.
		</comment>
		<comment id='4' author='chenzhehuai' date='2018-09-24T06:50:07Z'>
		I made some modification like the following. But still obtain this error
&lt;denchmark-code&gt;+def make_batchset(data, batch_size, max_length_in, max_length_out, num_batches=0, min_batch_size=1):
     # sort it by input lengths (long to short)
     sorted_data = sorted(data.items(), key=lambda data: int(
         data[1]['input'][0]['shape'][0]), reverse=True)
@@ -37,7 +37,7 @@ def make_batchset(data, batch_size, max_length_in, max_length_out, num_batches=0
         # if ilen = 1000 and max_length_in = 800
         # then b = batchsize / 2
         # and max(1, .) avoids batchsize = 0
-        b = max(1, int(batch_size / (1 + factor)))
+        b = max(min_batch_size, int(batch_size / (1 + factor)))
         end = min(len(sorted_data), start + b)
         minibatch.append(sorted_data[start:end])
         if end == len(sorted_data):```
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='chenzhehuai' date='2018-09-24T06:53:01Z'>
		Did you set min_batch_size=8?
If ngpu = 8, batchsize should be more than 8.
		</comment>
		<comment id='6' author='chenzhehuai' date='2018-09-24T14:44:10Z'>
		yes, i do so. then i still get the error before
		</comment>
		<comment id='7' author='chenzhehuai' date='2018-09-25T00:15:24Z'>
		Hmm...
Let me confirm some points.

Is it working with single gpu ?
Is your warp-ctc version matched with current master?
You can see how-to-check in README.
Is it working with chainer backend?

		</comment>
		<comment id='8' author='chenzhehuai' date='2018-09-26T07:16:49Z'>
		
it works


when i use py2.7, it works
when i use py3.7, the version check is fine. But then it complains pytorch is not installed with Numpy support (torch-0.4.1-cp37-cp37m-linux_x86_64.whl) during any runtime
then I install pytorch by myself (torch-0.4.1.post2-cp37-cp37m-linux_x86_64.whl), the version check is fail. it works in less than 7 gpus, and I observe the 8 gpu problem as above
3. i didn't check chainer
		</comment>
		<comment id='9' author='chenzhehuai' date='2018-09-26T12:27:18Z'>
		&lt;denchmark-link:https://github.com/chenzhehuai&gt;@chenzhehuai&lt;/denchmark-link&gt;
 Thank you for your kind report.
I have no idea about this error...
&lt;denchmark-link:https://github.com/sw005320&gt;@sw005320&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ShigekiKarita&gt;@ShigekiKarita&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jnishi&gt;@jnishi&lt;/denchmark-link&gt;

Do you have any idea to fix it?
		</comment>
		<comment id='10' author='chenzhehuai' date='2018-09-27T04:21:47Z'>
		What is numpy version using with pytorch? maybe you need to update numpy pip install -U numpy or need to use anaconda
		</comment>
		<comment id='11' author='chenzhehuai' date='2018-09-27T23:58:11Z'>
		numpy==1.15.1
		</comment>
		<comment id='12' author='chenzhehuai' date='2018-09-28T15:20:07Z'>
		Did you set CUDA_VISIBLE_DEVICES correctly? It looks like that you only got 7 gpus.
		</comment>
		<comment id='13' author='chenzhehuai' date='2018-09-29T03:10:36Z'>
		&lt;denchmark-link:https://github.com/bobchennan&gt;@bobchennan&lt;/denchmark-link&gt;
 it's not related to that
		</comment>
		<comment id='14' author='chenzhehuai' date='2018-10-31T18:58:10Z'>
		I am facing similar issue. Adjusting max_len_in and lax_len_out delays in getting the error.
		</comment>
		<comment id='15' author='chenzhehuai' date='2018-11-11T00:42:30Z'>
		Maybe this error is fixed in &lt;denchmark-link:https://github.com/espnet/espnet/pull/463&gt;#463&lt;/denchmark-link&gt;
.
		</comment>
	</comments>
</bug>