<bug id='1475' author='kaituoxu' open_date='2019-12-26T13:03:39Z' closed_time='2020-08-28T21:22:07Z'>
	<summary>transformer bug</summary>
	<description>
&lt;denchmark-link:https://github.com/espnet/espnet/blob/master/espnet/nets/pytorch_backend/e2e_asr_transformer.py#L172&gt;https://github.com/espnet/espnet/blob/master/espnet/nets/pytorch_backend/e2e_asr_transformer.py#L172&lt;/denchmark-link&gt;
 should be  ?
	</description>
	<comments>
		<comment id='1' author='kaituoxu' date='2019-12-26T14:17:42Z'>
		Here we want to make a mask for padded part so self.ignore_id (padding id) is correct.
		</comment>
		<comment id='2' author='kaituoxu' date='2019-12-26T14:23:05Z'>
		Oh, I checked the padding part and ys_in_pad is padded with eos. I will check more detail.
		</comment>
		<comment id='3' author='kaituoxu' date='2019-12-26T14:54:15Z'>
		ys_in_pad is padded with self.eos in add_sos_eos.



espnet/espnet/nets/pytorch_backend/transformer/add_sos_eos.py


        Lines 12 to 30
      in
      ddeb5da






 def add_sos_eos(ys_pad, sos, eos, ignore_id): 



 """Add &lt;sos&gt; and &lt;eos&gt; labels. 



  



     :param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax) 



     :param int sos: index of &lt;sos&gt; 



     :param int eos: index of &lt;eeos&gt; 



     :param int ignore_id: index of padding 



     :return: padded tensor (B, Lmax) 



     :rtype: torch.Tensor 



     :return: padded tensor (B, Lmax) 



     :rtype: torch.Tensor 



     """ 



 from espnet.nets.pytorch_backend.nets_utils import pad_list 



 _sos = ys_pad.new([sos]) 



 _eos = ys_pad.new([eos]) 



 ys = [y[y != ignore_id] for y in ys_pad]  # parse padded ys 



 ys_in = [torch.cat([_sos, y], dim=0) for y in ys] 



 ys_out = [torch.cat([y, _eos], dim=0) for y in ys] 



 return pad_list(ys_in, eos), pad_list(ys_out, ignore_id) 





In making the target mask, ys_in_pad and self.ignore_id are used.



espnet/espnet/nets/pytorch_backend/e2e_asr_transformer.py


         Line 172
      in
      ddeb5da






 ys_mask = target_mask(ys_in_pad, self.ignore_id) 





So ys_mask in target_mask() will be all of 1.



espnet/espnet/nets/pytorch_backend/transformer/mask.py


        Lines 32 to 42
      in
      ddeb5da






 def target_mask(ys_in_pad, ignore_id): 



 """Create mask for decoder self-attention. 



  



     :param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax) 



     :param int ignore_id: index of padding 



     :param torch.dtype dtype: result dtype 



     :rtype: torch.Tensor 



     """ 



 ys_mask = ys_in_pad != ignore_id 



 m = subsequent_mask(ys_mask.size(-1), device=ys_mask.device).unsqueeze(0) 



 return ys_mask.unsqueeze(-2) &amp; m 





I think this is a bug.
Could you check it? &lt;denchmark-link:https://github.com/ShigekiKarita&gt;@ShigekiKarita&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>