<bug id='279' author='ZhuoranLyu' open_date='2018-07-06T08:02:54Z' closed_time='2018-08-27T10:04:35Z'>
	<summary>Multi Gpus with pytorch backend problem.</summary>
	<description>
I am using Pytorch 0.4 with 4 GTX 1080Ti. When I run using pytorch backend and multi gpus, it gives me this error.
`# asr_train.py --ngpu 4 --backend pytorch --outdir exp/tr_en_vggblstmp_e4_subsample1_2_2_1_1_unit320_proj320_d1_unit300_location_aconvc10_aconvf100_mtlalpha0.5_adadelta_bs30_mli800_mlo150/results --debugmode 1 --dict data/lang_1char/tr_en_units.txt --debugdir exp/tr_en_vggblstmp_e4_subsample1_2_2_1_1_unit320_proj320_d1_unit300_location_aconvc10_aconvf100_mtlalpha0.5_adadelta_bs30_mli800_mlo150 --minibatches 0 --verbose 0 --resume --train-json dump/tr_en/deltafalse/data.json --valid-json dump/dt_en/deltafalse/data.json --etype vggblstmp --elayers 4 --eunits 320 --eprojs 320 --subsample 1_2_2_1_1 --dlayers 1 --dunits 300 --atype location --aconv-chans 10 --aconv-filts 100 --mtlalpha 0.5 --batch-size 30 --maxlen-in 800 --maxlen-out 150 --opt adadelta --epochs 100
&lt;denchmark-h:h1&gt;Started at Fri Jul  6 10:05:31 CST 2018&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;&lt;/denchmark-h&gt;

2018-07-06 10:05:31,582 (asr_train:146) WARNING: Skip DEBUG/INFO messages
2018-07-06 10:05:31,587 (asr_train:186) WARNING: CUDA_VISIBLE_DEVICES is not set.
2018-07-06 10:05:35,803 (e2e_asr_attctc_th:198) WARNING: Subsampling is not performed for vgg*. It is performed in max pooling layers at CNN.
Exception in main training loop: torch/csrc/autograd/variable.cpp:115: get_grad_fn: Assertion output_nr == 0 failed.
Traceback (most recent call last):
File "/home/lvzhuoran/code/espnet-master/tools/venv/local/lib/python2.7/site-packages/chainer/training/trainer.py", line 306, in run
update()
File "/home/lvzhuoran/code/espnet-master/tools/venv/local/lib/python2.7/site-packages/chainer/training/updaters/standard_updater.py", line 149, in update
self.update_core()
File "/home/lvzhuoran/code/espnet-master/src/asr/asr_pytorch.py", line 122, in update_core
loss = 1. / self.num_gpu * self.model(x)
File "/home/lvzhuoran/code/espnet-master/tools/venv/local/lib/python2.7/site-packages/torch/nn/modules/module.py", line 491, in call
result = self.forward(*input, **kwargs)
File "/home/lvzhuoran/code/espnet-master/src/asr/asr_pytorch.py", line 168, in forward
outputs = self.parallel_apply(replicas, inputs, kwargs)
File "/home/lvzhuoran/code/espnet-master/tools/venv/local/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py", line 124, in parallel_apply
return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
File "/home/lvzhuoran/code/espnet-master/tools/venv/local/lib/python2.7/site-packages/torch/nn/parallel/parallel_apply.py", line 65, in parallel_apply
raise output
Will finalize trainer extensions and updater before reraising the exception.
�[JTraceback (most recent call last):
File "/home/lvzhuoran/code/espnet-master/egs/voxforge/asr1/../../../src/bin/asr_train.py", line 224, in 
main()
File "/home/lvzhuoran/code/espnet-master/egs/voxforge/asr1/../../../src/bin/asr_train.py", line 218, in main
train(args)
File "/home/lvzhuoran/code/espnet-master/src/asr/asr_pytorch.py", line 377, in train
trainer.run()
File "/home/lvzhuoran/code/espnet-master/tools/venv/local/lib/python2.7/site-packages/chainer/training/trainer.py", line 320, in run
six.reraise(*sys.exc_info())
File "/home/lvzhuoran/code/espnet-master/tools/venv/local/lib/python2.7/site-packages/chainer/training/trainer.py", line 306, in run
update()
File "/home/lvzhuoran/code/espnet-master/tools/venv/local/lib/python2.7/site-packages/chainer/training/updaters/standard_updater.py", line 149, in update
self.update_core()
File "/home/lvzhuoran/code/espnet-master/src/asr/asr_pytorch.py", line 122, in update_core
loss = 1. / self.num_gpu * self.model(x)
File "/home/lvzhuoran/code/espnet-master/tools/venv/local/lib/python2.7/site-packages/torch/nn/modules/module.py", line 491, in call
result = self.forward(*input, **kwargs)
File "/home/lvzhuoran/code/espnet-master/src/asr/asr_pytorch.py", line 168, in forward
outputs = self.parallel_apply(replicas, inputs, kwargs)
File "/home/lvzhuoran/code/espnet-master/tools/venv/local/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py", line 124, in parallel_apply
return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
File "/home/lvzhuoran/code/espnet-master/tools/venv/local/lib/python2.7/site-packages/torch/nn/parallel/parallel_apply.py", line 65, in parallel_apply
raise output
RuntimeError: torch/csrc/autograd/variable.cpp:115: get_grad_fn: Assertion output_nr == 0 failed.`
I am using Pytorch 0.4. I googled it and found &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/7092&gt;this&lt;/denchmark-link&gt;
 link to be useful.
Thanks,
George.
	</description>
	<comments>
		<comment id='1' author='ZhuoranLyu' date='2018-07-06T09:06:39Z'>
		I am afraid this is because of using flatten_parameters with DataParallel.
		</comment>
		<comment id='2' author='ZhuoranLyu' date='2018-07-06T13:06:30Z'>
		Thanks for your report. &lt;denchmark-link:https://github.com/bobchennan&gt;@bobchennan&lt;/denchmark-link&gt;
, can you take a look at it?
		</comment>
		<comment id='3' author='ZhuoranLyu' date='2018-07-09T03:16:33Z'>
		BTW, if I switch to pytorch 0.3.1, everything works fine.
		</comment>
		<comment id='4' author='ZhuoranLyu' date='2018-07-09T13:15:53Z'>
		Thanks for your report. Yes, we first implemented it with pytorch 0.3.1, but we do not test it with pytrorch 0.4.0. We'll fix it soon.
		</comment>
		<comment id='5' author='ZhuoranLyu' date='2018-07-10T01:27:57Z'>
		Thx！
		</comment>
		<comment id='6' author='ZhuoranLyu' date='2018-07-11T13:02:32Z'>
		&lt;denchmark-link:https://github.com/espnet/espnet/pull/286&gt;#286&lt;/denchmark-link&gt;
 fixed the issue. Can you test it?
		</comment>
		<comment id='7' author='ZhuoranLyu' date='2018-07-12T02:11:59Z'>
		&lt;denchmark-link:https://github.com/sw005320&gt;@sw005320&lt;/denchmark-link&gt;
 It seems that there still has some problems. When the training reaches the end of one epoch, it gives me this error:
`# asr_train.py --ngpu 4 --backend pytorch --outdir exp/train_960_blstmp_e8_subsample1_2_2_1_1_unit320_proj320_d1_unit300_location_aconvc10_aconvf100_mtlalpha0.5_adadelta_bs50_mli800_mlo150_ngpu4/results --debugmode 1 --dict data/lang_1char/train_960_units.txt --debugdir exp/train_960_blstmp_e8_subsample1_2_2_1_1_unit320_proj320_d1_unit300_location_aconvc10_aconvf100_mtlalpha0.5_adadelta_bs50_mli800_mlo150_ngpu4 --minibatches 0 --verbose 0 --resume --train-json dump/train_960/deltafalse/data.json --valid-json dump/dev/deltafalse/data.json --etype blstmp --elayers 8 --eunits 320 --eprojs 320 --subsample 1_2_2_1_1 --dlayers 1 --dunits 300 --atype location --aconv-chans 10 --aconv-filts 100 --mtlalpha 0.5 --batch-size 50 --maxlen-in 800 --maxlen-out 150 --opt adadelta --epochs 30
&lt;denchmark-h:h1&gt;Started at Thu Jul 12 10:25:53 CST 2018&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;&lt;/denchmark-h&gt;

2018-07-12 10:25:53,428 (asr_train:146) WARNING: Skip DEBUG/INFO messages
2018-07-12 10:25:53,432 (asr_train:186) WARNING: CUDA_VISIBLE_DEVICES is not set.
/home/lvzhuoran/code/espnet-master/src/nets/e2e_asr_attctc_th.py:2107: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
ys, (hy, cy) = bilstm(xpack)
/home/lvzhuoran/code/espnet-master/src/asr/asr_pytorch.py:128: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
self.model.parameters(), self.grad_clip_threshold)
epoch       iteration   main/loss   main/loss_ctc  main/loss_att  validation/main/loss  validation/main/loss_ctc  validation/main/loss_att  main/acc    validation/main/acc  elapsed_time  eps
�[J0           100         515.312     558.079        472.545                                                                                  0.280089                         331.166       1e-08
�[J     total [..................................................]  0.19%
this epoch [##................................................]  5.61%
100 iter, 0 epoch / 30 epochs
inf iters/sec. Estimated time to finish: 0:00:00.
�[4A�[J0           200         484.743     556.646        412.841                                                                                  0.365731                         624.706       1e-08
�[J     total [..................................................]  0.37%
this epoch [#####.............................................] 11.21%
200 iter, 0 epoch / 30 epochs
0.34067 iters/sec. Estimated time to finish: 1 day, 19:28:34.779977.
�[4A�[J0           300         451.465     533.941        368.989                                                                                  0.402319                         912.455       1e-08
�[J     total [..................................................]  0.56%
this epoch [########..........................................] 16.82%
300 iter, 0 epoch / 30 epochs
0.34406 iters/sec. Estimated time to finish: 1 day, 18:58:00.765822.
�[4A�[J0           400         447.58      538.509        356.652                                                                                  0.428811                         1205.26       1e-08
�[J     total [..................................................]  0.75%
this epoch [###########.......................................] 22.42%
400 iter, 0 epoch / 30 epochs
0.34321 iters/sec. Estimated time to finish: 1 day, 18:59:33.354515.
�[4A�[J0           500         428.668     519.339        337.996                                                                                  0.445493                         1494.33       1e-08
�[J     total [..................................................]  0.93%
this epoch [##############....................................] 28.03%
500 iter, 0 epoch / 30 epochs
0.34389 iters/sec. Estimated time to finish: 1 day, 18:49:37.581074.
�[4A�[J0           600         409.639     488.171        331.107                                                                                  0.458199                         1788.97       1e-08
�[J     total [..................................................]  1.12%
this epoch [################..................................] 33.63%
600 iter, 0 epoch / 30 epochs
0.34298 iters/sec. Estimated time to finish: 1 day, 18:51:33.448509.
�[4A�[J0           700         385.52      451.587        319.453                                                                                  0.465711                         2083.16       1e-08
�[J     total [..................................................]  1.31%
this epoch [###################...............................] 39.24%
700 iter, 0 epoch / 30 epochs
0.34247 iters/sec. Estimated time to finish: 1 day, 18:50:33.496601.
�[4A�[J0           800         389.889     446.499        333.279                                                                                  0.474402                         2382.15       1e-08
�[J     total [..................................................]  1.49%
this epoch [######################............................] 44.84%
800 iter, 0 epoch / 30 epochs
0.3413 iters/sec. Estimated time to finish: 1 day, 18:54:28.482865.
�[4A�[J0           900         353.126     394.069        312.182                                                                                  0.481152                         2675.28       1e-08
�[J     total [..................................................]  1.68%
this epoch [#########################.........................] 50.45%
900 iter, 0 epoch / 30 epochs
0.34128 iters/sec. Estimated time to finish: 1 day, 18:49:44.337051.
�[4A�[J0           1000        345.15      374.333        315.968                                                                                  0.486325                         2976.16       1e-08
�[J     total [..................................................]  1.87%
this epoch [############################......................] 56.05%
1000 iter, 0 epoch / 30 epochs
0.34027 iters/sec. Estimated time to finish: 1 day, 18:52:30.189084.
�[4A�[J0           1100        334.35      352.732        315.968                                                                                  0.492383                         3276.25       1e-08
�[J     total [#.................................................]  2.06%
this epoch [##############################....................] 61.66%
1100 iter, 0 epoch / 30 epochs
0.33955 iters/sec. Estimated time to finish: 1 day, 18:53:01.303386.
�[4A�[J0           1200        316.798     324.8          308.796                                                                                  0.496248                         3576.24       1e-08
�[J     total [#.................................................]  2.24%
this epoch [#################################.................] 67.26%
1200 iter, 0 epoch / 30 epochs
0.33898 iters/sec. Estimated time to finish: 1 day, 18:52:27.585158.
�[4A�[J0           1300        315.68      313.791        317.57                                                                                   0.500907                         3878.5        1e-08
�[J     total [#.................................................]  2.43%
this epoch [####################################..............] 72.87%
1300 iter, 0 epoch / 30 epochs
0.33828 iters/sec. Estimated time to finish: 1 day, 18:52:48.223906.
�[4A�[J0           1400        278.863     271.011        286.714                                                                                  0.501564                         4168.21       1e-08
�[J     total [#.................................................]  2.62%
this epoch [#######################################...........] 78.48%
1400 iter, 0 epoch / 30 epochs
0.3388 iters/sec. Estimated time to finish: 1 day, 18:43:55.808160.
�[4A�[J0           1500        277.845     262.882        292.808                                                                                  0.504429                         4462.35       1e-08
�[J     total [#.................................................]  2.80%
this epoch [##########################################........] 84.08%
1500 iter, 0 epoch / 30 epochs
0.33889 iters/sec. Estimated time to finish: 1 day, 18:38:22.981319.
�[4A�[J0           1600        274.458     254.485        294.432                                                                                  0.509319                         4754.3        1e-08
�[J     total [#.................................................]  2.99%
this epoch [############################################......] 89.69%
1600 iter, 0 epoch / 30 epochs
0.33913 iters/sec. Estimated time to finish: 1 day, 18:31:39.526578.
�[4A�[J0           1700        260.814     236.497        285.131                                                                                  0.511621                         5048.18       1e-08
�[J     total [#.................................................]  3.18%
this epoch [###############################################...] 95.29%
1700 iter, 0 epoch / 30 epochs
0.3392 iters/sec. Estimated time to finish: 1 day, 18:26:12.250152.
�[4AException in main training loop: element 0 of tensors does not require grad and does not have a grad_fn
Traceback (most recent call last):
File "/home/lvzhuoran/code/espnet-master/tools/venv/local/lib/python2.7/site-packages/chainer/training/trainer.py", line 306, in run
update()
File "/home/lvzhuoran/code/espnet-master/tools/venv/local/lib/python2.7/site-packages/chainer/training/updaters/standard_updater.py", line 149, in update
self.update_core()
File "/home/lvzhuoran/code/espnet-master/src/asr/asr_pytorch.py", line 122, in update_core
loss.backward(torch.ones(self.num_gpu))  # Backprop
File "/home/lvzhuoran/code/espnet-master/tools/venv/local/lib/python2.7/site-packages/torch/tensor.py", line 93, in backward
torch.autograd.backward(self, gradient, retain_graph, create_graph)
File "/home/lvzhuoran/code/espnet-master/tools/venv/local/lib/python2.7/site-packages/torch/autograd/init.py", line 89, in backward
allow_unreachable=True)  # allow_unreachable flag
Will finalize trainer extensions and updater before reraising the exception.
�[JTraceback (most recent call last):
File "/home/lvzhuoran/code/espnet-master/egs/librispeech/asr1/../../../src/bin/asr_train.py", line 224, in 
main()
File "/home/lvzhuoran/code/espnet-master/egs/librispeech/asr1/../../../src/bin/asr_train.py", line 218, in main
train(args)
File "/home/lvzhuoran/code/espnet-master/src/asr/asr_pytorch.py", line 370, in train
trainer.run()
File "/home/lvzhuoran/code/espnet-master/tools/venv/local/lib/python2.7/site-packages/chainer/training/trainer.py", line 320, in run
six.reraise(*sys.exc_info())
File "/home/lvzhuoran/code/espnet-master/tools/venv/local/lib/python2.7/site-packages/chainer/training/trainer.py", line 306, in run
update()
File "/home/lvzhuoran/code/espnet-master/tools/venv/local/lib/python2.7/site-packages/chainer/training/updaters/standard_updater.py", line 149, in update
self.update_core()
File "/home/lvzhuoran/code/espnet-master/src/asr/asr_pytorch.py", line 122, in update_core
loss.backward(torch.ones(self.num_gpu))  # Backprop
File "/home/lvzhuoran/code/espnet-master/tools/venv/local/lib/python2.7/site-packages/torch/tensor.py", line 93, in backward
torch.autograd.backward(self, gradient, retain_graph, create_graph)
File "/home/lvzhuoran/code/espnet-master/tools/venv/local/lib/python2.7/site-packages/torch/autograd/init.py", line 89, in backward
allow_unreachable=True)  # allow_unreachable flag
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
&lt;denchmark-h:h1&gt;Accounting: time=5383 threads=1&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;Ended (code 1) at Thu Jul 12 11:55:36 CST 2018, elapsed time 5383 seconds`&lt;/denchmark-h&gt;

		</comment>
		<comment id='8' author='ZhuoranLyu' date='2018-07-12T10:59:05Z'>
		still having the same issue, although it only happens during validation (commit &lt;denchmark-link:https://github.com/espnet/espnet/commit/c72b0687ebee68f401a72a8fa98998cb87f5642e&gt;c72b068&lt;/denchmark-link&gt;
)
FYI, the error message:
&lt;denchmark-code&gt;Exception in main training loop: element 0 of tensors does not require grad and does not have a grad_fn
Traceback (most recent call last):###########################.] 98.18%
  File "/espnet/tools/venv/local/lib/python2.7/site-packages/chainer/training/trainer.py", line 306, in run
    update()ters/sec. Estimated time to finish: 6:18:59.725694.
  File "/espnet/tools/venv/local/lib/python2.7/site-packages/chainer/training/updaters/standard_updater.py", line 149, in update
    self.update_core()
  File "/espnet/src/asr/asr_pytorch.py", line 125, in update_core
    loss.backward(torch.ones(self.num_gpu))  # Backprop
  File "/espnet/tools/venv/local/lib/python2.7/site-packages/torch/tensor.py", line 93, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/espnet/tools/venv/local/lib/python2.7/site-packages/torch/autograd/__init__.py", line 89, in backward
    allow_unreachable=True)  # allow_unreachable flag
Will finalize trainer extensions and updater before reraising the exception.
Traceback (most recent call last):
  File "/espnet/egs/cgn/asr1/../../../src/bin/asr_train.py", line 224, in &lt;module&gt;
    main()
  File "/espnet/egs/cgn/asr1/../../../src/bin/asr_train.py", line 218, in main
    train(args)
  File "/espnet/src/asr/asr_pytorch.py", line 377, in train
    trainer.run()
  File "/espnet/tools/venv/local/lib/python2.7/site-packages/chainer/training/trainer.py", line 320, in run
    six.reraise(*sys.exc_info())
  File "/espnet/tools/venv/local/lib/python2.7/site-packages/chainer/training/trainer.py", line 306, in run
    update()
  File "/espnet/tools/venv/local/lib/python2.7/site-packages/chainer/training/updaters/standard_updater.py", line 149, in update
    self.update_core()
  File "/espnet/src/asr/asr_pytorch.py", line 125, in update_core
    loss.backward(torch.ones(self.num_gpu))  # Backprop
  File "/espnet/tools/venv/local/lib/python2.7/site-packages/torch/tensor.py", line 93, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/espnet/tools/venv/local/lib/python2.7/site-packages/torch/autograd/__init__.py", line 89, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='ZhuoranLyu' date='2018-07-12T11:28:50Z'>
		Shouldn't we be using &lt;denchmark-link:https://pytorch.org/docs/stable/distributed.html#launch-utility&gt;DistributedDataParallel&lt;/denchmark-link&gt;
 instead of DataParallel?
		</comment>
		<comment id='10' author='ZhuoranLyu' date='2018-07-12T17:01:54Z'>
		&lt;denchmark-link:https://github.com/kan-bayashi&gt;@kan-bayashi&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/bobchennan&gt;@bobchennan&lt;/denchmark-link&gt;
, do you have any ideas?
(I don't have two GPU slots now to test it....)
		</comment>
		<comment id='11' author='ZhuoranLyu' date='2018-07-12T22:20:30Z'>
		Unless you are using multiple nodes to train the system which is not supported yet, you should not use DistributedDataParallel. I am not sure about 0.4 since recently I am using TF.
		</comment>
		<comment id='12' author='ZhuoranLyu' date='2018-07-13T00:48:25Z'>
		I will investigate it today.
		</comment>
		<comment id='13' author='ZhuoranLyu' date='2018-07-13T03:52:07Z'>
		&lt;denchmark-link:https://github.com/ZhuoranLyu&gt;@ZhuoranLyu&lt;/denchmark-link&gt;
 Could you test this patch? &lt;denchmark-link:https://github.com/espnet/espnet/pull/294&gt;#294&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='ZhuoranLyu' date='2018-07-13T06:58:47Z'>
		&lt;denchmark-link:https://github.com/bobchennan&gt;@bobchennan&lt;/denchmark-link&gt;
 DistributedDataParallel works for single nodes and it has been proved to have much better performance than data parallel. Check &lt;denchmark-link:https://github.com/NVIDIA/sentiment-discovery/blob/master/analysis/scale.md&gt;this&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/SeanNaren/deepspeech.pytorch/issues/211&gt;this&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='ZhuoranLyu' date='2018-07-13T07:56:25Z'>
		&lt;denchmark-link:https://github.com/kan-bayashi&gt;@kan-bayashi&lt;/denchmark-link&gt;
 I think it works with no problem now. Thank you for your help. The only problem is that using multi gpus won't accelerate the loss converge too much. On the contrary, the loss can't converge as low as using single gpu. Here is the result. After first epoch, the loss of single gpu is 103 while the loss of multi gpus is 263. There is a huge gap. So I'd better use single gpu to train the model on librispeech. Anyway, thanks a lot.
		</comment>
		<comment id='16' author='ZhuoranLyu' date='2018-07-13T13:21:05Z'>
		&lt;denchmark-link:https://github.com/miguelvr&gt;@miguelvr&lt;/denchmark-link&gt;
 interesting to know. Clearly multiprocessing should be better since it potentially avoids GIL problem.
		</comment>
		<comment id='17' author='ZhuoranLyu' date='2018-07-13T13:28:42Z'>
		There might have some bug (in our implementation ?). Actually chainer backend multigpu has better scalability (2 GPU ~ 2x speed) and the performance degradation is not so much compared with the pytorch backend. Of course one solution would be a reduce part, and I'll check it.
		</comment>
		<comment id='18' author='ZhuoranLyu' date='2018-07-16T07:01:03Z'>
		&lt;denchmark-link:https://github.com/sw005320&gt;@sw005320&lt;/denchmark-link&gt;
 Thanks. Another quick question, if I use vggblstmp encoder, shall I set do_delta=true instead of false in the run.sh?
		</comment>
		<comment id='19' author='ZhuoranLyu' date='2018-07-16T15:19:12Z'>
		You don't have to. I tested it, and this does not make significant difference.
		</comment>
		<comment id='20' author='ZhuoranLyu' date='2018-07-27T04:03:01Z'>
		DistributedDataParallel will be much better for RNNs. Please use that if possible, even on single node.
Have a look at our Launch Utility documentation that cleanly describes how to use DistributedDataParallel: &lt;denchmark-link:https://pytorch.org/docs/stable/distributed.html#launch-utility&gt;https://pytorch.org/docs/stable/distributed.html#launch-utility&lt;/denchmark-link&gt;

You can treat your training script has not getting a split input, which also simplifies your code a lot.
		</comment>
		<comment id='21' author='ZhuoranLyu' date='2018-08-27T10:04:35Z'>
		The bug itself is fixed. I will close this issue.
		</comment>
	</comments>
</bug>