<bug id='1075' author='fanlu' open_date='2019-08-11T07:12:56Z' closed_time='2019-08-14T10:11:31Z'>
	<summary>multi gpu lm training throws CUDA out of memory exception</summary>
	<description>
There was an out of memory exception thrown when I use lm.yaml and lm_large.yaml in librispeech egs with multi gpu=4.
&lt;denchmark-code&gt;Exception in main training loop: CUDA out of memory. Tried to allocate 19.62 MiB (GPU 2; 22.38 GiB total capacity; 21.01 GiB already allocated; 10.06 MiB free; 634.29 MiB cached)                                   
Traceback (most recent call last):
  File "/mnt/cfs1/asr/users/fanlu/miniconda3/lib/python3.7/site-packages/chainer/training/trainer.py", line 316, in run                                                                                              
    update()
  File "/mnt/cfs1/asr/users/fanlu/miniconda3/lib/python3.7/site-packages/chainer/training/updaters/standard_updater.py", line 175, in update                                                                         
    self.update_core()
  File "/mnt/cfs1/asr/users/fanlu/espnet/espnet/lm/pytorch_backend/lm.py", line 277, in update_core
    loss.backward()  # Backprop
  File "/mnt/cfs1/asr/users/fanlu/miniconda3/lib/python3.7/site-packages/torch/tensor.py", line 102, in backward                                                                                                     
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/mnt/cfs1/asr/users/fanlu/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py", line 90, in backward                                                                                           
    allow_unreachable=True)  # allow_unreachable flag
Will finalize trainer extensions and updater before reraising the exception.
Traceback (most recent call last):
  File "/mnt/cfs1/asr/users/fanlu/espnet/espnet/bin/lm_train.py", line 156, in &lt;module&gt;
    main(sys.argv[1:])
  File "/mnt/cfs1/asr/users/fanlu/espnet/espnet/bin/lm_train.py", line 150, in main
    train(args)
  File "/mnt/cfs1/asr/users/fanlu/espnet/espnet/lm/pytorch_backend/lm.py", line 416, in train
    trainer.run()
  File "/mnt/cfs1/asr/users/fanlu/miniconda3/lib/python3.7/site-packages/chainer/training/trainer.py", line 349, in run                                                                                              
    six.reraise(*exc_info)
  File "/mnt/cfs1/asr/users/fanlu/miniconda3/lib/python3.7/site-packages/six.py", line 693, in reraise
    raise value
  File "/mnt/cfs1/asr/users/fanlu/miniconda3/lib/python3.7/site-packages/chainer/training/trainer.py", line 316, in run                                                                                              
    update()
  File "/mnt/cfs1/asr/users/fanlu/miniconda3/lib/python3.7/site-packages/chainer/training/updaters/standard_updater.py", line 175, in update                                                                         
    self.update_core()
  File "/mnt/cfs1/asr/users/fanlu/espnet/espnet/lm/pytorch_backend/lm.py", line 277, in update_core
    loss.backward()  # Backprop
  File "/mnt/cfs1/asr/users/fanlu/miniconda3/lib/python3.7/site-packages/torch/tensor.py", line 102, in backward                                                                                                     
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/mnt/cfs1/asr/users/fanlu/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py", line 90, in backward                                                                                           
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 19.62 MiB (GPU 2; 22.38 GiB total capacity; 21.01 GiB already allocated; 10.06 MiB free; 634.29 MiB cached)                                                      
# Accounting: time=746 threads=1
# Finished at Sun Aug 11 14:58:25 CST 2019 with status 1
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='fanlu' date='2019-08-11T08:21:32Z'>
		ensure your sentence length not too long, may be small than 128/256
		</comment>
		<comment id='2' author='fanlu' date='2019-08-11T12:57:38Z'>
		There is a same problem is egs tedlium3 too.And there was no problem in single gpu before.So Maybe It's a new bug in multi gpu recipe.
		</comment>
		<comment id='3' author='fanlu' date='2019-08-11T13:01:37Z'>
		Could you check whether it works with small batchsize?
In multi-gpu case, we automatically increase batchsize = batchsize * ngpu.
		</comment>
		<comment id='4' author='fanlu' date='2019-08-11T13:03:52Z'>
		Yes.I have changed the batchsize from 512 to 256/128/32. There are exists the same problem also.
		</comment>
		<comment id='5' author='fanlu' date='2019-08-11T13:04:50Z'>
		Thank you. There might be a bug.
We will investigate.
		</comment>
		<comment id='6' author='fanlu' date='2019-08-13T15:24:40Z'>
		I could reproduce this bug in TED-LIUM3. Because I tested multi GPU LM in AN4, I haven't met this problem. I suspect that this issue is related to chainer trainer because I just simply added torch.nn.DataParallel in the standard way. So I will create chainer-less lm_train.py for checking that.
		</comment>
		<comment id='7' author='fanlu' date='2019-08-13T15:47:33Z'>
		Maybe this is related to following part



espnet/espnet/lm/pytorch_backend/lm.py


        Lines 265 to 270
      in
      8fdd8e9






 for i in six.moves.range(sequence_length): 



 # Compute the loss at this time step and accumulate it 



 state, loss_batch = self.model(state, x[:, i], t[:, i]) 



 non_zeros = torch.sum(x[:, i] != 0, dtype=torch.float) 



 loss += loss_batch * non_zeros 



 count += int(non_zeros) 





The final loss calculation is conducted on only main gpu.
		</comment>
		<comment id='8' author='fanlu' date='2019-08-13T17:52:29Z'>
		Hmm, if so, this should be a pytorch bug. Maybe we can create the smallest example to reproduce this memory leak in multi GPUs to submit an issue to pytorch.
		</comment>
	</comments>
</bug>