<bug id='311' author='Fhrozen' open_date='2018-07-26T13:57:32Z' closed_time='2018-08-27T09:51:50Z'>
	<summary>MultiGPU error on WarpCTC</summary>
	<description>
I prepared a new docker container with the updated tools, and when I tried to run with multigpu, I got this error.
I am posting this here to know if someone else has a similar error with the new warpctc.
&lt;denchmark-code&gt;2018-07-26 13:51:18,146 (e2e_asr_attctc_th:99) INFO: mtl loss:417.586242676
Exception in main training loop: arguments are located on different GPUs at /pytorch/aten/src/THC/generated/../generic/THCTensorMathPointwise.cu:313
Traceback (most recent call last):
  File "/espnet/tools/venv/local/lib/python2.7/site-packages/chainer/training/trainer.py", line 306, in run
    update()
  File "/espnet/tools/venv/local/lib/python2.7/site-packages/chainer/training/updaters/standard_updater.py", line 149, in update
    self.update_core()
  File "/espnet/src/asr/asr_pytorch.py", line 131, in update_core
    loss.backward(torch.ones(self.num_gpu))  # Backprop
  File "/espnet/tools/venv/local/lib/python2.7/site-packages/torch/tensor.py", line 93, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/espnet/tools/venv/local/lib/python2.7/site-packages/torch/autograd/__init__.py", line 89, in backward
    allow_unreachable=True)  # allow_unreachable flag
  File "/espnet/tools/venv/local/lib/python2.7/site-packages/torch/autograd/function.py", line 76, in apply
    return self._forward_cls.backward(self, *args)
  File "build/bdist.linux-x86_64/egg/warpctc_pytorch/__init__.py", line 50, in backward
    return ctx.grads * grad_output.type_as(ctx.grads), None, None, None, None, None
Will finalize trainer extensions and updater before reraising the exception.
ESC[JTraceback (most recent call last):
  File "/espnet/egs/voxforge/asr1/../../../src/bin/asr_train.py", line 231, in &lt;module&gt;
    main()
  File "/espnet/egs/voxforge/asr1/../../../src/bin/asr_train.py", line 225, in main
    train(args)
  File "/espnet/src/asr/asr_pytorch.py", line 383, in train
    trainer.run()
  File "/espnet/tools/venv/local/lib/python2.7/site-packages/chainer/training/trainer.py", line 320, in run
    six.reraise(*sys.exc_info())
  File "/espnet/tools/venv/local/lib/python2.7/site-packages/chainer/training/trainer.py", line 306, in run
    update()
  File "/espnet/tools/venv/local/lib/python2.7/site-packages/chainer/training/updaters/standard_updater.py", line 149, in update
    self.update_core()
  File "/espnet/src/asr/asr_pytorch.py", line 131, in update_core
    loss.backward(torch.ones(self.num_gpu))  # Backprop
  File "/espnet/tools/venv/local/lib/python2.7/site-packages/torch/tensor.py", line 93, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/espnet/tools/venv/local/lib/python2.7/site-packages/torch/autograd/__init__.py", line 89, in backward
    allow_unreachable=True)  # allow_unreachable flag
  File "/espnet/tools/venv/local/lib/python2.7/site-packages/torch/autograd/function.py", line 76, in apply
    return self._forward_cls.backward(self, *args)
  File "build/bdist.linux-x86_64/egg/warpctc_pytorch/__init__.py", line 50, in backward

RuntimeError: arguments are located on different GPUs at /pytorch/aten/src/THC/generated/../generic/THCTensorMathPointwise.cu:313
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Fhrozen' date='2018-08-07T23:58:03Z'>
		&lt;denchmark-link:https://github.com/Fhrozen&gt;@Fhrozen&lt;/denchmark-link&gt;
 I met the same issues as you. I dig into it, it seems there was some python binding issues with warp-ctc. But I don't have time to modify that, so I rolled back to pytorch==0.3.1 other than using the latest pytorch 0.4, and also rolled back to Sean Naren's warp-CTC:
&lt;denchmark-code&gt;git clone https://github.com/SeanNaren/warp-ctc.git
git clone https://github.com/jnishi/warp-ctc.git
. venv/bin/activate; cd warp-ctc &amp;&amp; git checkout 9e5b238f8d9337b0c39b3fd01bbaff98ba523aa5 
&lt;/denchmark-code&gt;

Then I was good to go. Hope that could also work for you.
		</comment>
		<comment id='2' author='Fhrozen' date='2018-08-08T00:20:45Z'>
		This may not be a great solution as we are thinking of moving to pytorch 0.4.1+. See &lt;denchmark-link:https://github.com/espnet/espnet/pull/327&gt;#327&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='3' author='Fhrozen' date='2018-08-08T01:17:47Z'>
		I also met this when using pytorch-4.0. Maybe pytorch-3.0 will help
		</comment>
		<comment id='4' author='Fhrozen' date='2018-08-09T04:46:03Z'>
		I used the advice provided by &lt;denchmark-link:https://github.com/weiwchu&gt;@weiwchu&lt;/denchmark-link&gt;
, and solved the problem.
		</comment>
		<comment id='5' author='Fhrozen' date='2018-08-14T19:15:19Z'>
		&lt;denchmark-link:https://github.com/Fhrozen&gt;@Fhrozen&lt;/denchmark-link&gt;
 I also had similar issue (&lt;denchmark-link:https://github.com/espnet/espnet/issues/345&gt;#345&lt;/denchmark-link&gt;
). Can you please let me know if you have found any solution other than downgrading pytorch version?
		</comment>
		<comment id='6' author='Fhrozen' date='2018-08-17T00:57:16Z'>
		&lt;denchmark-link:https://github.com/Fhrozen&gt;@Fhrozen&lt;/denchmark-link&gt;
 I am having an exact similar issue. I was wondering if you have found a solution.
		</comment>
		<comment id='7' author='Fhrozen' date='2018-08-17T03:45:02Z'>
		&lt;denchmark-link:https://github.com/dneelagiri&gt;@dneelagiri&lt;/denchmark-link&gt;
  and &lt;denchmark-link:https://github.com/alirezadir&gt;@alirezadir&lt;/denchmark-link&gt;
 . Due to lack of time, I could not test any solution directly.  I use multigpu on chainer. but the solution given by &lt;denchmark-link:https://github.com/weiwchu&gt;@weiwchu&lt;/denchmark-link&gt;
 seems to be working. So it will be better if you rebuild the container modifying the required lines in /tools/Makefile
		</comment>
		<comment id='8' author='Fhrozen' date='2018-08-17T17:54:44Z'>
		Thanks &lt;denchmark-link:https://github.com/Fhrozen&gt;@Fhrozen&lt;/denchmark-link&gt;
, chainer worked for me :)
		</comment>
		<comment id='9' author='Fhrozen' date='2018-08-27T09:51:50Z'>
		Now this problem is fixed (&lt;denchmark-link:https://github.com/jnishi/warp-ctc/pull/2&gt;jnishi/warp-ctc#2&lt;/denchmark-link&gt;
).
Please re-setup warp-ctc using current master's Makefile.
		</comment>
	</comments>
</bug>