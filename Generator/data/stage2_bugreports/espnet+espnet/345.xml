<bug id='345' author='divyaneelagiri' open_date='2018-08-10T17:43:05Z' closed_time='2018-08-27T10:01:34Z'>
	<summary>Multi GPU error when using docker for training</summary>
	<description>
Hello,
I created a docker image and was able to train using that for single GPU. However when I try to use multi GPU for training I get below error.  Can you please let me know how I can fix this issue?
Traceback (most recent call last):
File "/espnet/tools/venv/local/lib/python2.7/site-packages/chainer/training/trainer.py", line 306, in run
update()
File "/espnet/tools/venv/local/lib/python2.7/site-packages/chainer/training/updaters/standard_updater.py", line 149, in update
self.update_core()
File "/espnet/src/asr/asr_pytorch.py", line 130, in update_core
loss.backward(torch.ones(self.num_gpu))  # Backprop
File "/espnet/tools/venv/local/lib/python2.7/site-packages/torch/tensor.py", line 93, in backward
torch.autograd.backward(self, gradient, retain_graph, create_graph)
File "/espnet/tools/venv/local/lib/python2.7/site-packages/torch/autograd/init.py", line 89, in backward
allow_unreachable=True)  # allow_unreachable flag
File "/espnet/tools/venv/local/lib/python2.7/site-packages/torch/autograd/function.py", line 76, in apply
return self._forward_cls.backward(self, *args)
File "build/bdist.linux-x86_64/egg/warpctc_pytorch/init.py", line 50, in backward
return ctx.grads * grad_output.type_as(ctx.grads), None, None, None, None, None
Will finalize trainer extensions and updater before reraising the exception.
^[[JTraceback (most recent call last):
File "/espnet/egs/an4/asr1/../../../src/bin/asr_train.py", line 224, in 
main()
File "/espnet/egs/an4/asr1/../../../src/bin/asr_train.py", line 218, in main
train(args)
File "/espnet/src/asr/asr_pytorch.py", line 383, in train
trainer.run()
File "/espnet/tools/venv/local/lib/python2.7/site-packages/chainer/training/trainer.py", line 320, in run
six.reraise(*sys.exc_info())
File "/espnet/tools/venv/local/lib/python2.7/site-packages/chainer/training/trainer.py", line 306, in run
update()
File "/espnet/tools/venv/local/lib/python2.7/site-packages/chainer/training/updaters/standard_updater.py", line 149, in update
self.update_core()
File "/espnet/src/asr/asr_pytorch.py", line 130, in update_core
loss.backward(torch.ones(self.num_gpu))  # Backprop
File "/espnet/tools/venv/local/lib/python2.7/site-packages/torch/tensor.py", line 93, in backward
torch.autograd.backward(self, gradient, retain_graph, create_graph)
File "/espnet/tools/venv/local/lib/python2.7/site-packages/torch/autograd/init.py", line 89, in backward
allow_unreachable=True)  # allow_unreachable flag
File "/espnet/tools/venv/local/lib/python2.7/site-packages/torch/autograd/function.py", line 76, in apply
return self._forward_cls.backward(self, *args)
allow_unreachable=True)  # allow_unreachable flag
File "/espnet/tools/venv/local/lib/python2.7/site-packages/torch/autograd/function.py", line 76, in apply
return self._forward_cls.backward(self, *args)
File "build/bdist.linux-x86_64/egg/warpctc_pytorch/init.py", line 50, in backward
RuntimeError: arguments are located on different GPUs at /pytorch/aten/src/THC/generated/../generic/THCTensorMathPointwise.cu:313
Thanks,
Divya
	</description>
	<comments>
		<comment id='1' author='divyaneelagiri' date='2018-08-17T03:39:53Z'>
		About docker, there is a new image that has been implemented for the new release of ESPnet and can be found at &lt;denchmark-link:https://github.com/espnet/espnet/pull/346&gt;#346&lt;/denchmark-link&gt;
, I tested it with multigpu on Chainer. However, if you are training with pytorch there is an active thread with the current problem &lt;denchmark-link:https://github.com/espnet/espnet/issues/311&gt;#311&lt;/denchmark-link&gt;
. Still working on the problem.
		</comment>
		<comment id='2' author='divyaneelagiri' date='2018-08-17T17:54:05Z'>
		Thanks &lt;denchmark-link:https://github.com/Fhrozen&gt;@Fhrozen&lt;/denchmark-link&gt;
 I was able to run Multi-GPU using chainer on my own docker.
		</comment>
		<comment id='3' author='divyaneelagiri' date='2018-08-27T10:01:33Z'>
		Duplicated (&lt;denchmark-link:https://github.com/espnet/espnet/issues/311&gt;#311&lt;/denchmark-link&gt;
).
		</comment>
	</comments>
</bug>