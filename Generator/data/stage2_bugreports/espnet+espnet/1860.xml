<bug id='1860' author='shanguanma' open_date='2020-04-22T10:37:15Z' closed_time='2020-04-25T07:19:13Z'>
	<summary>Training transducer model encounter an illegal memory access</summary>
	<description>
When I test transformer-transducer of espnet,
At training stage, it encounter an illegal memory access in pytorch 1.1.0. or pytorch 1.4.0.
&lt;denchmark-h:h1&gt;error log is as follows:&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;Exception in main training loop: CUDA error: an illegal memory access was encountered
Traceback (most recent call last):
  File "/home4/md510/w2020/espnet_20200422/espnet/tools/venv/lib/python3.7/site-packages/chainer/training/trainer.py", line 316, in run
    update()
  File "/home4/md510/w2020/espnet_20200422/espnet/espnet/asr/pytorch_backend/asr.py", line 241, in update
    self.update_core()
  File "/home4/md510/w2020/espnet_20200422/espnet/espnet/asr/pytorch_backend/asr.py", line 205, in update_core
    data_parallel(self.model, x, range(self.ngpu)).mean() / self.accum_grad
  File "/home4/md510/w2020/espnet_20200422/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 204, in data_parallel
    return module(*inputs[0], **module_kwargs[0])
  File "/home4/md510/w2020/espnet_20200422/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home4/md510/w2020/espnet_20200422/espnet/espnet/nets/pytorch_backend/e2e_asr_transducer.py", line 413, in forward
    loss = self.criterion(pred_pad, target, pred_len, target_len)
  File "/home4/md510/w2020/espnet_20200422/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home4/md510/w2020/espnet_20200422/espnet/espnet/nets/pytorch_backend/transducer/loss.py", line 44, in forward
    loss = self.trans_loss(pred_pad, target, pred_len, target_len)
  File "/home4/md510/w2020/espnet_20200422/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home4/md510/w2020/espnet_20200422/espnet/tools/venv/lib/python3.7/site-packages/warprnnt_pytorch-0.1-py3.7-linux-x86_64.egg/warprnnt_pytorch/__init__.py", line 100, in forward
    return self.loss(acts, labels, act_lens, label_lens, self.blank, self.reduction)
  File "/home4/md510/w2020/espnet_20200422/espnet/tools/venv/lib/python3.7/site-packages/warprnnt_pytorch-0.1-py3.7-linux-x86_64.egg/warprnnt_pytorch/__init__.py", line 40, in forward
    grads /= minibatch_size
Will finalize trainer extensions and updater before reraising the exception.
Traceback (most recent call last):
  File "/home4/md510/w2020/espnet_20200422/espnet/espnet/bin/asr_train.py", line 622, in &lt;module&gt;
    main(sys.argv[1:])
  File "/home4/md510/w2020/espnet_20200422/espnet/espnet/bin/asr_train.py", line 608, in main
    train(args)
  File "/home4/md510/w2020/espnet_20200422/espnet/espnet/asr/pytorch_backend/asr.py", line 813, in train
    trainer.run()
  File "/home4/md510/w2020/espnet_20200422/espnet/tools/venv/lib/python3.7/site-packages/chainer/training/trainer.py", line 349, in run
    six.reraise(*exc_info)
  File "/home4/md510/w2020/espnet_20200422/espnet/tools/venv/lib/python3.7/site-packages/six.py", line 703, in reraise
    raise value
  File "/home4/md510/w2020/espnet_20200422/espnet/tools/venv/lib/python3.7/site-packages/chainer/training/trainer.py", line 316, in run
    update()
  File "/home4/md510/w2020/espnet_20200422/espnet/espnet/asr/pytorch_backend/asr.py", line 241, in update
    self.update_core()
  File "/home4/md510/w2020/espnet_20200422/espnet/espnet/asr/pytorch_backend/asr.py", line 205, in update_core
    data_parallel(self.model, x, range(self.ngpu)).mean() / self.accum_grad
  File "/home4/md510/w2020/espnet_20200422/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 204, in data_parallel
    return module(*inputs[0], **module_kwargs[0])
  File "/home4/md510/w2020/espnet_20200422/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home4/md510/w2020/espnet_20200422/espnet/espnet/nets/pytorch_backend/e2e_asr_transducer.py", line 413, in forward
    loss = self.criterion(pred_pad, target, pred_len, target_len)
  File "/home4/md510/w2020/espnet_20200422/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home4/md510/w2020/espnet_20200422/espnet/espnet/nets/pytorch_backend/transducer/loss.py", line 44, in forward
    loss = self.trans_loss(pred_pad, target, pred_len, target_len)
  File "/home4/md510/w2020/espnet_20200422/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home4/md510/w2020/espnet_20200422/espnet/tools/venv/lib/python3.7/site-packages/warprnnt_pytorch-0.1-py3.7-linux-x86_64.egg/warprnnt_pytorch/__init__.py", line 100, in forward
    return self.loss(acts, labels, act_lens, label_lens, self.blank, self.reduction)
  File "/home4/md510/w2020/espnet_20200422/espnet/tools/venv/lib/python3.7/site-packages/warprnnt_pytorch-0.1-py3.7-linux-x86_64.egg/warprnnt_pytorch/__init__.py", line 40, in forward
    grads /= minibatch_size
RuntimeError: CUDA error: an illegal memory access was encountered
# Ended (code 256) at Wed Apr 22 18:22:55 SGT 2020, elapsed time 2169 seconds
&lt;/denchmark-code&gt;

&lt;denchmark-h:h1&gt;transformer-transducer config is as follows:&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;batch-size: 16  # defailt 30
maxlen-in: 512  # default 800
maxlen-out: 150

# optimization related
criterion: loss
early-stop-criterion: "validation/main/loss"
sortagrad: 0
opt: noam
epochs: 30
patience: 0
accum-grad: 4
grad-clip: 5.0

# transformer related
transformer-lr: 10.0
transformer-warmup-steps: 8000
transformer-attn-dropout-rate-encoder: 0.4
transformer-attn-dropout-rate-decoder: 0.1

# network architecture
## encoder related
etype: transformer
transformer-input-layer: vgg2l
elayers: 8
eunits: 320
dropout-rate: 0.4
## decoder related
dtype: transformer
dlayers: 2
dec-embed-dim: 300
dunits: 300
dropout-rate-decoder: 0.1
## attention related
adim: 320
aheads: 4
## joint network related
joint-dim: 300

# transducer related
mtlalpha: 1.0 # mtlalpha should be set to 1.0 (CTC) to use transducer
rnnt-mode: 'rnnt' # switch to 'rnnt-att' to use transducer with attention
model-module: "espnet.nets.pytorch_backend.e2e_asr_transducer:E2E"

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='shanguanma' date='2020-04-22T18:40:43Z'>
		Hi , try using transformer-input-layer =conv2d as what &lt;denchmark-link:https://github.com/b-flo&gt;@b-flo&lt;/denchmark-link&gt;
 suggested before and with small architecture it works fine.
		</comment>
		<comment id='2' author='shanguanma' date='2020-04-22T19:37:45Z'>
		Hi,
Thanks for the ping &lt;denchmark-link:https://github.com/ahmedalbahnasawy&gt;@ahmedalbahnasawy&lt;/denchmark-link&gt;
, the issue is in within warp-transducer in that case. I encountered this bug in the past but let it slide as I didn't met it again, I thought Mingkun fixed it. I'll take a look tomorrow and propose a fix, Sorry about that &lt;denchmark-link:https://github.com/shanguanma&gt;@shanguanma&lt;/denchmark-link&gt;
!
		</comment>
		<comment id='3' author='shanguanma' date='2020-04-23T01:14:39Z'>
		Thanks &lt;denchmark-link:https://github.com/sw005320&gt;@sw005320&lt;/denchmark-link&gt;
 ,&lt;denchmark-link:https://github.com/ahmedalbahnasawy&gt;@ahmedalbahnasawy&lt;/denchmark-link&gt;
  and &lt;denchmark-link:https://github.com/b-flo&gt;@b-flo&lt;/denchmark-link&gt;
, thanks for your reply, it doesn't matter.
		</comment>
		<comment id='4' author='shanguanma' date='2020-04-23T08:28:36Z'>
		Nice timing I should say, a fix on warp-transducer side was proposed for this issue yesterday. Could you re-build and try again please?
&lt;denchmark-code&gt;cd espnet/tools
rm -rf warp-transducer warp-transducer.done
make warp-transducer.done
&lt;/denchmark-code&gt;

I'll further investigate if it's not working.
		</comment>
		<comment id='5' author='shanguanma' date='2020-04-25T02:39:57Z'>
		&lt;denchmark-link:https://github.com/b-flo&gt;@b-flo&lt;/denchmark-link&gt;
 ,Thanks for your help, After testing, it is work now.
		</comment>
		<comment id='6' author='shanguanma' date='2020-06-29T06:32:02Z'>
		
Nice timing I should say, a fix on warp-transducer side was proposed for this issue yesterday. Could you re-build and try again please?
cd espnet/tools
rm -rf warp-transducer warp-transducer.done
make warp-transducer.done

I'll further investigate if it's not working.

Hello, I install espnet and warp_transducer on May 8.
The version is "commit &lt;denchmark-link:https://github.com/espnet/espnet/commit/97bb94a850743551289e8bb3921c1adef5da7b7e&gt;97bb94a&lt;/denchmark-link&gt;

Merge: &lt;denchmark-link:https://github.com/espnet/espnet/commit/075cee8d586957241be3e54c47846fbb12a32310&gt;075cee8&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/espnet/espnet/commit/f69512fcd52e19a59342c5b81b3afd874b7b6192&gt;f69512f&lt;/denchmark-link&gt;

Author: mergify[bot] &lt;37929162+mergify[bot]@users.noreply.github.com&gt;
Date:   Wed May 6 10:04:21 2020 +0000 "
I still got the promblem of illegal memory access after several iterations.
&lt;denchmark-code&gt;2020-06-29 11:58:52,876 (e2e_asr_transducer:37) INFO: loss:1350.66650390625
2020-06-29 11:58:52,876 (e2e_asr_transducer:37) INFO: loss:1450.1688232421875
2020-06-29 11:58:52,876 (e2e_asr_transducer:37) INFO: loss:1532.4638671875
2020-06-29 11:58:53,704 (e2e_asr_transducer:37) INFO: loss:1130.2525634765625
2020-06-29 11:58:53,721 (e2e_asr_transducer:37) INFO: loss:1120.7750244140625
2020-06-29 11:58:53,743 (e2e_asr_transducer:37) INFO: loss:1173.013916015625
2020-06-29 11:58:53,744 (e2e_asr_transducer:37) INFO: loss:1280.8448486328125
2020-06-29 11:58:53,774 (e2e_asr_transducer:37) INFO: loss:1221.851806640625
2020-06-29 11:58:53,775 (e2e_asr_transducer:37) INFO: loss:1502.5286865234375
2020-06-29 11:58:53,806 (e2e_asr_transducer:37) INFO: loss:1348.6053466796875
2020-06-29 11:58:53,819 (e2e_asr_transducer:37) INFO: loss:1443.14453125
2020-06-29 11:58:54,598 (e2e_asr_transducer:37) INFO: loss:1166.4251708984375
THCudaCheck FAIL file=/pytorch/aten/src/THC/THCReduceAll.cuh line=327 error=77 : an illegal memory access was encountered
THCudaCheck FAIL file=/pytorch/aten/src/THC/THCReduceAll.cuh line=327 error=77 : an illegal memory access was encountered
THCudaCheck FAIL file=/pytorch/aten/src/THC/THCReduceAll.cuh line=327 error=77 : an illegal memory access was encountered
Exception in main training loop: Caught RuntimeError in replica 1 on device 1.
Original Traceback (most recent call last):
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 60, in _worker
    output = module(*input, **kwargs)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/username/tools/espnet/espnet/nets/pytorch_backend/e2e_asr_transducer.py", line 413, in forward
    loss = self.criterion(pred_pad, target, pred_len, target_len)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/username/tools/espnet/espnet/nets/pytorch_backend/transducer/loss.py", line 44, in forward
    loss = self.trans_loss(pred_pad, target, pred_len, target_len)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/warprnnt_pytorch-0.1-py3.6-linux-x86_64.egg/warprnnt_pytorch/__init__.py", line 100, in forward
    return self.loss(acts, labels, act_lens, label_lens, self.blank, self.reduction)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/warprnnt_pytorch-0.1-py3.6-linux-x86_64.egg/warprnnt_pytorch/__init__.py", line 21, in forward
    certify_inputs(acts, labels, act_lens, label_lens)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/warprnnt_pytorch-0.1-py3.6-linux-x86_64.egg/warprnnt_pytorch/__init__.py", line 135, in certify_inputs
    max_U = torch.max(label_lengths)
RuntimeError: cuda runtime error (77) : an illegal memory access was encountered at /pytorch/aten/src/THC/THCReduceAll.cuh:327

Traceback (most recent call last):
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/chainer/training/trainer.py", line 316, in run
    update()
  File "/home/username/tools/espnet/espnet/asr/pytorch_backend/asr.py", line 250, in update
    self.update_core()
  File "/home/username/tools/espnet/espnet/asr/pytorch_backend/asr.py", line 214, in update_core
    data_parallel(self.model, x, self.gpu_list).mean() / self.accum_grad
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 207, in data_parallel
    outputs = parallel_apply(replicas, inputs, module_kwargs, used_device_ids)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 85, in parallel_apply
    output.reraise()
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/_utils.py", line 394, in reraise
    raise self.exc_type(msg)
Will finalize trainer extensions and updater before reraising the exception.
Traceback (most recent call last):
  File "/home/username/tools/espnet/egs/librispeech/asr1/../../../espnet/bin/asr_train.py", line 625, in &lt;module&gt;
    main(sys.argv[1:])
  File "/home/username/tools/espnet/egs/librispeech/asr1/../../../espnet/bin/asr_train.py", line 611, in main
    train(args)
  File "/home/username/tools/espnet/espnet/asr/pytorch_backend/asr.py", line 859, in train
    trainer.run()
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/chainer/training/trainer.py", line 349, in run
    six.reraise(*exc_info)
  File "/home/username/tools/kaldi/tools/sequitur-g2p/lib64/python2.7/site-packages/six-1.11.0-py2.7.egg/six.py", line 693, in reraise
    raise value
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/chainer/training/trainer.py", line 316, in run
    update()
  File "/home/username/tools/espnet/espnet/asr/pytorch_backend/asr.py", line 250, in update
    self.update_core()
  File "/home/username/tools/espnet/espnet/asr/pytorch_backend/asr.py", line 214, in update_core
    data_parallel(self.model, x, self.gpu_list).mean() / self.accum_grad
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 207, in data_parallel
    outputs = parallel_apply(replicas, inputs, module_kwargs, used_device_ids)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 85, in parallel_apply
    output.reraise()
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/_utils.py", line 394, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 1 on device 1.
Original Traceback (most recent call last):
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 60, in _worker
    output = module(*input, **kwargs)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/username/tools/espnet/espnet/nets/pytorch_backend/e2e_asr_transducer.py", line 413, in forward
    loss = self.criterion(pred_pad, target, pred_len, target_len)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/username/tools/espnet/espnet/nets/pytorch_backend/transducer/loss.py", line 44, in forward
    loss = self.trans_loss(pred_pad, target, pred_len, target_len)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/warprnnt_pytorch-0.1-py3.6-linux-x86_64.egg/warprnnt_pytorch/__init__.py", line 100, in forward
    return self.loss(acts, labels, act_lens, label_lens, self.blank, self.reduction)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/warprnnt_pytorch-0.1-py3.6-linux-x86_64.egg/warprnnt_pytorch/__init__.py", line 21, in forward
    certify_inputs(acts, labels, act_lens, label_lens)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/warprnnt_pytorch-0.1-py3.6-linux-x86_64.egg/warprnnt_pytorch/__init__.py", line 135, in certify_inputs
    max_U = torch.max(label_lengths)
RuntimeError: cuda runtime error (77) : an illegal memory access was encountered at /pytorch/aten/src/THC/THCReduceAll.cuh:327

# Accounting: time=2726 threads=1
# Finished at Mon Jun 29 11:59:04 CST 2020 with status 1
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='shanguanma' date='2020-06-29T07:29:06Z'>
		Hi,
Could you give me additional informations such as dataset used, training config and scripts, numbers of GPUs, system informations, commit version of warp-transducer (in tools/warp-transducer), etc?
I'm extensively working on transducer right now and I did not met any illegal memory so far. In your case, the error seems to different from the previous one we discussed here.
		</comment>
		<comment id='8' author='shanguanma' date='2020-06-29T08:12:35Z'>
		
Hi,
Could you give me additional informations such as dataset used, training config and scripts, numbers of GPUs, system informations, commit version of warp-transducer (in tools/warp-transducer), etc?
I'm extensively working on transducer right now and I didn't not met any illegal memory so far. In your case, the error seems to different from the previous one we discussed here.

Hello , here are some information:
Dataset: librispeech
run.sh
espnet commit version:
commit &lt;denchmark-link:https://github.com/espnet/espnet/commit/97bb94a850743551289e8bb3921c1adef5da7b7e&gt;97bb94a&lt;/denchmark-link&gt;

Merge: &lt;denchmark-link:https://github.com/espnet/espnet/commit/075cee8d586957241be3e54c47846fbb12a32310&gt;075cee8&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/espnet/espnet/commit/f69512fcd52e19a59342c5b81b3afd874b7b6192&gt;f69512f&lt;/denchmark-link&gt;

Author: mergify[bot] &lt;37929162+mergify[bot]@users.noreply.github.com&gt;
Date:   Wed May 6 10:04:21 2020 +0000
warp-commit version:
commit f546575109111c455354861a0567c8aa794208a2
Merge: c1a265f 5098002
Author: Mingkun Huang &lt;denchmark-link:mailto:mingkunhuang95@gmail.com&gt;mingkunhuang95@gmail.com&lt;/denchmark-link&gt;

Date:   Mon Apr 27 23:07:35 2020 +0800
numGpu is 8 and train.conf:
&lt;denchmark-code&gt;# minibatch related
batch-size: 10 
maxlen-in: 800
maxlen-out: 150

# optimization related
criterion: loss
early-stop-criterion: "validation/main/loss"
sortagrad: 0
opt: adam 
epochs: 60 
patience: 0
accum-grad: 8 
grad-clip: 5.0

# transformer related
transformer-lr: 1 
transformer-warmup-steps: 4000
transformer-attn-dropout-rate-encoder: 0.2
transformer-attn-dropout-rate-decoder: 0.1

# network architecture
## encoder related
etype: transformer
transformer-input-layer: vgg2l
elayers: 8
eunits: 320
dropout-rate: 0.2
## decoder related
dtype: lstm
dlayers: 2
dec-embed-dim: 300
dunits: 300
dropout-rate-decoder: 0.2
dropout-rate-embed-decoder: 0.1
## attention related
adim: 320
aheads: 4
## joint network related
joint-dim: 300

# transducer related
mtlalpha: 1.0 # mtlalpha should be set to 1.0 (CTC) to use transducer
rnnt-mode: 'rnnt' # switch to 'rnnt-att' to use transducer with attention
model-module: "espnet.nets.pytorch_backend.e2e_asr_transducer:E2E"
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='shanguanma' date='2020-06-29T08:19:34Z'>
		OK thanks, could you give me some informations about your espnet setup please?
Also, just to be sure could you check if the problem exists with ngpu=2 please? I don't have access to more than 2 GPUs right now and I want to reproduce exactly your test environment.
		</comment>
		<comment id='10' author='shanguanma' date='2020-06-29T08:23:39Z'>
		
Hi,
Could you give me additional informations such as dataset used, training config and scripts, numbers of GPUs, system informations, commit version of warp-transducer (in tools/warp-transducer), etc?
I'm extensively working on transducer right now and I didn't not met any illegal memory so far. In your case, the error seems to different from the previous one we discussed here.

I try many other options , most of them failed. Some of them worked out
&lt;denchmark-link:https://user-images.githubusercontent.com/9246556/85990216-45bfad00-ba24-11ea-8665-d7bc6adfe4a2.png&gt;&lt;/denchmark-link&gt;

It is very confusing.
The location of the collapse  is not always the same, but all of the failure is "an illegal memory access was encountered"
Another failure case is like this
&lt;denchmark-code&gt;j1-asr-train-v100-02:489125:489125 [7] NCCL INFO Ring 00 : 7[7] -&gt; 0[0] via direct shared memory
tj1-asr-train-v100-02:489125:489125 [0] NCCL INFO Launch mode Group/CGMD
Exception in main training loop: Caught RuntimeError in replica 1 on device 1.
Original Traceback (most recent call last):
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 60, in _worker
    output = module(*input, **kwargs)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/username/tools/espnet/espnet/nets/pytorch_backend/e2e_asr_transducer.py", line 413, in forward
    loss = self.criterion(pred_pad, target, pred_len, target_len)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/username/tools/espnet/espnet/nets/pytorch_backend/transducer/loss.py", line 44, in forward
    loss = self.trans_loss(pred_pad, target, pred_len, target_len)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/warprnnt_pytorch-0.1-py3.6-linux-x86_64.egg/warprnnt_pytorch/__init__.py", line 100, in forward
    return self.loss(acts, labels, act_lens, label_lens, self.blank, self.reduction)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/warprnnt_pytorch-0.1-py3.6-linux-x86_64.egg/warprnnt_pytorch/__init__.py", line 40, in forward
    grads /= minibatch_size
RuntimeError: CUDA error: an illegal memory access was encountered

Traceback (most recent call last):
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/chainer/training/trainer.py", line 316, in run
    update()
  File "/home/username/tools/espnet/espnet/asr/pytorch_backend/asr.py", line 250, in update
    self.update_core()
  File "/home/username/tools/espnet/espnet/asr/pytorch_backend/asr.py", line 214, in update_core
    data_parallel(self.model, x, self.gpu_list).mean() / self.accum_grad
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 207, in data_parallel
    outputs = parallel_apply(replicas, inputs, module_kwargs, used_device_ids)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 85, in parallel_apply
    output.reraise()
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/_utils.py", line 394, in reraise
    raise self.exc_type(msg)
Will finalize trainer extensions and updater before reraising the exception.
Traceback (most recent call last):
  File "/home/username/tools/espnet/egs/librispeech/asr1/../../../espnet/bin/asr_train.py", line 625, in &lt;module&gt;
    main(sys.argv[1:])
  File "/home/username/tools/espnet/egs/librispeech/asr1/../../../espnet/bin/asr_train.py", line 611, in main
    train(args)
  File "/home/username/tools/espnet/espnet/asr/pytorch_backend/asr.py", line 856, in train
    trainer.run()
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/chainer/training/trainer.py", line 349, in run
    six.reraise(*exc_info)
  File "/home/username/tools/kaldi/tools/sequitur-g2p/lib64/python2.7/site-packages/six-1.11.0-py2.7.egg/six.py", line 693, in reraise
    raise value
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/chainer/training/trainer.py", line 316, in run
    update()
  File "/home/username/tools/espnet/espnet/asr/pytorch_backend/asr.py", line 250, in update
    self.update_core()
  File "/home/username/tools/espnet/espnet/asr/pytorch_backend/asr.py", line 214, in update_core
    data_parallel(self.model, x, self.gpu_list).mean() / self.accum_grad
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 207, in data_parallel
    outputs = parallel_apply(replicas, inputs, module_kwargs, used_device_ids)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 85, in parallel_apply
    output.reraise()
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/_utils.py", line 394, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 1 on device 1.
Original Traceback (most recent call last):
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 60, in _worker
    output = module(*input, **kwargs)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/username/tools/espnet/espnet/nets/pytorch_backend/e2e_asr_transducer.py", line 413, in forward
    loss = self.criterion(pred_pad, target, pred_len, target_len)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/username/tools/espnet/espnet/nets/pytorch_backend/transducer/loss.py", line 44, in forward
    loss = self.trans_loss(pred_pad, target, pred_len, target_len)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/warprnnt_pytorch-0.1-py3.6-linux-x86_64.egg/warprnnt_pytorch/__init__.py", line 100, in forward
    return self.loss(acts, labels, act_lens, label_lens, self.blank, self.reduction)
  File "/home/username/tools/anaconda3/envs/py36/lib/python3.6/site-packages/warprnnt_pytorch-0.1-py3.6-linux-x86_64.egg/warprnnt_pytorch/__init__.py", line 40, in forward
    grads /= minibatch_size
RuntimeError: CUDA error: an illegal memory access was encountered

# Accounting: time=137 threads=1
# Finished at Wed Jun 17 17:51:09 CST 2020 with status 1
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='shanguanma' date='2020-06-29T08:26:04Z'>
		
OK thanks, could you give me some informations about your espnet setup please?
Also, just to be sure could you check if the problem exists with ngpu=2 please? I don't have access to more than 2 GPUs right now and I want to reproduce exactly your test environment.

OK ， I will try 2 gpu. Actually sometimes 4 or 8 GPU can work.
python: 3.6.1
pytorch : 1.4.0+cu100
		</comment>
		<comment id='12' author='shanguanma' date='2020-06-29T11:01:47Z'>
		I believe this is a bug of warp transducer with DataParallel, and not problem of espnet. You should report it as a warp transducer issue with minimum reproducing code, could you?
In general we can't always reproduce such errors in multi threading because the threads run in parallel and the timing of accessing depends on the environment. This is the reason sometimes happens and sometimes doesn't happens. It's hard to investigate the condition.
DistributedDataParallel may solve this problem as a workaround, but espnet1 doesn't support it.
		</comment>
		<comment id='13' author='shanguanma' date='2020-07-03T09:50:58Z'>
		
I believe this is a bug of warp transducer with DataParallel, and not problem of espnet. You should report it as a warp transducer issue with minimum reproducing code, could you?
In general we can't always reproduce such errors in multi threading because the threads run in parallel and the timing of accessing depends on the environment. This is the reason sometimes happens and sometimes doesn't happens. It's hard to investigate the condition.
DistributedDataParallel may solve this problem as a workaround, but espnet1 doesn't support it.

Ok . Thank you . I have reported the problem in warp transducer.
By the way,  Is accum-grad related to num of Gpus or batch size? How to set this parameter ?
		</comment>
	</comments>
</bug>