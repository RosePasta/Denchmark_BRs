<bug id='310' author='kan-bayashi' open_date='2018-07-26T13:06:33Z' closed_time='2019-06-24T16:03:25Z'>
	<summary>Memory leaking in tts_pytorch</summary>
	<description>
I found that tts_pytorch has memory leaking problem.
But I have not yet found the reason.
	</description>
	<comments>
		<comment id='1' author='kan-bayashi' date='2018-09-12T16:36:45Z'>
		&lt;denchmark-link:https://github.com/kan-bayashi&gt;@kan-bayashi&lt;/denchmark-link&gt;
 Do you still have this issue? Maybe I can help now because I'm trying some TTS recipes recently.
		</comment>
		<comment id='2' author='kan-bayashi' date='2018-09-13T01:49:41Z'>
		Thank you &lt;denchmark-link:https://github.com/ShigekiKarita&gt;@ShigekiKarita&lt;/denchmark-link&gt;
. I need your help.
Yes, tts recipe still has memory leaking issue.
Iâ€™ve tried several methods:

Use gc.colloct() at the end of update_core
removal of batch variable  at the end of update_core

But there is no effect, memory is still leaking...
		</comment>
		<comment id='3' author='kan-bayashi' date='2018-09-25T13:10:00Z'>
		&lt;denchmark-link:https://github.com/ShigekiKarita&gt;@ShigekiKarita&lt;/denchmark-link&gt;
 Did you get any idea?
		</comment>
		<comment id='4' author='kan-bayashi' date='2018-09-26T02:07:15Z'>
		Thanks for ping. I have done several experiments through this month with

egs: librispeech/tts1, ljspeech/tts1
pytorch: 0.4.1
python: 3.6

Strangely, I have never seen MemoryError. If you might use Python2.7 (I remember), please try Python 3 instead.
		</comment>
		<comment id='5' author='kan-bayashi' date='2018-09-26T03:09:19Z'>
		Thanks &lt;denchmark-link:https://github.com/ShigekiKarita&gt;@ShigekiKarita&lt;/denchmark-link&gt;
.
I will try to use python3.
		</comment>
		<comment id='6' author='kan-bayashi' date='2018-09-26T05:46:17Z'>
		And also I found a strange line here



espnet/src/tts/tts_pytorch.py


        Lines 260 to 266
      in
      f417aee






 train_iter = chainer.iterators.MultiprocessIterator( 



 TransformDataset(train_batchset, converter.transform), 



 batch_size=1, n_processes=2, n_prefetch=8, maxtasksperchild=20) 



 valid_iter = chainer.iterators.MultiprocessIterator( 



 TransformDataset(valid_batchset, converter.transform), 



 batch_size=1, repeat=False, shuffle=False, n_processes=2, n_prefetch=8, 



 maxtasksperchild=20) 





I do not believe that chainer.iterators.MultiprocessIterator has maxtasksperchild in chainer 4.4.0? I'm using this without maxtasksperchild instead. What chainer.__version__ are you using?
&lt;denchmark-link:http://docs.chainer.org/en/stable/reference/generated/chainer.iterators.MultiprocessIterator.html&gt;http://docs.chainer.org/en/stable/reference/generated/chainer.iterators.MultiprocessIterator.html&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='kan-bayashi' date='2018-09-26T05:48:19Z'>
		Oh this one &lt;denchmark-link:https://github.com/chainer/chainer/pull/4972&gt;chainer/chainer#4972&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='8' author='kan-bayashi' date='2018-09-26T12:20:27Z'>
		Yes, maybe that's one.
Now we use &lt;denchmark-link:https://github.com/jnishi&gt;@jnishi&lt;/denchmark-link&gt;
 patched chainer.
See &lt;denchmark-link:https://github.com/espnet/espnet/blob/master/tools/prefetch.patch&gt;https://github.com/espnet/espnet/blob/master/tools/prefetch.patch&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>