<bug id='1291' author='kan-bayashi' open_date='2019-10-18T11:13:04Z' closed_time='2019-11-15T00:21:33Z'>
	<summary>Attention weight of Transformer TTS is wrong in inference the cache</summary>
	<description>
We need to calculate the attention weight incrementally.



espnet/espnet/nets/pytorch_backend/e2e_tts_transformer.py


        Lines 692 to 697
      in
      03724e6






 # get attention weights 



 att_ws = [] 



 for name, m in self.named_modules(): 



 if isinstance(m, MultiHeadedAttention) and "src" in name: 



 att_ws += [m.attn] 



 att_ws = torch.cat(att_ws, dim=0) 





	</description>
	<comments>
		<comment id='1' author='kan-bayashi' date='2019-11-15T00:21:33Z'>
		Fixed in &lt;denchmark-link:https://github.com/espnet/espnet/pull/1331&gt;#1331&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>