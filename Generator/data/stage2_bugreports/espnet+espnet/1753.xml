<bug id='1753' author='fly12357' open_date='2020-03-31T10:38:24Z' closed_time='2020-04-14T03:50:36Z'>
	<summary>Some questions about the TTS attention alignment</summary>
	<description>
I trained the Tactron2 based on my own dataset; First, I use the default parameters in the CSMSC recipe. The alignment is:
&lt;denchmark-link:https://user-images.githubusercontent.com/40494145/78016906-0bd7ff80-737e-11ea-8cf2-355fd5036e2d.png&gt;&lt;/denchmark-link&gt;

Then I do the trim and set the reduction factor to 3, follows the steps mentioned in &lt;denchmark-link:https://github.com/espnet/espnet/issues/1360&gt;#1360&lt;/denchmark-link&gt;
 .
Then the alignment change to:
&lt;denchmark-link:https://user-images.githubusercontent.com/40494145/78017127-6bcea600-737e-11ea-8fa1-6ded6698dc9d.png&gt;&lt;/denchmark-link&gt;

I have no idea about the reason why the decoder is much longer than the encoder. I mean, why it is not a Diagonal?
The original wave is :
&lt;denchmark-link:https://user-images.githubusercontent.com/40494145/78017374-c536d500-737e-11ea-993e-01aec0da1274.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/40494145/78017382-c9fb8900-737e-11ea-9a49-65cd804d4b15.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='fly12357' date='2020-03-31T13:36:01Z'>
		Sorry, this is a bug of the attention visualization.
Maybe the attention itself is diagonal.
		</comment>
		<comment id='2' author='fly12357' date='2020-04-03T09:19:22Z'>
		
Sorry, this is a bug of the attention visualization.
Maybe the attention itself is diagonal.

Thanks for your reply. I'll check the bug of the attention visualization.
BTW, Is there any length limitation of the input tokens in Tactron2 model?
I meet the following alignment in my training process (21th epoch):
&lt;denchmark-link:https://user-images.githubusercontent.com/40494145/78344869-94011380-75cf-11ea-89e7-caf889d928bd.png&gt;&lt;/denchmark-link&gt;

I suspect it might be the length limitation or maybe it  just need more iterations to get better result.
Hope you can give me any advice!
		</comment>
		<comment id='3' author='fly12357' date='2020-04-03T13:19:17Z'>
		I think this is also related to the bug of attention plot.
I made patch to fix the plot &lt;denchmark-link:https://github.com/espnet/espnet/pull/1761&gt;#1761&lt;/denchmark-link&gt;
.
Please check it.
		</comment>
		<comment id='4' author='fly12357' date='2020-04-03T13:24:28Z'>
		The patch #1761 does not fix the issue of reduction_factor.
We need to crop the decoder index according to the reduction factor but currently it is not performed.
I will make another PR to fix it.
Update:
I also fixed the issue when reduction_factor &gt; 1 in &lt;denchmark-link:https://github.com/espnet/espnet/pull/1761&gt;#1761&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='5' author='fly12357' date='2020-04-04T06:34:46Z'>
		
The patch #1761 does not fix the issue of reduction_factor.
We need to crop the decoder index according to the reduction factor but currently it is not performed.
I will make another PR to fix it.
Update:
I also fixed the issue when reduction_factor &gt; 1 in #1761.

Thanks for your reply. I'll update my code and check the result.
		</comment>
	</comments>
</bug>