<bug id='2653' author='AdolfVonKleist' open_date='2020-11-08T15:16:41Z' closed_time='2020-11-09T07:20:32Z'>
	<summary>pytorch1.7 issue</summary>
	<description>
This is more of a 'note' than a bug, as I don't think what I am doing is currently supported, nor that it is actually a bug in espnet, but perhaps someone else will run into the same issues at some point, so I thought it could be helpful to share.
I have been setting up a fully local cuda 10.2 variant of the Dockerfile today, combined with pytorch 1.7.0.
It is now working [it's probably a bit hacky but I am happy to share it if there is interest], but when I tried to run the an4 eg2 test recipe I ran into another glitch.
This is the traceback from run.sh:
&lt;denchmark-code&gt;  File "/miniconda/envs/espnet/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/miniconda/envs/espnet/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/espnet/espnet2/bin/asr_train.py", line 23, in &lt;module&gt;
    main()
  File "/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/espnet/espnet2/tasks/abs_task.py", line 941, in main
    cls.main_worker(args)
  File "/espnet/espnet2/tasks/abs_task.py", line 1233, in main_worker
    distributed_option=distributed_option,
  File "/espnet/espnet2/train/trainer.py", line 210, in run
    options=trainer_options,
  File "/espnet/espnet2/train/trainer.py", line 383, in train_one_epoch
    loss, stats, weight = model(**batch)
  File "/miniconda/envs/espnet/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/espnet/espnet2/asr/espnet_model.py", line 125, in forward
    encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)
  File "/espnet/espnet2/asr/espnet_model.py", line 202, in encode
    encoder_out, encoder_out_lens, _ = self.encoder(feats, feats_lengths)
  File "/miniconda/envs/espnet/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/espnet/espnet2/asr/encoder/rnn_encoder.py", line 108, in forward
    xs_pad, ilens, states = module(xs_pad, ilens, prev_state=prev_state)
  File "/miniconda/envs/espnet/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/espnet/espnet/nets/pytorch_backend/rnn/encoders.py", line 77, in forward
    xs_pack = pack_padded_sequence(xs_pad, ilens, batch_first=True)
  File "/miniconda/envs/espnet/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 245, in pack_padded_sequence
    _VF._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: 'lengths' argument should be a 1D CPU int64 tensor, but got 1D cuda:0 Long tensor
&lt;/denchmark-code&gt;

digging a bit deeper I found this related pytorch issue:

pytorch/pytorch#16542 (comment)

there is a one-line fix in the related pull request:

pytorch/pytorch#16636

but it has been stuck for a while in review for what looks like a variety of reasons.  It is possible that there is some other, better way to fix this directly in espnet, or that it is preferred not to, but I found that applying the change suggested in the pull request resolves the issue in training, so for the moment I did not look further.
To be totally clear, the issue can be resolved in the container (or patched with the Docker file afterwards) by implementing this change:

https://github.com/pytorch/pytorch/pull/16636/files#diff-561e7cb210e877f70a6df844c7eaa13336eb11ccb2d36485fbce36ff28c22425

	</description>
	<comments>
		<comment id='1' author='AdolfVonKleist' date='2020-11-09T05:32:24Z'>
		Thanks, I confirmed this issue.
		</comment>
	</comments>
</bug>