<bug id='1949' author='leo1980' open_date='2020-05-22T00:31:47Z' closed_time='2020-06-06T23:11:29Z'>
	<summary>Error in decode stage - No such file or directory</summary>
	<description>
Environment:

Ubuntu 18.04
Cuda: 10.0
1 GPU card

I was using following command to train librispeech in egs folder
sudo ./run.sh --docker_os local --docker_cuda 10.0 --docker_gpu 0 --docker_egs librispeech/asr1 --docker_folders /home/leo/storage/asr_data --ngpu 1
Since I finished early stages - 1, 2, 3, I changed stage to 4 in run.sh. From log file, I can see it completed stage 4, then got error in stage 5 - decoding as shown below,
&lt;denchmark-code&gt;leo@leo-pc:~/asr/espnet/docker$ sudo ./run.sh --docker_os local --docker_cuda 10.0 --docker_gpu 0 --docker_egs librispeech/asr1 --docker_folders /home/leo/storage/asr_data --ngpu 1
[sudo] password for leo: 
Warning: Your user ID belongs to root users.
        Using Docker container with root instead of User-built container.
Using image espnet/espnet:gpu-cuda10.0-cudnn7-local.
Executing application in Docker
docker run --gpus 'device=0' -i --rm --name espnet_gpu0_20200518T2229 -v /home/leo/storage/asr/espnet/egs:/espnet/egs -v /home/leo/storage/asr/espnet/espnet:/espnet/espnet -v /home/leo/storage/asr/espnet/test:/espnet/test -v /home/leo/storage/asr/espnet/utils:/espnet/utils -v /home/leo/storage/asr_data:/home/leo/storage/asr_data espnet/espnet:gpu-cuda10.0-cudnn7-local /bin/bash -c 'cd /espnet/egs/librispeech/asr1; ./run.sh --ngpu 1; chmod -R 777 /espnet/egs/librispeech/asr1'
dictionary: data/lang_char/train_960_unigram5000_units.txt
stage 4: Network Training
stage 5: Decoding
best val scores = [0.16489187 0.16076851 0.15817078 0.15651147 0.15507215]
selected epochs = [11 10  9  8  7]
average over ['exp/train_960_pytorch_train_specaug/results/snapshot.ep.11', 'exp/train_960_pytorch_train_specaug/results/snapshot.ep.10', 'exp/train_960_pytorch_train_specaug/results/snapshot.ep.9', 'exp/train_960_pytorch_train_specaug/results/snapshot.ep.8', 'exp/train_960_pytorch_train_specaug/results/snapshot.ep.7']
2020-05-21 13:49:30,111 (splitjson:40) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/splitjson.py --parts 12 dump/dev_other/deltafalse/data_unigram5000.json
2020-05-21 13:49:30,111 (splitjson:40) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/splitjson.py --parts 12 dump/dev_clean/deltafalse/data_unigram5000.json
2020-05-21 13:49:30,111 (splitjson:40) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/splitjson.py --parts 12 dump/test_clean/deltafalse/data_unigram5000.json
2020-05-21 13:49:30,111 (splitjson:40) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/splitjson.py --parts 12 dump/test_other/deltafalse/data_unigram5000.json
2020-05-21 13:49:30,249 (splitjson:52) INFO: number of utterances = 2620
2020-05-21 13:49:30,254 (splitjson:52) INFO: number of utterances = 2703
2020-05-21 13:49:30,258 (splitjson:52) INFO: number of utterances = 2864
2020-05-21 13:49:30,263 (splitjson:52) INFO: number of utterances = 2939
2020-05-21 13:49:31,589 (concatjson:36) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/concatjson.py 'exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/data.*.json'
2020-05-21 13:49:31,589 (concatjson:36) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/concatjson.py 'exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/data.*.json'
Traceback (most recent call last):
  File "/espnet/egs/librispeech/asr1/../../../utils/concatjson.py", line 41, in &lt;module&gt;
    with codecs.open(x, encoding="utf-8") as f:
  File "/espnet/tools/venv/lib/python3.7/codecs.py", line 898, in open
    file = builtins.open(filename, mode, buffering)
FileNotFoundError: [Errno 2] No such file or directory: 'exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/data.*.json'
Traceback (most recent call last):
  File "/espnet/egs/librispeech/asr1/../../../utils/concatjson.py", line 41, in &lt;module&gt;
    with codecs.open(x, encoding="utf-8") as f:
  File "/espnet/tools/venv/lib/python3.7/codecs.py", line 898, in open
    file = builtins.open(filename, mode, buffering)
FileNotFoundError: [Errno 2] No such file or directory: 'exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/data.*.json'
2020-05-21 13:49:31,600 (concatjson:36) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/concatjson.py 'exp/train_960_pytorch_train_specaug/decode_test_clean_model.val5.avg.best_decode_lm/data.*.json'
2020-05-21 13:49:31,600 (concatjson:36) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/concatjson.py 'exp/train_960_pytorch_train_specaug/decode_test_other_model.val5.avg.best_decode_lm/data.*.json'
Traceback (most recent call last):
  File "/espnet/egs/librispeech/asr1/../../../utils/concatjson.py", line 41, in &lt;module&gt;
    with codecs.open(x, encoding="utf-8") as f:
  File "/espnet/tools/venv/lib/python3.7/codecs.py", line 898, in open
    file = builtins.open(filename, mode, buffering)
FileNotFoundError: [Errno 2] No such file or directory: 'exp/train_960_pytorch_train_specaug/decode_test_clean_model.val5.avg.best_decode_lm/data.*.json'
&lt;/denchmark-code&gt;

I attached log file and config files
&lt;denchmark-link:https://github.com/espnet/espnet/files/4665427/train.log&gt;train.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/espnet/espnet/files/4665435/conf.zip&gt;conf.zip&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='leo1980' date='2020-05-22T14:07:29Z'>
		The interesting part is that it seems that you are not decoding.
The line :
&lt;denchmark-code&gt;2020-05-21 13:49:31,589 (concatjson:36) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/concatjson.py 'exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/data.*.json'
&lt;/denchmark-code&gt;

should be called after decoding.
can you attach/paste the output of the exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/log/decode&lt;&gt;.log file.
		</comment>
		<comment id='2' author='leo1980' date='2020-05-23T02:55:56Z'>
		Here is zippped log files under exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/log
&lt;denchmark-link:https://github.com/espnet/espnet/files/4671043/decode_log.zip&gt;decode_log.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='leo1980' date='2020-05-23T03:27:29Z'>
		Ok, I cannot track the error. Your run.sh files seems ok (only changes on irrelevant variables) but the output is only showing a 0. At least it should display a warning "experimental API for custom LMs is selected by --api v2".



espnet/espnet/asr/pytorch_backend/recog.py


         Line 34
      in
      b193497






 logging.warning("experimental API for custom LMs is selected by --api v2") 





I am not sure if it is a memory issue either ( it ends with code 0). Can you try with  nj=1.
I am wondering if you changed any part in the folder espnet. if not can you please track where your programs is exiting?.
Add some comments (?) to generate output in file &lt;denchmark-link:https://github.com/espnet/espnet/blob/master/espnet/bin/asr_recog.py&gt;https://github.com/espnet/espnet/blob/master/espnet/bin/asr_recog.py&lt;/denchmark-link&gt;
. or you can change the verbose level by adding the flag  after this line maybe:


		</comment>
		<comment id='4' author='leo1980' date='2020-05-23T04:25:38Z'>
		Attached file  is the output of 'git diff' under root folder of espnet.
&lt;denchmark-link:https://github.com/espnet/espnet/files/4671128/changes.txt&gt;changes.txt&lt;/denchmark-link&gt;



I am not sure if it is a memory issue either ( it ends with code 0). Can you try with nj=1.
'nj=1' is to specify how many thread there, by limit thread to 1, means use less memory? My RAM is 32GB, graphics card's memory is 8G.

Add some comments (?) to generate output in file

What does this mean?
Later I will try with --verbose 2
		</comment>
		<comment id='5' author='leo1980' date='2020-05-23T04:37:41Z'>
		&lt;denchmark-h:h2&gt;I run  with --verbose 2, got error as following,&lt;/denchmark-h&gt;

leo@leo-pc:~/asr/espnet/docker$ sudo ./run.sh --docker_os local --docker_cuda 10.0 --docker_gpu 0 --docker_egs librispeech/asr1 --docker_folders /home/leo/storage/asr_data
Warning: Your user ID belongs to root users.
Using Docker container with root instead of User-built container.
Using image espnet/espnet:gpu-cuda10.0-cudnn7-local.
Executing application in Docker
docker run --gpus 'device=0' -i --rm --name espnet_gpu0_20200523T1235 -v /home/leo/storage/asr/espnet/egs:/espnet/egs -v /home/leo/storage/asr/espnet/espnet:/espnet/espnet -v /home/leo/storage/asr/espnet/test:/espnet/test -v /home/leo/storage/asr/espnet/utils:/espnet/utils -v /home/leo/storage/asr_data:/home/leo/storage/asr_data espnet/espnet:gpu-cuda10.0-cudnn7-local /bin/bash -c 'cd /espnet/egs/librispeech/asr1; ./run.sh ; chmod -R 777 /espnet/egs/librispeech/asr1'
dictionary: data/lang_char/train_960_unigram5000_units.txt
stage 5: Decoding
best val scores = [0.16489187 0.16076851 0.15817078 0.15651147 0.15507215]
selected epochs = [11 10  9  8  7]
average over ['exp/train_960_pytorch_train_specaug/results/snapshot.ep.11', 'exp/train_960_pytorch_train_specaug/results/snapshot.ep.10', 'exp/train_960_pytorch_train_specaug/results/snapshot.ep.9', 'exp/train_960_pytorch_train_specaug/results/snapshot.ep.8', 'exp/train_960_pytorch_train_specaug/results/snapshot.ep.7']
2020-05-23 04:35:28,787 (splitjson:40) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/splitjson.py --parts 12 dump/dev_other/deltafalse/data_unigram5000.json
2020-05-23 04:35:28,787 (splitjson:40) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/splitjson.py --parts 12 dump/dev_clean/deltafalse/data_unigram5000.json
2020-05-23 04:35:28,787 (splitjson:40) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/splitjson.py --parts 12 dump/test_other/deltafalse/data_unigram5000.json
2020-05-23 04:35:28,787 (splitjson:40) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/splitjson.py --parts 12 dump/test_clean/deltafalse/data_unigram5000.json
2020-05-23 04:35:28,901 (splitjson:52) INFO: number of utterances = 2939
2020-05-23 04:35:28,901 (splitjson:52) INFO: number of utterances = 2620
2020-05-23 04:35:28,906 (splitjson:52) INFO: number of utterances = 2703
2020-05-23 04:35:28,911 (splitjson:52) INFO: number of utterances = 2864
2020-05-23 04:35:30,104 (concatjson:36) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/concatjson.py 'exp/train_960_pytorch_train_specaug/decode_test_other_model.val5.avg.best_decode_lm/data..json'
Traceback (most recent call last):
File "/espnet/egs/librispeech/asr1/../../../utils/concatjson.py", line 41, in 
with codecs.open(x, encoding="utf-8") as f:
File "/espnet/tools/venv/lib/python3.7/codecs.py", line 898, in open
file = builtins.open(filename, mode, buffering)
FileNotFoundError: [Errno 2] No such file or directory: 'exp/train_960_pytorch_train_specaug/decode_test_other_model.val5.avg.best_decode_lm/data..json'
2020-05-23 04:35:30,179 (concatjson:36) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/concatjson.py 'exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/data..json'
Traceback (most recent call last):
File "/espnet/egs/librispeech/asr1/../../../utils/concatjson.py", line 41, in 
with codecs.open(x, encoding="utf-8") as f:
File "/espnet/tools/venv/lib/python3.7/codecs.py", line 898, in open
file = builtins.open(filename, mode, buffering)
FileNotFoundError: [Errno 2] No such file or directory: 'exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/data..json'
2020-05-23 04:35:30,205 (concatjson:36) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/concatjson.py 'exp/train_960_pytorch_train_specaug/decode_test_clean_model.val5.avg.best_decode_lm/data..json'
2020-05-23 04:35:30,205 (concatjson:36) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/concatjson.py 'exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/data..json'
Traceback (most recent call last):
File "/espnet/egs/librispeech/asr1/../../../utils/concatjson.py", line 41, in 
with codecs.open(x, encoding="utf-8") as f:
File "/espnet/tools/venv/lib/python3.7/codecs.py", line 898, in open
file = builtins.open(filename, mode, buffering)
FileNotFoundError: [Errno 2] No such file or directory: 'exp/train_960_pytorch_train_specaug/decode_test_clean_model.val5.avg.best_decode_lm/data..json'
Traceback (most recent call last):
File "/espnet/egs/librispeech/asr1/../../../utils/concatjson.py", line 41, in 
with codecs.open(x, encoding="utf-8") as f:
File "/espnet/tools/venv/lib/python3.7/codecs.py", line 898, in open
file = builtins.open(filename, mode, buffering)
FileNotFoundError: [Errno 2] No such file or directory: 'exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/data..json'
2020-05-23 04:35:30,255 (json2trn:43) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/json2trn.py exp/train_960_pytorch_train_specaug/decode_test_other_model.val5.avg.best_decode_lm/data.json data/lang_char/train_960_unigram5000_units.txt --num-spkrs 1 --refs exp/train_960_pytorch_train_specaug/decode_test_other_model.val5.avg.best_decode_lm/ref.trn --hyps exp/train_960_pytorch_train_specaug/decode_test_other_model.val5.avg.best_decode_lm/hyp.trn
2020-05-23 04:35:30,255 (json2trn:45) INFO: reading exp/train_960_pytorch_train_specaug/decode_test_other_model.val5.avg.best_decode_lm/data.json
Traceback (most recent call last):
File "/espnet/egs/librispeech/asr1/../../../utils/json2trn.py", line 86, in 
main(sys.argv[1:])
File "/espnet/egs/librispeech/asr1/../../../utils/json2trn.py", line 31, in main
convert(args.json, args.dict, args.refs, args.hyps, args.num_spkrs)
File "/espnet/egs/librispeech/asr1/../../../utils/json2trn.py", line 47, in convert
j = json.load(f)
File "/espnet/tools/venv/lib/python3.7/json/init.py", line 296, in load
parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)
File "/espnet/tools/venv/lib/python3.7/json/init.py", line 348, in loads
return _default_decoder.decode(s)
File "/espnet/tools/venv/lib/python3.7/json/decoder.py", line 337, in decode
obj, end = self.raw_decode(s, idx=_w(s, 0).end())
File "/espnet/tools/venv/lib/python3.7/json/decoder.py", line 355, in raw_decode
raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
sed: can't read exp/train_960_pytorch_train_specaug/decode_test_other_model.val5.avg.best_decode_lm/hyp.trn: No such file or directory
sclite: 
sclite Version: 2.10, SCTK Version: 1.3
Input Options:
-r reffile [  ]
Define the reference file, and it's format
-h hypfile [  &lt;title&gt; ]
Define the hypothesis file, it's format, and a 'title' used
for reports.  The default title is 'hypfile'.  This option
may be used more than once.
-i     Set the utterance id type.   (for transcript mode only)
-P          Accept the piped input from another utility.
-e gb|euc|utf-8 [ case-conversion-localization ]
Interpret characters as GB, EUC, utf-8, or the default, 8-bit ASCII.
Optionally, case conversion localization can be set to either 'generic',
'babel_turkish', or 'babel_vietnamese'
Alignment Options:
-s          Do Case-sensitive alignments.
-d          Use GNU diff for alignments.
-c [ NOASCII DH ]
Do the alignment on characters not on words as usual by split-
ting words into chars. The optional argument NOASCII does not
split ASCII words and the optional arg. DH deletes hyphens from
both the ref and hyp before alingment.   Exclusive with -d.
-L LM       CMU-Cambridge SLM Language model file to use in alignment and scoring.
-S algo1 lexicon [ ASCIITOO ]
-S algo2 lexicon [ ASCIITOO ]
Instead of performing word alignments, infer the word segmenta-
tion using algo1 or algo2.  See sclite(1) for algorithm details.
-F          Score fragments as correct.  Options -F and -d are exclusive.
-D          Score words marked optionally deletable as correct if deleted.
Options -D and -d are exclusive.
-T          Use time information, (if available), to calculated word-to-
word distances based on times. Options -F and -d are exlc.
-w wwl      Perform Word-Weight Mediated alignments, using the WWL file 'wwl'.
IF wwl is 'unity' use weight 1.o for all words.
-m [ ref | hyp ]
Only used for scoring a hyp/ctm file, against a ref/stm file.
When the 'ref' option is used, reduce the reference segments
to time range of the hyp file's words.  When the 'hyp' option
is used, reduce the hyp words to the time range of the ref
segments.  The two may be used together.  The argument -m
by itself defaults to '-m ref'.  Exclusive with -d.
Output Options:
-O output_dir
Writes all output files into output_dir. Defaults to the
hypfile's directory.
-f level    Defines feedback mode, default is 1
-l width    Defines the line width.
-p          Pipe the alignments to another sclite utility.  Sets -f to 0.
Scoring Report Options:
-o [ sum | rsum | pralign | all | sgml | stdout | lur | snt | spk |
dtl | prf | wws | nl.sgml | none ]
Defines the output reports. Default: 'sum stdout'
-C [ det | bhist | sbhist | hist | none ]
Defines the output formats for analysis of confidence scores.
Default: 'none'
-n name     Writes all outputs using 'name' as a root filename instead of
'hypfile'.  For multiple hypothesis files, the root filename
is 'name'.'hypfile'
sclite: Error, Reference file 'exp/train_960_pytorch_train_specaug/decode_test_other_model.val5.avg.best_decode_lm/ref.trn' does not exist
write a CER (or TER) result in exp/train_960_pytorch_train_specaug/decode_test_other_model.val5.avg.best_decode_lm/result.txt
/espnet/egs/librispeech/asr1/../../../utils/score_sclite.sh: line 53: exp/train_960_pytorch_train_specaug/decode_test_other_model.val5.avg.best_decode_lm/ref.trn: No such file or directory
/espnet/egs/librispeech/asr1/../../../utils/score_sclite.sh: line 54: exp/train_960_pytorch_train_specaug/decode_test_other_model.val5.avg.best_decode_lm/hyp.trn: No such file or directory
write a WER result in exp/train_960_pytorch_train_specaug/decode_test_other_model.val5.avg.best_decode_lm/result.wrd.txt
|  SPKR    |     # Snt           # Wrd     |  Corr     Sub      Del      Ins      Err    S.Err  |
|  Sum/Avg |         0               0     |   0.0     0.0      0.0      0.0      0.0      0.0  |
2020-05-23 04:35:30,328 (json2trn:43) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/json2trn.py exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/data.json data/lang_char/train_960_unigram5000_units.txt --num-spkrs 1 --refs exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/ref.trn --hyps exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/hyp.trn
2020-05-23 04:35:30,328 (json2trn:45) INFO: reading exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/data.json
Traceback (most recent call last):
File "/espnet/egs/librispeech/asr1/../../../utils/json2trn.py", line 86, in 
main(sys.argv[1:])
File "/espnet/egs/librispeech/asr1/../../../utils/json2trn.py", line 31, in main
convert(args.json, args.dict, args.refs, args.hyps, args.num_spkrs)
File "/espnet/egs/librispeech/asr1/../../../utils/json2trn.py", line 47, in convert
j = json.load(f)
File "/espnet/tools/venv/lib/python3.7/json/init.py", line 296, in load
parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)
File "/espnet/tools/venv/lib/python3.7/json/init.py", line 348, in loads
return _default_decoder.decode(s)
File "/espnet/tools/venv/lib/python3.7/json/decoder.py", line 337, in decode
obj, end = self.raw_decode(s, idx=_w(s, 0).end())
File "/espnet/tools/venv/lib/python3.7/json/decoder.py", line 355, in raw_decode
raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
sed: can't read exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/hyp.trn: No such file or directory
sclite: 
sclite Version: 2.10, SCTK Version: 1.3
Input Options:
-r reffile [  ]
Define the reference file, and it's format
-h hypfile [  &lt;title&gt; ]
Define the hypothesis file, it's format, and a 'title' used
for reports.  The default title is 'hypfile'.  This option
may be used more than once.
-i     Set the utterance id type.   (for transcript mode only)
-P          Accept the piped input from another utility.
-e gb|euc|utf-8 [ case-conversion-localization ]
Interpret characters as GB, EUC, utf-8, or the default, 8-bit ASCII.
Optionally, case conversion localization can be set to either 'generic',
'babel_turkish', or 'babel_vietnamese'
Alignment Options:
-s          Do Case-sensitive alignments.
-d          Use GNU diff for alignments.
-c [ NOASCII DH ]
Do the alignment on characters not on words as usual by split-
ting words into chars. The optional argument NOASCII does not
split ASCII words and the optional arg. DH deletes hyphens from
both the ref and hyp before alingment.   Exclusive with -d.
-L LM       CMU-Cambridge SLM Language model file to use in alignment and scoring.
-S algo1 lexicon [ ASCIITOO ]
-S algo2 lexicon [ ASCIITOO ]
Instead of performing word alignments, infer the word segmenta-
tion using algo1 or algo2.  See sclite(1) for algorithm details.
-F          Score fragments as correct.  Options -F and -d are exclusive.
-D          Score words marked optionally deletable as correct if deleted.
Options -D and -d are exclusive.
-T          Use time information, (if available), to calculated word-to-
word distances based on times. Options -F and -d are exlc.
-w wwl      Perform Word-Weight Mediated alignments, using the WWL file 'wwl'.
IF wwl is 'unity' use weight 1.o for all words.
-m [ ref | hyp ]
Only used for scoring a hyp/ctm file, against a ref/stm file.
When the 'ref' option is used, reduce the reference segments
to time range of the hyp file's words.  When the 'hyp' option
is used, reduce the hyp words to the time range of the ref
segments.  The two may be used together.  The argument -m
by itself defaults to '-m ref'.  Exclusive with -d.
Output Options:
-O output_dir
Writes all output files into output_dir. Defaults to the
hypfile's directory.
-f level    Defines feedback mode, default is 1
-l width    Defines the line width.
-p          Pipe the alignments to another sclite utility.  Sets -f to 0.
Scoring Report Options:
-o [ sum | rsum | pralign | all | sgml | stdout | lur | snt | spk |
dtl | prf | wws | nl.sgml | none ]
Defines the output reports. Default: 'sum stdout'
-C [ det | bhist | sbhist | hist | none ]
Defines the output formats for analysis of confidence scores.
Default: 'none'
-n name     Writes all outputs using 'name' as a root filename instead of
'hypfile'.  For multiple hypothesis files, the root filename
is 'name'.'hypfile'
sclite: Error, Reference file 'exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/ref.trn' does not exist
write a CER (or TER) result in exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/result.txt
/espnet/egs/librispeech/asr1/../../../utils/score_sclite.sh: line 53: exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/ref.trn: No such file or directory
/espnet/egs/librispeech/asr1/../../../utils/score_sclite.sh: line 54: exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/hyp.trn: No such file or directory
2020-05-23 04:35:30,354 (json2trn:43) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/json2trn.py exp/train_960_pytorch_train_specaug/decode_test_clean_model.val5.avg.best_decode_lm/data.json data/lang_char/train_960_unigram5000_units.txt --num-spkrs 1 --refs exp/train_960_pytorch_train_specaug/decode_test_clean_model.val5.avg.best_decode_lm/ref.trn --hyps exp/train_960_pytorch_train_specaug/decode_test_clean_model.val5.avg.best_decode_lm/hyp.trn
2020-05-23 04:35:30,354 (json2trn:45) INFO: reading exp/train_960_pytorch_train_specaug/decode_test_clean_model.val5.avg.best_decode_lm/data.json
Traceback (most recent call last):
File "/espnet/egs/librispeech/asr1/../../../utils/json2trn.py", line 86, in 
main(sys.argv[1:])
File "/espnet/egs/librispeech/asr1/../../../utils/json2trn.py", line 31, in main
convert(args.json, args.dict, args.refs, args.hyps, args.num_spkrs)
File "/espnet/egs/librispeech/asr1/../../../utils/json2trn.py", line 47, in convert
j = json.load(f)
File "/espnet/tools/venv/lib/python3.7/json/init.py", line 296, in load
parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)
File "/espnet/tools/venv/lib/python3.7/json/init.py", line 348, in loads
return _default_decoder.decode(s)
File "/espnet/tools/venv/lib/python3.7/json/decoder.py", line 337, in decode
obj, end = self.raw_decode(s, idx=_w(s, 0).end())
File "/espnet/tools/venv/lib/python3.7/json/decoder.py", line 355, in raw_decode
raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
2020-05-23 04:35:30,357 (json2trn:43) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/json2trn.py exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/data.json data/lang_char/train_960_unigram5000_units.txt --num-spkrs 1 --refs exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/ref.trn --hyps exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/hyp.trn
2020-05-23 04:35:30,357 (json2trn:45) INFO: reading exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/data.json
Traceback (most recent call last):
File "/espnet/egs/librispeech/asr1/../../../utils/json2trn.py", line 86, in 
main(sys.argv[1:])
File "/espnet/egs/librispeech/asr1/../../../utils/json2trn.py", line 31, in main
convert(args.json, args.dict, args.refs, args.hyps, args.num_spkrs)
File "/espnet/egs/librispeech/asr1/../../../utils/json2trn.py", line 47, in convert
j = json.load(f)
File "/espnet/tools/venv/lib/python3.7/json/init.py", line 296, in load
parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)
File "/espnet/tools/venv/lib/python3.7/json/init.py", line 348, in loads
return _default_decoder.decode(s)
File "/espnet/tools/venv/lib/python3.7/json/decoder.py", line 337, in decode
obj, end = self.raw_decode(s, idx=_w(s, 0).end())
File "/espnet/tools/venv/lib/python3.7/json/decoder.py", line 355, in raw_decode
raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
write a WER result in exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/result.wrd.txt
|  SPKR    |     # Snt          # Wrd     |  Corr      Sub      Del     Ins      Err    S.Err  |
|  Sum/Avg |         0              0     |   0.0      0.0      0.0     0.0      0.0      0.0  |
sed: can't read exp/train_960_pytorch_train_specaug/decode_test_clean_model.val5.avg.best_decode_lm/hyp.trn: No such file or directory
sclite: 
sclite Version: 2.10, SCTK Version: 1.3
Input Options:
-r reffile [  ]
Define the reference file, and it's format
-h hypfile [  &lt;title&gt; ]
Define the hypothesis file, it's format, and a 'title' used
for reports.  The default title is 'hypfile'.  This option
may be used more than once.
-i     Set the utterance id type.   (for transcript mode only)
-P          Accept the piped input from another utility.
-e gb|euc|utf-8 [ case-conversion-localization ]
Interpret characters as GB, EUC, utf-8, or the default, 8-bit ASCII.
Optionally, case conversion localization can be set to either 'generic',
'babel_turkish', or 'babel_vietnamese'
Alignment Options:
-s          Do Case-sensitive alignments.
-d          Use GNU diff for alignments.
-c [ NOASCII DH ]
Do the alignment on characters not on words as usual by split-
ting words into chars. The optional argument NOASCII does not
split ASCII words and the optional arg. DH deletes hyphens from
both the ref and hyp before alingment.   Exclusive with -d.
-L LM       CMU-Cambridge SLM Language model file to use in alignment and scoring.
-S algo1 lexicon [ ASCIITOO ]
-S algo2 lexicon [ ASCIITOO ]
Instead of performing word alignments, infer the word segmenta-
tion using algo1 or algo2.  See sclite(1) for algorithm details.
-F          Score fragments as correct.  Options -F and -d are exclusive.
-D          Score words marked optionally deletable as correct if deleted.
Options -D and -d are exclusive.
-T          Use time information, (if available), to calculated word-to-
word distances based on times. Options -F and -d are exlc.
-w wwl      Perform Word-Weight Mediated alignments, using the WWL file 'wwl'.
IF wwl is 'unity' use weight 1.o for all words.
-m [ ref | hyp ]
Only used for scoring a hyp/ctm file, against a ref/stm file.
When the 'ref' option is used, reduce the reference segments
to time range of the hyp file's words.  When the 'hyp' option
is used, reduce the hyp words to the time range of the ref
segments.  The two may be used together.  The argument -m
by itself defaults to '-m ref'.  Exclusive with -d.
Output Options:
-O output_dir
Writes all output files into output_dir. Defaults to the
hypfile's directory.
-f level    Defines feedback mode, default is 1
-l width    Defines the line width.
-p          Pipe the alignments to another sclite utility.  Sets -f to 0.
Scoring Report Options:
-o [ sum | rsum | pralign | all | sgml | stdout | lur | snt | spk |
dtl | prf | wws | nl.sgml | none ]
Defines the output reports. Default: 'sum stdout'
-C [ det | bhist | sbhist | hist | none ]
Defines the output formats for analysis of confidence scores.
Default: 'none'
-n name     Writes all outputs using 'name' as a root filename instead of
'hypfile'.  For multiple hypothesis files, the root filename
is 'name'.'hypfile'
sclite: Error, Reference file 'exp/train_960_pytorch_train_specaug/decode_test_clean_model.val5.avg.best_decode_lm/ref.trn' does not exist
write a CER (or TER) result in exp/train_960_pytorch_train_specaug/decode_test_clean_model.val5.avg.best_decode_lm/result.txt
/espnet/egs/librispeech/asr1/../../../utils/score_sclite.sh: line 53: exp/train_960_pytorch_train_specaug/decode_test_clean_model.val5.avg.best_decode_lm/ref.trn: No such file or directory
/espnet/egs/librispeech/asr1/../../../utils/score_sclite.sh: line 54: exp/train_960_pytorch_train_specaug/decode_test_clean_model.val5.avg.best_decode_lm/hyp.trn: No such file or directory
sed: can't read exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/hyp.trn: No such file or directory
write a CER (or TER) result in exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/result.txt
sclite: 
sclite Version: 2.10, SCTK Version: 1.3
Input Options:
-r reffile [  ]
Define the reference file, and it's format
-h hypfile [  &lt;title&gt; ]
Define the hypothesis file, it's format, and a 'title' used
for reports.  The default title is 'hypfile'.  This option
may be used more than once.
-i     Set the utterance id type.   (for transcript mode only)
-P          Accept the piped input from another utility.
-e gb|euc|utf-8 [ case-conversion-localization ]
Interpret characters as GB, EUC, utf-8, or the default, 8-bit ASCII.
Optionally, case conversion localization can be set to either 'generic',
'babel_turkish', or 'babel_vietnamese'
Alignment Options:
-s          Do Case-sensitive alignments.
-d          Use GNU diff for alignments.
-c [ NOASCII DH ]
Do the alignment on characters not on words as usual by split-
ting words into chars. The optional argument NOASCII does not
split ASCII words and the optional arg. DH deletes hyphens from
both the ref and hyp before alingment.   Exclusive with -d.
-L LM       CMU-Cambridge SLM Language model file to use in alignment and scoring.
-S algo1 lexicon [ ASCIITOO ]
-S algo2 lexicon [ ASCIITOO ]
Instead of performing word alignments, infer the word segmenta-
tion using algo1 or algo2.  See sclite(1) for algorithm details.
-F          Score fragments as correct.  Options -F and -d are exclusive.
-D          Score words marked optionally deletable as correct if deleted.
Options -D and -d are exclusive.
-T          Use time information, (if available), to calculated word-to-
word distances based on times. Options -F and -d are exlc.
-w wwl      Perform Word-Weight Mediated alignments, using the WWL file 'wwl'.
IF wwl is 'unity' use weight 1.o for all words.
-m [ ref | hyp ]
Only used for scoring a hyp/ctm file, against a ref/stm file.
When the 'ref' option is used, reduce the reference segments
to time range of the hyp file's words.  When the 'hyp' option
is used, reduce the hyp words to the time range of the ref
segments.  The two may be used together.  The argument -m
by itself defaults to '-m ref'.  Exclusive with -d.
Output Options:
-O output_dir
Writes all output files into output_dir. Defaults to the
hypfile's directory.
-f level    Defines feedback mode, default is 1
-l width    Defines the line width.
-p          Pipe the alignments to another sclite utility.  Sets -f to 0.
Scoring Report Options:
-o [ sum | rsum | pralign | all | sgml | stdout | lur | snt | spk |
dtl | prf | wws | nl.sgml | none ]
Defines the output reports. Default: 'sum stdout'
-C [ det | bhist | sbhist | hist | none ]
Defines the output formats for analysis of confidence scores.
Default: 'none'
-n name     Writes all outputs using 'name' as a root filename instead of
'hypfile'.  For multiple hypothesis files, the root filename
is 'name'.'hypfile'
sclite: Error, Reference file 'exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/ref.trn' does not exist
/espnet/egs/librispeech/asr1/../../../utils/score_sclite.sh: line 53: exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/ref.trn: No such file or directory
/espnet/egs/librispeech/asr1/../../../utils/score_sclite.sh: line 54: exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/hyp.trn: No such file or directory
write a WER result in exp/train_960_pytorch_train_specaug/decode_test_clean_model.val5.avg.best_decode_lm/result.wrd.txt
|  SPKR    |     # Snt           # Wrd     |  Corr     Sub      Del      Ins      Err    S.Err  |
|  Sum/Avg |         0               0     |   0.0     0.0      0.0      0.0      0.0      0.0  |
write a WER result in exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/result.wrd.txt
|  SPKR    |     # Snt          # Wrd     |  Corr      Sub      Del     Ins      Err    S.Err  |
|  Sum/Avg |         0              0     |   0.0      0.0      0.0     0.0      0.0      0.0  |
Finished
run.sh done.
leo@leo-pc:~/asr/espnet/docker$
		</comment>
		<comment id='6' author='leo1980' date='2020-05-23T06:17:19Z'>
		here:
/espnet/bin/asr_recog.py
@@ -123,14 +123,16 @@ def get_parser():
                         help='Offset margin')
     return parser
 
-
 def main(args):
     """Run the main decoding function."""
     parser = get_parser()
     args = parser.parse_args(args)
 
+    print(args.ngpu)
+    exit(0)
+
why is an exit(0) after print(args.ngpu). and why are u printing args.ngpu.  For decoding ngpu is not used.



espnet/egs/librispeech/asr1/run.sh


         Line 285
      in
      b193497






 ngpu=0 





Because, transformer and recog v2 does not support gpu I think.
Need to delete those two lines.
		</comment>
		<comment id='7' author='leo1980' date='2020-05-24T02:30:46Z'>
		Thanks. Its a problem. After removing those 2 lines, I can continue now. I added those 2 lines just to check if cuda was installed properly and my gpu was detected.
Now problem I got another problem,
&lt;denchmark-code&gt;leo@leo-pc:~/asr/espnet/docker$ sudo ./run.sh --docker_os local --docker_cuda 10.0 --docker_gpu 0 --docker_egs librispeech/asr1 --docker_folders /home/leo/storage/asr_data
[sudo] password for leo: 
Warning: Your user ID belongs to root users.
        Using Docker container with root instead of User-built container.
Using image espnet/espnet:gpu-cuda10.0-cudnn7-local.
Executing application in Docker
docker run --gpus 'device=0' -i --rm --name espnet_gpu0_20200524T0031 -v /home/leo/storage/asr/espnet/egs:/espnet/egs -v /home/leo/storage/asr/espnet/espnet:/espnet/espnet -v /home/leo/storage/asr/espnet/test:/espnet/test -v /home/leo/storage/asr/espnet/utils:/espnet/utils -v /home/leo/storage/asr_data:/home/leo/storage/asr_data espnet/espnet:gpu-cuda10.0-cudnn7-local /bin/bash -c 'cd /espnet/egs/librispeech/asr1; ./run.sh ; chmod -R 777 /espnet/egs/librispeech/asr1'
dictionary: data/lang_char/train_960_unigram5000_units.txt
stage 5: Decoding
best val scores = [0.16489187 0.16076851 0.15817078 0.15651147 0.15507215]
selected epochs = [11 10  9  8  7]
average over ['exp/train_960_pytorch_train_specaug/results/snapshot.ep.11', 'exp/train_960_pytorch_train_specaug/results/snapshot.ep.10', 'exp/train_960_pytorch_train_specaug/results/snapshot.ep.9', 'exp/train_960_pytorch_train_specaug/results/snapshot.ep.8', 'exp/train_960_pytorch_train_specaug/results/snapshot.ep.7']
2020-05-23 16:32:10,149 (splitjson:40) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/splitjson.py --parts 12 dump/dev_clean/deltafalse/data_unigram5000.json
2020-05-23 16:32:10,149 (splitjson:40) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/splitjson.py --parts 12 dump/test_other/deltafalse/data_unigram5000.json
2020-05-23 16:32:10,149 (splitjson:40) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/splitjson.py --parts 12 dump/dev_other/deltafalse/data_unigram5000.json
2020-05-23 16:32:10,149 (splitjson:40) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/splitjson.py --parts 12 dump/test_clean/deltafalse/data_unigram5000.json
2020-05-23 16:32:10,214 (splitjson:52) INFO: number of utterances = 2703
2020-05-23 16:32:10,255 (splitjson:52) INFO: number of utterances = 2864
2020-05-23 16:32:10,259 (splitjson:52) INFO: number of utterances = 2620
2020-05-23 16:32:10,275 (splitjson:52) INFO: number of utterances = 2939
bash: line 1:   153 Killed                  ( asr_recog.py --config conf/decode.yaml --ngpu 0 --backend pytorch --verbose 2 --batchsize 0 --recog-json dump/dev_clean/deltafalse/split12utt/data_unigram5000.8.json --result-label exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/data.8.json --model exp/train_960_pytorch_train_specaug/results/model.val5.avg.best --rnnlm exp/train_rnnlm_pytorch_lm_unigram5000_ngpu1/rnnlm.model.best --api v2 ) 2&gt;&gt; exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/log/decode.8.log &gt;&gt; exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/log/decode.8.log
bash: line 1:   472 Killed                  ( asr_recog.py --config conf/decode.yaml --ngpu 0 --backend pytorch --verbose 2 --batchsize 0 --recog-json dump/test_other/deltafalse/split12utt/data_unigram5000.8.json --result-label exp/train_960_pytorch_train_specaug/decode_test_other_model.val5.avg.best_decode_lm/data.8.json --model exp/train_960_pytorch_train_specaug/results/model.val5.avg.best --rnnlm exp/train_rnnlm_pytorch_lm_unigram5000_ngpu1/rnnlm.model.best --api v2 ) 2&gt;&gt; exp/train_960_pytorch_train_specaug/decode_test_other_model.val5.avg.best_decode_lm/log/decode.8.log &gt;&gt; exp/train_960_pytorch_train_specaug/decode_test_other_model.val5.avg.best_decode_lm/log/decode.8.log
bash: line 1:   277 Killed                  ( asr_recog.py --config conf/decode.yaml --ngpu 0 --backend pytorch --verbose 2 --batchsize 0 --recog-json dump/test_clean/deltafalse/split12utt/data_unigram5000.5.json --result-label exp/train_960_pytorch_train_specaug/decode_test_clean_model.val5.avg.best_decode_lm/data.5.json --model exp/train_960_pytorch_train_specaug/results/model.val5.avg.best --rnnlm exp/train_rnnlm_pytorch_lm_unigram5000_ngpu1/rnnlm.model.best --api v2 ) 2&gt;&gt; exp/train_960_pytorch_train_specaug/decode_test_clean_model.val5.avg.best_decode_lm/log/decode.5.log &gt;&gt; exp/train_960_pytorch_train_specaug/decode_test_clean_model.val5.avg.best_decode_lm/log/decode.5.log
bash: line 1:   144 Killed                  ( asr_recog.py --config conf/decode.yaml --ngpu 0 --backend pytorch --verbose 2 --batchsize 0 --recog-json dump/dev_clean/deltafalse/split12utt/data_unigram5000.2.json --result-label exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/data.2.json --model exp/train_960_pytorch_train_specaug/results/model.val5.avg.best --rnnlm exp/train_rnnlm_pytorch_lm_unigram5000_ngpu1/rnnlm.model.best --api v2 ) 2&gt;&gt; exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/log/decode.2.log &gt;&gt; exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/log/decode.2.log
bash: line 1:   431 Killed                  ( asr_recog.py --config conf/decode.yaml --ngpu 0 --backend pytorch --verbose 2 --batchsize 0 --recog-json dump/dev_other/deltafalse/split12utt/data_unigram5000.5.json --result-label exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/data.5.json --model exp/train_960_pytorch_train_specaug/results/model.val5.avg.best --rnnlm exp/train_rnnlm_pytorch_lm_unigram5000_ngpu1/rnnlm.model.best --api v2 ) 2&gt;&gt; exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/log/decode.5.log &gt;&gt; exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/log/decode.5.log
&lt;/denchmark-code&gt;

Processes were killed, I think it abnormal behaviour, but don't know why.
And I also notice that, there are still totally 35 decoding jobs running in parallel, but in librispeech/asr1/run.sh, I specify nj=8. I changed this because, the reason of process being killed is OOM, and you pointed out early can change nj to resolve this, I change reduce nj, now memory usage is around 19GB of total 32GB.
		</comment>
		<comment id='8' author='leo1980' date='2020-05-24T02:45:45Z'>
		You need to remember that all decoding tasks are launched at the same time. So it means that if you set nj=8, then you will have, for librispeech, because it has 4 decoding task 32 processes at the same time. If you want to run each decoding then delete the &amp; from this line:



espnet/egs/librispeech/asr1/run.sh


         Line 302
      in
      001f4a2






 ) &amp; 





or try to set even a lower nj. Check how many threads your cpu has.
		</comment>
		<comment id='9' author='leo1980' date='2020-05-25T01:44:44Z'>
		My machine has 8 cores, 16 threads. Now I changed nj=2, based on the formula you mentioned above it should be 8 processes, initially I checked processes, it turned out there are 8 jobs running, after one night, one process killed, now there are 7 processes still stunning. Memory usage is around 19GB of total 32GB. What should I do now?
&lt;denchmark-h:h2&gt;output&lt;/denchmark-h&gt;

leo@leo-pc:~/asr/espnet/docker$ sudo ./run.sh --docker_os local --docker_cuda 10.0 --docker_gpu 0 --docker_egs librispeech/asr1 --docker_folders /home/leo/storage/asr_data
[sudo] password for leo:
Warning: Your user ID belongs to root users.
Using Docker container with root instead of User-built container.
Using image espnet/espnet:gpu-cuda10.0-cudnn7-local.
Executing application in Docker
docker run --gpus 'device=0' -i --rm --name espnet_gpu0_20200525T0138 -v /home/leo/storage/asr/espnet/egs:/espnet/egs -v /home/leo/storage/asr/espnet/espnet:/espnet/espnet -v /home/leo/storage/asr/espnet/test:/espnet/test -v /home/leo/storage/asr/espnet/utils:/espnet/utils -v /home/leo/storage/asr_data:/home/leo/storage/asr_data espnet/espnet:gpu-cuda10.0-cudnn7-local /bin/bash -c 'cd /espnet/egs/librispeech/asr1; ./run.sh ; chmod -R 777 /espnet/egs/librispeech/asr1'
dictionary: data/lang_char/train_960_unigram5000_units.txt
stage 5: Decoding
best val scores = [0.16489187 0.16076851 0.15817078 0.15651147 0.15507215]
selected epochs = [11 10  9  8  7]
average over ['exp/train_960_pytorch_train_specaug/results/snapshot.ep.11', 'exp/train_960_pytorch_train_specaug/results/snapshot.ep.10', 'exp/train_960_pytorch_train_specaug/results/snapshot.ep.9', 'exp/train_960_pytorch_train_specaug/results/snapshot.ep.8', 'exp/train_960_pytorch_train_specaug/results/snapshot.ep.7']
2020-05-24 17:38:32,722 (splitjson:40) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/splitjson.py --parts 2 dump/test_other/deltafalse/data_unigram5000.json
2020-05-24 17:38:32,722 (splitjson:40) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/splitjson.py --parts 2 dump/dev_other/deltafalse/data_unigram5000.json
2020-05-24 17:38:32,722 (splitjson:40) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/splitjson.py --parts 2 dump/test_clean/deltafalse/data_unigram5000.json
2020-05-24 17:38:32,722 (splitjson:40) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/splitjson.py --parts 2 dump/dev_clean/deltafalse/data_unigram5000.json
2020-05-24 17:38:32,814 (splitjson:52) INFO: number of utterances = 2703
2020-05-24 17:38:32,826 (splitjson:52) INFO: number of utterances = 2620
2020-05-24 17:38:32,840 (splitjson:52) INFO: number of utterances = 2939
2020-05-24 17:38:32,851 (splitjson:52) INFO: number of utterances = 2864
bash: line 1:   101 Killed                  ( asr_recog.py --config conf/decode.yaml --ngpu 0 --backend pytorch --verbose 2 --batchsize 0 --recog-json dump/dev_other/deltafalse/split2utt/data_unigram5000.2.json --result-label exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/data.2.json --model exp/train_960_pytorch_train_specaug/results/model.val5.avg.best --rnnlm exp/train_rnnlm_pytorch_lm_unigram5000_ngpu1/rnnlm.model.best --api v2 ) 2&gt;&gt; exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/log/decode.2.log &gt;&gt; exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/log/decode.2.log
		</comment>
		<comment id='10' author='leo1980' date='2020-05-25T13:36:12Z'>
		Ok, this is related to a memory issue with respect to docker. I observe this when I was working with ESPnet 2, but I never expect to see this in ESPnet 1 docker. Try adding the line 


espnet/docker/run.sh


         Line 157
      in
      a845738






                     -v /dev/shm:/dev/shm" 




 to the vols, so you can manage the limitation of the shared memory. Try to check if you can also increase the number of nj if this is solved.
		</comment>
		<comment id='11' author='leo1980' date='2020-05-25T14:07:06Z'>
		The docker/run.sh file does not have that line. Instead I found this which I think is something similar with it,
&lt;denchmark-code&gt;117 vols="-v ${PWD}/egs:/espnet/egs -v ${PWD}/espnet:/espnet/espnet -v ${PWD}/test:/espnet/test -v ${PWD}/utils:/espnet/utils -v /dev/shm:/dev/shm"
118 if [ ! -z "${docker_folders}" ]; then                                           
119     docker_folders=$(echo ${docker_folders} | tr "," "\n")                      
120     for i in ${docker_folders[@]}                                               
121     do                                                                          
122         vols=${vols}" -v $i:$i";                                                
123     done                                                                        
124 fi    
&lt;/denchmark-code&gt;

I added -v /dev/shm:/dev/shm as shown above.
Btw, how long usually does it take for decoding of librispeech on machine:
cpu: 8 cores 16 threads, 3.6GHz
For command without '-v /dev/shm:/dev/shm', it already ran almost 24 hours, but still not finished. But now I ended it for running new run.sh with '-v /dev/shm:/dev/shm'.
If I also want to know log or progress of decoding, where can I check? just like for training I can use tensorboard and 'tail -f logfile'
		</comment>
		<comment id='12' author='leo1980' date='2020-05-25T14:17:00Z'>
		Librispeech uses too large beam size to get 0.1% better results to write a paper. Usually beam size 5 is enough. Then, it will save your memory and time.
		</comment>
		<comment id='13' author='leo1980' date='2020-05-28T00:14:08Z'>
		It eventually finished decoding with following output on console when I used 5 as beam size. But I notice error rate is very high. Take decode_test_other_model as example, for WER result,
&lt;denchmark-code&gt;write a WER result in exp/train_960_pytorch_train_specaug/decode_test_other_model.val5.avg.best_decode_lm/result.wrd.txt
|  SPKR          |  # Snt   # Wrd   |  Corr       Sub      Del       Ins        Err     S.Err   |
|  Sum/Avg       |  2939    52343   |  11.0      82.6      6.4     251.9      340.9     100.0   |
&lt;/denchmark-code&gt;

Correct decoding is of 11.0 but error is of 340.9. Is this expected? I don't if this is caused by I changed epcho from 120 to 12 in traim.yaml config file.
&lt;denchmark-code&gt;2020-05-27 17:34:24,641 (json2trn:45) INFO: reading exp/train_960_pytorch_train_specaug/decode_test_other_model.val5.avg.best_decode_lm/data.json
2020-05-27 17:34:24,671 (json2trn:49) INFO: reading data/lang_char/train_960_unigram5000_units.txt
write a CER (or TER) result in exp/train_960_pytorch_train_specaug/decode_test_other_model.val5.avg.best_decode_lm/result.txt
|  SPKR         |  # Snt    # Wrd  |  Corr       Sub      Del       Ins      Err     S.Err  |
|  Sum/Avg      |  2939     65187  |  10.5      82.7      6.8     218.4    307.8     100.0  |
write a WER result in exp/train_960_pytorch_train_specaug/decode_test_other_model.val5.avg.best_decode_lm/result.wrd.txt
|  SPKR          |  # Snt   # Wrd   |  Corr       Sub      Del       Ins        Err     S.Err   |
|  Sum/Avg       |  2939    52343   |  11.0      82.6      6.4     251.9      340.9     100.0   |
2020-05-27 19:20:03,243 (concatjson:36) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/concatjson.py exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/data.1.json exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/data.2.json
2020-05-27 19:20:03,270 (concatjson:46) INFO: new json has 2864 utterances
2020-05-27 19:20:03,550 (json2trn:43) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/json2trn.py exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/data.json data/lang_char/train_960_unigram5000_units.txt --num-spkrs 1 --refs exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/ref.trn --hyps exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/hyp.trn
2020-05-27 19:20:03,551 (json2trn:45) INFO: reading exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/data.json
2020-05-27 19:20:03,578 (json2trn:49) INFO: reading data/lang_char/train_960_unigram5000_units.txt
write a CER (or TER) result in exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/result.txt
|  SPKR         |  # Snt    # Wrd  |  Corr      Sub       Del      Ins      Err     S.Err  |
|  Sum/Avg      |  2864     63116  |  10.8     83.6       5.6    229.4    318.6     100.0  |
write a WER result in exp/train_960_pytorch_train_specaug/decode_dev_other_model.val5.avg.best_decode_lm/result.wrd.txt
|  SPKR          |  # Snt    # Wrd  |  Corr       Sub       Del       Ins      Err     S.Err   |
|  Sum/Avg       |  2864     50948  |  11.2      83.1       5.7     266.6    355.3     100.0   |
2020-05-27 21:27:33,907 (concatjson:36) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/concatjson.py exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/data.1.json exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/data.2.json
2020-05-27 21:27:33,934 (concatjson:46) INFO: new json has 2703 utterances
2020-05-27 21:27:34,184 (json2trn:43) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/json2trn.py exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/data.json data/lang_char/train_960_unigram5000_units.txt --num-spkrs 1 --refs exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/ref.trn --hyps exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/hyp.trn
2020-05-27 21:27:34,184 (json2trn:45) INFO: reading exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/data.json
2020-05-27 21:27:34,213 (json2trn:49) INFO: reading data/lang_char/train_960_unigram5000_units.txt
write a CER (or TER) result in exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/result.txt
|  SPKR         |  # Snt    # Wrd  |  Corr      Sub       Del      Ins      Err     S.Err  |
|  Sum/Avg      |  2703     68066  |  12.0     83.7       4.3    258.7    346.8     100.0  |
write a WER result in exp/train_960_pytorch_train_specaug/decode_dev_clean_model.val5.avg.best_decode_lm/result.wrd.txt
|  SPKR          |  # Snt    # Wrd  |  Corr       Sub       Del       Ins      Err     S.Err   |
|  Sum/Avg       |  2703     54402  |  13.3      82.8       3.9     300.7    387.4     100.0   |
2020-05-27 21:35:06,958 (concatjson:36) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/concatjson.py exp/train_960_pytorch_train_specaug/decode_test_clean_model.val5.avg.best_decode_lm/data.1.json exp/train_960_pytorch_train_specaug/decode_test_clean_model.val5.avg.best_decode_lm/data.2.json
2020-05-27 21:35:06,985 (concatjson:46) INFO: new json has 2620 utterances
2020-05-27 21:35:07,218 (json2trn:43) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/librispeech/asr1/../../../utils/json2trn.py exp/train_960_pytorch_train_specaug/decode_test_clean_model.val5.avg.best_decode_lm/data.json data/lang_char/train_960_unigram5000_units.txt --num-spkrs 1 --refs exp/train_960_pytorch_train_specaug/decode_test_clean_model.val5.avg.best_decode_lm/ref.trn --hyps exp/train_960_pytorch_train_specaug/decode_test_clean_model.val5.avg.best_decode_lm/hyp.trn
2020-05-27 21:35:07,218 (json2trn:45) INFO: reading exp/train_960_pytorch_train_specaug/decode_test_clean_model.val5.avg.best_decode_lm/data.json
2020-05-27 21:35:07,245 (json2trn:49) INFO: reading data/lang_char/train_960_unigram5000_units.txt
write a CER (or TER) result in exp/train_960_pytorch_train_specaug/decode_test_clean_model.val5.avg.best_decode_lm/result.txt
|  SPKR         |  # Snt    # Wrd  |  Corr       Sub      Del       Ins      Err     S.Err  |
|  Sum/Avg      |  2620     65951  |  12.0      84.3      3.7     293.4    381.4     100.0  |
write a WER result in exp/train_960_pytorch_train_specaug/decode_test_clean_model.val5.avg.best_decode_lm/result.wrd.txt
|  SPKR          |  # Snt    # Wrd   |  Corr       Sub      Del       Ins       Err     S.Err   |
|  Sum/Avg       |  2620     52576   |  13.0      83.2      3.7     341.7     428.7     100.0   |
Finished
run.sh done.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='14' author='leo1980' date='2020-05-28T03:34:23Z'>
		Probably beam 5 is too small and you may need to increase it but I guess this issue comes from the others.
		</comment>
		<comment id='15' author='leo1980' date='2020-05-28T04:23:05Z'>
		I am wondering how long have the model was trained? From the previous log i saw that :
&lt;denchmark-code&gt;best val scores = [0.16489187 0.16076851 0.15817078 0.15651147 0.15507215]
selected epochs = [11 10 9 8 7]
&lt;/denchmark-code&gt;

the val scores are too low. They should reach at least 0.8. Try to train your model a little longer. Or download the pretrained model otherwise.
		</comment>
		<comment id='16' author='leo1980' date='2020-05-28T08:55:28Z'>
		
at

Original train.yaml config file contains epoch: 120 which is for me too long, so I changed it from 120 to 12. Even with epcho=12, It took my 2-3 days to finish. My graphic card is 'nvidia gtx 2070 super'. I don't if this is normal. If I train with epoch=120, then it gonna take my 20-30 days. If in this case I would like to download pre-trained model. For pre-trained model, where can I download it? Is there a tutorial available on how to pre-trained model to decode a wave file?
		</comment>
		<comment id='17' author='leo1980' date='2020-05-28T11:37:59Z'>
		Sadly, Librispeech is a large corpus and you need a lot of resources to train a model with this corpus. A model trained with 8 GPUs (RTX2080ti) will take 4~5 days. So, as you mentioned, it will take 30 days to train. A model trained for 30 epochs is already good enough (WER 10.0 aprox).
The pretrained model can be found in &lt;denchmark-link:https://github.com/espnet/espnet/blob/master/egs/librispeech/asr1/RESULTS.md&gt;https://github.com/espnet/espnet/blob/master/egs/librispeech/asr1/RESULTS.md&lt;/denchmark-link&gt;
. In model link
		</comment>
		<comment id='18' author='leo1980' date='2020-05-29T00:07:42Z'>
		BTW, you can fine a tuto here: &lt;denchmark-link:https://github.com/espnet/notebook/blob/master/pretrained.ipynb&gt;https://github.com/espnet/notebook/blob/master/pretrained.ipynb&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>