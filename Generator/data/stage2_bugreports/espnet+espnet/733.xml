<bug id='733' author='Fhrozen' open_date='2019-05-17T09:33:12Z' closed_time='2019-06-13T15:09:45Z'>
	<summary>GPU issues with new devices [RTX 2080Ti/V100] inside docker containers</summary>
	<description>
I've been having a problem to execute the training in a server with V100, for chainer with mtlalpha &gt; 0. I think that the code of WarpCTC is not supporting them.
I just got a new gpu (RTX2080Ti) and is showing the same issue:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/espnet/egs/an4/asr1/../../../espnet/bin/asr_train.py", line 274, in &lt;module&gt;
    main(sys.argv[1:])
  File "/espnet/egs/an4/asr1/../../../espnet/bin/asr_train.py", line 259, in main
    train(args)
  File "/espnet/espnet/asr/chainer_backend/asr.py", line 470, in train
    trainer.run()
  File "/espnet/tools/venv/lib/python3.7/site-packages/chainer/training/trainer.py", line 329, in run
    six.reraise(*sys.exc_info())
  File "/espnet/tools/venv/lib/python3.7/site-packages/six.py", line 693, in reraise
    raise value
  File "/espnet/tools/venv/lib/python3.7/site-packages/chainer/training/trainer.py", line 315, in run
    update()
  File "/espnet/tools/venv/lib/python3.7/site-packages/chainer/training/updaters/standard_updater.py", line 165, in update
    self.update_core()
  File "/espnet/espnet/asr/chainer_backend/asr.py", line 100, in update_core
    loss = optimizer.target(*x) / self.accum_grad
  File "/espnet/espnet/nets/chainer_backend/e2e_asr.py", line 90, in __call__
    loss_ctc = self.ctc(hs, ys)
  File "/espnet/espnet/nets/chainer_backend/ctc.py", line 95, in __call__
    self.loss = warp_ctc(y_hat, ilens, [cuda.to_cpu(l.data) for l in ys])[0]
  File "/espnet/tools/venv/lib/python3.7/site-packages/chainer_ctc/warpctc.py", line 125, in ctc
    return CTC(seq_lengths, labels)(x)
  File "/espnet/tools/venv/lib/python3.7/site-packages/chainer/function.py", line 235, in __call__
    ret = node.apply(inputs)
  File "/espnet/tools/venv/lib/python3.7/site-packages/chainer/function_node.py", line 263, in apply
    outputs = self.forward(in_data)
  File "/espnet/tools/venv/lib/python3.7/site-packages/chainer/function.py", line 135, in forward
    return self._function.forward(inputs)
  File "/espnet/tools/venv/lib/python3.7/site-packages/chainer_ctc/warpctc.py", line 77, in forward
    raise e
  File "/espnet/tools/venv/lib/python3.7/site-packages/chainer_ctc/warpctc.py", line 71, in forward
    _ctc()
  File "/espnet/tools/venv/lib/python3.7/site-packages/chainer_ctc/warpctc.py", line 68, in _ctc
    stream.ptr)
  File "chainer_ctc/src/warp_ctc.pyx", line 67, in chainer_ctc.src.warp_ctc.ctc_compute_ctc_loss_gpu
  File "chainer_ctc/src/warp_ctc.pyx", line 87, in chainer_ctc.src.warp_ctc.ctc_compute_ctc_loss_gpu
  File "chainer_ctc/src/warp_ctc.pyx", line 47, in chainer_ctc.src.warp_ctc.check_status
chainer_ctc.src.warp_ctc.CTCError: CTC_STATUS_EXECUTION_FAILED: b'execution failed'
&lt;/denchmark-code&gt;

I did this with docker cuda 10.0 ( on a pc with ubuntu 18.04 and cuda 10.1)
The training does not raise this error when the mtlalpha is set to 0.0 (but the WER is pretty high).
Also, I tried to test espnet with pytorch in the same GPU but i got the following error:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/espnet/egs/an4/asr1/../../../espnet/bin/asr_train.py", line 274, in &lt;module&gt;
    main(sys.argv[1:])
  File "/espnet/egs/an4/asr1/../../../espnet/bin/asr_train.py", line 262, in main
    train(args)
  File "/espnet/espnet/asr/pytorch_backend/asr.py", line 267, in train
    model = model.to(device)
  File "/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 381, in to
    return self._apply(convert)
  File "/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 187, in _apply
    module._apply(fn)
  File "/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 187, in _apply
    module._apply(fn)
  File "/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 187, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/rnn.py", line 117, in _apply
    self.flatten_parameters()
  File "/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/rnn.py", line 113, in flatten_parameters
    self.batch_first, bool(self.bidirectional))
RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED
&lt;/denchmark-code&gt;

The image is the same for executing Chainer so I suppose that there should not be any bad setup on cudnn (This was also tested in another pc before update the docker containers in the hub).
I will check the containers and upload a new ones, but I would like to know if this is may related to a specific device or not.
	</description>
	<comments>
		<comment id='1' author='Fhrozen' date='2019-05-17T10:26:01Z'>
		Actually I've been using 2080ti and v100 with pytorch and chainer but I have not met such errors. What is your CUDA related library versions?
		</comment>
		<comment id='2' author='Fhrozen' date='2019-05-17T10:42:40Z'>
		From chainer.print_report:
&lt;denchmark-code&gt;Platform: Linux-4.18.0-20-generic-x86_64-with-debian-stretch-sid
Chainer: 5.0.0
NumPy: 1.16.2
CuPy:
  CuPy Version          : 5.0.0
  CUDA Root             : /usr/local/cuda
  CUDA Build Version    : 10000
  CUDA Driver Version   : 10010
  CUDA Runtime Version  : 10000
  cuDNN Build Version   : 7401
  cuDNN Version         : 7401
  NCCL Build Version    : 2402
&lt;/denchmark-code&gt;

these are the versions inside the docker container. I did not try outside the container.
		</comment>
		<comment id='3' author='Fhrozen' date='2019-06-02T18:34:08Z'>
		I'm wondering how's is this issues going ?
		</comment>
		<comment id='4' author='Fhrozen' date='2019-06-07T05:20:25Z'>
		Just upgrade to pytorch-1.1.0 with cudatoolkit-10.0 but evaluation error occur
RuntimeError: set_storage is not allowed on Tensor created from .data or .detach()
		</comment>
		<comment id='5' author='Fhrozen' date='2019-06-07T07:24:48Z'>
		I add this lines to asr.py after version up pytorch 1.1.0 and wrap_CTC
&lt;denchmark-code&gt;from torch.nn.parallel.replicate import replicate
from torch.nn.parallel.data_parallel import DataParallel

class DataParallel_(DataParallel):
    def __init__(self, module, device_ids=None, output_device=None, dim=0):
        super(DataParallel_, self).__init__(module, device_ids=device_ids, output_device=output_device, dim=dim)

    def replicate(self, module, device_ids):
        return replicate(module, device_ids, not torch.is_grad_enabled())
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='Fhrozen' date='2019-06-07T22:43:50Z'>
		Thanks a lot! &lt;denchmark-link:https://github.com/Fhrozen&gt;@Fhrozen&lt;/denchmark-link&gt;
, does it solve your issue? I don't have RTX 2080Ti/V100, and cannot check it...
		</comment>
		<comment id='7' author='Fhrozen' date='2019-06-07T23:22:21Z'>
		I just tested again the containers:
In a server with ubuntu 16.04, V100, Cuda 10.0 (driver), I could run pytorch with mtlalpha 0.5 and there is no more trouble. However, the chainer backend is still showing the same error:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/espnet/egs/an4/asr1/../../../espnet/bin/asr_train.py", line 274, in &lt;module&gt;
    main(sys.argv[1:])
  File "/espnet/egs/an4/asr1/../../../espnet/bin/asr_train.py", line 259, in main
    train(args)
  File "/espnet/espnet/asr/chainer_backend/asr.py", line 470, in train
    trainer.run()
  File "/espnet/tools/venv/lib/python3.7/site-packages/chainer/training/trainer.py", line 329, in run
    six.reraise(*sys.exc_info())
  File "/espnet/tools/venv/lib/python3.7/site-packages/six.py", line 693, in reraise
    raise value
  File "/espnet/tools/venv/lib/python3.7/site-packages/chainer/training/trainer.py", line 315, in run
    update()
  File "/espnet/tools/venv/lib/python3.7/site-packages/chainer/training/updaters/standard_updater.py", line 165, in update
    self.update_core()
  File "/espnet/espnet/asr/chainer_backend/asr.py", line 100, in update_core
    loss = optimizer.target(*x) / self.accum_grad
  File "/espnet/espnet/nets/chainer_backend/e2e_asr.py", line 90, in __call__
    loss_ctc = self.ctc(hs, ys)
  File "/espnet/espnet/nets/chainer_backend/ctc.py", line 95, in __call__
    self.loss = warp_ctc(y_hat, ilens, [cuda.to_cpu(l.data) for l in ys])[0]
  File "/espnet/tools/venv/lib/python3.7/site-packages/chainer_ctc/warpctc.py", line 125, in ctc
    return CTC(seq_lengths, labels)(x)
  File "/espnet/tools/venv/lib/python3.7/site-packages/chainer/function.py", line 235, in __call__
    ret = node.apply(inputs)
  File "/espnet/tools/venv/lib/python3.7/site-packages/chainer/function_node.py", line 263, in apply
    outputs = self.forward(in_data)
  File "/espnet/tools/venv/lib/python3.7/site-packages/chainer/function.py", line 135, in forward
    return self._function.forward(inputs)
  File "/espnet/tools/venv/lib/python3.7/site-packages/chainer_ctc/warpctc.py", line 77, in forward
    raise e
  File "/espnet/tools/venv/lib/python3.7/site-packages/chainer_ctc/warpctc.py", line 71, in forward
    _ctc()
  File "/espnet/tools/venv/lib/python3.7/site-packages/chainer_ctc/warpctc.py", line 68, in _ctc
    stream.ptr)
  File "chainer_ctc/src/warp_ctc.pyx", line 67, in chainer_ctc.src.warp_ctc.ctc_compute_ctc_loss_gpu
  File "chainer_ctc/src/warp_ctc.pyx", line 87, in chainer_ctc.src.warp_ctc.ctc_compute_ctc_loss_gpu
  File "chainer_ctc/src/warp_ctc.pyx", line 47, in chainer_ctc.src.warp_ctc.check_status
chainer_ctc.src.warp_ctc.CTCError: CTC_STATUS_EXECUTION_FAILED: b'execution failed'
# Accounting: time=8 threads=1
# Ended (code 1) at Fri Jun  7 23:11:02 UTC 2019, elapsed time 8 seconds
&lt;/denchmark-code&gt;

I suppose that this has to do with the chainer_ctc instead of the container or the driver.
Then the other setup is having problem with both pytorch and chainer
in the pc with ubuntu 18.04, RTX2080Ti, CUDA 10.1 (driver), the container cannot run for both with mtlalpha &gt;0, and the pytorch backend (for any alpha) is still showing the same error:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/espnet/egs/an4/asr1/../../../espnet/bin/asr_train.py", line 274, in &lt;module&gt;
    main(sys.argv[1:])
  File "/espnet/egs/an4/asr1/../../../espnet/bin/asr_train.py", line 262, in main
    train(args)
  File "/espnet/espnet/asr/pytorch_backend/asr.py", line 269, in train
    model = model.to(device)
  File "/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 381, in to
    return self._apply(convert)
  File "/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 187, in _apply
    module._apply(fn)
  File "/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 187, in _apply
    module._apply(fn)
  File "/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 187, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/rnn.py", line 117, in _apply
    self.flatten_parameters()
  File "/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/rnn.py", line 113, in flatten_parameters
    self.batch_first, bool(self.bidirectional))
RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED
&lt;/denchmark-code&gt;

In this case, I supposes that this has to do with the CUDA driver installed in the pc and the cudnn version of the container.
For the chainer_ctc problem, it will be better to modify the git and make it download the one coded by jnishi. Raplacing this line &lt;denchmark-link:https://github.com/jheymann85/chainer_ctc/blob/ae521bf581b9725a2d4b250985ae8c768e17a85c/install_warp-ctc.sh#L5&gt;https://github.com/jheymann85/chainer_ctc/blob/ae521bf581b9725a2d4b250985ae8c768e17a85c/install_warp-ctc.sh#L5&lt;/denchmark-link&gt;

For the pytorch, i will test building a container with cuda 10.1 and try training inside it.
		</comment>
		<comment id='8' author='Fhrozen' date='2019-06-11T08:39:33Z'>
		I just finished with the research and found a possible solution for this error:
For cuda 10.0 and GTX-series seems that making warp-ctc with  has no problem, but for 10.1 and RTX (probably more the cuda version) this is no longer possible, so I took the recommendation from the code to build with   and it successfully executed the docker container for both chainer/pytorch test.  Both forks of warp-ctc for chainer/pytorch can be found here: &lt;denchmark-link:https://github.com/Fhrozen/warp-ctc&gt;https://github.com/Fhrozen/warp-ctc&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Fhrozen/chainer_ctc&gt;https://github.com/Fhrozen/chainer_ctc&lt;/denchmark-link&gt;

I will recommend &lt;denchmark-link:https://github.com/jnishi&gt;@jnishi&lt;/denchmark-link&gt;
  to modify this in his warp-ctc fork to support CUDA10.1.
Another solution could be replacing since __shfl_down and __shfl_up with __shfl_sync but it seems to require a large modification in the .cu and .hpp files.
Currently, this is solve, but need to fix in ESPnet code ;)
		</comment>
		<comment id='9' author='Fhrozen' date='2019-06-11T14:23:06Z'>
		
gencode arch=compute_60,code=sm_70

Could you create PR for this? ,and does this modification also work in CUDA10.0?
		</comment>
		<comment id='10' author='Fhrozen' date='2019-06-11T14:39:18Z'>
		I tested this in docker container build with CUDA 10.0, inside a PC with CUDA 10.1 drivers/runtime so I suppose that yes. Besides, this solution was taken from the builder itself. When the warp-ctc is building, it suggest to make it with compute_60 and sm_70 in order to use __shlf_down, i will PR this to your fork. But the chainer one will be still failing, because of it uses the original code that has not support yet for the new gpu (code_60 or 70)
		</comment>
		<comment id='11' author='Fhrozen' date='2019-06-12T14:13:58Z'>
		&lt;denchmark-link:https://github.com/sw005320&gt;@sw005320&lt;/denchmark-link&gt;
, Could you comment about this?
		</comment>
		<comment id='12' author='Fhrozen' date='2019-06-12T15:11:44Z'>
		Sorry... I just could not fully follow the discussion. For the chainer warp ctc, &lt;denchmark-link:https://github.com/jheymann85&gt;@jheymann85&lt;/denchmark-link&gt;
 may help. &lt;denchmark-link:https://github.com/jheymann85&gt;@jheymann85&lt;/denchmark-link&gt;
, could you help us with the new GPU support?
		</comment>
		<comment id='13' author='Fhrozen' date='2019-06-13T03:48:40Z'>
		&lt;denchmark-link:https://github.com/jheymann85&gt;@jheymann85&lt;/denchmark-link&gt;
 , do you mind changing the warp-ctc link from &lt;denchmark-link:https://github.com/baidu-research/warp-ctc.git&gt;https://github.com/baidu-research/warp-ctc.git&lt;/denchmark-link&gt;
 to the one forked by jnishi &lt;denchmark-link:https://github.com/jnishi/warp-ctc&gt;https://github.com/jnishi/warp-ctc&lt;/denchmark-link&gt;
, this last one has already updated to support new CUDA versions and new devices. This may help the problem for now.
		</comment>
		<comment id='14' author='Fhrozen' date='2019-06-13T04:39:43Z'>
		Done. Could you please check if that resolves the issue? I don't have access to the hardware.
		</comment>
		<comment id='15' author='Fhrozen' date='2019-06-13T04:48:30Z'>
		I just started to rebuild the containers. I will notify and close this issue once I got the results testing in V100 with CUDA 10.0 and RTX 2080Ti with CUDA 10.1.
		</comment>
		<comment id='16' author='Fhrozen' date='2019-06-13T15:09:45Z'>
		RTX with CUDA 10.1 now is working with and observation that the pytorch backend is still not supported. I tested a docker container with espent 0.4 and pytorch 1.0.1 and it has not problem. So suppose that this error will be resolve when v.0.4.0 is merged.
For V100 and can not run test for now, but the only problem was chainer so I suppose that the containers that run in the newer version should run with V100. I will try to test once the server with a V100 is free.
		</comment>
	</comments>
</bug>