<bug id='1921' author='ivanelgran23' open_date='2020-05-13T07:27:08Z' closed_time='2020-05-15T03:52:35Z'>
	<summary>Increased ASR performance on pre-trained model</summary>
	<description>
I recognize audio using the pre-trained ru_open_stt model. Unfortunately, it has low recognition optimization (10 second recording is recognized in 6-7 seconds) with parameters: --backend pytorch --api v2 --ngpu 1
I tried changing the parameters to --backend chainer --api v1 --ngpu 1 and in response I get an error:
&lt;denchmark-code&gt;# Started at Wed May 13 07:22:33 Asia 2020
#
WARNING:root:CTC was set to builtin due to PyTorch version.
Traceback (most recent call last):
  File "/usr/app/egs/ru_open_stt/asr1/../../../espnet/bin/asr_recog.py", line 209, in &lt;module&gt;
    main(sys.argv[1:])
  File "/usr/app/egs/ru_open_stt/asr1/../../../espnet/bin/asr_recog.py", line 180, in main
    recog(args)
  File "/usr/app/espnet/asr/chainer_backend/asr.py", line 378, in recog
    chainer_load(args.model, model)
  File "/usr/app/espnet/asr/asr_utils.py", line 462, in chainer_load
    chainer.serializers.load_npz(path, model)
  File "/usr/local/lib/python3.6/dist-packages/chainer/serializers/npz.py", line 239, in load_npz
    with numpy.load(file, **_allow_pickle_kwargs) as f:
AttributeError: __enter__
# Accounting: time=2 threads=1
# Ended (code 1) at Wed May 13 07:22:35 Asia 2020, elapsed time 2 seconds
&lt;/denchmark-code&gt;

Question 1: Can I somehow fix the error to use this model with such parameters?
Question 2: Are there any other ways to increase recognition performance?
	</description>
	<comments>
		<comment id='1' author='ivanelgran23' date='2020-05-15T01:23:13Z'>
		The model is packed with the pytorch one. You cannot change the backend to chainer.
		</comment>
		<comment id='2' author='ivanelgran23' date='2020-05-15T03:35:32Z'>
		Okay Are there ways to increase productivity with pytorch backend?
		</comment>
		<comment id='3' author='ivanelgran23' date='2020-05-15T03:51:24Z'>
		Sorry that it is too little information for me to give you advice.
In general, you could train the model with large amounts of training data matching your scenario.
		</comment>
		<comment id='4' author='ivanelgran23' date='2020-05-15T03:52:35Z'>
		Okay. Thank you to your help!
		</comment>
		<comment id='5' author='ivanelgran23' date='2020-05-15T12:17:11Z'>
		Hi, I guess you asked about recognition speed. Here is what you can try to make it to run faster.

Disable LM by adding --use_lang_model false to recog_wav.sh.
Increase batch size by adding batchsize: 8 to conf/decode.yaml. More is better, but if it's too large, you'll get "out of memory" errors.
Use CPU instead of GPU. Change OMP_NUM_THREADS in path.sh to number of CPU cores for using as much cores as possible. This also needs batchsize in conf/decode.yaml.

I haven't tried it myself, so I don't know if it will help.
		</comment>
		<comment id='6' author='ivanelgran23' date='2020-05-17T06:41:33Z'>
		
Hi, I guess you asked about recognition speed. Here is what you can try to make it to run faster.

Disable LM by adding --use_lang_model false to recog_wav.sh.
Increase batch size by adding batchsize: 8 to conf/decode.yaml. More is better, but if it's too large, you'll get "out of memory" errors.
Use CPU instead of GPU. Change OMP_NUM_THREADS in path.sh to number of CPU cores for using as much cores as possible. This also needs batchsize in conf/decode.yaml.

I haven't tried it myself, so I don't know if it will help.

Thanks for your reply!
The advice from the first paragraph really helped, and slightly reduced the time for recognizing audio. But here the other two do not work, because with any version of api an error is issued: Batch decoding is not implemented
Reducing to beam-size: 1 in decode.yaml file also helps a little. In this regard, are there any other options to affect performance?
		</comment>
		<comment id='7' author='ivanelgran23' date='2020-05-17T19:20:00Z'>
		Sorry, my knowledge about batch size was outdated. It is not needed in decode.yaml. But I did not find the error message Batch decoding is not implemented in the current ESPnet code. Maybe you need to update ESPnet.
You can also try to set ctc-weight: 0.0 in decode.yaml.
Also you can try dtype: float16 in decode.yaml, but it can be helpful only if GPU is new enough.
		</comment>
		<comment id='8' author='ivanelgran23' date='2020-06-25T22:15:55Z'>
		i also got the same error as &lt;denchmark-link:https://github.com/ivanelgran23&gt;@ivanelgran23&lt;/denchmark-link&gt;
 when i set batchsize larger than 0. Reducing  definitely helps (default is 60) but you may want to experiment the optimal trade-off between performance and speed.
		</comment>
	</comments>
</bug>