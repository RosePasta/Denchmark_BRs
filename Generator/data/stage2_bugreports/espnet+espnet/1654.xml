<bug id='1654' author='qmpzzpmq' open_date='2020-03-06T01:42:58Z' closed_time='2020-05-15T10:02:35Z'>
	<summary>conflict with use_transfer=true with conf/train_transformer_transducer.yaml</summary>
	<description>
Hi, when run with config file train_transformer_transducer.yaml copied from vivos egs in egs voxforge with variable use_tranfer=true and  type_transfer=enc or both.
there is bug said:
&lt;denchmark-code&gt;usage: asr_train.py [-h] [--config CONFIG] [--config2 CONFIG2]
                    [--config3 CONFIG3] [--ngpu NGPU]
                    [--train-dtype {float16,float32,float64,O0,O1,O2,O3}]
                    [--backend {chainer,pytorch}] --outdir OUTDIR
                    [--debugmode DEBUGMODE] --dict DICT [--seed SEED]
                    [--debugdir DEBUGDIR] [--resume [RESUME]]
                    [--minibatches MINIBATCHES] [--verbose VERBOSE]
                    [--tensorboard-dir [TENSORBOARD_DIR]]
                    [--report-interval-iters REPORT_INTERVAL_ITERS]
                    [--save-interval-iters SAVE_INTERVAL_ITERS]
                    [--train-json TRAIN_JSON] [--valid-json VALID_JSON]
                    [--model-module MODEL_MODULE] [--num-encs NUM_ENCS]
                    [--ctc_type {builtin,warpctc}] [--mtlalpha MTLALPHA]
                    [--lsm-weight LSM_WEIGHT] [--report-cer] [--report-wer]
                    [--nbest NBEST] [--beam-size BEAM_SIZE]
                    [--penalty PENALTY] [--maxlenratio MAXLENRATIO]
                    [--minlenratio MINLENRATIO] [--ctc-weight CTC_WEIGHT]
                    [--rnnlm RNNLM] [--rnnlm-conf RNNLM_CONF]
                    [--lm-weight LM_WEIGHT] [--sym-space SYM_SPACE]
                    [--sym-blank SYM_BLANK] [--sortagrad [SORTAGRAD]]
                    [--batch-count {auto,seq,bin,frame}]
                    [--batch-size BATCH_SIZE] [--batch-bins BATCH_BINS]
                    [--batch-frames-in BATCH_FRAMES_IN]
                    [--batch-frames-out BATCH_FRAMES_OUT]
                    [--batch-frames-inout BATCH_FRAMES_INOUT] [--maxlen-in ML]
                    [--maxlen-out ML] [--n-iter-processes N_ITER_PROCESSES]
                    [--preprocess-conf [PREPROCESS_CONF]]
                    [--opt {adadelta,adam,noam}] [--accum-grad ACCUM_GRAD]
                    [--eps EPS] [--eps-decay EPS_DECAY]
                    [--weight-decay WEIGHT_DECAY] [--criterion {loss,acc}]
                    [--threshold THRESHOLD] [--epochs EPOCHS]
                    [--early-stop-criterion [EARLY_STOP_CRITERION]]
                    [--patience [PATIENCE]] [--grad-clip GRAD_CLIP]
                    [--num-save-attention NUM_SAVE_ATTENTION]
                    [--grad-noise GRAD_NOISE] [--num-spkrs {1,2}]
                    [--context-residual [CONTEXT_RESIDUAL]]
                    [--enc-init ENC_INIT] [--enc-init-mods ENC_INIT_MODS]
                    [--dec-init DEC_INIT] [--dec-init-mods DEC_INIT_MODS]
                    [--use-frontend USE_FRONTEND] [--use-wpe USE_WPE]
                    [--wtype {lstm,blstm,lstmp,blstmp,vgglstmp,vggblstmp,vgglstm,vggblstm,gru,bgru,grup,bgrup,vgggrup,vggbgrup,vgggru,vggbgru}]
                    [--wlayers WLAYERS] [--wunits WUNITS] [--wprojs WPROJS]
                    [--wdropout-rate WDROPOUT_RATE] [--wpe-taps WPE_TAPS]
                    [--wpe-delay WPE_DELAY]
                    [--use-dnn-mask-for-wpe USE_DNN_MASK_FOR_WPE]
                    [--use-beamformer USE_BEAMFORMER]
                    [--btype {lstm,blstm,lstmp,blstmp,vgglstmp,vggblstmp,vgglstm,vggblstm,gru,bgru,grup,bgrup,vgggrup,vggbgrup,vgggru,vggbgru}]
                    [--blayers BLAYERS] [--bunits BUNITS] [--bprojs BPROJS]
                    [--badim BADIM] [--bnmask BNMASK]
                    [--ref-channel REF_CHANNEL]
                    [--bdropout-rate BDROPOUT_RATE] [--stats-file STATS_FILE]
                    [--apply-uttmvn APPLY_UTTMVN]
                    [--uttmvn-norm-means UTTMVN_NORM_MEANS]
                    [--uttmvn-norm-vars UTTMVN_NORM_VARS]
                    [--fbank-fs FBANK_FS] [--n-mels N_MELS]
                    [--fbank-fmin FBANK_FMIN] [--fbank-fmax FBANK_FMAX]
                    [--etype {lstm,blstm,lstmp,blstmp,vgglstmp,vggblstmp,vgglstm,vggblstm,gru,bgru,grup,bgrup,vgggrup,vggbgrup,vgggru,vggbgru}]
                    [--elayers ELAYERS] [--eunits EUNITS] [--eprojs EPROJS]
                    [--subsample SUBSAMPLE]
                    [--atype {noatt,dot,add,location,coverage,coverage_location,location2d,location_recurrent,multi_head_dot,multi_head_add,multi_head_loc,multi_head_multi_res_loc}]
                    [--adim ADIM] [--awin AWIN] [--aheads AHEADS]
                    [--aconv-chans ACONV_CHANS] [--aconv-filts ACONV_FILTS]
                    [--dropout-rate DROPOUT_RATE] [--dtype {lstm,gru}]
                    [--dlayers DLAYERS] [--dunits DUNITS]
                    [--dropout-rate-decoder DROPOUT_RATE_DECODER]
                    [--sampling-probability SAMPLING_PROBABILITY]
                    [--lsm-type [{,unigram}]]
asr_train.py: error: argument --etype: invalid choice: 'transformer' (choose from 'lstm', 'blstm', 'lstmp', 'blstmp', 'vgglstmp', 'vggblstmp', 'vgglstm', 'vggblstm', 'gru', 'bgru', 'grup', 'bgrup', 'vgggrup', 'vggbgrup', 'vgggru', 'vggbgru')
# Accounting: time=1 threads=1
# Ended (code 2) at Fri Mar  6 09:31:28 CST 2020, elapsed time 1 seconds
&lt;/denchmark-code&gt;

it is seems get_parser.py in the asr_train.py haven't include the "transformer type". It encoder of transformer couldn't be initialized by pure CTC.
	</description>
	<comments>
		<comment id='1' author='qmpzzpmq' date='2020-03-06T07:56:28Z'>
		Hi,
Sorry this script was not updated since transformer support was added. I'll take a closer look but I guess the problem in this case is that the model-module is not changed from espnet.nets.pytorch_backend.e2e_asr:E2E to espnet.nets.pytorch_backend.e2e_asr_transformer:E2E and arguments are not adapted in created config.
Finetuning options should work with transformer-transducer though, the problem is in  egs/voxforge/asr1/local/prep_transducer_finetuning.sh
		</comment>
		<comment id='2' author='qmpzzpmq' date='2020-03-06T11:00:38Z'>
		Hum, in fact I'm wondering if I should "fix" this as it's not really a bug.  was made to 1) highlight how to use finetuning/pre-training for RNN-T, 2) create needed files on-fly instead of adding all of them from scratch in conf dir) and 3) easily reproduce reported results for voxforge.
The script can be used with other recipes but it is not intended as a general pre-training tool/script (for multiple reasons we don't have that right now), thus  should be modified according to the main config and recipe.
&lt;denchmark-link:https://github.com/sw005320&gt;@sw005320&lt;/denchmark-link&gt;
 what do you think?
--
&lt;denchmark-link:https://github.com/qmpzzpmq&gt;@qmpzzpmq&lt;/denchmark-link&gt;
 in your case, the modification for the generated  should be :

Remove etype: transformer
Add model-module: espnet.nets.pytorch_backend.e2e_asr_transformer:E2E
Select another transformer-input-layer as VGG2L is only available for T-T.

If transfer is set to dec or both and rnnt_mode=rnnt, this is another story as the generated file will be a LM config.
		</comment>
		<comment id='3' author='qmpzzpmq' date='2020-03-06T12:53:54Z'>
		I will test it tomorrow. And give you the feedback
		</comment>
		<comment id='4' author='qmpzzpmq' date='2020-03-06T14:32:54Z'>
		Sorry, I found out there are another problems:

Transformer has --transformer-attn-dropout-rate whereas T-T has --transformer-attn-dropout-rate-encoder and --transformer-attn-dropout-rate-decoder (parameters shouldn't be copied during pre-training configs creation)
Another rule should be added with dtype: transformer and transfer_type=dec|both, as if rnnt_mode=rnnt will create a LM config and rnnt_mode=rnnt_att won't work (Transformer encoder + RNN-decoder w/ att is not allowed)
Default module keys to transfer in local/prep_finetuning_transducer.sh are not suited for transformer

And some more I may have overlooked by just looking at the files.
In fact, it bothers me so I'll also do some testing to add a recipe for T-T and mixed transducer + transfer learning. I'll keep you updated.
		</comment>
		<comment id='5' author='qmpzzpmq' date='2020-03-18T11:11:16Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/21037898/76954688-bd713c80-694b-11ea-8440-72732b51a7f3.png&gt;&lt;/denchmark-link&gt;

there is another bug, I am not certain sure. but when I initialize my RNNT main module from the enc pre-train model. It seems the name of the main module is mismatched with the pre-train module.
so I guess 
 and 
 should be  and 
&lt;denchmark-link:https://github.com/b-flo&gt;@b-flo&lt;/denchmark-link&gt;
 How do you think that? I can do a PR
		</comment>
		<comment id='6' author='qmpzzpmq' date='2020-03-18T11:32:07Z'>
		Ha yes, when I did unification for RNN and Transformer I used  instead of  but didn't change default modules parameters to transfer (&lt;denchmark-link:https://github.com/espnet/espnet/blob/master/espnet/bin/asr_train.py#L182-L184&gt;https://github.com/espnet/espnet/blob/master/espnet/bin/asr_train.py#L182-L184&lt;/denchmark-link&gt;
), you can specify the correct one(s) in conf though!
I'll soon do a PR to extend transfer learning examples and also add more documentation. Sorry for the delay, it's mostly done but I have to take care of some high priority works outside of espnet and I can't find time right now.
		</comment>
		<comment id='7' author='qmpzzpmq' date='2020-03-18T12:06:45Z'>
		Just make it clear. &lt;denchmark-link:https://github.com/b-flo&gt;@b-flo&lt;/denchmark-link&gt;

I think I can just rename the weight name in "model.acc.best" and change the conf file. I guess these works, isn't it?
		</comment>
		<comment id='8' author='qmpzzpmq' date='2020-03-18T12:15:10Z'>
		Oh sorry, you mean you already had a transducer model from before unification. In that case, you could change self.encoder to self.enc and self.decoder to self.dec as you said previously or change the model keys before loading (enc.* -&gt; encoder.*, dec.* -&gt; decoder.*), yes.
		</comment>
	</comments>
</bug>