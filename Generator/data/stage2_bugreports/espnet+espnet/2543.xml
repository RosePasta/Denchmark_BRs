<bug id='2543' author='gafsd' open_date='2020-10-01T14:09:29Z' closed_time='2020-10-01T14:39:08Z'>
	<summary>CUDNN_STATUS_EXECUTION_FAILED when using token_type=char</summary>
	<description>
Describe the bug
I am training tacotron TTS with GST and using token_type=char I got this crash
Basic environments:

OS information: Linux 936c828f17cb 4.15.0-106-generic #107~16.04.1-Ubuntu SMP
python version:  3.7.7
pytorch version 1.6.0

Task information:

Task: TTS
Recipe: ljspeech + GST
ESPnet2 0.9.4
1x RTX 2080Ti
docker pytorch 1.6.0-CUDA 10.1 VM

Error logs
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/opt/conda/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/opt/conda/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/root/espnet/espnet2/bin/tts_train.py", line 22, in &lt;module&gt;
    main()
  File "/root/espnet/espnet2/bin/tts_train.py", line 18, in main
    TTSTask.main(cmd=cmd)
  File "/root/espnet/espnet2/tasks/abs_task.py", line 941, in main
    cls.main_worker(args)
  File "/root/espnet/espnet2/tasks/abs_task.py", line 1233, in main_worker
    distributed_option=distributed_option,
  File "/root/espnet/espnet2/train/trainer.py", line 210, in run
    options=trainer_options,
  File "/root/espnet/espnet2/train/trainer.py", line 383, in train_one_epoch
    loss, stats, weight = model(**batch)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/root/espnet/espnet2/tts/espnet_model.py", line 109, in forward
    **kwargs,
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/root/espnet/espnet2/tts/tacotron2.py", line 303, in forward
    xs, ilens, ys, olens, spembs
  File "/root/espnet/espnet2/tts/tacotron2.py", line 358, in _forward
    hs, hlens = self.enc(xs, ilens)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/root/espnet/espnet/nets/pytorch_backend/tacotron2/encoder.py", line 155, in forward
    xs, _ = self.blstm(xs)  # (B, Tmax, C)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py", line 580, in forward
    self.num_layers, self.dropout, self.training, self.bidirectional)
RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='gafsd' date='2020-10-01T14:17:29Z'>
		I think this is not related to token_type.
Or did you check whether token_type=phn works?
Maybe your nvidia driver or cuda version problem.
Please give us the information using torch.utils.collect_env as the template said.
&lt;denchmark-code&gt;cd &lt;espnet-root&gt;/tools
. ./activate_python.sh
python3 -m torch.utils.collect_env
&lt;/denchmark-code&gt;

See also: &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/17543&gt;pytorch/pytorch#17543&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='gafsd' date='2020-10-01T14:17:51Z'>
		In general, CUDNN_STATUS_EXECUTION_FAILED can be caused by two reason.

OOM error =&gt; try smaller batch_size or smaller network.
The other (The installation problem of CUDNN, some bugs of pytorch, etc. )

If 2, I recommend you trying it with disabling cudnn. If the problem of cudnn, there are a few things we can.
		</comment>
		<comment id='3' author='gafsd' date='2020-10-01T14:20:08Z'>
		I check with token_type=phn before and it was working even with different g2p, so I thought maybe related to token_type=char. I will try with a fresh VM.
		</comment>
		<comment id='4' author='gafsd' date='2020-10-01T14:21:37Z'>
		
I check with token_type=phn before and it was working even with different g2p, so I thought maybe related to token_type=char. I will try with a fresh VM.

Then, maybe OOM error is related as &lt;denchmark-link:https://github.com/kamo-naoyuki&gt;@kamo-naoyuki&lt;/denchmark-link&gt;
 said.
Please try with smaller batch size (or batch bins).
		</comment>
		<comment id='5' author='gafsd' date='2020-10-01T14:39:08Z'>
		I try with new VM and error is gone. Maybe OS/hardware issue. Sorry for waste you time, never seen that error before
		</comment>
	</comments>
</bug>