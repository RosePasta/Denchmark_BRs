<bug id='2084' author='psridhar-asapp' open_date='2020-06-23T22:20:46Z' closed_time='2020-09-20T16:06:06Z'>
	<summary>RuntimeError: arguments are located on different GPUs</summary>
	<description>
Hi I ran into the following issue when training an RNN-T model with 8 GPUs. Both the encoder and decoder are stacked LSTMs. I asked espnet to report cer/wer. The error always happens at that step. Here is the full stack
Traceback (most recent call last):
File "/home/psridhar/espnet/egs/lima/asr1/../../../espnet/bin/asr_train.py", line 628, in 
main(sys.argv[1:])
File "/home/psridhar/espnet/egs/lima/asr1/../../../espnet/bin/asr_train.py", line 614, in main
train(args)
File "/home/psridhar/espnet/espnet/asr/pytorch_backend/asr.py", line 833, in train
trainer.run()
File "/home/psridhar/espnet/tools/venv/lib/python3.7/site-packages/chainer/training/trainer.py", line 349, in run
six.reraise(*exc_info)
File "/home/psridhar/espnet/tools/venv/lib/python3.7/site-packages/six.py", line 703, in reraise
raise value
File "/home/psridhar/espnet/tools/venv/lib/python3.7/site-packages/chainer/training/trainer.py", line 319, in run
entry.extension(self)
File "/home/psridhar/espnet/espnet/utils/training/evaluator.py", line 10, in call
ret = super().call(trainer)
File "/home/psridhar/espnet/tools/venv/lib/python3.7/site-packages/chainer/training/extensions/evaluator.py", line 161, in call
result = self.evaluate()
File "/home/psridhar/espnet/espnet/asr/pytorch_backend/asr.py", line 134, in evaluate
data_parallel(self.model, x, range(self.ngpu))
File "/home/psridhar/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 188, in data_parallel
outputs = parallel_apply(replicas, inputs, module_kwargs, used_device_ids)
File "/home/psridhar/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in parallel_apply
raise output
File "/home/psridhar/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 59, in _worker
output = module(*input, **kwargs)
File "/home/psridhar/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 489, in call
result = self.forward(*input, **kwargs)
File "/home/psridhar/espnet/espnet/nets/pytorch_backend/e2e_asr_transducer.py", line 426, in forward
cer, wer = self.error_calculator(hs_pad, ys_pad)
File "/home/psridhar/espnet/espnet/nets/e2e_asr_common.py", line 305, in call
nbest_hyps = self.dec.recognize_beam(hs_pad[b], self.recog_args)
File "/home/psridhar/espnet/espnet/nets/pytorch_backend/transducer/rnn_decoders.py", line 268, in recognize_beam
ytu = F.log_softmax(self.joint(hi, y[0]), dim=0)
File "/home/psridhar/espnet/espnet/nets/pytorch_backend/transducer/rnn_decoders.py", line 143, in joint
z = torch.tanh(self.lin_enc(h_enc) + self.lin_dec(h_dec))
File "/home/psridhar/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 489, in call
result = self.forward(*input, **kwargs)
File "/home/psridhar/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 67, in forward
return F.linear(input, self.weight, self.bias)
File "/home/psridhar/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/functional.py", line 1354, in linear
output = input.matmul(weight.t())
RuntimeError: arguments are located on different GPUs at /opt/conda/conda-bld/pytorch_1549636813070/work/aten/src/THC/generic/THCTensorMathBlas.cu:253
	</description>
	<comments>
		<comment id='1' author='psridhar-asapp' date='2020-06-24T00:14:26Z'>
		I just want to figure out the problem.
Does it happen without cer/wer reporters?
Sorry that we don't fully test the multiple GPU decoding.
		</comment>
		<comment id='2' author='psridhar-asapp' date='2020-06-24T04:34:15Z'>
		It does not happen without CER/WER reporters. No worries. I'm glad to help debug things. Doing some googling it looks like this is a problem when .to() or cuda() is used improperly
&lt;denchmark-link:https://discuss.pytorch.org/t/dataparallel-arguments-are-located-on-different-gpus/42054&gt;https://discuss.pytorch.org/t/dataparallel-arguments-are-located-on-different-gpus/42054&lt;/denchmark-link&gt;

Not sure where in the recog step this is happening
		</comment>
		<comment id='3' author='psridhar-asapp' date='2020-08-01T07:23:37Z'>
		Hi,
Sorry didn't see this one. I'll fix that in my PR!
		</comment>
		<comment id='4' author='psridhar-asapp' date='2020-09-20T16:06:06Z'>
		Fixed in &lt;denchmark-link:https://github.com/espnet/espnet/pull/2444&gt;#2444&lt;/denchmark-link&gt;
. Sorry it took so long.
		</comment>
	</comments>
</bug>