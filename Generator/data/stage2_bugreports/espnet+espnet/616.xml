<bug id='616' author='kan-bayashi' open_date='2019-02-27T16:09:11Z' closed_time='2019-02-28T01:14:04Z'>
	<summary>Conversion of AttributeDict with vars() returns unexpected results</summary>
	<description>
I found a bug.
In training phase, train_args is argparse.Namespace.
So vars(train_args) convert into dict as follows.
(Pdb) train_args
Namespace(aconv_chans=10, aconv_filts=100, adim=320, aheads=4, asr_model=False, atype='location', awin=5, backend='pytorch', batch_size=30, beam_size=4, char_list=['&lt;blank&gt;', '&lt;unk&gt;', '&lt;space&gt;', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '&lt;eos&gt;'], criterion='acc', ctc_type='warpctc', ctc_weight=0.3, debugdir='exp/train_nodev_pytorch_blstmp_e4_subsample1_2_2_1_1_unit320_proj320_d1_unit300_location_aconvc10_aconvf100_mtlalpha0.5_adadelta_sampprob0.0_bs30_mli800_mlo150', debugmode=1, dict='data/lang_1char/train_nodev_units.txt', dlayers=1, dropout_rate=0.0, dropout_rate_decoder=0.0, dtype='lstm', dunits=300, early_stop_criterion='validation/main/acc', elayers=4, elayers_sd=4, epochs=20, eprojs=320, eps=1e-08, eps_decay=0.01, etype='blstmp', eunits=320, grad_clip=5, lm_weight=0.1, lsm_type='', lsm_weight=0.0, maxlen_in=800, maxlen_out=150, maxlenratio=0.0, minibatches=0, minlenratio=0.0, mt_model=False, mtlalpha=0.5, n_iter_processes=0, nbest=1, ngpu=1, num_save_attention=3, num_spkrs=1, opt='adadelta', outdir='exp/train_nodev_pytorch_blstmp_e4_subsample1_2_2_1_1_unit320_proj320_d1_unit300_location_aconvc10_aconvf100_mtlalpha0.5_adadelta_sampprob0.0_bs30_mli800_mlo150/results', patience=3, penalty=0.0, preprocess_conf=None, report_cer=False, report_wer=False, resume=None, rnnlm=None, rnnlm_conf=None, sampling_probability=0.0, seed=1, sortagrad=0, spa=False, subsample='1_2_2_1_1', sym_blank='&lt;blank&gt;', sym_space='&lt;space&gt;', tensorboard_dir='tensorboard/train_nodev_pytorch_blstmp_e4_subsample1_2_2_1_1_unit320_proj320_d1_unit300_location_aconvc10_aconvf100_mtlalpha0.5_adadelta_sampprob0.0_bs30_mli800_mlo150', threshold=0.0001, train_json='dump/train_nodev/deltafalse/data.json', valid_json='dump/train_dev/deltafalse/data.json', verbose=1, weight_decay=0.0)


(Pdb) vars(train_args)
{'aconv_chans': 10, 'aconv_filts': 100, 'adim': 320, 'aheads': 4, 'asr_model': False, 'atype': 'location', 'awin': 5, 'backend': 'pytorch', 'batch_size': 30, 'beam_size': 4, 'char_list': ['&lt;blank&gt;', '&lt;unk&gt;', '&lt;space&gt;', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '&lt;eos&gt;'], 'criterion': 'acc', 'ctc_type': 'warpctc', 'ctc_weight': 0.3, 'debugdir': 'exp/train_nodev_pytorch_blstmp_e4_subsample1_2_2_1_1_unit320_proj320_d1_unit300_location_aconvc10_aconvf100_mtlalpha0.5_adadelta_sampprob0.0_bs30_mli800_mlo150', 'debugmode': 1, 'dict': 'data/lang_1char/train_nodev_units.txt', 'dlayers': 1, 'dropout_rate': 0.0, 'dropout_rate_decoder': 0.0, 'dtype': 'lstm', 'dunits': 300, 'early_stop_criterion': 'validation/main/acc', 'elayers': 4, 'elayers_sd': 4, 'epochs': 20, 'eprojs': 320, 'eps': 1e-08, 'eps_decay': 0.01, 'etype': 'blstmp', 'eunits': 320, 'grad_clip': 5, 'lm_weight': 0.1, 'lsm_type': '', 'lsm_weight': 0.0, 'maxlen_in': 800, 'maxlen_out': 150, 'maxlenratio': 0.0, 'minibatches': 0, 'minlenratio': 0.0, 'mt_model': False, 'mtlalpha': 0.5, 'n_iter_processes': 0, 'nbest': 1, 'ngpu': 1, 'num_save_attention': 3, 'num_spkrs': 1, 'opt': 'adadelta', 'outdir': 'exp/train_nodev_pytorch_blstmp_e4_subsample1_2_2_1_1_unit320_proj320_d1_unit300_location_aconvc10_aconvf100_mtlalpha0.5_adadelta_sampprob0.0_bs30_mli800_mlo150/results', 'patience': 3, 'penalty': 0.0, 'preprocess_conf': None, 'report_cer': False, 'report_wer': False, 'resume': None, 'rnnlm': None, 'rnnlm_conf': None, 'sampling_probability': 0.0, 'seed': 1, 'sortagrad': 0, 'spa': False, 'subsample': '1_2_2_1_1', 'sym_blank': '&lt;blank&gt;', 'sym_space': '&lt;space&gt;', 'tensorboard_dir': 'tensorboard/train_nodev_pytorch_blstmp_e4_subsample1_2_2_1_1_unit320_proj320_d1_unit300_location_aconvc10_aconvf100_mtlalpha0.5_adadelta_sampprob0.0_bs30_mli800_mlo150', 'threshold': 0.0001, 'train_json': 'dump/train_nodev/deltafalse/data.json', 'valid_json': 'dump/train_dev/deltafalse/data.json', 'verbose': 1, 'weight_decay': 0.0}
However, in the testing phase, loaded train_args is AttributeDict.
Therefore, vars(train_args) return different results.
(Pdb) train_args
&lt;espnet.asr.asr_utils.AttributeDict object at 0x7f2323130a58&gt;

(Pdb) vars(train_args)
{'obj': {'aconv_chans': 10, 'aconv_filts': 100, 'adim': 320, 'aheads': 4, 'asr_model': False, 'atype': 'location', 'awin': 5, 'backend': 'pytorch', 'batch_size': 30, 'beam_size': 4, 'char_list': ['&lt;blank&gt;', '&lt;unk&gt;', '&lt;space&gt;', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '&lt;eos&gt;'], 'criterion': 'acc', 'ctc_type': 'warpctc', 'ctc_weight': 0.3, 'debugdir': 'exp/train_nodev_pytorch_blstmp_e4_subsample1_2_2_1_1_unit320_proj320_d1_unit300_location_aconvc10_aconvf100_mtlalpha0.5_adadelta_sampprob0.0_bs30_mli800_mlo150', 'debugmode': 1, 'dict': 'data/lang_1char/train_nodev_units.txt', 'dlayers': 1, 'dropout_rate': 0.0, 'dropout_rate_decoder': 0.0, 'dtype': 'lstm', 'dunits': 300, 'early_stop_criterion': 'validation/main/acc', 'elayers': 4, 'elayers_sd': 4, 'epochs': 20, 'eprojs': 320, 'eps': 1e-08, 'eps_decay': 0.01, 'etype': 'blstmp', 'eunits': 320, 'grad_clip': 5, 'lm_weight': 0.1, 'lsm_type': '', 'lsm_weight': 0.0, 'maxlen_in': 800, 'maxlen_out': 150, 'maxlenratio': 0.0, 'minibatches': 0, 'minlenratio': 0.0, 'mt_model': False, 'mtlalpha': 0.5, 'n_iter_processes': 0, 'nbest': 1, 'ngpu': 1, 'num_save_attention': 3, 'num_spkrs': 1, 'opt': 'adadelta', 'outdir': 'exp/train_nodev_pytorch_blstmp_e4_subsample1_2_2_1_1_unit320_proj320_d1_unit300_location_aconvc10_aconvf100_mtlalpha0.5_adadelta_sampprob0.0_bs30_mli800_mlo150/results', 'patience': 3, 'penalty': 0.0, 'preprocess_conf': None, 'report_cer': False, 'report_wer': False, 'resume': None, 'rnnlm': None, 'rnnlm_conf': None, 'sampling_probability': 0.0, 'seed': 1, 'sortagrad': 0, 'spa': False, 'subsample': '1_2_2_1_1', 'sym_blank': '&lt;blank&gt;', 'sym_space': '&lt;space&gt;', 'tensorboard_dir': 'tensorboard/train_nodev_pytorch_blstmp_e4_subsample1_2_2_1_1_unit320_proj320_d1_unit300_location_aconvc10_aconvf100_mtlalpha0.5_adadelta_sampprob0.0_bs30_mli800_mlo150', 'threshold': 0.0001, 'train_json': 'dump/train_nodev/deltafalse/data.json', 'valid_json': 'dump/train_dev/deltafalse/data.json', 'verbose': 1, 'weight_decay': 0.0}}
This causes unexpected behavior in following line.



espnet/espnet/nets/pytorch_backend/ctc.py


         Line 116
      in
      fb1cbd6






 ctc_type=vars(args).get('ctc_type', 'builtin'), reduce=reduce) 





vars(train_args).get(“ctc_type”) always return None, so vars(train_args).get(“ctc_type”, “builtin”) will always return “builtin”.
&lt;denchmark-link:https://github.com/gtache&gt;@gtache&lt;/denchmark-link&gt;
 Is there any reason why using  instead of ?
&lt;denchmark-link:https://github.com/sw005320&gt;@sw005320&lt;/denchmark-link&gt;
 What is your intension of using  in loading a config file?
	</description>
	<comments>
		<comment id='1' author='kan-bayashi' date='2019-02-27T16:18:34Z'>
		Well it looks like it is @simploier &lt;denchmark-link:https://github.com/espnet/espnet/commit/86cd16bd05934ee8791698906ed7a38624e486c4&gt;code&lt;/denchmark-link&gt;
, so I can't really answer that. But given that we have access to  here  makes more sense to me yes.
		</comment>
		<comment id='2' author='kan-bayashi' date='2019-02-27T16:20:12Z'>
		Oh sorry, &lt;denchmark-link:https://github.com/gtache&gt;@gtache&lt;/denchmark-link&gt;
.
&lt;denchmark-link:https://github.com/simpleoier&gt;@simpleoier&lt;/denchmark-link&gt;
 Do you any idea?
		</comment>
		<comment id='3' author='kan-bayashi' date='2019-02-27T16:21:31Z'>
		My bad, it's &lt;denchmark-link:https://github.com/kamo-naoyuki&gt;@kamo-naoyuki&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/espnet/espnet/commit/95dec2c6d167cd289f51d054f4b610681175f3a8&gt;in fact&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/simpleoier&gt;@simpleoier&lt;/denchmark-link&gt;
 only merged it I think
		</comment>
		<comment id='4' author='kan-bayashi' date='2019-02-28T00:11:22Z'>
		I see. It may not necessary to use get(), I'll fix it.
So, I found that my previous result at &lt;denchmark-link:https://github.com/espnet/espnet/pull/553&gt;#553&lt;/denchmark-link&gt;
 is not correct for this bug.
(Using warpctc at training phase,  while using pytorch.CTC at test phase.)
		</comment>
		<comment id='5' author='kan-bayashi' date='2019-02-28T01:10:47Z'>
		Thanks, &lt;denchmark-link:https://github.com/kamo-naoyuki&gt;@kamo-naoyuki&lt;/denchmark-link&gt;
.
But in testing phase,  is not called.
Just use  and , so your results are correct, I think.



espnet/espnet/nets/pytorch_backend/e2e_asr.py


        Lines 317 to 318
      in
      1e1a928






 if recog_args.ctc_weight &gt; 0.0: 



 lpz = self.ctc.log_softmax(h)[0] 








espnet/espnet/nets/pytorch_backend/ctc.py


        Lines 88 to 95
      in
      fb1cbd6






 def log_softmax(self, hs_pad): 



 """log_softmax of frame activations 



  



         :param torch.Tensor hs_pad: 3d tensor (B, Tmax, eprojs) 



         :return: log softmax applied 3d tensor (B, Tmax, odim) 



         :rtype: torch.Tensor 



         """ 



 return F.log_softmax(self.ctc_lo(hs_pad), dim=2) 








espnet/espnet/nets/pytorch_backend/decoders.py


        Lines 261 to 269
      in
      fb1cbd6






 if lpz is not None: 



 ctc_prefix_score = CTCPrefixScore(lpz.detach().numpy(), 0, self.eos, np) 



 hyp['ctc_state_prev'] = ctc_prefix_score.initial_state() 



 hyp['ctc_score_prev'] = 0.0 



 if ctc_weight != 1.0: 



 # pre-pruning based on attention scores 



 ctc_beam = min(lpz.shape[-1], int(beam * CTC_SCORING_RATIO)) 



 else: 



 ctc_beam = lpz.shape[-1] 





		</comment>
		<comment id='6' author='kan-bayashi' date='2019-02-28T01:14:39Z'>
		Fixed &lt;denchmark-link:https://github.com/espnet/espnet/pull/617&gt;#617&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/espnet/espnet/pull/618&gt;#618&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='kan-bayashi' date='2019-02-28T12:49:49Z'>
		&lt;denchmark-link:https://github.com/simpleoier&gt;@simpleoier&lt;/denchmark-link&gt;
 told me that we could also solve it by
&lt;denchmark-code&gt;ctc_type=vars(args).get('ctc_type', 'builtin'), reduce=reduce) 
&lt;/denchmark-code&gt;

to
&lt;denchmark-code&gt;ctc_type=vars(args).get(u'ctc_type', 'builtin'), reduce=reduce)
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>