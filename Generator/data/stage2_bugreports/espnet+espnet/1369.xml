<bug id='1369' author='kaituoxu' open_date='2019-11-18T08:40:01Z' closed_time='2020-08-28T21:15:36Z'>
	<summary>Transformer on Librispeech get RuntimeError: cuDNN error: CUDNN_STATUS_BAD_PARAM</summary>
	<description>
Hello, when I use Librispeech's config to train Transformer model on another dataset, I get RuntimeError: cuDNN error: CUDNN_STATUS_BAD_PARAM after about 500 iterations when I use 8 GPU. I can run the config successfully when I use 4GPU. The detailed traceback as follows:
&lt;denchmark-code&gt;Exception in main training loop: cuDNN error: CUDNN_STATUS_BAD_PARAM
Traceback (most recent call last):                                                                                                                                                              [45/368]
  File "/home/xukaituo/Project/espnet/egs/aishell2/asr1/../../../espnet/bin/asr_train.py", line 368, in &lt;module&gt;
    main(sys.argv[1:])
  File "/home/xukaituo/Project/espnet/egs/aishell2/asr1/../../../espnet/bin/asr_train.py", line 349, in main
    train(args)
  File "/home/xukaituo/Project/espnet/espnet/asr/pytorch_backend/asr.py", line 623, in train
    trainer.run()
  File "/home/xukaituo/Project/espnet/tools/venv/lib/python3.7/site-packages/chainer/training/trainer.py", line 349, in run
    six.reraise(*exc_info)
  File "/home/xukaituo/Project/espnet/tools/venv/lib/python3.7/site-packages/six.py", line 693, in reraise
    raise value
  File "/home/xukaituo/Project/espnet/tools/venv/lib/python3.7/site-packages/chainer/training/trainer.py", line 316, in run
    update()
  File "/home/xukaituo/Project/espnet/espnet/asr/pytorch_backend/asr.py", line 218, in update
    self.update_core()
  File "/home/xukaituo/Project/espnet/espnet/asr/pytorch_backend/asr.py", line 187, in update_core
    loss = data_parallel(self.model, x, range(self.ngpu)).mean() / self.accum_grad
  File "/home/xukaituo/Project/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 188, in data_parallel
    outputs = parallel_apply(replicas, inputs, module_kwargs, used_device_ids)
  File "/home/xukaituo/Project/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in parallel_apply
    raise output
  File "/home/xukaituo/Project/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 59, in _worker
    output = module(*input, **kwargs)
  File "/home/xukaituo/Project/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/xukaituo/Project/espnet/espnet/nets/pytorch_backend/e2e_asr_transformer.py", line 159, in forward
    hs_pad, hs_mask = self.encoder(xs_pad, src_mask)
  File "/home/xukaituo/Project/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/xukaituo/Project/espnet/espnet/nets/pytorch_backend/transformer/encoder.py", line 113, in forward
    xs, masks = self.embed(xs, masks)
  File "/home/xukaituo/Project/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/xukaituo/Project/espnet/espnet/nets/pytorch_backend/transformer/subsampling.py", line 36, in forward
    x = self.conv(x)
  File "/home/xukaituo/Project/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/xukaituo/Project/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/container.py", line 92, in forward
    input = module(input)
  File "/home/xukaituo/Project/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/xukaituo/Project/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/conv.py", line 320, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: cuDNN error: CUDNN_STATUS_BAD_PARAM
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;## Environments
- date: `Mon Nov 18 16:32:38 CST 2019`
- python version: `3.7.3 (default, Mar 27 2019, 22:11:17)  [GCC 7.3.0]`
- espnet version: `espnet 0.5.3`
- chainer version: `chainer 6.0.0`
- pytorch version: `pytorch 1.0.1.post2`
- Git hash: `42dd17e382157e1038787c5f8dae1ecf360797c3`
- Commit date: `Tue Oct 22 12:07:23 2019 +0900`
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='kaituoxu' date='2019-11-18T10:38:59Z'>
		It seems sometime the input x of x = self.conv(x) will be
&lt;denchmark-code&gt;2019-11-18 18:37:10,316 (subsampling:38) WARNING: torch.Size([1, 1, 1, 80])
&lt;/denchmark-code&gt;

Related issue and solution: &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/15446&gt;pytorch/pytorch#15446&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='kaituoxu' date='2019-11-18T23:34:01Z'>
		Thanks for the report.
I cannot fully understand it because it works with 4 GPUs....
Could you specify which minibatch would cause this issue and report the shape of this?
		</comment>
		<comment id='3' author='kaituoxu' date='2019-11-19T01:52:40Z'>
		For example, one minibatch is [8, 100, 80] (N, T, D), one sample of this minibatch is [1, 1, 80] so that it pad 99 along time axis, other samples are [1, 100, 80].
If split this minibatch between 4 gpus, each gpu will get [2, 100, 80], so there is no input's T less than conv's filter size(e.g. 3) .
But if split this minibatch betwwen 8 gpus, one gpu will get [1, 1, 80] (T=1), so on this gpu, the input's T is less than conv's filter size (e.g. 3), which will cause this PyTorch bug.
		</comment>
		<comment id='4' author='kaituoxu' date='2019-11-19T13:12:02Z'>
		Thanks for the clarification, but I'm not sure yet why it becomes [1, 1, 80] (T=1)? Do we have such very short samples or bugs?
		</comment>
		<comment id='5' author='kaituoxu' date='2019-11-20T08:50:31Z'>
		A sample in my dataset is [1, 1, 80]. Maybe it's a failure from feature extraction.
		</comment>
		<comment id='6' author='kaituoxu' date='2019-11-20T12:01:14Z'>
		You could remove the very short utterance by &lt;denchmark-link:https://github.com/espnet/espnet/blob/master/utils/remove_longshortdata.sh&gt;https://github.com/espnet/espnet/blob/master/utils/remove_longshortdata.sh&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>