<bug id='1203' author='CherrieWang97' open_date='2019-09-18T11:03:43Z' closed_time='2020-08-28T21:15:03Z'>
	<summary>Some bugs in training</summary>
	<description>
I find some bugs in mt.py and asr.py
In branch master, the code in espnet/mt/pytorch_backend/mt.py line 225-226:
 updater = CustomUpdater( model, args.grad_clip, train_iter, optimizer, converter, device, args.ngpu, args.accum_grad)
It lacks parameter grad_noise as the initialization function for CustomUpdater is defined as:
def __init__(self, model, grad_clip_threshold, train_iter, optimizer, converter, device, ngpu, grad_noise=False, accum_grad=1, use_apex=False):   (line 154-155 in espnet/asr/pytorch_backend/asr.py)
Thus, grad_noise is set to args.accum_grad which is 1 by default.  This bug exists in other branches.
In branch v.0.6.0, in espnet/st/pytorch_backend/st.py line 93-94:
 ys_pad_asr = None
It will cause error when run updater or evaluator, in espnet/asr/pytroch_backend/asr.py line 113 and 171:
`x = tuple(arr.to(self.device) for arr in batch)'
since ys_pad_asr is NoneType.
It should be
x = tuple(arr.to(self.device) if arr is not None else None for arr in batch)
	</description>
	<comments>
		<comment id='1' author='CherrieWang97' date='2019-09-25T15:42:15Z'>
		Sorry for my late response and thanks for finding critical bugs. You seem to include the bug fix of this in your PR &lt;denchmark-link:https://github.com/espnet/espnet/pull/1210&gt;#1210&lt;/denchmark-link&gt;
. Is it possible to make a separate PR? We can quickly merge this bug fix first.
		</comment>
	</comments>
</bug>