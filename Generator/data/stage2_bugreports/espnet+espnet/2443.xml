<bug id='2443' author='keshawnhsieh' open_date='2020-09-09T13:19:05Z' closed_time='2020-09-09T16:05:49Z'>
	<summary>Low gpu utilization with multi-gpu training while high gpu utilization with single-gpu training</summary>
	<description>
Describe the bug
When training on LibriSpeech with the Transformer-Transudcer model, It can be noticed that much higher gpu utilization with single-gpu mode while it drops a lot if changed to 4-gpu mode. Batch size is modified accordingly, x4 for 4-gpu training.
configs
single gpu
&lt;denchmark-code&gt;
batch-bins: 3750000

# optimization related
criterion: loss
early-stop-criterion: "validation/main/loss"
sortagrad: 0
opt: noam
epochs: 120
patience: 0
accum-grad: 4
grad-clip: 5.0
weight-decay: 1e-6

# transformer related
transformer-lr: 10.0
transformer-warmup-steps: 25000
transformer-attn-dropout-rate-encoder: 0.4
transformer-attn-dropout-rate-decoder: 0.1

# network architecture
## encoder related
etype: transformer
transformer-input-layer: vgg2l
elayers: 18
eunits: 1024
dropout-rate: 0.4
## decoder related
dtype: transformer
dlayers: 2
#dec-embed-dim: 256
dunits: 1024
dropout-rate-decoder: 0.1
## attention related
adim: 64
aheads: 8
## joint network related
joint-dim: 256


# transducer related
mtlalpha: 1.0 # mtlalpha should be set to 1.0 (CTC) to use transducer
rnnt-mode: 'rnnt' # switch to 'rnnt-att' to use transducer with attention
model-module: "espnet.nets.pytorch_backend.e2e_asr_transducer:E2E"
&lt;/denchmark-code&gt;

4-gpu
&lt;denchmark-code&gt;n-iter-processes: 16
batch-bins: 15000000

# optimization related
criterion: loss
early-stop-criterion: "validation/main/loss"
sortagrad: 0
opt: noam
epochs: 120
patience: 0
accum-grad: 4
grad-clip: 5.0
weight-decay: 1e-6

# transformer related
transformer-lr: 10.0
transformer-warmup-steps: 25000
transformer-attn-dropout-rate-encoder: 0.4
transformer-attn-dropout-rate-decoder: 0.1

# network architecture
## encoder related
etype: transformer
transformer-input-layer: vgg2l
elayers: 18
eunits: 1024
dropout-rate: 0.4
## decoder related
dtype: transformer
dlayers: 2
#dec-embed-dim: 256
dunits: 1024
dropout-rate-decoder: 0.1
## attention related
adim: 64
aheads: 8
## joint network related
joint-dim: 256

# transducer related
mtlalpha: 1.0 # mtlalpha should be set to 1.0 (CTC) to use transducer
rnnt-mode: 'rnnt' # switch to 'rnnt-att' to use transducer with attention
model-module: "espnet.nets.pytorch_backend.e2e_asr_transducer:E2E"
&lt;/denchmark-code&gt;

gpu utilizations
single gpu
&lt;denchmark-link:https://user-images.githubusercontent.com/29997158/92601651-aea68980-f2df-11ea-88dc-73f24efa66b0.png&gt;&lt;/denchmark-link&gt;

4-gpu
&lt;denchmark-link:https://user-images.githubusercontent.com/29997158/92603341-c5e67680-f2e1-11ea-911d-09b7393047cf.png&gt;&lt;/denchmark-link&gt;

The number of utilization changes frequently, seem to be very unstable.
Basic environments:

OS information: Linux 3.10.0-957.21.3.el7.x86_64 #1 SMP Tue Jun 18 16:35:19 UTC 2019 x86_64
python version: 3.7.9 (default, Aug 18 2020, 06:24:24)  [GCC 5.4.0 20160609]
espnet version: espnet 0.9.1
pytorch version: pytorch 1.4.0+cu100
Git hash: 3629c9190fd7528adfd502475129fc2f6283b2d7

Commit date: Sat Aug 15 16:27:57 2020 +0900



&lt;denchmark-code&gt;Collecting environment information...
PyTorch version: 1.4.0+cu100
Is debug build: No
CUDA used to build PyTorch: 10.0

OS: Ubuntu 16.04.6 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration:
GPU 0: Tesla V100-SXM2-32GB
GPU 1: Tesla V100-SXM2-32GB
GPU 2: Tesla V100-SXM2-32GB
GPU 3: Tesla V100-SXM2-32GB

Nvidia driver version: 410.48
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-ranger==0.1.1
[pip3] pytorch-wpe==0.0.0
[pip3] torch==1.4.0+cu100
[pip3] torch-complex==0.1.2
[pip3] torch-optimizer==0.0.1a15
[pip3] torchaudio==0.4.0
[pip3] warprnnt-pytorch==0.1
[conda] Could not collect
&lt;/denchmark-code&gt;

Task information:

Task: ASR
Recipe: librispeech
ESPnet1

	</description>
	<comments>
		<comment id='1' author='keshawnhsieh' date='2020-09-09T14:18:59Z'>
		Hi,
Thanks for the report! I just ran some experiments and observed the opposite: more memory consumption by GPU with multi-GPU (2 and 3) compared to single GPU, but nothing which would indicate a bug.
Honestly, I don't think I'll further investigate, at least in current implementation. Some massive changes related to transducer are in the way and I may have modified some parts indirectly or directly related to device usage. It may be user-related though.
I'll do some testing when the new version is available and if I observe such behaviour, I'll investigate thoroughly.
		</comment>
		<comment id='2' author='keshawnhsieh' date='2020-09-09T14:30:03Z'>
		
Hi,
Thanks for the report! I just ran some experiments and observed the opposite: more memory consumption by GPU with multi-GPU (2 and 3) compared to single GPU, but nothing which would indicate a bug.
Honestly, I don't think I'll further investigate, at least in current implementation. Some massive changes related to transducer are in the way and I may have modified some parts indirectly or directly related to device usage. It may be user-related though.
I'll do some testing when the new version is available and if I observe such behaviour, I'll investigate thoroughly.

Very thanks for your quick reply. But in order to avoid any misunderstandings, the utilization rate I want to express here refers to the utilization percentage of the GPU, which is noted as GPU-Util in table printed by nvidia-smi, not the GPU memory. I do noticed the lower GPU utilization under 4-gpu configuration and unstable behavior.
		</comment>
		<comment id='3' author='keshawnhsieh' date='2020-09-09T14:45:55Z'>
		My bad, I benchmarked memory and utilization rate but reported to you the wrong one! About utilization rate, given fixed workload and frequency, it's a bit higher with single GPU compared multi-GPU, yes. But nothing which indicates a bug.
Also, it seems to me other models (CTC, Att, MTL) have the same behaviour ?
		</comment>
		<comment id='4' author='keshawnhsieh' date='2020-09-09T14:46:06Z'>
		I think this is the limitation of I/O performance.
What is your storage setup?
For example, if you use non-raid HDD-based network storage with 1Gbps connection, it is difficult to fully utilize the GPUs.
		</comment>
		<comment id='5' author='keshawnhsieh' date='2020-09-09T15:05:56Z'>
		
I think this is the limitation of I/O performance.
What is your storage setup?
For example, if you use non-raid HDD-based network storage with 1Gbps connection, it is difficult to fully utilize the GPUs.

My training machine is rent from Tencent and I am not sure how they organized storage physically. But I have tested the performance of read by running time dd if=/data/1Gb.file bs=64k |dd of=/dev/null and got 1024000000 bytes (1.0 GB, 977 MiB) copied, 4.37648 s, 234 MB/s. That seems to be Ok to catch up the speed of GPU?
Btw, I also tried &lt;denchmark-link:http://nmon.sourceforge.net/pmwiki.php&gt;nmon&lt;/denchmark-link&gt;
 to monitor disk I/O while training but it shows 0 all the time. Are there any good tools to recommend? Thank you very much.
		</comment>
		<comment id='6' author='keshawnhsieh' date='2020-09-09T15:46:04Z'>
		You may ask this question because of the not good performance of multi GPU training. This is general problem, non only espnet.  Basically, multi-GPU training has some overhead and it can also cause lower utilization.

GPU-GPU communication
The overhead comes from multi-threading if DataParallel is used (ESPnet1 uses DataParallel)
The other GPUs must wait until the slowest GPU is finished.

In my understanding, the unstable utilization may come from 3.
The sequence lengths of each sample in a mini-batch are differrent in seq2seq task, so the forward-backward time at each GPU has large diversity.
(To reduce this problem, we gather near length samples in a mini-batch as possible)
		</comment>
		<comment id='7' author='keshawnhsieh' date='2020-09-09T15:58:48Z'>
		
You may ask this question because of the not good performance of multi GPU training. This is general problem, non only espnet. Basically, multi-GPU training has some overhead and it can also cause lower utilization.

GPU-GPU communication
The overhead comes from multi-threading if DataParallel is used (ESPnet1 uses DataParallel)
The other GPUs must wait until the slowest GPU is finished.

In my understanding, the unstable utilization may come from 3.
The sequence lengths of each sample in a mini-batch are differrent in seq2seq task, so the forward-backward time at each GPU has large diversity.
(To reduce this problem, we gather near length samples in a mini-batch as possible)

Thank you for your perfect summary of these three reasons. I agree with your idea of unbalance of different sequence in one batch. Maybe padding each sequence to the same length will stabilize GPU's utilization but those computation caused by blank padding is just meaningless.
		</comment>
		<comment id='8' author='keshawnhsieh' date='2020-09-09T16:04:22Z'>
		Interesting, have you tried  or &lt;denchmark-link:https://pytorch.org/docs/stable/bottleneck.html&gt;https://pytorch.org/docs/stable/bottleneck.html&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='9' author='keshawnhsieh' date='2020-09-09T16:11:16Z'>
		
Interesting, have you tried nvprof or https://pytorch.org/docs/stable/bottleneck.html?

I didn't try nvprof as the root cause is about performance gap between multi-gpu training configuration rather than the model itself. I'm not sure if nvprof can also profile multi-gpu training bottleneck analysis?
		</comment>
	</comments>
</bug>