<bug id='2664' author='windtoker' open_date='2020-11-10T01:27:42Z' closed_time='2020-11-20T12:27:15Z'>
	<summary>fastspeech2 reduction_fatcor=3 training problem</summary>
	<description>
Describe the bug
With custom japanese dataset, I trained tacotron2 with reduction_factor = 3 using espnet2 recipe.
Using that model, i was trying to train fastspeech2 default config with reduction_factor = 3.
I extracted durations using inference stage and with that, I ran the script run.sh.
However, the error occurred at the stage 5 while extracting pitch and energy stats.
For more information, I attached the error log below.
Seems like while the durations got reduced as a consequence of applying reduction_factor = 3,
pitches were not synchronized though.
Basic environments:

OS information:  Linux 4.15.0-112-generic #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020 x86_64
python version: 3.7.6 (default, Jan  8 2020, 19:59:22)  [GCC 7.3.0]
espnet version: espnet 0.9.4
pytorch version pytorch 1.4.0

Environments from torch.utils.collect_env:
Collecting environment information...
PyTorch version: 1.4.0
Is debug build: No
CUDA used to build PyTorch: 10.0
OS: Ubuntu 18.04.4 LTS
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
CMake version: version 3.10.2
Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration:
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti
Nvidia driver version: 440.100
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5
Versions of relevant libraries:
[pip] numpy==1.18.1
[pip] numpydoc==0.9.2
[pip] pytorch-ranger==0.1.1
[pip] pytorch-wpe==0.0.0
[pip] torch==1.4.0
[pip] torch-complex==0.0.3
[pip] torch-optimizer==0.0.1a15
[pip] torchaudio==0.4.0
[pip] warpctc-pytorch==0.2.1
[pip] warprnnt-pytorch==0.1
[conda] _pytorch_select           0.2                       gpu_0
[conda] blas                      1.0                         mkl
[conda] mkl                       2020.0                      166
[conda] mkl-service               2.3.0            py37he904b0f_0
[conda] mkl_fft                   1.0.15           py37ha843d7b_0
[conda] mkl_random                1.1.0            py37hd6b4f25_0
[conda] pytorch                   1.4.0           py3.7_cuda10.0.130_cudnn7.6.3_0    pytorch
[conda] pytorch-ranger            0.1.1                    pypi_0    pypi
[conda] pytorch-wpe               0.0.0                    pypi_0    pypi
[conda] torch                     1.4.0                    pypi_0    pypi
[conda] torch-complex             0.0.3                    pypi_0    pypi
[conda] torch-optimizer           0.0.1a15                 pypi_0    pypi
[conda] torchaudio                0.4.0                    pypi_0    pypi
[conda] warpctc-pytorch           0.2.1                    pypi_0    pypi
[conda] warprnnt-pytorch          0.1                      pypi_0    pypi
Task information:

Task: TTS
Recipe: custom dataset
ESPnet2

To Reproduce
Steps to reproduce the behavior:

train fastspeech2 with reduction factor=3 config and teacher model as tacotron2 reduction factor =3

Error logs
Paste the error logs. If applicable, add screenshots to help explain your problem.
File "/espnet2/tts/feats_extract/dio.py", line 176, in _average_by_duration
assert d.sum() == len(x), f"{d.sum()} == {len(x)} error"
AssertionError: 225 == 677 error
x shape : (202800,)
x shape : (369300,)
x shape : (362100,)
x shape : (347400,)
x shape : (332400,)
x shape : (380700,)
x shape : (430200,)
x shape : (149400,)
x shape : (302100,)
x shape : (240000,)
x shape : (290100,)
x shape : (282000,)
x shape : (354900,)
x shape : (346500,)
x shape : (338100,)
x shape : (384000,)
x shape : (338400,)
x shape : (314400,)
x shape : (319200,)
x shape : (213900,)
	</description>
	<comments>
		<comment id='1' author='windtoker' date='2020-11-10T02:14:12Z'>
		Thank you for your report.
This is a bug. I will fix it.
		</comment>
		<comment id='2' author='windtoker' date='2020-11-10T04:06:32Z'>
		okay. thank you for your support.
		</comment>
		<comment id='3' author='windtoker' date='2020-11-19T17:24:51Z'>
		&lt;denchmark-link:https://github.com/kan-bayashi&gt;@kan-bayashi&lt;/denchmark-link&gt;
 any updates on this? Maybe there is a quick fix?
		</comment>
		<comment id='4' author='windtoker' date='2020-11-20T03:03:19Z'>
		&lt;denchmark-link:https://github.com/kan-bayashi&gt;@kan-bayashi&lt;/denchmark-link&gt;
 Hi, Is this bug get fixed? I also got this in espnet2/tts/feats_extract/dio.py:

		</comment>
		<comment id='5' author='windtoker' date='2020-11-20T07:28:32Z'>
		Sorry for the late fixing.
Could you try the patch &lt;denchmark-link:https://github.com/espnet/espnet/pull/2704&gt;#2704&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='6' author='windtoker' date='2020-11-20T07:36:37Z'>
		
Sorry for the late fixing.
Could you try the patch #2704?

But the reduction_factor is 1 both in my Tacotron2 config and Fastspeech2 config, not 3.
And the AssertionError occurred. Is that usual?
		</comment>
		<comment id='7' author='windtoker' date='2020-11-20T08:13:14Z'>
		
Sorry for the late fixing.
Could you try the patch #2704?

I update the code by your new fix, but the AssertionError still occurred:
&lt;denchmark-code&gt;  File "xxxxx/espnet/espnet2/tts/feats_extract/dio.py", line 179, in _average_by_duration
    assert len(x) - d.sum() &lt; self.reduction_factor
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='windtoker' date='2020-11-20T08:15:59Z'>
		&lt;denchmark-link:https://github.com/Mingrg&gt;@Mingrg&lt;/denchmark-link&gt;
 Your problem is different from this issue.
Maybe you did not use teacher forcing during decoding.
See &lt;denchmark-link:https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE/tts1#fastspeech2-training&gt;https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE/tts1#fastspeech2-training&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='windtoker' date='2020-11-20T10:30:43Z'>
		&lt;denchmark-link:https://github.com/kan-bayashi&gt;@kan-bayashi&lt;/denchmark-link&gt;
 it seems that your patch works, thanks.
		</comment>
		<comment id='10' author='windtoker' date='2020-11-20T11:46:45Z'>
		
@Mingrg Your problem is different from this issue.
Maybe you did not use teacher forcing during decoding.
See https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE/tts1#fastspeech2-training

thx!
		</comment>
	</comments>
</bug>