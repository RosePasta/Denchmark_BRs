<bug id='2025' author='Cescfangs' open_date='2020-06-12T03:06:21Z' closed_time='2020-06-12T12:01:12Z'>
	<summary>espnet2 calculate_all_attention questions</summary>
	<description>
I believe we are plotting encoder-decoder alignments:



espnet/espnet2/main_funcs/calculate_all_attentions.py


        Lines 50 to 55
      in
      8da1e96






 



 def hook(module, input, output, name=name): 



 if isinstance(module, MultiHeadedAttention): 



 # att_w: (B, Tout, Tin) 



 att_w = output 



 outputs[name] = att_w.detach().cpu() 





but the output of MultiHeadedAttention is the context info (batch_size, time, d_model)



espnet/espnet/nets/pytorch_backend/transformer/attention.py


        Lines 73 to 78
      in
      8da1e96






 p_attn = self.dropout(self.attn) 



 x = torch.matmul(p_attn, v)  # (batch, head, time1, d_k) 



 x = ( 



 x.transpose(1, 2).contiguous().view(n_batch, -1, self.h * self.d_k) 



 )  # (batch, time1, d_model) 



 return self.linear_out(x)  # (batch, time1, d_model) 





So I guess the attention plot would be like that(x-axis is attention dim, 256 here not input time):
&lt;denchmark-link:https://user-images.githubusercontent.com/11382612/84460489-a4530000-ac9c-11ea-9d2b-434cf6df5b3d.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='Cescfangs' date='2020-06-12T04:40:12Z'>
		I see, thanks.
		</comment>
		<comment id='2' author='Cescfangs' date='2020-06-12T12:02:27Z'>
		I fixed. Please check it &lt;denchmark-link:https://github.com/espnet/espnet/pull/2028&gt;#2028&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>