<bug id='1508' author='luyizhou4' open_date='2020-01-13T11:19:29Z' closed_time='2020-02-20T13:31:17Z'>
	<summary>Pure CTC decoding bug</summary>
	<description>
Hi, there!
I'm currently decoding my CTC model (mtlalpha 1.0) with pure CTC mode (ctc_weight 1.0), and find that in current implementation, ctc-prefix-search will also compute &lt;blank&gt; token's next-label-score, which to my understanding shall be excluded. Details below:
With ctc_weight=1.0, ctc_beam becomes the size of whole vocabulary (including &lt;blank&gt; token)



espnet/espnet/nets/pytorch_backend/e2e_asr_transformer.py


        Lines 291 to 296
      in
      28ce90a






 if ctc_weight != 1.0: 



 # pre-pruning based on attention scores 



 from espnet.nets.pytorch_backend.rnn.decoders import CTC_SCORING_RATIO 



 ctc_beam = min(lpz.shape[-1], int(beam * CTC_SCORING_RATIO)) 



 else: 



 ctc_beam = lpz.shape[-1] 





Then &lt;blank&gt; token-id will be included in local_best_ids, and prefix-score computed. To my understanding, &lt;blank&gt; should be excluded here.



espnet/espnet/nets/pytorch_backend/e2e_asr_transformer.py


        Lines 328 to 335
      in
      28ce90a






 if lpz is not None: 



 local_best_scores, local_best_ids = torch.topk( 



 local_att_scores, ctc_beam, dim=1) 



 ctc_scores, ctc_states = ctc_prefix_score( 



 hyp['yseq'], local_best_ids[0], hyp['ctc_state_prev']) 



 local_scores = \ 



         (1.0 - ctc_weight) * local_att_scores[:, local_best_ids[0]] \ 



 + ctc_weight * torch.from_numpy(ctc_scores - hyp['ctc_score_prev']) 





	</description>
	<comments>
		<comment id='1' author='luyizhou4' date='2020-01-14T17:06:10Z'>
		I see.
Did you find any degradation?
&lt;denchmark-link:https://github.com/takaaki-hori&gt;@takaaki-hori&lt;/denchmark-link&gt;
, I think we should not consider the blank.
Can you check it?
		</comment>
		<comment id='2' author='luyizhou4' date='2020-01-14T19:13:19Z'>
		Yeah, the array local_best_ids should not include 0 (&lt;blank&gt;).
When ctc_weight != 1, there is no problem since the attention model gives a very low probability to &lt;blank&gt;, and local_best_ids does not include 0.
But, if ctc_weight = 1,  local_best_ids has all IDs.
So, we need to exclude 0 from local_best_ids, e.g., by making it with np.arange(1, odim).
It is also possible that we always set zero to the prefix probability for &lt;blank&gt; in CTCPrefixScore.
Anyway, I will handle this matter.
		</comment>
		<comment id='3' author='luyizhou4' date='2020-01-14T22:59:37Z'>
		Thanks!
		</comment>
		<comment id='4' author='luyizhou4' date='2020-01-15T06:01:31Z'>
		Thanks for reply! Excluding 0 from local_best_ids improves my CTC result from 12.54% to 12.08%, and for reference greedy decoding WER is 12.10%.
		</comment>
		<comment id='5' author='luyizhou4' date='2020-01-15T08:06:21Z'>
		thanks, this is significant.
		</comment>
		<comment id='6' author='luyizhou4' date='2020-02-07T12:50:12Z'>
		&lt;denchmark-link:https://github.com/takaaki-hori&gt;@takaaki-hori&lt;/denchmark-link&gt;
, could you think of fixing this?
		</comment>
		<comment id='7' author='luyizhou4' date='2020-02-12T22:10:55Z'>
		Yes, I am working on it now.
		</comment>
	</comments>
</bug>