<bug id='1780' author='qmeeus' open_date='2020-04-07T07:43:21Z' closed_time='2020-04-09T14:37:42Z'>
	<summary>Probably no dropout applied to the encoder in the RNN encoder/decoder ASR</summary>
	<description>
Hi,
When trying to apply dropout on the rnn encoder, I get this warning:
&lt;denchmark-code&gt;/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
&lt;/denchmark-code&gt;

Indeed, &lt;denchmark-link:https://github.com/espnet/espnet/blob/master/espnet/nets/pytorch_backend/rnn/encoders.py#L35&gt;after a quick check in the code&lt;/denchmark-link&gt;
:
rnn = torch.nn.LSTM(inputdim, cdim, dropout=dropout, num_layers=1, bidirectional=bidir,
                                batch_first=True) if "lstm" in typ \
                else torch.nn.GRU(inputdim, cdim, dropout=dropout, num_layers=1, bidirectional=bidir, batch_first=True)
	</description>
	<comments>
		<comment id='1' author='qmeeus' date='2020-04-07T08:10:37Z'>
		Hi,
Thanks for the report. We are aware of the problem: dropout won't be applied if use_projection=True, as RNNP is a stack of 1-layer torch.nn.LSTM and dropout is disabled if num_layers=1.
&lt;denchmark-link:https://github.com/sw005320&gt;@sw005320&lt;/denchmark-link&gt;
 It was already reported in multiple discussions but I'm not sure why we didn't fix that? There may have been a reason.
		</comment>
		<comment id='2' author='qmeeus' date='2020-04-07T11:11:02Z'>
		I wrote a fix, I will do a merge request after testing
		</comment>
		<comment id='3' author='qmeeus' date='2020-04-07T11:35:59Z'>
		
@sw005320 It was already reported in multiple discussions but I'm not sure why we didn't fix that? There may have been a reason.

It should be fixed...

I wrote a fix, I will do a merge request after testing

That is really helpful.
It would be great if you make a PR about it.
		</comment>
		<comment id='4' author='qmeeus' date='2020-04-07T12:10:21Z'>
		Here you go
&lt;denchmark-link:https://github.com/espnet/espnet/pull/1784&gt;#1784&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='qmeeus' date='2020-04-07T12:24:13Z'>
		Before closing this, is it intentional to use the same dropout rate for the attention layer and the encoder? If not, I suggest adding a dropout rate specific to the attention layer and renaming the argument "dropout_rate" to "encoder_dropout" or something like that
		</comment>
		<comment id='6' author='qmeeus' date='2020-04-07T12:28:41Z'>
		I think this is a good idea in general. Personally, I did not have good success in introducing dropout anyway and making dropout related development items in a lower priority due to that. But I think this function is quite reasonable.
		</comment>
		<comment id='7' author='qmeeus' date='2020-04-07T12:32:56Z'>
		My model overfits on the encoder side (CTC) but if I'm using dropout in the attention, the decoder is not able to attend to the right parts of the input anymore. I will look into it
		</comment>
	</comments>
</bug>