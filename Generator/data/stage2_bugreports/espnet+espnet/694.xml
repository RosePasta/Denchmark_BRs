<bug id='694' author='fanlu' open_date='2019-04-23T03:13:38Z' closed_time='2019-04-24T02:11:02Z'>
	<summary>tedlium asr1 problem</summary>
	<description>
When I user docker to run tedlium asr1 recipe
&lt;denchmark-code&gt;./run.sh --docker_gpu 0,1,2,3 --docker_egs tedlium/asr1 --ngpu 4
&lt;/denchmark-code&gt;

I got some bugs in stage 5
&lt;denchmark-code&gt;stage 5: Decoding
2019-04-22 17:47:02,929 (splitjson:35) INFO: /espnet/tools/venv/bin/python /espnet/egs/tedlium/asr1/../../../utils/splitjson.py --parts 32 dump/dev/deltafalse/data.json
2019-04-22 17:47:02,932 (splitjson:35) INFO: /espnet/tools/venv/bin/python /espnet/egs/tedlium/asr1/../../../utils/splitjson.py --parts 32 dump/test/deltafalse/data.json
2019-04-22 17:47:02,981 (splitjson:47) INFO: number of utterances = 507
2019-04-22 17:47:03,022 (splitjson:47) INFO: number of utterances = 1155
run.pl: 32 / 32 failed, log is in exp/train_trim_pytorch_vggblstmp_e4_subsample1_2_2_1_1_unit320_proj320_d1_unit300_location_adim320_aconvc10_aconvf100_mtlalpha0.5_adadelta_sampprob0.0_bs30_mli800_mlo150/decode_test_beam20_emodel.acc.best_p0.0_len0.0-0.0_ctcw0.3_rnnlm1.0_1layer_unit1000_sgd_bs300_word65000/log/decode.*.log
run.pl: 32 / 32 failed, log is in exp/train_trim_pytorch_vggblstmp_e4_subsample1_2_2_1_1_unit320_proj320_d1_unit300_location_adim320_aconvc10_aconvf100_mtlalpha0.5_adadelta_sampprob0.0_bs30_mli800_mlo150/decode_dev_beam20_emodel.acc.best_p0.0_len0.0-0.0_ctcw0.3_rnnlm1.0_1layer_unit1000_sgd_bs300_word65000/log/decode.*.log
./run.sh: 2 background jobs are failed.
run.sh done.
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;# asr_recog.py --ngpu 0 --backend pytorch --debugmode 1 --verbose 0 --recog-json dump/test/deltafalse/split32utt/data.1.json --result-label    exp/train_trim_pytorch_vggblstmp_e6_subsample1_2_2_1_1_unit320_proj320_d1_unit300_location_adim320_aconvc10_aconvf100_mtlalpha0.               5_adadelta_sampprob0.0_bs30_mli800_mlo150/decode_test_beam20_emodel.acc.best_p0.0_len0.0-0.0_ctcw0.3_rnnlm1.                                   0_1layer_unit1000_sgd_bs300_word65000/data.1.json --model exp/                                                                                 train_trim_pytorch_vggblstmp_e6_subsample1_2_2_1_1_unit320_proj320_d1_unit300_location_adim320_aconvc10_aconvf100_mtlalpha0.                   5_adadelta_sampprob0.0_bs30_mli800_mlo150/results/model.acc.best --beam-size 20 --penalty 0.0 --maxlenratio 0.0 --minlenratio 0.0 --ctc-       weight 0.3 --rnnlm exp/train_rnnlm_pytorch_1layer_unit1000_sgd_bs300_word65000/rnnlm.model.best --lm-weight 1.0 --word-rnnlm exp/              train_rnnlm_pytorch_1layer_unit1000_sgd_bs300_word65000/rnnlm.model.best
 # Started at Mon Apr 22 17:20:14 UTC 2019
 #
 2019-04-22 17:20:15,087 (asr_recog:91) WARNING: Skip DEBUG/INFO messages
 2019-04-22 17:20:17,292 (e2e_asr:85) WARNING: Subsampling is not performed for vgg*. It is performed in max pooling layers at CNN.
 Traceback (most recent call last):
   File "/espnet/egs/tedlium/asr1/../../../espnet/bin/asr_recog.py", line 135, in &lt;module&gt;
     main(sys.argv[1:])
   File "/espnet/egs/tedlium/asr1/../../../espnet/bin/asr_recog.py", line 123, in main
     recog(args)
   File "/espnet/espnet/asr/pytorch_backend/asr.py", line 443, in recog
     torch_load(args.rnnlm, rnnlm)
   File "/espnet/espnet/asr/asr_utils.py", line 390, in torch_load
     model.load_state_dict(model_state_dict)
   File "/espnet/tools/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 769, in load_state_dict
     self.__class__.__name__, "\n\t".join(error_msgs)))
 RuntimeError: Error(s) in loading state_dict for ClassifierWithState:
     size mismatch for predictor.embed.weight: copying a param with shape torch.Size([65003, 1000]) from checkpoint, the shape in current       model is torch.Size([34, 1000]).
     size mismatch for predictor.lo.weight: copying a param with shape torch.Size([65003, 1000]) from checkpoint, the shape in current model    is torch.Size([34, 1000]).
     size mismatch for predictor.lo.bias: copying a param with shape torch.Size([65003]) from checkpoint, the shape in current model is torch.  Size([34]).
 # Accounting: time=8 threads=1                                                                                                                 # Ended (code 1) at Mon Apr 22 17:20:22 UTC 2019, elapsed time 8 seconds
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='fanlu' date='2019-04-23T12:41:42Z'>
		It seems that you duplicate the  and  options. Please use either option. &lt;denchmark-link:https://github.com/takaaki-hori&gt;@takaaki-hori&lt;/denchmark-link&gt;
, could you add some checks to avoid both options are used at the same time?
		</comment>
		<comment id='2' author='fanlu' date='2019-04-24T02:11:02Z'>
		OK, I see.thanks you very much. &lt;denchmark-link:https://github.com/sw005320&gt;@sw005320&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='fanlu' date='2019-05-06T21:16:40Z'>
		Same problem with the current version of the repository in the tedlium/asr1 recipe (not in docker container) :
&lt;denchmark-code&gt;# asr_recog.py --ngpu 0 --backend pytorch --debugmode 1 --verbose 0 --recog-json dump/dev/deltafalse/split32utt/data.10.json --result-label exp/train_trim_py
torch_vggblstmp_e6_subsample1_2_2_1_1_unit320_proj320_d1_unit300_location_adim320_aconvc10_aconvf100_mtlalpha0.5_adadelta_sampprob0.0_bs30_mli800_mlo150/deco
de_dev_beam20_emodel.acc.best_p0.0_len0.0-0.0_ctcw0.3_rnnlm1.0_1layer_unit1000_sgd_bs300_word65000/data.10.json --model exp/train_trim_pytorch_vggblstmp_e6_s
ubsample1_2_2_1_1_unit320_proj320_d1_unit300_location_adim320_aconvc10_aconvf100_mtlalpha0.5_adadelta_sampprob0.0_bs30_mli800_mlo150/results/model.acc.best -
-beam-size 20 --penalty 0.0 --maxlenratio 0.0 --minlenratio 0.0 --ctc-weight 0.3 --rnnlm exp/train_rnnlm_pytorch_1layer_unit1000_sgd_bs300_word65000/rnnlm.mo
del.best --lm-weight 1.0 --word-rnnlm exp/train_rnnlm_pytorch_1layer_unit1000_sgd_bs300_word65000/rnnlm.model.best 
# Started at Sun May  5 18:30:38 CEST 2019
#
2019-05-05 18:30:39,287 (asr_recog:91) WARNING: Skip DEBUG/INFO messages
2019-05-05 18:30:45,175 (e2e_asr:85) WARNING: Subsampling is not performed for vgg*. It is performed in max pooling layers at CNN.
Traceback (most recent call last):
  File "/users/yesteve/tools/espnet/egs/tedlium/asr1/../../../espnet/bin/asr_recog.py", line 135, in &lt;module&gt;
    main(sys.argv[1:])
  File "/users/yesteve/tools/espnet/egs/tedlium/asr1/../../../espnet/bin/asr_recog.py", line 123, in main
    recog(args)
  File "/users/yesteve/.conda/envs/espenv/lib/python3.7/site-packages/espnet-0.3.1-py3.7.egg/espnet/asr/pytorch_backend/asr.py", line 443, in recog
  File "/users/yesteve/.conda/envs/espenv/lib/python3.7/site-packages/espnet-0.3.1-py3.7.egg/espnet/asr/asr_utils.py", line 390, in torch_load
  File "/users/yesteve/.conda/envs/espenv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 719, in load_state_dict
    self.__class__.__name__, "\n\t".join(error_msgs)))
RuntimeError: Error(s) in loading state_dict for ClassifierWithState:
	size mismatch for predictor.embed.weight: copying a param of torch.Size([34, 1000]) from checkpoint, where the shape is torch.Size([65003, 1000]) in 
current model.
	size mismatch for predictor.lo.weight: copying a param of torch.Size([34, 1000]) from checkpoint, where the shape is torch.Size([65003, 1000]) in cur
rent model.
	size mismatch for predictor.lo.bias: copying a param of torch.Size([34]) from checkpoint, where the shape is torch.Size([65003]) in current model.
# Accounting: time=21 threads=1
# Ended (code 1) at Sun May  5 18:30:59 CEST 2019, elapsed time 21 seconds

&lt;/denchmark-code&gt;

Thanks to &lt;denchmark-link:https://github.com/sw005320&gt;@sw005320&lt;/denchmark-link&gt;
 I am going to fix this on my side.
		</comment>
		<comment id='4' author='fanlu' date='2019-05-06T21:26:01Z'>
		I think the problem comes from the run.sh script: line 315 should be removed since value of ${recog_opts} in line 317 manages the use of word or character (?) rnnlm.
&lt;denchmark-code&gt;282     for rtask in ${recog_set}; do
283     (
284         decode_dir=decode_${rtask}_beam${beam_size}_e${recog_model}_p${penalty}_len${minlenratio}-${maxlenratio}_ctcw${ctc_weight}_rnnlm${lm_weight}_${lmtag}
285         if [ ${use_wordlm} = true ]; then
286             recog_opts="--word-rnnlm ${lmexpdir}/rnnlm.model.best"
287         else
288             recog_opts="--rnnlm ${lmexpdir}/rnnlm.model.best"
289         fi
290         if [ ${lm_weight} == 0 ]; then
291             recog_opts=""
292         fi
293         feat_recog_dir=${dumpdir}/${rtask}/delta${do_delta}
294 
295         # split data
296         splitjson.py --parts ${nj} ${feat_recog_dir}/data.json
297 
298         #### use CPU for decoding
299         ngpu=0
300 
301         ${decode_cmd} JOB=1:${nj} ${expdir}/${decode_dir}/log/decode.JOB.log \
302             asr_recog.py \
303             --ngpu ${ngpu} \
304             --backend ${backend} \
305             --debugmode ${debugmode} \
306             --verbose ${verbose} \
307             --recog-json ${feat_recog_dir}/split${nj}utt/data.JOB.json \
308             --result-label ${expdir}/${decode_dir}/data.JOB.json \
309             --model ${expdir}/results/${recog_model}  \
310             --beam-size ${beam_size} \
311             --penalty ${penalty} \
312             --maxlenratio ${maxlenratio} \
313             --minlenratio ${minlenratio} \
314             --ctc-weight ${ctc_weight} \
315             --rnnlm ${lmexpdir}/rnnlm.model.best \
316             --lm-weight ${lm_weight} \
317             ${recog_opts}
318 

&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>