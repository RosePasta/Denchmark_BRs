<bug id='177' author='npuichigo' open_date='2020-07-04T13:30:20Z' closed_time='2020-07-20T13:14:38Z'>
	<summary>set_epoch on DataLoader is needed for distributed training</summary>
	<description>
&lt;denchmark-link:https://pytorch.org/docs/master/data.html?highlight=distributedsampler#torch.utils.data.distributed.DistributedSampler&gt;DistributedSampler&lt;/denchmark-link&gt;
 in pytorch says that, in distributed mode, calling the  method at the beginning of each epoch before creating the DataLoader iterator is necessary to make shuffling work properly across multiple epochs. Otherwise, the same ordering will be always used.
It seems that it's missing here.
Example:
&gt;&gt;&gt; sampler = DistributedSampler(dataset) if is_distributed else None
&gt;&gt;&gt; loader = DataLoader(dataset, shuffle=(sampler is None),
...                     sampler=sampler)
&gt;&gt;&gt; for epoch in range(start_epoch, n_epochs):
...     if is_distributed:
...         sampler.set_epoch(epoch)
...     train(loader)
	</description>
	<comments>
		<comment id='1' author='npuichigo' date='2020-07-04T14:47:50Z'>
		&lt;denchmark-link:https://github.com/npuichigo&gt;@npuichigo&lt;/denchmark-link&gt;
 Thank you for pointing out.
I will check it.
		</comment>
	</comments>
</bug>