<bug id='131' author='dathudeptrai' open_date='2020-04-24T02:03:02Z' closed_time='2020-04-24T04:45:14Z'>
	<summary>Possible BUG may make your code not achieve the best performance. (background noise, etc.)</summary>
	<description>
Hi &lt;denchmark-link:https://github.com/kan-bayashi&gt;@kan-bayashi&lt;/denchmark-link&gt;
 , there is a bug i think that make ur code cann't achieve the best performance for both melgan and PWG. That is after training 1 step generator, you should re-compute y_ then use this y_ for discriminator, but seem ur code not do that. In my experiment, re-compute y_ is crucial for obtain best quality. I have my tensorflow code for melgan, i can get the same performance as ur code but just around 2M steps from scratch (don't need PWG auxiliary loss to help convergence speed), but if i don't recompute y_, my tf code at 2M steps not good as 2M steps when recompute y_
	</description>
	<comments>
		<comment id='1' author='dathudeptrai' date='2020-04-24T03:59:59Z'>
		Thank you for your suggestions!
I'm not sure which is the standard for GAN training, but it is better to change the update manner since your experiments shows better performance.
I will make PR.
		</comment>
		<comment id='2' author='dathudeptrai' date='2020-04-24T04:01:47Z'>
		BTW, how was the final quality? Is it improved? or just related to convergence speed?
		</comment>
		<comment id='3' author='dathudeptrai' date='2020-04-24T04:09:14Z'>
		I think that make sense, discriminator should use newest y_ for training, that make discriminator penalize generator better. U can ref two implementation on github (&lt;denchmark-link:https://github.com/seungwonpark/melgan/blob/master/utils/train.py/#L85&gt;https://github.com/seungwonpark/melgan/blob/master/utils/train.py/#L85&lt;/denchmark-link&gt;
) and official code (&lt;denchmark-link:https://github.com/descriptinc/melgan-neurips/blob/master/scripts/train.py/#L156&gt;https://github.com/descriptinc/melgan-neurips/blob/master/scripts/train.py/#L156&lt;/denchmark-link&gt;
). Note that official code training discriminator before generator but it's still re-compute D-fake for training generator. The quality on my language (vietnamese) is improve significantly. I'm training Ljspeech now on melgan only, i have 1M4 val audio here. I believe that the same improvement can achieve with PWG since the manner correctly.
&lt;denchmark-link:https://github.com/kan-bayashi/ParallelWaveGAN/files/4526567/1460000steps.zip&gt;1460000steps.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='dathudeptrai' date='2020-04-24T04:18:57Z'>
		Thank you for sharing samples!
It seems very good :) I will follow your suggested manner!
		</comment>
		<comment id='5' author='dathudeptrai' date='2020-04-24T04:22:21Z'>
		BTW, i'm creating TF framework for Speech, now focus TTS :)), just like ur ESPNet :D. TF now training faster than pytorch with exact parameter, batch_size, batch_max_step, the improvement on inference speed you knew before: D.
		</comment>
		<comment id='6' author='dathudeptrai' date='2020-04-24T04:34:29Z'>
		That sounds nice :)
But I love pytorch so I want facebook to make it faster ðŸš€
		</comment>
		<comment id='7' author='dathudeptrai' date='2020-04-24T04:37:16Z'>
		haha BTW, what is the license of this repo and ur ESPNET :)), i cann't retrain anything, there are many model and dataset, i plan to provide some training script, pretrained on my TF framework and the rest is convert from ur pytorch pretrained then support for inference only :D. Can i do that ?
		</comment>
		<comment id='8' author='dathudeptrai' date='2020-04-24T04:39:48Z'>
		That's very cool.
This repository is MIT license and ESPnet is Apache 2.0, so you can do it!
		</comment>
		<comment id='9' author='dathudeptrai' date='2020-04-24T04:44:45Z'>
		ok that nice, i will make TF models and pytorch models synchronized and use the same preprocesing here so atleast user can use pytorch for training and TF for inference :D. Again, thank for ur hard work :)).
		</comment>
	</comments>
</bug>