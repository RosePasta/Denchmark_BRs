<bug id='40' author='de-code' open_date='2019-07-05T08:21:33Z' closed_time='2020-02-03T00:03:48Z'>
	<summary>DataGenerator on_epoch_end / shuffle_pair not functional</summary>
	<description>
Disclaimer: I haven't actually tested it but can't see it working..
The shuffle_pair function doesn't shuffle the array in place as far as I can see, but is returning a shuffled view. Which in turn is not used by on_epoch_end.
    def shuffle_pair(self, a, b):
        # generate permutation index array
        permutation = np.random.permutation(a.shape[0])
        # shuffle the two arrays
        return a[permutation], b[permutation]

    def on_epoch_end(self):
        # shuffle dataset at each epoch
        if self.shuffle == True:
            if self.y is None:
                np.random.shuffle(self.x)
            else:      
                self.shuffle_pair(self.x,self.y)
For an in-place shuffle I found the following &lt;denchmark-link:https://stackoverflow.com/a/51526109/8676953&gt;StackOverflow answer&lt;/denchmark-link&gt;
 useful (essentially shuffling each array in-place with the same seed):
def shuffle_arrays(arrays, set_seed=-1):
    """Shuffles arrays in-place, in the same order, along axis=0

    Parameters:
    -----------
    arrays : List of NumPy arrays.
    set_seed : Seed value if int &gt;= 0, else seed is random.
    """
    assert all(len(arr) == len(arrays[0]) for arr in arrays)
    seed = np.random.randint(0, 2**(32 - 1) - 1) if set_seed &lt; 0 else set_seed

    for arr in arrays:
        rstate = np.random.RandomState(seed)  # pylint: disable=no-member
        rstate.shuffle(arr)
Otherwise you could of course use the shuffled numpy array views.
	</description>
	<comments>
		<comment id='1' author='de-code' date='2019-12-24T12:57:49Z'>
		The quickest fix would be to just use the view:
&lt;denchmark-code&gt;    def on_epoch_end(self):
        # shuffle dataset at each epoch
        if self.shuffle == True:
            if self.y is None:
                np.random.shuffle(self.x)
            else:
                self.x, self.y = self.shuffle_pair(self.x, self.y)
&lt;/denchmark-code&gt;

However here there is a shuffling at the end of each epoque, does this means that we might take again the same date in the next batch that is generated?
Unless I'm missing something (again ðŸ’¦) wouldn't be better to shuffle at the beginning?
		</comment>
		<comment id='2' author='de-code' date='2019-12-24T15:14:58Z'>
		
However here there is a shuffling at the end of each epoque, does this means that we might take again the same date in the next batch that is generated?
Unless I'm missing something (again ðŸ’¦) wouldn't be better to shuffle at the beginning?

The first epoch will use non shuffled data, then every epoch will take the shuffled data at the end of the previous epoch, so this is fine - every epoch will use different ordered data.
keras.utils.Sequence only implements a on_epoch_end so this is the way to do it I think!
About the error with the actual shuffling, I guess shuffling the array in-place should be a bit faster? I don't see a particular issue with modifying the original x, y for training/eval, although it is better to keep x untouched for the prediction.
		</comment>
		<comment id='3' author='de-code' date='2019-12-24T22:29:01Z'>
		
The first epoch will use non shuffled data, then every epoch will take the shuffled data at the end of the previous epoch, so this is fine - every epoch will use different ordered data.
keras.utils.Sequence only implements a on_epoch_end so this is the way to do it I think!

I understand. If I'm not wrong the idea of shuffling is to have more random data from one epoque to the following. In this way, I assume, the model might require more time to converge but should get more robust. Right?
I noticed that currently we shuffle the whole self.x, so the whole training set. There are possibility that the same data used for a previous epoque, is used again. I wonder we should shuffle only the remaining data instead. Or shuffle at the beginning when the data is loaded in the generator?

About the error with the actual shuffling, I guess shuffling the array in-place should be a bit faster? I don't see a particular issue with modifying the original x, y for training/eval, although it is better to keep x untouched for the prediction.

Good point, although, currently, in the prediction (when self.y is None I suppose) it seems that the x array is shuffled in place:  np.random.shuffle(self.x)
What about that we shuffle in-place both x and y (and features) only in training mode (when Y is not None)?
		</comment>
		<comment id='4' author='de-code' date='2019-12-30T19:47:39Z'>
		
I understand. If I'm not wrong the idea of shuffling is to have more random data from one epoque to the following. In this way, I assume, the model might require more time to converge but should get more robust.

No it's more that the order in training data is often not neutral. For instance the same entity will be repeated in a document, so if we take sentences in the order of the documents, they will tend to have a biased distribution. This is the case for CoNLL 2003 NER of FTB. It has no impact on training time I think.

I noticed that currently we shuffle the whole self.x, so the whole training set. There are possibility that the same data used for a previous epoque, is used again. I wonder we should shuffle only the remaining data instead. Or shuffle at the beginning when the data is loaded in the generator?

The same/complete data is always reused at each epoch (one epoch uses all traning data, divided in batch, each batch process in one "step"). So shuffling at the end of an epoch is correct (we don't shuffle at the end of a "step").

What about that we shuffle in-place both x and y (and features) only in training mode (when Y is not None)?

Actually thinking about it, I would never shuffle in place, in the case of n-fold training where we select a different dev set for each fold, I think it will mess everything. So the option "just use the view" is safer I think in every case (even if it's a bit slower, the impact should be negligible as compared to all the other crazy heavy computations elsewhere).
		</comment>
		<comment id='5' author='de-code' date='2020-01-03T10:50:18Z'>
		Generally I certainly prefer a functional approach and I would definitely think that the data passed to DataGenerator should not be changed. But it should be okay to modify a copy / view in place (as long as it's confined to the DataGenerator). I wonder whether the following might lead to a long chain of views with every epoch:
self.x, self.y = self.shuffle_pair(self.x, self.y)
I don't know enough about how numpy views work - whether a view of a view would reference the passed in view or whether it references only the original array. A simple solution (if it needed one) would be to always pass in the original x and y.
		</comment>
	</comments>
</bug>