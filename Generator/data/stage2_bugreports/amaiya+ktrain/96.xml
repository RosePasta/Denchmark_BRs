<bug id='96' author='aeshapar' open_date='2020-03-23T02:45:06Z' closed_time='2020-03-30T19:44:05Z'>
	<summary>Using ktrain for XLM-R preprocessing</summary>
	<description>
Hello,
Does ktrain provide a way to preprocess data for XLM-R?
	</description>
	<comments>
		<comment id='1' author='aeshapar' date='2020-03-23T19:55:56Z'>
		XLM-Roberta should be supported. However, I just noticed that there are two bugs.  The first is what appears to be a bug in the transformers library that causes the following to not work:
MODEL_NAME = 'xlm-roberta-base'
from transformers import *
# this doen't work i v2.5.1 of transformers
model = TFXLMRobertaForSequenceClassification.from_pretrained(MODEL_NAME)
# this also doesn't work in v.2.5.1 of transformers
model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_NAME)
The second is a bug in ktrain that causes XLM-Roberta models to be misinterpreted as XLM.
I will fix the second bug in the next release.  However, XLM-R will still not work until the other bug is fixed in the transformers library.
		</comment>
		<comment id='2' author='aeshapar' date='2020-03-30T19:44:04Z'>
		I've fixed the ktrain issue with this.  But, as of transformers v2.7.0, these transformer calls still don't work:
MODEL_NAME = 'xlm-roberta-base'
from transformers import *
# this doen't work in v2.7.0 of transformers
model = TFXLMRobertaForSequenceClassification.from_pretrained(MODEL_NAME)
# this also doesn't work in v.2.7.0 of transformers
model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_NAME)
If you need this model, you might consider opening a new issue on the transformers GitHub repo.  I'll close this issue for now.
		</comment>
		<comment id='3' author='aeshapar' date='2020-03-31T19:56:46Z'>
		The reason that doesn't work is because the TF model expects a different path. It should work if you try: TFXLMRobertaForSequenceClassification.from_pretrained("jplu/tf-xlm-roberta-base") instead of passing in "xlm-roberta-base".
		</comment>
	</comments>
</bug>