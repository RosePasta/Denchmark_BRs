<bug id='218' author='Konstantina-Lazaridou' open_date='2020-07-30T07:38:07Z' closed_time='2020-07-30T13:17:47Z'>
	<summary>Cuda runtime error: device-side assert triggered</summary>
	<description>
I am using the zero shot classifier to predict topics of news articles. I have one class and I split each article into sentences and predict per sentence.
I initially had Runtime errors when calling the predict function (couldn't allocate enough GPU memory), which I solved following the torch.no_grad() suggestion in another issue in this repo. After this problem is gone, I experienced the sigkill error, which I suppose is related to the GPU memory again. This problem is now gone by calling Python's garbage collector after I am done with the predictions of one article.
My current error is another runtime error, but this time when I create the zero shot classifier:
RuntimeError: cuda runtime error : device-side assert triggered.
I am using the 0.18.5 version of ktrain. I use Manjaro, NVIDIA Quadro T2000 GPU, tensorflow 2.1.0, cuda 10.1 (and 10.2, I think 10.1 is used though) and cudnn 7.6.5. My GPU has quick spikes when I suppose the predict method is called (I can tell from watch nvidia-smi). I also double checked with tf.test.gpu_device_name() and my GPU is indeed being used.
Not sure what could cause a cuda error while creating a classifier and not yet using it. Any thoughts?
	</description>
	<comments>
		<comment id='1' author='Konstantina-Lazaridou' date='2020-07-30T07:53:15Z'>
		Should I create the zero shot classifier once for the entire corpus, or once for each document? I am doing the second and maybe this is too much.
		</comment>
		<comment id='2' author='Konstantina-Lazaridou' date='2020-07-30T13:17:47Z'>
		The RuntimeError issue should be resolved in v0.19.1, if you upgrade  to the latest version.
As mentioned in &lt;denchmark-link:https://github.com/amaiya/ktrain/issues/215&gt;#215&lt;/denchmark-link&gt;
 , I was able to reproduce this Runtime Error when supplying longer documents.    In short, I added a  parameter to  which defaults to 512 tokens.  You can increase it if necessary, but the default should be more than adequate for your single sentences.  I will close this issue for now, but please reply or open a new issue if you have further issues.
In my tests, I used a single instance of ZeroShotClassifier and then invoked predict repeatedly on it.  The GPU memory and system memory both remained constant.  So, you can try that if you have memory issues.
		</comment>
	</comments>
</bug>