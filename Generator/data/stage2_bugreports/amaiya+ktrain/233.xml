<bug id='233' author='xirect' open_date='2020-08-25T15:10:30Z' closed_time='2020-08-25T20:18:23Z'>
	<summary>Memory Issues using krain text.Translator()</summary>
	<description>
Hello!
I've been using the Helsinki-NLP opus models for translating text. I have a lot of different news articles which I want to translate from one language to another at the fastest possible speed. I am running into issues right now with a simple for loop which floods my GPU VRAM.
&lt;denchmark-code&gt;from ktrain import text
import ktrain

translator = text.Translator(model_name="Helsinki-NLP/opus-mt-nl-en")
src_text = ["Text one", "Text two"] 
for item in src_text:
    translated = translator.translate(item)
&lt;/denchmark-code&gt;

With up to thousands items in "src_text", after around 10 to 15 articles my 6GB GPU is flooded. I tried clearing my garbage collectors and emptying the cuda cache.
	</description>
	<comments>
		<comment id='1' author='xirect' date='2020-08-25T16:20:56Z'>
		Thanks for posting issue. I think I know the cause of the problem and will release a fix a little later today.  I'll reply to this thread when it is released.
		</comment>
		<comment id='2' author='xirect' date='2020-08-25T20:18:23Z'>
		I have released an update in v0.20.1 that should hopefully fix the issue.  Please upgrade ktrain and try again.  I have tested this in a for loop on many articles and did not experience any memory issues.
Note that documents are split up into sentences and treated as a batch of examples that are fed together to model.  If there is a long document, this may result in a large batch which may exceed your 6GB of GPU memory.  If this is the case, you can split up the document into sentences, create smaller chunks (or batches) of sentences, and feed these chunks (i.e., batches) to translate method.  By feeding smaller batches to translate, you will use less memory.    You can use ktrain.text.textutils.sent_tokenize to split up document into sentences (or use NLTK).
I will close this issue, but please feel free to reply in this thread if you have further issues.
		</comment>
	</comments>
</bug>