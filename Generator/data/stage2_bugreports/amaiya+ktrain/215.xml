<bug id='215' author='mlaugharn' open_date='2020-07-28T20:27:36Z' closed_time='2020-07-28T21:50:56Z'>
	<summary>Set with torch.no_grad() automatically for zero shot learner</summary>
	<description>
Hi I was running into some OOM errors when I was using the ZSL classifier after a certain number of inputs. I figured out that this was due to a memory leak with the BART mnli model, and I traced this to be due to pytorch collecting gradients.
Doing
&lt;denchmark-code&gt;with torch.no_grad():
    for i in range(1000..):
        zsl.predict(document[i], topic_strings=topics)
&lt;/denchmark-code&gt;

then worked afterwards. Thanks for creating such a useful library!
	</description>
	<comments>
		<comment id='1' author='mlaugharn' date='2020-07-28T20:40:10Z'>
		Thanks a lot for not only posting the problem but also investigating and posting the solution.  A new version of ktrain is being released this week, and I'll include this fix.  Thanks again!
		</comment>
		<comment id='2' author='mlaugharn' date='2020-07-28T20:42:17Z'>
		Ahh looks like I spoke too soon. I got another bug a bit later on..
&lt;denchmark-code&gt;/usr/local/lib/python3.6/dist-packages/ktrain/text/zsl/core.py in predict(self, doc, topic_strings, include_labels, batch_size)
     61                 pairs.append( (premise, hypothesis) )
     62             batch = self.tokenizer.batch_encode_plus(pairs, return_tensors='pt', padding='longest').to(self.torch_device)
---&gt; 63             logits = self.model(batch['input_ids'], attention_mask=batch['attention_mask'])[0]
     64             entail_contradiction_logits = logits[:,[0,2]]
     65             probs = entail_contradiction_logits.softmax(dim=1)

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    548             result = self._slow_forward(*input, **kwargs)
    549         else:
--&gt; 550             result = self.forward(*input, **kwargs)
    551         for hook in self._forward_hooks.values():
    552             hook_result = hook(self, input, result)

/usr/local/lib/python3.6/dist-packages/transformers/modeling_bart.py in forward(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, labels, output_attentions, output_hidden_states, use_cache)
   1134         x = outputs[0]  # last hidden state
   1135         eos_mask = input_ids.eq(self.config.eos_token_id)
-&gt; 1136         if len(torch.unique(eos_mask.sum(1))) &gt; 1:
   1137             raise ValueError("All examples must have the same number of &lt;eos&gt; tokens.")
   1138         sentence_representation = x[eos_mask, :].view(x.size(0), -1, x.size(-1))[:, -1, :]

/usr/local/lib/python3.6/dist-packages/torch/_jit_internal.py in fn(*args, **kwargs)
    207             return if_true(*args, **kwargs)
    208         else:
--&gt; 209             return if_false(*args, **kwargs)
    210 
    211     if if_true.__doc__ is None and if_false.__doc__ is not None:

/usr/local/lib/python3.6/dist-packages/torch/_jit_internal.py in fn(*args, **kwargs)
    207             return if_true(*args, **kwargs)
    208         else:
--&gt; 209             return if_false(*args, **kwargs)
    210 
    211     if if_true.__doc__ is None and if_false.__doc__ is not None:

/usr/local/lib/python3.6/dist-packages/torch/functional.py in _return_output(input, sorted, return_inverse, return_counts, dim)
    535             return _unique_impl(input, sorted, return_inverse, return_counts, dim)
    536 
--&gt; 537     output, _, _ = _unique_impl(input, sorted, return_inverse, return_counts, dim)
    538     return output
    539 

/usr/local/lib/python3.6/dist-packages/torch/functional.py in _unique_impl(input, sorted, return_inverse, return_counts, dim)
    513             sorted=sorted,
    514             return_inverse=return_inverse,
--&gt; 515             return_counts=return_counts,
    516         )
    517     return output, inverse_indices, counts

RuntimeError: transform: failed to synchronize: cudaErrorAssert: device-side assert triggered
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='mlaugharn' date='2020-07-28T21:35:07Z'>
		I think it might have just been a particle article that has a bug on encoding. I skipped it and it seems to work
		</comment>
		<comment id='4' author='mlaugharn' date='2020-07-28T21:50:56Z'>
		I was actually able to reproduce the second issue when supplying zsl.predict with longer documents.  I have added a max_length parameter to zsl.predict that defaults to 512 tokens to prevent such errors.  You can adjust as necessary.  It'll be included in the next release.  Thanks.
		</comment>
	</comments>
</bug>