<bug id='216' author='Konstantina-Lazaridou' open_date='2020-07-29T20:52:32Z' closed_time='2020-07-30T07:32:09Z'>
	<summary>Zero shot classifier sigkill</summary>
	<description>
I am using the zero shot classifier to predict topics of news articles, and for a specific article I am getting an error that stops my script and forces it to exit abnormally with code 137. I am unsure whether the problem is the specific article or the issue was built up until that point (it's the 20th article I am classifying). I use one class and I split each article into sentences and predict per sentence.
I initially had  errors (couldn't allocate enough GPU memory), which I solved following the  suggestion in this &lt;denchmark-link:https://github.com/amaiya/ktrain/issues/215&gt;issue&lt;/denchmark-link&gt;
. After this problem is gone, I experience the sigkill error, which I suppose is related to GPU memory again. I have also increased the  to 32, because the prediction takes approximately one minute per 10 articles.
Any thoughts? I am now checking the size of the sentences, in case one sentence is too long and it creates memory issues. So far, even some unusually long sentences (~1000 characters) didn't cause an error and worked fine with the prediction.
My code:
&lt;denchmark-code&gt;zeroshotclassifier = text.ZeroShotClassifier()

relevant_sentences = []

 with torch.no_grad():
      # tokenize the article into sentences and find relevant sentences to keywords
        for sentence in nltk.sent_tokenize(content):
            print(len(sentence))
            try:
                prediction = zeroshotclassifier.predict(doc=sentence,
                                                        topic_strings=keywords,
                                                        include_labels=True,
                                                        batch_size=32)
                score = prediction[0][-1]
                if score &gt; 0.90:
                    relevant_sentences.append([sentence, prediction])
            except RuntimeError as e:
                print("CUDA error: {}".format(e))
        gc.collect()
    # sort sentences based on their score
    relevant_sentences = [x[0] for x in sorted(relevant_sentences, key=lambda x: x[1], reverse=True)]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Konstantina-Lazaridou' date='2020-07-29T21:05:36Z'>
		Hello:
I'll take a look and try to reproduce it.

How many topics are included in your keywords list that you are supplying to topic_strings parameter?
A new version of ktrain was just released (v0.19.0).  Are you using this latest version or a previous version (e.g., v0.18.5)?

		</comment>
		<comment id='2' author='Konstantina-Lazaridou' date='2020-07-29T22:03:58Z'>
		I wasn't able to reproduce this even when using 800 topics and a documents that are each over 1000 words. I was running on an Ubuntu 18.04 system with a TITAN V GPU.  Both the GPU memory and main memory remain stable and constant during the repeated calls to predict.

Can you please try your code again with the latest version of ktrain, which is v0.19.1 as of right now?
If you have a self-contained example to reproduce the problem, you can send to me and I can try that on my end.

		</comment>
		<comment id='3' author='Konstantina-Lazaridou' date='2020-07-30T07:29:34Z'>
		Thanks a lot for your time. I have only one keyword in the topic_strings parameter and I am using the 0.18.5  version. I use Manjaro, NVIDIA Quadro T2000 GPU, tensorflow 2.1.0, cuda 10.1 (and 10.2, I think 10.1 is used though) and cudnn 7.6.5. My GPU has quick spikes when I suppose the predict method is called (I can tell from watch nvidia-smi). I also double checked with tf.test.gpu_device_name() and my GPU is indeed being used.
I experience another error now, but when I create the zer shot classifier:
RuntimeError: cuda runtime error : device-side assert triggered. This is new - the previous runtime error I had was when I called predict and it didn't mention "device-side assert triggered".
I will try the next version of ktrain and also to write a self-contained example if possible.
		</comment>
		<comment id='4' author='Konstantina-Lazaridou' date='2020-07-30T07:32:09Z'>
		I think I will close this issue, because I checked earlier what happens if I remove the gc.collect() line from my code and actually the sigkill occurs again. So, I suppose this is the solution - at least for now. I am calling the garbage collector after I am done with the sentence-based predictions of each article. I will open another issue for the runtime error that appears during the creation of the zero shot classifier.
		</comment>
	</comments>
</bug>