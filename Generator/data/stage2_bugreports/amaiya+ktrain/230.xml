<bug id='230' author='hiber-niu' open_date='2020-08-16T09:25:04Z' closed_time='2020-08-19T11:38:08Z'>
	<summary>Train  don't fully utilized gpu resources and is time consuming</summary>
	<description>
Hi, when i run my python script using following settings, the gpu card 3 and 4 is not fully utilized, and the whole training process is slow.  Is this is normal or i miss someting?
Thank you for any help.
|   3  Tesla V100-PCIE...  On   | 0000:09:00.0     Off |                    0 |
| N/A   33C    P0    37W / 250W |    419MiB / 32510MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   4  Tesla V100-PCIE...  On   | 0000:85:00.0     Off |                    0 |
| N/A   34C    P0    37W / 250W |    419MiB / 32510MiB |      0%      Default |
os.environ["CUDA_VISIBLE_DEVICES"] = '3,4'

max_len = 32
batch_size = 32
# recommended learning rate for Adam 5e-5, 3e-5, 2e-5
learning_rate = 2e-5
	</description>
	<comments>
		<comment id='1' author='hiber-niu' date='2020-08-16T16:48:26Z'>
		Hi,
No, this isn't normal, as GPU should be used if available.    If you post the code in its entirety instead of just the top few lines of settings, I can take a look.   Have you verified the GPU is used when using a single GPU?
		</comment>
		<comment id='2' author='hiber-niu' date='2020-08-17T02:32:09Z'>
		yes, I tested a single GPU and it has same phenomenon as above.
And when script run t.get_classifier() throws

/root/anaconda3/envs/ktrain/lib/python3.7/site-packages/ktrain/text/preprocessor.py:994: UserWarning: no class labels were provided - treating as regression warnings.warn('no class labels were provided - treating as regression')

data I used was as follows:
&lt;denchmark-code&gt;label_a	一点二九乘以二十四等于多少
label_b	这首歌不好听给俺家宝宝唱一首酒醉的蝴蝶八
label_d	你给我我再傻了
label_e	嗯设置5:50
&lt;/denchmark-code&gt;

The link i was follow was: &lt;denchmark-link:https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed&gt;https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed&lt;/denchmark-link&gt;

And here is all my code
import os

import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras import Model
from tensorflow.keras.layers import Input, Flatten, Dense
import tensorflow as tf
import numpy as np
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical

import ktrain
from ktrain import text


os.environ["CUDA_VISIBLE_DEVICES"] = '3,4'

max_len = 32
batch_size = 32
# recommended learning rate for Adam 5e-5, 3e-5, 2e-5
learning_rate = 2e-5
# we will do just 1 epoch for illustration, though multiple epochs might be better as long as we will not overfit the model
number_of_epochs = 2

df = pd.read_csv('data/pv_function_all.txt', delimiter='\t', names=['label', 'query'])
print('Number of training sentences: {:,}\n'.format(df.shape[0]))

queries = df['query'].values
labels = df['label'].values
class_names = np.unique(labels)
n_classes = len(class_names)
n_examples = len(labels)

random_seed = 31
x_train, x_val, y_train, y_val = train_test_split(queries,
                                                  labels,
                                                  shuffle=True, test_size = 0.2,
                                                  random_state=random_seed,
                                                  stratify=labels)

t = text.Transformer('bert-base-chinese', maxlen=max_len)
trn = t.preprocess_train(x_train, y_train)
val = t.preprocess_test(x_val, y_val)

model = t.get_classifier()
learner = ktrain.get_learner(model, train_data=trn, val_data=val,
                             batch_size=batch_size)

# learner.lr_find(show_plot=True, max_epochs=2)
learner.fit_onecycle(5e-5, number_of_epochs)
learner.view_top_losses(n=20, preproc=t)

# predict
predictor = ktrain.get_predictor(learner.model, preproc=t)
predictor.save('./model/pv_predictor')
#predictor.predict(query)
#predictor.explain(query)
#predictor = ktrain.load_predict('')
		</comment>
		<comment id='3' author='hiber-niu' date='2020-08-17T14:23:52Z'>
		First, as the warning indicates, if this is a classification problem, you should supply a class_names argument (or the deprecated classes argument) to Transformer call which is what the article shows.  Otherwise, numerical labels (i.e., class IDs) are treated as regression target.  Please follow this tutorial, where this is explained.
First, the warning about  no class labels were provided - treating as regression is the result of a bug that has been fixed in v0.19.8.  Please upgrade ktrain, as this may have caused your model to be misconfigured in scenarios where labels in y_train and y_test are in string format (instead of being class IDs or 1-hot-encoded vectors).
Second, to use multiple GPUs, you'll need to follow the &lt;denchmark-link:https://www.tensorflow.org/guide/distributed_training&gt;TensorFlow documentation on distributed training&lt;/denchmark-link&gt;
. See &lt;denchmark-link:https://github.com/amaiya/ktrain/blob/master/FAQ.md#how-do-i-train-using-multiple-gpus&gt;this FAQ entry&lt;/denchmark-link&gt;
 for a  example.
Third, if running the example below that uses one GPU, you should see the GPU being used when running nvidia-smi during training (not data-prerpocessing):
import os
os.environ["CUDA_VISIBLE_DEVICES"] = '0'
# load text data
categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']
from sklearn.datasets import fetch_20newsgroups
train_b = fetch_20newsgroups(subset='train', categories=categories, shuffle=True)
test_b = fetch_20newsgroups(subset='test',categories=categories, shuffle=True)
(x_train, y_train) = (train_b.data, train_b.target)
(x_test, y_test) = (test_b.data, test_b.target)

# build, train, and validate model (Transformer is wrapper around transformers library)
import ktrain
from ktrain import text
MODEL_NAME = 'distilbert-base-uncased'
t = text.Transformer(MODEL_NAME, maxlen=500, class_names=train_b.target_names)
trn = t.preprocess_train(x_train, y_train)
val = t.preprocess_test(x_test, y_test)
model = t.get_classifier()
learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6)
learner.fit_onecycle(5e-5, 1)
&lt;denchmark-code&gt;user@server:~$ nvidia-smi
Mon Aug 17 10:20:40 2020       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  TITAN V             Off  | 00000000:21:00.0 Off |                  N/A |
| 51%   74C    P2   192W / 250W |  11671MiB / 12066MiB |     89%      Default |
+-------------------------------+----------------------+----------------------+
|   1  TITAN V             Off  | 00000000:31:00.0 Off |                  N/A |
| 29%   42C    P0    35W / 250W |      0MiB / 12066MiB |      7%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     68494      C   /usr/bin/python3                           11659MiB |
+-----------------------------------------------------------------------------+
&lt;/denchmark-code&gt;

If not, then your TensorFlow GPU setup is not correct, in which case you should follow &lt;denchmark-link:https://www.tensorflow.org/install/gpu&gt;these instructions&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='4' author='hiber-niu' date='2020-08-18T00:25:11Z'>
		After just noticing that your dataset has string labels (e.g., label_a, label_b, etc.), I realized that the warning about text regression (i.e.,  UserWarning: no class labels were provided - treating as regression) should NOT be displayed in your case and that this is the result of a bug that affected the way the model is configured. This may be part of your problem, as I fear your task was erroneously being treated as an invalid regression problem somehow.   The error only arises when the labels provided in y_train and y_test are string labels (instead of integer labels or being 1-hot-encoded).
The bug has been fixed in version &gt;=0.19.8.
The following simple example employs the use of string label just like your dataset.
Can you please update ktrain to 0.19.8 and re-try with this example below?  If you're GPU setup is correct, then you should see the GPU being utilized.
import os
os.environ["CUDA_VISIBLE_DEVICES"] = '1'
# load text data
categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']
from sklearn.datasets import fetch_20newsgroups
train_b = fetch_20newsgroups(subset='train', categories=categories, shuffle=True)
test_b = fetch_20newsgroups(subset='test',categories=categories, shuffle=True)
(x_train, y_train) = (train_b.data, train_b.target)
(x_test, y_test) = (test_b.data, test_b.target)

# convert to string labels like your dataset
y_train = [train_b.target_names[y] for y in y_train]
y_test = [train_b.target_names[y] for y in y_test]

# build, train, and validate model (Transformer is wrapper around transformers library)
import ktrain
from ktrain import text
MODEL_NAME = 'distilbert-base-uncased'
t = text.Transformer(MODEL_NAME, maxlen=500)
trn = t.preprocess_train(x_train, y_train)
val = t.preprocess_test(x_test, y_test)
model = t.get_classifier()
learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6)
learner.fit_onecycle(5e-5, 1)
Thanks for posting this issue.
		</comment>
		<comment id='5' author='hiber-niu' date='2020-08-19T07:24:06Z'>
		Thank your for your detailed reply, I found my cuda driver version is 10.0, which only support tensorflow2.0 and cannot use GPU for ktrain &gt;=0.19.8.
The machine I used cann't upgrade cuda recently. So as soon as I have a right environment, I will try again and post its result.
By the way, Is there any ktrain version that I can use for tensorflow 2.0?
		</comment>
		<comment id='6' author='hiber-niu' date='2020-08-19T11:38:08Z'>
		You can use TF 2.0 and CUDA 10.0 with the current version of ktrain (which is 0.19.9 as of today).   After ktrain is installed (which will automatically install TensorFlow &gt;=2.1.0), you can downgrade Tensorflow to 2.0 if you are using Python 3.6 or 3.7 (you can ignore any errors that pip outputs).  Everything should still work with the latest version of ktrain.  Do note that there are some bugs in TensorFlow 2.0 that affect certain things (e.g., slower image classification, I believe is one thing).
		</comment>
		<comment id='7' author='hiber-niu' date='2020-09-01T03:20:22Z'>
		This GPU issue is totally my fault with wrong running environment, after update to proper tf/ktrain/cuda version, this issue doesn't occur.
		</comment>
	</comments>
</bug>