<bug id='270' author='jorgeutd' open_date='2020-10-26T16:44:03Z' closed_time='2020-10-26T19:53:19Z'>
	<summary>InvalidArgumentError: indices[0,512] = 512 is not in [0, 512) [Op:ResourceGather]</summary>
	<description>
Hello Ktrain developers, I have been trying to use the QA system on my own data but I keep getting the following error:
~\Miniconda3\envs\k_train\lib\site-packages\tensorflow\python\framework\ops.py in raise_from_not_ok_status(e, name)
6841   message = e.message + (" name: " + name if name is not None else "")
6842   # pylint: disable=protected-access
-&gt; 6843   six.raise_from(core._status_to_exception(e.code, message), None)
6844   # pylint: enable=protected-access
6845
~\AppData\Roaming\Python\Python37\site-packages\six.py in raise_from(value, from_value)
InvalidArgumentError: indices[0,512] = 512 is not in [0, 512) [Op:ResourceGather]
This happen after I write a question to the QA object:
answers = qa.ask('what programs my insurance covers?', n_docs_considered=20, n_answers=25,
rerank_threshold=0.01)
I passed the dataset as a list, so not sure what is this.
Here is the full error:
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

InvalidArgumentError                      Traceback (most recent call last)
 in 
~\OneDrive - Quantum Health\jorge\ktrain\ktrain\text\qa\core.py in ask(self, question, n_docs_considered, n_answers, rerank_threshold)
348                 answer['similarity_score'] = 0.0
349                 continue
--&gt; 350             v2 = self.te.embed(answer['full_answer'], word_level=False)
351             score = v1 @ v2.T / (np.linalg.norm(v1)*np.linalg.norm(v2))
352             answer['similarity_score'] = float(np.squeeze(score))
~\OneDrive - Quantum Health\jorge\ktrain\ktrain\text\preprocessor.py in embed(self, texts, word_level)
1243         all_input_ids = np.array(all_input_ids)
1244         all_input_masks = np.array(all_input_masks)
-&gt; 1245         outputs = self.model(all_input_ids, attention_mask=all_input_masks)
1246         hidden_states = outputs[-1] # output_hidden_states=True
1247
~\Miniconda3\envs\k_train\lib\site-packages\tensorflow\python\keras\engine\base_layer.py in call(self, *args, **kwargs)
983
984         with ops.enable_auto_cast_variables(self._compute_dtype_object):
--&gt; 985           outputs = call_fn(inputs, *args, **kwargs)
986
987         if self._activity_regularizer:
~\Miniconda3\envs\k_train\lib\site-packages\transformers\modeling_tf_bert.py in call(self, inputs, **kwargs)
795     )
796     def call(self, inputs, **kwargs):
--&gt; 797         outputs = self.bert(inputs, **kwargs)
798
799         return outputs
~\Miniconda3\envs\k_train\lib\site-packages\tensorflow\python\keras\engine\base_layer.py in call(self, *args, **kwargs)
983
984         with ops.enable_auto_cast_variables(self._compute_dtype_object):
--&gt; 985           outputs = call_fn(inputs, *args, **kwargs)
986
987         if self._activity_regularizer:
~\Miniconda3\envs\k_train\lib\site-packages\transformers\modeling_tf_bert.py in call(self, inputs, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, training)
592             token_type_ids = tf.fill(input_shape, 0)
593
--&gt; 594         embedding_output = self.embeddings(input_ids, position_ids, token_type_ids, inputs_embeds, training=training)
595
596         # We create a 3D attention mask from a 2D tensor mask.
~\Miniconda3\envs\k_train\lib\site-packages\tensorflow\python\keras\engine\base_layer.py in call(self, *args, **kwargs)
983
984         with ops.enable_auto_cast_variables(self._compute_dtype_object):
--&gt; 985           outputs = call_fn(inputs, *args, **kwargs)
986
987         if self._activity_regularizer:
~\Miniconda3\envs\k_train\lib\site-packages\transformers\modeling_tf_bert.py in call(self, input_ids, position_ids, token_type_ids, inputs_embeds, mode, training)
153         """
154         if mode == "embedding":
--&gt; 155             return self._embedding(input_ids, position_ids, token_type_ids, inputs_embeds, training=training)
156         elif mode == "linear":
157             return self._linear(input_ids)
~\Miniconda3\envs\k_train\lib\site-packages\transformers\modeling_tf_bert.py in _embedding(self, input_ids, position_ids, token_type_ids, inputs_embeds, training)
179             inputs_embeds = tf.gather(self.word_embeddings, input_ids)
180
--&gt; 181         position_embeddings = tf.cast(self.position_embeddings(position_ids), inputs_embeds.dtype)
182         token_type_embeddings = tf.cast(self.token_type_embeddings(token_type_ids), inputs_embeds.dtype)
183         embeddings = inputs_embeds + position_embeddings + token_type_embeddings
~\Miniconda3\envs\k_train\lib\site-packages\tensorflow\python\keras\engine\base_layer.py in call(self, *args, **kwargs)
983
984         with ops.enable_auto_cast_variables(self._compute_dtype_object):
--&gt; 985           outputs = call_fn(inputs, *args, **kwargs)
986
987         if self._activity_regularizer:
~\Miniconda3\envs\k_train\lib\site-packages\tensorflow\python\keras\layers\embeddings.py in call(self, inputs)
187       out = embedding_ops.embedding_lookup_v2(self.embeddings.variables, inputs)
188     else:
--&gt; 189       out = embedding_ops.embedding_lookup_v2(self.embeddings, inputs)
190     return out
191
~\Miniconda3\envs\k_train\lib\site-packages\tensorflow\python\util\dispatch.py in wrapper(*args, **kwargs)
199     """Call target, and fall back on dispatchers if there is a TypeError."""
200     try:
--&gt; 201       return target(*args, **kwargs)
202     except (TypeError, ValueError):
203       # Note: convert_to_eager_tensor currently raises a ValueError, not a
~\Miniconda3\envs\k_train\lib\site-packages\tensorflow\python\ops\embedding_ops.py in embedding_lookup_v2(params, ids, max_norm, name)
392     ValueError: If params is empty.
393   """
--&gt; 394   return embedding_lookup(params, ids, "div", name, max_norm=max_norm)
395
396
~\Miniconda3\envs\k_train\lib\site-packages\tensorflow\python\util\dispatch.py in wrapper(*args, **kwargs)
199     """Call target, and fall back on dispatchers if there is a TypeError."""
200     try:
--&gt; 201       return target(*args, **kwargs)
202     except (TypeError, ValueError):
203       # Note: convert_to_eager_tensor currently raises a ValueError, not a
~\Miniconda3\envs\k_train\lib\site-packages\tensorflow\python\ops\embedding_ops.py in embedding_lookup(params, ids, partition_strategy, name, validate_indices, max_norm)
326       name=name,
327       max_norm=max_norm,
--&gt; 328       transform_fn=None)
329
330
~\Miniconda3\envs\k_train\lib\site-packages\tensorflow\python\ops\embedding_ops.py in _embedding_lookup_and_transform(params, ids, partition_strategy, name, max_norm, transform_fn)
136       with ops.colocate_with(params[0]):
137         result = _clip(
--&gt; 138             array_ops.gather(params[0], ids, name=name), ids, max_norm)
139         if transform_fn:
140           result = transform_fn(result)
~\Miniconda3\envs\k_train\lib\site-packages\tensorflow\python\util\dispatch.py in wrapper(*args, **kwargs)
199     """Call target, and fall back on dispatchers if there is a TypeError."""
200     try:
--&gt; 201       return target(*args, **kwargs)
202     except (TypeError, ValueError):
203       # Note: convert_to_eager_tensor currently raises a ValueError, not a
~\Miniconda3\envs\k_train\lib\site-packages\tensorflow\python\ops\array_ops.py in gather(failed resolving arguments)
4674     # TODO(apassos) find a less bad way of detecting resource variables
4675     # without introducing a circular dependency.
-&gt; 4676     return params.sparse_read(indices, name=name)
4677   except AttributeError:
4678     return gen_array_ops.gather_v2(params, indices, axis, name=name)
~\Miniconda3\envs\k_train\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py in sparse_read(self, indices, name)
686       variable_accessed(self)
687       value = gen_resource_variable_ops.resource_gather(
--&gt; 688           self._handle, indices, dtype=self._dtype, name=name)
689
690       if self._dtype == dtypes.variant:
~\Miniconda3\envs\k_train\lib\site-packages\tensorflow\python\ops\gen_resource_variable_ops.py in resource_gather(resource, indices, dtype, batch_dims, validate_indices, name)
554       return _result
555     except _core._NotOkStatusException as e:
--&gt; 556       _ops.raise_from_not_ok_status(e, name)
557     except _core._FallbackException:
558       pass
~\Miniconda3\envs\k_train\lib\site-packages\tensorflow\python\framework\ops.py in raise_from_not_ok_status(e, name)
6841   message = e.message + (" name: " + name if name is not None else "")
6842   # pylint: disable=protected-access
-&gt; 6843   six.raise_from(core._status_to_exception(e.code, message), None)
6844   # pylint: enable=protected-access
6845
~\AppData\Roaming\Python\Python37\site-packages\six.py in raise_from(value, from_value)
InvalidArgumentError: indices[0,532] = 532 is not in [0, 512) [Op:ResourceGather]
	</description>
	<comments>
		<comment id='1' author='jorgeutd' date='2020-10-26T17:21:28Z'>
		Hello:  I wasn't able to reproduce this problem using the latest version of ktrain.
Please reply to this message and include ALL calls to ktrain (including calls to text.SimpleQA.index_from_* ) and please also provide which versions of ktrain, TensorFlow, and transformers you're using.
I believe the problem is happening because at least one of the predicted (full) answers exceeds 512 characters (the limit for BERT), which causes problems when computing an embedding for the answer.
This will be fixed in the next release.
In the mean time, a temporary workaround that should work is to turn re-ranking off with rerank_threshold=None.
		</comment>
		<comment id='2' author='jorgeutd' date='2020-10-26T18:05:01Z'>
		tf.version==2.3.0
ktrain.version = 0.21.3
transformers.version=3.4.0
calls:
text.SimpleQA.initialize_index(INDEXDIR)
text.SimpleQA.index_from_list(docs, INDEXDIR, commit_every=len(docs),
multisegment=True, procs=4 # these args speed up indexing
# this slows indexing but speeds up answer retrieval
)
qa = text.SimpleQA(INDEXDIR)
answers = qa.ask('who is your provider?', n_docs_considered=20, n_answers=25,
rerank_threshold=0.01)
I am not sure why I can't upgrade to ktrain=0.23.0
		</comment>
		<comment id='3' author='jorgeutd' date='2020-10-26T18:34:46Z'>
		As mentioned in the edit above, the issue is related to the answer sentence exceeding the max length of BERT, which is a bug.  This will be fixed in the next version (v0.23.1), which will be released shortly.  A temporary workaround is to set rerank_threshold=None when invoking ask.
What is the ERROR you're getting when you try to upgrade to 0.23.0, though?    This version of ktrain (0.23.x) has much faster answer retrieval in QA module, so it is strongly recommended that you upgrade if using ktrain for Question-Answering.
		</comment>
		<comment id='4' author='jorgeutd' date='2020-10-26T18:50:09Z'>
		Thank you so much for all the help with this.
I am not sure what exaclty happening, when I ran in cmd (windows): pip freeze | findstr ktrain it does returns ktrain==0.23.0
but when I import in Jupyter Notebook shows: the 0.21.3 version
		</comment>
		<comment id='5' author='jorgeutd' date='2020-10-26T19:53:19Z'>
		This issue should now be fixed in ktrain&gt;=0.23.1. Thanks for reporting issue.
Regarding your problem with upgrading ktrain, maybe try running !pip install -U ktrain in a Jupyter notebook cell.
		</comment>
		<comment id='6' author='jorgeutd' date='2020-10-28T13:49:25Z'>
		Everything is working perfect now. Thank you so much.
		</comment>
	</comments>
</bug>