<bug id='2729' author='ninja18' open_date='2019-11-11T16:04:14Z' closed_time='2019-12-30T12:36:56Z'>
	<summary>Output pointer to pointer of an operation is duplicated twice</summary>
	<description>
Describe the bug
When a double pointer is operated with some function/operation the output tensor double pointer is stored twice in the worker containing the double pointer
To Reproduce
Steps to reproduce the behavior:
follow through the tutorial part 03 python notebook
exact image of reproducing step provided below
Expected behavior
The worker who have double pointer must store the output of the operation only once
Screenshots
&lt;denchmark-link:https://user-images.githubusercontent.com/19199814/68601735-f0ea1900-04ca-11ea-9b33-7cef52abaea2.png&gt;&lt;/denchmark-link&gt;

Desktop (please complete the following information):

OS: OSX Mojave

	</description>
	<comments>
		<comment id='1' author='ninja18' date='2019-11-13T23:06:38Z'>
		Hi, I think this is due to the object pointer in Alice having to be unwrapped  and then re-wrapped when getting the response.
However, when re-wrapping, the object is incorrectly registered to make the first copy in Alice (due to the default setting of &lt;denchmark-link:https://github.com/OpenMined/PySyft/blob/1cf824502fc83ef4b8e17135458a14d35753c745/syft/generic/pointers/object_pointer.py#L163&gt;register=True in  wrap()&lt;/denchmark-link&gt;
):
&lt;denchmark-code&gt;class ObjectPointer(AbstractObject):
    ...
    def wrap(self, register=True, type=None, **kwargs): 
&lt;/denchmark-code&gt;

Then, when we actually want to register the response as we normally would when executing the command, the registered response creates a second copy in Alice.
I'm not super familiar with the code to know if this intended, or actually makes sense (sorry if I've misunderstood something!), but thought it could be helpful.
		</comment>
		<comment id='2' author='ninja18' date='2019-12-17T20:45:32Z'>
		Approach
I currently have implemented a fix where I add a parameter to check if a tensor had been unwrapped and is now getting rewrapped in the hook response (rewrap=True which by default will be False to not break code).
It takes advantage of the &lt;denchmark-link:https://github.com/OpenMined/PySyft/blob/8f7705fbba623827f1edaf5a2a351a973e2e001d/syft/generic/frameworks/hook/hook_args.py#L254&gt;build_rule() in build_wrap_response_from_function()&lt;/denchmark-link&gt;
 to check if the data type is a tensor (and not int, str etc). When it's building a wrap function, it checks if both  on the args passed and , and if so, I set an attribute like  which gets checked in the  function as to whether to register.
Passes tests but is slow?
It currently passes 100% of tests BUT seems inefficient given the point of the build rule is to efficiently check if things need to be wrapped whereas this loops over the args.

Alternatively, I thought we could pass  for these tensors in the hook response, but it seems the &lt;denchmark-link:https://github.com/OpenMined/PySyft/blob/master/syft/frameworks/torch/hook/hook_args.py#L39-L46&gt;lambda for TorchTensor defined in backward_func() as&lt;/denchmark-link&gt;
:
&lt;denchmark-code&gt;backward_func = {
    TorchTensor: lambda i: i.wrap(), # how to give the lambda arg register=False?
    torch.Tensor: lambda i: i.wrap(),
    torch.nn.Parameter: lambda i: torch.nn.Parameter(data=i),
    AutogradTensor: lambda i: AutogradTensor(data=i).on(i, wrap=False),
    LoggingTensor: lambda i: LoggingTensor().on(i, wrap=False),
    PaillierTensor: lambda i: PaillierTensor().on(i, wrap=False),
}
&lt;/denchmark-code&gt;


Here is the code changes:
&lt;denchmark-link:https://github.com/linamnt/PySyft/commit/97a7a24b3c061c99a73482f6c6c9c3ab4cad3d42&gt;linamnt@97a7a24&lt;/denchmark-link&gt;

Do I submit a PR so it can be reviewed formally? or wait til the best approach is figured out?
		</comment>
	</comments>
</bug>