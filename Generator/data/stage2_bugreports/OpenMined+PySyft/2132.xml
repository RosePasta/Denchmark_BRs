<bug id='2132' author='SohamMazumder' open_date='2019-05-09T08:48:17Z' closed_time='2019-06-15T18:50:02Z'>
	<summary>Batchnorm Layer incompatible</summary>
	<description>
I am using trying to the notebook Part 8 - Federated Learning on MNIST using a CNN.ipynb but with a modified model. I have added a batchnorm layer as follows.
'
class Net(nn.Module):
def init(self):
super(Net, self).init()
self.conv1 = nn.Conv2d(1, 20, 5, 1)
self.conv2 = nn.Conv2d(20, 50, 5, 1)
self.fc1 = nn.Linear(4450, 500)
self.fc2 = nn.Linear(500, 10)
self.bn1 = nn.BatchNorm2d(20)
&lt;denchmark-code&gt;def forward(self, x):
    x = F.relu(self.conv1(x))
    x = self.bn1(x)
    x = F.max_pool2d(x, 2, 2)
    x = F.relu((self.conv2(x)))
    x = F.max_pool2d(x, 2, 2)
    x = x.view(-1, 4*4*50)
    x = F.relu(self.fc1(x))
    x = self.fc2(x)
    return F.log_softmax(x, dim=1)`
&lt;/denchmark-code&gt;

I get the following error :
`RuntimeError                              Traceback (most recent call last)
 in ()
4 optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment
5 for epoch in range(1, args.epochs + 1):
----&gt; 6     train(args, model, device, federated_train_loader, optimizer, epoch)
7     test(args, model, device, test_loader)
8
7 frames
 in train(args, model, device, federated_train_loader, optimizer, epoch)
5         data, target = data.to(device), target.to(device)
6         optimizer.zero_grad()
----&gt; 7         output = model(data)
8         loss = F.nll_loss(output, target)
9         loss.backward()
/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in call(self, *input, **kwargs)
487             result = self._slow_forward(*input, **kwargs)
488         else:
--&gt; 489             result = self.forward(*input, **kwargs)
490         for hook in self._forward_hooks.values():
491             hook_result = hook(self, input, result)
 in forward(self, x)
11     def forward(self, x):
12         x = F.relu(self.conv1(x))
---&gt; 13         x = self.bn1(x)
14         x = F.max_pool2d(x, 2, 2)
15         x = F.relu((self.conv2(x)))
/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in call(self, *input, **kwargs)
487             result = self._slow_forward(*input, **kwargs)
488         else:
--&gt; 489             result = self.forward(*input, **kwargs)
490         for hook in self._forward_hooks.values():
491             hook_result = hook(self, input, result)
/usr/local/lib/python3.6/dist-packages/torch/nn/modules/batchnorm.py in forward(self, input)
58     @weak_script_method
59     def forward(self, input):
---&gt; 60         self._check_input_dim(input)
61
62         exponential_average_factor = 0.0
/usr/local/lib/python3.6/dist-packages/torch/nn/modules/batchnorm.py in _check_input_dim(self, input)
239     @weak_script_method
240     def _check_input_dim(self, input):
--&gt; 241         if input.dim() != 4:
242             raise ValueError('expected 4D input (got {}D input)'
243                              .format(input.dim()))
/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook.py in overloaded_native_method(self, *args, **kwargs)
636                 except BaseException as e:
637                     # we can make some errors more descriptive with this method
--&gt; 638                     raise route_method_exception(e, self, args, kwargs)
639
640             else:  # means that there is a wrapper to remove
/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook.py in overloaded_native_method(self, *args, **kwargs)
630                 try:
631                     if isinstance(args, tuple):
--&gt; 632                         response = method(*args, **kwargs)
633                     else:
634                         response = method(args, **kwargs)
RuntimeError: bool value of Tensor with no values is ambiguous`
This error is I believe related to another issue. &lt;denchmark-link:https://github.com/OpenMined/PySyft/issues/2049&gt;#2049&lt;/denchmark-link&gt;

Any help is greatly appreciated!
	</description>
	<comments>
		<comment id='1' author='SohamMazumder' date='2019-05-16T20:50:49Z'>
		&lt;denchmark-link:https://github.com/SohamMazumder&gt;@SohamMazumder&lt;/denchmark-link&gt;
 We haven't hooked BatchNorm yet. So you will have to stick to Linear and CNN layers till then :/
		</comment>
		<comment id='2' author='SohamMazumder' date='2019-06-04T07:33:23Z'>
		I think the .dim() method has been fixed since this issue is open but I have now another error when trying to use Batchnorm with the same modification as above:
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

PureTorchTensorFoundError                 Traceback (most recent call last)
/usr/local/lib/python3.7/site-packages/syft-0.1.17-py3.7.egg/syft/frameworks/torch/tensors/interpreters/native.py in handle_func_command(cls, command)
198             new_args, new_kwargs, new_type, args_type = syft.frameworks.torch.hook_args.hook_function_args(
--&gt; 199                 cmd, args, kwargs, return_args_type=True
200             )
/usr/local/lib/python3.7/site-packages/syft-0.1.17-py3.7.egg/syft/frameworks/torch/hook/hook_args.py in hook_function_args(attr, args, kwargs, return_args_type)
157         # Run it
--&gt; 158         new_args = args_hook_function(args)
159
/usr/local/lib/python3.7/site-packages/syft-0.1.17-py3.7.egg/syft/frameworks/torch/hook/hook_args.py in (x)
341
--&gt; 342     return lambda x: f(lambdas, x)
343
/usr/local/lib/python3.7/site-packages/syft-0.1.17-py3.7.egg/syft/frameworks/torch/hook/hook_args.py in eight_fold(lambdas, args, **kwargs)
556         lambdas[0](args[0], **kwargs),
--&gt; 557         lambdas[1](args[1], **kwargs),
558         lambdas[2](args[2], **kwargs),
/usr/local/lib/python3.7/site-packages/syft-0.1.17-py3.7.egg/syft/frameworks/torch/hook/hook_args.py in (i)
319         # Last if not, rule is probably == 1 so use type to return the right transformation.
--&gt; 320         else lambda i: forward_func&lt;denchmark-link:i&gt;type(i)&lt;/denchmark-link&gt;

321         for a, r in zip(args, rules)  # And do this for all the args / rules provided
/usr/local/lib/python3.7/site-packages/syft-0.1.17-py3.7.egg/syft/frameworks/torch/hook/hook_args.py in (i)
50     if hasattr(i, "child")
---&gt; 51     else (_ for _ in ()).throw(PureTorchTensorFoundError),
52     torch.nn.Parameter: lambda i: i.child
/usr/local/lib/python3.7/site-packages/syft-0.1.17-py3.7.egg/syft/frameworks/torch/hook/hook_args.py in (.0)
50     if hasattr(i, "child")
---&gt; 51     else (_ for _ in ()).throw(PureTorchTensorFoundError),
52     torch.nn.Parameter: lambda i: i.child
PureTorchTensorFoundError:
During handling of the above exception, another exception occurred:
RuntimeError                              Traceback (most recent call last)
 in 
 in train(args, model, device, federated_train_loader, optimizer, epoch)
5         data, target = data.to(device), target.to(device)
6         optimizer.zero_grad()
----&gt; 7         output = model(data)
8         loss = F.nll_loss(output, target)
9         loss.backward()
/usr/local/lib/python3.7/site-packages/torch-1.0.1-py3.7-macosx-10.14-x86_64.egg/torch/nn/modules/module.py in call(self, *input, **kwargs)
487             result = self._slow_forward(*input, **kwargs)
488         else:
--&gt; 489             result = self.forward(*input, **kwargs)
490         for hook in self._forward_hooks.values():
491             hook_result = hook(self, input, result)
 in forward(self, x)
10     def forward(self, x):
11         x = F.relu(self.conv1(x))
---&gt; 12         x = self.bn1(x)
13         x = F.max_pool2d(x, 2, 2)
14         x = F.relu(self.conv2(x))
/usr/local/lib/python3.7/site-packages/torch-1.0.1-py3.7-macosx-10.14-x86_64.egg/torch/nn/modules/module.py in call(self, *input, **kwargs)
487             result = self._slow_forward(*input, **kwargs)
488         else:
--&gt; 489             result = self.forward(*input, **kwargs)
490         for hook in self._forward_hooks.values():
491             hook_result = hook(self, input, result)
/usr/local/lib/python3.7/site-packages/torch-1.0.1-py3.7-macosx-10.14-x86_64.egg/torch/nn/modules/batchnorm.py in forward(self, input)
74             input, self.running_mean, self.running_var, self.weight, self.bias,
75             self.training or not self.track_running_stats,
---&gt; 76             exponential_average_factor, self.eps)
77
78     def extra_repr(self):
/usr/local/lib/python3.7/site-packages/syft-0.1.17-py3.7.egg/syft/frameworks/torch/hook/hook.py in overloaded_func(*args, **kwargs)
706             cmd_name = f"{attr.module}.{attr.name}"
707             command = (cmd_name, None, args, kwargs)
--&gt; 708             response = TorchTensor.handle_func_command(command)
709             return response
710
/usr/local/lib/python3.7/site-packages/syft-0.1.17-py3.7.egg/syft/frameworks/torch/tensors/interpreters/native.py in handle_func_command(cls, command)
224             # in the execute_command function
225             if isinstance(args, tuple):
--&gt; 226                 response = eval(cmd)(*args, **kwargs)
227             else:
228                 response = eval(cmd)(args, **kwargs)
/usr/local/lib/python3.7/site-packages/torch-1.0.1-py3.7-macosx-10.14-x86_64.egg/torch/nn/functional.py in batch_norm(input, running_mean, running_var, weight, bias, training, momentum, eps)
1621     return torch.batch_norm(
1622         input, weight, bias, running_mean, running_var,
-&gt; 1623         training, momentum, eps, torch.backends.cudnn.enabled
1624     )
1625
&lt;denchmark-h:h2&gt;RuntimeError: running_mean should contain 11333 elements not 20&lt;/denchmark-h&gt;

Where the 11333 seems to be a bit random (I tried several times and the error does not give the same number each time). Maybe it is now due to the .size() method (&lt;denchmark-link:https://github.com/OpenMined/PySyft/issues/2201&gt;#2201&lt;/denchmark-link&gt;
)?
		</comment>
		<comment id='3' author='SohamMazumder' date='2019-06-15T18:50:02Z'>
		I think this is the same as &lt;denchmark-link:https://github.com/OpenMined/PySyft/issues/2175&gt;#2175&lt;/denchmark-link&gt;
, if not feel free to re-open.
		</comment>
	</comments>
</bug>