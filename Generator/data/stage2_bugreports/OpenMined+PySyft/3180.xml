<bug id='3180' author='avinath1998' open_date='2020-03-11T15:34:39Z' closed_time='2020-03-16T12:32:12Z'>
	<summary>Runtime Error asking all parameters to have requires_grad=True</summary>
	<description>
Describe the bug
I'm trying to finetune a alexnet model and i've set the parameters except for the final layer of the model to requires_grad=False and have created a new classification layer with the desired outputs i want. However the .send() function keeps throwing a runtime error `RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
&lt;denchmark-code&gt;import syft
import torch
from torchvision import models
import torch.nn as nn

hook = syft.TorchHook(torch)
worker = syft.VirtualWorker(hook, id="worker")

model = models.alexnet(pretrained=True)
for param in model.parameters():
    param.requires_grad=False
model.classifier[6] = nn.Linear(model.classifier[6].in_features, 3)
model.send(worker)
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-15-a250859d9a13&gt; in &lt;module&gt;
----&gt; 1 model.send(worker)

~/implementation/PyGrid/gateway/src/syft/syft/frameworks/torch/hook/hook.py in module_send_(nn_self, force_send, *dest, **kwargs)
    608 
    609             if module_is_missing_grad(nn_self):
--&gt; 610                 create_grad_objects(nn_self)
    611 
    612             for p in nn_self.parameters():

~/implementation/PyGrid/gateway/src/syft/syft/frameworks/torch/hook/hook.py in create_grad_objects(model)
    600             for p in model.parameters():
    601                 o = p.sum()
--&gt; 602                 o.backward()
    603                 if p.grad is not None:
    604                     p.grad -= p.grad

~/implementation/PyGrid/gateway/src/syft/syft/generic/frameworks/hook/trace.py in trace_wrapper(*args, **kwargs)
     81                 syft.hook.trace.logs.append((command, response))
     82             else:
---&gt; 83                 response = func(*args, **kwargs)
     84 
     85             return response

~/implementation/PyGrid/gateway/src/syft/syft/generic/frameworks/hook/hook.py in overloaded_native_method(self, *args, **kwargs)
    436                 except BaseException as e:
    437                     # we can make some errors more descriptive with this method
--&gt; 438                     raise route_method_exception(e, self, args, kwargs)
    439 
    440             else:  # means that there is a wrapper to remove

~/implementation/PyGrid/gateway/src/syft/syft/generic/frameworks/hook/hook.py in overloaded_native_method(self, *args, **kwargs)
    432 
    433                 try:
--&gt; 434                     response = method(*args, **kwargs)
    435 
    436                 except BaseException as e:

~/anaconda3/lib/python3.7/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)
    193                 products. Defaults to ``False``.
    194         """
--&gt; 195         torch.autograd.backward(self, gradient, retain_graph, create_graph)
    196 
    197     def register_hook(self, hook):

~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)
     97     Variable._execution_engine.run_backward(
     98         tensors, grad_tensors, retain_graph, create_graph,
---&gt; 99         allow_unreachable=True)  # allow_unreachable flag
    100 
    101 

RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
 
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='avinath1998' date='2020-03-11T21:22:59Z'>
		&lt;denchmark-link:https://github.com/avinath1998&gt;@avinath1998&lt;/denchmark-link&gt;
 If you are trying to change the whole  block, then your input dimension (which you put 2) is wrong. The input dimension of  should be 256x6x6.
refer: &lt;denchmark-link:https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py&gt;https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;self.classifier = nn.Sequential(
            nn.Dropout(),
            nn.Linear(256 * 6 * 6, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='avinath1998' date='2020-03-11T21:40:27Z'>
		&lt;denchmark-link:https://github.com/imraniac&gt;@imraniac&lt;/denchmark-link&gt;
 Oh shoot, i only added this as an example and didnt see that. Regardless, the error still comes up, ive updated the issue accordingly
		</comment>
		<comment id='3' author='avinath1998' date='2020-03-11T22:08:25Z'>
		Have you still updated the issue with an example? Because it should be 256 * 6 * 6 and not 224
		</comment>
		<comment id='4' author='avinath1998' date='2020-03-12T10:33:22Z'>
		&lt;denchmark-link:https://github.com/imraniac&gt;@imraniac&lt;/denchmark-link&gt;
 updated accordingly, still doesn't work, the error occurs
		</comment>
		<comment id='5' author='avinath1998' date='2020-03-12T18:51:54Z'>
		I will check this.
		</comment>
		<comment id='6' author='avinath1998' date='2020-03-12T19:18:26Z'>
		&lt;denchmark-link:https://github.com/tudorcebere&gt;@tudorcebere&lt;/denchmark-link&gt;
, assigned you
		</comment>
	</comments>
</bug>