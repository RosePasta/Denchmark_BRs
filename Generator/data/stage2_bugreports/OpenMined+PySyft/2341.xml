<bug id='2341' author='LaRiffle' open_date='2019-07-08T10:22:26Z' closed_time='2020-07-02T00:08:35Z'>
	<summary>SecureNN with n_workers &amp;gt;= 3</summary>
	<description>
Describe the bug
When used with n&gt;=3 workers with SecureNN
&lt;denchmark-code&gt;wks = [ sy.VirtualWorker(hook, id="w#%d" % i) for i in range(3) ]
crypto_prov = sy.VirtualWorker(hook, id="crypto_prov")
t = torch.zeros(3,3)
t = t.fix_prec().share(*wks, crypto_provider=crypto_prov)
t = t * 2
t = t.get()
t = t.float_prec()
print(t)
&lt;/denchmark-code&gt;

This doesn't work correctly:
&lt;denchmark-code&gt;tensor([[-1.0000e-03,  0.0000e+00,  1.0000e-03],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-1.0000e-03,  1.0000e-03,  1.8447e+13]])
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='LaRiffle' date='2019-07-08T21:39:39Z'>
		In this particular case where we multiply by a scalar, I think we do something useless:
we first multiply the multiplier by base ** precision_fractional and then truncate. When I remove this, the results are ok.
But I agree that there are still some problems when multiplying 2 ASTs shared between more than 2 workers
		</comment>
		<comment id='2' author='LaRiffle' date='2019-07-08T22:00:23Z'>
		EDIT not enough

Use a crypto provider and that should be sufficient

		</comment>
		<comment id='3' author='LaRiffle' date='2019-07-08T22:21:14Z'>
		Hmm &lt;denchmark-link:https://github.com/LaRiffle&gt;@LaRiffle&lt;/denchmark-link&gt;
, this doesn't seem sufficient to fix the problem of &gt;2 workers:
&lt;denchmark-code&gt;wks = [ sy.VirtualWorker(hook, id="w#%d" % i) for i in range(3) ]
crypto_prov = sy.VirtualWorker(hook, id="crypto_prov")
t = torch.zeros(3,3)
t = t.fix_prec().share(*wks, crypto_provider=crypto_prov)
t = t * 2
t = t.get()
t = t.float_prec()
print(t)

tensor([[-1.8447e+13,  1.8447e+13,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-1.0000e-03, -1.8447e+13,  1.8447e+13]])
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='LaRiffle' date='2019-07-09T07:12:40Z'>
		&lt;denchmark-link:https://github.com/LaRiffle&gt;@LaRiffle&lt;/denchmark-link&gt;
, a quick fix using the in-place operation somehow works:
&lt;denchmark-code&gt;wks = [ sy.VirtualWorker(hook, id="w#%d" % i) for i in range(3) ]
t = torch.zeros(3,3)
t = t.fix_prec().share(*wks)
t *= 2
t = t.get()
t.float_prec()
&lt;/denchmark-code&gt;

For the very specific problem of multiplying by an integer, I submitted a pull request and it passed all tests.
		</comment>
		<comment id='5' author='LaRiffle' date='2019-07-09T13:40:06Z'>
		Here's another example - calculating avg of multiple zero tensors shared to multiple workers.
Works fine with num = 2.
&lt;denchmark-code&gt;num = 10
wks = [ sy.VirtualWorker(hook, id="wk#%d" % i) for i in range(num) ]
tensors = [ torch.zeros(5, 5).fix_prec(precision_fractional=16).share(*wks) for _ in range(num) ]
avg = tensors[0]
for i in range(num-1):
  avg += tensors[i+1]
avg /= len(tensors)
avg = avg.get().float_prec()
print(avg)

tensor([[ 1.0000e-16, -1.8447e+02, -1.8447e+02, -1.8447e+02, -1.8447e+02],
        [ 1.8447e+02,  9.2234e+01,  1.8447e+02, -1.8447e+02,  9.2234e+01],
        [ 1.8447e+02,  1.8447e+02,  0.0000e+00, -1.8447e+02,  1.8447e+02],
        [ 1.0000e-16, -1.8447e+02, -1.8447e+02,  1.8447e+02, -1.0000e-16],
        [ 1.8447e+02,  1.0000e-16,  0.0000e+00,  1.8447e+02, -1.8447e+02]])
&lt;/denchmark-code&gt;

Expected is tensor of zeros.
		</comment>
		<comment id='6' author='LaRiffle' date='2019-07-09T14:15:31Z'>
		&lt;denchmark-link:https://github.com/vvmnnnkv&gt;@vvmnnnkv&lt;/denchmark-link&gt;
, Division in FixedPrecisionTensor is not even implemented. &lt;denchmark-link:https://github.com/Jasopaum&gt;@Jasopaum&lt;/denchmark-link&gt;
, maybe division by integer is a nice feature to be implemented soon.
		</comment>
		<comment id='7' author='LaRiffle' date='2019-07-09T19:05:20Z'>
		&lt;denchmark-link:https://github.com/Jasopaum&gt;@Jasopaum&lt;/denchmark-link&gt;
 A big problem with operations between more than 2 ast's is that when recombining each of them there is a greater likelyhood in losing precision in the least significant bit. An intuitive way to think about this problem is to think of the last digit as a float which adds up to 1 but is rounded. For the n=2 case, unless you split the number such that both are exactly equal to 0.5, one of the numbers will round to 1 and the other to 0. However, when n&gt;2, it is likely that all will round to 0 leading to a loss in precision
		</comment>
		<comment id='8' author='LaRiffle' date='2019-07-09T19:06:00Z'>
		We actually used to have test for this which was super flakey due to the above instability
		</comment>
		<comment id='9' author='LaRiffle' date='2019-07-09T21:27:06Z'>
		&lt;denchmark-link:https://github.com/joseilberto&gt;@joseilberto&lt;/denchmark-link&gt;
, I think we left out the division for FixedPrecisionTensor on purpose, to avoid a loss of precision when division isn't exact... But I'm not sure why we couldn't have something like a floor division, you're right. Maybe &lt;denchmark-link:https://github.com/LaRiffle&gt;@LaRiffle&lt;/denchmark-link&gt;
 knows a bit more about FixedPrecisionTensors?
		</comment>
		<comment id='10' author='LaRiffle' date='2019-07-10T10:16:50Z'>
		&lt;denchmark-link:https://github.com/joseilberto&gt;@joseilberto&lt;/denchmark-link&gt;
 we'd love to have division with fixed_precision! We haven't had a use case yet where division by a FixedPrecision divisor was needed, we only needed integer division, that's why you don't see an implementation so far.
One thing to notice, is that for example if you don't see a method (like __truediv__) in precision.py it doesn't mean it's not implemented, it means that the basic PyTorch behaviour will be used. For integer division for example you don't need to specify a specific behaviour different from PyTorch.
		</comment>
		<comment id='11' author='LaRiffle' date='2019-07-10T11:19:21Z'>
		&lt;denchmark-link:https://github.com/LaRiffle&gt;@LaRiffle&lt;/denchmark-link&gt;
, I do understand that what it does is exactly use whatever comes from the inheritance (that is why I suggested using the in-place multiplication operator and it works properly). In the division by integer case, we are constrained by the precision_fractional we have set and it performs poorly if we need more precision just as pointed by &lt;denchmark-link:https://github.com/vvmnnnkv&gt;@vvmnnnkv&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='12' author='LaRiffle' date='2019-07-10T13:24:12Z'>
		Hey yes you're right, but this is linked to the additive sharing part not the fixed_precision: take the same example  from &lt;denchmark-link:https://github.com/vvmnnnkv&gt;@vvmnnnkv&lt;/denchmark-link&gt;
 and remove .share() and .get() it will work
So as a recap  we have issues with additive shared tensor with mul or div with integers with n&gt;2 workers
		</comment>
		<comment id='13' author='LaRiffle' date='2020-04-21T18:33:14Z'>
		Is there any update on this issue? Multiplying and dividing tensors by integers is a pretty common operation, and it took me a while to understand why torch.mean returned an absurd value when working with n&gt;=3 workers.
		</comment>
		<comment id='14' author='LaRiffle' date='2020-04-21T19:53:19Z'>
		We just had a great PR merged (&lt;denchmark-link:https://github.com/OpenMined/PySyft/pull/2982&gt;#2982&lt;/denchmark-link&gt;
) which will help for this issue which is still open!
&lt;denchmark-link:https://github.com/OpenMined/PySyft/pull/3148&gt;#3148&lt;/denchmark-link&gt;
 also adresses secure comparison for more workers, but &lt;denchmark-link:https://github.com/artix41&gt;@artix41&lt;/denchmark-link&gt;
 if yoy're willing to do the checks for multiplication &amp; addition I'd be very graeful!
		</comment>
		<comment id='15' author='LaRiffle' date='2020-05-25T00:08:18Z'>
		This issue has been marked stale because it has been open 30 days with no activity. Leave a comment or remove the stale label to unmark it. Otherwise, this will be closed in 7 days.
		</comment>
		<comment id='16' author='LaRiffle' date='2020-06-25T00:06:58Z'>
		This issue has been marked stale because it has been open 30 days with no activity. Leave a comment or remove the stale label to unmark it. Otherwise, this will be closed in 7 days.
		</comment>
	</comments>
</bug>