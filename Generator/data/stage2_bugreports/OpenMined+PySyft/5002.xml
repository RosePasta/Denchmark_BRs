<bug id='5002' author='tudorcebere' open_date='2021-01-10T18:19:39Z' closed_time='2021-01-18T10:13:55Z'>
	<summary>Model parameters not preserving .grad property</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

.grad property is not preserved on a model parameters when it's downloaded locally.
&lt;denchmark-h:h2&gt;How to Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import syft as sy
import torch

alice = sy.VirtualMachine(name="alice")
alice_client = alice.get_root_client()
remote_torch = alice_client.torch

class SyNet(sy.Module):
    def __init__(self, torch_ref):
        super(SyNet, self).__init__(torch_ref=torch_ref)
        self.fc1 = self.torch_ref.nn.Linear(100, 10)

    def forward(self, x):
        return self.fc1(x)

model = SyNet(torch)
data = torch.randn(size=(1, 100))
result = model(data)
labels = torch.randn(size=(1, 10))
loss_func = torch.nn.L1Loss()
loss = loss_func(result, labels)
loss.backward()

print(model.parameters()[-1].grad) # exists


model_ptr = model.send(alice_client)
data_ptr = data.send(alice_client)
labels_ptr = labels.send(alice_client)
results_ptr = model_ptr(data_ptr)
remote_loss_func = alice_client.torch.nn.L1Loss()
remote_loss = remote_loss_func(results_ptr, labels_ptr)
remote_loss.backward()

print(model_ptr.parameters().get()[-1].grad) # exists
print(model_ptr.get().parameters()[-1].grad) # does not exist anymore
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

The .grad attribute should be present when we download the remote model.
	</description>
	<comments>
		<comment id='1' author='tudorcebere' date='2021-01-11T14:35:35Z'>
		I can work on this one. Please assign it to me.
		</comment>
		<comment id='2' author='tudorcebere' date='2021-01-12T09:54:07Z'>
		PyTorch's load_state_dict breaks the computational graph, so on get(), there won't be any gradients to retrieve.
The gradients will have to be retrieved via the model_ptr.parameters().get() call.
		</comment>
	</comments>
</bug>