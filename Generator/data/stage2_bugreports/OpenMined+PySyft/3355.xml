<bug id='3355' author='gmuraru' open_date='2020-04-14T14:28:43Z' closed_time='2020-04-14T17:37:51Z'>
	<summary>`test_local_remote_gradient_clipping` is flaky</summary>
	<description>
Describe the bug
An issue that started to happen when running the tests in random order.
To Reproduce
Run the test (or full suite) until it fails.
Expected behavior
The test should reliably pass.
Screenshots
&lt;denchmark-code&gt;        # Remote gradient clipping
        remote_parameters = alice_model.parameters()
        total_norm_remote = nn.utils.clip_grad_norm_(remote_parameters, 2)
    
        # Local gradient clipping
        local_alice_model = alice_model.get()
        local_parameters = local_alice_model.parameters()
        total_norm_local = nn.utils.clip_grad_norm_(local_parameters, 2)
    
        # Is the output of the remote gradient clipping version equal to
        # the output of the local gradient clipping version?
&gt;       assert total_norm_remote.get() == total_norm_local
E       assert tensor([1.3774]) == 1.3774276244053927
E         +tensor([1.3774])
E         -1.3774276244053927
&lt;/denchmark-code&gt;

Additional context
This started being an issue when the order of the test suite was randomized (specifically to shake out flaky tests like this.)
	</description>
	<comments>
		<comment id='1' author='gmuraru' date='2020-04-14T17:37:49Z'>
		Resolved by &lt;denchmark-link:https://github.com/OpenMined/PySyft/pull/3356&gt;#3356&lt;/denchmark-link&gt;
.
		</comment>
	</comments>
</bug>