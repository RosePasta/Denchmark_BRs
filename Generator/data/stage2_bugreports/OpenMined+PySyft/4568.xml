<bug id='4568' author='Hjeljeli' open_date='2020-09-18T09:40:30Z' closed_time='2020-11-19T13:30:59Z'>
	<summary>model.get() causes RuntimeError if the code is running on GPU with resnet model</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

After I train the model locally by a worker, I do model.get() to retrieve it and I have the following runtime error: "Expected object of device type cuda but got device type cpu for argument &lt;denchmark-link:https://github.com/OpenMined/PySyft/pull/1&gt;#1&lt;/denchmark-link&gt;
 'self' in call to ".
I am training on GPU (same code runs perfectly if I use CPU) and I am using resnet50 model.
&lt;denchmark-h:h2&gt;How to Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;optimizer = optim.Adam(model.parameters(), lr=lr) 
criterion = nn.CrossEntropyLoss()

model.train()
model.send(worker)
for batch_idx, (data, target) in enumerate(batches):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        loss = loss.get()
        model.get()  # &lt;-- This get causes the error
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;System Information&lt;/denchmark-h&gt;


syft: 0.2.9

	</description>
	<comments>
		<comment id='1' author='Hjeljeli' date='2020-09-18T12:24:35Z'>
		Hey, thanks for reporting!
can you alors provide your stack trace please? :)
		</comment>
		<comment id='2' author='Hjeljeli' date='2020-09-19T11:14:24Z'>
		Hi &lt;denchmark-link:https://github.com/Hjeljeli&gt;@Hjeljeli&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/LaRiffle&gt;@LaRiffle&lt;/denchmark-link&gt;
, I encountered the same problem a few months ago, please refer to the comments in &lt;denchmark-link:https://github.com/OpenMined/PySyft/issues/3848&gt;#3848&lt;/denchmark-link&gt;
 for further explanation.
		</comment>
		<comment id='3' author='Hjeljeli' date='2020-09-21T00:55:08Z'>
		&lt;denchmark-link:https://github.com/LaRiffle&gt;@LaRiffle&lt;/denchmark-link&gt;

Hi Theo, please find my stack trace. Merci pour ton aide!
&lt;denchmark-code&gt;&lt;ipython-input-16-1a450dc3c5d4&gt; in &lt;module&gt;
      3 logging.basicConfig(format=FORMAT, level=LOG_LEVEL)
      4 
----&gt; 5 main()

&lt;ipython-input-15-443eb06bbc7c&gt; in main()
    208     for epoch in range(1, epochs + 1):
    209         logger.warning("Starting epoch %s/%s", epoch, epochs)
--&gt; 210         model = train(model, device, federated_train_loader, test_loader, lr, federate_after_n_batches)
    211         test(model, device, test_loader)

&lt;ipython-input-15-443eb06bbc7c&gt; in train(model, device, federated_train_loader, test_loader, lr, federate_after_n_batches, abort_after_one)
    112             curr_batches = batches[worker]
    113             if curr_batches:
--&gt; 114                 local_models[worker] = train_on_batches(worker, curr_batches, model, device, test_loader, lr)
    115 
    116             else:

&lt;ipython-input-15-443eb06bbc7c&gt; in train_on_batches(worker, batches, model_in, device, test_loader, lr)
     42             t1 = time.time()
     43             # We measure accurancy of worker's model
---&gt; 44             model.get()
     45             accuracy = test(model, device, test_loader)
     46             accuracies[worker].append(accuracy)

/usr/local/lib/python3.7/dist-packages/syft-0.2.7-py3.7.egg/syft/frameworks/torch/hook/hook.py in module_get_(nn_self)
    669             for element_iter in tensor_iterator(nn_self):
    670                 for p in element_iter():
--&gt; 671                     p.get_()
    672 
    673             if isinstance(nn_self.forward, Plan):

/usr/local/lib/python3.7/dist-packages/syft-0.2.7-py3.7.egg/syft/frameworks/torch/tensors/interpreters/native.py in get_(self, *args, **kwargs)
    685         Calls get() with inplace option set to True
    686         """
--&gt; 687         return self.get(*args, inplace=True, **kwargs)
    688 
    689     def allow(self, user=None) -&gt; bool:

/usr/local/lib/python3.7/dist-packages/syft-0.2.7-py3.7.egg/syft/frameworks/torch/tensors/interpreters/native.py in get(self, inplace, user, reason, *args, **kwargs)
    672 
    673         if inplace:
--&gt; 674             self.set_(tensor)
    675             if hasattr(tensor, "child"):
    676                 self.child = tensor.child

RuntimeError: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_set_
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='Hjeljeli' date='2020-09-30T18:09:06Z'>
		+1, I also run into this issue with very similar code to the above, except using mobilenet instead of resnet. RuntimeError message is the same. The issue disappears when I use a neural net I specify for myself, so I think it could be interop with the torch model zoo models?
(for reference I had this same issue back in ~May on syft ~0.2.4 but didn't report- unfortunately some other projects pulled me away from this one)
		</comment>
		<comment id='5' author='Hjeljeli' date='2020-10-01T08:37:45Z'>
		Thank you for reporting it! It might be a problem that the tensors should be sent to the device (in this case GPU) before using set_
		</comment>
		<comment id='6' author='Hjeljeli' date='2020-11-13T00:16:47Z'>
		This issue has been marked stale because it has been open 30 days with no activity. Leave a comment or remove the stale label to unmark it. Otherwise, this will be closed in 7 days.
		</comment>
		<comment id='7' author='Hjeljeli' date='2020-11-13T00:27:53Z'>
		
This issue has been marked stale because it has been open 30 days with no activity. Leave a comment or remove the stale label to unmark it. Otherwise, this will be closed in 7 days.

Updating so this issue is active.
		</comment>
		<comment id='8' author='Hjeljeli' date='2020-11-19T13:30:59Z'>
		Hello! Just letting you know that we are no longer planning on supporting anything on the 0.2.x product line and that all work should be ported over to 0.3.x, which is considered a complete rebuild of PySyft. Because of that, I'll be closing this issue. If you feel this is a mistake, or if the issue actually applies to 0.3.x as well, please feel free to ping me on Slack and I'll reopen the issue.
		</comment>
		<comment id='9' author='Hjeljeli' date='2020-12-27T17:33:09Z'>
		is this issue fixed in 0.3.x? &lt;denchmark-link:https://github.com/Hjeljeli&gt;@Hjeljeli&lt;/denchmark-link&gt;
. Currently stuck with this bug in 0.2.9 :(.
		</comment>
		<comment id='10' author='Hjeljeli' date='2020-12-28T00:54:47Z'>
		&lt;denchmark-link:https://github.com/naveenggmu&gt;@naveenggmu&lt;/denchmark-link&gt;
 Hi Naveen, I could not test on 0.3.x as you know the releases 0.3.x do not support all the privacy-preserving techniques that 0.2.x used to support, for me I am using PySyft for Federated Learning.
		</comment>
	</comments>
</bug>