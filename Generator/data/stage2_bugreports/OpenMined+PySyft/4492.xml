<bug id='4492' author='buzem' open_date='2020-08-26T21:41:44Z' closed_time='2020-11-19T13:30:02Z'>
	<summary>test() causes error at Part 11 Tutorial Notebook</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

In the last cell of the notebook:
test(args, model, private_test_loader)
I am getting this error:

AssertionError: Searching for #fss_comp_plan_1 on alice. /!\ 0 found

I am new to the Pysyft library, although I tried to find the reason of this error, I couldn't find it.
Full text of error:
&lt;denchmark-code&gt;`---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
&lt;ipython-input-37-d96a4baef877&gt; in &lt;module&gt;()
----&gt; 1 test(args, model, private_test_loader)

22 frames
&lt;ipython-input-36-22b4a92d84ce&gt; in test(args, model, test_loader)
      5     with torch.no_grad():
      6         for data, target in test_loader[:n_test_batches]:
----&gt; 7             output = model(data)
      8             pred = output.argmax(dim=1)
      9             n_correct_priv += pred.eq(target.view_as(pred)).sum()

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    530             result = self._slow_forward(*input, **kwargs)
    531         else:
--&gt; 532             result = self.forward(*input, **kwargs)
    533         for hook in self._forward_hooks.values():
    534             hook_result = hook(self, input, result)

&lt;ipython-input-30-6d8ab0cff85f&gt; in forward(self, x)
      8         x = x.view(-1, 784)
      9         x = self.fc1(x)
---&gt; 10         x = F.relu(x)
     11         x = self.fc2(x)
     12         return x

/usr/local/lib/python3.6/dist-packages/syft/generic/frameworks/hook/hook.py in overloaded_func(*args, **kwargs)
    338                 handle_func_command = syft.framework.Tensor.handle_func_command
    339 
--&gt; 340             response = handle_func_command(command)
    341 
    342             return response

/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/tensors/interpreters/native.py in handle_func_command(cls, command)
    358             # Send it to the appropriate class and get the response
    359             try:
--&gt; 360                 response = new_type.handle_func_command(new_command)
    361             except RuntimeError:
    362                 # Change the library path to avoid errors on layers like AvgPooling

/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/tensors/interpreters/precision.py in handle_func_command(cls, command)
    810 
    811         # Send it to the appropriate class and get the response
--&gt; 812         response = new_type.handle_func_command(new_command)
    813 
    814         # Put back FixedPrecisionTensor on the tensors found in the response

/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/tensors/interpreters/additive_shared.py in handle_func_command(cls, command)
   1119 
   1120         if cmd is not None:
-&gt; 1121             return cmd(*args_, **kwargs_)
   1122 
   1123         tensor = args_[0] if not isinstance(args_[0], (tuple, list)) else args_[0][0]

/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/tensors/interpreters/additive_shared.py in relu(tensor_shares, inplace)
    928             def functional(module):
    929                 def relu(tensor_shares, inplace=False):
--&gt; 930                     return tensor_shares.relu()
    931 
    932                 module.relu = relu

/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/mpc/__init__.py in method(self, *args, **kwargs)
     32         def method(self, *args, **kwargs):
     33             f = protocol_store[(name, self.protocol)]
---&gt; 34             return f(self, *args, **kwargs)
     35 
     36         return method

/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/tensors/interpreters/additive_shared.py in relu(self)
    954     def relu(self):
    955         zero = self - self
--&gt; 956         return self * (self &gt;= zero)
    957 
    958     def positive(self):

/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/mpc/__init__.py in method(self, *args, **kwargs)
     32         def method(self, *args, **kwargs):
     33             f = protocol_store[(name, self.protocol)]
---&gt; 34             return f(self, *args, **kwargs)
     35 
     36         return method

/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/tensors/interpreters/additive_shared.py in __ge__(self, other)
    981     @crypto_protocol("fss")
    982     def __ge__(self, other):
--&gt; 983         return fss.le(other, self)
    984 
    985     def lt(self, other):

/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/mpc/fss.py in le(x1, x2)
    190 
    191 def le(x1, x2):
--&gt; 192     return fss_op(x1, x2, "comp")
    193 
    194 

/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/mpc/fss.py in fss_op(x1, x2, type_op)
    101         args = (x1.child[location.id], x2.child[location.id])
    102         share = request_run_plan(
--&gt; 103             me, f"#fss_{type_op}_plan_1", location, return_value=True, args=args
    104         )
    105         shares.append(share)

/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/mpc/fss.py in request_run_plan(worker, plan_tag, location, return_value, args, kwargs)
     73         return_value=return_value,
     74         kwargs_=kwargs,
---&gt; 75         args_=args,
     76     )
     77     return response

/usr/local/lib/python3.6/dist-packages/syft/workers/base.py in send_command(self, recipient, cmd_name, target, args_, kwargs_, return_ids, return_value)
    514                 cmd_name, target, args_, kwargs_, return_ids, return_value
    515             )
--&gt; 516             ret_val = self.send_msg(message, location=recipient)
    517         except ResponseSignatureError as e:
    518             ret_val = None

/usr/local/lib/python3.6/dist-packages/syft/workers/base.py in send_msg(self, message, location)
    315 
    316         # Step 2: send the message and wait for a response
--&gt; 317         bin_response = self._send_msg(bin_message, location)
    318 
    319         # Step 3: deserialize the response

/usr/local/lib/python3.6/dist-packages/syft/workers/virtual.py in _send_msg(self, message, location)
     14             sleep(self.message_pending_time)
     15 
---&gt; 16         return location._recv_msg(message)
     17 
     18     def _recv_msg(self, message: bin) -&gt; bin:

/usr/local/lib/python3.6/dist-packages/syft/workers/virtual.py in _recv_msg(self, message)
     18     def _recv_msg(self, message: bin) -&gt; bin:
     19         """receive message"""
---&gt; 20         return self.recv_msg(message)
     21 
     22     # For backwards compatibility with Udacity course

/usr/local/lib/python3.6/dist-packages/syft/workers/base.py in recv_msg(self, bin_message)
    355         for handler in self.message_handlers:
    356             if handler.supports(msg):
--&gt; 357                 response = handler.handle(msg)
    358                 break
    359         # TODO(karlhigley): Raise an exception if no handler is found

/usr/local/lib/python3.6/dist-packages/syft/generic/abstract/message_handler.py in handle(self, msg)
     18 
     19     def handle(self, msg):
---&gt; 20         return self.routing_table[type(msg)](msg)

/usr/local/lib/python3.6/dist-packages/syft/workers/message_handler.py in execute_tensor_command(self, cmd)
     53     def execute_tensor_command(self, cmd: TensorCommandMessage) -&gt; PointerTensor:
     54         if isinstance(cmd.action, ComputationAction):
---&gt; 55             return self.execute_computation_action(cmd.action)
     56         else:
     57             return self.execute_communication_action(cmd.action)

/usr/local/lib/python3.6/dist-packages/syft/workers/message_handler.py in execute_computation_action(self, action)
     86                     assert (
     87                         len(res) == 1
---&gt; 88                     ), f"Searching for {_self} on {self.worker.id}. /!\\ {len(res)} found"
     89                     _self = res[0]
     90             if sy.framework.is_inplace_method(op_name):

AssertionError: Searching for #fss_comp_plan_1 on alice. /!\ 0 found`
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;System Information&lt;/denchmark-h&gt;


run on Colab with GPU

&lt;denchmark-h:h2&gt;Additional Context&lt;/denchmark-h&gt;

Add any other context about the problem here.
	</description>
	<comments>
		<comment id='1' author='buzem' date='2020-08-28T12:55:59Z'>
		Hello, have you tried building from sources? This error might have been solved in the latest version that is not released yet.
		</comment>
		<comment id='2' author='buzem' date='2020-08-29T14:31:31Z'>
		&lt;denchmark-link:https://github.com/teo-milea&gt;@teo-milea&lt;/denchmark-link&gt;
 when were the sources updated at last?
		</comment>
		<comment id='3' author='buzem' date='2020-09-01T06:03:53Z'>
		The sources are updating (I think) at least once in every 3-4 days
&lt;denchmark-link:https://github.com/OpenMined/PySyft/issues/4473&gt;#4473&lt;/denchmark-link&gt;
 &lt;-- could you run the steps posted by &lt;denchmark-link:https://github.com/teo-milea&gt;@teo-milea&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='buzem' date='2020-09-01T09:21:48Z'>
		&lt;denchmark-link:https://github.com/gmuraru&gt;@gmuraru&lt;/denchmark-link&gt;
 the issue doesn't get resolved by following those steps.
		</comment>
		<comment id='5' author='buzem' date='2020-11-13T00:17:14Z'>
		This issue has been marked stale because it has been open 30 days with no activity. Leave a comment or remove the stale label to unmark it. Otherwise, this will be closed in 7 days.
		</comment>
		<comment id='6' author='buzem' date='2020-11-19T13:30:02Z'>
		Hello! Just letting you know that we are no longer planning on supporting anything on the 0.2.x product line and that all work should be ported over to 0.3.x, which is considered a complete rebuild of PySyft. Because of that, I'll be closing this issue. If you feel this is a mistake, or if the issue actually applies to 0.3.x as well, please feel free to ping me on Slack and I'll reopen the issue.
		</comment>
	</comments>
</bug>