<bug id='1975' author='iamtrask' open_date='2019-03-05T15:21:51Z' closed_time='2019-03-12T15:17:46Z'>
	<summary>Get .max() working on PointerTensor; multiple ptrs in response</summary>
	<description>
We can't return tuples of tensors when calling remote executions. This means that functions like .max() fail on PointerTensor objects.
	</description>
	<comments>
		<comment id='1' author='iamtrask' date='2019-03-05T21:37:20Z'>
		&lt;denchmark-code&gt;import torch 
import syft as sy
hook = sy.TorchHook(torch)
bob = sy.VirtualWorker(hook, id="bob")
x = torch.tensor([1,2,3,4,5])
x_ptr = x.send(bob)
print(x_ptr.max())
&lt;/denchmark-code&gt;

Currently, x_ptr.max() returns  (Wrapper)&gt;[PointerTensor | me:73509977871 -&gt; bob:73509977871]
and instead, should return 5 right?
		</comment>
		<comment id='2' author='iamtrask' date='2019-03-06T01:27:51Z'>
		&lt;denchmark-link:https://github.com/iamtrask&gt;@iamtrask&lt;/denchmark-link&gt;
 I was wondering, if we should build an interface to directly enable all torch tensor functionalities to pointer tensor? Is that something useful? Do we require all torch functionality for pointer tensor? Why do we specifically need max?
		</comment>
		<comment id='3' author='iamtrask' date='2019-03-06T08:43:51Z'>
		max is one between others (torch.split, etc.), basically all functions that can return more than one tensors with fail if they are run remotely as we explicitely always make the assumption the response is a unique tensor.
		</comment>
	</comments>
</bug>