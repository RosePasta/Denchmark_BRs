<bug id='1655' author='yanndupis' open_date='2018-11-09T06:09:13Z' closed_time='2019-02-05T15:17:33Z'>
	<summary>Tensor missing on worker when model contains several convolution layers</summary>
	<description>
Hello,
I am getting the error Exception: Tensor "XX" not found on worker "0"!!! when there are several convolution layers included in the model or a batch normalization layer. If I run the same model with one convolution layer, it will work. The error seems to be happening when initializing the gradients optimizer.zero_grad().
Here is an example: &lt;denchmark-link:https://github.com/yanndupis/PySyft/blob/pate-syft/examples/torch/PATE_SYFT.ipynb&gt;https://github.com/yanndupis/PySyft/blob/pate-syft/examples/torch/PATE_SYFT.ipynb&lt;/denchmark-link&gt;

Thank you!
	</description>
	<comments>
		<comment id='1' author='yanndupis' date='2018-12-02T18:35:36Z'>
		I am experiencing the same issue. It works for a simple nn.linear model but not for a simple MNIST model with a couple of convolution layers. Here is my code and the error message:
&lt;denchmark-code&gt;class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(50, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)

model = Net()

kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}
train_loader = torch.utils.data.DataLoader(
    datasets.MNIST('../data', train=True, download=True,
                   transform=transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((0.1307,), (0.3081,))
                   ])),
    batch_size=args.batch_size, shuffle=True, **kwargs)
test_loader = torch.utils.data.DataLoader(
    datasets.MNIST('../data', train=False, transform=transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((0.1307,), (0.3081,))
                   ])),
    batch_size=args.test_batch_size, shuffle=True, **kwargs)
dataset_train = datasets.MNIST('../data/', train=True, download=True,
                       transform=transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((0.1307,), (0.3081,))
                ]))

# create a couple workers

bob = sy.VirtualWorker(id="bob")
alice = sy.VirtualWorker(id="alice")
secure_worker = sy.VirtualWorker(id="secure_worker")

bob.add_workers([alice, secure_worker])
alice.add_workers([bob, secure_worker])
secure_worker.add_workers([alice, bob])

train_distributed_dataset  = []
for batch_idx, (data,target) in enumerate(train_loader):
    if batch_idx &gt; 4: break
    data = sy.Var(data)
    target = sy.Var(target.long())
    data.send(bob)
    target.send(bob)
    train_distributed_dataset.append((data, target))

bobs_model = model.copy().send(bob)
alices_model = model.copy().send(alice)

bobs_opt = optim.SGD(params=bobs_model.parameters(),lr=0.1)
alices_opt = optim.SGD(params=alices_model.parameters(),lr=0.1)

for batch_idx, (data,target) in enumerate(train_distributed_dataset):
    bobs_opt.zero_grad() # this is where it breaks.........
    bobs_pred = bobs_model(data)
    bobs_loss = F.nll_loss(bobs_pred, target)
    bobs_loss.backward()

    bobs_opt.step()
    bobs_loss = bobs_loss.get().data[0]
&lt;/denchmark-code&gt;


Traceback (most recent call last):
File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/syft-0.1.0-py3.6.egg/syft/core/workers/base.py", line 620, in get_obj
KeyError: 42641825531
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
File "test1.py", line 102, in 
bobs_opt.zero_grad() # this is where it breaks.........
File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/optim/optimizer.py", line 116, in zero_grad
p.grad.data.zero_()
File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/syft-0.1.0-py3.6.egg/syft/core/frameworks/torch/hook.py", line 531, in _execute_method_call
File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/syft-0.1.0-py3.6.egg/syft/core/workers/base.py", line 1035, in _execute_call
File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/syft-0.1.0-py3.6.egg/syft/core/frameworks/torch/tensor.py", line 1045, in handle_call
File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/syft-0.1.0-py3.6.egg/syft/core/workers/base.py", line 1149, in send_torch_command
File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/syft-0.1.0-py3.6.egg/syft/core/workers/base.py", line 1158, in send_command
File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/syft-0.1.0-py3.6.egg/syft/core/workers/base.py", line 251, in send_msg
File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/syft-0.1.0-py3.6.egg/syft/core/profiling.py", line 60, in wrapper
File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/syft-0.1.0-py3.6.egg/syft/core/workers/base.py", line 256, in _profiled_send_msg
File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/syft-0.1.0-py3.6.egg/syft/core/workers/virtual.py", line 113, in _send_msg
File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/syft-0.1.0-py3.6.egg/syft/core/workers/base.py", line 313, in receive_msg
File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/syft-0.1.0-py3.6.egg/syft/core/frameworks/encode.py", line 204, in decode
File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/syft-0.1.0-py3.6.egg/syft/core/frameworks/encode.py", line 323, in python_decode
File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/syft-0.1.0-py3.6.egg/syft/core/frameworks/encode.py", line 297, in python_decode
File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/syft-0.1.0-py3.6.egg/syft/core/frameworks/torch/tensor.py", line 2582, in deser
File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/syft-0.1.0-py3.6.egg/syft/core/frameworks/torch/tensor.py", line 259, in deser_routing
File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/syft-0.1.0-py3.6.egg/syft/core/frameworks/torch/tensor.py", line 1111, in deser
File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/syft-0.1.0-py3.6.egg/syft/core/workers/base.py", line 647, in get_obj
Exception: Tensor "42641825531" not found on worker "bob"!!!
You just tried to interact with an object ID:42641825531 on worker bob which does not exist!!! Use .send() and .get() on all your tensors to make sure they'reon the same machines.
If you think this tensor does exist, check the ._objects dictionaryon the worker and see for yourself!!! The most common reason this error happens is because someone calls.get() on the object's pointer without realizing it (which deletes the remote object and sends it to the pointer). Check your code to make sure you haven't already called .get() on this pointer!!!

		</comment>
		<comment id='2' author='yanndupis' date='2018-12-07T23:47:59Z'>
		&lt;denchmark-link:https://github.com/yanndupis&gt;@yanndupis&lt;/denchmark-link&gt;
  you have  line commented out. Not sure why linear layer works without it and convolution does not.
		</comment>
		<comment id='3' author='yanndupis' date='2019-02-05T15:18:13Z'>
		We're doing a massive refactor with the release of pytorch 1.0, please re-open an issue if you still have this problem, thanks! :)
		</comment>
		<comment id='4' author='yanndupis' date='2019-02-05T19:25:40Z'>
		Sounds good :). Thank you &lt;denchmark-link:https://github.com/LaRiffle&gt;@LaRiffle&lt;/denchmark-link&gt;
 !!
		</comment>
	</comments>
</bug>