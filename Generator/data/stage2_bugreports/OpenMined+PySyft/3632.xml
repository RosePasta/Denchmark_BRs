<bug id='3632' author='xcway' open_date='2020-05-29T08:48:54Z' closed_time='2020-07-06T00:08:19Z'>
	<summary>The MNIST train failed after several iterations with error 'typeError: addcmul...'</summary>
	<description>
I followed the tutorials listed as below
&lt;denchmark-link:https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/Part%2006%20-%20Federated%20Learning%20on%20MNIST%20using%20a%20CNN.ipynb&gt;https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/Part%2006%20-%20Federated%20Learning%20on%20MNIST%20using%20a%20CNN.ipynb&lt;/denchmark-link&gt;

to familiar with the Federated Learning on MNIST using CNN,but the train failed after several iterations with error 'typeError: addcmul_() takes 2 positional arguments but 3 were given' ,refer to below for the detail log :
&lt;denchmark-code&gt;Train Epoch: 1 [0/60032 (0%)]   Loss: 2.308438
Train Epoch: 1 [640/60032 (1%)] Loss: 1.284017
Train Epoch: 1 [1280/60032 (2%)]        Loss: 1.071603
Train Epoch: 1 [1920/60032 (3%)]        Loss: 0.472174
Train Epoch: 1 [2560/60032 (4%)]        Loss: 0.981234
Train Epoch: 1 [3200/60032 (5%)]        Loss: 0.343123
Train Epoch: 1 [3840/60032 (6%)]        Loss: 0.369976
Train Epoch: 1 [4480/60032 (7%)]        Loss: 0.250593
Train Epoch: 1 [5120/60032 (9%)]        Loss: 0.197793
Train Epoch: 1 [5760/60032 (10%)]       Loss: 0.254211
Train Epoch: 1 [6400/60032 (11%)]       Loss: 0.309393
Train Epoch: 1 [7040/60032 (12%)]       Loss: 0.252659
Train Epoch: 1 [7680/60032 (13%)]       Loss: 0.454773
Train Epoch: 1 [8320/60032 (14%)]       Loss: 0.271393
Train Epoch: 1 [8960/60032 (15%)]       Loss: 0.178069
Train Epoch: 1 [9600/60032 (16%)]       Loss: 0.119595
Train Epoch: 1 [10240/60032 (17%)]      Loss: 0.256008
Train Epoch: 1 [10880/60032 (18%)]      Loss: 0.191856
Train Epoch: 1 [11520/60032 (19%)]      Loss: 0.172837
Train Epoch: 1 [12160/60032 (20%)]      Loss: 0.249328
Train Epoch: 1 [12800/60032 (21%)]      Loss: 0.297089
Train Epoch: 1 [13440/60032 (22%)]      Loss: 0.254361
Train Epoch: 1 [14080/60032 (23%)]      Loss: 0.223871
Train Epoch: 1 [14720/60032 (25%)]      Loss: 0.200911
Train Epoch: 1 [15360/60032 (26%)]      Loss: 0.176920
Train Epoch: 1 [16000/60032 (27%)]      Loss: 0.117337
Train Epoch: 1 [16640/60032 (28%)]      Loss: 0.222319
Train Epoch: 1 [17280/60032 (29%)]      Loss: 0.344136
Train Epoch: 1 [17920/60032 (30%)]      Loss: 0.576891
Train Epoch: 1 [18560/60032 (31%)]      Loss: 0.170682
Train Epoch: 1 [19200/60032 (32%)]      Loss: 0.306101
Train Epoch: 1 [19840/60032 (33%)]      Loss: 0.282047
Train Epoch: 1 [20480/60032 (34%)]      Loss: 0.092759
Train Epoch: 1 [21120/60032 (35%)]      Loss: 0.097649
Train Epoch: 1 [21760/60032 (36%)]      Loss: 0.179511
Train Epoch: 1 [22400/60032 (37%)]      Loss: 0.194570
Train Epoch: 1 [23040/60032 (38%)]      Loss: 0.092463
Train Epoch: 1 [23680/60032 (39%)]      Loss: 0.105520
Train Epoch: 1 [24320/60032 (41%)]      Loss: 0.119190
Train Epoch: 1 [24960/60032 (42%)]      Loss: 0.097020
Train Epoch: 1 [25600/60032 (43%)]      Loss: 0.109687
Train Epoch: 1 [26240/60032 (44%)]      Loss: 0.071690
Train Epoch: 1 [26880/60032 (45%)]      Loss: 0.134879
Train Epoch: 1 [27520/60032 (46%)]      Loss: 0.249261
Train Epoch: 1 [28160/60032 (47%)]      Loss: 0.316798
Train Epoch: 1 [28800/60032 (48%)]      Loss: 0.065665
Train Epoch: 1 [29440/60032 (49%)]      Loss: 0.063461
Traceback (most recent call last):
  File "d.py", line 158, in &lt;module&gt;
    main()
  File "d.py", line 148, in main
    train(args, model, device, federated_train_loader, optimizer, epoch)
  File "d.py", line 50, in train
    optimizer.step()
  File "C:\Anaconda3\envs\pysyft\lib\site-packages\torch\optim\lr_scheduler.py", line 66, in wrapper
    return wrapped(*args, **kwargs)
  File "C:\Anaconda3\envs\pysyft\lib\site-packages\torch\optim\adadelta.py", line 72, in step
    square_avg.mul_(rho).addcmul_(1 - rho, grad, grad)
  File "C:\Anaconda3\envs\pysyft\lib\site-packages\syft\generic\frameworks\hook\hook.py", line 466, in overloaded_native_method
    response = method(*new_args, **new_kwargs)
  File "C:\Anaconda3\envs\pysyft\lib\site-packages\syft\generic\frameworks\hook\hook.py", line 628, in overloaded_pointer_method
    response = owner.send_command(location, command)
  File "C:\Anaconda3\envs\pysyft\lib\site-packages\syft\workers\base.py", line 638, in send_command
    ret_val = self.send_msg(message, location=recipient)
  File "C:\Anaconda3\envs\pysyft\lib\site-packages\syft\workers\base.py", line 290, in send_msg
    bin_response = self._send_msg(bin_message, location)
  File "C:\Anaconda3\envs\pysyft\lib\site-packages\syft\workers\virtual.py", line 15, in _send_msg
    return location._recv_msg(message)
  File "C:\Anaconda3\envs\pysyft\lib\site-packages\syft\workers\virtual.py", line 19, in _recv_msg
    return self.recv_msg(message)
  File "C:\Anaconda3\envs\pysyft\lib\site-packages\syft\workers\base.py", line 327, in recv_msg
    response = self._message_router[type(msg)](msg)
  File "C:\Anaconda3\envs\pysyft\lib\site-packages\syft\workers\base.py", line 469, in execute_tensor_command
    return self.execute_computation_action(cmd.action)
  File "C:\Anaconda3\envs\pysyft\lib\site-packages\syft\workers\base.py", line 507, in execute_computation_action
    getattr(_self, op_name)(*args_, **kwargs_)
  File "C:\Anaconda3\envs\pysyft\lib\site-packages\syft\generic\frameworks\hook\hook.py", line 427, in overloaded_native_method
    raise route_method_exception(e, self, args, kwargs)
  File "C:\Anaconda3\envs\pysyft\lib\site-packages\syft\generic\frameworks\hook\hook.py", line 423, in overloaded_native_method
    response = method(*args, **kwargs)
TypeError: addcmul_() takes 2 positional arguments but 3 were given
&lt;/denchmark-code&gt;

The python packages installed as below:
&lt;denchmark-code&gt;Package            Version
------------------ -------------------
aiounittest        1.3.1
alembic            1.4.2
appdirs            1.4.4
atomicwrites       1.4.0
attrs              19.3.0
black              19.10b0
bleach             3.1.0
certifi            2020.4.5.1
cffi               1.14.0
chardet            3.0.4
Click              7.0
colorama           0.4.3
coverage           5.1
cryptography       2.9.2
defusedxml         0.6.0
dnspython          1.16.0
entrypoints        0.3
eventlet           0.24.1
Flask              1.1.1
Flask-Cors         3.0.7
Flask-Executor     0.9.3
Flask-Migrate      2.5.3
Flask-SocketIO     4.2.1
Flask-Sockets      0.2.1
Flask-SQLAlchemy   2.4.3
Flask-Testing      0.8.0
gevent             1.4.0
gevent-websocket   0.10.1
greenlet           0.4.15
grid               0.7.1
gunicorn           19.9.0
idna               2.8
importlib-metadata 1.5.0
ipykernel          5.1.4
ipython            7.13.0
ipython-genutils   0.2.0
ipywidgets         7.5.1
itsdangerous       1.1.0
jedi               0.16.0
Jinja2             2.10.1
jsonschema         3.2.0
jupyter            1.0.0
jupyter-client     6.1.2
jupyter-console    6.1.0
jupyter-core       4.6.3
lz4                3.0.2
Mako               1.1.2
MarkupSafe         1.1.1
mkl-fft            1.0.15
mkl-random         1.1.0
mkl-service        2.3.0
monotonic          1.5
more-itertools     8.3.0
msgpack            1.0.0
nbconvert          5.6.1
nbformat           5.0.4
notebook           6.0.3
numpy              1.18.1
olefile            0.46
packaging          20.4
pathspec           0.8.0
phe                1.4.0
Pillow             6.2.2
pip                20.0.2
pluggy             0.13.1
prometheus-client  0.7.1
prompt-toolkit     3.0.4
protobuf           3.11.3
psycopg2-binary    2.8.5
py                 1.8.1
pycparser          2.20
Pygments           2.6.1
PyGrid             0.0.1
PyJWT              1.7.1
pyparsing          2.4.7
pyrsistent         0.16.0
pysyft             0.0.1
pytest             5.4.2
python-dateutil    2.8.1
python-editor      1.0.4
python-engineio    3.12.1
python-socketio    4.5.1
pywinpty           0.5.7
pyzmq              18.1.1
qtconsole          4.7.2
QtPy               1.9.0
regex              2020.5.14
requests           2.22.0
requests-toolbelt  0.9.1
scipy              1.4.1
Send2Trash         1.5.0
setuptools         46.1.3.post20200330
six                1.14.0
SQLAlchemy         1.3.17
syft               0.2.5
syft-proto         0.4.6
tblib              1.6.0
terminado          0.8.3
testpath           0.4.4
toml               0.10.1
torch              1.4.0
torchvision        0.5.0
tornado            4.5.3
traitlets          4.3.3
typed-ast          1.4.1
urllib3            1.25.8
wcwidth            0.1.9
webencodings       0.5.1
websocket-client   0.57.0
websockets         8.1
Werkzeug           0.15.3
wheel              0.34.2
widgetsnbextension 3.5.1
wincertstore       0.2
zipp               2.2.0

&lt;/denchmark-code&gt;

The code d.py attached
&lt;denchmark-code&gt;from __future__ import print_function
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.optim.lr_scheduler import StepLR

import syft as sy  # &lt;-- NEW: import the Pysyft library




class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout2d(0.25)
        self.dropout2 = nn.Dropout2d(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        output = F.log_softmax(x, dim=1)
        return output


def train(args, model, device, train_loader, optimizer, epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        model.send(data.location) # &lt;-- NEW: send the model to the right location
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        model.get() # &lt;-- NEW: get the model back
        '''
        if batch_idx % args.log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))
        '''
        if batch_idx % args.log_interval == 0:
            loss = loss.get() # &lt;-- NEW: get the loss back
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * args.batch_size, len(train_loader) * args.batch_size, #batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))
            
def test(model, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss
            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader.dataset)

    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))


def main():

    hook = sy.TorchHook(torch)  # &lt;-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning
    bob = sy.VirtualWorker(hook, id="bob")  # &lt;-- NEW: define remote worker bob
    alice = sy.VirtualWorker(hook, id="alice")  # &lt;-- NEW: and alice

    # Training settings
    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')
    parser.add_argument('--batch-size', type=int, default=64, metavar='N',
                        help='input batch size for training (default: 64)')
    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',
                        help='input batch size for testing (default: 1000)')
    parser.add_argument('--epochs', type=int, default=4, metavar='N',
                        help='number of epochs to train (default: 14)')
    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',
                        help='learning rate (default: 1.0)')
    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',
                        help='Learning rate step gamma (default: 0.7)')
    parser.add_argument('--no-cuda', action='store_true', default=False,
                        help='disables CUDA training')
    parser.add_argument('--seed', type=int, default=1, metavar='S',
                        help='random seed (default: 1)')
    parser.add_argument('--log-interval', type=int, default=10, metavar='N',
                        help='how many batches to wait before logging training status')

    parser.add_argument('--save-model', action='store_true', default=False,
                        help='For Saving the current Model')
    args = parser.parse_args()
    use_cuda = not args.no_cuda and torch.cuda.is_available()

    torch.manual_seed(args.seed)

    device = torch.device("cuda" if use_cuda else "cpu")

    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}
    '''
    train_loader = torch.utils.data.DataLoader(
        datasets.MNIST('../data', train=True, download=True,
                       transform=transforms.Compose([
                           transforms.ToTensor(),
                           transforms.Normalize((0.1307,), (0.3081,))
                       ])),
        batch_size=args.batch_size, shuffle=True, **kwargs)
    '''
    federated_train_loader = sy.FederatedDataLoader( # &lt;-- this is now a FederatedDataLoader 
            datasets.MNIST('../data', train=True, download=True,
                   transform=transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((0.1307,), (0.3081,))
                   ]))
    .federate((bob, alice)), # &lt;-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset
    batch_size=args.batch_size, shuffle=True, **kwargs)

    test_loader = torch.utils.data.DataLoader(
        datasets.MNIST('../data', train=False, transform=transforms.Compose([
                           transforms.ToTensor(),
                           transforms.Normalize((0.1307,), (0.3081,))
                       ])),
        batch_size=args.test_batch_size, shuffle=True, **kwargs)

    model = Net().to(device)
    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)

    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)
    for epoch in range(1, args.epochs + 1):
        train(args, model, device, federated_train_loader, optimizer, epoch)
        test(model, device, test_loader)
        scheduler.step()

    print ('done')
    #if args.save_model:
    #    torch.save(model.state_dict(), "mnist_cnn.pt")


if __name__ == '__main__':
    main()
`
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='xcway' date='2020-05-29T13:26:52Z'>
		Hmm, reading the stack trace, it looks like the optimizer passes the same grad to  twice, which looks right &lt;denchmark-link:https://pytorch.org/docs/master/generated/torch.addcmul.html#torch-addcmul&gt;according to the Torch docs&lt;/denchmark-link&gt;
.
&lt;denchmark-code&gt;  File "C:\Anaconda3\envs\pysyft\lib\site-packages\torch\optim\adadelta.py", line 72, in step
    square_avg.mul_(rho).addcmul_(1 - rho, grad, grad)
&lt;/denchmark-code&gt;

I wonder if it has something to do with the way the operation is de/serialized and reconstructed on the remote Worker? Might have to try to try to replicate and dig into it with a debugger.
		</comment>
		<comment id='2' author='xcway' date='2020-06-29T00:07:00Z'>
		This issue has been marked stale because it has been open 30 days with no activity. Leave a comment or remove the stale label to unmark it. Otherwise, this will be closed in 7 days.
		</comment>
	</comments>
</bug>