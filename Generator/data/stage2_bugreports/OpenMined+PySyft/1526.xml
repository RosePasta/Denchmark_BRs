<bug id='1526' author='iamtrask' open_date='2018-09-20T12:07:03Z' closed_time='2018-09-20T12:32:48Z'>
	<summary>Matrix Multiplication Broken</summary>
	<description>
Some combination of the following PRs broke Matrix Multiplication
&lt;denchmark-link:https://github.com/OpenMined/PySyft/pull/1519&gt;#1519&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/OpenMined/PySyft/pull/1518&gt;#1518&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/OpenMined/PySyft/pull/1515&gt;#1515&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/OpenMined/PySyft/pull/1514&gt;#1514&lt;/denchmark-link&gt;

At present, the branch "trask2" still has working matrix multiplication so i plan to add each of these changes to the branch and see which one broke it.
	</description>
	<comments>
		<comment id='1' author='iamtrask' date='2018-09-20T12:09:24Z'>
		To reproduce on master branch, run
&lt;denchmark-code&gt;import random
import syft as sy
from syft.core import utils
from syft.core.frameworks.torch import utils as torch_utils
from syft.core.frameworks import encode

from syft.core.frameworks.torch.tensor import _GeneralizedPointerTensor
from syft.mpc import spdz
from syft.core.frameworks.torch.tensor import _MPCTensor
import torch
import torch.nn.functional as F
from torch.autograd import Variable as Var
import json

hook = sy.TorchHook(verbose=True)

me = hook.local_worker
me.is_client_worker = False

bob = sy.VirtualWorker(id="bob", hook=hook, is_client_worker=False)
alice = sy.VirtualWorker(id="alice", hook=hook, is_client_worker=False)
james = sy.VirtualWorker(id="james", hook=hook, is_client_worker=False)

x = torch.LongTensor([[3,-2],[-3,-4]])
x = x.share(bob, alice)

y = torch.LongTensor([[5,6],[7,8]])
y = y.share(bob, alice)

x.mm(y).get()
&lt;/denchmark-code&gt;

and you'll see
&lt;denchmark-code&gt;1.7582e+09  4.9458e+08
 3.7146e+08  4.9147e+08
[syft.core.frameworks.torch.tensor.LongTensor of size 2x2]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='iamtrask' date='2018-09-20T12:25:54Z'>
		Ok, I've narrowed it down to a bug somewhere in generate_matmul_triple_communication in spdz.py
		</comment>
		<comment id='3' author='iamtrask' date='2018-09-20T12:31:43Z'>
		Found it!!! Tiny typo in spdz.py.
&lt;denchmark-code&gt;def generate_matmul_triple_communication(shapes, workers):
    r, s, t = generate_matmul_triple(shapes)

    n_workers = len(workers)
    r_shares = share(r, n_workers)
    s_shares = share(s, n_workers)
    t_shares = share(t, n_workers)

    # For r, s, t as a shared var, send each share to its worker
    for var_shares in [r_shares, s_shares, t_shares]:
        for var_share, worker in zip(var_shares, workers):
            var_share.send(worker)

    # Build the pointer dict for r, s, t. Note that we remove the head of the pointer (via .child)
    gp_r = sy._GeneralizedPointerTensor({
        share.location: share.child for share in r_shares
    }).on(r)
    gp_s = sy._GeneralizedPointerTensor({
        share.location: share.child for share in s_shares
    }).on(s)
    gp_t = sy._GeneralizedPointerTensor({
        share.location: share.child for share in r_shares
    }).on(t)
    triple = [gp_r, gp_s, gp_t]
    return triple
&lt;/denchmark-code&gt;

should be
&lt;denchmark-code&gt;def generate_matmul_triple_communication(shapes, workers):
    r, s, t = generate_matmul_triple(shapes)

    n_workers = len(workers)
    r_shares = share(r, n_workers)
    s_shares = share(s, n_workers)
    t_shares = share(t, n_workers)

    # For r, s, t as a shared var, send each share to its worker
    for var_shares in [r_shares, s_shares, t_shares]:
        for var_share, worker in zip(var_shares, workers):
            var_share.send(worker)

    # Build the pointer dict for r, s, t. Note that we remove the head of the pointer (via .child)
    gp_r = sy._GeneralizedPointerTensor({
        share.location: share.child for share in r_shares
    }).on(r)
    gp_s = sy._GeneralizedPointerTensor({
        share.location: share.child for share in s_shares
    }).on(s)
    gp_t = sy._GeneralizedPointerTensor({
        share.location: share.child for share in t_shares
    }).on(t)
    triple = [gp_r, gp_s, gp_t]
    return triple
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>