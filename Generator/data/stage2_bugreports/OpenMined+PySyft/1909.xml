<bug id='1909' author='LaRiffle' open_date='2019-02-14T10:51:34Z' closed_time='2020-05-25T00:09:17Z'>
	<summary>Optimizer and momentum for remote tensors</summary>
	<description>
As you know, optim with momentum keeps old gradients in a buffer.
However, when gradients are pointers, it becomes really tricky if you move the parameters from a worker to another, which is exactly what we do in tutorial Part 8. Let's say you move from bob to alice, then you will try to update parameters at alice using a buffer containing pointers to bob, and you get in real trouble.
So, how should we handle this? Drop the buffer by hand? Having different optimizers and different models? Or another solution?
	</description>
	<comments>
		<comment id='1' author='LaRiffle' date='2020-02-07T08:06:55Z'>
		Any updates for this?
		</comment>
		<comment id='2' author='LaRiffle' date='2020-04-02T12:16:52Z'>
		I think this was solved by &lt;denchmark-link:https://github.com/OpenMined/PySyft/pull/3179&gt;#3179&lt;/denchmark-link&gt;
, where each worker now maintains it's own optimizer. However I'm pretty new here so I may be missing something.
		</comment>
		<comment id='3' author='LaRiffle' date='2020-04-13T14:40:22Z'>
		I'm not sure that &lt;denchmark-link:https://github.com/OpenMined/PySyft/pull/3179&gt;#3179&lt;/denchmark-link&gt;
 was intended to add full support for momentum-based optimizers. It certainly seems like a step in the right direction, but it's not clear that momentum-based optimizers are fully compatible with the rest of PySyft.
		</comment>
		<comment id='4' author='LaRiffle' date='2020-05-25T00:09:16Z'>
		This issue has been marked stale because it has been open 30 days with no activity. Leave a comment or remove the stale label to unmark it. Otherwise, this will be closed in 7 days.
		</comment>
	</comments>
</bug>