<bug id='13549' author='ghost(ghost)' open_date='2019-11-13T07:13:09Z' closed_time='2019-11-25T15:42:24Z'>
	<summary>fit_generator is slow, and I/O ain't the cause</summary>
	<description>
OS: Ubuntu Server 18.04, with xanmod kernel
Python: Anaconda 3.7
Tensorflow: 2.0.0 MKL
Keras: TF built-in
CPU: Intel(R) Core(TM)2 Quad CPU Q9300
No GPU
model.fit works as expected
&lt;denchmark-code&gt;import numpy as np
import tensorflow.keras as keras
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.regularizers import l2
from tensorflow.keras import optimizers
from tensorflow.keras import backend as K
import time, itertools, random

def makeModel(arch, constructor, name):
    inputs = Input(arch)
    model = Model(inputs, constructor(inputs), name=name)
    try:
        model.load_weights("%s.h5"%name)
    except:
        print("new %s"%name)
    return model

def saveModel(model):
    model.save_weights("%s.h5"%model.name)

def g():
    return l2(.00001)

def sr2x(obj):
    for i in range(4):
        obj = Conv1D(96, 1, activation='tanh', kernel_initializer='he_uniform', kernel_regularizer=g(), bias_regularizer=g())(obj)
        obj = GaussianNoise(.001)(obj)
    obj = Conv1D(96, 1, kernel_initializer='he_uniform', kernel_regularizer=g(), bias_regularizer=g())(obj)
    obj = Lambda(lambda x: K.sum(x, axis=1))(obj)
    obj = Reshape((3, 32))(obj)
    obj = Activation("softmax")(obj)
    return obj

sr = makeModel((None, 5,), sr2x, "sr2x")
sr.summary()
sr.compile(optimizer=optimizers.Adam(.00001), loss='categorical_crossentropy', metrics=['categorical_accuracy'])
i = 0

data = np.mgrid[0:32, 0:32, 0:32].T.reshape(-1, 3)
inp = np.dstack(((np.exp(np.float32(data[:, None])*.1789)-1.0)/255.0, np.zeros((len(data), 1, 2), dtype=np.float32)))
out = keras.utils.to_categorical(data, 32)

print(inp.shape, out.shape)

class MyCallback(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        saveModel(sr)

try:
    sr.fit(inp, out, 32, epochs=100, callbacks=[MyCallback()])
except KeyboardInterrupt:
    print("one second...")
    time.sleep(1)
    print("saving...")
saveModel(sr)
print("saved...")
&lt;/denchmark-code&gt;

model.fit_generator is 16x slower. No I/O is performed. My tests show 60000hz minimum performance outside keras.
&lt;denchmark-code&gt;import numpy as np
import tensorflow.keras as keras
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.regularizers import l2
from tensorflow.keras import optimizers
from tensorflow.keras import backend as K
import time, itertools, random

def makeModel(arch, constructor, name):
    inputs = Input(arch)
    model = Model(inputs, constructor(inputs), name=name)
    try:
        model.load_weights("%s.h5"%name)
    except:
        print("new %s"%name)
    return model

def saveModel(model):
    model.save_weights("%s.h5"%model.name)

def g():
    return l2(.00001)

def sr2x(obj):
    for i in range(4):
        obj = Conv1D(96, 1, activation='tanh', kernel_initializer='he_uniform', kernel_regularizer=g(), bias_regularizer=g())(obj)
        obj = GaussianNoise(.001)(obj)
    obj = Conv1D(96, 1, kernel_initializer='he_uniform', kernel_regularizer=g(), bias_regularizer=g())(obj)
    obj = Lambda(lambda x: K.sum(x, axis=1))(obj)
    obj = Reshape((3, 32))(obj)
    obj = Activation("softmax")(obj)
    return obj

sr = makeModel((None, 5,), sr2x, "sr2x")
sr.summary()
sr.compile(optimizer=optimizers.Adam(.00001), loss='categorical_crossentropy', metrics=['categorical_accuracy'])
i = 0



def sampler():
    data = np.mgrid[0:32, 0:32, 0:32].T.reshape(-1, 3)
    inp = np.dstack(((np.exp(np.float32(data[:, None])*.1789)-1.0)/255.0, np.zeros((len(data), 1, 2), dtype=np.float32)))
    out = keras.utils.to_categorical(data, 32)
    rg = np.arange(len(inp), dtype=np.uint32)
    while True:
        np.random.shuffle(rg)
        inpc = inp[rg]
        outc = out[rg]
        for i in range(0, len(rg), 32):
            yield inpc[i:i+32], outc[i:i+32]

class MyCallback(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        saveModel(sr)

try:
    sr.fit_generator(sampler(), steps_per_epoch=1024, epochs=100, callbacks=[MyCallback()])
except KeyboardInterrupt:
    print("one second...")
    time.sleep(1)
    print("saving...")
saveModel(sr)
print("saved...")
&lt;/denchmark-code&gt;

Edit: fixed the bug. Data was not shuffled properly.
	</description>
	<comments>
		<comment id='1' author='ghost(ghost)' date='2019-11-13T07:21:20Z'>
		Also, merely using train_on_batch is also slower.
		</comment>
		<comment id='2' author='ghost(ghost)' date='2019-11-13T07:32:13Z'>
		Multiprocessing doesn't make it faster. Neither more workers do.
		</comment>
		<comment id='3' author='ghost(ghost)' date='2019-11-13T07:48:54Z'>
		Sequence version. It is slow, too. Multiprocessing doesn't matter.
&lt;denchmark-code&gt;import numpy as np
import tensorflow.keras as keras
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.regularizers import l2
from tensorflow.keras import optimizers
from tensorflow.keras import backend as K
import time, itertools, random

def makeModel(arch, constructor, name):
    inputs = Input(arch)
    model = Model(inputs, constructor(inputs), name=name)
    try:
        model.load_weights("%s.h5"%name)
    except:
        print("new %s"%name)
    return model

def saveModel(model):
    model.save_weights("%s.h5"%model.name)

def g():
    return l2(.00001)

def sr2x(obj):
    for i in range(4):
        obj = Conv1D(96, 1, activation='tanh', kernel_initializer='he_uniform', kernel_regularizer=g(), bias_regularizer=g())(obj)
        obj = GaussianNoise(.001)(obj)
    obj = Conv1D(96, 1, kernel_initializer='he_uniform', kernel_regularizer=g(), bias_regularizer=g())(obj)
    obj = Lambda(lambda x: K.sum(x, axis=1))(obj)
    obj = Reshape((3, 32))(obj)
    obj = Activation("softmax")(obj)
    return obj

sr = makeModel((None, 5,), sr2x, "sr2x")
sr.summary()
sr.compile(optimizer=optimizers.Adam(.00001), loss='categorical_crossentropy', metrics=['categorical_accuracy'])


class MySequence(keras.utils.Sequence):
    def __init__(self):
        data = np.mgrid[0:32, 0:32, 0:32].T.reshape(-1, 3)
        self.inp = np.dstack(((np.exp(np.float32(data[:, None])*.1789)-1.0)/255.0, np.zeros((len(data), 1, 2), dtype=np.float32)))
        self.out = keras.utils.to_categorical(data, 32)
        self.rg = np.arange(32768, dtype=np.uint32)
        self.on_epoch_end()
    def __len__(self):
        return 1024
    def __getitem__(self, idx):
        idx = idx*32
        return self.inpc[idx:idx+32], self.outc[idx:idx+32]
    def on_epoch_end(self):
        np.random.shuffle(self.rg)
        self.inpc = self.inp[self.rg]
        self.outc = self.out[self.rg]

class MyCallback(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        saveModel(sr)

try:
    sr.fit_generator(MySequence(), steps_per_epoch=1024, epochs=100, callbacks=[MyCallback()])
except KeyboardInterrupt:
    print("one second...")
    time.sleep(1)
    print("saving...")
saveModel(sr)
print("saved...")
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='ghost(ghost)' date='2019-11-13T07:52:53Z'>
		What happens if I interrupt fit_generator if multiprocessing is used.
&lt;denchmark-code&gt;^CError in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/util.py", line 265, in _run_finalizers
Process Keras_worker_ForkPoolWorker-7:
Process Keras_worker_ForkPoolWorker-5:
Process Keras_worker_ForkPoolWorker-8:
Process Keras_worker_ForkPoolWorker-6:
Traceback (most recent call last):
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
    self.run()
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/pool.py", line 110, in worker
    task = get()
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/queues.py", line 351, in get
    with self._rlock:
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/synchronize.py", line 95, in __enter__
    return self._semlock.__enter__()
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
    self.run()
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
    self.run()
KeyboardInterrupt
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/pool.py", line 110, in worker
    task = get()
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/pool.py", line 110, in worker
    task = get()
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/queues.py", line 351, in get
    with self._rlock:
Traceback (most recent call last):
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/queues.py", line 351, in get
    with self._rlock:
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/synchronize.py", line 95, in __enter__
    return self._semlock.__enter__()
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/synchronize.py", line 95, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
    self.run()
KeyboardInterrupt
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/pool.py", line 110, in worker
    task = get()
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/queues.py", line 351, in get
    with self._rlock:
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/synchronize.py", line 95, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
    finalizer()
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/util.py", line 189, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/pool.py", line 581, in _terminate_pool
    cls._help_stuff_finish(inqueue, task_handler, len(pool))
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/pool.py", line 566, in _help_stuff_finish
    inqueue._rlock.acquire()
KeyboardInterrupt
Exception ignored in: &lt;Finalize object, dead&gt;
Traceback (most recent call last):
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/util.py", line 189, in __call__
  File "/home/bckpkol/anaconda3/lib/python3.7/multiprocessing/heap.py", line 214, in free
TypeError: 'NoneType' object is not callable
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='ghost(ghost)' date='2019-11-13T11:46:51Z'>
		Hi,@bckpkol
I have experienced this issue too.
The problem is with generator function.
you can try explicitly calling gc collector (This fixed my issue but it was in tf2.0)
or check if there are no memory(data) leaks in your generator function.
&lt;denchmark-code&gt;def sampler():
    data = np.mgrid[0:32, 0:32, 0:32].T.reshape(-1, 3)
    inp = np.dstack(((np.exp(np.float32(data[:, None])*.1789)-1.0)/255.0, np.zeros((len(data), 1, 2), dtype=np.float32)))
    out = keras.utils.to_categorical(data, 32)
    rg = np.arange(len(inp), dtype=np.uint32)
    while True:
        np.random.shuffle(rg)
        inpc = inp[rg]
        outc = out[rg]
        for i in range(0, len(rg), 32):
            yield inp[i:i+32], out[i:i+32]
&lt;/denchmark-code&gt;

check with removing data,out,rg from the sampler and initializing it prior.
Let me know if it works.
Thanks.
		</comment>
		<comment id='6' author='ghost(ghost)' date='2019-11-20T02:42:45Z'>
		Hi, &lt;denchmark-link:https://github.com/kartik4949&gt;@kartik4949&lt;/denchmark-link&gt;
,
The problem is I can't feed the whole data at once. It's too big, and smaller chunks regularize better anyway. Still, I need to shuffle. If I remove data, inp, out and rg, I shall also remove shuffle, and my data will not be shuffled every epoch but just once.
Also, I had a bug.

should be

		</comment>
		<comment id='7' author='ghost(ghost)' date='2019-11-25T15:32:12Z'>
		That code is slow. No alloc in the generator. But after an update, all generator versions are just 3x slower. Checked with both BLAS Arch and MKL Anaconda builds.
Edit. Yes, now generator is 3x slower, but only the first epoch, next are equally fast.
&lt;denchmark-code&gt;import numpy as np
import tensorflow.keras as keras
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.regularizers import l2
from tensorflow.keras import optimizers
from tensorflow.keras import backend as K
import time, itertools, random

def makeModel(arch, constructor, name):
    inputs = Input(arch)
    model = Model(inputs, constructor(inputs), name=name)
    try:
        model.load_weights("%s.h5"%name)
    except:
        print("new %s"%name)
    return model

def saveModel(model):
    model.save_weights("%s.h5"%model.name)

def g():
    return l2(.00001)

def sr2x(obj):
    for i in range(4):
        obj = Conv1D(96, 1, activation='tanh', kernel_initializer='he_uniform', kernel_regularizer=g(), bias_regularizer=g())(obj)
        obj = GaussianNoise(.001)(obj)
    obj = Conv1D(96, 1, kernel_initializer='he_uniform', kernel_regularizer=g(), bias_regularizer=g())(obj)
    obj = Lambda(lambda x: K.sum(x, axis=1))(obj)
    obj = Reshape((3, 32))(obj)
    obj = Activation("softmax")(obj)
    return obj

sr = makeModel((None, 5,), sr2x, "sr2x")
sr.summary()
sr.compile(optimizer=optimizers.Adam(.00001), loss='categorical_crossentropy', metrics=['categorical_accuracy'])
i = 0


data = np.mgrid[0:32, 0:32, 0:32].T.reshape(-1, 3)
inp = np.dstack(((np.exp(np.float32(data[:, None])*.1789)-1.0)/255.0, np.zeros((len(data), 1, 2), dtype=np.float32)))
out = keras.utils.to_categorical(data, 32)
rg = np.arange(len(inp), dtype=np.uint32)
np.random.shuffle(rg)
inpc = inp[rg]
outc = out[rg]

def sampler():
    while True:
        for i in range(0, len(rg), 32):
            yield inpc[i:i+32], outc[i:i+32]

class MyCallback(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        saveModel(sr)

try:
    sr.fit_generator(sampler(), steps_per_epoch=1024, epochs=100, callbacks=[MyCallback()])
except KeyboardInterrupt:
    print("one second...")
    time.sleep(1)
    print("saving...")
saveModel(sr)
print("saved...")
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>