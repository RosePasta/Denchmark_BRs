<bug id='12594' author='bhaskar-dhariyal' open_date='2019-04-01T13:19:07Z' closed_time='2020-01-31T22:19:22Z'>
	<summary>NameError: name 'embed' is not defined</summary>
	<description>
Please make sure that the boxes below are checked before you submit your issue.
If your issue is an , please ask your question on &lt;denchmark-link:http://stackoverflow.com/questions/tagged/keras&gt;StackOverflow&lt;/denchmark-link&gt;
 or &lt;denchmark-link:https://keras-slack-autojoin.herokuapp.com/&gt;on the Keras Slack channel&lt;/denchmark-link&gt;
 instead of opening a GitHub issue.
Thank you!


 Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps


 Check that your version of TensorFlow is up-to-date. The installation instructions can be found here.


 Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).


I'm working on Keras model which uses Universal Sentence Embedding to encode the provided sentences. However, when I save the model for future usage, the mentioned error is thrown. NameError: name 'embed' is not defined
The sentences are converted to embedding using UniversalEmbedding(x) function. The code of whole model is taken from this link.
&lt;denchmark-code&gt;!wget https://raw.githubusercontent.com/Tony607/Keras-Text-Transfer-Learning/master/train_5500.txt
!wget https://raw.githubusercontent.com/Tony607/Keras-Text-Transfer-Learning/master/test_data.txt

import tensorflow as tf
import tensorflow_hub as hub
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import re
import seaborn as sns
import keras.layers as layers
from keras.models import Model
from keras import backend as K
np.random.seed(10)

def get_dataframe(filename):
    lines = open(filename, 'r').read().splitlines()
    data = []
    for i in range(0, len(lines)):
        label = lines[i].split(' ')[0]
        label = label.split(":")[0]
        text = ' '.join(lines[i].split(' ')[1:])
        text = re.sub('[^A-Za-z0-9 ,\?\'\"-._\+\!/\`@=;:]+', '', text)
        data.append([label, text])

    df = pd.DataFrame(data, columns=['label', 'text'])
    df.label = df.label.astype('category')
    return df

df_train = get_dataframe('train_5500.txt')
df_train = get_dataframe('test_data.txt')

category_counts = len(df_train.label.cat.categories)
module_url = "https://tfhub.dev/google/universal-sentence-encoder-large/3" 
embed = hub.Module(module_url)
embed_size = embed.get_output_info_dict()['default'].get_shape()[1].value

def UniversalEmbedding(x):
    return embed(tf.squeeze(tf.cast(x, tf.string)), signature="default", as_dict=True)["default"]

input_text = layers.Input(shape=(1,), dtype='string')
embedding = layers.Lambda(UniversalEmbedding, output_shape=(embed_size,))(input_text)
dense = layers.Dense(256, activation='relu')(embedding)
pred = layers.Dense(category_counts, activation='softmax')(dense)
model = Model(inputs=[input_text], outputs=pred)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

train_text = df_train['text'].tolist()
train_text = np.array(train_text, dtype=object)[:, np.newaxis]

train_label = np.asarray(pd.get_dummies(df_train.label), dtype = np.int8)

df_test = get_dataframe('test_data.txt')
test_text = df_test['text'].tolist()
test_text = np.array(test_text, dtype=object)[:, np.newaxis]
test_label = np.asarray(pd.get_dummies(df_test.label), dtype = np.int8)


with tf.Session() as session:
  K.set_session(session)
  session.run(tf.global_variables_initializer())
  session.run(tf.tables_initializer())
  history = model.fit(train_text, 
            train_label,
            validation_data=(test_text, test_label),
            epochs=2,
            batch_size=32)
  model.save_weights('./model.h5')
  model.save('mod.h5')
&lt;/denchmark-code&gt;

When I try to load the model like
&lt;denchmark-code&gt;from keras.models import load_model

load_model('mod.h5') 
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='bhaskar-dhariyal' date='2019-09-05T18:35:02Z'>
		You may include all needed imports and variables in your UniversalEmbedding(x) lambda function to have them in scope inside your UniversalEmbedding(x) lambda function when Keras loads and serializes your saved model with keras.models.load_model.
Example (worked for me):
&lt;denchmark-code&gt;def UniversalEmbedding(x):
    import tensorflow as tf
    import tensorflow_hub as hub
    module_url = "https://tfhub.dev/google/universal-sentence-encoder-large/3" 
    embed = hub.Module(module_url)
    return embed(tf.squeeze(tf.cast(x, tf.string)), signature="default", as_dict=True)["default"]
&lt;/denchmark-code&gt;

Another option would be to explicitly handle any custom objects inside your Keras Lambda Layer. Further information: &lt;denchmark-link:https://keras.io/getting-started/faq/#handling-custom-layers-or-other-custom-objects-in-saved-models&gt;https://keras.io/getting-started/faq/#handling-custom-layers-or-other-custom-objects-in-saved-models&lt;/denchmark-link&gt;

I have tried this, but could not make it handle the custom 'embed' object. Anyways, here the example of custom object handling in keras.models.load_model:
&lt;denchmark-code&gt;load_model('mod.h5', custom_objects={'tf': tf, 'hub': hub} )
&lt;/denchmark-code&gt;

Keras Version: 2.2.4
tensorflow Version: 1.12.0
Solution found here: &lt;denchmark-link:https://stackoverflow.com/a/54348075&gt;https://stackoverflow.com/a/54348075&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>