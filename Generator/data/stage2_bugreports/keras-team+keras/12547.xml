<bug id='12547' author='LukeBolly' open_date='2019-03-25T10:10:57Z' closed_time='2019-04-12T00:33:48Z'>
	<summary>Cannot restore a frozen graph .pb which contains a tf.keras.layers.BatchNormalization layer</summary>
	<description>
Freezing a Keras graph which contains a BatchNormalization Layer creates a graph which cannot be loaded by the Tensorflow API. The following error is produced:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "C:\Program Files\JetBrains\PyCharm 2018.2.5\helpers\pydev\pydevd.py", line 1664, in &lt;module&gt;
    main()
  File "C:\Program Files\JetBrains\PyCharm 2018.2.5\helpers\pydev\pydevd.py", line 1658, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File "C:\Program Files\JetBrains\PyCharm 2018.2.5\helpers\pydev\pydevd.py", line 1068, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File "C:\Program Files\JetBrains\PyCharm 2018.2.5\helpers\pydev\_pydev_imps\_pydev_execfile.py", line 18, in execfile
    exec(compile(contents+"\n", file, 'exec'), glob, loc)
  File "C:/code sandbox/keras_load_batchnorm.py", line 37, in &lt;module&gt;
    tf.import_graph_def(graph_def)
  File "C:\Users\lukeb\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\util\deprecation.py", line 488, in new_func
    return func(*args, **kwargs)
  File "C:\Users\lukeb\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\importer.py", line 422, in import_graph_def
    raise ValueError(str(e))
ValueError: Input 0 of node import/bn/cond/ReadVariableOp/Switch was passed float from import/bn/gamma:0 incompatible with expected resource.
&lt;/denchmark-code&gt;

It seems that the "convert_variables_to_constants" method doesn't know what to do with the Batch Normalization layer.
Here is a fully working example that demonstrates the error:
&lt;denchmark-code&gt;import os
import tensorflow as tf
from tensorflow.python.framework.graph_util import convert_variables_to_constants
from tensorflow.python.platform import gfile

path = "graph\\"
dirname = os.path.dirname(os.path.realpath(__file__))
filename = "frozen_graph.pb"
output_folder = os.path.join(dirname, path)
output_file = os.path.join(output_folder, filename)
os.makedirs(output_folder, exist_ok=True)

sess = tf.Session()
tf.keras.backend.set_session(sess)

input = tf.keras.layers.Input(shape=(1, 1, 1), name="input")
x = tf.keras.layers.BatchNormalization(name="bn")(input)
x = tf.keras.layers.Activation("relu", name="ouput")(x)
model = tf.keras.models.Model(input, x)
model.compile("adam", loss="mse")

sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])

input_graph_def = sess.graph.as_graph_def()
output_names = [node.op.name for node in model.outputs]
freeze_var_names = list(set(v.op.name for v in tf.global_variables()))

frozen_graph = convert_variables_to_constants(sess, input_graph_def, output_names, freeze_var_names)
tf.train.write_graph(frozen_graph, "", output_file, as_text=False)

with sess:
   with gfile.FastGFile(output_file,'rb') as f:
       graph_def = tf.GraphDef()
   graph_def.ParseFromString(f.read())
   sess.graph.as_default()
   tf.import_graph_def(graph_def)
&lt;/denchmark-code&gt;

I have tried the solutions suggested by the following:
-&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/3628#issuecomment-272149052&gt;Altering nodes before saving: &lt;/denchmark-link&gt;
This one doesn't fix it
-&lt;denchmark-link:https://stackoverflow.com/a/52823701/6936275&gt;Setting learning phase: &lt;/denchmark-link&gt;
This one cannot be used outside of a Keras environment
-&lt;denchmark-link:https://stackoverflow.com/a/51157850/6936275&gt;Rolling batchnorm layer into previous layers: &lt;/denchmark-link&gt;
I started this one but it gets very complicated when your layers are not simply stacked.
This is a real showstopper for getting models into production, especially with Keras becoming the main API for RNN's in tf 2.0.
	</description>
	<comments>
		<comment id='1' author='LukeBolly' date='2019-04-05T03:58:51Z'>
		&lt;denchmark-link:https://github.com/LukeBolly&gt;@LukeBolly&lt;/denchmark-link&gt;
 Could you try the solution provided &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/25721#issuecomment-477877677&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='LukeBolly' date='2019-04-12T00:33:45Z'>
		Since the related issue was closed in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/25721#issuecomment-477877677&gt;tensorflow/tensorflow#25721 (comment)&lt;/denchmark-link&gt;
, I am closing this one as well now. Feel free to reopen it if you hit further issue.
		</comment>
		<comment id='3' author='LukeBolly' date='2019-04-12T03:46:44Z'>
		I didn't try the solution you linked, but I resolved it by saving the trained weights, clearing the session, setting the learning phase, loading the model back, then restoring the trained weights into the (now evaluation) model. Doing this removes all of the 'switch' ops from the graph and converts your dropout nodes to identity ops. The model can then be frozen like usual.
&lt;denchmark-code&gt;trained_model.save_weights(output_weights)

tf.keras.backend.clear_session()
tf.keras.backend.set_learning_phase(0)    # This is the important part
eval_model = model_build_function()
eval_model.load_weights(output_weights, by_name=True)
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>