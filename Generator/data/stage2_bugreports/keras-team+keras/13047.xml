<bug id='13047' author='shivg7706' open_date='2019-07-02T07:01:22Z' closed_time='2019-07-31T04:54:30Z'>
	<summary>model created through Sequential and Functional APIs behaves differently.</summary>
	<description>
Created two same CNN one with Sequential API and another with Functional API. One created using Sequential API outputs good validation accuracy around 80-85% but another created with Functional API behaves weird and output validation accuracy about 10-20%.
	</description>
	<comments>
		<comment id='1' author='shivg7706' date='2019-07-02T08:10:45Z'>
		Can you share the source code?
		</comment>
		<comment id='2' author='shivg7706' date='2019-07-02T08:16:19Z'>
		&lt;denchmark-code&gt;def mobilenet_functional():
    inputs = Input(shape=(224, 224, 3))
    x = MobileNet(include_top=False, weights='imagenet', input_tensor=inputs, pooling='avg')
    x = Dense(7, activation='softmax')(x.output)
    
    model = Model(inputs, x)
    for layer in model.layers[:75]:
        layer.trainable = False
    
    model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])

    return model
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;def mobilenet_sequential():
    model = Sequential()
    model.add(MobileNet(include_top=False, weights='imagenet', input_shape=(224, 224, 3), pooling='avg'))
    model.add(Dense(7, activation='softmax'))
    
    
    for layer in model.layers[0].layers[:75]:
        layer.trainable = False
    
    model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])

    return model
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='shivg7706' date='2019-07-02T08:52:47Z'>
		I think there's a typo in for layer in model.layers[0].layers[:75]: inside mobilenet_sequential(). Did you mean for layer in model.layers[:75]:?
Also, are you sure that the rest of the stuff in your source code is the same for both models? Like, epochs, steps_per_epoch if using fit_generator, etc.
		</comment>
		<comment id='4' author='shivg7706' date='2019-07-02T08:58:13Z'>
		Hey &lt;denchmark-link:https://github.com/dabasajay&gt;@dabasajay&lt;/denchmark-link&gt;
, there is no typo mistake. Since  is added to model so this became first layer.
&lt;denchmark-link:https://user-images.githubusercontent.com/21190262/60499191-9bf4f580-9cd5-11e9-80dd-20e2164ab6aa.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='shivg7706' date='2019-07-02T08:58:40Z'>
		
I think there's a typo in for layer in model.layers[0].layers[:75]: inside mobilenet_sequential(). Did you mean for layer in model.layers[:75]:?
Also, are you sure that the rest of the stuff in your source code is the same for both models? Like, epochs, steps_per_epoch if using fit_generator, etc.

Everything is same
		</comment>
		<comment id='6' author='shivg7706' date='2019-07-02T09:35:00Z'>
		
Hey @dabasajay, there is no typo mistake. Since MobileNet is added to model so this became first layer.

Oh, my bad. Sequential uses Model class internally so there shouldn't be this much difference in performance. I believe the error is somewhere else in your source code but if that's not the case, I'm clueless about this.
		</comment>
		<comment id='7' author='shivg7706' date='2019-07-05T06:05:51Z'>
		A similar issue.  See&lt;denchmark-link:url&gt;https://github.com/tensorflow/tensorflow/issues/15026&lt;/denchmark-link&gt;
. Why???  &lt;denchmark-link:https://github.com/dabasajay&gt;@dabasajay&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='shivg7706' date='2019-07-05T15:13:48Z'>
		Hi &lt;denchmark-link:https://github.com/wangyexiang&gt;@wangyexiang&lt;/denchmark-link&gt;

This issue is pretty different from the one you've referenced.
For this issue,  API internally uses the functional  API itself so the error must lie in user-specific code and not with Keras. Though results may differ slightly due to random initializations this huge difference doesn't make sense to me.
For the issue you've referenced, the implementations are different so I'm not really sure whether the error is with user's code or with the library. fchollet suggested that the error is with the user's code.
		</comment>
		<comment id='9' author='shivg7706' date='2019-07-07T02:35:46Z'>
		
Hi @wangyexiang
This issue is pretty different from the one you've referenced.
For this issue, Sequential API internally uses the functional Model API itself so the error must lie in user-specific code and not with Keras. Though results may differ slightly due to random initializations this huge difference doesn't make sense to me.
For the issue you've referenced, the implementations are different so I'm not really sure whether the error is with user's code or with the library. fchollet suggested that the error is with the user's code.

Thanks. Well, I don't think the error is with the user's code.  As you can see, I set random seed for reproducibility and   for both keras and tf.keras(as fchollet suggested). However, I still get big different result. Why??? Where am I wrong? &lt;denchmark-link:https://github.com/dabasajay&gt;@dabasajay&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='shivg7706' date='2019-07-09T00:24:13Z'>
		&lt;denchmark-link:https://github.com/wangyexiang&gt;@wangyexiang&lt;/denchmark-link&gt;
 Can you provide a standalone code to reproduce the issue? Also, fill the &lt;denchmark-link:https://github.com/keras-team/keras/issues/new/choose&gt;template&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='11' author='shivg7706' date='2019-07-09T02:46:25Z'>
		
Can you provide a standalone code to reproduce the issue?

Well. Here is my code.
Source code(pure keras import)：
&lt;denchmark-code&gt;import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras import backend as K
&lt;/denchmark-code&gt;

Source code(tf.keras import):
&lt;denchmark-code&gt;from tensorflow import keras
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras import backend as K
&lt;/denchmark-code&gt;

commom parts:
&lt;denchmark-code&gt;import numpy as np
import random as rn
import tensorflow as tf

np.random.seed(2019)
rn.seed(2019)
tf.compat.v1.set_random_seed(2019)


batch_size = 128
num_classes = 10
epochs = 12

# input image dimensions
img_rows, img_cols = 28, 28

# the data, split between train and test sets
(x_train, y_train), (x_test, y_test) = mnist.load_data()

if K.image_data_format() == 'channels_first':
    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
    input_shape = (1, img_rows, img_cols)
else:
    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
    input_shape = (img_rows, img_cols, 1)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),
                 activation='relu',
                 strides=(1, 1),
                 padding='valid',
                 dilation_rate=(1, 1), 
                 use_bias=True, 
                 kernel_initializer='glorot_uniform',
                 bias_initializer='zeros',
                 input_shape=input_shape))
model.add(Conv2D(64, (3, 3),
                strides=(1, 1),
                 padding='valid',
                 dilation_rate=(1, 1), 
                 use_bias=True, 
                 kernel_initializer='glorot_uniform',
                 bias_initializer='zeros', 
                 activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', activation='softmax'))

model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])

model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test))
score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
&lt;/denchmark-code&gt;

Result(pure keras)：
&lt;denchmark-code&gt;Epoch 1/12
60000/60000 [==============================] - 10s 161us/step - loss: 0.2684 - acc: 0.9177 - val_loss: 0.0599 - val_acc: 0.9812
Epoch 2/12
60000/60000 [==============================] - 6s 95us/step - loss: 0.0893 - acc: 0.9731 - val_loss: 0.0408 - val_acc: 0.9859
Epoch 3/12
60000/60000 [==============================] - 6s 92us/step - loss: 0.0658 - acc: 0.9797 - val_loss: 0.0344 - val_acc: 0.9884
Epoch 4/12
60000/60000 [==============================] - 6s 93us/step - loss: 0.0534 - acc: 0.9843 - val_loss: 0.0315 - val_acc: 0.9888
Epoch 5/12
60000/60000 [==============================] - 6s 93us/step - loss: 0.0471 - acc: 0.9859 - val_loss: 0.0297 - val_acc: 0.9903
Epoch 6/12
60000/60000 [==============================] - 6s 93us/step - loss: 0.0431 - acc: 0.9869 - val_loss: 0.0273 - val_acc: 0.9910
Epoch 7/12
60000/60000 [==============================] - 6s 94us/step - loss: 0.0379 - acc: 0.9881 - val_loss: 0.0258 - val_acc: 0.9911
Epoch 8/12
60000/60000 [==============================] - 6s 93us/step - loss: 0.0338 - acc: 0.9896 - val_loss: 0.0253 - val_acc: 0.9917
Epoch 9/12
60000/60000 [==============================] - 6s 93us/step - loss: 0.0315 - acc: 0.9902 - val_loss: 0.0282 - val_acc: 0.9911
Epoch 10/12
60000/60000 [==============================] - 6s 94us/step - loss: 0.0277 - acc: 0.9908 - val_loss: 0.0262 - val_acc: 0.9920
Epoch 11/12
60000/60000 [==============================] - 6s 93us/step - loss: 0.0275 - acc: 0.9910 - val_loss: 0.0253 - val_acc: 0.9918
Epoch 12/12
60000/60000 [==============================] - 6s 92us/step - loss: 0.0252 - acc: 0.9921 - val_loss: 0.0273 - val_acc: 0.9916
Test loss: 0.02725972963081831
Test accuracy: 0.9916
&lt;/denchmark-code&gt;

Result(tf.keras):
&lt;denchmark-code&gt;Epoch 1/12
60000/60000 [==============================] - 7s 118us/sample - loss: 2.2976 - acc: 0.1186 - val_loss: 2.2629 - val_acc: 0.3077
Epoch 2/12
60000/60000 [==============================] - 4s 70us/sample - loss: 2.2391 - acc: 0.2488 - val_loss: 2.1936 - val_acc: 0.5507
Epoch 3/12
60000/60000 [==============================] - 4s 70us/sample - loss: 2.1677 - acc: 0.3645 - val_loss: 2.0994 - val_acc: 0.6332
Epoch 4/12
60000/60000 [==============================] - 4s 70us/sample - loss: 2.0664 - acc: 0.4451 - val_loss: 1.9656 - val_acc: 0.6864
Epoch 5/12
60000/60000 [==============================] - 4s 70us/sample - loss: 1.9266 - acc: 0.5110 - val_loss: 1.7900 - val_acc: 0.7180
Epoch 6/12
60000/60000 [==============================] - 4s 70us/sample - loss: 1.7591 - acc: 0.5528 - val_loss: 1.5817 - val_acc: 0.7464
Epoch 7/12
60000/60000 [==============================] - 4s 70us/sample - loss: 1.5767 - acc: 0.5961 - val_loss: 1.3670 - val_acc: 0.7686
Epoch 8/12
60000/60000 [==============================] - 4s 70us/sample - loss: 1.4031 - acc: 0.6257 - val_loss: 1.1722 - val_acc: 0.7870
Epoch 9/12
60000/60000 [==============================] - 4s 70us/sample - loss: 1.2534 - acc: 0.6560 - val_loss: 1.0119 - val_acc: 0.8047
Epoch 10/12
60000/60000 [==============================] - 4s 70us/sample - loss: 1.1304 - acc: 0.6804 - val_loss: 0.8855 - val_acc: 0.8194
Epoch 11/12
60000/60000 [==============================] - 4s 71us/sample - loss: 1.0355 - acc: 0.6964 - val_loss: 0.7891 - val_acc: 0.8317
Epoch 12/12
60000/60000 [==============================] - 4s 70us/sample - loss: 0.9585 - acc: 0.7164 - val_loss: 0.7156 - val_acc: 0.8390
Test loss: 0.7156027168273926
Test accuracy: 0.839
&lt;/denchmark-code&gt;

As you can see, It has a big different result. The code is mainly came from &lt;denchmark-link:url&gt;https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py&lt;/denchmark-link&gt;
. I just set random seed for reproducibility and same kernel_initializer, bias_initializer for both keras and tf.keras(as fchollet suggested). &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='shivg7706' date='2019-07-15T23:54:10Z'>
		&lt;denchmark-link:https://github.com/wangyexiang&gt;@wangyexiang&lt;/denchmark-link&gt;
 I could reproduce the issue. Thanks for detailed report.
		</comment>
		<comment id='13' author='shivg7706' date='2019-07-24T10:46:31Z'>
		It worked for me. See &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/15026#issuecomment-513819963&gt;here&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='shivg7706' date='2019-07-24T17:30:46Z'>
		&lt;denchmark-link:https://github.com/shivg7706&gt;@shivg7706&lt;/denchmark-link&gt;
 Can you provide a standalone code to reproduce the issue?
&lt;denchmark-link:https://github.com/wangyexiang&gt;@wangyexiang&lt;/denchmark-link&gt;
 issue was resolved by setting  in Adadelta optimizer. There is a difference in default  in Keras (1.0) and in tf.keras (1e-3)
&lt;denchmark-code&gt;model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(1.0),
              metrics=['accuracy'])
&lt;/denchmark-code&gt;

		</comment>
		<comment id='15' author='shivg7706' date='2019-07-31T04:54:30Z'>
		Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
	</comments>
</bug>