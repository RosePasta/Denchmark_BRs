<bug id='12961' author='saintthor' open_date='2019-06-14T14:59:49Z' closed_time='2019-07-09T16:19:18Z'>
	<summary>keras v 2.2.4-tf save model ValueError: Could not pack sequence.</summary>
	<description>
I am using tensorflow.keras on colab.research.google.com. It runs good till today.
I notice the tf version upgraded from 1.13 to 1.14.0-rc1, and tf.keras.version is 2.2.4-tf.
I have trained the same model and saved well before. But now, when I call model.save( ... ), an error occurs like:


 in ChkEvaluate()
18         MinEvaLoss = EvLoss
19         if EvLoss &lt; 0.47:
---&gt; 20             mo.save( DIR + 'model3_Rd' )
21             Saved += 1
22             print( '--------- model saved. ---------' )
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in save(self, filepath, overwrite, include_optimizer, save_format)
1209     ```
1210     """
-&gt; 1211     saving.save_model(self, filepath, overwrite, include_optimizer, save_format)
1212
1213   def save_weights(self, filepath, overwrite=True, save_format=None):
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format)
111           'or using save_weights.')
112     hdf5_format.save_model_to_hdf5(
--&gt; 113         model, filepath, overwrite, include_optimizer)
114     return
115
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py in save_model_to_hdf5(model, filepath, overwrite, include_optimizer)
97         {
98             'class_name': model.class.name,
---&gt; 99             'config': model.get_config()
100         },
101         default=serialization.get_json_type).encode('utf8')
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in get_config(self)
991       model_inputs.append(
992           tf_utils.ListWrapper([layer.name, new_node_index, tensor_index]))
--&gt; 993     model_inputs = nest.pack_sequence_as(self._nested_inputs, model_inputs)
994     # Preserve external Keras compat for Models with single input.
995     if not nest.is_sequence(model_inputs):
/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py in pack_sequence_as(structure, flat_sequence, expand_composites)
447           "Could not pack sequence. Structure had %d elements, but "
448           "flat_sequence had %d elements.  Structure: %s, flat_sequence: %s." %
--&gt; 449           (len(flat_structure), len(flat_sequence), structure, flat_sequence))
450   return _sequence_like(structure, packed)
451
ValueError: Could not pack sequence. Structure had 21 elements, but flat_sequence had 1 elements.  Structure: [&lt;tf.Tensor 'tshare_1:0' shape=(?, 240, 1) dtype=float32&gt;, &lt;tf.Tensor 'm_5_1:0' shape=(?, 3, 20, 1) dtype=float32&gt;, &lt;tf.Tensor 'm_15_1:0' shape=(?, 3, 20, 1) dtype=float32&gt;, &lt;tf.Tensor 'm_60_1:0' shape=(?, 3, 20, 1) dtype=float32&gt;, &lt;tf.Tensor 'm_240_1:0' shape=(?, 3, 20, 1) dtype=float32&gt;, &lt;tf.Tensor 'm1680_1:0' shape=(?, 3, 20, 1) dtype=float32&gt;, &lt;tf.Tensor 'm4800_1:0' shape=(?, 3, 20, 1) dtype=float32&gt;, &lt;tf.Tensor 'r15_1:0' shape=(?, 3, 15, 1) dtype=float32&gt;, &lt;tf.Tensor 'r60_1:0' shape=(?, 3, 15, 1) dtype=float32&gt;, &lt;tf.Tensor 'r240_1:0' shape=(?, 3, 15, 1) dtype=float32&gt;, &lt;tf.Tensor 'r1680_1:0' shape=(?, 3, 15, 1) dtype=float32&gt;, &lt;tf.Tensor 'r4800_1:0' shape=(?, 3, 15, 1) dtype=float32&gt;, &lt;tf.Tensor 'indik15_1:0' shape=(?, 3, 13, 1) dtype=float32&gt;, &lt;tf.Tensor 'indik60_1:0' shape=(?, 3, 13, 1) dtype=float32&gt;, &lt;tf.Tensor 'indik240_1:0' shape=(?, 3, 13, 1) dtype=float32&gt;, &lt;tf.Tensor 'indik1680_1:0' shape=(?, 3, 13, 1) dtype=float32&gt;, &lt;tf.Tensor 'indik4800_1:0' shape=(?, 3, 13, 1) dtype=float32&gt;, &lt;tf.Tensor 'ks_1:0' shape=(?, 60) dtype=float32&gt;, &lt;tf.Tensor 'dayinfo_1:0' shape=(?, 4) dtype=float32&gt;, &lt;tf.Tensor 'maday_1:0' shape=(?, 5, 10, 1) dtype=float32&gt;, &lt;tf.Tensor 'polyline_1:0' shape=(?, 20, 4, 1) dtype=float32&gt;], flat_sequence: [&lt;tensorflow.python.keras.utils.tf_utils.ListWrapper object at 0x7f44949ea2b0&gt;].
	</description>
	<comments>
		<comment id='1' author='saintthor' date='2019-06-14T15:15:50Z'>
		There is somthing different in model.summary(). The current summary is:
RDayDense0 (Dense)              (None, 96)           119904      flatten_32[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

dropout_107 (Dropout)           (None, 96)           0           RDayDense0[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

RDayDense1 (Dense)              (None, 64)           6208        dropout_107[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

dropout_108 (Dropout)           (None, 64)           0           RDayDense1[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

RDayDense2 (Dense)              (None, 8)            520         dropout_108[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

dropout_109 (Dropout)           (None, 8)            0           RDayDense2[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

tshare (InputLayer)             [(None, 240, 1)]     0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

m_5 (InputLayer)                [(None, 3, 20, 1)]   0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

m_15 (InputLayer)               [(None, 3, 20, 1)]   0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

m_60 (InputLayer)               [(None, 3, 20, 1)]   0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

m_240 (InputLayer)              [(None, 3, 20, 1)]   0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

m1680 (InputLayer)              [(None, 3, 20, 1)]   0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

m4800 (InputLayer)              [(None, 3, 20, 1)]   0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

r15 (InputLayer)                [(None, 3, 15, 1)]   0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

r60 (InputLayer)                [(None, 3, 15, 1)]   0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

r1680 (InputLayer)              [(None, 3, 15, 1)]   0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

r4800 (InputLayer)              [(None, 3, 15, 1)]   0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

indik15 (InputLayer)            [(None, 3, 13, 1)]   0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

indik60 (InputLayer)            [(None, 3, 13, 1)]   0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

indik240 (InputLayer)           [(None, 3, 13, 1)]   0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

indik1680 (InputLayer)          [(None, 3, 13, 1)]   0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

indik4800 (InputLayer)          [(None, 3, 13, 1)]   0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

ks (InputLayer)                 [(None, 60)]         0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

dayinfo (InputLayer)            [(None, 4)]          0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

maday (InputLayer)              [(None, 5, 10, 1)]   0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

polyline (InputLayer)           [(None, 20, 4, 1)]   0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;OutputDense (Dense)             (None, 1)            9           dropout_109[0][0]&lt;/denchmark-h&gt;

Total params: 136,913
Trainable params: 136,913
Non-trainable params: 0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

There are about 20 noused InputLayers listed, which weren't listed in model.summary() in the past versions. If delete these InputLayers, the model could be saved.
This is a big model. I would train different input data in different steps. To update the code may got trouble. How can I save the model without deleting the more InputLayers?
		</comment>
		<comment id='2' author='saintthor' date='2019-07-08T21:59:32Z'>
		&lt;denchmark-link:https://github.com/saintthor&gt;@saintthor&lt;/denchmark-link&gt;
 Can you share the colab gist. Thanks!
		</comment>
		<comment id='3' author='saintthor' date='2019-07-09T05:35:34Z'>
		
@saintthor Can you share the colab gist. Thanks!

sorry. I have alterd the model and deleted the no-used input layers.
		</comment>
		<comment id='4' author='saintthor' date='2019-07-09T16:19:18Z'>
		Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!
		</comment>
	</comments>
</bug>