<bug id='14037' author='Volker-Weissmann' open_date='2020-05-08T16:31:13Z' closed_time='2020-07-17T17:03:50Z'>
	<summary>Memory Leak when using keras.layers.Lambda and tf.map_fn</summary>
	<description>
System information

Have I written custom code (as opposed to using example directory):  Yes, see below
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Arch Linux
TensorFlow backend (yes / no):  yes
TensorFlow version:  2.2.0-rc3
Keras version:   2.3.1
Python version:  3.8.2
CUDA/cuDNN version:   V10.2.89
GPU model and memory:  GeForce GTX 1060 6GB

The following program produces a memory leak.
&lt;denchmark-code&gt;#!/usr/bin/env python
import keras
import numpy as np
import tensorflow as tf
import gc
import os

@tf.function
def extractVector(inp):
	return tf.stack([inp[0]]*40, axis=0)

x_train = np.random.rand(3200, 40)
y_train = np.random.rand(3200, 40, 40)
embedding_vecs = np.random.rand(4592, 300)

input_layer = keras.layers.Input(shape=(40,))
embedding_layer = keras.layers.Embedding(4592, 300, weights=[embedding_vecs], input_length=40, trainable=False)(input_layer)
lstm_layer = keras.layers.LSTM(40, return_sequences=True)(embedding_layer)
lambda_layer = keras.layers.Lambda(lambda inp: tf.map_fn(extractVector, inp))(lstm_layer)
output_layer = lambda_layer

model = keras.models.Model(inputs=input_layer, outputs=output_layer)
model.compile(loss="categorical_crossentropy", optimizer="adam")

class GarbageCollectorCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        gc.collect()

class PrintRamCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        os.system("pmap -x "+str(os.getpid())+" | tail -n 1")

model.fit(x_train, y_train, batch_size=128, epochs=100, callbacks=[GarbageCollectorCallback(), PrintRamCallback()], verbose=0)

&lt;/denchmark-code&gt;

The output of the program above is:

2020-05-08 18:09:00.187454: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
2020-05-08 18:09:00.218519: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3300655000 Hz
2020-05-08 18:09:00.218983: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5594d82c4200 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-08 18:09:00.219023: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-05-08 18:09:00.220331: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Using TensorFlow backend.
WARNING:tensorflow:AutoGraph could not transform &lt;function extractVector at 0x7f74044ca310&gt; and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, export AUTOGRAPH_VERBOSITY=10) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
total kB         3554580  613100  429920
total kB         3685652  753188  570008
total kB         3816724  893884  710704
total kB         3947796 1034252  851072
total kB         4078868 1174528  991348
total kB         4209940 1314880 1131700
total kB         4341012 1455372 1272192
total kB         4472084 1595540 1412360
total kB         4603156 1741396 1558216
total kB         4734228 1881492 1698312
total kB         4865300 2021600 1838420
total kB         4996372 2163068 1979888
total kB         5192980 2303140 2119960
total kB         5389588 2443292 2260112
total kB         5520660 2583584 2400404
total kB         5651732 2723784 2540604
total kB         5782804 2863836 2680656
total kB         5913876 3004144 2820964
total kB         6044948 3144256 2961076
total kB         6176020 3284412 3101232
total kB         6307092 3424500 3241320
total kB         6438164 3564720 3381540
total kB         6569236 3705064 3521884
total kB         6700308 3845276 3662096
total kB         6831380 3985636 3802456
total kB         7027988 4125896 3942716
total kB         7159060 4266096 4082916
total kB         7290132 4406160 4222980
total kB         7421204 4546380 4363200
total kB         7552276 4686528 4503348
total kB         7683348 4826604 4643424
total kB         7879956 4966828 4783648
total kB         8011028 5106936 4923756
total kB         8142100 5247296 5064116
total kB         8273172 5383988 5204424
total kB         8404244 5492128 5344856
total kB         8535316 5494272 5485284
total kB         8731924 5633596 5625676
total kB         8862996 5772616 5765768
total kB         8994068 5912720 5905752
total kB         9125140 6052912 6045960
total kB         9256212 6192840 6186032
total kB         9387284 6236800 6226492
total kB         9518356 6332196 6321296
total kB         9649428 6379904 6369476
total kB         9780500 6442764 6432232
total kB         9977108 6490412 6480072
total kB         10108180 6518504 6508008

You can see that it uses 430 MB to complete the first epoch, but 6500 MB for the 48'th Epoch .
	</description>
	<comments>
		<comment id='1' author='Volker-Weissmann' date='2020-07-13T20:10:44Z'>
		Hello &lt;denchmark-link:https://github.com/Volker-Weissmann&gt;@Volker-Weissmann&lt;/denchmark-link&gt;
 , i tried to reproduce with the same code but it is not showing any sign of memory leak.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): POP_OS
TensorFlow backend (yes / no): yes
TensorFlow version: 2.2.0
Keras version: 2.4.3
Python version: 3.7.6
CUDA/cuDNN version: V10.2
GPU model and memory: GeForce GTX 1080 8GB
Try to update and run, make sure you have installed keras-gpu.
&lt;denchmark-link:https://user-images.githubusercontent.com/26149730/87348730-d47b1080-c572-11ea-964f-ad8937b657fd.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='Volker-Weissmann' date='2020-07-17T17:03:48Z'>
		It did not work 4 days ago (even after updating), but it works now (without a memory leak).
Thank you for fixing it.
		</comment>
	</comments>
</bug>