<bug id='12877' author='smgutstein' open_date='2019-05-27T03:07:40Z' closed_time='2019-07-08T21:45:49Z'>
	<summary>keras.examples.cifar10_cnn.py  ignores batch_size in absence of data_augmentation</summary>
	<description>
System information

Have I written custom code (as opposed to using example directory):  No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 18/04
TensorFlow backend (yes / no):  yes
TensorFlow version:  1.13.1
Keras version:  2.2.4
Python version:  2.7.16
CUDA/cuDNN version:  10.1/7.6
GPU model and memory:  1080M / 8GB

Describe the current behavior
When cifar10_cnn.py is run without data_augmentation, it seems to ignore the fact that batch_size has been set to 32.
The training output appears as follows:
&lt;denchmark-code&gt;Epoch 1/100
50000/50000 [==============================] - 9s 172us/step - loss: 1.8029 - acc: 0.3383 - val_loss: 1.4823 - val_acc: 0.4573
Epoch 2/100
50000/50000 [==============================] - 7s 135us/step - loss: 1.4775 - acc: 0.4650 - val_loss: 1.3312 - val_acc: 0.5218

&lt;/denchmark-code&gt;

Describe the expected behavior
It should appear this way (since 50000/32 = 1562)
&lt;denchmark-code&gt;Epoch 1/100
1562/1562 [==============================] - 16s 10ms/step - loss: 1.8887 - acc: 0.3054 - val_loss: 1.5615 - val_acc: 0.4398
Epoch 2/100
1562/1562 [==============================] - 14s 9ms/step - loss: 1.5818 - acc: 0.4231 - val_loss: 1.3624 - val_acc: 0.5110


&lt;/denchmark-code&gt;

Code to reproduce the issue
Just run cifar10_cnn.py, after setting data_augmentation to False
Other info / logs
I think the problem lies in the way Progbar and ProgbarLogger are called from training_arrays.py in the fit_loop method, where around line 110, we see:
&lt;denchmark-code&gt;    if verbose:
        if steps_per_epoch is not None:
            count_mode = 'steps'
        else:
            count_mode = 'samples'
        _callbacks.append(
            cbks.ProgbarLogger(
                count_mode,
                stateful_metrics=model.stateful_metric_names))

&lt;/denchmark-code&gt;

Where the assumption is that unless steps_per_batch is specified, we are counting samples, rather than steps (i.e. batches). This assumption is transferred to the __init__ method of ProgbarLogger, which then transfers it to Progbar. My assumption is that if the creation of Progbar is prone to this error, then the actual training might be too, but I haven't verified that.
I'd assume that the fix would be to find all the places where steps_per_epoch is used to see if mini-batches are being used in training. For now, I think the best quick/dirty fix is to always specify steps_per_epoch in a manner consistent with batch_size
	</description>
	<comments>
		<comment id='1' author='smgutstein' date='2019-06-26T21:35:43Z'>
		&lt;denchmark-link:https://github.com/smgutstein&gt;@smgutstein&lt;/denchmark-link&gt;
 Could you provide a standalone code to reproduce the issue? Thanks!
		</comment>
		<comment id='2' author='smgutstein' date='2019-07-08T21:45:49Z'>
		Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
	</comments>
</bug>