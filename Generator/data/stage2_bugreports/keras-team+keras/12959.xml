<bug id='12959' author='jkamalu' open_date='2019-06-14T12:50:12Z' closed_time='2019-06-24T22:17:06Z'>
	<summary>tf.keras.Model.evaluate skews score in custom tf.keras.callbacks.Callback</summary>
	<description>
System information

Have I written custom code (as opposed to using example directory): YES
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04.2 LTS
TensorFlow backend (yes / no):  YES
TensorFlow version:  2.0.0-dev20190601
Keras version:  2.2.4-tf
Python version:  3.6
CUDA/cuDNN version:  7.3.1
GPU model and memory:  4 GeForce GTX 1080 w/8119MiB

Describe the current behavior
I am using the TensorFlow Dataset api with the Keras Model.fit function. When x is a Dataset (and therefor no y is provided), an error is thrown if we provide a batch_size argument. However, a batch_size is required is we want to use validation_data (I would already classify this as an error).
So, I wrote my own custom callback to perform a call to Model.evaluate with the validation dataset I had already prepared after every n-th batch and after every single epoch.
During training, at, say, step 1000 (and before the first epoch), the accuracy reported by Tensorboard (I use the Tensorboard Keras callback for train loss and accuracy), as well as the accuracy reported by the metrics display next to the progress bar, might be at ~19%.
However, after the CustomCallback runs it's function on_batch_end(self, batch, logs=None) and a call to self.model.evaluate is made, the training accuracy displayed by the progress bar and by Tensorboard picks up where the validation accuracy left off. That is, if val accuracy happened to be ~28% (higher due to no Dropout), then the training metric would also read 28% from then on.
Describe the expected behavior
Calling the model's evaluate function from within a custom callback should not change what is displayed for train accuracy. Also, built-in Dataset validation would be nice.
Code to reproduce the issue
This is the custom callback. Instantiating this callback with a subdirectory for valdiation events and a Dataset instance, and then including this callback in a call to model.fit should reproduce the error. I invite you to provide the data. I am abandoning the Keras Model.fit pipeline for now due to time constraints but I thought I would still log the error.
For this bug to be noticeable, the training accuracy must be noticeably lower due to dropout than the val acc. (the color is blue but this is train accuracy, I promise. The jump is at step 1000, at which point Model.evaluate was called).
&lt;denchmark-link:https://user-images.githubusercontent.com/11124194/59510020-efcca580-8eb2-11e9-8a3f-0261d69e4fdd.png&gt;&lt;/denchmark-link&gt;

`
class Validation(tf.keras.callbacks.Callback):
&lt;denchmark-code&gt;def __init__(self, log_dir, dataset, *args, **kwargs):
    self.dataset = dataset
    self.writer = tf.summary.create_file_writer(f"{log_dir}/validation")

def on_train_begin(self, logs=None):
    self.history = {}
    self.step = 0

def on_batch_end(self, batch, logs=None):
    self.step += 1
    if self.step % Config.val_log_freq == 0:
        metrics = self.model.evaluate(self.dataset, verbose=0)
        for k, v in zip(self.model.metrics_names, metrics):
            with self.writer.as_default():
                tf.summary.scalar(f"batch_{k}", v, step=self.step)
        self.writer.flush()
    
def on_epoch_end(self, epoch, logs=None):
    metrics = self.model.evaluate(self.dataset, verbose=0)
    for k, v in zip(self.model.metrics_names, metrics):
        self.history.setdefault(k, []).append(v)
        with self.writer.as_default():
            tf.summary.scalar(f"epoch_{k}", v, step=self.step)
    self.writer.flush()
&lt;/denchmark-code&gt;

validation_callback = Validation(log_dir, valid_dataset)`
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='jkamalu' date='2019-06-18T18:40:38Z'>
		&lt;denchmark-link:https://github.com/jkamalu&gt;@jkamalu&lt;/denchmark-link&gt;
 could you provide a simple standalone code to reproduce the issue? You could take mnist/fashion mnist data. Thanks!
		</comment>
		<comment id='2' author='jkamalu' date='2019-06-24T22:17:06Z'>
		Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
	</comments>
</bug>