<bug id='245' author='ZAJ0711' open_date='2020-12-07T07:07:57Z' closed_time='2020-12-13T11:23:25Z'>
	<summary>fit report</summary>
	<description>
when i just use the model in my program, the bug is shown
File "D:\Program Files\anaconda3\lib\site-packages\torch\nn\functional.py", line 1852, in embedding
return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
may be the cat_dims, or not ? the code is:
clf = TabNetClassifier(cat_idxs=cat_idxs,
cat_dims=cat_dims,
cat_emb_dim=1,
optimizer_fn=torch.optim.Adam,
optimizer_params=dict(lr=2e-2),
scheduler_params={"step_size": 50,  # how to use learning rate scheduler
"gamma": 0.9},
scheduler_fn=torch.optim.lr_scheduler.StepLR,
mask_type='entmax'  # "sparsemax"
)
clf.fit(
X_train=X_train, y_train=y_train,
eval_set=[(X_train, y_train), (X_valid, y_valid)],
eval_name=['train', 'valid'],
eval_metric=['auc'],
max_epochs=1, patience=20,
batch_size=1024, virtual_batch_size=128,
num_workers=0,
weights=1,
drop_last=False
)
	</description>
	<comments>
		<comment id='1' author='ZAJ0711' date='2020-12-07T07:27:41Z'>
		eval_metric=['auc'],
can i see the f1 score
		</comment>
		<comment id='2' author='ZAJ0711' date='2020-12-07T08:02:19Z'>
		Hi &lt;denchmark-link:https://github.com/ZAJ0711&gt;@ZAJ0711&lt;/denchmark-link&gt;
 ,
For your , the reason it happens is because your dataset has more modalities than the number you passed in  for some feature. I think the most probable reason is that there is a modality of some feature that is present in your validation or test sets, but not on your training. Could you check if that is the case?
as for the F1-score metric, there is an example on our README that you can use as inspiration:
from pytorch_tabnet.metrics import Metric
from sklearn.metrics import roc_auc_score

class Gini(Metric):
    def __init__(self):
        self._name = "gini"
        self._maximize = True

    def __call__(self, y_true, y_score):
        auc = roc_auc_score(y_true, y_score[:, 1])
        return max(2*auc - 1, 0.)

clf = TabNetClassifier()
clf.fit(
  X_train, Y_train,
  eval_set=[(X_valid, y_valid)],
  eval_metric=[Gini]
)
		</comment>
		<comment id='3' author='ZAJ0711' date='2020-12-07T08:08:59Z'>
		thanks for your reply
cat_dims is Required? When I don't give this parameter it works well and I don't find the Explanation in the document
Does this parameter： cat_dims  cause a conflict？
		</comment>
		<comment id='4' author='ZAJ0711' date='2020-12-07T08:50:53Z'>
		cat_dims is missing in the README, we need to add it.
It is mandatory if you wish to perform embeddings, if you don't specify it the model will continue as if all variables are numerical (that's probably the reason you don't have the error).
We probably need to make the README clearer and also maybe give a user warning if all the necessary parameters are not given.
In order to use embeddings it is mandatory to give two things:

cat_idxs : a list of integers that match categorical columns indices
cat_dims : a list of integers with same size as cat_idxs which represents the number of different modalities per categorical feature.

Moreover you'll have to LabelEncode your categorical features, so that they have integer values ranging from 0 to cat_dim -1. If any value is bigger than cat_dim -1 then you'll get IndexError: index out of range.
One last optional parameter is cat_emb_dim which  is 1 by default. It can either be an integer and this will mean that all categorical features will be embedding with size cat_emb_dim or a list of integers matching cat_idxs where you can specify a different embedding dimension for each categorical feature.
		</comment>
		<comment id='5' author='ZAJ0711' date='2020-12-07T10:01:28Z'>
		My topic is a two classification problem. The ratio of the two categories of data is about 7:3, and the total amount of data is about 8000
The category features in the model are coded by label directly. Can we try other coding (such as target, one hot) ？Do discrete numerical data with numerical relationship (similar to the number of people) need additional normalization?
Thanks for your help
		</comment>
		<comment id='6' author='ZAJ0711' date='2020-12-07T11:58:54Z'>
		
discrete numerical : no normalization is needed
you can try target encoding or one hot encoding, but then you must treat those columns as numerical (don't try to create embeddings on those columns)
if your categorical features are already label encoded then you just need to give cat_dim=np.unique(my_cat_feat) for every categorical column

		</comment>
		<comment id='7' author='ZAJ0711' date='2020-12-07T12:27:49Z'>
		you may say that after I use target encoding the result is put into the Tabnet as numerical，but  the third tip show that  cat_dim is needed， cat_dim is for categorical not for numerical
may be that  after I use target encoding, cat_dim is needed，but cat_emb_dim is unneeded？
		</comment>
		<comment id='8' author='ZAJ0711' date='2020-12-07T12:58:41Z'>
		No sorry edited my post:

third point was only for label encoded features

If you do target encoding or one hot encoding then none of the three params cat_idxs cat_dims and cat_emb_dim is useful.
		</comment>
		<comment id='9' author='ZAJ0711' date='2020-12-13T11:23:25Z'>
		&lt;denchmark-link:https://github.com/ZAJ0711&gt;@ZAJ0711&lt;/denchmark-link&gt;
 I'm closing this, please feel free to reopen if it did not solve your problem.
		</comment>
	</comments>
</bug>