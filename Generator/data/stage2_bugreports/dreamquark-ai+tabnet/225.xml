<bug id='225' author='noklam' open_date='2020-11-02T09:40:27Z' closed_time='2020-11-14T04:29:18Z'>
	<summary>Window 10x slower</summary>
	<description>
I am experiencing roughly 10x slower if I run this on Window. It still print cuda: True but I suspect it is not really using the GPU since the speed is roughly the same as I run with CPU.
	</description>
	<comments>
		<comment id='1' author='noklam' date='2020-11-02T09:46:19Z'>
		Hi &lt;denchmark-link:https://github.com/noklam&gt;@noklam&lt;/denchmark-link&gt;
 , thank you for the issue.
I don't have a Windows machine I could use to test, do you &lt;denchmark-link:https://github.com/Optimox&gt;@Optimox&lt;/denchmark-link&gt;
 ?
		</comment>
		<comment id='2' author='noklam' date='2020-11-02T09:54:49Z'>
		&lt;denchmark-link:https://github.com/eduardocarvp&gt;@eduardocarvp&lt;/denchmark-link&gt;
 nope...
Honnestly the only reason this could be slower in Windows is pytorch vs Windows. I don't see anything we can do about it.
&lt;denchmark-link:https://github.com/noklam&gt;@noklam&lt;/denchmark-link&gt;
 are you comparing from the same machine? I don't see why Windows would make it slower to be honest.
		</comment>
		<comment id='3' author='noklam' date='2020-11-02T09:55:02Z'>
		On Linux
&lt;denchmark-link:https://user-images.githubusercontent.com/18221871/97854424-654d4600-1d34-11eb-9c20-88adec762942.png&gt;&lt;/denchmark-link&gt;

On Window
&lt;denchmark-link:https://user-images.githubusercontent.com/18221871/97854442-6b432700-1d34-11eb-96b0-288e512e6330.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='noklam' date='2020-11-02T10:00:04Z'>
		&lt;denchmark-link:https://github.com/Optimox&gt;@Optimox&lt;/denchmark-link&gt;

Yes I am running on the same machine with GTX 1080 Ti and more than enough RAM. I don't think Window would slow down that much normally. I clone this code from a Kaggle Kernel and it is using Pytorch 1.7.
Does the implementation of using some distributed training out of the box that may not be supported well on Window?
		</comment>
		<comment id='5' author='noklam' date='2020-11-02T10:22:30Z'>
		There is no distributed training, we're not doing anything special.
What happens if you set the device to 'cpu' in linux ? If you have the same training time as in Windows I guess you are not really using  your GPU on Windows.
Can't really help much on this...
		</comment>
		<comment id='6' author='noklam' date='2020-11-14T04:28:21Z'>
		I am closing this issue as I found the problem seems to be i have a dataloader with num_worker&gt;0 which has some adverse effect on Window rather than the model itself.
thanks for the help.
		</comment>
	</comments>
</bug>