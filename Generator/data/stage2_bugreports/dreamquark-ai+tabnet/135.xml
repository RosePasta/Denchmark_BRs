<bug id='135' author='amartin211' open_date='2020-06-08T15:26:11Z' closed_time='2020-06-08T17:16:41Z'>
	<summary>RuntimeError: CUDA error: device-side assert triggered</summary>
	<description>
Describe the bug
I get this CUDA error when trying to fit the classifier (with GPU).
I've also tried switching to CPU and got a different error =&gt; "RuntimeError: Invalid index in gather at /opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:657" where now the error seems to be related to an index tensor that has invalid indices and I'm not sure on how to solve this.
What is the current behavior?
This error happen when fitting a classifier with exactly the same parameters as in the "census_examples" notebook but on the different dataset.
If the current behavior is a bug, please provide the steps to reproduce.
Expected behavior
Screenshots
Other relevant information:
poetry version:
python version:
Operating System:
Additional tools:
Additional context
&lt;denchmark-h:h2&gt;Here is the details of the error when running fit with CPUs :&lt;/denchmark-h&gt;

RuntimeError                              Traceback (most recent call last)
 in 
7     batch_size=512, virtual_batch_size=128,
8     num_workers=0,
----&gt; 9     drop_last=False
10 )
/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/tab_model.py in fit(self, X_train, y_train, X_valid, y_valid, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last)
165                self.patience_counter &lt; self.patience):
166             starting_time = time.time()
--&gt; 167             fit_metrics = self.fit_epoch(train_dataloader, valid_dataloader)
168
169             # leaving it here, may be used for callbacks later
/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/tab_model.py in fit_epoch(self, train_dataloader, valid_dataloader)
222                 DataLoader with valid set
223         """
--&gt; 224         train_metrics = self.train_epoch(train_dataloader)
225         valid_metrics = self.predict_epoch(valid_dataloader)
226
/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/tab_model.py in train_epoch(self, train_loader)
487
488         for data, targets in train_loader:
--&gt; 489             batch_outs = self.train_batch(data, targets)
490             if self.output_dim == 2:
491                 y_preds.append(torch.nn.Softmax(dim=1)(batch_outs["y_preds"])[:, 1]
/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/tab_model.py in train_batch(self, data, targets)
530         self.optimizer.zero_grad()
531
--&gt; 532         output, M_loss = self.network(data)
533
534         loss = self.loss_fn(output, targets)
/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in call(self, *input, **kwargs)
530             result = self._slow_forward(*input, **kwargs)
531         else:
--&gt; 532             result = self.forward(*input, **kwargs)
533         for hook in self._forward_hooks.values():
534             hook_result = hook(self, input, result)
/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/tab_network.py in forward(self, x)
254     def forward(self, x):
255         x = self.embedder(x)
--&gt; 256         return self.tabnet(x)
257
258     def forward_masks(self, x):
/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in call(self, *input, **kwargs)
530             result = self._slow_forward(*input, **kwargs)
531         else:
--&gt; 532             result = self.forward(*input, **kwargs)
533         for hook in self._forward_hooks.values():
534             hook_result = hook(self, input, result)
/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/tab_network.py in forward(self, x)
130
131         for step in range(self.n_steps):
--&gt; 132             M = self.att_transformers[step](prior, att)
133             M_loss += torch.mean(torch.sum(torch.mul(M, torch.log(M+self.epsilon)),
134                                            dim=1))
/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in call(self, *input, **kwargs)
530             result = self._slow_forward(*input, **kwargs)
531         else:
--&gt; 532             result = self.forward(*input, **kwargs)
533         for hook in self._forward_hooks.values():
534             hook_result = hook(self, input, result)
/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/tab_network.py in forward(self, priors, processed_feat)
290         x = self.bn(x)
291         x = torch.mul(x, priors)
--&gt; 292         x = self.sp_max(x)
293         return x
294
/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in call(self, *input, **kwargs)
530             result = self._slow_forward(*input, **kwargs)
531         else:
--&gt; 532             result = self.forward(*input, **kwargs)
533         for hook in self._forward_hooks.values():
534             hook_result = hook(self, input, result)
/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/sparsemax.py in forward(self, input)
89
90     def forward(self, input):
---&gt; 91         return sparsemax(input, self.dim)
92
93
/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/sparsemax.py in forward(ctx, input, dim)
41         max_val, _ = input.max(dim=dim, keepdim=True)
42         input -= max_val  # same numerical stability trick as for softmax
---&gt; 43         tau, supp_size = SparsemaxFunction._threshold_and_support(input, dim=dim)
44         output = torch.clamp(input - tau, min=0)
45         ctx.save_for_backward(supp_size, output)
/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/sparsemax.py in _threshold_and_support(input, dim)
74
75         support_size = support.sum(dim=dim).unsqueeze(dim)
---&gt; 76         tau = input_cumsum.gather(dim, support_size - 1)
77         tau /= support_size.to(input.dtype)
78         return tau, support_size
RuntimeError: Invalid index in gather at /opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:657
	</description>
	<comments>
		<comment id='1' author='amartin211' date='2020-06-08T15:31:43Z'>
		Hi &lt;denchmark-link:https://github.com/amartin211&gt;@amartin211&lt;/denchmark-link&gt;
 ,
These bugs can be a bit tricky to track since they are masked behind CUDA error. I had it happen to me before and very often the cause is a NaN value somewhere on the dataset that cannot be interpreted by pytorch.
Could you check if this might be the case?
		</comment>
		<comment id='2' author='amartin211' date='2020-06-08T17:16:37Z'>
		Indeed that was the issue. I'm working with a tricky dataset and had few 'inf' values in it. I've removed them and it seems to be working properly now. thanks for the quick reply.
		</comment>
		<comment id='3' author='amartin211' date='2020-12-20T16:26:51Z'>
		I'm getting the same error, data doesn't contain any nans or infs. (And I'm using .values to extract data from a dataframe)
		</comment>
		<comment id='4' author='amartin211' date='2020-12-20T16:28:37Z'>
		C:/w/1/s/tmp_conda_3.7_100118/conda/conda-bld/pytorch_1579082551706/work/aten/src/THC/THCTensorIndex.cu:361: block: [2,0,0], thread: [87,0,0] Assertion srcIndex &lt; srcSelectDimSize failed.
C:/w/1/s/tmp_conda_3.7_100118/conda/conda-bld/pytorch_1579082551706/work/aten/src/THC/THCTensorIndex.cu:361: block: [2,0,0], thread: [88,0,0] Assertion srcIndex &lt; srcSelectDimSize failed.
C:/w/1/s/tmp_conda_3.7_100118/conda/conda-bld/pytorch_1579082551706/work/aten/src/THC/THCTensorIndex.cu:361: block: [2,0,0], thread: [89,0,0] Assertion srcIndex &lt; srcSelectDimSize failed.
C:/w/1/s/tmp_conda_3.7_100118/conda/conda-bld/pytorch_1579082551706/work/aten/src/THC/THCTensorIndex.cu:361: block: [2,0,0], thread: [92,0,0] Assertion srcIndex &lt; srcSelectDimSize failed.
C:/w/1/s/tmp_conda_3.7_100118/conda/conda-bld/pytorch_1579082551706/work/aten/src/THC/THCTensorIndex.cu:361: block: [2,0,0], thread: [94,0,0] Assertion srcIndex &lt; srcSelectDimSize failed.
C:/w/1/s/tmp_conda_3.7_100118/conda/conda-bld/pytorch_1579082551706/work/aten/src/THC/THCTensorIndex.cu:361: block: [2,0,0], thread: [95,0,0] Assertion srcIndex &lt; srcSelectDimSize failed.
		</comment>
	</comments>
</bug>