<bug id='207' author='athewsey' open_date='2020-10-15T13:36:55Z' closed_time='2020-10-15T20:51:00Z'>
	<summary>prediction functions can't consume already-on-GPU tensors</summary>
	<description>
Describe the bug
Pinning DataLoader memory (as introduced by &lt;denchmark-link:https://github.com/dreamquark-ai/tabnet/commit/5a013596da597263aaf1b9f385732fc2442dda96&gt;5a01359&lt;/denchmark-link&gt;
) does not seem to be compatible with ingesting an already-on-GPU  into the DataLoader.
Some inference/serving frameworks may already pin the input data batch to GPU before calling the the user code where TabModel is invoked - which would require costly unloading to CPU &amp; reloading by pytorch-tabnet.
What is the current behavior?
Both training and prediction functions creating DataLoaders set pin_memory=True, which fails when the input data is already a CUDA/GPU-resident torch.Tensor - with error like:
&lt;denchmark-code&gt;cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
&lt;/denchmark-code&gt;

If the current behavior is a bug, please provide the steps to reproduce.

Train/load some TabModel variant (to GPU device)
Load an input matrix X as a torch.Tensor and send to GPU device.
Call a prediction function e.g. model.predict(X)

Expected behavior
If inference data has already been loaded to the correct device as a torch.Tensor when TabNet prediction function is called, it should be compatible without needing to unload/convert to numpy.
My gut feeling is that there's no need to offer this same flexibility on training-time functions, because I can't imagine a use case where somebody has pushed their whole X_train / y_train into GPU memory before kicking off a .fit()... But it's not really a strong opinion - would be open to the idea if it seems useful?
Screenshots
N/A

poetry version:  N/A
python version: 3.6
Operating System: Linux (Ubuntu-based, per the &lt;denchmark-link:https://github.com/aws/deep-learning-containers/blob/master/available_images.md#prior-versions&gt;PyTorch v1.4 DLC doc&lt;/denchmark-link&gt;
)
Additional tools: Amazon SageMaker's pre-built PyTorch v1.4 Framework Container
Additional context
I appreciate this is maybe a rare case because it's relevant only when using pytorch-tabnet together with some kind of pre-built inference framework that handles deserialization of input data for you and also goes so far as to push that data to GPU device...
However, in general would argue that such frameworks are a good thing which we should endeavour to support: Always better to leverage some existing/optimized engineering if possible, rather than getting data scientists involved with implementing servers! ðŸ˜‚
	</description>
	<comments>
	</comments>
</bug>