<bug id='212' author='tmontana' open_date='2020-10-16T12:13:12Z' closed_time='2020-10-16T14:17:03Z'>
	<summary>different score results (predict_proba) before and after save/load model</summary>
	<description>
Describe the bug
I would expect scoring by a trained model to be the same after saving and reloading the model.  The difference is slight but given I am averaging a number of models it ends up being significant for my use case.
What is the current behavior?
see above
If the current behavior is a bug, please provide the steps to reproduce.
preds=one_clf.predict_proba(X_test)
print(preds)
array([[0.05709733, 0.19917637, 0.290806  , 0.31649232, 0.13642798],
[0.28743955, 0.18413214, 0.15475626, 0.16400008, 0.20967196],
[0.28111738, 0.16206878, 0.13325973, 0.15758775, 0.2659663 ],
...,
[0.19621055, 0.24985024, 0.22525077, 0.20751362, 0.12117489],
[0.11154774, 0.27268788, 0.2859154 , 0.2358539 , 0.09399504],
[0.26680788, 0.17728016, 0.14768049, 0.16171692, 0.24651453]],
dtype=float32)
saved_name=one_clf.save_model('1_test_model')
Successfully saved model at 1_test_model.zip
clf_2=TabNetClassifier()
clf_2.load_model(saved_name)
preds_2=clf_2.predict_proba(X_test)
print(preds_2)
array([[0.05709732, 0.19917633, 0.290806  , 0.31649235, 0.13642798],
[0.28743964, 0.18413213, 0.15475623, 0.16400005, 0.20967199],
[0.28111738, 0.16206878, 0.13325973, 0.15758775, 0.26596636],
...,
[0.19621053, 0.24985024, 0.22525078, 0.20751363, 0.12117489],
[0.11154776, 0.27268788, 0.28591537, 0.23585393, 0.09399506],
[0.26680785, 0.17728017, 0.1476805 , 0.16171695, 0.24651451]],
dtype=float32)
Expected behavior
I would expect to obtain the same numbers exactly
Screenshots
Other relevant information:
poetry version:
python version:
Operating System:
Additional tools:
Additional context
version 2.0.1
Thank you
	</description>
	<comments>
		<comment id='1' author='tmontana' date='2020-10-16T12:58:43Z'>
		It seems it's machine precision order no? something like 1e-8 difference?
		</comment>
		<comment id='2' author='tmontana' date='2020-10-16T12:59:34Z'>
		Hey &lt;denchmark-link:https://github.com/tmontana&gt;@tmontana&lt;/denchmark-link&gt;
 .
The differences I'm seeing are probably related to float precision on your machine. Are you saving/loading on different environments?
		</comment>
		<comment id='3' author='tmontana' date='2020-10-16T13:00:50Z'>
		&lt;denchmark-link:https://github.com/eduardocarvp&gt;@eduardocarvp&lt;/denchmark-link&gt;
 no it's the same environment. I literally save and reload just after training.
		</comment>
		<comment id='4' author='tmontana' date='2020-10-16T13:02:02Z'>
		
It seems it's machine precision order no? something like 1e-8 difference?

Yes indeed it's a small difference.  I will run more tests to see if the difference is of that level for the whole test set.
Thanks
		</comment>
		<comment id='5' author='tmontana' date='2020-10-16T13:06:21Z'>
		My issue was that feeding the training set to the trained model for scoring should obviously result in the same predictions whether or not the model was just trained or reloaded from disk.  The difference triggered some errors on tests with zero tolerance.
My workaround is to save and reload the models before any scoring whatsoever.  With that I should get the same result feeding the train set for scoring the first time and thereafter when running tests so all good.
Happy to close at this point.  Thanks,
		</comment>
		<comment id='6' author='tmontana' date='2020-10-16T13:14:24Z'>
		You should probably make tests with some tolerance up to some precision, I think this is pytorch related more than tabnet related.
		</comment>
	</comments>
</bug>