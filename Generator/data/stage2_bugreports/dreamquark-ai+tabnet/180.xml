<bug id='180' author='carter840826' open_date='2020-09-08T09:13:52Z' closed_time='2020-09-24T09:05:48Z'>
	<summary>Final explain_matrix doesn't seems right.</summary>
	<description>
From the example notebook, I tried plot final explain_matrix.
And here is my question.
&lt;denchmark-link:https://user-images.githubusercontent.com/57344651/92456945-27311b80-f1f6-11ea-9007-a3eed90ee79a.png&gt;&lt;/denchmark-link&gt;

As fig showing above, the sum of mask value form single prediction isn't 1.
Just like:
&lt;denchmark-link:https://user-images.githubusercontent.com/57344651/92457166-7a0ad300-f1f6-11ea-9505-f8c21ace4e81.png&gt;&lt;/denchmark-link&gt;

,while some didn't have this problem:
&lt;denchmark-link:https://user-images.githubusercontent.com/57344651/92457288-9eff4600-f1f6-11ea-9532-85fd88538a6e.png&gt;&lt;/denchmark-link&gt;

Thanks for helping!
	</description>
	<comments>
		<comment id='1' author='carter840826' date='2020-09-08T09:18:51Z'>
		Hi &lt;denchmark-link:https://github.com/carter840826&gt;@carter840826&lt;/denchmark-link&gt;
,
Thanks for reporting this.
Is there a way to reproduce this in order to investigate more?
Could you share a minimal reproducible code showing this? Or at least give more information on your experiment?
Thanks!
		</comment>
		<comment id='2' author='carter840826' date='2020-09-08T09:28:07Z'>
		Hi,
What I did just was the example you provided.
&lt;denchmark-link:https://github.com/dreamquark-ai/tabnet/blob/develop/regression_example.ipynb&gt;https://github.com/dreamquark-ai/tabnet/blob/develop/regression_example.ipynb&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/57344651/92458698-55aff600-f1f8-11ea-8d9c-e6e7fec30601.png&gt;&lt;/denchmark-link&gt;

As you can see, the sum of mask value from single prediction isn't 1.
Instead of what paper shows:
&lt;denchmark-link:https://user-images.githubusercontent.com/57344651/92458930-9f98dc00-f1f8-11ea-8d05-0cbd203b8792.png&gt;&lt;/denchmark-link&gt;

Thanks for replying!
		</comment>
		<comment id='3' author='carter840826' date='2020-09-08T10:20:49Z'>
		Yes you are right the implementation differs from the original paper, we are not performing the normalization. As you can see from your examples the rows are not summing to 1 (even the 'working' one)
As a quick work around you can do it very easily by dividing by the sum of each row and you'll get the same results as in the paper.
Why don't we do it? I think it's because I don't fully agree with the original paper on this.
Let me explain:

I see a flaw in the definition of the explanation matrix. The fact of multiplying the mask with eta the sum of the n_d relu before final mapping in order to give feature importance can create a very weird behavior : if all the ReLUs are saturated and equal to 0 you're model will output 0. It's ok to output 0 right (as long as you expect 0 as your output)? but then your explanation matrix will give 0 importance to all features, disregarding the masks that were used in order to saturate the ReLus. I agree it's a degenerated case but sometimes you'll get all feature importances equal to 0 (and then you'll get a ZeroDivisionError). This is the reason why the explain method outputs also the masks, so that people that want to take a closer look can do it easily.
With that said, we are performing the normalization for feature importance, so the error would pop out anyway in the current implementation, so it would make sense to normalize things as in the paper. (Edit: but it's very unlikely as you'll need all your rows to be saturated)
According to the paper, we don't propagate the importance through the final mapping, so the given importance is not really a sensitivity importance. It's really hard to define what an explanation is, but I would also trust the normalized sum of the masks without multiplying with the sum of ReLUs.

Short answer:

you are right, we are missing the normalization (and should probably add it)
if you simply normalize your output row by row you'll get the paper's results
there's still open debate on research on what would be the best explanation of the model's output

		</comment>
		<comment id='4' author='carter840826' date='2020-09-09T01:05:01Z'>
		I see.
Thanks a lot for explanation!!
		</comment>
	</comments>
</bug>