<bug id='204' author='athewsey' open_date='2020-10-15T09:50:21Z' closed_time='2020-10-15T14:49:02Z'>
	<summary>load_model map_location not replicated to Py3.6 fallback branch</summary>
	<description>
Describe the bug
The recently-added  pass-through to  (from &lt;denchmark-link:https://github.com/dreamquark-ai/tabnet/commit/c2b560e72bc01e34e8dba7578f239e37bbd6782c&gt;c2b560e&lt;/denchmark-link&gt;
) was not replicated to the Python 3.6 fallback branch of the code - so in Py36 we are unable to load a model trained on GPU in a CPU-only environment.
What is the current behavior?
Cannot load_model() in a Python 3.6, CPU-only environment if the saved model was trained on GPU.
Error message from torch.load():
&lt;denchmark-code&gt;Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.
&lt;/denchmark-code&gt;

If the current behavior is a bug, please provide the steps to reproduce.

Train a model on CUDA/GPU device
save_model()
Copy the artifact to a CPU-only environment (no CUDA)
Load the model withload_model()

Expected behavior
The same parameters passed to torch.load() in the standard branch as in the Python 3.6 exception branch of the code: Enabling the library to successfully load the model with mapping to CPU.
Screenshots
N/A

poetry version:  N/A
python version: 3.6
Operating System: Linux (Ubuntu-based, per the &lt;denchmark-link:https://github.com/aws/deep-learning-containers/blob/master/available_images.md#prior-versions&gt;PyTorch v1.4 DLC doc&lt;/denchmark-link&gt;
)
Additional tools: Amazon SageMaker, PyTorch v1.4
Additional context
N/A
	</description>
	<comments>
		<comment id='1' author='athewsey' date='2020-10-15T09:56:37Z'>
		Thank you, I'll merge your fix as soon as the CI finishes!
		</comment>
		<comment id='2' author='athewsey' date='2020-10-15T13:56:09Z'>
		I wonder if it would be feasible or desirable to modify CircleCI config so that some tests run on different Python versions, as a lightweight way to try and protect against regressions / drive robustness? Seems like it might be possible to define additional executor containers, but IDK if there are cost implications or anything for doing that...
		</comment>
		<comment id='3' author='athewsey' date='2020-10-15T14:29:29Z'>
		I think this would indeed be feasible, it would simply make the CI longer. But why not! &lt;denchmark-link:https://github.com/Hartorn&gt;@Hartorn&lt;/denchmark-link&gt;
 ?
Edit: but we would have probably missed this anyway, because training on GPU and infering on CPU is not in our checks anyway.
Looks a bit specific to me. We can discuss about this in an other issue if you want. I'm closing this one as you solved it.
		</comment>
	</comments>
</bug>