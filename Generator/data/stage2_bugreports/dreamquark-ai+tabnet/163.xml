<bug id='163' author='athewsey' open_date='2020-07-28T08:48:23Z' closed_time='2020-08-04T15:48:30Z'>
	<summary>(Py3.6) torch.load requires seekable file</summary>
	<description>
Describe the bug
In Python &lt; 3.7,  fails because  returns a  file-like object, which  will not accept. (See &lt;denchmark-link:https://stackoverflow.com/q/33742544&gt;this StackOverflow question&lt;/denchmark-link&gt;
)
What is the current behavior?
Load model fails with:
&lt;denchmark-code&gt; File "/opt/conda/lib/python3.6/site-packages/pytorch_tabnet/tab_model.py", line 308, in load_model
    saved_state_dict = torch.load(f)
  File "/opt/conda/lib/python3.6/site-packages/torch/serialization.py", line 525, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/opt/conda/lib/python3.6/site-packages/torch/serialization.py", line 217, in _open_file_like
    return _open_buffer_reader(name_or_buffer)
  File "/opt/conda/lib/python3.6/site-packages/torch/serialization.py", line 202, in __init__
    _check_seekable(buffer)
  File "/opt/conda/lib/python3.6/site-packages/torch/serialization.py", line 292, in _check_seekable
    raise_err_msg(["seek", "tell"], e)
  File "/opt/conda/lib/python3.6/site-packages/torch/serialization.py", line 285, in raise_err_msg
    raise type(e)(msg)
io.UnsupportedOperation: seek. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.
&lt;/denchmark-code&gt;

If the current behavior is a bug, please provide the steps to reproduce.
From discussion on the above linked SO post I believe load_model() will always fail on any Python &lt;3.7 environment - but my specific configuration is provided below.
Expected behavior
Model saves and loads without error (I haven't seen any specific exclusions in the doc around Python &amp; PyTorch version support?)
Screenshots

poetry version:  N/A
python version: 3.6
Operating System:  Linux (Ubuntu-based, per &lt;denchmark-link:https://github.com/aws/deep-learning-containers/blob/master/available_images.md#prior-versions&gt;the PyTorch v1.4 DLC doc&lt;/denchmark-link&gt;
)
Additional tools:  Amazon SageMaker, PyTorch v1.4
Additional context
I could upgrade to PyTorch 1.5 but can't support Python 3.7+ in my environment just yet, so would really appreciate if we can keep compatibility!
Perhaps lightest solution would by an explicit try/catch inside load_model to buffer the .pt file to BytesIO memory if and only if torch.load() fails with io.UnsupportedOperation?
	</description>
	<comments>
		<comment id='1' author='athewsey' date='2020-09-03T03:11:12Z'>
		Hi &lt;denchmark-link:https://github.com/eduardocarvp&gt;@eduardocarvp&lt;/denchmark-link&gt;
 just checking, what's the timeline for next release to include this fix? I have some py36 environments I'd like to revert to PyPI rather than develop fork!
		</comment>
	</comments>
</bug>