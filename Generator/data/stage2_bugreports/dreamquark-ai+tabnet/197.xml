<bug id='197' author='tmontana' open_date='2020-10-11T16:47:09Z' closed_time='2020-10-26T09:21:33Z'>
	<summary>Unable to score on CPU if model trained on GPU?</summary>
	<description>
Describe the bug
getting errors:
RuntimeError: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:47
when trying to run .predict_proba on a GPU trained model even though it was loaded for CPU.
all_clf=torch.load(filename,map_location=torch.device('cpu'))
What is the current behavior?
If the current behavior is a bug, please provide the steps to reproduce.
Expected behavior
is that the expected behavior or should I be able to score on CPU?
Screenshots
Other relevant information:
poetry version:
python version:
Operating System:
Additional tools:
Additional context
thanks
	</description>
	<comments>
		<comment id='1' author='tmontana' date='2020-10-11T17:52:18Z'>
		Hello,
Could you please provide more information? like the exact code you are running?
Are you using the library loading scheme?
&lt;denchmark-code&gt;
saving_path_name = "./tabnet_model_test_1"
saved_filepath = clf.save_model(saving_path_name)


loaded_clf = TabNetClassifier()
loaded_clf.load_model(saved_filepath)
&lt;/denchmark-code&gt;

If yes you might wanna try loaded_clf.device_name='cpu'
		</comment>
		<comment id='2' author='tmontana' date='2020-10-11T19:05:05Z'>
		I was using torch.save and torch.load and getting a number of errors on top of the CPU issue.  your recommendation worked for the errors and I will try it on CPU tomorrow - looking good so far.  Thanks a lot for the help.
		</comment>
		<comment id='3' author='tmontana' date='2020-10-12T06:28:57Z'>
		Hi.  Doesn't seem to work.  Here is what I'm doing:
on a machine with GPU:
saved_name=one_clf.save_model('tabnet_model')
on a machine with CPU:
loaded_clf = TabNetClassifier()
loaded_clf.load_model(saved_name)
I also tried saving with:
one_clf.device_name='cpu'
saved_name=one_clf.save_model('tabnet_model')
or:
loading with:
loaded_clf = TabNetClassifier()
loaded_clf.device_name='cpu'
loaded_clf.load_model(saved_name)
In all cases I get this error:
RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.
​
Any idea?  Many thanks,
		</comment>
		<comment id='4' author='tmontana' date='2020-10-12T12:10:08Z'>
		&lt;denchmark-link:https://github.com/tmontana&gt;@tmontana&lt;/denchmark-link&gt;
 thanks for the detailed report, I'll try to look into it, but &lt;denchmark-link:https://github.com/eduardocarvp&gt;@eduardocarvp&lt;/denchmark-link&gt;
 is on it. We are probably missing the  part in  method.
		</comment>
		<comment id='5' author='tmontana' date='2020-10-12T22:07:57Z'>
		Thanks &lt;denchmark-link:https://github.com/tmontana&gt;@tmontana&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/Optimox&gt;@Optimox&lt;/denchmark-link&gt;
. Indeed a  seems to work for me. I trained a model on GPU and then tested loading on an environment where  and it worked, no problems.
The PR is linked above.
		</comment>
		<comment id='6' author='tmontana' date='2020-10-13T07:28:48Z'>
		&lt;denchmark-link:https://github.com/tmontana&gt;@tmontana&lt;/denchmark-link&gt;
 the bug is fixed on develop but we made some breaking changes before, so it might not work to load your previously trained model after the fix.
On thing you can do however is loading your model manually by copying each line of the load_model method and run it after replacing self with clf.
Otherwise you can refrain from develop and then it should work.
We’ll try to make a release today, so everything will be available on master branch.
		</comment>
		<comment id='7' author='tmontana' date='2020-10-13T09:01:29Z'>
		Hi.  I have tested it already.  Indeed having a problem with existing models so think I will retrain them.  Thanks a lot for the help.
		</comment>
		<comment id='8' author='tmontana' date='2020-10-16T09:54:13Z'>
		Hi all.  Unfortunately the new version (2.0.1) has not solved the issue for me.  Here is what I'm doing:
on GPU machine:
saved_name=one_clf.save_model(test_model)
on CPU machine:
&lt;denchmark-code&gt;loaded_clf=TabNetClassifier()
loaded_clf.load_model(saved_name)
&lt;/denchmark-code&gt;

output:
&lt;denchmark-h:h2&gt;Device used : cpu
Device used : cuda&lt;/denchmark-h&gt;

RuntimeError                              Traceback (most recent call last)
 in 
/anaconda/envs/tabnet_3/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py in load_model(self, filepath)
328         self.init(**loaded_params)
329
--&gt; 330         self._set_network()
331         self.network.load_state_dict(saved_state_dict)
332         self.network.eval()
/anaconda/envs/tabnet_3/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py in _set_network(self)
456     def _set_network(self):
457         """Setup the network and explain matrix."""
--&gt; 458         self.network = tab_network.TabNet(
459             self.input_dim,
460             self.output_dim,
/anaconda/envs/tabnet_3/lib/python3.8/site-packages/pytorch_tabnet/tab_network.py in init(self, input_dim, output_dim, n_d, n_a, n_steps, gamma, cat_idxs, cat_dims, cat_emb_dim, n_independent, n_shared, epsilon, virtual_batch_size, momentum, device_name, mask_type)
270                 device_name = 'cpu'
271         self.device = torch.device(device_name)
--&gt; 272         self.to(self.device)
273
274     def forward(self, x):
/anaconda/envs/tabnet_3/lib/python3.8/site-packages/torch/nn/modules/module.py in to(self, *args, **kwargs)
605             return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
606
--&gt; 607         return self._apply(convert)
608
609     def register_backward_hook(
/anaconda/envs/tabnet_3/lib/python3.8/site-packages/torch/nn/modules/module.py in _apply(self, fn)
352     def _apply(self, fn):
353         for module in self.children():
--&gt; 354             module._apply(fn)
355
356         def compute_should_use_set_data(tensor, tensor_applied):
/anaconda/envs/tabnet_3/lib/python3.8/site-packages/torch/nn/modules/module.py in _apply(self, fn)
352     def _apply(self, fn):
353         for module in self.children():
--&gt; 354             module._apply(fn)
355
356         def compute_should_use_set_data(tensor, tensor_applied):
/anaconda/envs/tabnet_3/lib/python3.8/site-packages/torch/nn/modules/module.py in _apply(self, fn)
374                 # with torch.no_grad():
375                 with torch.no_grad():
--&gt; 376                     param_applied = fn(param)
377                 should_use_set_data = compute_should_use_set_data(param, param_applied)
378                 if should_use_set_data:
/anaconda/envs/tabnet_3/lib/python3.8/site-packages/torch/nn/modules/module.py in convert(t)
603             if convert_to_format is not None and t.dim() == 4:
604                 return t.to(device, dtype if t.is_floating_point() else None, non_blocking, memory_format=convert_to_format)
--&gt; 605             return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
606
607         return self._apply(convert)
/anaconda/envs/tabnet_3/lib/python3.8/site-packages/torch/cuda/init.py in _lazy_init()
188             raise AssertionError(
189                 "libcudart functions unavailable. It looks like you have a broken build?")
--&gt; 190         torch._C._cuda_init()
191         # Some of the queued calls may reentrantly call _lazy_init();
192         # we need to just return without initializing in that case.
RuntimeError: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:47
		</comment>
		<comment id='9' author='tmontana' date='2020-10-16T11:55:49Z'>
		Does this work?
&lt;denchmark-code&gt;loaded_clf=TabNetClassifier()
loaded_clf.load_model(saved_name)
loaded_clf.device_name = "cpu"
&lt;/denchmark-code&gt;

Also what does torch.cuda.is_available() returns in your CPU inference version?
		</comment>
		<comment id='10' author='tmontana' date='2020-10-16T11:57:39Z'>
		No because it fails at
loaded_clf.load_model(saved_name)
so never makes it to next line
		</comment>
		<comment id='11' author='tmontana' date='2020-10-16T12:50:26Z'>
		&lt;denchmark-link:https://github.com/eduardocarvp&gt;@eduardocarvp&lt;/denchmark-link&gt;
 was it working for you?
		</comment>
		<comment id='12' author='tmontana' date='2020-10-16T13:00:14Z'>
		It had worked for me. I'll test it again and post the results here.
		</comment>
		<comment id='13' author='tmontana' date='2020-10-17T06:59:45Z'>
		^ Just to add, I recently checked and v2.0.1 did successfully fix the issue for me, so must be something specific to this environment.
I save_model and load with:
&lt;denchmark-code&gt;model = TabNetClassifier()
model.load_model("path/to/tabnet.zip")
# (No need to set model.device_name)
model.predict_proba(input_data)
&lt;/denchmark-code&gt;

...and see a single Device used : cpu output, so seems like the issue is related to that second cuda print...
Maybe more info on environment versions e.g. Python, PyTorch, what OS, etc would help?
		</comment>
		<comment id='14' author='tmontana' date='2020-10-17T08:21:14Z'>
		yes indeed I'm not sure why it says Device used : cpu then Device used : cuda
My environment:
conda create --name tabnet python=3.8
conda activate tabnet
pip install git+&lt;denchmark-link:https://github.com/dreamquark-ai/tabnet.git&gt;https://github.com/dreamquark-ai/tabnet.git&lt;/denchmark-link&gt;

pip install nbformat pandas
conda install ipykernel
python -m ipykernel install --user --name tabnet
Python 3.8, PyTorch is installed directly by tabnet install, OS is Ubuntu 16
Thanks
		</comment>
		<comment id='15' author='tmontana' date='2020-10-19T07:30:33Z'>
		Have you tried with python 3.7?
		</comment>
		<comment id='16' author='tmontana' date='2020-10-19T07:42:43Z'>
		same result:
Device used : cpu
Device used : cuda
then error message about no GPU
		</comment>
		<comment id='17' author='tmontana' date='2020-10-19T19:57:28Z'>
		&lt;denchmark-link:https://github.com/tmontana&gt;@tmontana&lt;/denchmark-link&gt;
 any news? it's hard to help if can't reproduce the error...
		</comment>
		<comment id='18' author='tmontana' date='2020-10-20T09:25:28Z'>
		Hi.  As you saw I tested on py37 with same results.  I'm out of testing ideas to figure out where the second Device used: cuda message comes from.  One thought is that I am running this on VMs within Azure ML.  They are standard Ubuntu 16 VMs with Conda installed.  I checked that on a non-GPU VM nvidia-smi does not work and correctly reports that no GPU drivers are available.  I'll try it on a VM from another provider or WSL to see if that makes a difference.
Thanks
		</comment>
		<comment id='19' author='tmontana' date='2020-10-26T09:09:51Z'>
		&lt;denchmark-link:https://github.com/Optimox&gt;@Optimox&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tmontana&gt;@tmontana&lt;/denchmark-link&gt;

I think I finally have an idea where this is coming from.
When defining TabNetClassifier if device_name is left untouched it will automatically be set to auto and in this case there should be no problem alternating between CPU and GPU.
However, if when defining the classifier you explicit device_name='cuda' then it becomes a loading parameter and it will try to set it to cuda when loading the model even though the device is not available anymore. I managed to reproduce your error be adding the device on instantiation before saving the model and I got the following traceback:
&lt;denchmark-code&gt;Device used : cpu
Device used : cuda

---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
&lt;ipython-input-8-7e2136ffde62&gt; in &lt;module&gt;
      1 # define new model with basic parameters and load state dict weights
      2 loaded_clf = TabNetClassifier()
----&gt; 3 loaded_clf.load_model("./tabnet_model_test_1.zip")

/work/pytorch_tabnet/abstract_model.py in load_model(self, filepath)
    335         self.__init__(**loaded_params)
    336 
--&gt; 337         self._set_network()
    338         self.network.load_state_dict(saved_state_dict)
    339         self.network.eval()

/work/pytorch_tabnet/abstract_model.py in _set_network(self)
    479             momentum=self.momentum,
    480             device_name=self.device_name,
--&gt; 481             mask_type=self.mask_type,
    482         ).to(self.device)
    483 

/work/pytorch_tabnet/tab_network.py in __init__(self, input_dim, output_dim, n_d, n_a, n_steps, gamma, cat_idxs, cat_dims, cat_emb_dim, n_independent, n_shared, epsilon, virtual_batch_size, momentum, device_name, mask_type)
    275                 device_name = 'cpu'
    276         self.device = torch.device(device_name)
--&gt; 277         self.to(self.device)
    278 
    279     def forward(self, x):

/work/.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages/torch/nn/modules/module.py in to(self, *args, **kwargs)
    605             return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
    606 
--&gt; 607         return self._apply(convert)
    608 
    609     def register_backward_hook(

/work/.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)
    352     def _apply(self, fn):
    353         for module in self.children():
--&gt; 354             module._apply(fn)
    355 
    356         def compute_should_use_set_data(tensor, tensor_applied):

/work/.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)
    352     def _apply(self, fn):
    353         for module in self.children():
--&gt; 354             module._apply(fn)
    355 
    356         def compute_should_use_set_data(tensor, tensor_applied):

/work/.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)
    352     def _apply(self, fn):
    353         for module in self.children():
--&gt; 354             module._apply(fn)
    355 
    356         def compute_should_use_set_data(tensor, tensor_applied):

/work/.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)
    374                 # `with torch.no_grad():`
    375                 with torch.no_grad():
--&gt; 376                     param_applied = fn(param)
    377                 should_use_set_data = compute_should_use_set_data(param, param_applied)
    378                 if should_use_set_data:

/work/.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages/torch/nn/modules/module.py in convert(t)
    603             if convert_to_format is not None and t.dim() == 4:
    604                 return t.to(device, dtype if t.is_floating_point() else None, non_blocking, memory_format=convert_to_format)
--&gt; 605             return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
    606 
    607         return self._apply(convert)

/work/.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages/torch/cuda/__init__.py in _lazy_init()
    184             raise RuntimeError(
    185                 "Cannot re-initialize CUDA in forked subprocess. " + msg)
--&gt; 186         _check_driver()
    187         if _cudart is None:
    188             raise AssertionError(

/work/.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages/torch/cuda/__init__.py in _check_driver()
     66 Found no NVIDIA driver on your system. Please check that you
     67 have an NVIDIA GPU and installed a driver from
---&gt; 68 http://www.nvidia.com/Download/index.aspx""")
     69         else:
     70             # TODO: directly link to the alternative bin that needs install

AssertionError: 
Found no NVIDIA driver on your system. Please check that you
have an NVIDIA GPU and installed a driver from
http://www.nvidia.com/Download/index.aspx
&lt;/denchmark-code&gt;

It should be simple to write a check on loading time, I will open a PR.
		</comment>
		<comment id='20' author='tmontana' date='2020-10-26T09:21:33Z'>
		That's amazing.  Thanks for looking into it!
		</comment>
		<comment id='21' author='tmontana' date='2020-10-30T18:21:55Z'>
		
@Optimox @tmontana
I think I finally have an idea where this is coming from.
When defining TabNetClassifier if device_name is left untouched it will automatically be set to auto and in this case there should be no problem alternating between CPU and GPU.
However, if when defining the classifier you explicit device_name='cuda' then it becomes a loading parameter and it will try to set it to cuda when loading the model even though the device is not available anymore. I managed to reproduce your error be adding the device on instantiation before saving the model and I got the following traceback:
Device used : cpu
Device used : cuda

---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
&lt;ipython-input-8-7e2136ffde62&gt; in &lt;module&gt;
      1 # define new model with basic parameters and load state dict weights
      2 loaded_clf = TabNetClassifier()
----&gt; 3 loaded_clf.load_model("./tabnet_model_test_1.zip")

/work/pytorch_tabnet/abstract_model.py in load_model(self, filepath)
    335         self.__init__(**loaded_params)
    336 
--&gt; 337         self._set_network()
    338         self.network.load_state_dict(saved_state_dict)
    339         self.network.eval()

/work/pytorch_tabnet/abstract_model.py in _set_network(self)
    479             momentum=self.momentum,
    480             device_name=self.device_name,
--&gt; 481             mask_type=self.mask_type,
    482         ).to(self.device)
    483 

/work/pytorch_tabnet/tab_network.py in __init__(self, input_dim, output_dim, n_d, n_a, n_steps, gamma, cat_idxs, cat_dims, cat_emb_dim, n_independent, n_shared, epsilon, virtual_batch_size, momentum, device_name, mask_type)
    275                 device_name = 'cpu'
    276         self.device = torch.device(device_name)
--&gt; 277         self.to(self.device)
    278 
    279     def forward(self, x):

/work/.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages/torch/nn/modules/module.py in to(self, *args, **kwargs)
    605             return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
    606 
--&gt; 607         return self._apply(convert)
    608 
    609     def register_backward_hook(

/work/.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)
    352     def _apply(self, fn):
    353         for module in self.children():
--&gt; 354             module._apply(fn)
    355 
    356         def compute_should_use_set_data(tensor, tensor_applied):

/work/.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)
    352     def _apply(self, fn):
    353         for module in self.children():
--&gt; 354             module._apply(fn)
    355 
    356         def compute_should_use_set_data(tensor, tensor_applied):

/work/.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)
    352     def _apply(self, fn):
    353         for module in self.children():
--&gt; 354             module._apply(fn)
    355 
    356         def compute_should_use_set_data(tensor, tensor_applied):

/work/.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)
    374                 # `with torch.no_grad():`
    375                 with torch.no_grad():
--&gt; 376                     param_applied = fn(param)
    377                 should_use_set_data = compute_should_use_set_data(param, param_applied)
    378                 if should_use_set_data:

/work/.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages/torch/nn/modules/module.py in convert(t)
    603             if convert_to_format is not None and t.dim() == 4:
    604                 return t.to(device, dtype if t.is_floating_point() else None, non_blocking, memory_format=convert_to_format)
--&gt; 605             return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
    606 
    607         return self._apply(convert)

/work/.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages/torch/cuda/__init__.py in _lazy_init()
    184             raise RuntimeError(
    185                 "Cannot re-initialize CUDA in forked subprocess. " + msg)
--&gt; 186         _check_driver()
    187         if _cudart is None:
    188             raise AssertionError(

/work/.cache/poetry/pytorch-tabnet-DJpFP61h-py3.7/lib/python3.7/site-packages/torch/cuda/__init__.py in _check_driver()
     66 Found no NVIDIA driver on your system. Please check that you
     67 have an NVIDIA GPU and installed a driver from
---&gt; 68 http://www.nvidia.com/Download/index.aspx""")
     69         else:
     70             # TODO: directly link to the alternative bin that needs install

AssertionError: 
Found no NVIDIA driver on your system. Please check that you
have an NVIDIA GPU and installed a driver from
http://www.nvidia.com/Download/index.aspx

It should be simple to write a check on loading time, I will open a PR.

Your fix works great by the way.  Thanks!
		</comment>
	</comments>
</bug>