<bug id='257' author='jchoi92' open_date='2021-01-01T03:14:42Z' closed_time='2021-01-02T13:45:14Z'>
	<summary>TabNetPretrainer uses n_d to initialize n_a for TabNetEncoder</summary>
	<description>
Describe the bug
TabNetPretrainer uses n_d to initialize n_a for TabNetEncoder, resulting in a wrong n_a if n_d != n_a
Expected behavior
This should be a simple fix - in the line with asterisks below, n_a=n_d should be updated to n_a=n_a.
&lt;denchmark-code&gt;class TabNetPretraining(torch.nn.Module):
    def __init__(
        self,
        input_dim,
        pretraining_ratio=0.2,
        n_d=8,
        n_a=8,
        n_steps=3,
        gamma=1.3,
        cat_idxs=[],
        cat_dims=[],
        cat_emb_dim=1,
        n_independent=2,
        n_shared=2,
        epsilon=1e-15,
        virtual_batch_size=128,
        momentum=0.02,
        mask_type="sparsemax",
    ):
        super(TabNetPretraining, self).__init__()

        self.cat_idxs = cat_idxs or []
        self.cat_dims = cat_dims or []
        self.cat_emb_dim = cat_emb_dim

        self.input_dim = input_dim
        self.n_d = n_d
        self.n_a = n_a
        self.n_steps = n_steps
        self.gamma = gamma
        self.epsilon = epsilon
        self.n_independent = n_independent
        self.n_shared = n_shared
        self.mask_type = mask_type
        self.pretraining_ratio = pretraining_ratio

        if self.n_steps &lt;= 0:
            raise ValueError("n_steps should be a positive integer.")
        if self.n_independent == 0 and self.n_shared == 0:
            raise ValueError("n_shared and n_independent can't be both zero.")

        self.virtual_batch_size = virtual_batch_size
        self.embedder = EmbeddingGenerator(input_dim, cat_dims, cat_idxs, cat_emb_dim)
        self.post_embed_dim = self.embedder.post_embed_dim

        self.masker = RandomObfuscator(self.pretraining_ratio)
        self.encoder = TabNetEncoder(
            input_dim=self.post_embed_dim,
            output_dim=self.post_embed_dim,
            n_d=n_d,
            n_a=n_d, ************ UPDATE ************
            n_steps=n_steps,
            gamma=gamma,
            n_independent=n_independent,
            n_shared=n_shared,
            epsilon=epsilon,
            virtual_batch_size=virtual_batch_size,
            momentum=momentum,
            mask_type=mask_type,
        )
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='jchoi92' date='2021-01-01T04:21:16Z'>
		It looks like the same change needs to be made for TabNetNoEmbeddings as well. Let me know if I'm missing something!
		</comment>
		<comment id='2' author='jchoi92' date='2021-01-01T12:13:23Z'>
		wow, thank you so much for finding this silent bug... It's quite new I think, it comes from a bad merge I did from other branch for self supervision...
		</comment>
	</comments>
</bug>