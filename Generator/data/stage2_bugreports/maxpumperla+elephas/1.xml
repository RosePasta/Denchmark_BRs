<bug id='1' author='ansrivas' open_date='2015-08-25T11:34:21Z' closed_time='2015-09-15T15:38:42Z'>
	<summary>Empty rdd throws an error while calling worker.train</summary>
	<description>
Hi , thanks for writing elephas , looks very interesting.
I was just playing around with the code.
I observed at some points  when the rdd is repartitioned , there might be empty partitions, they throw error in worker.train as keras tries to fit arrays with zero data.
	</description>
	<comments>
		<comment id='1' author='ansrivas' date='2015-09-14T14:21:52Z'>
		Hi, Thanks for this!
I noticed this, too, and think the problem is two-fold. First, in principle it should be OK to have partitions with zero data, although it's obviously not preferable. I have to look into how Spark does this exactly.
Second, the actual error that's thrown comes down to a local Keras variable in Callbacks that is only initialized if there is data. Now, of course you wouldn't do this in Keras, but at least the error you get should be indicative of this.
I want to submit a proposal soon, which should then resolve the problem.
		</comment>
		<comment id='2' author='ansrivas' date='2015-09-14T14:38:23Z'>
		I tried to check it as well, came up with something like this, may be you can verify this.
&lt;denchmark-link:https://github.com/ansrivas/elephas/blob/master/elephas/spark_model.py#L51-L63&gt;https://github.com/ansrivas/elephas/blob/master/elephas/spark_model.py#L51-L63&lt;/denchmark-link&gt;

Couple of posts on stack-overflow also suggested that in rdd.mapPartition(func), func should take care of empty partitions.
		</comment>
		<comment id='3' author='ansrivas' date='2015-09-14T15:09:14Z'>
		Sure, that should do as quick fix. I'll integrate it later, thank you! Have to remember to check up on the Keras thing itself.
		</comment>
		<comment id='4' author='ansrivas' date='2015-09-15T15:38:42Z'>
		Turns out you have to make sure there's at least batch_size + 1 samples on each partition, so your check did not quite suffice. I submitted a fix and will close this now. Thanks again.
		</comment>
	</comments>
</bug>