<bug id='192' author='aymericdamien' open_date='2016-07-10T12:23:26Z' closed_time='2016-07-11T08:39:12Z'>
	<summary>Dropout in RNNs</summary>
	<description>
It seems there is a bug where dropout in RNNs are still applied at prediction time, investigating if it is coming from tflearn or tensorflow.
	</description>
	<comments>
		<comment id='1' author='aymericdamien' date='2016-07-10T19:22:45Z'>
		I did a quick test and ran the sentiment analysis example &lt;denchmark-link:https://github.com/tflearn/tflearn/blob/master/examples/nlp/lstm.py&gt;1&lt;/denchmark-link&gt;
 and Kera's equivalent [2](using Tensorflow as a backend instead of Theano). Even with dropout, the Keras code doesn't have the same problem, which would suggest the bug is in tflearn.
		</comment>
		<comment id='2' author='aymericdamien' date='2016-07-11T08:40:33Z'>
		Actually the error was that the TF wrapper always apply dropout, so it has to be modified with TFLearn 'is_training' op condition, to only apply dropout at training time.
		</comment>
	</comments>
</bug>