<bug id='655' author='iamyihwa' open_date='2019-04-11T08:45:48Z' closed_time='2020-05-07T04:15:45Z'>
	<summary>empty list returned for the result of multi_label classifier case</summary>
	<description>
Hello
I have noticed that sometimes for the multi_label classifier case (using TextClassifier(flair.nn.Model)) , there is no value returned.
I saw in the implementation in &lt;denchmark-link:https://github.com/zalandoresearch/flair/blob/master/flair/models/text_classification_model.py&gt;text_classification_model.py&lt;/denchmark-link&gt;
 that _get_multi_label() and _get_single_label() were implemented differently, and get_multi_label only returns values when confidence is above 0.5.
Since it might be confusing if no value is returned, what about adding at the end of the _get_multi_label a check to see if the labels is empty, and if so, then add the same function as in _get_single_label? (I have attached a proposed change.)
Original _get_multi_label and _get_single_label:
&lt;denchmark-code&gt; def _get_multi_label(self, label_scores) -&gt; List[Label]:
        labels = []

        sigmoid = torch.nn.Sigmoid()

        results = list(map(lambda x: sigmoid(x), label_scores))
        for idx, conf in enumerate(results):
            if conf &gt; 0.5:
                label = self.label_dictionary.get_item_for_index(idx)
                labels.append(Label(label, conf.item()))

        return labels

    def _get_single_label(self, label_scores) -&gt; List[Label]:
        conf, idx = torch.max(label_scores, 0)
        label = self.label_dictionary.get_item_for_index(idx.item())

        return [Label(label, conf.item())]
&lt;/denchmark-code&gt;

Proposed modification of get_multi_label to return at least one value in any case:
&lt;denchmark-code&gt; def _get_multi_label(self, label_scores) -&gt; List[Label]:
        labels = []

        sigmoid = torch.nn.Sigmoid()

        results = list(map(lambda x: sigmoid(x), label_scores))
        for idx, conf in enumerate(results):
            if conf &gt; 0.5:
                label = self.label_dictionary.get_item_for_index(idx)
                labels.append(Label(label, conf.item()))
       
        if not labels:
               conf, idx = torch.max(label_scores, 0)
               label = self.label_dictionary.get_item_for_index(idx.item())
               labels = [label]

        return labels
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='iamyihwa' date='2019-04-16T09:59:32Z'>
		Hello &lt;denchmark-link:https://github.com/iamyihwa&gt;@iamyihwa&lt;/denchmark-link&gt;
 thanks for spotting this - I agree that it could be confusing that all multi label classifications with confidence below 0.5 are filtered out, while single label classifications are not.
Perhaps we should lower the threshold to a much smaller value (such as 0.1) so that all labels are added? If users want only high-confidence predictions they could still filter them out after prediction?
		</comment>
		<comment id='2' author='iamyihwa' date='2019-04-24T09:40:52Z'>
		Hello &lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 Sorry for getting back late!
I don't know what would be best for user experience.
However I see one problem with setting a hard threshold.
It is still possible that all of the labels get confidence below 0.1.. in which case a empty list will be returned.
It could also be a bit confusing when too many of them are returned ...
What do you think?
		</comment>
		<comment id='3' author='iamyihwa' date='2019-04-26T09:02:30Z'>
		&lt;denchmark-link:https://github.com/iamyihwa&gt;@iamyihwa&lt;/denchmark-link&gt;
 thanks good point - we'll think a bit on this. I'll let you know once we refactor this part!
		</comment>
		<comment id='4' author='iamyihwa' date='2019-04-30T21:11:30Z'>
		Yes, I think the user should be able to pass in an argument to _get_multi_label() specifying the min confidence level.  0.5 could be insufficient for many multi-label text classification tasks where the predicted confidence could be just below threshold like 0.49
I've love to re-run my classification model with, say conf = 0.0, as often we just want to assign to the sentence/document the label with the highest probability, regardless of confidence score.
		</comment>
		<comment id='5' author='iamyihwa' date='2019-05-02T07:22:29Z'>
		&lt;denchmark-link:https://github.com/cpoptic&gt;@cpoptic&lt;/denchmark-link&gt;
 Thanks for the input. We have a PR pending that implements such a mechanism for single label classification (&lt;denchmark-link:https://github.com/flairNLP/flair/pull/693&gt;#693&lt;/denchmark-link&gt;
). Once this is in, we can implement a similar interface for multi label.
		</comment>
		<comment id='6' author='iamyihwa' date='2020-04-30T02:10:45Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>