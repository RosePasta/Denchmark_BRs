<bug id='1094' author='tombburnell' open_date='2019-09-13T11:02:52Z' closed_time='2020-05-06T20:32:56Z'>
	<summary>Trained model with 1.0 f1-scores score but strange/poor results</summary>
	<description>
Describe the bug
A clear and concise description of what the bug is.
I've trained an LM  model using a bunch of news articles labelled with keywords, limited to only popular keywords. The training has scored very highly (almost suspiciously so).
MICRO_AVG: acc 0.9993 - f1-score 0.9997
MACRO_AVG: acc 0.9993 - f1-score 0.9996321428571429
americas   tp: 279 - fp: 0 - fn: 0 - tn: 8351 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
asia_pacific tp: 477 - fp: 0 - fn: 0 - tn: 8153 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
boris_johnson tp: 579 - fp: 0 - fn: 0 - tn: 8051 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
brexit     tp: 389 - fp: 0 - fn: 0 - tn: 8241 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
business   tp: 1280 - fp: 0 - fn: 0 - tn: 7350 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
china      tp: 213 - fp: 0 - fn: 0 - tn: 8417 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
crime      tp: 1119 - fp: 0 - fn: 0 - tn: 7511 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
culture    tp: 686 - fp: 0 - fn: 0 - tn: 7944 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
education  tp: 324 - fp: 0 - fn: 0 - tn: 8306 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
environment tp: 266 - fp: 0 - fn: 0 - tn: 8364 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
europe     tp: 699 - fp: 0 - fn: 0 - tn: 7931 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
european_union tp: 352 - fp: 0 - fn: 0 - tn: 8278 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
finance    tp: 568 - fp: 0 - fn: 4 - tn: 8058 - precision: 1.0000 - recall: 0.9930 - accuracy: 0.9930 - f1-score: 0.9965
law        tp: 322 - fp: 0 - fn: 0 - tn: 8308 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
love_island tp: 409 - fp: 0 - fn: 0 - tn: 8221 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
markets    tp: 208 - fp: 0 - fn: 0 - tn: 8422 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
music      tp: 281 - fp: 0 - fn: 0 - tn: 8349 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
news       tp: 3476 - fp: 0 - fn: 0 - tn: 5154 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
police     tp: 274 - fp: 0 - fn: 0 - tn: 8356 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
politics   tp: 1613 - fp: 0 - fn: 0 - tn: 7017 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
technology tp: 341 - fp: 0 - fn: 0 - tn: 8289 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
travel     tp: 323 - fp: 0 - fn: 0 - tn: 8307 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
trump      tp: 852 - fp: 0 - fn: 0 - tn: 7778 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
uk         tp: 861 - fp: 0 - fn: 4 - tn: 7765 - precision: 1.0000 - recall: 0.9954 - accuracy: 0.9954 - f1-score: 0.9977
us         tp: 440 - fp: 4 - fn: 0 - tn: 8186 - precision: 0.9910 - recall: 1.0000 - accuracy: 0.9910 - f1-score: 0.9955
western_europe tp: 428 - fp: 0 - fn: 0 - tn: 8202 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
world      tp: 363 - fp: 0 - fn: 0 - tn: 8267 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
world_news tp: 599 - fp: 0 - fn: 0 - tn: 8031 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
Some articles are predicting quite well, eg
A wallop for Boris Johnson... but Jacob Rees-Mogg takes it lying down: HENRY DEEDES sees the Leader of the House sprawl across the green benches like a 1970s lounge lizard as PM gulps down the briny taste of defeat
==&gt;  [news (0.9999940395355225), politics (0.9972768425941467), boris_johnson (0.8435016870498657), brexit (0.9373921751976013)]
However an article about sports (which was excluded from the training set) resulted in lots of unexpected label matches..
"After hitting four against Lithuania, Cristiano Ronaldo has scored against 40 different teams for Portugal"
==&gt; [world (0.7588407397270203), environment (0.9990266561508179), music (0.8719083070755005), culture (0.673468828201294), law (0.9973234534263611), boris_johnson (0.9677785634994507), police (0.9971749782562256), world_news (0.9998794794082642), travel (0.8145115971565247), trump (0.7244243621826172), americas (0.9826140999794006), asia_pacific (0.8884234428405762), china (0.9943141341209412), us (0.5811684727668762), education (0.8464401364326477), european_union (0.9843260645866394), love_island (0.9207460880279541), technology (0.926381528377533)]
I used the 'en' embedding for this.
What is this likely to be a symptom of?
A colleague suggested that the very high training score may be something to do with it learning how to predict the model very well but not very good at predicting new content and changing the neural network size may help.
	</description>
	<comments>
		<comment id='1' author='tombburnell' date='2019-10-02T08:10:14Z'>
		Another example - opinion_piece has got scores of 1.0 but in actual use, it's only about 0.5% accurate unlike news which is about  80% accurate. Is this just a data issue or are there things one can do to optimise the training?
news       tp: 9970 - fp: 440 - fn: 467 - tn: 12690 - precision: 0.9577 - recall: 0.9553 - accuracy: 0.9166 - f1-score: 0.9565
opinion_piece tp: 1030 - fp: 0 - fn: 0 - tn: 22537 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000
police     tp: 963 - fp: 24 - fn: 189 - tn: 22391 - precision: 0.9757 - recall: 0.8359 - accuracy: 0.8189 - f1-score: 0.9004
		</comment>
		<comment id='2' author='tombburnell' date='2019-10-02T08:14:39Z'>
		Good question, the score for opinion_piece seems way too high. Are you sure that you are not testing on data points that the model has seen during training?
		</comment>
		<comment id='3' author='tombburnell' date='2019-10-02T08:37:12Z'>
		ah -hi &lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
  you have seen it already! :)
I believe so, but I will check. I create the corpus, (train, dev, text) by generating a list of data points, shuffling, them, and then taking 80% as train and 10% as dev and 10% as test.
I'll check my logic and the corpus for duplicates.
		</comment>
		<comment id='4' author='tombburnell' date='2019-10-02T09:06:10Z'>
		I've just double checked and I have no duplicates in the corpus
		</comment>
		<comment id='5' author='tombburnell' date='2019-10-02T12:10:37Z'>
		Hm that's strange. Is there maybe some "giveaway" in the corpus, such as one word that only occurs in texts labeled as opinion_piece? Maybe there is some very specific and easy pattern that occurs in the training data that the classifier learns that then does not occur in actual data.
If you test documents from the training corpus labeled as opinion_piece, are they all correctly predicted? Is the data where it fails somehow different from the data it was trained on?
		</comment>
		<comment id='6' author='tombburnell' date='2019-10-02T12:50:15Z'>
		I discovered many of them had the authors names in the title with common punctuation, so it might well be this. I have removed the names and running again. However, it's very slow - hence my common on other ticket about low GPU utilisation (3-10% of 2080)
		</comment>
		<comment id='7' author='tombburnell' date='2019-10-02T13:37:12Z'>
		haha - I've removed the names and have been running the training for a hours or so and  testing against the best-model.pt after epoch 11 and DEV score =0.72, it is predicting everything as opinion_piece, even things that are not.
When it trains, does it go in any particular order? eg tag by tag? Would the partly completed model tend to be balanced or unbalanced?
		</comment>
		<comment id='8' author='tombburnell' date='2019-10-02T14:53:35Z'>
		The training data is shuffled at each epoch by default, so its always in a different order. The model typically reflects the imbalance in the training dataset, so if most of the training examples are from the same class it will tend to predict that class.
		</comment>
		<comment id='9' author='tombburnell' date='2019-10-02T15:04:57Z'>
		In this case there was another class that had a lot more members (news: 20k &gt; opinion_piece: 8k) and yet almost all news articles were categorised as opinion_piece. Presumably because it scored better overall.
news       tp: 8809 - fp: 4965 - fn: 1759 - tn: 8034 - precision: 0.6395 - recall: 0.8336 - accuracy: 0.5671 - f1-score: 0.7238
opinion_piece tp: 980 - fp: 1 - fn: 9 - tn: 22577 - precision: 0.9990 - recall: 0.9909 - accuracy: 0.9899 - f1-score: 0.9949
Is there anyway that Flair could internally re-balance/normalise to cater for data-sets that are inherently unbalanced.
Are there any recommended approaches to balancing the corpus?
I can see how reducing the size of the larger classes can help but then it also limits the size of the data set. Can duplicating members of smaller sets help or does this just cause over-training of that class?
		</comment>
		<comment id='10' author='tombburnell' date='2019-10-04T09:09:25Z'>
		We have some initial work already checked in for better sampling of imbalanced datasets, e.g. the ImbalancedClassificationDatasetSampler class. However, it is untested so not very likely that it will deliver great results. You can use it like this:
# load corpus
corpus = flair.datasets.TREC_6(in_memory=True)

# make dictionary
label_dict = corpus.make_label_dictionary()

# choose embeddings
document_embedding = DocumentRNNEmbeddings([WordEmbeddings('glove')])

# init classifier
classifier = TextClassifier(document_embedding, label_dictionary=label_dict)

# init trainer
trainer = ModelTrainer(classifier, corpus)

# train with imbalanced dataset sampler
trainer.train(
    "path/to/your/experiment/folder",
    sampler=ImbalancedClassificationDatasetSampler
)
		</comment>
		<comment id='11' author='tombburnell' date='2020-04-29T20:11:06Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>