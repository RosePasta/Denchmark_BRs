<bug id='855' author='m-michalek' open_date='2019-07-02T14:37:19Z' closed_time='2020-05-07T00:41:25Z'>
	<summary>DataLoader worker (pid 10252) is killed by signal: Killed</summary>
	<description>
Hello guys!
I followed the instructions of the tutorials to construct my own training data for NER.
All I want to do is train a custom NER model with my own data.
My data looks like this:

George PER
Washington PER
went O
to O
Washington LOC

When I start training using a jupyter notebook, I get the following error message: DataLoader worker (pid 10252) is killed by signal: Killed
When I try it using a python script it just says Killed.
The pid changes every time I try to train a model. The training always stops at the first epoch, sometimes it is able to get to iteration 6 but sometimes it stops before iteration 1.
I can not provide you with the original data and I was not able to produce some data which reproduces the problem. I created synthetic data which was able to train a model.
The original data was tokenized using the tokenizer of spacy and holds some Unicode characters like\xc2 \xb4 \xc2. When I load the corpus I have access to all the annotations.
This is my code:
&lt;denchmark-code&gt;from flair.data import Corpus
from flair.datasets import ColumnCorpus
from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings
from typing import List

columns = {0: 'text', 1: 'ner'}

data_folder = './data/'

corpus: Corpus = ColumnCorpus(data_folder, columns,
                              train_file='train.txt',
                              test_file='test.txt',
                              dev_file='dev.txt',
                              )
    
print(corpus)

tag_type = 'ner'

tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)
print(tag_dictionary.idx2item)

embedding_types: List[TokenEmbeddings] = [WordEmbeddings('glove')]

embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)

from flair.models import SequenceTagger

tagger: SequenceTagger = SequenceTagger(hidden_size=256,
                                        embeddings=embeddings,
                                        tag_dictionary=tag_dictionary,
                                        tag_type=tag_type,
                                        use_crf=True)

from flair.trainers import ModelTrainer

trainer: ModelTrainer = ModelTrainer(tagger, corpus)

trainer.train('resources/taggers/example-ner',
              learning_rate=0.1,
              mini_batch_size=32,
              max_epochs=150)

from flair.visual.training_curves import Plotter
plotter = Plotter()
plotter.plot_training_curves('resources/taggers/example-ner/loss.tsv')
plotter.plot_weights('resources/taggers/example-ner/weights.txt')

&lt;/denchmark-code&gt;

From the image you can see, my dataset is not quite big and the data has been loaded successfully.
&lt;denchmark-link:https://user-images.githubusercontent.com/20742170/60519323-9acac500-9ce3-11e9-94af-e617e26ac755.jpg&gt;&lt;/denchmark-link&gt;

I am using MacOs with flair version 0.4.2.
Does anyone have experience with the problem or know a solution?
I would be glad about your help
	</description>
	<comments>
		<comment id='1' author='m-michalek' date='2019-07-02T15:00:44Z'>
		That is strange because the code looks good. It could point to memory issues though. Could you try running with embeddings_in_memory=False? Also, I would recommend lowering the batch size if your data set is so small, otherwise the model will have difficulties to learn. So try like this:
trainer.train('resources/taggers/example-ner',
              learning_rate=0.05,
              mini_batch_size=8,
              max_epochs=150, 
              embeddings_in_memory=False)
		</comment>
		<comment id='2' author='m-michalek' date='2019-07-03T12:14:52Z'>
		Thank you, that worked for me, at least for a 60/20/20 split. But I also tried to use the StackedEmbeddings with glove, nl_forward, and nl_backward and it crashed again after a few hours. Is there a rule f thumb for selecting the correct learning rate and batch size?
Since I want to train the best possible model from my labeled data, would you recommend to use the train.txt which holds 100% of the labeled data and reference it as train_file, test_file and dev_file when loading the corpus? I understand that the evaluation result will be pretty much meaningless but I can not split the data because the performance of the model would decrease nor can I collect more annotated data for evaluation.
So the questions are, what can I do to train a model with contextual embeddings and how should I incorporate my complete dataset.
Thank you for your time and help!
		</comment>
		<comment id='3' author='m-michalek' date='2019-07-09T12:34:09Z'>
		&lt;denchmark-link:https://github.com/m-michalek&gt;@m-michalek&lt;/denchmark-link&gt;
 One question: do you use  for training? In that case you should also increase the  :)
		</comment>
		<comment id='4' author='m-michalek' date='2019-07-12T14:06:28Z'>
		&lt;denchmark-link:https://github.com/stefan-it&gt;@stefan-it&lt;/denchmark-link&gt;
 I am using &lt;denchmark-link:https://hub.docker.com/r/jupyter/scipy-notebook/&gt;jupyter/scipy-notebook&lt;/denchmark-link&gt;
 and as far as I know, it does not inherit from nvidia-docker. Thank you for that tip, I will try to increase the  of my container.
But I am still wondering why I was able to train a model with my synthetic data which was comparable to the original dataset in terms of sentence length and amount of sentences.
		</comment>
		<comment id='5' author='m-michalek' date='2020-04-30T00:10:44Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>