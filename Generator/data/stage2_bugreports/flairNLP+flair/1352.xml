<bug id='1352' author='CatarinaPC' open_date='2020-01-14T18:10:28Z' closed_time='2020-05-23T04:32:01Z'>
	<summary>Questions and Bugs (training sequence tagger + hyperparameter selection)</summary>
	<description>
I'm going to use this issue (bug report) to not only present some bugs I found as well as some questions.

After training a model (sequence tagger), testing is performed using the best model and the test set. What are those three values shown in the last line from this excerpt of the training output?

&lt;denchmark-code&gt;2020-01-12 09:41:12,567 Testing using best model ...
2020-01-12 09:41:12,567 loading file ../models/flair/default/best-model.pt
2020-01-12 09:41:38,126 0.7877	0.7698	0.7786
...
&lt;/denchmark-code&gt;


Is it "normal" for the training of a model to take 7 hours? I used all default parameters and as for word embedding I used French FastText embeddings. My corpus is made up of:
Corpus: "Corpus: 46133 train + 14417 dev + 11534 test sentences"
(It also ran until 87 epochs)

I also tried using as word embeddings Camembert (with all other parameters as default) and just one epoch took 30m. Is this expected?
(Sorry if my questions are not very good, I'm still a newbie in ML)


When performing hyperparameter selection, what is an acceptable number of max_epochs, training_runs and max_evals?


I wanted do perform hyperparameter selection. I found all word embedding that could work with French from your docs and added them to the search space:


&lt;denchmark-code&gt;search_space.add(Parameter.EMBEDDINGS, hp.choice, options=[
        WordEmbeddings('fr'),
        StackedEmbeddings([FlairEmbeddings('fr-forward'), FlairEmbeddings('fr-backward')]),
        StackedEmbeddings([
            WordEmbeddings('fr'),
            FlairEmbeddings('fr-forward'),
            FlairEmbeddings('fr-backward'),
        ]),
        StackedEmbeddings([WordEmbeddings('fr'), CharacterEmbeddings()]),
        BytePairEmbeddings('fr'),
        BytePairEmbeddings('multi'),
        CamembertEmbeddings(),
        BertEmbeddings('bert-base-multilingual-cased'),
        BertEmbeddings('distilbert-base-multilingual-cased'),
        XLMEmbeddings('xlm-mlm-enfr-1024'),
        XLMEmbeddings('xlm-clm-enfr-1024'),
        XLMEmbeddings('xlm-mlm-xnli15-1024'),

        XLMEmbeddings('xlm-mlm-tlm-xnli15-1024'),
        XLMEmbeddings('xlm-mlm-17-1280'),
        XLMEmbeddings('xlm-mlm-100-1280'),
        XLMRobertaEmbeddings('xlm-roberta-base'),
        XLMRobertaEmbeddings('xlm-roberta-large')
    ])
&lt;/denchmark-code&gt;

But when I run the code, and they are being added to the search space, a CUDA out of memory error occurs:
RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 3.95 GiB total capacity; 3.04 GiB already allocated; 11.50 MiB free; 32.36 MiB cached)
Specifically while loading the XLMEmbeddings('xlm-mlm-tlm-xnli15-1024'). But I tried changing the order to see if it was caused by these word embeddings in particular and I found it is not.
When I comment for example the last 5 embeddings the code is able to run.

When trying to use XLMEmbeddings('xlm-mlm-enfr-1024'), I get the following error before the first epoch even begins:

&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/home/catarinapc/cl-extraction-models/eval/eval_flair.py", line 192, in &lt;module&gt;
    param_selection("../datasets/", {'train': 'train/train_quaero.bio', 'test': 'test/test_quaero.bio', 'dev': 'dev/dev_quaero.bio'})
  File "/home/catarinapc/cl-extraction-models/eval/eval_flair.py", line 164, in param_selection
    param_selector.optimize(search_space, max_evals=2)
  File "/home/catarinapc/.local/lib/python3.6/site-packages/flair/hyperparameter/param_selection.py", line 162, in optimize
    self._objective, search_space, algo=tpe.suggest, max_evals=max_evals
  File "/home/catarinapc/.local/lib/python3.6/site-packages/hyperopt/fmin.py", line 422, in fmin
    rval.exhaust()
  File "/home/catarinapc/.local/lib/python3.6/site-packages/hyperopt/fmin.py", line 276, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.asynchronous)
  File "/home/catarinapc/.local/lib/python3.6/site-packages/hyperopt/fmin.py", line 241, in run
    self.serial_evaluate()
  File "/home/catarinapc/.local/lib/python3.6/site-packages/hyperopt/fmin.py", line 141, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/home/catarinapc/.local/lib/python3.6/site-packages/hyperopt/base.py", line 856, in evaluate
    rval = self.fn(pyll_rval)
  File "/home/catarinapc/.local/lib/python3.6/site-packages/flair/hyperparameter/param_selection.py", line 112, in _objective
    **training_params,
  File "/home/catarinapc/.local/lib/python3.6/site-packages/flair/trainers/trainer.py", line 329, in train
    loss = self.model.forward_loss(batch_step)
  File "/home/catarinapc/.local/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py", line 471, in forward_loss
    features = self.forward(data_points)
  File "/home/catarinapc/.local/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py", line 476, in forward
    self.embeddings.embed(sentences)
  File "/home/catarinapc/.local/lib/python3.6/site-packages/flair/embeddings.py", line 96, in embed
    self._add_embeddings_internal(sentences)
  File "/home/catarinapc/.local/lib/python3.6/site-packages/flair/embeddings.py", line 1386, in _add_embeddings_internal
    eos_token="&lt;/s&gt;",
  File "/home/catarinapc/.local/lib/python3.6/site-packages/flair/embeddings.py", line 1199, in _get_transformer_sentence_embeddings
    use_scalar_mix=use_scalar_mix,
  File "/home/catarinapc/.local/lib/python3.6/site-packages/flair/embeddings.py", line 1044, in _extract_embeddings
    first_embedding: torch.FloatTensor = current_embeddings[0]
IndexError: index 0 is out of bounds for dimension 0 with size 0
&lt;/denchmark-code&gt;

I'm no sure if it only happens on this particular XLMEmbedding.


During hyperparameter selection, all dev scores come back as 0 as well as the test results. I tried using both dev loss and dev score for the optimization value, but still get 0 in all scores.


If I use max_epochs=2, training_runs=1 and max_evals=1 for hyperparameter selection with the search space below, it takes super long to run just one epoch. However if I keep the same max_epochs, training_runs and max_evals but only put one word embedding in the search space, it is much faster. I don't understand why, because the number of evaluation runs is the same, meaning only one configuration will tested.


&lt;denchmark-code&gt;    # define your search space
    search_space = SearchSpace()

    # sequence tagger parameter
    search_space.add(Parameter.EMBEDDINGS, hp.choice, options=[
        WordEmbeddings('fr'),
        StackedEmbeddings([FlairEmbeddings('fr-forward'), FlairEmbeddings('fr-backward')]),
        StackedEmbeddings([
            WordEmbeddings('fr'),
            FlairEmbeddings('fr-forward'),
            FlairEmbeddings('fr-backward'),
        ]),
        StackedEmbeddings([WordEmbeddings('fr'), CharacterEmbeddings()]),
        BytePairEmbeddings('fr'),
        BytePairEmbeddings('multi'),
        CamembertEmbeddings(),
        BertEmbeddings('bert-base-multilingual-cased'),
        BertEmbeddings('distilbert-base-multilingual-cased'),
        XLMEmbeddings('xlm-mlm-enfr-1024'),
        XLMEmbeddings('xlm-clm-enfr-1024'),
        XLMEmbeddings('xlm-mlm-xnli15-1024')

        #XLMEmbeddings('xlm-mlm-tlm-xnli15-1024'),
        #XLMEmbeddings('xlm-mlm-17-1280'),
        #XLMEmbeddings('xlm-mlm-100-1280'),
        #XLMRobertaEmbeddings('xlm-roberta-base'),
        #XLMRobertaEmbeddings('xlm-roberta-large')
    ])
    search_space.add(Parameter.USE_CRF, hp.choice, options=[True, False])
    search_space.add(Parameter.DROPOUT, hp.uniform, low=0.0, high=0.75)
    search_space.add(Parameter.WORD_DROPOUT, hp.uniform, low=0.0, high=0.25)
    search_space.add(Parameter.LOCKED_DROPOUT, hp.uniform, low=0.0, high=0.5)
    search_space.add(Parameter.HIDDEN_SIZE, hp.choice, options=[32, 64, 128])
    search_space.add(Parameter.RNN_LAYERS, hp.choice, options=[1, 2])

    # model trainer parameter
    search_space.add(Parameter.OPTIMIZER, hp.choice, options=[SGD])

    # training parameter
    search_space.add(Parameter.MINI_BATCH_SIZE, hp.choice, options=[4, 8, 16, 32])
    search_space.add(Parameter.LEARNING_RATE, hp.uniform, low=0.01, high=1)
    search_space.add(Parameter.ANNEAL_FACTOR, hp.uniform, low=0.3, high=0.75)
    search_space.add(Parameter.PATIENCE, hp.choice, options=[3, 5])
    search_space.add(Parameter.WEIGHT_DECAY, hp.uniform, low=0.01, high=1)
&lt;/denchmark-code&gt;

Environment:

OS: Ubuntu 18.04.3 LTS
Version: 0.4.4 (I think I'm using the version on the master branch)
Python version: 3.6.8

	</description>
	<comments>
		<comment id='1' author='CatarinaPC' date='2020-01-14T19:09:23Z'>
		Hi &lt;denchmark-link:https://github.com/CatarinaPC&gt;@CatarinaPC&lt;/denchmark-link&gt;
, thanks for your interest - to your questions:
Question 1 (log line): This line should indicate precision, recall and F1 score on the test set with the best model after training. I think we need to improve the output so it becomes more human-readable.
Question 2 (speed): The training speed depends on the embeddings used and your hardware setup, specifically whether you use a GPU and how much CPU/GPU memory you have available. But 7 hours sounds about right for a dataset of this size.
For Camembert the first epoch is the one that generates embeddings. So the first epoch should be slow and all other epochs should be much faster, provided you can store embeddings in memory. In order to store embeddings in memory, you need to set  when you load the dataset and set the  to at least 'cpu' in the trainer. Check out some documentation &lt;denchmark-link:https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md#scalability-training-with-large-datasets&gt;here&lt;/denchmark-link&gt;
.
Questions 3,4,5, 6,7 (hyperparameter selection): This part of the framework is unfortunately still very rough and I would like to replace it entirely with a different implementation - potentially one not based on hyperopt - at some point. If you want to compare word embeddings, the best option right now would be to run these experiments with the normal ModelTrainer and just run them one after the other. Since training is non-deterministic it is typically good to run the same setup multiple times to get an average value for each setup. For XLMEmbeddings('xlm-mlm-enfr-1024') this might also fix the error, at least in my local setups I can instantiate and use these embeddings.
		</comment>
		<comment id='2' author='CatarinaPC' date='2020-01-14T22:06:32Z'>
		Thank you for your answer! I just needed a little more clarification in some things:

In order to store embeddings in memory, you need to set in_memory=True when you load the dataset and set the embeddings_storage_mode to at least 'cpu' in the trainer

You're only referring to training here (train function of the sequence tagger), right? embeddings_storage_mode does not exist during hyperparameter tunning.
I had to set in_memory=False when loading the dataset because I was getting memory errors. Does this mean my embeddings are not being stored in memory?

So the first epoch should be slow and all other epochs should be much faster, provided you can store embeddings in memory

Does this therefore mean I'm getting an even slower training?
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;


If you want to compare word embeddings, the best option right now would be to run these experiments with the normal ModelTrainer and just run them one after the other. Since training is non-deterministic it is typically good to run the same setup multiple times to get an average value for each setup.

So, should I reduce the number of max epochs? To make the "word embedding selection" process go faster? It that "okay"? And then when I find the best performing word embedding just train a final model with a much bigger number of epochs?
When you say

training is non-deterministic it is typically good to run the same setup multiple times

do you mean training a model several times with the same word embeddings and hyperparameters like the parameter training_runs in the SequenceTaggerParamSelector?
What about just selecting the other hyperparameters (e.g., rnn size)? Do I just skip performing hyperparameter selection? And taking into account I'm getting 0 in all scores, is the selection even being performed correctly?
Should I just use the default values for the (other) hyperparameters and see how the model improves when changing word embeddings?
		</comment>
		<comment id='3' author='CatarinaPC' date='2020-01-15T10:00:08Z'>
		&lt;denchmark-link:https://github.com/CatarinaPC&gt;@CatarinaPC&lt;/denchmark-link&gt;
 yes this is during training, however if the dataset is too large to fit into your memory then unfortunately you do not get this speedup: In this case, embeddings are always produced on-the-fly at each epoch so each epoch will take the same amount of time. If your dataset fits into memory, the first epoch will be slower and all others faster because the embeddings already exist and need not be generated again.
Yes, the parameter training_runs is for repeating an experiment multiple times to get a better estimate.
With regards to the 0 score, my suggestion would be to first run a setup with 'default' values, i.e. a normal training run with the  with all values like &lt;denchmark-link:https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md#training-a-sequence-labeling-model&gt;here&lt;/denchmark-link&gt;
 (but no downsampling of the data set) and some quick, standard embeddings for French, i.e.  or  to see what score you get. This would be a baseline and it should definitely not be 0. Could you run this experiment, share the training script snippet and report the numbers back?
		</comment>
		<comment id='4' author='CatarinaPC' date='2020-01-15T11:15:22Z'>
		Thank you so much!
I did ran an experiment just like you described. Below is my training script snippet and its results:
&lt;denchmark-code&gt;from flair.data import Corpus
from flair.datasets import ColumnCorpus
from flair.embeddings import WordEmbeddings
from flair.models import SequenceTagger
from flair.trainers import ModelTrainer


tag_type = "label"


corpus: Corpus = ColumnCorpus("../datasets/", {0: 'token', 1: 'label'},
                              train_file='train/train_quaero.bio',
                              test_file='test/test_quaero.bio',
                              dev_file='dev/dev_quaero.bio',
                              in_memory=False)

tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)

embedding = WordEmbeddings('fr')

tagger: SequenceTagger = SequenceTagger(hidden_size=256,
                                        embeddings=embedding,
                                        tag_dictionary=tag_dictionary,
                                        tag_type=tag_type,
                                        use_crf=True)

trainer: ModelTrainer = ModelTrainer(tagger, corpus)

trainer.train("../models/flair/default-values",
              learning_rate=0.1,
              mini_batch_size=32,
              max_epochs=150)
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;2020-01-12 09:41:12,567 Testing using best model ...
2020-01-12 09:41:12,567 loading file ../models/flair/default-values/best-model.pt
2020-01-12 09:41:38,126 0.7877	0.7698	0.7786
2020-01-12 09:41:38,127 
MICRO_AVG: acc 0.6375 - f1-score 0.7786
MACRO_AVG: acc 0.6198 - f1-score 0.7626666666666666
LOC        tp: 3360 - fp: 802 - fn: 798 - tn: 3360 - precision: 0.8073 - recall: 0.8081 - accuracy: 0.6774 - f1-score: 0.8077
ORG        tp: 1625 - fp: 712 - fn: 811 - tn: 1625 - precision: 0.6953 - recall: 0.6671 - accuracy: 0.5162 - f1-score: 0.6809
PER        tp: 4327 - fp: 996 - fn: 1176 - tn: 4327 - precision: 0.8129 - recall: 0.7863 - accuracy: 0.6658 - f1-score: 0.7994
2020-01-12 09:41:38,127
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='CatarinaPC' date='2020-01-17T02:16:35Z'>
		
Question 1 (log line): This line should indicate precision, recall and F1 score on the test set with the best model after training. I think we need to improve the output so it becomes more human-readable.

The precision, recall and f1-score take into account the "O" tags or only entity tags? Or in other words do the precision, recall and f1-score correspond to overall scores or are they the average over the entity tag types?
		</comment>
		<comment id='6' author='CatarinaPC' date='2020-01-17T02:34:50Z'>
		And just two more questions:

Does the model produced at the end of training correspond to the last model it was created or to the best model out of all the epochs? I'm pretty sure its the first option, but I just liked to be sure.


For Camembert the first epoch is the one that generates embeddings.

Is this only true for Camembert or for all word embeddings?
		</comment>
		<comment id='7' author='CatarinaPC' date='2020-05-16T03:30:24Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>