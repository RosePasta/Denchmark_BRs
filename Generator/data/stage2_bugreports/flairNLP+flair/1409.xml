<bug id='1409' author='BeWe11' open_date='2020-02-05T12:07:40Z' closed_time='2020-04-30T10:13:14Z'>
	<summary>High memory/swap when embedding with BERT</summary>
	<description>
Describe the bug
I'm embedding sentences using the BertEmbeddings class. I'm performing a single .embed call on a list of around 3000 Sentence objects. While the embeddings are being calculated, my swap increases by a huge amount, starting from almost nothing to around 50 GB. If I extract the embeddings afterwards as numpy arrays, the total size of the embeddings is around 15 MB.
To Reproduce
from flair.embeddings import BertEmbeddings, Sentence

embedder = BertEmbeddings(
    "bert-base-german-dbmdz-uncased",
    layers="0,1,2,3,4,5,6,7,8,9,10,11,12",
    use_scalar_mix=True,
    pooling_operation="first",
)
sentences = ["Ich suche drei Kisten Cola."] * 3000
flair_sentences = [Sentence(sentence) for sentence in sentences]
embedder.embed(flair_sentences)
Expected behavior
I'm not sure if this is an expected byproduct of using the BERT embedder. Is there some place inside the embedding code that caches large intermediate objects during a .embed call? My current workaround is to split the sentences into chunks and perform multiple .embed calls in parallel.
Environment (please complete the following information):

OS [e.g. iOS, Linux]: MacOS
Version [e.g. flair-0.3.2]: flair-0.4.5

	</description>
	<comments>
		<comment id='1' author='BeWe11' date='2020-04-28T18:39:30Z'>
		Hello &lt;denchmark-link:https://github.com/BeWe11&gt;@BeWe11&lt;/denchmark-link&gt;
 a bit late, but we've added a new implementation of transformer embeddings that seems to be more memory-effective. I can run your code on my laptop with no big memory buildup. This implementation is already in master branch and will be part of the next release.
There, you can set the batch_size in the embeddings object.
embedder = TransformerWordEmbeddings(
    "bert-base-german-dbmdz-uncased",
    layers="all",
    use_scalar_mix=True,
    pooling_operation="first",
    batch_size=256,
)
If all you want is the sentence representation as given by the CLS token, you can use the TransformerDocumentEmbeddings class instead.
		</comment>
		<comment id='2' author='BeWe11' date='2020-04-30T10:13:14Z'>
		Thanks &lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 !
		</comment>
	</comments>
</bug>