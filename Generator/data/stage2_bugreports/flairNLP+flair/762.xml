<bug id='762' author='AdityaSoni19031997' open_date='2019-05-28T18:30:56Z' closed_time='2019-05-28T19:49:55Z'>
	<summary>Can't increase the num_workers?</summary>
	<description>
PS Very New Flair User :)
Can't increase the num_workers...
To Reproduce
&lt;denchmark-code&gt;document_embeddings = DocumentRNNEmbeddings(word_embeddings,
                                            hidden_size=512,
                                            rnn_layers = 1,
                                            bidirectional=True,
                                            dropout = 0.23,
                                            reproject_words=True,
                                            reproject_words_dimension=256,
                                           )
classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)
trainer = ModelTrainer(classifier, corpus)
trainer.train('./stacked_embeddings/',num_workers=16, max_epochs=10, mini_batch_size= 32, 
              learning_rate=0.1, anneal_factor=0.5, train_with_dev=True)
&lt;/denchmark-code&gt;


I expected to allow the  to increase  the  as such.. &lt;denchmark-link:https://github.com/zalandoresearch/flair/blob/master/flair/trainers/trainer.py#L65&gt;reference&lt;/denchmark-link&gt;

Traceback
&lt;denchmark-code&gt;2019-05-28 18:15:22,483 Evaluation method: MICRO_F1_SCORE
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-22-6d2706307a23&gt; in &lt;module&gt;
      1 trainer = ModelTrainer(classifier, corpus)
      2 trainer.train('./stacked_embeddings/',num_workers=16, max_epochs=10, mini_batch_size= 32, 
----&gt; 3               learning_rate=0.1, anneal_factor=0.5, train_with_dev=True)

~/.local/lib/python3.7/site-packages/flair/trainers/trainer.py in train(self, base_path, evaluation_metric, learning_rate, mini_batch_size, eval_mini_batch_size, max_epochs, anneal_factor, patience, anneal_against_train_loss, train_with_dev, monitor_train, embeddings_in_memory, checkpoint, save_final_model, anneal_with_restarts, test_mode, param_selection_mode, **kwargs)
     80             weight_extractor = WeightExtractor(base_path)
     81 
---&gt; 82         optimizer = self.optimizer(self.model.parameters(), lr=learning_rate, **kwargs)
     83         if self.optimizer_state is not None:
     84             optimizer.load_state_dict(self.optimizer_state)

TypeError: __init__() got an unexpected keyword argument 'num_workers'
&lt;/denchmark-code&gt;

But it's defined &lt;denchmark-link:https://github.com/zalandoresearch/flair/blob/master/flair/trainers/trainer.py#L65&gt;here&lt;/denchmark-link&gt;
,
Enviornments

OS [Linux]:
Version [e.g. flair-0.4.1]:

	</description>
	<comments>
		<comment id='1' author='AdityaSoni19031997' date='2019-05-28T19:30:57Z'>
		Hello &lt;denchmark-link:https://github.com/AdityaSoni19031997&gt;@AdityaSoni19031997&lt;/denchmark-link&gt;
 did you install from pip? If so, this feature is not yet available. The feature is only available currently if you work on the master branch, which you could install like this:
!pip install --upgrade git+https://github.com/zalandoresearch/flair.git
		</comment>
		<comment id='2' author='AdityaSoni19031997' date='2019-05-28T19:46:58Z'>
		Yep I installed from pip!
Thanks for the info guys, will pull the master branch :)
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Wed, 29 May 2019, 01:01 Alan Akbik, ***@***.***&gt; wrote:
 Hello @AdityaSoni19031997 &lt;https://github.com/AdityaSoni19031997&gt; did you
 install from pip? If so, this feature is not yet available. The feature is
 only available currently if you work on the master branch, which you could
 install like this:

 !pip install --upgrade git+https://github.com/zalandoresearch/flair.git

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#762&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AFNPJJQ4ELYBVI52JG7DJX3PXWB7ZANCNFSM4HQF7OTA&gt;
 .



		</comment>
	</comments>
</bug>