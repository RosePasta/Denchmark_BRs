<bug id='1559' author='otahmasebi' open_date='2020-04-29T10:58:05Z' closed_time='2020-09-05T17:11:25Z'>
	<summary>Embedding Dataloader's workers are out of shared memory</summary>
	<description>
Describe the bug
After first epoch, training process killed by a kill signal and printed error msg was:

RuntimeError: DataLoader worker (pid 18801) is killed by signal: Bus error.
It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.

I set num_workers=2 and  I think 16G is enough space for shared memory. I started training again and this time I checked shared memory usage regularly. It seemed that there was a memory leak in /dev/shm directory.
Expected behavior
Something that avoid memory leak.

After a little research and googling, I realized that, this is a old and common bug in the Pytorch Dataloader and there is many issues about this bug. After some more research I found this solution &lt;denchmark-link:https://discuss.pytorch.org/t/training-crashes-due-to-insufficient-shared-memory-shm-nn-dataparallel/26396/18&gt;https://discuss.pytorch.org/t/training-crashes-due-to-insufficient-shared-memory-shm-nn-dataparallel/26396/18&lt;/denchmark-link&gt;
 .
I checked &lt;denchmark-link:https://github.com/flairNLP/flair/blob/818cb04767d9fc212932843ce23fa48978cc00ca/flair/trainers/language_model_trainer.py#L58&gt;__getitem__&lt;/denchmark-link&gt;
  method in &lt;denchmark-link:https://github.com/flairNLP/flair/blob/818cb04767d9fc212932843ce23fa48978cc00ca/flair/trainers/language_model_trainer.py#L25&gt;TextDataset&lt;/denchmark-link&gt;
  class and it returnes . So the explanations for the mentioned solution seem plausible. I haven't had time to implement this yet. But is there a simpler solution?!
Environment

OS: Ubuntu 18.04
32G RAM
GPU: Nvidia 2080 TI
Python 3.6
CUDA==10.2
torch==1.5.0
flair==0.4.5

	</description>
	<comments>
		<comment id='1' author='otahmasebi' date='2020-04-29T13:18:39Z'>
		Can you post a minimal script to reproduce? What's the dataset size you're working on?
		</comment>
		<comment id='2' author='otahmasebi' date='2020-04-29T19:11:07Z'>
		Of course,
I'm using this script
&lt;denchmark-code&gt;from flair.data import Dictionary
from flair.models import LanguageModel
from flair.trainers.language_model_trainer import LanguageModelTrainer, TextCorpus
from flair.optim import AdamW


char_dict_path = 'path_to_char_dict_pickle'
corpus_path = 'path_to_corpus'

is_forward_lm = True

# import pickle
dictionary = Dictionary.load_from_file(char_dict_path)

# get your corpus, process forward and at the character level
corpus = TextCorpus(corpus_path,
                    dictionary,
                    is_forward_lm,
                    character_level=True)

# instantiate your language model, set hidden size and number of layers
language_model = LanguageModel(dictionary,
                               is_forward_lm,
                               hidden_size=2048,
                               nlayers=1)

# train your language model
trainer = LanguageModelTrainer(language_model, corpus,
                               optimizer=AdamW)

trainer.train('./result',
              sequence_length=250,
              mini_batch_size=100,
              num_workers=2,
              max_epochs=10)
&lt;/denchmark-code&gt;

Also my dataset contains 10 files, that each file size is about 1 GB and has around 400000 sentences (each line has 1 sentence).
		</comment>
		<comment id='3' author='otahmasebi' date='2020-04-29T19:20:36Z'>
		Have you tried with more but smaller files (e.g. 100MB)?
		</comment>
		<comment id='4' author='otahmasebi' date='2020-04-29T21:43:07Z'>
		Yes, perhaps you could try splitting each file into 10 or 20 chunks?
		</comment>
		<comment id='5' author='otahmasebi' date='2020-05-01T15:01:16Z'>
		I split my dataset to about 300 files and till now (after first epoch) everything seems OK!
I'm waiting to see if anything bad happens or not.
		</comment>
		<comment id='6' author='otahmasebi' date='2020-08-29T15:26:46Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>