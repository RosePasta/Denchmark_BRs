<bug id='1043' author='stefan-it' open_date='2019-08-29T11:24:45Z' closed_time='2020-05-06T22:32:57Z'>
	<summary>Evaluation Metric: improvements</summary>
	<description>
Hi,
currently the evaluation output has some problems e.g. for accuracy-based experiments or NER F-Scores
&lt;denchmark-h:h1&gt;PoS tagging&lt;/denchmark-h&gt;

Here's an example for PoS tagging (example can be run on normal CPU):
from typing import List

import flair.datasets
from flair.data import Corpus
from flair.embeddings import (
    TokenEmbeddings,
    WordEmbeddings,
    StackedEmbeddings,
    FlairEmbeddings,
    CharacterEmbeddings,
)
from flair.training_utils import EvaluationMetric
from flair.visual.training_curves import Plotter

# 1. get the corpus
corpus: Corpus = flair.datasets.UD_ENGLISH().downsample(0.1)
print(corpus)

# 2. what tag do we want to predict?
tag_type = "upos"

# 3. make the tag dictionary from the corpus
tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)
print(tag_dictionary.idx2item)

# initialize embeddings
embedding_types: List[TokenEmbeddings] = [
    WordEmbeddings("glove"),
]

embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)

# initialize sequence tagger
from flair.models import SequenceTagger

tagger: SequenceTagger = SequenceTagger(
    hidden_size=256,
    embeddings=embeddings,
    tag_dictionary=tag_dictionary,
    tag_type=tag_type,
    use_crf=True,
)

# initialize trainer
from flair.trainers import ModelTrainer

trainer: ModelTrainer = ModelTrainer(tagger, corpus)

trainer.train(
    "resources/taggers/example-pos",
    learning_rate=0.1,
    mini_batch_size=32,
    max_epochs=1,
    shuffle=False,
)

# Evaluation
from flair.data import Sentence

loaded_tagger: SequenceTagger = SequenceTagger.load("resources/taggers/example-pos/best-model.pt")

number_tags = 0
number_correct_tags = 0

for sentence in corpus.test:
    tokens = sentence.tokens
    gold_tags = [token.tags['upos'].value for token in sentence.tokens]

    tagged_sentence = Sentence()
    tagged_sentence.tokens = tokens

    tagger.predict(tagged_sentence)

    predicted_tags = [token.tags['upos'].value for token in tagged_sentence.tokens]
    
    assert len(tokens) == len(gold_tags)
    assert len(gold_tags) == len(predicted_tags)
    
    number_tags += len(predicted_tags)
    
    for gold_tag, predicted_tag in zip(gold_tags, predicted_tags):
        if gold_tag == predicted_tag:
            number_correct_tags += 1

print(f'Accuracy: {number_correct_tags / number_tags}')
It outputs:
MICRO_AVG: acc 0.2413 - f1-score 0.3887
MACRO_AVG: acc 0.1386 - f1-score 0.20893529411764705
This output comes from Flair. The accuracy calculation outputs:
Accuracy: 0.38861588861588864
This values corresponds to the "f1-score" output of Flair. So this should be corrected :)
&lt;denchmark-h:h1&gt;NER&lt;/denchmark-h&gt;

I've seen some little differences between the Flair output (F-Score) and the official CoNLL script (yes, I pass IOB tags to that script ðŸ˜…).
It would be great to get back to the "old" evaluation logic that uses the CoNLL perl script. Maybe the train() method interface could be extended and get an evaluation_metric enum (flair_native, conll or even scikit-learn) ðŸ¤”
	</description>
	<comments>
		<comment id='1' author='stefan-it' date='2020-04-29T22:10:50Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>