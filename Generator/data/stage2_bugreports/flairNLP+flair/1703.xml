<bug id='1703' author='lucaventurini' open_date='2020-06-19T14:06:03Z' closed_time='2020-06-24T10:17:05Z'>
	<summary>Several errors with BERT embeddings in flair 0.5</summary>
	<description>
Hello,
I've been trying to replicate some experiments I did in the past with BERT embeddings on the new 0.5 release, without success.
The only update I did to the code was to replace BertEmbeddings with the new TransformerWordEmbeddings, as as I understood this is the recommended way to proceed. The embeddings I specify are 'bert-base-multilingual-cased', but I've been tried also with distilbert and gpt2.
In commit &lt;denchmark-link:https://github.com/flairNLP/flair/commit/63aeabf9a18bdf53af3bcba5bd80f43ac717656e&gt;63aeabf&lt;/denchmark-link&gt;
, I met the error specified in &lt;denchmark-link:https://github.com/flairNLP/flair/issues/1672&gt;#1672&lt;/denchmark-link&gt;
 , invalid shape. So I tried a more recent commit to see if this was solved.
Now I'm in &lt;denchmark-link:https://github.com/flairNLP/flair/commit/3bf14fd13c3833394c7283132148261f937c0a83&gt;3bf14fd&lt;/denchmark-link&gt;
, which fails due to the following error:
Traceback (most recent call last):
  File "train_flair.py", line 187, in &lt;module&gt;
    sys.exit(main(sys.argv[1:]))
  File "train_flair.py", line 182, in main
    trainer.train(args.outfolder, checkpoint=True, **hp)
  File "/opt/conda/lib/python3.6/site-packages/flair/trainers/trainer.py", line 364, in train
    loss = self.model.forward_loss(batch_step)
  File "/opt/conda/lib/python3.6/site-packages/flair/models/text_classification_model.py", line 146, in forward_loss
    scores = self.forward(data_points)
  File "/opt/conda/lib/python3.6/site-packages/flair/models/text_classification_model.py", line 100, in forward
    self.document_embeddings.embed(sentences)
  File "/opt/conda/lib/python3.6/site-packages/flair/embeddings/base.py", line 61, in embed
    self._add_embeddings_internal(sentences)
  File "/opt/conda/lib/python3.6/site-packages/flair/embeddings/document.py", line 369, in _add_embeddings_internal
    self.embeddings.embed(sentences)
  File "/opt/conda/lib/python3.6/site-packages/flair/embeddings/token.py", line 71, in embed
    embedding.embed(sentences)
  File "/opt/conda/lib/python3.6/site-packages/flair/embeddings/base.py", line 61, in embed
    self._add_embeddings_internal(sentences)
  File "/opt/conda/lib/python3.6/site-packages/flair/embeddings/token.py", line 892, in _add_embeddings_internal
    self._add_embeddings_to_sentences(batch)
  File "/opt/conda/lib/python3.6/site-packages/flair/embeddings/token.py", line 965, in _add_embeddings_to_sentences
    longest_sequence_in_batch: int = len(max(subtokenized_sentences, key=len))
ValueError: max() arg is an empty sequence
I thought it could be because of some empty sentence, but I always call corpus.filter_empty_sentences() before training, to be safe.
If I switch to turian embeddings, everything goes well. Is this a known issue?
	</description>
	<comments>
		<comment id='1' author='lucaventurini' date='2020-06-23T06:45:21Z'>
		&lt;denchmark-link:https://github.com/lucaventurini&gt;@lucaventurini&lt;/denchmark-link&gt;
 haven't seen this error yet. Could you provide a minimal code example to reproduce, for instance like embedding a single sentence that throws this error?
		</comment>
		<comment id='2' author='lucaventurini' date='2020-06-23T14:10:56Z'>
		Ok, it took me a while to find the offending sentence, but I think it has to due with newer emojis, like ðŸ¤Ÿ, and especially if the sentence is made only of these and does not contain any other token.
		</comment>
		<comment id='3' author='lucaventurini' date='2020-06-23T15:32:05Z'>
		Ah interesting, can you paste the sentence?
		</comment>
		<comment id='4' author='lucaventurini' date='2020-06-23T17:06:07Z'>
		Yes. It is just "ðŸ¤Ÿ".
		</comment>
		<comment id='5' author='lucaventurini' date='2020-06-24T10:17:34Z'>
		&lt;denchmark-link:https://github.com/lucaventurini&gt;@lucaventurini&lt;/denchmark-link&gt;
 I believe this is now fixed in master! Thanks for reporting the error!
		</comment>
		<comment id='6' author='lucaventurini' date='2020-06-25T07:30:05Z'>
		Thank you &lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 for the fix! What was the error exactly? BERT did not have an embedding for this token?
		</comment>
		<comment id='7' author='lucaventurini' date='2020-06-25T07:40:53Z'>
		Yes the BERT tokenizer ignores this token altogether, resulting in an empty sentence that was messing up the code. So with the fix now no error is thrown, but there is still no good embedding for the token. I think the RoBERTa tokenizer handles these things better!
		</comment>
		<comment id='8' author='lucaventurini' date='2020-06-25T15:28:08Z'>
		I don't know if I can reopen this, but I haven't solved my initial attempt, which was to replicate a training that was working in flair 0.4.
Now (I'm in commit
&lt;denchmark-link:https://github.com/flairNLP/flair/commit/ee19896ed978ed51178d9494e0b1fda9d895eb8d&gt;ee19896&lt;/denchmark-link&gt;
 ) the empty sequence error has disappeared, but I still have the errors of invalid shape that are described in &lt;denchmark-link:https://github.com/flairNLP/flair/issues/1672&gt;#1672&lt;/denchmark-link&gt;
 .
I don't know how to find the offending sentence this time, since simply calling embed on each of them works, differently from last time.
Also, I tried roberta with the same data and it works, so it's still a problem specific with BERT.
		</comment>
		<comment id='9' author='lucaventurini' date='2020-06-25T15:31:52Z'>
		p.s. I may add that probably I had this problem when I opened the issue as well, but since I shuffle sentences in the batch, I sometimes saw the empty sequence error, while now that that is solved I see always the invalid shape one.
		</comment>
		<comment id='10' author='lucaventurini' date='2020-06-26T15:30:10Z'>
		&lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 do you have any suggestions on a way to find the sentences that are giving this issue?
		</comment>
		<comment id='11' author='lucaventurini' date='2020-07-03T13:57:52Z'>
		&lt;denchmark-link:https://github.com/lucaventurini&gt;@lucaventurini&lt;/denchmark-link&gt;
 the tokenization mismatch error message prints the problematic sentence. Can you paste it here?
		</comment>
		<comment id='12' author='lucaventurini' date='2020-07-03T14:04:03Z'>
		Unfortunately I don't see this log message.
Here's the full log:
Traceback (most recent call last):
  File "train_flair.py", line 187, in &lt;module&gt;
    sys.exit(main(sys.argv[1:]))
  File "train_flair.py", line 182, in main
    trainer.train(args.outfolder, checkpoint=True, **hp)
  File "/opt/conda/lib/python3.6/site-packages/flair/trainers/trainer.py", line 364, in train
    loss = self.model.forward_loss(batch_step)
  File "/opt/conda/lib/python3.6/site-packages/flair/models/text_classification_model.py", line 146, in forward_loss
    scores = self.forward(data_points)
  File "/opt/conda/lib/python3.6/site-packages/flair/models/text_classification_model.py", line 100, in forward
    self.document_embeddings.embed(sentences)
  File "/opt/conda/lib/python3.6/site-packages/flair/embeddings/base.py", line 61, in embed
    self._add_embeddings_internal(sentences)
  File "/opt/conda/lib/python3.6/site-packages/flair/embeddings/document.py", line 397, in _add_embeddings_internal
    self.embeddings.embedding_length,
RuntimeError: shape '[8, 30, 3072]' is invalid for input of size 734208
		</comment>
		<comment id='13' author='lucaventurini' date='2020-07-03T17:28:08Z'>
		Since this happens during embedding, can you iterate through the sentences in your corpus and embed then one-by-one? Then we know which one throws the error.
		</comment>
		<comment id='14' author='lucaventurini' date='2020-07-03T17:35:24Z'>
		Unfortunately not. As I said some messages above, doing this does not result in any error.
		</comment>
		<comment id='15' author='lucaventurini' date='2020-07-03T19:36:05Z'>
		Ah hm, maybe this somehow happens due to mini-batching. Could you try embedding batch by batch to see if there are mini-batches that throw this error?
		</comment>
		<comment id='16' author='lucaventurini' date='2020-07-03T19:58:23Z'>
		I tried also this, but nothing, it seems it appears only during the training of a model.
		</comment>
		<comment id='17' author='lucaventurini' date='2020-07-03T20:19:54Z'>
		Can you give me a sample of the data to reproduce, or is it proprietary?
		</comment>
		<comment id='18' author='lucaventurini' date='2020-07-04T10:18:11Z'>
		It's proprietary, sorry.
I see that &lt;denchmark-link:https://github.com/flairNLP/flair/issues/1724&gt;#1724&lt;/denchmark-link&gt;
 is quite similar in syntomps : no mismatch log error, embedding single sentences works, training a model does not. So it might be the same problem, as soon as I get back to the data I'll check if I have the same characters as well. In the mean time, you might be more lucky trying to reproduce &lt;denchmark-link:https://github.com/flairNLP/flair/issues/1724&gt;#1724&lt;/denchmark-link&gt;
 maybe?
		</comment>
		<comment id='19' author='lucaventurini' date='2020-07-24T09:19:05Z'>
		Hi &lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 ,
I have tried v0.5.1 and finally the training completed without errors!
I still have some issues in using the classifier for predictions, though:
    classifier_loaded.predict(sent, multi_class_prob=True, mini_batch_size=1)
  File "/opt/conda/lib/python3.6/site-packages/flair/models/text_classification_model.py", line 222, in predict
    scores = self.forward(batch)
  File "/opt/conda/lib/python3.6/site-packages/flair/models/text_classification_model.py", line 99, in forward
    self.document_embeddings.embed(sentences)
  File "/opt/conda/lib/python3.6/site-packages/flair/embeddings/base.py", line 61, in embed
    self._add_embeddings_internal(sentences)
  File "/opt/conda/lib/python3.6/site-packages/flair/embeddings/document.py", line 374, in _add_embeddings_internal
    self.embeddings.embed(sentences)
  File "/opt/conda/lib/python3.6/site-packages/flair/embeddings/token.py", line 71, in embed
    embedding.embed(sentences)
  File "/opt/conda/lib/python3.6/site-packages/flair/embeddings/base.py", line 61, in embed
    self._add_embeddings_internal(sentences)
  File "/opt/conda/lib/python3.6/site-packages/flair/embeddings/token.py", line 897, in _add_embeddings_internal
    self._add_embeddings_to_sentences(batch)
  File "/opt/conda/lib/python3.6/site-packages/flair/embeddings/token.py", line 1053, in _add_embeddings_to_sentences
    final_embedding: torch.FloatTensor = current_embeddings[0]
IndexError: index 0 is out of bounds for dimension 0 with size 0
It seems to me that this is still related to the "missing embedding" issue of this thread.
		</comment>
		<comment id='20' author='lucaventurini' date='2020-08-21T13:15:47Z'>
		&lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 I finally have no more issues with this, at least in commit &lt;denchmark-link:https://github.com/flairNLP/flair/commit/040d386a9660c329148464c2dd3c3a6f6f8edad2&gt;040d386&lt;/denchmark-link&gt;
 .
I so managed to reproduce some experiments I did in the past in v0.4 (as described in the first message above) and noticed that the training is somewhat slower, maybe due to the use of transformers? The time spent on each epoch has passed from 8 minutes to 11, most of these 3 minutes extra seem spent in the final evaluation on the dev set. I don't know if you have noticed already similar behaviors.
		</comment>
		<comment id='21' author='lucaventurini' date='2020-08-24T07:03:59Z'>
		&lt;denchmark-link:https://github.com/lucaventurini&gt;@lucaventurini&lt;/denchmark-link&gt;
 interesting - are you using the same transformer embeddings as before? Can you share a minimal script to reproduce?
		</comment>
		<comment id='22' author='lucaventurini' date='2020-08-24T14:25:14Z'>
		I don't have a MWE to share, right now.
I'm using 'bert-base-multilingual-cased' for this example, which in v0.4 I used to load with BertEmbeddings, and now with TransformerWordEmbeddings, which I thought would be equivalent.
		</comment>
	</comments>
</bug>