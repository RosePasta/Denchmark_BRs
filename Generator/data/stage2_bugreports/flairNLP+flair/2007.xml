<bug id='2007' author='albertovilla' open_date='2020-12-05T11:01:15Z' closed_time='2020-12-06T08:48:44Z'>
	<summary>Bug with local downloaded TransformerWordEmbeddings</summary>
	<description>
Describe the bug
I've developed a NER tagger locally in my computer using Flair. Now, I need to deploy it to a VM where there is no internet connection so I have downloaded the required embeddings:

Flair embeddings: news-forward and news-backward
TransformerWordEmbedding: distilbert-base-uncased

When I create the model with internet connection i.e. by defining the embeddings just with the name e.g. TransformerWordEmbeddings('distilbert-base-uncased') everything works as expected, however, when I download the model and try to train it I get an error RuntimeError: CUDA error: device-side assert triggered.
I have managed to train a model locally using downloaded Flair embeddings, but whenever I add a TransformerWordEmbedding I get CUDA errors and the model breaks during training, always during the first epoch the not necessarily in the first iteration.
To Reproduce
As I have isolated the problem to the TransformerWordEmbedding I have defined the following test to isolate the bug.
&lt;denchmark-h:h4&gt;1) Define a model that works assuming there is internet connection&lt;/denchmark-h&gt;

model_2 = { 'embedding_types': [FlairEmbeddings('news-forward'),
                                FlairEmbeddings('news-backward'),
                                TransformerWordEmbeddings('distilbert-base-uncased')
                               ],
            'name': 'distilbert-ner',
            # Model folder
            'folder': 'resources/taggers/distilbert-ner',
            # Hyperparameters
            'hidden_size': 768,
            'rnn_layers': 2,
            'learning_rate': 0.1,
            'mini_batch_size': 32,
            'max_epochs': 50
          }
This model trains properly without any problem using the following code:
# Load embeddings
embedding_types : List[TokenEmbeddings] = model_2['embedding_types']
embeddings : StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)
    
# Create model tagger
tagger : SequenceTagger = SequenceTagger(hidden_size=model_2['hidden_size'],
                                         rnn_layers=model_2['rnn_layers'],
                                         embeddings=embeddings,
                                         tag_dictionary=tag_dictionary,
                                         tag_type=tag_type,
                                         use_crf=True)
    
# Create model trainer
trainer : ModelTrainer = ModelTrainer(tagger, corpus)

trainer.train(model_2['folder'],
              learning_rate=model_2['learning_rate'],
              mini_batch_size=model_2['mini_batch_size'],
              max_epochs=model_2['max_epochs'])
&lt;denchmark-h:h4&gt;2) Define a model that breaks loading a TransformerWordEmbedding locally&lt;/denchmark-h&gt;

I have downloaded the  model from &lt;denchmark-link:https://huggingface.co/distilbert-base-uncased/tree/main&gt;https://huggingface.co/distilbert-base-uncased/tree/main&lt;/denchmark-link&gt;
 and placed the files in a local folder . Then I create the model using the same code than with the first model but enforcing local load.
DISTILBERT = 'resources/embeddings/token/distilbert'

# Testing with local model
model_2 = { 'embedding_types': [FlairEmbeddings('news-forward'),
                                FlairEmbeddings('news-backward'),
                                TransformerWordEmbeddings(DISTILBERT)
                               ],
            'name': 'distilbert-ner',
            # Model folder
            'folder': 'resources/taggers/distilbert-ner',
            # Hyperparameters
            'hidden_size': 768,
            'rnn_layers': 2,
            'learning_rate': 0.1,
            'mini_batch_size': 32,
            'max_epochs': 50
          }
The above code executes without any error or warning but when I use the code for the training, exactly the same than for the first model. I get an error during epoch 1.
&lt;denchmark-code&gt;2020-12-05 11:32:49,298 ----------------------------------------------------------------------------------------------------
2020-12-05 11:32:49,300 Corpus: "Corpus: 5323 train + 592 dev + 1972 test sentences"
2020-12-05 11:32:49,300 ----------------------------------------------------------------------------------------------------
2020-12-05 11:32:49,301 Parameters:
2020-12-05 11:32:49,302  - learning_rate: "0.1"
2020-12-05 11:32:49,302  - mini_batch_size: "32"
2020-12-05 11:32:49,303  - patience: "3"
2020-12-05 11:32:49,304  - anneal_factor: "0.5"
2020-12-05 11:32:49,304  - max_epochs: "50"
2020-12-05 11:32:49,305  - shuffle: "True"
2020-12-05 11:32:49,306  - train_with_dev: "False"
2020-12-05 11:32:49,306  - batch_growth_annealing: "False"
2020-12-05 11:32:49,307 ----------------------------------------------------------------------------------------------------
2020-12-05 11:32:49,308 Model training base path: "resources\taggers\distilbert-ner"
2020-12-05 11:32:49,309 ----------------------------------------------------------------------------------------------------
2020-12-05 11:32:49,309 Device: cuda:0
2020-12-05 11:32:49,310 ----------------------------------------------------------------------------------------------------
2020-12-05 11:32:49,311 Embeddings storage mode: cpu
2020-12-05 11:32:49,314 ----------------------------------------------------------------------------------------------------
2020-12-05 11:33:08,907 epoch 1 - iter 16/167 - loss 13.00996539 - samples/sec: 26.14 - lr: 0.100000
2020-12-05 11:33:29,829 epoch 1 - iter 32/167 - loss 10.77191505 - samples/sec: 24.47 - lr: 0.100000
2020-12-05 11:33:49,754 epoch 1 - iter 48/167 - loss 9.50294068 - samples/sec: 25.70 - lr: 0.100000
2020-12-05 11:34:10,028 epoch 1 - iter 64/167 - loss 9.12441025 - samples/sec: 25.26 - lr: 0.100000
2020-12-05 11:34:32,542 epoch 1 - iter 80/167 - loss 8.85962501 - samples/sec: 22.74 - lr: 0.100000
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-11-66efbade56b8&gt; in &lt;module&gt;()
     17               learning_rate=model_2['learning_rate'],
     18               mini_batch_size=model_2['mini_batch_size'],
---&gt; 19               max_epochs=model_2['max_epochs'])

Z:\anaconda\envs\dc_ner\lib\site-packages\flair\trainers\trainer.py in train(self, base_path, learning_rate, mini_batch_size, mini_batch_chunk_size, max_epochs, scheduler, cycle_momentum, anneal_factor, patience, initial_extra_patience, min_learning_rate, train_with_dev, monitor_train, monitor_test, embeddings_storage_mode, checkpoint, save_final_model, anneal_with_restarts, anneal_with_prestarts, batch_growth_annealing, shuffle, param_selection_mode, write_weights, num_workers, sampler, use_amp, amp_opt_level, eval_on_train_fraction, eval_on_train_shuffle, **kwargs)
    369 
    370                         # forward pass
--&gt; 371                         loss = self.model.forward_loss(batch_step)
    372 
    373                         # Backward

Z:\anaconda\envs\dc_ner\lib\site-packages\flair\models\sequence_tagger_model.py in forward_loss(self, data_points, sort)
    617             self, data_points: Union[List[Sentence], Sentence], sort=True
    618     ) -&gt; torch.tensor:
--&gt; 619         features = self.forward(data_points)
    620         return self._calculate_loss(features, data_points)
    621 

Z:\anaconda\envs\dc_ner\lib\site-packages\flair\models\sequence_tagger_model.py in forward(self, sentences)
    622     def forward(self, sentences: List[Sentence]):
    623 
--&gt; 624         self.embeddings.embed(sentences)
    625 
    626         names = self.embeddings.get_names()

Z:\anaconda\envs\dc_ner\lib\site-packages\flair\embeddings\token.py in embed(self, sentences, static_embeddings)
     69 
     70         for embedding in self.embeddings:
---&gt; 71             embedding.embed(sentences)
     72 
     73     @property

Z:\anaconda\envs\dc_ner\lib\site-packages\flair\embeddings\base.py in embed(self, sentences)
     58 
     59         if not everything_embedded or not self.static_embeddings:
---&gt; 60             self._add_embeddings_internal(sentences)
     61 
     62         return sentences

Z:\anaconda\envs\dc_ner\lib\site-packages\flair\embeddings\token.py in _add_embeddings_internal(self, sentences)
    857         # embed each micro-batch
    858         for batch in sentence_batches:
--&gt; 859             self._add_embeddings_to_sentences(batch)
    860 
    861         return sentences

Z:\anaconda\envs\dc_ner\lib\site-packages\flair\embeddings\token.py in _add_embeddings_to_sentences(self, sentences)
    965         for s_id, sentence in enumerate(subtokenized_sentences):
    966             sequence_length = len(sentence)
--&gt; 967             input_ids[s_id][:sequence_length] = sentence
    968             mask[s_id][:sequence_length] = torch.ones(sequence_length)
    969 

RuntimeError: CUDA error: device-side assert triggered
&lt;/denchmark-code&gt;

Note that this problem doesn't happen with local Flair embeddings, so I can create a model which uses local news-forward and news-backward Flair embeddings and the model trains without any issue so I guess there must be something different when the model is loaded locally compared when Flair automatically downloads the files and loads them from the cache in .cache\torch\transformers.
Expected behavior
I think both models should work exactly the same but for some reason this is not the case.
Environment (please complete the following information):

Windows 10
Version flair 0.6.1.post1
GPU GIGABYTE GeForce GTX 1060 6GB GDDR5

I have tested this also in a VM using Windows Server 2016 with a Tesla V100 but I rather try to find a solution in my computer before trying again in AWS.
	</description>
	<comments>
		<comment id='1' author='albertovilla' date='2020-12-05T15:48:37Z'>
		Hi &lt;denchmark-link:https://github.com/albertovilla&gt;@albertovilla&lt;/denchmark-link&gt;
, it can be done as the documentation suggests:
&lt;denchmark-code&gt;from transformers import AutoTokenizer, AutoConfig, AutoModel
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
config = AutoConfig.from_pretrained("distilbert-base-uncased", output_hidden_states=True)
model = AutoModel.from_pretrained("distilbert-base-uncased", config=config)
&lt;/denchmark-code&gt;

Then store it to memory:
&lt;denchmark-code&gt;tokenizer.save_pretrained('./local_model_directory/')
model.save_pretrained('./local_model_directory/')
&lt;/denchmark-code&gt;

Then load it via TransformerWordEmbeddings:
&lt;denchmark-code&gt;from flair.embeddings import TransformerWordEmbeddings
local_model = TransformerWordEmbeddings("./local_model_directory/")
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='albertovilla' date='2020-12-06T08:48:44Z'>
		Thanks a lot &lt;denchmark-link:https://github.com/whoisjones&gt;@whoisjones&lt;/denchmark-link&gt;
 it works.
		</comment>
	</comments>
</bug>