<bug id='1603' author='sankaran45' open_date='2020-05-11T17:35:24Z' closed_time='2020-05-11T17:59:25Z'>
	<summary>Strange behavior with PooledFlairEmbeddings ???</summary>
	<description>
I am running a simple NER experiment with two labels with the default settings except for train_with_dev=True, max_epochs=150. I want to analyze the F1 scores obtained at 50, 100, 150 epochs respectively. So instead of stopping the training at these intervals and resuming i decided to run it for 150 epochs but just save the checkpoints at 50 and 100 by modifying trainer.py as its little easier this way in colab. Later when test is over, i finalize these checkpoint files using trainer.train which gives the predictions.
This works fine for Bert, Elmo, Flair embeddings as given below. For PooledFlair (pooling='mean' / pooling='min') the results are quite erratic although by 150 epochs it seems to catch up. Why is it the case ? Or what am i doing wrong ?
In the below, 2nd column is embedding, 3rd is epoch, and 4th is F1 score reported by trainer upon testing the best model at that time:
4	bert	50	86.61
5	bert	100	87.09666667
6	bert	150	87.32666667
11	elmo	50	87.205
12	elmo	100	87.295
13	elmo	150	87.08
14	flair	50	87.58
15	flair	100	88.14
16	flair	150	87.45
21	pf	50	74.7
22	pf	100	78.94333333
23	pf	150	87.62333333
	</description>
	<comments>
	</comments>
</bug>