<bug id='600' author='davechallis' open_date='2019-03-08T10:17:31Z' closed_time='2020-05-07T03:15:45Z'>
	<summary>Loading embeddings fails when using futures and process pool in 0.4.1</summary>
	<description>
Describe the bug
When attempting to load Flair embeddings in multiple processes (using concurrent.futures.ProcessPoolExecutor), it worked fine in 0.4.0, but fails with errors in 0.4.1.
To Reproduce
Sample script:
&lt;denchmark-code&gt;from concurrent import futures
import flair
from flair.embeddings import FlairEmbeddings

def load_embeddings():
    return str(FlairEmbeddings('news-forward-fast'))

with futures.ProcessPoolExecutor(max_workers=1) as executor:
    all_futures = [executor.submit(load_embeddings) for _ in range(2)]

    for future in futures.as_completed(all_futures):
        print(future.result())
&lt;/denchmark-code&gt;

Expected behavior
In 0.4.0, this prints the path to the loaded model twice, e.g.:
&lt;denchmark-code&gt;/home/dsc/.flair/embeddings/lm-news-english-forward-1024-v0.2rc.pt
/home/dsc/.flair/embeddings/lm-news-english-forward-1024-v0.2rc.pt
&lt;/denchmark-code&gt;

In 0.4.1 this instead results in the following error:
&lt;denchmark-code&gt;THCudaCheck FAIL file=/pytorch/torch/csrc/cuda/Module.cpp line=53 error=3 : initialization error
concurrent.futures.process._RemoteTraceback:
"""
Traceback (most recent call last):
  File "/home/dsc/miniconda3/envs/cc/lib/python3.7/concurrent/futures/process.py", line 232, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/tmp/t.py", line 8, in load_embeddings
    emb = FlairEmbeddings('news-forward-fast')
  File "/home/dsc/miniconda3/envs/cc/lib/python3.7/site-packages/flair/embeddings.py", line 835, in __init__
    self.lm = LanguageModel.load_language_model(model)
  File "/home/dsc/miniconda3/envs/cc/lib/python3.7/site-packages/flair/models/language_model.py", line 152, in load_language_model
    state = torch.load(str(model_file), map_location=flair.device)
  File "/home/dsc/miniconda3/envs/cc/lib/python3.7/site-packages/torch/serialization.py", line 367, in load
    return _load(f, map_location, pickle_module)
  File "/home/dsc/miniconda3/envs/cc/lib/python3.7/site-packages/torch/serialization.py", line 538, in _load
    result = unpickler.load()
  File "/home/dsc/miniconda3/envs/cc/lib/python3.7/site-packages/torch/serialization.py", line 504, in persistent_load
    data_type(size), location)
  File "/home/dsc/miniconda3/envs/cc/lib/python3.7/site-packages/torch/serialization.py", line 387, in restore_location
    return default_restore_location(storage, str(map_location))
  File "/home/dsc/miniconda3/envs/cc/lib/python3.7/site-packages/torch/serialization.py", line 113, in default_restore_location
    result = fn(storage, location)
  File "/home/dsc/miniconda3/envs/cc/lib/python3.7/site-packages/torch/serialization.py", line 95, in _cuda_deserialize
    return obj.cuda(device)
  File "/home/dsc/miniconda3/envs/cc/lib/python3.7/site-packages/torch/_utils.py", line 68, in _cuda
    with torch.cuda.device(device):
  File "/home/dsc/miniconda3/envs/cc/lib/python3.7/site-packages/torch/cuda/__init__.py", line 226, in __enter__
    self.prev_idx = torch._C._cuda_getDevice()
RuntimeError: cuda runtime error (3) : initialization error at /pytorch/torch/csrc/cuda/Module.cpp:53
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/tmp/t.py", line 15, in &lt;module&gt;
    print(future.result())
  File "/home/dsc/miniconda3/envs/cc/lib/python3.7/concurrent/futures/_base.py", line 425, in result
    return self.__get_result()
  File "/home/dsc/miniconda3/envs/cc/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
RuntimeError: cuda runtime error (3) : initialization error at /pytorch/torch/csrc/cuda/Module.cpp:53
&lt;/denchmark-code&gt;

Environment (please complete the following information):

Ubuntu linux 18.04
Flair 0.4.1

	</description>
	<comments>
		<comment id='1' author='davechallis' date='2020-04-30T02:53:31Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>