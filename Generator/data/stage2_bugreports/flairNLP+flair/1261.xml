<bug id='1261' author='0x7A31C7' open_date='2019-11-04T20:12:00Z' closed_time='2020-05-06T19:33:00Z'>
	<summary>XLNet and BERT embedding sizes are bigger than the values mentioned in the original papers</summary>
	<description>
&lt;denchmark-h:h3&gt;Description&lt;/denchmark-h&gt;

In Flair, the embedding sizes for XLNet and BERT models are 2 or 4 times bigger than the values mentioned in the original papers.
The &lt;denchmark-link:https://github.com/zihangdai/xlnet#released-models&gt;xlnet repo&lt;/denchmark-link&gt;
 mentions the following sizes:
: -hidden
: -hidden
The &lt;denchmark-link:https://github.com/google-research/bert#bert&gt;bert repo&lt;/denchmark-link&gt;
 mentions a hidden vector size of  for .
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

from flair.embeddings import XLNetEmbeddings, BertEmbeddings

xlnet_large = XLNetEmbeddings('xlnet-large-cased')
xlnet_base = XLNetEmbeddings('xlnet-base-cased')
bert = BertEmbeddings('bert-large-uncased')

print(f'XLNet large: {xlnet_large.embedding_length}') # Output: 2048
print(f'XLNet base: {xlnet_base.embedding_length}') # Output: 1536
print(f'BERT: {bert.embedding_length}') # Output: 4096
&lt;denchmark-h:h3&gt;Environment Info&lt;/denchmark-h&gt;


Ubuntu
Python 3.7
Flair 0.4.3

	</description>
	<comments>
		<comment id='1' author='0x7A31C7' date='2019-11-04T21:42:40Z'>
		Hi &lt;denchmark-link:https://github.com/0x7A31C7&gt;@0x7A31C7&lt;/denchmark-link&gt;
 ,
the returned sizes (dimensions) mainly depend on two parameters: a) number of used layers and b) pooling operation.
Almost all Transformer-based architectures rely on subword tokenization. E.g. the original token "Johanson" could be splitted into two subtokens "johan" and "##son". The BERT paper uses the first subword embedding (resulting in a dimension size of 1024) and the last four layers of the model. The last four layers are concatenated, so there's a final dimension size of 4 (number of layers) * 1024 = 4096.
For the XLNet paper did not mention any evaluation on NER tasks. I made some experiments and it turns out, that using the first and last subword embedding leads to better results (compared to using the first subword only). First and last subword means a dimension size of 2 * 768 = 1536 for the base model and 2 * 1024 = 2048 for the large XLNet model.
I hope this helps :)
		</comment>
		<comment id='2' author='0x7A31C7' date='2020-04-29T18:10:55Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>