<bug id='1012' author='pommedeterresautee' open_date='2019-08-21T12:51:38Z' closed_time='2019-08-21T16:28:11Z'>
	<summary>NER - Slow predictions on master branch compared to version 0.4.2</summary>
	<description>
Describe the bug
With the very same code, and the very same data, Inference is much more slower on NER task when I use the version on master branch compared to the released version 0.4.2 (from 84 seconds to 234 seconds).
I have tried storing method None, cpu and gpu (btw documentation is missing, in particular none option is not clear to me even when reading the code)
FYI training the model doesn't take more time on master branch.
nvidia-smi tells me that during inference the GPU is used at 10-12% when using master, and at 25 to 30% when using 0.4.2
To Reproduce
#  ...
    corpus: Corpus = prepare_flair_train_test_corpus(spacy_model=nlp, data_folder=data_folder, dev_size=dev_size)

    model_path = os.path.join(model_folder, 'best-model.pt')
    tagger: SequenceTagger = SequenceTagger.load(model_path)

    test_results, _ = tagger.evaluate(DataLoader(corpus.test, batch_size=50, num_workers=10))
    print(test_results.detailed_results)

    sentences_predict: List[Sentence] = corpus.train.sentences + corpus.test.sentences

    start = time.time()
    _ = tagger.predict(sentences_predict, mini_batch_size=50, verbose=True, embedding_storage_mode="gpu")
    print(time.time() - start)
    # 234 seconds on when using master branch
The training of the model looks like this:
   # ...
    embedding_types: List[TokenEmbeddings] = [
        WordEmbeddings('fr'),
        FlairEmbeddings('resources/flair_ner/lm/ca_forward/best-lm.pt'),
        FlairEmbeddings('resources/flair_ner/lm/ca_backward/best-lm.pt'),
    ]

    embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)

    tagger: SequenceTagger = SequenceTagger(hidden_size=256,
                                            embeddings=embeddings,
                                            tag_dictionary=tag_dictionary,
                                            tag_type='ner')
   # ...
Here it is a custom LM, but fr model from Zalendo has the same behaviour.
Expected behavior
I expect the time to process the same data with the same NER model to be the same than with master and 0.4.2.
Environment (please complete the following information):
Ubuntu 19.04
Flair master VS Flair 0.4.2
GPU 2080TI
	</description>
	<comments>
		<comment id='1' author='pommedeterresautee' date='2019-08-21T13:00:35Z'>
		Oha thanks for spotting this. Definitely something we need to investigate!
		</comment>
		<comment id='2' author='pommedeterresautee' date='2019-08-21T15:37:12Z'>
		Again thanks for spotting this. I was able to reproduce and find the source of the problem. The new version does not only return the most likely tag but also a probability distribution over all tags in case you want to know what the second best guess was etc. However, it seems the implementation of this feature imposes a CPU bottleneck, hence the slowdown and reduced GPU usage.
I'll push a PR that adds an optional flag to the predict() method that if not set will not do this distribution. evaluate() by default will not return the distribution. This speeds everything back up. Here the numbers of a v100:
Task - evaluating CoNLL-03 'ner' model
Release 0.4.2:   ~30 seconds
Current master: ~40 seconds
After fix:             ~20 seconds
Task - predict on all sentences in CoNLL-03 wirh 'ner' model
Release 0.4.2:   ~100 seconds
Current master: ~190 seconds
After fix:             ~85 seconds
		</comment>
		<comment id='3' author='pommedeterresautee' date='2019-08-21T16:58:06Z'>
		&lt;denchmark-link:https://github.com/pommedeterresautee&gt;@pommedeterresautee&lt;/denchmark-link&gt;
 just pushed the PR to master. Could you check if this works for you?
		</comment>
		<comment id='4' author='pommedeterresautee' date='2019-08-22T06:21:36Z'>
		I confirm that master is even more rapid than 0.4.2.
Thanks for the rapid fix!
		</comment>
	</comments>
</bug>