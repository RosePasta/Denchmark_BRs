<bug id='1302' author='PradyumnaGupta' open_date='2019-11-29T14:57:05Z' closed_time='2020-09-17T19:44:35Z'>
	<summary>Are the embedding models trainable or not ?</summary>
	<description>
According to other issues that I read regarding whether the transformer embedding models available are trainable or not, it has been informed that the transformer architectures like bert,xlnet etc. are not trainable. But if I print the number of parameters with requires_grad=True corresponding to the tagger model with BertEmbeddings then 125757484 parameters show up? Please explain this !!
&lt;denchmark-code&gt;from flair.data import Corpus
from flair.datasets import WNUT_17
from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings,BertEmbeddings
from typing import List

corpus: Corpus = WNUT_17().downsample(0.1)
tag_type = 'ner'
tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)
embedding_types: List[TokenEmbeddings] = [
    BertEmbeddings(),
]

embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)


from flair.models import SequenceTagger

tagger: SequenceTagger = SequenceTagger(hidden_size=256,
                                        embeddings=embeddings,
                                        tag_dictionary=tag_dictionary,
                                        tag_type=tag_type,
                                        use_crf=True)

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)
count_parameters(tagger)
&lt;/denchmark-code&gt;

Output-
125757484
Environment:
Google Colab
	</description>
	<comments>
		<comment id='1' author='PradyumnaGupta' date='2020-01-05T13:07:26Z'>
		In methods _add_embeddings_internal there is with torch.no_grad():, so it is stated dynamically.
The trainable parameters count method is not reliable in this case.
		</comment>
		<comment id='2' author='PradyumnaGupta' date='2020-02-20T17:49:35Z'>
		I was also trying to estimate the number of trainable parameters, and I lost some time trying to understand the huge value I was getting. Then I found this issue.
Is the info about this dynamically set no_grad documented somewhere?
And is there any reliable way to calculate the number of trainable parameters?
		</comment>
		<comment id='3' author='PradyumnaGupta' date='2020-05-12T17:45:20Z'>
		&lt;denchmark-link:https://github.com/djstrong&gt;@djstrong&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/falcaopetri&gt;@falcaopetri&lt;/denchmark-link&gt;
  I also need to estimate the parameters only of the sequence tagger - how to do it? I could not understand how to do it reading this discussion.
		</comment>
		<comment id='4' author='PradyumnaGupta' date='2020-05-13T12:48:04Z'>
		&lt;denchmark-link:https://github.com/teoML&gt;@teoML&lt;/denchmark-link&gt;
, if I remember correctly, I ended up using &lt;denchmark-link:https://pytorch.org/docs/stable/nn.html#torch.nn.Module.named_parameters&gt;model.named_parameters()&lt;/denchmark-link&gt;
 to print the name and the size of each parameter/layer. I didn't need to know exactly the number of parameters, so I did a manual check and added up only those parameters that where probably trainable.
Probably something like this (I haven't checked this snippet), and then manually adding some of the param sizes.
for name, param in model.named_parameters():
  if param.requires_grad:
    print(name, param.numel())
But I agree that there should be a straightforward way to get the correct number of trainable parameters.
		</comment>
		<comment id='5' author='PradyumnaGupta' date='2020-05-13T13:03:37Z'>
		&lt;denchmark-link:https://github.com/falcaopetri&gt;@falcaopetri&lt;/denchmark-link&gt;
 That's the easiest solution.
&lt;denchmark-link:https://github.com/teoML&gt;@teoML&lt;/denchmark-link&gt;
 You can count all parameters for SequenceTagger and subtract all parameters of Embeddings.
		</comment>
		<comment id='6' author='PradyumnaGupta' date='2020-05-13T17:43:30Z'>
		&lt;denchmark-link:https://github.com/djstrong&gt;@djstrong&lt;/denchmark-link&gt;
  how to count all the parameters for SequenceTagger ? Do you mean manually?
		</comment>
		<comment id='7' author='PradyumnaGupta' date='2020-05-13T19:33:18Z'>
		No
		</comment>
		<comment id='8' author='PradyumnaGupta' date='2020-09-10T19:35:27Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>