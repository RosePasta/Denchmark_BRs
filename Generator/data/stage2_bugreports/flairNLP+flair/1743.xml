<bug id='1743' author='plc-dev' open_date='2020-07-06T09:02:27Z' closed_time='2020-07-06T14:24:55Z'>
	<summary>AttributeError: 'BertTokenizer' object has no attribute 'unique_no_split_tokens'</summary>
	<description>
Describe the bug
After successfully training a NER model on Colab, loading the model again for inference in the same environment results in the following error, when calling model.evaluate().
&lt;denchmark-code&gt;---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-5-56e09b327a86&gt; in &lt;module&gt;
----&gt; 1 result, main_score = model.evaluate(corpus.test, out_path = f"output.txt")
      2 
      3 print(main_score)
      4 print(result.detailed_results)

~/.local/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py in evaluate(self, sentences, out_path, embedding_storage_mode, mini_batch_size, num_workers)
    515         # if span F1 needs to be used, use separate eval method
    516         if self._requires_span_F1_evaluation():
--&gt; 517             return self._evaluate_with_span_F1(data_loader, embedding_storage_mode, mini_batch_size, out_path)
    518 
    519         # else, use scikit-learn to evaluate

~/.local/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py in _evaluate_with_span_F1(self, data_loader, embedding_storage_mode, mini_batch_size, out_path)
    417                                 mini_batch_size=mini_batch_size,
    418                                 label_name='predicted',
--&gt; 419                                 return_loss=True)
    420             eval_loss += loss
    421             batch_no += 1

~/.local/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py in predict(self, sentences, mini_batch_size, all_tag_prob, verbose, label_name, return_loss, embedding_storage_mode)
    364                     continue
    365 
--&gt; 366                 feature = self.forward(batch)
    367 
    368                 if return_loss:

~/.local/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py in forward(self, sentences)
    602     def forward(self, sentences: List[Sentence]):
    603 
--&gt; 604         self.embeddings.embed(sentences)
    605 
    606         names = self.embeddings.get_names()

~/.local/lib/python3.6/site-packages/flair/embeddings/token.py in embed(self, sentences, static_embeddings)
     69 
     70         for embedding in self.embeddings:
---&gt; 71             embedding.embed(sentences)
     72 
     73     @property

~/.local/lib/python3.6/site-packages/flair/embeddings/base.py in embed(self, sentences)
     59 
     60         if not everything_embedded or not self.static_embeddings:
---&gt; 61             self._add_embeddings_internal(sentences)
     62 
     63         return sentences

~/.local/lib/python3.6/site-packages/flair/embeddings/legacy.py in _add_embeddings_internal(self, sentences)
   1195                 [
   1196                     self.tokenizer.tokenize(sentence.to_tokenized_string())
-&gt; 1197                     for sentence in sentences
   1198                 ],
   1199                 key=len,

~/.local/lib/python3.6/site-packages/flair/embeddings/legacy.py in &lt;listcomp&gt;(.0)
   1195                 [
   1196                     self.tokenizer.tokenize(sentence.to_tokenized_string())
-&gt; 1197                     for sentence in sentences
   1198                 ],
   1199                 key=len,

~/.local/lib/python3.6/site-packages/transformers/tokenization_utils.py in tokenize(self, text, **kwargs)
    361             )
    362 
--&gt; 363         no_split_token = self.unique_no_split_tokens
    364         tokenized_text = split_on_tokens(no_split_token, text)
    365         return tokenized_text

AttributeError: 'BertTokenizer' object has no attribute 'unique_no_split_tokens'

&lt;/denchmark-code&gt;

To Reproduce
Modell: "bert-base-multilingual-cased" finetuned on a NER-task
Dataset: Data is not publicly disclosable
Minimal script to reproduce:
&lt;denchmark-code&gt;from flair.data import Sentence
from flair.data import Corpus
from flair.embeddings import BertEmbeddings, StackedEmbeddings, TokenEmbeddings
from flair.models import SequenceTagger
from flair.trainers import ModelTrainer
from flair.datasets import ColumnCorpus
from flair.datasets import DataLoader

embedding_types: List[TokenEmbeddings] = [
    BertEmbeddings('bert-base-multilingual-cased')
]
columns = {0: 'text', 1: 'ner'}
tag_type = columns[1]
corpus: Corpus = ColumnCorpus('data',  columns, train_file='train.txt', in_memory=False)
tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)

embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)

tagger: SequenceTagger = SequenceTagger(hidden_size=512,
                                        embeddings=embeddings,
                                        tag_dictionary=tag_dictionary,
                                        tag_type=tag_type,
                                        use_crf=True)
trainer.train('model/',
              learning_rate=0.001,
              mini_batch_size=32,
              patience=2,
              anneal_factor=0.4,
              max_epochs=30,
              checkpoint=True,
              embeddings_storage_mode='None')

model = SequenceTagger.load('model/final-model.pt')
columns = {0: 'text', 1: 'ner'}
corpus: Corpus = ColumnCorpus('data/', columns,
                                train_file='train.txt',
                                test_file='test.txt',
                                dev_file='dev.txt')

result, main_score = model.evaluate(corpus.test, out_path = f"output.txt")
&lt;/denchmark-code&gt;

Expected behavior
Being able to use the trained model for inference.
Environment (please complete the following information):
Environment: Google Colab
GPU: Tesla P100
Versions:

Flair 0.5.1
torch 1.5.1+cu101
transformers 3.0.1

Additional context
A similar issue occured earlier, when resuming training from a checkpoint, which resulted in the same error when loading the model with the ModelTrainer.load() method. I was able to somewhat work around that error by resuming from an earlier checkpoint.
: Might correlate with &lt;denchmark-link:https://github.com/flairNLP/flair/issues/1741&gt;#1741&lt;/denchmark-link&gt;

: Correct paths in example code to match
	</description>
	<comments>
		<comment id='1' author='plc-dev' date='2020-07-06T13:37:37Z'>
		Hello &lt;denchmark-link:https://github.com/plc-dev&gt;@plc-dev&lt;/denchmark-link&gt;
 - I just tried the following code in CoLab and it works:
from flair.data import Sentence
from flair.data import Corpus
from flair.embeddings import BertEmbeddings, StackedEmbeddings, TokenEmbeddings
from flair.models import SequenceTagger
from flair.trainers import ModelTrainer
from flair.datasets import ColumnCorpus
from flair.datasets import DataLoader, WNUT_17

embedding_types = [
    BertEmbeddings('bert-base-multilingual-cased')
]
corpus: Corpus = WNUT_17()
tag_type = 'ner'
tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)

embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)

tagger: SequenceTagger = SequenceTagger(hidden_size=512,
                                        embeddings=embeddings,
                                        tag_dictionary=tag_dictionary,
                                        tag_type=tag_type,
                                        use_crf=True)

trainer = ModelTrainer(tagger, corpus)
trainer.train('model/',
              learning_rate=0.001,
              mini_batch_size=32,
              patience=2,
              anneal_factor=0.4,
              max_epochs=30,
              checkpoint=True,
              embeddings_storage_mode='None')

model = SequenceTagger.load('model/final-model.pt')
corpus: Corpus = WNUT_17()

result, main_score = model.evaluate(corpus.test, out_path = f"output.txt")
It's basically the same as your code. I noticed that you're loading the model from a different directory than the one you're saving it in. Is this maybe the error?
		</comment>
		<comment id='2' author='plc-dev' date='2020-07-06T14:24:54Z'>
		Thanks for the fast response &lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
.
I've fixed the paths, that was an oversight on my part when I edited the directories and files to make the code less cluttered after copying it from the notebook.
But I saw that the transformers-repo published a version while the model was trained a couple days ago, which explains the occurence of the error during train time, when being loaded from a checkpoint, with a presumably different version.
Although reverting back to the flair 0.5.0 and transformers 2.11.0 the error gets replaced by this one:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-5-56e09b327a86&gt; in &lt;module&gt;()
----&gt; 1 result, main_score = model.evaluate(corpus.test, out_path = f"/content/drive/My Drive/Paul Christ/Bachelorarbeit/models/{source}/flair/output.txt")
      2 
      3 print(main_score)
      4 print(result.detailed_results)

2 frames
/usr/local/lib/python3.6/dist-packages/flair/models/sequence_tagger_model.py in evaluate(self, data_loader, out_path, embedding_storage_mode)
    424 
    425                 with torch.no_grad():
--&gt; 426                     features = self.forward(batch)
    427                     loss = self._calculate_loss(features, batch)
    428                     tags, _ = self._obtain_labels(

/usr/local/lib/python3.6/dist-packages/flair/models/sequence_tagger_model.py in forward(self, sentences)
    513         self.embeddings.embed(sentences)
    514 
--&gt; 515         lengths: List[int] = [len(sentence.tokens) for sentence in sentences]
    516         longest_token_sequence_in_batch: int = max(lengths)
    517 

/usr/local/lib/python3.6/dist-packages/flair/models/sequence_tagger_model.py in &lt;listcomp&gt;(.0)
    513         self.embeddings.embed(sentences)
    514 
--&gt; 515         lengths: List[int] = [len(sentence.tokens) for sentence in sentences]
    516         longest_token_sequence_in_batch: int = max(lengths)
    517 

AttributeError: 'Token' object has no attribute 'tokens'
&lt;/denchmark-code&gt;

I've tested the code with both versions with models that were not affected by the error during train time and was able to run the code as expected.
So I guess the model file just got corrupted due to the version mismatch during training ¯_(ツ)_/¯
		</comment>
	</comments>
</bug>