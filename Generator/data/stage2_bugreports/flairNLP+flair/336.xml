<bug id='336' author='frtacoa' open_date='2018-12-28T06:57:52Z' closed_time='2019-02-02T12:56:07Z'>
	<summary>Japanese word embeddings</summary>
	<description>
Bug Description
There seems to be a problem with the embeddings from WordEmbeddings('ja'). Cosine similarity values, even for non-related words, are quite high (over 0.99) for some words that I tried.
How To Reproduce
You can use the following code to obtain the results as shown in the table. You can also try any other words in Sentence.
import pandas as pd
import scipy.spatial.distance as sp_sp_d

from flair.embeddings import WordEmbeddings
from flair.data import Sentence

def get_similarities(sentence_with_embeddings):
    def sim_cos(m_dist):
        """
        m_dist: NumPy matrix of cosine distances
        """
        return 1. - m_dist

    rows = []
    indexes = []
    for token in sentence_with_embeddings:
        indexes.append(token.text)
        rows.append(token.embedding.tolist())

    df_var = pd.DataFrame(data=rows, index=indexes)

    similarities = sim_cos(sp_sp_d.squareform(sp_sp_d.pdist(df_var, metric="cosine")))
    df_similarities = pd.DataFrame(similarities, columns=df_var.index, index=df_var.index)

    return df_similarities

emb_w = WordEmbeddings("ja")
sentence = Sentence('女性 男性 テレビ カメラ 良い 悪い 自動車 自転車')
emb_w.embed(sentence)
print(get_similarities(sentence))



-
女性
男性
テレビ
カメラ
良い
悪い
自動車
自転車




女性
1
0.999608
0.996616
0.996018
0.998129
0.998028
0.99521
0.995282


男性
0.999608
1
0.996403
0.995786
0.997812
0.997738
0.99489
0.995252


テレビ
0.996616
0.996403
1
0.995157
0.996294
0.996275
0.994128
0.9936


カメラ
0.996018
0.995786
0.995157
1
0.995801
0.995882
0.993967
0.99405


良い
0.998129
0.997812
0.996294
0.995801
1
0.999383
0.995239
0.995114


悪い
0.998028
0.997738
0.996275
0.995882
0.999383
1
0.995312
0.995094


自動車
0.99521
0.99489
0.994128
0.993967
0.995239
0.995312
1
0.995938


自転車
0.995282
0.995252
0.9936
0.99405
0.995114
0.995094
0.995938
1



Expected behavior
We tried with the embeddings for the corresponding English words and we got the following cosine similarity values:
...
emb_w = WordEmbeddings("en")
sentence = Sentence('woman man TV camera good bad car bicycle')
...




woman
man
TV
camera
good
bad
car
bicycle




woman
1
0.816452
0.354311
0.453366
0.284441
0.2667
0.475489
0.403567


man
0.816452
1
0.370918
0.468699
0.374932
0.336473
0.495137
0.447826


TV
0.354311
0.370918
1
0.537882
0.284823
0.300671
0.493515
0.322759


camera
0.453366
0.468699
0.537882
1
0.257566
0.26908
0.52313
0.423916


good
0.284441
0.374932
0.284823
0.257566
1
0.833116
0.246164
0.269814


bad
0.2667
0.336473
0.300671
0.26908
0.833116
1
0.283125
0.249673


car
0.475489
0.495137
0.493515
0.52313
0.246164
0.283125
1
0.586903


bicycle
0.403567
0.447826
0.322759
0.423916
0.269814
0.249673
0.586903
1



We were expecting that the cosine similarities for Japanese words look kind of similar to these.
Environment:

OS: Linux (Ubuntu 14.04)
Version: flair-0.4.0

Thank you in advance for all your help!
	</description>
	<comments>
		<comment id='1' author='frtacoa' date='2018-12-28T11:42:12Z'>
		Thanks for spotting this - I can confirm this error. We'll take a look!
		</comment>
		<comment id='2' author='frtacoa' date='2019-01-02T14:36:25Z'>
		This looks like an error in the original FastText embeddings for Wikipedia. I've put in an Issue in the FastText github.
		</comment>
		<comment id='3' author='frtacoa' date='2019-01-06T08:26:06Z'>
		Great! Thanks for the follow-up! Hope the problem can be solved.
		</comment>
		<comment id='4' author='frtacoa' date='2019-02-02T12:56:07Z'>
		There was a response on the corresponding fasttext thread: &lt;denchmark-link:https://github.com/facebookresearch/fastText/issues/710&gt;facebookresearch/fastText#710&lt;/denchmark-link&gt;

They advise to use CommonCrawl embeddings instead (WordEmbeddings('ja-crawl')) so I'm closing this issue!
		</comment>
		<comment id='5' author='frtacoa' date='2019-02-04T07:51:57Z'>
		Thanks for the heads-up!
Ok, we will keep using WordEmbeddings('ja-crawl') then.
Thanks!
		</comment>
	</comments>
</bug>