<bug id='2008' author='tiagokv' open_date='2020-12-06T23:42:07Z' closed_time='2020-12-07T19:31:31Z'>
	<summary>TransformerWordEmbeddings does not support Transformer-XL model</summary>
	<description>
Describe the bug
TransformerWordEmbeddings does not support Transformer-XL model.
To Reproduce
from flair.embeddings import TransformerWordEmbeddings
from flair.data import Sentence

model = TransformerWordEmbeddings("transfo-xl-wt103", "all", "mean")
model.embed(Sentence("tiago"))
Output is:
Traceback (most recent call last):
  File "test.py", line 8, in &lt;module&gt;
    model.embed(Sentence("tiago"))
  File "/home/tiago/.local/lib/python3.8/site-packages/flair/embeddings/base.py", line 60, in embed
    self._add_embeddings_internal(sentences)
  File "/home/tiago/.local/lib/python3.8/site-packages/flair/embeddings/token.py", line 861, in _add_embeddings_internal
    self._add_embeddings_to_sentences(batch)
  File "/home/tiago/.local/lib/python3.8/site-packages/flair/embeddings/token.py", line 972, in _add_embeddings_to_sentences
    hidden_states = self.model(input_ids, attention_mask=mask)[-1]
  File "/home/tiago/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
TypeError: forward() got an unexpected keyword argument 'attention_mask'
Expected behavior
Expected to support as it looks like it should be the class to concentrate all Transformer based models.
Screenshots
not relevant.
Environment (please complete the following information):

OS [e.g. iOS, Linux]: Linux
Version [e.g. flair-0.3.2]: 0.6.1

	</description>
	<comments>
	</comments>
</bug>