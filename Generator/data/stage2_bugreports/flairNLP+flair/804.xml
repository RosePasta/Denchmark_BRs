<bug id='804' author='stefan-it' open_date='2019-06-14T11:02:20Z' closed_time='2019-09-24T07:31:34Z'>
	<summary>CharacterEmbeddings: missing characters in vocab</summary>
	<description>
Hi,
there's one major issue with the  class: by default it uses a "common chars" dictionary (fetched from &lt;denchmark-link:https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models/common_characters&gt;here&lt;/denchmark-link&gt;
).
I manually loaded these common characters with:
from flair.data import Dictionary
d = Dictionary.load(name="chars")

len(d.get_items()) # Outputs 275
Then I used a Bulgarian word to check if all characters appear in the common characters dictionary:
for char in "оглуша":
  if char not in d.get_items():
    print(char)
Unfortunately, the ш character is not found.
I think we should update these common characters dictionary :)
	</description>
	<comments>
		<comment id='1' author='stefan-it' date='2019-06-14T11:35:55Z'>
		Hi &lt;denchmark-link:https://github.com/stefan-it&gt;@stefan-it&lt;/denchmark-link&gt;
 yes that's a good point. Common chars should at very least contain all cyrillic characters. We could additionally offer other standard dictionaries, or include a method that distills a dictionary from a corpus.
		</comment>
		<comment id='2' author='stefan-it' date='2019-06-17T10:42:46Z'>
		I created a larger vocab file on the latest Universal Dependencies v2.4 containing a character vocab for  83 languages.
Creation steps:


Download Universal Dependencies v2.4 from here


Extract archive and adjust path to the extracted folder in the following script:


import collections
import os

from pathlib import Path
import pickle

from flair.data import Dictionary
char_dictionary: Dictionary = Dictionary()

ud_base = 'ud-treebanks-v2.4/'

datasets = [Path(d) / Path(f) for d,_,fl in os.walk(ud_base) for f in fl if f.endswith('txt')]

vocab = collections.Counter()

for dataset in datasets:
    with open(dataset, 'rt') as f_p:
        for line in f_p:
            for char in line:
                vocab[char] += 1

for letter, _ in vocab.most_common():
    char_dictionary.add_item(letter)

with open('common-chars-large', 'wb') as f:
    mappings = {
        'idx2item': char_dictionary.idx2item,
        'item2idx': char_dictionary.item2idx
    }
    pickle.dump(mappings, f)
Using all *.txt is much more faster than parsing the CoNLL-U format for each language.
The resulting vocab contains 9001 characters.
It can be downloaded here:
wget https://schweter.eu/cloud/flair-misc/common-chars-large
Test it with:
from flair.data import Dictionary
d = Dictionary.load_from_file(filename='common-chars-large')

print(len(d.get_items()))
		</comment>
	</comments>
</bug>