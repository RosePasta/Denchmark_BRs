<bug id='1306' author='lucaventurini' open_date='2019-12-02T10:21:13Z' closed_time='2020-05-28T23:49:57Z'>
	<summary>Error with transformers 2.2.0</summary>
	<description>
Describe the bug
Function predict raises the following exception with transformers 2.2.0, on a model with bert embeddings. With previous transformers version everything is fine.
  File "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/flair/models/text_classification_model.py", line 200, in predict
    scores = self.forward(batch)
  File "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/flair/models/text_classification_model.py", line 82, in forward
    self.document_embeddings.embed(sentences)
  File "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/flair/embeddings.py", line 90, in embed
    self._add_embeddings_internal(sentences)
  File "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/flair/embeddings.py", line 2701, in _add_embeddings_internal
    self.embeddings.embed(sentences)
  File "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/flair/embeddings.py", line 171, in embed
    embedding.embed(sentences)
  File "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/flair/embeddings.py", line 90, in embed
    self._add_embeddings_internal(sentences)
  File "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/flair/embeddings.py", line 2109, in _add_embeddings_internal
    all_encoder_layers = self.model(all_input_ids, attention_mask=all_input_masks)[
  File "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in call
    result = self.forward(*input, **kwargs)
  File "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/modeling_bert.py", line 673, in forward
    if self.config.is_decoder:
AttributeError: 'BertConfig' object has no attribute 'is_decoder'
Environment (please complete the following information):

OS [e.g. iOS, Linux]: Linux
Version: flair 0.4.4

	</description>
	<comments>
		<comment id='1' author='lucaventurini' date='2019-12-02T10:56:10Z'>
		Hi &lt;denchmark-link:https://github.com/lucaventurini&gt;@lucaventurini&lt;/denchmark-link&gt;
, could you provide a code snippet for reproducing that error  I'll check it then :)
		</comment>
		<comment id='2' author='lucaventurini' date='2019-12-05T11:32:51Z'>
		Hi &lt;denchmark-link:https://github.com/stefan-it&gt;@stefan-it&lt;/denchmark-link&gt;
 , thank you for your reply.
The following code:
from flair.data import Sentence
from flair.models import TextClassifier

flair_model = "final-model.pt"

classifier_loaded = TextClassifier.load(flair_model)

sentences = [Sentence(sent, use_tokenizer=True) for sent in ["Hi!"]]

classifier_loaded.predict(sentences)
should fail if you try to load a model trained with bert embeddings (which I hope you have, I don't have one to share).
The traceback is:
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-5-414e0ebac4ba&gt; in &lt;module&gt;()
      8 sentences = [Sentence(sent, use_tokenizer=True) for sent in ["Hi!"]]
      9 
---&gt; 10 classifier_loaded.predict(sentences)

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/flair/models/text_classification_model.py in predict(self, sentences, mini_batch_size, embedding_storage_mode, multi_class_prob, verbose, use_tokenizer)
    198                     continue
    199 
--&gt; 200                 scores = self.forward(batch)
    201                 predicted_labels = self._obtain_labels(
    202                     scores, predict_prob=multi_class_prob

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/flair/models/text_classification_model.py in forward(self, sentences)
     80     def forward(self, sentences) -&gt; List[List[float]]:
     81 
---&gt; 82         self.document_embeddings.embed(sentences)
     83 
     84         text_embedding_list = [

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/flair/embeddings.py in embed(self, sentences)
     88 
     89         if not everything_embedded or not self.static_embeddings:
---&gt; 90             self._add_embeddings_internal(sentences)
     91 
     92         return sentences

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/flair/embeddings.py in _add_embeddings_internal(self, sentences)
   2699 
   2700         # embed words in the sentence
-&gt; 2701         self.embeddings.embed(sentences)
   2702 
   2703         lengths: List[int] = [len(sentence.tokens) for sentence in sentences]

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/flair/embeddings.py in embed(self, sentences, static_embeddings)
    169 
    170         for embedding in self.embeddings:
--&gt; 171             embedding.embed(sentences)
    172 
    173     @property

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/flair/embeddings.py in embed(self, sentences)
     88 
     89         if not everything_embedded or not self.static_embeddings:
---&gt; 90             self._add_embeddings_internal(sentences)
     91 
     92         return sentences

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/flair/embeddings.py in _add_embeddings_internal(self, sentences)
   2107         self.model.to(flair.device)
   2108         self.model.eval()
-&gt; 2109         all_encoder_layers = self.model(all_input_ids, attention_mask=all_input_masks)[
   2110             -1
   2111         ]

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    545             result = self._slow_forward(*input, **kwargs)
    546         else:
--&gt; 547             result = self.forward(*input, **kwargs)
    548         for hook in self._forward_hooks.values():
    549             hook_result = hook(self, input, result)

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)
    673         # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]
    674         if attention_mask.dim() == 2:
--&gt; 675             if self.config.is_decoder:
    676                 batch_size, seq_length = input_shape
    677                 seq_ids = torch.arange(seq_length, device=device)

AttributeError: 'BertConfig' object has no attribute 'is_decoder'
		</comment>
		<comment id='3' author='lucaventurini' date='2019-12-11T05:38:01Z'>
		i tried one of the older version of transformers and it worked...
but i don't remember that version....
please let us know if somebody tries...
		</comment>
		<comment id='4' author='lucaventurini' date='2019-12-19T06:37:54Z'>
		Installing transformers==2.0.0 works for me
		</comment>
		<comment id='5' author='lucaventurini' date='2019-12-19T10:16:31Z'>
		
Describe the bug
Function predict raises the following exception with transformers 2.2.0, on a model with bert embeddings. With previous transformers version everything is fine.
  File "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/flair/models/text_classification_model.py", line 200, in predict
    scores = self.forward(batch)
  File "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/flair/models/text_classification_model.py", line 82, in forward
    self.document_embeddings.embed(sentences)
  File "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/flair/embeddings.py", line 90, in embed
    self._add_embeddings_internal(sentences)
  File "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/flair/embeddings.py", line 2701, in _add_embeddings_internal
    self.embeddings.embed(sentences)
  File "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/flair/embeddings.py", line 171, in embed
    embedding.embed(sentences)
  File "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/flair/embeddings.py", line 90, in embed
    self._add_embeddings_internal(sentences)
  File "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/flair/embeddings.py", line 2109, in _add_embeddings_internal
    all_encoder_layers = self.model(all_input_ids, attention_mask=all_input_masks)[
  File "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in call
    result = self.forward(*input, **kwargs)
  File "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/modeling_bert.py", line 673, in forward
    if self.config.is_decoder:
AttributeError: 'BertConfig' object has no attribute 'is_decoder'
Environment (please complete the following information):

OS [e.g. iOS, Linux]: Linux
Version: flair 0.4.4


Hi, are you still facing this issue?
try to downgrade the transformers to v2.0.0
		</comment>
		<comment id='6' author='lucaventurini' date='2020-01-22T22:17:45Z'>
		Hi, when I downgrade to transformers v2.0.0, and replicating &lt;denchmark-link:https://github.com/lucaventurini&gt;@lucaventurini&lt;/denchmark-link&gt;
's code, I receive the following error. Are there any workarounds besides downgrading?
`

 22 from torch.nn import TransformerEncoderLayer, TransformerEncoder
 23 
 24 from transformers import (
 25     AlbertTokenizer,
 26     AlbertModel,

ImportError: cannot import name 'AlbertTokenizer'`

		</comment>
		<comment id='7' author='lucaventurini' date='2020-05-21T22:57:15Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>