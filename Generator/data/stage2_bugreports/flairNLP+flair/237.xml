<bug id='237' author='JoanEspasa' open_date='2018-11-22T16:03:47Z' closed_time='2018-11-23T08:56:51Z'>
	<summary>Bug in tokenizer?</summary>
	<description>
Here's a minimum viable code to reproduce:
&lt;denchmark-code&gt;from flair.data import Sentence
from flair.models import SequenceTagger

model = SequenceTagger.load("ner-ontonotes-fast")
full_text = "\"In the 1960s and 1970s...\" Then came Thierry Mugler and Gianni Versace."
sentence = Sentence(full_text, use_tokenizer=True)
model.predict(sentence)
print(f"full text  : {full_text}")
print(f"text length: {len(full_text)}")
print("tag\tstart\tend\tto_original_text()")
for entity in sentence.get_spans('ner'):
    print(f"{entity.tag}\t{entity.start_pos}\t{entity.end_pos}\t{entity.to_original_text()}")
&lt;/denchmark-code&gt;

Output:
&lt;denchmark-code&gt;full text  : "In the 1960s and 1970s..." Then came Thierry Mugler and Gianni Versace.
text length: 72
tag	start	end	to_original_text()
DATE	8	13	1960s
DATE	18	23	1970s
PERSON	81	94	ThierryMugler
PERSON	97	110	GianniVersace
&lt;/denchmark-code&gt;

Seems the resulting tokens have start_pos and end_pos indexes larger than the real text length. Note also that the method to_original_text() is eating the spaces, so I suppose it is related.
Any ideas about what is causing the trouble?
	</description>
	<comments>
		<comment id='1' author='JoanEspasa' date='2018-11-22T16:15:03Z'>
		Hi &lt;denchmark-link:https://github.com/JoanEspasa&gt;@JoanEspasa&lt;/denchmark-link&gt;
 - thanks for pointing this out! But I cannot reproduce this error. What version of Flair are you using?
		</comment>
		<comment id='2' author='JoanEspasa' date='2018-11-22T16:37:16Z'>
		&lt;denchmark-link:https://github.com/tabergma&gt;@tabergma&lt;/denchmark-link&gt;
 just told me she can reproduce the error, so checking it out now!
		</comment>
		<comment id='3' author='JoanEspasa' date='2018-11-22T16:38:25Z'>
		Hi &lt;denchmark-link:https://github.com/JoanEspasa&gt;@JoanEspasa&lt;/denchmark-link&gt;
!
The issue lies in the tokenizer itself. The tokenizer returns the following tokens:

The 8th token  is not in the original text but added by the tokenizer. We observed this issue already a couple of times, but unfortunately we cannot do anything about it.
Now the following happens: We want to get the start index of the  token in the text by calling , which returns 71 as this is the only position in the text that actually matches  .  This shifts the start positions of the following tokens. As the start positions of the tokens are now greater than the text length, the algorithm does not work as expected anymore.
I'll look into the issue. Maybe we can find a workaround.
		</comment>
		<comment id='4' author='JoanEspasa' date='2018-11-22T16:51:55Z'>
		Which version of segtok are you using? I had version 1.5.6 installed. I just updated to version 1.5.7 and the issue is solved.
		</comment>
		<comment id='5' author='JoanEspasa' date='2018-11-23T08:29:12Z'>
		&lt;denchmark-link:https://github.com/tabergma&gt;@tabergma&lt;/denchmark-link&gt;
 Indeed! I was using segtok-1.5.6. Upgrading to 1.5.7 fixes the issue 
I did not realize that you were using an external tokenizer, I'm sorry. I should have read the documentation more carefully and report to them.
I think this happened to me because in your setup.py there is segtok==1.5.6 and in requirements.txt
segtok==1.5.7. Installing via pip (as I did) seems to follow setup.py instead of requirements.txt.
Thanks again for looking into this &lt;denchmark-link:https://github.com/tabergma&gt;@tabergma&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 
PD: feel free to close the issue :)
		</comment>
		<comment id='6' author='JoanEspasa' date='2018-11-23T08:34:34Z'>
		Thanks for verifying! Will update the setup.py to use version segtok==1.5.7.
		</comment>
	</comments>
</bug>