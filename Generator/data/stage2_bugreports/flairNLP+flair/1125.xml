<bug id='1125' author='dhanachandran-ezdi' open_date='2019-09-18T10:31:27Z' closed_time='2020-04-29T21:25:08Z'>
	<summary>Training time for Sequence Tagger model with flair 0.4.2 and 0.4.3 is far slower than flair 0.4.1</summary>
	<description>
I am training a Sequence Tagger model with BERT embedding.  With flair 0.4.2 and 0.4.3 the training time per epoch is more than an hour whereas with flair 0.4.1 the training time per epoch is reduced to around 10 minutes. In both cases I used same dataset as well as set of hyper-parameters.
With flair 0.4.1
2019-09-11 14:32:55,656 epoch 2 - iter 0/3970 - loss 0.97338068
2019-09-11 14:33:49,827 epoch 2 - iter 397/3970 - loss 1.55385168
2019-09-11 14:34:44,221 epoch 2 - iter 794/3970 - loss 1.56232328
2019-09-11 14:35:37,523 epoch 2 - iter 1191/3970 - loss 1.53419682
2019-09-11 14:36:31,586 epoch 2 - iter 1588/3970 - loss 1.52745939
2019-09-11 14:37:25,264 epoch 2 - iter 1985/3970 - loss 1.51710631
2019-09-11 14:38:20,391 epoch 2 - iter 2382/3970 - loss 1.51415485
2019-09-11 14:39:14,297 epoch 2 - iter 2779/3970 - loss 1.50332664
2019-09-11 14:40:09,160 epoch 2 - iter 3176/3970 - loss 1.49171285
2019-09-11 14:41:02,996 epoch 2 - iter 3573/3970 - loss 1.48215480`
with flair 0.4.3 and 0.4.4
2019-09-17 09:43:04,970 epoch 2 - iter 0/3970 - loss 3.04009271
2019-09-17 09:50:32,591 epoch 2 - iter 397/3970 - loss 1.54281418
2019-09-17 09:58:32,604 epoch 2 - iter 794/3970 - loss 1.53454762
2019-09-17 10:06:06,666 epoch 2 - iter 1191/3970 - loss 1.53360289
2019-09-17 10:13:37,722 epoch 2 - iter 1588/3970 - loss 1.51916389
2019-09-17 10:21:18,720 epoch 2 - iter 1985/3970 - loss 1.50955056
2019-09-17 10:29:36,539 epoch 2 - iter 2382/3970 - loss 1.49568585
2019-09-17 10:36:58,278 epoch 2 - iter 2779/3970 - loss 1.48138234
2019-09-17 10:44:15,273 epoch 2 - iter 3176/3970 - loss 1.47001876
2019-09-17 10:52:16,362 epoch 2 - iter 3573/3970 - loss 1.46437050`
	</description>
	<comments>
		<comment id='1' author='dhanachandran-ezdi' date='2019-09-18T12:38:28Z'>
		can you check with the version from master branch?
		</comment>
		<comment id='2' author='dhanachandran-ezdi' date='2019-09-18T12:49:54Z'>
		Yes I have checked it.
		</comment>
		<comment id='3' author='dhanachandran-ezdi' date='2019-09-18T13:51:17Z'>
		Hm interesting, thanks for reporting this. Could you share the script you're using to run the experiment?
		</comment>
		<comment id='4' author='dhanachandran-ezdi' date='2019-09-19T08:49:31Z'>
		&lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;

from flair.data import Corpus
from flair.data_fetcher import NLPTaskDataFetcher
import torch.nn as nn
from flair.embeddings import BertEmbeddings
columns = {0: 'text', 1: 'pos', 2: 'ner'}
data_folder = 'data'

from flair.data import TaggedCorpus
corpus: TaggedCorpus = NLPTaskDataFetcher.load_column_corpus(data_folder, columns, train_file='trainData1.txt', test_file='testData1.txt', dev_file='testData1.txt')

#For flair 0.4.2 and 0.4.3
#corpus: Corpus = ColumnCorpus(data_folder, columns, train_file='trainData1.txt', dev_file='testData1.txt', test_file='testData1.txt')


tag_type = 'ner'
tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)
bert_embedding = BertEmbeddings('bert-base-uncased')

from flair.models import SequenceTagger
tagger: SequenceTagger = SequenceTagger(hidden_size=256,
embeddings=bert_embedding,
tag_dictionary=tag_dictionary,
tag_type=tag_type,
use_crf=True)

from flair.trainers import ModelTrainer
trainer: ModelTrainer = ModelTrainer(tagger, corpus)
trainer.train('resources/bert_base_fix_seed',
learning_rate=0.1,
mini_batch_size=32,
max_epochs=64,
checkpoint=True)
		</comment>
		<comment id='5' author='dhanachandran-ezdi' date='2019-09-19T09:19:45Z'>
		Thanks - could you try running like this with the current Flair version (i.e. set embeddings_storage_mode='gpu'):
trainer.train('resources/bert_base_fix_seed',
     learning_rate=0.1,
     mini_batch_size=32,
     max_epochs=64,
     checkpoint=True, 
     # set GPU storage mode
     embeddings_storage_mode='gpu',
)
		</comment>
		<comment id='6' author='dhanachandran-ezdi' date='2019-09-19T09:26:13Z'>
		With embeddings_storage_mode='gpu'
It shows OOM error.
I am training on Tesla V100, 16 GB GPU. I even reduced the mini_batch_size too. Still could the solve the issue.
And, one more thing is that with flair 0.4.1 also I trained with embeddings_storage_mode with default
		</comment>
		<comment id='7' author='dhanachandran-ezdi' date='2019-09-19T09:29:46Z'>
		And, one more observation is that with Flair 4.0.1 GPU Memory Usage is fluctuating from 20% - 24% most of the time.
But with Flair 4.0.2 and 4.0.3 GPU Memory Usage becomes 0% in most of the time.
		</comment>
		<comment id='8' author='dhanachandran-ezdi' date='2019-09-23T16:18:32Z'>
		Hello &lt;denchmark-link:https://github.com/dhanachandran-ezdi&gt;@dhanachandran-ezdi&lt;/denchmark-link&gt;
 - I've run your script with flair 0.4.1 and 0.4.3 on a aws p3 instance and I get similar speeds.
First 2 epochs, Flair 0.4.1:
2019-09-23 14:21:28,029 ----------------------------------------------------------------------------------------------------
2019-09-23 14:21:29,712 epoch 1 - iter 0/469 - loss 62.89373779
2019-09-23 14:22:46,711 epoch 1 - iter 46/469 - loss 10.23166132
2019-09-23 14:24:04,145 epoch 1 - iter 92/469 - loss 7.31668244
2019-09-23 14:25:18,105 epoch 1 - iter 138/469 - loss 6.12557161
2019-09-23 14:26:30,050 epoch 1 - iter 184/469 - loss 5.25811321
2019-09-23 14:27:42,275 epoch 1 - iter 230/469 - loss 4.69404712
2019-09-23 14:28:57,445 epoch 1 - iter 276/469 - loss 4.27328131
2019-09-23 14:30:12,499 epoch 1 - iter 322/469 - loss 3.97315693
2019-09-23 14:31:28,478 epoch 1 - iter 368/469 - loss 3.73506895
2019-09-23 14:32:41,367 epoch 1 - iter 414/469 - loss 3.53736693
2019-09-23 14:33:56,460 epoch 1 - iter 460/469 - loss 3.37370211
2019-09-23 14:34:07,955 ----------------------------------------------------------------------------------------------------
2019-09-23 14:34:07,955 EPOCH 1 done: loss 3.3494 - lr 0.1000 - bad epochs 0
2019-09-23 14:37:25,852 DEV  : loss 1.30106616 - f-score 0.8962 - acc 0.8119
2019-09-23 14:40:18,876 TEST : loss 1.30520952 - f-score 0.8620 - acc 0.7576
2019-09-23 14:40:20,091 ----------------------------------------------------------------------------------------------------
2019-09-23 14:40:20,237 epoch 2 - iter 0/469 - loss 1.46931815
2019-09-23 14:40:26,420 epoch 2 - iter 46/469 - loss 1.57467591
2019-09-23 14:40:32,604 epoch 2 - iter 92/469 - loss 1.52295251
2019-09-23 14:40:38,630 epoch 2 - iter 138/469 - loss 1.50688668
2019-09-23 14:40:44,754 epoch 2 - iter 184/469 - loss 1.51720075
2019-09-23 14:40:50,882 epoch 2 - iter 230/469 - loss 1.51083167
2019-09-23 14:40:57,082 epoch 2 - iter 276/469 - loss 1.49969282
2019-09-23 14:41:03,047 epoch 2 - iter 322/469 - loss 1.50688357
2019-09-23 14:41:09,063 epoch 2 - iter 368/469 - loss 1.49248255
2019-09-23 14:41:15,060 epoch 2 - iter 414/469 - loss 1.48903352
2019-09-23 14:41:21,113 epoch 2 - iter 460/469 - loss 1.45365272
2019-09-23 14:41:22,169 ----------------------------------------------------------------------------------------------------
2019-09-23 14:41:22,170 EPOCH 2 done: loss 1.4515 - lr 0.1000 - bad epochs 0
2019-09-23 14:41:41,937 DEV  : loss 0.95667487 - f-score 0.9083 - acc 0.8320
2019-09-23 14:42:00,354 TEST : loss 1.11644638 - f-score 0.8819 - acc 0.7888
2019-09-23 14:42:04,544 ----------------------------------------------------------------------------------------------------
First 2 epochs, Flair 0.4.3:
2019-09-23 14:54:31,132 ----------------------------------------------------------------------------------------------------
2019-09-23 14:54:32,565 epoch 1 - iter 0/469 - loss 40.10458755 - samples/sec: 1028.68
2019-09-23 14:55:51,103 epoch 1 - iter 46/469 - loss 9.17054667 - samples/sec: 18.78
2019-09-23 14:57:07,420 epoch 1 - iter 92/469 - loss 6.63299764 - samples/sec: 19.33
2019-09-23 14:58:29,750 epoch 1 - iter 138/469 - loss 5.45882170 - samples/sec: 17.91
2019-09-23 14:59:47,061 epoch 1 - iter 184/469 - loss 4.73194454 - samples/sec: 19.08
2019-09-23 15:01:04,053 epoch 1 - iter 230/469 - loss 4.24727531 - samples/sec: 19.16
2019-09-23 15:02:14,629 epoch 1 - iter 276/469 - loss 3.83893202 - samples/sec: 20.90
2019-09-23 15:03:30,355 epoch 1 - iter 322/469 - loss 3.52385646 - samples/sec: 19.48
2019-09-23 15:04:50,597 epoch 1 - iter 368/469 - loss 3.31408305 - samples/sec: 18.38
2019-09-23 15:06:04,433 epoch 1 - iter 414/469 - loss 3.12952239 - samples/sec: 19.98
2019-09-23 15:07:21,098 epoch 1 - iter 460/469 - loss 2.97245325 - samples/sec: 19.24
2019-09-23 15:07:33,220 ----------------------------------------------------------------------------------------------------
2019-09-23 15:07:33,220 EPOCH 1 done: loss 2.9507 - lr 0.1000
2019-09-23 15:10:57,535 DEV : loss 1.232349157333374 - score 0.8865
2019-09-23 15:10:57,649 BAD EPOCHS (no improvement): 0
2019-09-23 15:10:58,864 ----------------------------------------------------------------------------------------------------
2019-09-23 15:10:59,019 epoch 2 - iter 0/469 - loss 1.55783629 - samples/sec: 9587.28
2019-09-23 15:11:05,092 epoch 2 - iter 46/469 - loss 1.59324708 - samples/sec: 248.64
2019-09-23 15:11:11,231 epoch 2 - iter 92/469 - loss 1.55123521 - samples/sec: 245.85
2019-09-23 15:11:17,467 epoch 2 - iter 138/469 - loss 1.48162379 - samples/sec: 241.90
2019-09-23 15:11:23,585 epoch 2 - iter 184/469 - loss 1.41651344 - samples/sec: 246.70
2019-09-23 15:11:29,824 epoch 2 - iter 230/469 - loss 1.41891394 - samples/sec: 241.83
2019-09-23 15:11:36,100 epoch 2 - iter 276/469 - loss 1.41656691 - samples/sec: 240.38
2019-09-23 15:11:42,217 epoch 2 - iter 322/469 - loss 1.39698422 - samples/sec: 246.80
2019-09-23 15:11:48,202 epoch 2 - iter 368/469 - loss 1.37271643 - samples/sec: 252.28
2019-09-23 15:11:54,175 epoch 2 - iter 414/469 - loss 1.34715790 - samples/sec: 252.84
2019-09-23 15:12:00,349 epoch 2 - iter 460/469 - loss 1.33346256 - samples/sec: 244.51
2019-09-23 15:12:01,500 ----------------------------------------------------------------------------------------------------
2019-09-23 15:12:01,500 EPOCH 2 done: loss 1.3259 - lr 0.1000
2019-09-23 15:12:22,459 DEV : loss 0.9374335408210754 - score 0.9153
2019-09-23 15:12:22,574 BAD EPOCHS (no improvement): 0
2019-09-23 15:12:26,747 ----------------------------------------------------------------------------------------------------
So it looks like they are roughly the same speed-wise.
		</comment>
		<comment id='9' author='dhanachandran-ezdi' date='2019-09-23T16:59:49Z'>
		Which embedding are you using? In my experiment I use Bert embedding and
data size is larger than yours.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, Sep 23, 2019, 9:48 PM Alan Akbik ***@***.*** wrote:
 Hello @dhanachandran-ezdi &lt;https://github.com/dhanachandran-ezdi&gt; - I've
 run your script with flair 0.4.1 and 0.4.3 on a aws p3 instance and I get
 similar speeds.

 First 2 epochs, Flair 0.4.1:

 2019-09-23 14:21:28,029 ----------------------------------------------------------------------------------------------------2019-09-23 14:21:29,712 epoch 1 - iter 0/469 - loss 62.893737792019-09-23 14:22:46,711 epoch 1 - iter 46/469 - loss 10.231661322019-09-23 14:24:04,145 epoch 1 - iter 92/469 - loss 7.316682442019-09-23 14:25:18,105 epoch 1 - iter 138/469 - loss 6.125571612019-09-23 14:26:30,050 epoch 1 - iter 184/469 - loss 5.258113212019-09-23 14:27:42,275 epoch 1 - iter 230/469 - loss 4.694047122019-09-23 14:28:57,445 epoch 1 - iter 276/469 - loss 4.273281312019-09-23 14:30:12,499 epoch 1 - iter 322/469 - loss 3.973156932019-09-23 14:31:28,478 epoch 1 - iter 368/469 - loss 3.735068952019-09-23 14:32:41,367 epoch 1 - iter 414/469 - loss 3.537366932019-09-23 14:33:56,460 epoch 1 - iter 460/469 - loss 3.373702112019-09-23 14:34:07,955 ----------------------------------------------------------------------------------------------------2019-09-23 14:34:07,955 EPOCH 1 done: loss 3.3494 - lr 0.1000 - bad epochs 02019-09-23 14:37:25,852 DEV  : loss 1.30106616 - f-score 0.8962 - acc 0.81192019-09-23 14:40:18,876 TEST : loss 1.30520952 - f-score 0.8620 - acc 0.75762019-09-23 14:40:20,091 ----------------------------------------------------------------------------------------------------2019-09-23 14:40:20,237 epoch 2 - iter 0/469 - loss 1.469318152019-09-23 14:40:26,420 epoch 2 - iter 46/469 - loss 1.574675912019-09-23 14:40:32,604 epoch 2 - iter 92/469 - loss 1.522952512019-09-23 14:40:38,630 epoch 2 - iter 138/469 - loss 1.506886682019-09-23 14:40:44,754 epoch 2 - iter 184/469 - loss 1.517200752019-09-23 14:40:50,882 epoch 2 - iter 230/469 - loss 1.510831672019-09-23 14:40:57,082 epoch 2 - iter 276/469 - loss 1.499692822019-09-23 14:41:03,047 epoch 2 - iter 322/469 - loss 1.506883572019-09-23 14:41:09,063 epoch 2 - iter 368/469 - loss 1.492482552019-09-23 14:41:15,060 epoch 2 - iter 414/469 - loss 1.489033522019-09-23 14:41:21,113 epoch 2 - iter 460/469 - loss 1.453652722019-09-23 14:41:22,169 ----------------------------------------------------------------------------------------------------2019-09-23 14:41:22,170 EPOCH 2 done: loss 1.4515 - lr 0.1000 - bad epochs 02019-09-23 14:41:41,937 DEV  : loss 0.95667487 - f-score 0.9083 - acc 0.83202019-09-23 14:42:00,354 TEST : loss 1.11644638 - f-score 0.8819 - acc 0.78882019-09-23 14:42:04,544 ----------------------------------------------------------------------------------------------------

 First 2 epochs, Flair 0.4.3:

 2019-09-23 14:54:31,132 ----------------------------------------------------------------------------------------------------2019-09-23 14:54:32,565 epoch 1 - iter 0/469 - loss 40.10458755 - samples/sec: 1028.682019-09-23 14:55:51,103 epoch 1 - iter 46/469 - loss 9.17054667 - samples/sec: 18.782019-09-23 14:57:07,420 epoch 1 - iter 92/469 - loss 6.63299764 - samples/sec: 19.332019-09-23 14:58:29,750 epoch 1 - iter 138/469 - loss 5.45882170 - samples/sec: 17.912019-09-23 14:59:47,061 epoch 1 - iter 184/469 - loss 4.73194454 - samples/sec: 19.082019-09-23 15:01:04,053 epoch 1 - iter 230/469 - loss 4.24727531 - samples/sec: 19.162019-09-23 15:02:14,629 epoch 1 - iter 276/469 - loss 3.83893202 - samples/sec: 20.902019-09-23 15:03:30,355 epoch 1 - iter 322/469 - loss 3.52385646 - samples/sec: 19.482019-09-23 15:04:50,597 epoch 1 - iter 368/469 - loss 3.31408305 - samples/sec: 18.382019-09-23 15:06:04,433 epoch 1 - iter 414/469 - loss 3.12952239 - samples/sec: 19.982019-09-23 15:07:21,098 epoch 1 - iter 460/469 - loss 2.97245325 - samples/sec: 19.242019-09-23 15:07:33,220 ----------------------------------------------------------------------------------------------------2019-09-23 15:07:33,220 EPOCH 1 done: loss 2.9507 - lr 0.10002019-09-23 15:10:57,535 DEV : loss 1.232349157333374 - score 0.88652019-09-23 15:10:57,649 BAD EPOCHS (no improvement): 02019-09-23 15:10:58,864 ----------------------------------------------------------------------------------------------------2019-09-23 15:10:59,019 epoch 2 - iter 0/469 - loss 1.55783629 - samples/sec: 9587.282019-09-23 15:11:05,092 epoch 2 - iter 46/469 - loss 1.59324708 - samples/sec: 248.642019-09-23 15:11:11,231 epoch 2 - iter 92/469 - loss 1.55123521 - samples/sec: 245.852019-09-23 15:11:17,467 epoch 2 - iter 138/469 - loss 1.48162379 - samples/sec: 241.902019-09-23 15:11:23,585 epoch 2 - iter 184/469 - loss 1.41651344 - samples/sec: 246.702019-09-23 15:11:29,824 epoch 2 - iter 230/469 - loss 1.41891394 - samples/sec: 241.832019-09-23 15:11:36,100 epoch 2 - iter 276/469 - loss 1.41656691 - samples/sec: 240.382019-09-23 15:11:42,217 epoch 2 - iter 322/469 - loss 1.39698422 - samples/sec: 246.802019-09-23 15:11:48,202 epoch 2 - iter 368/469 - loss 1.37271643 - samples/sec: 252.282019-09-23 15:11:54,175 epoch 2 - iter 414/469 - loss 1.34715790 - samples/sec: 252.842019-09-23 15:12:00,349 epoch 2 - iter 460/469 - loss 1.33346256 - samples/sec: 244.512019-09-23 15:12:01,500 ----------------------------------------------------------------------------------------------------2019-09-23 15:12:01,500 EPOCH 2 done: loss 1.3259 - lr 0.10002019-09-23 15:12:22,459 DEV : loss 0.9374335408210754 - score 0.91532019-09-23 15:12:22,574 BAD EPOCHS (no improvement): 02019-09-23 15:12:26,747 ----------------------------------------------------------------------------------------------------

 So it looks like they are roughly the same speed-wise.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#1125&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ACWFRNA67DPRY3COBZEXFYLQLDT6JANCNFSM4IX4ZVVQ&gt;
 .



		</comment>
		<comment id='10' author='dhanachandran-ezdi' date='2019-09-23T17:04:13Z'>
		I used your script, so BertEmbeddings. But I used the CoNLL-03 dataset.
However it could be that the defaults for the BertEmbeddings have changed. In Flair 0.4.1 we still used pytorch-pretrained-bert as library, but now we use pytorch-transformers. MAybe by default it loads a larger BERT model?
		</comment>
		<comment id='11' author='dhanachandran-ezdi' date='2019-09-23T17:08:29Z'>
		I used my own pretrained Bert embeddings. Let me run the experiments with
Conll dataset again. And will share you the results. Thanks for your quick
response.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, Sep 23, 2019, 10:34 PM Alan Akbik ***@***.*** wrote:
 I used your script, so BertEmbeddings. But I used the CoNLL-03 dataset.

 However it could be that the defaults for the BertEmbeddings have
 changed. In Flair 0.4.1 we still used pytorch-pretrained-bert as library,
 but now we use pytorch-transformers. MAybe by default it loads a larger
 BERT model?

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#1125&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ACWFRND3OVAAU65XVB3XEMDQLDZJPANCNFSM4IX4ZVVQ&gt;
 .



		</comment>
		<comment id='12' author='dhanachandran-ezdi' date='2019-09-23T17:09:47Z'>
		You could also try with different embeddings such as FlairEmbeddings. If it is the same speed with these embeddings, then maybe its because of some detail in the Bert embeddings.
		</comment>
		<comment id='13' author='dhanachandran-ezdi' date='2019-09-23T17:11:33Z'>
		Yes sure, will try it out.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, Sep 23, 2019, 10:39 PM Alan Akbik ***@***.*** wrote:
 You could also try with different embeddings such as FlairEmbeddings. If
 it is the same speed with these embeddings, then maybe its because of some
 detail in the Bert embeddings.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#1125&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ACWFRNGLMLAP5FDTCM7RTI3QLDZ6LANCNFSM4IX4ZVVQ&gt;
 .



		</comment>
		<comment id='14' author='dhanachandran-ezdi' date='2020-04-29T21:10:51Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>