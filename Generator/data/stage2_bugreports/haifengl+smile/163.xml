<bug id='163' author='javierfvargas' open_date='2017-03-22T16:38:18Z' closed_time='2017-03-31T04:55:48Z'>
	<summary>classWeight paramater on RandomForest is not clear.</summary>
	<description>
As classWeight parameter is not clear I was forced to check into the code to make some sense of it. The code raised a question.  I am far of an expert therefore my question could be foolish.
It is regarding this line of code:



smile/core/src/main/java/smile/classification/RandomForest.java


         Line 354
      in
      ca15aac






 nj /= classWeight[l]; 





If you alter nj that way, aren't you truncating the sampling space? I mean, samples above nj/classWeight are not being used. Is it ok?
int xi = Math.randomInt(nj)
My understanding is that xi will never be greater than nj and if classWeight[l] is greater than 1 you will never use samples above the new value of nj.
Maybe I got it all wrong. In that case could you please explain how classWeight affects sampling.
Thanks.
	</description>
	<comments>
		<comment id='1' author='javierfvargas' date='2017-03-22T16:51:23Z'>
		Thank you very much! It is a bug. The fix is in master now.
		</comment>
		<comment id='2' author='javierfvargas' date='2017-03-22T18:25:45Z'>
		Thank you.
Just in case.  Bagging.java includes a very similar portion of code.
Also.  Could you elaborate a little on how classWeight works?  E.g, how I can achieve balanced class weight in unbalanced sets?
		</comment>
		<comment id='3' author='javierfvargas' date='2017-03-22T18:36:46Z'>
		Thanks! Updated it.
classWeight is the priors of the classes. The weights of each class is roughly the  ratio of samples in each class. For example, if  there are 400 positive samples and 100 negative samples, the classWeight should be [1, 4]  (assuming label 0 is of negative, label 1 is of positive). If the classWeight is null, we will assume that the data is balanced and assign the weight 1 to each class.
		</comment>
		<comment id='4' author='javierfvargas' date='2017-10-17T06:34:06Z'>
		I had to go to the code too to understand how to handle unbalanced data with random Forest .


i have highly unbalanced data with a ratio of 1 / 10 for the classWeight , but the optimal results for the randomForest is to  put everything in false class...    any advice to be sure I train the classifier the right way?


the default classWeight is [1, 1] but this default assumption (of a balanced data) is really unlikely. From a end-user point of view, why is this classWeight parameters not more visible in the public training methods? trainer.train(dataset, prior)  ?


should not be computed automatically from the training test to set the default classWeight values?


		</comment>
		<comment id='5' author='javierfvargas' date='2017-10-18T14:12:24Z'>
		

For unbalanced data, firstly try to set a proper classWeight. If your ratio is 1: 10 for false : true, start with [1, 10] for classWeight and do some adjustment based on experiment results. Second, use the posterior probability and choose a cutoff by yourself. The default predict() just use 0.5, which may not be proper to you.


There is no good theory to find the classWeight for all data. The caller should do some experiments to find the good one for their use cases. If I change the default value to other values, some other people will complain that it is not  good for their data :)


		</comment>
	</comments>
</bug>