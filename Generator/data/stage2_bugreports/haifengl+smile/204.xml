<bug id='204' author='pvillacorta-stratio' open_date='2017-08-02T07:17:20Z' closed_time='2017-08-09T03:12:45Z'>
	<summary>ArrayIndexOutOfBounds in DecisionTree.java - disable parallel processing?</summary>
	<description>
Hi,
&lt;denchmark-link:https://github.com/haifengl&gt;@haifengl&lt;/denchmark-link&gt;
 I am calling method smile.classification.cart from Scala inside a call to Spark's mapPartitions. The method should run sequentially in an executor with a portion ("partition") of the data (no shared memory or anything similar). Do you see any issue with this? I am getting an ArrayIndexOutOfBoundsException in line 599 of DecisionTree.java. It seems that "i" (which are the values of order[j]) gets some value greater than the length of the samples array. All my columns are numeric.
Can you think of any "global state" that could be working bad? I guess there is some sort of multi-core parallel processing under the hood in Smile, maybe I just have to disable that option... Note that if I repeat the experiment without Spark with exactly the same portion of data that caused the exception (just calling Smile from a sequential Scala code) then everything works fine.
Thanks
	</description>
	<comments>
		<comment id='1' author='pvillacorta-stratio' date='2017-08-02T13:42:29Z'>
		Smile uses multicores when possible. But Spark does that too. So there is some conflict. You can use the JVM flag -Dsmile.threads=1 to force smile to run in single thread.
		</comment>
		<comment id='2' author='pvillacorta-stratio' date='2017-08-28T07:15:32Z'>
		Thanks so much. Tried this before my vacation :-)
I noticed some change (it is actually faster!) but unfortunately it did not fix the problem. Is there anything else related to the parallel processing that can/should be disabled?
Thanks again
		</comment>
		<comment id='3' author='pvillacorta-stratio' date='2017-08-28T17:30:15Z'>
		Can you provide more information about the data, code snippets of calling smile, stack trace, etc? Thanks.
		</comment>
		<comment id='4' author='pvillacorta-stratio' date='2017-08-30T08:26:47Z'>
		This is my stack trace (Smile version 1.3.1 but I am getting the same error with 1.4.0):
java.lang.ArrayIndexOutOfBoundsException: 4122
at smile.classification.DecisionTree$TrainNode.findBestSplit(DecisionTree.java:599)
at smile.classification.DecisionTree$TrainNode.findBestSplit(DecisionTree.java:531)
at smile.classification.DecisionTree$TrainNode.split(DecisionTree.java:717)
at smile.classification.DecisionTree.(DecisionTree.java:987)
at smile.classification.DecisionTree.(DecisionTree.java:845)
at smile.classification.Operators$$anonfun$cart$1.apply(Operators.scala:585)
at smile.classification.Operators$$anonfun$cart$1.apply(Operators.scala:585)
at smile.util.package$time$.apply(package.scala:49)
at smile.classification.Operators$class.cart(Operators.scala:584)
at smile.classification.package$.cart(package.scala:130)
I am calling it like this:
smile.classification.cart(fold1selectedFeatures, fold1labels, fold1data.length / 4, selectedAttributes)
where selectedAttributes is an Array[Attribute] in which every element has been created like NumericAttribute(s"v$attIndex") with increasing attIndex numbers (no repeated). The most strange thing, as I said, is that running this outside of Spark does not yield this error. The dataset used (fold1selectedFeatures together with the corresponding element from fold1labels at the end of each line) is attached (gzipped csv,  522 examples, 973 features plus the label column)
&lt;denchmark-link:https://github.com/haifengl/smile/files/1262839/error_fold_data.txt.gz&gt;error_fold_data.txt.gz&lt;/denchmark-link&gt;

Thanks
		</comment>
		<comment id='5' author='pvillacorta-stratio' date='2017-09-07T08:39:45Z'>
		Any suggestion?
		</comment>
		<comment id='6' author='pvillacorta-stratio' date='2017-09-07T13:55:26Z'>
		Can you please provide complete code for me to reproduce? Thanks!
		</comment>
	</comments>
</bug>