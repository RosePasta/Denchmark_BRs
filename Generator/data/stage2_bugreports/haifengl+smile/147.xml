<bug id='147' author='mahendrathapa' open_date='2017-01-30T06:58:42Z' closed_time='2017-02-01T01:42:29Z'>
	<summary>Neural Network `computeOutputError()` method.</summary>
	<description>
At line number 708:  error += 0.5 * g * g.
where g = (output[i] - out) * out * (1.0 - out)
I understand the derivation of g but did not understand the derivation of error as error += 0.5 * g * g.
As I understood, according to the Geoffrey Hinton tutorial on Neural Network the error function is
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/10564070/22414640/3fb56fe2-e6e9-11e6-8247-ae753dfcd19b.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='mahendrathapa' date='2017-01-31T17:04:20Z'>
		Hi, the formula you post is not the error function, but the gradient of error function against to the weight. The g calculates this exactly (without x_i part). In the adjustWeights function, this gradient and x_i are used to update the weight.
But we do have a bug here after revisiting the code. The error should be 0.5 * (output[i] - out) ^2. That is that the lines
&lt;denchmark-code&gt;            if (errorFunction == ErrorFunction.LEAST_MEAN_SQUARES &amp;&amp; activationFunction == ActivationFunction.LOGISTIC_SIGMOID) {
                g *= out * (1.0 - out);
            }
&lt;/denchmark-code&gt;

should be moved down below the error calculation and above
&lt;denchmark-code&gt;gradient[i] = g;
&lt;/denchmark-code&gt;

Fortunately, this error is never used. So this bug should not caused any real problems. I fixed the bug. Thank you very much!
		</comment>
	</comments>
</bug>