<bug id='528' author='albertmolinermrf' open_date='2020-03-29T09:18:12Z' closed_time='2020-04-01T13:07:20Z'>
	<summary>Gradient backpropagation in ReLU</summary>
	<description>
&lt;denchmark-h:h2&gt;Expected behaviour&lt;/denchmark-h&gt;

In an MLP, the gradient of the upper layers has to be backpropagated to lower layers for updates to be calculated properly.
&lt;denchmark-h:h2&gt;Actual behaviour&lt;/denchmark-h&gt;

When using a v2.2.2 rectifier layer, the gradient from the upper layer is always overridden with either 0 or 1. This causes rectifier layers to be unable to minimize the cost function as they should.
I think the problem lies in this line:



smile/core/src/main/java/smile/base/mlp/ActivationFunction.java


         Line 95
      in
      acf15aa






 g[i] = y[i] &gt; 0 ? 1 : 0; 





A * is missing before =, so that the gradient from the upper layer is also taken into account.
I can provide examples of the consequences of the wrong behaviour if needed.
	</description>
	<comments>
		<comment id='1' author='albertmolinermrf' date='2020-03-29T15:21:55Z'>
		ReLU(x) gradient is 1 if x &gt; 0 or 0 otherwise. Note that y in g(g, y)  is the output of weight * input. That is, it is x in ReLU(x)
		</comment>
		<comment id='2' author='albertmolinermrf' date='2020-03-30T10:43:35Z'>
		Indeed, the gradient of max(0, x) is either 0 or 1.
However, the gradient of the cost function over a weight in a ReLU is, applying the chain rule, the gradient of the output of the ReLU over the weight (either 0 or 1) multiplied by the gradient of the cost function over the output of the ReLU (the incoming g). Currently, the latter term is discarded.
Consider how the gradient is applied to the particular case of bias, which is simpler to analyze:



smile/core/src/main/java/smile/base/mlp/Layer.java


         Line 144
      in
      482f63d






 double db = eta * gradient[i]; 





If in a ReLU gradient has only 0 or 1 values, then updateBias is always positive and bias will either remain the same or increase (always by the same amount), but never decrease.
Bias should be able to increase, decrease or remain stable depending on the characteristics of the data and model. The multiplication by the gradient of the cost function allows for this exactly in the required way, by the chain rule.
		</comment>
		<comment id='3' author='albertmolinermrf' date='2020-03-30T13:37:35Z'>
		Thanks. I fix it:
&lt;denchmark-code&gt;g[i] *= y[i] &gt; 0 ? 1 : 0;
&lt;/denchmark-code&gt;

The MSE of regression test is generally smaller.
		</comment>
		<comment id='4' author='albertmolinermrf' date='2020-04-01T13:07:19Z'>
		&lt;denchmark-link:https://github.com/albertmolinermrf&gt;@albertmolinermrf&lt;/denchmark-link&gt;
 v2.3 is out with the fix.
		</comment>
	</comments>
</bug>