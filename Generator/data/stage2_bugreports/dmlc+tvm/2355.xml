<bug id='2355' author='tqchen' open_date='2018-12-30T22:05:48Z' closed_time='2020-05-11T00:13:59Z'>
	<summary>[SPIRV] Incorrect Vulkan Result on Mobile GPU</summary>
	<description>
There have been several incidences reported about incorrect results when deploying to MobileGPU via vulkan. We will need to look into this.
More specifically, it would be really useful to produce a minimum reduced example from the end to end ones, to use debug_runtime to get the output of each intermediate step and compare them with the CPU or OpenCL version. Then we can look into which specific setup causes the problem.
We would love to have volunteers from the community to look into this issue.
Related threads:

https://discuss.tvm.ai/t/vulkan-deploy-issue-with-latest-code/604/7
https://discuss.tvm.ai/t/huawei-p20-pro-vulkan-output-is-not-proper/702

cc &lt;denchmark-link:https://github.com/eqy&gt;@eqy&lt;/denchmark-link&gt;

Possible steps to debug the issue:

Get an end to end example that produces the wrong result
Use debug runtime to run up to k-steps, dump to local files
Dump the output of each step using a different backend(opencl or cpu)
Check at which specific step the result went wrong, compare the generated code, use the dumped data to generate a minimum reproducible example.
Reduce the schedule of the errored code, to a minimum one to see if there is anything wrong.

	</description>
	<comments>
		<comment id='1' author='tqchen' date='2018-12-31T05:07:42Z'>
		I reported this issue in the post you mentioned &lt;denchmark-link:https://discuss.tvm.ai/t/vulkan-deploy-issue-with-latest-code/604/7&gt;here&lt;/denchmark-link&gt;
.

Get an end to end example that produces the wrong result

I can post a small sample to reproduce this for review and discussion in a github repo.

Use debug runtime to run up to k-steps, dump to local files ...

I was looking for mechanisms to support this from C++ (asked &lt;denchmark-link:https://discuss.tvm.ai/t/layer-serialization-from-c/1406&gt;here&lt;/denchmark-link&gt;
).   Can you share any pointers or examples that might help illustrate how to perform such logging from C++?
		</comment>
		<comment id='2' author='tqchen' date='2018-12-31T18:50:01Z'>
		You can setup an  RPC server in the mobile and use &lt;denchmark-link:https://docs.tvm.ai/api/python/graph_runtime.html#tvm.contrib.graph_runtime.GraphModule.debug_get_output&gt;https://docs.tvm.ai/api/python/graph_runtime.html#tvm.contrib.graph_runtime.GraphModule.debug_get_output&lt;/denchmark-link&gt;
 to get k-th output. Then you can just log the output using numpy's typical serialization mechanism
		</comment>
		<comment id='3' author='tqchen' date='2019-01-02T04:15:47Z'>
		Thanks.  That pointed me in the right direction.  I'm able to use the same mechanism from C++ with tvm.graph_runtime_debug.create and the get_output_by_layer packaged function calls.
&lt;denchmark-link:https://gist.github.com/headupinclouds/d87df2dcc9603589b3b9b9cb00f26011#file-tvm_deploy_gpu_sample-cpp-L521&gt;https://gist.github.com/headupinclouds/d87df2dcc9603589b3b9b9cb00f26011#file-tvm_deploy_gpu_sample-cpp-L521&lt;/denchmark-link&gt;

If I use opt_level=0 the example does return the correct result on the same Android device.  It is only when opt_level &gt; 0 where things break down.
&lt;denchmark-code&gt;with nnvm.compiler.build_config(opt_level=0):
    graph, lib, params = nnvm.compiler.build(sym, target, shape_dict, params=params, target_host=target_host)
&lt;/denchmark-code&gt;

The same code with opt_level=3 (any &gt;= 1) works on Android with llvm (cpu) and on the Ubuntu host for all tested back-ends: llvm (cpu), OpenGL, Vulkan, OpenCL, and gpu (cuda).
I'll see if I can understand the root cause.
		</comment>
		<comment id='4' author='tqchen' date='2019-01-06T22:32:43Z'>
		I've posted an initial C++ example to help reproduce the issue here:
&lt;denchmark-link:https://github.com/headupinclouds/tvm_cpp_test&gt;https://github.com/headupinclouds/tvm_cpp_test&lt;/denchmark-link&gt;

In addition to the observations noted above (the same example works in every tested configuration except Android w/ Vulkan), I've noticed the error:

is reproducible across runs (consistent but wrong)
occurs in the first actual nnvm fused layer (fuse_conv2d_broadcast_add_relu below)
is eliminated by using opt_level=0

head of compiled graph from_mxnet.json w/ opt_level=3 where output is incorrect (fusion)
&lt;denchmark-code&gt;{
  "nodes": [
    {
      "op": "null", 
      "name": "data", 
      "inputs": []
    }, 
    {
      "op": "null", 
      "name": "resnetv10_conv0_weight_sc", 
      "inputs": []
    }, 
    {
      "op": "null", 
      "name": "batch_norm0_add_beta_expand", 
      "inputs": []
    }, 
    {
      "op": "tvm_op", 
      "name": "relu0", 
      "attrs": {
        "flatten_data": "0", 
        "func_name": "fuse_conv2d_broadcast_add_relu", 
        "num_inputs": "3", 
        "num_outputs": "1"
      }, 
      "inputs": [[0, 0, 0], [1, 0, 0], [2, 0, 0]]
    }, 
&lt;/denchmark-code&gt;

head of compiled graph from_mxnet.json w/ opt_level=0 where output is okay (no fusion)
&lt;denchmark-code&gt;{
  "nodes": [
    {
      "op": "null", 
      "name": "data", 
      "inputs": []
    }, 
    {
      "op": "null", 
      "name": "resnetv10_conv0_weight", 
      "inputs": []
    }, 
    {
      "op": "tvm_op", 
      "name": "conv2d0", 
      "attrs": {
        "flatten_data": "0", 
        "func_name": "fuse_conv2d", 
        "num_inputs": "2", 
        "num_outputs": "1"
      }, 
      "inputs": [[0, 0, 0], [1, 0, 0]]
    }, 
    {
      "op": "null", 
      "name": "resnetv10_batchnorm0_running_var", 
      "inputs": []
    }, 
    {
      "op": "tvm_op", 
      "name": "batch_norm0_add_eps", 
      "attrs": {
        "flatten_data": "1", 
        "func_name": "fuse___add_scalar__", 
        "num_inputs": "1", 
        "num_outputs": "1"
      }, 
      "inputs": [[3, 0, 1]]
    }, 
&lt;/denchmark-code&gt;

I've saved the flattened tensor ascii logging from the tests outlined in the repository for convenience in github storage for that repository &lt;denchmark-link:https://github.com/headupinclouds/tvm_cpp_test/releases/download/v0.0.0/vulkan.tar.gz&gt;here&lt;/denchmark-link&gt;

You can download and inspect ubuntu vs android vulkan differences in the first fused layer as follows:
&lt;denchmark-code&gt;wget https://github.com/headupinclouds/tvm_cpp_test/releases/download/v0.0.0/vulkan.tar.gz
tar zxvf vulkan.tar.gz
name=tvm_0003_relu0.txt;  paste -d' ' android/${name} &lt;(awk '{print $NF}' ubuntu/${name}) | awk '{ s1=$(NF); s2=$(NF-1); d=(s1 &gt; s2) ? (s1-s2) : (s2-s1); if(d/(s2+s1+1e-6f) &gt; 0.05) { print " '$name' " NR " " $0 " (" d ")" } }' | less
&lt;/denchmark-code&gt;

The errors seems to be related to the codgen fusion step (opt_level &gt; 0).  The codegen details are still fairly opaque to me at this point, so any additional pointers or direction on how to proceed would be appreciated.  There is some overhead associated with setting up the experiment, so if there are additional tests or logging experiments I can perform, please let me know.
&lt;denchmark-link:https://github.com/tqchen&gt;@tqchen&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/eqy&gt;@eqy&lt;/denchmark-link&gt;
 
It is worth mentioning that the C++ executable in that example runs through to completion (w/ the wrong result) but it hangs on exit &lt;denchmark-link:https://github.com/headupinclouds/tvm_cpp_test/blob/9c41ead301545120975d55c3806689d7e610bb71/tvm_deploy_gpu_sample.cpp#L299&gt;here&lt;/denchmark-link&gt;
.  It terminates properly in all other tested configurations, so I suspect something is blocking in a tearm down step somewhere.  Please let me know if you think I should file an independent issue for that.
		</comment>
		<comment id='5' author='tqchen' date='2019-01-09T02:06:28Z'>
		Thanks for your work on this! Were you able to verify that it was the fusion pass in particular that produces the error (e.g., by disabling/enabling other passes to check &lt;denchmark-link:https://discuss.tvm.ai/t/different-output-values-when-setting-opt-level-3-in-nnvm-compiler-build-config/1392&gt;https://discuss.tvm.ai/t/different-output-values-when-setting-opt-level-3-in-nnvm-compiler-build-config/1392&lt;/denchmark-link&gt;
).
		</comment>
		<comment id='6' author='tqchen' date='2019-01-09T03:02:19Z'>
		
Were you able to verify that it was the fusion pass in particular that produces the error opt_level=2 triggers an error.

I don't know enough to say that specifically.
[EDIT: Although, since the output is incorrect for all opt_level != 0 (on Android), and OpFusion seems to be the only step that runs w/ opt_level=1, then I think it is safe to say that OpFusion is sufficient to trigger the issue on Android with this configuration for this case.  As mentioned previously, building for and running with Vulkan on an Ubuntu host works fine for all opt_level &gt;= 0, so I'm not sure it is OpFusion per se that is actually causing the issue, as opposed to uncovering a bug elsewhere related to the Vulkan back-end.  (I have little familiarity with TVM, and many parts of the framework are still black box to me, so I want to be careful about making strong statements based on these tests ðŸ˜„).]
Here is the output for opt_level=0,1,2,3 using the default OPT_PASS_LEVEL settings
&lt;denchmark-code&gt;OPT_PASS_LEVEL = {
    "SimplifyInference": 0,
    "PrecomputePrune": 2,
    "OpFusion": 1,
    "FoldScaleAxis": 3,
    "AlterOpLayout": 3,
}
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;opt_level=3&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;The maximum position in output vector is: 669
Expected 282 but got: 669
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;opt_level=2&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;error: [21:36:32] /dl/mxnet/3rdparty/tvm/apps/howto_deploy/../../src/runtime/module_util.cc:53: Check failed: ret == 0 (-1 vs. 0) [21:36:32] /dl/mxnet/3rdparty/tvm/apps/howto_deploy/../../src/runtime/vulkan/vulkan_module.cc:328: Check failed: __e == VK_SUCCESS Vulan Error, code=-3: VK_ERROR_INITIALIZATION_FAILED
terminating with uncaught exception of type dmlc::Error: [21:36:32] /dl/mxnet/3rdparty/tvm/apps/howto_deploy/../../src/runtime/vulkan/vulkan_device_api.cc:345: Check failed: __e == VK_SUCCESS Vulan Error, code=-3: VK_ERROR_INITIALIZATION_FAILED
Aborted 
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;opt_level=1&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;The maximum position in output vector is: 574
Expected 282 but got: 574
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;opt_level=0&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;The maximum position in output vector is: 282
&lt;/denchmark-code&gt;

Please let me know if there are other tests that could be useful.
Is it possible to debug this at the level of individual Vulkan commands somehow?
		</comment>
		<comment id='7' author='tqchen' date='2019-01-09T23:49:05Z'>
		Maybe a way to simplify the debugging is to see if the behavior changes for smaller pieces of the graph. As it is, it may be difficult to trace exactly the error is when doing a full end-to-end inference. Can you try checking what happens, say, when only running the first layer---adding layers until the problem resurfaces?
		</comment>
		<comment id='8' author='tqchen' date='2019-01-10T00:00:31Z'>
		Thanks for the feedback.  In this case it actually does seem to occur directly in the first real layer of the fused/optimized network  in the post &lt;denchmark-link:https://github.com/dmlc/tvm/issues/2355#issuecomment-451781350&gt;above&lt;/denchmark-link&gt;
.  I was hoping there might be some way to "lower" to human readable Vulkan instructions that I could test sequentially, but I don't know enough to understand if that is possible or not.
You can copy and paste this one liner to see just the diff for that layer:
&lt;denchmark-code&gt;name=tvm_0003_relu0; wget https://github.com/headupinclouds/tvm_cpp_test/releases/download/v0.0.0/vulkan.tar.gz &amp;&amp; tar zxvf vulkan.tar.gz &amp;&amp; diff vulkan/android/${name}.txt vulkan/ubuntu/${name}.txt
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='tqchen' date='2019-01-10T00:49:46Z'>
		Thanks for this; it is useful to know that the problem can be triggered with a single fused conv2d layer.
I think the next step is to check if the problem persists when using a default schedule for the operator.
For example, we could manually define a direct 2d convolution with minimal scheduling (e.g., just bind threads to a single dimension) while inlining the next operation (to simulate fusion).
		</comment>
		<comment id='10' author='tqchen' date='2019-01-11T13:57:38Z'>
		
I think the next step is to check if the problem persists when using a default schedule for the operator.

Okay, this sounds good, although I'll need help with the details (unless you have an example close to what you are suggesting already).  It sounds like this would be a small unit test entirely in TVM (no NNVM or end-to-end CNN) that runs something like conv2d + relu on a simple test input tensor ones(64,64,3)?  We would make a small python script, then build for Android + Vulkan and run the test as a C++ application on the device.  Right?
Maybe the following TVM tutorial is a reasonable starting point?
&lt;denchmark-link:https://docs.tvm.ai/tutorials/topi/intro_topi.html#fusing-convolutions&gt;https://docs.tvm.ai/tutorials/topi/intro_topi.html#fusing-convolutions&lt;/denchmark-link&gt;

Are there other generic unit tests we should be running (in addition) to help test the Vulkan back-end on Android?  Maybe these &lt;denchmark-link:https://github.com/dmlc/tvm/tree/master/tests/cpp&gt;https://github.com/dmlc/tvm/tree/master/tests/cpp&lt;/denchmark-link&gt;
?
[EDIT: It looks like &lt;denchmark-link:https://github.com/dmlc/tvm/blob/master/tests/cpp/packed_func_test.cc&gt;packed_func_test&lt;/denchmark-link&gt;
 is the most relevant one, since it is limited to run time functionality: .  Perhaps we can build on that?]
		</comment>
		<comment id='11' author='tqchen' date='2019-01-11T17:02:29Z'>
		Yes, your understanding is correct. That TVM tutorial should be almost exactly what we need. In this case, we would manually schedule the fused convolution (as minimally as possible) instead of relying on a built-in TOPI schedule.
If you have a C++ function deploy flow on device, we can use that, but otherwise it would be simpler to just use the RPC server so that we can make changes quickly without rebuilding anything manually for the device.
I am not sure we need to use the packed function test, because more basic functions (e.g., vector-add, or even un-fused conv2d would tell us that the runtime is working at a more primitive level).
Basically the purpose of this is to see if the problem lies within the schedule somewhere, or occurs any time we invoke fusion. I hope it is not a corner case where a specific schedule + fusion are needed to trigger it.
		</comment>
		<comment id='12' author='tqchen' date='2019-01-20T18:02:32Z'>
		I made a smaller test project &lt;denchmark-link:https://github.com/headupinclouds/tvm_intro_topi/blob/69a709e48ddb53ad7856c4c18bb99ea9f20d58c5/intro_topi.py#L101-L104&gt;tvm_intro_topi&lt;/denchmark-link&gt;
 containing just the conv2d + relu chain along with a C++ test executable.
&lt;denchmark-code&gt;data = tvm.placeholder((1, 3, 224, 224))
kernel = tvm.placeholder((10, 3, 5, 5))

with tvm.target.create("cuda"):
    conv = topi.nn.conv2d(data, kernel, strides=1, padding=2, dilation=1)
    out = topi.nn.relu(conv)
    sconv = topi.generic.nn.schedule_conv2d_nchw(out)
    print(tvm.lower(sconv, [data, kernel], simple_mode=True))
&lt;/denchmark-code&gt;

This is based on the original TVM TOPI tutorial &lt;denchmark-link:https://github.com/dmlc/tvm/blob/45456b1495048883fa3b3f2095c6747b8188db85/tutorials/topi/intro_topi.py#L116-L123&gt;intro_topi.py&lt;/denchmark-link&gt;
 we discussed above.
Both the C++ and Vulkan versions run correctly on the same Android test device.
[EDIT: I'm seeing minor floating point differences between the two examples, but they are a very close.]

we would manually schedule the fused convolution (as minimally as possible)

What modifications do you recommend to the topi_intro.py script to achieve this?
Thanks for the help.
		</comment>
		<comment id='13' author='tqchen' date='2019-01-20T20:52:19Z'>
		The from_mxnet.py example is using &lt;denchmark-link:https://github.com/headupinclouds/tvm_cpp_test/blob/9c41ead301545120975d55c3806689d7e610bb71/from_mxnet.py#L78&gt;resnet18_v&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;block = get_model('resnet18_v1', pretrained=True)
&lt;/denchmark-code&gt;

Since we seem to see issues in the first block, we can probably replicate that from the gluon-cv definition  &lt;denchmark-link:https://github.com/dmlc/gluon-cv/blob/5e7a3a2f86d0fe4c8ad394c03618c7abca07f608/gluoncv/model_zoo/resnet.py#L392-L395&gt;here&lt;/denchmark-link&gt;
.  I guess we want some (or all) of this part:
&lt;denchmark-code&gt;                self.features.add(nn.Conv2D(channels[0], 7, 2, 3, use_bias=False))
                self.features.add(norm_layer(**({} if norm_kwargs is None else norm_kwargs)))
                self.features.add(nn.Activation('relu'))
                self.features.add(nn.MaxPool2D(3, 2, 1))
&lt;/denchmark-code&gt;

The details of how to replicate the NNVM fusion (in particular, replicating OpFusion) are not currently clear to me.  I'll try read through that code in more detail.
		</comment>
		<comment id='14' author='tqchen' date='2019-01-22T23:35:05Z'>
		I think a simple thing to try would be to just tack on a few elementwise operations (e.g., add, subtract, multiply) following the convolution, and then trying .
&lt;denchmark-link:https://docs.tvm.ai/tutorials/language/schedule_primitives.html#compute-inline&gt;https://docs.tvm.ai/tutorials/language/schedule_primitives.html#compute-inline&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='tqchen' date='2019-02-18T23:38:47Z'>
		Do we have any followup on this &lt;denchmark-link:https://github.com/headupinclouds&gt;@headupinclouds&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/eqy&gt;@eqy&lt;/denchmark-link&gt;
 ?
		</comment>
		<comment id='16' author='tqchen' date='2019-02-19T14:35:40Z'>
		I added explicit batch-norm affine arithmetic and experimented with compute_inline in the test repo I posted, but it didn't seem to trigger an error.  I'll push the updated changes.   I guess we can try to translate the head of the resent18 example into the standalone test project to help isolate things.
&lt;denchmark-link:https://github.com/headupinclouds/tvm_intro_topi/blob/85e5a2330c1230f70dedcb961fdfbae869774712/intro_topi.py#L113-L121&gt;https://github.com/headupinclouds/tvm_intro_topi/blob/85e5a2330c1230f70dedcb961fdfbae869774712/intro_topi.py#L113-L121&lt;/denchmark-link&gt;

		</comment>
		<comment id='17' author='tqchen' date='2019-05-22T10:23:13Z'>
		Hi. I am getting mismatched result when running &lt;denchmark-link:https://github.com/dmlc/tvm/blob/master/apps/android_rpc/tests/android_rpc_test.py&gt;apps/android_rpc/tests/android_rpc_test.py&lt;/denchmark-link&gt;
 with vulkan, the example just runs a sum on mobile gpu, so no conv2d at all, did it already happen to you?
		</comment>
		<comment id='18' author='tqchen' date='2020-03-19T21:25:31Z'>
		NOTE: the vulkan runtime has been rewritten, would be great to confirm again if the result is correct under the new runtime.
		</comment>
		<comment id='19' author='tqchen' date='2020-05-11T00:14:26Z'>
		close for now due to inactive status, we will open new thread to track problems if there are more followups
		</comment>
	</comments>
</bug>