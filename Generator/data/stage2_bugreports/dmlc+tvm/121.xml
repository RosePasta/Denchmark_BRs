<bug id='121' author='yzhliu' open_date='2017-05-06T14:17:46Z' closed_time='2017-05-07T12:36:30Z'>
	<summary>cuModuleUnload returns CUDA_ERROR_DEINITIALIZED</summary>
	<description>
Env:

Tesla K80
Driver Version 352.39
Centos 7.3.1611
CUDA 7.5
cudnn 7.5 v5.1

Source code:
import tvm
import numpy as np

n = tvm.var("n")
m = 128
A = tvm.placeholder((n, m), name='A')
k = tvm.reduce_axis((0, m), "k")
B = tvm.compute((n,), lambda i: tvm.sum(A[i, k], axis=k), name="B")

s = tvm.create_schedule(B.op)
s[B.op].bind(B.op.axis[0], tvm.thread_axis("blockIdx.x"))
s[B.op].bind(k, tvm.thread_axis("threadIdx.x"))
print(tvm.lower(s, [A, B], with_api_wrapper=False))

fcuda = tvm.build(s, [A, B], "cuda")
print(fcuda.imported_modules[0].get_source())

ctx  = tvm.gpu(0)
a = tvm.nd.array(np.random.uniform(size=(m, m)).astype(A.dtype), ctx)
b = tvm.nd.array(np.zeros(m, dtype=B.dtype), ctx)
fcuda(a, b)
np.testing.assert_allclose(
    b.asnumpy(),  np.sum(a.asnumpy(), axis=1), rtol=1e-4)
Output:
&lt;denchmark-code&gt;$ python reduce_axis.py 
produce B {
  // attr [iter_var(blockIdx.x, , blockIdx.x)] thread_extent = n
  // attr [reduce_temp] storage_scope = "local"
  allocate reduce_temp[float32 * 1]
  // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 128
  // attr [comm_reducer(result=(x + y), args=[x, y], identity_element=0.000000f)] reduce_scope = 0.000000f
  reduce_temp[0] = tvm_thread_allreduce(A[((blockIdx.x*128) + threadIdx.x)], (uint1)1, threadIdx.x)
  if ((threadIdx.x == 0)) {
    B[blockIdx.x] = reduce_temp[0]
  }
}

extern "C" __global__ void default_function__kernel0( float* A,  float* B) {
  int blockIdx_x = blockIdx.x;
  __shared__ float red_buf[128];
  int threadIdx_x = threadIdx.x;
  ((volatile __shared__  float*)red_buf)[threadIdx_x] = A[((blockIdx_x * 128) + threadIdx_x)];
  __syncthreads();
  if ((threadIdx_x &lt; 64)) {
    ((volatile __shared__  float*)red_buf)[threadIdx_x] = (((volatile __shared__  float*)red_buf)[threadIdx_x] + ((volatile __shared__  float*)red_buf)[(threadIdx_x + 64)]);
  }
  __syncthreads();
  if ((threadIdx_x &lt; 32)) {
    ((volatile __shared__  float*)red_buf)[threadIdx_x] = (((volatile __shared__  float*)red_buf)[threadIdx_x] + ((volatile __shared__  float*)red_buf)[(threadIdx_x + 32)]);
  }
  __syncthreads();
  if ((threadIdx_x &lt; 16)) {
    ((volatile __shared__  float*)red_buf)[threadIdx_x] = (((volatile __shared__  float*)red_buf)[threadIdx_x] + ((volatile __shared__  float*)red_buf)[(threadIdx_x + 16)]);
    ((volatile __shared__  float*)red_buf)[threadIdx_x] = (((volatile __shared__  float*)red_buf)[threadIdx_x] + ((volatile __shared__  float*)red_buf)[(threadIdx_x + 8)]);
    ((volatile __shared__  float*)red_buf)[threadIdx_x] = (((volatile __shared__  float*)red_buf)[threadIdx_x] + ((volatile __shared__  float*)red_buf)[(threadIdx_x + 4)]);
    ((volatile __shared__  float*)red_buf)[threadIdx_x] = (((volatile __shared__  float*)red_buf)[threadIdx_x] + ((volatile __shared__  float*)red_buf)[(threadIdx_x + 2)]);
    ((volatile __shared__  float*)red_buf)[threadIdx_x] = (((volatile __shared__  float*)red_buf)[threadIdx_x] + ((volatile __shared__  float*)red_buf)[(threadIdx_x + 1)]);
  }
  if ((threadIdx_x == 0)) {
    B[blockIdx_x] = ((volatile __shared__  float*)red_buf)[0];
  }
}

[22:15:05] dmlc-core/include/dmlc/logging.h:300: [22:15:05] src/runtime/cuda/cuda_module.cc:43: CUDAError: cuModuleUnload(module_[i]) failed with error: 
terminate called after throwing an instance of 'dmlc::Error'
  what():  [22:15:05] src/runtime/cuda/cuda_module.cc:43: CUDAError: cuModuleUnload(module_[i]) failed with error: 
已放弃
&lt;/denchmark-code&gt;

I debugged and found the return value is CUDA_ERROR_DEINITIALIZED.
The calculation result is correct (if I remove the CUDA_DRIVER_CALL check)
Any idea why this happened? &lt;denchmark-link:https://github.com/tqchen&gt;@tqchen&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='yzhliu' date='2017-05-06T15:59:07Z'>
		OK, this is due to our old friend static initialization and destruction order.
When python program exits, the dependent libraries get unloaded(via static destructor), CUDA driver was one of them, then fcuda's destructor goes next calling cudaModuleUnload. We should send a patch to simply ignore this error
		</comment>
		<comment id='2' author='yzhliu' date='2017-05-06T16:00:24Z'>
		This problem did not surface before because most script we have place these in a function, and module unloaded before cuda driver exits.
		</comment>
		<comment id='3' author='yzhliu' date='2017-05-06T16:07:39Z'>
		should be fixed by &lt;denchmark-link:https://github.com/apache/tvm/pull/122&gt;#122&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>