<bug id='67' author='josiahdavis' open_date='2017-08-18T16:47:27Z' closed_time='2019-05-16T16:49:10Z'>
	<summary>No instance key in output of canned estimator census sample</summary>
	<description>
As other individuals have noted:

According to the ML Engine documentation, an instance key is required to match the returned predictions with the input data (Source)

Would it be possible to modify the &lt;denchmark-link:https://github.com/GoogleCloudPlatform/cloudml-samples/tree/master/census/estimator/trainer&gt;canned estimator census sample&lt;/denchmark-link&gt;
 to include the instance key in the output?
This was addressed on &lt;denchmark-link:https://stackoverflow.com/questions/44381879/training-and-predicting-with-instance-keys/44443380#44443380&gt;StackOverflow&lt;/denchmark-link&gt;
 but it's not clear to me how to make this modification.
Thank you in advance for your help!
	</description>
	<comments>
		<comment id='1' author='josiahdavis' date='2017-08-18T18:26:49Z'>
		&lt;denchmark-link:https://github.com/puneith&gt;@puneith&lt;/denchmark-link&gt;
 Do you want to bring the code snippet from StackOverflow into the canned estimator example?
		</comment>
		<comment id='2' author='josiahdavis' date='2017-08-18T19:14:29Z'>
		That would be amazing. Thank you!
		</comment>
		<comment id='3' author='josiahdavis' date='2017-08-18T19:25:28Z'>
		In the meantime &lt;denchmark-link:https://github.com/josiahdavis&gt;@josiahdavis&lt;/denchmark-link&gt;
 on StackOverflow do you want to clarify which part of implementing the workaround has you confusing?
		</comment>
		<comment id='4' author='josiahdavis' date='2017-08-18T20:43:45Z'>
		I was initially trying to communicate on StackOverflow... unfortunately, being a new contributor I do not meet the reputation threshold to leave a comment :/ and I wasn't sure if it merited it's own question.
I'm struggling with what needs to be done to modify the input functionality. I'm a little confused as to the difference between json_input_fn() and generate_input_fn() and which modifications to make for inclusion of the instance key.
		</comment>
		<comment id='5' author='josiahdavis' date='2017-08-18T21:04:56Z'>
		generate_input_fn is for training or evaluation input functions (note that it reads data from files).
json_input_fn is for building a SavedModel binary (note that it uses placeholders).
Since you only need keys in prediction (i.e. for batch prediction or online prediction), you only need to add a key placeholder to the tensors expected by, and returned from the json_input_fn I'll add this context to the the StackOverflow answer
		</comment>
		<comment id='6' author='josiahdavis' date='2017-08-21T18:58:02Z'>
		Thanks so much for your help... I'm still missing something on my end.
&lt;denchmark-h:h4&gt;First I copy-and-pasted the function that you provided to add the key.&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;KEY = 'key'
def key_model_fn_gen(estimator):
    def _model_fn(features, labels, mode, params):
        key = features.pop(KEY)
        model_fn_ops = estimator.model_fn(
           features=features, labels=labels, mode=mode, params=params)
        model_fn_ops.predictions[KEY] = key
        return model_fn_ops
    return _model_fn
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Second, I modified the json_input_fn as per your example on StackOverflow so that is looks like this:&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;def json_serving_input_fn():
    """Build the serving inputs."""
    inputs = {}
    inputs[KEY] = tf.placeholder([None], dtype=tf.int64)
    for feat in INPUT_COLUMNS:
        inputs[feat.name] = tf.placeholder(shape=[None], dtype=feat.dtype)

    features = {
      key: tf.expand_dims(tensor, -1)
      for key, tensor in inputs.items()
    }
    return tf.contrib.learn.InputFnOps(features, None, inputs)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Third, I replaced the original return of the build_estimator() function with my interpretation of what you said to replace it with (but for the DNNLinearCombinedClassifier instead of the DNNClassifier):&lt;/denchmark-h&gt;

Original build_estimator() return
&lt;denchmark-code&gt;tf.contrib.learn.DNNLinearCombinedClassifier(
      config=config,
      linear_feature_columns=wide_columns,
      dnn_feature_columns=deep_columns,
      dnn_hidden_units=hidden_units or [100, 70, 50, 25],
      fix_global_step_increment_bug=True
  )
&lt;/denchmark-code&gt;

Modified build_estimator() return
&lt;denchmark-code&gt;tf.contrib.learn.Estimator(
        model_fn=key_model_fn_gen(
            tf.contrib.learn.DNNLinearCombinedClassifier(
          config=config,
          linear_feature_columns=wide_columns,
          dnn_feature_columns=deep_columns,
          dnn_hidden_units=hidden_units or [100, 70, 50, 25],
          fix_global_step_increment_bug=True)
        ),
        config=config
    )
&lt;/denchmark-code&gt;

Then when I run it I get the following error (edited to update error).
&lt;denchmark-code&gt;&lt;ipython-input-2-0035b3d4ac3d&gt; in _model_fn(features, labels, mode, params)
      2 def key_model_fn_gen(estimator):
      3     def _model_fn(features, labels, mode, params):
----&gt; 4         key = features.pop(KEY)
      5         model_fn_ops = estimator.model_fn(
      6            features=features, labels=labels, mode=mode, params=params)

KeyError: 'key'
&lt;/denchmark-code&gt;

Do you either (a) notice what I am doing wrong, or (b) are their other changes that need to be made to support the instance key (perhaps other functions that need to be changed)?
Let me know if you'd like me to ask this on StackOverflow as a new question and I'm happy to do so! Thanks so much!
		</comment>
		<comment id='7' author='josiahdavis' date='2017-08-21T19:25:13Z'>
		&lt;denchmark-link:https://github.com/elibixby&gt;@elibixby&lt;/denchmark-link&gt;
 Sounds like a great change to include.
		</comment>
		<comment id='8' author='josiahdavis' date='2017-08-22T19:13:46Z'>
		&lt;denchmark-link:https://github.com/josiahdavis&gt;@josiahdavis&lt;/denchmark-link&gt;
 If you are not training with a  value in your dataset, you need to restrict the key modification to only prediction mode. So:
&lt;denchmark-code&gt;KEY = 'key'
def key_model_fn_gen(estimator):
    def _model_fn(features, labels, mode, params):
       if mode == ModeKeys.PREDICT:
          key = features.pop(KEY)
        model_fn_ops = estimator.model_fn(
           features=features, labels=labels, mode=mode, params=params)
        if mode == ModeKeys.PREDICT:
          model_fn_ops.predictions[KEY] = key
        return model_fn_ops
    return _model_fn
&lt;/denchmark-code&gt;

Note that you'll need to add include EVAL as well, if it's in the eval dataset, etc.
		</comment>
		<comment id='9' author='josiahdavis' date='2017-08-22T19:32:46Z'>
		OK I changed it so that I am making 'gender' the key. (Not a good key I know, just for illustrating the functionality if the key is in the data). I got everything running without errors, but for some reason the 'gender' key isn't showing up in the output after running the batch prediction, only the "probabilities" and "classes" as before.
		</comment>
		<comment id='10' author='josiahdavis' date='2017-08-22T19:33:37Z'>
		Here is the code block that is controlling the export (I haven't made any changes here):
&lt;denchmark-code&gt;learn_runner.run(
      generate_experiment_fn(
          min_eval_frequency=args.min_eval_frequency,
          eval_delay_secs=args.eval_delay_secs,
          train_steps=args.train_steps,
          eval_steps=args.eval_steps,
          export_strategies=[saved_model_export_utils.make_export_strategy(
              model.SERVING_FUNCTIONS[args.export_format],
              exports_to_keep=1,
              default_output_alternative_key=None,
          )]
      ),
      run_config=run_config.RunConfig(model_dir=args.job_dir),
      hparams=hparam.HParams(**args.__dict__)
  )
&lt;/denchmark-code&gt;

Do I need to modify this code block so that the prediction output definition is updated to include the key/gender value?
		</comment>
		<comment id='11' author='josiahdavis' date='2017-08-22T19:39:15Z'>
		Here is a gist of what I have so far for &lt;denchmark-link:https://gist.github.com/josiahdavis/d3c1de31d83902c049cb5481b15da385&gt;model.py&lt;/denchmark-link&gt;
 I have not made any changes to the task.py file.
		</comment>
		<comment id='12' author='josiahdavis' date='2017-08-22T19:43:09Z'>
		Thanks so much for all your help with this, I really appreciate it.
		</comment>
		<comment id='13' author='josiahdavis' date='2017-08-22T19:47:32Z'>
		Hmmm, I'm not sure that should work. Note that you're using dict.pop on the key, meaning there will be no gender column available to the internal estimator. So I think this should error out? I would try providing a genuine new feature, "key". I adapted the code in the stack overflow answer to ignore key if it isn't present. Try that.
		</comment>
		<comment id='14' author='josiahdavis' date='2017-08-22T20:26:14Z'>
		(For the time being) I've been leaving the gender field out of the estimator by commenting it out of deep_columns and wide_columns lists in the build_estimator(). That's why it's not giving me an error (I think).
I think that &lt;denchmark-link:https://stackoverflow.com/questions/44381879/training-and-predicting-with-instance-keys&gt;Dobbysock's StackOverflow update&lt;/denchmark-link&gt;
 is relevant:

I just needed to alter it slightly to update the output alternatives in the model_fn_ops instead of just the prediction dict.

Unfortunately he didn't post what his solution entailed. Perhaps a modification like the following:
&lt;denchmark-code&gt;KEY = 'gender'
def key_model_fn_gen(estimator):
    def _model_fn(features, labels, mode):
        key = features.pop(KEY)
        params = estimator.params
        model_fn_ops = estimator._model_fn(features=features, labels=labels, mode=mode, params=params)
        model_fn_ops.predictions[KEY] = key
        model_fn_ops.output_alternatives[KEY] = key # &lt;== I THINK I NEED SOMETHING LIKE THIS
        print('\nMODEL_FN_OPS', model_fn_ops.output_alternatives, '\n')
        return model_fn_ops
    return _model_fn
&lt;/denchmark-code&gt;

That doesn't work. Although, I feel as though we are converging on a solution. Here is the full traceback:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "trainer/task.py", line 204, in &lt;module&gt;
    hparams=hparam.HParams(**args.__dict__)
  File "/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py", line 210, in run
    return _execute_schedule(experiment, schedule)
  File "/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py", line 47, in _execute_schedule
    return task()
  File "/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py", line 502, in train_and_evaluate
    export_results = self._maybe_export(eval_result)
  File "/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py", line 597, in _maybe_export
    eval_result=eval_result))
  File "/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/export_strategy.py", line 87, in export
    return self.export_fn(estimator, export_path, **kwargs)
  File "/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py", line 412, in export_fn
    checkpoint_path=checkpoint_path)
  File "/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py", line 1275, in export_savedmodel
    model_fn_ops, default_output_alternative_key))
  File "/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py", line 241, in get_output_alternatives
    sorted(output_alternatives.keys())))
TypeError: '&lt;' not supported between instances of 'str' and 'NoneType'
&lt;/denchmark-code&gt;

		</comment>
		<comment id='15' author='josiahdavis' date='2017-08-22T21:37:42Z'>
		Looks good, except that one of your output_alternative keys is None (hence the sort failing), My code was a little confused between core and contrib estimator. For contrib you need to set output_alternative for core you need to set export_outputs.
		</comment>
		<comment id='16' author='josiahdavis' date='2017-08-23T05:53:11Z'>
		Ah, I see.  Thanks so much for your help &lt;denchmark-link:https://github.com/elibixby&gt;@elibixby&lt;/denchmark-link&gt;
.
&lt;denchmark-code&gt;KEY = 'gender'
def key_model_fn_gen(estimator):
    def _model_fn(features, labels, mode):
        key = features.pop(KEY)
        params = estimator.params
        model_fn_ops = estimator._model_fn(features=features, labels=labels, mode=mode, params=params)
        model_fn_ops.predictions[KEY] = key
        model_fn_ops.output_alternatives[None][1][KEY] = key # &lt;== UPDATED
        print(model_fn_ops.output_alternatives)
        return model_fn_ops
    return _model_fn
&lt;/denchmark-code&gt;

For others' reference, this is what the model_fn_ops.output_alternatives prints out as:
&lt;denchmark-code&gt; {None: (3, {'probabilities': &lt;tf.Tensor 'binary_logistic_head/predictions/probabilities:0' shape=(?, 2) dtype=float32&gt;, 'classes': &lt;tf.Tensor 'binary_logistic_head/_classification_output_alternatives/classes_tensor:0' shape=(?, ?) dtype=string&gt;, 'gender': &lt;tf.Tensor 'ExpandDims:0' shape=(?, 1) dtype=string&gt;})} 
&lt;/denchmark-code&gt;

		</comment>
		<comment id='17' author='josiahdavis' date='2017-10-03T20:56:19Z'>
		+1 on this being a great thing to get into the census estimator example out of the box. If you just added a row number field to the training and test data and then incorporated it into the sample that would be really useful.
I've nearly got something working, i think, but of course would be great if this was already backed into the census example :)
		</comment>
		<comment id='18' author='josiahdavis' date='2017-10-03T21:48:51Z'>
		Ok i got this working from following the advice here. Thanks all.
In case any use to others here is the &lt;denchmark-link:https://gist.github.com/andrewm4894/ebd3ac3c87e2ab4af8a10740e85073bb#file-with_keys_model-py&gt;model.py&lt;/denchmark-link&gt;
 i am using that is working for me. I had already adapted it for my own problem so the columns etc are different to the census estimator but had not changed much else.
When i trained it locally and then tried to deploy it failed prediction preconditions. I think might have been that i was training with 1.3 and trying to deploy with 1.2.
But when i train with something like:
&lt;denchmark-code&gt;gcloud ml-engine jobs submit training $JOB_NAME \
    --stream-logs \
    --job-dir $OUTPUT_PATH \
    --runtime-version 1.2 \
    --config $HPTUNING_CONFIG \
    --module-name trainer.task \
    --package-path $PACKAGE_PATH \
    --region $REGION \
    --scale-tier STANDARD_1 \
    -- \
    --train-files $TRAIN_DATA \
    --eval-files $EVAL_DATA \
    --train-steps 10 \
    --verbosity DEBUG  \
    --eval-steps 5
&lt;/denchmark-code&gt;

and then deploy with something like:
&lt;denchmark-code&gt;gcloud ml-engine versions create $MODEL_VERSION \
    --model $LOB_MODEL_NAME \
    --origin $DEPLOYMENT_SOURCE \
    --runtime-version=1.2
&lt;/denchmark-code&gt;

it deploys fine and doing an online prediction such with something like:
&lt;denchmark-code&gt;gcloud ml-engine predict --model hl_clickmodel --version v1 --json-instances clickmodel/test_data.json
&lt;/denchmark-code&gt;

where test_data.json looks like
&lt;denchmark-code&gt;{"key":1,"post_day_pst":"e_thu","post_hour_bin_pst":"b_6am_to_10am","post_word_count":256,"user_sessions_per_day":1.38,"user_pageviews_per_session":5.45}
{"key":99,"post_day_pst":"e_thu","post_hour_bin_pst":"b_6am_to_10am","post_word_count":256,"user_sessions_per_day":1.38,"user_pageviews_per_session":5.45}
&lt;/denchmark-code&gt;

returns:
&lt;denchmark-code&gt;CLASSES       KEY     PROBABILITIES
[u'0', u'1']  [1.0]   [4.802407147508347e-07, 0.9999995231628418]
[u'0', u'1']  [99.0]  [4.802407147508347e-07, 0.9999995231628418]
&lt;/denchmark-code&gt;

One thing i'm not sure on is i think the key may need to just be a number as i had to define it in my input cols with something like:
&lt;denchmark-code&gt;INPUT_COLUMNS = [
    tf.feature_column.numeric_column(
        'key'),
    ...
]
&lt;/denchmark-code&gt;

I'm sure i might not be doing this the best way to will defo be refactoring once the census example has a way to do this from others who know best approach to take (am newbie to tf), but it seems to work so far with the model.py i linked to above.
		</comment>
		<comment id='19' author='josiahdavis' date='2017-10-24T17:59:34Z'>
		&lt;denchmark-link:https://github.com/andrewm4894&gt;@andrewm4894&lt;/denchmark-link&gt;
 Do you want to send a PR?
		</comment>
		<comment id='20' author='josiahdavis' date='2018-01-03T19:20:09Z'>
		&lt;denchmark-link:https://github.com/elibixby&gt;@elibixby&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/josiahdavis&gt;@josiahdavis&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/andrewm4894&gt;@andrewm4894&lt;/denchmark-link&gt;
 -
Thanks for your prior work on figuring out how to pass through instance keys when using CMLE + canned estimators. The code snippets above, in this &lt;denchmark-link:https://gist.github.com/andrewm4894/ebd3ac3c87e2ab4af8a10740e85073bb#file-with_keys_model-py&gt;gist&lt;/denchmark-link&gt;
, and the &lt;denchmark-link:https://stackoverflow.com/questions/44381879/training-and-predicting-with-instance-keys/44443380#44443380&gt;SO discussion&lt;/denchmark-link&gt;
 have been really helpful.
Would any of you know how to make the corresponding required changes for TF 1.4?
Where I'm stuck

What would be the TF 1.4 equivalent of the model_fn_ops.output_alternatives[None][1][KEY] = key line in the key_model_fn_gen function that returns a custom model_fn?
I think this is the primary change that I'm missing. I've been able to make the other analogous TF 1.4-compatible changes to my CMLE + canned estimator example, but I haven't been able to figure out how to translate this model_fn_ops.output_alternatives line to TF 1.4. Perhaps I need to modify the export_outputs field of an EstimatorSpec, but not sure how.
I believe this is what's preventing my instance key from appearing among the input &amp; output fields of my saved model's signature.

Perhaps as a result, I'm unable to pass through instance keys when using gcloud ml-engine local predict.

if the json-instances file passed to this prediction call does not contain the instance key (in my case, 'user_id'), I can successfully get predictions, but they don't contain the instance key.
If the json-instances file does contain the instance key, the prediction job fails with the message: prediction_lib.PredictionError: Invalid inputs: Unexpected tensor name: user_id (Error code: 1) because the instance key isn't among the input fields of the signature, per the preprocess() and _get_columns() methods in .../cloud_ml_engine_sdk/prediction/prediction_lib.py.
Using the saved_model_clito inspect the SignatureDef of my saved model, I confirmed that the instance key ('user_id') is NOT listed among the inputs or outputs.

What I've been able to do thus far
I've made other changes to my own versions of key_model_fn_gen(), build_estimator() and json_serving_input_fn() which have enabled me to pass through my instance keys to the output of estimator.predict(), where my 'estimator' is a DNNClassifier instance.
But I'd really like to be able to use gcloud ml-engine predict rather than having to spin up my own Compute Engine instances to score my records.
In my key_model_fn_gen(), for instance, I'm doing the following, which reports additional metrics in evaluation mode and returns the instance keys in the predictions dict of my EstimatorSpec.
estimatorSpec = estimator._call_model_fn(
      features=features, labels=labels, mode=mode, config=estimator.config)

estimatorSpec.predictions[instance_key] = instance_key_feature

if mode == tf.estimator.ModeKeys.EVAL:
      # evaluate precision &amp; recall at various prob thresholds
      prob_thresholds = np.linspace(0.05, 0.50, 10).tolist()

      # Tensor of shape `TensorShape([Dimension(...), Dimension(2)])` 
      probs_pred = estimatorSpec.predictions['probabilities']

      # get slice of shape `TensorShape([Dimension(...), Dimension(1)])`
      # containing each user's prob of 'conversion', however defined
      probs_pred_1 = tf.slice(probs_pred, [0, 1], [tf.shape(probs_pred)[0], 1])

      # precision &amp; recall based on various thresholds for classification
      estimatorSpec.eval_metric_ops['precision_values'] = \
        tf.metrics.precision_at_thresholds(labels, probs_pred_1, prob_thresholds)

      estimatorSpec.eval_metric_ops['recall_values'] = \
        tf.metrics.recall_at_thresholds(labels, probs_pred_1, prob_thresholds)
My json_serving_input_fn is unchanged, but note that tf.feature_column.numeric_column(INSTANCE_KEY_COLUMN, dtype=tf.int64) is one of elements of the INPUT_COLUMNS list.
def json_serving_input_fn():
  """
  Build the serving inputs. 

  Changes: Added instance key to `inputs` dict.
  """
  inputs = {}

  for feat in INPUT_COLUMNS:
    inputs[feat.name] = tf.placeholder(shape=[None], dtype=feat.dtype)
  
  serving_input_rcvr = tf.estimator.export.ServingInputReceiver(inputs, inputs)
  tf.logging.info('serving input receiver: {}'.format(serving_input_rcvr))  
  return serving_input_rcvr
Thanks in advance for any insight you can provide here.
		</comment>
		<comment id='21' author='josiahdavis' date='2018-01-03T22:20:53Z'>
		&lt;denchmark-link:https://github.com/dawu76&gt;@dawu76&lt;/denchmark-link&gt;
 It might be helpful if you could clarify which parts of your code use contrib vs. core APIs.
If you are using contrib, I think the issue is in your json_serving_input_fn -- I think you want to return a function which returns an InputFnOps object (see the gist you linked). Try this:
&lt;denchmark-code&gt;def json_serving_input_fn():
  """
  Build the serving inputs. 

  Changes: Added instance key to `inputs` dict.
  """
  inputs = {}

  for feat in INPUT_COLUMNS:
    inputs[feat.name] = tf.placeholder(shape=[None], dtype=feat.dtype)
  
  serving_input_rcvr = tf.estimator.export.ServingInputReceiver(inputs, inputs) # I am suspicious of this line.
  tf.logging.info('serving input receiver: {}'.format(serving_input_rcvr))

  # Return InputFnOps
  def serving_input_fn():
    receiver = serving_input_rcvr()
    return tf.contrib.learn.InputFnOps(
        receiver.features, None, receiver.receiver_tensors)

  return serving_input_fn
&lt;/denchmark-code&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Also, is there a particular reason you're using estimator._call_model_fn to get an EstimatorSpec rather than creating and modifying a model_fn with the model_fn_ops approaches above?
You can try something like this:
&lt;denchmark-code&gt;from tensorflow.contrib.learn.python.learn.estimators.dnn import _dnn_model_fn

KEY = 'id'

def build_estimator(config, hidden_units=None, learning_rate=None, dropout=None):
    def custom_model_fn(model_fn):
        def _model_fn(features, labels, mode, params):
            key = features.pop(KEY, None)
            model_fn_ops = model_fn(
                features=features, labels=labels, mode=mode, params=params
            )
            if key is not None:
                model_fn_ops.predictions[KEY] = key
                model_fn_ops.output_alternatives[None][1][KEY] = key
            return model_fn_ops
        return _model_fn

    custom_estimator = tf.contrib.learn.Estimator(
        model_fn=custom_model_fn(_dnn_model_fn),
        params={}, # define whatever your particular model_fn needs
        config=config
    )

    return custom_estimator
&lt;/denchmark-code&gt;

Or you can create the  as &lt;denchmark-link:https://github.com/andrewm4894&gt;@andrewm4894&lt;/denchmark-link&gt;
 does in his &lt;denchmark-link:https://gist.github.com/andrewm4894/ebd3ac3c87e2ab4af8a10740e85073bb&gt;gist&lt;/denchmark-link&gt;
 that you linked.
&lt;denchmark-link:https://github.com/elibixby&gt;@elibixby&lt;/denchmark-link&gt;
 can probably help further if neither of these approaches work.
		</comment>
		<comment id='22' author='josiahdavis' date='2018-01-04T23:29:46Z'>
		Thanks for the pointers &lt;denchmark-link:https://github.com/rasmi&gt;@rasmi&lt;/denchmark-link&gt;
.
Perhaps I will need to try an approach based on the 'contrib' APIs rather than the recent 'core' APIs, along the lines of what you shared above. I'll add this to my list of things to try.
I also posted this &lt;denchmark-link:https://gist.github.com/dawu76/eae1f9f36b77aaf16b465296877cff40&gt;gist of my model.py&lt;/denchmark-link&gt;
 (with only a subset of feature columns displayed) that illustrates more clearly the current state of my model, which is using only the 'core' APIs.
I did make the following minor &lt;denchmark-link:https://gist.github.com/dawu76/eae1f9f36b77aaf16b465296877cff40#file-model-py-L226&gt;change&lt;/denchmark-link&gt;
 to my custom , which enabled my instance key to appear among the 'serving_default' SignatureDef  of my saved model (inspired by &lt;denchmark-link:https://github.com/tensorflow/models/blob/master/official/mnist/mnist.py#L95&gt;this line&lt;/denchmark-link&gt;
 of the official TF MNIST model):
estimatorSpec.export_outputs['serving_default'] = \
  tf.estimator.export.PredictOutput(estimatorSpec.predictions)
However, I still haven't found a working approach in which both: a) my instance key appears among the saved model's 'serving_default' SignatureDef outputs, and b) I'm able to successfully call gcloud ml-engine local predict --model_dir=... --json_instances=... to yield predictions with the instance key.

I feel like there is a minor one-line change I can make to get my current TF core 1.4 / tf.estimator-based approach working, but I just haven't been able to figure it out.
I've tried using various combinations of arguments to tf.estimator.export.ServingInputReceiver in my json_serving_input_fn, but all of them result in errors of some sort.
This gist documents the various combinations of ServingInputReceiver() arguments that I (blindly) tried (e.g. where the 1st arg contains the instance key while the 2nd arg does not, and vice versa), along with their resulting saved model signatures and prediction error messages.

For additional context on my choice of certain classes over others
I just started using Tensorflow for work-related projects last month so my approach has been heavily informed by the recent API docs and Google developer blog posts listed below. As a non-S/W engineer, I wanted to familiarize myself with just one set of APIs to start with, so I decided to go with the latest ones featured in these docs/repos, which focus on usage of tf.estimator, tf.estimator.EstimatorSpec, etc. in TF core, rather than analogous classes in TF contrib.

the tf.estimator doc, which has a detailed overview of custom estimator creation for a regression problem. This doc walks through the definition of a model_fn that returns a tf.estimator.EstimatorSpec.
the Introduction to Tensorflow Datasets and Estimators Google dev blog post, which discusses the tf.estimator API and tackles a classification problem using tf.estimator.DNNClassifier.
the Creating Custom Estimators in Tensorflow blog post, which tackles a classification problem using a custom estimator based on tf.estimator.Estimator. As in the tf.estimator doc, it goes step-by-step through the definition of a model_fn passed to the estimator, and it describes how to define the tf.estimator.EstimatorSpec instance returned by the function under different modes.
the Predicting Income with the Census Income Dataset code in this 'GoogleCloudPlatform/cloudml-samples' repo: in particular, the tf.estimator-based implementation in this directory, whose 'model.py' defines a json_serving_input_fn that returns a tf.estimator.export.ServingInputReceiver(inputs, inputs).

		</comment>
		<comment id='23' author='josiahdavis' date='2018-01-08T01:52:25Z'>
		&lt;denchmark-link:https://github.com/dawu76&gt;@dawu76&lt;/denchmark-link&gt;
 You are right about the tf.estimator.export.ServingInputReceiver -- this is another one of those contrib to core migration nuances. See here for some more details, which might help clarify: &lt;denchmark-link:https://towardsdatascience.com/how-to-move-from-tf-contrib-learn-estimator-to-core-tensorflow-tf-estimator-af07b2d21f34&gt;https://towardsdatascience.com/how-to-move-from-tf-contrib-learn-estimator-to-core-tensorflow-tf-estimator-af07b2d21f34&lt;/denchmark-link&gt;

The method I am familiar with is modifying the model_fn_ops returned by a canned or custom estimator's model_fn and creating an estimator with that, which is why I recommended that above with contrib APIs. I'm sure there is an analogous way to do so in core TF 1.4 APIs. I will take a closer look at your gist...
		</comment>
		<comment id='24' author='josiahdavis' date='2018-01-08T22:24:18Z'>
		Thanks for the link &lt;denchmark-link:https://github.com/rasmi&gt;@rasmi&lt;/denchmark-link&gt;
 -- that was a very clearly written post that I'm sure will be useful to many others in the coming months as well.
		</comment>
		<comment id='25' author='josiahdavis' date='2018-01-09T00:22:19Z'>
		This issue should be closed, I think.
Here's the way to pass in keys:
&lt;denchmark-code&gt;# create estimator as usual
estimator = tf.estimator.DNNClassifier(...)
# wrap it using forward_features
estimator = tf.contrib.estimator.forward_features(estimator, 'my-key-column')
# use estimator as normal
estimator.train_and_evaluate(...)
&lt;/denchmark-code&gt;

Now, when you call estimator.predict(), the returned JSON will include 'my-key-column'.
Note from &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/forward_features&gt;https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/forward_features&lt;/denchmark-link&gt;
 that it takes a tf.estimator
Lak
		</comment>
		<comment id='26' author='josiahdavis' date='2018-01-09T02:37:14Z'>
		Oh interesting! Thanks &lt;denchmark-link:https://github.com/lakshmanok&gt;@lakshmanok&lt;/denchmark-link&gt;
 - I guess I should familiarize myself with some of the utilities in .
Crossing my fingers that this will 'just work' with . My previous hacky approach  (in this &lt;denchmark-link:https://gist.github.com/dawu76/eae1f9f36b77aaf16b465296877cff40&gt;gist&lt;/denchmark-link&gt;
), where I was directly modifying the EstimatorSpec's 'predictions' dict, was able to get my instance key to appear in the result of  but was unable to get the keys to appear in the output of .
(Tried multiple variations of args to the ServingInputReceiver(...) returned by my json_serving_input_fn but could never get the instance key to appear in both the inputs &amp; outputs of the saved model's SignatureDef.)
		</comment>
		<comment id='27' author='josiahdavis' date='2018-01-11T01:10:11Z'>
		&lt;denchmark-link:https://github.com/lakshmanok&gt;@lakshmanok&lt;/denchmark-link&gt;
 - It doesn't look like  resolves this issue. To be more specific, the issue is not just about getting the instance key to appear in the output of  -- which I was able to do by modifying the EstimatorSpec in my &lt;denchmark-link:https://gist.github.com/dawu76/eae1f9f36b77aaf16b465296877cff40&gt;gist&lt;/denchmark-link&gt;
 -- but about getting it to appear in the output of  without having to use it as one of the model's predictors.
My understanding from skimming the  &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/estimator/python/estimator/extenders.py&gt;code&lt;/denchmark-link&gt;
 is that it expects the instance key(s) to be among the list of features passed to the model.
However, I don't want to include my instance key ('user_id' of type INT64) as a predictor in the model. If I actually do pass it as a predictor (as done in my first two attempts below to get this working), the resulting model loss unsurprisingly blows up. (And scaling this extraneous 'instance key' feature and/or vastly increasing the training epochs to prevent this would be an undesirable workaround.)
Summary of various attempts to use tf.contrib.estimator.forward_features
In each attempt, I call train_and_evaluate on a version of the estimator, inspect the saved model using the saved_model_cli, and then call gcloud ml-engine local predict model_dir=... --json_instances=... to get predictions using this saved model on JSON records that include the instance key.
----- 1st attempt -----
Here I train &amp; predict with an estimator constructed via the function below, which returns tf.contrib.estimator.forward_features(estimator, ...).
Summary: my instance key ('user_id') does NOT appear in the output of gcloud ml-engine local predict ...

Inspection of the resulting SavedModel SignatureDef indicates that the 'user_id' instance key is is among the inputs but NOT the outputs.
calling gcloud ml-engine local predict returns predictions, but WITHOUT the instance key. This is consistent with the instance key's absence from the SavedModel SignatureDef output above.
the resulting model loss is very high since the instance key was included as a predictor

def build_estimator(config, embedding_size=4, hidden_units=None):

  # note: instance key is in 'feat_continuous'
  feat_continuous, feat_bucket_ind, feat_cat_ind = \
    build_model_columns(embedding_size=embedding_size)
  
  # note: instance key is in this list of features
  features_all = feat_continuous + feat_bucket_ind + feat_cat_ind

  is_key_present = INSTANCE_KEY_COLUMN in features_all
  tf.logging.info('\ndo the features contain the instance key? : {}\n'.format(is_key_present))

  # note: instance key is in the features passed to 'feature_columns'
  dnn_clf_0 = tf.estimator.DNNClassifier(
    config=config,
    feature_columns=features_all,
    hidden_units=hidden_units or [150, 75, 50, 25])

  # returns a `tf.estimator.Estimator(model_fn=new_model_fn, ...)` where
  # `new_model_fn` is a 'model_fn' that returns an EstimatorSpec whose 
  # 'predictions' dict now has the instance key as a key
  dnn_clf_1 = tf.contrib.estimator.forward_features(dnn_clf_0, INSTANCE_KEY_COLUMN)
  
  return dnn_clf_1
----- 2nd attempt -----
Here I train &amp; predict with an estimator constructed via the function below, which uses a model_fn_extra wrapper function to further modify the estimatorSpec.export_outputs dict.
Summary: my instance key ('user_id') DOES appear in the output of gcloud ml-engine local predict after further modifying the export_outputs element of the estimator's EstimatorSpec.

Inspection of the resulting SavedModel SignatureDef indicates that the 'user_id' instance key is is present among the inputs AND the outputs.
calling gcloud ml-engine local predict returns predictions WITH the instance key. This is consistent with the instance key's presence among the SavedModel SignatureDef outputs.
the resulting model loss is again very high since the instance key was included as a predictor

def model_fn_extra(estimator, instance_key='user_id'):
  '''
  A function that takes a specified estimator and returns a function
  that in turn returns an EstimatorSpec. When the estimatorSpec's 
  `export_outputs` is defined, it updates it to a PredictOutput created
  from the existing `predictions` dict.

  Intended to be passed as an arg to the `model_fn` arg in a 
  `tf.estimator.Estimator(model_fn=...)` call. 
  '''

  def _model_fn(features, labels, mode):
    estimatorSpec = estimator._call_model_fn(
      features=features, labels=labels, mode=mode, config=estimator.config)

    if estimatorSpec.export_outputs:
      # in serving mode, return the same fields as in prediction mode; this 
      # ensures that the instance key in 'predictions' will be among the 
      # outputs of the SavedModel SignatureDef.
      estimatorSpec.export_outputs['predict'] = \
        tf.estimator.export.PredictOutput(estimatorSpec.predictions)

      estimatorSpec.export_outputs['serving_default'] = \
        tf.estimator.export.PredictOutput(estimatorSpec.predictions)

    tf.logging.info('\nestimatorSpec prediction keys: {}\n' \
    .format(estimatorSpec.predictions.keys()))

    return estimatorSpec
  return _model_fn

def build_estimator(config, embedding_size=4, hidden_units=None):

  # note: instance key is in 'feat_continuous'
  feat_continuous, feat_bucket_ind, feat_cat_ind = \
    build_model_columns(embedding_size=embedding_size)
  
  # note: instance key is in this list of features
  features_all = feat_continuous + feat_bucket_ind + feat_cat_ind

  is_key_present = INSTANCE_KEY_COLUMN in features_all
  tf.logging.info('\ndo the features contain the instance key? : {}\n'.format(is_key_present))

  # note: instance key is in the features passed to 'feature_columns'
  dnn_clf_0 = tf.estimator.DNNClassifier(
    config=config,
    feature_columns=features_all,
    hidden_units=hidden_units or [150, 75, 50, 25])

  # returns a `tf.estimator.Estimator(model_fn=new_model_fn, ...)` where
  # `new_model_fn` is a 'model_fn' that returns an EstimatorSpec whose 
  # 'predictions' dict now has the instance key as a key
  dnn_clf_1 = tf.contrib.estimator.forward_features(dnn_clf_0, INSTANCE_KEY_COLUMN)

  # update the `export_outputs` value in the estimator's EstimatorSpec
  dnn_clf_2 = tf.estimator.Estimator(
    model_fn=model_fn_extra(dnn_clf_1),
    config=config)

  return dnn_clf_2
----- 3rd attempt -----
Here I attempt to train &amp; predict with an estimator constructed via the function below. The model_fn_extra_no_key wrapper function used here is just like the model_fn_extra from the 2nd attempt above, but also removes the 'user_id' key from the model.
Summary: I'm unable to train this model. The estimator.train_and_evaluate call fails with a ValueError: Feature user_id is not in features dictionary. message.
def model_fn_extra_no_key(estimator, instance_key='user_id'):
  '''
  A function that takes a specified estimator and returns a function
  that in turn returns an EstimatorSpec. 

  When the estimatorSpec's `export_outputs` is defined, it updates 
  it to a PredictOutput created from the existing `predictions` dict.

  Also removes the instance key from the model's list of features.

  Intended to be passed as an arg to the `model_fn` arg in a 
  `tf.estimator.Estimator(model_fn=...)` call. 
  '''

  def _model_fn(features, labels, mode):
    # update the EstimatorSpec by removing the instance key from the
    # features and updating the 'export_outputs'
    instance_key_feature = features.pop(instance_key, None)

    estimatorSpec = estimator._call_model_fn(
      features=features, labels=labels, mode=mode, config=estimator.config)

    if estimatorSpec.export_outputs:
      # in serving mode, return the same fields as in prediction mode; this 
      # ensures that the instance key in 'predictions' will be among the 
      # outputs of the SavedModel SignatureDef.
      estimatorSpec.export_outputs['predict'] = \
        tf.estimator.export.PredictOutput(estimatorSpec.predictions)

      estimatorSpec.export_outputs['serving_default'] = \
        tf.estimator.export.PredictOutput(estimatorSpec.predictions)

    tf.logging.info('\nestimatorSpec prediction keys: {}\n'.format(estimatorSpec.predictions.keys()))

    return estimatorSpec
  return _model_fn

def build_estimator(config, embedding_size=4, hidden_units=None):

  # note: instance key is in 'feat_continuous'
  feat_continuous, feat_bucket_ind, feat_cat_ind = \
    build_model_columns(embedding_size=embedding_size)
  
  # note: instance key is in this list of features
  features_all = feat_continuous + feat_bucket_ind + feat_cat_ind

  is_key_present = INSTANCE_KEY_COLUMN in features_all
  tf.logging.info('\ndo the features contain the instance key? : {}\n'.format(is_key_present))

  # note: instance key is in the features passed to 'feature_columns'
  dnn_clf_0 = tf.estimator.DNNClassifier(
    config=config,
    feature_columns=features_all,
    hidden_units=hidden_units or [150, 75, 50, 25])

  # returns a `tf.estimator.Estimator(model_fn=new_model_fn, ...)` where
  # `new_model_fn` is a 'model_fn' that returns an EstimatorSpec whose 
  # 'predictions' dict now has the instance key as a key
  dnn_clf_1 = tf.contrib.estimator.forward_features(dnn_clf_0, INSTANCE_KEY_COLUMN)

  # update model's EstimatorSpec by modifying its `export_outputs` dict
  # and removing the instance key from the model's features
  dnn_clf_2 = tf.estimator.Estimator(
    model_fn=model_fn_extra_no_key(dnn_clf_1),
    config=config)

  return dnn_clf_2  
		</comment>
		<comment id='28' author='josiahdavis' date='2018-01-11T01:58:13Z'>
		The key should be part of the features, labels returned from the train/eval
input function. But should not be used to create a feature column to pass
onto the Estimator

Lak
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Wed, Jan 10, 2018, 5:10 PM hwu ***@***.***&gt; wrote:
 @lakshmanok &lt;https://github.com/lakshmanok&gt; - It doesn't look like
 tf.contrib.estimator.forward_features resolves this issue. To be more
 specific, the issue is not just about getting the instance key to appear in
 the output of estimator.predict(...) -- which I was able to do by
 modifying the EstimatorSpec in my gist
 &lt;https://gist.github.com/dawu76/eae1f9f36b77aaf16b465296877cff40&gt; -- but
 about getting it to appear in the output of gcloud ml-engine predict
 without having to use it as one of the model's predictors.

 My understanding from skimming the forward_features code
 &lt;https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/estimator/python/estimator/extenders.py&gt;
 is that it expects the instance key(s) to be among the list of features
 passed to the model.

 However, I don't want to include my instance key ('user_id' of type INT64)
 as a predictor in the model. If I actually do pass it as a predictor (as
 done in my first two attempts below to get this working), the resulting
 model loss unsurprisingly blows up. (And scaling this extraneous 'instance
 key' feature and/or vastly increasing the training epochs to prevent this
 would be an undesirable workaround.)

 *Summary of various attempts to use tf.contrib.estimator.forward_features*

 In each attempt, I call train_and_evaluate on a version of the estimator,
 inspect the saved model using the saved_model_cli, and then call gcloud
 ml-engine local predict model_dir=... --json_instances=... to get
 predictions using this saved model on JSON records that include the
 instance key.

 *----- 1st attempt -----*

 Here I train &amp; predict with an estimator constructed via the function
 below, which returns tf.contrib.estimator.forward_features(estimator, ...)
 .

 *Summary*: my instance key ('user_id') does NOT appear in the output of gcloud
 ml-engine local predict ...

    - Inspection of the resulting SavedModel SignatureDef indicates that
    the 'user_id' instance key is is among the inputs but NOT the outputs.
    - calling gcloud ml-engine local predict returns predictions, but
    WITHOUT the instance key. This is consistent with the instance key's
    absence from the SavedModel SignatureDef output above.
    - the resulting model loss is very high since the instance key was
    included as a predictor

 def build_estimator(config, embedding_size=4, hidden_units=None):

   # note: instance key is in 'feat_continuous'
   feat_continuous, feat_bucket_ind, feat_cat_ind = \
     build_model_columns(embedding_size=embedding_size)

   # note: instance key is in this list of features
   features_all = feat_continuous + feat_bucket_ind + feat_cat_ind

   is_key_present = INSTANCE_KEY_COLUMN in features_all
   tf.logging.info('\ndo the features contain the instance key? : {}\n'.format(is_key_present))

   # note: instance key is in the features passed to 'feature_columns'
   dnn_clf_0 = tf.estimator.DNNClassifier(
     config=config,
     feature_columns=features_all,
     hidden_units=hidden_units or [150, 75, 50, 25])

   # returns a `tf.estimator.Estimator(model_fn=new_model_fn, ...)` where
   # `new_model_fn` is a 'model_fn' that returns an EstimatorSpec whose
   # 'predictions' dict now has the instance key as a key
   dnn_clf_1 = tf.contrib.estimator.forward_features(dnn_clf_0, INSTANCE_KEY_COLUMN)

   return dnn_clf_1

 *----- 2nd attempt -----*

 Here I train &amp; predict with an estimator constructed via the function
 below, which uses a model_fn_extra wrapper function to further modify the
 estimatorSpec.export_outputs dict.

 *Summary*: my instance key ('user_id') *DOES* appear in the output of gcloud
 ml-engine local predict after further modifying the export_outputs
 element of the estimator's EstimatorSpec.

    - Inspection of the resulting SavedModel SignatureDef indicates that
    the 'user_id' instance key is is present among the inputs *AND* the
    outputs.
    - calling gcloud ml-engine local predict returns predictions WITH the
    instance key. This is consistent with the instance key's presence among the
    SavedModel SignatureDef outputs.
    - the resulting model loss is again very high since the instance key
    was included as a predictor

 def model_fn_extra(estimator, instance_key='user_id'):
   '''  A function that takes a specified estimator and returns a function  that in turn returns an EstimatorSpec. When the estimatorSpec's   `export_outputs` is defined, it updates it to a PredictOutput created  from the existing `predictions` dict.  Intended to be passed as an arg to the `model_fn` arg in a   `tf.estimator.Estimator(model_fn=...)` call.   '''

   def _model_fn(features, labels, mode):
     estimatorSpec = estimator._call_model_fn(
       features=features, labels=labels, mode=mode, config=estimator.config)

     if estimatorSpec.export_outputs:
       # in serving mode, return the same fields as in prediction mode; this
       # ensures that the instance key in 'predictions' will be among the
       # outputs of the SavedModel SignatureDef.
       estimatorSpec.export_outputs['predict'] = \
         tf.estimator.export.PredictOutput(estimatorSpec.predictions)

       estimatorSpec.export_outputs['serving_default'] = \
         tf.estimator.export.PredictOutput(estimatorSpec.predictions)

     tf.logging.info('\nestimatorSpec prediction keys: {}\n' \
     .format(estimatorSpec.predictions.keys()))

     return estimatorSpec
   return _model_fn
 def build_estimator(config, embedding_size=4, hidden_units=None):

   # note: instance key is in 'feat_continuous'
   feat_continuous, feat_bucket_ind, feat_cat_ind = \
     build_model_columns(embedding_size=embedding_size)

   # note: instance key is in this list of features
   features_all = feat_continuous + feat_bucket_ind + feat_cat_ind

   is_key_present = INSTANCE_KEY_COLUMN in features_all
   tf.logging.info('\ndo the features contain the instance key? : {}\n'.format(is_key_present))

   # note: instance key is in the features passed to 'feature_columns'
   dnn_clf_0 = tf.estimator.DNNClassifier(
     config=config,
     feature_columns=features_all,
     hidden_units=hidden_units or [150, 75, 50, 25])

   # returns a `tf.estimator.Estimator(model_fn=new_model_fn, ...)` where
   # `new_model_fn` is a 'model_fn' that returns an EstimatorSpec whose
   # 'predictions' dict now has the instance key as a key
   dnn_clf_1 = tf.contrib.estimator.forward_features(dnn_clf_0, INSTANCE_KEY_COLUMN)

   # update the `export_outputs` value in the estimator's EstimatorSpec
   dnn_clf_2 = tf.estimator.Estimator(
     model_fn=model_fn_extra(dnn_clf_1),
     config=config)

   return dnn_clf_2

 *----- 3rd attempt -----*

 Here I attempt to train &amp; predict with an estimator constructed via the
 function below. The model_fn_extra_no_key wrapper function used here is
 just like the model_fn_extra from the 2nd attempt above, but also removes
 the 'user_id' key from the model.

 *Summary*: I'm unable to train this model. The
 estimator.train_and_evaluate call fails with a ValueError: Feature
 user_id is not in features dictionary. message.

 def model_fn_extra_no_key(estimator, instance_key='user_id'):
   '''  A function that takes a specified estimator and returns a function  that in turn returns an EstimatorSpec.   When the estimatorSpec's `export_outputs` is defined, it updates   it to a PredictOutput created from the existing `predictions` dict.  Also removes the instance key from the model's list of features.  Intended to be passed as an arg to the `model_fn` arg in a   `tf.estimator.Estimator(model_fn=...)` call.   '''

   def _model_fn(features, labels, mode):
     # update the EstimatorSpec by removing the instance key from the
     # features and updating the 'export_outputs'
     instance_key_feature = features.pop(instance_key, None)

     estimatorSpec = estimator._call_model_fn(
       features=features, labels=labels, mode=mode, config=estimator.config)

     if estimatorSpec.export_outputs:
       # in serving mode, return the same fields as in prediction mode; this
       # ensures that the instance key in 'predictions' will be among the
       # outputs of the SavedModel SignatureDef.
       estimatorSpec.export_outputs['predict'] = \
         tf.estimator.export.PredictOutput(estimatorSpec.predictions)

       estimatorSpec.export_outputs['serving_default'] = \
         tf.estimator.export.PredictOutput(estimatorSpec.predictions)

     tf.logging.info('\nestimatorSpec prediction keys: {}\n'.format(estimatorSpec.predictions.keys()))

     return estimatorSpec
   return _model_fn
 def build_estimator(config, embedding_size=4, hidden_units=None):

   # note: instance key is in 'feat_continuous'
   feat_continuous, feat_bucket_ind, feat_cat_ind = \
     build_model_columns(embedding_size=embedding_size)

   # note: instance key is in this list of features
   features_all = feat_continuous + feat_bucket_ind + feat_cat_ind

   is_key_present = INSTANCE_KEY_COLUMN in features_all
   tf.logging.info('\ndo the features contain the instance key? : {}\n'.format(is_key_present))

   # note: instance key is in the features passed to 'feature_columns'
   dnn_clf_0 = tf.estimator.DNNClassifier(
     config=config,
     feature_columns=features_all,
     hidden_units=hidden_units or [150, 75, 50, 25])

   # returns a `tf.estimator.Estimator(model_fn=new_model_fn, ...)` where
   # `new_model_fn` is a 'model_fn' that returns an EstimatorSpec whose
   # 'predictions' dict now has the instance key as a key
   dnn_clf_1 = tf.contrib.estimator.forward_features(dnn_clf_0, INSTANCE_KEY_COLUMN)

   # update model's EstimatorSpec by modifying its `export_outputs` dict
   # and removing the instance key from the model's features
   dnn_clf_2 = tf.estimator.Estimator(
     model_fn=model_fn_extra_no_key(dnn_clf_1),
     config=config)

   return dnn_clf_2

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#67 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABqX548X85icioDynaP2rDKvS6prDTsBks5tJV93gaJpZM4O7y1T&gt;
 .



		</comment>
		<comment id='29' author='josiahdavis' date='2018-01-11T04:22:02Z'>
		Hmmm, really trying to understand how this works.
When I look at the  instance returned by the call to  in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/estimator/python/estimator/extenders.py#L238&gt;this line&lt;/denchmark-link&gt;
, I see that the updated  passed to this  calls :
def new_model_fn(features, labels, mode, config):  # pylint: disable=missing-docstring
    spec = estimator.model_fn(features, labels, mode, config)
    predictions = spec.predictions
    if predictions is None:
      return spec
    verify_keys_and_predictions(features, predictions)
   ...
This  &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/estimator/python/estimator/extenders.py#L202&gt;function&lt;/denchmark-link&gt;
 iterates over the keys passed into the  arg and raises a  if any key is not among the estimator's 'features':
  def verify_keys_and_predictions(features, predictions):
    if not isinstance(predictions, dict):
      raise ValueError(
          'Predictions should be a dict to be able to forward features. '
          'Given: {}'.format(type(predictions)))
    for key in get_keys(features):
      if key not in features:
        raise ValueError(
            'keys should be exist in features. Key "{}" is not in features '
            'dict. features dict has following keys: {}. Please check '
            'arguments of forward_features.'.format(key, features.keys()))
      if key in predictions:
        raise ValueError(
            'Cannot forward feature key ({}). Since it does exist in '
            'predictions. Existing prediction keys: {}. Please check arguments '
            'of forward_features.'.format(key, predictions.keys()))
It's not clear how I can get the verify_keys_and_predictions checks to pass without including the keys among the features passed to the model.
		</comment>
		<comment id='30' author='josiahdavis' date='2018-02-01T00:30:31Z'>
		For future reference, below is a code snippet where I applied &lt;denchmark-link:https://github.com/lakshmanok&gt;@lakshmanok&lt;/denchmark-link&gt;
's  suggestion for passing through instance keys, in case it's helpful for anyone trying to do this with a &lt;denchmark-link:https://www.tensorflow.org/versions/r1.4/api_docs/python/tf/estimator/Estimator&gt;tf.estimator.Estimator&lt;/denchmark-link&gt;
 in TF 1.4.
(Prior solutions for passing through instance keys, as found above and on StackOverflow, used TF 1.3 or before and tf.contrib.learn.Estimator.)
With this approach, I was able to get the instance key to appear in the prediction output of a gcloud ml-engine local predict or gcloud ml-engine jobs submit prediction job.
Incidentally, I also used another function defined in the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/estimator/python/estimator/extenders.py&gt;same file&lt;/denchmark-link&gt;
 -  - to add more metrics to the EstimatorSpec's 'eval_metric_ops' dict.
def model_fn_extra(estimator):
  '''
  A function that takes a specified estimator and returns a function
  that in turn returns an EstimatorSpec. When the estimatorSpec's 
  `export_outputs` is defined, it updates it to a PredictOutput created
  from the existing `predictions` dict.

  Intended to be passed as an arg to the `model_fn` arg in a 
  `tf.estimator.Estimator(model_fn=...)` call. 
  '''
  def _model_fn(features, labels, mode):
    estimatorSpec = estimator._call_model_fn(
      features=features, labels=labels, mode=mode, config=estimator.config)

    if estimatorSpec.export_outputs:
      # in serving mode, return the same fields as in prediction mode; this 
      # ensures that the instance key in 'predictions' will be among the 
      # outputs of the SavedModel SignatureDef.
      estimatorSpec.export_outputs['predict'] = \
        tf.estimator.export.PredictOutput(estimatorSpec.predictions)

      estimatorSpec.export_outputs['serving_default'] = \
        tf.estimator.export.PredictOutput(estimatorSpec.predictions)

    tf.logging.info('\nestimatorSpec prediction keys: {}\n' \
    .format(estimatorSpec.predictions.keys()))

    return estimatorSpec
  return _model_fn

def build_estimator(
  config, 
  embedding_size=4, 
  hidden_units=None, 
  dropout=None):
  ...

  # note: instance key is _NOT_ in the features passed to 'feature_columns'
  dnn_clf_0 = tf.estimator.DNNClassifier(
    config=config,
    feature_columns=features_all,
    hidden_units=hidden_units or [150, 75, 50, 25],
    dropout=dropout)

  dnn_clf_1 = tf.contrib.estimator.add_metrics(dnn_clf_0, get_prec_rec_at_thresholds)

  # returns a `tf.estimator.Estimator(model_fn=new_model_fn, ...)` where
  # `new_model_fn` is a 'model_fn' that returns an EstimatorSpec whose 
  # 'predictions' dict now has the instance key as a key
  dnn_clf_2 = tf.contrib.estimator.forward_features(dnn_clf_1, INSTANCE_KEY_COLUMN)

  # update the `export_outputs` value in the estimator's EstimatorSpec
  dnn_clf_3 = tf.estimator.Estimator(
    model_fn=model_fn_extra(dnn_clf_2),
    config=config)

  return dnn_clf_3
		</comment>
		<comment id='31' author='josiahdavis' date='2018-02-01T06:26:38Z'>
		Thanks for sharing, I'm actually trying to figure the same out now myself, based on adapting the Reddit example.
&lt;denchmark-link:https://stackoverflow.com/q/48543299/1919374&gt;https://stackoverflow.com/q/48543299/1919374&lt;/denchmark-link&gt;

		</comment>
		<comment id='32' author='josiahdavis' date='2018-02-01T12:03:45Z'>
		I tried adding forward_features approach &lt;denchmark-link:https://github.com/andrewm4894/my-google-cloudml-tensorflow-examples/blob/5f1727c40f2995e008da272fc98fe068130a2d04/reddit_classifier/trainer/task.py#L233&gt;here&lt;/denchmark-link&gt;

However i get AttributeError: 'DNNClassifier' object has no attribute 'model_fn'
		</comment>
		<comment id='33' author='josiahdavis' date='2018-02-01T13:08:15Z'>
		&lt;denchmark-link:https://github.com/dawu76&gt;@dawu76&lt;/denchmark-link&gt;
 thanks for sharing you approach - i tried to add it into my adapted reddit example &lt;denchmark-link:https://github.com/andrewm4894/my-google-cloudml-tensorflow-examples/blob/master/reddit_classifier/trainer/task.py&gt;here&lt;/denchmark-link&gt;
 but i'm getting this error when trying to train now.
&lt;denchmark-link:https://github.com/puneith&gt;@puneith&lt;/denchmark-link&gt;
 thanks for commenting on my SO post - any ideas here? I really am starting off with TF so adapting examples is as much as i can do but i think to solve &lt;denchmark-link:https://stackoverflow.com/questions/48543299/converting-google-cloud-ml-github-reddit-example-from-regression-to-classificati&gt;my SO question&lt;/denchmark-link&gt;
 requires much better understanding of the fundamentals as seems like they both (census model.py vs reddit task.py) use slightly different functions in setting it all up and then whgen you add moving to different versions of TF i'm kinda lost a bit.
No idea what it means - i found some reference to it &lt;denchmark-link:https://github.com/tensorflow/serving/issues/310&gt;here&lt;/denchmark-link&gt;
 but not sure how relevant it is.
From watching the logs (see image below) it looks like it trains a little and then fails so feels like i am making some progress here but something a bit different in the approach of the two examples (reddit vs census) and i just don't know enough to spot it.
&lt;denchmark-code&gt;message:  "Traceback (most recent call last):
  File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
    "__main__", fname, loader, pkg_name)
  File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
    exec code in run_globals
  File "/root/.local/lib/python2.7/site-packages/trainer/task.py", line 335, in &lt;module&gt;
    main()
  File "/root/.local/lib/python2.7/site-packages/trainer/task.py", line 330, in main
    output_dir=output_dir
  File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py", line 218, in run
    return _execute_schedule(experiment, schedule)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py", line 46, in _execute_schedule
    return task()
  File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py", line 625, in train_and_evaluate
    self.train(delay_secs=0)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py", line 367, in train
    hooks=self._train_monitors + extra_hooks)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py", line 807, in _call_train
    hooks=hooks)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py", line 302, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py", line 783, in _train_model
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py", line 521, in run
    run_metadata=run_metadata)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py", line 892, in run
    run_metadata=run_metadata)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py", line 967, in run
    raise six.reraise(*original_exc_info)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py", line 952, in run
    return self._sess.run(*args, **kwargs)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py", line 1032, in run
    run_metadata=run_metadata))
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py", line 452, in after_run
    self._save(run_context.session, global_step)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py", line 468, in _save
    self._get_saver().save(session, self._save_path, global_step=step)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py", line 1594, in save
    raise exc
NotFoundError: /tmp/tmp2jllvb/model.ckpt-1_temp_9530d2c5823d4462be53fa5415e429fd; No such file or directory
	 [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device="/job:ps/replica:0/task:0/device:CPU:0"](save/ShardedFilename, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, dnn/hiddenlayer_0/kernel/part_2/read, dnn/dnn/hiddenlayer_0/kernel/part_2/Adagrad/read, dnn/hiddenlayer_1/kernel/part_2/read, dnn/dnn/hiddenlayer_1/kernel/part_2/Adagrad/read, dnn/input_from_feature_columns/input_layer/subreddit_id_embedding/weights/part_0/read, dnn/dnn/input_from_feature_columns/input_layer/subreddit_id_embedding/weights/part_0/Adagrad/read, dnn/logits/bias/part_0/read, dnn/dnn/logits/bias/part_0/Adagrad/read, global_step)]]

Caused by op u'save/SaveV2', defined at:
  File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
    "__main__", fname, loader, pkg_name)
  File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
    exec code in run_globals
  File "/root/.local/lib/python2.7/site-packages/trainer/task.py", line 335, in &lt;module&gt;
    main()
  File "/root/.local/lib/python2.7/site-packages/trainer/task.py", line 330, in main
    output_dir=output_dir
  File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py", line 218, in run
    return _execute_schedule(experiment, schedule)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py", line 46, in _execute_schedule
    return task()
  File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py", line 625, in train_and_evaluate
    self.train(delay_secs=0)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py", line 367, in train
    hooks=self._train_monitors + extra_hooks)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py", line 807, in _call_train
    hooks=hooks)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py", line 302, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py", line 780, in _train_model
    log_step_count_steps=self._config.log_step_count_steps) as mon_sess:
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py", line 368, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py", line 673, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py", line 493, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py", line 851, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py", line 856, in _create_session
    return self._sess_creator.create_session()
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py", line 554, in create_session
    self.tf_sess = self._session_creator.create_session()
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py", line 419, in create_session
    self._scaffold.finalize()
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py", line 212, in finalize
    self._saver.build()
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py", line 1227, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py", line 1263, in _build
    build_save=build_save, build_restore=build_restore)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py", line 742, in _build_internal
    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py", line 381, in _AddShardedSaveOps
    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py", line 355, in _AddShardedSaveOpsForV2
    sharded_saves.append(self._AddSaveOps(sharded_filename, saveables))
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py", line 296, in _AddSaveOps
    save = self.save_op(filename_tensor, saveables)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py", line 239, in save_op
    tensors)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py", line 1163, in save_v2
    shape_and_slices=shape_and_slices, tensors=tensors, name=name)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 2956, in create_op
    op_def=op_def)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): /tmp/tmp2jllvb/model.ckpt-1_temp_9530d2c5823d4462be53fa5415e429fd; No such file or directory
	 [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device="/job:ps/replica:0/task:0/device:CPU:0"](save/ShardedFilename, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, dnn/hiddenlayer_0/kernel/part_2/read, dnn/dnn/hiddenlayer_0/kernel/part_2/Adagrad/read, dnn/hiddenlayer_1/kernel/part_2/read, dnn/dnn/hiddenlayer_1/kernel/part_2/Adagrad/read, dnn/input_from_feature_columns/input_layer/subreddit_id_embedding/weights/part_0/read, dnn/dnn/input_from_feature_columns/input_layer/subreddit_id_embedding/weights/part_0/Adagrad/read, dnn/logits/bias/part_0/read, dnn/dnn/logits/bias/part_0/Adagrad/read, global_step)]]

&lt;/denchmark-code&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/2178292/35679797-3eee7116-0750-11e8-8ba9-d07562bdeeea.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='34' author='josiahdavis' date='2018-02-05T06:08:07Z'>
		&lt;denchmark-link:https://github.com/andrewm4894&gt;@andrewm4894&lt;/denchmark-link&gt;
 Haven't gotten to it yet but in the mean time see PR &lt;denchmark-link:https://github.com/GoogleCloudPlatform/cloudml-samples/pull/158&gt;#158&lt;/denchmark-link&gt;
 for Custom estimator. You can always wrap the canned estimator into custom and achieve the desired outcome.
		</comment>
		<comment id='35' author='josiahdavis' date='2018-02-06T17:30:25Z'>
		&lt;denchmark-link:https://github.com/dawu76&gt;@dawu76&lt;/denchmark-link&gt;
 In your comments for the function  you say:

A function that takes a specified estimator and returns a function
that in turn returns an EstimatorSpec. When the estimatorSpec's
export_outputs is defined, it updates it to a PredictOutput created
from the existing predictions dict.
Intended to be passed as an arg to the model_fn arg in a
tf.estimator.Estimator(model_fn=...) call.

Does this mean i need to somewhere do something to define export_outputs for the estimatorSpec on the initial estimator being passed in?
Just still getting the below error and wondering if maybe setting up export_outputs is something i need to do upstream or not.
&lt;denchmark-code&gt;textPayload:  "The replica master 0 exited with a non-zero status of 1. Termination reason: Error. 
Traceback (most recent call last):
  [...]
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py", line 355, in _AddShardedSaveOpsForV2
    sharded_saves.append(self._AddSaveOps(sharded_filename, saveables))
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py", line 296, in _AddSaveOps
    save = self.save_op(filename_tensor, saveables)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py", line 239, in save_op
    tensors)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py", line 1163, in save_v2
    shape_and_slices=shape_and_slices, tensors=tensors, name=name)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 2956, in create_op
    op_def=op_def)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): /tmp/tmpChpK8o/model.ckpt-54_temp_adfbf622fe6242e98d9c10a73c1610be; No such file or directory
	 [[Node: save/SaveV2_3 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device="/job:ps/replica:0/task:3/device:CPU:0"](save/ShardedFilename_3, save/SaveV2_3/tensor_names, save/SaveV2_3/shape_and_slices, dnn/hiddenlayer_0/kernel/part_1/read, dnn/dnn/hiddenlayer_0/kernel/part_1/Adagrad/read, dnn/hiddenlayer_1/kernel/part_1/read, dnn/dnn/hiddenlayer_1/kernel/part_1/Adagrad/read, dnn/input_from_feature_columns/input_layer/comment_parent_body_bow_embedding/weights/part_0/read, dnn/dnn/input_from_feature_columns/input_layer/comment_parent_body_bow_embedding/weights/part_0/Adagrad/read, dnn/logits/kernel/part_0/read, dnn/dnn/logits/kernel/part_0/Adagrad/read)]]


To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=642488228368&amp;resource=ml_job%2Fjob_id%2Fredditcomments_20180206_172022&amp;advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%22redditcomments_20180206_172022%22"  
 timestamp:  "2018-02-06T17:27:39.034387226Z"  
&lt;/denchmark-code&gt;

		</comment>
		<comment id='36' author='josiahdavis' date='2018-02-11T07:24:01Z'>
		&lt;denchmark-link:https://github.com/andrewm4894&gt;@andrewm4894&lt;/denchmark-link&gt;
 - regarding whether you need to 'do something to define export_outputs for the estimatorSpec on the initial estimator being passed in': I don't think so -- I looked at my code again, and  was the only place where I had to reference .
Beyond that, I'm afraid I can't offer any concrete suggestions for your issue - just getting my plain-vanilla DNNClassifier model, heavily adapted from the Census example, to run with a few basic tweaks (e.g., the addition of instance keys and the use of a CSV rather than JSON serving function) took many hours of trial-and-error.
Out of curiousity, I googled the strings 'NotFoundError, _AddShardedSaveOpsForV2, _AddSaveOps' and found a few existing issues - e.g. &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/11549&gt;issue 11549&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/issues/246&gt;issue 246&lt;/denchmark-link&gt;
 - where users reported error messages very similar to yours, but didn't see a conclusive fix reported in them.
(For my own education, I'd love to see this Reddit example ported to TF 1.4, as mentioned in &lt;denchmark-link:https://github.com/GoogleCloudPlatform/cloudml-samples/issues/159&gt;#159&lt;/denchmark-link&gt;
, and I wouldn't be surprised if that made it easier to modify it to pass through instance keys.)
		</comment>
		<comment id='37' author='josiahdavis' date='2018-02-12T21:44:18Z'>
		&lt;denchmark-link:https://github.com/dawu76&gt;@dawu76&lt;/denchmark-link&gt;
 First of all, you are my hero!  I had the same issue and am able to train and save model based on your solution.
However, I had some problems when performing online prediction:
&lt;denchmark-code&gt;INSTANCE_KEY_COLUMN = 'instance_key'
INPUT_COLUMNS = [
	tf.feature_column.numeric_column(INSTANCE_KEY_COLUMN),
       ....]
(key_col, ....) = INPUT_COLUMNS

def model_fn_extra(estimator):
	def _model_fn(features, labels, mode):
		estimatorSpec = estimator._call_model_fn(
		  features=features, labels=labels, mode=mode, config=estimator.config)

		if estimatorSpec.export_outputs:
			estimatorSpec.export_outputs['predict'] = \
			tf.estimator.export.PredictOutput(estimatorSpec.predictions)

			estimatorSpec.export_outputs['serving_default'] = \
			tf.estimator.export.PredictOutput(estimatorSpec.predictions)

		return estimatorSpec
	return _model_fn


def build_estimator(
	config, 
	embedding_size=8, 
	hidden_units=None):

	dnn_clf_0 = tf.estimator.DNNClassifier(
		config=config,
		feature_columns=deep_columns,
		hidden_units=hidden_units or [150, 75, 50, 25],
		n_classes=2,
		optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01)
		)

	dnn_clf_2 = tf.contrib.estimator.forward_features(dnn_clf_0, INSTANCE_KEY_COLUMN)

	dnn_clf_3 = tf.estimator.Estimator(
		model_fn=model_fn_extra(dnn_clf_2),
		config=config)

	return dnn_clf_3


def json_serving_input_fn():
	inputs = {}
	for feat in INPUT_COLUMNS:
		inputs[feat.name] = tf.placeholder(shape=[None], dtype=feat.dtype)

	return tf.estimator.export.ServingInputReceiver(inputs, inputs)
&lt;/denchmark-code&gt;

My input test.json is like

{"instance_key":100, "feature1": "a" ....}

Messages from online prediction:

{
"error": "Prediction failed: Error during model execution: AbortionError(code=StatusCode.INVALID_ARGUMENT, details="Placeholder:0 is both fed and fetched.")"
}

Anything looks wrong by applying your code? Thanks!
		</comment>
		<comment id='38' author='josiahdavis' date='2018-02-12T22:07:11Z'>
		Found some threads that suggest maybe wrap it in tf.identity()

&lt;denchmark-link:https://stackoverflow.com/q/37076208/1919374&gt;https://stackoverflow.com/q/37076208/1919374&lt;/denchmark-link&gt;

&lt;denchmark-link:https://stackoverflow.com/q/39307108/1919374&gt;https://stackoverflow.com/q/39307108/1919374&lt;/denchmark-link&gt;


&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, 12 Feb 2018, 21:44 dwujellyfish, ***@***.***&gt; wrote:
 @dawu76 &lt;https://github.com/dawu76&gt; First of all, you are my hero! I had
 the same issue and am able to train and save model based on your solution.
 However, I had some problems when performing online prediction:
 INSTANCE_KEY_COLUMN = 'instance_key' INPUT_COLUMNS = [
 tf.feature_column.numeric_column(INSTANCE_KEY_COLUMN), ....] (key_col,
 ....) = INPUT_COLUMNS
 Your code is here:
 `def model_fn_extra(estimator):


 def _model_fn(features, labels, mode):
 estimatorSpec = estimator._call_model_fn(
 features=features, labels=labels, mode=mode, config=estimator.config)

 	if estimatorSpec.export_outputs:
 		# in serving mode, return the same fields as in prediction mode; this
 		# ensures that the instance key in 'predictions' will be among the
 		# outputs of the SavedModel SignatureDef.
 		estimatorSpec.export_outputs['predict'] = \
 		tf.estimator.export.PredictOutput(estimatorSpec.predictions)

 		estimatorSpec.export_outputs['serving_default'] = \
 		tf.estimator.export.PredictOutput(estimatorSpec.predictions)

 	tf.logging.info('\nestimatorSpec prediction keys: {}\n' \
 	.format(estimatorSpec.predictions.keys()))

 	return estimatorSpec
 return _model_fn

 def build_estimator(
 config,
 embedding_size=8,
 hidden_units=None):

 # note: instance key is _NOT_ in the features passed to 'feature_columns'
 dnn_clf_0 = tf.estimator.DNNClassifier(
 	config=config,
 	feature_columns=deep_columns,
 	hidden_units=hidden_units or [150, 75, 50, 25],
 	n_classes=2,
 	optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01)
 	)

 # dnn_clf_1 = tf.contrib.estimator.add_metrics(dnn_clf_0, get_prec_rec_at_thresholds)

 # returns a `tf.estimator.Estimator(model_fn=new_model_fn, ...)` where
 # `new_model_fn` is a 'model_fn' that returns an EstimatorSpec whose
 # 'predictions' dict now has the instance key as a key
 dnn_clf_2 = tf.contrib.estimator.forward_features(dnn_clf_0, INSTANCE_KEY_COLUMN)

 # update the `export_outputs` value in the estimator's EstimatorSpec
 dnn_clf_3 = tf.estimator.Estimator(
 	model_fn=model_fn_extra(dnn_clf_2),
 	config=config)

 return dnn_clf_3

 same json_serving_input_fn:


 def json_serving_input_fn():
 """Build the serving inputs."""
 inputs = {}
 for feat in INPUT_COLUMNS:
 inputs[feat.name] = tf.placeholder(shape=[None], dtype=feat.dtype)

 return tf.estimator.export.ServingInputReceiver(inputs, inputs)

 SERVING_FUNCTIONS = {
 'JSON': json_serving_input_fn,
 'EXAMPLE': example_serving_input_fn,
 'CSV': csv_serving_input_fn
 }
 My input test.json is like{"instance_key":100, "feature1": "a" ....}`

 Messages from online prediction:

 {
 "error": "Prediction failed: Error during model execution:
 AbortionError(code=StatusCode.INVALID_ARGUMENT, details="Placeholder:0 is
 both fed and fetched.")"
 }

 Anything looks wrong by applying your code? Thanks!

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#67 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ACE89KpPbA0v7IpWQ_nt3Kn5vfDuqi-Bks5tULC1gaJpZM4O7y1T&gt;
 .



		</comment>
		<comment id='39' author='josiahdavis' date='2018-02-13T14:25:49Z'>
		&lt;denchmark-link:https://github.com/andrewm4894&gt;@andrewm4894&lt;/denchmark-link&gt;
 I have now wrapped the placeholder within the identity:
&lt;denchmark-code&gt;def json_serving_input_fn():
	"""Build the serving inputs."""
	inputs = {}

	feat = INPUT_COLUMNS[0]
	inputs[feat.name] = tf.identity(tf.placeholder(shape=[None], dtype=feat.dtype))

	for feat in INPUT_COLUMNS[1:]:
		inputs[feat.name] = tf.placeholder(shape=[None], dtype=feat.dtype)

	return tf.estimator.export.ServingInputReceiver(inputs, inputs)
&lt;/denchmark-code&gt;

but I am getting this error message:

{
"error": "Prediction failed: Error during model execution: AbortionError(code=StatusCode.INVALID_ARGUMENT, details="Identity:0 is both fed and fetched.")"
}

		</comment>
		<comment id='40' author='josiahdavis' date='2018-02-13T14:43:35Z'>
		&lt;denchmark-link:https://github.com/dwujellyfish&gt;@dwujellyfish&lt;/denchmark-link&gt;
 looks like that's not it so...
is your instance key in deep_columns by any chance?
		</comment>
		<comment id='41' author='josiahdavis' date='2018-02-13T14:52:12Z'>
		&lt;denchmark-link:https://github.com/andrewm4894&gt;@andrewm4894&lt;/denchmark-link&gt;
 it is not in the deep column
		</comment>
		<comment id='42' author='josiahdavis' date='2018-02-15T04:09:56Z'>
		&lt;denchmark-link:https://github.com/dwujellyfish&gt;@dwujellyfish&lt;/denchmark-link&gt;
 - I ran into the same 'identity ... is both fed and fetched' issue in my  (but not my , since the CSV input records consist of just a single string Tensor field).
This is what my json_serving_input_fn looks like -- I had to pass separate 'inputs' and 'features' dicts to the tf.estimator.export.ServingInputReceiver call, with the tf.identity applied to the instance key in the 'features' dict, not the 'inputs' dict.
&lt;denchmark-code&gt;def json_serving_input_fn():
  inputs = {}
  features = {}

  for feat in INPUT_COLUMNS:
    inputs[feat.name] = tf.placeholder(shape=[None], dtype=feat.dtype)
    if feat.name == INSTANCE_KEY_COLUMN:
      features[feat.name] = tf.identity(inputs[feat.name])
    else:
      features[feat.name] = inputs[feat.name]

  # inputs: 'user_id': &lt;tf.Tensor 'Placeholder:0' shape=(?,) dtype=int64&gt;}
  # features: 'user_id': &lt;tf.Tensor 'Identity:0' shape=(?,) dtype=int64&gt;}

  serving_input_rcvr = tf.estimator.export.ServingInputReceiver(features, inputs)
  
  return serving_input_rcvr
&lt;/denchmark-code&gt;

		</comment>
		<comment id='43' author='josiahdavis' date='2018-02-15T19:25:25Z'>
		My strange error seemed to be related to parameter server and worker communications (h/t to &lt;denchmark-link:https://github.com/dawu76&gt;@dawu76&lt;/denchmark-link&gt;
  for &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/11549&gt;this one&lt;/denchmark-link&gt;
).
So dropped the type of the job from CUSTOM to BASIC in my config.yaml and i'm now getting a more informative error (below).
My updated code is &lt;denchmark-link:https://github.com/andrewm4894/my-google-cloudml-tensorflow-examples/tree/master/reddit_classifier&gt;here&lt;/denchmark-link&gt;
.
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
    "__main__", fname, loader, pkg_name)
  File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
    exec code in run_globals
  File "/root/.local/lib/python2.7/site-packages/trainer/task.py", line 344, in &lt;module&gt;
    main()
  File "/root/.local/lib/python2.7/site-packages/trainer/task.py", line 339, in main
    output_dir=output_dir
  File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py", line 218, in run
    return _execute_schedule(experiment, schedule)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py", line 46, in _execute_schedule
    return task()
  File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py", line 641, in train_and_evaluate
    export_results = self._maybe_export(eval_result)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py", line 744, in _maybe_export
    eval_result=eval_result))
  File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/export_strategy.py", line 87, in export
    return self.export_fn(estimator, export_path, **kwargs)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py", line 442, in export_fn
    checkpoint_path=checkpoint_path)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py", line 511, in export_savedmodel
    config=self.config)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py", line 694, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File "/root/.local/lib/python2.7/site-packages/trainer/task.py", line 202, in _model_fn
    features=features, labels=labels, mode=mode, config=estimator.config)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py", line 694, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/estimator/python/estimator/extenders.py", line 226, in new_model_fn
    verify_keys_and_predictions(features, predictions)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/estimator/python/estimator/extenders.py", line 212, in verify_keys_and_predictions
    'arguments of forward_features.'.format(key, features.keys()))
ValueError: keys should be exist in features. Key "example_id" is not in features dict. features dict has following keys: [u'subreddit_id', u'comment_body_bow', u'comment_parent_body_bow', u'toplevel', u'score', u'author_bow']. Please check arguments of forward_features.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='44' author='josiahdavis' date='2018-02-18T16:31:13Z'>
		&lt;denchmark-link:https://github.com/andrewm4894&gt;@andrewm4894&lt;/denchmark-link&gt;
 - I also encountered this  issue when trying to get  to work.
IIRC my issue was that the instance key wasn't among the fields returned by my  (defined almost exactly like the &lt;denchmark-link:https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/census/customestimator/trainer/model.py#L296&gt;input_fn&lt;/denchmark-link&gt;
 for the Cloud ML census classifier example).
The call to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/estimator/python/estimator/extenders.py#L139&gt;forward_features&lt;/denchmark-link&gt;
 returns a  with a custom  defined &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/estimator/python/estimator/extenders.py#L221&gt;here&lt;/denchmark-link&gt;
 in 

this custom model_fn calls verify_keys_and_predictions to confirm that the field specified in the keys argument of forward_features is actually present in the dict passed to the features argument of the model_fn.
This features dict is what's returned by the input_fn passed to the estimator's train, evaluate, and predict methods.
If the specified instance key is not among the features returned by input_fn, then the ValueError defined in this line of verify_keys_and_predictions is raised. Based on your error message, it looks like the input_fn in this Reddit classifier example returns a dict containing keys 'subreddit_id', 'comment_body_bow', 'comment_parent_body_bow', etc. but not 'example_id'.

		</comment>
		<comment id='45' author='josiahdavis' date='2018-02-19T19:02:27Z'>
		&lt;denchmark-link:https://github.com/dawu76&gt;@dawu76&lt;/denchmark-link&gt;
 That is precisely why it caused my problem.  I tried your json_serving_input_fn and it works! Thanks a lot!
		</comment>
		<comment id='46' author='josiahdavis' date='2018-03-04T20:38:36Z'>
		Folks, I finally got around to writing a guide on how to do this.  Hopefully, the two caveats at the end of the post explain the problems that people have been running into when using forward_features, and how to work around them:
&lt;denchmark-link:https://medium.com/@lakshmanok/how-to-extend-a-canned-tensorflow-estimator-to-add-more-evaluation-metrics-and-to-pass-through-ddf66cd3047d&gt;https://medium.com/@lakshmanok/how-to-extend-a-canned-tensorflow-estimator-to-add-more-evaluation-metrics-and-to-pass-through-ddf66cd3047d&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive/07_structured/babyweight/trainer/model.py#L157&gt;https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive/07_structured/babyweight/trainer/model.py#L157&lt;/denchmark-link&gt;

		</comment>
		<comment id='47' author='josiahdavis' date='2018-03-05T15:18:51Z'>
		Sweet - thanks a million &lt;denchmark-link:https://github.com/lakshmanok&gt;@lakshmanok&lt;/denchmark-link&gt;
 - going to have a crack at this again this week and if i can get it working on the Reddit example i will do a PR there maybe.
Cheers
		</comment>
		<comment id='48' author='josiahdavis' date='2018-03-05T22:46:50Z'>
		&lt;denchmark-link:https://github.com/lakshmanok&gt;@lakshmanok&lt;/denchmark-link&gt;
 i tried to implement what you suggested as part of a simplified reddit example.
&lt;denchmark-link:https://github.com/andrewm4894/my-google-cloudml-tensorflow-examples/tree/master/reddit_classifier_basic&gt;Here&lt;/denchmark-link&gt;
 is my repo. &lt;denchmark-link:https://github.com/andrewm4894/my-google-cloudml-tensorflow-examples/blob/6a8bde9f29394d209e43d190e40049ad6e0f9ff8/reddit_classifier_basic/trainer/task.py#L263&gt;This&lt;/denchmark-link&gt;
 is where i tried the approach you outlined, not sure what i did wrong.
&lt;denchmark-code&gt;message:  "Traceback (most recent call last):
  File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
    "__main__", fname, loader, pkg_name)
  File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
    exec code in run_globals
  File "/root/.local/lib/python2.7/site-packages/trainer/task.py", line 333, in &lt;module&gt;
    main()
  File "/root/.local/lib/python2.7/site-packages/trainer/task.py", line 328, in main
    output_dir=output_dir
  File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py", line 218, in run
    return _execute_schedule(experiment, schedule)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py", line 46, in _execute_schedule
    return task()
  File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py", line 641, in train_and_evaluate
    export_results = self._maybe_export(eval_result)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py", line 744, in _maybe_export
    eval_result=eval_result))
  File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/export_strategy.py", line 87, in export
    return self.export_fn(estimator, export_path, **kwargs)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py", line 442, in export_fn
    checkpoint_path=checkpoint_path)
  File "/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py", line 515, in export_savedmodel
    serving_input_receiver.receiver_tensors,
AttributeError: 'InputFnOps' object has no attribute 'receiver_tensors'
&lt;/denchmark-code&gt;

		</comment>
		<comment id='49' author='josiahdavis' date='2018-03-06T10:14:11Z'>
		So i found reference to my above  error &lt;denchmark-link:https://github.com/tensorflow/transform/issues/36&gt;here&lt;/denchmark-link&gt;
 which suggests using tft 0.3.
And &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/16456&gt;this&lt;/denchmark-link&gt;
 where it was suggested to use tf 1.5.
So i think part of what might be going on is tf and tft not being compatible which is something i have read &lt;denchmark-link:https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive/07_structured/4_preproc_tft.ipynb&gt;here&lt;/denchmark-link&gt;
.
So &lt;denchmark-link:https://github.com/andrewm4894/my-google-cloudml-tensorflow-examples/tree/master/reddit_classifier_basic_tft0dot4_tf1dot4&gt;this repo&lt;/denchmark-link&gt;
 is trying to use tft 0.4 and tf 1.4. But when i try &lt;denchmark-link:https://github.com/andrewm4894/my-google-cloudml-tensorflow-examples/blob/master/reddit_classifier_basic_tft0dot4_tf1dot4/run_preprocess.sh&gt;run_preprocess.sh&lt;/denchmark-link&gt;
 my dataflow job fails after 5 mins with:
&lt;denchmark-code&gt;Collecting apache-beam==2.2.0
  Using cached apache-beam-2.2.0.zip
  Saved /tmp/tmp3Lmn39/apache-beam-2.2.0.zip
Successfully downloaded apache-beam
/usr/local/lib/python2.7/dist-packages/apache_beam/io/gcp/gcsio.py:122: DeprecationWarning: object() takes no parameters
  super(GcsIO, cls).__new__(cls, storage_client))
Traceback (most recent call last):
  File "preprocess.py", line 250, in &lt;module&gt;
    main()
  File "preprocess.py", line 246, in main
    frequency_threshold=args.frequency_threshold)
  File "/usr/local/lib/python2.7/dist-packages/apache_beam/pipeline.py", line 346, in __exit__
    self.run().wait_until_finish()
  File "/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/dataflow_runner.py", line 966, in wait_until_finish
    (self.state, getattr(self._runner, 'last_error_msg', None)), self)
apache_beam.runners.dataflow.dataflow_runner.DataflowRuntimeException: Dataflow pipeline failed. State: FAILED, Error:
(945c74f32e18f617): Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/dataflow_worker/batchworker.py", line 582, in do_work
    work_executor.execute()
  File "/usr/local/lib/python2.7/dist-packages/dataflow_worker/executor.py", line 167, in execute
    op.start()
  File "apache_beam/runners/worker/operations.py", line 294, in apache_beam.runners.worker.operations.DoOperation.start
    def start(self):
  File "apache_beam/runners/worker/operations.py", line 295, in apache_beam.runners.worker.operations.DoOperation.start
    with self.scoped_start_state:
  File "apache_beam/runners/worker/operations.py", line 300, in apache_beam.runners.worker.operations.DoOperation.start
    pickler.loads(self.spec.serialized_fn))
  File "/usr/local/lib/python2.7/dist-packages/apache_beam/internal/pickler.py", line 225, in loads
    return dill.loads(s)
  File "/usr/local/lib/python2.7/dist-packages/dill/dill.py", line 277, in loads
    return load(file)
  File "/usr/local/lib/python2.7/dist-packages/dill/dill.py", line 266, in load
    obj = pik.load()
  File "/usr/lib/python2.7/pickle.py", line 858, in load
    dispatch[key](self)
  File "/usr/lib/python2.7/pickle.py", line 1217, in load_build
    setstate(state)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow_transform/tf_metadata/dataset_schema.py", line 282, in __setstate__
    self._dtype = tf.as_dtype(state['dtype'])
TypeError: string indices must be integers, not str
&lt;/denchmark-code&gt;

But strangely it seems to read in the eval data fine but not the train data:
&lt;denchmark-link:https://user-images.githubusercontent.com/2178292/37026633-e5ad2868-2126-11e8-9587-e89e02c4d3af.png&gt;&lt;/denchmark-link&gt;

I can see the error relates to &lt;denchmark-link:https://github.com/tensorflow/transform/blob/master/tensorflow_transform/tf_metadata/dataset_schema.py&gt;tensorflow_transform/tf_metadata/dataset_schema.py&lt;/denchmark-link&gt;
 but have no idea what i'm doing here thats the issue.
Arrrgghhh! :(
		</comment>
		<comment id='50' author='josiahdavis' date='2018-03-11T23:43:24Z'>
		&lt;denchmark-link:https://github.com/lakshmanok&gt;@lakshmanok&lt;/denchmark-link&gt;
 have been following your approach and got it working.
However i've been trying to follow another post of yours &lt;denchmark-link:https://towardsdatascience.com/how-to-do-text-classification-using-tensorflow-word-embeddings-and-cnn-edae13b3e575&gt;here&lt;/denchmark-link&gt;
 to get the reddit comments into a bow for wide and embedding for deep.
Am pretty sure i followed things exactly but am getting an error:
&lt;denchmark-code&gt;"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3514, in name_scope raise ValueError("'%s' is not a valid scope name" % name) ValueError: 'Tensor("Slice:0", shape=(?, 20), dtype=int64)_embedding' is not a valid scope name
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/andrewm4894/my-google-cloudml-tensorflow-examples/blob/e4ece9f8ebe6eeffde2022cedd75c576ae0e2d3f/reddit_score/trainer/model.py#L103&gt;Here&lt;/denchmark-link&gt;
 is the part of the code where i try featurize the comments and &lt;denchmark-link:https://stackoverflow.com/questions/48543299/converting-google-cloud-ml-github-reddit-example-from-regression-to-classificati&gt;"Update 3"&lt;/denchmark-link&gt;
 to my big SO post.
Any ideas? Have i missed something obvious?
Am keen to try get the reddit example working and maybe contribute it back to the cloud ml samples repo. Think this error is pretty much last step.
		</comment>
		<comment id='51' author='josiahdavis' date='2018-10-13T04:30:39Z'>
		&lt;denchmark-link:https://github.com/andrewm4894&gt;@andrewm4894&lt;/denchmark-link&gt;
 Have you tried to update TF and TFT versions? &lt;denchmark-link:https://github.com/elmer-garduno&gt;@elmer-garduno&lt;/denchmark-link&gt;
 do you have thoughts here?
		</comment>
		<comment id='52' author='josiahdavis' date='2018-12-11T14:43:19Z'>
		As of December 2018, I can confirm that &lt;denchmark-link:https://github.com/lakshmanok&gt;@lakshmanok&lt;/denchmark-link&gt;
 approach with &lt;denchmark-link:https://github.com/dawu76&gt;@dawu76&lt;/denchmark-link&gt;
 serving_input_fn() works. Thanks for the help!
		</comment>
		<comment id='53' author='josiahdavis' date='2019-02-27T06:18:27Z'>
		Hi, Could anyone help with &lt;denchmark-link:https://stackoverflow.com/questions/54899188/error-model-att-seq2seq-minimum0-is-both-fed-and-fetched&gt;this&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='54' author='josiahdavis' date='2019-05-16T16:49:09Z'>
		Going to close this issue due to age, please feel free to open a new issue for any help as these samples have changed and been updated since these issues were raised. :)
		</comment>
	</comments>
</bug>