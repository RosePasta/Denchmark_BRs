<bug id='175' author='bobleegogogo' open_date='2018-03-14T17:30:26Z' closed_time='2018-10-13T04:47:10Z'>
	<summary>Error using Object Detection API for Google cloud ML training</summary>
	<description>
For problems running the sample code please provide the following information.
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Ubuntu 16.04)
TensorFlow version 1.6 CPU
python version 3.5:

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Hello guys I am using Google Object Detection API and training my building detection model in Google cloud ML platform. I follow the example and build my model, after running for several steps like(1000), my job failed with the following logs, could someone maybe help me and tell me where shall I check the possible error?
In addition, my model could work and start training in local CPU:
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;The replica worker 4 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): [...] File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py", line 473, in exit c_api.TF_GetCode(self.status.status)) tensorflow.python.framework.errors_impl.UnavailableError: Stream removed During handling of the above exception, another exception occurred: Traceback (most recent call last): File "/usr/lib/python3.5/runpy.py", line 184, in _run_module_as_main "main", mod_spec) File "/usr/lib/python3.5/runpy.py", line 85, in _run_code exec(code, run_globals) File "/root/.local/lib/python3.5/site-packages/object_detection/train.py", line 167, in  tf.app.run() File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py", line 124, in run _sys.exit(main(argv)) File "/root/.local/lib/python3.5/site-packages/object_detection/train.py", line 163, in main worker_job_name, is_chief, FLAGS.train_dir) File "/root/.local/lib/python3.5/site-packages/object_detection/trainer.py", line 360, in train saver=saver) File "/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/slim/python/slim/learning.py", line 767, in train sess, train_op, global_step, train_step_kwargs) File "/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/slim/python/slim/learning.py", line 487, in train_step run_metadata=run_metadata) File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 895, in run run_metadata_ptr) File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1128, in _run feed_dict_tensor, options, run_metadata) File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1344, in _do_run options, run_metadata) File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1363, in _do_call raise type(e)(node_def, op, message) tensorflow.python.framework.errors_impl.UnavailableError: Stream removed To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=1019245068940&amp;resource=ml_job%2Fjob_id%2Fhd_hao_object_detection_1521046604&amp;advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%22hd_hao_object_detection_1521046604%22&lt;/denchmark-h&gt;

This is my training input.
{
"scaleTier": "CUSTOM",
"masterType": "standard_gpu",
"workerType": "standard_gpu",
"parameterServerType": "standard",
"workerCount": "5",
"parameterServerCount": "3",
"packageUris": [
"gs://deepvgihao/train/packages/8041fa8910403c6ef4ab9c71abd1b3f2a794eb0868d16c0c7a1596153eaeff53/object_detection-0.1.tar.gz",
"gs://deepvgihao/train/packages/8041fa8910403c6ef4ab9c71abd1b3f2a794eb0868d16c0c7a1596153eaeff53/slim-0.1.tar.gz"
],
"pythonModule": "object_detection.train",
"args": [
"--train_dir=gs://deepvgihao/train",
"--pipeline_config_path=gs://deepvgihao/ssd_inception_v2_coco.config"
],
"region": "us-central1",
"runtimeVersion": "1.5",
"jobDir": "gs://deepvgihao/train",
"pythonVersion": "3.5"
	</description>
	<comments>
		<comment id='1' author='bobleegogogo' date='2018-03-14T23:14:27Z'>
		Hi
I am looking into this issue and trying to figure out what went wrong. In order to do this, can you please provide the followings to me:
1- Which one of the samples in the "cloudml-samples" repo are you running?
2- I cannot access the content of the log file (with the link) in your post. Can you provide a copy of the log file?
Thanks
		</comment>
		<comment id='2' author='bobleegogogo' date='2018-03-15T08:57:18Z'>
		Hallo,
Basically, I am running a sample like 'pet', but with my own building training and test data. And using the model of "ssd_inception_v2_coco".
About the log file, It seems I could not export it from my google cloud ML platform,  I copy the error log here as attach.
I also got the question that: is this because I am using tensorflow 1.6 and only with the runtime [1.5?]
&lt;denchmark-link:https://github.com/GoogleCloudPlatform/cloudml-samples/files/1814605/error.log.docx&gt;error log.docx&lt;/denchmark-link&gt;

Thanks for your help
		</comment>
		<comment id='3' author='bobleegogogo' date='2018-03-15T09:02:09Z'>
		I think here is the main problem, I attach the screenshot.
&lt;denchmark-link:https://user-images.githubusercontent.com/33456076/37453430-e9fb9036-2837-11e8-93c6-a78dff89ef82.PNG&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='bobleegogogo' date='2018-03-15T18:17:43Z'>
		I have not seen this issue before, and it is a bit difficult to say what went wrong without having access to your code or data. I am also wondering why you posted this issue in this repo if your issue is not related to the code in this repo. Can you please elaborate?
		</comment>
		<comment id='5' author='bobleegogogo' date='2018-03-22T08:23:02Z'>
		I've been tackling a similar problem, but by running default scripts from object_detection module from 'tensorflow/models' so it does not seem like a problem specific to this code.
The replica worker 3 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File "/usr/lib/python2.7/runpy.py", line 174, in _run_module_as_main "__main__", fname, loader, pkg_name) File "/usr/lib/python2.7/runpy.py", line 72, in _run_code exec code in run_globals File "/root/.local/lib/python2.7/site-packages/object_detection/train.py", line 167, in &lt;module&gt; tf.app.run() File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py", line 124, in run _sys.exit(main(argv)) File "/root/.local/lib/python2.7/site-packages/object_detection/train.py", line 163, in main worker_job_name, is_chief, FLAGS.train_dir) File "/root/.local/lib/python2.7/site-packages/object_detection/trainer.py", line 360, in train saver=saver) File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py", line 767, in train sess, train_op, global_step, train_step_kwargs) File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py", line 487, in train_step run_metadata=run_metadata) File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 895, in run run_metadata_ptr) File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1128, in _run feed_dict_tensor, options, run_metadata) File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1344, in _do_run options, run_metadata) File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1363, in _do_call raise type(e)(node_def, op, message) UnavailableError: Stream removed To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=188784501564&amp;resource=ml_job%2Fjob_id%2Fobject_detection_123&amp;advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%22object_detection_123%22  
Training input:
&lt;denchmark-code&gt;{
  "scaleTier": "CUSTOM",
  "masterType": "standard_gpu",
  "workerType": "standard_gpu",
  "parameterServerType": "standard",
  "workerCount": "5",
  "parameterServerCount": "2",
  "packageUris": [
    "gs://ccfoto-model-data/models/train/packages/8880f9e0cab51704bb5efe288e49e8334edbb9b3ced889c8aa8fa4697969dbb6/object_detection-0.1.tar.gz",
    "gs://ccfoto-model-data/models/train/packages/8880f9e0cab51704bb5efe288e49e8334edbb9b3ced889c8aa8fa4697969dbb6/slim-0.1.tar.gz"
  ],
  "pythonModule": "object_detection.train",
  "args": [
    "--train_dir=gs://ccfoto-model-data/models/train",
    "--pipeline_config_path=gs://ccfoto-model-data/pipeline.config"
  ],
  "region": "europe-west1",
  "runtimeVersion": "1.5",
  "jobDir": "gs://ccfoto-model-data/models/train"
}
&lt;/denchmark-code&gt;

Posting this for possible help with the OP's issue, but if you know how to fix my error feel free to respond :)
		</comment>
		<comment id='6' author='bobleegogogo' date='2018-03-26T18:50:05Z'>
		I am hitting the same error as reported by &lt;denchmark-link:https://github.com/bobleegogogo&gt;@bobleegogogo&lt;/denchmark-link&gt;
 , it seems like it is related to the interaction of tensorflow and gcloud. The workers are dying.
Unfortunately there is no way for a consumer to debug this.
The error is also not completely reproducible - so it happens intermittently. For some of my runs it does not happen at all.
Should this be a ticket for tensorflow ? or cloud-ml ?
		</comment>
		<comment id='7' author='bobleegogogo' date='2018-03-27T02:52:46Z'>
		Same issue as well, using    --runtime-version 1.5 \
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1344, in _do_run options, run_metadata) File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1363, in _do_call raise type(e)(node_def, op, message) UnavailableError: Stream removed
		</comment>
		<comment id='8' author='bobleegogogo' date='2018-03-27T07:07:12Z'>
		I propose trying increased parameter server spec. &lt;denchmark-link:https://github.com/tensorflow/models/issues/3757#issuecomment-376419016&gt;Details&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='bobleegogogo' date='2018-05-31T15:26:52Z'>
		&lt;denchmark-link:https://github.com/TopHatCroat&gt;@TopHatCroat&lt;/denchmark-link&gt;

I tried you suggestion but I still have same issue
Is there another idea?
&lt;denchmark-code&gt; Training input

{
  "scaleTier": "CUSTOM",
  "masterType": "complex_model_m_gpu",
  "workerType": "complex_model_m_gpu",
  "parameterServerType": "complex_model_m",
  "workerCount": "3",
  "parameterServerCount": "3",
  "packageUris": [
    "gs://mask-data/train/packages/76d14420390a8e70658b58e6d81fc9589d6cd406b2bf813333fa5dbdaad8c19d/object_detection-0.1.tar.gz",
    "gs://mask-data/train/packages/76d14420390a8e70658b58e6d81fc9589d6cd406b2bf813333fa5dbdaad8c19d/slim-0.1.tar.gz"
  ],
  "pythonModule": "object_detection.train",
  "args": [
    "--train_dir=gs://mask-data/train",
    "--pipeline_config_path=gs://mask-data/data/mask_rcnn_inception_v2_GCS.config"
  ],
  "region": "us-central1",
  "runtimeVersion": "1.6",
  "jobDir": "gs://mask-data/train",
  "pythonVersion": "3.5"
}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='bobleegogogo' date='2018-06-01T11:26:42Z'>
		&lt;denchmark-link:https://github.com/Abduoit&gt;@Abduoit&lt;/denchmark-link&gt;
 can't tell much more about this as I was not able to fix it properly, instead just play with the numbers and hope for the best.
A good place to look are your images as well, they should be small ie. 500x500 pixels or so at most, I've realized that these can affect memory quite a bit, but before, take a look at your batch size in your training config, you could reduce that number as well to try and avoid OOM errors.
One last thing, You probably don't need 3 complex_model_m as parameter server, you can take a look at the memory and CPU usage from results in the console to get a feel for what you need to change (low utilization for CPU or memory on parameters server means you could reduce their number or spec as they are mostly unused resources you are paying for, higher usage (even when just around 40% of memory in my case) means you should probably increase it)
		</comment>
		<comment id='11' author='bobleegogogo' date='2018-06-01T14:09:14Z'>
		&lt;denchmark-link:https://github.com/TopHatCroat&gt;@TopHatCroat&lt;/denchmark-link&gt;
 Thanks for reply, my images are 800X600 pixels. I ll try to reduce them first, then I ll retrain again.
		</comment>
		<comment id='12' author='bobleegogogo' date='2018-06-27T21:53:47Z'>
		Any solution concerning this problem ? I am having the same error described above.
		</comment>
		<comment id='13' author='bobleegogogo' date='2018-10-13T04:47:08Z'>
		&lt;denchmark-link:https://github.com/bobleegogogo&gt;@bobleegogogo&lt;/denchmark-link&gt;
 can you please open an SO post on this and link this issue. The issue reported here is not for repo samples and so should be discussed on SO. I'm closing this issue.
		</comment>
		<comment id='14' author='bobleegogogo' date='2019-01-14T13:08:33Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/30552941/51114500-807f5680-182b-11e9-9a77-08451576fdfb.png&gt;&lt;/denchmark-link&gt;

Hi i am facing a similar issue.
		</comment>
	</comments>
</bug>