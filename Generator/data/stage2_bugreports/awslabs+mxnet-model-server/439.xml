<bug id='439' author='kalyc' open_date='2018-07-09T21:07:09Z' closed_time='2018-07-20T21:45:04Z'>
	<summary>MMS rounds up probabilities from training result</summary>
	<description>
My model trained in keras-mxnet predicts results as - [0.99998, 0.00002]
MMS however, rounds those up to - [1.0, 0.0]
While the predicted class is correct, the probability values appear a little misleading :)
Requesting the team to take a look at this issue.
This is my training script - &lt;denchmark-link:https://github.com/kalyc/SmileCNN/blob/master/train.py&gt;https://github.com/kalyc/SmileCNN/blob/master/train.py&lt;/denchmark-link&gt;

This is my evaluation script - &lt;denchmark-link:https://github.com/kalyc/SmileCNN/blob/master/evaluation.py&gt;https://github.com/kalyc/SmileCNN/blob/master/evaluation.py&lt;/denchmark-link&gt;

This is my custom service file - &lt;denchmark-link:https://github.com/kalyc/SmileCNN/blob/master/keras-mms/custom_service.py&gt;https://github.com/kalyc/SmileCNN/blob/master/keras-mms/custom_service.py&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='kalyc' date='2018-07-11T21:31:10Z'>
		Will look into it &lt;denchmark-link:https://github.com/kalyc&gt;@kalyc&lt;/denchmark-link&gt;
 , Thank you for reporting the issue
		</comment>
		<comment id='2' author='kalyc' date='2018-07-12T03:11:41Z'>
		&lt;denchmark-link:https://github.com/kalyc&gt;@kalyc&lt;/denchmark-link&gt;
 : Just out of curiosity, was the output of "postprocess" in your custom service rounded up or was it only the value returned by MMS?
		</comment>
		<comment id='3' author='kalyc' date='2018-07-12T21:05:34Z'>
		Value returned by MMS was rounded up - the postprocess is returning top probabilities and those were rounded up.
		</comment>
		<comment id='4' author='kalyc' date='2018-07-20T00:29:36Z'>
		The issue is arising from the preprocessing stage. When you are loading the data in your evaluation script, it is in the format of floats from 0 to 1.
[[[[0.3019608  0.23529412 0.2901961  ... 0.41960785 0.3372549
0.3137255 ]
[0.27058825 0.25490198 0.2784314  ... 0.45490196 0.35686275
0.34509805]
[0.24313726 0.21960784 0.28235295 ... 0.47058824 0.34117648
0.34509805]
...
[0.29411766 0.3372549  0.38039216 ... 0.46666667 0.4117647
0.23137255]
[0.2784314  0.34509805 0.34901962 ... 0.40784314 0.2627451
0.15686275]
[0.2509804  0.2627451  0.3647059  ... 0.25490198 0.1882353
0.1764706 ]]]]
However, it is loaded in your mxnet model server's custom service's preprocessing stage as ints from 0 to 255
[[[[ 77  60  74 ... 107  86  80]
[ 69  65  71 ... 116  91  88]
[ 62  56  72 ... 120  87  88]
...
[ 75  86  97 ... 119 105  59]
[ 71  88  89 ... 104  67  40]
[ 64  67  93 ...  65  48  45]]]]
The model symbols actually don't specify a data type, so it works anyway.  But, the difference in scale produces slightly different outputs.  Because you trained your network with floats, I recommend changing the preprocessing stage and converting it to produce floats as well.
		</comment>
		<comment id='5' author='kalyc' date='2018-07-20T16:57:39Z'>
		Hi &lt;denchmark-link:https://github.com/zachgk&gt;@zachgk&lt;/denchmark-link&gt;
 could you provide example code of how to do that?
		</comment>
		<comment id='6' author='kalyc' date='2018-07-20T17:02:31Z'>
		Reopening this issue as there is more conversation on this issue.
		</comment>
		<comment id='7' author='kalyc' date='2018-07-20T18:03:03Z'>
		Yeah.  In your custom service file /keras-mms/custom_service.py, add the following line to your _preprocess function:
    def _preprocess(self, data):
        img_list = []
        for idx, img in enumerate(data):
            input_shape = self.signature['inputs'][idx]['data_shape']
            [h, w] = input_shape[2:]
            img_arr = image.read(img, 0)
            img_arr = image.resize(img_arr, w, h)
            img_arr = image.transform_shape(img_arr)
            img_arr = img_arr.astype('float32') / 256.0 # Add this line
            img_list.append(img_arr)
        return img_list
This converts the image ndarray to the same format as you use both during training and with the evaluation script.  Once you add this, you should see the same results ([0.99998, 0.00002]) as you expect.
		</comment>
		<comment id='8' author='kalyc' date='2018-07-20T18:44:44Z'>
		&lt;denchmark-link:https://github.com/kalyc&gt;@kalyc&lt;/denchmark-link&gt;
 Please let us know if this fix worked for you.
		</comment>
		<comment id='9' author='kalyc' date='2018-07-20T21:45:04Z'>
		Yes it works. Thanks for looking into the issue!
		</comment>
	</comments>
</bug>