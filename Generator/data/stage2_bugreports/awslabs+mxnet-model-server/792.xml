<bug id='792' author='JustinhoCHN' open_date='2019-04-25T07:26:41Z' closed_time='2019-05-11T01:00:00Z'>
	<summary>Batching inference example code may cause crashes while decoding invalid image</summary>
	<description>
I tried to post an invalid image which opencv cannot decode it to mms, mms will raise an exception like:
&lt;denchmark-code&gt;[07:22:26] src/io/image_io.cc:147: Decoding failed. Invalid image file.
2019-04-25 07:22:26,767 [INFO ] W-9000-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - unknown storage type: -1
2019-04-25 07:22:26,767 [INFO ] W-9000-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Traceback (most recent call last):
2019-04-25 07:22:26,767 [INFO ] W-9000-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File "/home/model-server/tmp/models/ef593229f3f72ad3aeabcd48d66fc949cee8216d/retrieval_service_batching.py", line 164, in preprocess
2019-04-25 07:22:26,767 [INFO ] W-9000-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     img_arr = mx.image.imdecode(img, 1, True, None)
2019-04-25 07:22:26,767 [INFO ] W-9000-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File "/usr/local/lib/python2.7/dist-packages/mxnet/image/image.py", line 137, in imdecode
2019-04-25 07:22:26,767 [INFO ] W-9000-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     return _internal._cvimdecode(buf, *args, **kwargs)
2019-04-25 07:22:26,767 [INFO ] W-9000-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File "&lt;string&gt;", line 36, in _cvimdecode
2019-04-25 07:22:26,767 [INFO ] W-9000-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File "/usr/local/lib/python2.7/dist-packages/mxnet/_ctypes/ndarray.py", line 98, in _imperative_invoke
2019-04-25 07:22:26,767 [INFO ] W-9000-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     stype=out_stypes[0])
2019-04-25 07:22:26,768 [INFO ] W-9000-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File "/usr/local/lib/python2.7/dist-packages/mxnet/ndarray/sparse.py", line 1185, in _ndarray_cls
2019-04-25 07:22:26,768 [INFO ] W-9000-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     raise Exception("unknown storage type: %s"%stype)
2019-04-25 07:22:26,768 [INFO ] W-9000-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Exception: unknown storage type: -1
2019-04-25 07:22:26,768 [INFO ] W-9000-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - value 0 for Parameter num_args should be greater equal to 1, in operator stack(name="", num_args="0")
2019-04-25 07:22:26,768 [INFO ] W-9000-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Traceback (most recent call last):
2019-04-25 07:22:26,768 [INFO ] W-9000-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File "/home/model-server/tmp/models/ef593229f3f72ad3aeabcd48d66fc949cee8216d/retrieval_service_batching.py", line 218, in handle
2019-04-25 07:22:26,768 [INFO ] W-9000-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     data = _service.preprocess(data)
2019-04-25 07:22:26,768 [INFO ] W-9000-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File "/home/model-server/tmp/models/ef593229f3f72ad3aeabcd48d66fc949cee8216d/retrieval_service_batching.py", line 182, in preprocess
2019-04-25 07:22:26,768 [INFO ] W-9000-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     reqs = mx.nd.stack(*img_list)
2019-04-25 07:22:26,768 [INFO ] W-9000-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File "&lt;string&gt;", line 45, in stack
2019-04-25 07:22:26,768 [INFO ] W-9000-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File "/usr/local/lib/python2.7/dist-packages/mxnet/_ctypes/ndarray.py", line 92, in _imperative_invoke
2019-04-25 07:22:26,768 [INFO ] W-9000-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     ctypes.byref(out_stypes)))
2019-04-25 07:22:26,768 [INFO ] W-9000-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File "/usr/local/lib/python2.7/dist-packages/mxnet/base.py", line 252, in check_call
2019-04-25 07:22:26,768 [INFO ] W-9000-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     raise MXNetError(py_str(_LIB.MXGetLastError()))
2019-04-25 07:22:26,768 [INFO ] W-9000-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNetError: value 0 for Parameter num_args should be greater equal to 1, in operator stack(name="", num_args="0")
&lt;/denchmark-code&gt;

BUT if I post an normal image to mms again, mms cannot process it until I restart the mms!
&lt;denchmark-code&gt;This request was not processed successfully. Refer to mms.log for additional information
&lt;/denchmark-code&gt;

After some digging, I think I found the problem.
In &lt;denchmark-link:https://github.com/awslabs/mxnet-model-server/blob/master/examples/model_service_template/mxnet_vision_batching.py&gt;mxnet-model-server/examples/model_service_template/mxnet_vision_batching.py&lt;/denchmark-link&gt;
 , the init part:
&lt;denchmark-code&gt;class MXNetVisionServiceBatching(object):
    def __init__(self):
        """
        Initialization for MXNet Vision Service supporting batch inference
        """
        self.mxnet_ctx = None
        ......
        self.erroneous_reqs = set()
&lt;/denchmark-code&gt;

there's a python set() object initialized in the init method, which is used to store the request index that are not processed successfully, these requests either sending a none image, or the image file cannot be decoded correctly.
&lt;denchmark-code&gt;try:
    img_arr = mx.image.imdecode(img, 1, True, None)
except Exception as e:
    logging.error(e, exc_info=True)
    self.erroneous_reqs.add(idx)
    continue
&lt;/denchmark-code&gt;

If the request index is in the erroneous_reqs, it'll return the error message:
&lt;denchmark-code&gt;def postprocess(self, data):
    res = []
    for idx, resp in data[:self._num_requests]:
        if idx not in self.erroneous_reqs:
            res.append(self.top_probability(resp, self.labels, top=5))
        else:
            res.append("This request was not processed successfully. Refer to mms.log for additional information")
    return res
&lt;/denchmark-code&gt;

BUT the problem is, it didn't remove the index after returning the error message, when the next request comes, the index still in the erroneous_reqs, so this new request still be treated as wrong image!
So when I re-initialize the  self.erroneous_reqs in the preprocess method, this problem gone away.
&lt;denchmark-code&gt;def preprocess(self, request):
    """
    Decode all input images into ndarray.

    Note: This implementation doesn't properly handle error cases in batch mode,
    If one of the input images is corrupted, all requests in the batch will fail.

    :param request:
    :return:
    """
    self.erroneous_reqs = set()
    img_list = []
    ......
&lt;/denchmark-code&gt;

I don't know if this change will affect batching inference, since I just test it in batchsize=1.
	</description>
	<comments>
		<comment id='1' author='JustinhoCHN' date='2019-04-25T15:59:52Z'>
		&lt;denchmark-link:https://github.com/JustinhoCHN&gt;@JustinhoCHN&lt;/denchmark-link&gt;
 : Thanks for raising it. We will fix the example.
		</comment>
	</comments>
</bug>