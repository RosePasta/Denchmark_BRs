<bug id='2362' author='oshaikh13' open_date='2020-07-22T21:00:11Z' closed_time='2020-07-22T21:28:04Z'>
	<summary>Masked LM BERT training broken due to "has_marked_unused_parameters_"</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Ran the scripts verbatim &lt;denchmark-link:https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.pretraining.md&gt;here&lt;/denchmark-link&gt;
, except I used bert_base as the architecture.
Specifically, this script:
&lt;denchmark-code&gt;TOTAL_UPDATES=125000    # Total number of training steps
WARMUP_UPDATES=10000    # Warmup the learning rate over this many updates
PEAK_LR=0.0005          # Peak learning rate, adjust as needed
TOKENS_PER_SAMPLE=512   # Max sequence length
MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)
MAX_SENTENCES=16        # Number of sequences per batch (batch size)
UPDATE_FREQ=16          # Increase the batch size 16x

DATA_DIR=data-bin/wikitext-103

fairseq-train --fp16 $DATA_DIR \
    --task masked_lm --criterion masked_lm \
    --arch bert_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \
    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \
    --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \
    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \
    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \
    --max-update $TOTAL_UPDATES --log-format simple --log-interval 1
&lt;/denchmark-code&gt;

Here's the stack-trace error
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/nethome/oshaikh3/miniconda3/bin/fairseq-train", line 33, in &lt;module&gt;
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/nethome/oshaikh3/og_fairseq/fairseq_cli/train.py", line 349, in cli_main
    distributed_utils.call_main(args, main)
  File "/nethome/oshaikh3/og_fairseq/fairseq/distributed_utils.py", line 174, in call_main
    args.distributed_world_size,
  File "/nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 158, in start_processes
    while not context.join():
  File "/nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 119, in join
    raise Exception(msg)
Exception: 

-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File "/nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 20, in _wrap
    fn(i, *args)
  File "/nethome/oshaikh3/og_fairseq/fairseq/distributed_utils.py", line 156, in distributed_main
    main(args, **kwargs)
  File "/nethome/oshaikh3/og_fairseq/fairseq_cli/train.py", line 121, in main
    valid_losses, should_stop = train(args, trainer, task, epoch_itr)
  File "/nethome/oshaikh3/miniconda3/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "/nethome/oshaikh3/og_fairseq/fairseq_cli/train.py", line 216, in train
    log_output = trainer.train_step(samples)
  File "/nethome/oshaikh3/miniconda3/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "/nethome/oshaikh3/og_fairseq/fairseq/trainer.py", line 457, in train_step
    raise e
  File "/nethome/oshaikh3/og_fairseq/fairseq/trainer.py", line 431, in train_step
    ignore_grad=is_dummy_batch,
  File "/nethome/oshaikh3/og_fairseq/fairseq/tasks/fairseq_task.py", line 350, in train_step
    optimizer.backward(loss)
  File "/nethome/oshaikh3/og_fairseq/fairseq/optim/fp16_optimizer.py", line 117, in backward
    loss.backward()
  File "/nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: has_marked_unused_parameters_ INTERNAL ASSERT FAILED at /opt/conda/conda-bld/pytorch_1591914880026/work/torch/csrc/distributed/c10d/reducer.cpp:327, please report a bug to PyTorch.  (mark_variable_ready at /opt/conda/conda-bld/pytorch_1591914880026/work/torch/csrc/distributed/c10d/reducer.cpp:327)
frame #0: c10::Error::Error(c10::SourceLocation, std::string const&amp;) + 0x4e (0x7fdd2d48db5e in /nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #1: c10d::Reducer::mark_variable_ready(c10d::Reducer::VariableIndex) + 0x9ba (0x7fdd5ae6f3aa in /nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #2: c10d::Reducer::autograd_hook(c10d::Reducer::VariableIndex) + 0x2d0 (0x7fdd5ae6f910 in /nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #3: &lt;unknown function&gt; + 0x8a395c (0x7fdd5ae6495c in /nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #4: torch::autograd::Engine::evaluate_function(std::shared_ptr&lt;torch::autograd::GraphTask&gt;&amp;, torch::autograd::Node*, torch::autograd::InputBuffer&amp;) + 0x60d (0x7fdd5758100d in /nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)
frame #5: torch::autograd::Engine::thread_main(std::shared_ptr&lt;torch::autograd::GraphTask&gt; const&amp;, bool) + 0x3d2 (0x7fdd57582ed2 in /nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)
frame #6: torch::autograd::Engine::thread_init(int) + 0x39 (0x7fdd5757b549 in /nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)
frame #7: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7fdd5aacb638 in /nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #8: &lt;unknown function&gt; + 0xc819d (0x7fdd5d33f19d in /nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/lib/../../../.././libstdc++.so.6)
frame #9: &lt;unknown function&gt; + 0x76db (0x7fdd766776db in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #10: clone + 0x3f (0x7fdd763a088f in /lib/x86_64-linux-gnu/libc.so.6)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Model training starts.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


fairseq Version (e.g., 1.0 or master): master
PyTorch Version (e.g., 1.0): 1.5
OS (e.g., Linux): linux
How you installed fairseq (pip, source): source
Build command you used (if compiling from source):
Python version: 3.6
CUDA/cuDNN version: 10.1
GPU models and configuration: 4xV100
Any other relevant information:

&lt;denchmark-h:h3&gt;Additional Context&lt;/denchmark-h&gt;

This error was referenced in &lt;denchmark-link:https://github.com/pytorch/fairseq/pull/1709&gt;#1709&lt;/denchmark-link&gt;
 towards the end of the PR. Also similar to these issues:
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/32490&gt;pytorch/pytorch#32490&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/31035&gt;pytorch/pytorch#31035&lt;/denchmark-link&gt;
.
	</description>
	<comments>
		<comment id='1' author='oshaikh13' date='2020-07-22T21:19:10Z'>
		This error does not show up for RoBERTa training -- only for BERT architectures.
		</comment>
		<comment id='2' author='oshaikh13' date='2020-07-22T21:28:03Z'>
		Was able to fix  avoid this bug using --ddp-backend=no_c10d
Documenting this in case anyone runs into this in the future :)
		</comment>
	</comments>
</bug>