<bug id='1435' author='xiaoshengjun' open_date='2019-11-27T07:27:07Z' closed_time='2020-01-09T15:52:36Z'>
	<summary>Train a new translate model with lev_transform is wrong.</summary>
	<description>
Hi, I download the original  WMT'14 En-De dataset, and I processed it with scripts:
common_params='--source-lang en --target-lang de'
trainpref='./out/data_wm_raw/train.en-de'
validpref='./out/data_wm_raw/valid.en-de'
python preprocess.py 
$common_params 
--trainpref $trainpref 
--validpref $validpref 
--destdir './out/data_wm_be/' 
--workers 4 \
--joined-dictionary 
--thresholdtgt 5 \
--thresholdsrc 5 \
And then , I trained the model with the scripts:
DATA_BIN=./out/data_wm_be/
CUDA_VISIBLE_DEVICES=$device nohup python train.py $DATA_BIN 
--save-dir $MODELS 
--max-tokens 8000 
--ddp-backend=no_c10d 
--noise random_delete 
--share-all-embeddings 
--task translation_lev 
--train-subset train 
--valid-subset valid 
--criterion nat_loss 
--arch levenshtein_transformer 
--optimizer adam --adam-betas '(0.9, 0.98)' 
--lr 0.0005 --lr-scheduler inverse_sqrt 
--min-lr '1e-09' --warmup-updates 10000 
--warmup-init-lr '1e-07' --label-smoothing 0.1 
--dropout 0.3 --weight-decay 0.01 
--decoder-learned-pos 
--encoder-learned-pos 
--apply-bert-init 
--log-format 'simple' 
--fixed-validation-seed 7 \
--no-progress-bar 
--save-interval-updates 10000 
--max-update 300000 
--log-interval 1000  &gt; $OUT/log$exp.out 2&gt;&amp;1 &amp;
and the error is :
| model levenshtein_transformer, criterion LabelSmoothedDualImitationCriterion
| num. model params: 65084416 (num. trained: 65084416)
| training on 2 GPUs
| max tokens per GPU = 8000 and max sentences per GPU = None
| no existing checkpoint found out/models20191127wm/checkpoint_last.pt
| loading train data for epoch 0
| loaded 3961179 examples from: ./out/data_wm_be/train.en-de.en
| loaded 3961179 examples from: ./out/data_wm_be/train.en-de.de
| ./out/data_wm_be/ train en-de 3961179 examples
| NOTICE: your device may support faster training with --fp16
Traceback (most recent call last):
File "train.py", line 337, in 
cli_main()
File "train.py", line 329, in cli_main
nprocs=args.distributed_world_size,
File "/root/anaconda3/envs/len_transform/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 171, in spawn
while not spawn_context.join():
File "/root/anaconda3/envs/len_transform/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 107, in join
(error_index, name)
Exception: process 0 terminated with signal SIGSEGV
^C
Could you help me find out what makes this situation?
	</description>
	<comments>
		<comment id='1' author='xiaoshengjun' date='2019-12-03T16:04:32Z'>
		Does it work with 1 GPU? Try CUDA_VISIBLE_DEVICES=0 python train.py (...)
		</comment>
		<comment id='2' author='xiaoshengjun' date='2019-12-03T16:12:44Z'>
		Actually the LightConv LM seems not to be working -- apparently it never had test coverage so it's missing a few parameters :/
I'll submit a fix shortly.
		</comment>
		<comment id='3' author='xiaoshengjun' date='2019-12-04T01:48:00Z'>
		
Does it work with 1 GPU? Try CUDA_VISIBLE_DEVICES=0 python train.py (...)

no，I have tried using it with 1 GPU， but It doesn't work.
		</comment>
		<comment id='4' author='xiaoshengjun' date='2019-12-04T01:48:21Z'>
		
Actually the LightConv LM seems not to be working -- apparently it never had test coverage so it's missing a few parameters :/
I'll submit a fix shortly.

OK, thank you for your answer.
		</comment>
		<comment id='5' author='xiaoshengjun' date='2019-12-04T11:07:21Z'>
		
Actually the LightConv LM seems not to be working -- apparently it never had test coverage so it's missing a few parameters :/
I'll submit a fix shortly.

Sorry to bother you again, may I ask if the bug has been repaired?
		</comment>
		<comment id='6' author='xiaoshengjun' date='2019-12-04T16:07:13Z'>
		Sorry, I misread your original comment and thought this was about LightConv LM, but you're actually asking about Levenshtein Transformer. I'll take a look...
		</comment>
		<comment id='7' author='xiaoshengjun' date='2019-12-04T16:50:35Z'>
		I just tried your command using the master version of fairseq and it worked. Can you please try again?
		</comment>
		<comment id='8' author='xiaoshengjun' date='2019-12-05T03:06:49Z'>
		
I just tried your command using the master version of fairseq and it worked. Can you please try again?

Is there any impact on using Cuda virtual environment?  I just updated my code environment，the version of faieseq and pytorch is 0.9 and 1.2. But I got the same problem.
		</comment>
		<comment id='9' author='xiaoshengjun' date='2019-12-05T13:25:48Z'>
		Not sure, sorry. Does other PyTorch code work for you?
The fact that you're getting a SIGSEV suggests it's may be a problem with your environment.
In general, when you don't get a nice stack trace, it's useful to test on one GPU, use CUDA_LAUNCH_BLOCKING, and set dataloading to be in the main thread (using the --workers=0 flag).
So please try CUDA_VISIBLE_DEVICES=0 CUDA_LAUNCH_BLOCKING=1 python train.py --workers=0 (...)
		</comment>
		<comment id='10' author='xiaoshengjun' date='2019-12-06T02:27:10Z'>
		
Not sure, sorry. Does other PyTorch code work for you?
The fact that you're getting a SIGSEV suggests it's may be a problem with your environment.
In general, when you don't get a nice stack trace, it's useful to test on one GPU, use CUDA_LAUNCH_BLOCKING, and set dataloading to be in the main thread (using the --workers=0 flag).
So please try CUDA_VISIBLE_DEVICES=0 CUDA_LAUNCH_BLOCKING=1 python train.py --workers=0 (...)

Hi, sorry to bother you again. I changed the commands:
--arch levenshtein_transformer
--task translation_lev
to:
--arch transformer
--task translation
And delete the commands:
--apply-bert-init
--criterion nat_loss
--noise random_delete
and the model is trained normally. I tried `CUDA_VISIBLE_DEVICES=0 CUDA_LAUNCH_BLOCKING=1 python train.py', It also doesn't work and get the same problem.
		</comment>
		<comment id='11' author='xiaoshengjun' date='2019-12-06T07:35:13Z'>
		
Not sure, sorry. Does other PyTorch code work for you?
The fact that you're getting a SIGSEV suggests it's may be a problem with your environment.
In general, when you don't get a nice stack trace, it's useful to test on one GPU, use CUDA_LAUNCH_BLOCKING, and set dataloading to be in the main thread (using the --workers=0 flag).
So please try CUDA_VISIBLE_DEVICES=0 CUDA_LAUNCH_BLOCKING=1 python train.py --workers=0 (...)

Hi, sorry to bother you and thank you very much for your reply, I tried the example in &lt;denchmark-link:https://github.com/pytorch/fairseq/blob/master/examples/nonautoregressive_translation/scripts.md&gt;https://github.com/pytorch/fairseq/blob/master/examples/nonautoregressive_translation/scripts.md&lt;/denchmark-link&gt;
, and the 'NAT', 'NAT-CRF', 'iNAT', 'CMLM' is worked normally, but for 'Insertion Transformer' and 'Levenshtein Transformer' is not work.
So I think my environment is normal. I would like to ask in what environment do you run 'Levenshtein Transformer'. Does 'Levenshtein Transformer' have other requirements for data than ‘--joined-dict’?
Thanks for your replay.
		</comment>
		<comment id='12' author='xiaoshengjun' date='2019-12-16T20:09:55Z'>
		&lt;denchmark-link:https://github.com/MultiPath&gt;@MultiPath&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/kahne&gt;@kahne&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='xiaoshengjun' date='2019-12-16T20:33:33Z'>
		I don't think we have special requirements for the data. I still don't understand what does not work for Insertion Transformer and Levenshtein Transformer
		</comment>
		<comment id='14' author='xiaoshengjun' date='2019-12-30T15:36:52Z'>
		Please refer this &lt;denchmark-link:https://github.com/pytorch/fairseq/issues/1346#issuecomment-567516336&gt;#1346 (comment)&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>