<bug id='1617' author='mortonjt' open_date='2020-01-13T19:02:55Z' closed_time='2020-02-21T15:13:09Z'>
	<summary>Tokens still have spaces</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Long story short, I've trained roberta using a custom dictionary and now I am trying to extract features (code snippet below for reference).
roberta = RobertaModel.from_pretrained(
    path1, 'checkpoint_best.pt',
    path2,
    gpt2_encoder_json=custom_json,
    gpt2_vocab_bpe=custom_vocab)

tokens = roberta.encode('A B C D E G'))
When I try to run this, I get the following error below
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "attention_layers.py", line 80, in &lt;module&gt;
    tokens = roberta.encode(' '.join(list(s)))
  File "/home/jmorton/software/fairseq/fairseq/models/roberta/hub_interface.py", line 57, in encode
    bpe_sentence = '&lt;s&gt; ' + self.bpe.encode(sentence) + ' &lt;/s&gt;'
  File "/home/jmorton/software/fairseq/fairseq/data/encoders/gpt2_bpe.py", line 40, in encode
    return ' '.join(map(str, self.bpe.encode(x)))
  File "/home/jmorton/software/fairseq/fairseq/data/encoders/gpt2_bpe_utils.py", line 110, in encode
    bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))
  File "/home/jmorton/software/fairseq/fairseq/data/encoders/gpt2_bpe_utils.py", line 110, in &lt;genexpr&gt;
    bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))
KeyError: 'ƒ†'
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


fairseq Version: 0.9.0
PyTorch Version: 1.2.0
OS: Redhat
How you installed fairseq: pip
Python version: 3.6.2
CUDA/cuDNN version: just cpu
GPU models and configuration: just cpu
Any other relevant information:

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

Turns out that there is still spacing in the tokens when parsing for the particular example. The fix is presented here: &lt;denchmark-link:https://github.com/mortonjt/fairseq/pull/1/files&gt;https://github.com/mortonjt/fairseq/pull/1/files&lt;/denchmark-link&gt;

I'm raising this issue mainly to bring awareness around challenges of trying to plug in custom dictionaries.  Can push in PR if there is interest.
	</description>
	<comments>
		<comment id='1' author='mortonjt' date='2020-01-13T19:32:44Z'>
		Note that the GPT-2 BPE is intended to be fully reversible, so spaces are a part of the vocab. In particular the GPT-2 BPE uses leading spaces: 


fairseq/fairseq/models/roberta/hub_interface.py


        Lines 47 to 55
      in
      b31849a






         The BPE encoding follows GPT-2. One subtle detail is that the GPT-2 BPE 



         requires leading spaces. For example:: 



  



             &gt;&gt;&gt; roberta.encode('Hello world').tolist() 



             [0, 31414, 232, 2] 



             &gt;&gt;&gt; roberta.encode(' world').tolist() 



             [0, 232, 2] 



             &gt;&gt;&gt; roberta.encode('world').tolist() 



             [0, 8331, 2] 





		</comment>
		<comment id='2' author='mortonjt' date='2020-02-21T15:17:07Z'>
		Following up on this, by default RobertaModel.from_pretrained will use the GPT-2 BPE.
However you can override this with any of the supported BPE modules: &lt;denchmark-link:https://github.com/pytorch/fairseq/tree/master/fairseq/data/encoders&gt;https://github.com/pytorch/fairseq/tree/master/fairseq/data/encoders&lt;/denchmark-link&gt;

For example, XLM-R uses bpe='sentencepiece': 


fairseq/fairseq/models/roberta/model_xlmr.py


         Line 26
      in
      08dcd08






 def from_pretrained(cls, model_name_or_path, checkpoint_file='model.pt', data_name_or_path='.', bpe='sentencepiece', **kwargs): 





		</comment>
	</comments>
</bug>