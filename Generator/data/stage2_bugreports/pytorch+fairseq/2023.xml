<bug id='2023' author='zixiliuUSC' open_date='2020-04-18T03:22:29Z' closed_time='2020-04-20T13:51:53Z'>
	<summary>get different size of dictionary in preprocessing and subword-nmt get-vocab</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Run the following command and get results as showing blow the code

&lt;denchmark-code&gt;&gt; cd examples/translation/
&gt; bash prepare-iwslt14.sh
&gt; cd ../..
&gt; TEXT=examples/translation/iwslt14.tokenized.de-en
&gt; fairseq-preprocess --source-lang de --target-lang en \
    --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \
    --destdir data-bin/iwslt14.tokenized.de-en
&lt;/denchmark-code&gt;

result:
&lt;denchmark-code&gt;2020-04-18 03:03:25 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bpe=None, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='./temp/iwslt14.tokenized.de-en', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer='nag', padding_factor=8, seed=1, source_lang='de', srcdict=None, target_lang='en', task='translation', tensorboard_logdir='', testpref='examples/translation/iwslt14.tokenized.de-en/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, trainpref='examples/translation/iwslt14.tokenized.de-en/train', user_dir=None, validpref='examples/translation/iwslt14.tokenized.de-en/valid', workers=1)
2020-04-18 03:03:48 | INFO | fairseq_cli.preprocess | [de] Dictionary: 8847 types
2020-04-18 03:04:20 | INFO | fairseq_cli.preprocess | [de] examples/translation/iwslt14.tokenized.de-en/train.de: 160239 sents, 4035591 tokens, 0.0% replaced by &lt;unk&gt;
2020-04-18 03:04:20 | INFO | fairseq_cli.preprocess | [de] Dictionary: 8847 types
2020-04-18 03:04:21 | INFO | fairseq_cli.preprocess | [de] examples/translation/iwslt14.tokenized.de-en/valid.de: 7283 sents, 182592 tokens, 0.0192% replaced by &lt;unk&gt;
2020-04-18 03:04:21 | INFO | fairseq_cli.preprocess | [de] Dictionary: 8847 types
2020-04-18 03:04:22 | INFO | fairseq_cli.preprocess | [de] examples/translation/iwslt14.tokenized.de-en/test.de: 6750 sents, 161838 tokens, 0.0636% replaced by &lt;unk&gt;
2020-04-18 03:04:22 | INFO | fairseq_cli.preprocess | [en] Dictionary: 6631 types
2020-04-18 03:04:53 | INFO | fairseq_cli.preprocess | [en] examples/translation/iwslt14.tokenized.de-en/train.en: 160239 sents, 3949114 tokens, 0.0% replaced by &lt;unk&gt;
2020-04-18 03:04:53 | INFO | fairseq_cli.preprocess | [en] Dictionary: 6631 types
2020-04-18 03:04:54 | INFO | fairseq_cli.preprocess | [en] examples/translation/iwslt14.tokenized.de-en/valid.en: 7283 sents, 178622 tokens, 0.00448% replaced by &lt;unk&gt;
2020-04-18 03:04:54 | INFO | fairseq_cli.preprocess | [en] Dictionary: 6631 types
2020-04-18 03:04:55 | INFO | fairseq_cli.preprocess | [en] examples/translation/iwslt14.tokenized.de-en/test.en: 6750 sents, 156928 tokens, 0.00892% replaced by &lt;unk&gt;
2020-04-18 03:04:55 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to ./temp/iwslt14.tokenized.de-en
&lt;/denchmark-code&gt;


get dictionary using subword-nmt

&lt;denchmark-code&gt;cd examples/translation/iwslt14.tokenized.de-en
subword-nmt get-vocab &lt; train.en &gt; train.en.vocab
subword-nmt get-vocab &lt; train.de &gt; train.de.vocab
&lt;/denchmark-code&gt;

manually check the size of english vocab, which is 6628 and for german, it is 8842.
So, there is a small difference in the dictionary size, and I want to know why the difference happen.
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

I think maybe fairseq adds several tokens which are necessary for model training, but why german vocab is adding 5 tokens but english vocab adding 4 tokens?
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


fairseq Version: 0.9.0
PyTorch Version: 1.4.0
OS (e.g., Linux): colab
How you installed fairseq: I git clone fairseq and run the python file directly
Python version: 3.6.9
CUDA/cuDNN version: None

	</description>
	<comments>
		<comment id='1' author='zixiliuUSC' date='2020-04-20T13:18:00Z'>
		fairseq-proprocess has the following arg --padding-factor (Pad dictionary size to be multiple of N, Default: 8).
Made-up tokens are added to the dictionary to have a vocab size which is a multiple of 8 (look at the bottom of your dictionary).
		</comment>
		<comment id='2' author='zixiliuUSC' date='2020-04-20T13:51:52Z'>
		Yes, as &lt;denchmark-link:https://github.com/villmow&gt;@villmow&lt;/denchmark-link&gt;
 said, fairseq will add dummy tokens to pad the dictionary to even multiple of .  You can confirm by looking at the last few tokens in the dictionary (, , etc.).
		</comment>
	</comments>
</bug>