<bug id='2707' author='ykim362' open_date='2020-10-07T20:35:46Z' closed_time='2020-10-13T15:11:17Z'>
	<summary>translation_moe train fails with multi GPU</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

translation_moe training fails with an error below.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

I followed the steps in README.


bash prepare-wmt14en2de.sh


TEXT=examples/translation/wmt17_en_de
fairseq-preprocess 
--source-lang en --target-lang de 
--trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test 
--destdir data-bin/wmt17_en_de --thresholdtgt 0 --thresholdsrc 0 
--workers 20 --joined-dictionary


fairseq-train --ddp-backend='no_c10d' 
data-bin/wmt17_en_de 
--max-update 100000 
--task translation_moe --user-dir examples/translation_moe/src 
--method hMoElp --mean-pool-gating-network 
--num-experts 3 
--arch transformer_wmt_en_de --share-all-embeddings 
--optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 
--lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 4000 
--lr 0.0007 --min-lr 1e-09 
--dropout 0.1 --weight-decay 0.0 --criterion cross_entropy 
--max-tokens 3584


Error occurs in command number 3.
&lt;denchmark-code&gt;-- Process 7 terminated with the following error:
Traceback (most recent call last):
  File "/home/youki/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 20, in _wrap
    fn(i, *args)
  File "/home/youki/.local/lib/python3.7/site-packages/fairseq-0.9.0-py3.7-linux-x86_64.egg/fairseq/distributed_utils.py", line 228, in distributed_main
    main(args, **kwargs)
  File "/home/youki/.local/lib/python3.7/site-packages/fairseq-0.9.0-py3.7-linux-x86_64.egg/fairseq_cli/train.py", line 125, in main
    valid_losses, should_stop = train(args, trainer, task, epoch_itr)
  File "/home/youki/anaconda3/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "/home/youki/.local/lib/python3.7/site-packages/fairseq-0.9.0-py3.7-linux-x86_64.egg/fairseq_cli/train.py", line 208, in train
    log_output = trainer.train_step(samples)
  File "/home/youki/anaconda3/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "/home/youki/.local/lib/python3.7/site-packages/fairseq-0.9.0-py3.7-linux-x86_64.egg/fairseq/trainer.py", line 532, in train_step
    logging_outputs, sample_size, ooms, train_time, ignore=is_dummy_batch,
  File "/home/youki/.local/lib/python3.7/site-packages/fairseq-0.9.0-py3.7-linux-x86_64.egg/fairseq/trainer.py", line 874, in _aggregate_logging_outputs
    logging_outputs, *extra_stats_to_sum, ignore=ignore
  File "/home/youki/.local/lib/python3.7/site-packages/fairseq-0.9.0-py3.7-linux-x86_64.egg/fairseq/trainer.py", line 937, in _fast_stat_sync_sum
    group=self.data_parallel_process_group
  File "/home/youki/.local/lib/python3.7/site-packages/fairseq-0.9.0-py3.7-linux-x86_64.egg/fairseq/distributed_utils.py", line 383, in all_reduce_dict
    cpu_data = _all_reduce_dict(cpu_data)
  File "/home/youki/.local/lib/python3.7/site-packages/fairseq-0.9.0-py3.7-linux-x86_64.egg/fairseq/distributed_utils.py", line 379, in _all_reduce_dict
    buf = torch.stack(list(data.values())).to(device=device)
RuntimeError: stack expects each tensor to be equal size, but got [] at entry 0 and [3] at entry 7

&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

No change in the code. I just ran the command in README.
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Training starts.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


fairseq Version (e.g., 1.0 or master): current master
PyTorch Version (e.g., 1.0) Pytorch 1.6
OS (e.g., Linux): Ubuntu 16.04
How you installed fairseq (pip, source): source (python setup.py install --user)
Build command you used (if compiling from source):
Python version: 3.7
CUDA/cuDNN version: 10.2
GPU models and configuration: 8 Titan x
Any other relevant information:

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='ykim362' date='2020-10-07T22:59:25Z'>
		It works fine with a single GPU. The problem seems related to the reduction of posterior probability output for data parallel training.
		</comment>
	</comments>
</bug>