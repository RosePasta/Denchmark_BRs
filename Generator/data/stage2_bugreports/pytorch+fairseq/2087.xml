<bug id='2087' author='zixiliuUSC' open_date='2020-05-03T18:11:30Z' closed_time='2020-05-19T18:17:47Z'>
	<summary>Using match_source_len=True in model.sample and get assert step &amp;lt; max_len AssertionError</summary>
	<description>
&lt;denchmark-h:h2&gt;‚ùì Questions and Help&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Before asking:&lt;/denchmark-h&gt;


search the issues: #1655
search the docs: official command line tool

&lt;denchmark-h:h4&gt;What is your question?&lt;/denchmark-h&gt;

I build a custom generator py file myself and use it to generate the sentence, and I can generate successfully without match_source_len=True in sampling function. But since my task is GEC task, I want to use this flag so that the corrected sentence can match on the input.
&lt;denchmark-h:h4&gt;Code&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;generate_custom.py
import torch 
import sys
from fairseq.models.transformer import TransformerModel
#import argparse
#parser = argparse.ArgumentParser()


datapath = sys.argv[1]
modeldir = sys.argv[2]
modelname = sys.argv[3]
bpe_type = sys.argv[4]
bpe_codes = sys.argv[5]
source = sys.argv[6]
target = sys.argv[7]
tokenizer = sys.argv[8]

#parser.add_argument('--tokenizer',default='nltk')
#args = parser.parse_args(['--tokenizer',tokenizer])

model = TransformerModel.from_pretrained(
    modeldir,
    checkpoint_file=modelname,
    data_name_or_path=datapath,
    bpe=bpe_type,
    bpe_codes=bpe_codes,
    tokenizer=tokenizer,max_target_positions=1500
    )
model.cuda()
model.eval()
model.half()
count = 1
bsz = 500
with open(source,'r') as source, open(target, 'w') as fout:
    sline = source.readline().strip()
    slines = [sline]
    for sline in source:
        if count % bsz == 0:
            with torch.no_grad():
                hypotheses_batch = model.sample(slines, beam=4, lenpen=2.0, max_len_b=600, min_len=1, no_repeat_ngram_size=3,match_source_len=True)

            for hypothesis in hypotheses_batch:
                fout.write(hypothesis + '\n')
                fout.flush()
            slines = []

        slines.append(sline.strip())
        count += 1
    if slines != []:
        hypotheses_batch = model.sample(slines, beam=4, lenpen=2.0, max_len_b=600, min_len=1, no_repeat_ngram_size=3,match_source_len=True)
        for hypothesis in hypotheses_batch:
            fout.write(hypothesis + '\n')
            fout.flush()

generate_custom.sh
datapath=temp/bpe/bin
modeldir=checkpoints/transformer_fp16
modelname=checkpoint.best_loss_0.90.pt
bpe_type=subword_nmt
bpe_codes=temp/bpe/code
source=temp/test.src
target=temp/test_inference.txt
tokenizer=nltk

CUDA_VISIBLE_DEVICES=0 python generate_custom.py $datapath $modeldir $modelname $bpe_type $bpe_codes $source $target $tokenizer
&lt;/denchmark-code&gt;

the error trace back is
&lt;denchmark-code&gt;bash generate_custom.sh 
/opt/conda/conda-bld/pytorch_1587428398394/work/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.
Traceback (most recent call last):
  File "generate_custom.py", line 39, in &lt;module&gt;
    hypotheses_batch = model.sample(slines, beam=4, lenpen=2.0, max_len_b=600, min_len=1, no_repeat_ngram_size=3,match_source_len=True)
  File "/home/zixi/EE-599/new/fairseq/fairseq/hub_utils.py", line 135, in sample
    batched_hypos = self.generate(tokenized_sentences, beam, verbose, **kwargs)
  File "/home/zixi/EE-599/new/fairseq/fairseq/hub_utils.py", line 172, in generate
    generator, self.models, batch, **inference_step_args
  File "/home/zixi/EE-599/new/fairseq/fairseq/tasks/fairseq_task.py", line 354, in inference_step
    return generator.generate(models, sample, prefix_tokens=prefix_tokens)
  File "/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 15, in decorate_context
    return func(*args, **kwargs)
  File "/home/zixi/EE-599/new/fairseq/fairseq/sequence_generator.py", line 160, in generate
    return self._generate(sample, **kwargs)
  File "/home/zixi/EE-599/new/fairseq/fairseq/sequence_generator.py", line 357, in _generate
    assert step &lt; max_len
AssertionError
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;What have you tried?&lt;/denchmark-h&gt;

&lt;denchmark-h:h4&gt;What's your environment?&lt;/denchmark-h&gt;


fairseq Version (e.g., 1.0 or master):0.9
PyTorch Version (e.g., 1.0):1.5
OS (e.g., Linux):ubuntu 18.04LTS
How you installed fairseq (pip, source):source
Build command you used (if compiling from source):pip install --editable
Python version:3.7
CUDA/cuDNN version:10.1
GPU models and configuration:RTX 2070
Any other relevant information:

	</description>
	<comments>
		<comment id='1' author='zixiliuUSC' date='2020-05-19T18:17:47Z'>
		Fixed in &lt;denchmark-link:https://github.com/pytorch/fairseq/commit/9a718e29855713a51877237b2dcc25e39c234c82&gt;9a718e2&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>