<bug id='1949' author='darsh10' open_date='2020-03-31T15:25:30Z' closed_time='2020-04-02T11:28:42Z'>
	<summary>Bug while decoding a trained es-en translation model</summary>
	<description>
This is the error that I get on using the fairseq-generate command on a trained model
load_state_dict
self.class.name, "\n\t".join(error_msgs)))
RuntimeError: Error(s) in loading state_dict for FConvModel:
Unexpected key(s) in state_dict: "decoder.convolutions.0._linearized_weight", "decoder.convolutions.1._linearized_weight", "decoder.convolutions.2._linearized_weight".
Surprisingly, the translation and generate work for en-es pair and not reverse.
	</description>
	<comments>
		<comment id='1' author='darsh10' date='2020-03-31T15:26:40Z'>
		Can you please include your code?
		</comment>
		<comment id='2' author='darsh10' date='2020-03-31T15:29:06Z'>
		airseq-preprocess --source-lang es --target-lang en --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test --destdir data-bin/blah.es-en
fairseq-train data-bin/blah.es-en/ --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 --arch fconv_iwslt_de_en --save-dir checkpoints/fconv--max-source-positions 44 --max-target-positions 44 --skip-invalid-size-inputs-valid-test --max-sentences 24 --max-sentences-valid 24 --eval-bleu
fairseq-generate data-bin/blah.es-en     --path checkpoints/fconv/checkpoint_best.pt     --batch-size 128 --beam 5 --skip-invalid-size-inputs-valid-test
		</comment>
		<comment id='3' author='darsh10' date='2020-03-31T15:32:13Z'>
		Seems like a duplicate of &lt;denchmark-link:https://github.com/pytorch/fairseq/issues/1903&gt;#1903&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='darsh10' date='2020-03-31T15:33:26Z'>
		Well, that is unclear. Since the reverse direction. en-es works perfectly. (With the identical commands , with obvious changes)
		</comment>
		<comment id='5' author='darsh10' date='2020-04-02T03:43:31Z'>
		Seems this is caused by a mismatch between the state_dict(s) in the pre-trained FConv models and the initialized one during inference.
So in fairseq/fairseq/modules/linearized_convolution.py, _linearized_weight is initialized to None, which will not be included in the model's state_dict (see task.build_model(...) in checkpoint_utils.py). That probably causes the "lost key error" when loading from a saved checkpoint.
A simple fix should be setting "strict=False" when calling load_state_dict, but any better solutions?
		</comment>
		<comment id='6' author='darsh10' date='2020-04-02T11:28:38Z'>
		Thank you very much for this great fix &lt;denchmark-link:https://github.com/jiangfeng1124&gt;@jiangfeng1124&lt;/denchmark-link&gt;
 
		</comment>
		<comment id='7' author='darsh10' date='2020-05-04T16:23:03Z'>
		Fixed by &lt;denchmark-link:https://github.com/pytorch/fairseq/commit/b2ee110c853c5effdd8d21f50a8437485bafb285&gt;b2ee110&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>