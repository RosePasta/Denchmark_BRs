<bug id='1339' author='fuzihaofzh' open_date='2019-11-02T08:56:08Z' closed_time='2020-02-08T02:23:46Z'>
	<summary>attn_mask is calculated but not used in encoder</summary>
	<description>



fairseq/fairseq/modules/transformer_layer.py


         Line 87
      in
      828c1ca






 attn_mask = attn_mask.masked_fill(attn_mask.bool(), -1e8) 





attn_mask is calculated in encoder layer. But it hasn't been used. Why don't we send it directly into the self_attention layer? Is there any problem if we directly send it into the self_attention layer?
	</description>
	<comments>
		<comment id='1' author='fuzihaofzh' date='2019-11-21T16:08:26Z'>
		I would also like to know why it is done that way, it just seems like a bug
		</comment>
		<comment id='2' author='fuzihaofzh' date='2019-12-16T21:58:03Z'>
		It's not directly a bug for most use cases, but can be problematic if one tries to pass in this parameter. This is being fixed soon.
		</comment>
	</comments>
</bug>