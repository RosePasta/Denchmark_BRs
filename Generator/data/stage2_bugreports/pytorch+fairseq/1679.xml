<bug id='1679' author='smart-patrol' open_date='2020-02-06T12:33:42Z' closed_time='2020-02-11T14:59:44Z'>
	<summary>Multi GPU Training Crashes</summary>
	<description>
&lt;denchmark-h:h4&gt;What is your question?&lt;/denchmark-h&gt;

I am getting a really weird error when trying to run the IWSLT 17 example on ja and zh.
&lt;denchmark-h:h4&gt;Code&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;CUDA_VISIBLE_DEVICES=1,2,3 fairseq-train ./data-bin/ted.ja_zh.en.bpe16k/ \
    --max-epoch 50 \
    --ddp-backend=no_c10d \
    --num-workers 32 \
    --task multilingual_translation --lang-pairs ja-en,zh-en \
    --arch multilingual_transformer_iwslt_de_en \
    --share-decoders --share-decoder-input-output-embed \
    --optimizer adam --adam-betas '(0.9, 0.98)' \
    --lr 0.0005 --lr-scheduler inverse_sqrt --min-lr '1e-09' \
    --warmup-updates 4000 --warmup-init-lr '1e-07' \
    --label-smoothing 0.1 --criterion label_smoothed_cross_entropy \
    --dropout 0.3 --weight-decay 0.0001 \
    --save-dir checkpoints/multilingual_transformer \
    --max-tokens 4000 
&lt;/denchmark-code&gt;

Which loads up to the epoch 001 bar but the then hangs and runs the following error:
&lt;denchmark-code&gt;Traceback (most recent call last):                                                                                                                    
  File "/home/ubuntu/anaconda3/envs/multi/bin/fairseq-train", line 11, in &lt;module&gt;
    load_entry_point('fairseq', 'console_scripts', 'fairseq-train')()
  File "/home/ubuntu/fairseq/fairseq_cli/train.py", line 297, in cli_main
    torch.multiprocessing.spawn(
  File "/home/ubuntu/anaconda3/envs/multi/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 171, in spawn
    while not spawn_context.join():
  File "/home/ubuntu/anaconda3/envs/multi/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 118, in join
    raise Exception(msg)
Exception: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/ubuntu/anaconda3/envs/multi/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 19, in _wrap
    fn(i, *args)
  File "/home/ubuntu/fairseq/fairseq_cli/train.py", line 267, in distributed_main
    main(args, init_distributed=True)
  File "/home/ubuntu/fairseq/fairseq_cli/train.py", line 101, in main
    train(args, trainer, task, epoch_itr)
  File "/home/ubuntu/anaconda3/envs/multi/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/ubuntu/fairseq/fairseq_cli/train.py", line 168, in train
    log_output = trainer.train_step(samples)
  File "/home/ubuntu/anaconda3/envs/multi/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/ubuntu/fairseq/fairseq/trainer.py", line 347, in train_step
    logging_outputs, sample_size, ooms = self._aggregate_logging_outputs(
  File "/home/ubuntu/fairseq/fairseq/trainer.py", line 611, in _aggregate_logging_outputs
    return self._fast_stat_sync_sum(logging_outputs, *extra_stats_to_sum)
  File "/home/ubuntu/fairseq/fairseq/trainer.py", line 649, in _fast_stat_sync_sum
    stats = [0.] + list(extra_stats_to_sum) + [
  File "/home/ubuntu/fairseq/fairseq/trainer.py", line 650, in &lt;listcomp&gt;
    sum(log.get(k, 0) for log in logging_outputs)
TypeError: unsupported operand type(s) for +: 'int' and 'dict'

/home/ubuntu/anaconda3/envs/multi/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 96 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;What have you tried?&lt;/denchmark-h&gt;

I viewed previously closed issues and modified batch-size and update-freq to see if that might help. Number of workers now set to max.
This runs perfectly fine on a single GPU
&lt;denchmark-h:h4&gt;What's your environment?&lt;/denchmark-h&gt;


fairseq Version (e.g., 1.0 or master):  0.9.0
PyTorch Version (e.g., 1.0):1.4.0
OS (e.g., Linux): Ubuntu 16.04.6
How you installed fairseq (pip, source): source
Build command you used (if compiling from source): pip install e
Python version: 3.8.1
CUDA/cuDNN version: 10
GPU models and configuration: p2.8xlarge

	</description>
	<comments>
		<comment id='1' author='smart-patrol' date='2020-02-09T13:04:25Z'>
		&lt;denchmark-link:https://github.com/pipibjc&gt;@pipibjc&lt;/denchmark-link&gt;
 has a fix for this, which heâ€™ll merge soon. The 0.9.0 release should work fine, you can roll back to that for now.
		</comment>
		<comment id='2' author='smart-patrol' date='2020-02-10T16:14:16Z'>
		sorry for the delay. PR &lt;denchmark-link:https://github.com/pytorch/fairseq/pull/1691&gt;#1691&lt;/denchmark-link&gt;
 should fix it. I'll merge it today.
		</comment>
		<comment id='3' author='smart-patrol' date='2020-02-11T01:07:05Z'>
		I rolled back to the previous version and was able to run distributed.
		</comment>
		<comment id='4' author='smart-patrol' date='2020-02-11T14:59:44Z'>
		Sounds like this is resolved.
		</comment>
	</comments>
</bug>