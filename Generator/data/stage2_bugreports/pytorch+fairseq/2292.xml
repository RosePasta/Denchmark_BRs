<bug id='2292' author='hscspring' open_date='2020-07-01T15:44:31Z' closed_time='2020-07-04T06:11:05Z'>
	<summary>TransformerModel with share-all-embeddings flag cannot `from_tretrained` after training</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior (always include the command you ran):

Run cmd '....'


Train a transformer model with share-all-embeddings flag
run model = TransformerModel.from_pretrained(...)


See error

RuntimeError: Error(s) in loading state_dict for TransformerModel:
	Unexpected key(s) in state_dict: "decoder.output_projection.weight". 
The reason might be features_only=False, so the output_layer is used.
# fairseq/models/transformer.py   line 608-633
        self.adaptive_softmax = None
        self.output_projection = None
        if args.adaptive_softmax_cutoff is not None:
            self.adaptive_softmax = AdaptiveSoftmax(
                len(dictionary),
                self.output_embed_dim,
                options.eval_str_list(args.adaptive_softmax_cutoff, type=int),
                dropout=args.adaptive_softmax_dropout,
                adaptive_inputs=embed_tokens if args.tie_adaptive_weights else None,
                factor=args.adaptive_softmax_factor,
                tie_proj=args.tie_adaptive_proj,
            )
        elif self.share_input_output_embed:
            # Here is the output_projection
            # self.share_input_output_embed = args.share_decoder_input_output_embed
            # if args.share_all_embeddings: args.share_decoder_input_output_embed = True
           
            self.output_projection = nn.Linear(
                self.embed_tokens.weight.shape[1],
                self.embed_tokens.weight.shape[0],
                bias=False,
            )
            self.output_projection.weight = self.embed_tokens.weight
        else:
            self.output_projection = nn.Linear(
                self.output_embed_dim, len(dictionary), bias=False
            )
            nn.init.normal_(
                self.output_projection.weight, mean=0, std=self.output_embed_dim ** -0.5
            )
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


fairseq Version (e.g., 1.0 or master): master
PyTorch Version (e.g., 1.0): 1.5.0
OS (e.g., Linux): MacOS Mojave version 10.14.6
How you installed fairseq (pip, source): source
Build command you used (if compiling from source):
Python version: python3.7.7
CUDA/cuDNN version: 10.2
GPU models and configuration: see training sh file
Any other relevant information:

# training sh file
CUDA_VISIBLE_DEVICES=0 fairseq-train \
    ${DATA_BIN_DIR} \
    --save-dir ${OUT_DIR} \
    --task translation \
    --arch transformer \
    --share-all-embeddings \
    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \
    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \
    --dropout 0.3 --weight-decay 0.0001 \
    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \
    --max-tokens 4096 \
    --eval-bleu \
    --eval-bleu-args '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}' \
    --eval-bleu-print-samples \
    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric


&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='hscspring' date='2020-07-02T13:33:13Z'>
		I'm unable to reproduce this.  I ran the following without any errors:
mkdir checkpoints

# preprocess data following https://github.com/pytorch/fairseq/tree/master/examples/scaling_nmt#training-a-new-model-on-wmt16-en-de
cp data-bin/wmt16_en_de_bpe32k/dict* checkpoints/

fairseq-train \
    data-bin/wmt16_en_de_bpe32k \
    --save-dir checkpoints \
    --task translation \
    --arch transformer \
    --share-all-embeddings \
    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \
    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \
    --dropout 0.3 --weight-decay 0.0001 \
    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \
    --max-tokens 4096 \
    --eval-bleu \
    --eval-bleu-args '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}' \
    --eval-bleu-print-samples \
    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric
In a python REPL:
from fairseq.models.transformer import TransformerModel
mdl = TransformerModel.from_pretrained('checkpoints', checkpoint_file='checkpoint_best.pt')
		</comment>
		<comment id='2' author='hscspring' date='2020-07-02T15:13:46Z'>
		&lt;denchmark-link:https://github.com/lematt1991&gt;@lematt1991&lt;/denchmark-link&gt;
 I'm very sorry that I've made a mistake in the issue. Now it has been updated (actually I installed from the source).
Addition, the model works well if I manually removed the output_projection.
		</comment>
	</comments>
</bug>