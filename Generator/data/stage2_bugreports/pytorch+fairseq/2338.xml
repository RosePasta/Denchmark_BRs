<bug id='2338' author='kkissmart' open_date='2020-07-17T07:16:13Z' closed_time='2020-07-21T16:09:24Z'>
	<summary>Benchmarking Transformer on V3-8 and GPU</summary>
	<description>
Hi Myle,
My command to run on TPU:
python train.py /home/xwu/pytorch-tutorial-data/wmt18_en_de_bpej32k  --arch=transformer_vaswani_wmt_en_de_big  -s en -t de --criterion cross_entropy --encoder-normalize-before --decoder-normalize-before --task translation --optimizer adam --adam-eps 1e-06 --adam-betas '(0.9, 0.98)' --lr-scheduler polynomial_decay --lr 1e-04 --min-lr -1 --warmup-updates 10000 --total-num-update 500000 --dropout 0.0 --attention-dropout 0.0 --weight-decay 0.0 --max-tokens 2052 --seed 2 --log-format simple --log-interval 100  --max-source-positions 1026 --max-target-positions 1026 --save-interval-updates 5000 --skip-invalid-size-inputs-valid-test  --num-batch-buckets 8 --tpu
My command to run on GPU:
ython train.py /data/pytorch-tutorial-data/wmt18_en_de_bpej32k  --arch=transformer_vaswani_wmt_en_de_big  -s en -t de --criterion cross_entropy --encoder-normalize-before --decoder-normalize-before --task translation --optimizer adam --adam-eps 1e-06 --adam-betas '(0.9, 0.98)' --lr-scheduler polynomial_decay --lr 1e-04 --min-lr -1 --warmup-updates 10000 --total-num-update 500000 --dropout 0.0 --attention-dropout 0.0 --weight-decay 0.0 --max-tokens 2048 --seed 2 --log-format simple --log-interval 100  --max-source-positions 1026 --max-target-positions 1026 --save-interval-updates 5000 --skip-invalid-size-inputs-valid-test --fp16
TPU logs:
2020-07-17 06:57:45 | INFO | train_inner | epoch 001:    100 / 160368 loss=14.065, ppl=17143.1, wps=0, ups=0, wpb=1669, bsz=40, num_updates=100, lr=1e-06, gnorm=4.168, train_wall=321, wall=439
2020-07-17 06:57:45 | INFO | root | NOTE: XLA compilation detected; too many of these can lead to slow training, but we expect a few in the beginning
2020-07-17 06:58:14 | INFO | root | NOTE: XLA compilation detected; too many of these can lead to slow training, but we expect a few in the beginning
2020-07-17 06:58:33 | INFO | train_inner | epoch 001:    200 / 160368 loss=13.504, ppl=11620.5, wps=9.6, ups=0.02, wpb=460, bsz=8, num_updates=200, lr=2e-06, gnorm=4.103, train_wall=34, wall=487
2020-07-17 06:58:33 | INFO | root | NOTE: XLA compilation detected; too many of these can lead to slow training, but we expect a few in the beginning
2020-07-17 06:59:03 | INFO | train_inner | epoch 001:    300 / 160368 loss=12.501, ppl=5798.48, wps=52.5, ups=0.03, wpb=1585, bsz=56, num_updates=300, lr=3e-06, gnorm=2.206, train_wall=16, wall=517
2020-07-17 06:59:33 | INFO | train_inner | epoch 001:    400 / 160368 loss=12.314, ppl=5092.42, wps=15.3, ups=0.03, wpb=463, bsz=8, num_updates=400, lr=4e-06, gnorm=2.927, train_wall=16, wall=547
2020-07-17 07:00:04 | INFO | train_inner | epoch 001:    500 / 160368 loss=12.06, ppl=4268.62, wps=55.5, ups=0.03, wpb=1686, bsz=40, num_updates=500, lr=5e-06, gnorm=1.576, train_wall=16, wall=578
2020-07-17 07:00:33 | INFO | train_inner | epoch 001:    600 / 160368 loss=11.612, ppl=3129.2, wps=57.5, ups=0.03, wpb=1668, bsz=48, num_updates=600, lr=6e-06, gnorm=1.75, train_wall=15, wall=607
2020-07-17 07:01:02 | INFO | train_inner | epoch 001:    700 / 160368 loss=11.286, ppl=2497.29, wps=62.6, ups=0.03, wpb=1822, bsz=64, num_updates=700, lr=7e-06, gnorm=1.592, train_wall=15, wall=636
2020-07-17 07:01:30 | INFO | train_inner | epoch 001:    800 / 160368 loss=10.75, ppl=1721.78, wps=15, ups=0.03, wpb=428, bsz=8, num_updates=800, lr=8e-06, gnorm=2.819, train_wall=14, wall=664
2020-07-17 07:01:59 | INFO | train_inner | epoch 001:    900 / 160368 loss=10.877, ppl=1880.37, wps=57.5, ups=0.03, wpb=1652, bsz=48, num_updates=900, lr=9e-06, gnorm=1.48, train_wall=14, wall=693
2020-07-17 07:02:28 | INFO | train_inner | epoch 001:   1000 / 160368 loss=10.626, ppl=1580.54, wps=17, ups=0.03, wpb=502, bsz=8, num_updates=1000, lr=1e-05, gnorm=2.802, train_wall=15, wall=723
2020-07-17 07:02:58 | INFO | train_inner | epoch 001:   1100 / 160368 loss=11.066, ppl=2144.5, wps=19, ups=0.03, wpb=550, bsz=8, num_updates=1100, lr=1.1e-05, gnorm=3.895, train_wall=14, wall=752
2020-07-17 07:03:27 | INFO | train_inner | epoch 001:   1200 / 160368 loss=10.876, ppl=1878.94, wps=16.4, ups=0.03, wpb=480, bsz=8, num_updates=1200, lr=1.2e-05, gnorm=3.95, train_wall=15, wall=781
2020-07-17 07:03:56 | INFO | train_inner | epoch 001:   1300 / 160368 loss=9.903, ppl=957.11, wps=16.6, ups=0.03, wpb=478, bsz=8, num_updates=1300, lr=1.3e-05, gnorm=3.182, train_wall=14, wall=810
2020-07-17 07:04:24 | INFO | train_inner | epoch 001:   1400 / 160368 loss=10.77, ppl=1745.88, wps=16.7, ups=0.03, wpb=482, bsz=8, num_updates=1400, lr=1.4e-05, gnorm=3.08, train_wall=14, wall=838
2020-07-17 07:04:53 | INFO | train_inner | epoch 001:   1500 / 160368 loss=10.418, ppl=1368.59, wps=15.4, ups=0.04, wpb=434, bsz=8, num_updates=1500, lr=1.5e-05, gnorm=3.361, train_wall=13, wall=867
2020-07-17 07:05:22 | INFO | train_inner | epoch 001:   1600 / 160368 loss=9.917, ppl=966.55, wps=15.5, ups=0.03, wpb=455, bsz=8, num_updates=1600, lr=1.6e-05, gnorm=3.666, train_wall=15, wall=896
2020-07-17 07:05:51 | INFO | train_inner | epoch 001:   1700 / 160368 loss=10.035, ppl=1049.38, wps=59.2, ups=0.03, wpb=1744, bsz=72, num_updates=1700, lr=1.7e-05, gnorm=2.067, train_wall=15, wall=925
2020-07-17 07:06:19 | INFO | train_inner | epoch 001:   1800 / 160368 loss=9.286, ppl=624.27, wps=16.6, ups=0.04, wpb=465, bsz=8, num_updates=1800, lr=1.8e-05, gnorm=4.038, train_wall=13, wall=953
2020-07-17 07:06:48 | INFO | train_inner | epoch 001:   1900 / 160368 loss=9.64, ppl=797.76, wps=62.2, ups=0.04, wpb=1757, bsz=88, num_updates=1900, lr=1.9e-05, gnorm=2.211, train_wall=13, wall=982
2020-07-17 07:07:16 | INFO | train_inner | epoch 001:   2000 / 160368 loss=10.043, ppl=1055.05, wps=18.3, ups=0.03, wpb=525, bsz=8, num_updates=2000, lr=2e-05, gnorm=3.888, train_wall=14, wall=1010
2020-07-17 07:07:45 | INFO | train_inner | epoch 001:   2100 / 160368 loss=9.814, ppl=899.94, wps=61.5, ups=0.04, wpb=1748, bsz=72, num_updates=2100, lr=2.1e-05, gnorm=3.225, train_wall=13, wall=1039
2020-07-17 07:08:13 | INFO | train_inner | epoch 001:   2200 / 160368 loss=9.796, ppl=888.87, wps=61, ups=0.04, wpb=1721, bsz=40, num_updates=2200, lr=2.2e-05, gnorm=2.058, train_wall=13, wall=1067
2020-07-17 07:08:41 | INFO | train_inner | epoch 001:   2300 / 160368 loss=9.557, ppl=753.38, wps=60.9, ups=0.04, wpb=1723, bsz=40, num_updates=2300, lr=2.3e-05, gnorm=1.864, train_wall=13, wall=1095
2020-07-17 07:09:10 | INFO | train_inner | epoch 001:   2400 / 160368 loss=9.778, ppl=877.69, wps=58.2, ups=0.03, wpb=1683, bsz=48, num_updates=2400, lr=2.4e-05, gnorm=2.108, train_wall=14, wall=1124
2020-07-17 07:09:38 | INFO | train_inner | epoch 001:   2500 / 160368 loss=9.563, ppl=756.6, wps=60.1, ups=0.04, wpb=1690, bsz=40, num_updates=2500, lr=2.5e-05, gnorm=2.048, train_wall=13, wall=1152
2020-07-17 07:10:06 | INFO | train_inner | epoch 001:   2600 / 160368 loss=10.827, ppl=1816.58, wps=21.5, ups=0.04, wpb=600, bsz=8, num_updates=2600, lr=2.6e-05, gnorm=2.829, train_wall=13, wall=1180
2020-07-17 07:10:34 | INFO | train_inner | epoch 001:   2700 / 160368 loss=9.462, ppl=705.37, wps=61.3, ups=0.04, wpb=1713, bsz=40, num_updates=2700, lr=2.7e-05, gnorm=1.84, train_wall=13, wall=1208
2020-07-17 07:11:02 | INFO | train_inner | epoch 001:   2800 / 160368 loss=9.255, ppl=611.12, wps=57.3, ups=0.04, wpb=1613, bsz=80, num_updates=2800, lr=2.8e-05, gnorm=3.23, train_wall=13, wall=1236
2020-07-17 07:11:31 | INFO | train_inner | epoch 001:   2900 / 160368 loss=8.979, ppl=504.73, wps=53.5, ups=0.04, wpb=1504, bsz=64, num_updates=2900, lr=2.9e-05, gnorm=2.39, train_wall=13, wall=1265
2020-07-17 07:12:00 | INFO | train_inner | epoch 001:   3000 / 160368 loss=9.376, ppl=664.29, wps=16.2, ups=0.03, wpb=469, bsz=8, num_updates=3000, lr=3e-05, gnorm=3.214, train_wall=14, wall=1294
2020-07-17 07:12:27 | INFO | train_inner | epoch 001:   3100 / 160368 loss=9.251, ppl=609.46, wps=60.4, ups=0.04, wpb=1661, bsz=48, num_updates=3100, lr=3.1e-05, gnorm=1.859, train_wall=12, wall=1321
2020-07-17 07:12:55 | INFO | train_inner | epoch 001:   3200 / 160368 loss=10.278, ppl=1241.54, wps=16.9, ups=0.04, wpb=469, bsz=8, num_updates=3200, lr=3.2e-05, gnorm=3.02, train_wall=13, wall=1349
GPU logs:
2020-07-17 06:48:46 | INFO | train_inner | epoch 001:    100 / 167138 loss=15.073, ppl=34470.4, wps=4004.3, ups=4.08, wpb=980.3, bsz=35.4, num_updates=100, lr=1e-06, gnorm=7.216, loss_scale=128, train_wall=25, wall=83
2020-07-17 06:49:10 | INFO | train_inner | epoch 001:    200 / 167138 loss=13.403, ppl=10832.5, wps=3854.3, ups=4.09, wpb=943.4, bsz=30, num_updates=200, lr=2e-06, gnorm=3.941, loss_scale=128, train_wall=24, wall=108
2020-07-17 06:49:35 | INFO | train_inner | epoch 001:    300 / 167138 loss=12.732, ppl=6803.28, wps=3758.6, ups=4.08, wpb=921.2, bsz=33.3, num_updates=300, lr=3e-06, gnorm=3.067, loss_scale=128, train_wall=24, wall=132
2020-07-17 06:49:59 | INFO | train_inner | epoch 001:    400 / 167138 loss=12.379, ppl=5326.36, wps=3601.1, ups=4.08, wpb=883.3, bsz=28.8, num_updates=400, lr=4e-06, gnorm=2.736, loss_scale=128, train_wall=24, wall=157
2020-07-17 06:50:24 | INFO | train_inner | epoch 001:    500 / 167138 loss=12.012, ppl=4130.8, wps=3976.9, ups=4.07, wpb=976.2, bsz=35.4, num_updates=500, lr=5e-06, gnorm=2.507, loss_scale=128, train_wall=24, wall=181
2020-07-17 06:50:48 | INFO | train_inner | epoch 001:    600 / 167138 loss=11.653, ppl=3220.91, wps=3852.7, ups=4.07, wpb=946.4, bsz=32.1, num_updates=600, lr=6e-06, gnorm=2.44, loss_scale=128, train_wall=24, wall=206
2020-07-17 06:51:13 | INFO | train_inner | epoch 001:    700 / 167138 loss=11.342, ppl=2596.52, wps=3730.3, ups=4.06, wpb=918.1, bsz=29.9, num_updates=700, lr=7e-06, gnorm=2.396, loss_scale=128, train_wall=25, wall=230
2020-07-17 06:51:38 | INFO | train_inner | epoch 001:    800 / 167138 loss=11.027, ppl=2086.98, wps=3781.3, ups=4.07, wpb=929, bsz=31.7, num_updates=800, lr=8e-06, gnorm=2.348, loss_scale=128, train_wall=24, wall=255
2020-07-17 06:52:02 | INFO | train_inner | epoch 001:    900 / 167138 loss=10.9, ppl=1910.22, wps=3707.8, ups=4.07, wpb=911.3, bsz=30.9, num_updates=900, lr=9e-06, gnorm=2.447, loss_scale=128, train_wall=24, wall=279
2020-07-17 06:52:27 | INFO | train_inner | epoch 001:   1000 / 167138 loss=10.692, ppl=1654.68, wps=3481, ups=4.06, wpb=856.8, bsz=28, num_updates=1000, lr=1e-05, gnorm=2.537, loss_scale=128, train_wall=25, wall=304
2020-07-17 06:52:51 | INFO | train_inner | epoch 001:   1100 / 167138 loss=10.556, ppl=1505.76, wps=3732.9, ups=4.07, wpb=917.5, bsz=32.2, num_updates=1100, lr=1.1e-05, gnorm=2.572, loss_scale=128, train_wall=24, wall=329
2020-07-17 06:53:16 | INFO | train_inner | epoch 001:   1200 / 167138 loss=10.477, ppl=1425.54, wps=3522.9, ups=4.07, wpb=866.6, bsz=28.7, num_updates=1200, lr=1.2e-05, gnorm=2.642, loss_scale=128, train_wall=25, wall=353
2020-07-17 06:53:41 | INFO | train_inner | epoch 001:   1300 / 167138 loss=10.335, ppl=1291.53, wps=3713.6, ups=4.07, wpb=913.1, bsz=29.7, num_updates=1300, lr=1.3e-05, gnorm=2.615, loss_scale=128, train_wall=24, wall=378
2020-07-17 06:54:05 | INFO | train_inner | epoch 001:   1400 / 167138 loss=10.329, ppl=1286.27, wps=3648.2, ups=4.06, wpb=899.5, bsz=28.8, num_updates=1400, lr=1.4e-05, gnorm=2.669, loss_scale=128, train_wall=25, wall=402
2020-07-17 06:54:30 | INFO | train_inner | epoch 001:   1500 / 167138 loss=10.197, ppl=1174.06, wps=3499.8, ups=4.05, wpb=863.4, bsz=28, num_updates=1500, lr=1.5e-05, gnorm=2.778, loss_scale=128, train_wall=25, wall=427
2020-07-17 06:54:55 | INFO | train_inner | epoch 001:   1600 / 167138 loss=10.183, ppl=1162.2, wps=3441.2, ups=4.05, wpb=850.2, bsz=27, num_updates=1600, lr=1.6e-05, gnorm=2.936, loss_scale=128, train_wall=25, wall=452
2020-07-17 06:55:19 | INFO | train_inner | epoch 001:   1700 / 167138 loss=10.076, ppl=1079.72, wps=3432.7, ups=4.04, wpb=848.9, bsz=29, num_updates=1700, lr=1.7e-05, gnorm=2.85, loss_scale=128, train_wall=25, wall=477
2020-07-17 06:55:44 | INFO | train_inner | epoch 001:   1800 / 167138 loss=10.019, ppl=1037.53, wps=3773, ups=4.06, wpb=929, bsz=30.4, num_updates=1800, lr=1.8e-05, gnorm=2.732, loss_scale=128, train_wall=25, wall=501
2020-07-17 06:56:09 | INFO | train_inner | epoch 001:   1900 / 167138 loss=9.935, ppl=979.13, wps=3662.8, ups=4.05, wpb=904.6, bsz=29.6, num_updates=1900, lr=1.9e-05, gnorm=2.863, loss_scale=128, train_wall=25, wall=526
2020-07-17 06:56:33 | INFO | train_inner | epoch 001:   2000 / 167138 loss=9.865, ppl=932.27, wps=3459.6, ups=4.05, wpb=854.2, bsz=26.3, num_updates=2000, lr=2e-05, gnorm=2.875, loss_scale=128, train_wall=25, wall=551
2020-07-17 06:56:58 | INFO | train_inner | epoch 001:   2100 / 167138 loss=9.754, ppl=863.65, wps=3779.8, ups=4.07, wpb=929.5, bsz=32.5, num_updates=2100, lr=2.1e-05, gnorm=2.861, loss_scale=128, train_wall=24, wall=575
2020-07-17 06:57:23 | INFO | train_inner | epoch 001:   2200 / 167138 loss=9.741, ppl=855.5, wps=3619.4, ups=4.07, wpb=889, bsz=29.3, num_updates=2200, lr=2.2e-05, gnorm=2.932, loss_scale=128, train_wall=24, wall=600
2020-07-17 06:57:47 | INFO | train_inner | epoch 001:   2300 / 167138 loss=9.618, ppl=785.55, wps=3834.2, ups=4.08, wpb=940.3, bsz=33.9, num_updates=2300, lr=2.3e-05, gnorm=2.784, loss_scale=128, train_wall=24, wall=624
2020-07-17 06:58:12 | INFO | train_inner | epoch 001:   2400 / 167138 loss=9.509, ppl=728.74, wps=3961.5, ups=4.07, wpb=973.1, bsz=33.9, num_updates=2400, lr=2.4e-05, gnorm=2.783, loss_scale=128, train_wall=24, wall=649
2020-07-17 06:58:36 | INFO | train_inner | epoch 001:   2500 / 167138 loss=9.486, ppl=717.05, wps=3512.7, ups=4.05, wpb=866.6, bsz=26.6, num_updates=2500, lr=2.5e-05, gnorm=2.851, loss_scale=128, train_wall=25, wall=673
2020-07-17 06:59:01 | INFO | train_inner | epoch 001:   2600 / 167138 loss=9.371, ppl=661.99, wps=3738.4, ups=4.07, wpb=918.2, bsz=31.4, num_updates=2600, lr=2.6e-05, gnorm=2.809, loss_scale=128, train_wall=24, wall=698
2020-07-17 06:59:26 | INFO | train_inner | epoch 001:   2700 / 167138 loss=9.308, ppl=633.77, wps=3681.4, ups=4.06, wpb=907, bsz=28.7, num_updates=2700, lr=2.7e-05, gnorm=2.841, loss_scale=128, train_wall=25, wall=723
2020-07-17 06:59:50 | INFO | train_inner | epoch 001:   2800 / 167138 loss=9.185, ppl=582.03, wps=3664, ups=4.06, wpb=903.1, bsz=30.4, num_updates=2800, lr=2.8e-05, gnorm=2.726, loss_scale=128, train_wall=25, wall=747
2020-07-17 07:00:15 | INFO | train_inner | epoch 001:   2900 / 167138 loss=9.128, ppl=559.68, wps=3830.1, ups=4.07, wpb=940.1, bsz=32.2, num_updates=2900, lr=2.9e-05, gnorm=2.75, loss_scale=128, train_wall=24, wall=772
2020-07-17 07:00:39 | INFO | train_inner | epoch 001:   3000 / 167138 loss=9.132, ppl=561.23, wps=3668.7, ups=4.05, wpb=905.1, bsz=28.8, num_updates=3000, lr=3e-05, gnorm=2.85, loss_scale=128, train_wall=25, wall=797
2020-07-17 07:01:04 | INFO | train_inner | epoch 001:   3100 / 167138 loss=9.076, ppl=539.76, wps=3558.3, ups=4.06, wpb=877.1, bsz=27.8, num_updates=3100, lr=3.1e-05, gnorm=2.736, loss_scale=128, train_wall=25, wall=821
2020-07-17 07:01:29 | INFO | train_inner | epoch 001:   3200 / 167138 loss=8.952, ppl=495.35, wps=3694.6, ups=4.07, wpb=908.7, bsz=29.9, num_updates=3200, lr=3.2e-05, gnorm=2.724, loss_scale=128, train_wall=24, wall=846
2020-07-17 07:01:53 | INFO | train_inner | epoch 001:   3300 / 167138 loss=8.91, ppl=480.92, wps=3925.6, ups=4.07, wpb=965, bsz=32.3, num_updates=3300, lr=3.3e-05, gnorm=2.608, loss_scale=128, train_wall=24, wall=870
2020-07-17 07:02:18 | INFO | train_inner | epoch 001:   3400 / 167138 loss=8.839, ppl=457.86, wps=3751.3, ups=4.06, wpb=923.5, bsz=30.6, num_updates=3400, lr=3.4e-05, gnorm=2.602, loss_scale=128, train_wall=25, wall=895
2020-07-17 07:02:43 | INFO | train_inner | epoch 001:   3500 / 167138 loss=8.793, ppl=443.65, wps=3807.2, ups=4.05, wpb=939.4, bsz=31.6, num_updates=3500, lr=3.5e-05, gnorm=2.686, loss_scale=128, train_wall=25, wall=920
2020-07-17 07:03:07 | INFO | train_inner | epoch 001:   3600 / 167138 loss=8.645, ppl=400.34, wps=3720.7, ups=4.05, wpb=918.2, bsz=33.7, num_updates=3600, lr=3.6e-05, gnorm=2.661, loss_scale=128, train_wall=25, wall=944
2020-07-17 07:03:32 | INFO | train_inner | epoch 001:   3700 / 167138 loss=8.624, ppl=394.59, wps=3671.7, ups=4.06, wpb=903.7, bsz=30.7, num_updates=3700, lr=3.7e-05, gnorm=2.671, loss_scale=128, train_wall=25, wall=969
2020-07-17 07:03:56 | INFO | train_inner | epoch 001:   3800 / 167138 loss=8.612, ppl=391.22, wps=3601.4, ups=4.07, wpb=884.9, bsz=28.3, num_updates=3800, lr=3.8e-05, gnorm=2.699, loss_scale=128, train_wall=24, wall=994
2020-07-17 07:04:21 | INFO | train_inner | epoch 001:   3900 / 167138 loss=8.575, ppl=381.31, wps=3618.8, ups=4.06, wpb=891.2, bsz=28.5, num_updates=3900, lr=3.9e-05, gnorm=2.666, loss_scale=128, train_wall=25, wall=1018
2020-07-17 07:04:46 | INFO | train_inner | epoch 001:   4000 / 167138 loss=8.53, ppl=369.76, wps=3240.5, ups=4.04, wpb=801.4, bsz=24.1, num_updates=4000, lr=4e-05, gnorm=2.708, loss_scale=128, train_wall=25, wall=1043
2020-07-17 07:05:10 | INFO | train_inner | epoch 001:   4100 / 167138 loss=8.514, ppl=365.54, wps=3703, ups=4.07, wpb=910.9, bsz=30.3, num_updates=4100, lr=4.1e-05, gnorm=2.657, loss_scale=128, train_wall=24, wall=1068
2020-07-17 07:05:35 | INFO | train_inner | epoch 001:   4200 / 167138 loss=8.422, ppl=342.92, wps=3606.6, ups=4.06, wpb=887.8, bsz=29.6, num_updates=4200, lr=4.2e-05, gnorm=2.588, loss_scale=128, train_wall=25, wall=1092
2020-07-17 07:05:59 | INFO | train_inner | epoch 001:   4300 / 167138 loss=8.45, ppl=349.72, wps=3786.9, ups=4.08, wpb=928.8, bsz=28.4, num_updates=4300, lr=4.3e-05, gnorm=2.572, loss_scale=128, train_wall=24, wall=1117
2020-07-17 07:06:24 | INFO | train_inner | epoch 001:   4400 / 167138 loss=8.32, ppl=319.68, wps=3741.7, ups=4.07, wpb=919.6, bsz=30.7, num_updates=4400, lr=4.4e-05, gnorm=2.57, loss_scale=128, train_wall=24, wall=1141
2020-07-17 07:06:49 | INFO | train_inner | epoch 001:   4500 / 167138 loss=8.321, ppl=319.77, wps=3761.8, ups=4.07, wpb=923.7, bsz=29.7, num_updates=4500, lr=4.5e-05, gnorm=2.577, loss_scale=128, train_wall=24, wall=1166
2020-07-17 07:07:13 | INFO | train_inner | epoch 001:   4600 / 167138 loss=8.263, ppl=307.26, wps=3806.7, ups=4.06, wpb=937.6, bsz=31.2, num_updates=4600, lr=4.6e-05, gnorm=2.533, loss_scale=128, train_wall=25, wall=1190
2020-07-17 07:07:38 | INFO | train_inner | epoch 001:   4700 / 167138 loss=8.237, ppl=301.71, wps=3571.1, ups=4.06, wpb=880.2, bsz=27.3, num_updates=4700, lr=4.7e-05, gnorm=2.579, loss_scale=128, train_wall=25, wall=1215
2020-07-17 07:08:02 | INFO | train_inner | epoch 001:   4800 / 167138 loss=8.033, ppl=261.86, wps=3924.6, ups=4.08, wpb=962, bsz=36.1, num_updates=4800, lr=4.8e-05, gnorm=2.512, loss_scale=128, train_wall=24, wall=1240
they seem to behave very differently.
this aligns with the other arch
&lt;denchmark-link:https://github.com/pytorch/fairseq/issues/2332&gt;#2332&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/myleott&gt;@myleott&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/taylanbil&gt;@taylanbil&lt;/denchmark-link&gt;

let me know your thought. or any hint on how to debug TPU.
Thanks
	</description>
	<comments>
		<comment id='1' author='kkissmart' date='2020-07-17T17:43:14Z'>
		&lt;denchmark-link:https://github.com/myleott&gt;@myleott&lt;/denchmark-link&gt;
 , could this be related to the same issue as how wps is not aggregated across iterations? maybe loss not being aggregated has this side effect?
		</comment>
		<comment id='2' author='kkissmart' date='2020-07-17T19:47:54Z'>
		two more things I have tested:

the ppl is way larger on valid data set after training 15000 steps on TPU than GPU
I trained 2 epochs on GPU and load the saved model in TPU -- the ppl looks similar to that on GPU.
My guess -- something is incorrect in gradient aggregation?

		</comment>
		<comment id='3' author='kkissmart' date='2020-07-17T21:19:41Z'>
		I applied the following patch and took a peek @ the raw loss:
~/kkissmart-fairseq$ cat _report.patch
diff --git a/fairseq/criterions/cross_entropy.py b/fairseq/criterions/cross_entropy.py
index 4e750f6..a029d98 100644
--- a/fairseq/criterions/cross_entropy.py
+++ b/fairseq/criterions/cross_entropy.py
@@ -17,8 +17,9 @@ class CrossEntropyCriterion(FairseqCriterion):
     def __init__(self, task, sentence_avg):
         super().__init__(task)
         self.sentence_avg = sentence_avg
+        self._report = 0

-    def forward(self, model, sample, reduce=True):
+    def forward(self, model, sample, reduce=True, report=False):
         """Compute the loss for the given sample.

         Returns a tuple with three elements:
@@ -29,6 +30,10 @@ class CrossEntropyCriterion(FairseqCriterion):
         net_output = model(**sample['net_input'])
         loss, _ = self.compute_loss(model, net_output, sample, reduce=reduce)
         sample_size = sample['target'].size(0) if self.sentence_avg else sample['ntokens']
+        self._report += 1
+        report = not self._report % 100
+        if report:
+            print('RAWLOSS @', self._report, loss.data, flush=True)
         logging_output = {
             'loss': loss.data,
             'ntokens': sample['ntokens'],
Now on tpu, I do have 1 epoch consisting of 160368 steps as in &lt;denchmark-link:https://github.com/pytorch/fairseq/issues/2338#issue-658952262&gt;comment above&lt;/denchmark-link&gt;
, but running on 1 gpu it was consisting on 85127 steps using the command above, so this may not be apples-to-apples. But here are what the raw tensors look like:
&lt;denchmark-h:h2&gt;TPU:&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;RAWLOSS @ 100 tensor(16271.6709, device='xla:1')
RAWLOSS @ 200 tensor(4305.8403, device='xla:1')
RAWLOSS @ 300 tensor(13734.5820, device='xla:1')
RAWLOSS @ 400 tensor(3951.9402, device='xla:1')
RAWLOSS @ 500 tensor(14093.3574, device='xla:1')
RAWLOSS @ 600 tensor(13424.9531, device='xla:1')
RAWLOSS @ 700 tensor(14253.4355, device='xla:1')
RAWLOSS @ 800 tensor(3189.0784, device='xla:1')
RAWLOSS @ 900 tensor(12454.7998, device='xla:1')
RAWLOSS @ 1000 tensor(3697.4902, device='xla:1')
RAWLOSS @ 1100 tensor(4218.8638, device='xla:1')
RAWLOSS @ 1200 tensor(3618.4631, device='xla:1')
RAWLOSS @ 1300 tensor(3280.9521, device='xla:1')
RAWLOSS @ 1400 tensor(3598.1392, device='xla:1')
RAWLOSS @ 1500 tensor(3134.1470, device='xla:1')
RAWLOSS @ 1600 tensor(3127.5469, device='xla:1')
RAWLOSS @ 1700 tensor(12131.1826, device='xla:1')
RAWLOSS @ 1800 tensor(2993.0081, device='xla:1')
RAWLOSS @ 1900 tensor(11739.9414, device='xla:1')
RAWLOSS @ 2000 tensor(3654.7056, device='xla:1')
RAWLOSS @ 2100 tensor(11890.4590, device='xla:1')
RAWLOSS @ 2200 tensor(11685.5146, device='xla:1')
RAWLOSS @ 2300 tensor(11414.1367, device='xla:1')
RAWLOSS @ 2400 tensor(11406.1914, device='xla:1')
RAWLOSS @ 2500 tensor(11202.7383, device='xla:1')
RAWLOSS @ 2600 tensor(4502.8257, device='xla:1')
RAWLOSS @ 2700 tensor(11235.0830, device='xla:1')
RAWLOSS @ 2800 tensor(10347.8613, device='xla:1')
RAWLOSS @ 2900 tensor(9360.9414, device='xla:1')
RAWLOSS @ 3000 tensor(3047.8965, device='xla:1')
RAWLOSS @ 3100 tensor(10651.2812, device='xla:1')
RAWLOSS @ 3200 tensor(3341.2048, device='xla:1')
RAWLOSS @ 3300 tensor(3044.8315, device='xla:1')
RAWLOSS @ 3400 tensor(10202.1943, device='xla:1')
RAWLOSS @ 3500 tensor(3139.4858, device='xla:1')
RAWLOSS @ 3600 tensor(11102.5098, device='xla:1')
RAWLOSS @ 3700 tensor(2788.9561, device='xla:1')
RAWLOSS @ 3800 tensor(10285.9717, device='xla:1')
RAWLOSS @ 3900 tensor(2947.7607, device='xla:1')
RAWLOSS @ 4000 tensor(2861.3862, device='xla:1')
RAWLOSS @ 4100 tensor(2989.0706, device='xla:1')
RAWLOSS @ 4200 tensor(2949.4807, device='xla:1')
RAWLOSS @ 4300 tensor(9865.3926, device='xla:1')
RAWLOSS @ 4400 tensor(2625.6353, device='xla:1')
RAWLOSS @ 4500 tensor(2652.5664, device='xla:1')
RAWLOSS @ 4600 tensor(3053.9490, device='xla:1')
RAWLOSS @ 4700 tensor(9665.3604, device='xla:1')
RAWLOSS @ 4800 tensor(9742.5576, device='xla:1')
RAWLOSS @ 4900 tensor(10070.6123, device='xla:1')
RAWLOSS @ 5000 tensor(10332.0527, device='xla:1')
RAWLOSS @ 5100 tensor(8249.6953, device='xla:1')
RAWLOSS @ 5200 tensor(9693.1504, device='xla:1')
RAWLOSS @ 5300 tensor(10425.6953, device='xla:1')
RAWLOSS @ 5400 tensor(9584.0508, device='xla:1')
RAWLOSS @ 5500 tensor(7530.0693, device='xla:1')
RAWLOSS @ 5600 tensor(9581.3604, device='xla:1')
RAWLOSS @ 5700 tensor(1942.0975, device='xla:1')
RAWLOSS @ 5800 tensor(1978.2012, device='xla:1')
RAWLOSS @ 5900 tensor(3089.1594, device='xla:1')
RAWLOSS @ 6000 tensor(3161.7144, device='xla:1')
RAWLOSS @ 6100 tensor(3109.3250, device='xla:1')
RAWLOSS @ 6200 tensor(3101.6497, device='xla:1')
RAWLOSS @ 6300 tensor(2754.1189, device='xla:1')
RAWLOSS @ 6400 tensor(2459.5911, device='xla:1')
RAWLOSS @ 6500 tensor(3520.3318, device='xla:1')
RAWLOSS @ 6600 tensor(3049.6931, device='xla:1')
RAWLOSS @ 6700 tensor(2971.7424, device='xla:1')
RAWLOSS @ 6800 tensor(2363.5664, device='xla:1')
RAWLOSS @ 6900 tensor(2116.4128, device='xla:1')
RAWLOSS @ 7000 tensor(2472.9563, device='xla:1')
RAWLOSS @ 7100 tensor(7948.1338, device='xla:1')
RAWLOSS @ 7200 tensor(9299.0332, device='xla:1')
RAWLOSS @ 7300 tensor(2767.5945, device='xla:1')
RAWLOSS @ 7400 tensor(2785.6047, device='xla:1')
RAWLOSS @ 7500 tensor(2810.0420, device='xla:1')
RAWLOSS @ 7600 tensor(2942.9753, device='xla:1')
RAWLOSS @ 7700 tensor(2776.1958, device='xla:1')
RAWLOSS @ 7800 tensor(2171.0554, device='xla:1')
RAWLOSS @ 7900 tensor(2611.3005, device='xla:1')
RAWLOSS @ 8000 tensor(2587.1780, device='xla:1')
RAWLOSS @ 8100 tensor(2670.9426, device='xla:1')
RAWLOSS @ 8200 tensor(2645.4097, device='xla:1')
RAWLOSS @ 8300 tensor(2425.9375, device='xla:1')
RAWLOSS @ 8400 tensor(2767.1636, device='xla:1')
RAWLOSS @ 8500 tensor(2720.4702, device='xla:1')
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;GPU&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;RAWLOSS @ 100 tensor(18076.6445, device='cuda:0')
RAWLOSS @ 200 tensor(16155.3896, device='cuda:0')
RAWLOSS @ 300 tensor(12830.5098, device='cuda:0')
RAWLOSS @ 400 tensor(16126.9912, device='cuda:0')
RAWLOSS @ 500 tensor(15554.2080, device='cuda:0')
RAWLOSS @ 600 tensor(13906.5693, device='cuda:0')
RAWLOSS @ 700 tensor(14809.2363, device='cuda:0')
RAWLOSS @ 800 tensor(13544.0186, device='cuda:0')
RAWLOSS @ 900 tensor(13945.7871, device='cuda:0')
RAWLOSS @ 1000 tensor(12123.0469, device='cuda:0')
RAWLOSS @ 1100 tensor(12470.1611, device='cuda:0')
RAWLOSS @ 1200 tensor(11691.9697, device='cuda:0')
RAWLOSS @ 1300 tensor(12631.0664, device='cuda:0')
RAWLOSS @ 1400 tensor(13754.5391, device='cuda:0')
RAWLOSS @ 1500 tensor(12661.5537, device='cuda:0')
RAWLOSS @ 1600 tensor(13142.5898, device='cuda:0')
RAWLOSS @ 1700 tensor(12612.7432, device='cuda:0')
RAWLOSS @ 1800 tensor(13044.6055, device='cuda:0')
RAWLOSS @ 1900 tensor(10983.5615, device='cuda:0')
RAWLOSS @ 2000 tensor(13658.7295, device='cuda:0')
RAWLOSS @ 2100 tensor(12312.7080, device='cuda:0')
RAWLOSS @ 2200 tensor(13380.3184, device='cuda:0')
RAWLOSS @ 2300 tensor(12464.9170, device='cuda:0')
RAWLOSS @ 2400 tensor(11629.6230, device='cuda:0')
RAWLOSS @ 2500 tensor(11528.7705, device='cuda:0')
RAWLOSS @ 2600 tensor(8757.1787, device='cuda:0')
RAWLOSS @ 2700 tensor(11427.9746, device='cuda:0')
RAWLOSS @ 2800 tensor(11753.9551, device='cuda:0')
RAWLOSS @ 2900 tensor(11126.0508, device='cuda:0')
RAWLOSS @ 3000 tensor(10910.9473, device='cuda:0')
RAWLOSS @ 3100 tensor(9039.7832, device='cuda:0')
RAWLOSS @ 3200 tensor(10339.9102, device='cuda:0')
RAWLOSS @ 3300 tensor(11314.0576, device='cuda:0')
RAWLOSS @ 3400 tensor(11175.5537, device='cuda:0')
RAWLOSS @ 3500 tensor(11704.0283, device='cuda:0')
RAWLOSS @ 3600 tensor(9460.0703, device='cuda:0')
RAWLOSS @ 3700 tensor(10354.9727, device='cuda:0')
RAWLOSS @ 3800 tensor(7397.9468, device='cuda:0')
RAWLOSS @ 3900 tensor(11164.2383, device='cuda:0')
RAWLOSS @ 4000 tensor(10348.8701, device='cuda:0')
RAWLOSS @ 4100 tensor(11647.0459, device='cuda:0')
RAWLOSS @ 4200 tensor(10325.2422, device='cuda:0')
RAWLOSS @ 4300 tensor(9680.9678, device='cuda:0')
RAWLOSS @ 4400 tensor(8314.2598, device='cuda:0')
RAWLOSS @ 4500 tensor(10642.8398, device='cuda:0')
RAWLOSS @ 4600 tensor(10229.3760, device='cuda:0')
RAWLOSS @ 4700 tensor(9392.4668, device='cuda:0')
RAWLOSS @ 4800 tensor(9815.5117, device='cuda:0')
RAWLOSS @ 4900 tensor(7353.0601, device='cuda:0')
&lt;/denchmark-code&gt;

These are pretty different patterns, and probably due to the input preparation?
And btw here is the full loss schedule on tpus:
&lt;denchmark-code&gt;$ grep wps tpu-repro.txt | cut -d ':' -f4- | cut -d ',' -f1
    100 / 160368 loss=14.065
    200 / 160368 loss=13.504
    300 / 160368 loss=12.501
    400 / 160368 loss=12.314
    500 / 160368 loss=12.06
    600 / 160368 loss=11.612
    700 / 160368 loss=11.286
    800 / 160368 loss=10.75
    900 / 160368 loss=10.877
   1000 / 160368 loss=10.626
   1100 / 160368 loss=11.066
   1200 / 160368 loss=10.876
   1300 / 160368 loss=9.903
   1400 / 160368 loss=10.77
   1500 / 160368 loss=10.418
   1600 / 160368 loss=9.917
   1700 / 160368 loss=10.035
   1800 / 160368 loss=9.286
   1900 / 160368 loss=9.64
   2000 / 160368 loss=10.043
   2100 / 160368 loss=9.814
   2200 / 160368 loss=9.796
   2300 / 160368 loss=9.557
   2400 / 160368 loss=9.778
   2500 / 160368 loss=9.563
   2600 / 160368 loss=10.827
   2700 / 160368 loss=9.462
   2800 / 160368 loss=9.255
   2900 / 160368 loss=8.979
   3000 / 160368 loss=9.376
   3100 / 160368 loss=9.251
   3200 / 160368 loss=10.278
   3300 / 160368 loss=8.874
   3400 / 160368 loss=8.948
   3500 / 160368 loss=9.023
   3600 / 160368 loss=8.7
   3700 / 160368 loss=8.418
   3800 / 160368 loss=8.643
   3900 / 160368 loss=9.087
   4000 / 160368 loss=8.565
   4100 / 160368 loss=8.556
   4200 / 160368 loss=8.393
   4300 / 160368 loss=8.166
   4400 / 160368 loss=8.892
   4500 / 160368 loss=8.265
   4600 / 160368 loss=8.235
   4700 / 160368 loss=8.018
   4800 / 160368 loss=8.513
   4900 / 160368 loss=8.36
   5000 / 160368 loss=8.07
   5100 / 160368 loss=8.372
   5200 / 160368 loss=7.209
   5300 / 160368 loss=7.847
   5400 / 160368 loss=8.447
   5500 / 160368 loss=8.313
   5600 / 160368 loss=7.781
   5700 / 160368 loss=8.174
   5800 / 160368 loss=9.667
   5900 / 160368 loss=8.22
   6000 / 160368 loss=7.312
   6100 / 160368 loss=8.252
   6200 / 160368 loss=7.185
   6300 / 160368 loss=7.286
   6400 / 160368 loss=8.586
   6500 / 160368 loss=7.792
   6600 / 160368 loss=7.774
   6700 / 160368 loss=7.44
   6800 / 160368 loss=7.26
   6900 / 160368 loss=8.55
   7000 / 160368 loss=7.423
   7100 / 160368 loss=7.207
   7200 / 160368 loss=6.993
   7300 / 160368 loss=8.541
   7400 / 160368 loss=7.015
   7500 / 160368 loss=6.71
   7600 / 160368 loss=7.615
   7700 / 160368 loss=7.393
   7800 / 160368 loss=7.333
   7900 / 160368 loss=7.297
   8000 / 160368 loss=8.05
   8100 / 160368 loss=7.584
   8200 / 160368 loss=6.955
   8300 / 160368 loss=6.387
&lt;/denchmark-code&gt;

and on gpus:
&lt;denchmark-code&gt;$ grep wps gpu-repro.txt  | cut -d : -f4- | cut -d ',' -f1
    100 / 85127 loss=15.045
    201 / 85127 loss=13.348
    301 / 85127 loss=12.711
    402 / 85127 loss=12.316
    502 / 85127 loss=11.955
    602 / 85127 loss=11.522
    703 / 85127 loss=11.214
    803 / 85127 loss=10.944
    903 / 85127 loss=10.765
   1003 / 85127 loss=10.633
   1103 / 85127 loss=10.421
   1203 / 85127 loss=10.263
   1303 / 85127 loss=10.225
   1403 / 85127 loss=10.066
   1503 / 85127 loss=9.945
   1603 / 85127 loss=9.934
   1703 / 85127 loss=9.834
   1803 / 85127 loss=9.753
   1903 / 85127 loss=9.654
   2003 / 85127 loss=9.607
   2103 / 85127 loss=9.48
   2203 / 85127 loss=9.327
   2303 / 85127 loss=9.184
   2403 / 85127 loss=9.266
   2503 / 85127 loss=9.05
   2603 / 85127 loss=9.012
   2703 / 85127 loss=8.865
   2803 / 85127 loss=8.791
   2903 / 85127 loss=8.717
   3003 / 85127 loss=8.685
   3103 / 85127 loss=8.651
   3203 / 85127 loss=8.497
   3303 / 85127 loss=8.432
   3403 / 85127 loss=8.387
   3503 / 85127 loss=8.274
   3603 / 85127 loss=8.243
   3703 / 85127 loss=8.132
   3803 / 85127 loss=8.095
   3903 / 85127 loss=8.011
   4003 / 85127 loss=7.99
   4103 / 85127 loss=7.884
   4203 / 85127 loss=7.84
   4303 / 85127 loss=7.694
   4403 / 85127 loss=7.651
   4503 / 85127 loss=7.655
   4603 / 85127 loss=7.54
   4703 / 85127 loss=7.539
   4803 / 85127 loss=7.533
   4903 / 85127 loss=7.358
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='kkissmart' date='2020-07-17T22:09:33Z'>
		&lt;denchmark-link:https://github.com/taylanbil&gt;@taylanbil&lt;/denchmark-link&gt;

add --num-batch-buckets 8 to GPU command as well, that you can get similar # steps for each epoch.
		</comment>
		<comment id='5' author='kkissmart' date='2020-07-20T20:01:58Z'>
		runing one one device (gpu vs tpu) and 1 bucket, I'm getting very similar results in terms of raw loss tensors. See:
&lt;denchmark-link:https://gist.github.com/taylanbil/62a1d2eca60ca6b86d906e94e088c347&gt;https://gist.github.com/taylanbil/62a1d2eca60ca6b86d906e94e088c347&lt;/denchmark-link&gt;

The full logs: &lt;denchmark-link:https://gist.github.com/taylanbil/7241f441dad7aa3dfb2cdf287a72cf76&gt;tpu&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://gist.github.com/taylanbil/30ba3fe4cd3f4270383c18c610f9862c#file-fairseq-trainslation-gpu-1bucket-loss-txt&gt;gpu&lt;/denchmark-link&gt;
.
Notice how the effect I pointed out in the previous comment shows in &lt;denchmark-link:https://gist.github.com/taylanbil/806e3594d0f25ac70b6aa03d294db1ec&gt;this gist&lt;/denchmark-link&gt;
. gpu updates are smoother because they are averaged out, but tpu updates are noisier because they belong to the last step only.
Commands:
gpu command:
&lt;denchmark-code&gt;python tpu_fairseq/train.py /home/taylanbil/data/wmt18_en_de_bpej32k --arch=transformer_vaswani_wmt_en_de_big -s en -t de --criterion cross_entropy --encoder-normalize-before --decoder-normalize-before --task translation --optimizer adam --adam-eps 1e-06 --adam-betas '(0.9, 0.98)' --lr-scheduler polynomial_decay --lr 1e-04 --min-lr -1 --warmup-updates 10000 --total-num-update 500000 --dropout 0.0 --attention-dropout 0.0 --weight-decay 0.0 --max-tokens 2048 --seed 2 --log-format simple --log-interval 100 --max-source-positions 1026 --max-target-positions 1026 --save-interval-updates 5000 --skip-invalid-size-inputs-valid-test --fp16 --distributed-world-size 1 --num-batch-buckets 1
&lt;/denchmark-code&gt;

tpu command:
&lt;denchmark-code&gt;python train.py $datapath --arch=transformer_vaswani_wmt_en_de_big -s en -t de --criterion cross_entropy --encoder-normalize-before --decoder-normalize-before --task translation --optimizer adam --adam-eps 1e-06 --adam-betas '(0.9, 0.98)' --lr-scheduler polynomial_decay --lr 1e-04 --min-lr -1 --warmup-updates 10000 --total-num-update 500000 --dropout 0.0 --attention-dropout 0.0 --weight-decay 0.0 --max-tokens 2052 --seed 2 --log-format simple --log-interval 100 --max-source-positions 1026 --max-target-positions 1026 --save-interval-updates 5000 --skip-invalid-size-inputs-valid-test --num-batch-buckets 1 --tpu --distributed-world-size 1
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='kkissmart' date='2020-07-20T20:52:45Z'>
		And here's comparison runs w/ 8 devices, and 1 bucket: losses still match up very well.

gpu
tpu

		</comment>
		<comment id='7' author='kkissmart' date='2020-07-21T16:11:39Z'>
		Verified that raw losses do match up for 8 devices, 3 buckets as well. This task seems to be doing ok. The only issue is in the delta in reporting, gpus do average over the whole --log-interval steps whereas tpus take the last step.
		</comment>
	</comments>
</bug>