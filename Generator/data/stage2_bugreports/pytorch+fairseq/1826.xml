<bug id='1826' author='freewym' open_date='2020-03-12T05:04:42Z' closed_time='2020-03-21T18:15:44Z'>
	<summary>CUDA error when training with multiple GPUs and criterion's logging_outputs_can_be_summed() is False</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Currently almost all subclass of FairseqCriterion 's logging_outputs_can_be_summed() returns True, and it trains OK with multiple GPUs. But if I change, for example, CrossEntropyCriterion's  logging_outputs_can_be_summed () to return False and train with 2 GPUs , it gives error:
File "/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 118, in join
raise Exception(msg)
Exception:
-- Process 1 terminated with the following error:
Traceback (most recent call last):
File "/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 19, in _wrap
fn(i, *args)
File "/home/ywang/fairseq6/train.py", line 286, in distributed_main
main(args, init_distributed=True)
File "/home/ywang/fairseq6/train.py", line 96, in main
train(args, trainer, task, epoch_itr)
File "/export/b02/ywang/anaconda3/lib/python3.7/contextlib.py", line 74, in inner
return func(*args, **kwds)
File "/home/ywang/fairseq6/train.py", line 176, in train
log_output = trainer.train_step(samples)
File "/export/b02/ywang/anaconda3/lib/python3.7/contextlib.py", line 74, in inner
return func(*args, **kwds)
File "/home/ywang/fairseq6/fairseq/trainer.py", line 347, in train_step
logging_outputs, sample_size, ooms, ignore=is_dummy_batch,
File "/home/ywang/fairseq6/fairseq/trainer.py", line 616, in _aggregate_logging_outputs
logging_outputs, *extra_stats_to_sum, ignore=ignore
File "/home/ywang/fairseq6/fairseq/trainer.py", line 634, in _all_gather_list_sync
max_size=getattr(self.args, 'all_gather_list_size', 16384),
File "/home/ywang/fairseq6/fairseq/distributed_utils.py", line 176, in all_gather_list
result.append(pickle.loads(bytes(out_buffer[header_size:header_size + enc_size].tolist())))
File "/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/storage.py", line 134, in _load_from_bytes
return torch.load(io.BytesIO(b))
File "/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/serialization.py", line 529, in load
return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
File "/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/serialization.py", line 702, in _legacy_load
result = unpickler.load()
File "/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/serialization.py", line 665, in persistent_load
deserialized_objects[root_key] = restore_location(obj, location)
File "/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/serialization.py", line 156, in default_restore_location
result = fn(storage, location)
File "/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/serialization.py", line 136, in _cuda_deserialize
return storage_type(obj.size())
File "/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/cuda/init.py", line 480, in _lazy_new
return super(_CudaBase, cls).new(cls, *args, **kwargs)
RuntimeError: CUDA error: all CUDA-capable devices are busy or unavailable
The reason why I want it to return False is, I have some stats in logging_output which is a tensor not scalar, to be aggregated in reduce_matrices().
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior (always include the command you ran):

Just have CrossEntropyCriterion.logging_outputs_can_be_summed() to return False and train with multi-GPUs
See error

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


fairseq Version (e.g., 1.0 or master): master
PyTorch Version (e.g., 1.0): 1.4
OS (e.g., Linux): Linux
How you installed fairseq (pip, source): source
Build command you used (if compiling from source):
Python version: 3.7
CUDA/cuDNN version: 10.2
GPU models and configuration:  GeForce GTX 1080
Any other relevant information:

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='freewym' date='2020-03-20T21:29:40Z'>
		Merging the above fix, please reopen if this is still an issue after this is merged!
		</comment>
	</comments>
</bug>