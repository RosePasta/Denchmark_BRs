<bug id='1343' author='mingruimingrui' open_date='2019-11-04T04:52:23Z' closed_time='2019-12-24T07:00:41Z'>
	<summary>fp32 inference is faster than fp16, pytorch &amp;gt;= 1.2</summary>
	<description>
I'd like to leave an interesting nugget.
On pytorch==1.0, fp16 inference is faster than fp32 but on pytorch&gt;=1.2,
On german news-test 2014



Precision
Inference speed (sents/sec)




fp32
324.16


fp16
307.11



*german news-test 2014 was pre-sorted according to the number of tokens in each sentence before batching. This is to minimize the number of padding tokens required.
Environmental setup
CPU: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz
GPU: V100
CUDA==10.0 (conda)
cuDNN==7.6.3 (conda)
python==3.6
fairseq==0.8.0 (pip)
pytorch==1.3 (conda)
&lt;denchmark-link:https://github.com/myleott&gt;@myleott&lt;/denchmark-link&gt;
 Could I check with you if these results are expected?
	</description>
	<comments>
		<comment id='1' author='mingruimingrui' date='2019-12-16T23:20:57Z'>
		This is not expected.
		</comment>
	</comments>
</bug>