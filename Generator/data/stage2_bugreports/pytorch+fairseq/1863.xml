<bug id='1863' author='mgaido91' open_date='2020-03-18T16:01:33Z' closed_time='2020-05-27T14:50:45Z'>
	<summary>Speech recognition fails due to NaN if there is an input of size 1</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

If there is an input sample with a single spectrogram, the training fails because everything is converted to NaN. This is because of the normalization which is applied to each item. Despite uncommon, this situation happens in common datasets, such as mozilla-voice.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Run your command on a dataset which has a sample with 1 spectrogram and if distributed the command fails in the check gradient; if not, the loss becomes always NaN.
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;
&gt;&gt;&gt; t=torch.tensor([[-0.7661, -1.3889, -2.0972, -0.9134, -0.7071, -0.9765, -0.8700, -0.8283,
...           0.7512,  1.3211,  2.1532,  2.1174,  1.2800,  1.2633,  1.6147,  1.6322,
...           2.0723,  3.1522,  3.2852,  2.2309,  2.5569,  2.2183,  2.2862,  1.5886,
...           0.8773,  0.8725,  1.2662,  0.9899,  1.1069,  1.3926,  1.2795,  1.1199,
...           1.1477,  1.2687,  1.3843,  1.1903,  0.8355,  1.1367,  1.2639,  1.4707]])
&gt;&gt;&gt; apply_mv_norm(t)
tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Despite such input may be unexpected/wrong, the training shouldn't fail because of a single bad sample. The training should go on successfully, and the normalization should not cause NaN
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


fairseq Version (e.g., 1.0 or master): master
PyTorch Version (e.g., 1.0): 1.4
OS (e.g., Linux): linux
How you installed fairseq (pip, source): source
Build command you used (if compiling from source): pip install -e .
Python version: 3.7.6
CUDA/cuDNN version: 10
GPU models and configuration: 8 K80
Any other relevant information:

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='mgaido91' date='2020-05-26T19:07:13Z'>
		&lt;denchmark-link:https://github.com/mgaido91&gt;@mgaido91&lt;/denchmark-link&gt;
 , looks like the NaN happens in mean/variance normalization () ? I can guess this is because we are using unbiased variance estimation, i.e., \sum_{i} (x_i - mean)^2/(N-1), which naturally happens to NaN when N=1. To confirm that, could you please point me which  you are using ?
		</comment>
		<comment id='2' author='mgaido91' date='2020-05-26T22:47:36Z'>
		Hi &lt;denchmark-link:https://github.com/yqwangustc&gt;@yqwangustc&lt;/denchmark-link&gt;
, yes, I definitely agree with your analysis. Please see the PR I opened for this issue to check the function being used (please notice that in the current codebase there is no way to skip this normalization). Thanks.
		</comment>
	</comments>
</bug>