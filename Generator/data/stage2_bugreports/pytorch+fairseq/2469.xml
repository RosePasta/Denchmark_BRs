<bug id='2469' author='erip' open_date='2020-08-12T14:30:55Z' closed_time='2020-08-12T16:46:08Z'>
	<summary>Scripted transformer fails on forward pass</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When using a transformer that has been scripted for translation, the model fails to decode.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior (always include the command you ran):

Install fairseq from source
Preprocess:

fairseq-preprocess --source-lang $SRC --target-lang $TGT \
    --trainpref data/bpe_pretrain --validpref data/bpe_valid --testpref data/bpe_test \
    --destdir data-bin/ \
    --workers 20

Train:

fairseq-train data-bin \
    --source-lang $SRC --target-lang $TGT \
    --clip-norm 0.1 --dropout 0.2 --max-tokens 10240 \
    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \
    --lr  5e-4 --lr-scheduler inverse_sqrt \
    --label-smoothing 0.1 \
    --warmup-init-lr 1e-7 \
    --warmup-updates 8000 \
    --criterion label_smoothed_cross_entropy \
    --save-dir "checkpoints" \
    --arch transformer \
    --keep-last-epochs 10 \
    --max-epoch 100 \
    --num-workers 8 \
    --skip-invalid-size-inputs-valid-test \
    --ddp-backend no_c10d \
    --tensorboard-logdir "logdir-$SLURM_JOB_ID"

Export:

#!/usr/bin/env python

import torch

from fairseq.sequence_generator import SequenceGenerator
from fairseq.models.transformer import TransformerModel
from fairseq.data import Dictionary

if __name__ == "__main__":
    tgt_dict = Dictionary.load(open('dict.fr.txt'))
    model = TransformerModel.from_pretrained('.', 'checkpoints/checkpoint_best.pt', '.', bpe='sentencepiece', sentencepiece_model='spm.model')

    generator = SequenceGenerator(model.models, tgt_dict)

    scripted_gen = torch.jit.script(generator)
    scripted_gen.save('checkpoint_best.scripted.pt')


Use checkpoint_best.scripted.pt to generate sequence.


See error


&lt;denchmark-code&gt;26548: Error: Traceback of TorchScript, original code (most recent call last):
26548: Error:   File "/Users/erippeth/miniconda3/envs/fairseq-temp/lib/python3.6/site-packages/torch/autograd/grad_mode.py", line 116, in forward
26548: Error:                 (default: self.eos)
26548: Error:         """
26548: Error:         return self._generate(sample, prefix_tokens, bos_token)
26548: Error:                ~~~~~~~~~~~~~~ &lt;--- HERE
26548: Error:   File "/Users/erippeth/miniconda3/envs/fairseq-temp/lib/python3.6/site-packages/fairseq/models/fairseq_encoder.py", line 48, in forward_torchscript
26548: Error:         """
26548: Error:         if torch.jit.is_scripting():
26548: Error:             return self.forward(
26548: Error:                    ~~~~~~~~~~~~ &lt;--- HERE
26548: Error:                 src_tokens=net_input["src_tokens"],
26548: Error:                 src_lengths=net_input["src_lengths"],
26548: Error:   File "/Users/erippeth/miniconda3/envs/fairseq-temp/lib/python3.6/site-packages/fairseq/models/transformer.py", line 399, in forward
26548: Error:                   Only populated if *return_all_hiddens* is True.
26548: Error:         """
26548: Error:         x, encoder_embedding = self.forward_embedding(src_tokens)
26548: Error:                                ~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE
26548: Error:         # B x T x C -&gt; T x B x C
26548: Error:   File "/Users/erippeth/miniconda3/envs/fairseq-temp/lib/python3.6/site-packages/torch/nn/modules/sparse.py", line 124, in forward
26548: Error:     def forward(self, input: Tensor) -&gt; Tensor:
26548: Error:         return F.embedding(
26548: Error:                ~~~~~~~~~~~ &lt;--- HERE
26548: Error:             input, self.weight, self.padding_idx, self.max_norm,
26548: Error:             self.norm_type, self.scale_grad_by_freq, self.sparse)
26548: Error:   File "/Users/erippeth/miniconda3/envs/fairseq-temp/lib/python3.6/site-packages/torch/nn/functional.py", line 1814, in embedding
26548: Error:         # remove once script supports set_grad_enabled
26548: Error:         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
26548: Error:     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
26548: Error:            ~~~~~~~~~~~~~~~ &lt;--- HERE
26548: Error: RuntimeError: index out of range in self
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

Little hard to untangle. :-)
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Shapes should match and index should not be out of range when running a forward pass.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


fairseq Version (e.g., 1.0 or master): master
PyTorch Version (e.g., 1.0) 1.6.0
OS (e.g., Linux): OS X
How you installed fairseq (pip, source): source
Build command you used (if compiling from source): CC=clang CFLAGS='-stdlib=libc++' pip install .
Python version: 3.6.10
CUDA/cuDNN version: N/A
GPU models and configuration: N/A
Any other relevant information:

	</description>
	<comments>
		<comment id='1' author='erip' date='2020-08-12T16:11:15Z'>
		It seems like there's some inconsistency around this error depending on the input tokens. I'm trying to debug it, but it seems like there's an issue with the decoder embedding dimension. I've confirmed that the sentences it chokes on post-scripting are handled fine pre-scripting.
		</comment>
		<comment id='2' author='erip' date='2020-08-12T16:46:08Z'>
		It seems like an issue with my driver code and handling dictionaries. I had left  as true which was adding OOVs at runtime thus overrunning the embedding dimension. I have been looking for how this is handled in fairseq, and it looks like it's something like what's seen &lt;denchmark-link:https://github.com/pytorch/fairseq/blob/b31849aa9282755bbb9eecd9384b2e0fc2b9c0a1/fairseq/models/roberta/hub_interface.py#L61&gt;here&lt;/denchmark-link&gt;
. I'll close this for now as I think it's user error. 
		</comment>
	</comments>
</bug>