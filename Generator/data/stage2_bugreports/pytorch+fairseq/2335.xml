<bug id='2335' author='qavion' open_date='2020-07-16T23:41:50Z' closed_time='2020-08-27T17:12:04Z'>
	<summary>Incorrect build of dataset during inference of translation_lev task.</summary>
	<description>
&lt;denchmark-h:h2&gt;🐛 Bug&lt;/denchmark-h&gt;

In translation_lev task, bos should add when building a dataset, but it doesn't for inference.
As a result, the output in fairseq-interactive becomes strange and differs from the output in fairseq-generate.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;


Prepare data, pre-process and learn a Levenshtein Transformer model.
Run cmd fairseq-interactive data-bin/lang8_mojimoji --buffer-size 1000 --input data/lang8/test.tok.mjmj.bpe.src --task translation_lev --path checkpoints/levt_lang8_mojimoji/checkpoint47.pt --iter-decode-max-iter 9 --iter-decode-eos-penalty 0 --max-tokens 2000 &gt; interactive.out
Run cmd fairseq-generate data-bin/lang8_mojimoji --task translation_lev --path checkpoints/levt_lang8_mojimoji/checkpoint47.pt --iter-decode-max-iter 9 --iter-decode-eos-penalty 0 --max-tokens 2000 &gt; generate.out


pre-processed data directory: data-bin/lang8_mojimoji
source file: data/lang8/test.tok.mjmj.bpe.src
learned model: checkpoints/levt_lang8_mojimoji/checkpoint47.pt

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

I expect that the generated results of the interactive.out and the generate.out will be equal.
However, the output of the interactive.out is strange.
interactive.out
&lt;denchmark-code&gt;︙
2020-07-17 08:32:18 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/levt_lang8_mojimoji/checkpoint39.pt
2020-07-17 08:32:25 | INFO | fairseq_cli.interactive | Sentence buffer size: 1000
2020-07-17 08:32:25 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2
2020-07-17 08:32:25 | INFO | fairseq_cli.interactive | Type the input sentence and press return:
S-0	▁男性 ▁と ▁女性 ▁の ▁言葉 ▁の ▁違い ▁?
H-0	-0.6313951015472412	▁と ▁女性 ▁の ▁言葉 ▁の ▁男性 ▁の ▁違い ▁?
D-0	-0.6313951015472412	▁と ▁女性 ▁の ▁言葉 ▁の ▁男性 ▁の ▁違い ▁?
P-0	0.0000 -0.8670 -0.6678 -0.6739 -0.4745 -0.3614 -2.1308 -0.9909 -0.6863 -0.0927 0.0000
︙
S-6	▁勉強 ▁つもり ▁だっ ▁た ▁。 ▁。 ▁。 ▁( ▁2 ▁回 ▁)
H-6	-0.1993567794561386	▁つもり ▁だっ ▁た ▁。 ▁。 ▁。 ▁( ▁2 ▁回 ▁)
D-6	-0.1993567794561386	▁つもり ▁だっ ▁た ▁。 ▁。 ▁。 ▁( ▁2 ▁回 ▁)
P-6	0.0000 -0.5071 -0.1198 -0.2476 -0.4717 -0.1938 -0.1455 -0.2593 -0.1719 -0.2082 -0.0674 0.0000
︙
&lt;/denchmark-code&gt;

In interactive, the first token expected to be output may not be output.
generate.out
&lt;denchmark-code&gt;︙
2020-07-17 08:33:42 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/levt_lang8_mojimoji/checkpoint39.pt
︙
S-0	▁男性 ▁と ▁女性 ▁の ▁言葉 ▁の ▁違い ▁?
T-0	▁男性 ▁と ▁女性 ▁の ▁言葉 ▁の ▁違い ▁?
H-0	-0.527859091758728	▁男性 ▁と ▁女性 ▁の ▁言葉 ▁の ▁違い ▁は ▁ある ▁?
D-0	-0.527859091758728	▁男性 ▁と ▁女性 ▁の ▁言葉 ▁の ▁違い ▁は ▁ある ▁?
P-0	0.0000 -0.0860 -0.3473 -0.1767 -0.7589 -0.4253 -1.4291 -0.7916 -1.0230 -1.2009 -0.0955 0.0000
︙
S-6	▁勉強 ▁つもり ▁だっ ▁た ▁。 ▁。 ▁。 ▁( ▁2 ▁回 ▁)
T-6	▁勉強 ▁する ▁つもり ▁でし ▁た ▁。 ▁。 ▁。 ▁( ▁2 ▁回 ▁)
H-6	-0.22894743084907532	▁勉強 ▁する ▁つもり ▁だっ ▁た ▁。 ▁。 ▁。 ▁( ▁2 ▁回 ▁)
D-6	-0.22894743084907532	▁勉強 ▁する ▁つもり ▁だっ ▁た ▁。 ▁。 ▁。 ▁( ▁2 ▁回 ▁)
P-6	0.0000 -0.0959 -0.2483 -0.2130 -0.6924 -1.2037 -0.0973 -0.0615 -0.1622 -0.0879 -0.1282 -0.1485 -0.0664 0.0000
︙
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


fairseq Version (e.g., 1.0 or master): master
How you installed fairseq (pip, source): source
Build command you used (if compiling from source): pip install --editable ./ python setup.py build_ext --inplace

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

I've already modified the code, so I'll send a pull request.
	</description>
	<comments>
		<comment id='1' author='qavion' date='2020-08-27T17:12:04Z'>
		Fixed in &lt;denchmark-link:https://github.com/pytorch/fairseq/commit/93f5128509278f425afb6bcf0da574c0af0e0c16&gt;93f5128&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>