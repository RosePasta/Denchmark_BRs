<bug id='2062' author='14H034160212' open_date='2020-04-25T15:06:37Z' closed_time='2020-05-01T11:13:10Z'>
	<summary>Wired issue when I test a binary classification task by using RoBERTa-large</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Hi, I use RoBERTa-large as pre-trained model and use RACE as intermediate training dataset, then I use a binary classification task dataset as the fine-tuning downstream task. The accuracy for the binary classification task on the train and dev dataset is quite high which is nearly 0.99. But in the test dataset which is nearly 0.01 which is quite wired. I have double check the test script which is fine. I use 1 to represent the True label and use 0 to represent False label. So, It will be great if any of you can make any comment. Thanks a lot.
Here is the data format script
&lt;denchmark-code&gt;import argparse
import os
import random
from glob import glob
import json_lines
import re

random.seed(0)

def main(args):
    for split in ['train', 'dev', 'test']:
        samples = []
        cur_dir = os.path.join(args.datadir, split)
        for filename in os.listdir(cur_dir):
            cur_path = os.path.join(cur_dir, filename)
            with open(cur_path, 'r') as f:
                for item in json_lines.reader(f):
                    questions = item["questions"]
                    context = item["context"].replace("\n", " ")
                    context = re.sub(r'\s+', ' ', context)
                    for i in range(len(questions)):
                        text = questions[i]["text"]
                        label = questions[i]["label"]
                        if label == True:
                            label_num = 1
                        else:
                            label_num = 0
                        text = re.sub(r'\s+', ' ', text)
                        context_text = context + ' ' + text
                        samples.append((context_text, label_num))

        random.shuffle(samples)
        out_fname = 'train' if split == 'train' else 'dev'
        f1 = open(os.path.join(args.datadir, out_fname + '.input0'), 'w')
        f2 = open(os.path.join(args.datadir, out_fname + '.label'), 'w')
        for sample in samples:
            f1.write(sample[0] + '\n')
            f2.write(str(sample[1]) + '\n')
        f1.close()
        f2.close()

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--datadir', default='fairseq-master/depth-5')
    args = parser.parse_args()
    main(args)
&lt;/denchmark-code&gt;

Here is the test script
&lt;denchmark-code&gt;from fairseq.models.roberta import RobertaModel
import json_lines
import re

roberta = RobertaModel.from_pretrained(
    'checkpoints/',
    checkpoint_file='checkpoint_best.pt',
    data_name_or_path='PARARULE-bin'
)

ncorrect, nsamples = 0, 0
roberta.cuda(2)
roberta.eval()
with open('test/test.jsonl', 'r') as f:
    for item in json_lines.reader(f):
        questions = item["questions"]
        context = item["context"].replace("\n", " ")
        context = re.sub(r'\s+', ' ', context)
        for i in range(len(questions)):
            text = questions[i]["text"]
            label = questions[i]["label"]
            if label == True:
                label_num = 1
            else:
                label_num = 0
            text = re.sub(r'\s+', ' ', text)
            context_text = context + ' ' + text
            tokens = roberta.encode(context_text)
            prediction = roberta.predict('PARARULE_head', tokens).argmax().item()
            ncorrect += int(prediction == label_num)
            nsamples += 1
print('| Accuracy: ', float(ncorrect) / float(nsamples))
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='14H034160212' date='2020-04-26T16:34:59Z'>
		So you ran the exact same test script on your validation set and get 0.99?  Are you sure that there is no overlap between your train and test set?
		</comment>
		<comment id='2' author='14H034160212' date='2020-04-27T10:08:48Z'>
		Yes, the train, dev and test file are all independent and no overlap. Actually, I got 0.99 train and dev accuracy, but got 0.01 accuracy in the test dataset. The ideal situation should be 0.99 accuracy on the test dataset, but it is on the opposite. I guess there exist a swap of the label which is quite wired.
		</comment>
		<comment id='3' author='14H034160212' date='2020-05-01T11:13:10Z'>
		Yeah, sounds like labels are swapped in your preprocessing somewhere :)
		</comment>
	</comments>
</bug>