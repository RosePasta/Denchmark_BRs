<bug id='2175' author='shonenkov' open_date='2020-05-22T22:55:46Z' closed_time='2020-05-24T09:59:30Z'>
	<summary>mBart cc25 doesn't work</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior (always include the command you ran):

load https://dl.fbaipublicfiles.com/fairseq/models/mbart/mbart.CC25.tar.gz
extract tar.gz
use python:

from fairseq.models.bart import BARTModel
model = BARTModel.from_pretrained(model_name_or_path='./mbart.CC25')

&gt;&gt;&gt; /usr/local/lib/python3.6/site-packages/fairseq/models/bart/model.py in from_pretrained(cls, model_name_or_path, checkpoint_file, data_name_or_path, bpe, **kwargs)
    109             bpe=bpe,
    110             load_checkpoint_heads=True,
--&gt; 111             **kwargs,
    112         )
    113         return BARTHubInterface(x['args'], x['task'], x['models'][0])

/usr/local/lib/python3.6/site-packages/fairseq/hub_utils.py in from_pretrained(model_name_or_path, checkpoint_file, data_name_or_path, archive_map, **kwargs)
     71     models, args, task = checkpoint_utils.load_model_ensemble_and_task(
     72         [os.path.join(model_path, cpt) for cpt in checkpoint_file.split(os.pathsep)],
---&gt; 73         arg_overrides=kwargs,
     74     )
     75 

/usr/local/lib/python3.6/site-packages/fairseq/checkpoint_utils.py in load_model_ensemble_and_task(filenames, arg_overrides, task, strict, suffix)
    209         # build model for ensemble
    210         model = task.build_model(args)
--&gt; 211         model.load_state_dict(state["model"], strict=strict, args=args)
    212         ensemble.append(model)
    213     return ensemble, args, task

/usr/local/lib/python3.6/site-packages/fairseq/models/fairseq_model.py in load_state_dict(self, state_dict, strict, args)
     91         self.upgrade_state_dict(state_dict)
     92         new_state_dict = prune_state_dict(state_dict, args)
---&gt; 93         return super().load_state_dict(new_state_dict, strict)
     94 
     95     def upgrade_state_dict(self, state_dict):

/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py in load_state_dict(self, state_dict, strict)
    828         if len(error_msgs) &gt; 0:
    829             raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
--&gt; 830                                self.__class__.__name__, "\n\t".join(error_msgs)))
    831         return _IncompatibleKeys(missing_keys, unexpected_keys)
    832 

RuntimeError: Error(s) in loading state_dict for BARTModel:
	Unexpected key(s) in state_dict: "encoder.layernorm_embedding.weight", "encoder.layernorm_embedding.bias", "decoder.layernorm_embedding.weight", "decoder.layernorm_embedding.bias". 
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

matched all layers success
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


fairseq Version (e.g., 1.0 or master): master
PyTorch Version (e.g., 1.0): 1.4.0
OS (e.g., Linux): Ubuntu
How you installed fairseq (pip, source): source
Build command you used (if compiling from source):

!git clone https://github.com/pytorch/fairseq
!pip install --no-deps './fairseq'

Python version: 3.6
CUDA/cuDNN version: 10.0
GPU models and configuration: using cpu
Any other relevant information: -

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='shonenkov' date='2020-05-24T06:23:35Z'>
		Instead of this line,
model = BARTModel.from_pretrained(model_name_or_path='./mbart.CC25')
Please pass layernorm-embedding as
model = BARTModel.from_pretrained(model_name_or_path='./mbart.CC25', layernorm_embedding=True)
		</comment>
		<comment id='2' author='shonenkov' date='2020-05-24T09:59:16Z'>
		thank you! it helped
		</comment>
	</comments>
</bug>