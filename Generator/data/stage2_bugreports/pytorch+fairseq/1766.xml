<bug id='1766' author='tuhinjubcse' open_date='2020-03-03T07:21:46Z' closed_time='2020-03-05T15:52:00Z'>
	<summary>BART fine tuning pauses</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Tried finetuning BART and it pauses in epoch 001 a 3%
script used
TOTAL_NUM_UPDATES=20000
WARMUP_UPDATES=500
LR=3e-05
AX_TOKENS=1024
UPDATE_FREQ=16
BART_PATH=/nas/home/tuhinc/fairseq/bart.large/model.pt
python train.py fullstory
--restore-file $BART_PATH 
--max-tokens $MAX_TOKENS 
--task translation 
--source-lang source --target-lang target 
--truncate-source 
--layernorm-embedding 
--share-all-embeddings 
--share-decoder-input-output-embed 
--reset-optimizer --reset-dataloader --reset-meters 
--required-batch-size-multiple 1 
--arch bart_large 
--criterion label_smoothed_cross_entropy 
--label-smoothing 0.1 
--dropout 0.1 --attention-dropout 0.1 
--weight-decay 0.01 --optimizer adam --adam-betas "(0.9, 0.999)" --adam-eps 1e-08 
--clip-norm 0.1 
--lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_NUM_UPDATES --warmup-updates $WARMUP_UPDATES 
--memory-efficient-fp16 --update-freq $UPDATE_FREQ 
--save-dir "checkpoint-fullstory" 
--ddp-backend=no_c10d  
--skip-invalid-size-inputs-valid-test 
--find-unused-parameters;
~
Steps to reproduce the behavior (always include the command you ran):

Run cmd '....'
See error

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

I have finetuned BART previously n this never happened. The length of individual samples have increased as I was training previously on truncated version of same dataset
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


fairseq Version (e.g., 1.0 or master): 0.9.0
PyTorch Version (e.g., 1.0) 1.3.1
OS (e.g., Linux): Linux
How you installed fairseq (pip, source):
Build command you used (if compiling from source):
Python version: 3.7.3
CUDA/cuDNN version: 10.1.243
GPU models and configuration: RTX 2080 ( 4 gpus)

&lt;denchmark-link:https://user-images.githubusercontent.com/3104771/75778944-9a207d80-5d0d-11ea-8b13-dcc62cc28edb.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='tuhinjubcse' date='2020-03-03T13:13:03Z'>
		&lt;denchmark-link:https://github.com/myleott&gt;@myleott&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ngoyal2707&gt;@ngoyal2707&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='tuhinjubcse' date='2020-03-03T14:53:38Z'>
		Hmm, often when things get stuck it's because of some failure on one of the workers, which blocks the rest of the workers from continuing. Does it still get stuck if you use a single GPU? What about if you lower the batch size?
		</comment>
		<comment id='3' author='tuhinjubcse' date='2020-03-04T00:13:20Z'>
		it works on a single gpu
		</comment>
		<comment id='4' author='tuhinjubcse' date='2020-03-05T15:51:41Z'>
		Assuming this is the same underlying setup as &lt;denchmark-link:https://github.com/pytorch/fairseq/issues/1772&gt;#1772&lt;/denchmark-link&gt;
, it's almost definitely an OOM on one of the workers causing the other to hang :/ Let's discuss there.
		</comment>
	</comments>
</bug>