<bug id='1682' author='sshleifer' open_date='2020-02-07T15:28:32Z' closed_time='2020-02-07T20:30:28Z'>
	<summary>BART attn_mask unused</summary>
	<description>
Does anyone know why attn_mask is not passed to self.self_attn after 


fairseq/fairseq/modules/transformer_layer.py


         Line 84
      in
      4e48c4a






 attn_mask = attn_mask.masked_fill(attn_mask.bool(), -1e8) 




?
	</description>
	<comments>
		<comment id='1' author='sshleifer' date='2020-02-07T20:30:28Z'>
		It's a bug, but not a major one since this parameter isn't usually passed in. See &lt;denchmark-link:https://github.com/pytorch/fairseq/issues/1339&gt;#1339&lt;/denchmark-link&gt;
. We had a fix but it didn't get merged... I'll merge one now.
Fix here: &lt;denchmark-link:https://github.com/pytorch/fairseq/commit/42935dcc8c2d390547d14c29cc71f3f3f379bcfb&gt;42935dc&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>