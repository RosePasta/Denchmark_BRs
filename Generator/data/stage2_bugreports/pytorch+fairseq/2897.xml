<bug id='2897' author='alealv' open_date='2020-11-16T09:37:46Z' closed_time='2020-11-16T19:09:39Z'>
	<summary>Hydra error since commit 4ea1c1ee</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Commit &lt;denchmark-link:https://github.com/pytorch/fairseq/commit/4ea1c1eee077cbf85b1110e6f25d691e53270a7b&gt;4ea1c1e&lt;/denchmark-link&gt;
 introduce this  bug:
Traceback (most recent call last):
  File "train.py", line 14, in &lt;module&gt;
    cli_main()
  File "/home/aalvarez/Projects/fairseq/fairseq_cli/train.py", line 392, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/aalvarez/Projects/fairseq/fairseq/distributed_utils.py", line 313, in call_main
    torch.multiprocessing.spawn(
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 199, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 157, in start_processes
    while not context.join():
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 118, in join
    raise Exception(msg)
Exception: 

-- Process 3 terminated with the following error:
Traceback (most recent call last):
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/hydra/_internal/config_loader_impl.py", line 513, in _apply_overrides_to_config
    OmegaConf.update(cfg, key, value, merge=True)
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/omegaconf/omegaconf.py", line 613, in update
    root.__setattr__(last_key, value)
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/omegaconf/dictconfig.py", line 278, in __setattr__
    self._format_and_raise(key=key, value=value, cause=e)
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/omegaconf/base.py", line 95, in _format_and_raise
    format_and_raise(
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/omegaconf/_utils.py", line 694, in format_and_raise
    _raise(ex, cause)
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/omegaconf/_utils.py", line 610, in _raise
    raise ex  # set end OC_CAUSE=1 for full backtrace
omegaconf.errors.ValidationError: Invalid value assigned : str is not a subclass of ListConfig or list.
	full_key: model.latent_temp
	reference_type=Optional[Wav2Vec2Config]
	object_type=Wav2Vec2Config

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 19, in _wrap
    fn(i, *args)
  File "/home/aalvarez/Projects/fairseq/fairseq/distributed_utils.py", line 300, in distributed_main
    main(cfg, **kwargs)
  File "/home/aalvarez/Projects/fairseq/fairseq_cli/train.py", line 74, in main
    model = task.build_model(cfg.model)
  File "/home/aalvarez/Projects/fairseq/fairseq/tasks/audio_pretraining.py", line 197, in build_model
    model = super().build_model(model_cfg)
  File "/home/aalvarez/Projects/fairseq/fairseq/tasks/fairseq_task.py", line 272, in build_model
    model = models.build_model(cfg, self)
  File "/home/aalvarez/Projects/fairseq/fairseq/models/__init__.py", line 86, in build_model
    return model.build_model(cfg, task)
  File "/home/aalvarez/Projects/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py", line 168, in build_model
    w2v_encoder = Wav2VecEncoder(args, task.target_dictionary)
  File "/home/aalvarez/Projects/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py", line 330, in __init__
    state = checkpoint_utils.load_checkpoint_to_cpu(
  File "/home/aalvarez/Projects/fairseq/fairseq/checkpoint_utils.py", line 237, in load_checkpoint_to_cpu
    state = _upgrade_state_dict(state)
  File "/home/aalvarez/Projects/fairseq/fairseq/checkpoint_utils.py", line 458, in _upgrade_state_dict
    state["cfg"] = convert_namespace_to_omegaconf(state["args"])
  File "/home/aalvarez/Projects/fairseq/fairseq/dataclass/utils.py", line 325, in convert_namespace_to_omegaconf
    composed_cfg = compose("config", overrides=overrides, strict=False)
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/hydra/experimental/compose.py", line 31, in compose
    cfg = gh.hydra.compose_config(
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 507, in compose_config
    cfg = self.config_loader.load_configuration(
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/hydra/_internal/config_loader_impl.py", line 151, in load_configuration
    return self._load_configuration(
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/hydra/_internal/config_loader_impl.py", line 277, in _load_configuration
    ConfigLoaderImpl._apply_overrides_to_config(config_overrides, cfg)
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/hydra/_internal/config_loader_impl.py", line 520, in _apply_overrides_to_config
    raise ConfigCompositionException(
hydra.errors.ConfigCompositionException: Error merging override model.latent_temp='(2.0,0.1,0.999995)'
Then, commit &lt;denchmark-link:https://github.com/pytorch/fairseq/commit/6815772651fd639ed16360074aa23e238b29c6ce&gt;6815772&lt;/denchmark-link&gt;
 changed it to:
Traceback (most recent call last):
  File "train.py", line 14, in &lt;module&gt;
    cli_main()
  File "/home/aalvarez/Projects/fairseq/fairseq_cli/train.py", line 392, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/aalvarez/Projects/fairseq/fairseq/distributed_utils.py", line 313, in call_main
    torch.multiprocessing.spawn(
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 199, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 157, in start_processes
    while not context.join():
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 118, in join
    raise Exception(msg)
Exception: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 19, in _wrap
    fn(i, *args)
  File "/home/aalvarez/Projects/fairseq/fairseq/distributed_utils.py", line 300, in distributed_main
    main(cfg, **kwargs)
  File "/home/aalvarez/Projects/fairseq/fairseq_cli/train.py", line 74, in main
    model = task.build_model(cfg.model)
  File "/home/aalvarez/Projects/fairseq/fairseq/tasks/audio_pretraining.py", line 198, in build_model
    model = super().build_model(model_cfg)
  File "/home/aalvarez/Projects/fairseq/fairseq/tasks/fairseq_task.py", line 272, in build_model
    model = models.build_model(cfg, self)
  File "/home/aalvarez/Projects/fairseq/fairseq/models/__init__.py", line 85, in build_model
    return model.build_model(cfg, task)
  File "/home/aalvarez/Projects/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py", line 168, in build_model
    w2v_encoder = Wav2VecEncoder(args, task.target_dictionary)
  File "/home/aalvarez/Projects/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py", line 330, in __init__
    state = checkpoint_utils.load_checkpoint_to_cpu(
  File "/home/aalvarez/Projects/fairseq/fairseq/checkpoint_utils.py", line 237, in load_checkpoint_to_cpu
    state = _upgrade_state_dict(state)
  File "/home/aalvarez/Projects/fairseq/fairseq/checkpoint_utils.py", line 458, in _upgrade_state_dict
    state["cfg"] = convert_namespace_to_omegaconf(state["args"])
  File "/home/aalvarez/Projects/fairseq/fairseq/dataclass/utils.py", line 318, in convert_namespace_to_omegaconf
    overrides, deletes = override_module_args(args)
  File "/home/aalvarez/Projects/fairseq/fairseq/dataclass/utils.py", line 306, in override_module_args
    overrides.extend(_override_attr("model", dc, args))
  File "/home/aalvarez/Projects/fairseq/fairseq/dataclass/utils.py", line 213, in _override_attr
    and not issubclass(field_type, Enum)  # not choices enum
TypeError: issubclass() arg 1 must be a class
But commit &lt;denchmark-link:https://github.com/pytorch/fairseq/commit/a66cc28b14ca8ff95e3aadfd3ca77bfb5b00c136&gt;a66cc28&lt;/denchmark-link&gt;
 restored the previous
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;python -W ignore train.py /mnt/data/ale/manifest_ml2 \
--save-dir ~/trainings \
--wer-args '("/mnt/data/ale/kenlm-models/openwebtext/v5/4-gram-265M-25-10-2020-pruned-300K-1.bin","/mnt/data/ale/manifest_ml2/dev.lex",2,-1)' \
--post-process letter  \
--valid-subset dev \
--best-checkpoint-metric wer  \
--num-workers 10 \
--max-update 80000  \
--sentence-avg  \
--task audio_pretraining  \
--arch wav2vec_ctc  \
--w2v-path /mnt/data/ale/models/wav2vec_vox_new.pt \
--labels ltr  \
--apply-mask  \
--mask-selection static  \
--mask-other 0  \
--mask-length 10  \
--mask-prob 0.5  \
--layerdrop 0.1 \
--mask-channel-selection static  \
--mask-channel-other 0  \
--mask-channel-length 64  \
--mask-channel-prob 0.5  \
--zero-infinity \
--feature-grad-mult 0.0  \
--freeze-finetune-updates 10000  \
--validate-after-updates 10000  \
--optimizer adam \
--adam-betas '(0.9, 0.98)'  \
--adam-eps 1e-08  \
--lr 2e-05  \
--lr-scheduler tri_stage  \
--warmup-steps 8000  \
--hold-steps 32000 \
--decay-steps 40000  \
--final-lr-scale 0.05  \
--final-dropout 0.0  \
--dropout 0.0  \
--activation-dropout 0.1  \
--criterion ctc \
--attention-dropout 0.0  \
--seed 2337  \
--log-format json  \
--log-interval 500  \
--ddp-backend no_c10d \
--normalize \
--batch-size 10 \
--distributed-world-size 4 \
--fp16
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


fairseq Version (e.g., 1.0 or master): master
PyTorch Version (e.g., 1.0): 1.7+cuda11
OS (e.g., Linux): Ubuntu 20.04
How you installed fairseq (pip, source): pip
Build command you used (if compiling from source):
Python version: v3.8
CUDA/cuDNN version: CUDA v11.1 cuDNN v8
GPU models and configuration: RTX 2080 Ti
Any other relevant information:

	</description>
	<comments>
		<comment id='1' author='alealv' date='2020-11-16T10:42:53Z'>
		It seems like I've run into a similar problem where the checkpoint model in pretraining phase saves the latent temp parameters as a string. When loading it in finetuning it loads and passes it as string to the GumbelVectorQuantizer which expects a tuple.
Traceback (most recent call last): File "/home/rokas/ino-voice/fairseq/fairseq/train.py", line 14, in &lt;module&gt; cli_main() File "/home/rokas/ino-voice/fairseq/fairseq/fairseq_cli/train.py", line 392, in cli_main distributed_utils.call_main(cfg, main) File "/home/rokas/ino-voice/fairseq/fairseq/fairseq/distributed_utils.py", line 334, in call_main main(cfg, **kwargs) File "/home/rokas/ino-voice/fairseq/fairseq/fairseq_cli/train.py", line 74, in main model = task.build_model(cfg.model) File "/home/rokas/ino-voice/fairseq/fairseq/fairseq/tasks/audio_pretraining.py", line 201, in build_model model = super().build_model(model_cfg) File "/home/rokas/ino-voice/fairseq/fairseq/fairseq/tasks/fairseq_task.py", line 281, in build_model model = models.build_model(cfg, self) File "/home/rokas/ino-voice/fairseq/fairseq/fairseq/models/__init__.py", line 86, in build_model return model.build_model(cfg, task) File "/home/rokas/ino-voice/fairseq/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py", line 168, in build_model w2v_encoder = Wav2VecEncoder(args, task.target_dictionary) File "/home/rokas/ino-voice/fairseq/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py", line 350, in __init__ model = task.build_model(w2v_args.model) File "/home/rokas/ino-voice/fairseq/fairseq/fairseq/tasks/audio_pretraining.py", line 201, in build_model model = super().build_model(model_cfg) File "/home/rokas/ino-voice/fairseq/fairseq/fairseq/tasks/fairseq_task.py", line 281, in build_model model = models.build_model(cfg, self) File "/home/rokas/ino-voice/fairseq/fairseq/fairseq/models/__init__.py", line 86, in build_model return model.build_model(cfg, task) File "/home/rokas/ino-voice/fairseq/fairseq/fairseq/models/wav2vec/wav2vec2.py", line 331, in build_model return cls(cfg) File "/home/rokas/ino-voice/fairseq/fairseq/fairseq/models/wav2vec/wav2vec2.py", line 284, in __init__ time_first=True, File "/home/rokas/ino-voice/fairseq/fairseq/fairseq/modules/gumbel_vector_quantizer.py", line 76, in __init__ assert len(temp) == 3, temp
		</comment>
		<comment id='2' author='alealv' date='2020-11-16T15:20:58Z'>
		&lt;denchmark-link:https://github.com/alexeib&gt;@alexeib&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='alealv' date='2020-11-16T15:45:23Z'>
		&lt;denchmark-link:https://github.com/alealv&gt;@alealv&lt;/denchmark-link&gt;
 i cant repro this with the published checkpoint or a newly trained model on master; it shouldnt happen because of this:
&lt;denchmark-link:https://github.com/pytorch/fairseq/blame/0a848245f3e00ee39a68fddf54f738de11dd8cc8/fairseq/models/wav2vec/wav2vec2_asr.py#L335&gt;https://github.com/pytorch/fairseq/blame/0a848245f3e00ee39a68fddf54f738de11dd8cc8/fairseq/models/wav2vec/wav2vec2_asr.py#L335&lt;/denchmark-link&gt;

however i did hit this error as well when trying to load a model from a different script without going through wav2vec_asr models with one of the pre-hydra models (e.g. all the published ones) when doing something like this:
w2v_args = state["args"]
task = fairseq.tasks.setup_task(w2v_args)
model = task.build_model(w2v_args)
this is getting fixed later today
		</comment>
		<comment id='4' author='alealv' date='2020-11-16T16:36:06Z'>
		I've just debugged those lines, and it never enters the if case that you've mentioned. So w2v_args is not None.
I think this is set by the model, right? I'm using wav2vec_vox_new.pt
		</comment>
		<comment id='5' author='alealv' date='2020-11-16T18:08:02Z'>
		that means the model was trained after hydra migration and so it has "cfg" in the checkpoint. i am not sure why your copy of wav2vec_vox_new.pt has cfg in the checkpoint. here's mine:
&lt;denchmark-code&gt;&gt;&gt;&gt; import torch
&gt;&gt;&gt; cp = torch.load('/private/home/abaevski/models/wav2vec2/wav2vec_vox_new.pt', map_location='cpu')
&gt;&gt;&gt; cp.keys()
dict_keys(['args', 'model', 'optimizer_history', 'extra_state', 'last_optimizer_state'])
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='alealv' date='2020-11-16T19:15:06Z'>
		Me neither. I'm confused.
‚ûú python
Python 3.8.5 (default, Jul 28 2020, 12:59:40) 
[GCC 9.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; import torch
&gt;&gt;&gt; cp = torch.load("/mnt/data/ale/models/wav2vec_vox_new.pt")
&gt;&gt;&gt; cp.keys()
dict_keys(['args', 'model', 'optimizer_history', 'extra_state', 'last_optimizer_state'])
		</comment>
		<comment id='7' author='alealv' date='2020-11-16T19:22:15Z'>
		I still have the error after rebasing to &lt;denchmark-link:https://github.com/pytorch/fairseq/commit/52d774cda9e8926ab42210da05715789fc567d8e&gt;52d774c&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='alealv' date='2020-11-16T19:27:06Z'>
		which one
btw i ran the command you have under "to reproduce" (except without --wer-args) and it works for me..
		</comment>
		<comment id='9' author='alealv' date='2020-11-17T09:36:24Z'>
		
which one

The first one

btw i ran the command you have under "to reproduce" (except without --wer-args) and it works for me..

I also ran it again without --wer-args and is still failing.

@alealv i cant repro this with the published checkpoint or a newly trained model on master; it shouldnt happen because of this:
https://github.com/pytorch/fairseq/blame/0a848245f3e00ee39a68fddf54f738de11dd8cc8/fairseq/models/wav2vec/wav2vec2_asr.py#L335

I realized that the error is thrown at line 330, so it's before reaching the line that you mention:
&lt;denchmark-code&gt;  File "/home/aalvarez/Projects/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py", line 330, in __init__
    state = checkpoint_utils.load_checkpoint_to_cpu(
&lt;/denchmark-code&gt;

I've been debugging a little further. The error is throw in this section:
512                     try:                                                                  
513                         OmegaConf.update(cfg, key, value, merge=True)                     
514                     except (ConfigAttributeError, ConfigKeyError) as ex:                  
515                         raise ConfigCompositionException(                                 
516                             f"Could not override '{override.key_or_group}'."              
517                             f"\nTo append to your config use +{override.input_line}"      
518                         ) from ex                                                         
519             except OmegaConfBaseException as ex:                                          
520                 print(cfg)
521                 print(f"key: {key}\nval:{value}")                                 
522                 raise ConfigCompositionException(
523                     f"Error merging override {override.input_line}"
524                 ) from ex
I confirm that the error happens because of different types as expected:
&lt;denchmark-code&gt;omegaconf.errors.ValidationError: Invalid value assigned : str is not a subclass of ListConfig or list.
	full_key: model.latent_temp
	reference_type=Optional[Wav2Vec2Config]
	object_type=Wav2Vec2Config
&lt;/denchmark-code&gt;


The model has a value:  latent_temp="(2.0,0.1,0.999995)", ====&gt; string of tuples
Hydra cfg variable has the value: "latent_temp": [2.0, 0.5, 0.999995], ====&gt; list

When I changed to list it disappeared.
		</comment>
		<comment id='10' author='alealv' date='2020-11-17T19:08:32Z'>
		that should already be taken care of:
this evaluates strings that are declared as something else, so you should end up with a tuple value:
&lt;denchmark-link:https://github.com/pytorch/fairseq/blob/master/fairseq/dataclass/utils.py#L216&gt;https://github.com/pytorch/fairseq/blob/master/fairseq/dataclass/utils.py#L216&lt;/denchmark-link&gt;

then, (because hydra doesnt understand tuples), we convert tuple to list here:
&lt;denchmark-link:https://github.com/pytorch/fairseq/blob/master/fairseq/dataclass/utils.py#L218&gt;https://github.com/pytorch/fairseq/blob/master/fairseq/dataclass/utils.py#L218&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='alealv' date='2020-11-23T12:09:41Z'>
		Sorry for the late reply. I've debugged the issue more deeply taking into account the lines that you mention

https://github.com/pytorch/fairseq/blob/master/fairseq/dataclass/utils.py#L216
https://github.com/pytorch/fairseq/blob/master/fairseq/dataclass/utils.py#L218

I inserted this code after &lt;denchmark-link:https://github.com/pytorch/fairseq/blob/master/fairseq/dataclass/utils.py#L216&gt;line 216&lt;/denchmark-link&gt;

        if k == "latent_temp":
            logger.info(f"key: {k}")
            logger.info(f"v: {v}")
            logger.info(f"v type: {type(v.type)}")
            logger.info(f"val: {val}")
            logger.info(f"val type: {type(val)}")
            logger.info(f"field_type: {field_type}")
            logger.info(f"inspect.isclass(field_type): {inspect.isclass(field_type)}")
            logger.info(f"issublclass: {issubclass(field_type, Enum)}")
            logger.info("\n\n")
And got this output:
2020-11-23 13:04:24 | INFO | fairseq.dataclass.utils | key: latent_temp
2020-11-23 13:04:24 | INFO | fairseq.dataclass.utils | v: Field(name='latent_temp',type=typing.Tuple[float, float, float],default=(2, 0.5, 0.999995),default_factory=&lt;dataclasses._MISSING_TYPE object at 0x7f8febd9eb50&gt;,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'help': 'temperature for latent variable sampling. can be tuple of 3 values (start, end, decay)'}),_field_type=_FIELD)
2020-11-23 13:04:24 | INFO | fairseq.dataclass.utils | v.type: typing.Tuple[float, float, float]
2020-11-23 13:04:24 | INFO | fairseq.dataclass.utils | val: (2.0,0.1,0.999995)
2020-11-23 13:04:24 | INFO | fairseq.dataclass.utils | type(val): &lt;class 'str'&gt;
2020-11-23 13:04:24 | INFO | fairseq.dataclass.utils | field_type: typing.Tuple[float, float, float]
2020-11-23 13:04:24 | INFO | fairseq.dataclass.utils | inspect.isclass(field_type): False
Traceback (most recent call last):
  File "/home/aalvarez/Projects/fairseq/fairseq_cli/hydra_train.py", line 70, in &lt;module&gt;
    cli_main()
  File "/home/aalvarez/Projects/fairseq/fairseq_cli/hydra_train.py", line 66, in cli_main
    hydra_main()
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/hydra/main.py", line 32, in decorated_main
    _run_hydra(
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/hydra/_internal/utils.py", line 346, in _run_hydra
    run_and_report(
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/hydra/_internal/utils.py", line 201, in run_and_report
    raise ex
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/hydra/_internal/utils.py", line 198, in run_and_report
    return func()
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/hydra/_internal/utils.py", line 347, in &lt;lambda&gt;
    lambda: hydra.run(
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 107, in run
    return run_job(
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/hydra/core/utils.py", line 125, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/aalvarez/Projects/fairseq/fairseq_cli/hydra_train.py", line 38, in hydra_main
    distributed_utils.call_main(cfg, pre_main)
  File "/home/aalvarez/Projects/fairseq/fairseq/distributed_utils.py", line 313, in call_main
    torch.multiprocessing.spawn(
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 199, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 157, in start_processes
    while not context.join():
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 118, in join
    raise Exception(msg)
Exception: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 19, in _wrap
    fn(i, *args)
  File "/home/aalvarez/Projects/fairseq/fairseq/distributed_utils.py", line 300, in distributed_main
    main(cfg, **kwargs)
  File "/home/aalvarez/Projects/fairseq/fairseq_cli/train.py", line 74, in main
    model = task.build_model(cfg.model)
  File "/home/aalvarez/Projects/fairseq/fairseq/tasks/audio_pretraining.py", line 201, in build_model
    model = super().build_model(model_cfg)
  File "/home/aalvarez/Projects/fairseq/fairseq/tasks/fairseq_task.py", line 282, in build_model
    model = models.build_model(cfg, self)
  File "/home/aalvarez/Projects/fairseq/fairseq/models/__init__.py", line 86, in build_model
    return model.build_model(cfg, task)
  File "/home/aalvarez/Projects/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py", line 147, in build_model
    w2v_encoder = Wav2VecEncoder(cfg, task.target_dictionary)
  File "/home/aalvarez/Projects/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py", line 281, in __init__
    state = checkpoint_utils.load_checkpoint_to_cpu(cfg.w2v_path, arg_overrides)
  File "/home/aalvarez/Projects/fairseq/fairseq/checkpoint_utils.py", line 237, in load_checkpoint_to_cpu
    state = _upgrade_state_dict(state)
  File "/home/aalvarez/Projects/fairseq/fairseq/checkpoint_utils.py", line 468, in _upgrade_state_dict
    state["cfg"] = convert_namespace_to_omegaconf(state["args"])
  File "/home/aalvarez/Projects/fairseq/fairseq/dataclass/utils.py", line 348, in convert_namespace_to_omegaconf
    overrides, deletes = override_module_args(args)
  File "/home/aalvarez/Projects/fairseq/fairseq/dataclass/utils.py", line 336, in override_module_args
    overrides.extend(_override_attr("model", dc, args))
  File "/home/aalvarez/Projects/fairseq/fairseq/dataclass/utils.py", line 225, in _override_attr
    logger.info(f"issublclass: {issubclass(field_type, Enum)}")
TypeError: issubclass() arg 1 must be a class
So I understand that it should be entering to the &lt;denchmark-link:https://github.com/pytorch/fairseq/blob/master/fairseq/dataclass/utils.py#L217&gt;if sentence&lt;/denchmark-link&gt;
, but an exception is happening at &lt;denchmark-link:https://github.com/pytorch/fairseq/blob/master/fairseq/dataclass/utils.py#L222&gt;line 222&lt;/denchmark-link&gt;
 and that's why it's not converted to a  and then to a .
&lt;denchmark-h:h3&gt;Reproduce error&lt;/denchmark-h&gt;

&gt;&gt;&gt; from enum import Enum
&gt;&gt;&gt; from typing import Tuple
&gt;&gt;&gt; a = Tuple[float, float, float]
&gt;&gt;&gt; a
typing.Tuple[float, float, float]
&gt;&gt;&gt; type(a)
&lt;class 'typing._GenericAlias'&gt;
&gt;&gt;&gt; issubclass(a, Enum)
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
TypeError: issubclass() arg 1 must be a class
		</comment>
		<comment id='12' author='alealv' date='2020-11-23T18:44:42Z'>
		ok, our outputs are different. what verison of python are you on?
Python 3.6.10 |Anaconda, Inc.| (default, May  8 2020, 02:54:21)
[GCC 7.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; from enum import Enum
&gt;&gt;&gt; from typing import Tuple
&gt;&gt;&gt; a = Tuple[float, float, float]
&gt;&gt;&gt; a
typing.Tuple[float, float, float]
&gt;&gt;&gt; type(a)
&lt;class 'typing.TupleMeta'&gt;
&gt;&gt;&gt; issubclass(a, Enum)
False
		</comment>
		<comment id='13' author='alealv' date='2020-11-23T18:46:37Z'>
		i can repro with 3.7:
Python 3.7.9 (default, Aug 31 2020, 12:42:55)
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; from enum import Enum
&gt;&gt;&gt; from typing import Tuple
&gt;&gt;&gt; a = Tuple[float, float, float]
&gt;&gt;&gt; a
typing.Tuple[float, float, float]
&gt;&gt;&gt; type(a)
&lt;class 'typing._GenericAlias'&gt;
&gt;&gt;&gt; issubclass(a, Enum)
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
TypeError: issubclass() arg 1 must be a class
&gt;&gt;&gt; import inspect
&gt;&gt;&gt; inspect.isclass(a)
False
&gt;&gt;&gt;
		</comment>
		<comment id='14' author='alealv' date='2020-11-23T18:50:38Z'>
		i guess it can be fixed by changing the last condition to something like:
and (not inspect.isclass(field_type) or not issubclass(field_type, Enum))  # not choices enum
``` ?
		</comment>
	</comments>
</bug>