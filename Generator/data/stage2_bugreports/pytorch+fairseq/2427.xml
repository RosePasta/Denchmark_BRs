<bug id='2427' author='blmoistawinde' open_date='2020-08-05T03:00:12Z' closed_time='2020-08-17T18:21:31Z'>
	<summary>cannot import name 'libbleu', encountered when running torch hub translation example</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I am witnessing , when running this &lt;denchmark-link:https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/pytorch_fairseq_translation.ipynb&gt;torch hub translation example on Google Colab&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

run the second code snippet of the &lt;denchmark-link:https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/pytorch_fairseq_translation.ipynb&gt;notebook&lt;/denchmark-link&gt;
. And the error occurs at the  line.
Error message
&lt;denchmark-link:https://user-images.githubusercontent.com/32953014/89366784-20504e00-d70a-11ea-96de-a59b077628c6.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


fairseq Version: 0.9.0 in torch.hub
PyTorch Version (e.g., 1.0): torch==1.6.0+cu101
OS (e.g., Linux): Linux
How you installed fairseq (pip, source): in torch.hub

	</description>
	<comments>
		<comment id='1' author='blmoistawinde' date='2020-08-05T06:27:28Z'>
		have you tried running "pip install --editable ." like the error message says ?
		</comment>
		<comment id='2' author='blmoistawinde' date='2020-08-05T07:40:06Z'>
		
have you tried running "pip install --editable ." like the error message says ?

I have run "pip install --editable ."  successfully, but still get the same error ):
		</comment>
		<comment id='3' author='blmoistawinde' date='2020-08-05T08:07:51Z'>
		I meet the same problem  ):
		</comment>
		<comment id='4' author='blmoistawinde' date='2020-08-05T08:45:27Z'>
		A temporary solution found

on colab:

clone this repo and pip install --editable .
restart the kernel


on Linux server:

copy the files from the git cloned fairseq, and replace the files in torch.hub cache, with directory look like /home/{username}/.cache/torch/hub/pytorch_fairseq_master



Though the solution works, the existence of problem is obvious. I think the problem comes from the  version that torch.hub uses (&lt;denchmark-link:https://github.com/pytorch/fairseq/archive/master.zip&gt;https://github.com/pytorch/fairseq/archive/master.zip&lt;/denchmark-link&gt;
). Hope that can be solved.
Below is how I found the problem.
&lt;denchmark-h:h2&gt;Colab test&lt;/denchmark-h&gt;

Tried with pip install --editable, the error was not solved, and another error ModuleNotFoundError: No module named 'fairseq.hub_utils' occurred for the previous code
&lt;denchmark-link:https://user-images.githubusercontent.com/32953014/89386695-fe6bc100-d733-11ea-8352-713ddbf69c29.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/32953014/89386731-0d527380-d734-11ea-97d9-5bf25de1b37c.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/32953014/89386764-1d6a5300-d734-11ea-9b1d-eead81bc1504.png&gt;&lt;/denchmark-link&gt;

Suprisingly, when I restart the kernel. The errors are gone.
&lt;denchmark-link:https://user-images.githubusercontent.com/32953014/89388391-8a7ee800-d736-11ea-9acb-933b10a99961.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;another test&lt;/denchmark-h&gt;

However, the "solution" on Colab still can't help me solve my previous similar problem on my Linux server.
The situation on my server is: I can use torch.hub.load normally in my python cmdline. However, when I run python xx.py in bash that contains the torch.hub.load line, it will fail.
Steps:


The test environment is ipython


run python backtranslation/back_translation_server.py  that contains torch.hub.load('pytorch/fairseq','transformer.wmt16.en-de', tokenizer='moses', bpe='subword_nmt')


&lt;denchmark-code&gt;IPython 7.16.1 -- An enhanced Interactive Python. Type '?' for help.

In [1]: !python backtranslation/back_translation_server.py                                                                    
Torch version: 1.5.0
Starting to load English to German Translation Model:
Using cache found in /home/XXX/.cache/torch/hub/pytorch_fairseq_master
ERROR: missing libbleu.so. run `pip install --editable .`
Traceback (most recent call last):
  File "backtranslation/back_translation_server.py", line 13, in &lt;module&gt;
    en2de = torch.hub.load('pytorch/fairseq','transformer.wmt16.en-de', tokenizer='moses', bpe='subword_nmt')
  File "/home/XXX/.conda/envs/synqg/lib/python3.6/site-packages/torch/hub.py", line 365, in load
    hub_module = import_module(MODULE_HUBCONF, repo_dir + '/' + MODULE_HUBCONF)
  File "/home/XXX/.conda/envs/synqg/lib/python3.6/site-packages/torch/hub.py", line 75, in import_module
    spec.loader.exec_module(module)
  File "&lt;frozen importlib._bootstrap_external&gt;", line 678, in exec_module
  File "&lt;frozen importlib._bootstrap&gt;", line 219, in _call_with_frames_removed
  File "/home/XXX/.cache/torch/hub/pytorch_fairseq_master/hubconf.py", line 8, in &lt;module&gt;
    from fairseq.hub_utils import BPEHubInterface as bpe  # noqa
  File "/home/XXX/.cache/torch/hub/pytorch_fairseq_master/fairseq/__init__.py", line 18, in &lt;module&gt;
    import fairseq.models  # noqa
  File "/home/XXX/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/__init__.py", line 132, in &lt;module&gt;
    module = importlib.import_module('fairseq.models.' + model_name)
  File "/home/XXX/.conda/envs/synqg/lib/python3.6/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/home/XXX/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/transformer_from_pretrained_xlm.py", line 12, in &lt;module&gt;
    from fairseq.models.transformer import (
  File "/home/XXX/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/transformer.py", line 11, in &lt;module&gt;
    from fairseq import options, utils
  File "/home/XXX/.cache/torch/hub/pytorch_fairseq_master/fairseq/options.py", line 12, in &lt;module&gt;
    from fairseq import scoring, utils
  File "/home/XXX/.cache/torch/hub/pytorch_fairseq_master/fairseq/scoring/__init__.py", line 22, in &lt;module&gt;
    importlib.import_module("fairseq.scoring." + module)
  File "/home/XXX/.conda/envs/synqg/lib/python3.6/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/home/XXX/.cache/torch/hub/pytorch_fairseq_master/fairseq/scoring/bleu.py", line 18, in &lt;module&gt;
    raise e
  File "/home/XXX/.cache/torch/hub/pytorch_fairseq_master/fairseq/scoring/bleu.py", line 13, in &lt;module&gt;
    from fairseq import libbleu
ImportError: cannot import name 'libbleu'
&lt;/denchmark-code&gt;


However, I can run this line without any problem directly in ipython with the same env

&lt;denchmark-code&gt;In [2]: from fairseq import libbleu                                                                                           

In [3]: import torch                                                                                                          

In [4]: torch.hub.list('pytorch/fairseq')                                                                                     
Using cache found in /home/XXX/.cache/torch/hub/pytorch_fairseq_master
Out[4]: 
['bart.base',
 'bart.large',
 'bart.large.cnn',
 'bart.large.mnli',
 'bart.large.xsum',
 'bpe',
 'camembert',
 'camembert-base',
 'camembert-base-ccnet',
 'camembert-base-ccnet-4gb',
 'camembert-base-oscar-4gb',
 'camembert-base-wikipedia-4gb',
 'camembert-large',
 'camembert.v0',
 'conv.stories',
 'conv.stories.pretrained',
 'conv.wmt14.en-de',
 'conv.wmt14.en-fr',
 'conv.wmt17.en-de',
 'data.stories',
 'dynamicconv.glu.wmt14.en-fr',
 'dynamicconv.glu.wmt16.en-de',
 'dynamicconv.glu.wmt17.en-de',
 'dynamicconv.glu.wmt17.zh-en',
 'dynamicconv.no_glu.iwslt14.de-en',
 'dynamicconv.no_glu.wmt16.en-de',
 'lightconv.glu.wmt14.en-fr',
 'lightconv.glu.wmt16.en-de',
 'lightconv.glu.wmt17.en-de',
 'lightconv.glu.wmt17.zh-en',
 'lightconv.no_glu.iwslt14.de-en',
 'lightconv.no_glu.wmt16.en-de',
 'roberta.base',
 'roberta.large',
 'roberta.large.mnli',
 'roberta.large.wsc',
 'tokenizer',
 'transformer.wmt14.en-fr',
 'transformer.wmt16.en-de',
 'transformer.wmt18.en-de',
 'transformer.wmt19.de-en',
 'transformer.wmt19.de-en.single_model',
 'transformer.wmt19.en-de',
 'transformer.wmt19.en-de.single_model',
 'transformer.wmt19.en-ru',
 'transformer.wmt19.en-ru.single_model',
 'transformer.wmt19.ru-en',
 'transformer.wmt19.ru-en.single_model',
 'transformer_lm.gbw.adaptive_huge',
 'transformer_lm.wiki103.adaptive',
 'transformer_lm.wmt19.de',
 'transformer_lm.wmt19.en',
 'transformer_lm.wmt19.ru',
 'xlmr.base',
 'xlmr.large']

In [8]: en2de = torch.hub.load('pytorch/fairseq','transformer.wmt16.en-de', tokenizer='moses', bpe='subword_nmt')             
Using cache found in /home/XXX/.cache/torch/hub/pytorch_fairseq_master
  0%|                                                                       | 1029120/2193287384 [00:05&lt;3:21:40, 181167.62B/s
&lt;/denchmark-code&gt;

On my server, fairseq was install by pip install --editable ., cloned from the master branch.
I notice torch.hub uses a separatedly downloaded fairseq.
&lt;denchmark-code&gt;Downloading: "https://github.com/pytorch/fairseq/archive/master.zip" to /home/XXX/.cache/torch/hub/pytorch_fairseq_master
&lt;/denchmark-code&gt;

Then I copy the files from the git cloned fairseq, and replace the files in torch.hub cache /home/XXX/.cache/torch/hub/pytorch_fairseq_master, and the problem is gone!
		</comment>
		<comment id='5' author='blmoistawinde' date='2020-08-05T09:13:13Z'>
		
A temporary solution found


on colab:

clone this repo and pip install --editable .
restart the kernel



on Linux server:

copy the files from the git cloned fairseq, and replace the files in torch.hub cache, with directory look like /home/{username}/.cache/torch/hub/pytorch_fairseq_master



Though the solution works, the existence of problem is obvious. I think the problem comes from the fairseq version that torch.hub uses (https://github.com/pytorch/fairseq/archive/master.zip). Hope that can be solved.
Below is how I found the problem.
Colab test
Tried with pip install --editable, the error was not solved, and another error ModuleNotFoundError: No module named 'fairseq.hub_utils' occurred for the previous code



Suprisingly, when I restart the kernel. The errors are gone.

another test
However, the "solution" on Colab still can't help me solve my previous similar problem on my Linux server.
The situation on my server is: I can use torch.hub.load normally in my python cmdline. However, when I run python xx.py in bash that contains the torch.hub.load line, it will fail.
Steps:

The test environment is ipython
run python backtranslation/back_translation_server.py  that contains torch.hub.load('pytorch/fairseq','transformer.wmt16.en-de', tokenizer='moses', bpe='subword_nmt')

IPython 7.16.1 -- An enhanced Interactive Python. Type '?' for help.

In [1]: !python backtranslation/back_translation_server.py                                                                    
Torch version: 1.5.0
Starting to load English to German Translation Model:
Using cache found in /home/XXX/.cache/torch/hub/pytorch_fairseq_master
ERROR: missing libbleu.so. run `pip install --editable .`
Traceback (most recent call last):
  File "backtranslation/back_translation_server.py", line 13, in &lt;module&gt;
    en2de = torch.hub.load('pytorch/fairseq','transformer.wmt16.en-de', tokenizer='moses', bpe='subword_nmt')
  File "/home/XXX/.conda/envs/synqg/lib/python3.6/site-packages/torch/hub.py", line 365, in load
    hub_module = import_module(MODULE_HUBCONF, repo_dir + '/' + MODULE_HUBCONF)
  File "/home/XXX/.conda/envs/synqg/lib/python3.6/site-packages/torch/hub.py", line 75, in import_module
    spec.loader.exec_module(module)
  File "&lt;frozen importlib._bootstrap_external&gt;", line 678, in exec_module
  File "&lt;frozen importlib._bootstrap&gt;", line 219, in _call_with_frames_removed
  File "/home/XXX/.cache/torch/hub/pytorch_fairseq_master/hubconf.py", line 8, in &lt;module&gt;
    from fairseq.hub_utils import BPEHubInterface as bpe  # noqa
  File "/home/XXX/.cache/torch/hub/pytorch_fairseq_master/fairseq/__init__.py", line 18, in &lt;module&gt;
    import fairseq.models  # noqa
  File "/home/XXX/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/__init__.py", line 132, in &lt;module&gt;
    module = importlib.import_module('fairseq.models.' + model_name)
  File "/home/XXX/.conda/envs/synqg/lib/python3.6/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/home/XXX/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/transformer_from_pretrained_xlm.py", line 12, in &lt;module&gt;
    from fairseq.models.transformer import (
  File "/home/XXX/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/transformer.py", line 11, in &lt;module&gt;
    from fairseq import options, utils
  File "/home/XXX/.cache/torch/hub/pytorch_fairseq_master/fairseq/options.py", line 12, in &lt;module&gt;
    from fairseq import scoring, utils
  File "/home/XXX/.cache/torch/hub/pytorch_fairseq_master/fairseq/scoring/__init__.py", line 22, in &lt;module&gt;
    importlib.import_module("fairseq.scoring." + module)
  File "/home/XXX/.conda/envs/synqg/lib/python3.6/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/home/XXX/.cache/torch/hub/pytorch_fairseq_master/fairseq/scoring/bleu.py", line 18, in &lt;module&gt;
    raise e
  File "/home/XXX/.cache/torch/hub/pytorch_fairseq_master/fairseq/scoring/bleu.py", line 13, in &lt;module&gt;
    from fairseq import libbleu
ImportError: cannot import name 'libbleu'


However, I can run this line without any problem directly in ipython with the same env

In [2]: from fairseq import libbleu                                                                                           

In [3]: import torch                                                                                                          

In [4]: torch.hub.list('pytorch/fairseq')                                                                                     
Using cache found in /home/XXX/.cache/torch/hub/pytorch_fairseq_master
Out[4]: 
['bart.base',
 'bart.large',
 'bart.large.cnn',
 'bart.large.mnli',
 'bart.large.xsum',
 'bpe',
 'camembert',
 'camembert-base',
 'camembert-base-ccnet',
 'camembert-base-ccnet-4gb',
 'camembert-base-oscar-4gb',
 'camembert-base-wikipedia-4gb',
 'camembert-large',
 'camembert.v0',
 'conv.stories',
 'conv.stories.pretrained',
 'conv.wmt14.en-de',
 'conv.wmt14.en-fr',
 'conv.wmt17.en-de',
 'data.stories',
 'dynamicconv.glu.wmt14.en-fr',
 'dynamicconv.glu.wmt16.en-de',
 'dynamicconv.glu.wmt17.en-de',
 'dynamicconv.glu.wmt17.zh-en',
 'dynamicconv.no_glu.iwslt14.de-en',
 'dynamicconv.no_glu.wmt16.en-de',
 'lightconv.glu.wmt14.en-fr',
 'lightconv.glu.wmt16.en-de',
 'lightconv.glu.wmt17.en-de',
 'lightconv.glu.wmt17.zh-en',
 'lightconv.no_glu.iwslt14.de-en',
 'lightconv.no_glu.wmt16.en-de',
 'roberta.base',
 'roberta.large',
 'roberta.large.mnli',
 'roberta.large.wsc',
 'tokenizer',
 'transformer.wmt14.en-fr',
 'transformer.wmt16.en-de',
 'transformer.wmt18.en-de',
 'transformer.wmt19.de-en',
 'transformer.wmt19.de-en.single_model',
 'transformer.wmt19.en-de',
 'transformer.wmt19.en-de.single_model',
 'transformer.wmt19.en-ru',
 'transformer.wmt19.en-ru.single_model',
 'transformer.wmt19.ru-en',
 'transformer.wmt19.ru-en.single_model',
 'transformer_lm.gbw.adaptive_huge',
 'transformer_lm.wiki103.adaptive',
 'transformer_lm.wmt19.de',
 'transformer_lm.wmt19.en',
 'transformer_lm.wmt19.ru',
 'xlmr.base',
 'xlmr.large']

In [8]: en2de = torch.hub.load('pytorch/fairseq','transformer.wmt16.en-de', tokenizer='moses', bpe='subword_nmt')             
Using cache found in /home/XXX/.cache/torch/hub/pytorch_fairseq_master
  0%|                                                                       | 1029120/2193287384 [00:05&lt;3:21:40, 181167.62B/s

On my server, fairseq was install by pip install --editable ., cloned from the master branch.
I notice torch.hub uses a separatedly downloaded fairseq.
Downloading: "https://github.com/pytorch/fairseq/archive/master.zip" to /home/XXX/.cache/torch/hub/pytorch_fairseq_master

Then I copy the files from the git cloned fairseq, and replace the files in torch.hub cache /home/XXX/.cache/torch/hub/pytorch_fairseq_master, and the problem is gone!

thanks, it solved my problem
		</comment>
		<comment id='6' author='blmoistawinde' date='2020-08-05T09:49:17Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/23466003/89398813-2cf19800-d744-11ea-8c65-3f32ad4254ef.png&gt;&lt;/denchmark-link&gt;

another problem üòÇ
		</comment>
		<comment id='7' author='blmoistawinde' date='2020-08-17T18:21:31Z'>
		Fixed by &lt;denchmark-link:https://github.com/pytorch/fairseq/commit/6f9ed78059f67ce0bbc107f513064a931f551392&gt;6f9ed78&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>