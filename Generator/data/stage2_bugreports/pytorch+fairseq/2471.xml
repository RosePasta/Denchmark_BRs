<bug id='2471' author='ziweiji' open_date='2020-08-12T16:05:00Z' closed_time='2020-08-20T15:28:13Z'>
	<summary>fairseq-generate RuntimeError: Trying to pass too many CPU scalars to CUDA kernel!</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

RuntimeError: Trying to pass too many CPU scalars to CUDA kernel!
occur when run fairseq-generate
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior (always include the command you ran):

Run cmd '....'


CUDA_VISIBLE_DEVICES=5 fairseq-generate data-bin 
--path models/checkpoint2.pt 
--batch-size 32 --beam 1 --sampling --sampling-topk 4 --nbest 1 --temperature 0.8 
--source-lang dialog --target-lang scene 
--max-len-b 50 --min-len 1 --lenpen 1.0 
--results-path 'try' 
--no-repeat-ngram-size 4 
--skip-invalid-size-inputs-valid-test 
--remove-bpe \


See error

Traceback (most recent call last):
File "/home/jiziwei/anaconda3/bin/fairseq-generate", line 33, in 
sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-generate')())
File "/home/jiziwei/fairseq/fairseq_cli/generate.py", line 274, in cli_main
main(args)
File "/home/jiziwei/fairseq/fairseq_cli/generate.py", line 36, in main
return _main(args, h)
File "/home/jiziwei/fairseq/fairseq_cli/generate.py", line 150, in _main
hypos = task.inference_step(generator, models, sample, prefix_tokens)
File "/home/jiziwei/fairseq/fairseq/tasks/fairseq_task.py", line 361, in inference_step
return generator.generate(models, sample, prefix_tokens=prefix_tokens)
File "/home/jiziwei/anaconda3/lib/python3.6/site-packages/torch/autograd/grad_mode.py", line 15, in decorate_context
return func(*args, **kwargs)
File "/home/jiziwei/fairseq/fairseq/sequence_generator.py", line 159, in generate
return self._generate(sample, **kwargs)
File "/home/jiziwei/fairseq/fairseq/sequence_generator.py", line 314, in _generate
lprobs = self._no_repeat_ngram(tokens, lprobs, bsz, beam_size, step)
File "/home/jiziwei/fairseq/fairseq/sequence_generator.py", line 658, in _no_repeat_ngram
] = torch.tensor(-math.inf, dtype=torch.float)
RuntimeError: Trying to pass too many CPU scalars to CUDA kernel!
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


fairseq Version 0.9.0
PyTorch Version 1.6
OS Linux
How you installed fairseq source
Build command you used (if compiling from source):
Python version: 3.6
CUDA/cuDNN version: 11.0
GPU models and configuration:
Any other relevant information:

	</description>
	<comments>
		<comment id='1' author='ziweiji' date='2020-08-13T02:07:10Z'>
		I am getting it too. any resolution?
		</comment>
		<comment id='2' author='ziweiji' date='2020-08-13T07:07:27Z'>
		I replace the function  _no_repeat_ngram with another one, and it works.
refer to  &lt;denchmark-link:url&gt;https://github.com/rachel708/BartWithRL/blob/6961988e133a4ca1c5992aae38f0dada9bb9b42d/fairseq/sequence_generator.py&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;@torch.jit.unused
def _no_repeat_ngram(self, tokens, lprobs, bsz: int, beam_size: int, step: int):
        # for each beam and batch sentence, generate a list of previous ngrams
        gen_ngrams = [{} for bbsz_idx in range(bsz * beam_size)]
        cpu_tokens = tokens.cpu()
        for bbsz_idx in range(bsz * beam_size):
            gen_tokens = cpu_tokens[bbsz_idx].tolist()
            for ngram in zip(
                *[gen_tokens[i:] for i in range(self.no_repeat_ngram_size)]
            ):
                if ngram[-1] != self.pad:
                    gen_ngrams[bbsz_idx][tuple(ngram[:-1])] = gen_ngrams[bbsz_idx].get(
                        tuple(ngram[:-1]), []
                    ) + [ngram[-1]]


        def calculate_banned_tokens(bbsz_idx):
            # before decoding the next token, prevent decoding of ngrams that have already appeared
            ngram_index = tuple(
                cpu_tokens[
                    bbsz_idx, step + 2 - self.no_repeat_ngram_size : step + 1
                ].tolist()
            )
            banned_tokens_per_sample = gen_ngrams[bbsz_idx].get(ngram_index, [])
            banned_tokens_per_sample = [(bbsz_idx, t) for t in banned_tokens_per_sample]
            return banned_tokens_per_sample

        banned_tokens = []
        if step + 2 - self.no_repeat_ngram_size &gt;= 0:
            # no banned tokens if we haven't generated no_repeat_ngram_size tokens yet
            for bbsz_idx in range(bsz * beam_size):
                banned_tokens.extend(calculate_banned_tokens(bbsz_idx))
        
        if banned_tokens:
            banned_tokens = torch.LongTensor(banned_tokens)
            lprobs.index_put_(
                tuple(banned_tokens.t()),
                lprobs.new_tensor([-math.inf] * len(banned_tokens)),
            )
        return lprobs
&lt;/denchmark-code&gt;

But I still don't know how to correct, and why the error occur.
&lt;denchmark-code&gt;for bbsz_idx in range(bsz * beam_size):
            lprobs[bbsz_idx][torch.tensor(banned_tokens[bbsz_idx]).long()] = torch.tensor(-math.inf, dtype=torch.float)
&lt;/denchmark-code&gt;

If anyone has some idea, please tell me.
		</comment>
		<comment id='3' author='ziweiji' date='2020-08-14T17:16:15Z'>
		down grade to torch 1.4.0 worked for me.
		</comment>
		<comment id='4' author='ziweiji' date='2020-08-18T08:34:14Z'>
		
down grade to torch 1.4.0 worked for me.

it not work for me
		</comment>
		<comment id='5' author='ziweiji' date='2020-08-20T15:28:13Z'>
		This should be fixed. Please reopen if it's still an issue.
		</comment>
	</comments>
</bug>