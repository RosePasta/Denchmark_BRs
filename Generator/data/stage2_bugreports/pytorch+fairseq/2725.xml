<bug id='2725' author='turian' open_date='2020-10-12T20:01:51Z' closed_time='2020-11-30T07:05:46Z'>
	<summary>Error(s) in loading state_dict for Wav2VecModel:</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I receive message 'Error(s) in loading state_dict for Wav2VecModel:' with wav2vec_small.pt
I also tried wav2vec_big_960h.pt and got a similar error.
I have searched issues but don't find any info.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

I follow the example code &lt;denchmark-link:https://github.com/pytorch/fairseq/tree/master/examples/wav2vec&gt;here&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;wget -c https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_small.pt
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;import torch
from fairseq.models.wav2vec import Wav2VecModel

cp = torch.load('wav2vec_small.pt')
model = Wav2VecModel.build_model(cp['args'], task=None)
model.load_state_dict(cp['model'])
model.eval()
&lt;/denchmark-code&gt;

Here is my stack trace
&lt;denchmark-code&gt;RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-9-8b7e23d59ca9&gt; in &lt;module&gt;()
      4 cp = torch.load('wav2vec_small.pt')
      5 model = Wav2VecModel.build_model(cp['args'], task=None)
----&gt; 6 model.load_state_dict(cp['model'])
      7 model.eval()

1 frames
/content/fairseq/fairseq/models/fairseq_model.py in load_state_dict(self, state_dict, strict, args)
     94         self.upgrade_state_dict(state_dict)
     95         new_state_dict = prune_state_dict(state_dict, args)
---&gt; 96         return super().load_state_dict(new_state_dict, strict)
     97 
     98     def upgrade_state_dict(self, state_dict):

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in load_state_dict(self, state_dict, strict)
   1043         if len(error_msgs) &gt; 0:
   1044             raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
-&gt; 1045                                self.__class__.__name__, "\n\t".join(error_msgs)))
   1046         return _IncompatibleKeys(missing_keys, unexpected_keys)
   1047 

RuntimeError: Error(s) in loading state_dict for Wav2VecModel:
	Missing key(s) in state_dict: "feature_extractor.conv_layers.1.2.weight", "feature_extractor.conv_layers.1.2.bias", "feature_extractor.conv_layers.2.2.weight", "feature_extractor.conv_layers.2.2.bias", "feature_extractor.conv_layers.3.2.weight", "feature_extractor.conv_layers.3.2.bias", "feature_extractor.conv_layers.4.2.weight", "feature_extractor.conv_layers.4.2.bias", "feature_extractor.conv_layers.5.2.weight", "feature_extractor.conv_layers.5.2.bias", "feature_extractor.conv_layers.6.2.weight", "feature_extractor.conv_layers.6.2.bias", "feature_aggregator.conv_layers.0.1.weight", "feature_aggregator.conv_layers.0.1.bias", "feature_aggregator.conv_layers.0.3.weight", "feature_aggregator.conv_layers.0.3.bias", "feature_aggregator.conv_layers.1.1.weight", "feature_aggregator.conv_layers.1.1.bias", "feature_aggregator.conv_layers.1.3.weight", "feature_aggregator.conv_layers.1.3.bias", "feature_aggregator.conv_layers.2.1.weight", "feature_aggregator.conv_layers.2.1.bias", "feature_aggregator.conv_layers.2.3.weight", "feature_aggregator.conv_layers.2.3.bias", "feature_aggregator.conv_layers.3.1.weight", "feature_aggregator.conv_layers.3.1.bias", "feature_aggregator.conv_layers.3.3.weight", "feature_aggregator.conv_layers.3.3.bias", "feature_aggregator.conv_layers.4.1.weight", "feature_aggregator.conv_layers.4.1.bias", "feature_aggregator.conv_layers.4.3.weight", "feature_aggregator.conv_layers.4.3.bias", "feature_aggregator.conv_layers.5.1.weight", "feature_aggregator.conv_...
	Unexpected key(s) in state_dict: "encoder.pos_conv.0.bias", "encoder.pos_conv.0.weight_g", "encoder.pos_conv.0.weight_v", "encoder.layers.0.self_attn.k_proj.weight", "encoder.layers.0.self_attn.k_proj.bias", "encoder.layers.0.self_attn.v_proj.weight", "encoder.layers.0.self_attn.v_proj.bias", "encoder.layers.0.self_attn.q_proj.weight", "encoder.layers.0.self_attn.q_proj.bias", "encoder.layers.0.self_attn.out_proj.weight", "encoder.layers.0.self_attn.out_proj.bias", "encoder.layers.0.self_attn_layer_norm.weight", "encoder.layers.0.self_attn_layer_norm.bias", "encoder.layers.0.fc1.weight", "encoder.layers.0.fc1.bias", "encoder.layers.0.fc2.weight", "encoder.layers.0.fc2.bias", "encoder.layers.0.final_layer_norm.weight", "encoder.layers.0.final_layer_norm.bias", "encoder.layers.1.self_attn.k_proj.weight", "encoder.layers.1.self_attn.k_proj.bias", "encoder.layers.1.self_attn.v_proj.weight", "encoder.layers.1.self_attn.v_proj.bias", "encoder.layers.1.self_attn.q_proj.weight", "encoder.layers.1.self_attn.q_proj.bias", "encoder.layers.1.self_attn.out_proj.weight", "encoder.layers.1.self_attn.out_proj.bias", "encoder.layers.1.self_attn_layer_norm.weight", "encoder.layers.1.self_attn_layer_norm.bias", "encoder.layers.1.fc1.weight", "encoder.layers.1.fc1.bias", "encoder.layers.1.fc2.weight", "encoder.layers.1.fc2.bias", "encoder.layers.1.final_layer_norm.weight", "encoder.layers.1.final_layer_norm.bias", "encoder.layers.2.self_attn.k_proj.weight", "encoder.layers.2.self_attn.k_proj...
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

See above
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Model should load cleanly.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

Google Colab

fairseq Version (e.g., 1.0 or master): master
PyTorch Version (e.g., 1.0): 1.6.0+cu101
OS (e.g., Linux): Google Colab (Linux)
How you installed fairseq (pip, source): source
Build command you used (if compiling from source): pip install --editable ./
Python version: 3.6.9
CUDA/cuDNN version: Unknown
GPU models and configuration: Unknown
Any other relevant information:

	</description>
	<comments>
		<comment id='1' author='turian' date='2020-10-12T20:45:46Z'>
		It seems to work if I use this code:
&lt;denchmark-code&gt;checkpoint = torch.load(fname)
args = checkpoint["args"]
model = Wav2VecModel.build_model(args, None)
#model.load_state_dict(checkpoint["model"])
model.load_state_dict(checkpoint["model"], strict=False)
&lt;/denchmark-code&gt;

which I found in here: &lt;denchmark-link:https://github.com/pytorch/fairseq/issues/2495&gt;#2495&lt;/denchmark-link&gt;

Is this correct? Could it be explained/documented? The code in the example/README.md doesn't work
		</comment>
		<comment id='2' author='turian' date='2020-10-14T08:58:57Z'>
		Hi All,
I had the same error message/issue when loading the same pre-trained model wav2vec_small_960h.pt.
I tried to load it in a finetune-ctc stage as described into example/README.md.
Looking at error messages, it seems that there is a conflict between dictionaries: the one present into the downloaded checkpoint, and the one defined into the current model.
RuntimeError: Error(s) in loading state_dict for Wav2VecCtc:
size mismatch for w2v_encoder.proj.weight: copying a param with shape torch.Size([32, 768]) from checkpoint, the shape in current model is torch.Size([512, 768]).
size mismatch for w2v_encoder.proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([512]).
The last random layer added by wav2vec_ctc model on top of the unsupervised stage, has a mismatch: 32 versus 512.
The dictionary I used is the letter one (letters plus word boundary char).
Any comments? Am I using the right pre-trained model?
Let me try the fix above &lt;denchmark-link:https://github.com/pytorch/fairseq/issues/2495&gt;#2495&lt;/denchmark-link&gt;
..
		</comment>
		<comment id='3' author='turian' date='2020-10-14T11:05:33Z'>
		w.r.t. examples/README.md guideline, ctc finetune works when I use

wav2vec_small.pt as --w2v-path, and
wav2vec_small_960h.pt as "latest checkpoint".

In practice, in the ctc finetune is like I should do some extra iteration with pre-trained model.
Before, I was using wav2vec_small_960h.pt as --w2v-path.
		</comment>
		<comment id='4' author='turian' date='2020-11-30T07:05:46Z'>
		the checkpoint you are trying to load is a wav2vec 2.0 model ("wav2vec_small.pt"). but the model you are trying to load it INTO is a wav2vec 1.0 model. try creating a wav2vec 2.0 model instead:
&lt;denchmark-code&gt;&gt;&gt;&gt; import torch
&gt;&gt;&gt; from fairseq.models.wav2vec.wav2vec2 import Wav2Vec2Model
&gt;&gt;&gt; from fairseq.dataclass.utils import convert_namespace_to_omegaconf
&gt;&gt;&gt; cp = torch.load('/private/home/abaevski/models/wav2vec2/wav2vec_small.pt', map_location='cpu')
&gt;&gt;&gt; model = Wav2Vec2Model.build_model(convert_namespace_to_omegaconf(cp['args']).model, task=None)
&gt;&gt;&gt; model.load_state_dict(cp['model'])
&lt;All keys matched successfully&gt;
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='turian' date='2020-12-05T02:58:56Z'>
		&lt;denchmark-link:https://github.com/alexeib&gt;@alexeib&lt;/denchmark-link&gt;
 correct is what I ended up doing. I was feeling confused that all the variations (1.0 model 2.0 checkpoint and 2.0 model 2.0 checkpoint) required strict=False---why doesn't 2.0 checkpoint load without strict for 2.0 model?---but mixing 1 and 2 didn't make sense to me. Sorry for not closing the issue.
If you are curious, one our pitch benchmark we found that 2.0 LV-60 no-fine-tuning with 2.0 wav2vec model actually had very similar scores as 2.0 small no-fine-tuning with 1.0 wav2vec.
		</comment>
		<comment id='6' author='turian' date='2020-12-05T03:54:08Z'>
		it is better to load it like this:
model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task(['/path/to/checkpoint.pt'])
re: results, trying using transformer layers other than the last one (you will have to modify code to do this)
		</comment>
		<comment id='7' author='turian' date='2020-12-07T20:35:54Z'>
		This is what I tried according to REAME:
&lt;denchmark-code&gt;HYDRA_FULL_ERROR=1 python fairseq_cli/hydra_train.py \
    task.data=/dataset/timit/data/ \
    model.w2v_path=/dataset/libri960_big.pt \
    distributed_training.distributed_world_size=0 \
    +optimization.update_freq='[24]' \
    --config-dir examples/wav2vec/config/finetuning/ \
    --config-name base_960h
&lt;/denchmark-code&gt;

I tried the combination of model.w2v_path=wav2vec_big_960h.pt and --config-name vox_960h or --config-name base_960h. And here is what I've got at every attempts:
&lt;denchmark-code&gt;  ...
  File "/app/fairseq/models/wav2vec/wav2vec2_asr.py", line 147, in build_model
    w2v_encoder = Wav2VecEncoder(cfg, task.target_dictionary)
  File "/app/fairseq/models/wav2vec/wav2vec2_asr.py", line 302, in __init__
    model.load_state_dict(state["model"], strict=True)
  File "/app/fairseq/models/fairseq_model.py", line 115, in load_state_dict
    return super().load_state_dict(new_state_dict, strict)
  File "/opt/bitnami/miniconda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1052, in load_state_dict
    self.__class__.__name__, "\n\t".join(error_msgs)))
RuntimeError: Error(s) in loading state_dict for Wav2VecCtc:
        size mismatch for w2v_encoder.proj.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([43, 1024]).
        size mismatch for w2v_encoder.proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([43]).
&lt;/denchmark-code&gt;

As &lt;denchmark-link:https://github.com/alexeib&gt;@alexeib&lt;/denchmark-link&gt;
 pointed out earlier, had I accidentally loaded the checkpoint to wav2vec 1.0 model? But as I can see, the firstline off logs is , so it's probably v2.0.
Or should I write a python script for it?
		</comment>
	</comments>
</bug>