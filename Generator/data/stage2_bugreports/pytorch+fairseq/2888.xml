<bug id='2888' author='manikbhandari' open_date='2020-11-12T17:25:49Z' closed_time='2020-11-12T17:34:44Z'>
	<summary>Error in generating summaries with BART fine-tuned on CNN/DM</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I was following this &lt;denchmark-link:https://github.com/pytorch/fairseq/blob/master/examples/bart/README.summarization.md&gt;tutorial&lt;/denchmark-link&gt;
 to finetune BART on the CNN/DM dataset. After finetuning, I get an error while generating the summaries (full stack trace below).
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Run the following python code after downloading the data shared in the above mentioned tutorial.

import torch
from fairseq.models.bart import BARTModel

bart = BARTModel.from_pretrained(
    'checkpoints/',
    checkpoint_file='checkpoint_best.pt',
    data_name_or_path='cnn_dm-bin'
)

bart.cuda()
bart.eval()
bart.half()
count = 1
bsz = 32
with open('cnn_dm/test.source') as source, open('cnn_dm/test.hypo', 'w') as fout:
    sline = source.readline().strip()
    slines = [sline]
    for sline in source:
        if count % bsz == 0:
            with torch.no_grad():
                hypotheses_batch = bart.sample(slines, beam=4, lenpen=2.0, max_len_b=140, min_len=55, no_repeat_ngram_size=3)

            for hypothesis in hypotheses_batch:
                fout.write(hypothesis + '\n')
                fout.flush()
            slines = []

        slines.append(sline.strip())
        count += 1
    if slines != []:
        hypotheses_batch = bart.sample(slines, beam=4, lenpen=2.0, max_len_b=140, min_len=55, no_repeat_ngram_size=3)
        for hypothesis in hypotheses_batch:
            fout.write(hypothesis + '\n')
            fout.flush()

See error

&lt;denchmark-code&gt;Traceback (most recent call last):
  File "generate.py", line 67, in &lt;module&gt;
    main()
  File "generate.py", line 26, in main
    hypotheses_batch = bart.sample(slines,
  File "/projects/tir4/users/mbhandar2/misc/fairseq/fairseq/hub_utils.py", line 130, in sample
    batched_hypos = self.generate(tokenized_sentences, beam, verbose, **kwargs)
  File "/projects/tir4/users/mbhandar2/misc/fairseq/fairseq/models/bart/hub_interface.py", line 105, in generate
    return super().generate(
  File "/projects/tir4/users/mbhandar2/misc/fairseq/fairseq/hub_utils.py", line 177, in generate
    translations = self.task.inference_step(
  File "/projects/tir4/users/mbhandar2/misc/fairseq/fairseq/tasks/fairseq_task.py", line 434, in inference_step
    return generator.generate(
  File "/projects/tir4/users/mbhandar2/anaconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 15, in decorate_context
    return func(*args, **kwargs)
  File "/projects/tir4/users/mbhandar2/misc/fairseq/fairseq/sequence_generator.py", line 177, in generate
    return self._generate(sample, **kwargs)
  File "/projects/tir4/users/mbhandar2/misc/fairseq/fairseq/sequence_generator.py", line 342, in _generate
    lprobs, tokens, scores = self._prefix_tokens(
  File "/projects/tir4/users/mbhandar2/misc/fairseq/fairseq/sequence_generator.py", line 548, in _prefix_tokens
    prefix_lprobs = lprobs.gather(-1, prefix_toks.unsqueeze(-1))
RuntimeError: Size does not match at dimension 0 expected index [128, 1] to be smaller than src [20, 50264] apart from dimension 1
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


fairseq Version: master
PyTorch Version: 1.6.0
OS: Linux
How you installed fairseq: source
Build command you used: pip install --editable ./
Python version: 3.8
CUDA/cuDNN version: 10.1
GPU models and configuration: Tesla V100

	</description>
	<comments>
		<comment id='1' author='manikbhandari' date='2020-11-12T17:34:44Z'>
		I figured the source of the bug and a hack to fix it. The bug creeps in because prefix tokens are calculated &lt;denchmark-link:https://github.com/pytorch/fairseq/blob/e607911dde205e2188d3e62dcde592a6d84b4c46/fairseq/models/bart/hub_interface.py#L101&gt;here&lt;/denchmark-link&gt;
 whereas during inference the batches are generated again &lt;denchmark-link:https://github.com/pytorch/fairseq/blob/e607911dde205e2188d3e62dcde592a6d84b4c46/fairseq/hub_utils.py#L169&gt;here&lt;/denchmark-link&gt;
 from the input list of sentences causing a mismatch in the sizes of the prefix tokens and the sizes of the input batch. The hack that I did was to calculate the prefix tokens as each batch is generated in the for loop. This seems to fix this but I'm not sure if this is the ideal fix.
		</comment>
		<comment id='2' author='manikbhandari' date='2020-12-04T08:58:45Z'>
		Hi,
Actually, I also met this problem and I modified one place in line 451 in master/fairseq/tasks/fairseq_task.py and solved the problem.



fairseq/fairseq/tasks/fairseq_task.py


         Line 451
      in
      bc4ebca






 models, sample, prefix_tokens=prefix_tokens, constraints=constraints 





Modified:
&lt;denchmark-code&gt;          `  models, sample, prefix_tokens=None, constraints=constraints`
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>