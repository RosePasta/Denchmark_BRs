<bug id='2841' author='Balidajr14' open_date='2020-11-03T20:09:24Z' closed_time='2020-11-06T18:31:14Z'>
	<summary>Problems translating and generating</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I've got an error on the parameters while running both fairseq-generateand the translate function after load a pre-trained model.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:
When I run:
self.ja2en = TransformerModel.from_pretrained(
'./checkpoints',
checkpoint_file='checkpoint_best.pt',
bpe='subword_nmt',
bpe_codes='checkpoints/codes.32000.bpe.ja'
)
ja2en.translate("Some text")
and when I run:
fairseq-generate $DATADIR --path $TRAIN/checkpoint_best.pt  
--beam 5 --nbest 2 --batch-size 512
I get this error:
Traceback (most recent call last):
File "trans_ex.py", line 28, in 
result = ft.translate(texts)
File "/root/ja2en/own_dataset/translator.py", line 33, in translate
self.class.instance = MyFairseqTranslator()
File "/root/ja2en/own_dataset/translator.py", line 45, in init
bpe_codes='checkpoints/codes.32000.bpe.ja'
File "/root/fairseq/fairseq/models/fairseq_model.py", line 280, in from_pretrained
**kwargs,
File "/root/fairseq/fairseq/hub_utils.py", line 75, in from_pretrained
arg_overrides=kwargs,
File "/root/fairseq/fairseq/checkpoint_utils.py", line 292, in load_model_ensemble_and_task
state = load_checkpoint_to_cpu(filename, arg_overrides)
File "/root/fairseq/fairseq/checkpoint_utils.py", line 242, in load_checkpoint_to_cpu
overwrite_args_by_name(state["cfg"], arg_overrides)
File "/root/fairseq/fairseq/dataclass/utils.py", line 360, in overwrite_args_by_name
overwrite_args_by_name(cfg[k], overrides)
File "/root/fairseq/fairseq/dataclass/utils.py", line 359, in overwrite_args_by_name
if isinstance(cfg[k], DictConfig):
File "/usr/local/lib/python3.6/dist-packages/omegaconf/dictconfig.py", line 313, in getitem
self._format_and_raise(key=key, value=None, cause=e)
File "/usr/local/lib/python3.6/dist-packages/omegaconf/base.py", line 101, in _format_and_raise
type_override=type_override,
File "/usr/local/lib/python3.6/dist-packages/omegaconf/_utils.py", line 694, in format_and_raise
_raise(ex, cause)
File "/usr/local/lib/python3.6/dist-packages/omegaconf/_utils.py", line 610, in _raise
raise ex  # set end OC_CAUSE=1 for full backtrace
omegaconf.errors.MissingMandatoryValue: Missing mandatory value: bpe.bpe_codes
full_key: bpe.bpe_codes
reference_type=Any
object_type=dict
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

This two commands worked for me before pulling the master on fairseq and I haven't change anything.
Thank you!
	</description>
	<comments>
		<comment id='1' author='Balidajr14' date='2020-11-04T14:06:38Z'>
		I'm actually seeing a somewhat different error, but it's likely that this is due to different commits from which we're building. It looks like within the common config, bpe is None but at the global config, bpe is sentencepiece. The sentencepiece model is always populated with the default ??? Which obviously cannot be loaded
		</comment>
	</comments>
</bug>