<bug id='1683' author='miaodl' open_date='2020-02-07T20:59:33Z' closed_time='2020-05-13T16:15:17Z'>
	<summary>using Memory-Efficiency-FP16, reload model and training cause RuntimeError: A tensor was not cuda. in fused_adam.py</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

I start to train a translation task. with arguments    "  --memory-efficient-fp16   ".
After training some epochs, I stop training and restart.
When all dataset is loaded, optimizer.step() cause error below.
"A tensor was not cuda."
File "/home/tangyue/.pyenv/versions/torch_3.6.8/lib/python3.6/site-packages/apex/multi_tensor_apply/multi_tensor_apply.py", line 30
, in call    *args)
RuntimeError: A tensor was not cuda. (multi_tensor_apply at csrc/multi_tensor_apply.cuh:60)
""

Run cmd '....'
See error

Traceback (most recent call last):
File "/home/tangyue/fairseq/fairseq_cli/train_dynamic.py", line 343, in 
cli_main()
File "/home/tangyue/fairseq/fairseq_cli/train_dynamic.py", line 335, in cli_main
nprocs=args.distributed_world_size,
File "/home/tangyue/.pyenv/versions/torch_3.6.8/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 171, in spawn
while not spawn_context.join():
File "/home/tangyue/.pyenv/versions/torch_3.6.8/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 118, in join
raise Exception(msg)
Exception:
-- Process 7 terminated with the following error:
Traceback (most recent call last):
File "/home/tangyue/.pyenv/versions/torch_3.6.8/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 19, in _wrap
fn(i, *args)
File "/home/tangyue/fairseq/fairseq_cli/train_dynamic.py", line 302, in distributed_main
main(args, init_distributed=True)
File "/home/tangyue/fairseq/fairseq_cli/train_dynamic.py", line 104, in main
train(args, trainer, task, epoch_itr)
File "/home/tangyue/.pyenv/versions/3.6.8/lib/python3.6/contextlib.py", line 52, in inner
return func(*args, **kwds)
File "/home/tangyue/fairseq/fairseq_cli/train_dynamic.py", line 203, in train
log_output = trainer.train_step(samples)
File "/home/tangyue/.pyenv/versions/3.6.8/lib/python3.6/contextlib.py", line 52, in inner
return func(*args, **kwds)
File "/home/tangyue/fairseq/fairseq/trainer.py", line 412, in train_step
raise e
File "/home/tangyue/fairseq/fairseq/trainer.py", line 376, in train_step
self.optimizer.step()
File "/home/tangyue/fairseq/fairseq/optim/fp16_optimizer.py", line 374, in step
self.wrapped_optimizer.step(closure)
File "/home/tangyue/fairseq/fairseq/optim/fairseq_optimizer.py", line 95, in step
self.optimizer.step(closure)
File "/home/tangyue/fairseq/fairseq/optim/fused_adam.py", line 283, in step
group['weight_decay'])
File "/home/tangyue/.pyenv/versions/torch_3.6.8/lib/python3.6/site-packages/apex/multi_tensor_apply/multi_tensor_apply.py", line 30
, in     *args)
RuntimeError: A tensor was not cuda. (multi_tensor_apply at csrc/multi_tensor_apply.cuh:60)
frame #0: c10::Error::Error(c10::SourceLocation, std::string const&amp;) + 0x33 (0x7fd89d4f6813 in /home/tangyue/.pyenv/versions/torch_3.
6.8/lib/python3.6/site-packages/torch/lib/libc10.so)frame &lt;denchmark-link:https://github.com/pytorch/fairseq/issues/1&gt;#1&lt;/denchmark-link&gt;
: void multi_tensor_apply&lt;4, AdamFunctor, float, float, float, float, float, float, adamMode_t, float&gt;(int, int, at::T
ensor const&amp;, std::vector&lt;std::vector&lt;at::Tensor, std::allocatorat::Tensor &gt;, std::allocator&lt;std::vector&lt;at::Tensor, std::allocatorat::Tensor &gt; &gt; &gt; const&amp;, AdamFunctor, float, float, float, float, float, float, adamMode_t, float) + 0x24c (0x7fd89b5b5e5c in /home/tangyue/.pyenv/versions/torch_3.6.8/lib/python3.6/site-packages/amp_C.cpython-36m-x86_64-linux-gnu.so)frame &lt;denchmark-link:https://github.com/pytorch/fairseq/issues/2&gt;#2&lt;/denchmark-link&gt;
: multi_tensor_adam_cuda(int, at::Tensor, std::vector&lt;std::vector&lt;at::Tensor, std::allocatorat::Tensor &gt;, std::allocator&lt;st
d::vector&lt;at::Tensor, std::allocatorat::Tensor &gt; &gt; &gt;, float, float, float, float, int, int, int, float) + 0x6d8 (0x7fd89b5b24b8 in /home/tangyue/.pyenv/versions/torch_3.6.8/lib/python3.6/site-packages/amp_C.cpython-36m-x86_64-linux-gnu.so)frame &lt;denchmark-link:https://github.com/pytorch/fairseq/issues/3&gt;#3&lt;/denchmark-link&gt;
:  + 0x22f7b (0x7fd89b571f7b in /home/tangyue/.pyenv/versions/torch_3.6.8/lib/python3.6/site-packages/amp_C
.cpython-36m-x86_64-linux-gnu.so)frame &lt;denchmark-link:https://github.com/pytorch/fairseq/issues/4&gt;#4&lt;/denchmark-link&gt;
:  + 0x2303e (0x7fd89b57203e in /home/tangyue/.pyenv/versions/torch_3.6.8/lib/python3.6/site-packages/amp_C
.cpython-36m-x86_64-linux-gnu.so)frame &lt;denchmark-link:https://github.com/pytorch/fairseq/issues/5&gt;#5&lt;/denchmark-link&gt;
:  + 0x1db21 (0x7fd89b56cb21 in /home/tangyue/.pyenv/versions/torch_3.6.8/lib/python3.6/site-packages/amp_C
.cpython-36m-x86_64-linux-gnu.so)frame &lt;denchmark-link:https://github.com/pytorch/fairseq/issues/6&gt;#6&lt;/denchmark-link&gt;
: PyCFunction_Call + 0xe9 (0x4afca9 in /home/tangyue/.pyenv/versions/torch_3.6.8/bin/python)
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


fairseq  (v0.9 master 2020/2/8):
PyTorch 1.3.1
OS ubuntu 1804
installed fairseq with editable source code
Python version: 3.6.8
CUDA version: 10.2
GPU models and configuration: 8 nvidai GTX 2080ti
nvidia/apex 0.1

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

It seems like I have fixed this bug:
if len(state) == 0:
# Exponential moving average of gradient values
state['exp_avg'] = torch.zeros_like(p.data, dtype=torch.float)
# Exponential moving average of squared gradient values
state['exp_avg_sq'] = torch.zeros_like(p.data, dtype=torch.float)
&lt;denchmark-h:h3&gt;fix here&lt;/denchmark-h&gt;

if state['exp_avg'].device!=p.device:
state['exp_avg']=state['exp_avg'].to(p.device)
if state['exp_avg_sq'].device!=p.device:
state['exp_avg_sq']=state['exp_avg_sq'].to(p.device)
	</description>
	<comments>
	</comments>
</bug>