<bug id='2483' author='joeqi0370' open_date='2020-08-15T08:52:46Z' closed_time='2020-08-25T00:36:19Z'>
	<summary>Can not train the Translator.  Buffer dtype mismatch, expected 'DTYPE_t' but got 'long'</summary>
	<description>
&lt;denchmark-h:h2&gt;What is your question?&lt;/denchmark-h&gt;

Can not train the translator
&lt;denchmark-h:h4&gt;Code&lt;/denchmark-h&gt;

The commands I runned:
fairseq-preprocess --source-lang en --target-lang de --trainpref D:\fairseq\test_translateEN-DE\data\train --validpref D:\fairseq\test_translateEN-DE\data\val --testpref D:\fairseq\test_translateEN-DE\data\test --destdir D:\fairseq\test_translateEN-DE\model.bin
#Preprocess seems ok
##########################
2020-08-15 16:38:32 | INFO | fairseq_cli.preprocess | [en] Dictionary: 24992 types
2020-08-15 16:38:33 | INFO | fairseq_cli.preprocess | [en] D:\fairseq\test_translateEN-DE\data\train.en: 10000 sents, 234434 tokens, 0.0% replaced by 
2020-08-15 16:38:33 | INFO | fairseq_cli.preprocess | [en] Dictionary: 24992 types
2020-08-15 16:38:34 | INFO | fairseq_cli.preprocess | [en] D:\fairseq\test_translateEN-DE\data\val.en: 1419 sents, 40092 tokens, 10.5% replaced by 
2020-08-15 16:38:34 | INFO | fairseq_cli.preprocess | [en] Dictionary: 24992 types
2020-08-15 16:38:34 | INFO | fairseq_cli.preprocess | [en] D:\fairseq\test_translateEN-DE\data\test.en: 1581 sents, 34996 tokens, 11.2% replaced by 
2020-08-15 16:38:34 | INFO | fairseq_cli.preprocess | [de] Dictionary: 35808 types
2020-08-15 16:38:35 | INFO | fairseq_cli.preprocess | [de] D:\fairseq\test_translateEN-DE\data\train.de: 10000 sents, 223074 tokens, 0.0% replaced by 
2020-08-15 16:38:35 | INFO | fairseq_cli.preprocess | [de] Dictionary: 35808 types
2020-08-15 16:38:36 | INFO | fairseq_cli.preprocess | [de] D:\fairseq\test_translateEN-DE\data\val.de: 1419 sents, 40031 tokens, 17.0% replaced by 
2020-08-15 16:38:36 | INFO | fairseq_cli.preprocess | [de] Dictionary: 35808 types
2020-08-15 16:38:36 | INFO | fairseq_cli.preprocess | [de] D:\fairseq\test_translateEN-DE\data\test.de: 1581 sents, 34635 tokens, 16.3% replaced by 
2020-08-15 16:38:36 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to D:\fairseq\test_translateEN-DE\model.bin
##########################
But the following command reports error
fairseq-train D:\fairseq\test_translateEN-DE\model.bin --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 --arch fconv_iwslt_de_en --save-dir D:\fairseq\test_translateEN-DE\
Error info:
##########################
2020-08-15 16:39:08 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-08-15 16:39:08 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None
2020-08-15 16:39:08 | INFO | fairseq.trainer | no existing checkpoint found D:\fairseq\test_translateEN-DE\checkpoint_last.pt
2020-08-15 16:39:08 | INFO | fairseq.trainer | loading train data for epoch 1
2020-08-15 16:39:08 | INFO | fairseq.data.data_utils | loaded 10000 examples from: D:\fairseq\test_translateEN-DE\model.bin\train.en-de.en
2020-08-15 16:39:08 | INFO | fairseq.data.data_utils | loaded 10000 examples from: D:\fairseq\test_translateEN-DE\model.bin\train.en-de.de
2020-08-15 16:39:08 | INFO | fairseq.tasks.translation | D:\fairseq\test_translateEN-DE\model.bin train en-de 10000 examples
Traceback (most recent call last):
File "D:\Python\Anaconda3\envs\NMT\Scripts\fairseq-train-script.py", line 33, in 
sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
File "d:\fairseq\fairseq\fairseq_cli\train.py", line 333, in cli_main
distributed_utils.call_main(args, main)
File "d:\fairseq\fairseq\fairseq\distributed_utils.py", line 189, in call_main
main(args, **kwargs)
File "d:\fairseq\fairseq\fairseq_cli\train.py", line 109, in main
extra_state, epoch_itr = checkpoint_utils.load_checkpoint(args, trainer)
File "d:\fairseq\fairseq\fairseq\checkpoint_utils.py", line 187, in load_checkpoint
epoch_itr = trainer.get_train_iterator(
File "d:\fairseq\fairseq\fairseq\trainer.py", line 335, in get_train_iterator
return self.task.get_batch_iterator(
File "d:\fairseq\fairseq\fairseq\tasks\fairseq_task.py", line 214, in get_batch_iterator
batch_sampler = dataset.batch_by_size(
File "d:\fairseq\fairseq\fairseq\data\fairseq_dataset.py", line 118, in batch_by_size
return data_utils.batch_by_size(
File "d:\fairseq\fairseq\fairseq\data\data_utils.py", line 256, in batch_by_size
return batch_by_size_fast(
File "fairseq\data\data_utils_fast.pyx", line 27, in fairseq.data.data_utils_fast.batch_by_size_fast
cpdef list batch_by_size_fast(
ValueError: Buffer dtype mismatch, expected 'DTYPE_t' but got 'long'
##########################
&lt;denchmark-h:h4&gt;What have you tried?&lt;/denchmark-h&gt;


I have tried to remove all &amp;amps; tokens from the texts and make sure that the lines are equal in training files.
change the --arch from fconv_iwslt_de_en to other values

&lt;denchmark-h:h4&gt;What's your environment?&lt;/denchmark-h&gt;


fairseq Version (e.g., 1.0 or master): master
PyTorch Version (e.g., 1.0)  : 1.2
OS (e.g., Linux):  Windows 10
How you installed fairseq (pip, source): install from source
Build command you used (if compiling from source):
git clone https://github.com/pytorch/fairseq
cd fairseq
python setup.py build_ext --inplace
pip install --editable ./

git clone &lt;denchmark-link:https://github.com/roy-ht/editdistance&gt;https://github.com/roy-ht/editdistance&lt;/denchmark-link&gt;

cd editdistance
pip install --editable ./

Python version: 3.8.5
CUDA/cuDNN version: cudatoolkit 10.0.130
GPU models and configuration: rank   0: capabilities =  6.1  ; total memory = 12.000 GB ; name = TITAN X (Pascal)
Any other relevant information:

	</description>
	<comments>
		<comment id='1' author='joeqi0370' date='2020-08-19T12:08:52Z'>
		same here, any update?
		</comment>
		<comment id='2' author='joeqi0370' date='2020-08-19T18:53:45Z'>
		We don't support PyTorch 1.2 (&gt;= 1.4 only) and Windows support is a bit flaky. &lt;denchmark-link:https://github.com/hungviet0304&gt;@hungviet0304&lt;/denchmark-link&gt;
 are you also on Windows?
		</comment>
		<comment id='3' author='joeqi0370' date='2020-08-19T18:58:28Z'>
		Ah, I think I know the issue though. Will submit a fix shortly.
		</comment>
		<comment id='4' author='joeqi0370' date='2020-08-20T03:24:50Z'>
		Yes, I have tried to run on Windows and this problem probably appear only on Windows. I also have tried to run on other OS - works perfectly.
		</comment>
		<comment id='5' author='joeqi0370' date='2020-08-20T03:42:41Z'>
		&lt;denchmark-link:https://github.com/myleott&gt;@myleott&lt;/denchmark-link&gt;
  oh,I also pulled the newest commit (master) and re-installed faisreq but the problem still shows up
		</comment>
		<comment id='6' author='joeqi0370' date='2020-08-20T14:28:15Z'>
		I only just merged the fix this morning (&lt;denchmark-link:https://github.com/pytorch/fairseq/commit/adbd89fd4be9e68100bf9a4ba9eed1e7fb2e4040&gt;adbd89f&lt;/denchmark-link&gt;
). Can you try again?
		</comment>
		<comment id='7' author='joeqi0370' date='2020-08-21T01:01:45Z'>
		&lt;denchmark-link:https://github.com/myleott&gt;@myleott&lt;/denchmark-link&gt;
  It seems the problem is still there.

I nuked everthing I have changed locally
git reset --hard HEAD
git clean -xffd
git pull

result output:
&lt;denchmark-code&gt;remote: Enumerating objects: 139, done.
remote: Counting objects: 100% (139/139), done.
remote: Compressing objects: 100% (49/49), done.
remote: Total 142 (delta 100), reused 126 (delta 90), pack-reused 3
Receiving objects: 100% (142/142), 73.81 KiB | 123.00 KiB/s, done.
Resolving deltas: 100% (101/101), completed with 53 local objects.
From https://github.com/pytorch/fairseq
   bd20dbda..83d701ac  master      -&gt; origin/master
 + 10608fd3...e3464e77 itr_bounds  -&gt; origin/itr_bounds  (forced update)
 * [new branch]        itr_bounds2 -&gt; origin/itr_bounds2
 * [new branch]        misc_fixes  -&gt; origin/misc_fixes
Updating bd20dbda..83d701ac
Fast-forward
 README.md                                          |   3 +
 docs/getting_started.rst                           |   3 +
 examples/constrained_decoding/README.md            | 124 +++++
 examples/constrained_decoding/normalize.py         |  26 ++
 examples/constrained_decoding/tok.py               |  31 ++
 examples/translation_moe/src/translation_moe.py    |   3 +-
 examples/wav2vec/README.md                         |  17 +-
 examples/wav2vec/libri_labels.py                   |   4 +-
 fairseq/__init__.py                                |   2 +
 fairseq/clib/libnat_cuda/edit_dist.cu              |  36 +-
 fairseq/criterions/legacy_masked_lm.py             |  18 +-
 fairseq/criterions/masked_lm.py                    |   2 +-
 fairseq/criterions/wav2vec_criterion.py            |   2 +-
 fairseq/data/audio/raw_audio_dataset.py            |   2 +-
 fairseq/data/data_utils_fast.pyx                   |  37 +-
 fairseq/data/dictionary.py                         |   2 +-
 fairseq/data/iterators.py                          |  34 +-
 fairseq/data/language_pair_dataset.py              |  16 +
 fairseq/data/token_block_utils_fast.pyx            |   4 +-
 fairseq/file_io.py                                 |  10 +
 fairseq/iterative_refinement_generator.py          |   6 +-
 .../modules/transformer_sentence_encoder_layer.py  |   6 +-
 fairseq/models/nat/levenshtein_utils.py            |   2 +-
 fairseq/models/wav2vec/wav2vec2.py                 |   3 +
 fairseq/modules/adaptive_softmax.py                |   2 +-
 fairseq/optim/fp16_optimizer.py                    |  46 +-
 fairseq/options.py                                 |   2 +
 fairseq/scoring/bleu.py                            |  24 +-
 fairseq/scoring/wer.py                             |   3 +-
 fairseq/search.py                                  | 341 +++++++++++++-
 fairseq/sequence_generator.py                      | 102 ++++-
 fairseq/tasks/fairseq_task.py                      |  11 +-
 fairseq/tasks/language_modeling.py                 |   5 +-
 fairseq/tasks/masked_lm.py                         |  11 +-
 fairseq/tasks/multilingual_translation.py          |   8 +-
 fairseq/tasks/sentence_prediction.py               |  14 +-
 fairseq/tasks/translation.py                       |  15 +-
 fairseq/tasks/translation_from_pretrained_bart.py  |   6 +-
 fairseq/tasks/translation_lev.py                   |   6 +-
 fairseq/tasks/translation_multi_simple_epoch.py    |   8 +-
 fairseq/token_generation_constraints.py            | 500 +++++++++++++++++++++
 fairseq/trainer.py                                 |  12 +-
 fairseq/utils.py                                   |   4 +-
 fairseq_cli/eval_lm.py                             |   2 +-
 fairseq_cli/generate.py                            |   6 +-
 fairseq_cli/interactive.py                         |  85 +++-
 scripts/constraints/extract.py                     |  83 ++++
 scripts/constraints/validate.py                    |  33 ++
 tests/test_constraints.py                          | 254 +++++++++++
 tests/test_fp16_optimizer.py                       |  77 ++++
 tests/test_inference_dropout.py                    |   5 +
 tests/test_train.py                                |  11 +-
 52 files changed, 1888 insertions(+), 181 deletions(-)
 create mode 100644 examples/constrained_decoding/README.md
 create mode 100755 examples/constrained_decoding/normalize.py
 create mode 100755 examples/constrained_decoding/tok.py
 create mode 100644 fairseq/token_generation_constraints.py
 create mode 100755 scripts/constraints/extract.py
 create mode 100755 scripts/constraints/validate.py
 create mode 100755 tests/test_constraints.py
 create mode 100644 tests/test_fp16_optimizer.py
&lt;/denchmark-code&gt;



reinstall the fairseq with:
pip install --editable ./


the pre-process command:
fairseq-preprocess --source-lang de --target-lang en --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test --destdir data-bin/iwslt14.tokenized.de-en


train the model again(The latest version ask me to input optimizer, so I add "--reset-optimizer" parameter)


&lt;denchmark-code&gt;Traceback (most recent call last):
  File "D:\Python\Anaconda3\envs\Fairseq\Scripts\fairseq-train-script.py", line 33, in &lt;module&gt;
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "d:\fairseq\fairseq\fairseq_cli\train.py", line 333, in cli_main
    distributed_utils.call_main(args, main)
  File "d:\fairseq\fairseq\fairseq\distributed_utils.py", line 189, in call_main
    main(args, **kwargs)
  File "d:\fairseq\fairseq\fairseq_cli\train.py", line 109, in main
    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(args, trainer)
  File "d:\fairseq\fairseq\fairseq\checkpoint_utils.py", line 163, in load_checkpoint
    extra_state = trainer.load_checkpoint(
  File "d:\fairseq\fairseq\fairseq\trainer.py", line 275, in load_checkpoint
    self._build_optimizer()
  File "d:\fairseq\fairseq\fairseq\trainer.py", line 212, in _build_optimizer
    self._optimizer = optim.build_optimizer(self.args, params)
  File "d:\fairseq\fairseq\fairseq\registry.py", line 36, in build_x
    raise ValueError('--{} is required!'.format(registry_name))
ValueError: --optimizer is required!
&lt;/denchmark-code&gt;

final command：
fairseq-train data-bin/iwslt14.tokenized.de-en --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 --arch fconv_iwslt_de_en --save-dir checkpoints/fconv --reset-optimizer
which outputs the same error:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "D:\Python\Anaconda3\envs\Fairseq\Scripts\fairseq-train-script.py", line 33, in &lt;module&gt;
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "d:\fairseq\fairseq\fairseq_cli\train.py", line 333, in cli_main
    distributed_utils.call_main(args, main)
  File "d:\fairseq\fairseq\fairseq\distributed_utils.py", line 189, in call_main
    main(args, **kwargs)
  File "d:\fairseq\fairseq\fairseq_cli\train.py", line 109, in main
    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(args, trainer)
  File "d:\fairseq\fairseq\fairseq\checkpoint_utils.py", line 187, in load_checkpoint
    epoch_itr = trainer.get_train_iterator(
  File "d:\fairseq\fairseq\fairseq\trainer.py", line 335, in get_train_iterator
    return self.task.get_batch_iterator(
  File "d:\fairseq\fairseq\fairseq\tasks\fairseq_task.py", line 214, in get_batch_iterator
    batch_sampler = dataset.batch_by_size(
  File "d:\fairseq\fairseq\fairseq\data\fairseq_dataset.py", line 118, in batch_by_size
    return data_utils.batch_by_size(
  File "d:\fairseq\fairseq\fairseq\data\data_utils.py", line 256, in batch_by_size
    return batch_by_size_fast(
  File "fairseq\data\data_utils_fast.pyx", line 28, in fairseq.data.data_utils_fast.batch_by_size_fast
ValueError: Buffer dtype mismatch, expected 'DTYPE_t' but got 'long'
&lt;/denchmark-code&gt;

In addition:

I have try the command in Anaconda console and Cygwin's Terminal, both failed.
I have tried fairseq 0.9 latest stable version, everything is OK. I am satisfied with the frist trained result.

Oh, Maybe it is time for me to drop Windows 10 @@!
		</comment>
		<comment id='8' author='joeqi0370' date='2020-08-24T14:58:52Z'>
		I'm running into this, too. It seems like a stray default dtype is lingering somewhere in the code that's causing an issue.
		</comment>
		<comment id='9' author='joeqi0370' date='2020-08-24T18:54:04Z'>
		I tracked this down. The issue is as expected: default (non-portable) dtypes in FairseqDataset#ordered_indices are causing this issue. I've included the patch below; I'm happy to submit a PR in a few hours or someone else can take it if they'd like:
diff --git fairseq/data/fairseq_dataset.py fairseq/data/fairseq_dataset.py
index 2c972127..f196aff1 100644
--- fairseq/data/fairseq_dataset.py
+++ fairseq/data/fairseq_dataset.py
@@ -50,7 +50,7 @@ class FairseqDataset(torch.utils.data.Dataset, EpochListening):
     def ordered_indices(self):
         """Return an ordered list of indices. Batches will be constructed based
         on this order."""
-        return np.arange(len(self))
+        return np.arange(len(self), dtype=np.int64)

     @property
     def supports_prefetch(self):
diff --git fairseq/data/language_pair_dataset.py fairseq/data/language_pair_dataset.py
index aed54d61..fba3d37b 100644
--- fairseq/data/language_pair_dataset.py
+++ fairseq/data/language_pair_dataset.py
@@ -372,9 +372,9 @@ class LanguagePairDataset(FairseqDataset):
         """Return an ordered list of indices. Batches will be constructed based
         on this order."""
         if self.shuffle:
-            indices = np.random.permutation(len(self))
+            indices = np.random.permutation(len(self)).astype(np.int64)
         else:
-            indices = np.arange(len(self))
+            indices = np.arange(len(self), dtype=np.int64)
         if self.buckets is None:
             # sort by target length, then source length
             if self.tgt_sizes is not None:

		</comment>
		<comment id='10' author='joeqi0370' date='2020-08-25T04:27:28Z'>
		It works perfectly. Thanks so much &lt;denchmark-link:https://github.com/myleott&gt;@myleott&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/erip&gt;@erip&lt;/denchmark-link&gt;
 !!!
		</comment>
	</comments>
</bug>