<bug id='2502' author='cywang97' open_date='2020-08-20T09:39:35Z' closed_time='2020-08-25T01:16:08Z'>
	<summary>Errors when decoding with language model in wav2vec2.0</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I am trying to reproduce the number in wav2vec2.0 paper with the released model.  The code works well when I set --w2l-decoder=viterbi to reproduce the CTC results. However, when I set --w2l-decoder='kenlm' or 'fairseqlm'. I met the following errors:
For Kenlm, I download the lm_librispeech_kenlm_word_4g_200kvocab.bin in &lt;denchmark-link:https://github.com/facebookresearch/wav2letter/tree/master/recipes/models/sota/2019&gt;https://github.com/facebookresearch/wav2letter/tree/master/recipes/models/sota/2019&lt;/denchmark-link&gt;
. Running command
'python examples/speech_recognition/infer.py /path/to/manifest/ --task audio_pretraining --nbest 1 --path /path/to/mymodel --gen-subset dev_clean --results-path /path/to/results --w2l-decoder kenlm --lm-model /path/to/lm_librispeech_word_transformer.pt --lm-weight 2 --word-score -1 --sil-weight 0 --criterion ctc --labels ltr --max-tokens 4000000 --post-process letter'
The code reports error in line 137 "self.lexicon = load_words(args.lexicon)" in fairseq/examples/speech_recognition/w2l_decoder.py as args.lexicon is None. I can't find lexicon file in model repository. Can you upload it?
For Transformer, I download the released transformer language model and dictionary in wav2letter repository.
My command is
'python examples/speech_recognition/infer.py /path/to/manifest/ --task audio_pretraining --nbest 1 --path /path/to/mymodel --gen-subset dev_clean --results-path /path/to/results --w2l-decoder fairseqlm --lm-model /path/to/lm_librispeech_word_transformer.pt --lm-weight 2 --word-score -1 --sil-weight 0 --criterion ctc --labels ltr --max-tokens 4000000 --post-process letter'
There are several issues:

The arguments in the released model do not match the arguments defined in transformer_lm.py. For example, there is no args.decoder_layers_to_keep in lm_librispeech_word_transformer.pt. It seems the language model is not trained with the code in master branch.
I have tried to replace fairseq/models/transformer_lm.py and fairseq/models/transformer.py with the same files in branch v0.7.0.  The command didn't report errors. However, the hypothesis is too too bad (much worse than viterbi decoding)! Can you upload the pretrained model that compatible with master branch and give more details of hyperparameters?

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

I use the docker wav2letter/wav2letter:cuda-latest

fairseq Version : master
PyTorch Version :  1.4.0
OS : Ubuntu18.04
How you installed fairseq (pip, source): pip
Build command you used (if compiling from source):
Python version: 3.6.9
CUDA/cuDNN version: 10.0

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='cywang97' date='2020-08-20T14:44:05Z'>
		Hi &lt;denchmark-link:https://github.com/cywang97&gt;@cywang97&lt;/denchmark-link&gt;
 , I'm actually trying to use this wav2vec 2.0 model, to use as ASR, so that it can listen to the audio files and give an transcript of it as output, can you suggest how can I proceed with it ?
Thanks in advance..
		</comment>
		<comment id='2' author='cywang97' date='2020-08-20T23:49:09Z'>
		
Hi @cywang97 , I'm actually trying to use this wav2vec 2.0 model, to use as ASR, so that it can listen to the audio files and give an transcript of it as output, can you suggest how can I proceed with it ?
Thanks in advance..

Hi, I think you can simply run the proposed decoding command with --w2l-decoder viterbi to get the raw number without language model.
The complete command is 'python examples/speech_recognition/infer.py /path/to/manifest/ --task audio_pretraining --nbest 1 --path /path/to/model --gen-subset dev_clean --results-path /path/to/results --w2l-decoder viterbi --word-score -1 --sil-weight 0 --criterion ctc --labels ltr --max-tokens 4000000 --post-process letter'.
Please make sure you have python binding of wav2letter.
In your /path/to/manifest, you need to prepare the following files (take dev_clean subset as an example):
dev_clean.ltr
dev_clean.tsv
dev_clean.wrd.txt
dict.ltr.txt
		</comment>
		<comment id='3' author='cywang97' date='2020-08-21T01:45:08Z'>
		Here's the lexicon:
&lt;denchmark-link:https://dl.fbaipublicfiles.com/fairseq/wav2vec/librispeech_lexicon.lst&gt;https://dl.fbaipublicfiles.com/fairseq/wav2vec/librispeech_lexicon.lst&lt;/denchmark-link&gt;

I will link it in the readme soon
regarding the language models: we just used the models provided by the wav2letter team. I think they trained them some time ago and the codebase has changed since then, but they should still work in the master branch. i will double check.
		</comment>
		<comment id='4' author='cywang97' date='2020-08-21T04:30:15Z'>
		Thanks &lt;denchmark-link:https://github.com/cywang97&gt;@cywang97&lt;/denchmark-link&gt;
 for your response. I'll try it but have some doubts if you may help. In the command, how will I add my raw audio file without any labels, so that the model can create the transcript by itself and about " dev_clean.ltr, dev_clean.tsv, dev_clean.wrd.txt " are these the files that were created by "manifest" command using the pretrained Librispeech model ?
		</comment>
		<comment id='5' author='cywang97' date='2020-08-21T17:46:02Z'>
		the tsv file is generated by the wav2vec_manifest.py, which works on any dataset, not just librispeech (as long as it has audio files readable by soundfile library)
		</comment>
		<comment id='6' author='cywang97' date='2020-08-21T17:53:58Z'>
		
the tsv file is generated by the wav2vec_manifest.py, which works on any dataset, not just librispeech (as long as it has audio files readable by soundfile library)

Thanks &lt;denchmark-link:https://github.com/alexeib&gt;@alexeib&lt;/denchmark-link&gt;
 for your response. I'm getting what you're saying. But what I'm asking for is in the command

I'm not seeing any column to add my raw audio files that doesn't have any text files and I want the model to create the text files of the audio files after processing it.
		</comment>
		<comment id='7' author='cywang97' date='2020-08-21T19:17:49Z'>
		those files will be in the --results-path above
		</comment>
		<comment id='8' author='cywang97' date='2020-08-23T11:39:18Z'>
		Hi &lt;denchmark-link:https://github.com/cywang97&gt;@cywang97&lt;/denchmark-link&gt;
 , I'm using the codes as suggested by you i.e --&gt;

But I'm getting below error --&gt;

Traceback (most recent call last):
File "examples/speech_recognition/infer.py", line 428, in 
cli_main()
File "examples/speech_recognition/infer.py", line 424, in cli_main
main(args)
File "examples/speech_recognition/infer.py", line 361, in main
hypos = task.inference_step(generator, models, sample, prefix_tokens)
File "/path/fairseq/fairseq/tasks/fairseq_task.py", line 403, in inference_step
return generator.generate(models, sample, prefix_tokens=prefix_tokens, constraints=constraints)
TypeError: generate() got an unexpected keyword argument 'constraints'

And I also tried using the 'kenlm' as decoder but faced similar issue as reported by you at the top. I downloaded the librispeech_lexicon.lst as suggested by Alexei but my model is not recognising it.
		</comment>
		<comment id='9' author='cywang97' date='2020-08-23T18:13:54Z'>
		that error is a bug recently introduced by constraints-based decoding. i'll push a fix through, meanwhile you can just checkout a copy of the code before that commit or you can change the signature of "generate" in w2l_decoder.py to
&lt;denchmark-code&gt;def generate(self, models, sample, **unused):
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='cywang97' date='2020-08-24T08:00:53Z'>
		Thanks &lt;denchmark-link:https://github.com/alexeib&gt;@alexeib&lt;/denchmark-link&gt;
 for quick response, really helped.
		</comment>
		<comment id='11' author='cywang97' date='2020-09-09T10:09:49Z'>
		
Thanks @alexeib for quick response, really helped.

&lt;denchmark-link:https://github.com/MrityunjoyS&gt;@MrityunjoyS&lt;/denchmark-link&gt;
 What were the content for dev_clean.ltr,  dev_clean.wrd.txt . I could get tsv file from manifest but not able to know what will be content of these file. I am also trying to get text from raw audio file. Could you help with sample command or file?
		</comment>
	</comments>
</bug>