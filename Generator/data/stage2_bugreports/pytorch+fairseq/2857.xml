<bug id='2857' author='alealv' open_date='2020-11-05T18:47:34Z' closed_time='2020-11-05T23:37:06Z'>
	<summary>Variable `logging_output` is referenced before assignment</summary>
	<description>
&lt;denchmark-h:h3&gt;üêõ Bug&lt;/denchmark-h&gt;

After an OOM error in my GPUs I got a: Variable logging_output is referenced before assignment
2020-11-05 19:30:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2020-11-05 19:30:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
Traceback (most recent call last):
  File "train.py", line 12, in &lt;module&gt;
    main()
  File "train.py", line 8, in main
    cli_main()
  File "/home/aalvarez/Projects/fairseq/fairseq_cli/train.py", line 392, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/aalvarez/Projects/fairseq/fairseq/distributed_utils.py", line 305, in call_main
    torch.multiprocessing.spawn(
  File "/home/aalvarez/.virtualenvs/wav2vec-training-XQtg4Z6z-py3.8/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 199, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/aalvarez/.virtualenvs/wav2vec-training-XQtg4Z6z-py3.8/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 157, in start_processes
    while not context.join():
  File "/home/aalvarez/.virtualenvs/wav2vec-training-XQtg4Z6z-py3.8/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 118, in join
    raise Exception(msg)
Exception: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/aalvarez/.virtualenvs/wav2vec-training-XQtg4Z6z-py3.8/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 19, in _wrap
    fn(i, *args)
  File "/home/aalvarez/Projects/fairseq/fairseq/distributed_utils.py", line 292, in distributed_main
    main(cfg, **kwargs)
  File "/home/aalvarez/Projects/fairseq/fairseq_cli/train.py", line 130, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/aalvarez/Projects/fairseq/fairseq_cli/train.py", line 219, in train
    log_output = trainer.train_step(samples)
  File "/usr/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/aalvarez/Projects/fairseq/fairseq/trainer.py", line 761, in train_step
    return logging_output
UnboundLocalError: local variable 'logging_output' referenced before assignment

/usr/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 32 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


fairseq Version (e.g., 1.0 or master): master
PyTorch Version (e.g., 1.0): 1.7
OS (e.g., Linux): Linux
How you installed fairseq (pip, source): source
Build command you used (if compiling from source): clone git repository
Python version: 3.8.5
CUDA/cuDNN version: 11.1
GPU models and configuration: 2 x GeForce RTX 2080 Ti

	</description>
	<comments>
		<comment id='1' author='alealv' date='2020-11-05T23:37:06Z'>
		This should be fixed now. Note that OOM recovery is pretty fragile, so you may be better off reducing the batch size or using activation checkpointing (--checkpoint-activations if you're using transformer).
		</comment>
	</comments>
</bug>