<bug id='2783' author='mawright' open_date='2020-10-23T19:16:38Z' closed_time='2020-11-06T18:31:14Z'>
	<summary>fairseq-generate errors, incorrectly saying I didn't specify the sentencepiece_model</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I have a model that uses Sentencepiece but the new argument parsing system seems to not be able to find the value for sentencepiece-model from the command line.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Running the command
&lt;denchmark-code&gt;fairseq-generate --user-dir="/data/mwright/attn_approx/" \
        --task translation -s en -t fr --gen-subset test \
        --beam=4 --lenpen=0.6 \
        --bpe=sentencepiece \
        --sentencepiece-model /work/mwright/test/raw_data/wmt14/en-fr/sentencepiece/sentencepiece.bpe.model \
        --sacrebleu \
        --path $save_dir/checkpoint_best.pt \
        $data_dir &amp;&gt; $save_dir/results.txt
&lt;/denchmark-code&gt;

gave me
&lt;denchmark-code&gt;2020-10-23 12:05:08 | INFO | fairseq_cli.generate | {'common': {'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'memory_efficient_bf16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': '/data/mwright/attn_approx/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False}, 'distributed_training': {'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'local_rank': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'broadcast_buffers': False, 'distributed_wrapper': 'DDP', 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'distributed_world_size': 1, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False}, 'dataset': {'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 12000, 'batch_size': None, 'required_batch_size_multiple': 8, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': False, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'max_tokens_valid': None, 'batch_size_valid': None, 'required_seq_len_multiple': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0}, 'optimization': {'max_epoch': 0, 'max_update': 0, 'clip_norm': 25.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'min_lr': -1.0, 'use_bmuf': False, 'stop_time_hours': 0}, 'checkpoint': {'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'finetune_from_model': None, 'checkpoint_shard_count': 1, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'block_lr': 1, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'task': Namespace(_name='translation', all_gather_list_size=16384, batch_size=None, batch_size_valid=None, beam=4, best_checkpoint_metric='loss', bf16=False, bpe='sentencepiece', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', constraints=None, cpu=False, criterion='cross_entropy', curriculum=0, data='/work/mwright/test/data/wmt14/en-fr/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoding_format=None, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', diverse_beam_groups=-1, diverse_beam_strength=0.5, diversity_rate=-1.0, empty_cache_freq=0, eos=2, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, iter_decode_with_beam=1, iter_decode_with_external_reranker=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, left_pad_source='True', left_pad_target='False', lenpen=0.6, lm_path=None, lm_weight=0.0, load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', model_parallel_size=1, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_repeat_ngram_size=0, no_save=False, no_save_optimizer_state=False, no_seed_provided=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer=None, optimizer_overrides='{}', pad=1, path='./checkpoint_best.pt', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, post_process=None, prefix_size=0, print_alignment=False, print_step=False, profile=False, quantization_config_path=None, quiet=False, replace_unk=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', results_path=None, retain_dropout=False, retain_dropout_modules=None, retain_iter_history=False, sacrebleu=True, sampling=False, sampling_topk=-1, sampling_topp=-1.0, save_dir='checkpoints', save_interval=1, save_interval_updates=0, score_reference=False, scoring='bleu', seed=1, sentencepiece_model='/work/mwright/test/raw_data/wmt14/en-fr/sentencepiece/sentencepiece.bpe.model', shard_id=0, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', target_lang='fr', task='translation', temperature=1.0, tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='/data/mwright/attn_approx/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=0, zero_sharding='none'), 'criterion': {'sentence_avg': False, '_name': 'cross_entropy'}, 'common_eval': {'path': './checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'generation': {'beam': 4, 'nbest': 1, 'max_len_a': 0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 0.6, 'unkpen': 0, 'replace_unk': None, 'sacrebleu': True, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': False, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'buffer_size': 0, 'input': '-'}, 'bpe': {'_name': 'sentencepiece', 'sentencepiece_model': '/work/mwright/test/raw_data/wmt14/en-fr/sentencepiece/sentencepiece.bpe.model'}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'tokenizer': None, 'optimizer': None, 'lr_scheduler': Namespace(_name='fixed', all_gather_list_size=16384, batch_size=None, batch_size_valid=None, beam=4, best_checkpoint_metric='loss', bf16=False, bpe='sentencepiece', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', constraints=None, cpu=False, criterion='cross_entropy', curriculum=0, data='/work/mwright/test/data/wmt14/en-fr/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoding_format=None, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', diverse_beam_groups=-1, diverse_beam_strength=0.5, diversity_rate=-1.0, empty_cache_freq=0, eos=2, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, iter_decode_with_beam=1, iter_decode_with_external_reranker=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, left_pad_source='True', left_pad_target='False', lenpen=0.6, lm_path=None, lm_weight=0.0, load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', model_parallel_size=1, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_repeat_ngram_size=0, no_save=False, no_save_optimizer_state=False, no_seed_provided=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer=None, optimizer_overrides='{}', pad=1, path='./checkpoint_best.pt', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, post_process=None, prefix_size=0, print_alignment=False, print_step=False, profile=False, quantization_config_path=None, quiet=False, replace_unk=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', results_path=None, retain_dropout=False, retain_dropout_modules=None, retain_iter_history=False, sacrebleu=True, sampling=False, sampling_topk=-1, sampling_topp=-1.0, save_dir='checkpoints', save_interval=1, save_interval_updates=0, score_reference=False, scoring='bleu', seed=1, sentencepiece_model='/work/mwright/test/raw_data/wmt14/en-fr/sentencepiece/sentencepiece.bpe.model', shard_id=0, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', target_lang='fr', task='translation', temperature=1.0, tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='/data/mwright/attn_approx/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=0, zero_sharding='none'), 'model': None}
2020-10-23 12:05:09 | INFO | fairseq.tasks.translation | [en] dictionary: 32400 types
2020-10-23 12:05:09 | INFO | fairseq.tasks.translation | [fr] dictionary: 32400 types
2020-10-23 12:05:09 | INFO | fairseq.data.data_utils | loaded 3003 examples from: /work/mwright/test/data/wmt14/en-fr/test.en-fr.en
2020-10-23 12:05:09 | INFO | fairseq.data.data_utils | loaded 3003 examples from: /work/mwright/test/data/wmt14/en-fr/test.en-fr.fr
2020-10-23 12:05:09 | INFO | fairseq.tasks.translation | /work/mwright/test/data/wmt14/en-fr/ test en-fr 3003 examples
2020-10-23 12:05:09 | INFO | fairseq_cli.generate | loading model(s) from ./checkpoint_best.pt
Traceback (most recent call last):
  File "/data/mwright/anaconda3/envs/cpu/bin/fairseq-generate", line 33, in &lt;module&gt;
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-generate')())
  File "/work/mwright/fairseq/fairseq_cli/generate.py", line 392, in cli_main
    main(args)
  File "/work/mwright/fairseq/fairseq_cli/generate.py", line 53, in main
    return _main(cfg, sys.stdout)
  File "/work/mwright/fairseq/fairseq_cli/generate.py", line 106, in _main
    num_shards=cfg.checkpoint.checkpoint_shard_count,
  File "/work/mwright/fairseq/fairseq/checkpoint_utils.py", line 264, in load_model_ensemble
    num_shards,
  File "/work/mwright/fairseq/fairseq/checkpoint_utils.py", line 288, in load_model_ensemble_and_task
    state = load_checkpoint_to_cpu(filename, arg_overrides)
  File "/work/mwright/fairseq/fairseq/checkpoint_utils.py", line 238, in load_checkpoint_to_cpu
    overwrite_args_by_name(state["cfg"], arg_overrides)
  File "/work/mwright/fairseq/fairseq/dataclass/utils.py", line 355, in overwrite_args_by_name
    overwrite_args_by_name(cfg[k], overrides)
  File "/work/mwright/fairseq/fairseq/dataclass/utils.py", line 354, in overwrite_args_by_name
    if isinstance(cfg[k], DictConfig):
  File "/data/mwright/anaconda3/envs/cpu/lib/python3.7/site-packages/omegaconf/dictconfig.py", line 313, in __getitem__
    self._format_and_raise(key=key, value=None, cause=e)
  File "/data/mwright/anaconda3/envs/cpu/lib/python3.7/site-packages/omegaconf/base.py", line 101, in _format_and_raise
    type_override=type_override,
  File "/data/mwright/anaconda3/envs/cpu/lib/python3.7/site-packages/omegaconf/_utils.py", line 694, in format_and_raise
    _raise(ex, cause)
  File "/data/mwright/anaconda3/envs/cpu/lib/python3.7/site-packages/omegaconf/_utils.py", line 610, in _raise
    raise ex  # set end OC_CAUSE=1 for full backtrace
omegaconf.errors.MissingMandatoryValue: Missing mandatory value: bpe.sentencepiece_model
	full_key: bpe.sentencepiece_model
	reference_type=Any
	object_type=dict
&lt;/denchmark-code&gt;

In the log above when the arguments are logged I see a dict
'bpe': {'_name': 'sentencepiece', 'sentencepiece_model': '/work/mwright/test/raw_data/wmt14/en-fr/sentencepiece/sentencepiece.bpe.model'} but it does not seem to be found according to the error.
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

N/A
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

fairseq-generate generates translations.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


fairseq Version (e.g., 1.0 or master): 2409d5a
PyTorch Version (e.g., 1.0): 1.6.0
OS (e.g., Linux): Ubuntu Linux
How you installed fairseq (pip, source): source
Build command you used (if compiling from source): pip install -e .
Python version: Anaconda 3.7.9
CUDA/cuDNN version: N/A, this is CPU
GPU models and configuration: None
Any other relevant information:

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='mawright' date='2020-10-26T08:21:33Z'>
		I encountered the same problem. It seems that it was caused by updating fairseq with Hydra compatible.
		</comment>
		<comment id='2' author='mawright' date='2020-10-26T21:11:53Z'>
		I have the same issue. After updating fairseq through git pull, I get this error when I use fairseq-generate:
omegaconf.errors.MissingMandatoryValue: Missing mandatory value: bpe.bpe_codes
		</comment>
		<comment id='3' author='mawright' date='2020-10-28T22:16:07Z'>
		i cant repro this on latest master. we had an issue with this awhile ago but its been fixed for a few days. can you try again after pulling?
		</comment>
		<comment id='4' author='mawright' date='2020-10-29T10:17:52Z'>
		Still happening after pooling...
		</comment>
		<comment id='5' author='mawright' date='2020-10-29T18:20:50Z'>
		PYTHONPATH=. python fairseq_cli/generate.py /path/to/data --path /path/to/transformer_translation_model.pt --beam 4 --lenpen 0.6 --bpe=sentencepiece --sentencepiece-model a/b/c --sacrebleu
i get
Traceback (most recent call last):
File "fairseq_cli/generate.py", line 393, in 
cli_main()
File "fairseq_cli/generate.py", line 389, in cli_main
main(args)
File "fairseq_cli/generate.py", line 50, in main
return _main(cfg, sys.stdout)
File "fairseq_cli/generate.py", line 171, in _main
bpe = encoders.build_bpe(cfg.bpe)
File "/private/home/abaevski/fairseq-py-master/fairseq/registry.py", line 52, in build_x
return builder(cfg, *extra_args, **extra_kwargs)
File "/private/home/abaevski/fairseq-py-master/fairseq/data/encoders/sentencepiece_bpe.py", line 23, in init
sentencepiece_model = file_utils.cached_path(cfg.sentencepiece_model)
File "/private/home/abaevski/fairseq-py-master/fairseq/file_utils.py", line 166, in cached_path
raise EnvironmentError("file {} not found".format(url_or_filename))
OSError: file a/b/c not found
so it is able to read the parameter
		</comment>
		<comment id='6' author='mawright' date='2020-10-29T23:16:08Z'>
		I get the error on using bpe = subword_nmt. So, I get this error message
omegaconf.errors.MissingMandatoryValue: Missing mandatory value: bpe.bpe_codes
in two situations: while loading a pretrained model and using it to translate
self.ja2en = TransformerModel.from_pretrained( './checkpoints', checkpoint_file='checkpoint_best.pt', bpe='subword_nmt', bpe_codes='checkpoints/codes.32000.bpe.ja' )
ja2en.translate("something")
and when I execute this
fairseq-generate $DATADIR --path $TRAIN/checkpoint_best.pt  \ --beam 5 --nbest 2 --batch-size 512
I thought that I had to put the flag --bpe_codesand --bpe, but it doesn't recognize them...
This code worked for me before I pull the repo
Thanks for answering!
		</comment>
		<comment id='7' author='mawright' date='2020-10-30T04:53:02Z'>
		plz use --bpe-codes now
		</comment>
		<comment id='8' author='mawright' date='2020-10-30T09:37:52Z'>
		Using --bpe-codes I got the same error:
omegaconf.errors.MissingMandatoryValue: Missing mandatory value: bpe.bpe_codes
And in the other situation I cannot change bpe_codes to bpe-codes, i got this:
SyntaxError: keyword can't be an expression
Thanks again!
Obviously, I'm working with the latest version of fairseq
		</comment>
		<comment id='9' author='mawright' date='2020-11-04T00:00:50Z'>
		I just tried again with the current master (&lt;denchmark-link:https://github.com/pytorch/fairseq/commit/b120fbbe8fdb6fc8412149916fe09c54757bdaf6&gt;b120fbb&lt;/denchmark-link&gt;
) and still get the same error. Full console output below:
&lt;denchmark-code&gt;2020-11-03 15:55:59 | INFO | fairseq_cli.generate | {'common': {'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'memory_efficient_bf16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False}, 'distributed_training': {'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'local_rank': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'broadcast_buffers': False, 'distributed_wrapper': 'DDP', 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'distributed_world_size': 1, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False}, 'dataset': {'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 12000, 'batch_size': None, 'required_batch_size_multiple': 8, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': False, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'max_tokens_valid': None, 'batch_size_valid': None, 'required_seq_len_multiple': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0}, 'optimization': {'max_epoch': 0, 'max_update': 0, 'clip_norm': 25.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'min_lr': -1.0, 'use_bmuf': False, 'stop_time_hours': 0}, 'checkpoint': {'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'finetune_from_model': None, 'checkpoint_shard_count': 1, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'block_lr': 1, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'task': Namespace(_name='translation', all_gather_list_size=16384, batch_size=None, batch_size_valid=None, beam=4, best_checkpoint_metric='loss', bf16=False, bpe='sentencepiece', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', constraints=None, cpu=False, criterion='cross_entropy', curriculum=0, data='/work/mwright/test/data/iwslt14/de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoding_format=None, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', diverse_beam_groups=-1, diverse_beam_strength=0.5, diversity_rate=-1.0, empty_cache_freq=0, eos=2, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, iter_decode_with_beam=1, iter_decode_with_external_reranker=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, left_pad_source='True', left_pad_target='False', lenpen=0.6, lm_path=None, lm_weight=0.0, load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', model_parallel_size=1, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_repeat_ngram_size=0, no_save=False, no_save_optimizer_state=False, no_seed_provided=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer=None, optimizer_overrides='{}', pad=1, path='./checkpoint_best.pt', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, post_process=None, prefix_size=0, print_alignment=False, print_step=False, profile=False, quantization_config_path=None, quiet=False, replace_unk=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', results_path=None, retain_dropout=False, retain_dropout_modules=None, retain_iter_history=False, sacrebleu=True, sampling=False, sampling_topk=-1, sampling_topp=-1.0, save_dir='checkpoints', save_interval=1, save_interval_updates=0, score_reference=False, scoring='bleu', seed=1, sentencepiece_model='/work/mwright/test/raw_data/iwslt14/de-en//sentencepiece.bpe.model', shard_id=0, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', target_lang='en', task='translation', temperature=1.0, tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, unkpen=0, unnormalized=False, upsample_primary=1, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=0, zero_sharding='none'), 'criterion': {'sentence_avg': False, '_name': 'cross_entropy'}, 'bpe': {'_name': 'sentencepiece', 'sentencepiece_model': '/work/mwright/test/raw_data/iwslt14/de-en//sentencepiece.bpe.model'}, 'common_eval': {'path': './checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'generation': {'beam': 4, 'nbest': 1, 'max_len_a': 0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 0.6, 'unkpen': 0, 'replace_unk': None, 'sacrebleu': True, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': False, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'buffer_size': 0, 'input': '-'}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'tokenizer': None, 'optimizer': None, 'lr_scheduler': Namespace(_name='fixed', all_gather_list_size=16384, batch_size=None, batch_size_valid=None, beam=4, best_checkpoint_metric='loss', bf16=False, bpe='sentencepiece', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', constraints=None, cpu=False, criterion='cross_entropy', curriculum=0, data='/work/mwright/test/data/iwslt14/de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoding_format=None, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', diverse_beam_groups=-1, diverse_beam_strength=0.5, diversity_rate=-1.0, empty_cache_freq=0, eos=2, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, iter_decode_with_beam=1, iter_decode_with_external_reranker=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, left_pad_source='True', left_pad_target='False', lenpen=0.6, lm_path=None, lm_weight=0.0, load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', model_parallel_size=1, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_repeat_ngram_size=0, no_save=False, no_save_optimizer_state=False, no_seed_provided=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer=None, optimizer_overrides='{}', pad=1, path='./checkpoint_best.pt', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, post_process=None, prefix_size=0, print_alignment=False, print_step=False, profile=False, quantization_config_path=None, quiet=False, replace_unk=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', results_path=None, retain_dropout=False, retain_dropout_modules=None, retain_iter_history=False, sacrebleu=True, sampling=False, sampling_topk=-1, sampling_topp=-1.0, save_dir='checkpoints', save_interval=1, save_interval_updates=0, score_reference=False, scoring='bleu', seed=1, sentencepiece_model='/work/mwright/test/raw_data/iwslt14/de-en//sentencepiece.bpe.model', shard_id=0, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', target_lang='en', task='translation', temperature=1.0, tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, unkpen=0, unnormalized=False, upsample_primary=1, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=0, zero_sharding='none'), 'model': None}
2020-11-03 15:55:59 | INFO | fairseq.tasks.translation | [de] dictionary: 8000 types
2020-11-03 15:55:59 | INFO | fairseq.tasks.translation | [en] dictionary: 8000 types
2020-11-03 15:55:59 | INFO | fairseq.data.data_utils | loaded 6750 examples from: /work/mwright/test/data/iwslt14/de-en/test.de-en.de
2020-11-03 15:55:59 | INFO | fairseq.data.data_utils | loaded 6750 examples from: /work/mwright/test/data/iwslt14/de-en/test.de-en.en
2020-11-03 15:55:59 | INFO | fairseq.tasks.translation | /work/mwright/test/data/iwslt14/de-en/ test de-en 6750 examples
2020-11-03 15:55:59 | INFO | fairseq_cli.generate | loading model(s) from ./checkpoint_best.pt
/data/mwright/anaconda3/envs/cpu/lib/python3.7/site-packages/hydra/_internal/hydra.py:71: UserWarning: 
@hydra.main(strict) flag is deprecated and will removed in the next version.
See https://hydra.cc/docs/next/upgrades/0.11_to_1.0/strict_mode_flag_deprecated
  warnings.warn(message=msg, category=UserWarning)
Traceback (most recent call last):
  File "/data/mwright/anaconda3/envs/cpu/lib/python3.7/site-packages/omegaconf/dictconfig.py", line 307, in __getitem__
    return self._get_impl(key=key, default_value=DEFAULT_VALUE_MARKER)
  File "/data/mwright/anaconda3/envs/cpu/lib/python3.7/site-packages/omegaconf/dictconfig.py", line 360, in _get_impl
    key=key, value=node, default_value=default_value
  File "/data/mwright/anaconda3/envs/cpu/lib/python3.7/site-packages/omegaconf/basecontainer.py", line 74, in _resolve_with_default
    raise MissingMandatoryValue("Missing mandatory value: $FULL_KEY")
omegaconf.errors.MissingMandatoryValue: Missing mandatory value: $FULL_KEY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/mwright/anaconda3/envs/cpu/bin/fairseq-generate", line 33, in &lt;module&gt;
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-generate')())
  File "/work/mwright/fairseq/fairseq_cli/generate.py", line 389, in cli_main
    main(args)
  File "/work/mwright/fairseq/fairseq_cli/generate.py", line 50, in main
    return _main(cfg, sys.stdout)
  File "/work/mwright/fairseq/fairseq_cli/generate.py", line 103, in _main
    num_shards=cfg.checkpoint.checkpoint_shard_count,
  File "/work/mwright/fairseq/fairseq/checkpoint_utils.py", line 264, in load_model_ensemble
    num_shards,
  File "/work/mwright/fairseq/fairseq/checkpoint_utils.py", line 288, in load_model_ensemble_and_task
    state = load_checkpoint_to_cpu(filename, arg_overrides)
  File "/work/mwright/fairseq/fairseq/checkpoint_utils.py", line 238, in load_checkpoint_to_cpu
    overwrite_args_by_name(state["cfg"], arg_overrides)
  File "/work/mwright/fairseq/fairseq/dataclass/utils.py", line 358, in overwrite_args_by_name
    overwrite_args_by_name(cfg[k], overrides)
  File "/work/mwright/fairseq/fairseq/dataclass/utils.py", line 357, in overwrite_args_by_name
    if isinstance(cfg[k], DictConfig):
  File "/data/mwright/anaconda3/envs/cpu/lib/python3.7/site-packages/omegaconf/dictconfig.py", line 313, in __getitem__
    self._format_and_raise(key=key, value=None, cause=e)
  File "/data/mwright/anaconda3/envs/cpu/lib/python3.7/site-packages/omegaconf/base.py", line 101, in _format_and_raise
    type_override=type_override,
  File "/data/mwright/anaconda3/envs/cpu/lib/python3.7/site-packages/omegaconf/_utils.py", line 694, in format_and_raise
    _raise(ex, cause)
  File "/data/mwright/anaconda3/envs/cpu/lib/python3.7/site-packages/omegaconf/_utils.py", line 610, in _raise
    raise ex  # set end OC_CAUSE=1 for full backtrace
omegaconf.errors.MissingMandatoryValue: Missing mandatory value: bpe.sentencepiece_model
        full_key: bpe.sentencepiece_model
        reference_type=Any
        object_type=dict
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='mawright' date='2020-11-04T22:14:06Z'>
		After some debugging, it looks like the issue is that the arguments to generate.py aren't passed to load_model_ensemble to override the saved arguments from the saved checkpoint. So, if you don't specify --sentencepiece-model to train.py it will run the training perfectly fine (since it is not a necessary argument for train.py) but when calling generate.py it will load the checkpoint and error because the saved argument dict doesn't have a value for it.
		</comment>
		<comment id='11' author='mawright' date='2020-11-06T04:49:19Z'>
		this is an issue with overrides of args from checkpoint. i have a fix
		</comment>
	</comments>
</bug>