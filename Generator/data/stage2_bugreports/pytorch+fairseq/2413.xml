<bug id='2413' author='Skylixia' open_date='2020-08-03T09:27:01Z' closed_time='2020-08-03T12:52:26Z'>
	<summary>RuntimeError: "LayerNormKernelImpl" not implemented for 'Half'</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

After following the Roberta &lt;denchmark-link:https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.pretraining.md&gt;pretraining documentation&lt;/denchmark-link&gt;
 with the exact same steps and data.
When running fairseq-train I get an error
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;TOTAL_UPDATES=125000    # Total number of training steps
WARMUP_UPDATES=10000    # Warmup the learning rate over this many updates
PEAK_LR=0.0005          # Peak learning rate, adjust as needed
TOKENS_PER_SAMPLE=512   # Max sequence length
MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)
MAX_SENTENCES=16        # Number of sequences per batch (batch size)
UPDATE_FREQ=16          # Increase the batch size 16x

DATA_DIR=data-bin/wikitext-103

fairseq-train --fp16 $DATA_DIR \
    --task masked_lm --criterion masked_lm \
    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \
    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \
    --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \
    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \
    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \
    --max-update $TOTAL_UPDATES --log-format simple --log-interval 1
&lt;/denchmark-code&gt;

Then I get the following stack trace:
&lt;denchmark-code&gt;
2020-08-03 09:13:25 | INFO | fairseq_cli.train | begin training epoch 1
Traceback (most recent call last):
  File "/usr/local/bin/fairseq-train", line 33, in &lt;module&gt;
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/fairseq/fairseq_cli/train.py", line 350, in cli_main
    distributed_utils.call_main(args, main)
  File "/fairseq/fairseq/distributed_utils.py", line 189, in call_main
    main(args, **kwargs)
  File "/fairseq/fairseq_cli/train.py", line 121, in main
    valid_losses, should_stop = train(args, trainer, task, epoch_itr)
  File "/usr/lib/python3.6/contextlib.py", line 52, in inner
    return func(*args, **kwds)
  File "/fairseq/fairseq_cli/train.py", line 217, in train
    log_output = trainer.train_step(samples)
  File "/usr/lib/python3.6/contextlib.py", line 52, in inner
    return func(*args, **kwds)
  File "/fairseq/fairseq/trainer.py", line 457, in train_step
    raise e
  File "/fairseq/fairseq/trainer.py", line 431, in train_step
    ignore_grad=is_dummy_batch,
  File "/fairseq/fairseq/tasks/fairseq_task.py", line 347, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/fairseq/fairseq/criterions/masked_lm.py", line 52, in forward
    logits = model(**sample['net_input'], masked_tokens=masked_tokens)[0]
  File "/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/fairseq/fairseq/models/roberta/model.py", line 119, in forward
    x, extra = self.encoder(src_tokens, features_only, return_all_hiddens, **kwargs)
  File "/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/fairseq/fairseq/models/roberta/model.py", line 337, in forward
    x, extra = self.extract_features(src_tokens, return_all_hiddens=return_all_hiddens)
  File "/fairseq/fairseq/models/roberta/model.py", line 345, in extract_features
    last_state_only=not return_all_hiddens,
  File "/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/fairseq/fairseq/modules/transformer_sentence_encoder.py", line 250, in forward
    x = self.emb_layer_norm(x)
  File "/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/.local/lib/python3.6/site-packages/torch/nn/modules/normalization.py", line 170, in forward
    input, self.normalized_shape, self.weight, self.bias, self.eps)
  File "/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 2049, in layer_norm
    torch.backends.cudnn.enabled)
RuntimeError: "LayerNormKernelImpl" not implemented for 'Half'
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


fairseq Version : master
PyTorch Version : 1.6.0
OS : Linux (Ubuntu)
How you installed fairseq (pip, source): source
Build command you used (if compiling from source):

&lt;denchmark-code&gt;git clone https://github.com/pytorch/fairseq
cd fairseq
pip install --editable .
&lt;/denchmark-code&gt;


Python version: 3.6.9
CUDA/cuDNN version: 11.0

	</description>
	<comments>
		<comment id='1' author='Skylixia' date='2020-08-03T12:52:26Z'>
		Solved after reinstalling CUDA and NCCL
		</comment>
		<comment id='2' author='Skylixia' date='2020-09-04T07:05:48Z'>
		got the same error, restarting the notebook solved it for me
notebook - 39_tutorial.transformers.ipynb
		</comment>
		<comment id='3' author='Skylixia' date='2020-10-20T06:32:22Z'>
		I get the same exception even when running on cpu. Did you find a solution?
		</comment>
	</comments>
</bug>