<bug id='2799' author='turian' open_date='2020-10-26T15:08:16Z' closed_time='2020-10-26T23:31:35Z'>
	<summary>train.py checkpoint args are None</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I am trying to pretrain and featurize a small dataset of percussive sounds, &gt;= 1024 samples.
Following the README to &lt;denchmark-link:https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#training-a-new-model-with-the-cli-tools-1&gt;train a new model with the CLI tools&lt;/denchmark-link&gt;
,  cannot read the checkpoint generated. This is because  is None.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

You can try in colab here:
&lt;denchmark-link:https://colab.research.google.com/drive/1AP66z1JXCJlU92aWUkPeA-8oqLJDk6D8?usp=sharing&gt;https://colab.research.google.com/drive/1AP66z1JXCJlU92aWUkPeA-8oqLJDk6D8?usp=sharing&lt;/denchmark-link&gt;

It gets this error:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "examples/wav2vec/wav2vec_featurize.py", line 238, in &lt;module&gt;
    use_feat=args.use_feat,
  File "examples/wav2vec/wav2vec_featurize.py", line 140, in __init__
    self.model = Prediction(self.model_fname, gpu)
  File "examples/wav2vec/wav2vec_featurize.py", line 87, in __init__
    self.model = PretrainedWav2VecModel(fname).cuda(gpu)
  File "examples/wav2vec/wav2vec_featurize.py", line 40, in __init__
    model = Wav2VecModel.build_model(self.args, None)
  File "/content/fairseq/fairseq/models/wav2vec/wav2vec.py", line 214, in build_model
    base_wav2vec_architecture(args)
  File "/content/fairseq/fairseq/models/wav2vec/wav2vec.py", line 690, in base_wav2vec_architecture
    args.conv_feature_layers = getattr(args, "conv_feature_layers", conv_feature_layers)
AttributeError: 'NoneType' object has no attribute 'conv_feature_layers'
&lt;/denchmark-code&gt;

If I torch.load the checkpoint, it looks like it has a model but args is None.
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;!git clone https://github.com/pytorch/fairseq
# %cd fairseq

!pip3 install torch soundfile
!pip install -q --editable ./

# https://zenodo.org/record/3665275
!wget -c https://zenodo.org/record/3665275/files/one_shot_percussive_sounds.zip
!unzip -o one_shot_percussive_sounds.zip

# Remove files under 1024 samples
import glob, os, soundfile
for f in glob.glob("one_shot_percussive_sounds/*/*wav"):
  x, sr = soundfile.read(f)
  if x.shape[0] &lt; 1024:
    os.remove(f)
#    print("Removing", f)

!mkdir tmpmanifest tmpmodel ; python examples/wav2vec/wav2vec_manifest.py one_shot_percussive_sounds/1/ --dest tmpmanifest --ext wav

!python train.py tmpmanifest --save-dir tmpmodel --num-workers 6 --fp16 --max-update 10 --save-interval 1 --no-epoch-checkpoints --arch wav2vec --task audio_pretraining --lr 1e-06 --min-lr 1e-09 --optimizer adam --max-lr 0.005 --lr-scheduler cosine --conv-feature-layers '[(512, 10, 5), (512, 8, 4), (512, 4, 2), (512, 4, 2), (512, 4, 2), (512, 1, 1), (512, 1, 1)]' --conv-aggregator-layers '[(512, 2, 1), (512, 3, 1), (512, 4, 1), (512, 5, 1), (512, 6, 1), (512, 7, 1), (512, 8, 1), (512, 9, 1), (512, 10, 1), (512, 11, 1), (512, 12, 1), (512, 13, 1)]' --skip-connections-agg --residual-scale 0.5 --log-compression --warmup-updates 500 --warmup-init-lr 1e-07 --criterion wav2vec --num-negatives 10 --max-sample-size 150000 --max-tokens 1500000 --skip-invalid-size-inputs-valid-test

!mkdir tmpoutput
!python examples/wav2vec/wav2vec_featurize.py --input one_shot_percussive_sounds/ --output tmpoutput \
--model tmpmodel/checkpoint_last.pt --split train valid test
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

train.py should generate a checkpoint that contains the arguments (checkpoint["args"] shouldn't be None). Then, wav2vec_featurize can correctly featurize the input corpus.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


fairseq Version (e.g., 1.0 or master): master
PyTorch Version (e.g., 1.0): 1.6.0+cu101
OS (e.g., Linux): Ubuntu 18.04.5 LTS
How you installed fairseq (pip, source): source
Build command you used (if compiling from source): pip install -q --editable ./
Python version: 3.6.9
CUDA/cuDNN version: 10.1
GPU models and configuration: Tesla P100
Any other relevant information: Google Colab

	</description>
	<comments>
		<comment id='1' author='turian' date='2020-10-26T17:48:06Z'>
		Note that the above example uses wav2vec 1.0 because the audio samples in that corpus are short (1 second long). Here is a minimal example using wav2vec 2.0 showing args is still None in the checkpoint.
It uses longer samples, and does not use  (because that script &lt;denchmark-link:https://github.com/pytorch/fairseq/issues/2495#issuecomment-677993300&gt;does not work with 2.0&lt;/denchmark-link&gt;
):
&lt;denchmark-link:https://colab.research.google.com/drive/1wFmGVZK977J_xtqkGaVH2NLsohT-wQDz?usp=sharing&gt;https://colab.research.google.com/drive/1wFmGVZK977J_xtqkGaVH2NLsohT-wQDz?usp=sharing&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;!git clone https://github.com/pytorch/fairseq
# %cd fairseq

!pip3 install torch soundfile
!pip install -q --editable ./
!rm -Rf tmpinput; mkdir tmpinput
!wget -c https://github.com/karoldvl/ESC-50/archive/master.zip
!unzip -o master.zip

import glob
import os.path
from tqdm.auto import tqdm
import soundfile
for f in tqdm(glob.glob("ESC-50-master/*/*.wav")):
  x, sr = soundfile.read(f)
  newf = "tmpinput/" + os.path.split(f)[1]
  soundfile.write(newf, x, samplerate=16000)

!mkdir tmpmanifest tmppmodel ; python examples/wav2vec/wav2vec_manifest.py tmpinput/ --dest tmpmanifest --ext wav
#!python train.py --distributed-world-size 64 --distributed-port $PORT drummanifest \
!python train.py tmpmanifest \
--save-dir tmpmodel --fp16 --num-workers 6 --task audio_pretraining --criterion wav2vec --arch wav2vec2 \
--log-keys '["prob_perplexity","code_perplexity","temp"]' --quantize-targets --extractor-mode default \
--conv-feature-layers '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] * 2' --final-dim 256 --latent-vars 320 \
--latent-groups 2 --latent-temp '(2,0.5,0.999995)' --infonce --optimizer adam \
--adam-betas '(0.9,0.98)' --adam-eps 1e-06 --lr-scheduler polynomial_decay --total-num-update 400000 \
--lr 0.0005 --warmup-updates 32000 --mask-length 10 --mask-prob 0.65 --mask-selection static --mask-other 0 \
--encoder-layerdrop 0.05 --dropout-input 0.1 --dropout-features 0.1 --feature-grad-mult 0.1 \
--loss-weights '[0.1, 10]' --conv-pos 128 --conv-pos-groups 16 --num-negatives 100 --cross-sample-negatives 0 \
--max-sample-size 250000 --min-sample-size 32000 --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \
--max-tokens 1400000 --max-update 10 --skip-invalid-size-inputs-valid-test --ddp-backend no_c10d

#--max-tokens 1400000 --max-update 400000 --skip-invalid-size-inputs-valid-test --ddp-backend no_c10d

import os
import torch
from fairseq.models.wav2vec import Wav2Vec2Model

cp = torch.load("tmpmodel/checkpoint_best.pt")
model = Wav2Vec2Model.build_model(cp['args'], None)
model.load_state_dict(cp['model'], strict=False)
model.eval()
&lt;/denchmark-code&gt;

gives
&lt;denchmark-code&gt;---------------------------------------------------------------------------

AttributeError                            Traceback (most recent call last)

&lt;ipython-input-16-aeb5c14e3534&gt; in &lt;module&gt;()
      7 print(repr(cp.keys()))
      8 
----&gt; 9 model = Wav2Vec2Model.build_model(cp['args'], None)
     10 model.load_state_dict(cp['model'], strict=False)
     11 model.eval()

1 frames

/content/fairseq/fairseq/models/wav2vec/wav2vec2.py in build_model(cls, args, task)
    408 
    409         # make sure all arguments are present
--&gt; 410         base_architecture(args)
    411 
    412         return cls(args)

/content/fairseq/fairseq/models/wav2vec/wav2vec2.py in base_architecture(args)
    962 @register_model_architecture("wav2vec2", "wav2vec2")
    963 def base_architecture(args):
--&gt; 964     args.extractor_mode = getattr(args, "extractor_mode", "default")
    965 
    966     args.encoder_layers = getattr(args, "encoder_layers", 12)

AttributeError: 'NoneType' object has no attribute 'extractor_mode'
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='turian' date='2020-10-26T22:13:40Z'>
		What happens when you use from_pretrained? That's the normal idiom.
		</comment>
		<comment id='3' author='turian' date='2020-10-26T23:31:35Z'>
		Thank you, I was unaware of this idiom. I assumed from the filename extension  ofcheckpoint_best.pt it would be loaded as per the README.
I was able to read the pre-trained checkpoint and access the model using this code:
&lt;denchmark-code&gt;Wav2VecModel.from_pretrained("tmpmodel", "checkpoint_best.pt").models[0]
&lt;/denchmark-code&gt;

(for wav2vec 1.0 as per the original issue)
		</comment>
	</comments>
</bug>