<bug id='2275' author='sk-g' open_date='2020-06-26T20:33:50Z' closed_time='2020-06-30T13:05:22Z'>
	<summary>task translation_moe is in the wrong place</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Tried following the instructions to run Mixture of learners from: &lt;denchmark-link:https://github.com/pytorch/fairseq/tree/master/examples/translation_moe&gt;https://github.com/pytorch/fairseq/tree/master/examples/translation_moe&lt;/denchmark-link&gt;

Part of the instruction is to invoke a  task as  but the task registry module is in the  folder. Should this be moved to &lt;denchmark-link:https://github.com/pytorch/fairseq/tree/master/fairseq/tasks&gt;https://github.com/pytorch/fairseq/tree/master/fairseq/tasks&lt;/denchmark-link&gt;
 and  needs to be included in the task registry as well?
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Run the example configuration from the readme.
Steps to reproduce the behavior (always include the command you ran):

Run cmd

&lt;denchmark-code&gt;+ /d4m/material/software/python/singularity/bin/singularity-python.sh -i python3.6-cuda10.0 -v pytorch1.4-gpu --gpu -l
    /d4m/material/releases/fairseq/R2020_06_25:/d4m/material/releases/hycube/R2020_05_18/pycube:/d4m/material/releases/tensorflow-models/R2018_12_05:/d4m/material\
    /releases/tensor2tensor/R2019_03_15 /d4m/material/releases/fairseq/R2020_06_25/train.py --adam-betas '(0.9, 0.98)' --arch transformer --attention-dropout 0.1
    --clip-norm 0 --criterion label_smoothed_cross_entropy --dropout 0.3 --keep-last-epochs 10 --label-smoothing 0.1 --log-format simple --lr 0.0007
    --lr-scheduler inverse_sqrt --max-tokens 4000 --max-update 150000 --mean-pool-gating-network --method hMoElp --min-lr 1e-9 --optimizer adam --save-dir
    /d4m/ears/expts/46895_pashto_nmt_tr_1.forward/expts/fairseq-train --save-interval 1 --seed 7 --share-decoder-input-output-embed --task translation_moe
    --update-freq 8 --validate-interval 1 --warmup-init-lr 1e-07 --warmup-updates 4000 --weight-decay 0.0001
    /d4m/ears/expts/46882.trans.word.eval2020.60k/expts/fairseq-data
&lt;/denchmark-code&gt;


Error

&lt;denchmark-code&gt;usage: train.py [-h] [--no-progress-bar] [--log-interval N]
  48                 [--log-format {json,none,simple,tqdm}]
  49                 [--tensorboard-logdir DIR] [--seed N] [--cpu] [--tpu] [--bf16]
  50                 [--fp16] [--memory-efficient-bf16] [--memory-efficient-fp16]
  51                 [--fp16-no-flatten-grads] [--fp16-init-scale FP16_INIT_SCALE]
  52                 [--fp16-scale-window FP16_SCALE_WINDOW]
  53                 [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]
  54                 [--min-loss-scale D]
  55                 [--threshold-loss-scale THRESHOLD_LOSS_SCALE]
  56                 [--user-dir USER_DIR] [--empty-cache-freq EMPTY_CACHE_FREQ]
  57                 [--all-gather-list-size ALL_GATHER_LIST_SIZE]
  58                 [--model-parallel-size N]
  59                 [--checkpoint-suffix CHECKPOINT_SUFFIX]
  60                 [--quantization-config-path QUANTIZATION_CONFIG_PATH]
  61                 [--generate-lattice] [--lattice-outdir LATTICE_OUTDIR]
  62                 [--token-boosting-file TOKEN_BOOSTING_FILE]
  63                 [--criterion                                                                                                                                        {cross_entropy,binary_cross_entropy,label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,sentence_ranking,legacy_masked_lm_loss,adaptive_l\     oss,masked_lm,nat_loss,composite_loss,sentence_prediction,vocab_parallel_cross_entropy}]
  64                 [--tokenizer {space,nltk,moses}]
  65                 [--bpe {bytes,characters,gpt2,fastbpe,byte_bpe,hf_byte_bpe,subword_nmt,sentencepiece,bert}]
  66                 [--optimizer {adadelta,adafactor,nag,adamax,lamb,adagrad,adam,sgd}]
  67                 [--lr-scheduler {triangular,inverse_sqrt,cosine,reduce_lr_on_plateau,polynomial_decay,tri_stage,fixed}]
  68                 [--task TASK] [--num-workers N]
  69                 [--skip-invalid-size-inputs-valid-test] [--max-tokens N]
  70                 [--max-sentences N] [--required-batch-size-multiple N]
  71                 [--dataset-impl FORMAT] [--data-buffer-size N]
  72                 [--train-subset SPLIT] [--valid-subset SPLIT]
  73                 [--validate-interval N] [--fixed-validation-seed N]
  74                 [--disable-validation] [--max-tokens-valid N]
  75                 [--max-sentences-valid N] [--curriculum N]
  76                 [--distributed-world-size N]
  77                 [--distributed-rank DISTRIBUTED_RANK]
  78                 [--distributed-backend DISTRIBUTED_BACKEND]
  79                 [--distributed-init-method DISTRIBUTED_INIT_METHOD]                                                                                              80                 [--distributed-port DISTRIBUTED_PORT] [--device-id DEVICE_ID]
  81                 [--distributed-no-spawn] [--ddp-backend {c10d,no_c10d}]
  82                 [--bucket-cap-mb MB] [--fix-batches-to-gpus]
  83                 [--find-unused-parameters] [--fast-stat-sync]
  84                 [--broadcast-buffers] [--distributed-wrapper {DDP,SlowMo}]                                                                                       85                 [--slowmo-momentum SLOWMO_MOMENTUM]                                                                                                              86                 [--slowmo-algorithm {LocalSGD,SGP}]                                                                                                              87                 [--localsgd-frequency LOCALSGD_FREQUENCY]
  88                 [--nprocs-per-node N] [--arch ARCH] [--max-epoch N]
  89                 [--max-update N] [--clip-norm NORM] [--sentence-avg]
  90                 [--update-freq N1,N2,...,N_K] [--lr LR_1,LR_2,...,LR_N]
  91                 [--min-lr LR] [--use-bmuf] [--save-dir DIR]
  92                 [--restore-file RESTORE_FILE] [--reset-dataloader]
  93                 [--reset-lr-scheduler] [--reset-meters] [--reset-optimizer]
  94                 [--optimizer-overrides DICT] [--save-interval N]
  95                 [--save-interval-updates N] [--keep-interval-updates N]
  96                 [--keep-last-epochs N] [--keep-best-checkpoints N] [--no-save]
  97                 [--no-epoch-checkpoints] [--no-last-checkpoints]
  98                 [--no-save-optimizer-state]
  99                 [--best-checkpoint-metric BEST_CHECKPOINT_METRIC]
 100                 [--maximize-best-checkpoint-metric] [--patience N]
 101 train.py: error: argument --task: invalid choice: 'translation_moe' (choose from 'translation', 'translation_from_pretrained_xlm', 'denoising',
     'cross_lingual_lm', 'multilingual_masked_lm', 'sentence_ranking', 'legacy_masked_lm', 'translation_lev', 'multilingual_denoising',
     'multilingual_translation', 'semisupervised_translation', 'language_modeling', 'masked_lm', 'audio_pretraining', 'translation_from_pretrained_bart',
     'sentence_prediction', 'dummy_lm', 'dummy_masked_lm')
 102 7.15user 7.59system 0:07.60elapsed 193%CPU (0avgtext+0avgdata 1234268maxresident)k
 103 63026inputs+64outputs (337major+372614minor)pagefaults 0swaps
 104 0.00user 0.00system 0:00.00elapsed 66%CPU (0avgtext+0avgdata 552maxresident)k
 105 0inputs+0outputs (0major+249minor)pagefaults 0swaps
 106 GENERIC JOB FOR   CRASHED AT  Fri Jun 26 16:20:36 2020
 107 STATUS CODE  2
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


fairseq Version (e.g., 1.0 or master): master
PyTorch Version : 1.4
OS (e.g., Linux): centOS7
How you installed fairseq (pip, source):
Build command you used (if compiling from source):
Python version: 3.6
CUDA/cuDNN version: 10.0
GPU models and configuration: v100
Any other relevant information:

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='sk-g' date='2020-06-26T22:06:25Z'>
		Update: I moved all the files in examples/translation_moe/src/ to fairseq/tasks/ and this is working.
		</comment>
		<comment id='2' author='sk-g' date='2020-06-30T13:05:22Z'>
		You need to specify --user-dir examples/translation_moe/src.  This is what will include the translation_moe task.
Try running the exact command listed in the example:
&lt;denchmark-code&gt;fairseq-train --ddp-backend='no_c10d' \
    data-bin/wmt17_en_de \
    --max-update 100000 \
    --task translation_moe --user-dir examples/translation_moe/src \
    --method hMoElp --mean-pool-gating-network \
    --num-experts 3 \
    --arch transformer_wmt_en_de --share-all-embeddings \
    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \
    --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 4000 \
    --lr 0.0007 --min-lr 1e-09 \
    --dropout 0.1 --weight-decay 0.0 --criterion cross_entropy \
    --max-tokens 3584
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>