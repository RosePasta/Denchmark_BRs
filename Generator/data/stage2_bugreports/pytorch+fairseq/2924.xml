<bug id='2924' author='yuyan2do' open_date='2020-11-19T23:30:54Z' closed_time='2020-11-21T00:21:18Z'>
	<summary>Distributed training error [invalid usage] when using torch.distributed.launch for fairseq 0.10.0 and master branch</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Run torch.distributed.launch under 1 machine with 4 gpus. I tried to run it in several machines, but found this error can reproduce even in single machine.
&lt;denchmark-code&gt;python -m torch.distributed.launch --nproc_per_node=${NPROC_PER_NODE} \
    --nnodes=${NNODE} --node_rank=${NODE_RANK} --master_addr=${MASTER_ADDR} \
    --master_port=${MASTER_PORT} \
/opt/conda/bin/fairseq-train ... --distributed-no-spawn --ddp-backend no_c10d
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

fairseq 0.9.0: works well
fairseq 1.0.0: fairseq-train: error: unrecognized arguments: --local_rank=0
fairseq-master: RuntimeError: NCCL error in: ../torch/lib/c10d/ProcessGroupNCCL.cpp:514, invalid usage, NCCL version 2.6.3
&lt;denchmark-h:h4&gt;--- more error message log when run in master branch ---&lt;/denchmark-h&gt;

Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
File "/opt/conda/bin/fairseq-train", line 33, in 
File "/opt/conda/bin/fairseq-train", line 33, in 
File "/opt/conda/bin/fairseq-train", line 33, in 
Traceback (most recent call last):
File "/opt/conda/bin/fairseq-train", line 33, in 
sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
File "./git/tag/fairseq/fairseq_cli/train.py", line 392, in cli_main
sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
File "./git/tag/fairseq/fairseq_cli/train.py", line 392, in cli_main
File "./git/tag/fairseq/fairseq_cli/train.py", line 392, in cli_main
sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
File "./git/tag/fairseq/fairseq_cli/train.py", line 392, in cli_main
distributed_utils.call_main(cfg, main)
distributed_utils.call_main(cfg, main)
distributed_utils.call_main(cfg, main)
File "./git/tag/fairseq/fairseq/distributed_utils.py", line 322, in call_main
File "./git/tag/fairseq/fairseq/distributed_utils.py", line 322, in call_main
File "./git/tag/fairseq/fairseq/distributed_utils.py", line 322, in call_main
distributed_utils.call_main(cfg, main)
File "./git/tag/fairseq/fairseq/distributed_utils.py", line 322, in call_main
distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
File "./git/tag/fairseq/fairseq/distributed_utils.py", line 294, in distributed_main
File "./git/tag/fairseq/fairseq/distributed_utils.py", line 294, in distributed_main
File "./git/tag/fairseq/fairseq/distributed_utils.py", line 294, in distributed_main
distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
File "./git/tag/fairseq/fairseq/distributed_utils.py", line 294, in distributed_main
cfg.distributed_training.distributed_rank = distributed_init(cfg)
cfg.distributed_training.distributed_rank = distributed_init(cfg)
cfg.distributed_training.distributed_rank = distributed_init(cfg)
File "./git/tag/fairseq/fairseq/distributed_utils.py", line 248, in distributed_init
File "./git/tag/fairseq/fairseq/distributed_utils.py", line 248, in distributed_init
File "./git/tag/fairseq/fairseq/distributed_utils.py", line 248, in distributed_init
cfg.distributed_training.distributed_rank = distributed_init(cfg)
File "./git/tag/fairseq/fairseq/distributed_utils.py", line 248, in distributed_init
dist.all_reduce(torch.zeros(1).cuda())
dist.all_reduce(torch.zeros(1).cuda())
dist.all_reduce(torch.zeros(1).cuda())
File "/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 898, in all_reduce
File "/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 898, in all_reduce
File "/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 898, in all_reduce
dist.all_reduce(torch.zeros(1).cuda())
File "/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 898, in all_reduce
work = _default_pg.allreduce([tensor], opts)
work = _default_pg.allreduce([tensor], opts)
work = _default_pg.allreduce([tensor], opts)
RuntimeError: NCCL error in: ../torch/lib/c10d/ProcessGroupNCCL.cpp:514, invalid usage, NCCL version 2.6.3
RuntimeError: NCCL error in: ../torch/lib/c10d/ProcessGroupNCCL.cpp:514, invalid usage, NCCL version 2.6.3
work = _default_pg.allreduce([tensor], opts)
RuntimeError: NCCL error in: ../torch/lib/c10d/ProcessGroupNCCL.cpp:514, invalid usage, NCCL version 2.6.3
RuntimeError: NCCL error in: ../torch/lib/c10d/ProcessGroupNCCL.cpp:514, invalid usage, NCCL version 2.6.3
Traceback (most recent call last):
File "/opt/conda/lib/python3.6/runpy.py", line 193, in _run_module_as_main
"main", mod_spec)
File "/opt/conda/lib/python3.6/runpy.py", line 85, in _run_code
exec(code, run_globals)
File "/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py", line 263, in 
main()
File "/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py", line 259, in main
cmd=cmd)
	</description>
	<comments>
		<comment id='1' author='yuyan2do' date='2020-11-20T16:12:45Z'>
		This has been fixed in master but not the 0.10.0 release. We plan to release 0.10.1 soon and will include a fix for this.
The NCCL error on master doesn't make a lot of sense to me though... is there any difference in NCCL versions from when you tested 0.9.0 and master? The error is at this line, which is a simple dummy operation to test that NCCL is working:



fairseq/fairseq/distributed_utils.py


        Lines 246 to 248
      in
      3b77a61






 # perform a dummy all-reduce to initialize the NCCL communicator 



 if torch.cuda.is_available(): 



 dist.all_reduce(torch.zeros(1).cuda()) 





		</comment>
		<comment id='2' author='yuyan2do' date='2020-11-20T18:55:38Z'>
		Thanks Myle. Could you kindly point which PR fix this issue?
I tested under two docker (nvcr.io/nvidia/pytorch:20.03-py3 with NCCL version 2.6.3, and nvcr.io/nvidia/pytorch:20.10-py3 with NCCL version 2.7.8). Just pull the code in master, and run again, still see the same issue.
		</comment>
		<comment id='3' author='yuyan2do' date='2020-11-20T21:54:48Z'>
		Found the issue. These two options should actually be a single option:



fairseq/fairseq/dataclass/configs.py


        Lines 211 to 221
      in
      d464af2






 device_id: int = field( 



 default=0, 



 metadata={"help": "which GPU to use (usually configured automatically)"}, 



 ) 



 local_rank: int = field( 



 default=0, 



 metadata={ 



 "help": "which GPU to use (usually configured automatically)", 



 "argparse_alias": "--local_rank", 



     }, 



 ) 





Should become:
    device_id: int = field(
        default=0,
        metadata={
            "help": "which GPU to use (usually configured automatically)",
            "argparse_alias": "--local_rank",
        },
    )
I thought we had fixed this in &lt;denchmark-link:https://github.com/pytorch/fairseq/commit/108f7204f6ccddb676e6d52006da219ce96a02dc&gt;108f720&lt;/denchmark-link&gt;
, but it wasn't quite right.
		</comment>
		<comment id='4' author='yuyan2do' date='2020-11-21T00:21:18Z'>
		It works, thanks for the quick help. I will close this issue.
		</comment>
	</comments>
</bug>