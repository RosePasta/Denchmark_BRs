<bug id='2749' author='erip' open_date='2020-10-18T18:05:56Z' closed_time='2020-10-20T11:30:29Z'>
	<summary>`SequenceGenerator` is somewhat flaky on all platforms in `TestTranslation::test_multilingual_translation_latent_depth`</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Sometimes &lt;denchmark-link:https://github.com/pytorch/fairseq/blob/master/fairseq/sequence_generator.py#L416&gt;this&lt;/denchmark-link&gt;
 assertion is triggered in the test TestTranslation::test_multilingual_translation_latent_depth. The reason seems to be related to the fact that &lt;denchmark-link:https://github.com/pytorch/fairseq/blob/master/fairseq/sequence_generator.py#L391&gt;this condition&lt;/denchmark-link&gt;
 is sometimes not met, meaning that the num remaining sentences is never decremented.
This is likely due to some beam search non-determinism.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior (always include the command you ran):

Clone and install
Install pytest
run pytest tests/test_binaries.py::TestTranslation::test_multilingual_translation_latent_depth several times until observe failure

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

The test should not be flaky.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


fairseq Version (e.g., 1.0 or master): master
PyTorch Version (e.g., 1.0) 1.6.0
OS (e.g., Linux): OS X, Windows
How you installed fairseq (pip, source): source
Build command you used (if compiling from source): normal install commands
Python version: 3.6
CUDA/cuDNN version: n/a
GPU models and configuration: n/a
Any other relevant information: n/a

	</description>
	<comments>
		<comment id='1' author='erip' date='2020-10-18T18:13:52Z'>
		Something interesting is when it fails, lprobs from &lt;denchmark-link:https://github.com/pytorch/fairseq/blob/master/fairseq/sequence_generator.py#L304&gt;here&lt;/denchmark-link&gt;
 becomes a tensor completely full of nans.
		</comment>
		<comment id='2' author='erip' date='2020-10-18T18:23:40Z'>
		On second thought, it is likely due to the translation data generation process. Could it be the case that the data generated by create_dummy_data is numerically unstable? Or that some process in the pipeline is? It's odd that this is the only flaky test since everything uses sequence generators...
		</comment>
		<comment id='3' author='erip' date='2020-10-18T23:33:49Z'>
		Yeah there's something weird about it. I think it's because it's doing some gumbel softmax stuff. It seemed it's only flakey on MacOS, so I was going to skip the test if sys.platform.lower() == "darwin". Have you seen flakiness on linux? Perhaps we just disable for darwin + windows?
		</comment>
		<comment id='4' author='erip' date='2020-10-18T23:36:44Z'>
		I haven't tested on Linux, but it seems like CI is clear on Ubuntu. I am happy to put a skip around it for now with a TODO.
		</comment>
		<comment id='5' author='erip' date='2020-10-20T11:30:29Z'>
		I disabled it on macOS, seems okay now? I‚Äôll follow up with the author of that test about it.
		</comment>
	</comments>
</bug>