<bug id='1686' author='mingruimingrui' open_date='2020-02-08T17:37:57Z' closed_time='2020-02-08T18:01:36Z'>
	<summary>Massive performance bug: fairseq&amp;gt;=0.7 is not doing incremental decoding</summary>
	<description>
&lt;denchmark-link:https://github.com/pytorch/fairseq/blob/master/fairseq/models/transformer.py#L314&gt;TransformerModel.forward_decoder&lt;/denchmark-link&gt;
 function is not using . Meaning all official  scripts are not actually doing incremental decoding.
I first noticed something is wrong when the inference speed dropped when upgrading from fairseq-0.6.1. cProfiler told me that fairseq-0.9.0 is doing tons of extra matmul. Digging a little into the code reveals &lt;denchmark-link:https://github.com/pytorch/fairseq/blob/master/fairseq/models/transformer.py#L314-L339&gt;this&lt;/denchmark-link&gt;
...
    def forward_decoder(
        self,
        prev_output_tokens,
        encoder_out=None,
        incremental_state=None,
        features_only=False,
        **extra_args,
    ):
        attn_args = {
            "alignment_layer": self.alignment_layer,
            "alignment_heads": self.alignment_heads,
        }
        decoder_out = self.decoder(prev_output_tokens, encoder_out, **attn_args)

        if self.full_context_alignment:
            attn_args["full_context_alignment"] = self.full_context_alignment
            _, alignment_out = self.decoder(
                prev_output_tokens,
                encoder_out,
                features_only=True,
                **attn_args,
                **extra_args,
            )
            decoder_out[1]["attn"] = alignment_out["attn"]

        return decoder_out
Notice that incremental_state is not passed to self.decoder.
On a side note, fairseq-0.6.1 calls model.decoder.forward directly so there's no problem.
### in sequence_generator.py
# 0.6.1
decoder_out = list(model.decoder(tokens, encoder_out, incremental_state=incremental_states[model]))

# 0.9.0
decoder_out = list(model.forward_decoder(tokens, encoder_out=encoder_out, incremental_state=self.incremental_states[model]))
I apologize for the click bait-ish title.
	</description>
	<comments>
		<comment id='1' author='mingruimingrui' date='2020-02-08T18:01:36Z'>
		Ah my bad, that was the transformer Alignment Model. thought it still doesn't explain the massive difference in run time for matmul
		</comment>
	</comments>
</bug>