<bug id='1274' author='villmow' open_date='2019-10-19T13:55:02Z' closed_time='2020-07-14T15:42:37Z'>
	<summary>Using ResamplingDataset causes trouble with filtering to large samples</summary>
	<description>
Hi, thanks for the awesome library!
Problem: I use the ResamplingDataset to downsample frequent examples in a custom task. After every epoch a subset of the full training data is selected for the next epoch. This selection affects the filtering by size of the samples which is implemented in get_batch_iterator. However after the filtered indices are calculated, set_epoch is called again (with an incremented epoch count)! This causes a resampling, which again invalidates the filtered indices!
What happens:

I reset the dataset_to_epoch_iter to get a new iterator after each epoch.
FairseqTask.get_batch_iterator is called
Inside  get_batch_iterator the datasets ResamplingDataset.set_epoch is called, in which the resampling is implemented:



fairseq/fairseq/tasks/fairseq_task.py


        Lines 135 to 136
      in
      b8d024e






 # initialize the dataset with the correct starting epoch 



 dataset.set_epoch(epoch) 





The samples are filtered by size



fairseq/fairseq/tasks/fairseq_task.py


        Lines 138 to 152
      in
      b8d024e






 # get indices ordered by example size 



 with data_utils.numpy_seed(seed): 



 indices = dataset.ordered_indices() 



 



 # filter examples that are too large 



 if max_positions is not None: 



 indices = data_utils.filter_by_size( 



 indices, dataset, max_positions, raise_exception=(not ignore_invalid_inputs), 



     ) 



 



 # create mini-batches with given size constraints 



 batch_sampler = data_utils.batch_by_size( 



 indices, dataset.num_tokens, max_tokens=max_tokens, max_sentences=max_sentences, 



 required_batch_size_multiple=required_batch_size_multiple, 



 ) 





An EpochIterator is instantiated for which next_epoch_itr is called



fairseq/fairseq/tasks/fairseq_task.py


        Lines 155 to 164
      in
      b8d024e






 epoch_iter = iterators.EpochBatchIterator( 



 dataset=dataset, 



 collate_fn=dataset.collater, 



 batch_sampler=batch_sampler, 



 seed=seed, 



 num_shards=num_shards, 



 shard_id=shard_id, 



 num_workers=num_workers, 



 epoch=epoch, 



 ) 





The trainer calls next_epoch_itr and inside of it the epoch counter is incremented and then ResamplingDataset.set_epoch is called again, which resamples the data (and thus the filtering indices are wrong).



fairseq/fairseq/data/iterators.py


        Lines 187 to 191
      in
      b8d024e






 self.epoch += 1 



 self._cur_epoch_itr = self._get_iterator_for_epoch( 



 self.epoch, shuffle, fix_batches_to_gpus=fix_batches_to_gpus, 



     ) 



 self.dataset.set_epoch(self.epoch) 





This iterator is used and samples with wrong sizes are filtered.

I hope I could describe the issue understandable. As a quick fix for me I uncommented the set_epoch call in EpochBatchIterator, which I guess is not the right approach.
What is the correct way to handle the change of sizes after each epoch?
	</description>
	<comments>
		<comment id='1' author='villmow' date='2020-05-16T12:25:14Z'>
		I am training a model with the multilingual_denoising task, which uses the ResamplingDataset. Training crashes deterministically form me after the first epoch, which I think is because of this issue.
What happens is the after the first epoch, I get OOM exceptions and I have found that the reason is the ntokens in batches is random and usually larger than args.max_tokens. The iterator uses batches with wrong sizes.
		</comment>
		<comment id='2' author='villmow' date='2020-06-19T02:54:33Z'>
		I think this should have been fixed by &lt;denchmark-link:https://github.com/pytorch/fairseq/commit/aa79bb9c37b27e3f84e7a4e182175d3b50a79041&gt;aa79bb9&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='villmow' date='2020-07-14T15:42:37Z'>
		Please reopen if this is still an issue.
		</comment>
	</comments>
</bug>