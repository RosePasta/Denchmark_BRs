<bug id='2042' author='mohammadKhalifa' open_date='2020-04-22T08:11:49Z' closed_time='2020-05-01T11:05:58Z'>
	<summary>Masked LM training with BERT fails</summary>
	<description>
Training with a masked lm objective as follows:
fairseq-train $DATA_DIR \
		--task masked_lm --criterion masked_lm\
		--arch bert_base \
		--optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 1.0 \
		--lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \
		--dropout 0.1 --weight-decay 0.01 \
		--max-tokens $TOKENS_PER_SAMPLE --update-freq $UPDATE_FREQ \
		--max-update $TOTAL_UPDATES --log-format simple --log-interval 1 --save-interval-updates 1000\
		--valid-subset valid \
		--mask-prob 0.25\
		--random-token-prob 0.2\
		--skip-invalid-size-inputs-valid-test \
Gives me the following exception:
&lt;denchmark-code&gt;File "/home/mkhalifa/py36/bin/fairseq-train", line 11, in &lt;module&gt;
    load_entry_point('fairseq', 'console_scripts', 'fairseq-train')()
  File "/home/mkhalifa/fairseq/fairseq_cli/train.py", line 321, in cli_main
    main(args)
  File "/home/mkhalifa/fairseq/fairseq_cli/train.py", line 96, in main
    train(args, trainer, task, epoch_itr)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.6.3/lib/python3.6/contextlib.py", line 52, in inner
    return func(*args, **kwds)
  File "/home/mkhalifa/fairseq/fairseq_cli/train.py", line 176, in train
    log_output = trainer.train_step(samples)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.6.3/lib/python3.6/contextlib.py", line 52, in inner
    return func(*args, **kwds)
  File "/home/mkhalifa/fairseq/fairseq/trainer.py", line 319, in train_step
    ignore_grad=is_dummy_batch,
  File "/home/mkhalifa/fairseq/fairseq/tasks/fairseq_task.py", line 337, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/home/mkhalifa/py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/mkhalifa/fairseq/fairseq/criterions/masked_lm.py", line 52, in forward
    ignore_index=self.padding_idx,
  File "/home/mkhalifa/py36/lib/python3.6/site-packages/torch/nn/functional.py", line 1836, in nll_loss
    .format(input.size(0), target.size(0)))
ValueError: Expected input batch_size (892) to match target batch_size (223).

&lt;/denchmark-code&gt;

The same error occurs when using other masked_lm architectures such as xlm_base. Roberta architectures work fine.
	</description>
	<comments>
		<comment id='1' author='mohammadKhalifa' date='2020-04-22T14:43:33Z'>
		Confirmed that I can reproduce this on my end.  CC &lt;denchmark-link:https://github.com/ngoyal2707&gt;@ngoyal2707&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/myleott&gt;@myleott&lt;/denchmark-link&gt;
, it seems that &lt;denchmark-link:https://github.com/pytorch/fairseq/commit/718677ebb044e27aaf1a30640c2f7ab6b8fa8509&gt;718677e&lt;/denchmark-link&gt;
 breaks .  Specifically,  doesn't do anything with the  parameter in its  method.
		</comment>
		<comment id='2' author='mohammadKhalifa' date='2020-04-23T09:52:14Z'>
		&lt;denchmark-link:https://github.com/lematt1991&gt;@lematt1991&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ngoyal2707&gt;@ngoyal2707&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/myleott&gt;@myleott&lt;/denchmark-link&gt;

I created a pull request &lt;denchmark-link:https://github.com/pytorch/fairseq/pull/2050&gt;#2050&lt;/denchmark-link&gt;
 that fixed the issue
		</comment>
	</comments>
</bug>