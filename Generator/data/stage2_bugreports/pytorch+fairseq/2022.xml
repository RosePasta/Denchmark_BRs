<bug id='2022' author='villmow' open_date='2020-04-17T21:26:00Z' closed_time='2020-05-10T13:13:17Z'>
	<summary>Recent SequenceGenerator update makes it unusable with sub-classes that have a different `net_input`</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

This is not a real bug, I rather want to discuss a recent change in master.
Situation: My (transformer) encoder sub-class needs additional input during the forward path. Currently I'm feeding this to my encoder by adding it to the net_input dictionary. During validation/generation I need the additional input as well. This worked out of the box for generation, because the generator only used to forward the net_input to the encoder:



fairseq/fairseq/sequence_generator.py


         Line 133
      in
      630701e






 encoder_outs = model.forward_encoder(encoder_input) 





With a recent change of the SequenceGenerator this is not possible anymore. The generator now only feeds src_tokens and src_lengths to the encoder (probably for torchscript):



fairseq/fairseq/sequence_generator.py


        Lines 197 to 200
      in
      57526c6






 encoder_outs = self.model.forward_encoder( 



 src_tokens=encoder_input["src_tokens"], 



 src_lengths=encoder_input["src_lengths"], 



 ) 





This change requires me to sub-class the generator and add the input there as well.
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

I should be able to add additional input to my encoder, without having to sub-class the generator.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


fairseq Version (e.g., 1.0 or master): master

	</description>
	<comments>
	</comments>
</bug>