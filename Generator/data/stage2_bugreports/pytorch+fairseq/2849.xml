<bug id='2849' author='CheungZeeCn' open_date='2020-11-05T07:38:18Z' closed_time='2020-11-08T15:54:14Z'>
	<summary>Is this a bug in the implementation of  the "early stop" of  Lev-T's decoder ?</summary>
	<description>
&lt;denchmark-h:h2&gt;🐛 Bug&lt;/denchmark-h&gt;

the paper says:
&lt;denchmark-link:https://user-images.githubusercontent.com/2025362/98210196-21984d80-1f7b-11eb-9d66-8adf855a790c.png&gt;&lt;/denchmark-link&gt;

Let's say  self.early_exit[1] == 2 and layers_msk is not None,  the input will go through the first 2 layers of self.layers_msk  without sharing the modules of the fisrt 2 layers self.layers.
refer to the code: fairseq/models/nat/levenshtein_transformer.py,
&lt;denchmark-code&gt;        # in extract_features(self, prev_output_tokens, encoder_out=None, early_exit=None, layers=None, **unused)
        # input will go through the first early_exit layer's if layers is not None
        for _, layer in enumerate(layers[: early_exit]):
            x, attn, _ = layer(
                x,
                encoder_out.encoder_out if encoder_out is not None else None,
                encoder_out.encoder_padding_mask if encoder_out is not None else None,
                self_attn_mask=None,
                self_attn_padding_mask=decoder_padding_mask,
            )
            inner_states.append(x)

        # but here, if self.early_exit[1] == 2 and layers_msk is not None, the input will not go through the first 2 layers of self.layers. 
        def forward_mask_ins(self, normalize, encoder_out, prev_output_tokens, **unused):
            features, extra = self.extract_features(
                prev_output_tokens, encoder_out=encoder_out, early_exit=self.early_exit[1], layers=self.layers_msk, **unused
            )
&lt;/denchmark-code&gt;

	</description>
	<comments>
	</comments>
</bug>