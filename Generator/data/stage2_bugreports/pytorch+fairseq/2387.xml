<bug id='2387' author='xx-zhou16' open_date='2020-07-28T08:01:32Z' closed_time='2020-08-07T22:08:11Z'>
	<summary>Distributed training crashes without `--distributed-no-spawn`</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I run distributed training on a single node with 4 GPUs.
My training script works normally with  --distributed-no-spawn. But when I delete --distributed-no-spawn and run the script, it crashes.
I find the argument in the documentation, where it shows that "do not spawn multiple processes even if multiple GPUs are visible, Default: False". Actually, I think I need the program to spawn multiple processes in my situation. But it crashes without that flag.   What does the flag really mean? When should I use it? And when shouldn't I use it? Why does my program crash without it?
The stdout and stderr are as the following:
&lt;denchmark-code&gt;*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal per
formance in your application as needed.
*****************************************
2020-07-28 07:52:07 | INFO | fairseq.distributed_utils | distributed init (rank 3): env://
2020-07-28 07:52:07 | INFO | fairseq.distributed_utils | distributed init (rank 4): env://
2020-07-28 07:52:07 | INFO | fairseq.distributed_utils | distributed init (rank 5): env://
2020-07-28 07:52:07 | INFO | fairseq.distributed_utils | distributed init (rank 0): env://
2020-07-28 07:52:07 | INFO | fairseq.distributed_utils | distributed init (rank 1): env://
2020-07-28 07:52:07 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 1
2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | distributed init (rank 5): env://
2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 5
2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | distributed init (rank 2): env://
2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 2
2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | distributed init (rank 3): env://
2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 3
2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 0
2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | distributed init (rank 4): env://
2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 4
2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | distributed init (rank 1): env://
2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 1
2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | distributed init (rank 2): env://
2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 2
2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | distributed init (rank 2): env://
2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 2
2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | distributed init (rank 4): env://
2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 4
2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 3
2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | distributed init (rank 3): env://
2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 3
2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | distributed init (rank 6): env://
2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 6
2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 3
2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 4
2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 5
2020-07-28 07:52:12 | INFO | fairseq_cli.train | Namespace(activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_input_cutoff=No
ne, adaptive_input_factor=4, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_softmax_factor=4, add_bos_token=False, all_gather_list_size=16384, arch='
transformer_lm', attention_dropout=0.0, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, char_embedder_highway_layers=2, c
haracter_embedding_dim=4, character_embeddings=False, character_filters='[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', checkpoint_suffix='', cl
ip_norm=0.0, cpu=False, criterion='cross_entropy', curriculum=0, data='/mnt/distributed/bin', data_buffer_size=10, dataset_impl=None, ddp
_backend='c10d', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layerdrop=0, decoder_layers=3, decoder_l
ayers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl',
 distributed_init_method='env://', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=4, distributed_wrapper='DDP', dropout=0.0,
 empty_cache_freq=0, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_
flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, future_target=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, laye
rnorm_embedding=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0007], lr_scheduler='inverse_sqrt', max_epoch=200, max_sentences=None, max_sentence
s_valid=None, max_target_positions=None, max_tokens=8000, max_tokens_valid=8000, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory
_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_decoder_final_norm=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no
_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node
=4, num_workers=20, optimizer='adam', optimizer_overrides='{}', output_dictionary_size=-1, past_target=False, patience=-1, profile=False, quant_noise_pq=0, quant_noise_p
q_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False
, reset_optimizer=False, restore_file='checkpoint_last.pt', sample_break_mode='eos', save_dir='/mnt/distributed/checkpoints/transformer_l
m/1blm-3L-1024ED-4096FD-16H-shared-lr0.0007-SEED1-DROPOUT0.0-CRITERIONcross_entropy', save_interval=1, save_interval_updates=0, seed=1, self_target=Fa
lse, sentence_avg=False, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', skip_invalid_size_inputs_valid_test=False, slowmo_algo
rithm='LocalSGD', slowmo_momentum=None, task='language_modeling', tensorboard_logdir='/mnt/distributed/checkpoints/transformer_lm/1blm-3L
-1024ED-4096FD-16H-shared-lr0.0007-SEED1-DROPOUT0.0-CRITERIONcross_entropy', threshold_loss_scale=None, tie_adaptive_proj=False, tie_adaptive_weights=
False, tokenizer=None, tokens_per_sample=1024, tpu=False, train_subset='train', update_freq=[8], use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid',
 validate_interval=1, warmup_init_lr=1e-07, warmup_updates=8000, weight_decay=0.0)
2020-07-28 07:52:13 | INFO | fairseq.tasks.language_modeling | dictionary: 50261 types
2020-07-28 07:52:13 | INFO | fairseq.data.data_utils | loaded 300613 examples from: /mnt/distributed/bin/valid
Traceback (most recent call last):
  File "/home/msrauser/anaconda3/envs/fairseq-py36-torch14/bin/fairseq-train", line 33, in &lt;module&gt;
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py", line 372, in cli_main
    cli_main_helper(args)
  File "/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py", line 387, in cli_main_helper
    nprocs=torch.cuda.device_count(),
  File "/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 171, in spawn
    while not spawn_context.join():
  File "/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 118, in join
    raise Exception(msg)
Exception:

-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File "/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 19, in _wrap
    fn(i, *args)
  File "/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py", line 360, in distributed_main
    args, init_distributed=True, after_distributed_init_fn=after_distributed_init_fn
  File "/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py", line 63, in main
    args.distributed_rank = distributed_utils.distributed_init(args)
  File "/home/msrauser/xiangxin/project/fairseq/fairseq/distributed_utils.py", line 99, in distributed_init
    dist.all_reduce(torch.zeros(1).cuda())
  File "/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 902, in all_reduce
    work = _default_pg.allreduce([tensor], opts)
RuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1579022034529/work/torch/lib/c10d/../c10d/NCCLUtils.hpp:78, invalid argument, NCCL version 2.4.8
Traceback (most recent call last):
  File "/home/msrauser/anaconda3/envs/fairseq-py36-torch14/bin/fairseq-train", line 33, in &lt;module&gt;
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py", line 372, in cli_main
    cli_main_helper(args)
  File "/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py", line 387, in cli_main_helper
    nprocs=torch.cuda.device_count(),
  File "/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 171, in spawn
    while not spawn_context.join():
  File "/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 118, in join
    raise Exception(msg)
Exception:

-- Process 2 terminated with the following error:
Traceback (most recent call last):
  File "/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 19, in _wrap
    fn(i, *args)
  File "/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py", line 360, in distributed_main
    args, init_distributed=True, after_distributed_init_fn=after_distributed_init_fn
  File "/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py", line 63, in main
    args.distributed_rank = distributed_utils.distributed_init(args)
  File "/home/msrauser/xiangxin/project/fairseq/fairseq/distributed_utils.py", line 99, in distributed_init
    dist.all_reduce(torch.zeros(1).cuda())
  File "/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 902, in all_reduce
    work = _default_pg.allreduce([tensor], opts)
RuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1579022034529/work/torch/lib/c10d/../c10d/NCCLUtils.hpp:78, invalid argument, NCCL version 2.4.8
Traceback (most recent call last):
  File "/home/msrauser/anaconda3/envs/fairseq-py36-torch14/bin/fairseq-train", line 33, in &lt;module&gt;
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py", line 372, in cli_main
    cli_main_helper(args)
  File "/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py", line 387, in cli_main_helper
    nprocs=torch.cuda.device_count(),
  File "/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 171, in spawn
    while not spawn_context.join():
  File "/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 118, in join
    raise Exception(msg)
Exception:

-- Process 3 terminated with the following error:
Traceback (most recent call last):
  File "/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 19, in _wrap
    fn(i, *args)
  File "/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py", line 360, in distributed_main
    args, init_distributed=True, after_distributed_init_fn=after_distributed_init_fn
  File "/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py", line 63, in main
    args.distributed_rank = distributed_utils.distributed_init(args)
  File "/home/msrauser/xiangxin/project/fairseq/fairseq/distributed_utils.py", line 99, in distributed_init
    dist.all_reduce(torch.zeros(1).cuda())
  File "/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 902, in all_reduce
    work = _default_pg.allreduce([tensor], opts)
RuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1579022034529/work/torch/lib/c10d/../c10d/NCCLUtils.hpp:78, invalid argument, NCCL version 2.4.8
Traceback (most recent call last):
  File "/home/msrauser/anaconda3/envs/fairseq-py36-torch14/bin/fairseq-train", line 33, in &lt;module&gt;
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py", line 372, in cli_main
    cli_main_helper(args)
  File "/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py", line 387, in cli_main_helper
    nprocs=torch.cuda.device_count(),
  File "/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 171, in spawn
    while not spawn_context.join():
  File "/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 118, in join
    raise Exception(msg)
Exception:

-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File "/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 19, in _wrap
    fn(i, *args)
  File "/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py", line 360, in distributed_main
    args, init_distributed=True, after_distributed_init_fn=after_distributed_init_fn
  File "/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py", line 63, in main
    args.distributed_rank = distributed_utils.distributed_init(args)
  File "/home/msrauser/xiangxin/project/fairseq/fairseq/distributed_utils.py", line 99, in distributed_init
    dist.all_reduce(torch.zeros(1).cuda())
  File "/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 902, in all_reduce
    work = _default_pg.allreduce([tensor], opts)
RuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1579022034529/work/torch/lib/c10d/ProcessGroupNCCL.cpp:410, unhandled system error, NCCL version 2.4.8
Traceback (most recent call last):
  File "/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/distributed/launch.py", line 263, in &lt;module&gt;
    main()
  File "/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/distributed/launch.py", line 259, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/home/msrauser/anaconda3/envs/fairseq-py36-torch14/bin/python', '-u', '/home/msrauser/anaconda3/envs/fairseq-py36-torch14/bin/f
airseq-train', '--local_rank=3', '/mnt/distributed-xiangxin-50006/sampling-distill/bin', '--task', 'language_modeling', '--share-decoder-input-output-embed', '--arch', '
transformer_lm', '--clip-norm', '0.0', '--lr-scheduler', 'inverse_sqrt', '--warmup-init-lr', '1e-07', '--warmup-updates', '8000', '--optimizer', 'adam', '--adam-betas',
'(0.9, 0.98)', '--decoder-layers', '3', '--decoder-embed-dim', '1024', '--decoder-ffn-embed-dim', '4096', '--decoder-attention-heads', '16', '--lr', '0.0007', '--min-lr'
, '1e-09', '--weight-decay', '0.0', '--criterion', 'cross_entropy', '--dropout', '0.0', '--save-dir', '/mnt/distributed/checkpoints/trans
former_lm/1blm-3L-1024ED-4096FD-16H-shared-lr0.0007-SEED1-DATAsampling-beam1-DROPOUT0.0-CRITERIONcross_entropy', '--max-epoch', '200', '--max-tokens', '8000', '--update-
freq', '8', '--no-progress-bar', '--seed', '1', '--sample-break-mode', 'eos', '--num-workers', '20', '--tensorboard-logdir', '/mnt/distributed/checkpoints/transformer_lm/1blm-3L-1024ED-4096FD-16H-shared-lr0.0007-SEED1-DROPOUT0.0-CRITERIONcross_entropy', '--distributed-backend', 'nccl']'
 returned non-zero exit status 1.

&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

My training script is as the following:
&lt;denchmark-code&gt;MASTER_IP="127.0.0.1"
MASTER_PORT=1234
TOTAL_NODES_NUM=1
NODE_RANK=$1

DATA_DIR=/mnt/distributed/bin

arch=transformer_lm
seed=1
lr=0.0007
decoder_layers=3
decoder_embed_dim=1024
decoder_ffn_embed_dim=4096
decoder_attention_heads=16
dropout=0.0
criterion=cross_entropy
label_smoothing=0.0

SAVE_DIR=/mnt/distributed/checkpoints/${arch}/1blm-${decoder_layers}L-${decoder_embed_dim}ED-${decoder_ffn_embed_dim}FD-${decoder_attention_heads}H-shared-lr${lr}-SEED${seed}-DROPOUT${dropout}-CRITERION${criterion}

RESTORE_FILE=/mnt/distributed/restore-checkpoint/checkpoint23.pt

RESTORE_FILE=/mnt/distributed-xiangxin-${SERVER_PORT}/sampling-distill/restore-checkpoint/checkpoint23.pt
CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node=4 \
    --nnodes=${TOTAL_NODES_NUM} --node_rank=${NODE_RANK} --master_addr=${MASTER_IP} \
    --master_port=${MASTER_PORT} \
    $(which fairseq-train) ${DATA_DIR} \
    --task language_modeling \
    --share-decoder-input-output-embed \
    --arch ${arch} \
    --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 8000 \
    --optimizer adam --adam-betas '(0.9, 0.98)' \
    --decoder-layers ${decoder_layers} \
    --decoder-embed-dim ${decoder_embed_dim} \
    --decoder-ffn-embed-dim ${decoder_ffn_embed_dim} \
    --decoder-attention-heads ${decoder_attention_heads} \
    --lr ${lr} --min-lr 1e-09 --weight-decay 0.0 \
    --criterion ${criterion} --dropout ${dropout} \
    --save-dir ${SAVE_DIR} \
    --max-epoch 200 --max-tokens 8000 --update-freq 8 \
    --no-progress-bar --seed ${seed} \
    --sample-break-mode eos \
    --num-workers 20 \
    --tensorboard-logdir ${SAVE_DIR} \
    --distributed-backend nccl \
    --distributed-no-spawn \
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


fairseq Version (e.g., 1.0 or master): master
PyTorch Version (e.g., 1.0) : 1.4.0
OS (e.g., Linux): Ubuntu 16.04
How you installed fairseq (pip, source): source
Build command you used (if compiling from source):
Python version: python3.6
CUDA/cuDNN version: CUDA10.1
GPU models and configuration: 4 V100
Any other relevant information:

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='xx-zhou16' date='2020-07-30T17:18:57Z'>
		The problem is that torch.distributed.launch with --nproc_per_node=4 will launch 4 processes per node, so we don't want fairseq to additionally launch processes. We can infer this automatically though, it shouldn't require the user to specify the flag. I'll submit a fix.
		</comment>
		<comment id='2' author='xx-zhou16' date='2020-08-07T22:08:11Z'>
		This was fixed in &lt;denchmark-link:https://github.com/pytorch/fairseq/commit/93f5128509278f425afb6bcf0da574c0af0e0c16&gt;93f5128&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>