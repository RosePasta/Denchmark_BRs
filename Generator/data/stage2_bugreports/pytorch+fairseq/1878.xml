<bug id='1878' author='patil-suraj' open_date='2020-03-22T10:06:37Z' closed_time='2020-03-24T03:06:33Z'>
	<summary>BART multi-gpu fine-tuning error</summary>
	<description>
the train.py gives this error while fine-tuning bart on CNN/DM
&lt;denchmark-code&gt;2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:16775
2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | distributed init (rank 4): tcp://localhost:16775
2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:16775
2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | distributed init (rank 6): tcp://localhost:16775
2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | initialized host instance-3 as rank 6
2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:16775
2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | initialized host instance-3 as rank 3
2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | distributed init (rank 7): tcp://localhost:16775
2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | initialized host instance-3 as rank 7
2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | distributed init (rank 5): tcp://localhost:16775
2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:16775
2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | initialized host instance-3 as rank 5
2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | initialized host instance-3 as rank 1
2020-03-22 10:03:56 | INFO | fairseq.distributed_utils | initialized host instance-3 as rank 2
2020-03-22 10:03:56 | INFO | fairseq.distributed_utils | initialized host instance-3 as rank 4
2020-03-22 10:03:56 | INFO | fairseq.distributed_utils | initialized host instance-3 as rank 0
Traceback (most recent call last):
  File "train.py", line 11, in &lt;module&gt;
    cli_main()
  File "/home/surajp815/fairseq/fairseq_cli/train.py", line 317, in cli_main
    nprocs=args.distributed_world_size,
  File "/home/surajp815/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 171, in spawn
    while not spawn_context.join():
  File "/home/surajp815/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 107, in join
    (error_index, name)
Exception: process 0 terminated with signal SIGKILL
&lt;/denchmark-code&gt;

I've installed fairseq from master branch
Training on 8 V100 GPU's
torch version 1.4.0
	</description>
	<comments>
		<comment id='1' author='patil-suraj' date='2020-03-22T10:23:46Z'>
		When I use 1 or 2 GPUs the training runs fine
		</comment>
		<comment id='2' author='patil-suraj' date='2020-03-22T10:38:01Z'>
		Turned out to be RAM issue, I had 14 GB RAM and it got full. How much RAM is recommended for fine-tuning BART on CNN/DM with 8 GPUs ?
		</comment>
	</comments>
</bug>