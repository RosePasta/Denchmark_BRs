<bug id='2811' author='thpun' open_date='2020-10-28T15:13:41Z' closed_time='2020-11-04T04:45:20Z'>
	<summary>Got "'NoneType' object has no attribute 'zero_'" when running with zero sharding</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Failed to train with --zero-sharding os. Got the captioned error once the training really starts.
I'm doing multilingual fine-tuning in mBART.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;


Run cmd

PREFIX=mbart-mlft
lang_pairs=...
DATA=train-data/
lang_list=models/$PREFIX/lang_list
MODEL=models/$PREFIX/model.pt
CUDA_VISIBLE_DEVICES=0,1,2,3 fairseq-train $DATA \
  --finetune-from-model $MODEL \
  --encoder-normalize-before --decoder-normalize-before \
  --arch mbart_large --layernorm-embedding \
  --task translation_multi_simple_epoch \
  --sampling-method "temperature" \
  --sampling-temperature 5 \
  --encoder-langtok "src" --decoder-langtok \
  --lang-dict "$lang_list" --lang-pairs "$lang_pairs" \
  --fp16-no-flatten-grads --zero-sharding os \
  --criterion label_smoothed_cross_entropy --label-smoothing 0.2 \
  --optimizer adam --adam-eps 1e-06 --adam-betas '(0.9, 0.98)' \
  --lr-scheduler inverse_sqrt --lr 6e-05 --min-lr -1 --warmup-updates 2500 \
  --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0 \
  --max-tokens 1536 --update-freq 16 --upsample-primary 3 \
  --save-interval-updates 5000 --no-epoch-checkpoints \
  --seed 222 --log-format simple --log-interval 10 --ddp-backend no_c10d \
  --fp16 --max-update 250000 --save-dir models/$PREFIX

See error

&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 20, in _wrap
    fn(i, *args)
  File "/workspace/fairseq/fairseq/distributed_utils.py", line 283, in distributed_main
    main(cfg, **kwargs)
  File "/workspace/fairseq/fairseq_cli/train.py", line 124, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/opt/conda/lib/python3.6/contextlib.py", line 52, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq/fairseq_cli/train.py", line 202, in train
    log_output = trainer.train_step(samples)
  File "/opt/conda/lib/python3.6/contextlib.py", line 52, in inner
    return func(*args, **kwds)
  File "/workspace/fairseq/fairseq/trainer.py", line 459, in train_step
    self.zero_grad()
  File "/workspace/fairseq/fairseq/trainer.py", line 783, in zero_grad
    self.optimizer.zero_grad()
  File "/workspace/fairseq/fairseq/optim/fp16_optimizer.py", line 218, in zero_grad
    p32.grad.zero_()
AttributeError: 'NoneType' object has no attribute 'zero_'
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


fairseq Version (e.g., 1.0 or master): Master (commit 1bc83c7)
fairscale Version: 0.0.3
PyTorch Version (e.g., 1.0) 1.7.0a0+8deb4fe
OS (e.g., Linux): Linux
How you installed fairseq (pip, source): source
Build command you used (if compiling from source): pip install --editable .
Python version: 3.6.10
CUDA/cuDNN version: 11.0
GPU models and configuration: 4x V100 in one machine

	</description>
	<comments>
		<comment id='1' author='thpun' date='2020-11-03T21:43:00Z'>
		There's a problem with the latest fairscale release. You can downgrade to unblock yourself: pip install fairscale=0.0.2. Alternatively I'll merge a fix shortly.
		</comment>
	</comments>
</bug>