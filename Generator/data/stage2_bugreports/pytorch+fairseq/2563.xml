<bug id='2563' author='arminarj' open_date='2020-09-03T08:01:24Z' closed_time='2020-09-04T03:37:12Z'>
	<summary>wav2vec2 same-quantizer return None as the input_quantizer</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

For reproducing models based on the Wav2Vec2 platform, there is an option for using the input quantizer and the target quantizer and use the same quantizer if it's needed.
However, it seems there is a small issue when using the same_quantizer flag. Firstly, it seems the same_quantizer is not defined, and secondly the input_quantizer flag will be None after that.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

You can reproduce a model with the same quantizer module with the below flags.
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

based on the documentation we can train the base model with these flags, &lt;denchmark-link:https://github.com/pytorch/fairseq/tree/master/examples/wav2vec&gt;Documentation&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;$ python train.py --distributed-world-size 64 --distributed-port $PORT /manifest/path \
--save-dir /model/path --fp16 --num-workers 6 --task audio_pretraining --criterion wav2vec --arch wav2vec2 \
--log-keys '["prob_perplexity","code_perplexity","temp"]' --quantize-targets --extractor-mode default \
--conv-feature-layers '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] * 2' --final-dim 256 --latent-vars 320 \
--latent-groups 2 --latent-temp '(2,0.5,0.999995)' --infonce --optimizer adam \
--adam-betas '(0.9,0.98)' --adam-eps 1e-06 --lr-scheduler polynomial_decay --total-num-update 400000 \
--lr 0.0005 --warmup-updates 32000 --mask-length 10 --mask-prob 0.65 --mask-selection static --mask-other 0 \
--encoder-layerdrop 0.05 --dropout-input 0.1 --dropout-features 0.1 --feature-grad-mult 0.1 \
--loss-weights '[0.1, 10]' --conv-pos 128 --conv-pos-groups 16 --num-negatives 100 --cross-sample-negatives 0 \
--max-sample-size 250000 --min-sample-size 32000 --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \
--max-tokens 1400000 --max-update 400000 --skip-invalid-size-inputs-valid-test --ddp-backend no_c10d
&lt;/denchmark-code&gt;

And if we want to use the same quantizer module as the input_ quantizer and target_ quantizer, we should use these additional flags,
&lt;denchmark-code&gt;--quantize-input  --quantize-targets --same_quantizer
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;



We expect there should be a flag called same_quantizer as this line explained link


There should exist one same Quantizer module in the self.input_quantizer and the self.quantizer when we call the --same-quantizer flag. However, the self.input_quantizer becomes the None as the self.quantizer will be defined in the next lines.


	</description>
	<comments>
		<comment id='1' author='arminarj' date='2020-09-03T20:13:58Z'>
		thanks for the catch, i must have introduced this bug when i was porting code from my dev branch. will fix
		</comment>
	</comments>
</bug>