<bug id='6318' author='AndriyMulyar' open_date='2020-10-28T21:34:19Z' closed_time='2021-01-07T07:48:34Z'>
	<summary>Cannot use BiLSTM encoder with transition based NER parser. [spacy-nightly]</summary>
	<description>
I get the following error when attempting to use the BiLSTM encoder with the NER transition based parser:
ℹ Using CPU

=========================== Initializing pipeline ===========================
✘ Can't construct config: calling registry function
(build_Tok2Vec_model) failed
spacy.Tok2Vec.v1   "Cannot get dimension 'nO' for model 'with_padded(with_padded(pytorch))'"

{'model': {'@architectures': 'spacy.Tok2Vec.v1', 'embed': {'@architectures': 'spacy.MultiHashEmbed.v1', 'width': 100, 'attrs': ['ORTH', 'SHAPE'], 'rows': [5000, 2500], 'include_static_vectors': 'True'}, 'encode': {'@architectures': 'spacy.TorchBiLSTMEncoder.v1', 'width': 100, 'depth': 2, 'dropout': 0.30000000000000004}}}
Could the pytorch BiLSTM wrapper encoder perhaps be missing a line like this (which appears in the CNN encoders):



spaCy/spacy/ml/models/tok2vec.py


         Line 274
      in
      dc816bb






 model.set_dim("nO", width) 








spaCy/spacy/ml/models/tok2vec.py


         Line 302
      in
      dc816bb






 @registry.architectures.register("spacy.TorchBiLSTMEncoder.v1") 





Also the docstring is not accurate (seems to be copied from the CNNs)
My model components look like this:
[components]

[components.tok2vec]
factory = "tok2vec"

[components.tok2vec.model]
@architectures = "spacy.Tok2Vec.v1"

[components.tok2vec.model.embed]
@architectures = "spacy.MultiHashEmbed.v1"
width = ${components.tok2vec.model.encode.width}
attrs = ["ORTH", "SHAPE"]
rows = [5000, 2500]
include_static_vectors = True

[components.tok2vec.model.encode]
@architectures = "spacy.TorchBiLSTMEncoder.v1"
width = 100
depth = 2
dropout = ${training.dropout}

[components.ner]
factory = "ner"

[components.ner.model]
@architectures = "spacy.TransitionBasedParser.v1"
state_type = "ner"
extra_state_tokens = false
hidden_width = 64
maxout_pieces = 2
use_upper = true
nO = null

[components.ner.model.tok2vec]
@architectures = "spacy.Tok2VecListener.v1"
width = ${components.tok2vec.model.encode.width}
	</description>
	<comments>
		<comment id='1' author='AndriyMulyar' date='2020-10-29T09:06:39Z'>
		Thanks for the report! That does look suspicious. Usually the nO dimension should be set by initializing the model with example input &amp; output, so that it can infer the dimensions, but this may not work with the Torch encoder. You could be right that we need to set it specifically. I'll have a look!
		</comment>
		<comment id='2' author='AndriyMulyar' date='2020-11-24T16:45:59Z'>
		This issue was a bit more involved than I had initially hoped, but I think &lt;denchmark-link:https://github.com/explosion/spaCy/pull/6442&gt;#6442&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/explosion/thinc/pull/432&gt;explosion/thinc#432&lt;/denchmark-link&gt;
 together should fix this. At least the training now runs for me when I replicate your config.
Thanks again for the detailed report!
[EDIT: it works when I set include_static_vectors to False and fails with a different error otherwise, but that's another issue that I'm looking into]
[EDIT 2: Never mind the above, it works with static vectors as well when setting e.g. vectors = "en_core_web_lg"]
		</comment>
	</comments>
</bug>