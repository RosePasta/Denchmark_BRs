<bug id='4554' author='adrianeboyd' open_date='2019-10-30T11:54:49Z' closed_time='2019-12-16T08:12:59Z'>
	<summary>Issues with gold alignment</summary>
	<description>
&lt;denchmark-h:h2&gt;How to reproduce the behaviour&lt;/denchmark-h&gt;

The new alignment method from &lt;denchmark-link:https://github.com/tamuhey&gt;@tamuhey&lt;/denchmark-link&gt;
 is very nice (&lt;denchmark-link:https://github.com/explosion/spaCy/pull/4526&gt;#4526&lt;/denchmark-link&gt;
) but it doesn't allow substitutions, which is problem for some of our corpora with minor differences between the raw texts and token orths in things like quotes. Ideally we would require that the token orths exactly match the texts, but since we do have some slightly noisy corpora, I think we need to keep using the old align, or at least a method that allows some noise.
Comparing the new and old align functions was very useful because it brought our attention to the old alignment code, which has some bugs we can fix:


j2i_multi is incorrect (from #4525). The problem is that _get_missing() isn't symmetric, modifies miss regions as it's working, and doesn't quite backtrack enough when alignments aren't found (I think, anyway). It could probably be rewritten, but a quick solution is to call it twice with reversed arguments to get i2j_multi and j2i_multi separately, since the first returned alignment i2j_multi (which is all we use in spacy.gold, as far as I know) is correct.


The Levenshtein distance costs are different with and without substitutions (old vs. new). An example:


&lt;denchmark-code&gt;words1 = ["a", "b", "cd"]
words2 = ["ab", "c", "d"]
&lt;/denchmark-code&gt;

The old alignment code says the cost is 3 (3 substitutions) and the new alignment code says it's 6 (3 insertions and 3 deletions).
The old alignment produces this:
&lt;denchmark-code&gt;cost:         3
i2j:          [-1  1 -1]
j2i:          [-1  1 -1]
i2j_multi:    {}
j2i_multi:    {}
&lt;/denchmark-code&gt;

New alignment:
&lt;denchmark-code&gt;cost:         6
i2j:          [-1 -1 -1]
j2i:          [-1 -1 -1]
i2j_multi:    {0: 0, 1: 0}
j2i_multi:    {1: 2, 2: 2}
&lt;/denchmark-code&gt;


Looking at the previous example, I think that the results for i2j are a little unexpected for the old alignment. I think it's odd that tokens of the same length count as substitutions while tokens of different lengths don't and that's the only thing taken into consideration.




spaCy/spacy/_align.pyx


        Lines 107 to 112
      in
      c2f5f9f






 for i in range(i2j.shape[0]): 



 if i2j[i] &gt;= 0 and len(S[i]) != len(T[i2j[i]]): 



         i2j[i] = -1 



 for j in range(j2i.shape[0]): 



 if j2i[j] &gt;= 0 and len(T[j]) != len(S[j2i[j]]): 



         j2i[j] = -1 





This might (nearly) work for the kind of punctuation noise present in the English data, but I wouldn't be surprised if there are some strange alignments for other corpora.
I think this could be solved in a few ways, either by increasing the cost of substitution in the Levenshtein calculation in most cases or modifying the "unaligned" filtering for i2j to be a little more sophisticated.
I think a custom substitution cost would be the most general way to handle this, with a cost that's higher than insert+deletion for everything except certain character classes like punctuation. I kind of hesitate to have much language-specific or writing system-specific code here, but maybe it's a good idea to limit the kinds of noise in the data to some degree and not allow just any same-length string substitutions.

The simple alignment does not consider multiple equivalent alignments and the multi alignments do not find partial alignments (see #4569).

&lt;denchmark-code&gt;words1 = ["a", "b", "c", "d", "d", "e"]
words2 = ["ab", "d", "e"]
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;cost:       4
i2j:        [-1, -1, -1,  1, -1,  2]
j2i:        [-1,  3,  5]
i2j_multi:  {}
j2i_multi:  {}
&lt;/denchmark-code&gt;

The first d is aligned and the equivalent alignment with the second d is not considered. The multi alignment does not find ab within a b c. I suspect that handling this well is going to get too complicated.

This is more GoldParse than the align code, but subtoks can lead to incorrect NER labels, including None in some cases. I haven't looked at the details yet, but here's an example:

&lt;denchmark-code&gt;284 T Token subtok None
285 o Token subtok None
286 k Token subtok None
287 e Token subtok None
288 n Token compound:nn None
289 令 令牌 subtok I-PRODUCT
290 牌 令牌 nmod:prep L-PRODUCT
&lt;/denchmark-code&gt;

In a document with a leading space (that I introduced and could normalize, but this still shouldn't happen), the first space is included into subtok and shouldn't be:
&lt;denchmark-code&gt;0   USBKey subtok B-PRODUCT
1 U USBKey subtok I-PRODUCT
2 S USBKey subtok I-PRODUCT
3 B USBKey subtok I-PRODUCT
4 K USBKey subtok I-PRODUCT
5 e USBKey subtok I-PRODUCT
6 y USBKey nsubj L-PRODUCT
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Minor issues&lt;/denchmark-h&gt;


There's no need for separate fill_i2j() and fill_j2i() functions. fill_j2i() is just fill_i2j(matrix.transpose()).

	</description>
	<comments>
		<comment id='1' author='adrianeboyd' date='2019-12-16T08:12:58Z'>
		There were only a few differences between raw and orth in existing training corpora that were easy to fix by hand, so we decided to disallow substitutions and replace the old alignment method with the new one.
		</comment>
		<comment id='2' author='adrianeboyd' date='2020-01-01T17:28:11Z'>
		I created an alignment library for this purpose. &lt;denchmark-link:https://github.com/tamuhey/tokenizations&gt;https://github.com/tamuhey/tokenizations&lt;/denchmark-link&gt;

This is based on "shortest edit script" and can handle noisy tokens.
I'll try to introduce it to spaCy.
		</comment>
		<comment id='3' author='adrianeboyd' date='2020-01-31T20:25:38Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>