<bug id='719' author='mlehl88' open_date='2017-01-04T18:36:36Z' closed_time='2017-03-18T15:17:43Z'>
	<summary>Tokens can be lemmatized into the empty string</summary>
	<description>
It seems like the token 's' can sometimes become lemmatized into the empty string.
&lt;denchmark-code&gt;&gt;&gt;&gt; from spacy.en import English
&gt;&gt;&gt; nlp = English()
&gt;&gt;&gt; my_string = """s..."""
&gt;&gt;&gt; tokens = nlp(my_string)
&gt;&gt;&gt; for t in tokens:
...     print("token: %s, lemma: %s" % (t, t.lemma_))
... 
token: s, lemma: 
token: ..., lemma: ...
&lt;/denchmark-code&gt;

In contrast, this does not happen if the string 's' is not followed by ellipsis dots:
&lt;denchmark-code&gt;&gt;&gt;&gt; my_string = """s"""
&gt;&gt;&gt; tokens = nlp(my_string)
&gt;&gt;&gt; for t in tokens:
...     print("token: %s, lemma: %s" % (t, t.lemma_))
... 
token: s, lemma: s
&lt;/denchmark-code&gt;

This behaviour might cause unexpected results in downstream applications. One consequence is that textacy sometimes extracts the empty string as a keyword.
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;


Operating System: Mac OS X Version 10.10.5
Python Version Used: 3.6.0
spaCy Version Used: 1.5
Environment Information: Anaconda virtual environment
Spacy model: en-1.1.0

	</description>
	<comments>
		<comment id='1' author='mlehl88' date='2018-05-09T01:39:07Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>