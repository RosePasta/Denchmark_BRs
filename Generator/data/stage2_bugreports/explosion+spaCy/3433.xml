<bug id='3433' author='carlsonhang' open_date='2019-03-19T17:04:44Z' closed_time='2019-03-23T12:46:28Z'>
	<summary>Saving/loading custom model in v2.1.0 causing unexpected parsing</summary>
	<description>
&lt;denchmark-h:h2&gt;How to reproduce the behaviour&lt;/denchmark-h&gt;

I have updated both Spacy to v2.1.0 and the base "en_core_web_sm" model to v2.1.0 and attempting to re-train my models. For some reason, before saving out the newly trained model, the parsing is perfectly fine with good accuracy. But when I save out the model, and try to load it in elsewhere for testing, the parsing is completely off. For example:
This is a comparison that I make after training the model to manually compute accuracy before saving it (free fluid is merged):
Sentence: a mild degree of free fluid in the pelvis is noted
Labeled Data: ['-', 'amod', 'nmod', 'prep', 'ROOT', 'prep', '-', 'refer', 'prep', 'amod']
Labeled Data: [2, 2, 3, 4, 4, 4, 7, 5, 4, 8]
Trained Model: ['-', 'amod', 'nmod', 'prep', 'ROOT', 'prep', '-', 'refer', 'amod', 'amod']
Trained Model: [2, 2, 3, 4, 4, 4, 7, 5, 4, 8]
Accuracy: 0.95
This is after saving out the model and loading it in for testing:
['a', 'mild', 'degree', 'of', 'free fluid', 'in', 'the', 'pelvis', 'is', 'noted']
Trained Model: ['amod', 'amod', 'amod', 'ROOT', 'refer', 'refer', 'ROOT', 'prep', 'ROOT', 'ROOT']
Trained Model: [1, 2, 6, 4, 2, 4, 7, 8, 8, 9]
The weirdest thing is that I didn't have this issue in v2.0.18, and I didn't change how my custom factories were loaded. In case you need it, here's my to_disk and from_disk from my custom tokenizer. (All of my factories are loaded in the tokenizer class because of this &lt;denchmark-link:https://support.prodi.gy/t/dep-teach-doesnt-use-same-tokenenization-as-pretrained-model/1177&gt;issue&lt;/denchmark-link&gt;
 I had before with Prodigy).
&lt;denchmark-code&gt;    def from_disk(self, path, **cfg):
        with open(path.parent / 'tokdata' / 'data.json') as f:
            self.data = json.load(f)
        self.find = FindingsRecognizer(self.vocab, data=self.data['find'])
        self.anat = AnatomyRecognizer(self.vocab, data=self.data['anat'])
        self.path = PathologyRecognizer(self.vocab, data=self.data['path'])
        self.etio = EtiologyRecognizer(self.vocab, data=self.data['etio'])
        self.neg = NegativeRecognizer(self.vocab, data=self.data['neg'])
        self.tag.from_disk(path.parent / 'tagger')

    def to_disk(self, path, **cfg):
        newpath = path.parent / 'tokdata'
        if not newpath.is_dir():
            newpath.mkdir()
        with open(newpath / 'data.json', 'w+') as f:
            json.dump(self.data, f)
        self.tag.to_disk(path.parent / 'tagger')
&lt;/denchmark-code&gt;

The tagger is directly loaded from the "en_core_web_sm-2.1.0" before training, so it saves the exact same tagger in the last line. And the factories loaded fine since it merged "free fluid". Is there something different I need to do with this update?
&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


spaCy version: 2.1.0
Platform: Windows-10-10.0.16299-SP0
Python version: 3.7.0

	</description>
	<comments>
		<comment id='1' author='carlsonhang' date='2019-03-19T17:13:09Z'>
		Hmm, maybe the labels are being mapped to classes differently after loading? I'm not sure why that would happen, but can you print [(i, parser.moves.get_class_name(i)) for i in range(parser.moves.n_moves)] before and after loading, to see if they're the same?
		</comment>
		<comment id='2' author='carlsonhang' date='2019-03-19T17:23:19Z'>
		Thanks for the quick reply,
After training: [(0, 'L-subtok'), (1, 'L-dep'), (2, 'R-subtok'), (3, 'R-dep'), (4, 'B-ROOT'), (5, 'S-amod'), (6, 'S-nmod'), (7, 'S-prep'), (8, 'S-ROOT'), (9, 'S-det'), (10, 'S--'), (11, 'S-refer'), (12, 'S-combo'), (13, 'S-conj'), (14, 'S-c-amod'), (15, 'D-amod'), (16, 'D-nmod'), (17, 'D-prep'), (18, 'D-ROOT'), (19, 'D-det'), (20, 'D--'), (21, 'D-refer'), (22, 'D-combo'), (23, 'D-conj'), (24, 'D-c-amod'), (25, 'L-amod'), (26, 'L-nmod'), (27, 'L-prep'), (28, 'L-ROOT'), (29, 'L-det'), (30, 'L--'), (31, 'L-refer'), (32, 'L-combo'), (33, 'L-conj'), (34, 'L-c-amod'), (35, 'R-amod'), (36, 'R-nmod'), (37, 'R-prep'), (38, 'R-ROOT'), (39, 'R-det'), (40, 'R--'), (41, 'R-refer'), (42, 'R-combo'), (43, 'R-conj'), (44, 'R-c-amod'), (45, 'B-amod'), (46, 'B-nmod'), (47, 'B-prep'), (48, 'B-det'), (49, 'B--'), (50, 'B-refer'), (51, 'B-combo'), (52, 'B-conj'), (53, 'B-c-amod')]
After loading: [(0, 'L-subtok'), (1, 'L-dep'), (2, 'R-subtok'), (3, 'R-dep'), (4, 'B-ROOT'), (5, 'S-amod'), (6, 'D-amod'), (7, 'L-amod'), (8, 'R-amod'), (9, 'B-amod'), (10, 'S-nmod'), (11, 'D-nmod'), (12, 'L-nmod'), (13, 'R-nmod'), (14, 'B-nmod'), (15, 'S-prep'), (16, 'D-prep'), (17, 'L-prep'), (18, 'R-prep'), (19, 'B-prep'), (20, 'S-det'), (21, 'D-det'), (22, 'L-det'), (23, 'R-det'), (24, 'B-det'), (25, 'S-ROOT'), (26, 'D-ROOT'), (27, 'L-ROOT'), (28, 'R-ROOT'), (29, 'S--'), (30, 'D--'), (31, 'L--'), (32, 'R--'), (33, 'B--'), (34, 'S-refer'), (35, 'D-refer'), (36, 'L-refer'), (37, 'R-refer'), (38, 'B-refer'), (39, 'S-combo'), (40, 'D-combo'), (41, 'L-combo'), (42, 'R-combo'), (43, 'B-combo'), (44, 'S-conj'), (45, 'D-conj'), (46, 'L-conj'), (47, 'R-conj'), (48, 'B-conj'), (49, 'S-c-amod'), (50, 'D-c-amod'), (51, 'L-c-amod'), (52, 'R-c-amod'), (53, 'B-c-amod')]
It seems the ordering is different from 6 onwards if that makes a difference.
		</comment>
		<comment id='3' author='carlsonhang' date='2019-03-19T23:05:01Z'>
		
It seems the ordering is different from 6 onwards if that makes a difference.

Yep, it definitely does! If the label-to-class mapping is different, then when the model predicts class 8 or whatever, the parser will take the wrong action, and everything will get messed up. So now we have to figure out what's going wrong there.
How are you adding the labels to the parser? During spacy train I usually pass all the data in to nlp.begin_training(), and it builds the label set that way. It should still work if the labels are added as the parser proceeds, but that's a different code path, so it might not be getting tested properly.
		</comment>
		<comment id='4' author='carlsonhang' date='2019-03-21T13:28:33Z'>
		Hello, I've been sick, apologies for the late reply.
I add the labels similarly to one of the code examples for training like this:
&lt;denchmark-code&gt;    for _, annotations in TRAIN_DEP:
        for dep in annotations.get('deps', []):
            parser.add_label(dep)
    nlp.add_pipe(parser)
&lt;/denchmark-code&gt;

And my training loop is basic:
&lt;denchmark-code&gt;    optimizer = newnlp.begin_training()
    #iterate training data
    for itn in range(iter):
        random.shuffle(data)
        losses = {}
        batches = minibatch(data) #default batch size
        for batch in batches:
            for doc, gold in batch:
                try:
                    newnlp.update([doc], [gold], drop=.1, sgd=optimizer, losses=losses) #static drop for now
                except ValueError as e:
                    print(doc, e)
                    continue
&lt;/denchmark-code&gt;

I didn't know you could pass data to begin_training, does it replace the first loop?. The way I've done it above worked in v2.0.18, so I would think it would work similarly in v2.1.0. Maybe I should make sure it only adds unique labels? Did something change in the workflow?
Since the model makes accurate predictions right after it's trained, maybe the way the parser is being saved got changed?
		</comment>
		<comment id='5' author='carlsonhang' date='2019-03-23T02:38:47Z'>
		Sounds like the same issue I'm having. I asked on stack overflow and commented back to this issue (&lt;denchmark-link:https://stackoverflow.com/questions/55291802/custom-ner-model-produces-bad-results-after-being-saved-to-disk&gt;https://stackoverflow.com/questions/55291802/custom-ner-model-produces-bad-results-after-being-saved-to-disk&lt;/denchmark-link&gt;
)
		</comment>
		<comment id='6' author='carlsonhang' date='2019-03-23T10:12:34Z'>
		&lt;denchmark-link:https://github.com/carlsonhang&gt;@carlsonhang&lt;/denchmark-link&gt;
 Your code  work. If you want to mitigate the current bug, you could replace the first loop by passing the  argument to . The function should return a sequence of your training examples. We pass a function so that you can have your data in a generator, without the generator getting exhausted.
The bug should be fixed in the next version.
		</comment>
		<comment id='7' author='carlsonhang' date='2019-03-25T06:35:19Z'>
		Thanks,  the problem seems to have been fixed in 2.1.3 version.
		</comment>
		<comment id='8' author='carlsonhang' date='2019-04-24T06:46:17Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>