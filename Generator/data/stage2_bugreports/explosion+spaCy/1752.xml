<bug id='1752' author='tokestermw' open_date='2017-12-21T02:03:43Z' closed_time='2018-03-28T21:10:02Z'>
	<summary>Multiple NER models in one pipeline</summary>
	<description>
Hi, so let's say I've trained 3 different NER models from scratch: ner_A, ner_B, ner_C.
I train using the EntityRecognizer pipeline with a different .name attribute.
Since EntityRecognizer adds the .ents attributes to each Doc object, it doesn't seem like it's possible to add all the new NER models in one pipeline.
A workaround is to save each NER model individually: ner_A.to_disk('ner_A'), ner_B.to_disk('ner_B'), ner_C.to_disk('ner_C').
For production, we can load them back in individually: ner_A.from_disk('ner_A', vocab=False), ner_B.from_disk('ner_B', vocab=False), ner_C.from_disk('ner_C', vocab=False).
Then we can load the default nlp model: nlp = spacy.load('en_core_web_sm'). Run the original pipeline doc = nlp('how are you?'), then run the new NERs individually: ner_A(doc), ner_B(doc), ner_C(doc).
Then maybe add the results to custom attributes (e.g. doc._.ner_A_ents).
Would this be a valid way to add models? I like using the spaCy sequence models since it has all the transition magic.
Thanks!
&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


spaCy version: 2.0.5
Platform: Darwin-17.3.0-x86_64-i386-64bit
Python version: 3.6.3
Models: en_core_web_sm

	</description>
	<comments>
		<comment id='1' author='tokestermw' date='2017-12-22T23:59:22Z'>
		Looks like this method doesn't work when there is more than one ner component in the pipeline (gives segfault as there is more than one doc.ents).
edit: I can work around this by tokenizing every time and making a frosh doc.
&lt;denchmark-code&gt;ner_A(nlp.make_doc(text)).ents
ner_B(nlp.make_doc(text)).ents
ner_C(nlp.make_doc(text)).ents
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='tokestermw' date='2018-02-10T12:24:19Z'>
		&lt;denchmark-link:https://github.com/tokestermw&gt;@tokestermw&lt;/denchmark-link&gt;
 Sorry I missed this before.
Is this still an issue? It seems like it shouldn't be --- I thought the entity recognizer respected the pre-set entities. If it doesn't we can definitely fix that.
		</comment>
		<comment id='3' author='tokestermw' date='2018-02-12T22:49:42Z'>
		To recreate:

Train the NER in examples, output to here directory.

&lt;denchmark-code&gt;python examples/training/train_ner.py -o here
&lt;/denchmark-code&gt;


Load this model and the original NLP pipeline that includes ner.

&lt;denchmark-code&gt;import spacy
new_nlp = spacy.load('here')
nlp = spacy.load('en_core_web_sm')
&lt;/denchmark-code&gt;


Add the new component to the nlp pipeline

&lt;denchmark-code&gt;nlp.add_pipe(new_nlp.pipeline[-1][-1], 'new_ner')
&lt;/denchmark-code&gt;


segfaults when the two NERs predicts on the same spans.

&lt;denchmark-code&gt;nlp("I am Barack Obama that is living in France.")  # SEGFAULT
&lt;/denchmark-code&gt;

--
But I have a workaround to load the component separately and attaching each new component to a custom attribute. So it's not a problem for me anymore.
		</comment>
		<comment id='4' author='tokestermw' date='2018-03-09T17:33:58Z'>
		&lt;denchmark-link:https://github.com/tokestermw&gt;@tokestermw&lt;/denchmark-link&gt;
 Pretty sure I've gotten to the bottom of this! In , we're not resetting the  correctly. This means the  and  values can be out-of-synch, which causes the error.
		</comment>
		<comment id='5' author='tokestermw' date='2018-03-27T06:47:09Z'>
		can anyone on the thread who managed to find a workaround share snippet, please?
I'm trying to use en model and xx for ner and running into similar issue:

if I do

&lt;denchmark-code&gt;nlp = en_core_web_md.load()
ner = xx_ent_wiki_sm.load()
&lt;/denchmark-code&gt;

and then do doc = nlp(sent); ners = ner(sent) I get
&lt;denchmark-code&gt;/usr/local/lib/python3.6/site-packages/thinc/neural/_classes/static_vectors.py in begin_update(self, ids, drop)
     65                 sgd(self._mem.weights, self._mem.gradient, key=self.id)
     66             return None
---&gt; 67         dotted = self.ops.batch_dot(vectors, self.W)
     68         mask = self.ops.get_dropout_mask((dotted.shape[1],), drop)
     69         if mask is not None:

ops.pyx in thinc.neural.ops.NumpyOps.batch_dot()

ValueError: shapes (15,0) and (300,128) not aligned: 0 (dim 1) != 300 (dim 0)
&lt;/denchmark-code&gt;


if I do

&lt;denchmark-code&gt;nlp = spacy.load('en_core_web_md')
ner = spacy.load('xx', disable=['tokenizer', 'tagger', 'parser'])
nlp.replace_pipe('ner', ner.pipeline[0][1])
&lt;/denchmark-code&gt;

I get the same error as above
UPD: I probably should move this comment to &lt;denchmark-link:https://github.com/explosion/spaCy/issues/1660&gt;#1660&lt;/denchmark-link&gt;
 ?
Also, using
&lt;denchmark-code&gt;nlp = spacy.load('en', disable=['ner'])
ner = spacy.load('xx')
&lt;/denchmark-code&gt;

seems to solve issue of misalignment for me
		</comment>
		<comment id='6' author='tokestermw' date='2018-05-07T17:53:25Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>