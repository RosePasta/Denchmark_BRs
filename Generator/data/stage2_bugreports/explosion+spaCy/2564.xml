<bug id='2564' author='mittalsuraj18' open_date='2018-07-18T06:16:34Z' closed_time='2018-07-19T13:55:18Z'>
	<summary>Information loss when trying to deserialise from Doc bytes</summary>
	<description>
&lt;denchmark-code&gt;import spacy
from spacy.vocab import Vocab
from spacy.tokens import Doc
nlp = spacy.load('en',disable=["ner","textcat"])
docs = nlp.pipe(processed,n_threads=16,disable=["ner","textcat"],batch_size=100)
doc = next(docs)
list(doc.noun_chunks)
&lt;/denchmark-code&gt;

[the operations,
International patients,
both the centers,
HCF,
more queries,
internal Drs,
the treatment plan,
the patients,
regular updates,
the international patient,
the HCF,
internal team,
external partners,
better services,
the international patient,
care,
the pick,
drop,
the international patients,
the airport,
the FRRO,
all the international patients,
all the HCF,
prospective patients,
all the given formats,
end,
assistance,
international patients,

Good Communication skill]

&lt;denchmark-code&gt;doc_bytes = doc.to_bytes()
new_doc = Doc(nlp.vocab).from_bytes(doc_bytes)
list(Doc(nlp.vocab).from_bytes(doc_bytes).noun_chunks)
&lt;/denchmark-code&gt;

[]
&lt;denchmark-code&gt;len(doc_bytes)
&lt;/denchmark-code&gt;

77035
&lt;denchmark-h:h2&gt;Info about spaCy&lt;/denchmark-h&gt;


spaCy version: 2.0.11
Platform: Linux-3.13.0-149-generic-x86_64-with-debian-jessie-sid
Python version: 3.6.5
Models: en

	</description>
	<comments>
		<comment id='1' author='mittalsuraj18' date='2018-07-18T16:12:59Z'>
		Thanks for the report â€“ I just tried it in the v2.1.0 nightly and I can't reproduce the error there. This could mean that whatever caused this issue was fixed in the new version.
I didn't have your original text, but here's my test code:
import spacy
from spacy.tokens import Doc

nlp = spacy.load('en_core_web_sm')
texts = ["The New York Times is an American newspaper based in New York City with worldwide influence and readership."]
docs = nlp.pipe(texts)
doc = next(docs)
print(list(doc.noun_chunks))
# [The New York Times, an American newspaper, New York City, worldwide influence, readership]

doc_bytes = doc.to_bytes()
new_doc = Doc(nlp.vocab).from_bytes(doc_bytes)
print(list(new_doc.noun_chunks))
# [The New York Times, an American newspaper, New York City, worldwide influence, readership]
Does the above test case result in the same error for you?
		</comment>
		<comment id='2' author='mittalsuraj18' date='2018-07-19T12:57:10Z'>
		nope. the above test case is producing the expected results.
However I was able to reproduce this error by changing the code to following
&lt;denchmark-code&gt;import spacy
from spacy.tokens import Doc
nlp = spacy.load('en_core_web_sm',disable=['ner','textcat'])
texts = ["The New York Times is an American newspaper based in New York City with worldwide influence and readership.","This should be good","is this working good?"]
docs = nlp.pipe(texts,disable=['ner','textcat'],batch_size=2,n_threads=16)
doc = next(docs)
print(list(doc.noun_chunks))
# [The New York Times, an American newspaper, New York City, worldwide influence, readership]
doc_bytes = doc.to_bytes()
new_doc = Doc(nlp.vocab).from_bytes(doc_bytes)
print(list(new_doc.noun_chunks))
#[]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='mittalsuraj18' date='2018-07-19T13:35:18Z'>
		Thanks for the test code! It works correctly in the v2.1.0 nightly, but when I run it in v2.0.11, I can reproduce the behaviour. I also found the cause of the problem: When using nlp.pipe, the tagger doesn't always set the Doc object's is_tagged value correctly, which results in the data being serialized without part-of-speech tags. And without part-of-speech tags, spaCy can't produce the noun chunks.
Here's a super basic test case that shows the problem:
nlp = spacy.load('en_core_web_sm')
docs = nlp.pipe(['hello', 'world'])
doc = next(docs)
assert doc.is_tagged
		</comment>
		<comment id='4' author='mittalsuraj18' date='2018-07-20T06:07:17Z'>
		Awesome.
Until the stable version is released,
Manually setting the flags helps.
&lt;denchmark-code&gt;doc.is_parsed = True
doc.is_tagged = True
&lt;/denchmark-code&gt;

The above lines can be added before serialising the doc object.
Thanks for the help.
		</comment>
		<comment id='5' author='mittalsuraj18' date='2018-08-19T06:21:42Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>