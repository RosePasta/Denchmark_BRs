<bug id='3112' author='RohitSingh-ML' open_date='2019-01-03T04:48:08Z' closed_time='2019-02-21T18:57:48Z'>
	<summary>Spacy - Adding new-entities on pre-trained model works on CPU but not on GPU</summary>
	<description>
I want to add new entities to pre-trained model. I am able to do it on CPU(time-consuming) but not on GPU.
Note - Training on GPU works well when training with entities it has been trained before but if I add few new entities then it crashes.
Following is the error trace:-
&lt;denchmark-code&gt;KeyError Traceback (most recent call last)
&lt;ipython-input-10-8262f0a96a59&gt; in &lt;module&gt;
      6 output_dir=os.getcwd()+'/test'
      7 n_iter=10
----&gt; 8 train_ner(TRAIN_DATA, EVAL_DATA ,model, output_dir, n_iter)

&lt;ipython-input-6-f50c9b890e27&gt; in train_ner(TRAIN_DATA, EVAL_DATA, model, output_dir, n_iter)
     50                 for batch in batches:
     51                     texts, annotations = zip(*batch)
---&gt; 52                     nlp.update(texts, annotations, sgd=optimizer, drop=0.35,losses=losses)
     53                     sumi+=sum(len(doc) for doc in texts)
     54                     pbar.update(sumi)

~/Downloads/program_files/anaconda3/envs/spacy/lib/python3.6/site-packages/spacy/language.py in update(self, docs, golds, drop, sgd, losses)
    419                 continue
    420             grads = {}
--&gt; 421             proc.update(docs, golds, drop=drop, sgd=get_grads, losses=losses)
    422             for key, (W, dW) in grads.items():
    423                 sgd(W, dW, key=key)

nn_parser.pyx in spacy.syntax.nn_parser.Parser.update()

~/Downloads/program_files/anaconda3/envs/spacy/lib/python3.6/site-packages/thinc/api.py in begin_update(self, X, drop)
     59         callbacks = []
     60         for layer in self._layers:
---&gt; 61             X, inc_layer_grad = layer.begin_update(X, drop=drop)
     62             callbacks.append(inc_layer_grad)
     63         def continue_update(gradient, sgd=None):

~/Downloads/program_files/anaconda3/envs/spacy/lib/python3.6/site-packages/thinc/check.py in checked_function(wrapped, instance, args, kwargs)
    144                     raise ExpectedTypeError(check, ['Callable'])
    145                 check(arg_id, fix_args, kwargs)
--&gt; 146         return wrapped(*args, **kwargs)
    147 
    148     def arg_check_adder(func):

~/Downloads/program_files/anaconda3/envs/spacy/lib/python3.6/site-packages/thinc/neural/_classes/affine.py in begin_update(self, input__BI, drop)
     54     @check.arg(1, has_shape(('nB', 'nI')))
     55     def begin_update(self, input__BI, drop=0.):
---&gt; 56         output__BO = self.predict(input__BI)
     57         def finish_update(grad__BO, sgd=None):
     58             self.d_W += self.ops.batch_outer(grad__BO, input__BI)

~/Downloads/program_files/anaconda3/envs/spacy/lib/python3.6/site-packages/thinc/check.py in checked_function(wrapped, instance, args, kwargs)
    144                     raise ExpectedTypeError(check, ['Callable'])
    145                 check(arg_id, fix_args, kwargs)
--&gt; 146         return wrapped(*args, **kwargs)
    147 
    148     def arg_check_adder(func):

~/Downloads/program_files/anaconda3/envs/spacy/lib/python3.6/site-packages/thinc/neural/_classes/affine.py in predict(self, input__BI)
     50     @check.arg(1, has_shape(('nB', 'nI')))
     51     def predict(self, input__BI):
---&gt; 52         return self.ops.affine(self.W, self.b, input__BI)
     53 
     54     @check.arg(1, has_shape(('nB', 'nI')))

ops.pyx in thinc.neural.ops.Ops.affine()
ops.pyx in thinc.neural.ops.Ops.batch_dot()

~/Downloads/program_files/anaconda3/envs/spacy/lib/python3.6/site-packages/cupy/linalg/product.py in dot(a, b, out)
     33     """
     34     # TODO(okuta): check type
---&gt; 35     return a.dot(b, out)
     36 
     37 
cupy/core/core.pyx in cupy.core.core.ndarray.__array__()
cupy/core/core.pyx in cupy.core.core.ndarray.astype()
cupy/core/_kernel.pyx in cupy.core._kernel.ufunc.__call__()
cupy/util.pyx in cupy.util.memoize.decorator.ret()
cupy/core/_kernel.pyx in cupy.core._kernel._get_ufunc_kernel()
cupy/core/_kernel.pyx in cupy.core._kernel._get_kernel_params()
cupy/core/_scalar.pyx in cupy.core._scalar.get_typename()
cupy/core/_scalar.pyx in cupy.core._scalar.get_typename()
KeyError: &lt;class 'numpy.object_'&gt;
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Info about spaCy&lt;/denchmark-h&gt;


spaCy version: 2.0.18
Platform: Linux-4.15.0-39-generic-x86_64-with-debian-stretch-sid
Python version: 3.6.6
Models: en, en_core_web_md

Please Help.
	</description>
	<comments>
		<comment id='1' author='RohitSingh-ML' date='2019-01-05T11:53:06Z'>
		&lt;denchmark-link:https://github.com/gitoo7&gt;@gitoo7&lt;/denchmark-link&gt;
 Could you try on the latest ? I think the bug should be fixed there. I've also fixed some other bugs associated with adding entities to pre-trained models. Specifically, the optimizer tends to favour solutions where many classes receive large negative scores. When adding a new class, we were ending up with scores of 0 for the new entity, which corresponded to the entity receiving significant probability. Now that this is fixed, your new class should be a fair bit easier to train.
		</comment>
		<comment id='2' author='RohitSingh-ML' date='2019-01-08T11:31:28Z'>
		&lt;denchmark-link:https://github.com/honnibal&gt;@honnibal&lt;/denchmark-link&gt;
 I have tried it with 
Result is same as in previous case that i am able to add new-entities on cpu but not on gpu, But this time error is different.
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/home/rosingh/MYDRIVE/chatbot-docs/ner-data-new/my_train.py", line 223, in &lt;module&gt;
    main()
  File "/home/rosingh/MYDRIVE/chatbot-docs/ner-data-new/my_train.py", line 221, in main
    train_ner(n_words, TRAIN_DATA, EVAL_DATA, model, output_dir, n_iter)
  File "/home/rosingh/MYDRIVE/chatbot-docs/ner-data-new/my_train.py", line 123, in train_ner
    ner.add_label(ele)
  File "nn_parser.pyx", line 167, in spacy.syntax.nn_parser.Parser.add_label
  File "_parser_model.pyx", line 215, in spacy.syntax._parser_model.ParserModel.resize_output
  File "cupy/core/core.pyx", line 1688, in cupy.core.core.ndarray.__setitem__
  File "cupy/core/core.pyx", line 3579, in cupy.core.core._scatter_op
  File "cupy/core/core.pyx", line 526, in cupy.core.core.ndarray.fill
ValueError: non-scalar numpy.ndarray cannot be used for fill
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='RohitSingh-ML' date='2019-01-09T17:06:52Z'>
		If I may chime in. I just tried to run &lt;denchmark-link:https://github.com/explosion/spacy/blob/master/examples/training/train_ner.py&gt;the ner example&lt;/denchmark-link&gt;
 without any arguments (but with a  added after the spacy import) with nightly, and I get the original error ().
I used the following minimal Dockerfile:
&lt;denchmark-code&gt;     FROM nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04
     RUN apt-get update &amp;&amp; apt-get install -y python3 python3-dev python3-pip
     RUN pip3 install --pre spacy-nightly[cuda90]
&lt;/denchmark-code&gt;

So there seems to a problem even when not adding new entities (or with my Dockerfile).
		</comment>
		<comment id='4' author='RohitSingh-ML' date='2019-01-30T09:51:50Z'>
		&lt;denchmark-link:https://github.com/honnibal&gt;@honnibal&lt;/denchmark-link&gt;
 I have the same issue mentioned before by &lt;denchmark-link:https://github.com/gitoo7&gt;@gitoo7&lt;/denchmark-link&gt;
 in both versions (2.1.0a6 and 2.0.18).  The training training process only works with version 2.0.18 and if the process starts with a blank model. Using a pre-trained model I got the same erros, even if I try to run without add new entities.
&lt;denchmark-h:h2&gt;Info about spaCy&lt;/denchmark-h&gt;


spaCy version: 2.1.0a6
Platform: Linux-4.14.88-72.73.amzn1.x86_64-x86_64-with-glibc2.9
Python version: 3.6.5
Models: pt, en

&lt;denchmark-code&gt;TYPE      NAME              MODEL             VERSION
package   pt-core-news-sm   pt_core_news_sm   2.1.0a5   ✔
package   en-core-web-sm    en_core_web_sm    2.1.0a5   ✔
link      pt                pt_core_news_sm   2.1.0a5   ✔
link      pt_core_news_sm   pt_core_news_sm   2.1.0a5   ✔
link      en_core_web_sm    en_core_web_sm    2.1.0a5   ✔
link      en                en_core_web_sm    2.1.0a5   ✔
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Using a pre-trained model:&lt;/denchmark-h&gt;

Loaded model 'pt'
Traceback (most recent call last):
File "./train_ner2.py", line 99, in 
plac.call(main)
File "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/plac_core.py", line 328, in call
cmd, result = parser.consume(arglist)
File "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/plac_core.py", line 207, in consume
return cmd, self.func(*(args + varargs + extraopts), **kwargs)
File "./train_ner2.py", line 50, in main
ner.add_label(str(ent[2]))
File "nn_parser.pyx", line 167, in spacy.syntax.nn_parser.Parser.add_label
File "_parser_model.pyx", line 219, in spacy.syntax._parser_model.ParserModel.resize_output
File "cupy/core/core.pyx", line 1689, in cupy.core.core.ndarray.setitem
File "cupy/core/core.pyx", line 3588, in cupy.core.core._scatter_op
File "cupy/core/core.pyx", line 527, in cupy.core.core.ndarray.fill
ValueError: non-scalar numpy.ndarray cannot be used for fill
&lt;denchmark-h:h2&gt;Starting with a blank model&lt;/denchmark-h&gt;

Created blank 'pt' model
Traceback (most recent call last):
File "./train_ner2.py", line 99, in 
plac.call(main)
File "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/plac_core.py", line 328, in call
cmd, result = parser.consume(arglist)
File "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/plac_core.py", line 207, in consume
return cmd, self.func(*(args + varargs + extraopts), **kwargs)
File "./train_ner2.py", line 71, in main
losses=losses,
File "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/spacy/language.py", line 449, in update
proc.update(docs, golds, drop=drop, sgd=get_grads, losses=losses)
File "nn_parser.pyx", line 416, in spacy.syntax.nn_parser.Parser.update
File "_parser_model.pyx", line 264, in spacy.syntax._parser_model.ParserStepModel.begin_update
File "cupy/core/_kernel.pyx", line 787, in cupy.core._kernel.ufunc.call
File "cupy/core/_kernel.pyx", line 86, in cupy.core._kernel._preprocess_args
TypeError: Unsupported type &lt;class 'numpy.ndarray'&gt;
		</comment>
		<comment id='5' author='RohitSingh-ML' date='2019-02-21T08:31:05Z'>
		&lt;denchmark-link:https://github.com/lhgomes&gt;@lhgomes&lt;/denchmark-link&gt;
 Thanks, I think I fixed the issue when using a blank model. Still looking into the issue when resuming training.
		</comment>
		<comment id='6' author='RohitSingh-ML' date='2019-02-21T10:12:01Z'>
		Just tested both cases, and they're working for me. Thanks again for the reports. We don't have a CI server for the GPU code paths, so we really need the reports when things break.
		</comment>
		<comment id='7' author='RohitSingh-ML' date='2019-02-21T12:38:19Z'>
		&lt;denchmark-link:https://github.com/honnibal&gt;@honnibal&lt;/denchmark-link&gt;
 I can confirm that the issue with training process using a blank model was solved, but the training using a previously trained model still has an error:
Loaded model 'pt'
Traceback (most recent call last):
File "./train_ner2.py", line 101, in 
plac.call(main)
File "/home/ec2-user/anaconda3/lib/python3.6/site-packages/plac_core.py", line 328, in call
cmd, result = parser.consume(arglist)
File "/home/ec2-user/anaconda3/lib/python3.6/site-packages/plac_core.py", line 207, in consume
return cmd, self.func(*(args + varargs + extraopts), **kwargs)
File "./train_ner2.py", line 51, in main
ner.add_label(label)
File "nn_parser.pyx", line 167, in spacy.syntax.nn_parser.Parser.add_label
File "_parser_model.pyx", line 219, in spacy.syntax._parser_model.ParserModel.resize_output
File "cupy/core/core.pyx", line 1689, in cupy.core.core.ndarray.setitem
File "cupy/core/core.pyx", line 3588, in cupy.core.core._scatter_op
File "cupy/core/core.pyx", line 527, in cupy.core.core.ndarray.fill
ValueError: non-scalar numpy.ndarray cannot be used for fill
&lt;denchmark-h:h2&gt;Info about spaCy&lt;/denchmark-h&gt;


spaCy version: 2.1.0a8
Platform: Linux-4.14.88-72.73.amzn1.x86_64-x86_64-with-glibc2.9
Python version: 3.6.5
Models: pt, pt_core_news_sm, en_core_web_sm, en

		</comment>
		<comment id='8' author='RohitSingh-ML' date='2019-02-21T12:51:02Z'>
		&lt;denchmark-link:https://github.com/lhgomes&gt;@lhgomes&lt;/denchmark-link&gt;
 That's strange. I tested that too. You're calling  before  right?
		</comment>
		<comment id='9' author='RohitSingh-ML' date='2019-02-21T15:55:14Z'>
		
@lhgomes That's strange. I tested that too. You're calling spacy.require_gpu() before spacy.load() right?

Yes. these are the top lines of my script:
&lt;denchmark-code&gt;#!/usr/bin/env python
#coding: utf8
from __future__ import unicode_literals, print_function

import io
import shutil
import plac
import random
from pathlib import Path
import spacy
from spacy.util import minibatch, compounding
spacy.require_gpu()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='RohitSingh-ML' date='2019-02-21T16:13:13Z'>
		I didn't test this on the GPU server yet (and I'm confused why my previous GPU test worked...) -- but I think this should fix it.
		</comment>
		<comment id='11' author='RohitSingh-ML' date='2019-02-21T18:41:00Z'>
		You should be able to test it now with pip install -U spacy-nightly==2.1.0a9.dev0
		</comment>
		<comment id='12' author='RohitSingh-ML' date='2019-02-21T18:56:56Z'>
		
You should be able to test it now with pip install -U spacy-nightly==2.1.0a9.dev0

Great &lt;denchmark-link:https://github.com/honnibal&gt;@honnibal&lt;/denchmark-link&gt;
, the version 2.1.0a9.dev0 is working !!
		</comment>
		<comment id='13' author='RohitSingh-ML' date='2019-03-23T19:21:22Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>