<bug id='3552' author='BramVanroy' open_date='2019-04-08T10:37:45Z' closed_time='2019-10-03T19:57:14Z'>
	<summary>Multiprocessing spaCy: Can't find model 'en_model.vectors' in en_core_web_lg</summary>
	<description>
I am trying to process multiple files in parallel (a single NLP instantiation) and do sentence segmentation on them. Every process reads a file, and every line in that file is a JSON string. The JSON contains a text field, which I want to segment.
This seems to work fine with small and medium models, but for the large spaCy model I get the error

OSError: [E050] Can't find model 'en_model.vectors'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.

Looking at the &lt;denchmark-link:https://spacy.io/models/en&gt;English models&lt;/denchmark-link&gt;
, I'd assume that the small model doesn't have vectors - not the large model. On top of that, I'm not sure why vectors are required for this operation. So what is the problem?
I have wondered whether this is actually a memory issue, but running this with 3 threads and 16Gb of RAM, I don't think that should be an issue: even if the model is loaded three times, the memory should be able to hold that.
Finally, if there is a faster way to do only sentence segmentation that's better than the following, please do let me know. I'm also not sure whether having only one nlp instance is good practice in a multiprocessing context. Should it be copied?
docs = list(self.nlp.pipe(lines))
sents = [sent for doc in docs for sent in doc.sents]
&lt;denchmark-h:h2&gt;How to reproduce the behaviour&lt;/denchmark-h&gt;


Clone this repo.
Install spaCy and the small and large model
Run python debug.py data/raw/ data/articles/ -s en_core_web_lg

That should fail. Use the en_core_web_sm model, and it should run just fine.
&lt;denchmark-h:h2&gt;Info about spaCy&lt;/denchmark-h&gt;


spaCy version: 2.1.3
Platform: Windows-10-10.0.17134-SP0
Python version: 3.7.3

	</description>
	<comments>
		<comment id='1' author='BramVanroy' date='2019-06-23T18:43:50Z'>
		I believe this issue is caused by the way thinc and spacy packages are connected.
In Thinc, "load_nlp.py" relies on global variables VECTORS and SPACY_MODELS to obtain the language models, and these global variables are set inside spacy's function "_ml.py" (for VECTORS).
Under Spark or some other parallel packages, global variables are not carried over from main into each map/reduce worker, thus these variables are reset.
You can overcome this issue by placing the nlp = spacy.load() line into map/reduce functions. First, have the nlp = spacy.load('en_core_web_lg') in global scope. Inside map/reduce functions, check if 'thinc.extra.load_nlp.VECTORS' dict is empty or not. This requires an import of thinc package. If it is empty, and you are using a model that contains vectors, call nlp = spacy.load('en_core_web_lg') again. thinc's global variables should be set again and this time persistence within each worker.
Not optimal, but should fix the problem for now.....
		</comment>
		<comment id='2' author='BramVanroy' date='2019-07-11T11:28:51Z'>
		&lt;denchmark-link:https://github.com/wang159&gt;@wang159&lt;/denchmark-link&gt;
 Thanks for the work-around. I think your analysis is correct.
The way this works really sucks, and I'm annoyed that I've had so much trouble getting a good solution to this seemingly-simple problem.
For future reference when we're working on this, it's worth noting that we should think about both the multiprocessing and multi-threading cases. In Thinc recently I fixed a similarish problem with the operator overloading, which touches a global variable. This introduced a race condition for multi-threading. I fixed that by making the variable thread-local.
		</comment>
		<comment id='3' author='BramVanroy' date='2019-08-10T16:51:01Z'>
		Any updates on the fix for this issue?
		</comment>
		<comment id='4' author='BramVanroy' date='2019-09-09T22:18:00Z'>
		&lt;denchmark-link:https://github.com/wang159&gt;@wang159&lt;/denchmark-link&gt;
 : instead of loading a model again within a worker, is it possible to add the vectors to the empty 'thinc.extra.load_nlp.VECTORS' dict within the worker? Loading the model many times (if there are a lot of workers) will take up a lot of memory, I assume?
		</comment>
		<comment id='5' author='BramVanroy' date='2019-10-03T19:57:14Z'>
		Merging with &lt;denchmark-link:https://github.com/explosion/spaCy/issues/4349&gt;#4349&lt;/denchmark-link&gt;
. Thanks for your patience on this, I agree that it's a frustrating bug.
		</comment>
		<comment id='6' author='BramVanroy' date='2019-11-02T21:54:30Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>