<bug id='6325' author='AndriyMulyar' open_date='2020-10-30T03:51:11Z' closed_time='2020-12-08T08:13:43Z'>
	<summary>Cannot pretrain tok2vec model due to error reading corpora.pretrain from config. [spacy-nightly]</summary>
	<description>
&lt;denchmark-h:h2&gt;How to reproduce the behaviour&lt;/denchmark-h&gt;

Running:
python -m spacy pretrain scripts/train/config/pretrain_window_cnn.cfg ./pretrained_window_cnn --paths.raw a_valid_path
Produces Error:
⚠ Output directory is not empty.
It is better to use an empty directory or refer to a new output path, then the
new directory will be created for you.
ℹ Using CPU
ℹ Loading config from: scripts/train/config/pretrain_window_cnn.cfg
✔ Saved config file in the output directory
Traceback (most recent call last):
  File "/usr/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/andriy/dev/radstraction/env/lib/python3.6/site-packages/spacy/__main__.py", line 4, in &lt;module&gt;
    setup_cli()
  File "/home/andriy/dev/radstraction/env/lib/python3.6/site-packages/spacy/cli/_util.py", line 65, in setup_cli
    command(prog_name=COMMAND)
  File "/home/andriy/dev/radstraction/env/lib/python3.6/site-packages/click/core.py", line 829, in __call__
    return self.main(*args, **kwargs)
  File "/home/andriy/dev/radstraction/env/lib/python3.6/site-packages/click/core.py", line 782, in main
    rv = self.invoke(ctx)
  File "/home/andriy/dev/radstraction/env/lib/python3.6/site-packages/click/core.py", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/home/andriy/dev/radstraction/env/lib/python3.6/site-packages/click/core.py", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/andriy/dev/radstraction/env/lib/python3.6/site-packages/click/core.py", line 610, in invoke
    return callback(*args, **kwargs)
  File "/home/andriy/dev/radstraction/env/lib/python3.6/site-packages/typer/main.py", line 497, in wrapper
    return callback(**use_params)  # type: ignore
  File "/home/andriy/dev/radstraction/env/lib/python3.6/site-packages/spacy/cli/pretrain.py", line 76, in pretrain_cli
    silent=False,
  File "/home/andriy/dev/radstraction/env/lib/python3.6/site-packages/spacy/training/pretrain.py", line 41, in pretrain
    corpus = dot_to_object(T, P["corpus"])
  File "/home/andriy/dev/radstraction/env/lib/python3.6/site-packages/spacy/util.py", line 1223, in dot_to_object
    raise KeyError(Errors.E952.format(name=section)) from None
KeyError: "[E952] The section 'corpora.pretrain' is not a valid section in the provided config."
but my config clearly has a corpora.pretrain section as given in the nightly API docs:
[paths]
train = null
dev = null
raw = null



[system]
gpu_allocator = pytorch
seed = 0

[initialize]
vectors = my_vecs
;init_tok2vec = "/path/to/pretrain.bin"



[nlp]
lang = "en"
pipeline = ["tok2vec"]
tokenizer = {"@tokenizers": "spacy.Tokenizer.v1"}
disabled = []
before_creation = null
after_creation = null
after_pipeline_creation = null

[components]

[components.tok2vec]
factory = "tok2vec"

[components.tok2vec.model]
@architectures = "spacy.Tok2Vec.v1"

[components.tok2vec.model.embed]
@architectures = "spacy.MultiHashEmbed.v1"
width = ${components.tok2vec.model.encode.width}
attrs = ["ORTH", "SHAPE"]
rows = [5000, 2500]
include_static_vectors = True

[components.tok2vec.model.encode]
@architectures = "spacy.MaxoutWindowEncoder.v1"
width = 100
depth = 4
window_size = 1
maxout_pieces = 3

;[components.tok2vec.model.encode]
;@architectures = "spacy.TorchBiLSTMEncoder.v1"
;width = 100
;depth = 2
;dropout = ${training.dropout}



[corpora]

[corpora.pretrain]
@readers = "spacy.JsonlCorpus.v1"
path = ${paths.raw}
min_length = 5
max_length = 500

[corpora.train]
@readers = "spacy.Corpus.v1"
path = ${paths.train}
max_length = 2000

[corpora.dev]
@readers = "spacy.Corpus.v1"
path = ${paths.dev}
max_length = 0


# Training hyper-parameters and additional features.
[training]
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
dropout = 0.3
accumulate_gradient = 5
# Controls early-stopping. 0 or -1 mean unlimited.
patience = 1600
max_epochs = 200
max_steps = 20000
eval_frequency = 100
# Control how scores are printed and checkpoints are evaluated.
score_weights = {}
# Names of pipeline components that shouldn't be updated during training
frozen_components = []
# Location in the config where the dev corpus is defined
dev_corpus = "corpora.dev"
# Location in the config where the train corpus is defined
train_corpus = "corpora.train"
# Optional callback before nlp object is saved to disk after training
before_to_disk = null

[training.logger]
@loggers = "spacy.ConsoleLogger.v1"
progress_bar = True

[training.batcher]
@batchers = "spacy.batch_by_sequence.v1"
;discard_oversize = false
;tolerance = 0.2

[training.batcher.size]
@schedules = "compounding.v1"
start = 3
stop = 30
compound = 1.001

[training.optimizer]
@optimizers = "Adam.v1"
beta1 = 0.9
beta2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
grad_clip = 1.0
use_averages = true
eps = 1e-8
learn_rate = 0.001

[pretraining]
max_epochs = 1000
dropout = 0.2
n_save_every = 5
component = "tok2vec"
layer = ""
corpus = "corpora.pretrain"

[pretraining.batcher]
@batchers = "spacy.batch_by_words.v1"
size = 3000
discard_oversize = false
tolerance = 0.2
get_length = null

[pretraining.objective]
type = "vectors"
loss = cosine

[pretraining.optimizer]
@optimizers = "Adam.v1"
beta1 = 0.9
beta2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
grad_clip = 1.0
use_averages = true
eps = 1e-8
learn_rate = 0.001
&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Info about spaCy&lt;/denchmark-h&gt;


spaCy version: 3.0.0rc2
Platform: Linux-5.4.0-1025-aws-x86_64-with-Ubuntu-18.04-bionic
Python version: 3.6.9

	</description>
	<comments>
		<comment id='1' author='AndriyMulyar' date='2020-10-30T08:58:55Z'>
		Thanks for the report!
It looks like a bug sneaked in, the relevant section "corpora.pretrain" is searched only in the "training" portion of the config, instead of the whole file. I'll hopefully have a PR ready to address this quite quickly.
		</comment>
		<comment id='2' author='AndriyMulyar' date='2020-10-30T09:31:02Z'>
		The bugfix will be in the next release (3.0.0rc3), but if you want, you could fix your local installation by mimicing the changes from the PR: &lt;denchmark-link:https://github.com/explosion/spaCy/pull/6326/files&gt;https://github.com/explosion/spaCy/pull/6326/files&lt;/denchmark-link&gt;
 (or pulling the latest nightly from GH directly)
		</comment>
		<comment id='3' author='AndriyMulyar' date='2020-10-30T19:28:08Z'>
		I have tried out this fix and the execution now passes that line of code.
I now hit the following error further into the pretraining execution (same set-up and config file as above):
&lt;denchmark-code&gt;  File "/home/andriy/dev/lib/env/lib/python3.6/site-packages/thinc/layers/maxout.py", line 77, in init
    W_shape = (model.get_dim("nO"), model.get_dim("nP"), model.get_dim("nI"))
  File "/home/andriy/dev/lib/env/lib/python3.6/site-packages/thinc/model.py", line 175, in get_dim
    raise ValueError(err)
ValueError: Cannot get dimension 'nO' for model 'maxout': value unset
&lt;/denchmark-code&gt;

Do you want another issue for this?
		</comment>
		<comment id='4' author='AndriyMulyar' date='2020-10-30T20:42:03Z'>
		Oh weird, I didn't get that error. If you run into this with exact the same config, we can keep it in this issue.
		</comment>
		<comment id='5' author='AndriyMulyar' date='2020-11-02T15:24:57Z'>
		
Oh weird, I didn't get that error. If you run into this with exact the same config, we can keep it in this issue.

Could you share the config used in your pre-training test?
		</comment>
		<comment id='6' author='AndriyMulyar' date='2020-11-12T20:42:45Z'>
		The config that works for me, is generated by running:
&lt;denchmark-code&gt;python -m spacy init config tagger_cpu.cfg -p "tagger" --cpu
python -m spacy init fill-config tagger_cpu.cfg tagger_cpu_pretrain.cfg --pretraining
&lt;/denchmark-code&gt;

I can get your config to work by setting
&lt;denchmark-code&gt;[pretraining.objective]
type = "characters"
n_characters = 4
&lt;/denchmark-code&gt;

and
&lt;denchmark-code&gt;include_static_vectors = false
&lt;/denchmark-code&gt;

I think the Cannot get dimension 'nO' for model 'maxout': value unset error occurs when you set include_static_vectors to false while using the vectors pretraining objective.
That said, it looks like there's another bug still Buffer and memoryview are not contiguous in the same dimension. when setting include_static_vectors = true - I'll look into it further.
		</comment>
		<comment id='7' author='AndriyMulyar' date='2020-11-25T17:58:05Z'>
		We're looking at refactoring the  section to avoid these issues, cf. PR &lt;denchmark-link:https://github.com/explosion/spaCy/pull/6451&gt;#6451&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='AndriyMulyar' date='2020-12-08T08:14:36Z'>
		This should be addressed in PR &lt;denchmark-link:https://github.com/explosion/spaCy/pull/6451&gt;#6451&lt;/denchmark-link&gt;
, which will be available from v3.0.0rc3 onwards.
		</comment>
	</comments>
</bug>