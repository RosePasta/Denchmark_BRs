<bug id='3442' author='nahidalam' open_date='2019-03-20T02:55:09Z' closed_time='2019-03-20T15:51:17Z'>
	<summary>Error executing spacy pretrain - IndexError: too many indices for array</summary>
	<description>
&lt;denchmark-h:h2&gt;How to reproduce the behaviour&lt;/denchmark-h&gt;

I created a sample JSONL file using below code
&lt;denchmark-code&gt;import spacy
import srsly
nlp = spacy.load('en_vectors_web_lg')
data = [{"text": "Some text"}, {"text": "More..."}]
srsly.write_jsonl("./data.jsonl", data)
&lt;/denchmark-code&gt;

And then run below commands
&lt;denchmark-code&gt;python -m spacy download en_vectors_web_lg
python -m spacy pretrain ./data.jsonl en_vectors_web_lg ./pretrained-model
&lt;/denchmark-code&gt;

But the spacy pretain command is leading to the error:
&lt;denchmark-code&gt;
ℹ Not using GPU
✔ Saved settings to config.json
✔ Loaded input texts
✔ Loaded model 'en_vectors_web_lg'

========================= Pre-training tok2vec layer =========================
  #      # Words   Total Loss     Loss    w/s
Traceback (most recent call last):
  File "/Users/nahidalam/anaconda3/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/Users/nahidalam/anaconda3/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/Users/nahidalam/anaconda3/lib/python3.6/site-packages/spacy/__main__.py", line 38, in &lt;module&gt;
    plac.call(commands[command], sys.argv[1:])
  File "/Users/nahidalam/anaconda3/lib/python3.6/site-packages/plac_core.py", line 328, in call
    cmd, result = parser.consume(arglist)
  File "/Users/nahidalam/anaconda3/lib/python3.6/site-packages/plac_core.py", line 207, in consume
    return cmd, self.func(*(args + varargs + extraopts), **kwargs)
  File "/Users/nahidalam/anaconda3/lib/python3.6/site-packages/spacy/cli/pretrain.py", line 126, in pretrain
    loss = make_update(model, docs, optimizer, drop=dropout)
  File "/Users/nahidalam/anaconda3/lib/python3.6/site-packages/spacy/cli/pretrain.py", line 157, in make_update
    predictions, backprop = model.begin_update(docs, drop=drop)
  File "/Users/nahidalam/anaconda3/lib/python3.6/site-packages/spacy/_ml.py", line 690, in mlm_forward
    output, backprop = model.begin_update(docs, drop=drop)
  File "/Users/nahidalam/anaconda3/lib/python3.6/site-packages/thinc/neural/_classes/feed_forward.py", line 46, in begin_update
    X, inc_layer_grad = layer.begin_update(X, drop=drop)
  File "/Users/nahidalam/anaconda3/lib/python3.6/site-packages/thinc/api.py", line 264, in begin_update
    X, bp_layer = layer.begin_update(layer.ops.flatten(seqs_in, pad=pad), drop=drop)
  File "/Users/nahidalam/anaconda3/lib/python3.6/site-packages/thinc/neural/_classes/feed_forward.py", line 46, in begin_update
    X, inc_layer_grad = layer.begin_update(X, drop=drop)
  File "/Users/nahidalam/anaconda3/lib/python3.6/site-packages/thinc/api.py", line 340, in uniqued_fwd
    keys = X[:, column]
IndexError: too many indices for array
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


Operating System: MacOS
spaCy version: 2.1.0
Platform: Darwin-18.2.0-x86_64-i386-64bit
Python version: 3.6.5

	</description>
	<comments>
		<comment id='1' author='nahidalam' date='2019-03-20T11:15:01Z'>
		Thanks for the report! I'm not 100% positive as I haven't tested yet, but I suspect if you're running exactly that example, the problem would be that none of the texts are meeting the minimum length requirement. We should catch this case and raise an error, but instead I think we're ending up with an empty array, which causes the confusing message.
Could you try with a bigger file, or setting the minimum length to 1?
		</comment>
		<comment id='2' author='nahidalam' date='2019-03-20T15:07:33Z'>
		&lt;denchmark-link:https://github.com/honnibal&gt;@honnibal&lt;/denchmark-link&gt;
 how do I set minimum length to 1?
		</comment>
		<comment id='3' author='nahidalam' date='2019-03-20T15:51:17Z'>
		Ok got it working after using longer text
		</comment>
		<comment id='4' author='nahidalam' date='2019-04-19T16:33:24Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>