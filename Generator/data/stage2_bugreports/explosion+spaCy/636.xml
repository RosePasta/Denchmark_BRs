<bug id='636' author='rajhans' open_date='2016-11-18T02:28:00Z' closed_time='2017-05-07T16:22:58Z'>
	<summary>to_bytes and from_bytes changes the token lemma</summary>
	<description>



import spacy
nlp=spacy.load('en')
x1=nlp('I cant do this.')
[t.lemma_ for t in x1]
['i', 'can', 'not', 'do', 'this', '.']
g=x1.to_bytes()
d=spacy.tokens.doc.Doc(nlp.vocab)
d.from_bytes(g)
I cant do this.
[t.lemma_ for t in d]
['i', 'ca', 'nt', 'do', 'this', '.']



	</description>
	<comments>
		<comment id='1' author='rajhans' date='2016-11-18T02:35:58Z'>
		FYI, I am using 1.1.2
		</comment>
		<comment id='2' author='rajhans' date='2016-11-18T02:39:06Z'>
		Thanks. I think the bug here is that the serializer tries to get away with not saving the lemmas, because it thinks it can recalculate them given the POS tags. This turns out to be untrue in this case, because the lemma is a special-case. Hmm.
		</comment>
		<comment id='3' author='rajhans' date='2016-11-21T00:14:23Z'>
		I see. Maybe as a stop gap for my project, is it possible to know for which words (like cant) can this problem arise? As in if it is a finite knowable set of words, I can just hackishly fix for those.
		</comment>
		<comment id='4' author='rajhans' date='2016-11-21T00:21:15Z'>
		Yes, the special-case rules are listed in spacy/en/language_data.py. You want to check whether the "F" value matches the the token.text. If it does, you want to assign the value keyed by "L".
		</comment>
		<comment id='5' author='rajhans' date='2016-11-22T08:57:55Z'>
		thanks!
		</comment>
		<comment id='6' author='rajhans' date='2017-05-07T16:22:58Z'>
		Closing this and making &lt;denchmark-link:https://github.com/explosion/spaCy/issues/1045&gt;#1045&lt;/denchmark-link&gt;
 the master issue. Work in progress for spaCy v2.0!
		</comment>
		<comment id='7' author='rajhans' date='2018-05-08T22:38:33Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>