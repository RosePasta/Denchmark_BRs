<bug id='4589' author='lonnekeeijdems' open_date='2019-11-05T12:33:57Z' closed_time='2019-11-22T09:22:10Z'>
	<summary>Lookups data not serialized with latest nl_core_news_sm</summary>
	<description>
&lt;denchmark-h:h2&gt;How to reproduce the behaviour&lt;/denchmark-h&gt;

doc = nlp(('Hallo, ik ben Piet. Ik heb gisteren iets gekocht.'))
for w in doc:
print(w.text, w.pos_, w.lemma_)
The result is:
hallo X hallo
, PUNCT ,
ik PRON ik
ben VERB ben
piet NOUN piet
. PUNCT .
ik PRON ik
heb VERB heb
gisteren ADV gisteren
iets PRON iets
gekocht VERB gekocht
. PUNCT .
So no word is lemmatized. This is happening since the new release. The pos tagger does work
	</description>
	<comments>
		<comment id='1' author='lonnekeeijdems' date='2019-11-05T12:55:51Z'>
		Are you using the latest model? You can run python -m spacy validate to check.
I just tried it locally using the latest spaCy (v2.2.2) and the latest compatible version of the model (v2.2.1) and my results look like this:
&lt;denchmark-code&gt;Hallo NOUN hallo
, PUNCT ,
ik PRON ik
ben VERB zijn
Piet NOUN piet
. PUNCT .
Ik PRON ik
heb VERB hebben
gisteren ADV gisteren
iets PRON iets
gekocht VERB kopen
. PUNCT .
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='lonnekeeijdems' date='2019-11-05T13:31:44Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/57399169/68211410-ca624480-ffd7-11e9-8a0d-77138dc6c41d.png&gt;&lt;/denchmark-link&gt;

Yes I use the versions you mentioned above. (see printscreen) I tried installing other packages, but it doesn't solve the issue.
		</comment>
		<comment id='3' author='lonnekeeijdems' date='2019-11-05T14:38:39Z'>
		The Dutch lemmatizer makes extensive use of lookup tables. These have been moved to a separate package in the new release. Have you already tried pip install spacy-lookups-data -- or, alternatively -- pip install spacy[lookups]?
		</comment>
		<comment id='4' author='lonnekeeijdems' date='2019-11-05T14:40:04Z'>
		It only works with spacy-lookups-data installed, so I think the tables didn't get packaged with the model as intended?
		</comment>
		<comment id='5' author='lonnekeeijdems' date='2019-11-05T14:53:10Z'>
		Prepare to be surprised, but that's actually the expected behavior. The idea is that lemmatization data is an attribute of individual Language subclasses (e.g., as defined in spacy/lang/nl/__init__.py). In fact, previous versions included this data as large dictionaries in python modules that sometimes took up upwards of 10MB in disk space. Trouble is -- as more languages added lemmatization data,  spaCy came to mean a fairly hefty download. The core developers have begun remedying this situation in the latest release.
		</comment>
		<comment id='6' author='lonnekeeijdems' date='2019-11-05T16:16:48Z'>
		The lemmatizer data should be shipped in the models, though, and serialize to disk when you save out the nlp object. If that's not the case, that's definitely a bug.
I just checked and the vocab directory of the installed model doesn't include the lookup files â€“ maybe the lookups data package wasn't available when the model was trained. So we should fix that. In the meantime, installing spacy-lookups-data is a fine workaround.
		</comment>
		<comment id='7' author='lonnekeeijdems' date='2019-11-05T19:05:16Z'>
		Huh... I stand corrected. My bad!
		</comment>
		<comment id='8' author='lonnekeeijdems' date='2019-11-06T10:20:46Z'>
		&lt;denchmark-link:https://github.com/lemontheme&gt;@lemontheme&lt;/denchmark-link&gt;
 Everything else you said and how you explained the lookups was correct, though!
		</comment>
		<comment id='9' author='lonnekeeijdems' date='2019-11-22T09:22:10Z'>
		This has been fixed in newer nl model releases.
		</comment>
		<comment id='10' author='lonnekeeijdems' date='2019-12-22T10:01:49Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>