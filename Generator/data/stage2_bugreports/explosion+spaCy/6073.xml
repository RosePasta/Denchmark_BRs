<bug id='6073' author='simonschoe' open_date='2020-09-16T07:53:10Z' closed_time='2020-09-22T19:52:43Z'>
	<summary>[E102] Can't merge non-disjoint spans</summary>
	<description>
I feel my issue is related to &lt;denchmark-link:https://github.com/explosion/spaCy/issues/5393&gt;#5393&lt;/denchmark-link&gt;
, in my case occurring with German text.
&lt;denchmark-h:h2&gt;How to reproduce the behaviour&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import spacy
import de_core_news_md
#load the German NLP spacy model
nlp = spacy.load('de_core_news_md')

merge_nps = nlp.create_pipe("merge_noun_chunks")
nlp.add_pipe(merge_nps)

texts = ["Weil Deutschlands Topkonzerne bestens aufgestellt sind, hält der\nAufwärtstrend in diesem Jahr an, sagen die vom Handelsblatt befragten\nKapitalmarktexperten."]

nlp_pipeline = nlp.pipe(texts, disable=[], batch_size = 20) 
for doc in nlp_pipeline:
    lemma = [token.lemma_.lower() for token in doc]
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


spaCy version: 2.3.2
Platform: Windows-10-10.0.18362-SP0
Python version: 3.8.5

&lt;denchmark-h:h2&gt;Error Message&lt;/denchmark-h&gt;

ValueError: [E102] Can't merge non-disjoint spans. 'Deutschlands' is already part of tokens to merge. If you want to find the longest non-overlapping spans, you can use the util.filter_spans helper: https://spacy.io/api/top-level#util.filter_spans
	</description>
	<comments>
		<comment id='1' author='simonschoe' date='2020-09-16T08:20:47Z'>
		Thanks for the report! I can replicate the error and it does look related to the earlier issue. It looks like the German noun chunk iterator wasn't modified in that earlier fix, but it may require different modifications anyway. We will take a look...
		</comment>
	</comments>
</bug>