<bug id='2690' author='asantos-6' open_date='2018-08-21T15:34:02Z' closed_time='2018-10-12T13:08:50Z'>
	<summary>Can't deserialize nlp object with from_bytes()</summary>
	<description>
&lt;denchmark-h:h2&gt;Definition&lt;/denchmark-h&gt;

I retrained the 'pt_core_news_sm' to include a new entity type and it works fine when I load it locally like so:
&lt;denchmark-code&gt;nlp = spacy.load('path_to_my_model')
&lt;/denchmark-code&gt;

Then I serialize it with the to_bytes function:
&lt;denchmark-code&gt;nlp_bytes = nlp.to_bytes()
&lt;/denchmark-code&gt;

Up until now everything works fine, the problem is when I try to load this model onto a new blank Language model using the from_bytes function:
&lt;denchmark-code&gt;nlp2 = Portuguese()
nlp2 = nlp2.from_bytes(nlp_bytes)
&lt;/denchmark-code&gt;

According to your documentation in your website this was supposed to be enough. However, it doesn't load the model correctly onto nlp2 since its entity structure inside says that there is "No component 'ner' found in the pipeline. Available names: []" and the results using this model are way off.
Are you aware of this or am I just doing something wrong? Note that I want to be able to do this because I don't want to load the model I trained from disk but from a serialized file where I would write the nlp_bytes.
Thank you in advance
&lt;denchmark-h:h2&gt;My Environment&lt;/denchmark-h&gt;


Operating System: Ubuntu 18.04.1
Python Version Used: Python 2.7
spaCy Version Used: spaCy 2.0.11

	</description>
	<comments>
		<comment id='1' author='asantos-6' date='2018-08-26T17:23:50Z'>
		Would you mind trying this in v2.1.0a1, which you could install with pip install spacy-nightly? I think it should be fixed there.
		</comment>
		<comment id='2' author='asantos-6' date='2018-08-27T10:13:32Z'>
		I did, but apparently there's an incompatibility with the model saved with the version 2.0.11 and version 2.1.0a1 because when I try to run nlp = spacy.load('path_to_my_model') the following error occurs:
&lt;denchmark-code&gt;IOError: [Errno 2] No such file or directory: '.../my_model/parser/model'
&lt;/denchmark-code&gt;

Does this mean that the way things are saved with Language.to_disk() changed? And if so, that means I would have to train and save the model again to a new folder and then load it to test if indeed the serialization/desserialization problem is solved, correct?
Thanks for answering by the way!
P.S.: Apparently I closed this issue by mistake, had to reopen it, sorry
		</comment>
		<comment id='3' author='asantos-6' date='2018-09-28T12:46:30Z'>
		You'll need to retrain the model between v2.0.x and v2.1.x --- the settings are different, so models aren't compatible across those versions. Could you let us know whether the problem is still occurring after you retrain?
		</comment>
		<comment id='4' author='asantos-6' date='2018-10-12T13:08:47Z'>
		This issue has been automatically closed because there has been no response to a request for more information from the original author. With only the information that is currently in the issue, there's not enough information to take action. If you're the original author, feel free to reopen the issue if you have or find the answers needed to investigate further.
		</comment>
		<comment id='5' author='asantos-6' date='2018-11-11T13:28:58Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>