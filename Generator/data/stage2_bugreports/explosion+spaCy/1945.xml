<bug id='1945' author='thomasopsomer' open_date='2018-02-06T17:46:10Z' closed_time='2018-02-12T11:13:23Z'>
	<summary>Missing matches in PhraseMatcher</summary>
	<description>
Hi,
I found a bug with the PhraseMatcher. See the following code to reproduce.
from __future__ import unicode_literals
from spacy.lang.en import English
from spacy.matcher import PhraseMatcher

text = "deep machine learning"
mw_list = ["machine learning", "deep blue", "planing machine"]

nlp = English()
matcher = PhraseMatcher(nlp.vocab)
matcher.add("MWE", None, *[nlp.tokenizer(item) for item in mw_list])

assert len(matcher(nlp(text))) == 1
I guess the problem comes from the latest matcher refactoring because it works well in spacy 2.0.5.
I didn't look into the matcher code, it took me so long to find what was going wrong ^^, but the pattern causing this issue is quite clear is the example.
&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


spaCy version: 2.0.7
Platform: Darwin-16.7.0-x86_64-i386-64bit
Python version: 3.6.2
Models: en, en_core_web_lg, en_core_web_md

	</description>
	<comments>
		<comment id='1' author='thomasopsomer' date='2018-02-06T21:01:06Z'>
		running into similar issues as well
		</comment>
		<comment id='2' author='thomasopsomer' date='2018-02-07T00:11:04Z'>
		Thanks -- will investigate
		</comment>
		<comment id='3' author='thomasopsomer' date='2018-02-07T01:07:18Z'>
		&lt;denchmark-link:https://github.com/GregDubbin&gt;@GregDubbin&lt;/denchmark-link&gt;
 I think this is occurring because the matcher isn't returning the start and end indices correctly if the same pattern matches multiple times in the string. I see that this situation is covered by your comments -- apologies for not thinking the requirements through more carefully when I was reviewing your patch.
For reference, the following hackery causes the current test to pass, while turning one of your tests for greedy matching red:
+++ b/spacy/matcher.pyx
@@ -389,6 +389,7 @@ cdef class Matcher:
             while j &lt; n_partials:
                 state = partials[j]
                 action = get_action(state.pattern, token)
+                print("Token", token_i, "partial", j, "action", action)
                 j += 1
                 # Skip patterns that would overlap with an existing match
                 # Patterns overlap an existing match if they point to the
@@ -397,9 +398,12 @@ cdef class Matcher:
                 # Different patterns with the same label are allowed to 
                 # overlap.
                 state_match = state.last_match
-                if (state.start &gt; state_match.start 
-                    and state.start &lt; state_match.end):
-                    continue
+                #if (state.start &gt; state_match.start 
+                #    and state.start &lt; state_match.end):
+                #    print("Reject overlap 0")
+                #    print("state.start", state.start,
+                #          "state_match.start", state_match.start, "state_match.end", state_match.end)
+                #    continue
                 if action == PANIC:
                     raise Exception("Error selecting action in matcher")
                 while action == ADVANCE_ZERO:
@@ -433,6 +437,7 @@ cdef class Matcher:
                         overlap = True
                         break
                 if overlap:
+                    print("Reject overlap 1")
                     continue
                 overlap=False
                 for i in range(q):
@@ -440,6 +445,7 @@ cdef class Matcher:
                         overlap = True
                         break
                 if overlap:
+                    print("Reject overlap 2")
                     continue
 
     
@@ -466,7 +472,7 @@ cdef class Matcher:
                     # match (i.e. it starts after this match starts).
                     state_match = state.last_match
 
-                    if start &gt;= state_match.end:
+                    if start &gt;= state_match.end or True:
                         state_match.start = start
                         state_match.end = end
                         state_match.offset = len(matches)
@@ -493,10 +499,12 @@ cdef class Matcher:
                 # state_match = pattern.last_match
                 state_match = &amp;last_matches[i]
                 i+=1
-                if (token_i &gt; state_match.start 
-                    and token_i &lt; state_match.end):
-                    continue
+                #if (token_i &gt; state_match.start 
+                #    and token_i &lt; state_match.end):
+                #    continue
                 action = get_action(pattern, token)
+                print("Token", token_i, "open pattern", "action", action)
                 if action == PANIC:
                     raise Exception("Error selecting action in matcher")
                 while action in (ADVANCE_PLUS,ADVANCE_ZERO):
		</comment>
		<comment id='4' author='thomasopsomer' date='2018-02-07T15:41:24Z'>
		&lt;denchmark-link:https://github.com/honnibal&gt;@honnibal&lt;/denchmark-link&gt;
 Yes I was unsure about the desired behavior in that case and decided to implement it in a way that would match python's re.findall.  I hadn't considered how it would effect the PhraseMatcher.
		</comment>
		<comment id='5' author='thomasopsomer' date='2018-02-09T06:59:46Z'>
		&lt;denchmark-link:https://github.com/GregDubbin&gt;@GregDubbin&lt;/denchmark-link&gt;
 I actually had to check that to make sure:
&gt;&gt;&gt; re.findall(r'ab|bc', 'abc')
['ab']
I'll try making a toy implementation in pure Python to think this through. I'm worried the requirements might be contradictory.
		</comment>
		<comment id='6' author='thomasopsomer' date='2018-02-09T07:35:13Z'>
		Current sketch of a pure Python implementation.
EDIT: Superceded; see comment below.
def find_matches(patterns, doc):                                                                                         
    partials = []                                                                                                        
    matches = []                                                                                                         
    for token in doc:                                                                                                    
        nexts = []                                                                                                       
        for partial in (partials + patterns):                                                                            
            matches, nexts = transition(partial, token, matches, nexts)                                                  
        partials = nexts                                                                                                 
    return matches                                                                                                       
                                                                                                                         
                                                                                                                         
def transition(partial, token, matches, nexts):                                                                          
    pattern, state, start = partial                                                                                      
    is_match, keep_state, advance_state = get_action(state, token)                                                       
    if is_match:                                                                                                         
        matches.append((pattern, start.i, token.i+1))                                                                    
    if keep_state:                                                                                                       
        nexts.append(partial)                                                                                            
    if advance_state:                                                                                                    
        nexts.append((pattern, state[1:], start))                                                                        
    return (matches, nexts)                                                                                              
                                                                                                                         
                                                                                                                         
def get_action(state, token):                                                                                            
    '''We need to consider:                                                                                              
                                                                                                                         
    a) Does the token match the specification? [Yes, No]                                                                 
    b) What's the quantifier? [!, 1, +]                                                                                  
    c) Is this the last specification? [final, non-final]                                                                
                                                                                                                         
    We therefore have 12 cases to consider. For each case, we need to know                                               
    whether to emit a match, whether to keep the current state in the partials,                                          
    and whether to add an advanced state to the partials.                                                                
                                                                                                                         
    We therefore have six possible results for these three booleans, which                                               
    we'll code as 000, 001 etc.
    
    - Yes, !, non-final
      000
    - Yes, 1, non-final
      001
    - Yes, +, non-final
      011
    - No, !, non-final
      010
    - No, 1, non-final
      000
    - No, +, non-final
      000
    - Yes, !, final
      000
    - Yes, 1, final
      100
    - Yes, +, final
      111
    - No, !, final
      100
    - No, 1, final
      000
    - No, +, final
      000

    Problem: If a quantifier is matching, we're adding a lot of open partials
    '''
    pass
		</comment>
		<comment id='7' author='thomasopsomer' date='2018-02-11T10:51:33Z'>
		Simplified the above and started fleshing it out. Now leaving aside the '!' operator, as this is better handled as an 'invert match' semantics.
EDIT: Superceded, see below.
import pytest                                                                                                            
                                                                                                                         
class Vocab(object):                                                                                                     
    pass                                                                                                                 
                                                                                                                         
class Doc(list):                                                                                                         
    def __init__(self, vocab, words=None):                                                                               
        list.__init__(self)                                                                                              
        self.extend([Token(i, w) for i, w in enumerate(words)])                                                          
                                                                                                                         
                                                                                                                         
class Token(object):                                                                                                     
    def __init__(self, i, word):                                                                                         
        self.i = i                                                                                                       
        self.text = word                                                                                                 
                                                                                                                         
                                                                                                                         
def find_matches(patterns, doc):                                                                                         
    partials = []                                                                                                        
    matches = []                                                                                                         
    for token in doc:                                                                                                    
        nexts = []                                                                                                       
        for partial in (partials + patterns):                                                                            
            matches, nexts = transition(partial, token, matches, nexts)                                                  
        partials = nexts                                                                                                 
    return matches                                                                                                       
                                                                                                                         
                                                                                                                         
def transition(partial, token, matches, nexts):                                                                          
    pattern, state, start = partial
    action = get_action(state, token)                                                                                   
    is_match, keep_state, advance_state = [bool(int(c)) for c in action]                                                
    if is_match:                                                                                                         
        matches.append((pattern, start.i, token.i+1))                                                                    
    if keep_state:                                                                                                       
        nexts.append(partial)                                                                                            
    if advance_state:
        nexts.append((pattern, state[1:], start))
    return (matches, nexts)


def get_action(state, token):
    '''We need to consider:

    a) Does the token match the specification? [Yes, No]
    b) What's the quantifier? [1, +]
    c) Is this the last specification? [final, non-final]

    We therefore have 8 cases to consider. For each case, we need to know
    whether to emit a match, whether to keep the current state in the partials,
    and whether to add an advanced state to the partials.

    We therefore have eight possible results for these three booleans, which
    we'll code as 000, 001 etc.
    
    - No match:
      000
    - Match, final:
        1: 100
        +: 110
    - Match, non-final:
        1: 001
        +: 011

    Problem: If a quantifier is matching, we're adding a lot of open partials
    '''
    is_match = get_is_match(state, token)
    operator = get_operator(state, token)
    is_final = get_is_final(state, token)
    if not is_match:
        return '000'
    elif operator == '1':
        if is_final:
            return '100'
        else:
            return '001'
    elif operator == '+':
        if is_final:
            return '110'
        else:
            return '011'
    else:
        print(operator, is_match, is_final)
        raise ValueError


def get_is_match(state, token):
    pattern, i, start = state
    is_match = token.text == pattern[i]['spec']
    if pattern[i].get('invert'):
        return not is_match
    else:
        return is_match

def get_is_final(state, token):
    pattern, i, start = state
    return i == len(pattern)-1

def get_operator(state, token):
    pattern, i, start = state
    return pattern[i].get('op', '1')


def test_get_action_simple_match():
    pattern = [{'spec': 'a', 'op': '1'}]
    doc = Doc(Vocab(), words=['a'])
    state = (pattern, 0, None)
    action = get_action(state, doc[0])
    assert action == '100'


def test_get_action_simple_reject():
    pattern = [{'spec': 'b', 'op': '1'}]
    doc = Doc(Vocab(), words=['a'])
    state = (pattern, 0, None)
    action = get_action(state, doc[0])
    assert action == '000'

def test_get_action_simple_match_match():
    pattern = [{'spec': 'a', 'op': '1'}, {'spec': 'a', 'op': '1'}]
    doc = Doc(Vocab(), words=['a', 'a'])
    state = (pattern, 0, None)
    action = get_action(state, doc[0])
    assert action == '001'
    state = (pattern, 1, 0)
    action = get_action(state, doc[1])
    assert action == '100'

def test_get_action_simple_match_reject():
    pattern = [{'spec': 'a', 'op': '1'}, {'spec': 'b', 'op': '1'}]
    doc = Doc(Vocab(), words=['a', 'a'])
    state = (pattern, 0, None)
    action = get_action(state, doc[0])
    assert action == '001'
    state = (pattern, 1, 0)
    action = get_action(state, doc[1])
    assert action == '000'

def test_get_action_simple_match_reject():
    pattern = [{'spec': 'a', 'op': '1'}, {'spec': 'b', 'op': '1'}]
    doc = Doc(Vocab(), words=['a', 'a'])
    state = (pattern, 0, None)
    action = get_action(state, doc[0])
    assert action == '001'
    state = (pattern, 1, 0)
    action = get_action(state, doc[1])
    assert action == '000'

def test_get_action_plus_match():
    pattern = [{'spec': 'a', 'op': '+'}]
    doc = Doc(Vocab(), words=['a'])
    state = (pattern, 0, None)
    action = get_action(state, doc[0])
    assert action == '110'

def test_get_action_plus_match_match():
    pattern = [{'spec': 'a', 'op': '+'}]
    doc = Doc(Vocab(), words=['a', 'a'])
    state = (pattern, 0, None)
    action = get_action(state, doc[0])
    assert action == '110'
    state = (pattern, 0, 0)
    action = get_action(state, doc[1])
    assert action == '110'
		</comment>
		<comment id='8' author='thomasopsomer' date='2018-02-11T15:02:56Z'>
		Bug fixes, more tests:
import pytest


class Vocab(object):
    pass


class Doc(list):
    def __init__(self, vocab, words=None):
        list.__init__(self)
        self.extend([Token(i, w) for i, w in enumerate(words)])


class Token(object):
    def __init__(self, i, word):
        self.i = i
        self.text = word


def find_matches(patterns, doc):
    init_states = [(pattern, 0, None) for pattern in patterns]
    curr_states = []
    matches = []
    for token in doc:
        nexts = []
        for state in (curr_states + init_states):
            matches, nexts = transition(state, token, matches, nexts)
        curr_states = nexts
    return matches
 

def transition(state, token, matches, nexts):
    action = get_action(state, token)
    is_match, keep_state, advance_state = [bool(int(c)) for c in action]
    pattern, i, start = state
    if start is None:
        start = token.i
    if is_match:
        matches.append((pattern, start, token.i+1))
    if keep_state:
        nexts.append((pattern, i, start))
    if advance_state:
        nexts.append((pattern, i+1, start))
    return (matches, nexts)


def get_action(state, token):
    '''We need to consider:

    a) Does the token match the specification? [Yes, No]
    b) What's the quantifier? [1, +]
    c) Is this the last specification? [final, non-final]

    We therefore have 8 cases to consider. For each case, we need to know
    whether to emit a match, whether to keep the current state in the partials,
    and whether to add an advanced state to the partials.

    We therefore have eight possible results for these three booleans, which
    we'll code as 000, 001 etc.
    
    - No match:
      000
    - Match, final:
        1: 100
        +: 110
    - Match, non-final:
        1: 001
        +: 011

    Problem: If a quantifier is matching, we're adding a lot of open partials
    '''
    is_match = get_is_match(state, token)
    operator = get_operator(state, token)
    is_final = get_is_final(state, token)
    if not is_match:
        return '000'
    elif operator == '1':
        if is_final:
            return '100'
        else:
            return '001'
    elif operator == '+':
        if is_final:
            return '110'
        else:
            return '011'
    else:
        print(operator, is_match, is_final)
        raise ValueError


def get_is_match(state, token):
    pattern, i, start = state
    is_match = token.text == pattern[i]['spec']
    if pattern[i].get('invert'):
        return not is_match
    else:
        return is_match

def get_is_final(state, token):
    pattern, i, start = state
    return i == len(pattern)-1

def get_operator(state, token):
    pattern, i, start = state
    return pattern[i].get('op', '1')

########################
# Tests for get_action #
########################


def test_get_action_simple_match():
    pattern = [{'spec': 'a', 'op': '1'}]
    doc = Doc(Vocab(), words=['a'])
    state = (pattern, 0, None)
    action = get_action(state, doc[0])
    assert action == '100'


def test_get_action_simple_reject():
    pattern = [{'spec': 'b', 'op': '1'}]
    doc = Doc(Vocab(), words=['a'])
    state = (pattern, 0, None)
    action = get_action(state, doc[0])
    assert action == '000'


def test_get_action_simple_match_match():
    pattern = [{'spec': 'a', 'op': '1'}, {'spec': 'a', 'op': '1'}]
    doc = Doc(Vocab(), words=['a', 'a'])
    state = (pattern, 0, None)
    action = get_action(state, doc[0])
    assert action == '001'
    state = (pattern, 1, 0)
    action = get_action(state, doc[1])
    assert action == '100'


def test_get_action_simple_match_reject():
    pattern = [{'spec': 'a', 'op': '1'}, {'spec': 'b', 'op': '1'}]
    doc = Doc(Vocab(), words=['a', 'a'])
    state = (pattern, 0, None)
    action = get_action(state, doc[0])
    assert action == '001'
    state = (pattern, 1, 0)
    action = get_action(state, doc[1])
    assert action == '000'


def test_get_action_simple_match_reject():
    pattern = [{'spec': 'a', 'op': '1'}, {'spec': 'b', 'op': '1'}]
    doc = Doc(Vocab(), words=['a', 'a'])
    state = (pattern, 0, None)
    action = get_action(state, doc[0])
    assert action == '001'
    state = (pattern, 1, 0)
    action = get_action(state, doc[1])
    assert action == '000'

def test_get_action_plus_match():
    pattern = [{'spec': 'a', 'op': '+'}]
    doc = Doc(Vocab(), words=['a'])
    state = (pattern, 0, None)
    action = get_action(state, doc[0])
    assert action == '110'


def test_get_action_plus_match_match():
    pattern = [{'spec': 'a', 'op': '+'}]
    doc = Doc(Vocab(), words=['a', 'a'])
    state = (pattern, 0, None)
    action = get_action(state, doc[0])
    assert action == '110'
    state = (pattern, 0, 0)
    action = get_action(state, doc[1])
    assert action == '110'

##########################
# Tests for find_matches #
##########################

def test_find_matches_simple_accept():
    pattern = [{'spec': 'a', 'op': '1'}]
    doc = Doc(Vocab(), words=['a'])
    matches = find_matches([pattern], doc)
    assert matches == [(pattern, 0, 1)]


def test_find_matches_simple_reject():
    pattern = [{'spec': 'a', 'op': '1'}]
    doc = Doc(Vocab(), words=['b'])
    matches = find_matches([pattern], doc)
    assert matches == []


def test_find_matches_match_twice():
    pattern = [{'spec': 'a', 'op': '1'}]
    doc = Doc(Vocab(), words=['a', 'a'])
    matches = find_matches([pattern], doc)
    assert matches == [(pattern, 0, 1), (pattern, 1, 2)]


def test_find_matches_longer_pattern():
    pattern = [{'spec': 'a', 'op': '1'}, {'spec': 'b', 'op': '1'}]
    doc = Doc(Vocab(), words=['a', 'b'])
    matches = find_matches([pattern], doc)
    assert matches == [(pattern, 0, 2)]


def test_find_matches_two_patterns():
    patterns = [[{'spec': 'a', 'op': '1'}], [{'spec': 'b', 'op': '1'}]]
    doc = Doc(Vocab(), words=['a', 'b'])
    matches = find_matches(patterns, doc)
    assert matches == [(patterns[0], 0, 1), (patterns[1], 1, 2)]


def test_find_matches_two_patterns_overlap():
    patterns = [[{'spec': 'a'}, {'spec': 'b'}],
                [{'spec': 'b'}, {'spec': 'c'}]]
    doc = Doc(Vocab(), words=['a', 'b', 'c'])
    matches = find_matches(patterns, doc)
    assert matches == [(patterns[0], 0, 2), (patterns[1], 1, 3)]


def test_find_matches_greedy():
    patterns = [[{'spec': 'a', 'op': '+'}]]
    doc = Doc(Vocab(), words=['a'])
    matches = find_matches(patterns, doc)
    assert matches == [(patterns[0], 0, 1)]
    doc = Doc(Vocab(), words=['a', 'a'])
    matches = find_matches(patterns, doc)
    assert matches == [(patterns[0], 0, 1), (patterns[0], 0, 2), (patterns[0], 1, 2)]
		</comment>
		<comment id='9' author='thomasopsomer' date='2018-02-11T15:12:29Z'>
		Okay, I'm convinced that the overall approach in the current Matcher is sound --- it's just that the implementation's gotten out of control.
The fundamental thing is that we don't have quantifiers scoping over multi-token expressions. This means we don't need to back-track, so we can indeed get away with looping over the tokens, and then looping over the patterns.
The main thing left in the Python prototype above is to improve the efficiency of the "+" quantifier. Currently if we're matching a "+" we open a new partial pattern on each matching token. These partial patterns then have to match on each token.
I wonder whether it's sufficient to hash the token specifiers, and then cache their results on each token. That way, if we ask the same question of a token twice, we can look up the answer. This might mean a proliferation of partial matches doesn't matter. If this allows the logic to be simpler, this approach might be more efficient.
		</comment>
		<comment id='10' author='thomasopsomer' date='2018-02-11T15:32:44Z'>
		The main loop and data structures in Cython would look like this:
cdef struct AttrValueC:
    attr_id_t attr
    attr_t value

cdef struct TokenPatternC:
    AttrValueC* attrs
    int32_t nr_attr
    quantifier_t quantifier

cdef struct PatternStateC:
    TokenPatternC** state
    int32_t pattern_id
    int32_t start_token


cdef struct MatchEntryC:
    int32_t pattern_id
    int32_t start_token
    int32_t end_token


def find_matches(TokenPatternC** patterns, int n, Doc doc):
    cdef vector[PatternStateC] init_states
    for i in range(n):
        init_states.push_back(PatternStateC(&amp;patterns[i], i, -1))
    cdef vector[PatternStateC] curr_states
    cdef vector[PatternStateC] nexts
    cdef vector[MatchEntryC] matches
    cdef void* cache
    for i in range(doc.length):
        nexts.clear()
        for j in range(curr_states.size()):
            action = get_action(curr_states[j], &amp;doc.c[i], cache)
            transition(action, curr_states[j], matches, nexts, i)
        for j in range(init_states.size()):
            action = get_action(init_states[j], &amp;doc.c[i], cache)
            transition(action, init_states[j], matches, nexts, i)
    return matches
		</comment>
		<comment id='11' author='thomasopsomer' date='2018-02-12T11:13:23Z'>
		Merging this with the master issue &lt;denchmark-link:https://github.com/explosion/spaCy/issues/1971&gt;#1971&lt;/denchmark-link&gt;
!
		</comment>
		<comment id='12' author='thomasopsomer' date='2018-05-07T23:54:59Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>