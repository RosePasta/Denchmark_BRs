<bug id='4819' author='kormilitzin' open_date='2019-12-18T17:18:36Z' closed_time='2020-02-14T13:18:41Z'>
	<summary>spacy pretrain CLI fails with custom parameters</summary>
	<description>
I am using spacy pretrain CLI command to train -t2v to further use for spacy train, howeer, if I change the parameters (network size) in spacy pretrain, I can't then use the pretrained weights with spacy train`.
Specifically:
This combination work OK:
&lt;denchmark-code&gt;$ python -m spacy pretrain /data en_vectors_web_lg /spacy_pretrain_vectors_01 --use-vectors -i 1
$ python -m spacy train en ./cli_models/tmp_01 /data_train /data_dev -t2v spacy_pretrain_vectors_01/model0.bin -p ner -g 0 
&lt;/denchmark-code&gt;

If I create a large embeddings, it is still OK:
&lt;denchmark-code&gt;$ python -m spacy pretrain /data en_vectors_web_lg /spacy_pretrain_vectors_02 --use-vectors -i 1 -cw 128
$ token_vector_width=128 python -m spacy train en ./cli_models/tmp_02 /data_train /data_dev -t2v spacy_pretrain_vectors_02/model0.bin -p ner -g 0
&lt;/denchmark-code&gt;

However, if I change the depth, then it fails:
&lt;denchmark-code&gt;$ python -m spacy pretrain /data en_vectors_web_lg /spacy_pretrain_vectors_03 --use-vectors -i 1 -cd 8
$ python -m spacy train en ./cli_models/tmp_03 /data_train /data_dev -t2v spacy_pretrain_vectors_03/model0.bin -p ner -g 0
&lt;/denchmark-code&gt;

with the following error:
AttributeError: 'HashEmbed' object has no attribute 'G'
if I include LSTM in pretraining then it also fails:
&lt;denchmark-code&gt;$ python -m spacy pretrain /data en_vectors_web_lg /spacy_pretrain_vectors_04 --use-vectors -i 1 -lstm 2
$ python -m spacy train en ./cli_models/tmp_04 /data_train /data_dev -t2v spacy_pretrain_vectors_04/model0.bin -p ner -g 0
&lt;/denchmark-code&gt;

with the following error:
TypeError: byte indices must be integers or slices, not bytes
I guess I am missing some parameters that should be included in spacy training to account for new parameters values.
Thanks!
&lt;denchmark-h:h2&gt;Info about spaCy&lt;/denchmark-h&gt;


spaCy version: 2.2.3
Platform: Linux-4.15.0-70-generic-x86_64-with-Ubuntu-18.04-bionic
Python version: 3.6.8
Models: en

	</description>
	<comments>
		<comment id='1' author='kormilitzin' date='2020-02-14T13:14:19Z'>
		Thanks for the helpful report! It looks like the  script was not yet suited to account for these kind of parameter changes, PR &lt;denchmark-link:https://github.com/explosion/spaCy/pull/5021&gt;#5021&lt;/denchmark-link&gt;
 should fix that.
So what happens behind the scenes is this: during pretrain, a Tok2Vec component is built with the parameters provided, and the raw bytes data is stored in the modelX.bin files. Then when you run the train command, first a pipeline is created with the parameters provided. During creation, pipeline components initialize their Tok2Vec components.
Then, when you've specified init_tok2vec with the t2v flag, the binary data is loaded into the component's Tok2Vec. This means that the two Tok2vec models (the one defined by pretrain and the one defined by train) need to have the exact same parameters and settings, or thinc may throw all kind of errors relating to different sizes or parameters missing.
So after the PR is merged:

if you use --use-vectors in pretrain, you should also use -v in train
if you use any of these parameters in pretrain, you should use the exact same in train: cw, cW, cd, cP, chr, lstm, er

The upcoming spaCy v.3 should further facilitate these kind of parameter passings by working with proper config files :-)
		</comment>
		<comment id='2' author='kormilitzin' date='2020-02-14T13:18:39Z'>
		Thanks a lot &lt;denchmark-link:https://github.com/svlandeg&gt;@svlandeg&lt;/denchmark-link&gt;
 ! After a while I discovered these params and eventually ended up specifying them as hyperparams. Also, I found a simple script of Matt, which he has written to pick such parameters with Prodigy (though not for lstm and a few others). It will be great to see a similar automation with spacy CLI train in future release as you mentioned. Thanks again!
		</comment>
		<comment id='3' author='kormilitzin' date='2020-03-17T13:26:35Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>