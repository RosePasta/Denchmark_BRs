<bug id='684' author='pokey' open_date='2016-12-13T13:49:33Z' closed_time='2016-12-18T21:52:02Z'>
	<summary>Different ways of loading data model</summary>
	<description>
I get different word vectors depending how I load the data model.  For example:
&lt;denchmark-code&gt;import spacy.en
import spacy
import numpy

nlp1 = spacy.en.English()
nlp2 = spacy.load('en')

word = 'shop'

# The following assert fails:
assert(numpy.allclose(nlp1.vocab[word].vector, nlp2.vocab[word].vector))
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;My Environment&lt;/denchmark-h&gt;


OS X 10.11.6
Python 3.5.2
spacy 1.2.0

	</description>
	<comments>
		<comment id='1' author='pokey' date='2016-12-18T21:51:51Z'>
		Thanks, this was a bad bug! When I switched default support to the GloVe vectors in 1.0, I added a hack to the spacy.load() function, to provide temporary backwards compatibility to existing data installations. This hack should have been inserted into spacy.en.English().
The version of the vectors loaded by spacy.load() is the "correct" one, which will continue to be supported. If you want to see whether this bug affected some model you've been using, the easiest way to check whether you're using the correct GloVe vector data is to count the number of lexemes with a word vector:
&gt;&gt;&gt; import spacy
&gt;&gt;&gt; nlp = spacy.load('en')
&gt;&gt;&gt; sum(w.has_vector for w in nlp.vocab)
645315
If you see a ~300,000, your model has the older vectors trained on Wikipedia loaded.
		</comment>
		<comment id='2' author='pokey' date='2018-05-09T05:38:57Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>