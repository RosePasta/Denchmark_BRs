<bug id='4099' author='dxiao2003' open_date='2019-08-08T20:23:00Z' closed_time='2019-10-02T15:24:58Z'>
	<summary>Prevent base English models from producing "subtok" parse?</summary>
	<description>
&lt;denchmark-h:h2&gt;How to reproduce the behaviour&lt;/denchmark-h&gt;

Using the base English en_core_web_sm model, it's possible for the dependency parser to produce tokens with the "subtok" label. This seems like unexpected behavior and it's definitely problematic since training the parser on examples with "subtok" labels causes crashes.
To reproduce:
&lt;denchmark-code&gt;&gt; nlp = spacy.load("en_core_web_sm")
&gt; d = nlp("I'd rather have one Sharks with no crunch mustard instead of 205S")
&gt; d[11].dep_
'subtok'
&lt;/denchmark-code&gt;

Granted the above isn't a particularly well-formed English sentence, but nevertheless I would not expect the "subtok" label. Is there a flag we can set that prevents the model from generating this label?
Also, running merge_subtokens afterwards isn't a solution because I need "of" to be a separate token.
&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Info about spaCy&lt;/denchmark-h&gt;


spaCy version: 2.1.8
Platform: Linux-4.15.0-1040-aws-x86_64-with-debian-9.9
Python version: 3.7.3

	</description>
	<comments>
		<comment id='1' author='dxiao2003' date='2019-08-09T10:44:25Z'>
		The  dependency labels are a known problem (&lt;denchmark-link:https://github.com/explosion/spaCy/issues/3830&gt;#3830&lt;/denchmark-link&gt;
) with the current models, but I think the models need to be retrained to fix it, which as far as I know is planned for 2.2. No simple/quick solution here, sorry.
The fact that including a subtok label in your data causes crashes when training the parser is a separate issue that should definitely be addressed.
		</comment>
		<comment id='2' author='dxiao2003' date='2019-08-12T17:28:20Z'>
		I think I understand why training is crashing. If I understand correctly subtok labelled tokens as produced by the built-in models always have the next token as their head. But for our examples sometimes we have to massage them and introduce intervening tokens, so that now the subtok token's head is two or three tokens away. I'm guessing this may be what's causing training to fail.
		</comment>
		<comment id='3' author='dxiao2003' date='2019-10-02T15:24:58Z'>
		Just released v2.2, which fixes the subtok issue!
		</comment>
		<comment id='4' author='dxiao2003' date='2019-11-01T15:54:37Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>