<bug id='3013' author='emarcey' open_date='2018-12-05T23:12:54Z' closed_time='2018-12-18T23:35:35Z'>
	<summary>Memory leak using en_core_web_sm nlp</summary>
	<description>
&lt;denchmark-h:h2&gt;How to reproduce the behaviour&lt;/denchmark-h&gt;

Using the en_core_web_sm model to tokenize text and we noticed our memory was climbing and led to OOM errors.
Below is a script that performs the following steps:

Loads en_core_web_sm
Then runs a subprocess 10 times
First, creates a deep copy of the nlp model
Then, processes 1000 randomly created words
At the end of the subprocess, the deep copy is deleted and garbage is collected
Finally, at the end of the file, the nlp model is manually deleted and garbage is collected.

The results show that, despite the efforts to clean up the environment, the memory still rises.
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Info about spaCy&lt;/denchmark-h&gt;


spaCy version: 2.0.16
Platform: Darwin-18.0.0-x86_64-i386-64bit

Issue has also occurred on Alpine 3.6


Python version: 3.6.5
Models: en_core_web_sm, en

&lt;denchmark-h:h2&gt;Code:&lt;/denchmark-h&gt;

from copy import deepcopy
import gc
import os
import psutil
import random
import spacy
import string

num_runs = 10
num_words = 1000


def randomword(length):
   letters = string.ascii_lowercase
   return ''.join(random.choice(letters) for i in range(length))

def get_memory(process):
    return f"{round(process.memory_info()[0] / float(2 ** 20), 6)} MB"

if __name__ == '__main__':
    process = psutil.Process(os.getpid())

    print("Loading model.")
    print(f"Pre-load memory: {get_memory(process)}")
    nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])
    print(f"Post-load memory: {get_memory(process)}")
    print("")

    for i in range(num_runs):
        print(f"Run {i}.")
        print(f"\tPre-copy memory: {get_memory(process)}")
        nlp_deep_copy = deepcopy(nlp)
        print(f"\tPost-copy memory: {get_memory(process)}")

        for j in range(num_words):
            word = randomword(j)
            nlp_deep_copy(word)
        print(f"\tPost-execution memory: {get_memory(process)}")
        del nlp_deep_copy
        gc.collect()
        print(f"\tPost-copy-delete memory: {get_memory(process)}")

    print("")
    print(f"Post-process memory: {get_memory(process)}")
    del nlp
    gc.collect()
    print(f"Post-delete memory: {get_memory(process)}")
&lt;denchmark-h:h2&gt;Results&lt;/denchmark-h&gt;


Loading model.
Pre-load memory: 49.285156 MB
Post-load memory: 157.777344 MB
Run 0.
Pre-copy memory: 157.777344 MB
Post-copy memory: 246.890625 MB
Post-execution memory: 248.726562 MB
Post-copy-delete memory: 208.40625 MB
Run 1.
Pre-copy memory: 208.40625 MB
Post-copy memory: 254.191406 MB
Post-execution memory: 254.535156 MB
Post-copy-delete memory: 211.253906 MB
Run 2.
Pre-copy memory: 211.253906 MB
Post-copy memory: 257.320312 MB
Post-execution memory: 257.628906 MB
Post-copy-delete memory: 213.441406 MB
Run 3.
Pre-copy memory: 213.441406 MB
Post-copy memory: 258.066406 MB
Post-execution memory: 258.070312 MB
Post-copy-delete memory: 213.417969 MB
Run 4.
Pre-copy memory: 213.417969 MB
Post-copy memory: 258.003906 MB
Post-execution memory: 258.011719 MB
Post-copy-delete memory: 215.285156 MB
Run 5.
Pre-copy memory: 215.285156 MB
Post-copy memory: 257.511719 MB
Post-execution memory: 257.523438 MB
Post-copy-delete memory: 213.511719 MB
Run 6.
Pre-copy memory: 213.511719 MB
Post-copy memory: 257.988281 MB
Post-execution memory: 257.988281 MB
Post-copy-delete memory: 213.296875 MB
Run 7.
Pre-copy memory: 213.296875 MB
Post-copy memory: 257.523438 MB
Post-execution memory: 257.523438 MB
Post-copy-delete memory: 213.511719 MB
Run 8.
Pre-copy memory: 213.511719 MB
Post-copy memory: 258.121094 MB
Post-execution memory: 258.121094 MB
Post-copy-delete memory: 213.679688 MB
Run 9.
Pre-copy memory: 213.679688 MB
Post-copy memory: 258.15625 MB
Post-execution memory: 258.15625 MB
Post-copy-delete memory: 213.679688 MB
Post-process memory: 213.679688 MB
Post-delete memory: 137.910156 MB

	</description>
	<comments>
		<comment id='1' author='emarcey' date='2018-12-06T14:20:10Z'>
		Thanks for the script. If you give this many many more runs, does it keep rising? The problem is Python is pretty notorious for fragmenting its memory and having memory slowly rise in its internal allocation pools, so it's really hard to tell from a small test whether there's an actual leak, or whether it's just Python being Python. If you run this for like 1000 runs, do you eventually end up with gigs of memory lost?
		</comment>
		<comment id='2' author='emarcey' date='2018-12-06T14:28:20Z'>
		We are running a job which has been parsing news articles constantly and
builds to an OOM error over a period of a couple hours. This is where we
first noticed the issue, but I can execute this script on a larger scale to
ensure the problem carries over.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Dec 6, 2018, 9:20 AM Matthew Honnibal ***@***.***&gt; wrote:
 Thanks for the script. If you give this many many more runs, does it keep
 rising? The problem is Python is pretty notorious for fragmenting its
 memory and having memory slowly rise in its internal allocation pools, so
 it's really hard to tell from a small test whether there's an actual leak,
 or whether it's just Python being Python. If you run this for like 1000
 runs, do you eventually end up with gigs of memory lost?

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 &lt;#3013 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AolKS-LI4ac5_1YxPsgbLaGLGY8Ju5Lgks5u2SetgaJpZM4ZFc9c&gt;
 .



		</comment>
		<comment id='3' author='emarcey' date='2018-12-07T15:40:38Z'>
		I ran three tests:

The same script with 1000 runs
1000 runs without manual garbage collection
1000 runs with a single instance of nlp (no deep copies)

To a degree, I think you're right: over 1000 runs the change in memory was less significant for tests 1 and 2.
&lt;denchmark-h:h2&gt;For 1:&lt;/denchmark-h&gt;


Pre-load memory: 49.382812 MB
Post-load memory: 157.59375 MB
Post-process memory: 182.167969 MB
Post-delete memory: 129.457031 MB

&lt;denchmark-h:h2&gt;For 2:&lt;/denchmark-h&gt;


Pre-load memory: 49.292969 MB
Post-load memory: 157.671875 MB
Post-process memory: 298.347656 MB

&lt;denchmark-h:h2&gt;For 3:&lt;/denchmark-h&gt;

However, I noticed that when I didn't use the deep copies, the original memory began to scale considerably:

Pre-load memory: 49.246094 MB
Post-load memory: 157.644531 MB
Post-process memory: 634.773438 MB

Over the course of the run, I increased about half a GB in memory. Is the model expected to grow in size as it parses text? I believe this is the source of the OOM error on our job.
		</comment>
		<comment id='4' author='emarcey' date='2018-12-07T19:30:21Z'>
		There are a few internal caches which are expected to grow, yes. In your tests you're parsing with entirely random words, which is extremely unlike linguistic workloads, where most tokens are of a limited number of types. If you parse actual text, I expect you'll see a fairly modest increase in memory use, that should level off once the vocab cache is full (from memory, at 100,000 entries).
		</comment>
		<comment id='5' author='emarcey' date='2018-12-07T21:33:22Z'>
		The job we are using is parsing complete news articles, however, and the cache continues to grow well past 100,000 articles, let alone words
		</comment>
		<comment id='6' author='emarcey' date='2018-12-07T23:41:12Z'>
		Well it does take more than 100,000 articles to see 100,000 unique word types.
		</comment>
		<comment id='7' author='emarcey' date='2018-12-13T15:16:08Z'>
		I see, is the vocab cache size configurable?
		</comment>
		<comment id='8' author='emarcey' date='2018-12-18T23:35:35Z'>
		It's currently hard-coded, unfortunately. That's definitely something we can change though.
I'll close this thread for now though, as I think it seems like this isn't actually a memory leak.
		</comment>
		<comment id='9' author='emarcey' date='2019-01-17T21:12:09Z'>
		&lt;denchmark-link:https://github.com/honnibal&gt;@honnibal&lt;/denchmark-link&gt;
 Is there a separate issue for configurable vocab cache size?
		</comment>
		<comment id='10' author='emarcey' date='2019-02-16T21:57:37Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>