<bug id='3870' author='chssch' open_date='2019-06-20T17:28:53Z' closed_time='2019-07-10T08:35:55Z'>
	<summary>Losses vary across runs despite fix_random_seed()</summary>
	<description>
&lt;denchmark-h:h2&gt;How to reproduce the behaviour&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import spacy

spacy.util.fix_random_seed()

nlp = spacy.blank("en")

ner = nlp.create_pipe("ner")
ner.add_label("TEST")
nlp.add_pipe(ner)

losses = {}
nlp.begin_training()

for i in range(10):
    
    nlp.update(
        ["test"],  # batch of texts
        [{"entities": [(0, 4, "TEST")]}],  # batch of annotations
        losses=losses,
    )
    print(losses)
&lt;/denchmark-code&gt;

I get different losses every time I run the code (only on first iteration it stays the same). I would expect that with the seed settings, the losses should stay stable?
Related
&lt;denchmark-link:https://github.com/explosion/spaCy/issues/3182#issue-401435816&gt;#3182 (comment)&lt;/denchmark-link&gt;

(It works in this example, because it is stable for the first iteration)
&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Info about spaCy&lt;/denchmark-h&gt;


spaCy version: 2.1.4
Platform: Linux-4.15.0-45-generic-x86_64-with-Ubuntu-16.04-xenial
Python version: 3.6.7

	</description>
	<comments>
		<comment id='1' author='chssch' date='2019-06-20T21:54:13Z'>
		Thanks, it does look like there's a bug here. I'm quite puzzled though -- I'm not sure what I could've broken.
		</comment>
		<comment id='2' author='chssch' date='2019-06-21T08:54:52Z'>
		One theory: array.max() and array.argmax() might not always be deterministic? If not, we'd get different gradient calculations.
		</comment>
		<comment id='3' author='chssch' date='2019-06-22T17:21:02Z'>
		I might have tracked it down:
The seed for the HashEmbed seems to be always different:
&lt;denchmark-link:https://github.com/explosion/thinc/blob/master/thinc/neural/_classes/hash_embed.py#L55&gt;https://github.com/explosion/thinc/blob/master/thinc/neural/_classes/hash_embed.py#L55&lt;/denchmark-link&gt;

The parameter is not recognized in  and just set to the object-id
&lt;denchmark-link:https://github.com/explosion/thinc/blob/master/thinc/neural/_classes/hash_embed.py#L37&gt;https://github.com/explosion/thinc/blob/master/thinc/neural/_classes/hash_embed.py#L37&lt;/denchmark-link&gt;

My current hotfix looks like this for :
&lt;denchmark-link:https://github.com/explosion/spaCy/blob/master/spacy/_ml.py#L322&gt;https://github.com/explosion/spaCy/blob/master/spacy/_ml.py#L322&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;        norm = HashEmbed(width, embed_size, column=cols.index(NORM), name="embed_norm")
        norm.seed = 23

        if subword_features:
            prefix = HashEmbed(
                width, embed_size // 2, column=cols.index(PREFIX), name="embed_prefix"
            )
            prefix.seed = 23
            suffix = HashEmbed(
                width, embed_size // 2, column=cols.index(SUFFIX), name="embed_suffix"
            )
            suffix.seed = 23
            shape = HashEmbed(
                width, embed_size // 2, column=cols.index(SHAPE), name="embed_shape"
            )
            shape.seed = 23
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='chssch' date='2019-07-13T12:16:46Z'>
		For documentation, if somebody else is running into the problem:  I was just wondering why sometimes the problem occurs and sometimes (e.g. if I use Prodigy) I get stable results. The answer is clear now. If you run the training in the same Python session (like a Notebook / IPython) the Model.id isn't reinitialized and you get different hash embeddings.
Workarounds: always run a fresh execution (e.g. restart script / kernel) or initialize the Model.id before running the training: thinc.neural._classes.model.Model.id = 0
To eventually fix this one solution could be to draw the hash seed in thinc from the random generator? I'm not 100% sure about the model serialization though, if it's implicit serializing the seed over Model.id at the moment.
		</comment>
		<comment id='5' author='chssch' date='2019-08-12T12:42:27Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>