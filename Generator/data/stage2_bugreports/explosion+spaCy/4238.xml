<bug id='4238' author='adrianeboyd' open_date='2019-09-04T18:26:27Z' closed_time='2019-09-08T18:52:47Z'>
	<summary>Tokenizer cache doesn't handle modifications to special cases or token_match correctly</summary>
	<description>
&lt;denchmark-h:h2&gt;How to reproduce the behaviour&lt;/denchmark-h&gt;

The github suggested related issues were actually helpful! &lt;denchmark-link:https://github.com/explosion/spaCy/issues/1061&gt;#1061&lt;/denchmark-link&gt;
 seems to have snuck back in. It works in 2.0.18, not in 2.1.0.
Modifications to special cases and token_match don't work if the pipeline has been run at least once due to the tokenizer cache.
&lt;denchmark-code&gt;import spacy
from spacy.symbols import ORTH

text = '(_SPECIAL_) A/B'

nlp = spacy.load('en_core_web_sm')
nlp.tokenizer.add_special_case('_SPECIAL_', [{ORTH: '_SPECIAL_'}])
nlp.tokenizer.add_special_case('A/B', [{ORTH: 'A/B'}])
print([token.text for token in nlp(text)])
# ['(', '_SPECIAL_', ')', 'A/B']

nlp = spacy.load('en_core_web_sm')
print([token.text for token in nlp(text)])
# ['(', '_', 'SPECIAL', '_', ')', 'A', '/', 'B']
nlp.tokenizer.add_special_case('_SPECIAL_', [{ORTH: '_SPECIAL_'}])
nlp.tokenizer.add_special_case('A/B', [{ORTH: 'A/B'}])
print([token.text for token in nlp(text)])
# ['(', '_', 'SPECIAL', '_', ')', 'A/B']

text = "This is a URL: http://example.com/file.html."

nlp = spacy.load('en_core_web_sm')
nlp.tokenizer.token_match = None
print([token.text for token in nlp(text)])
# ['This', 'is', 'a', 'URL', ':', 'http://example.com', '/', 'file.html', '.']

nlp = spacy.load('en_core_web_sm')
print([token.text for token in nlp(text)])
# ['This', 'is', 'a', 'URL', ':', 'http://example.com/file.html', '.']
nlp.tokenizer.token_match = None
print([token.text for token in nlp(text)])
# ['This', 'is', 'a', 'URL', ':', 'http://example.com/file.html', '.']
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Info about spaCy&lt;/denchmark-h&gt;


spaCy version: 2.1.8
Platform: Linux-4.19.0-5-amd64-x86_64-with-debian-10.0
Python version: 3.7.3

	</description>
	<comments>
		<comment id='1' author='adrianeboyd' date='2019-09-06T08:01:12Z'>
		This caching problem has been making me think I was losing my mind while testing special cases and token_match with the tokenizer.
Here's the commit that went missing in from v1-&gt;v2 that deals with the cache problem:
&lt;denchmark-link:https://github.com/explosion/spaCy/commit/4b2e5e59eda15c5f60710acbfb8624f748a169fc&gt;4b2e5e5&lt;/denchmark-link&gt;

I think that a solution like this could fix the problem, but I'm not sure it's 100% correct for v2.
When I test this with 2.0.18 it seems to work, but I'm not sure why given the minimal differences in the tokenizer between 2.0.18 and 2.1.8.
		</comment>
		<comment id='2' author='adrianeboyd' date='2019-09-08T11:01:51Z'>
		Wow, no idea how that patch went missing! Glad I wrote some notes on that...
So can we just take that commit?
		</comment>
		<comment id='3' author='adrianeboyd' date='2019-09-08T11:05:05Z'>
		No, it doesn't quite work, either. I have a new version coming...
		</comment>
		<comment id='4' author='adrianeboyd' date='2019-10-08T19:42:54Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>