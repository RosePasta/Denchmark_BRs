<bug id='2088' author='mralexpopa' open_date='2018-03-12T13:09:42Z' closed_time='2018-04-29T02:48:18Z'>
	<summary>Unexpected vocab.prune_vectors() behaviour</summary>
	<description>
I am trying to load the German &lt;denchmark-link:https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md&gt;fasttext&lt;/denchmark-link&gt;
 (wiki.de.vec) fasttext pretrained vectors, as suggested in the &lt;denchmark-link:https://spacy.io/usage/vectors-similarity#custom-loading-other&gt;example&lt;/denchmark-link&gt;
.
After loading is finished (~2hrs on my local machine), the number of word vectors is 2 747 988 (by running len(nlp.vocab.vectors).
I try to prune them with
removed_words = nlp.vocab.prune_vectors(n_vectors)
and something interesting happens. I have run this code with different values of n_vectors and I get very different results. Up to around n_vectors = 280k, it seems to behave correctly, but with values larger than  (3-4-500k) I always get an empty dictionary for after pruning and a model saved that only contains 59 keys (value in meta), which I believe is quite strange. The 59 keys value is reproducible and so far I have noticed it happens for  300k &lt; n_vectors &lt; 2 747 988. I am using nlp.to_disk() to save the model to disk.
Code:
&lt;denchmark-code&gt;def main(vectors_loc, lang=None):
    if lang is None:
        nlp = Language()
    else:
        # create empty language class â€“ this is required if you're planning to
        # save the model to disk and load it back later (models always need a
        # "lang" setting). Use 'xx' for blank multi-language class.
        nlp = spacy.blank(lang)
    with open(vectors_loc, 'rb') as file_:
        header = file_.readline()
        nr_row, nr_dim = header.split()
        nlp.vocab.reset_vectors(width=int(nr_dim))
        for line in file_:
            line = line.rstrip().decode('utf8')
            pieces = line.rsplit(' ', int(nr_dim))
            word = pieces[0]
            vector = numpy.asarray([float(v) for v in pieces[1:]], dtype='f')
            nlp.vocab.set_vector(word, vector)  # add the vectors to the vocab
    print(len(nlp.vocab.vectors))   # it will be 2 747 988 
    n_vectors = 400000
    removed_words = nlp.vocab.prune_vectors(n_vectors)
    nlp.to_disk('pruned_de_vectors_{}_vect'.format(n_vectors))
    print(removed_words)   # it will be {} 
    print(len(nlp.vocab.vectors))    #it will be 59 for values of n_vectors &gt;~ 300k
    # test the vectors and similarity

    text = 'class colspan'
    doc = nlp(text)
    print(text, doc[0].similarity(doc[1]))




python3 load_vectors_de.py wiki.de.vec 'de'
&lt;/denchmark-code&gt;

Any idea what might be going wrong?
&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


Operating System: Ubuntu 16.04, 64bit
Python Version Used: Python 3.5.2
spaCy Version Used: 2.0.9
Environment Information:

	</description>
	<comments>
		<comment id='1' author='mralexpopa' date='2018-04-09T15:23:43Z'>
		Update: the issue seems to be resolved by spaCy v2.0.11
Thanks!!
		</comment>
		<comment id='2' author='mralexpopa' date='2018-05-29T03:07:51Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>