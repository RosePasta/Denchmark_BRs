<bug id='4766' author='wavymazy' open_date='2019-12-04T22:55:26Z' closed_time='2020-01-16T09:30:31Z'>
	<summary>Procesing text by batch or by single string gives inconsistent doc.tensor values</summary>
	<description>
&lt;denchmark-h:h2&gt;How to reproduce the behaviour&lt;/denchmark-h&gt;

So I've been working with the en_trf_bertbaseuncased_lg model and ended up getting different classification results depending on how I processed text initially.
If I process text with the Language object using only 1 string
for ex: nlp(my_text) or nlp.pipe(my _text, batchsize=1)
I end up with different doc.tensor had I used :
nlp.pipe(my_texts, batchsize=500)
I compared the resulting doc, for each string, on a 1 by 1 basis, and the resulting vectors weren'T the same. I believe this to be the reason my classification results vary, and possibly why I get bad results when ingesting the same data by batch.
Thanks for reading and please let me know if I'm missing something, or if this is expected results. üëç
cheers!
&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


Operating System: ubuntu 18.04
Python Version Used: 3.6
spaCy Version Used: 2.2.1
Environment Information: I'm working with RASA.

	</description>
	<comments>
		<comment id='1' author='wavymazy' date='2019-12-05T11:14:43Z'>
		Hi, thanks for the report! I could imagine that there could be an unintended interaction related to batching but I can't reproduce this, with either transformer or non-transformer models. Could you provide a minimal example that shows the problem?
		</comment>
		<comment id='2' author='wavymazy' date='2019-12-05T20:54:48Z'>
		Hey &lt;denchmark-link:https://github.com/adrianeboyd&gt;@adrianeboyd&lt;/denchmark-link&gt;
,
Thanks you for your reply.
Here is a scipt I run on my machine with 2 sentences that, on my machine, show me the tensors are different.
&lt;denchmark-code&gt;import spacy
import numpy as np

nlp = spacy.load('en_trf_bertbaseuncased_lg')

messages = ["Hello my name is daniel", "I would like to participate in interesting events"]

docs_by_batch = nlp.pipe(messages, batch_size=2)
docs_standard = [nlp(message) for message in messages]

for doc_by_batch, doc_standard in zip(docs_by_batch, docs_standard):
    if np.array_equal(doc_by_batch.tensor, doc_standard.tensor):
        print("Identical Vectors")
    else:
        print("Different Vectors")
&lt;/denchmark-code&gt;

Let me know how it goes when you test it.
Cheers !
		</comment>
		<comment id='3' author='wavymazy' date='2019-12-06T08:07:42Z'>
		Huh, the behavior is stranger than I expected and when I tested things I just lucked into test cases that don't show this problem. Some more test cases:
&lt;denchmark-code&gt;import spacy
import numpy as np

nlp = spacy.load('en_trf_bertbaseuncased_lg')

messages = [["Hello my name is daniel", "I would like to participate in interesting events"], ["I would like to participate in interesting events", "Hello my name is daniel", "one"], ["one", "two"], ["a sentence", "another sentence"], ["a sentence", "Hello my name is daniel"], ["Hello my name is daniel", "a sentence", "one", "two"]]

for i, texts in enumerate(messages):
    print("Round", i + 1)
    print("=======")
    docs_by_batch = nlp.pipe(texts, batch_size=2)
    docs_standard = [nlp(text) for text in texts]

    for doc_by_batch, doc_standard in zip(docs_by_batch, docs_standard):
        if np.array_equal(doc_by_batch.tensor, doc_standard.tensor):
            print("Identical Vectors:", doc_by_batch.text)
        else:
            print("Different Vector:", doc_by_batch.text)
    print("\n")
&lt;/denchmark-code&gt;

Output:
&lt;denchmark-code&gt;Round 1
=======
Different Vector: Hello my name is daniel
Identical Vectors: I would like to participate in interesting events


Round 2
=======
Identical Vectors: I would like to participate in interesting events
Different Vector: Hello my name is daniel
Identical Vectors: one


Round 3
=======
Different Vector: one
Different Vector: two


Round 4
=======
Identical Vectors: a sentence
Identical Vectors: another sentence


Round 5
=======
Different Vector: a sentence
Identical Vectors: Hello my name is daniel


Round 6
=======
Identical Vectors: Hello my name is daniel
Different Vector: a sentence
Different Vector: one
Different Vector: two
&lt;/denchmark-code&gt;

We'll look into it!
		</comment>
		<comment id='4' author='wavymazy' date='2019-12-18T09:13:31Z'>
		It looks like this happens for all models with vectors and all models on GPU, so the only model without this result is a small model on CPU.
Update: it's all spacy models on GPU, all transformers models with or without GPU.
		</comment>
		<comment id='5' author='wavymazy' date='2020-01-16T09:30:31Z'>
		I looked into this again and it really just seems to be due to float rounding differences. Checking whether the tensors are close rather than identical shows that the results are very similar. (Whew!)
		</comment>
		<comment id='6' author='wavymazy' date='2020-01-23T20:50:58Z'>
		Hi &lt;denchmark-link:https://github.com/adrianeboyd&gt;@adrianeboyd&lt;/denchmark-link&gt;
,
So I tested it out and tried with this script that rounds the vectors to 2 decimals:
&lt;denchmark-code&gt;import spacy
import numpy as np

nlp = spacy.load('en_trf_bertbaseuncased_lg')

messages = ["Hello my name is daniel", "I would like to participate in interesting events",
            "something is very strange with SpaCy", "The vectors aren't similar",
            "let us try to round both vectors to 5 decimals"]

docs_by_batch = nlp.pipe(messages, batch_size=2)
docs_standard = [nlp(message) for message in messages]

for doc_by_batch, doc_standard in zip(docs_by_batch, docs_standard):
    if np.array_equal(np.around(doc_by_batch.tensor, 2), np.around(doc_standard.tensor, 2)):
        print("Identical Vectors")
    else:
        print("Different Vectors")
&lt;/denchmark-code&gt;

But unfortunately, results were very inconsistent. The second, third and fifth sentences in the list had identical vectors, but the first and fourth were different.
		</comment>
		<comment id='7' author='wavymazy' date='2020-02-04T11:13:49Z'>
		Hmm, you're right, sorry for not getting back to this sooner! I think I was focused on CPU vs. GPU for spacy and didn't check the transformers model carefully.

Update: it's all spacy models on GPU, all transformers models with or without GPU.

This seems to still basically be true:

spacy models show some small float rounding differences on GPU
transformers models show major differences both on CPU and GPU

Spacy seems okay, so I'll open this as a new spacy-transformers issue.
		</comment>
		<comment id='8' author='wavymazy' date='2020-03-10T19:05:05Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>