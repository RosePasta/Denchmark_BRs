<bug id='1601' author='kristianwoodsend' open_date='2017-11-17T15:18:57Z' closed_time='2018-05-21T00:09:36Z'>
	<summary>noun_chunks iterator with deserialised document throws error</summary>
	<description>
With a document that has been serialised and then de-serialised into a new Doc with empty Vocab, the noun_chunks iterator does not work. It does work if the new doc is provided with the vocab of the original document though.
In this example:
import spacy
from spacy.tokens import Doc
from spacy.vocab import Vocab

text = "Mary had a little lamb."
nlp = spacy.load('en')
doc_orig = nlp(text)
doc_copy = Doc(Vocab()).from_bytes(doc_orig.to_bytes())

assert doc_orig.text == doc_copy.text  # Text is OK
list(doc_orig.noun_chunks)  # This is OK
list(doc_copy.noun_chunks)  # Gives TypeError
the final line gives the error
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "doc.pyx", line 489, in __get__
TypeError: 'NoneType' object is not callable
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


spaCy version: 2.0.2
Platform: Linux-3.10.0-693.5.2.el7.x86_64-x86_64-with-centos-7.4.1708-Core
Python version: 3.6.3
Models: en, en_core_web_lg

	</description>
	<comments>
		<comment id='1' author='kristianwoodsend' date='2017-11-17T16:12:16Z'>
		I get a similar problem with entities, where accessing them in the deserialised document causes an error -- an AssertionError this time. In this example:
import spacy
from spacy.tokens import Doc
from spacy.vocab import Vocab

text = "Her Ice Palace stands here."

nlp = spacy.load('en')
doc_orig = nlp(text)
doc_copy = Doc(Vocab()).from_bytes(doc_orig.to_bytes())

doc_orig.ents
doc_copy.ents
the final line causes
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "doc.pyx", line 415, in spacy.tokens.doc.Doc.ents.__get__
  File "span.pyx", line 61, in spacy.tokens.span.Span.__cinit__
AssertionError: 9191306739292312949
&lt;/denchmark-code&gt;

Is it a related problem? Or should I raise a separate issue? Same environment as before, BTW.
		</comment>
		<comment id='2' author='kristianwoodsend' date='2017-11-23T12:16:56Z'>
		I dug a little deeper into this and it looks like the two problems are slightly different.
For the noun chunker, the problem is that doc_copy doesn't have a lang language attribute set up, and lang is needed to get a sensible iterator from _get_chunker. So a work-around is to make doc_copy with a vocab object of the right language, like spacy.lang.en.English().vocab or the original model vocab. I don't know how you want to fix the problem properly -- would including the language in the serialization help or would it be too late once the new doc has been created?
The problem with entities is that the entity labels need to be in the string store. Currently serialization includes the text but not the labels, and reading back into an empty Vocab object means the labels are missing.
So the work around I have for both problems is to deserialise into a doc created with the vocab object of the original model, nlp.vocab in the example above. That way I get the language and entity labels already set up and I can still pass a document between machines. Is this good enough for all situations? Or do you think it is important to be able to deserialize into an empty vocab?
		</comment>
		<comment id='3' author='kristianwoodsend' date='2018-03-02T21:34:54Z'>
		I'm putting this comment here because a specific and very long number has come up in a different place.
I created some docs and then  wanted to output all the "token.ent_type" values it came across.
Many of them seemed to be logical numerical choices (storing them as ints for efficiency's sake of course), such as:
CARDINAL 394
GPE 382
PERSON 378
But the numerical code (ent_type) for ent_type_ "FAC" is:  9191306739292312949
Doesn't that strike you as a little high?
Then you throw in the fact that kristianwoodsend saw this same number in when copying a doc (see above) ...
		</comment>
		<comment id='4' author='kristianwoodsend' date='2018-05-21T00:09:35Z'>
		&lt;denchmark-link:https://github.com/kristianwoodsend&gt;@kristianwoodsend&lt;/denchmark-link&gt;
 The main issues you've reported should now be fixed  I also just tried your example with the latest version on  and it works fine for me â€“ if the new  is created with the original vocab:
doc_copy = Doc(doc_orig.vocab).from_bytes(doc_orig.to_bytes())
Even though the hash IDs are now consistent, you still always need the vocab to be able to resolve the IDs back to their strings (among other things).
		</comment>
		<comment id='5' author='kristianwoodsend' date='2018-06-20T00:45:59Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>