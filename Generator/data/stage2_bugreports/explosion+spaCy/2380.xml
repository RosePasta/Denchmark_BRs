<bug id='2380' author='HanVincent' open_date='2018-05-27T12:58:13Z' closed_time='2018-12-16T16:45:18Z'>
	<summary>Unpickle the parse result without pos / tag</summary>
	<description>
Hi, I want to preprocess all the sentences first, so I save the parsed result in pickle file.
But after I unpickle it, I can't get token.pos_ or token.tag_
Is there anything I ignore? Many thanks! : )
Save:
nlp = spacy.load('en_core_web_lg') # ('en')

fs = open('test.txt', 'r', encoding='utf8')
docs = nlp.pipe([line.strip().lower() for line in fs]) # minibatching - faster

doc_bytes = [doc.to_bytes(user_data=True) for doc in docs]
vocab_bytes = nlp.vocab.to_bytes()

with open("processed.pickle","wb") as handle:
    pickle.dump((doc_bytes, vocab_bytes), handle)
Restore:
with open("processed.pickle", "rb") as handle:
    doc_bytes, vocab_bytes = pickle.load(handle)
    
nlp.vocab.from_bytes(vocab_bytes)
docs = [Doc(nlp.vocab).from_bytes(b) for b in doc_bytes]
Result:
tk = docs[1][6]
print(tk) # 'is'
print(tk.text)  # 'is', works well
print(tk.tag_) # get empty ''
print(tk.pos_) # get empty ''
&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


Python version: 3.5.2
spaCy version: 2.0.11
Platform: Linux-4.14.12-041412-generic-x86_64-with-Ubuntu-16.04-xenial

	</description>
	<comments>
		<comment id='1' author='HanVincent' date='2018-06-04T22:41:04Z'>
		It really looks like you're doing everything correctly here, so I'd say this is likely to be a bug in the way the Morphology class is serialized and deserialized. This component (which lives at nlp.vocab.morphology) controls the assignment of the tag attributes to the tokens, and sets the implied coarse-grained POS as well.
The weird thing is, if this is broken how isn't the deserialization of tags broken much more generally?! We do have tests for this, so I'm not sure what's going on. If you compare the .tag value, rather than the string it maps to, is that 0 as well, or does it get set? You can also use msgpack.loads() to deserialize the bytes representation, so you can poke around and see what the data's like.
		</comment>
		<comment id='2' author='HanVincent' date='2018-06-05T01:54:01Z'>
		Yeah, I also tried .tag/.pos to see its value, it still return 0.
print(tk) # 'is'
print(tk.text)  # 'is', works well
print(tk.tag) # get 0
print(tk.pos) # get 0
For msgpack.dump / load, it gets the same result, too.
		</comment>
		<comment id='3' author='HanVincent' date='2018-06-05T02:21:30Z'>
		I took a little time to test it, and I found that after replacing
docs = nlp.pipe([line.strip().lower() for line in fs]) # minibatching - faster
with
docs = [nlp(line.strip().lower()) for line in fs] 
I can get tag/pos now. But I am still not sure the reason.
		</comment>
		<comment id='4' author='HanVincent' date='2018-08-22T02:03:20Z'>
		I use msgpack to dump the docs. When reading back the object I get is the same behavior for the POS attribute (all zeros/nulls, even though they are there correctly before serialization), but the tags are actually made it through. Only difference is I used the en_core_web_sm instead of lg and I used msgpack instead of pickle. I do something like this:
docs = [(doc_id, nlp(text)) for doc_id, text in texts]
vocab = docs[0].vocab
d = {doc_id: parsed.to_bytes(tensor=False) for doc_id, parsed in docs}
d['vocab'] = vocab.to_bytes()
with open(filename, 'wb') as f:
    f.write(msgpack.dumps(d))
Then later I read it back like so:
m = msgpack.load(open(filename, 'rb'))
v = Vocab().from_bytes(m['vocab'])
d = Doc(v).from_bytes(m[some_doc_id])
any([t.pos_ for t in d]), any([t.tag_ for t in d])
First thing is False, second thing is True.
Using spacy version 2.0.12
		</comment>
		<comment id='5' author='HanVincent' date='2018-08-22T03:06:35Z'>
		OK I think the problem is actually pretty clear. Somewhere in the  method there needs to be an additional item for the POS identifier: &lt;denchmark-link:https://github.com/explosion/spacy/blob/master/spacy/tokens/doc.pyx#L778&gt;https://github.com/explosion/spacy/blob/master/spacy/tokens/doc.pyx#L778&lt;/denchmark-link&gt;

Indeed, adding the  variable to the end of the  list solves the issue (after recompiling spacy from source).
That said I would be interested in a work-around if anyone can think of one. The issue is in a pyrex file so monkey patching doesn't work (I think?).
		</comment>
		<comment id='6' author='HanVincent' date='2018-08-28T20:56:34Z'>
		&lt;denchmark-link:https://github.com/ines&gt;@ines&lt;/denchmark-link&gt;
 Hoping this ping reaches you. This one's a quick fix, hoping I don't have to run my own fork here as it's... not ideal.
		</comment>
		<comment id='7' author='HanVincent' date='2018-12-16T16:45:14Z'>
		&lt;denchmark-link:https://github.com/HanVincent&gt;@HanVincent&lt;/denchmark-link&gt;
 Just failed to reproduce this now, and realised that the issue was resolved in a prior patch. The problem was that the  attribute was being set at the incorrect level of indentation during the  method. This caused the attribute to only be set correctly during the  method, not , because in  we only have one doc per batch.
&lt;denchmark-link:https://github.com/hadsed&gt;@hadsed&lt;/denchmark-link&gt;
 The  attribute should only be set when the  attribute is set, so we shouldn't need to serialize them separately. So long as the  attribute is set correctly, I think there shouldn't be a problem. Let me know if troubles persist. And of course, you can always submit a PR instead of running your own fork :).
		</comment>
		<comment id='8' author='HanVincent' date='2019-01-01T23:54:26Z'>
		&lt;denchmark-link:https://github.com/honnibal&gt;@honnibal&lt;/denchmark-link&gt;
 Happy to do a PR, just wanted to make sure this is the right thing to fix.
You can run the following code exactly to see the behavior (version 2.0.18). The POS tags just don't seem to be coming through when deserialized.
import spacy
from spacy.vocab import Vocab
from spacy.tokens import Doc
import msgpack

nlp = spacy.load('en_core_web_sm')

# create texts and parse them
texts = [(101, 'mary had a lamb.'), (202, 'so did fred.')]
docs = [(doc_id, nlp(text)) for doc_id, text in texts]
vocab = docs[0][1].vocab

# serialize and check attributes
serialized_docs = {doc_id: parsed.to_bytes(tensor=True) for doc_id, parsed in docs}
serialized_docs['vocab'] = vocab.to_bytes()
print('Before serializing:', [k.pos_ for k in docs[0][1]], [k.tag_ for k in docs[0][1]], ', is tagged:', docs[0][1].is_tagged)

# save to file
filename = '/tmp/doc_dump.msgpk'
with open(filename, 'wb') as f:
    f.write(msgpack.dumps(serialized_docs))

# load it back and check same attributes as before
m = msgpack.load(open(filename, 'rb'))
v = Vocab().from_bytes(m[b'vocab'])
d = Doc(v).from_bytes(m[101])
print('When read back in:', [k.pos_ for k in d], [k.tag_ for k in d], ', is tagged:', d.is_tagged)
And the above code outputs the following:
&lt;denchmark-code&gt;Before serializing: ['NOUN', 'VERB', 'DET', 'NOUN', 'PUNCT'] ['NN', 'VBD', 'DT', 'NN', '.'] , is tagged: True
When read back in: ['', '', '', '', ''] ['NN', 'VBD', 'DT', 'NN', '.'] , is tagged: True
&lt;/denchmark-code&gt;

So to round this out, I'm a little confused about when POS tags should be available. Outline of how I'm thinking it's meant to work:

Parse the string using some model (nlp('the doc'))
Check the POS tags (they are there as long as the model was initialized with the entire pipeline)
Serialize them with .to_bytes()
Serialize the vocab as well in the same way
Write those all to file
Read them back
POS tags should still be there

		</comment>
		<comment id='9' author='HanVincent' date='2019-02-01T07:27:36Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>