<bug id='1806' author='sz640' open_date='2018-01-06T00:52:10Z' closed_time='2018-12-08T11:34:21Z'>
	<summary>Error with resuming training from saved model on GPU</summary>
	<description>
Hi
I finally got spaCy ner training working on AWS gpu instances. I have no problem training on GPU and saving the model to disk. But when I load the saved model and continue training on more data, it throws errors:
&lt;denchmark-code&gt;Loaded model './models/animal_ner'
Traceback (most recent call last):
  File "train_new_entity_type.py", line 128, in &lt;module&gt;
    plac.call(main)
  File "/usr/local/lib/python3.5/dist-packages/plac_core.py", line 328, in call
    cmd, result = parser.consume(arglist)
  File "/usr/local/lib/python3.5/dist-packages/plac_core.py", line 207, in consume
    return cmd, self.func(*(args + varargs + extraopts), **kwargs)
  File "train_new_entity_type.py", line 100, in main
    losses=losses)
  File "/usr/local/lib/python3.5/dist-packages/spacy/language.py", line 407, in update
    proc.update(docs, golds, drop=drop, sgd=get_grads, losses=losses)
  File "nn_parser.pyx", line 589, in spacy.syntax.nn_parser.Parser.update
  File "nn_parser.pyx", line 688, in spacy.syntax.nn_parser.Parser._make_updates
  File "/usr/local/lib/python3.5/dist-packages/thinc/api.py", line 67, in continue_update
    gradient = callback(gradient, sgd)
  File "/usr/local/lib/python3.5/dist-packages/spacy/_ml.py", line 536, in finish_update
    return ops.unflatten(d_X, lengths, pad=0)
  File "ops.pyx", line 136, in thinc.neural.ops.Ops.unflatten
TypeError: slice indices must be integers or None or have an __index__ method
&lt;/denchmark-code&gt;

You can reproduce this error with the &lt;denchmark-link:https://github.com/explosion/spacy/blob/master/examples/training/train_new_entity_type.py&gt;train_new_entity_type.py&lt;/denchmark-link&gt;
 example script by adding in  then running:
python3 train_new_entity_type.py -n 10 -o ./models/animal_ner
followed by
python3 train_new_entity_type.py -n 10 -m ./models/animal_ner
I've actually tried simply loading the model and doing inference and it works fine. And all above steps work on CPU with no errors.
Any thoughts?
Thanks!
&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


Operating System: Ubuntu 16.04 on AWS Deep Learning AMI
Python Version Used: Python 3.5.2
spaCy Version Used: 2.0.2
Environment Information: AWS P2.xlarge instance, cuda 8.0

	</description>
	<comments>
		<comment id='1' author='sz640' date='2018-01-11T17:52:19Z'>
		Having same error when I run it on Google GPU machine with my code.
I am running with the following environment:
cupy==2.2.0
spacy==2.0.5
thinc==6.10.2
CUDA 9
OS - Ubuntu 17.04
		</comment>
		<comment id='2' author='sz640' date='2018-03-28T15:08:53Z'>
		Any updates on this? Thanks!
Bug still present in spacy 2.0.10.
Ubuntu 17.10
Python 3.6.3
nvidia-390 driver
nvidia-cuda-toolkit 8.0
cupy 2.5.0
thinc 6.10.2
spacy 2.0.10
		</comment>
		<comment id='3' author='sz640' date='2018-03-28T17:47:36Z'>
		&lt;denchmark-link:https://github.com/avadhpatel&gt;@avadhpatel&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/sz640&gt;@sz640&lt;/denchmark-link&gt;

The problem is actually with thinc. Here is a quick fix that worked for me:

Uninstall thinc ($ sudo pip3 uninstall thinc)
Download thinc locally ($ wget https://github.com/explosion/thinc/archive/v6.10.2.tar.gz ... or any other version compatible with spacy).
untar it ($ tar -xvf thinc-"version")
open file "thinc untar-ed folder"/thinc/neural/ops.pyx
change lines 139, 140 and 141, type casting pad and length to integer (i.e., int(pad) and int(length))
re-tar it ($ tar -czvf thinc.tar.gz thinc-"your_version"/)
re-pip it using your tar-ed version ($ sudo pip3 install thinc.tar.gz)
(it's probably not necessary to re-install spacy but I did it anyway.)

I hope that helps you guys as well.
L.
		</comment>
		<comment id='4' author='sz640' date='2018-03-29T09:18:56Z'>
		&lt;denchmark-link:https://github.com/leloss&gt;@leloss&lt;/denchmark-link&gt;
 Thanks for the fix! Will push that to Thinc.
I'm very confued by your process though. You've edited the .pyx file and not recompiled it with Cython, so the .cpp file within the archive will be the same. How can this work? You should end up with the same shared object file before and after your changes.
		</comment>
		<comment id='5' author='sz640' date='2018-03-30T16:26:57Z'>
		&lt;denchmark-link:https://github.com/honnibal&gt;@honnibal&lt;/denchmark-link&gt;

I just checked and my ops.cpp does look different and shows the changes I made in ops.pyx. I'm not an expert but I believe pip executes the package's setup.py file and compiles them accordingly.
		</comment>
		<comment id='6' author='sz640' date='2018-12-08T11:34:21Z'>
		This should be fixed in recent versions (e.g. v2.0.18).
		</comment>
		<comment id='7' author='sz640' date='2019-01-07T12:28:39Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>