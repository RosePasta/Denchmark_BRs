<bug id='448' author='michaelcapizzi' open_date='2016-07-06T19:05:14Z' closed_time='2016-10-19T00:31:20Z'>
	<summary>loading custom word vectors</summary>
	<description>
The tutorial &lt;denchmark-link:https://spacy.io/docs/tutorials/load-new-word-vectors&gt;here&lt;/denchmark-link&gt;
 indicates that you would load the  using .  When I do that, however, I have the same problem that &lt;denchmark-link:https://github.com/xumx&gt;@xumx&lt;/denchmark-link&gt;
 had &lt;denchmark-link:https://github.com/spacy-io/spaCy/issues/95&gt;here&lt;/denchmark-link&gt;
 (showing  dimensions).
So I looked at the &lt;denchmark-link:https://github.com/spacy-io/spaCy/blob/master/spacy/vocab.pyx&gt;source code&lt;/denchmark-link&gt;
, and it looks like there are   methods, one of which is  which takes a  file.
And having written a  file as &lt;denchmark-link:https://github.com/henningpeters&gt;@henningpeters&lt;/denchmark-link&gt;
 described &lt;denchmark-link:https://github.com/spacy-io/spaCy/issues/95&gt;here&lt;/denchmark-link&gt;
, this worked successfully....with one problem:
&lt;denchmark-code&gt;&gt;&gt;&gt;giga = nlp.vocab.load_vectors_from_bin_loc("path/to/file.bin")
&gt;&gt;&gt;giga
117
&lt;/denchmark-code&gt;

The vectors return a size of 117.  They are 200-dimensions, and yet when I test the shape of a vector, they are of size 300.
&lt;denchmark-code&gt;&gt;&gt;&gt;t = Token()
&gt;&gt;&gt;t.vector.shape
(300,)
&lt;/denchmark-code&gt;

I made sure to test that the original (Goldberg) vectors had been overwritten, but I'm still not sure how my 200-dimension vectors became 300.
&lt;denchmark-code&gt;&gt;&gt;&gt; gold = s.word.vector         #default vector for a given Token
&gt;&gt;&gt; gold.shape
(300,)
&gt;&gt;&gt; giga_loaded = nlp.vocab.load_vectors_from_bin_loc("/path/to/my/vectors.bin")
&gt;&gt;&gt; giga_loaded
117
&gt;&gt;&gt; giga = s.word.vector
&gt;&gt;&gt; giga.shape
(300,)
&gt;&gt;&gt; giga == gold
array([False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False], dtype=bool)
&lt;/denchmark-code&gt;

Any ideas?
	</description>
	<comments>
		<comment id='1' author='michaelcapizzi' date='2018-05-09T10:12:31Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>