<bug id='2662' author='dkarmon' open_date='2018-08-13T10:46:05Z' closed_time='2018-11-03T17:33:21Z'>
	<summary>NER training fails: Process finished with exit code -1073741819 (0xC0000005)</summary>
	<description>
I tried to train a new NER model based on the sample code in your site with my data (less than 4K records).
After only a few training iterations the training process suddenly fails with the following error message:

Process finished with exit code -1073741819 (0xC0000005)

I suspected that it had something to do with the batch, size but even if I try to train it example by example the training process fails.
I could not reproduce the error during debug - so maybe it's a memory leak issue? (allocation\releasing)
Please advise
&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


Operating System: Win10
Python Version Used:  3.6.4
spaCy Version Used: 2.0.12

	</description>
	<comments>
		<comment id='1' author='dkarmon' date='2018-09-05T12:45:39Z'>
		I've managed to reproduce the error in debug mode. It seems that the batch size is indeed the root cause as if the batch contains more than 8 items the training process would eventually fail with that error code. However, if I switch to small batches, the training process would be completed.
&lt;denchmark-link:https://github.com/honnibal&gt;@honnibal&lt;/denchmark-link&gt;
 \ &lt;denchmark-link:https://github.com/ines&gt;@ines&lt;/denchmark-link&gt;
  please advise
		</comment>
		<comment id='2' author='dkarmon' date='2018-09-10T15:04:24Z'>
		I'm stuck at the exact problem for four days now. Getting that same error  especially when I use training dataset of about 200 or more.  I've no idea what the cause is. I'm trying to add a new entity type to a Spanish model But I'm stuck. Can someone please, please help. Thank you.
Environment:
&lt;denchmark-code&gt;spaCy version      2.0.12
Platform           Windows-10-10.0.17134-SP0
Python version     3.6.5
Models             en, es
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='dkarmon' date='2018-09-10T15:22:12Z'>
		&lt;denchmark-link:https://github.com/Nandee89&gt;@Nandee89&lt;/denchmark-link&gt;
 The most likely cause is a batch that has an unusually large number of total words. How many words are in each of your documents, and how big is your batch size?
		</comment>
		<comment id='4' author='dkarmon' date='2018-09-10T15:42:07Z'>
		&lt;denchmark-link:https://github.com/honnibal&gt;@honnibal&lt;/denchmark-link&gt;
 Thank you for responding. I'm training it on a batch size of 799 examples which has a total of 26506 words. Documents/texts are of different sizes.
		</comment>
		<comment id='5' author='dkarmon' date='2018-09-10T15:47:46Z'>
		I also suspected that the dataset is too big..But I thought the larger the data the better model I would get. So I train a model on a smaller data and then try to retrain that model with another 150 batch hoping that the weights get updated. That approach lowered the quality of the model.
		</comment>
		<comment id='6' author='dkarmon' date='2018-09-10T17:13:36Z'>
		&lt;denchmark-link:https://github.com/explosion/spacy/blob/master/examples/training/train_new_entity_type.py&gt;https://github.com/explosion/spacy/blob/master/examples/training/train_new_entity_type.py&lt;/denchmark-link&gt;

I'm trying to train it using the script found at the above link but with a Spanish model
		</comment>
		<comment id='7' author='dkarmon' date='2018-09-10T22:57:17Z'>
		It seems that the problem is with using a pre-existing model. I trained a blank Language class with the 799 training examples and I didn't crash at all. I trained two times and worked well in both.
if model is not None:
nlp = spacy.load(model)  # load existing spaCy model
print("Loaded model '%s'" % model)
else:
nlp = spacy.blank('es')  # create blank Language class
print("Created blank 'es' model")
		</comment>
		<comment id='8' author='dkarmon' date='2018-09-13T04:55:53Z'>
		&lt;denchmark-link:https://github.com/honnibal&gt;@honnibal&lt;/denchmark-link&gt;
 I used a batch size of 64 where each instance consists of approximately 15 tokens.
		</comment>
		<comment id='9' author='dkarmon' date='2018-11-03T17:33:21Z'>
		This problem should now be resolved --- v2.0.17 has a fix for a memory error I think was at fault. You can already try it with pip install spacy==2.0.17.dev1 if you see this before 2.0.17 is out.
		</comment>
		<comment id='10' author='dkarmon' date='2018-12-03T18:47:02Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>