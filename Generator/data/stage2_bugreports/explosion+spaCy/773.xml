<bug id='773' author='ejschoen' open_date='2017-01-24T21:27:58Z' closed_time='2017-03-31T10:10:25Z'>
	<summary>SpaCy NER training example from version 1.5.0 doesn't work in 1.6.0</summary>
	<description>
I tried to use the training example here:
&lt;denchmark-link:https://github.com/explosion/spaCy/blob/master/examples/training/train_ner.py&gt;https://github.com/explosion/spaCy/blob/master/examples/training/train_ner.py&lt;/denchmark-link&gt;

with SpaCy 1.6.0.  I get results like this:
&lt;denchmark-code&gt;Who is Shaka Khan?
Who 1228 554 WP  2
is 474 474 VBZ PERSON 3
Shaka 57550 129921 NNP PERSON 1
Khan 12535 48600 NNP LOC 3
? 482 482 . LOC 3

I like London and Berlin
I 467 570 PRP LOC 3
like 502 502 VBP LOC 1
London 4003 24340 NNP LOC 3
and 470 470 CC PERSON 3
Berlin 11964 60816 NNP PERSON 1
&lt;/denchmark-code&gt;

The tagging is odd, and from Khan is recognized as a LOC and Berlin as a PERSON.  If I back up to version 1.5.0, the result is as expected:
&lt;denchmark-code&gt;Who is Shaka Khan?
Who 1228 554 WP  2
is 474 474 VBZ  2
Shaka 57550 129921 NNP PERSON 3
Khan 12535 48600 NNP PERSON 1
? 482 482 .  2

I like London and Berlin
I 467 570 PRP  2
like 502 502 VBP  2
London 4003 24340 NNP LOC 3
and 470 470 CC  2
Berlin 11964 60816 NNP LOC 3
&lt;/denchmark-code&gt;

Could this be an issue with the off the shelf English model that spacy.en.download 1.6.0 fetched?
	</description>
	<comments>
		<comment id='1' author='ejschoen' date='2017-01-27T11:13:45Z'>
		&lt;denchmark-h:h2&gt;TL;DR&lt;/denchmark-h&gt;

I made a bug fix to thinc for 1.6 that's messed up the example, as it's written.
The best fix is to not call .end_training() after updating the model. I'm working on making this less confusing.
&lt;denchmark-h:h2&gt;What's going on&lt;/denchmark-h&gt;

spaCy 1.x uses the Averaged Perceptron algorithm for all its machine learning. You can read about the algorithm in the POS tagger blog post, where you can also find a straight-forward Python implementation: &lt;denchmark-link:https://explosion.ai/blog/part-of-speech-pos-tagger-in-python&gt;https://explosion.ai/blog/part-of-speech-pos-tagger-in-python&lt;/denchmark-link&gt;

AP uses the Averaged Parameter Trick for SGD. There are two copies of the weights:

The current weights,
The averaged weights

During training predictions are made with the current weights, and the averaged weights are updated in the background. At the end of training, we swap the current for the averages. This makes a huge difference for most training scenarios.
However, when I wrote the code, I didn't pay much attention to the current use-case of "resuming" training, in order to add another class. I recently fixed a long-standing error in the averaged perceptron code:
After loading a model, Thinc was not initialising the averages to the newly loaded weights. This saves memory, because the averages require another copy of the weights, and also additional book-keeping. The consequence of this bug was that when you updated a feature after resuming training, you wiped the weights that were previously associated with it. This is really bad --- it means that as you train new examples, you're deleting all the information previously associated with it.
I finally fixed this bug in this commit: &lt;denchmark-link:https://github.com/explosion/thinc/commit/09b030b4aa0e58fd3eef0eda5340795fd079b248&gt;explosion/thinc@09b030b&lt;/denchmark-link&gt;

The consequence of this is that the correction makes the model behave differently on these small-data example cases.
What's still unclear is, how should we compute an average between the old weights and the new ones? The old weights were trained on about 20 passes over about 80,000 sentences of annotation. So the new 5 passes over 5 examples shouldn't change the weights at all if we take an unbiased average. This seems undesirable.
If you have so little data, it's probably not a good idea to average.
&lt;denchmark-h:h2&gt;About NER and training more generally (making this the megathread)&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/explosion/spaCy/issues/762&gt;#762&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/explosion/spaCy/issues/612&gt;#612&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/explosion/spaCy/issues/701&gt;#701&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/explosion/spaCy/issues/665&gt;#665&lt;/denchmark-link&gt;
 . Attn: @savvopoulos, &lt;denchmark-link:https://github.com/viksit&gt;@viksit&lt;/denchmark-link&gt;

People are having a lot of pain with training the NER system. Some of the problems are easy to fix --- the current workflow around saving and loading data is pretty bad, and it's made worse by some Python 2/3 unicode save/load bugs in the example scripts.
What's hard to solve is that people seem to want to train the NER system on like, 5 examples. The current algorithm expects more like 5,000. I realise I never wrote this anywhere, and the examples all show five examples. I guess I've been doing this stuff too long, and it's no longer obvious to me what is and isn't obvious. I think has been the root cause of a lot of confusion.
Things will improve with spaCy 2.0 a little bit. You might be able to get a useful model with as little as 500 or 1,000 sentences annotated with a new NER class. Maybe.
We're working on ways to make all of this more efficient. We're working on &lt;denchmark-link:http://www.slideshare.net/InesMontani/teaching-ai-about-human-knowledge/8&gt;making annotation projects less expensive and more consistent&lt;/denchmark-link&gt;
, and we're working on algorithms that require fewer annotated examples. But there will always be limits.
The thing is...I think most teams should be annotating literally 10,000x as much data as they're currently trying to get away with. You should have at least 1,000 sentences just of evaluation data, that your machine learning model never sees. Otherwise how will you know that your system is working? By typing stuff into it, manually? You wouldn't test your other code like that, would you? :)
		</comment>
		<comment id='2' author='ejschoen' date='2017-02-15T19:06:50Z'>
		Are there alternative models that are more robust with respect to smaller datasets?  Playing with luis.ai and wit.ai, their NERs seem to handle smaller datasets, but I'm not sure what they're using behind the scenes.  Their models retrain pretty quickly, so they're likely not complex.
		</comment>
		<comment id='3' author='ejschoen' date='2017-02-15T19:39:45Z'>
		&lt;denchmark-link:https://github.com/etchen99&gt;@etchen99&lt;/denchmark-link&gt;
 : Neural network models will do better at this, because we'll be able to use transfer learning --- we can import knowledge from other tasks, about the language in general. That helps a lot when you don't have much data.
But, again: "not much data" is here "a few thousand sentences". I get that people want to train on a few dozen sentences. I think people shouldn't want that.
Annotated data will never not be a part of this type of machine learning, no matter what algorithm you're using --- because you're always going to need evaluation data. That won't ever change. If you're making a few thousand sentences of evaluation data, you may as well make a few thousand more for training.
		</comment>
		<comment id='4' author='ejschoen' date='2017-03-08T04:43:17Z'>
		&lt;denchmark-link:https://github.com/honnibal&gt;@honnibal&lt;/denchmark-link&gt;

Thanks for your explanation.
Currently, the  of training and updating NER in the &lt;denchmark-link:https://spacy.io/docs/usage/entity-recognition#updating&gt;document&lt;/denchmark-link&gt;
 , which is obviously not enough (I realize it after reading your comment).
I think if you put your explanation in the document, that will be better. Everyone tries to read the doc to learn something, they go to the issues only if they could not find what they want in the doc.
&lt;denchmark-h:h2&gt;More problems about the example code&lt;/denchmark-h&gt;



How to use the updated NER model?
Update: find an example here: https://spacy.io/docs/usage/training#train-entity


Seems the example is trying to retrain a NER model, not update the original one?


&gt;&gt;&gt; # after running the example code, it does not work
&gt;&gt;&gt; nlp(u'Who is Chaka Khan?').ents
()
		</comment>
		<comment id='5' author='ejschoen' date='2017-03-08T04:51:32Z'>
		According to this &lt;denchmark-link:https://github.com/explosion/spacy-dev-resources/blob/master/spacy-annotator/displacy/server.py&gt;repo&lt;/denchmark-link&gt;
, I did find a way to update the original NER model. However, it does not support training new entities.
Example of training to extract the degress:
nlp = spacy.load('en')
ner = nlp.entity
text, tags = (u'B.S. in Mathmatics', [(0, 4, 'DEGREE')])  
doc = nlp.make_doc(text)
gold = GoldParse(doc, entities=tags)
ner.update(doc, gold)
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "&lt;stdin&gt;", line 8, in &lt;module&gt;
  File "spacy/syntax/parser.pyx", line 247, in spacy.syntax.parser.Parser.update (spacy/syntax/parser.cpp:7892)
  File "spacy/syntax/ner.pyx", line 93, in spacy.syntax.ner.BiluoPushDown.preprocess_gold (spacy/syntax/ner.cpp:4783)
  File "spacy/syntax/ner.pyx", line 123, in spacy.syntax.ner.BiluoPushDown.lookup_transition (spacy/syntax/ner.cpp:5379)
KeyError: u'U-DEGREE'
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='ejschoen' date='2017-03-31T10:10:25Z'>
		The bugs around this should now be resolved, as of 1.7.3. See further discussion in &lt;denchmark-link:https://github.com/explosion/spaCy/issues/910&gt;#910&lt;/denchmark-link&gt;
.
Usability around the retrained NER still isn't great, but the situation is improving. This will be fully resolved once:

The docs are improved
The examples are updated
The training CLI interface is finished and documented
The save/load process is easier, once models can be pickled.

All of these things are underway in other threads, so I'll close this one.
		</comment>
		<comment id='7' author='ejschoen' date='2018-05-09T00:39:13Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>