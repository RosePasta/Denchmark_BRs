<bug id='544' author='michaelcapizzi' open_date='2016-10-20T20:41:16Z' closed_time='2016-10-23T15:51:58Z'>
	<summary>size confusion when loading custom vectors</summary>
	<description>
This is a follow up to &lt;denchmark-link:https://github.com/explosion/spaCy/issues/448&gt;this issue&lt;/denchmark-link&gt;
 which still persists.
I am not confident that spacy is housing my vectors after loading.
First of all, I have created a bin of my vectors using vocab.write_binary_vectors().  They are 200-dimensions, but after successfully loading them into my existing instance of English(), they still appear to be 300-dimensions.
&lt;denchmark-code&gt;&gt;&gt;&gt; nlp = English(vectors=lambda vocab: vocab.load_vectors_from_bin_loc("/path/to/my/binary/vectors/w2v.bin"))
&gt;&gt;&gt; nlp.vocab.__getitem__("this").vector.shape
(300,)
&lt;/denchmark-code&gt;

The weirdest thing, though, is that these vectors are not the "original" vectors loaded by spacy (GloVe 200-dimensions):
&lt;denchmark-code&gt;&gt;&gt;&gt; nlp2 = English()
&gt;&gt;&gt; nlp2.vocab.__getitem__("this").vector.shape
(300,)
&gt;&gt;&gt; nlp.vocab.__getitem__("this").vector == nlp2.vocab.__getitem__("this").vector
array([False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False], dtype=bool)
&lt;/denchmark-code&gt;

So this vector is different from the "original", preloaded vector for "this", but it's still 300 dimensions.
The same thing happens if I use vocab.load_vectors() instead of vocab.load_vectors_from_bin_loc():
&lt;denchmark-code&gt;&gt;&gt;&gt; nlp3 = English(vectors=lambda vocab: vocab.load_vectors("/path/to/my/vectors/in/text/format/w2v.txt"))
&gt;&gt;&gt; nlp3.vocab.__getitem__("this").vector.shape
(300,)
&gt;&gt;&gt; nlp3.vocab.__getitem__("this").vector == nlp.vocab.__getitem__("this").vector
array([False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False], dtype=bool)
&lt;/denchmark-code&gt;

At least, however, they are the same as the vectors that were loaded from the bin file:
&lt;denchmark-code&gt;&gt;&gt;&gt; nlp3.vocab.__getitem__("this").vector == nlp2.vocab.__getitem__("this").vector
array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True], dtype=bool)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='michaelcapizzi' date='2016-10-20T20:52:49Z'>
		I need to update the docs — this was indeed broken at the 1.0 release (actually the GloVe loading was also broken). Can you try again with v1.0.5, using the keyword add_vectors instead of vectors?
		</comment>
		<comment id='2' author='michaelcapizzi' date='2016-10-20T21:30:03Z'>
		Sure thing.  But, I'm sorry, I'm a bit unclear.  I'm not sure what command to use and where add_vectors comes in.
So I tried to load vectors from a text file in 1.0.5 using vocab.add_vectors(path/to/vectors.txt):
&lt;denchmark-code&gt;&gt;&gt;&gt; from spacy.en import English
&gt;&gt;&gt; nlp = English()
&gt;&gt;&gt; this_vector = nlp.vocab.__getitem__("this").vector
&gt;&gt;&gt; this_vector.shape
(300,)
&gt;&gt;&gt; nlp2 = English(vectors=lambda vocab: vocab.add_vectors("/Users/mcapizzi/Github/nlp-pipeline/jupyter_notebooks/data/sample_w2v.txt"))
&gt;&gt;&gt; this_vector_2 = nlp2.vocab.__getitem__("this").vector
&gt;&gt;&gt; this_vector_2.shape
(300,)
&gt;&gt;&gt; this_vector == this_vector_2
array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True], dtype=bool)
&lt;/denchmark-code&gt;

So now it appears that the vectors did not get loaded in as the vector for "this" didn't change.
Then I tried vocab.add_vectors(path/to/binary/vectors.bin):
&lt;denchmark-code&gt;&gt;&gt;&gt; nlp3 = English(vectors=lambda vocab: vocab.add_vectors("/Users/mcapizzi/Github/nlp-pipeline/jupyter_notebooks/data/sample_w2v.bin"))
&gt;&gt;&gt; this_vector_3 = nlp3.vocab.__getitem__("this").vector
&gt;&gt;&gt; this_vector_3.shape
(300,)
&gt;&gt;&gt; this_vector == this_vector_3
array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True], dtype=bool)
&lt;/denchmark-code&gt;

Same result.
If I misunderstood what you wanted me to try, please clarify and I'll happily test it out.
		</comment>
		<comment id='3' author='michaelcapizzi' date='2016-10-20T23:46:25Z'>
		Sorry, I meant like this:
nlp2 = English(add_vectors=lambda vocab: vocab.load_vectors("/Users/mcapizzi/Github/nlp-pipeline/jupyter_notebooks/data/sample_w2v.txt"))
		</comment>
		<comment id='4' author='michaelcapizzi' date='2016-10-21T14:11:37Z'>
		Got it.
Unfortunately, a new error (again, I'm in 1.0.5:
&lt;denchmark-code&gt;&gt;&gt;&gt; nlp = English(add_vectors=lambda vocab: vocab.load_vectors("/Users/mcapizzi/Github/nlp-pipeline/jupyter_notebooks/data/sample_w2v.txt")
... )
&gt;&gt;&gt; nlp.vocab.__getitem__("this").vector.shape
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "spacy/lexeme.pyx", line 105, in spacy.lexeme.Lexeme.vector.__get__ (spacy/lexeme.cpp:4614)
ValueError: Word vectors set to length 0. This may be because the data is not installed. If you haven't already, run
python -m spacy.en.download all
to install the data.
&lt;/denchmark-code&gt;

And it does the same thing when trying to load a bin file:
&lt;denchmark-code&gt;&gt;&gt;&gt; nlp = English(add_vectors=lambda vocab: vocab.load_vectors("/Users/mcapizzi/Github/nlp-pipeline/jupyter_notebooks/data/sample_w2v.bin"))
&gt;&gt;&gt; nlp.vocab.__getitem__("this").vector.shape
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "spacy/lexeme.pyx", line 105, in spacy.lexeme.Lexeme.vector.__get__ (spacy/lexeme.cpp:4614)
ValueError: Word vectors set to length 0. This may be because the data is not installed. If you haven't already, run
python -m spacy.en.download all
to install the data.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='michaelcapizzi' date='2016-10-21T14:28:48Z'>
		Hmm. Thanks for your patience.
		</comment>
		<comment id='6' author='michaelcapizzi' date='2016-10-21T15:11:11Z'>
		I really should've slowed down and tested this more carefully — I'm trying to do too many things at once.
I've added a method vocab.resize_vectors(new_size), to support the workflow where you want to assign the new vectors on lexeme.vector = new_vector. I think I fixed the vector loading too.
If you want to try this out, you can do:
pip install cython==0.23
pip install cymem thinc preshed
pip install https://github.com/explosion/spaCy/archive/master.zip
		</comment>
		<comment id='7' author='michaelcapizzi' date='2016-10-23T15:51:58Z'>
		This should be fixed in 1.1.0. Please reopen if it's not!
		</comment>
		<comment id='8' author='michaelcapizzi' date='2016-11-08T14:59:38Z'>
		Thanks &lt;denchmark-link:https://github.com/honnibal&gt;@honnibal&lt;/denchmark-link&gt;
 .  It indeed works in .
One clarification for anyone who may be having trouble:  the argument to vocab.load_vectors() must be a buffer not a path to a file:
&lt;denchmark-code&gt;f = open("path/to/vectors.txt", "r")
p.vocab.load_vectors(f)
f.close()
&lt;/denchmark-code&gt;

This is clear in the source code and documentation, but could be easily overlooked.
		</comment>
		<comment id='9' author='michaelcapizzi' date='2018-05-08T19:27:55Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>