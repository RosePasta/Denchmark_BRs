<bug id='4791' author='SachinGarg10' open_date='2019-12-11T06:42:51Z' closed_time='2020-04-23T14:58:24Z'>
	<summary>Getting Error: gold.pyx in spacy.gold.GoldParse.__init__()  IndexError: list index out of range</summary>
	<description>
Hello,
I'm training spacy english blank model with entities, tags, and parser in the normal training data format(not json).
Following is the code that I'm using for training.
&lt;denchmark-code&gt;for i in tqdm(range(300)): 
    random.shuffle(Train_data) 
    losses = {} 
#     for text, annotations in Train_data:
#         nlp.update([text], [{'words': annotations['words'], 'entities': annotations['entities'], 'heads': annotations['heads'], 'deps': annotations['deps'], 'tags': annotations['tags']}], sgd=optimizer, losses=losses, drop=0.5)
#         nlp.update([text], [annotations], sgd=optimizer, losses=losses, drop=0.5)

    batches = minibatch(Train_data, size=compounding(4.0, 32.0, 1.001)) 
    for batch in batches: 
        texts, annotations = zip(*batch) 
        nlp.update(texts, annotations, sgd=optimizer, losses=losses, drop=0.5) 
        
    print("Losses: ", losses) 
&lt;/denchmark-code&gt;

While training, if I use USD symbol('$') in training data, everything works fine but as I replace the dollar symbol with Indian Rupee symbol('₹') in the training data, I get following error:
&lt;denchmark-code&gt;IndexError   Traceback (most recent call last)
&lt;ipython-input-12-4dc4a5e9998c&gt; in &lt;module&gt;
     12     for batch in batches:
     13         texts, annotations = zip(*batch)
---&gt; 14         nlp.update(texts, annotations, sgd=optimizer, losses=losses, drop=0.5)
     15 
     16     print("Losses: ", losses)

~/Documents/venv/spacy3.6/lib/python3.6/site-packages/spacy/language.py in update(self, docs, golds, drop, sgd, losses, component_cfg)
    494             sgd = self._optimizer
    495         # Allow dict of args to GoldParse, instead of GoldParse objects.
--&gt; 496         docs, golds = self._format_docs_and_golds(docs, golds)
    497         grads = {}
    498 

~/Documents/venv/spacy3.6/lib/python3.6/site-packages/spacy/language.py in _format_docs_and_golds(self, docs, golds)
    466                     err = Errors.E151.format(unexp=unexpected, exp=expected_keys)
    467                     raise ValueError(err)
--&gt; 468                 gold = GoldParse(doc, **gold)
    469             doc_objs.append(doc)
    470             gold_objs.append(gold)

gold.pyx in spacy.gold.GoldParse.__init__()

IndexError: list index out of range

&lt;/denchmark-code&gt;

Please help and thanks in advance
&lt;denchmark-h:h2&gt;System Environment&lt;/denchmark-h&gt;


Operating System: 16.04 Ubuntu
Python Version: 3.6.9
SpaCy Version: 2.2.2

	</description>
	<comments>
		<comment id='1' author='SachinGarg10' date='2019-12-11T07:46:32Z'>
		Hi, can you provide an example that works and an example that leads to this error?
		</comment>
		<comment id='2' author='SachinGarg10' date='2019-12-11T08:26:10Z'>
		yes, here it is...
Getting error in this example:
&lt;denchmark-code&gt;("I'll return the ₹54 amount that i owe you", {
'words': ['I', "'ll", 'return', 'the', '₹', '54', 'amount', 'that', 'i', 'owe', 'you'],
'entities': [(17, 23, 'MONEY')],
'heads': [2, 2, 2, 6, 5, 6, 2, 9, 9, 6, 9],
'deps': ['nsubj', 'aux', 'ROOT', 'det', 'nmod', 'nummod', 'dobj', 'dative', 'nsubj', 'relcl', 'dative'],
'tags': ['PRP', 'MD', 'VB', 'DT', '$', 'CD', 'NN', 'WDT', 'PRP', 'VBP', 'PRP']
})
&lt;/denchmark-code&gt;

but works fine for this:
&lt;denchmark-code&gt;("I'll return the $54 amount that i owe you", {
'words': ['I', "'ll", 'return', 'the', '$', '54', 'amount', 'that', 'i', 'owe', 'you'],
'entities': [(17, 23, 'MONEY')],
'heads': [2, 2, 2, 6, 5, 6, 2, 9, 9, 6, 9],
'deps': ['nsubj', 'aux', 'ROOT', 'det', 'nmod', 'nummod', 'dobj', 'dative', 'nsubj', 'relcl', 'dative'],
'tags': ['PRP', 'MD', 'VB', 'DT', '$', 'CD', 'NN', 'WDT', 'PRP', 'VBP', 'PRP']
})
&lt;/denchmark-code&gt;

Even this data contain INR symbol, it works fine:
&lt;denchmark-code&gt;('the cost of this plastic bottle is ₹343', {
 'words': ['the', 'cost', 'of', 'this', 'plastic', 'bottle', 'is', '₹', '343'],
 'entities': [(36, 39, 'MONEY')],
 'heads': [1, 6, 1, 5, 5, 6, 6, 8, 6],
 'deps': ['det', 'nsubj', 'prep', 'det', 'amod', 'pobj', 'ROOT', 'nmod', 'dobj'],
 'tags': ['DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBZ', '$', 'CD']
 })
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='SachinGarg10' date='2019-12-11T10:22:25Z'>
		Thanks for the report!
The difference between $54 and ₹54 is that spacy's default English tokenizer currently splits $54 into two tokens but keeps ₹54 as one token. spacy should be able to handle these kinds of misalignments in the tokenization, but it looks like there's a bug in how the entities are handled when you have entities as character offsets AND the tokenization isn't the same. I'll look into it!
Given your data, I would recommend customizing the tokenizer to add  ₹ as a prefix.
&lt;denchmark-code&gt;prefixes = nlp.Defaults.prefixes + ("₹",)
prefix_regex = spacy.util.compile_prefix_regex(prefixes)
nlp.tokenizer.prefix_search = prefix_regex.search
&lt;/denchmark-code&gt;

I initially thought the fact that your entity offsets were not on token boundaries might be part of the problem, but it isn't (and in any case it shouldn't crash). But just in case this is a real example, note that:
&lt;denchmark-code&gt;"I'll return the ₹54 amount that i owe you"[17:23] == "54 amo"
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='SachinGarg10' date='2019-12-11T10:45:50Z'>
		As an internal note, this is actually a somewhat nasty alignment issue:



spaCy/spacy/gold.pyx


        Lines 709 to 715
      in
      38e1bc1






 else: 



 # Translate the None values to '-', to make processing easier. 



 # See Issue #2603 



     entities = [(ent if ent is not None else "-") for ent in entities] 



 if not isinstance(entities[0], basestring): 



 # Assume we have entities specified by character offset. 



         entities = biluo_tags_from_offsets(doc, entities) 





biluo_tags_from_offsets() refers to a doc with spacy's tokenization rather than the words tokenization, so you can end up with entities as a different length than words. You only get the error above when entities is shorter than words, but neither case is good in terms of the training instance.
You could rewrite it to try to get the right number of tokens:
biluo_tags_from_offsets(Doc(doc.vocab, words=words), entities)
but then the character offsets are incorrect because you don't (immediately) have the spaces info to get the right text for the character offsets. To fix this properly, I think you'd have to modify Doc to do all the alignment to construct a doc from text + words.
As a short-term solution, you could add a warning and set entities back to ["O" for _ in words]?
I thought this might be an issue for the new proposed JSON format, but since that includes the token character offsets, it's much easier to align the entity spans with token spans.
Edit: spaces for Doc is fine, but words is allowed to omit whitespace tokens that would need to be in the constructed doc, potentially throwing off the token offsets.
		</comment>
	</comments>
</bug>