<bug id='530' author='tkhan3' open_date='2016-10-17T06:05:57Z' closed_time='2016-11-04T09:47:50Z'>
	<summary>Issues while loading Textacy corpus from disk.</summary>
	<description>
I am working with scrapped data from Twitter. I am treating each individual tweets as document. And I am creating a new corpus by adding documents one by one. To avoid creating corpus every time I have saved it to disk and trying to load. My collected data is in UTF-8 format. But I am facing issues. In fact i tried to load the corpus in the same program where it was saved it's failing with error:
Initially I had raised issue with &lt;denchmark-link:https://github.com/bdewilde&gt;@bdewilde&lt;/denchmark-link&gt;
  and he suggested error is occurring in spacy's serialize.packer, on which the save/load functionality in textacy relies. Please help.
Error
&lt;denchmark-code&gt;Corpus Created*******************
Corpus(27472 docs; 535575 tokens)
Traceback (most recent call last):
  File "C:\Users\workspace\textacy\src\corpus.py", line 74, in &lt;module&gt;
    main()
  File "C:\Users\workspace\textacy\src\corpus.py", line 56, in main
    tweet_corpus = textacy.Corpus.load('c:\\corpus',name='zika',compression='gzip')
  File "C:\Anaconda2\lib\site-packages\textacy-0.3.0-py2.7.egg\textacy\corpus.py", line 263, in load
    for spacy_doc, metadata in zip(spacy_docs, metadata_stream):
  File "C:\Anaconda2\lib\site-packages\textacy-0.3.0-py2.7.egg\textacy\fileio\read.py", line 173, in read_spacy_docs
    yield SpacyDoc(spacy_vocab).from_bytes(bytes_string)
  File "spacy/tokens/doc.pyx", line 423, in spacy.tokens.doc.Doc.from_bytes (spacy/tokens/doc.cpp:10859)
    self.vocab.serializer.unpack_into(data[4:], self)
  File "spacy/serialize/packer.pyx", line 125, in spacy.serialize.packer.Packer.unpack_into (spacy/serialize/packer.cpp:6144)
    self._char_decode(bits, -length, doc)
  File "spacy/serialize/packer.pyx", line 191, in spacy.serialize.packer.Packer._char_decode (spacy/serialize/packer.cpp:7677)
    doc.push_back(lex, is_spacy)
  File "spacy/tokens/doc.pyx", line 286, in spacy.tokens.doc.Doc.push_back (spacy/tokens/doc.cpp:8624)
    assert t.lex.orth != 0
AssertionError
&lt;/denchmark-code&gt;

Code that I am running is:
def main():

    dirname = sys.argv[1]
    filename = sys.argv[2]

    print dirname
    print filename
    filepath = os.path.join(dirname,filename)

    mytext = textacy.fileio.read_csv(filepath,encoding=u"utf-8")
    tweet_corpus = textacy.Corpus(u'en',metadatas=None)
    #meta = {}
    #meta[u"name"] = u"tanveer"
    count = 0

    for item in mytext:   ## read.csv will return a generator object. Extracting each row
        try:
            text = u""
            text = unicode(item[51])
            meta = {}
            time = unicode(item[61])
            meta["time"] = time
            text = textacy.preprocess_text(text,lowercase=True)
            text = re.sub(u"@[A-Za-z0-9]+",u"",text)
            text = textacy.preprocess.replace_urls(text, replace_with=u"")
            doc = textacy.Doc(text,lang=u'en',metadata=meta) #creating textacy object. It expects data in unicode.
            tweet_corpus.add_doc(doc,metadata=meta) ## Adding one document/tweet object to corpus
        except IndexError:
            count = count + 1
            meta = {}
            time = unicode(item[61])
            meta["time"] = time
            doc = textacy.Doc(unicode("unknown"),lang=u"en",metadata=meta)
            tweet_corpus.add_doc(doc,metadata=meta) 
            print "******Bad Record Number : ",count



    print "Corpus Created*******************"
    print tweet_corpus
    tweet_corpus.save("c:\\corpus", name='zika', compression='gzip')
    tweet_corpus = textacy.Corpus.load('c:\\corpus',name='zika',compression='gzip')
    print tweet_corpus


if __name__ == "__main__":
    main()
Context
Want to create a corpus once and want to use it later for analysis.
Environment
&lt;denchmark-code&gt;    Version used: 0.3
    Environment name and version : Python 2,7 Anaconda
    Operating System and version: Windows 7
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='tkhan3' date='2016-10-24T12:06:25Z'>
		Would you be able to get me a specific text that this fails on? Also, could you test with the latest spaCy (1.1.2)?
I've made a couple of improvements to the serializer, so there's a chance this is fixed. But if it's not, it's hard to debug without the specific failure.
		</comment>
		<comment id='2' author='tkhan3' date='2016-11-04T09:47:50Z'>
		Please reopen if problem persists in the current version.
		</comment>
		<comment id='3' author='tkhan3' date='2017-09-05T20:39:34Z'>
		I am still experiencing this same issue using python 3.6 and spacy 1.9.
		</comment>
		<comment id='4' author='tkhan3' date='2018-05-08T17:27:55Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>