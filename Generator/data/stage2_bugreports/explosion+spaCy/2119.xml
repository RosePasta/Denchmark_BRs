<bug id='2119' author='pzelasko' open_date='2018-03-20T10:35:08Z' closed_time='2018-12-29T17:08:43Z'>
	<summary>Model data lost when passing model between Python processes</summary>
	<description>
I've run into problems when parallelizing preparation of different SpaCy nlp models with multiprocessing and gathering them back in the main process. As far as I was able to understand, the problem is that Cython does not support pickling classes out of the box, and multiprocessing in Python uses pickling + pipes to transfer arguments and results between processes. Pickling can probably be enabled in Cython classes with definition of some additional methods (never done that, but these are results of my search):

https://stackoverflow.com/questions/12646436/pickle-cython-class
https://snorfalorpagus.net/blog/2016/04/16/pickling-cython-classes

The most problematic thing is that the behaviour is different on different platforms, i.e. on my machine I was still able to use the pickled-piped-unpickled models for word similarity computation, but on a different machine with the same Python version, Ubuntu 17.04 and GCC 7.2.0, these models had empty embeddings (vocab/vectors) after the inter-process transfer.
I suggest either a warning/error when attempting to pickle or implementing full pickling support to reduce user confusion.
The code which can be used for reproduction of this error (on the 16.04 Ubuntu machine, only the last assertion fails):
&lt;denchmark-code&gt;import random
import string
from concurrent.futures import ProcessPoolExecutor
from spacy.language import Language


def prepare_model(embeddings):
    nlp = Language()
    nlp.vocab.reset_vectors(width=len(embeddings[0][1]))
    for word, vector in embeddings:
        nlp.vocab.set_vector(word, vector)
    return nlp


def randomword():
    letters = string.ascii_lowercase
    length = random.randint(5, 15)
    return ''.join(random.choice(letters) for _ in range(length))


def generate_random_embeddings(num_words, embedding_size):
    for _ in range(num_words):
        yield randomword(), list(random.random() for _ in range(embedding_size))


def test_parallel_embedding_loading():

    n_words = 520
    embedding_size = 123

    embeddings = list(generate_random_embeddings(n_words, embedding_size))

    model_sequential = prepare_model(embeddings)

    with ProcessPoolExecutor(1) as executor:
        model_parallel = list(executor.map(prepare_model, [embeddings]))[0]

    assert model_sequential.vocab.vectors_length == model_parallel.vocab.vectors_length

    assert list(model_sequential.vocab.vectors.keys()) == list(model_parallel.vocab.vectors.keys())

    assert (model_sequential.vocab.vectors.data == model_parallel.vocab.vectors.data).all()

    assert list(model_sequential.vocab.vectors.values()) == list(model_parallel.vocab.vectors.values())
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Info about spaCy&lt;/denchmark-h&gt;


spaCy version: 2.0.7
Platform: Linux-4.13.0-36-generic-x86_64-with-debian-stretch-sid (Ubuntu 16.04)
Python version: 3.6.3
Models: en
GCC: 5.4.0

	</description>
	<comments>
		<comment id='1' author='pzelasko' date='2018-03-27T22:47:02Z'>
		That platform dependency is very strange --- I can't see why that would be the case.
We do intend full pickle support; it's just quite difficult. I suggest you try serialising the data you need with the nlp.to_bytes() method, likely doing nlp.to_bytes(disable=['vocab']), and then loading the vocab on the receiving side. This lets you avoid making a lot of copies of the vectors, which are static, and which take up most of the data. It also gives you fine-grained control over what to serialize.
		</comment>
		<comment id='2' author='pzelasko' date='2018-03-28T09:30:52Z'>
		Thanks for your suggestions. I'm aware that these models can be serialized in other ways, I just wanted to point out that there is some unexpected behavior when using Python's built-in multiprocessing modules which (to the best of my knowledge) communicate only by pickling objects.
As to the platform dependency, my best guess is that typical C issues propagate to Python - when a Cython object is pickled with "default support" for pickling protocol, then the underlying C code may perform some action classified as &lt;denchmark-link:http://en.cppreference.com/w/cpp/language/ub&gt;undefined/unspecified/implementetation-defined behavior&lt;/denchmark-link&gt;
, and thus the outcome varies between different versions of the C compiler.
		</comment>
		<comment id='3' author='pzelasko' date='2018-03-28T10:54:31Z'>
		I have a hard time believing Pickle is really triggering undefined behaviour that would differ this much.
I suspect an explanation like different compression libraries being available would be more likely? Maybe numpy or msgpack-numpy does something dumb if zlib is not available and the data is supposed to be compressed?
		</comment>
		<comment id='4' author='pzelasko' date='2018-12-29T17:08:42Z'>
		This should now be fixed in spacy-nightly. Please open a new issue referencing this if the problem persists. ðŸŽ‰
		</comment>
		<comment id='5' author='pzelasko' date='2019-01-28T18:05:22Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>