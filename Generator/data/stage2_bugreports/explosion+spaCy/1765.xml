<bug id='1765' author='stanpcf' open_date='2017-12-25T04:50:24Z' closed_time='2018-12-08T12:15:12Z'>
	<summary>bug about long text. Tokenizer will loop forever</summary>
	<description>
os version, python version, spacy version will be find in the picture
&lt;denchmark-link:https://user-images.githubusercontent.com/13925796/34332934-914be346-e971-11e7-9611-c1813f6fd11e.png&gt;&lt;/denchmark-link&gt;

the var text is a true comment from a web. it can't execute nlp(text)over
	</description>
	<comments>
		<comment id='1' author='stanpcf' date='2017-12-25T09:18:05Z'>
		A string consisting of 175k characters is very long. I'm not sure how Ines and Matthew think about this but I would suggest that you generally split your input text into chunks, either by natural paragraphs/linebreaks or via a hard character limit.
And depending on your application, you might think about skipping such a long web comment since the author is most likely trolling ;-)
		</comment>
		<comment id='2' author='stanpcf' date='2017-12-25T14:36:17Z'>
		&lt;denchmark-link:https://github.com/Liebeck&gt;@Liebeck&lt;/denchmark-link&gt;
 I select first of the 100 char of this comment to replace itself. But I think this should be a bug. spacy should raise exception when deal such the text instead of loop(my app in this place for some hours when I force it to stop) if it can't deal with such a text.  If possible and the developer of this tool is busy I will make a pull-request after some weeks
		</comment>
		<comment id='3' author='stanpcf' date='2018-12-08T12:15:12Z'>
		Merging this with &lt;denchmark-link:https://github.com/explosion/spaCy/issues/2744&gt;#2744&lt;/denchmark-link&gt;
!
		</comment>
		<comment id='4' author='stanpcf' date='2019-01-07T12:28:06Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>