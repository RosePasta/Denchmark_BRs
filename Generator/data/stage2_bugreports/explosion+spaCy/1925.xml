<bug id='1925' author='azar923' open_date='2018-02-02T20:06:39Z' closed_time='2018-03-27T19:30:58Z'>
	<summary>Strange behavior of document serialization in spaCy 1.10</summary>
	<description>
Hello!
There seems to be a bug in spacy 1.10 - fail to serialize some documents. Some are serialized successfully and some fail with following exception:
 serialized_doc = doc.to_bytes() File "spacy/tokens/doc.pyx", line 684, in spacy.tokens.doc.Doc.to_bytes (spacy/tokens/doc.cpp:13156) File "spacy/serialize/packer.pyx", line 105, in spacy.serialize.packer.Packer.pack (spacy/serialize/packer.cpp:5604) File "spacy/serialize/packer.pyx", line 144, in spacy.serialize.packer.Packer._orth_encode (spacy/serialize/packer.cpp:6694) IndexError: too many indices for array
Some of texts that are processed and serialized successfully:
I am using spaCy
using spaCy
spaCy
I spaCy
I am spaCy
spaCy
Some of texts that are processed and fail to be serialized:
I am using
am using
using
In spaCy 1.9 I am not experiencing such a problem.
I understand that main focus now is 2.0+, but, considering that 1.10.0 is intended to be the latest stable release of 1.x, it would be great to understand a reason for such behavior.
Model version : en_core_web_md-1.2.1
&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


Operating System: Ubuntu 16.04
Python Version Used: 2.7
spaCy Version Used: 1.10

	</description>
	<comments>
		<comment id='1' author='azar923' date='2018-02-03T00:07:29Z'>
		I have reproduced this in python3.6+spacy1.10, but to_bytes still works on these cases in python2.7+spacy1.10
		</comment>
		<comment id='2' author='azar923' date='2018-02-03T00:22:38Z'>
		The "too many indices" error leads me to suspect that there's a mismatch between the number of bytes (the underlying representation of a py2.7 string) and the number of unicode chars (the underlying rep of a py3.x string)
		</comment>
		<comment id='3' author='azar923' date='2018-03-27T19:30:07Z'>
		The serializer in v1.x was pretty complicated: it uses Huffman encoding to compress the data, and backs-off to a character representation for out-of-vocabulary words. This gave nicely small representations, but it was always buggy.
In v2 that whole submodule has been removed, and we instead just use doc.to_array(). The resulting array can be compressed with standard stuff, so it's a better approach overall. You might want to try doing it this way for your v1 code as well: just use doc.to_array() to export the attributes you care about, and then write a small function to put the document back together.
		</comment>
		<comment id='4' author='azar923' date='2018-05-07T20:52:43Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>