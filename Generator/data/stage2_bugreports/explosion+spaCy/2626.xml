<bug id='2626' author='datawrestler' open_date='2018-08-03T13:12:55Z' closed_time='2018-12-20T12:25:58Z'>
	<summary>Infinite loop in tokenizer._split_affixes</summary>
	<description>
I believe I have encountered an infinite loop during the process of tokenizing and converting a string into a spacy document. I have the following piece of text (it is messy, and I tried preprocessing steps before using spacy which didn't solve the problem):
text = """
ABLEItemColumn IAcceptance Limits of ErrorIn-Service Limits of ErrorColumn IIColumn IIIColumn IVColumn VComputed VolumeUnder Registration of\xa0VolumeOver Registration of\xa0VolumeUnder Registration of\xa0VolumeOver Registration of\xa0VolumeCubic FeetCubic FeetCubic FeetCubic FeetCubic Feet1Up to 10.0100.0050.0100.005220.0200.0100.0200.010350.0360.0180.0360.0184100.0500.0250.0500.0255Over 100.5% of computed volume0.25% of computed volume0.5% of computed volume0.25% of computed volume TABLE ItemColumn IAcceptance Limits of ErrorIn-Service Limits of ErrorColumn IIColumn IIIColumn IVColumn VComputed VolumeUnder Registration of\xa0VolumeOver Registration of\xa0VolumeUnder Registration of\xa0VolumeOver Registration of\xa0VolumeCubic FeetCubic FeetCubic FeetCubic FeetCubic Feet1Up to 10.0100.0050.0100.005220.0200.0100.0200.010350.0360.0180.0360.0184100.0500.0250.0500.0255Over 100.5% of computed volume0.25% of computed volume0.5% of computed volume0.25% of computed volume ItemColumn IAcceptance Limits of ErrorIn-Service Limits of ErrorColumn IIColumn IIIColumn IVColumn VComputed VolumeUnder Registration of\xa0VolumeOver Registration of\xa0VolumeUnder Registration of\xa0VolumeOver Registration of\xa0VolumeCubic FeetCubic FeetCubic FeetCubic FeetCubic Feet1Up to 10.0100.0050.0100.005220.0200.0100.0200.010350.0360.0180.0360.0184100.0500.0250.0500.0255Over 100.5% of computed volume0.25% of computed volume0.5% of computed volume0.25% of computed volume
"""
I use the following code to process this text using spacy:
import spacy
nlp = spacy.load('path_to_model')
doc = nlp.make_doc(text)
I am using spacy version 2.0.11 and python 3.6.4.
When I interupt the process (after hours on this single document as I was hoping an error message would come through), the code is getting hung on
tokenizer.pyx in spacy.tokenizer.Tokenizer._split_affixes()
	</description>
	<comments>
		<comment id='1' author='datawrestler' date='2018-08-05T11:38:11Z'>
		Thanks for the report. I'm not seeing the error on v2.0.12. Could you try upgrading and check that it still occurs for you?
		</comment>
		<comment id='2' author='datawrestler' date='2018-08-05T11:44:46Z'>
		I can't reproduce this on v2.0.11 either. Are you sure it's not a problem you introduced for your custom model? What locale are you using --- maybe it's an encoding issue?
		</comment>
		<comment id='3' author='datawrestler' date='2018-08-07T18:38:44Z'>
		I originally stripped all characters with ordinals greater than 128 and the error persisted, so not sure if it is an encoding issue. Using the following series of regular expressions, I was able to get it to work:
def process_regex_pipe(doc):
    digit_to_alpha = re.compile('(?P&lt;digit&gt;[0-9])(?P&lt;alpha&gt;[A-Za-z])', re.IGNORECASE)
    alpha_to_digit = re.compile('(?P&lt;alpha&gt;[A-Za-z])(?P&lt;digit&gt;[0-9])', re.IGNORECASE)
    lower_to_upper = re.compile('(?P&lt;lower&gt;[a-z])(?P&lt;upper&gt;[A-Z])')
    upper_to_lower = re.compile('(?P&lt;upper&gt;[A-Z])(?P&lt;lower&gt;[A-Z][a-z])')
    digit_masker = re.compile('([0-9]{1,20}\.[0-9]{1,20})*')
    digit_masker2 = re.compile('(\.){1,20}[0-9]{1,20}')
    
    tmp = digit_to_alpha.sub('\g&lt;digit&gt; \g&lt;alpha&gt;', clean_doc(doc))
    tmp = alpha_to_digit.sub('\g&lt;alpha&gt; \g&lt;digit&gt;', tmp)
    tmp = upper_to_lower.sub('\g&lt;upper&gt; \g&lt;lower&gt;', tmp)
    tmp = lower_to_upper.sub('\g&lt;lower&gt; \g&lt;upper&gt;', tmp)
    tmp = digit_masker.sub('', tmp)
    tmp = digit_masker2.sub('', tmp)
    
    return tmp
I have another full text example of where it freezes:
&lt;denchmark-code&gt;'230 The limits of error set out in the following table apply to all solid volume machines designed to establish the space in cubic decimetres taken up by packages, parcels and crates: TABLEItemColumn IAcceptance Limits of ErrorIn-Service Limits of ErrorColumn IIColumn IIIColumn IVColumn VComputed VolumeUnder Registration of\xa0VolumeOver Registration of\xa0VolumeUnder Registration of\xa0VolumeOver Registration of\xa0VolumeCubic DecimetresCubic DecimetresCubic DecimetresCubic DecimetresCubic Decimetres1Up to 300.30.150.30.152500.50.250.50.2531000.80.400.80.4042001.20.601.20.6053001.50.751.50.756Over 3000.5% of computed volume0.25% of computed volume0.5% of computed volume0.25% of computed volume TABLE ItemColumn IAcceptance Limits of ErrorIn-Service Limits of ErrorColumn IIColumn IIIColumn IVColumn VComputed VolumeUnder Registration of\xa0VolumeOver Registration of\xa0VolumeUnder Registration of\xa0VolumeOver Registration of\xa0VolumeCubic DecimetresCubic DecimetresCubic DecimetresCubic DecimetresCubic Decimetres1Up to 300.30.150.30.152500.50.250.50.2531000.80.400.80.4042001.20.601.20.6053001.50.751.50.756Over 3000.5% of computed volume0.25% of computed volume0.5% of computed volume0.25% of computed volume ItemColumn IAcceptance Limits of ErrorIn-Service Limits of ErrorColumn IIColumn IIIColumn IVColumn VComputed VolumeUnder Registration of\xa0VolumeOver Registration of\xa0VolumeUnder Registration of\xa0VolumeOver Registration of\xa0VolumeCubic DecimetresCubic DecimetresCubic DecimetresCubic DecimetresCubic Decimetres1Up to 300.30.150.30.152500.50.250.50.2531000.80.400.80.4042001.20.601.20.6053001.50.751.50.756Over 3000.5% of computed volume0.25% of computed volume0.5% of computed volume0.25% of computed volume       ItemColumn IAcceptance Limits of ErrorIn-Service Limits of ErrorColumn IIColumn IIIColumn IVColumn VComputed VolumeUnder Registration of\xa0VolumeOver Registration of\xa0VolumeUnder Registration of\xa0VolumeOver Registration of\xa0Volume ItemColumn IAcceptance Limits of ErrorIn-Service Limits of Error Item Column I Acceptance Limits of Error In-Service Limits of Error Column IIColumn IIIColumn IVColumn V Column II Column III Column IV Column V Computed VolumeUnder Registration of\xa0VolumeOver Registration of\xa0VolumeUnder Registration of\xa0VolumeOver Registration of\xa0Volume Computed Volume Under Registration of\xa0Volume Over Registration of\xa0Volume Under Registration of\xa0Volume Over Registration of\xa0Volume Cubic DecimetresCubic DecimetresCubic DecimetresCubic DecimetresCubic Decimetres1Up to 300.30.150.30.152500.50.250.50.2531000.80.400.80.4042001.20.601.20.6053001.50.751.50.756Over 3000.5% of computed volume0.25% of computed volume0.5% of computed volume0.25% of computed volume Cubic DecimetresCubic DecimetresCubic DecimetresCubic DecimetresCubic Decimetres  Cubic Decimetres Cubic Decimetres Cubic Decimetres Cubic Decimetres Cubic Decimetres Cubic Decimetres Cubic Decimetres Cubic Decimetres Cubic Decimetres Cubic Decimetres 1Up to 300.30.150.30.15 1 Up to 30 0.3 0.15 0.3 0.15 2500.50.250.50.25 2 50 0.5 0.25 0.5 0.25 31000.80.400.80.40 3 100 0.8 0.40 0.8 0.40 42001.20.601.20.60 4 200 1.2 0.60 1.2 0.60 53001.50.751.50.75 5 300 1.5 0.75 1.5 0.75 6Over 3000.5% of computed volume0.25% of computed volume0.5% of computed volume0.25% of computed volume 6 Over 300 0.5% of computed volume 0.25% of computed volume 0.5% of computed volume 0.25% of computed volume SOR/89-570, s. 6(F). SOR/89-570, s. 6(F). SOR/89-570, s. 6(F).'
&lt;/denchmark-code&gt;

When I apply the above series of regular expressions to each document, it works just fine. I will try upgrading to the latest version and see if that does the trick.
		</comment>
		<comment id='4' author='datawrestler' date='2018-08-08T17:30:35Z'>
		&lt;denchmark-link:https://github.com/honnibal&gt;@honnibal&lt;/denchmark-link&gt;
 regarding the custom model, I am using the large english model straight from spacy. Is this what you mean?
		</comment>
		<comment id='5' author='datawrestler' date='2018-12-20T12:25:58Z'>
		Just tested it in the latest  and seems like this issue is now fixed. It was likely related to the same issue as &lt;denchmark-link:https://github.com/explosion/spaCy/issues/2744&gt;#2744&lt;/denchmark-link&gt;
, a problem with how the  was compiled.
		</comment>
		<comment id='6' author='datawrestler' date='2019-01-19T12:33:34Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>