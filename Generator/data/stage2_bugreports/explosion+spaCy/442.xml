<bug id='442' author='aie0' open_date='2016-07-01T11:54:24Z' closed_time='2016-10-23T12:35:17Z'>
	<summary>doc vs token similarity</summary>
	<description>
There is a different when comparing single word document with another token and comparing document's token?
&lt;denchmark-code&gt;last_token = spacy("assign last to Victor")[1]
token1 = spacy('recent')
token2 = spacy('recent')[0]

assert last_token.similarity(token1) == last_token.similarity(token2) # 0.71 != 0.5
# or rather
assert token1.vector_norm == token2.vector_norm
&lt;/denchmark-code&gt;

What is the correct way of comparing the tokens?
	</description>
	<comments>
		<comment id='1' author='aie0' date='2016-07-01T12:35:38Z'>
		That definitely looks like a bug. Thanks.
		</comment>
		<comment id='2' author='aie0' date='2016-08-26T13:34:35Z'>
		Also,
&lt;denchmark-code&gt;doc_apple = en_nlp('apple')
tok_apple = doc_apple[0]
print(doc_apple.similarity(doc_apple)) # almost 1.0
print(tok_apple.similarity(tok_apple)) # almost 0.5 (!)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='aie0' date='2016-10-23T12:35:17Z'>
		Finally fixed this.
Thanks for continuing to report, even when there wasn't much response â€” this issue was very helpful.
		</comment>
		<comment id='4' author='aie0' date='2018-05-09T08:11:49Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>