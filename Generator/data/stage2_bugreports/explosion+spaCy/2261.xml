<bug id='2261' author='rohithkodali' open_date='2018-04-25T15:40:59Z' closed_time='2018-09-28T12:42:11Z'>
	<summary>stack smashing detected when running ner training</summary>
	<description>
I have spacy setup for noth python 2.7 and 3.5 but when i try to run ner training on spacy it throw the following error
Created blank 'en' model
Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))
*** stack smashing detected ***: python terminated
Aborted (core dumped)
where mught be it went wrong?
&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


Python version: 2.7.12
Platform: Linux-4.4.0-121-generic-x86_64-with-Ubuntu-16.04-xenial
spaCy version: 2.0.12.dev0
Models: en, te

	</description>
	<comments>
		<comment id='1' author='rohithkodali' date='2018-05-03T11:16:10Z'>
		update:
if we use only part of dataset  2300 sentences (in my case) it works without above error.
		</comment>
		<comment id='2' author='rohithkodali' date='2018-06-19T13:53:08Z'>
		hi &lt;denchmark-link:https://github.com/honnibal&gt;@honnibal&lt;/denchmark-link&gt;
 any update on this we still face the same issue when we used the more than 2300 sentences to train an NER
		</comment>
		<comment id='3' author='rohithkodali' date='2018-07-18T01:56:42Z'>
		Hi &lt;denchmark-link:https://github.com/rohithkodali&gt;@rohithkodali&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/honnibal&gt;@honnibal&lt;/denchmark-link&gt;
,
I had same issue, add the vector name before calling the begin_training.
nlp.vocab.vectors.name = 'spacy_pretrained_vectors'
optimizer = nlp.begin_training()
The reason after investigating the source code, it gives you warning if you don't put name into vectors before calling begin_training.



spaCy/spacy/_ml.py


        Lines 244 to 250
      in
      61ef073






 def link_vectors_to_models(vocab): 



 vectors = vocab.vectors 



 if vectors.name is None: 



 vectors.name = VECTORS_KEY 



 print( 



 "Warning: Unnamed vectors -- this won't allow multiple vectors " 



 "models to be loaded. (Shape: (%d, %d))" % vectors.data.shape) 





Hope helps
		</comment>
		<comment id='4' author='rohithkodali' date='2018-07-18T03:57:56Z'>
		No luck still getting the same error, &lt;denchmark-link:https://github.com/kororo&gt;@kororo&lt;/denchmark-link&gt;
  did it solved for you?  adding this just solved me the warning that i get.
		</comment>
		<comment id='5' author='rohithkodali' date='2018-07-18T06:20:02Z'>
		&lt;denchmark-link:https://github.com/rohithkodali&gt;@rohithkodali&lt;/denchmark-link&gt;
 ah sorry, the snippet I gave, just for the warning fix that I found.
The 2300 sentences is related to something else. I am curious so I trained 2880 sentences and I have no problem at all.

How many iteration and drop rate you have?
Are you using GPU to train?

That error messages mostly happen on GPU computation. Try to check on your CUDA version. Otherwise, post some of your training code, probs I could spot something up.
		</comment>
		<comment id='6' author='rohithkodali' date='2018-07-18T06:40:20Z'>
		it stops at first iteration only when i use full daya but when i use 2300 sentenecs it worked well for iterations.
i use GPU to train.
`
#!/usr/bin/env python
&lt;denchmark-h:h1&gt;coding: utf8&lt;/denchmark-h&gt;

from future import unicode_literals, print_function
import plac
import random
from pathlib import Path
import time
import spacy
import thinc.neural.gpu_ops
from nerDATA import *
def main(model='te', output_dir='telugu-nermodel6000v4', n_iter=50):
"""Load the model, set up the pipeline and train the entity recognizer."""
if model is not None:
nlp = spacy.load(model)  # load existing spaCy model
print("Loaded model '%s'" % model)
else:
nlp = spacy.blank('te')  # create blank Language class
print("Created blank 'te' model")
&lt;denchmark-code&gt;# create the built-in pipeline components and add them to the pipeline
# nlp.create_pipe works for built-ins that are registered with spaCy
if 'ner' not in nlp.pipe_names:
    ner = nlp.create_pipe('ner')
    nlp.add_pipe(ner, last=True)
# otherwise, get it so we can add labels
else:
    ner = nlp.get_pipe('ner')

# add labels
for _, annotations in TRAIN_DATA:
    for ent in annotations.get('entities'):
        ner.add_label(ent[2])
print(ner)


# get names of other pipes to disable them during training
other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
with nlp.disable_pipes(*other_pipes):  # only train NER
    nlp.vocab.vectors.name = 'spacy_pretrained_vectors'
    optimizer = nlp.begin_training(device=0)
    for itn in range(n_iter):
        random.shuffle(TRAIN_DATA)
        losses = {}
        start_time=time.time()
        for text, annotations in TRAIN_DATA:
            print(text)
            nlp.update(
                [text],  # batch of texts
                [annotations],  # batch of annotations
                drop=0.5,  # dropout - make it harder to memorise data
                sgd=optimizer,  # callable to update weights
                losses=losses)
        print(losses)
    print("time for iteration: ", time.time()-start_time)



# save model to output directory
if output_dir is not None:
    output_dir = Path(output_dir)
    if not output_dir.exists():
        output_dir.mkdir()
    nlp.to_disk(output_dir)
    print("Saved model to", output_dir)
&lt;/denchmark-code&gt;

if name == 'main':
plac.call(main)
`
this is my code to train
		</comment>
		<comment id='7' author='rohithkodali' date='2018-07-18T15:41:08Z'>
		Thanks for the report. This will be difficult to track down, but I hope we can get to the bottom of it.
		</comment>
		<comment id='8' author='rohithkodali' date='2018-07-18T22:36:24Z'>
		&lt;denchmark-link:https://github.com/rohithkodali&gt;@rohithkodali&lt;/denchmark-link&gt;
 I am more convince that something to do with your CUDA version since you are using GPU to train.

What is your CUDA version by calling "nvcc --version"
Could you remove your device=0 in "optimizer = nlp.begin_training(device=0)" and try whether this is confirm GPU?
What is your GPU spec? Is it in AWS?

From what I know, the latest CUDA version causes this issue.
Thanks
		</comment>
		<comment id='9' author='rohithkodali' date='2018-07-19T05:17:30Z'>
		&lt;denchmark-link:https://github.com/kororo&gt;@kororo&lt;/denchmark-link&gt;


here is my "nvcc --version" output

nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2016 NVIDIA Corporation
Built on Wed_May__4_21:01:56_CDT_2016
Cuda compilation tools, release 8.0, V8.0.26


i tried to use only CPU but still got the same error


my GPU is gtx 1080 on my local server (not using any cloud)


		</comment>
		<comment id='10' author='rohithkodali' date='2018-09-04T15:31:15Z'>
		hello,
i have the same issue
&lt;denchmark-link:https://github.com/rohithkodali&gt;@rohithkodali&lt;/denchmark-link&gt;
 have you found a solution?
unfortunately, i have other problems with training existing model, so i cant feed the data in batches :(
thanks
		</comment>
		<comment id='11' author='rohithkodali' date='2018-09-28T12:42:11Z'>
		I believe this issue is now fixed  . See here: &lt;denchmark-link:https://github.com/explosion/spaCy/issues/2800&gt;#2800&lt;/denchmark-link&gt;

Thanks for your help reporting.
		</comment>
		<comment id='12' author='rohithkodali' date='2018-10-28T13:34:58Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>