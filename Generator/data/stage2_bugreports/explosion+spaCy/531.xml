<bug id='531' author='vikrantsharma7' open_date='2016-10-18T08:52:53Z' closed_time='2016-10-20T02:33:32Z'>
	<summary>Pre-existing tokenization generates empty doc</summary>
	<description>
Tried to create doc using pre-existing tokenization.
&lt;denchmark-link:url&gt;https://spacy.io/docs/tutorials/byo-annotations&lt;/denchmark-link&gt;

tokens = [u'A', u'list', u'of', u'strings', u'.']
has_space = [True, True, True, False, False]
doc = Doc(nlp.vocab, orth_and_spaces=zip(tokens, has_space))
assert len(doc) == len(tokens)
The above is straight from the docs.
First of all, the keyword argument orth_and_spaces is actually orths_and_spaces.
But most importantly, the immediately following assertion assert len(doc) == len(tokens) fails as the doc is empty
	</description>
	<comments>
		<comment id='1' author='vikrantsharma7' date='2016-10-19T17:57:42Z'>
		Thanks, I know this was a frustrating bug.
This is fixed in the 1.0 release (just pushed). I've also deprecated the orths_and_spaces name, in favour of two keyword arguments words and spaces.
		</comment>
		<comment id='2' author='vikrantsharma7' date='2016-10-20T02:33:32Z'>
		Thanks! Will upgrade.
		</comment>
		<comment id='3' author='vikrantsharma7' date='2018-05-09T10:12:23Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>