<bug id='1826' author='sanjeeku' open_date='2018-01-11T10:46:38Z' closed_time='2018-12-08T11:27:22Z'>
	<summary>Segmentation fault if text too long or doc parsed twice</summary>
	<description>
I just installed Spacy 2.0.5 in Python 3.6.4 (that Anaconda).
I also installed the default model ('en')
Spacy is giving seg fault when I try to load my text file (it is about 2MB in size).
Here's the code the reproduces it:
Python 3.6.4 |Anaconda, Inc.| (default, Dec 21 2017, 21:42:08)
[GCC 7.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.



import spacy
nlp = spacy.load('en')
with open('wf.txt') as f:
...     text = f.read()
...
doc = nlp(text)
Segmentation fault (core dumped)



I tried with another text file (slightly larger though) with the same result.
&lt;denchmark-h:h2&gt;Info about spaCy&lt;/denchmark-h&gt;


spaCy version: 2.0.5
Platform: Linux-4.10.0-42-generic-x86_64-with-debian-stretch-sid
Python version: 3.6.4
Models: en

Is there any other information I can provide to troubleshoot this seg fault?
	</description>
	<comments>
		<comment id='1' author='sanjeeku' date='2018-01-11T11:14:55Z'>
		An update:  I tried on a new/clean aws instance where I installed Spacy differently (using conda forge). I still got the same seg fault.
Here's the environment info:
(py3) ubuntu@ip-172-31-16-211:~$ python -m spacy info --markdown
&lt;denchmark-h:h2&gt;Info about spaCy&lt;/denchmark-h&gt;


spaCy version: 2.0.4
Platform: Linux-4.4.0-1048-aws-x86_64-with-debian-stretch-sid
Python version: 3.6.4
Models: en

		</comment>
		<comment id='2' author='sanjeeku' date='2018-01-11T11:21:20Z'>
		Further update:
I had an old conda env with spacy 1.9.0 installed.
Both text files were parsed perfectly.
So the SegFault issue is only with Spacy 2.0 or later (I have tested with 2.0.4 and 2.0.5)
		</comment>
		<comment id='3' author='sanjeeku' date='2018-01-30T17:03:41Z'>
		I am experiencing a similar issue, though it occurs when using the English language model parse method. The problem occurs &lt; 1% of the time in a corpus of 250K documents, but I have yet to determine its root cause. An example paragraph is shown below.
Similarly, the problem occurs in 2.0.5, but is not present in 1.9.0. I have reproduced this across multiple machines.

spaCy version: 2.0.5
Platform: Ubuntu 14.04
Python version: 2.7
Models: en

&lt;denchmark-code&gt;import spacy

nlp  = spacy.load('en')
text = u'This will be a prospective study in patients aged between 18 and 70 years old who have already been screened and planned for elective bariatric surgery. In bariatric surgery, a large portion of the stomach will be removed. Pneumoperitoneum is also known as the abdominal pressure which will be the experimental aspect in this study. Laparoscopy surgery will be performed by introducing the camera (optical trocar) after making an incision at the belly button (umbilicus), and carbon dioxide which will be given at a rate of 5 L/min until the intra abdominal pressure of either 8 10 mmHg (low pressure group) or 12 15 mmHg (standard pressure group) is achieved. The remaining three standard ports will be placed and the laparoscopic sleeve gastrectomy will be performed at an insufflation rate of 15 L/min. The greater omentum will be divided at the greater curvature of the stomach using an ultrasonic dissector, beginning from the proximal antrum until the fundus. The omentum will be divided close to the stomach wall hence preserving the gastro epiploic vessels. Short gastric vessels will be divided entirely from the stomach and this dissection will continue until the left crus of the diaphragm are exposed. Endoscopic staplers will then be used to staple and divide the stomach until the angle of His. A 39Fr gastric calibration tube will be placed along the lesser curvature of the stomach, acts as a guide during the division of the stomach. Finally, the divided stomach will be removed through a 12mm port site and the incision will be closed with sutures. Towards the end of the surgery, all residual pneumoperitoneum will be evacuated by keeping the trocar valves open under direct telescopic vision. The duration of surgery or any intraoperative complications will be recorded. The starting of surgery will be regarded after the induction of anaesthesia and the end of surgery is regarded when the end of skin closure. Operating field or also known as surgical view is defined as the view of the intra abdomen. A clear operating field allows a good working space for the surgeon. Numeric rating score will be used to access the operating field during the surgery. Post operative pain will be rated on a Visual Analog Scale at rest and with movement.'

doc = nlp(text) # works
nlp.parse(doc) # Segmentation fault/core dump

&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='sanjeeku' date='2018-02-05T05:53:32Z'>
		Confirming that this bug continues to exists in Spacy v2.0.7
&lt;denchmark-link:https://github.com/honnibal&gt;@honnibal&lt;/denchmark-link&gt;
 - I am happy to privately send you the text files on which it is bombing. Please let me know where to send.
		</comment>
		<comment id='5' author='sanjeeku' date='2018-02-17T21:21:36Z'>
		&lt;denchmark-link:https://github.com/sanjeeku&gt;@sanjeeku&lt;/denchmark-link&gt;
 Thanks, could you mail to &lt;denchmark-link:mailto:matt@explosion.ai&gt;matt@explosion.ai&lt;/denchmark-link&gt;
 ?
		</comment>
		<comment id='6' author='sanjeeku' date='2018-03-05T16:52:49Z'>
		&lt;denchmark-link:https://github.com/honnibal&gt;@honnibal&lt;/denchmark-link&gt;
 -- Just emailed the text file.
		</comment>
		<comment id='7' author='sanjeeku' date='2018-03-29T20:29:29Z'>
		&lt;denchmark-link:https://github.com/sanjeeku&gt;@sanjeeku&lt;/denchmark-link&gt;
 Thanks for the text, finally got to this.
Your issue is simply that the text is too long. This is rather frustrating --- I wish we used less temporary memory per word than the neural networks currently do. However, I don't see a way around this without significantly impacting performance.
I've added an error message and added an option on the Language class to note the problem. In your case, the solution is very simple: just process each newline individually.
&lt;denchmark-link:https://github.com/godelstheory&gt;@godelstheory&lt;/denchmark-link&gt;
 Your problem is different. I think the problem occurs from parsing the text twice. This shouldn't cause a segfault, but as a workaround, you can avoid doing that for now? You can verify that the double-parsing is the problem by changing the first line to .
		</comment>
		<comment id='8' author='sanjeeku' date='2019-01-07T11:28:03Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>