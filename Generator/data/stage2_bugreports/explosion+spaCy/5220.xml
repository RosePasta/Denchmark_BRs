<bug id='5220' author='lingvisa' open_date='2020-03-27T23:36:30Z' closed_time='2020-04-07T17:53:32Z'>
	<summary>Can't find 'tag_''s hash value in multi-processing</summary>
	<description>
Spacy 2.2.3:
Mac
Chinese
My code works fine in single processing mode:
&lt;denchmark-code&gt;   for ent in doc.ents:
        label = ent.label_
        word = ''
        for tok in ent:
            tok_text = tok.text
            if len(tok_text) == 1 or tok.tag_ == 'm':
                continue
&lt;/denchmark-code&gt;

However, in multiprocessing,  it reports:
&lt;denchmark-code&gt; if len(tok_text) == 1 or tok.tag_ == 'm':
  File "token.pyx", line 884, in spacy.tokens.token.Token.tag_.__get__
  File "strings.pyx", line 136, in spacy.strings.StringStore.__getitem__
KeyError: "[E018] Can't retrieve string for hash '14557525498149931471'. This usually refers to an issue with the `Vocab` or `StringStore`."
Process Process-6:
Process Process-1:
&lt;/denchmark-code&gt;

The way I am using the multiprocessing is below:
for doc in self.nlp.pipe(textlines, n_process=-1) yield doc
In Chinese's init.py, I tried to add  jieba's tag into the stringsstore, but it doesn't help.
&lt;denchmark-code&gt;TAG_MAP = {
    "AS": {POS: PART},
    "DEC": {POS: PART},
    "DEG": {POS: PART},
    "DER": {POS: PART},
    "DEV": {POS: PART},
    "ETC": {POS: PART},
    "LC": {POS: PART},
    "MSP": {POS: PART},
    "SP": {POS: PART},
    "BA": {POS: X},
    "FW": {POS: X},
    "IJ": {POS: INTJ},
    "LB": {POS: X},
    "ON": {POS: X},
    "SB": {POS: X},
    "X": {POS: X},
    "URL": {POS: X},
    "INF": {POS: X},
    "NN": {POS: NOUN},
    "NR": {POS: NOUN},
    "NT": {POS: NOUN},
    "VA": {POS: VERB},
    "VC": {POS: VERB},
    "VE": {POS: VERB},
    "VV": {POS: VERB},
    "CD": {POS: NUM},
    "M": {POS: NUM},
    "OD": {POS: NUM},
    "DT": {POS: DET},
    "CC": {POS: CCONJ},
    "CS": {POS: SCONJ},
    "AD": {POS: ADV},
    "JJ": {POS: ADJ},
    "P": {POS: ADP},
    "PN": {POS: PRON},
    "PU": {POS: PUNCT},
    "_SP": {POS: SPACE},

    # jieba pos tags
    "ag": {POS: ADJ},
    "ad": {POS: ADV},
    "an": {POS: ADJ},
    "b": {POS: PART},
    "c": {POS: CCONJ},
    "dg": {POS: ADV},
    "d": {POS: ADV},
    "e": {POS: INTJ},
    "f": {POS: NOUN},
    "g": {POS: X},
    "h": {POS: X},
    "i": {POS: IDIOM},
    "j": {POS: X},
    "k": {POS: X},
    "l": {POS: IDIOM},
    "m": {POS: NUM},
    "ng": {POS: NOUN},
    "n": {POS: NOUN},
    "nr": {POS: NNP},
    "ns": {POS: NNP},
    "nt": {POS: NNP},
    "nz": {POS: NNP},
    "o": {POS: X},
    "p": {POS: PREP},
    "q": {POS: X},
    "r": {POS: PRON},
    "s": {POS: NOUN},
    "tg": {POS: NOUN},
    "t": {POS: NOUN},
    "u": {POS: AUX},
    "vg": {POS: VERB},
    "v": {POS: VERB},
    "vd": {POS: VERB},
    "vn": {POS: VERB},
    "x": {POS: X},
    "y": {POS: INTJ},
    "z": {POS: PART},
    "uj": {POS: X},
    "un": {POS: UN},
    "pu": {POS: PUNCT},
    "eng": {POS: ENG},
}
&lt;/denchmark-code&gt;

In the tag map, I added Jieba's lowercase tags to the map from the line "# jieba pos tags".
Why does the token.tag_ attribute have something to do with multiprocessing?
	</description>
	<comments>
		<comment id='1' author='lingvisa' date='2020-03-30T09:22:19Z'>
		The multiprocessing feature can have some surprising side effects when the child process spawns (Windows, Mac) instead of forks (Linux). Basically it's bad practice to rely on global state being available (like what happens when forking), and we need to make sure that any state is properly copied to the child processes. For reference, see PR &lt;denchmark-link:https://github.com/explosion/spaCy/pull/5006&gt;#5006&lt;/denchmark-link&gt;
 where we fixed this for the data in .
Now, with respect to your report ... it looks like at some point a new item is added to the Stringstore, but then maybe a child process does not have access to the updated StringStore? I'm not sure, and I can't yet replicate your issue, but it certainly does look like a bug...
		</comment>
		<comment id='2' author='lingvisa' date='2020-04-07T08:41:46Z'>
		Actually, I would have hoped this got fixed by PR &lt;denchmark-link:https://github.com/explosion/spaCy/pull/5081&gt;#5081&lt;/denchmark-link&gt;
. Could you try again with spaCy 2.2.4 ?
		</comment>
		<comment id='3' author='lingvisa' date='2020-04-07T11:08:35Z'>
		&lt;denchmark-link:https://github.com/svlandeg&gt;@svlandeg&lt;/denchmark-link&gt;
 I don't think the vectors will affect this?
&lt;denchmark-link:https://github.com/lingvisa&gt;@lingvisa&lt;/denchmark-link&gt;
 I think you can fix the immediate problem here by adding all the possible tags that jieba can produce in advance, just like a built-in pipeline component would do when it's loaded. The one that's missing here is . I looked for all the tags produced by jieba in :
&lt;denchmark-code&gt;['a', 'ad', 'ag', 'an', 'b', 'bg', 'c', 'd', 'df', 'dg', 'e', 'en', 'f', 'g', 'h', 'i', 'in', 'j', 'jn', 'k', 'l', 'ln', 'm', 'mg', 'mq', 'n', 'ng', 'nr', 'nrfg', 'nrt', 'ns', 'nt', 'nz', 'o', 'p', 'q', 'qe', 'qg', 'r', 'rg', 'rr', 'rz', 's', 't', 'tg', 'u', 'ud', 'ug', 'uj', 'ul', 'uv', 'uz', 'v', 'vd', 'vg', 'vi', 'vn', 'vq', 'w', 'x', 'y', 'yg', 'z', 'zg']
&lt;/denchmark-code&gt;

I can't replicate this kind of problem with similar annotation like a rule-based lemmatizer that adds to the vocab using fork or spawn, but I've only tested in linux. I know that with distributed computing (obviously) you would need to sync the Vocabs/StringStores, but I haven't seen any reports that this is a problem for multiprocessing. It seems like it ought to be a problem, though, so I'm not sure why I didn't run into any problems with spawn...
		</comment>
		<comment id='4' author='lingvisa' date='2020-04-07T11:19:57Z'>
		You're probably right &lt;denchmark-link:https://github.com/adrianeboyd&gt;@adrianeboyd&lt;/denchmark-link&gt;
, I think I got confused this morning.
Oddly enough, I can't replicate the exact same error on Windows. With n_process=1 it just works, with n_process=2 it seems to hang for some time, then gives me this:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "&lt;string&gt;", line 1, in &lt;module&gt;
  File "envs\myspacy\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "envs\myspacy\lib\multiprocessing\spawn.py", line 115, in _main
    self = reduction.pickle.load(from_parent)
  File "vectors.pyx", line 24, in spacy.vectors.unpickle_vectors
    return Vectors().from_bytes(bytes_data)
  File "vectors.pyx", line 459, in spacy.vectors.Vectors.from_bytes
    util.from_bytes(data, deserializers, [])
  File "...\spacy\util.py", line 634, in from_bytes
    setter(msg[key])
  File "vectors.pyx", line 453, in spacy.vectors.Vectors.from_bytes.deserialize_weights
    self.data = srsly.msgpack_loads(b)
  File "envs\myspacy\lib\site-packages\srsly\_msgpack_api.py", line 29, in msgpack_loads
    msg = msgpack.loads(data, raw=False, use_list=use_list)
  File "envs\myspacy\lib\site-packages\srsly\msgpack\__init__.py", line 60, in unpackb
    return _unpackb(packed, **kwargs)
  File "_unpacker.pyx", line 191, in srsly.msgpack._unpacker.unpackb
MemoryError
Traceback (most recent call last):
  File "...\spacy\language.py", line 819, in pipe
    for doc in docs:
  File "...\spacy\language.py", line 872, in _multiprocessing_pipe
    proc.start()
  File "envs\myspacy\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "envs\myspacy\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "envs\myspacy\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "envs\myspacy\lib\multiprocessing\popen_spawn_win32.py", line 65, in __init__
    reduction.dump(process_obj, to_child)
  File "envs\myspacy\lib\multiprocessing\reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
BrokenPipeError: [Errno 32] Broken pipe
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='lingvisa' date='2020-04-07T17:15:58Z'>
		&lt;denchmark-link:https://github.com/adrianeboyd&gt;@adrianeboyd&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/svlandeg&gt;@svlandeg&lt;/denchmark-link&gt;
 Recently I just upgraded to 2.2.4, and I haven't seen this error. But I didn't had exactly the same code to test for some reason. So far so good. It might be due to some implementation of my own components.
		</comment>
		<comment id='6' author='lingvisa' date='2020-04-07T17:53:32Z'>
		Ah, hm, so maybe PR &lt;denchmark-link:https://github.com/explosion/spaCy/pull/5081&gt;#5081&lt;/denchmark-link&gt;
 did fix it? This might explain why we can't replicate either. Will close for now, but let us know if you run into trouble again!
		</comment>
		<comment id='7' author='lingvisa' date='2020-04-09T22:05:09Z'>
		&lt;denchmark-link:https://github.com/svlandeg&gt;@svlandeg&lt;/denchmark-link&gt;
 sorry to comment on a closed issue but i had a simple related question: Does spaCy indeed store some global state? in the StringStore? If i have multiple threads using, one of them may find a stored entry that came from the other thread?
		</comment>
		<comment id='8' author='lingvisa' date='2020-04-09T23:19:04Z'>
		Hi &lt;denchmark-link:https://github.com/pmallela&gt;@pmallela&lt;/denchmark-link&gt;
. There is currently indeed some global state in spaCy, e.g. the custom attributes in , and the vectors data. We're looking into refactoring some of that for spaCy 3.0. We've been seeing some bugs related to that and multi-processing. You're usually fine on Linux, where different child processes  of the parent thread, and inherit its state. Things become buggy when using macOS or Windows, where  is the default, and global state needs to be carefully defined and passed on.
If you run into specific issues with 2.2.4 or above - feel free to post a code snippet that helps us reproduce the issue!
		</comment>
		<comment id='9' author='lingvisa' date='2020-05-20T22:50:16Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>