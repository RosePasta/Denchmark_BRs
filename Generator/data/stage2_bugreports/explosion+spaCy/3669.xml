<bug id='3669' author='apuranik1' open_date='2019-05-03T00:30:02Z' closed_time='2019-07-23T21:08:44Z'>
	<summary>Segmentation fault when using span.as_doc() method</summary>
	<description>
&lt;denchmark-h:h2&gt;How to reproduce the behaviour&lt;/denchmark-h&gt;

I am trying to parse a number of reddit comments, dumped from pushshift.io. A file with a sample of these comments (about 130,000) can be found here: &lt;denchmark-link:https://drive.google.com/file/d/1eG4hNb7xAD_DcB4pcoUJIfZxxqeffNnL/view?usp=sharing&gt;comment_sample_100k.csv&lt;/denchmark-link&gt;
.
The following script is a heavily shortened version of the one I'm using to do the parsing: &lt;denchmark-link:https://gist.github.com/apuranik1/714440fee410e7f4356f417e6ac43125&gt;spacy_failure.py&lt;/denchmark-link&gt;
. There are two custom components in the pipeline: the tokenizer and the sentence boundary setter. The script builds the custom Language object, reads lines in csv format from stdin, parses them, and throws away the result. Running the script looks like:
$ python failure_script.py &lt; comment_sample_100k.csv
More than half the time, this command will eventually segfault. It's nondeterministic - sometimes it crashes in 30 seconds, other times it runs for 10 minutes, and sometimes it finishes successfully.
This is about as short as I could make the script while reproducing the error. Lots of seemingly unrelated lines will prevent the error from occurring. Certain small details that prevent the error are marked with comments. Certain other lines that aren't critical to the error but seem to increase the crash rate are also marked. Occasionally, instead of a segfault, the CSV reader will be corrupted and raise an exception.
My best guess is the issue is related to resource cleanup of the object returned by sent.as_doc().to_bytes(). Changing where the Sentence object gets garbage collected seems to change the outcome.
I'm using spaCy 2.1.0, but I've replicated it on 2.1.3 as well.
&lt;denchmark-h:h2&gt;Info about spaCy&lt;/denchmark-h&gt;


spaCy version: 2.1.0
Platform: Darwin-18.2.0-x86_64-i386-64bit
Python version: 3.7.2
Models: en

	</description>
	<comments>
		<comment id='1' author='apuranik1' date='2019-05-03T15:42:00Z'>
		Thanks for the report.
Could you try avoiding the span.as_doc() call? I've had trouble with this before, and it doesn't work the way it was originally intended. Originally I wanted it to be a zero-copy operation, but that didn't work. I'm very suspicious that this could be where the bug is, as it's a quite untested method that had bugs previously.
You might find the serialization code here useful: &lt;denchmark-link:https://github.com/explosion/spaCy/blob/master/spacy/tokens/_serialize.py&gt;https://github.com/explosion/spaCy/blob/master/spacy/tokens/_serialize.py&lt;/denchmark-link&gt;
 This lets you serialize a collection of  objects in an efficient format. It should let you then reconstruct your sentences, given the  objects.
		</comment>
		<comment id='2' author='apuranik1' date='2019-05-03T16:42:37Z'>
		Thanks for the response! In my own testing the code ran fine without the as_doc call, so I'm glad to hear that's the most likely source of the issue. I can work around calling that method.
		</comment>
		<comment id='3' author='apuranik1' date='2019-06-08T19:54:03Z'>
		I confirm there is something happening with the as_doc method :/ The pb seems to be in the Span.to_array method.
I used to do model(span.as_doc()) to apply a model like NER or TextCategorizer on a span but it seems to be breaking now. What would be the best way to apply a model on a span ?
		</comment>
		<comment id='4' author='apuranik1' date='2019-06-26T21:26:49Z'>
		Also running into this issue. Similar to &lt;denchmark-link:https://github.com/thomasopsomer&gt;@thomasopsomer&lt;/denchmark-link&gt;
, we're trying to run a  on each entity in  to do some extra processing
		</comment>
		<comment id='5' author='apuranik1' date='2019-06-27T22:26:34Z'>
		Did some more investigation here. It seems that the issue only shows itself when the DependencyParser component is enabled in the pipeline. With it disabled, we aren't able to reproduce the segfault.
EDIT: Did some more digging. Made a replica of  and removed  from &lt;denchmark-link:https://github.com/explosion/spaCy/blob/4f1dae1c6bc8f72bef2594dee6ee0a6c0fdb602c/spacy/tokens/span.pyx#L218&gt;the list of attrs&lt;/denchmark-link&gt;
 and the segfaults stop don't happen!
		</comment>
		<comment id='6' author='apuranik1' date='2019-07-15T22:30:41Z'>
		&lt;denchmark-link:https://github.com/dpraul&gt;@dpraul&lt;/denchmark-link&gt;
 : yep, you're right. I tried fixing this in &lt;denchmark-link:https://github.com/explosion/spaCy/pull/3969&gt;#3969&lt;/denchmark-link&gt;
: instead of simply removing all head information from the  (), we can just keep the ones that refer to tokens inside the .
So I am hopeful that this PR would fix your issue (and others above), too.
		</comment>
		<comment id='7' author='apuranik1' date='2019-08-22T21:42:28Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>