<bug id='835' author='acowlikeobject' open_date='2017-02-16T12:37:10Z' closed_time='2017-09-26T13:51:16Z'>
	<summary>How do I create a custom tokenizer (docs seem out of date)?</summary>
	<description>
&lt;denchmark-link:https://spacy.io/docs/usage/customizing-tokenizer&gt;The docs&lt;/denchmark-link&gt;
 say:
&lt;denchmark-code&gt;import re
from spacy.tokenizer import Tokenizer

prefix_re = re.compile(r'''[\[\("']''')
suffix_re = re.compile(r'''[\]\)"']''')
def create_tokenizer(nlp):
    return Tokenizer(nlp.vocab,
            prefix_search=prefix_re.search,
            suffix_search=suffix_re.search)

nlp = spacy.load('en', tokenizer=create_make_doc)
&lt;/denchmark-code&gt;

Since create_make_doc isn't defined, I'm guessing there's an error there?
I tried:
&lt;denchmark-code&gt;nlp = spacy.load('en', create_make_doc=create_tokenizer)
&lt;/denchmark-code&gt;

But get a TypeError: __init__() takes at least 5 positional arguments (1 given).
Also tried:
&lt;denchmark-code&gt;nlp = spacy.load('en', tokenizer=create_tokenizer)
nlp('Hello world!')
&lt;/denchmark-code&gt;

But got a AttributeError: 'str' object has no attribute 'vocab', suggesting load() is expecting an actual Tokenizer, not a callback to create one.
What's a minimal example of creating a custom tokenizer?

Operating System: Debian 8
Python Version Used: 3.5
spaCy Version Used: 1.6.0

	</description>
	<comments>
		<comment id='1' author='acowlikeobject' date='2017-02-16T18:44:48Z'>
		I've noticed this as well.
With version 1.6.0 of SpaCy perhaps the following is correct?
&lt;denchmark-code&gt;def create_tokenizer(nlp):
    prefix_re = spacy.util.compile_prefix_regex(nlp.Defaults.prefixes)
    suffix_re = spacy.util.compile_suffix_regex(nlp.Defaults.suffixes)
    infix_re = spacy.util.compile_infix_regex(nlp.Defaults.infixes)

    tokenizer = Tokenizer(nlp.vocab,
                          nlp.Defaults.tokenizer_exceptions,
                          prefix_re.search,
                          suffix_re.search,
                          infix_re.finditer,
                          token_match = None)
    make_doc = lambda text: tokenizer(text)
    return make_doc

nlp = spacy.load('en', create_make_doc=create_tokenizer)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='acowlikeobject' date='2017-02-16T22:57:48Z'>
		Thanks.
I think the code is wrong here, not the docs --- create_make_doc is supposed to work but doesn't. If you have time to do the patch + test I'd appreciate it. Otherwise I'll likely get to it over the next couple of weeks.
		</comment>
		<comment id='3' author='acowlikeobject' date='2017-02-16T23:14:31Z'>
		There may be a problem in the code, although, the example I included in my response to the original issue does work. There are a couple of errors in the docs:


create_tokenizer is not a named parameter, create_make_doc is. The example in the documentation has these reversed.


in 1.6.0, Tokenizer now requires 5 positional parameters
def __init__(self, Vocab vocab, rules, prefix_search, suffix_search, infix_finditer, token_match=None):


(the docs do not indicate how rules should be constructed, infix_finditer as well (although the naming of infix_finditer gives a strong hint)
		</comment>
		<comment id='4' author='acowlikeobject' date='2017-02-17T00:16:20Z'>
		Thanks &lt;denchmark-link:https://github.com/rappdw&gt;@rappdw&lt;/denchmark-link&gt;
 .  I'll try your snippet.


So, rules are the exceptions added via tokenizer().add_special_case(), correct?


What is token_match?  The source says A boolean function matching strings that becomes tokens, but would be great to see how it fits into tokenizer_pseudo_code().


Just to make sure I understand the pieces, say I'd like to customize the tokenizer to separate numbers and strings. E.g., '6miles' -&gt; ('6', 'miles') and '2y' -&gt; ('2',  'y').  Because I can't use generic patterns in add_special_case() (like (\d+)([A-Za-z]+) and then use the first capture group to create the first token), seems like my best bet would be the suffixes?  Change the (?&lt;=[0-9])km that's already there to (?&lt;=[0-9])[A-Za-z]+?


(Sorry for jamming point &lt;denchmark-link:https://github.com/explosion/spaCy/pull/3&gt;#3&lt;/denchmark-link&gt;
 in this issue!)
		</comment>
		<comment id='5' author='acowlikeobject' date='2017-09-25T19:41:15Z'>
		I just came across this Issue and have tracked down the answers in the source:


So, rules are the exceptions added via tokenizer().add_special_case(), correct?


Correct.


What is token_match? The source says A boolean function matching strings that becomes tokens, but would be great to see how it fits into tokenizer_pseudo_code().


It's useful for URLs and numbers with commas/dots, so that they don't get split into myriad tiny tokens. I'll post a PR that demonstrates usage in the doc.


Just to make sure I understand the pieces, say I'd like to customize the tokenizer to separate numbers and strings.


The way you suggested does work. I tested by modifying the doc example, changing only the suffix_re line. compile_suffix_regex is just joining multiple regex for us.
prefix_re = re.compile(r'''[\[\("']''')
suffix_re = spacy.util.compile_suffix_regex(Language.Defaults.suffixes + (r'''(?&lt;=[0-9])(?:[A-Za-z]+)''',))
infix_re = re.compile(r'''[-~]''')
def create_tokenizer(nlp):
    return Tokenizer(nlp.vocab,
            rules={},
            prefix_search=prefix_re.search,
            suffix_search=suffix_re.search,
            infix_finditer=infix_re.finditer
            )
However it seems you can use regex with infix_finditer as you initially wanted, and I would find this a bit more elegant.
prefix_re = re.compile(r'''[\[\("']''')
suffix_re = re.compile(r'''[\]\)"']''')
infix_re = spacy.util.compile_infix_regex((r'''[-~]''', r'''(?&lt;=[0-9])(?=[A-Za-z])'''))
def create_tokenizer(nlp):
    return Tokenizer(nlp.vocab,
            rules={},
            prefix_search=prefix_re.search,
            suffix_search=suffix_re.search,
            infix_finditer=infix_re.finditer
            )

list(nlp(u'Drink 8cups water every-day.'))
# [Drink, 8, cups, water, every-day, .]
		</comment>
		<comment id='6' author='acowlikeobject' date='2018-05-08T16:27:41Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>