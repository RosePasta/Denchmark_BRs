<bug id='2061' author='meshiguge' open_date='2018-03-06T07:32:31Z' closed_time='2018-05-21T00:01:06Z'>
	<summary>bug with merge method for noun_chunks</summary>
	<description>
&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


Operating System:  osx
Python Version Used: 2.7
spaCy Version Used:  2.0
Environment Information: en

&lt;denchmark-h:h4&gt;I want to merge noun_chunks in doc , then get a new doc tokens for match ,but the result lemma_ is not expected&lt;/denchmark-h&gt;

according to the explanation &lt;denchmark-link:https://spacy.io/api/doc#merge&gt;doc.merge &lt;/denchmark-link&gt;


Attributes to assign to the merged token. By default, attributes are inherited from the syntactic root token of the span.

but the merged token is not equal to root token's  attributes
nlp = spacy.load('en', disable=['ner', 'textcat'] )
doc = nlp(u'A phrase with another phrase occurs.') 
for np in doc.noun_chunks:
    print (np.text , np.root.tag_,  np.root.lemma_)
# output: 
""" 
(u'A phrase', u'NN', u'phrase')
(u'another phrase', u'NN', u'phrase')

"""

for np in doc.noun_chunks:
    np.merge(  tag=np.root.tag_, lemma=np.root.lemma_, ent_type=np.root.ent_type_ )

for t in doc:
        print ( t.text ,  t.tag_, t.lemma_  )
# output : 
""" 
###  ( t.text ,  t.tag_, t.lemma_  )
(u'A phrase', u'NN', u'a')  # bug  
(u'with', u'IN', u'with')
(u'another phrase', u'NN', u'another')  # bug
(u'occurs', u'VBZ', u'occur')  
(u'.', u'.', u'.')
""" 
&lt;denchmark-h:h4&gt;But if I just use lemma=np.root.lemma_ seeds get the correct result&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;for np in doc.noun_chunks:
    np.merge(  lemma=np.root.lemma_  )

for t in doc:
        print ( t.text ,  t.tag_, t.lemma_  )
"""
(u'A phrase', u'DT', u'phrase')
(u'with', u'IN', u'with')
(u'another phrase', u'DT', u'phrase')
(u'occurs', u'VBZ', u'occur')
(u'.', u'.', u'.')
"""
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='meshiguge' date='2018-03-08T10:26:42Z'>
		expected result achieved if i separate  np.merge(  tag=np.root.tag_, lemma=np.root.lemma_) to 2 steps :
for np in doc.noun_chunks:
    root_lemma=np.root.lemma_
    np.merge(  tag=np.root.tag_ )
    np.merge( lemma= root_lemma)

for t in doc:
        print ( t.text ,  t.tag_, t.lemma_  )
"""
(u'A phrase', u'NN', u'phrase')
(u'And', u'CC', u'and')
(u'another phrase', u'NN', u'phrase')
(u'occurs', u'VBZ', u'occur')
(u'.', u'.', u'.')

"""
		</comment>
		<comment id='2' author='meshiguge' date='2018-05-21T00:01:06Z'>
		This will be fixed by the new retokenizer (see &lt;denchmark-link:https://github.com/explosion/spaCy/issues/1487&gt;#1487&lt;/denchmark-link&gt;
), which will be included in the upcoming v2.1.0 alpha. You can then do the following:
with doc.retokenize() as retokenizer:
    for np in doc.noun_chunks:
        attrs = {'TAG': np.root.tag_, 'LEMMA': np.root.lemma_, 'ENT_TYPE': np.root.ent_type_}
        retokenizer.merge(np, attrs=attrs)
An experimental version of this is already included in the latest release â€“ but it doesn't yet support string values, so you'll need to supply all attributes and values as integers (e.g. via the StringStore). Alternatively, you can also test the feature on the develop branch.
		</comment>
		<comment id='3' author='meshiguge' date='2018-05-23T23:02:00Z'>
		&lt;denchmark-link:https://github.com/ines&gt;@ines&lt;/denchmark-link&gt;
 can you confirm if the fix applies to the scenario below as well? I'm not using s, just merging s. I trained a model with custom semantics that gives me the following tokens for input :
&lt;denchmark-code&gt;TEXT        LEMMA       POS   TAG  DEP    HEAD
DUMMY_ROOT  dummy_root  VERB  VBZ  ROOT   DUMMY_ROOT
$           $           SYM   $    VALUE  100
100         100         NUM   CD   VALUE  DUMMY_ROOT
&lt;/denchmark-code&gt;

And after merging the last two tokens, the merged one looks like this (I'm omitting DUMMY_ROOT):
&lt;denchmark-code&gt;TEXT        LEMMA       POS   TAG  DEP    HEAD
$100        $           SYM   $    dobj   DUMMY_ROOT
&lt;/denchmark-code&gt;

Since $ is a child of 100 when they're still separate tokens I expected LEMMA to be "100", POS to be "NUM" and TAG to be "CD"... is that right?
		</comment>
		<comment id='4' author='meshiguge' date='2018-06-22T23:35:32Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>