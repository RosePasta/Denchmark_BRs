<bug id='2203' author='stephenfrench9' open_date='2018-04-10T21:47:16Z' closed_time='2019-03-11T00:31:34Z'>
	<summary>Saving and loading the parse of a large document corrupts lemmas.</summary>
	<description>
We are trying to parse a lot of text and save the parses. We notice that when save and load the parse for a large document, some of the lemmas have been corrupted.  In parsing 240K tokens, we find about 8K corruptions.
If I parse a large enough document using spaCy, then save it, and then load it, the saved-and-loaded version is different than the original version. This only happens when the analyzed text is sufficiently large. The code below reproduces the error.
def min_repro(text_filename) -&gt; None:
    sp = spacy.load('en_core_web_lg')

    # tiny -- OK
    # txt = 'I hope this serializes correctly.

    # small -- OK
    # txt = "sentence 1 is interesting . when he , barboncito and manuelito arrived , they found james henry carleton 's experiment of resettlement a complete failure . here is another sentence ."

    # large -- bug
    with open(text_filename) as f:
        txt = f.read()

    doc1 = sp(txt)
    doc2 = Doc(sp.vocab).from_bytes(doc1.to_bytes())

    arr1 = msgpack.loads(doc1.to_bytes())['array_body']
    arr2 = msgpack.loads(doc2.to_bytes())['array_body']
    print(sum(arr1 == arr2))
When I run this code over wikisample.txt, I see that there are 48,330 tokens that were included in the parse. For columns 1,2,4,5,6,7,8 I see that the saved-and-loaded version is the same as the original version. For column 3, I see that the saved-and-load version was the same as the original version only 48,324 times.
spacy/tokens/doc.pyx:772 leads me to believe that the third column is the lemma column.
&lt;denchmark-h:h3&gt;Output&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;[48330 48330 48324 48330 48330 48330 48330 48330]
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


Operating System: Ubuntu 16.04
Python Version Used: 3.6.5
spaCy Version Used: 2.0.11
Environment Information:

File to reproduce the error:
&lt;denchmark-link:https://github.com/explosion/spaCy/files/1896496/wikisample.txt&gt;wikisample.txt&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='stephenfrench9' date='2019-03-10T23:16:27Z'>
		Sorry for only getting to this now. Here's a more straightforward example without msgpack and with the small model that shows the same problem:
import spacy
from spacy.tokens import Doc

nlp = spacy.load("en_core_web_sm")
with open('wikisample.txt') as f:
    txt = f.read()
doc1 = nlp(txt)
doc2 = Doc(nlp.vocab).from_bytes(doc1.to_bytes())
arr1 = doc1.to_array(["ORTH", "LEMMA"])
arr2 = doc2.to_array(["ORTH", "LEMMA"])
orth_counts, lemma_counts = sum(arr1 == arr2)
print(orth_counts, lemma_counts)
assert orth_counts == lemma_counts
And here are the ones it fails on, i.e. where the lemmas don't match:
&lt;denchmark-code&gt;-PRON-  i
would   d
will    'll
will    'll
&lt;/denchmark-code&gt;

So this seems to be a problem with how the lemmas are set in Doc.from_array, which is called within Doc.from_bytes. I think we already have this figured out, so there should be a fix soon.
		</comment>
		<comment id='2' author='stephenfrench9' date='2019-04-10T01:22:07Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>