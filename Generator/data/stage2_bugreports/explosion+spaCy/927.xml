<bug id='927' author='codedeft' open_date='2017-03-26T12:19:00Z' closed_time='2017-05-07T16:23:18Z'>
	<summary>Deserialization fails with a new model instance.</summary>
	<description>
Running the following code results in an error:
&lt;denchmark-code&gt;import spacy
import en_core_web_md

nlp = en_core_web_md.load()
doc = nlp(u'North America  United States Communications Equipment (GICS)  Handheld Devices (Citi)')
serialized = doc.to_bytes()
new_nlp = en_core_web_md.load()
new_doc = spacy.tokens.doc.Doc(new_nlp.vocab)
new_doc.from_bytes(serialized)
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
&lt;ipython-input-53-d77de8988116&gt; in &lt;module&gt;()
      7 new_nlp = en_core_web_md.load()
      8 new_doc = spacy.tokens.doc.Doc(new_nlp.vocab)
----&gt; 9 new_doc.from_bytes(serialized)

/data/anaconda3/lib/python3.6/site-packages/spacy/tokens/doc.pyx in spacy.tokens.doc.Doc.from_bytes (spacy/tokens/doc.cpp:12323)()

/data/anaconda3/lib/python3.6/site-packages/spacy/serialize/packer.pyx in spacy.serialize.packer.Packer.unpack_into (spacy/serialize/packer.cpp:6258)()

/data/anaconda3/lib/python3.6/site-packages/spacy/serialize/packer.pyx in spacy.serialize.packer.Packer._char_decode (spacy/serialize/packer.cpp:7791)()

/data/anaconda3/lib/python3.6/site-packages/spacy/tokens/doc.pyx in spacy.tokens.doc.Doc.push_back (spacy/tokens/doc.cpp:10057)()

AssertionError: 

&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


Operating System: Ubuntu 14.04.5 LTS
Python Version Used: 3.6 (anaconda)
spaCy Version Used: 1.7.2
Environment Information:

	</description>
	<comments>
		<comment id='1' author='codedeft' date='2017-03-26T12:25:32Z'>
		Relevant discussion from gitter:

Sung H. Chung @codedeft 07:36
Hi there. I am using spaCy to process English text documents that contain not just ASCII but a fair number of unicode characters as well.
In order to save time, I process them using the en_core_web_md model and then serialize them using to_bytes and then save them so that I can retrieve them later.
The problem is that, if documents contain unicode characters, it seems that retrieving them later and then deserializing them with 'from_bytes' result in all kinds of errors, such as AssertError during parsing or totally different doc from the one I parsed.
I also noticed that the serialized values in two different machines are different if they were generated from the doc that was generated from the same unicode document.
This doesn't seem to be the case if the document only contains ascii characters.
Matthew Honnibal @honnibal 10:25
@codedeft Sounds like a bug. I'm surprised this hasn't come up yet.
To give you some context on how the code is working:
The vocab maps strings to ints, and the serializer uses a Huffman tree so that low-integer values are given shorter binary strings. For out-of-vocabulary words, we back off to characters.
This is where the bug becomes plausible: if we're serialising and deserialising characters that weren't seen by the encoder, then we'll get nonsensical results.
I think there are two problems here:

Whatever the expected behaviour should be, the observed behaviour can't be correct. So, there's at least a should-raise-error bug here
I think you need to be saving and loading a frequencies file for the serialiser, which would prevent that. But the docs don't describe this I think, and honestly I have to look at the code right now to check how this is done.
These decisions in the serialiser should be reviewed. Could we instead save out the string, and serialise the lengths of the tokens? How much larger would this make things? I think it would be better to make the serialised representation 2x bigger in order to make the code simpler and the serialised data more robust. We would probably also deserialise quicker.

Matthew Honnibal @honnibal 10:39
Notes to self/others on how the code works...
https://github.com/explosion/spaCy/blob/master/spacy/serialize/packer.pyx
The character frequencies aren't saved or loaded anywhere, which really confused me. Instead, they're reconstucted from the provided vocabulary object. This works so long as you save out the vocab after processing, load it back, and make sure that the serialiser is created with this newly loaded vocab.
As of 1.7 there's now experimental support for pickling the vocab. So you can try:
with (nlp.path / 'vocab' / 'vocab.pickle').open('wb') as file_:
pickle.dump(vocab, file_, -1)
print("mode_dir = ", nlp.path)
Later
def load_nlp(model_dir):
with (model_dir / 'vocab' / 'vocab.pickle').open('rb') as file_:
vocab = pickle.load(file_)
nlp = spacy.load('en', vocab=vocab)
return nlp
The serializer is actually a child of the Vocab, so in theory this would work. In practice I expect there will be at least one problem.
13:20
@honnibal Thanks for the detailed answer. I'm not knowledgeable about the details you mentioned, but do you mean that this could happen if there are words in the document that are not in the vocab object?
What I found interesting is that as long as I'm using the same nlp instance within the same python process, I can serialize/deserialize fine.
E.g.,
nlp = en_core_web_md.load()
doc = nlp(content)
serialized = doc.to_bytes()
new_doc = spacy.tokens.doc.Doc(nlp.vocab)
new_doc.from_bytes(serialized) # same doc.
The problem happens if I use a new python instance to load from a file and then run it.
nlp = en_core_web_md.load()
... load serialized from the disk ...
new_doc = spacy.tokens.doc.Doc(nlp.vocab)
new_doc.from_bytes(serialized) # Errors
So does it mean that nlp.vocab gets updated with new vocabulary when I use nlp to parse a new document containing new words?
Thomas @thomaskern 13:25
@codedeft yes
Sung H. Chung @codedeft 13:49
@thomaskern @honnibal I see. Hmm... I'm currently using multiple machines to parse/serialize millions of documents in parallel. Does this mean that every machine will have a different vocab since they are all looking at different documents, and thus different word mapping for serialization?
Is there a way such that deserialization would only depend on my initial vocab (en_core_web_md)?
Although spaCy is super fast, it seems to me that deserializing previous result is still much faster. So we'd prefer not to nlp-process each document each time we want to explore some aspects of the spaCy's outputs
Matthew Honnibal @honnibal 13:52
@codedeft The problem only occurs when different vocabs see different unicode characters. The serializer is set up to correctly "fall back" to representing words that weren't in the original vocabulary...but it does that by taking this per-character representation. If the input has a character it hasn't seen, we hit this failure.
That's why it works fine in-memory like that --- it's seen the characters before.
Hmm. I wonder whether I've got this wrong actually. I think I thought of this, and it should be serialising the utf8 representation.
Yes the serializer works on the utf8 representation
so my analysis can't be correct
Matthew Honnibal @honnibal 13:58
@codedeft Could you try to find a minimal example, and post the bug on the tracker?
Sung H. Chung @codedeft 14:00
@honnibal I just found that this happens not just with utf-8, but with ASCII documents as well.
E.g., A machine parses a set of documents, serializes them and stores the serialized results to a shared file system. These documents contain a lot of terms that probably won't exist in the en_core_web_md vocab.
A different machine later comes in and tries to read from the shared file system and deserialize a subset of those documents. Some of these deserialization fails, or the results look bogus.
@honnibal I'll do that soon. Thanks
Sung H. Chung @codedeft 14:19
@honnibal I found a minimal example and filed an issue using the github issue tracker. Let me know. Thanks!

		</comment>
		<comment id='2' author='codedeft' date='2017-04-05T14:13:45Z'>
		I've opened an issue on Textacy's repo that seems related (see the issue referenced by Burton above).
For convenience, below are some example strings that cause problems in my case (dealing with "ŵ" and "ŷ" from Welsh named entities in English texts). The code is based on &lt;denchmark-link:https://github.com/CodeFT&gt;@CodeFT&lt;/denchmark-link&gt;
's minimal example, but note that each string fails differently.
&lt;denchmark-code&gt;import spacy

def test(problem_text):
    nlp = spacy.load("en") 
    doc = nlp(problem_text)
    serialized = doc.to_bytes()

    new_nlp = spacy.load("en") 
    new_doc = spacy.tokens.doc.Doc(new_nlp.vocab)
    new_doc.from_bytes(serialized)
    print (new_doc)

problem_texts = ["tŷ is house in Welsh and dŵr is water", # prints tŰ arhouse in Welsh and dŵr is water
                 "tŷ is house in Welsh", # Exception: Buffer exhausted at 4/5 symbols read.
                 "paying tribute to the work of the Tŷ Gwyn Education Centre"] # AssertionError

for problem_text in problem_texts:
    test(problem_text) 
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='codedeft' date='2017-04-05T14:55:26Z'>
		The resolution we came to on the Gitter chat is that we should retire this Serializer subsystem, and just store (string, numpy_array) tuples. The user can control the header in the numpy array.
This already works. If you're happy to repeat the tokenization, you can just store [HEAD, DEP, TAG, ENT_TYPE, ENT_IOB]. If you'd rather store the tokens explicitly, you can store LENGTH, SPACY as well.
Multiple documents could be serialized as a (string, annotations, doc_lengths) triple, where doc_lengths specifies the length in tokens of the original documents. This should compress excellently using numpy's built-in gzip functionality, because there are such limited ranges of values for all fields (HEAD is specified as an offset, and we're writing the lengths in characters --- also very low entropy.)
		</comment>
		<comment id='4' author='codedeft' date='2017-04-05T15:24:12Z'>
		Hi &lt;denchmark-link:https://github.com/honnibal&gt;@honnibal&lt;/denchmark-link&gt;
 , thanks for the suggestion! Are you planning to incorporate this process into spacy's API? Also, dumb question: How are you saving the  tuples to disk? I tried a couple things, including , but it didn't work. I'm probably missing something easy and obvious... 
		</comment>
		<comment id='5' author='codedeft' date='2017-04-05T15:28:08Z'>
		I was thinking we'd keep the same API and just replace the mechanics with this.
You could just use pickle to persist the data?
		</comment>
		<comment id='6' author='codedeft' date='2017-04-05T15:33:22Z'>
		Aha, I figured pickle would work, but the comment about numpy's built-in gzip functionality led me astray. Glad to hear that serialization will get switched out under the hood. Thanks for the quick replies!
		</comment>
		<comment id='7' author='codedeft' date='2017-05-07T16:23:18Z'>
		Closing this and making &lt;denchmark-link:https://github.com/explosion/spaCy/issues/1045&gt;#1045&lt;/denchmark-link&gt;
 the master issue. Work in progress for spaCy v2.0!
		</comment>
		<comment id='8' author='codedeft' date='2018-05-08T22:38:29Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>