<bug id='2657' author='romikforest' open_date='2018-08-11T12:30:59Z' closed_time='2018-09-12T13:36:57Z'>
	<summary>Doc Example with bad html tags: Html tags are not marked as stop words</summary>
	<description>
The sample from the docs &lt;denchmark-link:https://spacy.io/usage/linguistic-features&gt;https://spacy.io/usage/linguistic-features&lt;/denchmark-link&gt;
 (with small changes to demonstrate an error) :
&lt;denchmark-code&gt;import spacy
from spacy.matcher import Matcher
from spacy.tokens import Token

# we're using a class because the component needs to be initialised with
# the shared vocab via the nlp object
class BadHTMLMerger(object):
    def __init__(self, nlp):
        # register a new token extension to flag bad HTML
        Token.set_extension('bad_html', default=False, force=True)
        self.matcher = Matcher(nlp.vocab)
        self.matcher.add('BAD_HTML', None,
            [{'ORTH': '&lt;'}, {'LOWER': 'br'}, {'ORTH': '&gt;'}],
            [{'ORTH': '&lt;'}, {'LOWER': 'br/'}, {'ORTH': '&gt;'}])

    def __call__(self, doc):
        # this method is invoked when the component is called on a Doc
        matches = self.matcher(doc)
        spans = []  # collect the matched spans here
        for match_id, start, end in matches:
            spans.append(doc[start:end])
        for span in spans:
            span.merge()   # merge
            for token in span:
                token._.bad_html = True  # mark token as bad HTML
                doc.vocab[token.text].is_stop = True  # mark lexeme as stop word
        return doc

nlp = spacy.load('en_core_web_sm')
html_merger = BadHTMLMerger(nlp)
nlp.add_pipe(html_merger, last=True)  # add component to the pipeline
doc = nlp(u"Hello&lt;br&gt;world! &lt;br/&gt; This &lt;br&gt; is a test.")
for token in doc:
    print(token.text, token._.bad_html, token.is_stop)

&lt;/denchmark-code&gt;

Here the first &lt;br&gt; is not marked as stop word. And token.is_stop is not writable. How I can mark all  tags as stop words?
spacy 2.0.11, python Python 3.6.6 :: Anaconda custom (64-bit), win 10 64 bit
	</description>
	<comments>
		<comment id='1' author='romikforest' date='2018-08-11T12:41:06Z'>
		Thanks for the report! It looks like this is potentially related to the caching of lexical attributes (which are also included in the model data)? The first instance of lexeme doesn't have the correct flags set, while any subsequent lexemes do. Also, if you create your nlp object from just the English language class with no model data loaded in, it works as expected:
from spacy.lang.en import English
nlp = English()
By the way, the reason  is not writable is that it's a property on the , the context-insensitive entry in the vocabulary. You can find some background on this (including planned changes in the upcoming version) in &lt;denchmark-link:https://github.com/explosion/spaCy/issues/2390&gt;#2390&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='romikforest' date='2018-08-21T21:44:13Z'>
		A different approach might avoid this problem: Use an HTML parser like BeautifulSoup4 to first parse the text as a fragment (ie., doesn't need DOCTYPE or  wrappers), then use get_text() to extract the text, which will yield the raw text minus all the HTML formatting. This can then be passed to nlp(...).
		</comment>
		<comment id='3' author='romikforest' date='2018-10-12T13:58:31Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>