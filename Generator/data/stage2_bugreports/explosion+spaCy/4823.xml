<bug id='4823' author='kormilitzin' open_date='2019-12-19T17:04:25Z' closed_time='2019-12-21T12:07:26Z'>
	<summary>CLI spacy train fails with large amount of data</summary>
	<description>
I am training NER model with 7 categories and the data set contains 200K examples (texts) with average 60K annotated spans per category. However spacy train fails if I use all data. When I randomly subsample, then it works normally. The error I receive when use all data:
$ python -m spacy train en ....

Training pipeline: ['ner']
Starting with blank model 'en'
Counting training words (limit=0)
Traceback (most recent call last):
File "/usr/lib/python3.6/runpy.py", line 193, in _run_module_as_main
"main", mod_spec)
File "/usr/lib/python3.6/runpy.py", line 85, in _run_code
exec(code, run_globals)
File "/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/spacy/main.py", line 33, in 
plac.call(commands[command], sys.argv[1:])
File "/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/plac_core.py", line 367, in call
cmd, result = parser.consume(arglist)
File "/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/plac_core.py", line 232, in consume
return cmd, self.func(*(args + varargs + extraopts), **kwargs)
File "/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/spacy/cli/train.py", line 230, in train
corpus = GoldCorpus(train_path, dev_path, limit=n_examples)
File "gold.pyx", line 224, in spacy.gold.GoldCorpus.init
File "gold.pyx", line 235, in spacy.gold.GoldCorpus.write_msgpack
File "gold.pyx", line 280, in read_tuples
File "gold.pyx", line 545, in read_json_file
File "gold.pyx", line 592, in _json_iterate
OverflowError: value too large to convert to int

Is there any way to overcome this problem? Thanks.
	</description>
	<comments>
		<comment id='1' author='kormilitzin' date='2019-12-20T08:53:56Z'>
		That does look like spaCy is crashing on the large training file. Could you provide a little more information to help us look into this:

the exact command you ran
which spaCy version you're using (from source or installed with pip / conda ? which version number ?)
how large (in MB or GB) your training file is on disk

		</comment>
		<comment id='2' author='kormilitzin' date='2019-12-21T08:40:16Z'>
		This is a duplicate of &lt;denchmark-link:https://github.com/explosion/spaCy/issues/4703&gt;#4703&lt;/denchmark-link&gt;
. I guess we should add a useful warning and there's really no reason not to change it to .
		</comment>
		<comment id='3' author='kormilitzin' date='2019-12-21T12:07:26Z'>
		Merging this with &lt;denchmark-link:https://github.com/explosion/spaCy/issues/4703&gt;#4703&lt;/denchmark-link&gt;
!
		</comment>
		<comment id='4' author='kormilitzin' date='2020-01-24T12:01:43Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>