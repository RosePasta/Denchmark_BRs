<bug id='3289' author='AmitMY' open_date='2019-02-18T12:12:55Z' closed_time='2019-02-21T08:43:46Z'>
	<summary>to_bytes serializes incorrectly if model uninitialized</summary>
	<description>
&lt;denchmark-h:h2&gt;How to reproduce the behavior&lt;/denchmark-h&gt;

I create an nlp object:
nlp = spacy.load('nl')
Just like your example, I add a text categorizer:
# add the text classifier to the pipeline if it doesn't exist
# nlp.create_pipe works for built-ins that are registered with spaCy
if 'textcat' not in self.nlp.pipe_names:
    textcat = self.nlp.create_pipe('textcat')
    self.nlp.add_pipe(textcat, last=True)
# otherwise, get it, so we can add labels to it
else:
    textcat = self.nlp.get_pipe('textcat')
I do some training, and stuff works nice, it reaches good accuracy.
I then save and load:
nlp = sapcy.blank('nl').from_bytes(self.nlp.to_bytes())
And I get:

KeyError: "[E001] No component 'textcat' found in pipeline. Available names: []"

If I try spacy.load instead of spacy.blank, I get:

Traceback (most recent call last):
File "/home/nlp/amit/classy/models/spacy/main.py", line 128, in 
res_model = model.train(test_data, test_data)
File "/home/nlp/amit/classy/models/runner.py", line 64, in train
return pickle.loads(model_runner)
File "/home/nlp/amit/classy/models/spacy/main.py", line 104, in setstate
recovery = spacy.load(self.lang_code).from_bytes(state["recovery"])
File "/home/nlp/amit/anaconda2/envs/classy/lib/python3.7/site-packages/spacy/language.py", line 690, in from_bytes
msg = util.from_bytes(bytes_data, deserializers, {})
File "/home/nlp/amit/anaconda2/envs/classy/lib/python3.7/site-packages/spacy/util.py", line 493, in from_bytes
setter(msg[key])
File "/home/nlp/amit/anaconda2/envs/classy/lib/python3.7/site-packages/spacy/language.py", line 689, in 
deserializers[i] = lambda b, proc=proc: proc.from_bytes(b, vocab=False)
File "pipeline.pyx", line 605, in spacy.pipeline.Tagger.from_bytes
File "/home/nlp/amit/anaconda2/envs/classy/lib/python3.7/site-packages/spacy/util.py", line 493, in from_bytes
setter(msg[key])
File "pipeline.pyx", line 603, in spacy.pipeline.Tagger.from_bytes.lambda14
File "pipeline.pyx", line 590, in spacy.pipeline.Tagger.from_bytes.load_model
File "/home/nlp/amit/anaconda2/envs/classy/lib/python3.7/site-packages/thinc/neural/_classes/model.py", line 351, in from_bytes
dest = getattr(layer, name)
AttributeError: 'FunctionLayer' object has no attribute 'W'

&lt;denchmark-h:h2&gt;Additional information&lt;/denchmark-h&gt;

I do this because I want to pickle my class that includes an instance of nlp, which for I use:
    def __getstate__(self):
        d = {**self.__dict__}
        if d["nlp"]:
            d["recovery"] = self.nlp.to_bytes()
        del d["optimizer"]
        del d["nlp"]

        return d

    def __setstate__(self, state):
        for k, v in state.items():
            self.__dict__[k] = v

        if "recovery" in state:
            recovery = spacy.load(self.lang_code).from_bytes(state["recovery"])
            del self.__dict__["recovery"]
            self.set_nlp(recovery)
&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


spaCy version: 2.0.18
Platform: Linux-3.10.0-957.5.1.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Python version: 3.7.1
Models: nl

	</description>
	<comments>
		<comment id='1' author='AmitMY' date='2019-02-18T12:50:40Z'>
		I think the problem here is that if you serialize the nlp object to bytes, you also need to store the language ('nl') and pipeline information separately and restore that before you load in the data. Otherwise, it gets confused because it's trying to add data to a 'textcat' component, but there's none in the pipeline.
This is also how spaCy does it internally: when you load a model from disk, it checks the meta.json to get the name of the language class and the names of the pipeline componemts. It then creates a blank language class, iterates over the pipeline component names, creates them and adds them to the pipeline and then calls from_disk to load in the data.
So you want to be doing something like this:
# Serialize
language = nlp.lang  # 'nl'
pipeline = nlp.pipe_names  # '['tagger', 'parser', 'textcat']' or something 
bytes_data = nlp.to_bytes()

# Deserialize
nlp = spacy.blank(language)
for pipe_name in pipeline:
    pipe = nlp.create_pipe(pipe_name)
    nlp.add_pipe(pipe)
nlp.from_bytes(bytes_data)
You could also take the nlp.meta and save it as a JSON file – this should contain all information you need in order to restore the pipeline and nlp object.

AttributeError: 'FunctionLayer' object has no attribute 'W'

This sounds like a different problem and possibly a model incompatibility. You might want to run python -m spacy validate and check that the model available as the shortcut 'nl' is up to date. Sometimes uninstalling, removing the shortcut link and reinstalling helps, too.
		</comment>
		<comment id='2' author='AmitMY' date='2019-02-18T13:23:13Z'>
		Thanks, &lt;denchmark-link:https://github.com/ines&gt;@ines&lt;/denchmark-link&gt;
 !
I changed the serializer and deserializer the way you said:
class SpacyPickle:
    def __init__(self):
        self.nlp = spacy.load('nl')

    def __getstate__(self):
        d = {**self.__dict__}
        if d["nlp"]:
            d["recovery"] = {
                "lang": self.nlp.lang,
                "pipeline": self.nlp.pipe_names,
                "bytes": self.nlp.to_bytes()
            }
        del d["nlp"]

        return d

    def __setstate__(self, state):
        for k, v in state.items():
            self.__dict__[k] = v

        if "recovery" in state:
            self.nlp = spacy.blank(state["recovery"]["lang"])

            for pipe_name in state["recovery"]["pipeline"]:
                self.nlp.add_pipe(self.nlp.create_pipe(pipe_name))

                self.nlp.from_bytes(state["recovery"]["bytes"])

            del self.__dict__["recovery"]
I validated:

lang = nl
pipeline = ['textcat'] (I remove all others. I also tried without removing)

And now it fails on the same from_bytes call with: (even without training first)

Traceback (most recent call last):
File "/home/nlp/amit/classy/models/spacy/main.py", line 134, in 
pickle.loads(dump)
File "/home/nlp/amit/classy/models/spacy/main.py", line 113, in setstate
nlp.from_bytes(state["recovery"]["bytes"])
File "/home/nlp/amit/anaconda2/envs/classy/lib/python3.7/site-packages/spacy/language.py", line 690, in from_bytes
msg = util.from_bytes(bytes_data, deserializers, {})
File "/home/nlp/amit/anaconda2/envs/classy/lib/python3.7/site-packages/spacy/util.py", line 493, in from_bytes
setter(msg[key])
File "/home/nlp/amit/anaconda2/envs/classy/lib/python3.7/site-packages/spacy/language.py", line 689, in 
deserializers[i] = lambda b, proc=proc: proc.from_bytes(b, vocab=False)
File "pipeline.pyx", line 220, in spacy.pipeline.Pipe.from_bytes
File "/home/nlp/amit/anaconda2/envs/classy/lib/python3.7/site-packages/spacy/util.py", line 493, in from_bytes
setter(msg[key])
File "pipeline.pyx", line 213, in spacy.pipeline.Pipe.from_bytes.load_model
File "/home/nlp/amit/anaconda2/envs/classy/lib/python3.7/site-packages/thinc/neural/_classes/model.py", line 335, in from_bytes
data = msgpack.loads(bytes_data, encoding='utf8')
File "/home/nlp/amit/anaconda2/envs/classy/lib/python3.7/site-packages/msgpack_numpy.py", line 184, in unpackb
return _unpackb(packed, **kwargs)
File "msgpack/_unpacker.pyx", line 195, in msgpack._unpacker.unpackb
File "msgpack/_unpacker.pyx", line 152, in msgpack._unpacker.get_data_from_buffer
TypeError: a bytes-like object is required, not 'bool'

Note, I just installed a new conda environment, isntalled spacy, and downloaded nl
		</comment>
		<comment id='3' author='AmitMY' date='2019-02-18T15:32:54Z'>
		
And now it fails on the same from_bytes call with: (even without training first)

What happens if you do train or at least call nlp.begin_training? If your model isn't initialized or trained, there's nothing to serialize. spaCy should definitely raise a better error here, though – I'll check if we already do that in spacy-nightly.
		</comment>
		<comment id='4' author='AmitMY' date='2019-02-18T16:46:32Z'>
		Cool. Only if I do the begin_training after initializing the pipeline the serialization works.
Close this if this satisfies you or if not if you want to make it better error :)
Thanks!
		</comment>
		<comment id='5' author='AmitMY' date='2019-02-21T08:43:45Z'>
		Thanks for the report. Updated  to fix the bug &lt;denchmark-link:https://github.com/ines&gt;@ines&lt;/denchmark-link&gt;
 filed a test for, where the serialization broke if the model was uninitialized.
		</comment>
		<comment id='6' author='AmitMY' date='2019-03-23T08:56:23Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>