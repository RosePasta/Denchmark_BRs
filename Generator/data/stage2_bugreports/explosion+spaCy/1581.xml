<bug id='1581' author='fabriciomaster55' open_date='2017-11-15T01:55:09Z' closed_time='2018-10-28T15:10:26Z'>
	<summary>python executable error 0xc0000005 \spacy\syntax\ner.cp36-win_amd64.pyd</summary>
	<description>
NER training routine, available with several training examples, the system returns an error:
Name of failed application: python.exe, version: 3.6.3150.1013, timestamp: 0x59d3d3a3
Name of failed module: ner.cp36-win_amd64.pyd, version: 0.0.0.0, timestamp: 0x5a0b671a
Exception code: 0xc0000005
Displacement of failure: 0x0000000000011a42
Process ID failed: 0x220c
Start time of failed application: 0x01d35d97e23b13a0
App path failed: C: \ Users \ fabricio \ AppData \ Local \ Programs \ Python \ Python36 \ python.exe
Path of failed module: C: \ Users \ fabricio \ AppData \ Local \ Programs \ Python \ Python36 \ lib \ site-packages \ spacy \ syntax \ ner.cp36-win_amd64.pyd
Report ID: ea1d9fe3-27f8-4931-874d-fd9bc9d5173e
Full name of failed package:
Application ID for the failed package:
Can someone please help me?
Thank you!
&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


Operating System: Windows 10
Python Version Used: 3.6.3
spaCy Version Used: 2.0
Environment Information: \spacy\syntax\ner.cp36-win_amd64.pyd

	</description>
	<comments>
		<comment id='1' author='fabriciomaster55' date='2017-11-20T15:28:38Z'>
		Same here, on Python 2.7 and Spacy 2.0.3:

Faulting application name: python.exe, version: 0.0.0.0, time stamp: 0x59e339e0
Faulting module name: ner.pyd, version: 0.0.0.0, time stamp: 0x5a0ed117
Exception code: 0xc0000005
Fault offset: 0x000000000000af0e
Faulting process id: 0x2860
Faulting application start time: 0x01d362138377a2b2
Faulting application path: C:\ProgramData\Anaconda2\python.exe
Faulting module path: C:\ProgramData\Anaconda2\lib\site-packages\spacy\syntax\ner.pyd

		</comment>
		<comment id='2' author='fabriciomaster55' date='2017-12-12T21:40:42Z'>
		Same problem. I am using Python 3.6 and Spacy 2.0.5...
Problem signature:
Problem Event Name:	APPCRASH
Application Name:	python.exe
Application Version:	3.6.2150.1013
Application Timestamp:	5970e8c6
Fault Module Name:	ner.cp36-win_amd64.pyd
Fault Module Version:	0.0.0.0
Fault Module Timestamp:	5a2fee4c
Exception Code:	c0000005
Exception Offset:	0000000000013972
OS Version:	6.3.9600.2.0.0.400.8
Locale ID:	1033
Additional Information 1:	6bd6
Additional Information 2:	6bd6f098f16f414b06de782d47bbca42
Additional Information 3:	54f8
Additional Information 4:	54f89d7e874d3c0f29695be086af330b
		</comment>
		<comment id='3' author='fabriciomaster55' date='2017-12-13T09:55:16Z'>
		I was able to solve my problem by segmenting the training in stages:
&lt;denchmark-code&gt;import random
from pathlib import Path
import spacy
from dados_treino import texto_teste,treino_pessoa,treino_org

def main(model=None, output_dir=r"e:\temp\model", n_iter=50):
   """Load the model, set up the pipeline and train the entity recognizer."""
   if model is not None:
       nlp = spacy.load(model)  # load existing spaCy model
       nlp.disable_pipes()
       print("Loaded model '%s'" % model)
   else:
       nlp = spacy.blank('pt')  # create blank Language class
       print("Created blank 'pt' model")


   # create the built-in pipeline components and add them to the pipeline
   # nlp.create_pipe works for built-ins that are registered with spaCy
   nlp.disable_pipes()
   if 'ner' not in nlp.pipe_names:
       ner = nlp.create_pipe('ner')
       nlp.add_pipe(ner)
   # otherwise, get it so we can add labels
   else:
       ner = nlp.get_pipe('ner', last=True)

   contpessoa,contorg,cont=0,0,0
   maxpessoa=len(treino_pessoa)
   maxorg=len(treino_org)

   while cont==0:
    if contpessoa&lt;maxpessoa:
       dados_treino = [treino_pessoa[contpessoa]]
       contpessoa+=1
       treino(ner, nlp, n_iter, dados_treino)

    if contorg &lt; maxorg:
        dados_treino = [treino_org[contorg]]
        contorg += 1
        treino(ner, nlp, n_iter, dados_treino)
    if contorg&gt;=maxorg and contpessoa&gt;=maxpessoa:
        cont=1

   # save model to output directory
   output_dir = Path(output_dir)
   if not output_dir.exists():
       output_dir.mkdir()
       nlp.to_disk(output_dir)
       print("Saved model to", output_dir)

   # test the saved model
   print("Loading from", output_dir)
   nlp2 = spacy.load(output_dir)
   doc = nlp2(texto_teste)
   print('Entities1', [(ent.text, ent.label_) for ent in doc.ents])
   print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])
   spacy.displacy.serve(doc, style='ent')


def treino(ner,nlp,n_iter,dados_treino):
   other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
   with nlp.disable_pipes(*other_pipes):  # only train NER
       optimizer = nlp.begin_training()

       # add labels
       for _, annotations in dados_treino:
           for ent in annotations.get('entities'):
               ner.add_label(ent[2])

               # get names of other pipes to disable them during training
           for itn in range(n_iter):
               random.shuffle(dados_treino)
               losses = {}
               for text, annotations in dados_treino:
                   nlp.update(
                       [text],  # batch of texts
                       [annotations],  # batch of annotations
                       drop=0.5,  # dropout - make it harder to memorise data
                       sgd=optimizer,  # callable to update weights
                       losses=losses)
               print(losses)


main()


&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='fabriciomaster55' date='2018-01-16T12:39:33Z'>
		Thanks for the code! Any insight into what's going on here?
		</comment>
		<comment id='5' author='fabriciomaster55' date='2018-02-19T09:21:21Z'>
		I tried to segment my training as well, but it still crashes often, even when I am literally training on a single instance.
		</comment>
		<comment id='6' author='fabriciomaster55' date='2018-03-29T20:15:43Z'>
		How long is the text you're processing? We've just added an error message for this and an explicit length limit, so I want to check it's not that.
		</comment>
		<comment id='7' author='fabriciomaster55' date='2018-06-04T08:02:38Z'>
		What is the length limit? How long can the example text be for the crash not to happen?
		</comment>
		<comment id='8' author='fabriciomaster55' date='2018-09-10T11:50:10Z'>
		Process finished with exit code -1073741819 (0xC0000005)
Please help: When I increase my training data size to 200 or more samples, I'm getting that error.
I'm trying to train an NER by adding new entity type to a Spanish model('es').
spaCy version: spaCy v2.0.12, Windows 10. Thanks in advance.
TRAIN_DATA = get_data(data_path)
LABEL = 'FRU'
out_dir = r'C:\Users\DataScience\OpinRankDatasetWithJudgments'
@plac.annotations(
model=("Model name. Defaults to blank 'en' model.", "option", "m", str),
output_dir=("Optional output directory", "option", "o", Path),
n_iter=("Number of training iterations", "option", "n", int))
def train(model=None, output_dir=None, n_iter=None):
"""Load the model, set up the pipeline and train the entity recognizer."""
if model is not None:
nlp = spacy.load(model)  # load existing spaCy model
print("Loaded model '%s'" % model)
else:
nlp = spacy.blank('en')  # create blank Language class
print("Created blank 'en' model")
&lt;denchmark-code&gt;# create the built-in pipeline components and add them to the pipeline
# nlp.create_pipe works for built-ins that are registered with spaCy
if 'ner' not in nlp.pipe_names:
    ner = nlp.create_pipe('ner')
    nlp.add_pipe(ner, last=True)
# otherwise, get it so we can add labels
else:
    ner = nlp.get_pipe('ner')

# add labels
ner.add_label(LABEL)
for _, annotations in TRAIN_DATA:
    for ent in annotations.get('entities'):
        ner.add_label(ent[2])

# get names of other pipes to disable them during training
other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
with nlp.disable_pipes(*other_pipes):  # only train NER
    optimizer = nlp.begin_training()
    for itn in range(n_iter):
        random.shuffle(TRAIN_DATA)
        losses = {}
        for text, annotations in TRAIN_DATA:
            nlp.update(
                [text],  # batch of texts
                [annotations],  # batch of annotations
                drop=0.5,  # dropout - make it harder to memorise data
                sgd=optimizer,  # callable to update weights
                losses=losses)
        print(losses)

# save model to output directory
if output_dir is not None:
    output_dir = Path(output_dir)
    if not output_dir.exists():
        output_dir.mkdir()
    nlp.to_disk(output_dir)
    print("Saved model to", output_dir)

    '''# test the saved model
    print("Loading from", output_dir)
    nlp2 = spacy.load(output_dir)
    for text, _ in TRAIN_DATA:
        doc = nlp2(text)
        print('Entities', [(ent.text, ent.label_) for ent in doc.ents])
        print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])'''
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='fabriciomaster55' date='2018-10-28T15:10:26Z'>
		&lt;denchmark-link:https://github.com/Nandee89&gt;@Nandee89&lt;/denchmark-link&gt;
 This should be fixed now  . Thanks for your help reporting.
See &lt;denchmark-link:https://github.com/explosion/spaCy/commit/ad068f51be6a2579c19c35bcd8b5c1767441ffc1&gt;ad068f5&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='fabriciomaster55' date='2018-11-27T15:27:49Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>