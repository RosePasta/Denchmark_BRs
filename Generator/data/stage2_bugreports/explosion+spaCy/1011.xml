<bug id='1011' author='wejradford' open_date='2017-04-24T03:35:47Z' closed_time='2017-05-07T16:24:01Z'>
	<summary>Doc.to_bytes() throws KeyError on label-names for rule-matched spans</summary>
	<description>
I'd like to run rule-based matching to augment the entities recognised by the model, then write the document out for downstream processing, but it looks like the serialiser can't handle labels I've added myself.
It looks as if it's missing the book-keeping in nlp.vocab.strings, so doesn't know how to dump these things out. Is there a neater way to do what I'm trying to, or add the book-keeping manually?
Cheers, Will.
&lt;denchmark-h:h2&gt;Info about spaCy&lt;/denchmark-h&gt;


spaCy version: 1.7.5
Platform: Darwin-16.5.0-x86_64-i386-64bit
Python version: 3.6.1
Installed models: en

&lt;denchmark-h:h2&gt;To replicate&lt;/denchmark-h&gt;

import spacy
from spacy.matcher import Matcher
nlp = spacy.load('en')
doc = nlp('The cat sat on the mat in Scotland.')
print('Entities\t{}'.format(doc.ents))

print('Dumping')
as_bytes = doc.to_bytes()

matcher = Matcher(nlp.vocab)
matcher.add_entity('ANIMAL')
matcher.add_pattern('ANIMAL', [{'ORTH': 'cat'}], label='ANIMAL')
matches = matcher(doc)
matches = list((None, label_id, start, end)
           for entity_key, label_id, start, end in matches)

print('Matches\t{}'.format(matches))

doc.ents = matches + [(None, e.label, e.start, e.end) for e in doc.ents]
print('Entities + matches\t{}'.format(doc.ents))

print('Dumping')
as_bytes = doc.to_bytes()
This outputs:
&lt;denchmark-code&gt;Entities	(Scotland,)
Dumping
Matches	[(None, 63489, 1, 2)]
Entities + matches	(cat, Scotland)
Dumping
Traceback (most recent call last):
  File "replication.py", line 23, in &lt;module&gt;
    as_bytes = doc.to_bytes()
  File "spacy/tokens/doc.pyx", line 607, in spacy.tokens.doc.Doc.to_bytes (spacy/tokens/doc.cpp:12152)
  File "spacy/serialize/packer.pyx", line 112, in spacy.serialize.packer.Packer.pack (spacy/serialize/packer.cpp:5770)
  File "spacy/serialize/huffman.pyx", line 63, in spacy.serialize.huffman.HuffmanCodec.encode (spacy/serialize/huffman.cpp:2554)
KeyError: 63489
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='wejradford' date='2017-04-24T10:19:09Z'>
		Thanks!
The general-purpose solution would be to persist and reload the Vocab, including the StringStore, in order to save/load the document. This is actually the workflow I imagined when I designed the serializer. However, it's not entirely convenient.
The current plan is to replace the existing serializer with a much simpler system that stores the document as a (text, array) tuple, with the annotations described in the array. However, you're totally right that this will still raise the problem of these missing strings. I'm not sure what to do about this.
In the meantime, the current mitigation for this problem would involve making sure your labels are entered into the StringStore, and that they're mapping to the same integer ID during both encoding and decoding.
		</comment>
		<comment id='2' author='wejradford' date='2017-04-24T11:12:40Z'>
		Cool. I could imagine some kind of overflow string store in the document
itself. Most of the time this would be fine and it means you don't need to
block on saving a model to serialise these things out. Although you might
find that the same extra string would get different IDs in different docs 

On 24 Apr 2017 8:19 p.m., "Matthew Honnibal" &lt;notifications@github.com&gt; wrote:

Thanks!

The general-purpose solution would be to persist and reload the Vocab,
including the StringStore, in order to save/load the document. This is
actually the workflow I imagined when I designed the serializer. However,
it's not entirely convenient.

The current plan is to replace the existing serializer with a much simpler
system that stores the document as a (text, array) tuple, with the
annotations described in the array. However, you're totally right that this
will still raise the problem of these missing strings. I'm not sure what to
do about this.

â€”
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub
&lt;&lt;denchmark-link:https://github.com/explosion/spaCy/issues/1011#issuecomment-296607272&gt;#1011 (comment)&lt;/denchmark-link&gt;
&gt;,
or mute
the thread
&lt;&lt;denchmark-link:https://github.com/notifications/unsubscribe-auth/ADbp0MlJLS0v9XFI0zwve8Nsi12dN4wmks5rzHchgaJpZM4NFqgb&gt;https://github.com/notifications/unsubscribe-auth/ADbp0MlJLS0v9XFI0zwve8Nsi12dN4wmks5rzHchgaJpZM4NFqgb&lt;/denchmark-link&gt;
&gt;
.
		</comment>
		<comment id='3' author='wejradford' date='2017-05-07T16:24:01Z'>
		Closing this and making &lt;denchmark-link:https://github.com/explosion/spaCy/issues/1045&gt;#1045&lt;/denchmark-link&gt;
 the master issue. Work in progress for spaCy v2.0!
		</comment>
		<comment id='4' author='wejradford' date='2018-05-08T22:38:25Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>