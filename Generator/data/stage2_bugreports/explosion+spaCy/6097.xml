<bug id='6097' author='ereday' open_date='2020-09-21T11:08:43Z' closed_time='2020-09-21T11:56:11Z'>
	<summary>nlp pipeline doesn't work when 'merge_noun_chunks' is given as parameter</summary>
	<description>
&lt;denchmark-h:h2&gt;How to reproduce the behaviour&lt;/denchmark-h&gt;

For many of the sentences in my dataset, I'm receiving the error below. I shared only one of them but I can provide many more if requested. I should also mention that I have tried this code snippet with many different spacy versions and unfortunately none of them have worked for me so far.
&lt;denchmark-code&gt;import spacy
nlp = spacy.load('de_core_news_lg')
nlp.add_pipe(nlp.create_pipe('merge_noun_chunks'))

text = 'Offenbar handelt es sich aber um ein Revanche gegen Deutschland, das zuvor den Bau von Ungarns Grenzzaun zu Serbien kritisiert hatte.'
doc = nlp(text)

&lt;/denchmark-code&gt;

Note that the code above works without any problem if I replace the word Ungarns with Ungarn in the sentence.
Error Log:
&lt;denchmark-code&gt;
Traceback (most recent call last):
  File "main.py", line 141, in find_subjects
    doc_sub = nlp2(sent.text)
  File "virtual-environments/spacy2.3/lib/python3.6/site-packages/spacy/language.py", line 449, in __call__
    doc = proc(doc, **component_cfg.get(name, {}))
  File "virtual-environments/spacy2.3/lib/python3.6/site-packages/spacy/pipeline/functions.py", line 27, in merge_noun_chunks
    retokenizer.merge(np, attrs=attrs)
  File "_retokenize.pyx", line 62, in spacy.tokens._retokenize.Retokenizer.merge
ValueError: [E102] Can't merge non-disjoint spans. 'Ungarns' is already part of tokens to merge. If you want to find the longest non-overlapping spans, you can use the util.filter_spans helper:
https://spacy.io/api/top-level#util.filter_spans

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "&lt;console&gt;", line 1, in &lt;module&gt;
  File "virtual-environments/spacy2.3/lib/python3.6/site-packages/spacy/language.py", line 449, in __call__
    doc = proc(doc, **component_cfg.get(name, {}))
  File "virtual-environments/spacy2.3/lib/python3.6/site-packages/spacy/pipeline/functions.py", line 27, in merge_noun_chunks
    retokenizer.merge(np, attrs=attrs)
  File "_retokenize.pyx", line 62, in spacy.tokens._retokenize.Retokenizer.merge
ValueError: [E102] Can't merge non-disjoint spans. 'Ungarns' is already part of tokens to merge. If you want to find the longest non-overlapping spans, you can use the util.filter_spans helper:
https://spacy.io/api/top-level#util.filter_spans
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Info about spaCy&lt;/denchmark-h&gt;


spaCy version: 2.3.2
Platform: Linux-5.7.16-200.fc32.x86_64-x86_64-with-fedora-32-Thirty_Two
Python version: 3.6.5

	</description>
	<comments>
		<comment id='1' author='ereday' date='2020-09-21T11:56:10Z'>
		Sorry, this is a bug in the German  iterator that leads to overlapping spans. This is a duplicate of &lt;denchmark-link:https://github.com/explosion/spaCy/issues/6073&gt;#6073&lt;/denchmark-link&gt;
, so I'll merge this issue with that one and close this for now. Thanks for the additional example!
		</comment>
	</comments>
</bug>