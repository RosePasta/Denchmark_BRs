<bug id='2044' author='garyblocks' open_date='2018-02-28T23:50:55Z' closed_time='2018-12-29T15:29:03Z'>
	<summary>Is there any way to set the vector size for nlp.pipe()?</summary>
	<description>
I notice the result from nlp(text) has a vector size of 384 while with nlp.pipe(texts), the returned vectors all have vector size 128, is there anyway to configure that?

Operating System: mac
Python Version Used: 3.5
spaCy Version Used: 2.0.9
Environment Information:

	</description>
	<comments>
		<comment id='1' author='garyblocks' date='2018-03-22T19:49:27Z'>
		I've noticed this as well -- switched my model to use nlp.pipe() to combat some of the known issues with parallelization, but now my model results are different.
		</comment>
		<comment id='2' author='garyblocks' date='2018-03-27T22:53:16Z'>
		This is a bug in __call__.
Incidentally are you finding these predicted vectors useful? They bloat the doc object representation and I'm not sure they're actually helpful.
		</comment>
		<comment id='3' author='garyblocks' date='2018-03-28T23:53:18Z'>
		&lt;denchmark-link:https://github.com/honnibal&gt;@honnibal&lt;/denchmark-link&gt;
 I would definitely prefer to have per-token vectors, if that's what you're asking.  If I didn't, I wouldn't use spacy for many projects. I currently have a model where this is Spacy's only purpose.
Perhaps you could make the per-token vector storage an optional part in the pipeline.
		</comment>
		<comment id='4' author='garyblocks' date='2018-12-29T15:29:03Z'>
		Should be fixed now, if you try in either v2.0.18 or v2.1.0
&lt;denchmark-link:https://github.com/jlmcgehee21&gt;@jlmcgehee21&lt;/denchmark-link&gt;
 Thanks for the input!
		</comment>
		<comment id='5' author='garyblocks' date='2019-01-28T16:05:26Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>