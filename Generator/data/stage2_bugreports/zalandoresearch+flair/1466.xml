<bug id='1466' author='kjans123' open_date='2020-03-06T16:40:49Z' closed_time='2020-03-06T17:07:15Z'>
	<summary>Bert/Roberta Errors</summary>
	<description>
Anytime I try to document embed any Bert or Roberta word embedding, I receive CUDA errors. This is very similar to issue &lt;denchmark-link:https://github.com/flairNLP/flair/issues/1432&gt;#1432&lt;/denchmark-link&gt;

Relevant code is below:
&lt;denchmark-code&gt;word_embeddings = [XLMRobertaEmbeddings('xlm-roberta-large')]

document_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(word_embeddings,
                                                                     hidden_size=512,
                                                                     reproject_words=False,
                                                                     reproject_words_dimension=512,
                                                                       bidirectional=True,
                                                                   )


import torch
embeds = []
for ii,i in enumerate(df_c['CMR_TEXT'].values):
    sent = Sentence(i)
    embed = document_embeddings.embed(sent)

    embeds.append(sent.get_embedding().cpu().data.numpy())
    del sent
    del embed
    torch.cuda.empty_cache()
    print(ii)
embeds = np.array(embeds)
np.save('xlm-rob.npy',embeds)
&lt;/denchmark-code&gt;

With Flair embeddings, the loop performs normally and I can save out any array of document embedding for use in another TF/Keras neural net.
Here are the last few lines of the error trace:
&lt;denchmark-link:https://user-images.githubusercontent.com/35381053/76102693-5e501700-5f9e-11ea-9577-afbb88791b2a.png&gt;&lt;/denchmark-link&gt;

Environment (please complete the following information):

OS: Windows 10 (Anaconda)
Version: flair 0.4.5

I have tried this on two separate GPU's (1080 Ti and RTX 2080). Both give CUDA errors.
The loop works 12 times and then errors out, regardless if Bert or Roberta embeddings are used.
	</description>
	<comments>
		<comment id='1' author='kjans123' date='2020-03-06T17:07:15Z'>
		Closing this issue. I'd forgotten about the 512 sequence limit on Bert and Roberta. Modified this one line of code:
&lt;denchmark-code&gt;sent = Sentence(i[:512])
&lt;/denchmark-code&gt;

works perfectly now
		</comment>
		<comment id='2' author='kjans123' date='2020-03-06T17:16:25Z'>
		You are limiting to 512 characters - it is safe, but the limit is 512 subtokens (about 500 words).
		</comment>
	</comments>
</bug>