<bug id='1270' author='jantrienes' open_date='2019-11-12T08:13:14Z' closed_time='2020-01-16T12:20:26Z'>
	<summary>CUDA out of memory since Flair 0.4.4</summary>
	<description>
Describe the bug
CUDA memory fills up during the first training epoch of Dutch NER model. The bug started to occur since flair-0.4.4. Training works as intended in flair-0.4.3
To Reproduce
from typing import List

from flair.data import Corpus
from flair.datasets import CONLL_03_DUTCH
from flair.embeddings import (PooledFlairEmbeddings, StackedEmbeddings,
                              TokenEmbeddings, WordEmbeddings)
from flair.models import SequenceTagger
from flair.trainers import ModelTrainer

corpus: Corpus = CONLL_03_DUTCH()
tag_type = 'ner'

tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)

embedding_types: List[TokenEmbeddings] = [
    WordEmbeddings('nl'),
    PooledFlairEmbeddings('dutch-forward', pooling='mean'),
    PooledFlairEmbeddings('dutch-backward', pooling='mean'),
]

embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)

tagger: SequenceTagger = SequenceTagger(hidden_size=256,
                                        embeddings=embeddings,
                                        tag_dictionary=tag_dictionary,
                                        tag_type=tag_type)

trainer: ModelTrainer = ModelTrainer(tagger, corpus)
trainer.train('output/models/conll03-ner/',
              train_with_dev=True,
              max_epochs=150)
Environment:

OS: Debian GNU/Linux 9.8 (stretch) x86_64, Kernel: 4.9.0-8-amd64
GPU: NVIDIA GeForce RTX 2080 Ti
CUDA Version: 10.1
Python: 3.7.3

Output of pip freeze:
&lt;denchmark-code&gt;atomicwrites==1.3.0
attrs==19.3.0
backcall==0.1.0
boto==2.49.0
boto3==1.10.15
botocore==1.13.15
bpemb==0.3.0
certifi==2019.9.11
cffi==1.13.2
chardet==3.0.4
Click==7.0
cloudpickle==1.2.2
cycler==0.10.0
Cython==0.29.14
dartsclone==0.6
decorator==4.4.1
Deprecated==1.2.7
docutils==0.15.2
flair==0.4.4
future==0.18.2
gensim==3.8.1
hyperopt==0.2.2
idna==2.8
importlib-metadata==0.23
ipython==7.6.1
ipython-genutils==0.2.0
jedi==0.15.1
jmespath==0.9.4
joblib==0.14.0
kiwisolver==1.1.0
kytea==0.1.4
langdetect==1.0.7
matplotlib==3.1.1
more-itertools==7.2.0
mpld3==0.3
natto-py==0.9.0
networkx==2.2
numpy==1.17.4
packaging==19.2
parso==0.5.1
pexpect==4.7.0
pickleshare==0.7.5
Pillow==6.2.1
pluggy==0.13.0
prompt-toolkit==2.0.10
ptyprocess==0.6.0
py==1.8.0
pycparser==2.19
Pygments==2.4.2
pymongo==3.9.0
pyparsing==2.4.5
pytest==5.2.2
python-dateutil==2.8.1
pytorch-transformers==1.2.0
regex==2019.11.1
requests==2.22.0
s3transfer==0.2.1
sacremoses==0.0.35
scikit-learn==0.21.3
scipy==1.3.2
segtok==1.5.7
sentencepiece==0.1.83
six==1.13.0
sklearn==0.0
smart-open==1.9.0
sortedcontainers==2.1.0
sqlitedict==1.6.0
SudachiPy==0.4.0
tabulate==0.8.5
tiny-tokenizer==3.0.1
torch==1.3.1
torchvision==0.4.2
tqdm==4.38.0
traitlets==4.3.3
transformers==2.1.1
urllib3==1.24.3
wcwidth==0.1.7
wrapt==1.11.2
zipp==0.6.0
&lt;/denchmark-code&gt;

Additional context
Training log:
&lt;denchmark-code&gt;2019-11-12 09:10:16,480 Reading data from /home/username/.flair/datasets/conll_03_dutch
2019-11-12 09:10:16,480 Train: /home/username/.flair/datasets/conll_03_dutch/ned.train
2019-11-12 09:10:16,480 Dev: /home/username/.flair/datasets/conll_03_dutch/ned.testa
2019-11-12 09:10:16,480 Test: /home/username/.flair/datasets/conll_03_dutch/ned.testb
2019-11-12 09:10:28,986 ----------------------------------------------------------------------------------------------------
2019-11-12 09:10:28,987 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): WordEmbeddings('nl')
    (list_embedding_1): PooledFlairEmbeddings(
      (context_embeddings): FlairEmbeddings(
        (lm): LanguageModel(
          (drop): Dropout(p=0.1, inplace=False)
          (encoder): Embedding(7632, 100)
          (rnn): LSTM(100, 2048)
          (decoder): Linear(in_features=2048, out_features=7632, bias=True)
        )
      )
    )
    (list_embedding_2): PooledFlairEmbeddings(
      (context_embeddings): FlairEmbeddings(
        (lm): LanguageModel(
          (drop): Dropout(p=0.1, inplace=False)
          (encoder): Embedding(7632, 100)
          (rnn): LSTM(100, 2048)
          (decoder): Linear(in_features=2048, out_features=7632, bias=True)
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=8492, out_features=8492, bias=True)
  (rnn): LSTM(8492, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=20, bias=True)
)"
2019-11-12 09:10:28,987 ----------------------------------------------------------------------------------------------------
2019-11-12 09:10:28,987 Corpus: "Corpus: 15806 train + 2895 dev + 5195 test sentences"
2019-11-12 09:10:28,987 ----------------------------------------------------------------------------------------------------
2019-11-12 09:10:28,987 Parameters:
2019-11-12 09:10:28,987  - learning_rate: "0.1"
2019-11-12 09:10:28,987  - mini_batch_size: "32"
2019-11-12 09:10:28,987  - patience: "3"
2019-11-12 09:10:28,987  - anneal_factor: "0.5"
2019-11-12 09:10:28,987  - max_epochs: "150"
2019-11-12 09:10:28,987  - shuffle: "True"
2019-11-12 09:10:28,987  - train_with_dev: "True"
2019-11-12 09:10:28,987  - batch_growth_annealing: "False"
2019-11-12 09:10:28,987 ----------------------------------------------------------------------------------------------------
2019-11-12 09:10:28,987 Model training base path: "output/models/conll03-ner-test"
2019-11-12 09:10:28,987 ----------------------------------------------------------------------------------------------------
2019-11-12 09:10:28,987 Device: cuda:0
2019-11-12 09:10:28,987 ----------------------------------------------------------------------------------------------------
2019-11-12 09:10:28,987 Embeddings storage mode: cpu
2019-11-12 09:10:28,988 ----------------------------------------------------------------------------------------------------
train mode resetting embeddings
train mode resetting embeddings
2019-11-12 09:10:29,408 epoch 1 - iter 0/585 - loss 49.54404449 - samples/sec: 4426.54
Traceback (most recent call last):
  File "embeddings/flair_evaluate_conll03-dutch.py", line 44, in &lt;module&gt;
    max_epochs=150)
  File "/home/username/.conda/envs/flair-test/lib/python3.7/site-packages/flair/trainers/trainer.py", line 325, in train
    loss.backward()
  File "/home/username/.conda/envs/flair-test/lib/python3.7/site-packages/torch/tensor.py", line 166, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/username/.conda/envs/flair-test/lib/python3.7/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 276.00 MiB (GPU 0; 10.73 GiB total capacity; 8.33 GiB already allocated; 121.56 MiB free; 1.43 GiB cached)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='jantrienes' date='2019-11-12T08:33:07Z'>
		&lt;denchmark-link:https://github.com/jantrienes&gt;@jantrienes&lt;/denchmark-link&gt;
 could you try using normal  instead of ? Does the error occur then as well?
		</comment>
		<comment id='2' author='jantrienes' date='2019-11-12T08:43:33Z'>
		&lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 As far as I can tell, the error does not occur when I use  instead of . I exited training after the second epoch. GPU memory remained stable at about 5gb once the first few batches completed.
		</comment>
		<comment id='3' author='jantrienes' date='2019-11-22T10:25:10Z'>
		In my case it also worked with FlairEmbeddings, but I receive a cuda out of memory error using  PooledFlairEmbeddings
		</comment>
		<comment id='4' author='jantrienes' date='2019-11-23T16:31:51Z'>
		Same here. Out of memory with Pooled. Any ideas whether this could be fixed any time soon??? :-) Thx!
		</comment>
		<comment id='5' author='jantrienes' date='2019-11-25T16:14:19Z'>
		Hi, I just wanted to report that I experienced the same (OOM when using pooled, fine using non-pooled). I was training on English NER using glove, and Flair news-forward and Flair news-backward
		</comment>
		<comment id='6' author='jantrienes' date='2019-12-17T11:14:57Z'>
		Any suggestions how to solve the memory issue with the pooled flairembeddings?
		</comment>
		<comment id='7' author='jantrienes' date='2020-01-07T18:58:45Z'>
		Hi all, I've just merged a PR that fixes the error in my local setup - can you check if it works for you? You can install the current master version like this:
pip install --upgrade git+https://github.com/zalandoresearch/flair.git
		</comment>
		<comment id='8' author='jantrienes' date='2020-01-08T15:51:28Z'>
		I upgraded to the latest master version and the error still persists with the code snippet that I posted in the original issue.
		</comment>
		<comment id='9' author='jantrienes' date='2020-01-08T19:47:04Z'>
		Hello &lt;denchmark-link:https://github.com/jantrienes&gt;@jantrienes&lt;/denchmark-link&gt;
 I've made another change. It seems to work now for me on a GPU runtime on Colab, can you check if it also works for you?
		</comment>
		<comment id='10' author='jantrienes' date='2020-01-09T07:10:22Z'>
		Hi &lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
, that change does indeed seem to fix the problem. GPU memory usage remains constant after the first epoch. Thanks for fixing this!
		</comment>
		<comment id='11' author='jantrienes' date='2020-01-09T14:54:05Z'>
		Great, happy to hear!
		</comment>
	</comments>
</bug>