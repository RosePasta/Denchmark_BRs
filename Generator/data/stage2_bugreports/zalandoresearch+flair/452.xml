<bug id='452' author='alipetiwala' open_date='2019-02-05T07:42:40Z' closed_time='2019-02-06T16:01:04Z'>
	<summary>TypeError: unsupported operand type(s) for +: 'int' and 'NoneType'`</summary>
	<description>
Getting following error while resuming a training model:
Describe the bug
`TypeError                                 Traceback (most recent call last)
 in ()
      2 
      3 trainer = ModelTrainer.load_from_checkpoint('resources/taggers/example-art/char/best-model.pt', 'SequenceTagger', corpus)
----&gt; 4 trainer.train('resources/taggers/example-art/resume-best-model.pt',checkpoint=True)

~/anaconda3/lib/python3.6/site-packages/flair/trainers/trainer.py in train(self, base_path, evaluation_metric, learning_rate, mini_batch_size, eval_mini_batch_size, max_epochs, anneal_factor, patience, anneal_against_train_loss, train_with_dev, monitor_train, embeddings_in_memory, checkpoint, save_final_model, anneal_with_restarts, test_mode, param_selection_mode, **kwargs)
    111             previous_learning_rate = learning_rate
    112 
--&gt; 113             for epoch in range(0 + self.epoch, max_epochs + self.epoch):
    114                 log_line(log)
    115 

TypeError: unsupported operand type(s) for +: 'int' and 'NoneType'`

To Reproduce
Resume existing model with .pt extension
Environment (please complete the following information):
Ubuntu 18.x
	</description>
	<comments>
		<comment id='1' author='alipetiwala' date='2019-02-05T07:59:42Z'>
		 def load_checkpoint(cls, model_file: Union[str, Path]):
        state = SequenceTagger._load_state(model_file)
        model = SequenceTagger.load_from_file(model_file)

        **epoch = state['epoch'] if 'epoch' in state else None**
        loss = state['loss'] if 'loss' in state else None
        optimizer_state_dict = state['optimizer_state_dict'] if 'optimizer_state_dict' in state else None
        scheduler_state_dict = state['scheduler_state_dict'] if 'scheduler_state_dict' in state else None

        return {
            'model': model, 'epoch': epoch, 'loss': loss,
            'optimizer_state_dict': optimizer_state_dict, 'scheduler_state_dict': scheduler_state_dict
        }

The epoch info is not saved in the model file which is causing this issue.
		</comment>
		<comment id='2' author='alipetiwala' date='2019-02-05T08:02:38Z'>
		Are you trying to load a checkpoint file or an already trained and finished model? I can remember this kind of error message when I tried to load a non-checkpoint model.
		</comment>
		<comment id='3' author='alipetiwala' date='2019-02-05T08:08:49Z'>
		&lt;denchmark-link:https://github.com/stefan-it&gt;@stefan-it&lt;/denchmark-link&gt;
 its a trained best model file.
		</comment>
		<comment id='4' author='alipetiwala' date='2019-02-05T09:09:19Z'>
		If you want to use the load_checkpoint method, then you need to use the checkpoint.pt file.
I think you enabled checkpointing in the example code snippet, so that should work.
If you do not have any model checkpoint, then you can easily use the following snippet:
language_model = LanguageModel.load_language_model('resources/taggers/language_model_forward/best-lm.pt')
To load the language model. Then you can use the same parameters as you used in your initial training, e.g.:
&lt;denchmark-code&gt;trainer = LanguageModelTrainer(language_model, corpus)

trainer.train('resources/taggers/language_model_forward_resumed',
              sequence_length=250,
              mini_batch_size=200,
              max_epochs=1,
              checkpoint=True)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='alipetiwala' date='2019-02-06T16:01:04Z'>
		Yes the  file is needed to resume training - they are written out every epoch if checkpointing is enabled during training. &lt;denchmark-link:https://github.com/alipetiwala&gt;@alipetiwala&lt;/denchmark-link&gt;
 does this work for you? If not, feel free to reopen the issue!
		</comment>
	</comments>
</bug>