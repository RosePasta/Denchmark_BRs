<bug id='1628' author='sankaran45' open_date='2020-05-24T12:10:42Z' closed_time='2020-06-03T17:21:26Z'>
	<summary>use_dropout not getting saved in checkpoint ???</summary>
	<description>
I think the use_dropout parameter does not saved as part of state_dict due to bug in
def _get_state_dict(self)
as a result of which if we set a dropout layer in sequence tagger, and resume checkpoint training, the behavior changes.
	</description>
	<comments>
		<comment id='1' author='sankaran45' date='2020-06-03T06:50:37Z'>
		&lt;denchmark-link:https://github.com/sankaran45&gt;@sankaran45&lt;/denchmark-link&gt;
 thanks for spotting this! I'll fix in a PR!
		</comment>
	</comments>
</bug>