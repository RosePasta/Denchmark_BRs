<bug id='1829' author='ugurcanozalp' open_date='2020-08-25T13:12:08Z' closed_time='2020-12-31T10:16:55Z'>
	<summary>CUDA out of memory error when very long word is passed to NER model.</summary>
	<description>
Hello. I have a string consisting of just one repeated character 'a' (300.000 times). The problem is that when I try to find NER in this string model just hangs and never finishes the process.
Describe the bug
Hello. I have a string consisting of just one repeated character 'a' (300.000 times). The problem is that when I try to find NER in this string ner model (en-ner-conll03-v0.4) stucks and throws cuda out of memory error after a long time. When I debugged the code, I realized that this model uses LanguageModel module (in flair/models/language_model) and divides sentences into chunks to get smaller sentences (in get_representation method of LanguageModel class, defaults to 512 chars). However, it seems that this is not able to resolve memory problem.
To Reproduce
&lt;denchmark-code&gt;from flair.tokenization import SegtokSentenceSplitter
from flair.models import SequenceTagger
file = 'aas.txt' # A file containing character a 300k times
with open(file,'r') as f:
    text = f.read()

tagger = SequenceTagger.load('ner')
splitter = SegtokSentenceSplitter()
textobjs = splitter.split(text)
tagger.predict(textobjs)
&lt;/denchmark-code&gt;

RuntimeError: CUDA out of memory. Tried to allocate 2.29 GiB (GPU 0; 5.93 GiB total capacity; 2.56 GiB already allocated; 2.07 GiB free; 2.78 GiB reserved in total by PyTorch)
Expected behavior
I expect to get conventional result, finished process with no entities found.
Environment (please complete the following information):

OS [Ubuntu 18.04]:
Version [flair-0.6]:

	</description>
	<comments>
		<comment id='1' author='ugurcanozalp' date='2020-12-24T08:27:57Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>