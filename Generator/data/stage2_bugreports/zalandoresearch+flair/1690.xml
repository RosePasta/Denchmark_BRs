<bug id='1690' author='niedakh' open_date='2020-06-12T18:57:02Z' closed_time='2020-08-13T13:10:29Z'>
	<summary>Problem with loading dialogpt</summary>
	<description>
Describe the bug
&lt;denchmark-code&gt;----------------------------------------------------------------------------------------------------
2020-06-12 18:44:49,152 Model training base path: "dialogpt"
2020-06-12 18:44:49,152 ----------------------------------------------------------------------------------------------------

2020-06-12 18:44:49,153 Device: cuda:0
2020-06-12 18:44:49,154 ----------------------------------------------------------------------------------------------------
2020-06-12 18:44:49,155 Embeddings storage mode: cpu
2020-06-12 18:44:49,230 ----------------------------------------------------------------------------------------------------
2020-06-12 18:49:00,669 epoch 1 - iter 76/766 - loss 1.11887558 - samples/sec: 9.67

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-7-e12f92b71d0b&gt; in &lt;module&gt;
      7         mini_batch_chunk_size=2, # set this if you get OOM errors
      8         max_epochs=1, # very few epochs of fine-tuning
----&gt; 9         embeddings_storage_mode='cpu',
     10         #checkpoint=True
     11     )

~/venv36/lib64/python3.6/site-packages/flair/trainers/trainer.py in train(self, base_path, learning_rate, mini_batch_size, mini_batch_chunk_size, max_epochs, scheduler, anneal_factor, patience, initial_extra_patience, min_learning_rate, train_with_dev, monitor_train, monitor_test, embeddings_storage_mode, checkpoint, save_final_model, anneal_with_restarts, anneal_with_prestarts, batch_growth_annealing, shuffle, param_selection_mode, num_workers, sampler, use_amp, amp_opt_level, eval_on_train_fraction, eval_on_train_shuffle, **kwargs)
    376                         if not param_selection_mode:
    377                             weight_extractor.extract_weights(
--&gt; 378                                 self.model.state_dict(), iteration
    379                             )
    380 

~/venv36/lib64/python3.6/site-packages/flair/training_utils.py in extract_weights(self, state_dict, iteration)
    262             vec = state_dict[key]
    263             weights_to_watch = min(
--&gt; 264                 self.number_of_weights, reduce(lambda x, y: x * y, list(vec.size()))
    265             )
    266 

TypeError: reduce() of empty sequence with no initial value

&lt;/denchmark-code&gt;

To Reproduce
Steps to reproduce the behavior (e.g. which model did you train? what parameters did you use? etc.).
Take a tagging corpus of choice and run:
&lt;denchmark-code&gt;embeddings = TransformerWordEmbeddings(
        'microsoft/DialoGPT-small',
        layers="-1",
        pooling_operation='first_last',
        fine_tune=True,
    )

tagger: SequenceTagger = SequenceTagger(
        hidden_size=256,
        embeddings=embeddings,
        tag_dictionary=tag_dictionary,
        tag_type=TAG_NAME,
        use_crf=False,
        use_rnn=False,
    )

trainer = ModelTrainer(tagger, corpus, optimizer=torch.optim.Adam)

trainer.train('dialogpt',
        learning_rate=3e-5,
        mini_batch_chunk_size=2,
        max_epochs=1,
        embeddings_storage_mode='cpu'
    )


&lt;/denchmark-code&gt;

Expected behavior
The models should train, instead I get the error.
Environment (please complete the following information):

Centos7
Flair 0.5.0

	</description>
	<comments>
		<comment id='1' author='niedakh' date='2020-06-13T22:27:34Z'>
		The same error happens with distilgpt2. The ctrl model also does not work (embedding tensor is 0-sized).
		</comment>
		<comment id='2' author='niedakh' date='2020-06-14T06:30:50Z'>
		IMO these models are not suitable for this task and are not supported by transformers for tagging and classification.
		</comment>
		<comment id='3' author='niedakh' date='2020-06-26T09:43:03Z'>
		Also encountering this with GPT2 for TextClassificationModel
		</comment>
		<comment id='4' author='niedakh' date='2020-07-29T09:40:48Z'>
		vec = state_dict[key] weights_to_watch = min( self.number_of_weights, reduce(lambda x, y: x * y, list(vec.size())) )
I think the error is occurring as the list in this code is remaining empty somehow for these transformers. So, if we are able to add an exception here for these transformers, it will work ig.  So, from what I can see in the above comments, does that mean Flair seems to have this issue for GPT transformers?
BTW, one interesting thing I found was on setting param_selection_mode = True in trainer.train(), we are able to avoid the bug (I have checked it for Distilgpt2. But, nevertheless this should be fixed.
		</comment>
		<comment id='5' author='niedakh' date='2020-08-03T08:14:59Z'>
		I'm not sure that setting  you truly "avoided" the bug. My guess is that most probably you're avoiding a specific sentence that is causing trouble (which maybe was assigned to the test set). If it is a problem of missing embeddings for that sentence, I have had the same issue with BERT in &lt;denchmark-link:https://github.com/flairNLP/flair/issues/1703&gt;#1703&lt;/denchmark-link&gt;
 .
		</comment>
		<comment id='6' author='niedakh' date='2020-08-03T13:44:37Z'>
		Hmm, it is not a problem of missing embeddings. What I observed is, whenever I use a GPT model like GPT2, DistilGPT2, DialoGPT, I encounter the above error. This error is solved everytime I use the setting param_selection_mode = True, now this setting tests the model against dev set. I guess you might be right when you said that I have not truly avoided the bug. It will be pretty interesting to figure out what is happening.
		</comment>
	</comments>
</bug>