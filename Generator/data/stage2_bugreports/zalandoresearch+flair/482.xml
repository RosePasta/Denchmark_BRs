<bug id='482' author='gccome' open_date='2019-02-12T04:13:15Z' closed_time='2019-02-12T15:42:08Z'>
	<summary>can't assign a list to a torch.cuda.LongTensor</summary>
	<description>
I recently ran into the following issue when I was training a sequence tagger. Does anyone know why this is happening? I use the code in tutorial.
&lt;denchmark-link:https://user-images.githubusercontent.com/44276950/52611179-3e5d2b00-2e52-11e9-980e-df88ffba0746.png&gt;&lt;/denchmark-link&gt;

Thanks!
	</description>
	<comments>
		<comment id='1' author='gccome' date='2019-02-12T10:28:15Z'>
		Hi &lt;denchmark-link:https://github.com/gccome&gt;@gccome&lt;/denchmark-link&gt;
 which version of Flair are you using, and which embeddings? Could you paste the code that you use when you get this error?
		</comment>
		<comment id='2' author='gccome' date='2019-02-12T10:34:01Z'>
		I've seen this error while playing around with ELMo Transformer yesterday. It worked before &lt;denchmark-link:https://github.com/flairNLP/flair/pull/459&gt;#459&lt;/denchmark-link&gt;
, so &lt;denchmark-link:https://github.com/gccome&gt;@gccome&lt;/denchmark-link&gt;
 could you try to go back to &lt;denchmark-link:https://github.com/flairNLP/flair/commit/f4a5033a56c0182e73890b9b7247e1e249ad8282&gt;f4a5033&lt;/denchmark-link&gt;
 and I think this error message won't appear?
I wasn't able to reproduce it on another (public) dataset, but I'm still searching for a step-by-step example to reproduce.
		</comment>
		<comment id='3' author='gccome' date='2019-02-12T13:57:17Z'>
		Hi &lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 I was using the most up-to-date Flair code on this repo. For embedding, I tried various combinations, but even the simplest one (only characterembedding) didn't work. Below is my code:
Hi &lt;denchmark-link:https://github.com/stefan-it&gt;@stefan-it&lt;/denchmark-link&gt;
 Thanks for the tip. I went back to &lt;denchmark-link:https://github.com/flairNLP/flair/pull/450&gt;#450&lt;/denchmark-link&gt;
  and the error disappeared. But does it mean I won't be able to use the new release?
&lt;denchmark-code&gt;    # 1. load dataset
    corpus = NLPTaskDataFetcher.load_column_corpus(data_folder=data_dir,
                                                   column_format=column_format,
                                                   train_file=train_filename,
                                                   dev_file=val_filename,
                                                   test_file=test_filename)
    logger.info("Corpus info - {}".format(str(corpus)))

    # 2. what tag do we want to predict?
    tag_type = 'ner'

    # 3. make the tag dictionary from the corpus
    tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)
    logger.info("Tag info - {}".format(str(tag_dictionary.idx2item)))

    # 4. initialize embeddings
    embedding_types = [
        CharacterEmbeddings()
    ]

   #  This is my customized embedding, already converted to Gensim format
    if custom_embedding_paths is not None:
        custom_embeddings = [WordEmbeddings(custom_embedding_path) for custom_embedding_path in custom_embedding_paths]
        embedding_types += custom_embeddings

    embeddings = StackedEmbeddings(embeddings=embedding_types)

    # 5. initialize sequence tagger

    tagger = SequenceTagger(hidden_size=hidden_size,
                            embeddings=embeddings,
                            tag_dictionary=tag_dictionary,
                            tag_type=tag_type,
                            use_crf=use_crf,
                            use_rnn=True,
                            rnn_layers=rnn_layers,
                            dropout=0.0,
                            word_dropout=0.05,
                            locked_dropout=0.5
                            )

    # 6. initialize trainer

    trainer = ModelTrainer(tagger, corpus, optimizer=Adam)

    # 7. start training
    training_hist = trainer.train(resource_dir,
                                  learning_rate=learning_rate,
                                  mini_batch_size=mini_batch_size,
                                  eval_mini_batch_size=eval_mini_batch_size,
                                  max_epochs=max_epochs,
                                  anneal_factor=0.5,
                                  patience=patience,
                                  anneal_against_train_loss=False,
                                  train_with_dev=False,
                                  monitor_train=True,
                                  embeddings_in_memory=True,
                                  checkpoint=False,
                                  save_final_model=True,
                                  anneal_with_restarts=False,
                                  test_mode=False,
                                  param_selection_mode=False)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='gccome' date='2019-02-12T14:14:33Z'>
		Thanks! Could it be that the error comes from the CharacterEmbeddings class? What if you try a setup with out character embeddings?
		</comment>
		<comment id='5' author='gccome' date='2019-02-12T14:55:15Z'>
		&lt;denchmark-link:https://github.com/gccome&gt;@gccome&lt;/denchmark-link&gt;
 should be fixed now on master - could you check?
		</comment>
		<comment id='6' author='gccome' date='2019-02-12T15:03:53Z'>
		Let me check and update you soon.
		</comment>
		<comment id='7' author='gccome' date='2019-02-12T15:39:31Z'>
		Hi &lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 , thanks for the quick fix! The error disappeared.
		</comment>
		<comment id='8' author='gccome' date='2019-02-12T15:42:08Z'>
		Great, thanks for reporting this!
		</comment>
	</comments>
</bug>