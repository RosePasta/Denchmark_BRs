<bug id='586' author='pertschuk' open_date='2019-03-04T06:16:30Z' closed_time='2019-05-27T15:15:53Z'>
	<summary>Character Embeddings &amp;lt;= 0</summary>
	<description>
Running into a Pytorch issue telling me that a PackedSequence has length &lt;= 0 when training with custom Gensim 200-dim embeddings stacked with CharacterEmbeddings. Maybe related to this? &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/4582&gt;pytorch/pytorch#4582&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;File "/home/poloma/.local/lib/python3.6/site-packages/flair/trainers/trainer.p                                                                     p
y", line 150, in train
    loss = self.model.forward_loss(batch)
  File "/home/poloma/.local/lib/python3.6/site-packages/flair/models/sequence_ta                                                                     a
gger_model.py", line 262, in forward_loss
    features, lengths, tags = self.forward(sentences, sort=sort)
  File "/home/poloma/.local/lib/python3.6/site-packages/flair/models/sequence_ta                                                                     a
gger_model.py", line 315, in forward
    self.embeddings.embed(sentences)
  File "/home/poloma/.local/lib/python3.6/site-packages/flair/embeddings.py", li                                                                ne 130, in embed
    embedding.embed(sentences)
  File "/home/poloma/.local/lib/python3.6/site-packages/flair/embeddings.py", li                                                                     i
ne 63, in embed
    self._add_embeddings_internal(sentences)
  File "/home/poloma/.local/lib/python3.6/site-packages/flair/embeddings.py", li                                                                     i
ne 611, in _add_embeddings_internal
    packed = torch.nn.utils.rnn.pack_padded_sequence(character_embeddings,chars                                                                     s
2_length)
  File "/home/poloma/.local/lib/python3.6/site-packages/torch/nn/utils/rnn.py",
line 148, in pack_padded_sequence
    return PackedSequence(torch._C._VariableFunctions._pack_padded_sequence(inpu                                                                     u
t, lengths, batch_first))
RuntimeError: Length of all samples has to be greater than 0, but found aneleme                                                                    e
nt in 'lengths' that is &lt;= 0
&lt;/denchmark-code&gt;

To Reproduce
&lt;denchmark-code&gt;columns = {0: 'text', 1: 'pos', 2: 'ner'}

   # this is the folder in which train, test and dev files reside
   data_folder = '.'

   # retrieve corpus using column format, data folder and the names of the train, dev and test files
   corpus = NLPTaskDataFetcher.load_column_corpus(data_folder, columns,
       train_file='train.txt',
       test_file='test.txt',
       dev_file='dev.txt')

   corpus.train = [sentence for sentence in corpus.train if len(sentence) &gt; 0]
   corpus.test = [sentence for sentence in corpus.test if len(sentence) &gt; 0]
   corpus.dev = [sentence for sentence in corpus.dev if len(sentence) &gt; 0]

   custom_embedding = WordEmbeddings('pubmed.gensim')

   character_embeddings = CharacterEmbeddings()
   stacked_embeddings = StackedEmbeddings(
       embeddings=[custom_embedding, character_embeddings])

   # just embed a sentence using the StackedEmbedding as you would with any single embedding.
   # stacked_embeddings.embed(sentence)
   tag_dictionary = corpus.make_tag_dictionary(tag_type='ner')
   
   tagger = SequenceTagger(hidden_size=256,
       embeddings=stacked_embeddings,
       tag_dictionary=tag_dictionary,
       tag_type='ner',
       use_crf=True)

   trainer = ModelTrainer(tagger, corpus)
   trainer.train('ner-model/',
             learning_rate=0.1,
             mini_batch_size=32,
             max_epochs=150)
&lt;/denchmark-code&gt;

Environment (please complete the following information):

OS Ubuntu
Version flair-0.4.1

Additional context
Using a custom corpus.
	</description>
	<comments>
		<comment id='1' author='pertschuk' date='2019-03-06T21:57:27Z'>
		Hello &lt;denchmark-link:https://github.com/pertschuk&gt;@pertschuk&lt;/denchmark-link&gt;
 thanks for reporting this. I'd like to try to reproduce the error if possible.
Could you set mini-batch size to 1 or 2 and isolate the mini-batch that throws the error? Then, could you share these sentences? That would allow me to make a small test to reproduce the error!
Also, does this error occur if you use only CharacterEmbeddings?
		</comment>
		<comment id='2' author='pertschuk' date='2019-05-27T15:15:52Z'>
		Closing due to inactivity, but feel free to reopen with more questions / comments.
		</comment>
	</comments>
</bug>