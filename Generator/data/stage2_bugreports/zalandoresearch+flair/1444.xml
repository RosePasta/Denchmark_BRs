<bug id='1444' author='kdk2612' open_date='2020-02-23T17:05:20Z' closed_time='2020-09-28T09:22:45Z'>
	<summary>RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.75 GiB total capacity; 11.00 GiB already allocated; 2.88 MiB free; 14.69 GiB reserved in total by PyTorch)</summary>
	<description>

The CUDA runs out of memory. I have tried the mentioned solution in the other similar issue.
The version of library installed is - 0.4.5 - master branch
pip install --upgrade git+&lt;denchmark-link:https://github.com/zalandoresearch/flair.git&gt;https://github.com/zalandoresearch/flair.git&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;----------------------------------------------------------------------------------------------------
Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.1, inplace=False)
        (encoder): Embedding(275, 100)
        (rnn): LSTM(100, 128)
        (decoder): Linear(in_features=128, out_features=275, bias=True)
      )
    )
    (list_embedding_1): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.1, inplace=False)
        (encoder): Embedding(275, 100)
        (rnn): LSTM(100, 128)
        (decoder): Linear(in_features=128, out_features=275, bias=True)
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=256, out_features=256, bias=True)
  (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=30, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
----------------------------------------------------------------------------------------------------
Corpus: "Corpus: 14098 train + 3021 dev + 3022 test sentences"
----------------------------------------------------------------------------------------------------
Parameters:
 - learning_rate: "0.001"
 - mini_batch_size: "16"
 - patience: "3"
 - anneal_factor: "0.5"
 - max_epochs: "10"
 - shuffle: "True"
 - train_with_dev: "True"
 - batch_growth_annealing: "False"
----------------------------------------------------------------------------------------------------
Model training base path: "Flair/ner_model"
----------------------------------------------------------------------------------------------------
Device: cuda:0
----------------------------------------------------------------------------------------------------
Embeddings storage mode: cpu
----------------------------------------------------------------------------------------------------
epoch 1 - iter 107/1070 - loss 594.20078684 - samples/sec: 7.06
epoch 1 - iter 214/1070 - loss 503.68683268 - samples/sec: 7.40
epoch 1 - iter 321/1070 - loss 430.24771304 - samples/sec: 6.74
epoch 1 - iter 428/1070 - loss 374.14354688 - samples/sec: 6.83
epoch 1 - iter 535/1070 - loss 331.84327782 - samples/sec: 6.92
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-10-8ba508100c66&gt; in &lt;module&gt;()
      7              monitor_test=True,
      8              checkpoint=True,
----&gt; 9               train_with_dev=True)

~/anaconda3/envs/python3/lib/python3.6/site-packages/flair/trainers/trainer.py in train(self, base_path, learning_rate, mini_batch_size, mini_batch_chunk_size, max_epochs, anneal_factor, patience, min_learning_rate, train_with_dev, monitor_train, monitor_test, embeddings_storage_mode, checkpoint, save_final_model, anneal_with_restarts, batch_growth_annealing, shuffle, param_selection_mode, num_workers, sampler, use_amp, amp_opt_level, eval_on_train_fraction, eval_on_train_shuffle, **kwargs)
    329 
    330                         # forward pass
--&gt; 331                         loss = self.model.forward_loss(batch_step)
    332 
    333                         # Backward

~/anaconda3/envs/python3/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py in forward_loss(self, data_points, sort)
    492     ) -&gt; torch.tensor:
    493         features = self.forward(data_points)
--&gt; 494         return self._calculate_loss(features, data_points)
    495 
    496     def forward(self, sentences: List[Sentence]):

~/anaconda3/envs/python3/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py in _calculate_loss(self, features, sentences)
    626             tags, _ = pad_tensors(tag_list)
    627 
--&gt; 628             forward_score = self._forward_alg(features, lengths)
    629             gold_score = self._score_sentence(features, tags, lengths)
    630 

~/anaconda3/envs/python3/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py in _forward_alg(self, feats, lens_)
    823             agg_ = torch.log(torch.sum(torch.exp(tag_var), dim=2))
    824 
--&gt; 825             cloned = forward_var.clone()
    826             cloned[:, i + 1, :] = max_tag_var + agg_
    827 

RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.75 GiB total capacity; 11.00 GiB already allocated; 2.88 MiB free; 14.69 GiB reserved in total by PyTorch)

&lt;/denchmark-code&gt;

To Reproduce
&lt;denchmark-code&gt;from flair.data import Corpus
from flair.datasets import ColumnCorpus
from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings
from typing import List


# define columns
columns = {0: 'text', 1: 'ner'}

# this is the folder in which train, test and dev files reside
data_folder = "Flair/ner_corpus_iob/"

corpus using column format, data folder and the names of the train, dev and test files
corpus: Corpus = ColumnCorpus(data_folder, columns,
                              train_file='train/train.txt',
                              test_file='test.txt',
                              dev_file='valid.txt',
                             in_memory=False)
#                              document_separator_token = "\n")
   
tag_type = 'ner'
tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)
print(tag_dictionary.idx2item)
embedding_types: List[TokenEmbeddings] = [

    FlairEmbeddings('Flair/language_model_forward/best-lm.pt'),
    FlairEmbeddings('Flair/language_model_backward/best-lm.pt')
]
from flair.models import SequenceTagger

tagger: SequenceTagger = SequenceTagger(hidden_size=256,
                                        embeddings=embeddings,
                                        tag_dictionary=tag_dictionary,
                                        tag_type=tag_type,
                                        use_crf=True)
    
from flair.trainers import ModelTrainer

trainer: ModelTrainer = ModelTrainer(tagger, corpus)

embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)

trainer.train('Flair/ner_model',
              learning_rate=0.001,
              mini_batch_size=16,
              max_epochs=10,
             monitor_train=True,
             monitor_test=True,
             checkpoint=True,
              train_with_dev=True)
&lt;/denchmark-code&gt;

Expected behavior
I was expecting the model to train with the required data.

I am training the model on AWS - Sagemaker - ml.p3.2xlarge machine
The version of library installed is - 0.4.5 - master branch
pip install --upgrade git+&lt;denchmark-link:https://github.com/zalandoresearch/flair.git&gt;https://github.com/zalandoresearch/flair.git&lt;/denchmark-link&gt;

Additional context
I reduced the mini_batch_size from 16 to 1 and the model seems to be working then. But the training is very slow and the results are not good.
	</description>
	<comments>
		<comment id='1' author='kdk2612' date='2020-02-23T17:21:31Z'>
		Maybe learning rate 0.1? You can try mini batch chunk size.
		</comment>
		<comment id='2' author='kdk2612' date='2020-02-24T07:54:58Z'>
		Yes, agree with a higher learning rate. Generally, a value of around 0.1 seems to work well for SGD.
How big is your language model (how many layers, hidden states)? And what is the longest sentence in the corpus in terms of characters?
As &lt;denchmark-link:https://github.com/djstrong&gt;@djstrong&lt;/denchmark-link&gt;
 wrote, you could try setting a chunk size in the  - this will internally split long sentences into smaller chunks for less memory-intensive processing, but not alter the results. You can set it like this (using ):
embedding_types: List[TokenEmbeddings] = [

    FlairEmbeddings('Flair/language_model_forward/best-lm.pt', chars_per_chunk=128),
    FlairEmbeddings('Flair/language_model_backward/best-lm.pt', chars_per_chunk=128)
]
		</comment>
		<comment id='3' author='kdk2612' date='2020-02-24T13:35:22Z'>
		I have tried that, for one of the run i was using
chars_per_chunk = 64
the longest sentence is of  = 26218 chars
The following code was used for training the embeddings.
&lt;denchmark-code&gt;fldr_path = "Flair/embedding_corpus/"

# are you training a forward or backward LM?
is_forward_lm = True

# load the default character dictionary
dictionary: Dictionary = Dictionary.load('chars')

fldr_path = Path("Flair/embedding_corpus/")

# get your corpus, process forward and at the character level
corpus = TextCorpus(path = fldr_path,
                    dictionary = dictionary,
                    forward = is_forward_lm,
                    character_level=True,
                   random_case_flip = True,
                   shuffle_lines = True)

# instantiate your language model, set hidden size and number of layers
language_model = LanguageModel(dictionary,
                               is_forward_lm,
                               hidden_size=128,
                               nlayers=1)

trainer = LanguageModelTrainer(language_model, corpus)

trainer.train('Flair/language_model_forward',
              sequence_length=25,
              mini_batch_size=20,
              max_epochs=10)


&lt;/denchmark-code&gt;

setting the mini_batch_size=1 is working. But takes a long time.
		</comment>
		<comment id='4' author='kdk2612' date='2020-02-24T13:44:23Z'>
		&lt;denchmark-link:https://github.com/kdk2612&gt;@kdk2612&lt;/denchmark-link&gt;
 your longest sentence is extremely long for a sequence labeling training dataset - 26k characters would fill pages of text. Does your training data have labels for so much text?
Could you try filtering all sentences that are longer than, say, 1000 or 2000 chars (which is still really long) and running with a higher mini-batch size?
		</comment>
		<comment id='5' author='kdk2612' date='2020-02-24T14:44:24Z'>
		Yes. These are paragraphs that we are using for sequence-labeling task.
The issue is that we cannot chunk the paragraphs into sentences because the punctuations are an important identifier of the entities, and learning which punctuation is a sentence splitter v/s which is inside the entity becomes more complicated (maybe even chicken-egg prob)
I will try using only sentences under 2000 chars and report the results soon. Thanks
		</comment>
		<comment id='6' author='kdk2612' date='2020-02-27T14:25:40Z'>
		Hi, I still have not tried reducing the paragraph length, but I was using the following code to train a new model.
&lt;denchmark-code&gt;# 4. initialize embeddings
embedding_types: List[TokenEmbeddings] = [
    FlairEmbeddings('Flair/language_model_forward/best-lm.pt',chars_per_chunk=64),
    FlairEmbeddings('Flair/language_model_backward/best-lm.pt',chars_per_chunk=64)
]

embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)


tagger: SequenceTagger = SequenceTagger(hidden_size=256,
                                        embeddings=embeddings,
                                        tag_dictionary=tag_dictionary,
                                        tag_type=tag_type,
                                        use_crf=True)
    
# 6. initialize trainer
from flair.trainers import ModelTrainer

trainer: ModelTrainer = ModelTrainer(tagger, corpus)

trainer.train('Flair/ner_model2',
              learning_rate=0.1,
              mini_batch_size=1,
              max_epochs=10,
             monitor_train=True,
             monitor_test=True,
             checkpoint=True,
              train_with_dev=True)

&lt;/denchmark-code&gt;

It ran till 6 epochs and then stopped and no model was saved.
Trying to run the code again, without restarting the machine or the notebook kept giving me the error:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
&lt;ipython-input-11-bdbba8bf24f2&gt; in &lt;module&gt;()
      7              monitor_test=True,
      8              checkpoint=True,
----&gt; 9               train_with_dev=True)

~/anaconda3/envs/python3/lib/python3.6/site-packages/flair/trainers/trainer.py in train(self, base_path, learning_rate, mini_batch_size, mini_batch_chunk_size, max_epochs, anneal_factor, patience, min_learning_rate, train_with_dev, monitor_train, monitor_test, embeddings_storage_mode, checkpoint, save_final_model, anneal_with_restarts, batch_growth_annealing, shuffle, param_selection_mode, num_workers, sampler, use_amp, amp_opt_level, eval_on_train_fraction, eval_on_train_shuffle, **kwargs)
    310                 # process mini-batches
    311                 batch_time = 0
--&gt; 312                 for batch_no, batch in enumerate(batch_loader):
    313                     start_time = time.time()
    314 

~/anaconda3/envs/python3/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __iter__(self)
    277             return _SingleProcessDataLoaderIter(self)
    278         else:
--&gt; 279             return _MultiProcessingDataLoaderIter(self)
    280 
    281     @property

~/anaconda3/envs/python3/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __init__(self, loader)
    717             #     before it starts, and __del__ tries to join but will get:
    718             #     AssertionError: can only join a started process.
--&gt; 719             w.start()
    720             self._index_queues.append(index_queue)
    721             self._workers.append(w)

~/anaconda3/envs/python3/lib/python3.6/multiprocessing/process.py in start(self)
    103                'daemonic processes are not allowed to have children'
    104         _cleanup()
--&gt; 105         self._popen = self._Popen(self)
    106         self._sentinel = self._popen.sentinel
    107         # Avoid a refcycle if the target function holds an indirect

~/anaconda3/envs/python3/lib/python3.6/multiprocessing/context.py in _Popen(process_obj)
    221     @staticmethod
    222     def _Popen(process_obj):
--&gt; 223         return _default_context.get_context().Process._Popen(process_obj)
    224 
    225 class DefaultContext(BaseContext):

~/anaconda3/envs/python3/lib/python3.6/multiprocessing/context.py in _Popen(process_obj)
    275         def _Popen(process_obj):
    276             from .popen_fork import Popen
--&gt; 277             return Popen(process_obj)
    278 
    279     class SpawnProcess(process.BaseProcess):

~/anaconda3/envs/python3/lib/python3.6/multiprocessing/popen_fork.py in __init__(self, process_obj)
     17         util._flush_std_streams()
     18         self.returncode = None
---&gt; 19         self._launch(process_obj)
     20 
     21     def duplicate_for_child(self, fd):

~/anaconda3/envs/python3/lib/python3.6/multiprocessing/popen_fork.py in _launch(self, process_obj)
     64         code = 1
     65         parent_r, child_w = os.pipe()
---&gt; 66         self.pid = os.fork()
     67         if self.pid == 0:
     68             try:

OSError: [Errno 12] Cannot allocate memory
&lt;/denchmark-code&gt;

I tried to check the memory using
!nvidia-smi
gave the following error
&lt;denchmark-code&gt;---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
&lt;ipython-input-12-d49d150280e6&gt; in &lt;module&gt;()
----&gt; 1 get_ipython().system('nvidia-smi')

~/anaconda3/envs/python3/lib/python3.6/site-packages/IPython/core/interactiveshell.py in system_piped(self, cmd)
   2259         # a non-None value would trigger :func:`sys.displayhook` calls.
   2260         # Instead, we store the exit_code in user_ns.
-&gt; 2261         self.user_ns['_exit_code'] = system(self.var_expand(cmd, depth=1))
   2262 
   2263     def system_raw(self, cmd):

~/anaconda3/envs/python3/lib/python3.6/site-packages/IPython/utils/_process_posix.py in system(self, cmd)
    154                 child = pexpect.spawnb(self.sh, args=['-c', cmd]) # Pexpect-U
    155             else:
--&gt; 156                 child = pexpect.spawn(self.sh, args=['-c', cmd])  # Vanilla Pexpect
    157             flush = sys.stdout.flush
    158             while True:

~/anaconda3/envs/python3/lib/python3.6/site-packages/pexpect/pty_spawn.py in __init__(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)
    202             self.name = '&lt;pexpect factory incomplete&gt;'
    203         else:
--&gt; 204             self._spawn(command, args, preexec_fn, dimensions)
    205         self.use_poll = use_poll
    206 

~/anaconda3/envs/python3/lib/python3.6/site-packages/pexpect/pty_spawn.py in _spawn(self, command, args, preexec_fn, dimensions)
    301 
    302         self.ptyproc = self._spawnpty(self.args, env=self.env,
--&gt; 303                                      cwd=self.cwd, **kwargs)
    304 
    305         self.pid = self.ptyproc.pid

~/anaconda3/envs/python3/lib/python3.6/site-packages/pexpect/pty_spawn.py in _spawnpty(self, args, **kwargs)
    312     def _spawnpty(self, args, **kwargs):
    313         '''Spawn a pty and return an instance of PtyProcess.'''
--&gt; 314         return ptyprocess.PtyProcess.spawn(args, **kwargs)
    315 
    316     def close(self, force=True):

~/anaconda3/envs/python3/lib/python3.6/site-packages/ptyprocess/ptyprocess.py in spawn(cls, argv, cwd, env, echo, preexec_fn, dimensions)
    220 
    221         if use_native_pty_fork:
--&gt; 222             pid, fd = pty.fork()
    223         else:
    224             # Use internal fork_pty, for Solaris

~/anaconda3/envs/python3/lib/python3.6/pty.py in fork()
     95 
     96     master_fd, slave_fd = openpty()
---&gt; 97     pid = os.fork()
     98     if pid == CHILD:
     99         # Establish a new session.

OSError: [Errno 12] Cannot allocate memory
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='kdk2612' date='2020-03-04T19:39:14Z'>
		This could be a regular memory issue. Could you try using embedding_storage_mode='none' when running the code? Like so:
trainer.train('Flair/ner_model2',
              learning_rate=0.1,
              mini_batch_size=1,
              max_epochs=10,
              monitor_train=True,
              monitor_test=True,
              checkpoint=True,
              train_with_dev=True,               
              embeddings_storage_mode="none",
              )
By default, Flair will keep embeddings of the training dataset in memory, but if the dataset is so large, this may not be possible. If you set it to 'none', Flair will not keep them in memory.
Still, your batch size is tiny - if you split paragraphs at least a little you may lose some signal but the training will be more stable and faster with a higher mini batch size.
		</comment>
		<comment id='8' author='kdk2612' date='2020-04-14T17:23:15Z'>
		Update:
I am using a technique called - Slide and Stride + Max Context (similar to rolling window) for reducing the length of the sentences/sequences set to 512 and 64.
Something similar is used in BERT for sequences that are longer than 512. I have been able to get good results for the initial experiments that were done. The training time is still slow, like takes a 2-2.5 days for 10 epochs : for "Corpus: 140545 train + 33321 dev + 33038 test sentences"
am able to use mini_batch size of 128.
One question I had was, for my project the RECALL is more important and was interested in knowing if there is a way I can optimize for the same.
		</comment>
		<comment id='9' author='kdk2612' date='2020-04-14T18:40:14Z'>
		Could you share the code for slide + stride?
Check beta and loss_weights parameters in SequenceTagger.
		</comment>
		<comment id='10' author='kdk2612' date='2020-04-14T20:31:54Z'>
		This is a part of the SQuAD example and is used in HuggingFace i guess.
&lt;denchmark-code&gt;from itertools import tee
def check_max_context(doc_spans, cur_span_index, position):
    best_score = None
    best_span_index = None
    for (span_index, doc_span) in enumerate(doc_spans):
        start, length = doc_span
        end = start + length - 1
        if position &lt; start:
            continue
        if position &gt; end:
            continue
        num_left_context = position - start
        num_right_context = end - position
        score = min(num_left_context, num_right_context) + 0.01 * length
        if best_score is None or score &gt; best_score:
            best_score = score
            best_span_index = span_index
    return cur_span_index == best_span_index


def slide_and_stride(tokens, labels, max_seq_length = 512, doc_stride= 64):
    
    max_tokens_for_doc = max_seq_length - 1
    
    doc_spans = []
    start_offset = 0
    while start_offset &lt; len(tokens):
        length = len(tokens) - start_offset
        if length &gt; max_tokens_for_doc:
            length = max_tokens_for_doc
        doc_spans.append((start_offset, length))
        if start_offset + length == len(tokens):
            break
        start_offset += min(length, doc_stride)
    
    tokens_list = []
    label_list = []
    max_context_list = []
    infos = []
    
    for (doc_span_index, doc_span) in enumerate(doc_spans):

        start, length = doc_span
        token_is_max_context = {}
        ntokens = []
        label_ids = []
        segment_ids = []

        for i in range(length):
            split_token_index = start + i

            is_max_context = check_max_context(doc_spans, doc_span_index,
                                                   split_token_index)
            token_is_max_context[split_token_index] = is_max_context
            ntokens.append(tokens[split_token_index])
            label_ids.append(labels[split_token_index])
            segment_ids.append(0)

        while len(label_ids) &lt; max_seq_length:
            label_ids.append("X") # X
            ntokens.append("[PAD]") # O
            
        assert len(label_ids) == max_seq_length
        assert len(ntokens) == max_seq_length

        tokens_list.append(ntokens)
        label_list.append(label_ids)
        max_context_list.append(token_is_max_context)
        infos.append((ntokens, label_ids, token_is_max_context))
    return infos
&lt;/denchmark-code&gt;

I am using this to generate the training data by creating multiple sentences for a sentence with len greater than 512 tokens. Also using it during prediction to take the prediction of tokens where the associated max_context is set to True. Hope this helps
		</comment>
		<comment id='11' author='kdk2612' date='2020-04-14T20:35:07Z'>
		
Could you share the code for slide + stride?
Check beta and loss_weights parameters in SequenceTagger.

&lt;denchmark-link:https://github.com/djstrong&gt;@djstrong&lt;/denchmark-link&gt;
 Can you please explain the beta parameter a little better? how can it be used for calculating recall? Not able to find documentation or examples related to it.
		</comment>
		<comment id='12' author='kdk2612' date='2020-04-14T21:08:34Z'>
		It is the F-beta metric. &lt;denchmark-link:https://github.com/flairNLP/flair/releases/tag/v0.4.5&gt;https://github.com/flairNLP/flair/releases/tag/v0.4.5&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='kdk2612' date='2020-05-24T06:02:19Z'>
		I tried training a new model with following Stats
{
"TRAIN": {
"dataset": "TRAIN",
"total_number_of_documents": 329601,
"number_of_documents_per_class": {},
"number_of_tokens_per_tag": {},
"number_of_tokens": {
"total": 37026045,
"min": 1,
"max": 512,
"avg": 112.33596075254626
}
},
"TEST": {
"dataset": "TEST",
"total_number_of_documents": 74384,
"number_of_documents_per_class": {},
"number_of_tokens_per_tag": {},
"number_of_tokens": {
"total": 8070746,
"min": 1,
"max": 512,
"avg": 108.50110238761025
}
},
"DEV": {
"dataset": "DEV",
"total_number_of_documents": 77786,
"number_of_documents_per_class": {},
"number_of_tokens_per_tag": {},
"number_of_tokens": {
"total": 8429286,
"min": 1,
"max": 512,
"avg": 108.36507854883912
}
}
}
I keep getting the CUDA out of memory error. I was able to train for 1 Epoch with lower batch size of 8 and downsampling to 1% of the data.
Any idea on why this happens? and how to solve it?
I am using 3 embeddings models - Word2vec and Forward and Backward Flair embeddings that are custom trained on the corpus
Also in the current data I have sentences with no annotations as well. Is it recommended to remove them and just use sentences with annotations or is it ok to keep them?
		</comment>
		<comment id='14' author='kdk2612' date='2020-09-21T08:10:56Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>