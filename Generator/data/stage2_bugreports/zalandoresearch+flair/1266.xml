<bug id='1266' author='shoarora' open_date='2019-11-08T18:55:53Z' closed_time='2020-02-05T19:09:22Z'>
	<summary>BertEmbeddings unnecessarily moves embeddings to CPU</summary>
	<description>

in , we call  and  unnecessarily inside the double for-loop &lt;denchmark-link:https://github.com/zalandoresearch/flair/blob/master/flair/embeddings.py#L2203&gt;here&lt;/denchmark-link&gt;
.

this whole block is run inside torch.no_grad(), so .detach() is basically a no-op.
token.set_embedding() should handle moving the tensor to the right device.  the only reason I can think of for moving off of GPU in the loop is that you need to make room to do something else immediately, but I can't see where that would be the case, since this entire block of code is dedicated to just aggregating the correct tensors.

Moving each layer's token embedding onto CPU one at a time is an unnecessary slowdown, when it could be done after all the tensors are concatenated together.
To Reproduce
# in embedding.py
for token_index, _ in enumerate(feature.tokens):
                    all_layers = []
                    for layer_index in self.layer_indexes:
                        if self.use_scalar_mix:
                            layer_output = all_encoder_layers[int(layer_index)][
                                sentence_index
                            ]
                        else:
                            layer_output = (
                                all_encoder_layers[int(layer_index)]
                                .detach()                               # &lt;---- remove this line
                                .cpu()[sentence_index]        # &lt;---- and this cpu() call
                            )
                        all_layers.append(layer_output[token_index])
Toggle those two lines of code, and do a timing comparison on BertEmbeddings.embed()
Expected behavior
A clear and concise description of what you expected to happen.
I have limited devices/environments I can test on, but from what I've seen, this should give significant speed-up in runtime.  The exact improvement is highly dependent on both batch size and sentence length since it's inside this double for-loop.
Screenshots
If applicable, add screenshots to help explain your problem.
Environment (please complete the following information):

OS [e.g. iOS, Linux]: Ubuntu 16.04
Version: flair-0.4.4

run on an NVIDIA T4
Additional context
Add any other context about the problem here.
I'll try to put together a gist with the exact profiling code and outputs that I observed, but in the meantime this should be enough to begin the discussion.  Moreover, I could totally just be missing some important part of the system that explains these lines.
EDIT &lt;denchmark-link:https://gist.github.com/shoarora/a8aa6ac39ee814b667907ced4bfda6c9&gt;gist is here&lt;/denchmark-link&gt;

I compared the flair runtime against just the huggingface runtime for the same model.  flair should take longer, since it runs the underlying huggingface model and does additional processing, but the  ratio decreases after removing the two lines mentioned
	</description>
	<comments>
	</comments>
</bug>