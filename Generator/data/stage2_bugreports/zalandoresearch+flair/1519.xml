<bug id='1519' author='ayushjaiswal' open_date='2020-04-09T00:01:05Z' closed_time='2020-06-10T09:50:28Z'>
	<summary>Problem with max_sequence_length in BertEmbeddings</summary>
	<description>
Currently, BertEmbeddings does not account for the maximum sequence length supported by the underlying (transformers) BertModel. Since BERT creates subtokens, it becomes somewhat challenging to check sequence-length and trim sentence externally before feeding it to BertEmbeddings in flair.
I see a problem in &lt;denchmark-link:https://github.com/flairNLP/flair/blob/master/flair/embeddings.py#L2678--L2687&gt;https://github.com/flairNLP/flair/blob/master/flair/embeddings.py#L2678--L2687&lt;/denchmark-link&gt;

        # first, find longest sentence in batch
        longest_sentence_in_batch: int = len(
            max(
                [
                    self.tokenizer.tokenize(sentence.to_tokenized_string())
                    for sentence in sentences
                ],
                key=len,
            )
        )
This is passed to
        # prepare id maps for BERT model
        features = self._convert_sentences_to_features(
            sentences, longest_sentence_in_batch
        )
which sets max_sequence_length in:
&lt;denchmark-link:https://github.com/flairNLP/flair/blob/master/flair/embeddings.py#L2620-L2622&gt;https://github.com/flairNLP/flair/blob/master/flair/embeddings.py#L2620-L2622&lt;/denchmark-link&gt;

    _convert_sentences_to_features(
        self, sentences, max_sequence_length: int
    )
But this does not account for or check the max-sequence-length supported by the BERT model, which is accessible in either of the above functions through  self.model.config.max_position_embeddings.
	</description>
	<comments>
		<comment id='1' author='ayushjaiswal' date='2020-04-09T09:11:06Z'>
		Hi &lt;denchmark-link:https://github.com/ayushjaiswal&gt;@ayushjaiswal&lt;/denchmark-link&gt;
 we are in the process of refactoring the transformer-based embeddings classes. See &lt;denchmark-link:https://github.com/flairNLP/flair/pull/1494&gt;#1494&lt;/denchmark-link&gt;
. Instead of separate classes for each transformer embedding, we will have a unified class that gets the transformer model as string in the constructor. So initialization will be like this:
# example sentence
sentence = Sentence('The grass is green')

# a BERT model
embeddings = TransformerWordEmbeddings(model="bert-base-uncased", layers="-1", pooling_operation='first')
embeddings.embed(sentence)

# a roBERTa model
embeddings = TransformerWordEmbeddings(model="distilroberta-base", layers="-1", pooling_operation='first')
embeddings.embed(sentence)
There is now also a corresponding TransformerDocumentEmbeddings class in case you want document embeddings out of the transformer.
We're also looking at different ways for handling overlong sequences as part of the refactoring. We will add handling for this soon.
		</comment>
		<comment id='2' author='ayushjaiswal' date='2020-04-09T10:21:07Z'>
		&lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 Thanks for the quick response! Great to hear about the refactoring and handling of overlong sequences.  definitely needs to be accounted for so that the input sequence during forward pass of the  does not encounter sequences of length greater than that. Currently, when the length does exceed the limit, a  occurs caused by a  which corrupts the CUDA context and requires re-initialization of the CUDA session. Even if the input sequence is trimmed, I guess it will create a problem with assigning embeddings to  tokens. It seems somewhat tricky 
		</comment>
		<comment id='3' author='ayushjaiswal' date='2020-04-15T11:01:44Z'>
		&lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;

Maybe a sliding window approach, as implemented &lt;denchmark-link:https://github.com/ThilinaRajapakse/simpletransformers/blob/e289efa09e280955207fefbec8aedcced8449be3/simpletransformers/language_modeling/language_modeling_utils.py&gt;here&lt;/denchmark-link&gt;
 , might be a good idea to tackle the length limitation of BERT.
I've resorted a lot to using the linked package instead of flair, solely for this feature, as the results seem to be better compared to simply truncating the sentences.
Would love to see this feature in flair!
		</comment>
		<comment id='4' author='ayushjaiswal' date='2020-04-15T12:27:09Z'>
		Thanks for the pointer - yes this looks promising so we might integrate it!
		</comment>
		<comment id='5' author='ayushjaiswal' date='2020-04-16T03:04:35Z'>
		Looking forward to this ðŸ˜„
		</comment>
		<comment id='6' author='ayushjaiswal' date='2020-05-05T03:02:06Z'>
		&lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 is there any update on this? 
		</comment>
		<comment id='7' author='ayushjaiswal' date='2020-06-08T14:48:39Z'>
		Unfortunately, we haven't gotten around to this yet. But you could try the recently added "longformer" models which can handle longer sequences:
embeddings = TransformerWordEmbeddings('allenai/longformer-base-4096')

embeddings.embed(sentence)
		</comment>
	</comments>
</bug>