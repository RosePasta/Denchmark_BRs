<bug id='1229' author='Solid7' open_date='2019-10-20T16:03:44Z' closed_time='2020-04-29T21:41:51Z'>
	<summary>Runtime error when training text classifier. RuntimeError: Expected object of backend CUDA but got backend CPU for sequence element 11 in sequence argument at position #1 'tensors'</summary>
	<description>
First time using Flair but I think I might have stumbled up on a bug or I'm doing something wrong.
Describe the bug
When training a simple classifier you get the following runtime error:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "flair_test.py", line 53, in &lt;module&gt;
    train_flair()
  File "flair_test.py", line 50, in train_flair
    mini_batch_size=32, anneal_factor=0.5) #, embeddings_storage_mode='gpu')
  File "C:\Users\foo\Anaconda3\envs\py37\lib\site-packages\flair\trainers\trainer.py", line 318, in train
    loss = self.model.forward_loss(batch_step)
  File "C:\Users\foo\Anaconda3\envs\py37\lib\site-packages\flair\models\text_classification_model.py", line 117, in forward_loss
    scores = self.forward(data_points)
  File "C:\Users\foo\Anaconda3\envs\py37\lib\site-packages\flair\models\text_classification_model.py", line 82, in forward
    self.document_embeddings.embed(sentences)
  File "C:\Users\foo\Anaconda3\envs\py37\lib\site-packages\flair\embeddings.py", line 2913, in embed
    word_embeddings_tensor = torch.cat(word_embeddings, 0).to(flair.device)
RuntimeError: Expected object of backend CUDA but got backend CPU for sequence element 11 in sequence argument at position #1 'tensors'
&lt;/denchmark-code&gt;

To Reproduce
&lt;denchmark-code&gt;import pandas as pd
import numpy as np
from sklearn.datasets import fetch_20newsgroups
from flair.data_fetcher import NLPTaskDataFetcher
from flair.embeddings import (WordEmbeddings, 
FlairEmbeddings, DocumentLSTMEmbeddings, RoBERTaEmbeddings, 
BertEmbeddings, DocumentPoolEmbeddings)
from flair.models import TextClassifier
from flair.trainers import ModelTrainer

# Uncomment to avoid RuntimeError
# import flair, torch
# flair.device = torch.device('cpu') 

def create_dataset():
    newsgroups_train = fetch_20newsgroups(subset='train')
    newsgroups_test = fetch_20newsgroups(subset='test')
    labels = np.array(newsgroups_test.target_names)

    df_train = pd.DataFrame(dict(label=labels[newsgroups_train.target], text=newsgroups_train.data))
    df_train['label'] = '__label__' + df_train['label'].astype('str')

    df_test = pd.DataFrame(dict(label=labels[newsgroups_test.target], text=newsgroups_test.data))
    df_test['label'] = '__label__' + df_test['label'].astype('str')

    df_test = df_test.iloc[0:int(len(df_test)*0.5)]
    df_dev = df_test.iloc[int(len(df_test)*0.5):]
    
    for df, name in zip([df_train, df_test, df_dev], ['train', 'test', 'dev']):
        df.to_csv(name + '.csv', sep=' ', index=False, header=False, quotechar=' ')

    corpus = NLPTaskDataFetcher.load_classification_corpus('./', train_file='train.csv', 
    test_file='test.csv', dev_file='dev.csv')

    return corpus

def train_flair():
    corpus = create_dataset()

    word_embeddings = [WordEmbeddings('glove'), FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast')]
    
    document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=128, reproject_words=True)

    dictionary = corpus.make_label_dictionary()
    classifier = TextClassifier(document_embeddings, label_dictionary=dictionary, multi_label=False)

    trainer = ModelTrainer(classifier, corpus, use_tensorboard=True)
    trainer.train('resources/taggers/classy', 
        max_epochs=25, patience=1, learning_rate=0.2, 
        mini_batch_size=32, anneal_factor=0.5) #, embeddings_storage_mode='gpu')

create_dataset()
train_flair()
&lt;/denchmark-code&gt;

Expected behavior
Expected no runtime error.
Environment (please complete the following information):

Windows
Version: flair-0.4.3

Possible fix
emeddings.py, line 2906, add -&gt; .to(flair.device)
&lt;denchmark-code&gt; # PADDING: pad shorter sentences out
        for add in range(longest_token_sequence_in_batch - len(sentence.tokens)):
            word_embeddings.append(
                torch.zeros(
                    self.length_of_all_token_embeddings, dtype=torch.float
                ).unsqueeze(0) #.to(flair.device)
            )
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Solid7' date='2019-10-22T14:39:37Z'>
		Hi &lt;denchmark-link:https://github.com/Solid7&gt;@Solid7&lt;/denchmark-link&gt;
 it looks like the files you are generating are not proper FastText formatted files. In FastText format, each line is one data point, i.e. you can have no line breaks and each line needs to start with a  formatted label.
		</comment>
		<comment id='2' author='Solid7' date='2019-10-23T16:51:35Z'>
		Thanks for the help =). I belive you are right. Fixing this did make things look better, although I get out of memory erros.
Just to verify and avoid the 'out of memory' error, training on subset of sentences seems to work much better without the line endings.
		</comment>
		<comment id='3' author='Solid7' date='2020-04-29T20:10:42Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>