<bug id='108' author='tabergma' open_date='2018-09-13T07:19:49Z' closed_time='2018-09-20T12:57:45Z'>
	<summary>Fix cuda out of memory issue for text classification</summary>
	<description>
When training a text classifier on large datasets, the trainer runs into memory problems.
Investigate why we are running into out of memory issues and fix them.
	</description>
	<comments>
		<comment id='1' author='tabergma' date='2018-09-13T10:06:07Z'>
		I can confirm that issue. Currently I'm trying to train on the GermEval 2018 dataset, and passing embeddings_in_memory=False to the train function helps a bit. I also removed evaluation on training dataset and then no "cuda out of memory" messages appear.
Update: results are not very promising at the moment.
		</comment>
		<comment id='2' author='tabergma' date='2018-09-14T11:28:23Z'>
		&lt;denchmark-link:https://github.com/stefan-it&gt;@stefan-it&lt;/denchmark-link&gt;
 It seems like the evaluation method takes a lot of memory and causes the  problems. We are looking into it.
Yesterday we fixed a bug in training a text classifier on a single label task (see &lt;denchmark-link:https://github.com/flairNLP/flair/issues/109&gt;#109&lt;/denchmark-link&gt;
). So in case you used single label training, you might want to try again with the current version in the master branch.
		</comment>
		<comment id='3' author='tabergma' date='2018-09-20T12:57:45Z'>
		We could not find any memory leak. However, training a text classifier on a large data set which contains large documents/sentences using contextual string embeddings is quite expensive and can lead to out of memory issues. To avoid this, you can now
(1) disable the evaluation on the training data set
(2) use a smaller batch size for evaluation
		</comment>
	</comments>
</bug>