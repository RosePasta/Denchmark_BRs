<bug id='325' author='mvss80' open_date='2018-12-21T22:13:24Z' closed_time='2019-01-15T07:45:00Z'>
	<summary>torch errors when I try Bert embeddings</summary>
	<description>
I'm on Ubuntu 18.04 with Intel® Core™ i5-8400 CPU @ 2.80GHz × 6  and GeForce GTX 1080/PCIe/SSE2.
I have pytorch 1.0 and installed flair. When I try the following code snippet
&lt;denchmark-code&gt;from flair.embeddings import BertEmbeddings, FlairEmbeddings
from flair.data import Sentence

BertEmbedding = BertEmbeddings()
sent = Sentence('I love Austin')
BertEmbedding.embed(sent)

# fe = FlairEmbeddings('news-forward')
# fe.embed(sent)
&lt;/denchmark-code&gt;

I get this error. My pytorch installation seems to be fine. Any ideas on how to fix this?
&lt;denchmark-code&gt;---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-1-47fbfcb3fd4f&gt; in &lt;module&gt;
      5 BertEmbedding = BertEmbeddings()
      6 sent = Sentence('I love Austin')
----&gt; 7 BertEmbedding.embed(sent)
      8 # fe.embed(sent)

~/anaconda3/envs/gpu-dev/lib/python3.6/site-packages/flair/embeddings.py in embed(self, sentences)
     54 
     55         if not everything_embedded or not self.static_embeddings:
---&gt; 56             self._add_embeddings_internal(sentences)
     57 
     58         return sentences

~/anaconda3/envs/gpu-dev/lib/python3.6/site-packages/flair/embeddings.py in _add_embeddings_internal(self, sentences)
    957         # put encoded batch through BERT model to get all hidden states of all encoder layers
    958         self.model.eval()
--&gt; 959         all_encoder_layers, _ = self.model(all_input_ids, token_type_ids=None, attention_mask=all_input_masks)
    960 
    961         with torch.no_grad():

~/anaconda3/envs/gpu-dev/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    487             result = self._slow_forward(*input, **kwargs)
    488         else:
--&gt; 489             result = self.forward(*input, **kwargs)
    490         for hook in self._forward_hooks.values():
    491             hook_result = hook(self, input, result)

~/anaconda3/envs/gpu-dev/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py in forward(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)
    607         extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0
    608 
--&gt; 609         embedding_output = self.embeddings(input_ids, token_type_ids)
    610         encoded_layers = self.encoder(embedding_output,
    611                                       extended_attention_mask,

~/anaconda3/envs/gpu-dev/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    487             result = self._slow_forward(*input, **kwargs)
    488         else:
--&gt; 489             result = self.forward(*input, **kwargs)
    490         for hook in self._forward_hooks.values():
    491             hook_result = hook(self, input, result)

~/anaconda3/envs/gpu-dev/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py in forward(self, input_ids, token_type_ids)
    191             token_type_ids = torch.zeros_like(input_ids)
    192 
--&gt; 193         words_embeddings = self.word_embeddings(input_ids)
    194         position_embeddings = self.position_embeddings(position_ids)
    195         token_type_embeddings = self.token_type_embeddings(token_type_ids)

~/anaconda3/envs/gpu-dev/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    487             result = self._slow_forward(*input, **kwargs)
    488         else:
--&gt; 489             result = self.forward(*input, **kwargs)
    490         for hook in self._forward_hooks.values():
    491             hook_result = hook(self, input, result)

~/anaconda3/envs/gpu-dev/lib/python3.6/site-packages/torch/nn/modules/sparse.py in forward(self, input)
    116         return F.embedding(
    117             input, self.weight, self.padding_idx, self.max_norm,
--&gt; 118             self.norm_type, self.scale_grad_by_freq, self.sparse)
    119 
    120     def extra_repr(self):

~/anaconda3/envs/gpu-dev/lib/python3.6/site-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   1452         # remove once script supports set_grad_enabled
   1453         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 1454     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   1455 
   1456 

RuntimeError: Expected object of backend CPU but got backend CUDA for argument #3 'index'
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='mvss80' date='2018-12-23T08:58:18Z'>
		Even I have the same issue. It works well in the CPU.
		</comment>
		<comment id='2' author='mvss80' date='2018-12-23T09:03:32Z'>
		Are you using CUDA 10? I installed PyTorch via:
pip3 install https://download.pytorch.org/whl/cu100/torch-1.0.0-cp36-cp36m-linux_x86_64.whl
And training (also with BERT embeddings) is working fine here.
		</comment>
		<comment id='3' author='mvss80' date='2018-12-23T17:16:16Z'>
		I'm using cuda 10. I reinstalled pytorch with your link and tried again, getting the same error.
		</comment>
		<comment id='4' author='mvss80' date='2018-12-27T08:48:43Z'>
		Same error on Win10+CUDA9.0+pytorch 1.0
		</comment>
		<comment id='5' author='mvss80' date='2018-12-27T09:57:30Z'>
		Hm interesting. Does this only affect BertEmbeddings or also other embeddings classes? Could you try:
embedding = BertEmbeddings()
sentence = Sentence('I love Austin')
embedding.embed(sentence)
and
embedding = FlairEmbeddings('news-forward')
sentence = Sentence('I love Austin')
embedding.embed(sentence)
Do both give the same error?
		</comment>
		<comment id='6' author='mvss80' date='2018-12-27T10:33:24Z'>
		The FlairEmbeddings works well for me.
And I've noticed similar error messages in Pytorch's issues:
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/12406&gt;pytorch/pytorch#12406&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/15005&gt;pytorch/pytorch#15005&lt;/denchmark-link&gt;

Maybe there are some relations?
		</comment>
		<comment id='7' author='mvss80' date='2018-12-27T16:00:19Z'>
		It happens only for BertEmbeddings... FlairEmbeddings work fine for me.

Hm interesting. Does this only affect BertEmbeddings or also other embeddings classes? Could you try:
embedding = BertEmbeddings()
sentence = Sentence('I love Austin')
embedding.embed(sentence)
and
embedding = FlairEmbeddings('news-forward')
sentence = Sentence('I love Austin')
embedding.embed(sentence)
Do both give the same error?

		</comment>
		<comment id='8' author='mvss80' date='2018-12-29T00:50:49Z'>
		&lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 - not sure if this helps but I checked if this was an issue with pytorch-pretrained-bert from huggingface - I was able to run their example code snippets with no issues.
&lt;denchmark-code&gt;import torch
from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM

# Load pre-trained model tokenizer (vocabulary)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenized input
text = "Who was Jim Henson ? Jim Henson was a puppeteer"
tokenized_text = tokenizer.tokenize(text)

# Mask a token that we will try to predict back with `BertForMaskedLM`
masked_index = 6
tokenized_text[masked_index] = '[MASK]'
assert tokenized_text == ['who', 'was', 'jim', 'henson', '?', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer']

# Convert token to vocabulary indices
indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)
segments_ids = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]

# Convert inputs to PyTorch tensors
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])

model = BertModel.from_pretrained('bert-base-uncased')
model.eval()

# Predict hidden states features for each layer
encoded_layers, _ = model(tokens_tensor, segments_tensors)
# We have a hidden states for each of the 12 layers in model bert-base-uncased
assert len(encoded_layers) == 12
print(encoded_layers[0])
&lt;/denchmark-code&gt;

gives me the result below
&lt;denchmark-code&gt;tensor([[[ 0.1076,  0.0848, -0.1428,  ..., -0.1101, -0.0188,  0.1174],
         [ 0.1772, -0.6673, -0.2267,  ...,  0.3663,  0.2764,  0.0222],
         [ 0.1237, -0.8573,  0.1050,  ...,  0.8937,  0.4174, -0.6162],
         ...,
         [ 0.8103, -0.2219,  0.1801,  ...,  0.3659,  0.1980,  0.5411],
         [ 1.2984, -0.5256, -0.6261,  ..., -0.1529, -0.1193,  0.8076],
         [ 1.0819,  0.0539, -0.7493,  ...,  0.3401,  0.0281,  0.1370]]],
       grad_fn=&lt;AddBackward0&gt;)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='mvss80' date='2019-01-07T15:49:29Z'>
		Same error here
		</comment>
		<comment id='10' author='mvss80' date='2019-01-08T16:35:56Z'>
		I just fixed this debug:

find the class BertEmbeddings in file flair.embeddings, and in the function _add_embeddings_internal():
change the following line:
all_encoder_layers, _ = self.model(all_input_ids, token_type_ids=None, attention_mask=all_input_masks)
to this:
all_encoder_layers, _ = self.model(all_input_ids.cpu(), token_type_ids=None, attention_mask=all_input_masks.cpu())

		</comment>
		<comment id='11' author='mvss80' date='2019-01-09T14:12:48Z'>
		&lt;denchmark-link:https://github.com/blmoistawinde&gt;@blmoistawinde&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/mvss80&gt;@mvss80&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ghaddarAbs&gt;@ghaddarAbs&lt;/denchmark-link&gt;
 sorry for not progressing faster on this - we are having some problems reproducing this error. But given that so many people are experiencing this error, I think we need to make it a priority.
Hello @cococold - thanks for looking into this. With this fix, it seems everything is always put on CPU. Does BERT processing then still happen on GPU or is it now slower / i.e. always on CPU even if a GPU is there?
		</comment>
		<comment id='12' author='mvss80' date='2019-01-09T14:15:38Z'>
		&lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 .... Absolutely true ... so it's not an effective solution
		</comment>
		<comment id='13' author='mvss80' date='2019-01-09T15:06:27Z'>
		&lt;denchmark-link:https://github.com/peteriz&gt;@peteriz&lt;/denchmark-link&gt;
 fixed this with a PR. Fix is now in master branch and will be part of the upcoming 0.4.1.
		</comment>
		<comment id='14' author='mvss80' date='2019-02-15T19:24:28Z'>
		For those running across this error before 0.4.1 has come out, you can fix this by doing the following (in your virtual environment, if necessary):
&lt;denchmark-code&gt;pip uninstall flair
git clone https://github.com/zalandoresearch/flair.git
cd flair
python setup.py install
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>