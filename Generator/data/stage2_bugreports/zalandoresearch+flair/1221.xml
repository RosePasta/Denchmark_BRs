<bug id='1221' author='isarth' open_date='2019-10-18T09:58:38Z' closed_time='2019-12-03T09:57:07Z'>
	<summary>Error while using XLNet and RoBERTa embeddings but work fine with BERT embeddings</summary>
	<description>
Describe the bug
While training custom NER model using LSTM with CRF. The model works fine with BERT and glove but gives an error with XLNet and Roberta embeddings.
File "/home/rajivratn/miniconda3/envs/sarthak_new/lib/python3.6/site-packages/flair/embeddings.py", line 1116, in _get_transformer_sentence_embeddings
use_scalar_mix=use_scalar_mix,
File "/home/rajivratn/miniconda3/envs/sarthak_new/lib/python3.6/site-packages/flair/embeddings.py", line 975, in _extract_embeddings
first_embedding: torch.FloatTensor = current_embeddings[0]
IndexError: index 0 is out of bounds for dimension 0 with size 0
Environment (please complete the following information):

Ubuntu
Version 0.4.3

	</description>
	<comments>
		<comment id='1' author='isarth' date='2019-10-18T11:05:20Z'>
		Could you provide a code snippet for reproducing the error? What dataset do you use ðŸ¤”
		</comment>
		<comment id='2' author='isarth' date='2019-10-23T03:09:42Z'>
		I am also having these exact errors, not sure what's going on.
		</comment>
		<comment id='3' author='isarth' date='2019-10-29T21:38:10Z'>
		I got the same error while using XLNetEmbeddings. In my case, the issue was with some of the unicode characters.
For example,  returns an empty list for the unicode character  (U+02DC). This means that the length of subwords for the corresponding token in  is zero (see function &lt;denchmark-link:https://github.com/zalandoresearch/flair/blob/b8e68ae1a147ee9e8700d63fbc9528cf63fd1d58/flair/embeddings.py#L1078&gt;_build_token_subwords_mapping&lt;/denchmark-link&gt;
).
This is problematic when evaluating  in function &lt;denchmark-link:https://github.com/zalandoresearch/flair/blob/b8e68ae1a147ee9e8700d63fbc9528cf63fd1d58/flair/embeddings.py#L1026&gt;_extract_embeddings&lt;/denchmark-link&gt;
, because now you get an empty Tensor and an exception is thrown when you try to index it.
		</comment>
		<comment id='4' author='isarth' date='2019-10-29T22:29:23Z'>
		&lt;denchmark-link:https://github.com/0x7A31C7&gt;@0x7A31C7&lt;/denchmark-link&gt;
 I'll check it again :) I think that we can use the  symbol whenever the tokenizer returns an empy token list in flair.
		</comment>
		<comment id='5' author='isarth' date='2019-10-29T22:30:45Z'>
		Similar issue was raised in  today: &lt;denchmark-link:https://github.com/huggingface/transformers/issues/1662&gt;huggingface/transformers#1662&lt;/denchmark-link&gt;
, so maybe we can wait for an upstream fix 
		</comment>
		<comment id='6' author='isarth' date='2019-12-01T20:06:12Z'>
		Is there a fix ?
		</comment>
		<comment id='7' author='isarth' date='2019-12-01T20:21:11Z'>
		
@0x7A31C7 I'll check it again :) I think that we can use the unk_token symbol whenever the tokenizer returns an empty token list in flair.

Where can I make this change?
		</comment>
		<comment id='8' author='isarth' date='2019-12-02T10:48:11Z'>
		&lt;denchmark-link:https://github.com/isarth&gt;@isarth&lt;/denchmark-link&gt;
 Could you try to checkout the new  branch? This should fix all tokenization problems by checking the subword length after tokenization. Whenever a tokenizer returns a length of 0 ( -&gt; `` for the XLNetTokenizer) then the  is used instead of an empty token.
		</comment>
	</comments>
</bug>