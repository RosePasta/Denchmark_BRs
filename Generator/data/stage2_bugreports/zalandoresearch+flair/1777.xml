<bug id='1777' author='tylerlekang' open_date='2020-07-24T20:22:28Z' closed_time='2020-12-19T00:25:17Z'>
	<summary>Loading data with ColumnCorpus causes CUDA out of memory, but loading flair dataset BIOFID runs fine, all other parameters the same</summary>
	<description>
Describe the bug
I manually created train/valid(dev)/test sets as plain text files with 2 columns (space separated): 1-token and 2-custom ner tags with IOBES prefixes. All lines end with '\n' and one line with only '\n' , between sentences. This format has worked fine for me so far, by loading them using a direct call to the ColumnCorpus class.
However, my run last night failed because of "CUDA out of memory" error.
As far as I can tell, my dataset is equivalent to the BIOFID format, as far as PyTorch is concerned. It results in the same in:512, out:28 sized (linear) layer. All the other parts of the network are defined by the embeddings, which I kept the same. All the parameters of ColumnCorpus, SequenceTagger, and ModelTrainer.train are kept the same.
But with the BIOFID corpus, it runs just fine.
To Reproduce
Here is the code that runs the training:
&lt;denchmark-code&gt;# Create ColumnCorpus from data
from flair.datasets import ColumnCorpus

columns = {0: 'text', 1: 'ner'}
data_folder = '/main/experiments/experiment_3/combined_data'
corpus = ColumnCorpus(data_folder, columns,
                      train_file='train_IOBES_6',
                      test_file='test_IOBES',
                      dev_file='valid_IOBES_6',
                     )

# # Import BIOFID dataset as an example
# from flair.datasets import BIOFID

# corpus = BIOFID()

# Make the tag dictionary from the corpus
tag_dict = corpus.make_tag_dictionary(tag_type='ner')

# Embeddings
from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings

# set Pytorch version to '1.6.0' due to a bug in Flair 0.5 for FlairEmbeddings
# that cannot handle non-digits in the PyTorch version string
import torch
torch.__version__ = '1.6.0'

# Initialize embeddings
embs = [WordEmbeddings('glove'),
        FlairEmbeddings('language_model/dgx.pt'), # based on news-forward
        FlairEmbeddings('news-backward'),
       ]

stacked_embeddings = StackedEmbeddings(embs)

# Initialize sequence tagger
from flair.models import SequenceTagger

tagger = SequenceTagger(hidden_size=256,
                        embeddings=stacked_embeddings,
                        tag_dictionary=tag_dict,
                        tag_type='ner')

# initialize trainer
from flair.trainers import ModelTrainer

trainer = ModelTrainer(tagger, corpus)

# Start training
trainer.train('/main/tagger_model',
              max_epochs=100,
              embeddings_storage_mode='gpu', # for DGX
             )
&lt;/denchmark-code&gt;

Comment out the different corpus lines, to choose which to load.
If doing a raw import in python of my data set lines, they all look like this:
&lt;denchmark-code&gt;This O/n
is O/n
a S-tag1/n
sentence O/n
that B-tag2/n
says I-tag2/n
words E-tag2/n
and O/n
stuff O/n
and O/n
other S-tag3/n
things O/n
. O/n
/n
New O/n
sentence O/n
. O/n
/n
&lt;/denchmark-code&gt;

As I said, this has worked fine for me up until now.
Expected behavior
It doesn't make any sense why if I load corpus with BIOFID, it runs fine. But if I load using direct ColumnCorpus, with a smaller data set, it fails for CUDA out of memory?
Screenshots
Running the BIOFID data (which is larger), this happens:
&lt;denchmark-code&gt;2020-07-24 19:36:56,384 Reading data from /root/.flair/datasets/biofid
2020-07-24 19:36:56,384 Train: /root/.flair/datasets/biofid/train.conll
2020-07-24 19:36:56,384 Dev: /root/.flair/datasets/biofid/dev.conll
2020-07-24 19:36:56,384 Test: /root/.flair/datasets/biofid/test.conll
2020-07-24 19:37:10,757 ----------------------------------------------------------------------------------------------------
2020-07-24 19:37:10,758 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): WordEmbeddings('glove')
    (list_embedding_1): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.05, inplace=False)
        (encoder): Embedding(300, 100)
        (rnn): LSTM(100, 2048)
        (decoder): Linear(in_features=2048, out_features=300, bias=True)
      )
    )
    (list_embedding_2): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.05, inplace=False)
        (encoder): Embedding(300, 100)
        (rnn): LSTM(100, 2048)
        (decoder): Linear(in_features=2048, out_features=300, bias=True)
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=4196, out_features=4196, bias=True)
  (rnn): LSTM(4196, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=28, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2020-07-24 19:37:10,758 ----------------------------------------------------------------------------------------------------
2020-07-24 19:37:10,758 Corpus: "Corpus: 12668 train + 1584 dev + 1584 test sentences"
2020-07-24 19:37:10,758 ----------------------------------------------------------------------------------------------------
2020-07-24 19:37:10,758 Parameters:
2020-07-24 19:37:10,759  - learning_rate: "0.1"
2020-07-24 19:37:10,759  - mini_batch_size: "32"
2020-07-24 19:37:10,759  - patience: "3"
2020-07-24 19:37:10,759  - anneal_factor: "0.5"
2020-07-24 19:37:10,759  - max_epochs: "100"
2020-07-24 19:37:10,759  - shuffle: "True"
2020-07-24 19:37:10,759  - train_with_dev: "False"
2020-07-24 19:37:10,759  - batch_growth_annealing: "False"
2020-07-24 19:37:10,759 ----------------------------------------------------------------------------------------------------
2020-07-24 19:37:10,759 Model training base path: "/main/tagger_model"
2020-07-24 19:37:10,760 ----------------------------------------------------------------------------------------------------
2020-07-24 19:37:10,760 Device: cuda:0
2020-07-24 19:37:10,760 ----------------------------------------------------------------------------------------------------
2020-07-24 19:37:10,760 Embeddings storage mode: gpu
2020-07-24 19:37:10,768 ----------------------------------------------------------------------------------------------------
2020-07-24 19:37:27,231 epoch 1 - iter 39/396 - loss 33.68186672 - samples/sec: 75.83
2020-07-24 19:37:47,254 epoch 1 - iter 78/396 - loss 31.62588775 - samples/sec: 64.39
2020-07-24 19:38:03,018 epoch 1 - iter 117/396 - loss 28.28389192 - samples/sec: 82.45
2020-07-24 19:38:20,701 epoch 1 - iter 156/396 - loss 26.10188338 - samples/sec: 73.24
2020-07-24 19:38:37,111 epoch 1 - iter 195/396 - loss 24.65146203 - samples/sec: 79.35
2020-07-24 19:38:55,441 epoch 1 - iter 234/396 - loss 23.43243355 - samples/sec: 70.57
2020-07-24 19:39:16,644 epoch 1 - iter 273/396 - loss 22.45316207 - samples/sec: 61.05
2020-07-24 19:39:32,507 epoch 1 - iter 312/396 - loss 21.40785006 - samples/sec: 82.18
2020-07-24 19:39:50,482 epoch 1 - iter 351/396 - loss 20.65745848 - samples/sec: 72.04
2020-07-24 19:40:07,186 epoch 1 - iter 390/396 - loss 19.95446966 - samples/sec: 77.82
2020-07-24 19:40:09,851 ----------------------------------------------------------------------------------------------------
2020-07-24 19:40:09,851 EPOCH 1 done: loss 19.8492 - lr 0.1000000
2020-07-24 19:40:18,529 DEV : loss 9.141180038452148 - score 0.3995
2020-07-24 19:40:18,544 BAD EPOCHS (no improvement): 0
saving best model
2020-07-24 19:40:22,735 ----------------------------------------------------------------------------------------------------
2020-07-24 19:40:31,986 epoch 2 - iter 39/396 - loss 12.15399767 - samples/sec: 134.99
2020-07-24 19:40:43,023 epoch 2 - iter 78/396 - loss 12.09782454 - samples/sec: 120.13
^C2020-07-24 19:40:46,969 ----------------------------------------------------------------------------------------------------
2020-07-24 19:40:46,970 Exiting from training early.
2020-07-24 19:40:46,970 Saving model ...
2020-07-24 19:40:51,071 Done.
2020-07-24 19:40:51,072 ----------------------------------------------------------------------------------------------------
2020-07-24 19:40:51,072 Testing using best model ...
2020-07-24 19:40:51,073 loading file /main/tagger_model/best-model.pt
2020-07-24 19:41:04,678 0.4290  0.3603  0.3917
2020-07-24 19:41:04,679
MICRO_AVG: acc 0.3920 - f1-score 0.3917
MACRO_AVG: acc 0.3202 - f1-score 0.3197
LOC        tp: 90 - fp: 192 - fn: 640 - tn: 94 - precision: 0.3191 - recall: 0.1233 - accuracy: 0.1811 - f1-score: 0.1779
ORG        tp: 0 - fp: 16 - fn: 72 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000
OTHER      tp: 83 - fp: 259 - fn: 707 - tn: 83 - precision: 0.2427 - recall: 0.1051 - accuracy: 0.1466 - f1-score: 0.1466
PER        tp: 179 - fp: 230 - fn: 217 - tn: 179 - precision: 0.4377 - recall: 0.4520 - accuracy: 0.4447 - f1-score: 0.4447
TAX        tp: 703 - fp: 1024 - fn: 672 - tn: 705 - precision: 0.4071 - recall: 0.5113 - accuracy: 0.4536 - f1-score: 0.4533
TME        tp: 337 - fp: 132 - fn: 163 - tn: 335 - precision: 0.7186 - recall: 0.6740 - accuracy: 0.6949 - f1-score: 0.6956
2020-07-24 19:41:04,679 ----------------------------------------------------------------------------------------------------
&lt;/denchmark-code&gt;

(note: I only let it run for 1 epoch here, but assume that if it makes it through an epoch then CUDA out of memory won't happen, I have let it run for a few epochs as well and no error)
Running my data (which is smaller), this happens:
&lt;denchmark-code&gt;2020-07-24 19:44:31,034 Reading data from /main/experiments/experiment_3/combined_data
2020-07-24 19:44:31,034 Train: /main/experiments/experiment_3/combined_data/train_IOBES_6
2020-07-24 19:44:31,034 Dev: /main/experiments/experiment_3/combined_data/valid_IOBES_6
2020-07-24 19:44:31,034 Test: /main/experiments/experiment_3/combined_data/test_IOBES
2020-07-24 19:44:40,912 ----------------------------------------------------------------------------------------------------
2020-07-24 19:44:40,913 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): WordEmbeddings('glove')
    (list_embedding_1): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.05, inplace=False)
        (encoder): Embedding(300, 100)
        (rnn): LSTM(100, 2048)
        (decoder): Linear(in_features=2048, out_features=300, bias=True)
      )
    )
    (list_embedding_2): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.05, inplace=False)
        (encoder): Embedding(300, 100)
        (rnn): LSTM(100, 2048)
        (decoder): Linear(in_features=2048, out_features=300, bias=True)
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=4196, out_features=4196, bias=True)
  (rnn): LSTM(4196, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=28, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2020-07-24 19:44:40,913 ----------------------------------------------------------------------------------------------------
2020-07-24 19:44:40,913 Corpus: "Corpus: 7614 train + 846 dev + 70 test sentences"
2020-07-24 19:44:40,913 ----------------------------------------------------------------------------------------------------
2020-07-24 19:44:40,913 Parameters:
2020-07-24 19:44:40,913  - learning_rate: "0.1"
2020-07-24 19:44:40,913  - mini_batch_size: "32"
2020-07-24 19:44:40,913  - patience: "3"
2020-07-24 19:44:40,914  - anneal_factor: "0.5"
2020-07-24 19:44:40,914  - max_epochs: "100"
2020-07-24 19:44:40,914  - shuffle: "True"
2020-07-24 19:44:40,914  - train_with_dev: "False"
2020-07-24 19:44:40,914  - batch_growth_annealing: "False"
2020-07-24 19:44:40,914 ----------------------------------------------------------------------------------------------------
2020-07-24 19:44:40,914 Model training base path: "/main/tagger_model"
2020-07-24 19:44:40,914 ----------------------------------------------------------------------------------------------------
2020-07-24 19:44:40,914 Device: cuda:0
2020-07-24 19:44:40,914 ----------------------------------------------------------------------------------------------------
2020-07-24 19:44:40,914 Embeddings storage mode: gpu
2020-07-24 19:44:40,920 ----------------------------------------------------------------------------------------------------
Traceback (most recent call last):
  File "train_taggerModel.py", line 60, in &lt;module&gt;
    embeddings_storage_mode='gpu', # for DGX
  File "/opt/conda/lib/python3.6/site-packages/flair/trainers/trainer.py", line 349, in train
    loss = self.model.forward_loss(batch_step)
  File "/opt/conda/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py", line 509, in forward_loss
    return self._calculate_loss(features, data_points)
  File "/opt/conda/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py", line 643, in _calculate_loss
    forward_score = self._forward_alg(features, lengths)
  File "/opt/conda/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py", line 840, in _forward_alg
    cloned = forward_var.clone()
RuntimeError: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 31.72 GiB total capacity; 30.56 GiB already allocated; 11.88 MiB free; 30.64 GiB reserved in total by PyTorch)
&lt;/denchmark-code&gt;


All code is running in Docker container maintained by Nvidia for PyTorch: &lt;denchmark-link:https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel_20-06.html#rel_20-06&gt;https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel_20-06.html#rel_20-06&lt;/denchmark-link&gt;


Ubuntu 18.04
Python 3.6.10
PyTorch 1.6.0a0+9907a3e

And I built the image using Flair 0.5.0
	</description>
	<comments>
		<comment id='1' author='tylerlekang' date='2020-07-24T20:28:17Z'>
		Also, I have tried running it with my data using all kinds of other parameters.
I've done 'in_memory=False' on ColumnCorpus. I've done embeddings storage as 'cpu' and 'none'. And I've set the mini batch size smaller than the default (which is 32), such as 16, 8, and 4. I've tried setting num_workers=0 (which helped fix another error on the Language Model training).
None of these work. They always give CUDA out of memory. I could try setting the mini batch even smaller, but: 1) that would be painfully slow, and 2) it seems ridiculous when the default settings run just fine with BOIFID.
		</comment>
		<comment id='2' author='tylerlekang' date='2020-07-30T20:45:24Z'>
		Some updates:

since the error came from some lines within the CRF part of the output network, I did try the run again with use_crf=False in the Sequence Tagger model init. This did allow the run to complete with no errors, but at a pretty decent cost in accuracy/F1 score. So obviously the better solution would be to fix the error.
I also tried changing the prefixes to IOB and while this does cut down on the tag dictionary size, it still causes the CUDA out of memory error. Also in testing this against other smaller runs that ran with the IOBES, it does also cost a small amount of acc/F1. Which makes sense to me as it might be harder for the network to differentiate the B- prefix when it can mean both single word and beginning of a span.
so I suspect something in the code responsible for the CRF. But I still don't understand why the run would work with the BIOFID() dataset. Nothing in there seems to turn off CRF automatically.

Obviously there are lots and lots of people who've had CUDA out of memory errors, of all kinds and for all reasons. Even some other issues I saw on Flair Issues. But I wonder if it could be related to something like this? &lt;denchmark-link:https://discuss.pytorch.org/t/cuda-out-of-memory-error-when-training-a-simple-bilstm/71330&gt;https://discuss.pytorch.org/t/cuda-out-of-memory-error-when-training-a-simple-bilstm/71330&lt;/denchmark-link&gt;
  It basically seemed to boil down to not (enough?)  methods being called?
		</comment>
		<comment id='3' author='tylerlekang' date='2020-08-13T11:55:00Z'>
		Hello &lt;denchmark-link:https://github.com/tylerlekang&gt;@tylerlekang&lt;/denchmark-link&gt;
 sorry for the late response. Setting embedding_storage_mode to 'cpu' combined with in_memory=True for the corpus is the way to go here if you have enough RAM. Otherwise, set it to 'none'. You should not set embedding storage mode to 'gpu' since normally you don't have enough CUDA memory.
The problem is likely caused by one or multiple really long sentences that get packed into the same mini-batch. It could be that your dataset has longer sentences than BIOFID? You could try setting mini_batch_chunk_size=1 in the trainer. If that works, you can try setting mini_batch_size to a higher value again. Alternatively, you could try filtering out long sentences: sometimes a dataset has only a handful really long sentences as outlier, these can be filtered.
We have a method for this in master branch which will be part of the next Flair release:
corpus.filter_long_sentences(250)
		</comment>
		<comment id='4' author='tylerlekang' date='2020-08-13T15:06:25Z'>
		Hello &lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 ,
Based on my understanding of the Flair language model, which is built on character-level and not word level, shouldn't the sequence length be set by the language model? If so, then I don't understand how the length of the sentence is the problem. I'm sure I need a better understanding of the fine details for the whole sequence tagging model. How do you suggest the best way to do this? The code seems very dense. But I suppose if enough print statements are put in, some sense of how it fits together under the hood could be parsed out. Or maybe PyTorch has some good summary and/or visualization options? What do you recommend?
I did try all types of embedding_storage_mode settings, they never helped with the error. I don't recall which of those I tried in combination with in_memory=False (I believe the True setting is the default). But I did try that setting too, and it also did not help. Our GPU has 32GB memory, so seems unlikely that it would fill up, unless there was some kind of memory leak?
The other curiosity for me is, why would setting use_crf=False solve the problem? It seems if there was a memory leak or some other related error, the problem should then be confined in that section of code?
Thank you very much!
		</comment>
		<comment id='5' author='tylerlekang' date='2020-08-13T22:14:11Z'>
		The language model in the Flair embeddings will try to tag any text you pass to it. Normally it is fine, but if the text is very long, the resulting hidden states might be too much to fit into cuda memory. But this is only a theory, I am not sure if this is the problem with your data. If you filter long sentences perhaps we can verify if this is the problem.
Also not sure about the CRF. Do you have very many tags in your dataset? Normally it should not make so much difference memory-wise.
		</comment>
		<comment id='6' author='tylerlekang' date='2020-12-12T00:11:04Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>