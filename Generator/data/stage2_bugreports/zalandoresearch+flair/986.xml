<bug id='986' author='BernierCR' open_date='2019-08-09T17:19:45Z' closed_time='2019-09-17T20:52:53Z'>
	<summary>find_learning_rate step problem</summary>
	<description>
Describe the bug
find_learning_rate does not work. It will use the learning rate from iteration 2 for all iterations after 2.
To Reproduce
This will happen with any application of find_learning_rate. You can reproduce with the tutorial code if you like or any code.
Expected behavior
I expect iteration 3 to use the learning rate for iteration 3, and 4 for 4, and so on. It should end at the maximum learning rate set.
Screenshots
If applicable, add screenshots to help explain your problem.
Environment (please complete the following information):

OS: Linux of course
Version MASTER

Additional context
scheduler = ExpAnnealLR(optimizer, end_learning_rate, iterations)
scheduler.step(1)
learning_rate = scheduler.get_lr()[0]
def get_lr(self):
iteration = self.last_epoch + 1
pct = iteration / self.iterations
return [base_lr * (self.end_lr / base_lr) ** pct for base_lr in self.base_lrs]
So get_lr is iterating based on last_epoch. Though it's really supposed to be iterating based on mini-batches. Anyways, it gets worse.
ExpAnnealLR does not have it's own step method. So it uses the base class one.
def step(self, epoch=None):
if epoch is None:
epoch = self.last_epoch + 1
self.last_epoch = epoch
for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):
param_group['lr'] = lr
This function expects the correct epoch number to be passed to it.  Fine.
But it's used like this in this case.
scheduler.step(1)
This makes the epoch always come out as two.
so either

find_learning_rate needs to pass the correct iteration number to step
or,
the ExpAnnealLR  class needs it's own step function to overide the default, that will correctly increment itself.

Considering nothing else uses ExpAnnealLR, I'm going with solution 2 in my copy. You do what you like in your package.
	</description>
	<comments>
		<comment id='1' author='BernierCR' date='2019-08-09T18:26:35Z'>
		Also, in class _LRScheduler ,
last_epoch is not saved. It should be saved to self.last_epoch. But this comes from torch, so I just set it in ExpAnnealLR.
Sorry I can't publish my actual changes. I'd need company approval to contribute, so I better do a bunch more work on this before going through that long approval process.
		</comment>
		<comment id='2' author='BernierCR' date='2019-08-09T20:48:40Z'>
		I also changed the logic to use a repeating iterator instead of the oneshot kind, to loop through batches. People with a low number of mini_batches relative to training set size, need more then one epochs worth of iterations, so it needs to go back to the beginning and keep going until it diverges, or max iterations is reached.
		</comment>
		<comment id='3' author='BernierCR' date='2019-09-17T10:00:18Z'>
		&lt;denchmark-link:https://github.com/BernierCR&gt;@BernierCR&lt;/denchmark-link&gt;
 thanks for sharing these findings. I've added this to the learning rate finder, i.e. iterating over multiple epochs if necessary and increasing the step manually.
		</comment>
	</comments>
</bug>