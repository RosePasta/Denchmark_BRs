<bug id='816' author='angeloziletti' open_date='2019-06-19T06:56:36Z' closed_time='2019-06-20T08:59:32Z'>
	<summary>Non-deterministic behaviour for BERT embeddings</summary>
	<description>
Describe the bug
I am using FLAIR (great software, thanks a lot!) to get BERT embeddings.
For a given input, I get different BERT embeddings every time I run the code.
This might be a PyTorch related issue (&lt;denchmark-link:https://github.com/huggingface/transformers/issues/695&gt;huggingface/transformers#695&lt;/denchmark-link&gt;
)
Is there a way to get deterministic BERT embeddings using FLAIR?
	</description>
	<comments>
		<comment id='1' author='angeloziletti' date='2019-06-19T07:19:01Z'>
		Hi &lt;denchmark-link:https://github.com/angeloziletti&gt;@angeloziletti&lt;/denchmark-link&gt;
 ,
could you provide a short code snippet that shows how you use the BERT embeddings?
		</comment>
		<comment id='2' author='angeloziletti' date='2019-06-20T08:59:32Z'>
		I just found out that in my code there was an operation that did not mainted the order of the words in a sentence (I overlooked that before because I was using word2vec). That was causing the problem.
BERT embeddings are indeed deterministic within FLAIR.
Apologies for the confusion, and thanks a lot for your fast reply.
		</comment>
	</comments>
</bug>