<bug id='1092' author='shoarora' open_date='2019-09-13T01:34:26Z' closed_time='2019-09-16T17:52:46Z'>
	<summary>token.get_embedding() with multiple embedding types returns in different order</summary>
	<description>
token.get_embedding() does not necessarily return the embeddings in deterministic order across different environments.
This is hugely problematic when trying to transfer a model to a different runtime environment (local vs remote) and/or when flair's embeddings are being fed into a different service.  Models are trained to expect certain sub-vectors in certain positions in the concatenated final vector.
token.get_embedding() sorts the embeddings by key when concatenating them into a single vector to return.
&lt;denchmark-code&gt;embeddings = [
        self._embeddings[embed] for embed in sorted(self._embeddings.keys())
]
&lt;/denchmark-code&gt;

The keys come from self.name properties of the Embedding classes they came from.  However, these names are often just the paths to the model, which are hardly deterministic across environments, especially if we're loading our own weights.
A concrete example:
Locally:
&lt;denchmark-code&gt;stack = StackedEmbeddings [
        WordEmbeddings('glove'),  # self.name is cached path.  '/root/.gensim' or something.
        FlairEmbeddings('local/fine_tuned/embeddings.pt')
]
&lt;/denchmark-code&gt;

On token.get_embedding() this will put Flair first, since it comes first alphabetically when you sort their keys.
Deployment environment:
&lt;denchmark-code&gt;stack = StackedEmbeddings [
        WordEmbeddings('glove'),  # self.name is cached path.  '/root/.gensim' or something.
        FlairEmbeddings('/z/extra/resources/go/somewhere/different/embeddings.pt')
]
&lt;/denchmark-code&gt;

On token.get_embedding() this will put Glove first, since it comes first alphabetically when you sort their keys.
This will break a model trained on these embeddings when you try to deploy.  Once we tracked this down, our workaround is to override self.name after init.  Documentation on this functionality, or eliminating the need for this would be great.
	</description>
	<comments>
		<comment id='1' author='shoarora' date='2019-09-13T09:31:14Z'>
		&lt;denchmark-link:https://github.com/shoarora&gt;@shoarora&lt;/denchmark-link&gt;
 thanks for pointing this out. If you train a model such as a  or so with embeddings, the saved model will contain the original  of each embedding (since embeddings get packaged into the trained model), so we haven't encountered this error yet even though we prequently deploy models trained on one setup to another.
But in the case as you describe where you set different paths in different environments and have a service that just consumes these embeddings, this will cause an error. One easy fix would be to have the StackedEmbeddings add a prefix to each embedding name in the constructor and so force the order to always be the same. I can put in a PR for this!
		</comment>
		<comment id='2' author='shoarora' date='2019-09-16T17:53:16Z'>
		&lt;denchmark-link:https://github.com/shoarora&gt;@shoarora&lt;/denchmark-link&gt;
 this should now be fixed in master branch. Could you check if this solution works for you?
		</comment>
		<comment id='3' author='shoarora' date='2019-09-16T17:56:14Z'>
		
@shoarora this should now be fixed in master branch. Could you check if this solution works for you?

looks great.  thank you!
		</comment>
	</comments>
</bug>