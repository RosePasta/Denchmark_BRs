<bug id='445' author='gccome' open_date='2019-02-01T18:46:59Z' closed_time='2019-02-06T16:02:36Z'>
	<summary>sequence tagger has evaluation issue</summary>
	<description>
I think sequence tagger also has the same issue as &lt;denchmark-link:https://github.com/flairNLP/flair/pull/439&gt;#439&lt;/denchmark-link&gt;
 because of
&lt;denchmark-code&gt;             for tag, gold in gold_tags:
                    if (tag, gold) not in predicted_tags:
                        metric.add_fn(tag)
                    else:
                        metric.add_tn(tag)
&lt;/denchmark-code&gt;

in _evaluate_sequence_tagger()
I am having the incorrect results:
tp: 2834 - fp: 671 - fn: 1057 - tn: 2834 - precision: 0.8086 - recall: 0.7283 - accuracy: 0.7664 - f1-score: 0.7664
	</description>
	<comments>
		<comment id='1' author='gccome' date='2019-02-01T18:48:19Z'>
		Ah, thanks for spotting this! Care to do another pull request? :)
		</comment>
		<comment id='2' author='gccome' date='2019-02-01T18:54:09Z'>
		I am quite new to github, but let me try my best:) I believe it is a great learning process for me.
		</comment>
		<comment id='3' author='gccome' date='2019-02-02T09:10:54Z'>
		Generally speaking, moving Metric class to a new file, separating evaluation from trainer and adding some unit tests would be very helpful. Is it ok if I introduce such refactoring and unit tests in future PR, &lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
  ?
		</comment>
		<comment id='4' author='gccome' date='2019-02-02T12:14:22Z'>
		&lt;denchmark-link:https://github.com/kubapok&gt;@kubapok&lt;/denchmark-link&gt;
 sure that would be great! How would you separate evaluation from the trainer?
		</comment>
		<comment id='5' author='gccome' date='2019-02-04T18:58:14Z'>
		&lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 I gave it a second thought. Does it really make sense to calculate  for sequence tagger? If so, how to define  for each type of tag? It is different from text classification where  for each label can be easily calculated.
		</comment>
		<comment id='6' author='gccome' date='2019-02-06T16:02:36Z'>
		Yes I guess that is the problem - perhaps we should not count true negatives for sequence tagging. I'll close the issue then, but feel free to reopen if you have further comments!
		</comment>
		<comment id='7' author='gccome' date='2019-02-06T16:18:56Z'>
		Yes we should not count true negatives. We can ignore tn in the output at this point of time. Or we can simply comment out metric.add_tn(tag), then tn will be initiated as 0.
		</comment>
	</comments>
</bug>