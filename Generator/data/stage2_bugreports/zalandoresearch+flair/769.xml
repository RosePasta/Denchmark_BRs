<bug id='769' author='teichert' open_date='2019-05-30T21:38:09Z' closed_time='2019-05-31T15:23:06Z'>
	<summary>embed function on StackedEmbeddings returns nothing</summary>
	<description>
WordEmbeddings work for me as expected, but calling .embed on a StackedEmbedding doesn't return any result:
import flair, torch
from flair.embeddings import CharacterEmbeddings, StackedEmbeddings 
from flair.data import Sentence 
flair.device = torch.device('cpu') 
s = Sentence('this is a test') 
embedding = CharacterEmbeddings() 
stacked_embedding = StackedEmbeddings([embedding]) 
print(embedding.embed(s)) 
print(stacked_embedding.embed(s))
... gives the following output:

[Sentence: "this is a test" - 4 Tokens]
None


I expected to get:

[Sentence: "this is a test" - 4 Tokens]
[Sentence: "this is a test" - 4 Tokens]


I'm using Python 3.7.3 and flair 0.4.1 on a Linux machine (with device=cpu).
It seems unlikely that this would go unnoticed until now, so I'm expecting someone will point out what I'm doing wrong.  I got the same behavior if I stacked a longer list of embeddings, and I got the same result with other types of embeddings.  I'm using CharacterEmbeddings to avoid needing to download a large model to run my example.
	</description>
	<comments>
		<comment id='1' author='teichert' date='2019-05-31T09:09:15Z'>
		The intended behavior (according to the &lt;denchmark-link:https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_3_WORD_EMBEDDING.md&gt;docs&lt;/denchmark-link&gt;
) is that the embeddings are added to the Sentence object when you call embeddings.embed(s).
You can then inspect the sentence itself to get the embeddings e.g.:
stacked_embedding.embed(s)
for token in s:
    print(token)
    print(token.embedding)
Maybe the return statement could be removed from the &lt;denchmark-link:https://github.com/zalandoresearch/flair/blob/1634152c1f9e28aa692ed1ddd4f87f8c98980cdf/flair/embeddings.py#L78&gt;Embeddings base class&lt;/denchmark-link&gt;
 for consistency. As far is I can see this return value is only used when embedding the dummy sentence to determine the embedding length.
		</comment>
		<comment id='2' author='teichert' date='2019-05-31T15:23:06Z'>
		Okay, that makes sense.  Thanks!  So what I should have checked was the following:
import flair, torch
from flair.embeddings import CharacterEmbeddings, StackedEmbeddings 
from flair.data import Sentence 
flair.device = torch.device('cpu') 
embedding = CharacterEmbeddings()
stacked_embedding = StackedEmbeddings([embedding])
s1 = Sentence('this is a test')
s2 = Sentence('this is a test')
embedding.embed(s1)
assert(len(s1[0].embedding) == 50)  # s1 is modified to include the embedding
assert(len(s2[0].embedding) == 0)  # s2 has no embedding yet
stacked_embedding.embed(s2)
assert(len(s2[0].embedding) == 50)  # s2 now has the embedding
embedding.embed(s2)
assert(len(s2[0].embedding) == 50)  # s2 including the embedding again doesn't change it
And that indeed works.  (Also, it looks like the "adding" is done in a smart way so that if the same embedding is added twice it is still only included once.) Thank you again.
		</comment>
	</comments>
</bug>