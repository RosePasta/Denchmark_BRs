<bug id='421' author='lisette-garciamoya' open_date='2019-01-24T11:55:21Z' closed_time='2019-01-31T14:11:59Z'>
	<summary>CUDA Error during CharacterEmbeddings</summary>
	<description>
My code
from flair.embeddings import CharacterEmbeddings

sentence = Sentence('La casa es muy bonita.', use_tokenizer=True)

embedding = CharacterEmbeddings()
embedding.embed(sentence)

for token in sentence:
    print(token)
    print(token.embedding)

&lt;denchmark-link:https://user-images.githubusercontent.com/18166309/51676189-296d4480-1fd6-11e9-885f-12ba930e925c.png&gt;&lt;/denchmark-link&gt;

Environment:

OS: Ubuntu 18.04.1
Version: code from master branch
Nvidia:


	</description>
	<comments>
		<comment id='1' author='lisette-garciamoya' date='2019-01-24T17:35:26Z'>
		Thanks for reporting :) I think this could be fixed by:
Current &lt;denchmark-link:https://github.com/zalandoresearch/flair/blob/master/flair/embeddings.py#L386&gt;code&lt;/denchmark-link&gt;
:
# chars for rnn processing
chars = torch.LongTensor(tokens_mask)
chars = chars.to(flair.device)

character_embeddings = self.char_embedding(chars).transpose(0, 1)
Fix:
# chars for rnn processing
chars = torch.LongTensor(tokens_mask)
chars = chars.to(flair.device)
chars = chars.detach().cpu()   # &lt;-- added

character_embeddings = self.char_embedding(chars).transpose(0, 1)
		</comment>
		<comment id='2' author='lisette-garciamoya' date='2019-01-25T09:25:47Z'>
		Thank you. It solves the problem.
		</comment>
		<comment id='3' author='lisette-garciamoya' date='2019-01-25T17:55:38Z'>
		Could you also try this fix?
# chars for rnn processing
chars = torch.LongTensor(tokens_mask)
chars = chars.cpu()   # &lt;-- added (don't detach!)

character_embeddings = self.char_embedding(chars).transpose(0, 1)
I think that if you detach the vector, the gradients cannot flow into the character model during training and features are not trained. I.e. the character features would stay random.
If you do not detach with the code above, training will be slower but this way the character features are always trained on the downstream task like proposed by Lample et al., 2016.
		</comment>
		<comment id='4' author='lisette-garciamoya' date='2019-01-29T10:17:26Z'>
		It seemed to be fixed but now I get the same error while executing the real program.
(I tested it with .detach() and without .detach() -&gt; same error)
		</comment>
		<comment id='5' author='lisette-garciamoya' date='2019-01-31T05:48:16Z'>
		I think same error occurred when I tried the BERT tutorial example.
It is also about "RuntimeError: Expected object of backend CPU but got backend CUDA for argument &lt;denchmark-link:https://github.com/flairNLP/flair/issues/3&gt;#3&lt;/denchmark-link&gt;
 'index'".
		</comment>
		<comment id='6' author='lisette-garciamoya' date='2019-01-31T12:36:52Z'>
		Hello &lt;denchmark-link:https://github.com/lisette-garciamoya&gt;@lisette-garciamoya&lt;/denchmark-link&gt;
 what do you mean by executing the real program?
		</comment>
		<comment id='7' author='lisette-garciamoya' date='2019-01-31T13:07:29Z'>
		Hi &lt;denchmark-link:https://github.com/lisette-garciamoya&gt;@lisette-garciamoya&lt;/denchmark-link&gt;
 - I was able to understand where the error is coming from. In fact, the original code of the CharacterEmbeddings class is correct.
However, when you instantiate the CharacterEmbeddings, by default it is only instantiated on CPU. The ModelTrainer then puts it on GPU which is why the training works. But if you instantiate the CharacterEmbeddings yourself, it is only on CPU even if you are on a GPU machine, which causes the error.
For now, the simplest fix is to do this:
 from flair.embeddings import CharacterEmbeddings

sentence = Sentence('La casa es muy bonita.', use_tokenizer=True)

embedding = CharacterEmbeddings()
embeddings = embeddings.cuda() # add this line to put the embeddings on CUDA
embedding.embed(sentence)

for token in sentence:
    print(token)
    print(token.embedding)
Could you test if this works for you?
We will also set up a PR that fixes this behavior. Default behavior should be that embeddings are instantiated on cuda if available.
Thanks for finding this error and reporting it!
		</comment>
		<comment id='8' author='lisette-garciamoya' date='2019-01-31T14:13:13Z'>
		Should be fixed by the latest PR. Feel free to reopen if there are still issues!
Thanks again for reporting the error!
		</comment>
		<comment id='9' author='lisette-garciamoya' date='2019-02-25T06:58:11Z'>
		
Hi @lisette-garciamoya - I was able to understand where the error is coming from. In fact, the original code of the CharacterEmbeddings class is correct.
However, when you instantiate the CharacterEmbeddings, by default it is only instantiated on CPU. The ModelTrainer then puts it on GPU which is why the training works. But if you instantiate the CharacterEmbeddings yourself, it is only on CPU even if you are on a GPU machine, which causes the error.
For now, the simplest fix is to do this:
 from flair.embeddings import CharacterEmbeddings

sentence = Sentence('La casa es muy bonita.', use_tokenizer=True)

embedding = CharacterEmbeddings()
embeddings = embeddings.cuda() # add this line to put the embeddings on CUDA
embedding.embed(sentence)

for token in sentence:
    print(token)
    print(token.embedding)
Could you test if this works for you?
We will also set up a PR that fixes this behavior. Default behavior should be that embeddings are instantiated on cuda if available.
Thanks for finding this error and reporting it!

To me, I used
from flair.embeddings import BertEmbeddings
from flair.data import Sentence
from flair.embeddings import FlairEmbeddings
&lt;denchmark-h:h1&gt;init embedding&lt;/denchmark-h&gt;

flair_embedding_forward = FlairEmbeddings('news-forward')
bert_embedding = BertEmbeddings('bert-large-cased').cuda()
&lt;denchmark-h:h1&gt;create a sentence&lt;/denchmark-h&gt;

sentence = Sentence('The grass is green .')
&lt;denchmark-h:h1&gt;embed words in sentence&lt;/denchmark-h&gt;

x = bert_embedding.embed(sentence)
for token in sentence:
print(token)
print(token.embedding)
it works
		</comment>
	</comments>
</bug>