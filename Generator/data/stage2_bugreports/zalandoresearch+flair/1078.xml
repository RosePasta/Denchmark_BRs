<bug id='1078' author='maciek16180' open_date='2019-09-06T13:01:31Z' closed_time='2019-09-10T11:32:45Z'>
	<summary>`embeddings_storage_mode` not impacting GPU memory consumption</summary>
	<description>
Describe the bug
embeddings_storage_mode parameter has no effect on GPU memory usage or training speed during training of SequenceTagger.
To Reproduce
I train SequenceTagger using pretrained FlairEmbeddings (forward + backward) and finetuned BertEmbeddings.
Parameters other than embeddings_storage_mode:
FlairEmbeddings

hidden_size=1024
mini_batch_size=200
sequence_length=250

BertEmbeddings: default values from bert-base-uncased
SequenceTagger training

hidden_size=128
mini_batch_size=8
I use apex for mixed-precision training

Expected behavior
I expected GPU memory consumption to depend on the embeddings_storage_mode parameter. Meanwhile, there is seemingly no difference, as shown in the charts below. Additionally, I expected the none setting to drastically slow down training after 1st epoch, but it doesn't happen. I attach the training logs from all three settings.


&lt;denchmark-link:https://user-images.githubusercontent.com/53229577/64427472-9939c680-d0b1-11e9-8d21-51a4db5adf18.png&gt;&lt;/denchmark-link&gt;


&lt;denchmark-link:https://user-images.githubusercontent.com/53229577/64427475-9ccd4d80-d0b1-11e9-92a3-2666b3cd0361.png&gt;&lt;/denchmark-link&gt;


&lt;denchmark-link:https://user-images.githubusercontent.com/53229577/64427482-9fc83e00-d0b1-11e9-8016-af38984d757e.png&gt;&lt;/denchmark-link&gt;


&lt;denchmark-link:https://github.com/zalandoresearch/flair/files/3583976/none.txt&gt;none.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/zalandoresearch/flair/files/3583978/gpu.txt&gt;gpu.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/zalandoresearch/flair/files/3583977/cpu.txt&gt;cpu.txt&lt;/denchmark-link&gt;

Environment

GPU: RTX 2080 Ti (11GB memory)
Ubuntu 16.04.5
Python 3.7
CUDA 10.0.130, apex 0.1
flair 0.4.3
spacy 2.1.8
pytorch_transformers 1.1.0


The training sessions are made with a downsampled dataset (). When I try to train on the entire dataset (150k sentences), I get a CUDA out of memory error after about 1/10 of an epoch. This seems to be already mentioned here: &lt;denchmark-link:https://github.com/flairNLP/flair/issues/563#issuecomment-515323911&gt;#563 (comment)&lt;/denchmark-link&gt;
.
	</description>
	<comments>
		<comment id='1' author='maciek16180' date='2019-09-06T13:06:57Z'>
		&lt;denchmark-link:https://github.com/maciek16180&gt;@maciek16180&lt;/denchmark-link&gt;
 thanks for reporting this. Do you get CUDA oom memory errors on the large dataset even with embeddings_storage_mode set to 'cpu' or 'gpu'? Could you paste the specific oom error?
		</comment>
		<comment id='2' author='maciek16180' date='2019-09-06T13:29:50Z'>
		&lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 As far as I remember I do run into memory problems regardless of this setting. Sometimes I get  and sometimes . Unfortunatelly I have only training logs, which don't include the stack trace. I just run it again, with  and , which crashed in half an hour last time. I will let you know.
		</comment>
		<comment id='3' author='maciek16180' date='2019-09-06T13:42:52Z'>
		&lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 Meanwhile, could you answer one question? I thought when  works properly, if I set it to , the GPU memory usage should not depend on the dataset size. Is that correct? Or is there something else (other than embeddings) that needs to be stored in GPU memory for each sentence in the dataset?
		</comment>
		<comment id='4' author='maciek16180' date='2019-09-06T13:53:24Z'>
		Yes that is (mostly) correct - there should be no buildup of stored embeddings in GPU memory because they are stored on CPU.
However, in practice you often see a buildup anyway. The reason for this is that the GPU always takes as much memory as needed to do a mini-batch. So, every time you encounter a mini-batch that was larger than everything that came before (for instance, a batch with really long sentences), the GPU takes on additional memory in order to be able to handle this batch. That's why it would be interesting to see the specific error thrown in your case; it could be that the problem is a mini-batch with a really long sentence.
		</comment>
		<comment id='5' author='maciek16180' date='2019-09-06T14:03:27Z'>
		Thanks, I was expected something like this. Hovewer, in my case, there are no sentences longer than 128 tokens. Also, the GPU memory consumption raises linearly, which would be strange if this was the reason. So it looks like I can hope that this is a matter of embeddings not being moved out of the GPU properly.
		</comment>
		<comment id='6' author='maciek16180' date='2019-09-06T14:10:34Z'>
		Here is the error I got this time (the console log):

2019-09-06 13:26:30,250 ----------------------------------------------------------------------------------------------------
2019-09-06 13:26:30,250 Corpus: "Corpus: 31977 train + 6847 dev + 6862 test sentences"
2019-09-06 13:26:30,250 ----------------------------------------------------------------------------------------------------
2019-09-06 13:26:30,250 Parameters:
2019-09-06 13:26:30,250  - learning_rate: "0.1"
2019-09-06 13:26:30,250  - mini_batch_size: "8"
2019-09-06 13:26:30,250  - patience: "3"
2019-09-06 13:26:30,251  - anneal_factor: "0.5"
2019-09-06 13:26:30,251  - max_epochs: "152"
2019-09-06 13:26:30,251  - shuffle: "True"
2019-09-06 13:26:30,251  - train_with_dev: "False"
2019-09-06 13:26:30,251 ----------------------------------------------------------------------------------------------------
2019-09-06 13:26:30,251 Model training base path: "/root/model/models/sequence_tagging/model"
2019-09-06 13:26:30,251 ----------------------------------------------------------------------------------------------------
2019-09-06 13:26:30,251 Device: cuda:1
2019-09-06 13:26:30,251 ----------------------------------------------------------------------------------------------------
2019-09-06 13:26:30,251 Embeddings storage mode: 'cpu'
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.
Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
2019-09-06 13:26:30,263 ----------------------------------------------------------------------------------------------------
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
2019-09-06 13:26:30,920 epoch 1 - iter 0/3998 - loss 36.26910400 - samples/sec: 4875.31
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
2019-09-06 13:32:47,695 epoch 1 - iter 399/3998 - loss 6.04767312 - samples/sec: 8.48
2019-09-06 13:39:15,944 epoch 1 - iter 798/3998 - loss 4.77969909 - samples/sec: 8.22
2019-09-06 13:46:03,326 epoch 1 - iter 1197/3998 - loss 4.31963900 - samples/sec: 7.84
2019-09-06 13:52:52,490 epoch 1 - iter 1596/3998 - loss 3.98768912 - samples/sec: 7.80
2019-09-06 13:59:52,177 epoch 1 - iter 1995/3998 - loss 3.74694794 - samples/sec: 7.61
ERROR: [pid 753] Worker Worker(salt=290287918, workers=1, host=486d5369062d, username=root, pid=753) failed    TrainSequenceTagger(use_forward_flair_embeddings=True, use_backward_flair_embe
ddings=True, use_glove_embeddings=False, use_bert_embeddings=True, bert_model=custom, downsample=0.2, max_tokens=128, hidden_size=258, max_epochs=152, mini_batch_size=8, torch_seed=42, embe
ddings_storage_mode='cpu', use_mixed_precision=True, model_dirname=model)
Traceback (most recent call last):
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/luigi/worker.py", line 199, in run
new_deps = self._run_get_new_deps()
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/luigi/worker.py", line 152, in _run_get_new_deps
requires = task_gen.send(next_send)
File "/root/model/src/tasks/model_tagger.py", line 143, in run
use_amp=self.use_mixed_precision,
File "/root/model/src/models/custom_flair_trainers/custom_model_trainer.py", line 240, in train
loss = self.model.forward_loss(batch)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/flair/models/sequence_tagger_model.py", line 344, in forward_loss
features = self.forward(data_points)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/flair/models/sequence_tagger_model.py", line 456, in forward
rnn_output, hidden = self.rnn(packed)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/torch/nn/modules/module.py", line 547, in call
result = self.forward(*input, **kwargs)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/torch/nn/modules/rnn.py", line 562, in forward
return self.forward_packed(input, hx)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/torch/nn/modules/rnn.py", line 554, in forward_packed
output, hidden = self.forward_impl(input, hx, batch_sizes, max_batch_size, sorted_indices)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/torch/nn/modules/rnn.py", line 529, in forward_impl
self.num_layers, self.dropout, self.training, self.bidirectional)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/apex/amp/wrap.py", line 264, in wrapper
return orig_fn(*new_args)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/apex/amp/rnn_compat.py", line 9, in wrapper
return getattr(_VF, name)(*args, **kwargs)
RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED
2019-09-06 14:04:16,946 [pid 753] Worker Worker(salt=290287918, workers=1, host=486d5369062d, username=root, pid=753) failed    TrainSequenceTagger(use_forward_flair_embeddings=True, use_ba
ckward_flair_embeddings=True, use_glove_embeddings=False, use_bert_embeddings=True, bert_model=custom, downsample=0.2, max_tokens=128, hidden_size=258, max_epochs=152, mini_batch_size=8, to
rch_seed=42, embeddings_storage_mode='cpu', use_mixed_precision=True, model_dirname=model)
Traceback (most recent call last):
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/luigi/worker.py", line 199, in run
new_deps = self._run_get_new_deps()
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/luigi/worker.py", line 152, in _run_get_new_deps
requires = task_gen.send(next_send)
File "/root/model/src/tasks/model_tagger.py", line 143, in run
use_amp=self.use_mixed_precision,
File "/root/model/src/models/custom_flair_trainers/custom_model_trainer.py", line 240, in train
loss = self.model.forward_loss(batch)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/flair/models/sequence_tagger_model.py", line 344, in forward_loss
features = self.forward(data_points)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/flair/models/sequence_tagger_model.py", line 456, in forward
rnn_output, hidden = self.rnn(packed)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/torch/nn/modules/module.py", line 547, in call
result = self.forward(*input, **kwargs)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/torch/nn/modules/rnn.py", line 562, in forward
return self.forward_packed(input, hx)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/torch/nn/modules/rnn.py", line 554, in forward_packed
output, hidden = self.forward_impl(input, hx, batch_sizes, max_batch_size, sorted_indices)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/torch/nn/modules/rnn.py", line 529, in forward_impl
self.num_layers, self.dropout, self.training, self.bidirectional)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/apex/amp/wrap.py", line 264, in wrapper
return orig_fn(*new_args)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/apex/amp/rnn_compat.py", line 9, in wrapper
return getattr(_VF, name)(*args, **kwargs)
RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED

And the memory consumption:
&lt;denchmark-link:https://user-images.githubusercontent.com/9105882/64434434-d5285800-d0c0-11e9-84a8-e2dc91e991aa.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='maciek16180' date='2019-09-06T14:40:03Z'>
		Thanks for the additional info. I haven't seen this error before - I wonder if this is somehow related to apex. Could you do a training run with use_amp=False to see if you get the error there as well?
		</comment>
		<comment id='8' author='maciek16180' date='2019-09-06T14:45:29Z'>
		Yes, but I have to wait until tomorrow. Do you think it's possible to still somehow train this on a big dataset using current flair, maybe by preparing a bunch of smaller files and training a few epochs on each? Or should I wait for the fix?
		</comment>
		<comment id='9' author='maciek16180' date='2019-09-06T14:53:16Z'>
		It should be possible without splitting the dataset - we've trained models on huge datasets with the current version by setting the storage mode to 'none' or using machines with lots of memory and setting storage mode to 'cpu'. So this should work, we just need to find out what the problem is and if necessary fix it.
		</comment>
		<comment id='10' author='maciek16180' date='2019-09-06T15:40:51Z'>
		Hey, im basically having the same problem when using the Textclassifier with the Flairembeddings. The cpu usage doesn't change when cpu as embedding memory storage option. It only uses gpu memory.
		</comment>
		<comment id='11' author='maciek16180' date='2019-09-06T15:58:22Z'>
		&lt;denchmark-link:https://github.com/AhmedIdr&gt;@AhmedIdr&lt;/denchmark-link&gt;
 thanks for reporting this - I'll take a closer look!
		</comment>
		<comment id='12' author='maciek16180' date='2019-09-07T12:33:22Z'>
		&lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 I run it again, with the same parameters, but without apex. I got the same error:

2019-09-07 11:47:04,842 ----------------------------------------------------------------------------------------------------
2019-09-07 11:47:04,842 Corpus: "Corpus: 31977 train + 6847 dev + 6862 test sentences"
2019-09-07 11:47:04,842 ----------------------------------------------------------------------------------------------------
2019-09-07 11:47:04,842 Parameters:
2019-09-07 11:47:04,842  - learning_rate: "0.1"
2019-09-07 11:47:04,842  - mini_batch_size: "8"
2019-09-07 11:47:04,842  - patience: "3"
2019-09-07 11:47:04,842  - anneal_factor: "0.5"
2019-09-07 11:47:04,842  - max_epochs: "152"
2019-09-07 11:47:04,842  - shuffle: "True"
2019-09-07 11:47:04,842  - train_with_dev: "False"
2019-09-07 11:47:04,842 ----------------------------------------------------------------------------------------------------
2019-09-07 11:47:04,843 Model training base path: "/root/model/models/sequence_tagging/ner-cuda012"
2019-09-07 11:47:04,843 ----------------------------------------------------------------------------------------------------
2019-09-07 11:47:04,843 Device: cuda:0
2019-09-07 11:47:04,843 ----------------------------------------------------------------------------------------------------
2019-09-07 11:47:04,843 Embeddings storage mode: 'cpu'
2019-09-07 11:47:04,844 ----------------------------------------------------------------------------------------------------
2019-09-07 11:47:05,484 epoch 1 - iter 0/3998 - loss 36.26685333 - samples/sec: 5079.10
2019-09-07 11:53:19,862 epoch 1 - iter 399/3998 - loss 5.58143167 - samples/sec: 8.53
2019-09-07 11:59:55,140 epoch 1 - iter 798/3998 - loss 4.54265042 - samples/sec: 8.08
2019-09-07 12:06:45,380 epoch 1 - iter 1197/3998 - loss 4.15768358 - samples/sec: 7.78
2019-09-07 12:13:18,016 epoch 1 - iter 1596/3998 - loss 3.87608014 - samples/sec: 8.13
ERROR: [pid 434] Worker Worker(salt=825260296, workers=1, host=486d5369062d, username=root, pid=434) failed    TrainSequenceTagger(use_forward_flair_embeddings=True, use_backward_flair_embe
ddings=True, use_glove_embeddings=False, use_bert_embeddings=True, bert_model=custom, downsample=0.2, max_tokens=128, hidden_size=258, max_epochs=152, mini_batch_size=8, torch_seed=42, embe
ddings_storage_mode='cpu', use_mixed_precision=False, model_dirname=ner-cuda012)
Traceback (most recent call last):
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/luigi/worker.py", line 199, in run
new_deps = self._run_get_new_deps()
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/luigi/worker.py", line 152, in _run_get_new_deps
requires = task_gen.send(next_send)
File "/root/model/src/tasks/model_tagger.py", line 143, in run
use_amp=self.use_mixed_precision,
File "/root/model/src/models/custom_flair_trainers/custom_model_trainer.py", line 240, in train
loss = self.model.forward_loss(batch)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/flair/models/sequence_tagger_model.py", line 344, in forward_loss
features = self.forward(data_points)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/flair/models/sequence_tagger_model.py", line 456, in forward
rnn_output, hidden = self.rnn(packed)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/torch/nn/modules/module.py", line 547, in call
result = self.forward(*input, **kwargs)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/torch/nn/modules/rnn.py", line 562, in forward
return self.forward_packed(input, hx)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/torch/nn/modules/rnn.py", line 554, in forward_packed
output, hidden = self.forward_impl(input, hx, batch_sizes, max_batch_size, sorted_indices)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/torch/nn/modules/rnn.py", line 529, in forward_impl
self.num_layers, self.dropout, self.training, self.bidirectional)
RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED
2019-09-07 12:14:50,672 [pid 434] Worker Worker(salt=825260296, workers=1, host=486d5369062d, username=root, pid=434) failed    TrainSequenceTagger(use_forward_flair_embeddings=True, use_ba
ckward_flair_embeddings=True, use_glove_embeddings=False, use_bert_embeddings=True, bert_model=custom, downsample=0.2, max_tokens=128, hidden_size=258, max_epochs=152, mini_batch_size=8, to
rch_seed=42, embeddings_storage_mode='cpu', use_mixed_precision=False, model_dirname=ner-cuda012)
Traceback (most recent call last):
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/luigi/worker.py", line 199, in run
new_deps = self._run_get_new_deps()
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/luigi/worker.py", line 152, in _run_get_new_deps
requires = task_gen.send(next_send)
File "/root/model/src/tasks/model_tagger.py", line 143, in run
use_amp=self.use_mixed_precision,
File "/root/model/src/models/custom_flair_trainers/custom_model_trainer.py", line 240, in train
loss = self.model.forward_loss(batch)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/flair/models/sequence_tagger_model.py", line 344, in forward_loss
features = self.forward(data_points)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/flair/models/sequence_tagger_model.py", line 456, in forward
rnn_output, hidden = self.rnn(packed)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/torch/nn/modules/module.py", line 547, in call
result = self.forward(*input, **kwargs)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/torch/nn/modules/rnn.py", line 562, in forward
return self.forward_packed(input, hx)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/torch/nn/modules/rnn.py", line 554, in forward_packed
output, hidden = self.forward_impl(input, hx, batch_sizes, max_batch_size, sorted_indices)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/torch/nn/modules/rnn.py", line 529, in forward_impl
self.num_layers, self.dropout, self.training, self.bidirectional)
RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED

&lt;denchmark-link:https://user-images.githubusercontent.com/9105882/64474929-dc606c00-d17b-11e9-8530-72df8eda9faf.png&gt;&lt;/denchmark-link&gt;

One more thing: you can see custom_model_trainer.py in stack trace, but this is just copy-pasted ModelTrainer with a few additional lines of logging in a train function. So it should't make any difference.
		</comment>
		<comment id='13' author='maciek16180' date='2019-09-09T07:03:08Z'>
		&lt;denchmark-link:https://github.com/maciek16180&gt;@maciek16180&lt;/denchmark-link&gt;
 thanks for the additional info. My suspicion now is that the  operation does not delete the original cuda tensor, so it is only  to cpu, not moved. I'll look into this today and report back.
		</comment>
		<comment id='14' author='maciek16180' date='2019-09-09T11:18:45Z'>
		&lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 I did a very simple test for myself and it looks like that's the case.
&lt;denchmark-link:https://user-images.githubusercontent.com/53229577/64526532-ddbca080-d303-11e9-9436-2c132e6d8fb0.png&gt;&lt;/denchmark-link&gt;

The docs say that Tensor.to() method has a copy parameter (False by default) but it works in a different way than I thought :)
		</comment>
		<comment id='15' author='maciek16180' date='2019-09-09T11:40:18Z'>
		&lt;denchmark-link:https://github.com/stermedia-team&gt;@stermedia-team&lt;/denchmark-link&gt;
 thanks :) I'm now a bit confused as to what the copy parameter actually does.
&lt;denchmark-link:https://github.com/maciek16180&gt;@maciek16180&lt;/denchmark-link&gt;
 I've tried reproducing the error on my machine, but I cannot; if I set the storage mode to 'cpu' I can process huge datasets that I cannot with embedding storage more 'gpu'. With storage mode set to 'cpu', GPU memory usage is flat so everything seems to work as it should. The reason may be that as soon as we copy the tensor over to CPU memory with  it becomes out of scope (no pointers point to it anymore) which means the GPU version gets garbage collected.
I see some reference to workers in your error message. Could it be that you're running the program in some environment that somehow hinders garbage collection?
		</comment>
		<comment id='16' author='maciek16180' date='2019-09-09T11:54:36Z'>
		Regarding GPU memory consumption, looking at nvidia-smi may give an incorrect idea of what is happening. Basically, the memory is freed only when required or when garbage collection is force (). Therefore it appear to only increase.
More info there: &lt;denchmark-link:https://discuss.pytorch.org/t/how-can-we-release-gpu-memory-cache/14530/3&gt;https://discuss.pytorch.org/t/how-can-we-release-gpu-memory-cache/14530/3&lt;/denchmark-link&gt;

Regarding speed, when training NER with storage set to CPU, epoch 1 is on my computer 1.4X slower than epoch 2 and more (which makes sense, representations are not anymore to be built).
&lt;denchmark-link:https://github.com/maciek16180&gt;@maciek16180&lt;/denchmark-link&gt;
 can you check with this version of the lib  (it includes many optimizations not yet pushed with some focusing on reducing memory foot print)
		</comment>
		<comment id='17' author='maciek16180' date='2019-09-09T13:16:17Z'>
		&lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 I am running it inside a  &lt;denchmark-link:https://luigi.readthedocs.io/en/stable/&gt;https://luigi.readthedocs.io/en/stable/&lt;/denchmark-link&gt;
 pipeline. I don't know if that affects anything, but I will check it.
&lt;denchmark-link:https://github.com/pommedeterresautee&gt;@pommedeterresautee&lt;/denchmark-link&gt;
 Thank you, I will see if I can run it on .
		</comment>
		<comment id='18' author='maciek16180' date='2019-09-09T13:35:39Z'>
		&lt;denchmark-link:https://github.com/pommedeterresautee&gt;@pommedeterresautee&lt;/denchmark-link&gt;
 I installed  using your command and re-run the script. I got an error much faster:

ERROR: [pid 1363] Worker Worker(salt=779589082, workers=1, host=486d5369062d, username=root, pid=1363) failed    TrainSequenceTagger(use_forward_flair_embeddings=True, use_backward_flair_em
beddings=True, use_glove_embeddings=False, use_bert_embeddings=True, bert_model=custom, downsample=0.2, max_tokens=128, hidden_size=258, max_epochs=152, mini_batch_size=8, torch_seed=42, em
beddings_storage_mode='cpu', use_mixed_precision=True, model_dirname=cuda013)
Traceback (most recent call last):
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/luigi/worker.py", line 199, in run
new_deps = self._run_get_new_deps()
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/luigi/worker.py", line 152, in _run_get_new_deps
requires = task_gen.send(next_send)
File "/root/model/src/tasks/model_tagger.py", line 143, in run
use_amp=self.use_mixed_precision,
File "/root/model/src/models/custom_flair_trainers/custom_model_trainer.py", line 240, in train
loss = self.model.forward_loss(batch)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/flair/models/sequence_tagger_model.py", line 356, in forward_loss
features = self.forward(data_points)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/flair/models/sequence_tagger_model.py", line 445, in forward
self.embeddings.embed(sentences)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/flair/embeddings.py", line 165, in embed
embedding.embed(sentences)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/flair/embeddings.py", line 85, in embed
self._add_embeddings_internal(sentences)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/flair/embeddings.py", line 2140, in _add_embeddings_internal
all_encoder_layers = self.model(all_input_ids, attention_mask=all_input_masks)[
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/torch/nn/modules/module.py", line 547, in call
result = self.forward(*input, **kwargs)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/pytorch_transformers/modeling_bert.py", line 715, in forward
head_mask=head_mask)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/torch/nn/modules/module.py", line 547, in call
result = self.forward(*input, **kwargs)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/pytorch_transformers/modeling_bert.py", line 437, in forward
layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/torch/nn/modules/module.py", line 547, in call
result = self.forward(*input, **kwargs)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/pytorch_transformers/modeling_bert.py", line 415, in forward
attention_outputs = self.attention(hidden_states, attention_mask, head_mask)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/torch/nn/modules/module.py", line 547, in call
result = self.forward(*input, **kwargs)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/pytorch_transformers/modeling_bert.py", line 372, in forward
self_outputs = self.self(input_tensor, attention_mask, head_mask)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/torch/nn/modules/module.py", line 547, in call
result = self.forward(*input, **kwargs)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/pytorch_transformers/modeling_bert.py", line 311, in forward
attention_scores = attention_scores + attention_mask
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/apex/amp/wrap.py", line 57, in wrapper
kwargs)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/apex/amp/utils.py", line 78, in casted_args
new_args.append(cast_fn(x))
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/apex/amp/utils.py", line 71, in maybe_float
return x.float()
RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.73 GiB total capacity; 9.31 GiB already allocated; 5.56 MiB free; 504.71 MiB cached)

		</comment>
		<comment id='19' author='maciek16180' date='2019-09-09T13:40:03Z'>
		This version is more rapid than last released version, it may explain why you get much faster OOM
Have you used it with Luigi?
		</comment>
		<comment id='20' author='maciek16180' date='2019-09-09T13:49:39Z'>
		&lt;denchmark-link:https://github.com/pommedeterresautee&gt;@pommedeterresautee&lt;/denchmark-link&gt;
 Yes, with luigi. I don't have a script ready to run it without luigi, I'm preparing it right now.
		</comment>
		<comment id='21' author='maciek16180' date='2019-09-09T14:52:46Z'>
		&lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 I just run it without luigi and it looks like it works as expected. The GPU memory consumption stays flat on flair 0.4.3 when I set the storage mode to "cpu". I would really like to use luigi though, do you have any insight into what may cause this behavior?
EDIT: I might have spoken too soon. The memory consumption seems to still raise a bit, but it's much slower increase and possibly not a problem. I will let you know if it was able to finish.
&lt;denchmark-link:https://user-images.githubusercontent.com/9105882/64542098-3a7b8380-d323-11e9-96b8-397970b378e8.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='22' author='maciek16180' date='2019-09-09T18:45:24Z'>
		Thanks, the slow increase is likely when it reserves more memory for a particularly large mini-batch.
		</comment>
		<comment id='23' author='maciek16180' date='2019-09-09T20:45:19Z'>
		&lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 You're right about that, it eventually stabilized. However, it looks like this time I have a problem with apex:

2019-09-09 15:33:38,481 epoch 1 - iter 2793/3998 - loss 3.32563293 - samples/sec: 7.50
2019-09-09 15:40:29,867 epoch 1 - iter 3192/3998 - loss 3.19705379 - samples/sec: 7.76
2019-09-09 15:48:01,277 epoch 1 - iter 3591/3998 - loss 3.10942468 - samples/sec: 7.07
2019-09-09 15:55:09,689 epoch 1 - iter 3990/3998 - loss 3.03135505 - samples/sec: 7.45
2019-09-09 15:55:14,812 ----------------------------------------------------------------------------------------------------
2019-09-09 15:55:14,812 EPOCH 1 done: loss 3.0284 - lr 0.1000
2019-09-09 16:11:12,235 DEV : loss 2.259472131729126 - score 0.8061
2019-09-09 16:11:13,025 BAD EPOCHS (no improvement): 0
2019-09-09 16:11:13,643 ----------------------------------------------------------------------------------------------------
Traceback (most recent call last):
File "src/tasks/train_noluigi.py", line 117, in 
use_amp=use_mixed_precision,
File "/root/model/src/models/custom_flair_trainers/custom_model_trainer.py", line 240, in train
loss = self.model.forward_loss(batch)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/flair/models/sequence_tagger_model.py", line 344, in forward_loss
features = self.forward(data_points)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/flair/models/sequence_tagger_model.py", line 424, in forward
[token.get_embedding().unsqueeze(0) for token in sentence], 0
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/flair/models/sequence_tagger_model.py", line 424, in 
[token.get_embedding().unsqueeze(0) for token in sentence], 0
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/flair/data.py", line 240, in get_embedding
return torch.cat(embeddings, dim=0)
File "/root/.local/share/virtualenvs/model-tCvPbRU9/lib/python3.7/site-packages/apex/amp/wrap.py", line 85, in wrapper
return orig_fn(cast_seq, *args, **kwargs)
RuntimeError: Expected object of scalar type Half but got scalar type Float for sequence element 2 in sequence argument at position #1 'tensors'

This is strange, because in the first post I attached logs where it run fine (on smaller dataset and hidden_size, with luigi).
		</comment>
		<comment id='24' author='maciek16180' date='2019-09-10T08:08:23Z'>
		I have the very same error
&lt;denchmark-link:https://github.com/flairNLP/flair/issues/1036&gt;#1036&lt;/denchmark-link&gt;

		</comment>
		<comment id='25' author='maciek16180' date='2019-09-10T11:32:45Z'>
		I found the source of the problem, it was me ü§¶‚Äç‚ôÇÔ∏è
It's the way of parsing arguments from config file.

2019-09-07 11:47:04,843 Embeddings storage mode: 'cpu'

See the apostrophes? They shouldn't be there.
I will close this, because now it boils down to &lt;denchmark-link:https://github.com/flairNLP/flair/issues/1036&gt;#1036&lt;/denchmark-link&gt;

I guess I got mislead by the fact that it didn't say anything about invalid parameter value, but it's my fault anyway. Sorry to waste your time.
		</comment>
	</comments>
</bug>