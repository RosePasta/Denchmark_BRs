<bug id='1808' author='stephenhage' open_date='2020-08-13T18:10:04Z' closed_time='2020-12-19T14:05:27Z'>
	<summary>ModelTrainer Requiring Sampler</summary>
	<description>
Describe the bug
I have a pandas data frame of text and labels for a classification problem. This is legal text and I am trying to fine-tune a language model to the specific sections (each as a document) of the larger corpus. When I execute the trainer.train() function I get this error:
ValueError: num_samples should be a positive integer value, but got num_samples=0
Of note, I am following tutorial in section 3.2 &lt;denchmark-link:https://towardsdatascience.com/text-classification-with-state-of-the-art-nlp-library-flair-b541d7add21f&gt;here&lt;/denchmark-link&gt;
.
To Reproduce
I split my data frame into train/dev/test sets saved as csv files (train.csv, dev.csv, and test.csv) with two columns: label and text. I've tried this both with tab and comma delimiters. They are all saved in the same folder as my Jupyter notebook.
This is the code block I am trying to execute:
&lt;denchmark-code&gt;from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings
from flair.models import TextClassifier
from flair.trainers import ModelTrainer
from pathlib import Path
corpus = NLPTaskDataFetcher.load_classification_corpus(Path('./'), 
                                                       test_file='test.csv', 
                                                       dev_file='dev.csv', 
                                                       train_file='train.csv')
word_embeddings = [WordEmbeddings('glove'), 
                   FlairEmbeddings('news-forward-fast'), 
                   FlairEmbeddings('news-backward-fast')]
document_embeddings = DocumentLSTMEmbeddings(word_embeddings, 
                                             hidden_size=512, 
                                             reproject_words=True, 
                                             reproject_words_dimension=256)
classifier = TextClassifier(document_embeddings, 
                            label_dictionary=corpus.make_label_dictionary(), 
                            multi_label=False)
trainer = ModelTrainer(classifier, corpus)
trainer.train(Path('./'),
              learning_rate=0.1,
              mini_batch_size=8,
              max_epochs = 5)
&lt;/denchmark-code&gt;

I can execute each line without failure up until trainer.train(Path('./'),.... That appears to be the troublesome section.
Expected behavior
Based on the tutorial I expected this code to create a language model that is tuned to the particular text in the train/test/dev sets.
Environment (please complete the following information):

OS: Mac OS 10.15.4
Python: 3.7.6
Version: flair-0.5.1

Additional context
The full output from the code above is:
&lt;denchmark-code&gt;2020-08-13 13:02:23,708 Reading data from .
2020-08-13 13:02:23,711 Train: train.csv
2020-08-13 13:02:23,711 Dev: dev.csv
2020-08-13 13:02:23,712 Test: test.csv
/Users/stephenhage/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated function (or staticmethod) load_classification_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.
  if __name__ == '__main__':
2020-08-13 13:02:25,099 Computing label dictionary. Progress:
/Users/stephenhage/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated method __init__. (The functionality of this class is moved to 'DocumentRNNEmbeddings') -- Deprecated since version 0.4.
  app.launch_new_instance()
0it [00:00, ?it/s]
2020-08-13 13:02:25,102 []
2020-08-13 13:02:25,107 ----------------------------------------------------------------------------------------------------
2020-08-13 13:02:25,108 Model: "TextClassifier(
  (document_embeddings): DocumentLSTMEmbeddings(
    (embeddings): StackedEmbeddings(
      (list_embedding_0): WordEmbeddings('glove')
      (list_embedding_1): FlairEmbeddings(
        (lm): LanguageModel(
          (drop): Dropout(p=0.25, inplace=False)
          (encoder): Embedding(275, 100)
          (rnn): LSTM(100, 1024)
          (decoder): Linear(in_features=1024, out_features=275, bias=True)
        )
      )
      (list_embedding_2): FlairEmbeddings(
        (lm): LanguageModel(
          (drop): Dropout(p=0.25, inplace=False)
          (encoder): Embedding(275, 100)
          (rnn): LSTM(100, 1024)
          (decoder): Linear(in_features=1024, out_features=275, bias=True)
        )
      )
    )
    (word_reprojection_map): Linear(in_features=2148, out_features=256, bias=True)
    (rnn): GRU(256, 512)
    (dropout): Dropout(p=0.5, inplace=False)
  )
  (decoder): Linear(in_features=512, out_features=0, bias=True)
  (loss_function): CrossEntropyLoss()
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2020-08-13 13:02:25,109 ----------------------------------------------------------------------------------------------------
2020-08-13 13:02:25,121 Corpus: "Corpus: 0 train + 0 dev + 0 test sentences"
2020-08-13 13:02:25,122 ----------------------------------------------------------------------------------------------------
2020-08-13 13:02:25,123 Parameters:
2020-08-13 13:02:25,124  - learning_rate: "0.1"
2020-08-13 13:02:25,125  - mini_batch_size: "8"
2020-08-13 13:02:25,126  - patience: "3"
2020-08-13 13:02:25,127  - anneal_factor: "0.5"
2020-08-13 13:02:25,128  - max_epochs: "5"
2020-08-13 13:02:25,129  - shuffle: "True"
2020-08-13 13:02:25,130  - train_with_dev: "False"
2020-08-13 13:02:25,131  - batch_growth_annealing: "False"
2020-08-13 13:02:25,132 ----------------------------------------------------------------------------------------------------
2020-08-13 13:02:25,132 Model training base path: "."
2020-08-13 13:02:25,133 ----------------------------------------------------------------------------------------------------
2020-08-13 13:02:25,134 Device: cpu
2020-08-13 13:02:25,135 ----------------------------------------------------------------------------------------------------
2020-08-13 13:02:25,136 Embeddings storage mode: cpu
2020-08-13 13:02:25,138 ----------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-7-15c181ba8be6&gt; in &lt;module&gt;
     22               learning_rate=0.1,
     23               mini_batch_size=8,
---&gt; 24               max_epochs = 5)

~/opt/anaconda3/lib/python3.7/site-packages/flair/trainers/trainer.py in train(self, base_path, learning_rate, mini_batch_size, mini_batch_chunk_size, max_epochs, scheduler, anneal_factor, patience, initial_extra_patience, min_learning_rate, train_with_dev, monitor_train, monitor_test, embeddings_storage_mode, checkpoint, save_final_model, anneal_with_restarts, anneal_with_prestarts, batch_growth_annealing, shuffle, param_selection_mode, num_workers, sampler, use_amp, amp_opt_level, eval_on_train_fraction, eval_on_train_shuffle, **kwargs)
    314                     shuffle=shuffle,
    315                     num_workers=num_workers,
--&gt; 316                     sampler=sampler,
    317                 )
    318 

~/opt/anaconda3/lib/python3.7/site-packages/flair/datasets/base.py in __init__(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, drop_last, timeout, worker_init_fn)
     59             drop_last=drop_last,
     60             timeout=timeout,
---&gt; 61             worker_init_fn=worker_init_fn,
     62         )
     63 

~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py in __init__(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator)
    222             else:  # map-style
    223                 if shuffle:
--&gt; 224                     sampler = RandomSampler(dataset, generator=generator)
    225                 else:
    226                     sampler = SequentialSampler(dataset)

~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/sampler.py in __init__(self, data_source, replacement, num_samples, generator)
     94         if not isinstance(self.num_samples, int) or self.num_samples &lt;= 0:
     95             raise ValueError("num_samples should be a positive integer "
---&gt; 96                              "value, but got num_samples={}".format(self.num_samples))
     97 
     98     @property

ValueError: num_samples should be a positive integer value, but got num_samples=0
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='stephenhage' date='2020-08-13T18:12:26Z'>
		Apologies for the bad formatting in the original post...updated to make it easier to read. If I've neglected to include any details I'm happy to do so.
		</comment>
		<comment id='2' author='stephenhage' date='2020-08-14T10:05:14Z'>
		Hello &lt;denchmark-link:https://github.com/stephenhage&gt;@stephenhage&lt;/denchmark-link&gt;
 the code in the tutorial is a bit outdated as for instance the  has been deprecated for a while. You can find current instructions &lt;denchmark-link:https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md#training-a-text-classification-model&gt;here&lt;/denchmark-link&gt;
 (train classifier with LSTM) and &lt;denchmark-link:https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md#training-a-text-classification-model-with-transformer&gt;here&lt;/denchmark-link&gt;
 (train classifier with transformer, ).
In both cases though, no language model is trained. This code will train a classifier.
		</comment>
		<comment id='3' author='stephenhage' date='2020-12-12T13:48:55Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>