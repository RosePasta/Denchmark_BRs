<bug id='1744' author='tylerlekang' open_date='2020-07-06T22:33:16Z' closed_time='2020-07-09T08:34:57Z'>
	<summary>FlairEmbeddings function gives ValueError in python 3.6 (latest Nvidia Pytorch Docker container)</summary>
	<description>

Simply running the code  gives a , in Python 3.6.10 (Conda), which is the python environment included in the most recent PyTorch Docker container from Nvidia. (&lt;denchmark-link:https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel_20-06.html#rel_20-06&gt;https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel_20-06.html#rel_20-06&lt;/denchmark-link&gt;
)
Here is the error message:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "fineTune_langModel.py", line 10, in &lt;module&gt;
    language_model = FlairEmbeddings('news-forward').lm
  File "/opt/conda/lib/python3.6/site-packages/flair/embeddings/token.py", line 578, in __init__
    self.lm: LanguageModel = LanguageModel.load_language_model(model)
  File "/opt/conda/lib/python3.6/site-packages/flair/models/language_model.py", line 202, in load_language_model
    dropout=state["dropout"],
  File "/opt/conda/lib/python3.6/site-packages/flair/models/language_model.py", line 63, in __init__
    self.to(flair.device)
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 465, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.6/site-packages/flair/models/language_model.py", line 404, in _apply
    for info in torch.__version__.replace("+",".").split('.') if info.isdigit())
ValueError: not enough values to unpack (expected at least 3, got 2)
&lt;/denchmark-code&gt;

To Reproduce
Run this code in the container (after pip installing flair):
from flair.embeddings import FlairEmbeddings
FlairEmbeddings('news-forward')
Expected behavior
The function should work with no problems (in particular, the .lm is intended to be given to the LanguageModelTrainer function).
Environment (please complete the following information):

Docker container running Linux (all details of Ubuntu, Python, PyTorch, etc. versions is in the Nvidia link above)
Flair version is 0.5

Additional context
Running the code in Python 3.7.7 (Conda) gives no problems.
	</description>
	<comments>
		<comment id='1' author='tylerlekang' date='2020-07-07T09:46:09Z'>
		&lt;denchmark-link:https://github.com/tylerlekang&gt;@tylerlekang&lt;/denchmark-link&gt;
 thanks for reporting this. Could you print the torch version you get with ? Also, can you try updating to Flair 0.5.1?
		</comment>
		<comment id='2' author='tylerlekang' date='2020-07-07T14:53:12Z'>
		&lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
  reports  , which matches as shown in &lt;denchmark-link:https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel_20-06.html#rel_20-06&gt;https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel_20-06.html#rel_20-06&lt;/denchmark-link&gt;

Used pip install --upgrade flair to upgrade to 0.5.1. The same error persists:
&lt;denchmark-code&gt;&gt;&gt;&gt; flair.__version__
'0.5.1'
&gt;&gt;&gt;
&gt;&gt;&gt; from flair.embeddings import FlairEmbeddings
&gt;&gt;&gt; FlairEmbeddings('news-forward')
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "/opt/conda/lib/python3.6/site-packages/flair/embeddings/token.py", line 586, in __init__
    self.lm: LanguageModel = LanguageModel.load_language_model(model)
  File "/opt/conda/lib/python3.6/site-packages/flair/models/language_model.py", line 202, in load_language_model
    dropout=state["dropout"],
  File "/opt/conda/lib/python3.6/site-packages/flair/models/language_model.py", line 63, in __init__
    self.to(flair.device)
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 465, in to
    return self._apply(convert)
  File "/opt/conda/lib/python3.6/site-packages/flair/models/language_model.py", line 404, in _apply
    for info in torch.__version__.replace("+",".").split('.') if info.isdigit())
ValueError: not enough values to unpack (expected at least 3, got 2)
&lt;/denchmark-code&gt;

Did you test with this container? It seems like a common and important container to verify, as it is official Nvidia optimized for PyTorch applications.
Thank you very much for your support! :)
		</comment>
		<comment id='3' author='tylerlekang' date='2020-07-07T15:03:41Z'>
		&lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
  In the models/language_model.py code, the first line of _apply is (starts at line 402):
&lt;denchmark-code&gt;major, minor, build, *_ = (int(info)
                                for info in torch.__version__.replace("+",".").split('.') if info.isdigit())
&lt;/denchmark-code&gt;

If I simply run torch.__version__.replace("+",".").split('.') in the container (python 3.6.10) it returns ['1', '6', '0a0', '9907a3e']. Then if I run for i in (int(info) for info in torch.__version__.replace("+",".").split('.') if info.isdigit()): print(i) it prints:
&lt;denchmark-code&gt;1
6
&lt;/denchmark-code&gt;

However, on my local machine running vanilla python 3.7.7, the torch version is just 1.5.0. So this may be the problem.
I have no idea why Nvidia has chosen this version of Pytorch with letters in the version number, but they did make this choice and this container is supposed to be an easy solution for highly optimized GPU runs on their hardware.
Do you have any workaround ideas?
		</comment>
		<comment id='4' author='tylerlekang' date='2020-07-07T17:40:01Z'>
		&lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 could I just hardcode the major, minor, build numbers, in my local version of language_model.py if there is no workaround?
&lt;denchmark-code&gt;major = 1
minor = 6
build = 0
&lt;/denchmark-code&gt;

It seems the code just checks that the major.minor is &gt;= 1.4 ? But I don't want to mess up any other parts of the code.
		</comment>
		<comment id='5' author='tylerlekang' date='2020-07-07T19:01:52Z'>
		Yes, I guess you could just overwrite torch.__version__ as a quick fix by calling this before your script:
import torch

torch.__version__ = '1.5.0'
Meanwhile, I will put in a PR to fix the error.
		</comment>
		<comment id='6' author='tylerlekang' date='2020-07-07T19:05:14Z'>
		&lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 just wanting to triple-confirm, that shouldn't cause any problems with the rest of the FlairEmbeddings or LanguageModelTrainer codes? Thank you!
		</comment>
		<comment id='7' author='tylerlekang' date='2020-07-07T19:12:16Z'>
		It shouldn't cause any problems on the flair side. We use the string to determine whether and old version of torch is used (&lt;1.4.0) or not, so changing it to another string that is above 1.4.0 won't change anything.
		</comment>
	</comments>
</bug>