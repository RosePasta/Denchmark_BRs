<bug id='351' author='rookie32' open_date='2019-01-02T17:26:00Z' closed_time='2019-01-17T15:23:14Z'>
	<summary>RuntimeError: storage has wrong size when loading en-sentiment</summary>
	<description>
When trying to load the en-sentiment model, I get the error:
RuntimeError: storage has wrong size: expected -1862414276 got 22700
I am able to load other models such as de-offensive-language and the ner SequenceTagger.
Code:
&lt;denchmark-code&gt;from flair.models import TextClassifier
from flair.data import Sentence
classifier = TextClassifier.load('en-sentiment')
&lt;/denchmark-code&gt;

Possibly related to issue &lt;denchmark-link:https://github.com/flairNLP/flair/issues/172&gt;#172&lt;/denchmark-link&gt;
 ?
But the fix for that was merged back in November? &lt;denchmark-link:https://github.com/flairNLP/flair/pull/194&gt;#194&lt;/denchmark-link&gt;

Have tried deleting and redownloading the model.
Platform: Windows 10, Torch 1.0 (cpu), Python 3.6.4 (Anaconda)
	</description>
	<comments>
		<comment id='1' author='rookie32' date='2019-01-02T19:40:32Z'>
		Hi &lt;denchmark-link:https://github.com/rookie32&gt;@rookie32&lt;/denchmark-link&gt;
 - yes it is the same bug as reported in &lt;denchmark-link:https://github.com/flairNLP/flair/pull/194&gt;#194&lt;/denchmark-link&gt;
, namely that very large models (over 2GB) cannot be de-serialized in certain windows and mac setups. This is a known issue in the pickle library which is used by pytorch to serialize.
We currently have a workaround in place in which we make sure that models do not go over 2GB in size, but overlooked this with the en-sentiment model. We will fix this for the next release!
		</comment>
		<comment id='2' author='rookie32' date='2019-01-02T19:51:54Z'>
		Here also the link to the corresponding &lt;denchmark-link:https://bugs.python.org/issue24658&gt;pickle issue&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='3' author='rookie32' date='2019-01-09T09:45:07Z'>
		I am facing the same issue on my windows machine (windows 10, 8 GB RAM). Do we have a fix for this issue?
		</comment>
		<comment id='4' author='rookie32' date='2019-01-09T14:23:42Z'>
		Yes, we are working on this at the moment. We will push an updated version with the upcoming 0.4.1 release, within the next weeks.
		</comment>
		<comment id='5' author='rookie32' date='2019-01-11T09:29:23Z'>
		It's pretty weird to me that this hasn't been solved, as the discussion about this bug in pickle was reported in 2015.
Anyways, I created a temporary workaround.
Disclaimer: this will use a big amount of RAM. If there's a more memory efficient solution that doesn't change the way Torch serializes the models (i.e. doesn't change serialization.py), let us know
Step 1:
Create a text_classifier.py file, which should be a copy of flair/models/text_classification_model.py
Step 2:
Replace the  function with the following two functions:
&lt;denchmark-link:https://gist.github.com/highway11git/1fa64621ea1b04e6494bf6672a750b24&gt;https://gist.github.com/highway11git/1fa64621ea1b04e6494bf6672a750b24&lt;/denchmark-link&gt;

Step 3:
Instead of using from flair.models import TextClassifier in your project, import the TextClassifier class from your newly created file. Like so: from text_classifier import TextClassifier
		</comment>
		<comment id='6' author='rookie32' date='2019-01-11T17:00:21Z'>
		Hi &lt;denchmark-link:https://github.com/highway11git&gt;@highway11git&lt;/denchmark-link&gt;
 thanks very much for this! We'll test it and see if we can directly integrate it. Perhaps we could add a special flag to the model loader to allow users to specify which method they want to use to load a model, with the default being the current one.
		</comment>
		<comment id='7' author='rookie32' date='2019-01-11T21:06:18Z'>
		I've updated the gist:

load_big_file now uses mmap instead of BytesIO, which makes it much more memory efficient
_load_state now also uses the load_big_file function if the user doesn't use cuda

Thanks for testing it, and keep us up to date on any progress.
		</comment>
		<comment id='8' author='rookie32' date='2019-01-13T12:10:50Z'>
		Cool, thanks! We'll test the solution and let you know!
		</comment>
		<comment id='9' author='rookie32' date='2019-01-16T13:13:22Z'>
		&lt;denchmark-link:https://github.com/highway11git&gt;@highway11git&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/pdsing&gt;@pdsing&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/rookie32&gt;@rookie32&lt;/denchmark-link&gt;
 I've added the fix proposed by &lt;denchmark-link:https://github.com/highway11git&gt;@highway11git&lt;/denchmark-link&gt;
 into a pull request.
Since we could not reproduce the error on our setups, could you check if this fixed it for you?
		</comment>
		<comment id='10' author='rookie32' date='2020-02-03T14:42:08Z'>
		Using the newest Flair and torch on Win 10 I still get this error for (other models worked):
tagger = SequenceTagger.load('frame-fast')
or
tagger = SequenceTagger.load('frame')
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

NotImplementedError                       Traceback (most recent call last)
 in 
----&gt; 1 tagger = SequenceTagger.load('frame-fast')
2
3 sentence_1 = Sentence('George returned to Berlin to return his hat .')
4 sentence_2 = Sentence('He had a look at different hats .')
5
c:\program files\python37\lib\site-packages\flair\nn.py in load(cls, model)
84             # see &lt;denchmark-link:https://github.com/flairNLP/flair/issues/351&gt;#351&lt;/denchmark-link&gt;

85             f = file_utils.load_big_file(str(model_file))
---&gt; 86             state = torch.load(f, map_location=flair.device)
87
88         model = cls._init_model_with_state_dict(state)
c:\program files\python37\lib\site-packages\torch\serialization.py in load(f, map_location, pickle_module, **pickle_load_args)
527             with _open_zipfile_reader(f) as opened_zipfile:
528                 return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
--&gt; 529         return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
530
531
c:\program files\python37\lib\site-packages\torch\serialization.py in _legacy_load(f, map_location, pickle_module, **pickle_load_args)
700     unpickler = pickle_module.Unpickler(f, **pickle_load_args)
701     unpickler.persistent_load = persistent_load
--&gt; 702     result = unpickler.load()
703
704     deserialized_storage_keys = pickle_module.load(f, **pickle_load_args)
c:\program files\python37\lib\pathlib.py in new(cls, *args, **kwargs)
1023         if not self._flavour.is_supported:
1024             raise NotImplementedError("cannot instantiate %r on your system"
-&gt; 1025                                       % (cls.name,))
1026         self._init()
1027         return self
NotImplementedError: cannot instantiate 'PosixPath' on your system
		</comment>
	</comments>
</bug>