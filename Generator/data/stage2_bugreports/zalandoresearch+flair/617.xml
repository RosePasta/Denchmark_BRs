<bug id='617' author='fstorme' open_date='2019-03-19T11:04:15Z' closed_time='2020-05-07T03:15:50Z'>
	<summary>Best model not saved during training</summary>
	<description>
Hi,
I'm training a NER tagger with BERT embeddings that are not fine tuned, and I don't use a CRF as the final classification layer.
Here is my code:
import datetime

from flair.embeddings import StackedEmbeddings, BertEmbeddings
from flair.trainers import ModelTrainer
from flair.models import SequenceTagger
from flair.data_fetcher import NLPTaskDataFetcher, NLPTask
from flair.data import TaggedCorpus

corpus: TaggedCorpus = NLPTaskDataFetcher.load_corpus(NLPTask.WIKINER_FRENCH).downsample(0.01)

tag_type = 'ner'
tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)
embedding_types = [
    BertEmbeddings('bert-base-multilingual-cased'),
]
embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)

tagger: SequenceTagger = SequenceTagger(embeddings=embeddings,
                                        hidden_size=1,
                                        tag_dictionary=tag_dictionary,
                                        tag_type='ner',
                                        # use CRF as classifier for NER task if false, it should use a simple classifier
                                        use_crf=False,
                                        # no LSTM layers between the embeddings and the CRF
                                        use_rnn=False,
                                        )
tagger.relearn_embeddings = False  # if False, Embeddings are not retrained (no fine tunning)

trainer: ModelTrainer = ModelTrainer(tagger, corpus)

date_str = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
trainer.num_workers = 8
trainer.train('resources/taggers/bert-lin-classifier-norelearn_'+date_str,
              learning_rate=0.17,
              mini_batch_size=64,
              max_epochs=40,
              )
Note that the learning_rate=0.17 has been chosen using the optimization process in ModelTrainer.
During training I get good enough results according to the training.log file:
&lt;denchmark-code&gt;2019-03-07 15:13:17,615 DEV  : loss 0.27856252 - f-score 0.6502 - acc 0.4818
2019-03-07 15:13:58,377 TEST : loss 0.28655991 - f-score 0.6596 - acc 0.4920
&lt;/denchmark-code&gt;

However, at the end of the training I get:
&lt;denchmark-code&gt;2019-03-07 15:31:48,522 Testing using best model ...
2019-03-07 15:31:48,523 loading file resources/taggers/bert-lin-classifier-norelearn_2019-03-07_10-20-51/best-model.pt
2019-03-07 15:32:39,379 MICRO_AVG: acc 0.0 - f1-score 0.0
2019-03-07 15:32:39,379 MACRO_AVG: acc 0.0 - f1-score 0.0
2019-03-07 15:32:39,379 LOC        tp: 0 - fp: 5 - fn: 11616 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000
2019-03-07 15:32:39,379 MISC       tp: 0 - fp: 0 - fn: 3839 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000
2019-03-07 15:32:39,379 ORG        tp: 0 - fp: 0 - fn: 2568 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000
2019-03-07 15:32:39,379 PER        tp: 0 - fp: 0 - fn: 7409 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000
&lt;/denchmark-code&gt;

Similarly if I try another homemade script for the task:
import copy

from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings, BertEmbeddings
from flair.data import Sentence, Token, Label
from flair.trainers import ModelTrainer
from flair.training_utils import Metric
from flair.models import SequenceTagger
from flair.data_fetcher import NLPTaskDataFetcher, NLPTask
from flair.data import TaggedCorpus

corpus : TaggedCorpus = NLPTaskDataFetcher.load_corpus(NLPTask.WIKINER_FRENCH)
tagger = SequenceTagger.load_from_file(
    'bert-lin-classifier-norelearn_2019-03-07_10-20-51/best-model.pt')
test_metric, test_loss = ModelTrainer.evaluate(tagger, corpus.test)
print(f'MICRO_AVG: acc {test_metric.micro_avg_accuracy()} - f1-score {test_metric.micro_avg_f_score()}')
print(f'MACRO_AVG: acc {test_metric.macro_avg_accuracy()} - f1-score {test_metric.macro_avg_f_score()}')
for class_name in test_metric.get_classes():
    print(f'{class_name:&lt;10} tp: {test_metric.get_tp(class_name)} - fp: {test_metric.get_fp(class_name)} - '
                     f'fn: {test_metric.get_fn(class_name)} - tn: {test_metric.get_tn(class_name)} - precision: '
                     f'{test_metric.precision(class_name):.4f} - recall: {test_metric.recall(class_name):.4f} - '
                     f'accuracy: {test_metric.accuracy(class_name):.4f} - f1-score: '
                     f'{test_metric.f_score(class_name):.4f}')
The result is:
&lt;denchmark-code&gt;MICRO_AVG: acc 0.0 - f1-score 0.0
MACRO_AVG: acc 0.0 - f1-score 0.0
LOC        tp: 0 - fp: 5 - fn: 11745 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000
MISC       tp: 0 - fp: 0 - fn: 3854 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000
ORG        tp: 0 - fp: 0 - fn: 2412 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000
PER        tp: 0 - fp: 4 - fn: 7509 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000
&lt;/denchmark-code&gt;

Hence I assume the best model is not saved.
	</description>
	<comments>
		<comment id='1' author='fstorme' date='2020-04-30T02:53:23Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>