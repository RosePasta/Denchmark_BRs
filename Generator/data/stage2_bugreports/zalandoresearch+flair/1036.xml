<bug id='1036' author='pommedeterresautee' open_date='2019-08-26T14:53:18Z' closed_time='2020-04-30T10:55:08Z'>
	<summary>[NER] use_amp crashes when used for NER training</summary>
	<description>
Describe the bug
Using master branch, simple NER training crashes when use_amp=True.
It happens at the very beginning of the second epoch (I tried many times).
The exception looks like:
&lt;denchmark-code&gt;2019-08-26 16:38:22,851 Reading data from /tmp
2019-08-26 16:38:22,851 Train: /tmp/tmpl7l954km
2019-08-26 16:38:22,851 Dev: /tmp/tmpfhh1l1z3
2019-08-26 16:38:22,851 Test: /tmp/tmpfhh1l1z3
2019-08-26 16:38:32,482 ----------------------------------------------------------------------------------------------------
2019-08-26 16:38:32,483 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): WordEmbeddings('fr')
    (list_embedding_1): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.5, inplace=False)
        (encoder): Embedding(275, 100)
        (rnn): LSTM(100, 1024)
        (decoder): Linear(in_features=1024, out_features=275, bias=True)
      )
    )
    (list_embedding_2): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.5, inplace=False)
        (encoder): Embedding(275, 100)
        (rnn): LSTM(100, 1024)
        (decoder): Linear(in_features=1024, out_features=275, bias=True)
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=2348, out_features=2348, bias=True)
  (rnn): LSTM(2348, 256, bidirectional=True)
  (linear): Linear(in_features=512, out_features=42, bias=True)
)"
2019-08-26 16:38:32,483 ----------------------------------------------------------------------------------------------------
2019-08-26 16:38:32,483 Corpus: "Corpus: 6968 train + 2148 dev + 2148 test sentences"
2019-08-26 16:38:32,483 ----------------------------------------------------------------------------------------------------
2019-08-26 16:38:32,483 Parameters:
2019-08-26 16:38:32,483  - learning_rate: "0.1"
2019-08-26 16:38:32,483  - mini_batch_size: "32"
2019-08-26 16:38:32,483  - patience: "3"
2019-08-26 16:38:32,483  - anneal_factor: "0.5"
2019-08-26 16:38:32,483  - max_epochs: "100"
2019-08-26 16:38:32,483  - shuffle: "True"
2019-08-26 16:38:32,483  - train_with_dev: "False"
2019-08-26 16:38:32,483 ----------------------------------------------------------------------------------------------------
2019-08-26 16:38:32,483 Model training base path: "resources/flair_ner/abc"
2019-08-26 16:38:32,483 ----------------------------------------------------------------------------------------------------
2019-08-26 16:38:32,483 Device: cuda:0
2019-08-26 16:38:32,483 ----------------------------------------------------------------------------------------------------
2019-08-26 16:38:32,483 Embeddings storage mode: cpu
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
2019-08-26 16:38:32,487 ----------------------------------------------------------------------------------------------------
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
2019-08-26 16:38:33,343 epoch 1 - iter 0/218 - loss 100.49298096 - samples/sec: 785.44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
2019-08-26 16:38:43,584 epoch 1 - iter 21/218 - loss 40.48044270 - samples/sec: 65.72
2019-08-26 16:38:53,553 epoch 1 - iter 42/218 - loss 27.45308078 - samples/sec: 67.51
2019-08-26 16:39:04,410 epoch 1 - iter 63/218 - loss 21.81346110 - samples/sec: 61.98
2019-08-26 16:39:16,687 epoch 1 - iter 84/218 - loss 18.24704664 - samples/sec: 54.81
2019-08-26 16:39:27,822 epoch 1 - iter 105/218 - loss 15.73898192 - samples/sec: 60.43
2019-08-26 16:39:40,383 epoch 1 - iter 126/218 - loss 14.07157230 - samples/sec: 53.56
2019-08-26 16:39:51,389 epoch 1 - iter 147/218 - loss 12.75365825 - samples/sec: 61.14
2019-08-26 16:40:03,459 epoch 1 - iter 168/218 - loss 11.72439751 - samples/sec: 55.75
2019-08-26 16:40:14,550 epoch 1 - iter 189/218 - loss 10.80741645 - samples/sec: 60.71
2019-08-26 16:40:26,553 epoch 1 - iter 210/218 - loss 10.04385874 - samples/sec: 56.05
2019-08-26 16:40:29,826 ----------------------------------------------------------------------------------------------------
2019-08-26 16:40:29,826 EPOCH 1 done: loss 9.8243 - lr 0.1000
2019-08-26 16:41:17,493 DEV : loss 3.2933993339538574 - score 0.6285
2019-08-26 16:41:17,704 BAD EPOCHS (no improvement): 0
2019-08-26 16:41:23,738 ----------------------------------------------------------------------------------------------------
Traceback (most recent call last):
  File "flair_train.py", line 72, in &lt;module&gt;
    nb_epochs=int(args.epoch))
  File "flair_train.py", line 64, in main
    checkpoint=False)
  File "/home/geantvert/.local/share/virtualenvs/anonymisation/venv/lib/python3.7/site-packages/flair/trainers/trainer.py", line 265, in train
    loss = self.model.forward_loss(batch)
  File "/home/geantvert/.local/share/virtualenvs/anonymisation/venv/lib/python3.7/site-packages/flair/models/sequence_tagger_model.py", line 330, in forward_loss
    features = self.forward(data_points)
  File "/home/geantvert/.local/share/virtualenvs/anonymisation/venv/lib/python3.7/site-packages/flair/models/sequence_tagger_model.py", line 410, in forward
    [token.get_embedding().unsqueeze(0) for token in sentence], 0
  File "/home/geantvert/.local/share/virtualenvs/anonymisation/venv/lib/python3.7/site-packages/flair/models/sequence_tagger_model.py", line 410, in &lt;listcomp&gt;
    [token.get_embedding().unsqueeze(0) for token in sentence], 0
  File "/home/geantvert/.local/share/virtualenvs/anonymisation/venv/lib/python3.7/site-packages/flair/data.py", line 240, in get_embedding
    return torch.cat(embeddings, dim=0)
  File "/home/geantvert/.local/share/virtualenvs/anonymisation/venv/lib/python3.7/site-packages/apex/amp/wrap.py", line 85, in wrapper
    return orig_fn(cast_seq, *args, **kwargs)
RuntimeError: Expected object of scalar type Float but got scalar type Half for sequence element 1 in sequence argument at position #1 'tensors'
make: *** [/home/geantvert/workspace/anonymisation/Makefile:98: flair_train_ca] Error 1

&lt;/denchmark-code&gt;

To Reproduce
The train code looks like this:
    # ...
    corpus: Corpus = ...  # this corpus works very well when use_amp=False
    tag_dictionary = corpus.make_tag_dictionary(tag_type='ner')

    embedding_types: List[TokenEmbeddings] = [
        WordEmbeddings('fr'),
        FlairEmbeddings('fr-forward'),
        FlairEmbeddings('fr-backward'),
    ]

    embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)

    tagger: SequenceTagger = SequenceTagger(hidden_size=256,
                                            embeddings=embeddings,
                                            use_crf=True,
                                            tag_dictionary=tag_dictionary,
                                            tag_type='ner')

    trainer: ModelTrainer = ModelTrainer(tagger, corpus)

    trainer.train(model_folder,
                  max_epochs=nb_epochs,
                  use_amp=True,
                  embeddings_storage_mode="cpu",
                  checkpoint=False)
Expected behavior
I expect the training to work with use_amp=True.
Environment (please complete the following information):
Flair master branch
Ubuntu 19.04
Other Info
Using amp_opt_level="O0" or "O2", "O3" make it crash at the beginning of the first epoch, with another error message (embeddings are set to None).
	</description>
	<comments>
		<comment id='1' author='pommedeterresautee' date='2019-09-10T13:22:45Z'>
		I would add the fact that this doesn't happen for me when embeddings_storage_mode='gpu'. It seems to be something with trying to reuse embeddings after the first epoch.
		</comment>
		<comment id='2' author='pommedeterresautee' date='2019-09-10T13:53:31Z'>
		A quick and dirty fix I used for myself is to force the conversion to torch.float16 when moving embeddings to different device.
In Token class:
&lt;denchmark-code&gt;def to(self, device: str):
    for name, vector in self._embeddings.items():
        self._embeddings[name] = vector.to(device, dtype=torch.float16, non_blocking=True)
&lt;/denchmark-code&gt;

The same change in Sentence.to() method.
		</comment>
		<comment id='3' author='pommedeterresautee' date='2019-09-10T14:00:14Z'>
		Ah interesting, thanks for digging deeper into this. Maybe we could devise a fix based on this.
		</comment>
		<comment id='4' author='pommedeterresautee' date='2019-09-10T15:23:41Z'>
		Just spoke to &lt;denchmark-link:https://github.com/kashif&gt;@kashif&lt;/denchmark-link&gt;
 about this: The problem is likely that when moving the tensors to and from CPU memory they get converted to  causing the error. We could extend the  method with an additional parameter for apex to addres this error.
However, if I set  for training NER, it seems that training speed does not improve. Are you seeing different results &lt;denchmark-link:https://github.com/maciek16180&gt;@maciek16180&lt;/denchmark-link&gt;
? I just reinstalled apex from the current head version.
		</comment>
		<comment id='5' author='pommedeterresautee' date='2019-09-10T15:27:30Z'>
		&lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 No, I didn't see a speed improvement with apex for .  seemed to run a bit faster though.
		</comment>
		<comment id='6' author='pommedeterresautee' date='2019-09-10T15:35:24Z'>
		Ok thanks, looks like amp is mainly good for training language models at this stage (we saw huge speed increases there).
		</comment>
		<comment id='7' author='pommedeterresautee' date='2019-09-11T04:40:21Z'>
		I have not made any measure yet, but if my understanding is right there is also a large decrease of memory foot print, so in some cases, it will enable GPU training on limited hardware / large dataset. Moreover, right now the GPU is not yet fully used (on my 2080TI I went from 10-20% during training few weeks ago to something around 40-50% now on my dataset), so may be when will push more and more optimizations amp will make more sense. LM training have a 100% usage rate.
		</comment>
		<comment id='8' author='pommedeterresautee' date='2019-10-05T15:08:48Z'>
		I saw speedups of 1.5x to 1.8x with the fixed suggested by alanakbik. Although the GPU is not fully used, it is still a sizeable increase, especially for NER. Also, one cause of this might also be because I use CRF layer at the end as well.

A quick and dirty fix I used for myself is to force the conversion to torch.float16 when moving embeddings to different device.
In Token class:
def to(self, device: str):
    for name, vector in self._embeddings.items():
        self._embeddings[name] = vector.to(device, dtype=torch.float16, non_blocking=True)

The same change in Sentence.to() method.

		</comment>
		<comment id='9' author='pommedeterresautee' date='2019-10-05T22:20:58Z'>
		&lt;denchmark-link:https://github.com/damitkwr&gt;@damitkwr&lt;/denchmark-link&gt;
 the improvement is on the training or the inference?
		</comment>
		<comment id='10' author='pommedeterresautee' date='2020-04-29T20:11:04Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>