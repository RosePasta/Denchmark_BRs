<bug id='1050' author='lambdaofgod' open_date='2019-08-30T17:51:13Z' closed_time='2019-08-31T11:42:47Z'>
	<summary>ELMo Embeddings return empty tensor</summary>
	<description>
Describe the bug
I get empty tensor when I try to run ELMo Embeddings.
To Reproduce
&lt;denchmark-code&gt;from flair import embeddings as flair_embeddings
from flair.data import Sentence

elmo_model = flair_embeddings.ELMoEmbeddings()

sentence = Sentence("I love to code")
elmo_model.embed(sentence)[0].embedding
&lt;/denchmark-code&gt;

Gives

tensor([])

Environment (please complete the following information):

Ubuntu 18.04





flair.version
'0.4.3'
allennlp.version
'0.8.5'




	</description>
	<comments>
		<comment id='1' author='lambdaofgod' date='2019-08-30T18:49:52Z'>
		I'll check it :)
		</comment>
		<comment id='2' author='lambdaofgod' date='2019-08-30T18:57:05Z'>
		I assume you want to extract the embedding for the first token.
This works,
elmo_model.embed(sentence)[0][0].embedding
		</comment>
		<comment id='3' author='lambdaofgod' date='2019-08-30T19:00:59Z'>
		Just call the embed function without any index access:
elmo_model.embed(sentence)
Then you can access the embedding of each token:
sentence[0].embedding
or for all tokens:
for token in sentence.tokens:
  print(token.embedding)
:)
		</comment>
		<comment id='4' author='lambdaofgod' date='2019-08-31T11:42:47Z'>
		Ok, it's not a bug, it's just confusing. I thought that if every token has an embedding, I should be able to call embedding of whole sentence, and it would just return concatenated embeddings.
		</comment>
	</comments>
</bug>