<bug id='1013' author='lucaventurini' open_date='2019-08-21T14:11:35Z' closed_time='2019-08-22T10:26:51Z'>
	<summary>Memory usage augmented since last release</summary>
	<description>

I was using v0.4.2 of flair and had some tests that I used while developing on a AWS instance with 4GB RAM. Before, running the tests on training did not consume all the RAM. I've upgraded to commit &lt;denchmark-link:https://github.com/flairNLP/flair/commit/24b72a6dfd1d0c2519794e1183cacc42c932c4f4&gt;24b72a6&lt;/denchmark-link&gt;
 (to try out the new MUSE embeddings) and now the same code consumes all the memory, freezing the instance. Everything works fine on a 8GB instance.

I can't share the data, but this is the part of the code that describes the network I'm using:
word_embeddings = [WordEmbeddings('turian')]

document_embeddings = DocumentRNNEmbeddings(word_embeddings, 
                                             hidden_size=512, 
                                             reproject_words=True, 
                                             reproject_words_dimension=256)
    
classifier = TextClassifier(document_embeddings, 
                            label_dictionary=corpus.make_label_dictionary(), 
                            multi_label=True)

trainer = ModelTrainer(classifier, corpus)
Expected behavior
The memory usage should be the same.
Environment (please complete the following information):

OS [e.g. iOS, Linux]: Ubuntu

	</description>
	<comments>
		<comment id='1' author='lucaventurini' date='2019-08-21T15:40:26Z'>
		Hi &lt;denchmark-link:https://github.com/lucaventurini&gt;@lucaventurini&lt;/denchmark-link&gt;
 - In master, we added the  as parameter in train. Are you using this? For instance, if you set  to , it should not consume much RAM (but be slower). Could you try this?
    trainer.train(
        'your/experiment/folder',
        learning_rate=0.1,
        mini_batch_size=32,
        embeddings_storage_mode='none',
    )
		</comment>
		<comment id='2' author='lucaventurini' date='2019-08-22T09:37:37Z'>
		Hi &lt;denchmark-link:https://github.com/alanakbik&gt;@alanakbik&lt;/denchmark-link&gt;
 , thank you. I was not setting explicitely the option, now that I set it to 'none' it runs again under 4GB RAM (even if it seems slower than it was at v0.4.2, but I haven't measure it). Was this the default behavior in the last release? And what does the default do now?
		</comment>
		<comment id='3' author='lucaventurini' date='2019-08-22T09:52:31Z'>
		The default is the 'cpu' option which stores all embeddings on regular memory. This way expensive embeddings need not be recomputed at each epoch. However, since many embeddings are really large it might be too much, so if you set it to 'none' it will delete embeddings directly after each mini-batch and then recompute at every epoch when needed. This is slower but more memory effective. This should be equivalent speed-wise to setting embeddings_in_memory=False in v0.4.2.
If you have a GPU with enough memory, you can also set the mode to 'gpu': in this case, all embeddings are stored on GPU memory which is faster than regular memory because no shuffling of the objects takes place, but of course that only works if there is enough GPU memory.
		</comment>
		<comment id='4' author='lucaventurini' date='2019-08-22T10:26:51Z'>
		Ok, thank you!
		</comment>
	</comments>
</bug>