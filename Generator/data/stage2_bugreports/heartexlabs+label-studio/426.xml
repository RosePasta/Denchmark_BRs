<bug id='426' author='andreaferretti' open_date='2020-09-17T10:35:12Z' closed_time='2020-09-24T13:50:11Z'>
	<summary>With prediction-score-min strategy, no labeling tasks are found</summary>
	<description>
Describe the bug
I tried to setup an active learning environment, where the interface always suggests the labeling task on which the model is most uncertain. To do so, I connected an ML backend and ran label-studio with "sampling": "prediction-score-min". Now I cannot label anything
To Reproduce
Steps to reproduce the behavior:

Create and connect a ML backend
Start Label-studio with "sampling": "prediction-score-min"
Import some tasks
Go to labeling page: no labeling tasks are found

Expected behavior
I would expected to be presented with labeling tasks, in increasing order of model prediction score.

&lt;denchmark-link:https://user-images.githubusercontent.com/1962818/93459571-3002ab00-f8e2-11ea-83a6-ee4aff0839ac.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/1962818/93459591-35f88c00-f8e2-11ea-8490-d8bf338bcf00.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='andreaferretti' date='2020-09-18T08:04:57Z'>
		&lt;denchmark-link:https://github.com/andreaferretti&gt;@andreaferretti&lt;/denchmark-link&gt;
 are you sure that tasks have predictions?
		</comment>
		<comment id='2' author='andreaferretti' date='2020-09-18T09:01:47Z'>
		They don't. Shouldn't predictions be created on the fly? When I choose sequential method, whenever I open a new labeling task, I see a prediction - I assume that the frontend queries the model on the fly.
What should I do to ensure that the ML backend is called on the existing tasks, so that labeling tasks show up here?
By the way, if there are no predictions, it would be better to show some labeling task at random, instead of nothing
		</comment>
		<comment id='3' author='andreaferretti' date='2020-09-24T08:56:32Z'>
		&lt;denchmark-link:https://github.com/andreaferretti&gt;@andreaferretti&lt;/denchmark-link&gt;
 when you are using  mode for sampling, it expects that every task in a dataset has a prediction, then selecting the task with minimal prediction score. Connected ML backend provides only prediction for the currently selected task. You can still create predictions for the entire dataset using your ML backend by:

Specifying "enable_predictions_button": true in project/config.json
Clicking on "Start Predictions" button on the Model page

Then, after all predictions are completed, the prediction-score-min sampling method should work as expected.
But your suggestion about sampling fallback is definitely reasonable, we'll include it in a future version
		</comment>
		<comment id='4' author='andreaferretti' date='2020-09-24T10:32:07Z'>
		Thank you, now it is more clear
		</comment>
		<comment id='5' author='andreaferretti' date='2020-09-24T10:44:43Z'>
		&lt;denchmark-link:https://github.com/andreaferretti&gt;@andreaferretti&lt;/denchmark-link&gt;
 can we close this issue? or you have more questions?
		</comment>
		<comment id='6' author='andreaferretti' date='2020-09-24T13:50:11Z'>
		It is ok to close, thanks
		</comment>
	</comments>
</bug>