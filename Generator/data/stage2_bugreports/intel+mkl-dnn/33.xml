<bug id='33' author='tensor-tang' open_date='2017-03-15T02:52:42Z' closed_time='2017-03-18T00:12:36Z'>
	<summary>Slow with 1x1 kernel conv</summary>
	<description>
Found a performance issue:
I just run only one test case below, which is the res5a_branch1_conv in ResNet50.
 PARAMS(FMT_DATA_BLOCKED, FMT_WEIGHTS_BLOCKED, FMT_NO_BIAS, FMT_DATA_BLOCKED, 64, 1, 1024, 14, 14, 2048, 7, 7, 1, 1, 0, 0, 2, 2)

BTW, need change Line 49 at Convolution_common.h
#define FMT_NO_BIAS format_undef

But it takes about 10 seconds computing backward data on E5-2699 v4, is that normal?
Time between:
stream(stream::kind::eager).submit(pipeline).wait();
I suppose it should be several ms.
	</description>
	<comments>
		<comment id='1' author='tensor-tang' date='2017-03-17T21:24:59Z'>
		Hi,
That is normal for specified convolution parameters (non-unit strides) and data format (nChw8c).
MKL-DNN optimized backward data propagation for convolutions with non-unit strides uses memory format "nchw". For above case mkl-dnn falls back to reference implementation that is very slow.
In real usage it is preferable to specify format "any" to enable convolution primitive to choose the data format that is most suitable for its input parameters (convolution kernel sizes, strides, padding, and so on). If the resulting format is different from the original format , data needs to be transformed to the format required for the convolution.
Thanks for remark regarding FMT_NO_BIAS format definition. It will be fixed.
		</comment>
		<comment id='2' author='tensor-tang' date='2017-03-18T00:12:23Z'>
		Updated FMT_NO_BIAS definition in &lt;denchmark-link:https://github.com/oneapi-src/oneDNN/commit/00f2ee4bd030309111253eb6cbd2cbaddf7aa504&gt;00f2ee4&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='3' author='tensor-tang' date='2017-03-18T08:53:20Z'>
		Thanks &lt;denchmark-link:https://github.com/ankalinin&gt;@ankalinin&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/vpirogov&gt;@vpirogov&lt;/denchmark-link&gt;
.
Yes indeed, the recommend format returned is nchw here.
However, this specific layer it is effective if use any format here, but for the whole topology like Resnet and Googlenet, many reorders between nchw and nChw8c are needed then.
I thought this kind of converting are so unnecessary for the whole picture.
So I was wondering that some better implements with nChw8c/nChw16c for 1x1 kernel ?
Then it could have better performance, saving much more reorder time.
Thanks.
		</comment>
	</comments>
</bug>