<bug id='26' author='tensor-tang' open_date='2017-02-21T03:49:11Z' closed_time='2017-03-17T22:04:06Z'>
	<summary>performance drop with GooglenetV1</summary>
	<description>
Before v0.5 I used padR=padding in pooling layer, it works well and got a very good perf with googlenet scoring.
However with this new release, there is a check point in Pooling.cpp:
&lt;denchmark-code&gt; for (int i = 2; i &lt;= 3; ++i)
        consistency = consistency &amp;&amp; (
                (src_desc-&gt;dims[i] - kernel[i - 2] + padding_l[i - 2]
                 + padding_r[i - 2]) / strides[i - 2] + 1
                == dst_desc-&gt;dims[i]);
    if (!consistency) return invalid_arguments;
&lt;/denchmark-code&gt;

So I have to use padR just like the test case:
&lt;denchmark-code&gt;for (int i = 0; i &lt; 2; ++i) {
        if ((pd.ih + pd.padh + padR[0] - pd.kh)/pd.strh + 1 &lt; pd.oh) ++padR[0];
        if ((pd.iw + pd.padw + padR[1] - pd.kw)/pd.strw + 1 &lt; pd.ow) ++padR[1];
 }
&lt;/denchmark-code&gt;

Then the fps drops to HALF before.
And there are about 4 pooling cases in GooglenetV1 this padR would be used, taking much more time than before.
I thought the output memory would be clear to zero, before pooling right? So even if I use padding(instead of padR), the loss result was still right with much better performance.
PS: padding_kind::zero
So any possible improvement ?
	</description>
	<comments>
		<comment id='1' author='tensor-tang' date='2017-02-22T23:41:49Z'>
		Thanks for catching this. We changed the way we handled padding in pooling to be consistent with TensorFlow and MXNet frameworks, but this resulted in code falling back to reference pooling implementation in case of uneven padding. We will address the issue and performance for this case will be on par with what you observed before.
		</comment>
		<comment id='2' author='tensor-tang' date='2017-02-23T02:09:52Z'>
		Thanks, looking forward.
		</comment>
		<comment id='3' author='tensor-tang' date='2017-03-17T22:04:06Z'>
		&lt;denchmark-link:https://github.com/oneapi-src/oneDNN/commit/378dcf97e28be7301e9ca2d3448a2a272de2820e&gt;378dcf9&lt;/denchmark-link&gt;
 fixes this issue.
		</comment>
	</comments>
</bug>