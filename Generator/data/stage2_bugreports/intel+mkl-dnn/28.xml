<bug id='28' author='uu-praveeng' open_date='2017-02-24T11:09:39Z' closed_time='2017-03-16T13:35:09Z'>
	<summary>make test stalled</summary>
	<description>
Hi,
I built the mkl-dnn but make test wont run.
Running tests...
Test project /home/gaurav/GPraveen/mkl-dnn/build
Start  1: simple-net-c
It is halting with the above error.
Intel processor that my PC has in Intel(R) Core(TM) i3-2120 CPU @ 3.30GHz with out avx2.
Is this the issue will it work only for intel Xeon processors that they have mentioned?
Thanks in advance
	</description>
	<comments>
		<comment id='1' author='uu-praveeng' date='2017-02-24T21:49:12Z'>
		Thanks for your report. This library is not yet optimized for processors without Intel(R) AVX2 support. It will run reference implementation that is rather slow and tests might take long time to complete. This might be what you are experiencing.
I also do not see specific error message in the log you provide. Is this all the output you get?
		</comment>
		<comment id='2' author='uu-praveeng' date='2017-02-27T04:32:56Z'>
		There are no error messages as such. But it is taking long enough time. Dint wait that much long to see the output to come up.
I thought there could be some thing that I was missing hence I posted.
How much time will it take to run the test if the processor in non Intel Xeon?
		</comment>
		<comment id='3' author='uu-praveeng' date='2017-02-27T06:38:40Z'>
		mkl-dnn is only optimized for AVX2 and (partially) AVX512. This means that it should work on any mobile/desktop/server CPU that supports those instruction sets, so it does not require a Xeon CPU.
For older CPUs that do not support at least AVX2, mkl-dnn falls back to reference implementations that are very slow. We also seem to have a bug where we do not correctly check for supported ISA and try executing AVX2 code on CPUs that do not support it.
		</comment>
		<comment id='4' author='uu-praveeng' date='2017-04-01T02:04:15Z'>
		&lt;denchmark-link:https://github.com/rsdubtso&gt;@rsdubtso&lt;/denchmark-link&gt;
  Does mkl-dnn will be optimized for CPU with AVX in the future? I tested the inference results with mkl-dnn on AVX CPU, but it's much too slower than im2col + MKL GEMM method.
		</comment>
		<comment id='5' author='uu-praveeng' date='2017-04-02T13:46:44Z'>
		There are no such plans at the moment. Of course we will consider pull requests that add such optimizations.
		</comment>
	</comments>
</bug>