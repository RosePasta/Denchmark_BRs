<bug id='285' author='ekuznetsov139' open_date='2018-07-23T07:40:44Z' closed_time='2018-09-04T20:23:04Z'>
	<summary>Poor performance with 1D FP32 input</summary>
	<description>
I am attempting to add mkl-dnn as a backend for a sound processing app. It takes a large 1D FP32 array (might be 100M elements or more) at the input and produces a significantly shorter but also 1D array at the output.
Right now I have a hand-coded/hand-threaded AVX2 implementation that manages one pass of inference in 400 ms or so on a dual E5-2699 v4 (two sockets of 22 cores each).
I got mkl-dnn hooked up following the code from the simple_net.cpp example, and I get correct results but the performance is abysmal. Instead of 400 ms, I see 15 seconds. According to the profiler, I have plain C code running in a single thread most of that time.
With MKLDNN_VERBOSE enabled, I get the following trace output (a single inference run):
&lt;denchmark-code&gt;mkldnn_verbose,exec,convolution,gemm:blas,forward_inference,fsrc:nchw fwei:oihw fbia:x fdst:nchw,alg:convolution_direct,mb1_g1ic1oc4_ih1oh1kh1sh1dh0ph0_iw177596372ow88798183kw8sw2dw0pw0,2782.12
mkldnn_verbose,exec,eltwise,jit:avx2,forward_inference,fdata:nchw fdiff:undef,alg:eltwise_elu,mb1ic4ih1iw88798183,67.1451
mkldnn_verbose,exec,pooling,nchw_pooling:any,forward_inference,fdata:nchw fws:undef,alg:pooling_max,mb1ic4_ih1oh1kh1sh1ph0_iw88798183ow22199545kw4sw4pw0,3279.45
mkldnn_verbose,exec,convolution,gemm:blas,forward_inference,fsrc:nchw fwei:oihw fbia:x fdst:nchw,alg:convolution_direct,mb1_g1ic4oc16_ih1oh1kh1sh1dh0ph0_iw22199545ow22199530kw16sw1dw0pw0,4368.15
mkldnn_verbose,exec,eltwise,jit:avx2,forward_inference,fdata:nchw fdiff:undef,alg:eltwise_elu,mb1ic16ih1iw22199530,54.7363
mkldnn_verbose,exec,pooling,nchw_pooling:any,forward_inference,fdata:nchw fws:undef,alg:pooling_max,mb1ic16_ih1oh1kh1sh1ph0_iw22199530ow5549882kw4sw4pw0,3559.88
mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nchw out:f32_nChw8c,num:1,1x16x1x5549882,127.804
mkldnn_verbose,exec,convolution,jit:avx2,forward_inference,fsrc:nChw8c fwei:OIhw8i8o fbia:x fdst:nChw8c,alg:convolution_direct,mb1_g1ic16oc16_ih1oh1kh1sh1dh0ph0_iw5549882ow5549867kw16sw1dw0pw0,973.692
mkldnn_verbose,exec,eltwise,jit:avx2,forward_inference,fdata:nChw8c fdiff:undef,alg:eltwise_elu,mb1ic16ih1iw5549867,13.5695
mkldnn_verbose,exec,pooling,jit:avx2,forward_inference,fdata:nChw8c fws:undef,alg:pooling_max,mb1ic16_ih1oh1kh1sh1ph0_iw5549867ow693733kw8sw8pw0,56.7494
mkldnn_verbose,exec,convolution,jit:avx2,forward_inference,fsrc:nChw8c fwei:OIhw8i8o fbia:x fdst:nChw8c,alg:convolution_direct,mb1_g1ic16oc16_ih1oh1kh1sh1dh0ph0_iw693733ow693606kw128sw1dw0pw0,928.545
mkldnn_verbose,exec,eltwise,jit:avx2,forward_inference,fdata:nChw8c fdiff:undef,alg:eltwise_elu,mb1ic16ih1iw693606,0.635416
mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nChw8c out:f32_nchw,num:1,1x16x1x693606,19.2519
mkldnn_verbose,exec,convolution,gemm:blas,forward_inference,fsrc:nchw fwei:oihw fbia:x fdst:nchw,alg:convolution_direct,mb1_g1ic16oc1_ih1oh1kh1sh1dh0ph0_iw693606ow693606kw1sw1dw0pw0,3.47846
&lt;/denchmark-code&gt;

As you can see, there are mentions of avx2 in there, but almost all time is spent in unoptimized code.
All convolution input/output formats are left at "any" except the top and the bottom level, which I set to nchw with h=1 (I don't see any better options). I tried to poke inside the library to understand why it's so bad, but I can't make heads or tails of it.
Is this just an unsupported configuration, or am I doing something wrong? Any suggestions would be welcome.
	</description>
	<comments>
		<comment id='1' author='ekuznetsov139' date='2018-07-23T15:52:05Z'>
		Hi &lt;denchmark-link:https://github.com/ekuznetsov139&gt;@ekuznetsov139&lt;/denchmark-link&gt;
,
Thanks for the report!
There are some limitation on the optimized implementations we have. In particular to use an optimized  implementation Intel MKL-DNN needs oc to be divisible by 8 and ic to be either divisible by 8 or to be equal to 1 or 3. This is a known limitation and we are going to address it soon.
&lt;denchmark-link:https://intel.github.io/mkl-dnn/understanding_memory_formats.html&gt;This page&lt;/denchmark-link&gt;
 describes how we are doing that.
But actually even for the jitted code the efficiency is very low. Most likely the issue is that we lack of parallelization for 1D cases (we don't have a parallelization over ow as of now).
		</comment>
		<comment id='2' author='ekuznetsov139' date='2018-07-23T16:31:15Z'>
		Hi &lt;denchmark-link:https://github.com/ekuznetsov139&gt;@ekuznetsov139&lt;/denchmark-link&gt;
,
As &lt;denchmark-link:https://github.com/emfomenk&gt;@emfomenk&lt;/denchmark-link&gt;
 already mentioned we have a lack of parralelization for oh=1 case.
To workaround this I would suggest to implement 1D convolution via 2D with ow=1 instead of oh=1 as you do now, e.g. try to use following shape mb1_g1ic16oc16_iw1ow1kw1sw1dw0pw0_ih693733oh693606kh128sh1dh0ph0 instead of mb1_g1ic16oc16_ih1oh1kh1sh1dh0ph0_iw693733ow693606kw128sw1dw0pw0.
		</comment>
		<comment id='3' author='ekuznetsov139' date='2018-07-23T20:59:19Z'>
		Okay, first of all, rebuilding mkl-dnn with Intel C++ 18.0 instead of MSVC (2015 native compiler) cut the time in half, to 7 seconds, mainly because the first two pooling levels went down from 3000 ms to 130 ms each.
Switching from h=1 to w=1 takes me down to 4 s. And switching the top layer to 8 output channels takes me down to 2270 ms.
&lt;denchmark-code&gt;mkldnn_verbose,exec,convolution,gemm:blas,forward_inference,fsrc:nchw fwei:oihw fbia:x fdst:nchw,alg:convolution_direct,mb1_g1ic1oc8_ih177596372oh88798183kh8sh2dh0ph0_iw1ow1kw1sw1dw0pw0,1264.46
mkldnn_verbose,exec,eltwise,jit:avx2,forward_inference,fdata:nchw fdiff:undef,alg:eltwise_elu,mb1ic8ih88798183iw1,138.531
mkldnn_verbose,exec,pooling,nchw_pooling:any,forward_inference,fdata:nchw fws:undef,alg:pooling_max,mb1ic8_ih88798183oh22199545kh4sh4ph0_iw1ow1kw1sw1pw0,287.989
mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nchw out:f32_nChw8c,num:1,1x8x22199545x1,48.4339
mkldnn_verbose,exec,convolution,jit:avx2,forward_inference,fsrc:nChw8c fwei:OIhw8i8o fbia:x fdst:nChw8c,alg:convolution_direct,mb1_g1ic8oc16_ih22199545oh22199530kh16sh1dh0ph0_iw1ow1kw1sw1dw0pw0,221.771
mkldnn_verbose,exec,eltwise,jit:avx2,forward_inference,fdata:nChw8c fdiff:undef,alg:eltwise_elu,mb1ic16ih22199530iw1,69.3555
mkldnn_verbose,exec,pooling,jit:avx2,forward_inference,fdata:nChw8c fws:undef,alg:pooling_max,mb1ic16_ih22199530oh5549882kh4sh4ph0_iw1ow1kw1sw1pw0,54.8959
mkldnn_verbose,exec,convolution,jit:avx2,forward_inference,fsrc:nChw8c fwei:OIhw8i8o fbia:x fdst:nChw8c,alg:convolution_direct,mb1_g1ic16oc16_ih5549882oh5549867kh16sh1dh0ph0_iw1ow1kw1sw1dw0pw0,102.455
mkldnn_verbose,exec,eltwise,jit:avx2,forward_inference,fdata:nChw8c fdiff:undef,alg:eltwise_elu,mb1ic16ih5549867iw1,16.5553
mkldnn_verbose,exec,pooling,jit:avx2,forward_inference,fdata:nChw8c fws:undef,alg:pooling_max,mb1ic16_ih5549867oh693733kh8sh8ph0_iw1ow1kw1sw1pw0,13.3512
mkldnn_verbose,exec,convolution,jit:avx2,forward_inference,fsrc:nChw8c fwei:OIhw8i8o fbia:x fdst:nChw8c,alg:convolution_direct,mb1_g1ic16oc16_ih693733oh693606kh128sh1dh0ph0_iw1ow1kw1sw1dw0pw0,76.827
mkldnn_verbose,exec,eltwise,jit:avx2,forward_inference,fdata:nChw8c fdiff:undef,alg:eltwise_elu,mb1ic16ih693606iw1,1.18266
mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nChw8c out:f32_nchw,num:1,1x16x693606x1,3.68887
mkldnn_verbose,exec,convolution,gemm:blas,forward_inference,fsrc:nchw fwei:oihw fbia:x fdst:nchw,alg:convolution_direct,mb1_g1ic16oc1_ih693606oh693606kh1sh1dh0ph0_iw1ow1kw1sw1dw0pw0,2.71661
&lt;/denchmark-code&gt;

The biggest outstanding issue is that the first convolution layer and the first pooling layer are still running in plain C.
In addition, the convolution layer fails to make use of multithreading and runs in a single thread most of the time. The pooling layer does use lots of threads, but most of them are busy doing this:
&lt;denchmark-link:https://user-images.githubusercontent.com/12205429/43102655-8d69711c-8e80-11e8-9614-6bb288a81cbc.png&gt;&lt;/denchmark-link&gt;

I can only guess that all those idiv's are courtesy of OpenMP (the pooling function is maximum).
		</comment>
		<comment id='4' author='ekuznetsov139' date='2018-08-08T22:54:18Z'>
		Hi &lt;denchmark-link:https://github.com/ekuznetsov139&gt;@ekuznetsov139&lt;/denchmark-link&gt;

I created &lt;denchmark-link:https://gist.github.com/kwiersch/300a56b3a1fe7f3990a5b09601d005ed&gt;a patch&lt;/denchmark-link&gt;
 that allows your 1st convolution to dispatch to jit:avx2 path. It should provide some speedup over current gemm:blas path. Please let me know if it works for you.
		</comment>
		<comment id='5' author='ekuznetsov139' date='2018-08-30T20:38:49Z'>
		Hi &lt;denchmark-link:https://github.com/ekuznetsov139&gt;@ekuznetsov139&lt;/denchmark-link&gt;
,
Have you had a chance to try kwiersch's patch? Now it's available in master. We also recently promoted changes that enable jitted optimized path for shapes where ic/oc is not divisible on simd width.
I can also suggest to fuse relu with convolution via post ops attributes. This will significantly improve your inference time.
		</comment>
		<comment id='6' author='ekuznetsov139' date='2018-09-04T20:23:04Z'>
		Closing due to lack of activity
		</comment>
	</comments>
</bug>