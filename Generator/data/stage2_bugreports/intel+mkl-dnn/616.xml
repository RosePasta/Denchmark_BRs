<bug id='616' author='nafest' open_date='2019-12-12T16:56:57Z' closed_time='2019-12-24T20:08:12Z'>
	<summary>Backward pass of the logsoftmax primitive seems to be incorrect</summary>
	<description>
&lt;denchmark-h:h1&gt;Summary&lt;/denchmark-h&gt;

Compared to the cuDNN implementation of the logsoftmax backward pass, I get different results with DNNL
&lt;denchmark-h:h1&gt;Version&lt;/denchmark-h&gt;

Lastest version of master branch
&lt;denchmark-link:https://github.com/oneapi-src/oneDNN/commit/95d7487b51305c83a326b7dea0a2910fb4c85606&gt;95d7487&lt;/denchmark-link&gt;

&lt;denchmark-h:h1&gt;Problem&lt;/denchmark-h&gt;

If I look at the implementation of the backward pass in ref_softmax.cpp the logsoftmax bwd pass in DNNL is computed the following way
diff_src[i] = diff_dst[i] - (diff_dst \sdot dest)
If I understood everything correctly, diff_dst is the output of the logsoftmax forward pass and diff_dst the gradient to be propagated.
From my point of view, the correct way to compute the bwd pass of logsoftmax should be
diff_src[i] = diff_dest[i] - exp(dest[i]) * sum(diff_dst)
At least with this formula, I can reproduce the results of the respective cuDNN functionality.
Please also have a look at the logsoftmax implementation of PyTorch: &lt;denchmark-link:https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/SoftMax.cpp#L67&gt;https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/SoftMax.cpp#L67&lt;/denchmark-link&gt;

	</description>
	<comments>
	</comments>
</bug>