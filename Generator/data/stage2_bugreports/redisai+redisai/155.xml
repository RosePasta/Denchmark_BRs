<bug id='155' author='hhsecond' open_date='2019-07-02T16:46:34Z' closed_time='2019-09-17T15:12:03Z'>
	<summary>RedisAI doesn't throw error/warning if user tries to set GPU when GPU is not available</summary>
	<description>
Other than PyTorch, none of the backends throws an error or a warning if the user tries to set the model with GPU when GPU is not available. There are #TODOs in the source to fix this already. Keeping the issue open to track it for the GA perhaps, because it could cause serious skepticism in certain situations.
	</description>
	<comments>
		<comment id='1' author='hhsecond' date='2019-09-17T12:44:00Z'>
		ONNXRuntime still needs to do this check dynamically but since the dynamic allocation of CUDA is decided to do in the future, this chek can also be implemented then
		</comment>
		<comment id='2' author='hhsecond' date='2019-09-17T12:47:41Z'>
		I think since we can load backends dynamically and we can build ONNXRuntime with multiple backends executors (CPU, GPU, TensorRT, OpenVINO, etc), we can eventually leave the dynamicity for when the user chooses what backend to load, rather than within ONNXRuntime.
		</comment>
		<comment id='3' author='hhsecond' date='2019-09-17T12:48:40Z'>
		We can have several ONNXRuntime backends, one per executor. This way they'll also be smaller and with fewer dependencies.
		</comment>
		<comment id='4' author='hhsecond' date='2019-09-17T12:51:08Z'>
		Make sense. In that case, I should leave this issue opened since the CUDA check won't happen for ORT if the default installation is CPU
		</comment>
		<comment id='5' author='hhsecond' date='2019-09-17T12:52:49Z'>
		Are you saying that if ORT is built for CPU and you request GPU we won't return an error, right?
		</comment>
		<comment id='6' author='hhsecond' date='2019-09-17T13:05:11Z'>
		No, if ORT is built with CPU, then we will return an error if user request for a GPU execution.
But if RedisAI is built without DEVICE var (or DEVICE=cpu) it throws an error if user request GPU even if the loaded ORT backend is running on GPU. We have problem for the other case as well -  RedisAI is built for GPU and loaded ORT backend is CPU only



RedisAI/src/backends/onnxruntime.c


        Lines 269 to 279
      in
      19d15c4






 #if RAI_ONNXRUNTIME_USE_CUDA 



 if (device == RAI_DEVICE_GPU) { 



 OrtSessionOptionsAppendExecutionProvider_CUDA(session_options, deviceid); 



   } 



 #else 



 // TODO: Do dynamic device/provider check with GetExecutionProviderType or something else 



 if (device == RAI_DEVICE_GPU) { 



 RAI_SetError(error, RAI_EMODELCREATE, "GPU requested but ONNX couldn't find CUDA"); 



 return NULL; 



   } 



 #endif 





		</comment>
		<comment id='7' author='hhsecond' date='2019-09-17T13:12:47Z'>
		Say we have two ORT backends, like redisai_onnxruntime_cpu.so and redisai_onnxruntime_gpu.so

the moment we load redisai_onnxruntime_gpu.so it will throw an error if there's no GPU or CUDA on the machine
if we load redisai_onnxruntime_cpu.so and request GPU, it returns an error (which is correct)

In other words, it doesn't really matter how you build redisai.so, only the backends counts. Does this make sense or I'm being overly dense? :-)
		</comment>
		<comment id='8' author='hhsecond' date='2019-09-17T14:19:59Z'>
		Thanks for the clarification. Closing the issue
		</comment>
	</comments>
</bug>