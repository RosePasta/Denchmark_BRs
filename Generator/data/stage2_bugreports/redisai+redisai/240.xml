<bug id='240' author='filipecosta90' open_date='2019-11-05T04:27:00Z' closed_time='2019-11-15T13:40:11Z'>
	<summary>verbose logging of "Run took %fms" is causing performance impact on the default configuration and a bad user experience on docker</summary>
	<description>
If running RedisAI for the first time, or with a verbose log level ( default on RedisAI image (( non containerized redis default is notice )))
&lt;denchmark-code&gt;docker run -p 6379:6379 redisai/redisai:latest
&lt;/denchmark-code&gt;

we receive an enormous amount of logs, as seen on the example bellow. Considering that the information of backend inference duration will be very important in the future, we should not remove the calculation but adjust the log level in accordance or ( even better ) keep the information but dont log it ( and provide it in the future on the info command or similar )
&lt;denchmark-code&gt;1:C 05 Nov 2019 04:16:28.947 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
1:C 05 Nov 2019 04:16:28.947 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started
1:C 05 Nov 2019 04:16:28.947 # Configuration loaded
1:M 05 Nov 2019 04:16:28.948 * Running mode=standalone, port=6379.
1:M 05 Nov 2019 04:16:28.948 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.
1:M 05 Nov 2019 04:16:28.948 # Server initialized
1:M 05 Nov 2019 04:16:28.948 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.
1:M 05 Nov 2019 04:16:29.077 * Module 'ai' loaded from /usr/lib/redis/modules/redisai.so
1:M 05 Nov 2019 04:16:29.077 * Ready to accept connections
2019-11-05 04:16:53.596371: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
1:M 05 Nov 2019 04:17:32.446 * &lt;ai&gt; RAI_ModelRun took 0.050000s
1:M 05 Nov 2019 04:17:32.447 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.448 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.450 * &lt;ai&gt; RAI_ModelRun took 0.001000s
1:M 05 Nov 2019 04:17:32.451 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.452 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.454 * &lt;ai&gt; RAI_ModelRun took 0.001000s
1:M 05 Nov 2019 04:17:32.455 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.456 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.458 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.459 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.460 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.461 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.463 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.464 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.465 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.467 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.468 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.470 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.471 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.472 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.474 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.475 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.477 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.478 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.480 * &lt;ai&gt; RAI_ModelRun took 0.001000s
1:M 05 Nov 2019 04:17:32.481 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.482 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.484 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.486 * &lt;ai&gt; RAI_ModelRun took 0.001000s
1:M 05 Nov 2019 04:17:32.487 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.489 * &lt;ai&gt; RAI_ModelRun took 0.001000s
1:M 05 Nov 2019 04:17:32.490 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.491 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.493 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.494 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.496 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.497 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.499 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.501 * &lt;ai&gt; RAI_ModelRun took 0.001000s
1:M 05 Nov 2019 04:17:32.502 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.504 * &lt;ai&gt; RAI_ModelRun took 0.001000s
1:M 05 Nov 2019 04:17:32.505 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.507 * &lt;ai&gt; RAI_ModelRun took 0.001000s
1:M 05 Nov 2019 04:17:32.508 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.509 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.511 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.512 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.514 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.515 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.517 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.518 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.520 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.521 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.523 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.524 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.526 * &lt;ai&gt; RAI_ModelRun took 0.001000s
1:M 05 Nov 2019 04:17:32.527 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.528 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.530 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.531 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.533 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.534 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.536 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.537 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.539 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.540 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.542 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.544 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.545 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.546 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.548 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.549 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.551 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.552 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.554 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.556 * &lt;ai&gt; RAI_ModelRun took 0.001000s
1:M 05 Nov 2019 04:17:32.557 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.559 * &lt;ai&gt; RAI_ModelRun took 0.001000s
1:M 05 Nov 2019 04:17:32.560 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.562 * &lt;ai&gt; RAI_ModelRun took 0.001000s
1:M 05 Nov 2019 04:17:32.563 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.565 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.566 * &lt;ai&gt; RAI_ModelRun took 0.000000s
1:M 05 Nov 2019 04:17:32.568 * &lt;ai&gt; RAI_ModelRun took 0.001000s
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='filipecosta90' date='2019-11-05T14:10:05Z'>
		&lt;denchmark-link:https://github.com/filipecosta90&gt;@filipecosta90&lt;/denchmark-link&gt;
 Thank you. I was actually looking into the AI info API for modules, it looks like it will fit our bill. This would allow us to just use the  command to log execution times, avoiding , which has some advantages for integrations that already use the  command.
		</comment>
		<comment id='2' author='filipecosta90' date='2019-11-14T17:07:24Z'>
		looking at the model info PR on redis that is already merged ( &lt;denchmark-link:https://github.com/redis/redis/pull/6270&gt;redis/redis#6270&lt;/denchmark-link&gt;
 ) we will still need to keep that info within some context of RedisAI which will then be used to reported back in a new method with the following signature:

with that in mind I would suggest adding ( at least ) two fields two the  struct in the same manner that redis does to have stats:
&lt;denchmark-code&gt;long long backend_calls;
long long backend_microseconds;
&lt;/denchmark-code&gt;

afterwards, when someone called INFO we would report for each model the average backend_duration. basically RAI_Model would look like:
&lt;denchmark-code&gt;typedef struct RAI_Model {
  void* model;
  // TODO: use session pool? The ideal would be to use one session per client.
  //       If a client disconnects, we dispose the session or reuse it for
  //       another client.
  void *session;
  RAI_Backend backend;
  char* devicestr;
  char **inputs;
  size_t ninputs;
  char **outputs;
  size_t noutputs;
  long long refCount;
  long long backend_calls;
  long long backend_microseconds;
  void* data;
} RAI_Model;
&lt;/denchmark-code&gt;

what do you think &lt;denchmark-link:https://github.com/lantiga&gt;@lantiga&lt;/denchmark-link&gt;
 ? This would be the minimal to expected metric to have. The holly gral would be to have something that would report not singleton metrics ( like a latency histogram ), but I would go for the simple solution now and remove the LogLine in exchange to keeping the info on the Model and reporting the average when requested.
PS: I dont mind taking this one forward.
		</comment>
		<comment id='3' author='filipecosta90' date='2019-11-15T13:40:11Z'>
		solved by &lt;denchmark-link:https://github.com/RedisAI/RedisAI/pull/248&gt;#248&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>