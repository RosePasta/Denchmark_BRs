<bug id='757' author='rajeevbaalwan' open_date='2020-07-21T06:57:05Z' closed_time='2020-07-23T05:29:48Z'>
	<summary>Issue in Running SOTA Transformer_ctc training</summary>
	<description>
&lt;denchmark-h:h3&gt;Issue Description&lt;/denchmark-h&gt;

I have been trying to run &lt;denchmark-link:https://github.com/facebookresearch/wav2letter/blob/master/recipes/models/sota/2019/librispeech/train_am_transformer_ctc.cfg&gt;https://github.com/facebookresearch/wav2letter/blob/master/recipes/models/sota/2019/librispeech/train_am_transformer_ctc.cfg&lt;/denchmark-link&gt;

having following content:
--runname=librispeech_clean_transformer
--rundir=/hdd1/nfshare_hulk/wav2vec/libri_exp
--archdir=/hdd1/nfshare_hulk/wav2vec/libri_exp/librispeech_clean_transformer
--arch=am_transformer_ctc.arch
--tokensdir=/hdd1/nfshare_hulk/wav2vec/libri_exp/am
--tokens=librispeech-train-all-unigram-10000.tokens
--lexicon=/hdd1/nfshare_hulk/wav2vec/libri_exp/am/librispeech-train+dev-unigram-10000-nbest10.lexicon
--train=/hdd1/nfshare_hulk/wav2vec/libri_exp/lists/train.trans.clean100.flac.tsv
--valid=/hdd1/nfshare_hulk/wav2vec/libr_exp/lists/test.trans.flac.tsv
--criterion=ctc
--mfsc
--usewordpiece=true
--wordseparator=_
--labelsmooth=0.05
--dataorder=output_spiral
--inputbinsize=25
--softwstd=4
--memstepsize=5000000
--pcttraineval=1
--pctteacherforcing=99
--sampletarget=0.01
--netoptim=adadelta
--critoptim=adadelta
--lr=0.4
--lrcrit=0.4
--linseg=0
--momentum=0.0
--maxgradnorm=1.0
--onorm=target
--sqnorm
--nthread=6
--batchsize=8
--filterbanks=80
--minisz=200
--mintsz=2
--minloglevel=0
--logtostderr
--enable_distributed
and as per the documentation i have used below files provided in repo for token and lexicon:
token _file= &lt;denchmark-link:https://dl.fbaipublicfiles.com/wav2letter/sota/2019/librispeech/models/am/librispeech-train-all-unigram-10000.tokens&gt;https://dl.fbaipublicfiles.com/wav2letter/sota/2019/librispeech/models/am/librispeech-train-all-unigram-10000.tokens&lt;/denchmark-link&gt;

lexicon_file = &lt;denchmark-link:https://dl.fbaipublicfiles.com/wav2letter/sota/2019/librispeech/models/am/librispeech-train%2Bdev-unigram-10000-nbest10.lexicon&gt;https://dl.fbaipublicfiles.com/wav2letter/sota/2019/librispeech/models/am/librispeech-train%2Bdev-unigram-10000-nbest10.lexicon&lt;/denchmark-link&gt;

But once i start training i am getting this issue :
terminate called after throwing an instance of 'std::runtime_error'
what():  [loadWords] Invalid line: _the
Aborted at 1595313656 (unix time) try "date -d @1595313656" if you are using GNU date
PC: @     0x7fc3d34d6e97 gsignal
SIGABRT (@0x5a7) received by PID 1447 (TID 0x7fc418da6380) from PID 1447; stack trace:
@     0x7fc4110bc890 (unknown)
@     0x7fc3d34d6e97 gsignal
@     0x7fc3d34d8801 abort
@     0x7fc3d3ecb957 (unknown)
@     0x7fc3d3ed1ab6 (unknown)
@     0x7fc3d3ed1af1 std::terminate()
@     0x7fc3d3ed1d24 __cxa_throw
@     0x55c57c1865bf w2l::loadWords()
@     0x55c57c00adec main
@     0x7fc3d34b9b97 __libc_start_main
@     0x55c57c06f7aa _start
mpirun noticed that process rank 0 with PID 0 on node b0a1891c9327 exited on signal 6 (Aborted)
I have also checked the code and found these line which are responsible to load the lexicon files:
LexiconMap loadWords(const std::string&amp; filename, int maxWords) {
LexiconMap lexicon;
std::string line;
std::ifstream infile(filename);
if (!infile) {
throw std::invalid_argument("Cannot open " + filename);
}
//Add at most maxWords words into the lexicon.
//If maxWords is negative then no limit is applied.
while (maxWords != lexicon.size() &amp;&amp; std::getline(infile, line)) {
// Parse the line into two strings: word and spelling.
auto fields = splitOnWhitespace(line, true);
if (fields.size() &lt; 2) {
throw std::runtime_error("[loadWords] Invalid line: " + line);
}
const std::string&amp; word = fields[0];
std::vectorstd::string spelling(fields.size() - 1);
std::copy(fields.begin() + 1, fields.end(), spelling.begin());
&lt;denchmark-code&gt;//Add the word into the dictionary.
if (lexicon.find(word) == lexicon.end()) {
  lexicon[word] = {};
}
//Add the current spelling of the words to the list of spellings.
lexicon[word].push_back(spelling);
&lt;/denchmark-code&gt;

}
//Insert unknown word.
lexicon[kUnkToken] = {};
return lexicon;
}
As it can be seen in code which is trying split on white-space which means the lexicon file is still expected space separated format like:
the t h e |
coming	c o m i n g |
as	a s |
it	i t |
did	d i d |
but lexicon file provided in repo to download contains words as show below:
_the
_and
_of
_to
_a
s
_in
_i
_he
_that
What am i missing here?
	</description>
	<comments>
		<comment id='1' author='rajeevbaalwan' date='2020-07-22T05:28:01Z'>
		&lt;denchmark-link:https://github.com/rajeevbaalwan&gt;@rajeevbaalwan&lt;/denchmark-link&gt;
 thanks for reporting this!
Yep, lexicon should be like "hello _hel lo" in our case. The error in the uploaded file (tokens are uploaded as lexicon). Will fix the link tmr and let you know.
		</comment>
		<comment id='2' author='rajeevbaalwan' date='2020-07-22T11:43:57Z'>
		
@rajeevbaalwan thanks for reporting this!
Yep, lexicon should be like "hello _hel lo" in our case. The error in the uploaded file (tokens are uploaded as lexicon). Will fix the link tmr and let you know.

hi i'm having the same issue:
&lt;denchmark-code&gt;root@fc6464776c28:~/wav2letter# ./build/Train fork pre_model/am_resnet_ctc_librispeech_dev_other.bin --flagsfile train.cfg
I0722 11:39:41.949297 30001 Train.cpp:114] Parsing command line flags
I0722 11:39:41.949313 30001 Train.cpp:115] Overriding flags should be mutable when using `fork`
I0722 11:39:41.949333 30001 Train.cpp:120] Reading flags from filetrain.cfg
Initialized Gloo successfully!
I0722 11:39:42.096266 30001 Train.cpp:151] Gflags after parsing
--flagfile=; --fromenv=; --tryfromenv=; --undefok=; --tab_completion_columns=80; --tab_completion_word=; --help=false; --helpfull=false; --helpmatch=; --helpon=; --helppackage=false; --helpshort=false; --helpxml=false; --version=false; --adambeta1=0.90000000000000002; --adambeta2=0.999; --am=; --am_decoder_tr_dropout=0; --am_decoder_tr_layerdrop=0; --am_decoder_tr_layers=1; --arch=network.arch; --archdir=/root/wav2letter/tutorials/1-librispeech_clean/; --attention=content; --attentionthreshold=0; --attnWindow=no; --attnconvchannel=0; --attnconvkernel=0; --attndim=0; --batchsize=4; --beamsize=2500; --beamsizetoken=250000; --beamthreshold=25; --blobdata=false; --channels=1; --criterion=ctc; --critoptim=sgd; --datadir=/root/wav2letter/; --dataorder=input; --decoderattnround=1; --decoderdropout=0; --decoderrnnlayer=1; --decodertype=wrd; --devwin=0; --emission_dir=; --emission_queue_size=3000; --enable_distributed=true; --encoderdim=0; --eosscore=0; --eostoken=false; --everstoredb=false; --fftcachesize=1; --filterbanks=40; --flagsfile=train.cfg; --framesizems=25; --framestridems=10; --gamma=1; --gumbeltemperature=1; --input=wav; --inputbinsize=100; --inputfeeding=false; --isbeamdump=false; --iter=1000000; --itersave=false; --labelsmooth=0.050000000000000003; --leftWindowSize=50; --lexicon=/root/wav2letter/pre_model/librispeech-train+dev-unigram-10000-nbest10.lexicon; --linlr=-1; --linlrcrit=-1; --linseg=0; --lm=; --lm_memory=5000; --lm_vocab=; --lmtype=kenlm; --lmweight=0; --localnrmlleftctx=0; --localnrmlrightctx=0; --logadd=false; --lr=0.10000000000000001; --lr_decay=9223372036854775807; --lr_decay_step=9223372036854775807; --lrcosine=true; --lrcrit=0; --max_devices_per_node=8; --maxdecoderoutputlen=200; --maxgradnorm=1; --maxisz=9223372036854775807; --maxload=-1; --maxrate=10; --maxsil=50; --maxtsz=9223372036854775807; --maxword=-1; --melfloor=1; --memstepsize=10485760; --mfcc=false; --mfcccoeffs=13; --mfsc=true; --minisz=200; --minrate=3; --minsil=0; --mintsz=2; --momentum=0.59999999999999998; --netoptim=sgd; --noresample=false; --nthread=4; --nthread_decoder=1; --nthread_decoder_am_forward=1; --numattnhead=8; --onorm=target; --optimepsilon=1e-08; --optimrho=0.90000000000000002; --outputbinsize=5; --pctteacherforcing=100; --pcttraineval=100; --pow=false; --pretrainWindow=0; --replabel=1; --reportiters=2000; --rightWindowSize=50; --rndv_filepath=; --rundir=/root/wav2letter/training/; --runname=talk51_trainlogs; --samplerate=16000; --sampletarget=0.01; --samplingstrategy=rand; --saug_fmaskf=27; --saug_fmaskn=2; --saug_start_update=-1; --saug_tmaskn=2; --saug_tmaskp=1; --saug_tmaskt=100; --sclite=; --seed=0; --show=false; --showletters=false; --silscore=0; --smearing=none; --smoothingtemperature=1; --softwoffset=10; --softwrate=5; --softwstd=5; --sqnorm=true; --stepsize=1000000; --surround=|; --tag=; --target=ltr; --test=; --tokens=librispeech-train-all-unigram-10000.tokens; --tokensdir=/root/wav2letter/pre_model; --train=audioProcess/lists/train_talk51.lst; --trainWithWindow=false; --transdiag=0; --unkscore=-inf; --use_memcache=false; --uselexicon=true; --usewordpiece=true; --valid=audioProcess/lists/valid_talk51.lst; --warmup=1; --weightdecay=0; --wordscore=0; --wordseparator=_; --world_rank=0; --world_size=1; --alsologtoemail=; --alsologtostderr=false; --colorlogtostderr=false; --drop_log_memory=true; --log_backtrace_at=; --log_dir=; --log_link=; --log_prefix=true; --logbuflevel=0; --logbufsecs=30; --logemaillevel=999; --logfile_mode=436; --logmailer=/bin/mail; --logtostderr=true; --max_log_size=1800; --minloglevel=0; --stderrthreshold=2; --stop_logging_if_full_disk=false; --symbolize_stacktrace=true; --v=0; --vmodule=;
I0722 11:39:42.096786 30001 Train.cpp:152] Experiment path: /root/wav2letter/training/talk51_trainlogs
I0722 11:39:42.096791 30001 Train.cpp:153] Experiment runidx: 1
I0722 11:39:42.102267 30001 Train.cpp:199] Number of classes (network): 9999
terminate called after throwing an instance of 'std::runtime_error'
  what():  [loadWords] Invalid line: _the
*** Aborted at 1595417982 (unix time) try "date -d @1595417982" if you are using GNU date ***
PC: @     0x7f6e2804ae97 gsignal
*** SIGABRT (@0x7531) received by PID 30001 (TID 0x7f6e33880800) from PID 30001; stack trace: ***
    @     0x7f6e28f7f890 (unknown)
    @     0x7f6e2804ae97 gsignal
    @     0x7f6e2804c801 abort
    @     0x7f6e28a3f957 (unknown)
    @     0x7f6e28a45ae6 (unknown)
    @     0x7f6e28a45b21 std::terminate()
    @     0x7f6e28a45d54 __cxa_throw
    @     0x7f6e33a654df w2l::loadWords()
    @     0x7f6e338e4e6c main
    @     0x7f6e2802db97 __libc_start_main
    @     0x7f6e3394b10a _start
Aborted (core dumped)
&lt;/denchmark-code&gt;

and i'm not sure where the problem is
		</comment>
		<comment id='3' author='rajeevbaalwan' date='2020-07-22T11:47:02Z'>
		Hi &lt;denchmark-link:https://github.com/Rootian&gt;@Rootian&lt;/denchmark-link&gt;

The issue is with lexicon file. Did you created lexicon file yourself or used the one provided in repo?
		</comment>
		<comment id='4' author='rajeevbaalwan' date='2020-07-22T11:49:15Z'>
		
Hi @Rootian
The issue is with lexicon file. Did you created lexicon file yourself or used the one provided in repo?

the lexicon file i use is librispeech-train+dev-unigram-10000-nbest10.lexicon, same as you
		</comment>
		<comment id='5' author='rajeevbaalwan' date='2020-07-22T11:54:45Z'>
		

Hi @Rootian
The issue is with lexicon file. Did you created lexicon file yourself or used the one provided in repo?

the lexicon file i use is librispeech-train+dev-unigram-10000-nbest10.lexicon, same as you

Okay, the issue is with lexicon file , by the time lexicon file is updated you can create your own instead of using downloaded file.
		</comment>
		<comment id='6' author='rajeevbaalwan' date='2020-07-23T03:55:02Z'>
		Have some problem with caching on AWS. Let you know as correct file will be uploaded there. For now you can simply run data preparation step where lexicon is prepared.
		</comment>
		<comment id='7' author='rajeevbaalwan' date='2020-07-23T04:50:41Z'>
		&lt;denchmark-link:https://github.com/rajeevbaalwan&gt;@rajeevbaalwan&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Rootian&gt;@Rootian&lt;/denchmark-link&gt;

The file is fixed, feel free to download it again and rerun training. Let me know if the issue is solved for you.
		</comment>
		<comment id='8' author='rajeevbaalwan' date='2020-07-23T05:29:48Z'>
		&lt;denchmark-link:https://github.com/tlikhomanenko&gt;@tlikhomanenko&lt;/denchmark-link&gt;
  Thanks, meantime i have prepared lexicon file following data preparation step.
I'm closing this issue as of now, will reopen this if any issue occurs.
		</comment>
	</comments>
</bug>