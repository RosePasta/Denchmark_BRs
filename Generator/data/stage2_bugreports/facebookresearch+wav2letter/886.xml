<bug id='886' author='abhinavkulkarni' open_date='2020-11-12T06:10:41Z' closed_time='2020-11-12T09:32:24Z'>
	<summary>Unable to run streaming convnets recipe</summary>
	<description>
&lt;denchmark-h:h3&gt;Bug Description&lt;/denchmark-h&gt;

Hi,
I am unable to run &lt;denchmark-link:https://github.com/facebookresearch/wav2letter/tree/v0.2/recipes/models/streaming_convnets/librispeech&gt;streaming convnets recipe&lt;/denchmark-link&gt;
 from branch . Please note that I am able to run the &lt;denchmark-link:https://github.com/facebookresearch/wav2letter/blob/v0.2/tutorials/1-librispeech_clean&gt;tutorial recipe&lt;/denchmark-link&gt;
 using the same data.
Here's my &lt;denchmark-link:https://github.com/facebookresearch/wav2letter/blob/v0.2/recipes/models/streaming_convnets/librispeech/train_am_500ms_future_context.cfg&gt;train_am_500ms_future_context.cfg&lt;/denchmark-link&gt;
 for streaming convnets recipe:
# Training config for Librispeech using Time-Depth Separable Convolutions
# Replace `[...]`, `[MODEL_DST]`, `[DATA_DST]`, `[DATA_DST_librilight] with appropriate paths
--runname=inference_2019
--rundir=/home/w2luser/w2l/rundir/am_500ms_future_context
--tokensdir=/home/w2luser/w2l/am
--archdir=/home/w2luser/Projects/wav2letter/recipes/models/streaming_convnets/librispeech
--train=/home/w2luser/w2l/lists/train-clean-100.lst,/home/w2luser/w2l/lists/train-clean-360.lst,/home/w2luser/w2l/lists/train-other-500.lst
--valid=dev-clean:/home/w2luser/w2l/lists/dev-clean.lst,dev-other:/home/w2luser/w2l/lists/dev-other.lst
--lexicon=/home/w2luser/w2l/am/librispeech-train+dev-unigram-10000-nbest10.lexicon
--arch=am_500ms_future_context.arch
--tokens=librispeech-train-all-unigram-10000.tokens
--criterion=ctc
--batchsize=8
--lr=0.4
--momentum=0.0
--maxgradnorm=0.5
--reportiters=1000
--nthread=6
--mfsc=true
--usewordpiece=true
--wordseparator=_
--filterbanks=80
--minisz=200
--mintsz=2
--maxisz=33000
--enable_distributed=true
--pcttraineval=1
--minloglevel=0
--logtostderr
--onorm=target
--sqnorm
--localnrmlleftctx=300
I get the following output (I have trimmed the output for the sake of brevity - ... lines indicate trimmed output):
/home/w2luser/Projects/wav2letter/cmake-build-debug-libtorch-remote/Train train --flagfile /home/w2luser/Projects/wav2letter/recipes/models/streaming_convnets/librispeech/train_am_500ms_future_context.cfg
I20201111 21:35:12.995518 64900 Train.cpp:56] Parsing command line flags
No protocol specified
I20201111 21:35:13.722399 64900 Train.cpp:151] Gflags after parsing 
--flagfile=/home/w2luser/Projects/wav2letter/recipes/models/streaming_convnets/librispeech/train_am_500ms_future_context.cfg; --fromenv=; --tryfromenv=; --undefok=; --tab_completion_columns=80; --tab_completion_word=; --help=false; --helpfull=false; --helpmatch=; --helpon=; --helppackage=false; --helpshort=false; --helpxml=false; --version=false; --adambeta1=0.90000000000000002; --adambeta2=0.999; --am=; --am_decoder_tr_dropout=0; --am_decoder_tr_layerdrop=0; --am_decoder_tr_layers=1; --arch=am_500ms_future_context.arch; --archdir=/home/w2luser/Projects/wav2letter/recipes/models/streaming_convnets/librispeech; --attention=content; --attentionthreshold=2147483647; --attnWindow=no; --attnconvchannel=0; --attnconvkernel=0; --attndim=0; --batchsize=8; --beamsize=2500; --beamsizetoken=250000; --beamthreshold=25; --blobdata=false; --channels=1; --criterion=ctc; --critoptim=sgd; --datadir=; --dataorder=input; --decoderattnround=1; --decoderdropout=0; --decoderrnnlayer=1; --decodertype=wrd; --devwin=0; --emission_dir=; --emission_queue_size=3000; --enable_distributed=true; --encoderdim=0; --eosscore=0; --eostoken=false; --everstoredb=false; --fftcachesize=1; --filterbanks=80; --flagsfile=; --framesizems=25; --framestridems=10; --gamma=1; --gumbeltemperature=1; --input=flac; --inputbinsize=100; --inputfeeding=false; --isbeamdump=false; --iter=9223372036854775807; --itersave=false; --labelsmooth=0; --leftWindowSize=50; --lexicon=/home/w2luser/w2l/am/librispeech-train+dev-unigram-10000-nbest10.lexicon; --linlr=-1; --linlrcrit=-1; --linseg=0; --lm=; --lm_memory=5000; --lm_vocab=; --lmtype=kenlm; --lmweight=0; --localnrmlleftctx=300; --localnrmlrightctx=0; --logadd=false; --lr=0.40000000000000002; --lr_decay=9223372036854775807; --lr_decay_step=9223372036854775807; --lrcosine=false; --lrcrit=0; --max_devices_per_node=8; --maxdecoderoutputlen=200; --maxgradnorm=0.5; --maxisz=33000; --maxload=-1; --maxrate=10; --maxsil=50; --maxtsz=9223372036854775807; --maxword=-1; --melfloor=1; --memstepsize=10485760; --mfcc=false; --mfcccoeffs=13; --mfsc=true; --minisz=200; --minrate=3; --minsil=0; --mintsz=2; --momentum=0; --netoptim=sgd; --noresample=false; --nthread=6; --nthread_decoder=1; --nthread_decoder_am_forward=1; --numattnhead=8; --onorm=target; --optimepsilon=1e-08; --optimrho=0.90000000000000002; --outputbinsize=5; --pctteacherforcing=100; --pcttraineval=1; --pow=false; --pretrainWindow=0; --replabel=0; --reportiters=1000; --rightWindowSize=50; --rndv_filepath=; --rundir=/home/w2luser/w2l/rundir/am_500ms_future_context; --runname=inference_2019; --samplerate=16000; --sampletarget=0; --samplingstrategy=rand; --saug_fmaskf=27; --saug_fmaskn=2; --saug_start_update=-1; --saug_tmaskn=2; --saug_tmaskp=1; --saug_tmaskt=100; --sclite=; --seed=0; --show=false; --showletters=false; --silscore=0; --smearing=none; --smoothingtemperature=1; --softwoffset=10; --softwrate=5; --softwstd=5; --sqnorm=true; --stepsize=9223372036854775807; --surround=; --tag=; --target=tkn; --test=; --tokens=librispeech-train-all-unigram-10000.tokens; --tokensdir=/home/w2luser/w2l/am; --train=/home/w2luser/w2l/lists/train-clean-100.lst,/home/w2luser/w2l/lists/train-clean-360.lst,/home/w2luser/w2l/lists/train-other-500.lst; --trainWithWindow=false; --transdiag=0; --unkscore=-inf; --use_memcache=false; --uselexicon=true; --usewordpiece=true; --valid=dev-clean:/home/w2luser/w2l/lists/dev-clean.lst,dev-other:/home/w2luser/w2l/lists/dev-other.lst; --validbatchsize=-1; --warmup=1; --weightdecay=0; --wordscore=0; --wordseparator=_; --world_rank=0; --world_size=1; 
I20201111 21:35:13.722756 64900 Train.cpp:152] Experiment path: /home/w2luser/w2l/rundir/am_500ms_future_context/inference_2019
I20201111 21:35:13.722760 64900 Train.cpp:153] Experiment runidx: 1
I20201111 21:35:13.741053 64900 Train.cpp:199] Number of classes (network): 9998
I20201111 21:35:16.753304 64900 Train.cpp:206] Number of words: 85969
I20201111 21:35:16.830524 64900 Train.cpp:220] Loading architecture file from /home/w2luser/Projects/wav2letter/recipes/models/streaming_convnets/librispeech/am_500ms_future_context.arch
I20201111 21:35:17.256362 64900 Train.cpp:252] [Network] Sequential [input -&gt; (0) -&gt; (1) -&gt; (2) -&gt; (3) -&gt; (4) -&gt; (5) -&gt; (6) -&gt; (7) -&gt; (8) -&gt; (9) -&gt; (10) -&gt; (11) -&gt; (12) -&gt; (13) -&gt; (14) -&gt; (15) -&gt; (16) -&gt; (17) -&gt; (18) -&gt; (19) -&gt; (20) -&gt; (21) -&gt; (22) -&gt; (23) -&gt; (24) -&gt; (25) -&gt; (26) -&gt; (27) -&gt; (28) -&gt; (29) -&gt; (30) -&gt; (31) -&gt; (32) -&gt; (33) -&gt; (34) -&gt; (35) -&gt; (36) -&gt; (37) -&gt; (38) -&gt; (39) -&gt; output]
	(0): View (-1 80 1 0)
	(1): SpecAugment ( W: 80, F: 27, mF: 2, T: 100, p: 1, mT: 2 )
	(2): Padding (0, { (5, 3), (0, 0), (0, 0), (0, 0), })
	(3): Conv2D (1-&gt;15, 10x1, 2,1, 0,0, 1, 1) (with bias)
	(4): ReLU
	(5): Dropout (0.100000)
	(6): LayerNorm ( axis : { 1 2 } , size : -1)
	(7): Time-Depth Separable Block (9, 80, 15) [1200 -&gt; 1200 -&gt; 1200]
	(8): Time-Depth Separable Block (9, 80, 15) [1200 -&gt; 1200 -&gt; 1200]
	(9): Padding (0, { (7, 1), (0, 0), (0, 0), (0, 0), })
	(10): Conv2D (15-&gt;19, 10x1, 2,1, 0,0, 1, 1) (with bias)
	(11): ReLU
	(12): Dropout (0.100000)
	(13): LayerNorm ( axis : { 1 2 } , size : -1)
	(14): Time-Depth Separable Block (9, 80, 19) [1520 -&gt; 1520 -&gt; 1520]
	(15): Time-Depth Separable Block (9, 80, 19) [1520 -&gt; 1520 -&gt; 1520]
	(16): Time-Depth Separable Block (9, 80, 19) [1520 -&gt; 1520 -&gt; 1520]
	(17): Padding (0, { (9, 1), (0, 0), (0, 0), (0, 0), })
	(18): Conv2D (19-&gt;23, 12x1, 2,1, 0,0, 1, 1) (with bias)
	(19): ReLU
	(20): Dropout (0.100000)
	(21): LayerNorm ( axis : { 1 2 } , size : -1)
	(22): Time-Depth Separable Block (11, 80, 23) [1840 -&gt; 1840 -&gt; 1840]
	(23): Time-Depth Separable Block (11, 80, 23) [1840 -&gt; 1840 -&gt; 1840]
	(24): Time-Depth Separable Block (11, 80, 23) [1840 -&gt; 1840 -&gt; 1840]
	(25): Time-Depth Separable Block (11, 80, 23) [1840 -&gt; 1840 -&gt; 1840]
	(26): Padding (0, { (10, 0), (0, 0), (0, 0), (0, 0), })
	(27): Conv2D (23-&gt;27, 11x1, 1,1, 0,0, 1, 1) (with bias)
	(28): ReLU
	(29): Dropout (0.100000)
	(30): LayerNorm ( axis : { 1 2 } , size : -1)
	(31): Time-Depth Separable Block (11, 80, 27) [2160 -&gt; 2160 -&gt; 2160]
	(32): Time-Depth Separable Block (11, 80, 27) [2160 -&gt; 2160 -&gt; 2160]
	(33): Time-Depth Separable Block (11, 80, 27) [2160 -&gt; 2160 -&gt; 2160]
	(34): Time-Depth Separable Block (11, 80, 27) [2160 -&gt; 2160 -&gt; 2160]
	(35): Time-Depth Separable Block (11, 80, 27) [2160 -&gt; 2160 -&gt; 2160]
	(36): Reorder (2,1,0,3)
	(37): View (2160 -1 1 0)
	(38): Linear (2160-&gt;9998) (with bias)
	(39): View (9998 0 -1 1)
I20201111 21:35:17.256469 64900 Train.cpp:253] [Network Params: 115111823]
I20201111 21:35:17.256486 64900 Train.cpp:254] [Criterion] ConnectionistTemporalClassificationCriterion
I20201111 21:35:17.256525 64900 Train.cpp:262] [Network Optimizer] SGD
I20201111 21:35:17.256527 64900 Train.cpp:263] [Criterion Optimizer] SGD
I20201111 21:35:32.111979 64900 W2lListFilesDataset.cpp:141] 148688 files found. 
I20201111 21:35:32.120299 64900 Utils.cpp:102] Filtered 275946/275946 samples
I20201111 21:35:32.120335 64900 W2lListFilesDataset.cpp:62] Total batches (i.e. iters): 0
I20201111 21:35:32.611999 64900 W2lListFilesDataset.cpp:141] 2703 files found. 
I20201111 21:35:32.612103 64900 Utils.cpp:102] Filtered 2703/2703 samples
I20201111 21:35:32.612115 64900 W2lListFilesDataset.cpp:62] Total batches (i.e. iters): 0
I20201111 21:35:33.095692 64900 W2lListFilesDataset.cpp:141] 2864 files found. 
I20201111 21:35:33.095808 64900 Utils.cpp:102] Filtered 2864/2864 samples
I20201111 21:35:33.095818 64900 W2lListFilesDataset.cpp:62] Total batches (i.e. iters): 0
Falling back to using letters as targets for the unknown word: praecordia
Falling back to using letters as targets for the unknown word: eradication
Falling back to using letters as targets for the unknown word: debilitation
Initialized NCCL 2.7.8 successfully!
Falling back to using letters as targets for the unknown word: presuppositions
Falling back to using letters as targets for the unknown word: boscastle
Falling back to using letters as targets for the unknown word: sherley
Falling back to using letters as targets for the unknown word: strader
Falling back to using letters as targets for the unknown word: nasmyth
Falling back to using letters as targets for the unknown word: kaimak
Falling back to using letters as targets for the unknown word: nadab
Falling back to using letters as targets for the unknown word: athaliah
Falling back to using letters as targets for the unknown word: chippewas
Falling back to using letters as targets for the unknown word: voltairean
Falling back to using letters as targets for the unknown word: spiritualises
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
Falling back to using letters as targets for the unknown word: impropitious
Falling back to using letters as targets for the unknown word: eske
Falling back to using letters as targets for the unknown word: ungenial
Falling back to using letters as targets for the unknown word: brobdignagians
Falling back to using letters as targets for the unknown word: tanner's
Falling back to using letters as targets for the unknown word: linnell's
Falling back to using letters as targets for the unknown word: s'death
Falling back to using letters as targets for the unknown word: swearin
Falling back to using letters as targets for the unknown word: garcilaso
Falling back to using letters as targets for the unknown word: ghentese
Falling back to using letters as targets for the unknown word: puncher
Falling back to using letters as targets for the unknown word: mex
Falling back to using letters as targets for the unknown word: eetes's
Falling back to using letters as targets for the unknown word: melas
I20201111 21:36:51.010005 64900 Train.cpp:568] Shuffling trainset
I20201111 21:36:55.099699 64900 Train.cpp:575] Epoch 1 started!
I20201111 21:36:56.431582 64900 Train.cpp:568] Shuffling trainset
I20201111 21:36:57.502470 64900 Train.cpp:575] Epoch 2 started!
I20201111 21:36:58.612769 64900 Train.cpp:568] Shuffling trainset
I20201111 21:36:59.611886 64900 Train.cpp:575] Epoch 3 started!
I20201111 21:37:01.960247 64900 Train.cpp:568] Shuffling trainset
I20201111 21:37:02.892838 64900 Train.cpp:575] Epoch 4 started!
I20201111 21:37:04.320720 64900 Train.cpp:568] Shuffling trainset
I20201111 21:37:05.647351 64900 Train.cpp:575] Epoch 5 started!
I20201111 21:37:06.984242 64900 Train.cpp:568] Shuffling trainset
I20201111 21:37:07.974943 64900 Train.cpp:575] Epoch 6 started!
I20201111 21:37:09.230994 64900 Train.cpp:568] Shuffling trainset

&lt;Manually terminated&gt; 
Here's my train.cfg for tutorial recipe:
# Training config for Mini Librispeech
# Replace `[...]` with appropriate paths
--datadir=/home/w2luser/w2l
--rundir=/home/w2luser/w2l/rundir/tutorial
--archdir=/home/w2luser/Projects/wav2letter/tutorials/1-librispeech_clean
--train=lists/train-clean-100.lst
--valid=lists/dev-clean.lst
--input=flac
--arch=network.arch
--tokens=/home/w2luser/w2l/am/tokens.txt
--lexicon=/home/w2luser/w2l/am/lexicon.txt
--criterion=ctc
--lr=0.1
--maxgradnorm=1.0
--replabel=1
--surround=|
--onorm=target
--sqnorm=true
--mfsc=true
--filterbanks=40
--nthread=4
--batchsize=4
--runname=librispeech_clean_trainlogs
--iter=25
I get the following output (I have trimmed the output for the sake of brevity - ... lines indicate trimmed output):
/home/w2luser/Projects/wav2letter/cmake-build-debug-libtorch-remote/Train train --flagsfile /home/w2luser/Projects/wav2letter/tutorials/1-librispeech_clean/train.cfg
I20201111 23:40:50.662101 95799 Train.cpp:56] Parsing command line flags
I20201111 23:40:50.662155 95799 Train.cpp:59] Reading flags from file /home/w2luser/Projects/wav2letter/tutorials/1-librispeech_clean/train.cfg
I20201111 23:40:50.662310 95799 Train.cpp:151] Gflags after parsing 
--flagfile=; --fromenv=; --tryfromenv=; --undefok=; --tab_completion_columns=80; --tab_completion_word=; --help=false; --helpfull=false; --helpmatch=; --helpon=; --helppackage=false; --helpshort=false; --helpxml=false; --version=false; --adambeta1=0.90000000000000002; --adambeta2=0.999; --am=; --am_decoder_tr_dropout=0; --am_decoder_tr_layerdrop=0; --am_decoder_tr_layers=1; --arch=network.arch; --archdir=/home/w2luser/Projects/wav2letter/tutorials/1-librispeech_clean; --attention=content; --attentionthreshold=2147483647; --attnWindow=no; --attnconvchannel=0; --attnconvkernel=0; --attndim=0; --batchsize=4; --beamsize=2500; --beamsizetoken=250000; --beamthreshold=25; --blobdata=false; --channels=1; --criterion=ctc; --critoptim=sgd; --datadir=/home/w2luser/w2l; --dataorder=input; --decoderattnround=1; --decoderdropout=0; --decoderrnnlayer=1; --decodertype=wrd; --devwin=0; --emission_dir=; --emission_queue_size=3000; --enable_distributed=false; --encoderdim=0; --eosscore=0; --eostoken=false; --everstoredb=false; --fftcachesize=1; --filterbanks=40; --flagsfile=/home/w2luser/Projects/wav2letter/tutorials/1-librispeech_clean/train.cfg; --framesizems=25; --framestridems=10; --gamma=1; --gumbeltemperature=1; --input=flac; --inputbinsize=100; --inputfeeding=false; --isbeamdump=false; --iter=25; --itersave=false; --labelsmooth=0; --leftWindowSize=50; --lexicon=/home/w2luser/w2l/am/lexicon.txt; --linlr=-1; --linlrcrit=-1; --linseg=0; --lm=; --lm_memory=5000; --lm_vocab=; --lmtype=kenlm; --lmweight=0; --localnrmlleftctx=0; --localnrmlrightctx=0; --logadd=false; --lr=0.10000000000000001; --lr_decay=9223372036854775807; --lr_decay_step=9223372036854775807; --lrcosine=false; --lrcrit=0; --max_devices_per_node=8; --maxdecoderoutputlen=200; --maxgradnorm=1; --maxisz=9223372036854775807; --maxload=-1; --maxrate=10; --maxsil=50; --maxtsz=9223372036854775807; --maxword=-1; --melfloor=1; --memstepsize=10485760; --mfcc=false; --mfcccoeffs=13; --mfsc=true; --minisz=0; --minrate=3; --minsil=0; --mintsz=0; --momentum=0; --netoptim=sgd; --noresample=false; --nthread=4; --nthread_decoder=1; --nthread_decoder_am_forward=1; --numattnhead=8; --onorm=target; --optimepsilon=1e-08; --optimrho=0.90000000000000002; --outputbinsize=5; --pctteacherforcing=100; --pcttraineval=100; --pow=false; --pretrainWindow=0; --replabel=1; --reportiters=0; --rightWindowSize=50; --rndv_filepath=; --rundir=/home/w2luser/w2l/rundir; --runname=librispeech_clean_trainlogs; --samplerate=16000; --sampletarget=0; --samplingstrategy=rand; --saug_fmaskf=27; --saug_fmaskn=2; --saug_start_update=-1; --saug_tmaskn=2; --saug_tmaskp=1; --saug_tmaskt=100; --sclite=; --seed=0; --show=false; --showletters=false; --silscore=0; --smearing=none; --smoothingtemperature=1; --softwoffset=10; --softwrate=5; --softwstd=5; --sqnorm=true; --stepsize=9223372036854775807; --surround=|; --tag=; --target=tkn; --test=; --tokens=/home/w2luser/w2l/am/tokens.txt; --tokensdir=; --train=lists/train-clean-100.lst; --trainWithWindow=false; --transdiag=0; --unkscore=-inf; --use_memcache=false; --uselexicon=true; --usewordpiece=false; --valid=lists/dev-clean.lst; --validbatchsize=-1; --warmup=1; --weightdecay=0; --wordscore=0; --wordseparator=|; --world_rank=0; --world_size=1; 
I20201111 23:40:50.662626 95799 Train.cpp:152] Experiment path: /home/w2luser/w2l/rundir/librispeech_clean_trainlogs
I20201111 23:40:50.662629 95799 Train.cpp:153] Experiment runidx: 1
I20201111 23:40:50.662994 95799 Train.cpp:199] Number of classes (network): 30
I20201111 23:40:50.864882 95799 Train.cpp:206] Number of words: 32079
I20201111 23:40:50.889309 95799 Train.cpp:220] Loading architecture file from /home/w2luser/Projects/wav2letter/tutorials/1-librispeech_clean/network.arch
I20201111 23:40:51.412645 95799 Train.cpp:252] [Network] Sequential [input -&gt; (0) -&gt; (1) -&gt; (2) -&gt; (3) -&gt; (4) -&gt; (5) -&gt; (6) -&gt; (7) -&gt; (8) -&gt; (9) -&gt; (10) -&gt; (11) -&gt; (12) -&gt; (13) -&gt; (14) -&gt; (15) -&gt; (16) -&gt; (17) -&gt; (18) -&gt; (19) -&gt; (20) -&gt; output]
	(0): View (-1 1 40 0)
	(1): Conv2D (40-&gt;256, 8x1, 2,1, SAME,SAME, 1, 1) (with bias)
	(2): ReLU
	(3): Conv2D (256-&gt;256, 8x1, 1,1, SAME,SAME, 1, 1) (with bias)
	(4): ReLU
	(5): Conv2D (256-&gt;256, 8x1, 1,1, SAME,SAME, 1, 1) (with bias)
	(6): ReLU
	(7): Conv2D (256-&gt;256, 8x1, 1,1, SAME,SAME, 1, 1) (with bias)
	(8): ReLU
	(9): Conv2D (256-&gt;256, 8x1, 1,1, SAME,SAME, 1, 1) (with bias)
	(10): ReLU
	(11): Conv2D (256-&gt;256, 8x1, 1,1, SAME,SAME, 1, 1) (with bias)
	(12): ReLU
	(13): Conv2D (256-&gt;256, 8x1, 1,1, SAME,SAME, 1, 1) (with bias)
	(14): ReLU
	(15): Conv2D (256-&gt;256, 8x1, 1,1, SAME,SAME, 1, 1) (with bias)
	(16): ReLU
	(17): Reorder (2,0,3,1)
	(18): Linear (256-&gt;512) (with bias)
	(19): ReLU
	(20): Linear (512-&gt;30) (with bias)
I20201111 23:40:51.412698 95799 Train.cpp:253] [Network Params: 3900958]
I20201111 23:40:51.412706 95799 Train.cpp:254] [Criterion] ConnectionistTemporalClassificationCriterion
I20201111 23:40:51.412731 95799 Train.cpp:262] [Network Optimizer] SGD
I20201111 23:40:51.412737 95799 Train.cpp:263] [Criterion Optimizer] SGD
I20201111 23:40:53.071475 95799 W2lListFilesDataset.cpp:141] 23244 files found. 
I20201111 23:40:53.072247 95799 Utils.cpp:102] Filtered 0/23244 samples
I20201111 23:40:53.084225 95799 W2lListFilesDataset.cpp:62] Total batches (i.e. iters): 5811
I20201111 23:40:53.238716 95799 W2lListFilesDataset.cpp:141] 2703 files found. 
I20201111 23:40:53.238827 95799 Utils.cpp:102] Filtered 0/2703 samples
I20201111 23:40:53.240041 95799 W2lListFilesDataset.cpp:62] Total batches (i.e. iters): 676
I20201111 23:40:53.242017 95799 Train.cpp:568] Shuffling trainset
I20201111 23:40:53.244246 95799 Train.cpp:575] Epoch 1 started!
I20201111 23:41:35.351274 95799 Train.cpp:345] epoch:        1 | nupdates:           26 | lr: 0.050000 | lrcriterion: 0.000000 | runtime: 00:00:02 | bch(ms): 111.86 | smp(ms): 43.82 | fwd(ms): 47.99 | crit-fwd(ms): 24.57 | bwd(ms): 17.32 | optim(ms): 1.50 | loss:   55.12622 | train-TER: 99.42 | train-WER: 100.00 | lists/dev-clean.lst-loss:   31.75184 | lists/dev-clean.lst-TER: 100.00 | lists/dev-clean.lst-WER: 100.00 | avg-isz: 1240 | avg-tsz: 208 | max-tsz: 292 | hrs:    0.36 | thrpt(sec/sec): 443.62
Memory Manager Stats
MemoryManager type: CachingMemoryManager
Number of allocated blocks:20
Size of free block pool (small):16
Size of free block pool (large):10
Total native mallocs:20
Total native frees:0
I20201111 23:41:35.437580 95799 Trai.cpp:751] Finished training

Process finished with exit code 0
I am not sure all those words getting filtered out in both cases is to be expected. It looks like, all the files are getting filtered out in case of streaming convnets.
Does this indicate some sort of mistake while preparing data?
&lt;denchmark-h:h3&gt;Platform and Hardware&lt;/denchmark-h&gt;

Ubuntu 20.04, nVidia GPU 1080
Thanks!
	</description>
	<comments>
		<comment id='1' author='abhinavkulkarni' date='2020-11-12T06:37:45Z'>
		It looks like most of the words (may be all of them) are getting filtered out. Why would this be the case?
I created the data the following way:
cd /home/w2luser/Projects/wav2letter/recipes/models/utilities

python3 prepare_librispeech_wp_and_official_lexicon.py --data_dst /home/w2luser/w2l/ --model_dst /home/w2luser/w2l/
		</comment>
		<comment id='2' author='abhinavkulkarni' date='2020-11-12T08:36:39Z'>
		So, looks like these values specified in &lt;denchmark-link:https://github.com/facebookresearch/wav2letter/blob/v0.2/recipes/models/streaming_convnets/librispeech/train_am_500ms_future_context.cfg&gt;train_am_500ms_future_context.cfg&lt;/denchmark-link&gt;
 file are causing all the samples to be filtered out &lt;denchmark-link:https://github.com/facebookresearch/wav2letter/blob/v0.2/src/data/Utils.cpp#L83&gt;here in the code&lt;/denchmark-link&gt;
:
&lt;denchmark-code&gt;--minisz=200
--mintsz=2
--maxisz=33000
&lt;/denchmark-code&gt;

The values are absent in the &lt;denchmark-link:https://github.com/facebookresearch/wav2letter/blob/v0.2/tutorials/1-librispeech_clean/train.cfg&gt;train.cfg&lt;/denchmark-link&gt;
 file for the tutorial recipe, hence I was able to run it.
Why were these values left in the config file? And how come everybody else before me were able to train their models with these values present in the config that cause all the samples to be filtered out?
Asking these questions to make sure that I am not missing any important details out as I am about to embark on training a model on a much larger training set.
Thanks!
		</comment>
		<comment id='3' author='abhinavkulkarni' date='2020-11-12T09:13:53Z'>
		Hi,
Could you share first few lines from your train/valid files.
		</comment>
		<comment id='4' author='abhinavkulkarni' date='2020-11-12T09:22:06Z'>
		&lt;denchmark-link:https://github.com/vineelpratap&gt;@vineelpratap&lt;/denchmark-link&gt;
: Thanks! Here you go:
(base) w2luser@w2luser-MS-7A40:~$ ls $W2LDIR/lists
dev-clean.lst  dev-other.lst  test-clean.lst  test-other.lst  train-clean-100.lst  train-clean-360.lst  train-other-500.lst

(base) w2luser@w2luser-MS-7A40:~$ head -3 $W2LDIR/lists/*
==&gt; /home/w2luser/w2l/lists/dev-clean.lst &lt;==
dev-clean-1272-128104-0000      /home/w2luser/w2l/audio/LibriSpeech/dev-clean/1272/128104/1272-128104-0000.flac 5.855   mister quilter is the apostle of the middle classes and we are glad to welcome his gospel
dev-clean-1272-128104-0001      /home/w2luser/w2l/audio/LibriSpeech/dev-clean/1272/128104/1272-128104-0001.flac 4.815   nor is mister quilter's manner less interesting than his matter
dev-clean-1272-128104-0002      /home/w2luser/w2l/audio/LibriSpeech/dev-clean/1272/128104/1272-128104-0002.flac 12.485  he tells us that at this festive season of the year with christmas and roast beef looming before us similes drawn from eating and its results occur most readily to the mind

==&gt; /home/w2luser/w2l/lists/dev-other.lst &lt;==
dev-other-116-288045-0000       /home/w2luser/w2l/audio/LibriSpeech/dev-other/116/288045/116-288045-0000.flac   10.65   as i approached the city i heard bells ringing and a little later i found the streets astir with throngs of well dressed people in family groups wending their way hither and thither
dev-other-116-288045-0001       /home/w2luser/w2l/audio/LibriSpeech/dev-other/116/288045/116-288045-0001.flac   8.635   looking about me i saw a gentleman in a neat black dress smiling and his hand extended to me with great cordiality
dev-other-116-288045-0002       /home/w2luser/w2l/audio/LibriSpeech/dev-other/116/288045/116-288045-0002.flac   9.625   he must have realized i was a stranger and wished to tender his hospitality to me i accepted it gratefully i clasped his hand he pressed mine

==&gt; /home/w2luser/w2l/lists/test-clean.lst &lt;==
test-clean-1089-134686-0000     /home/w2luser/w2l/audio/LibriSpeech/test-clean/1089/134686/1089-134686-0000.flac        10.435  he hoped there would be stew for dinner turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick peppered flour fattened sauce
test-clean-1089-134686-0001     /home/w2luser/w2l/audio/LibriSpeech/test-clean/1089/134686/1089-134686-0001.flac        3.275   stuff it into you his belly counselled him
test-clean-1089-134686-0002     /home/w2luser/w2l/audio/LibriSpeech/test-clean/1089/134686/1089-134686-0002.flac        6.625   after early nightfall the yellow lamps would light up here and there the squalid quarter of the brothels

==&gt; /home/w2luser/w2l/lists/test-other.lst &lt;==
test-other-1688-142285-0000     /home/w2luser/w2l/audio/LibriSpeech/test-other/1688/142285/1688-142285-0000.flac        15.0    there's iron they say in all our blood and a grain or two perhaps is good but his he makes me harshly feel has got a little too much of steel anon
test-other-1688-142285-0001     /home/w2luser/w2l/audio/LibriSpeech/test-other/1688/142285/1688-142285-0001.flac        12.625  margaret said mister hale as he returned from showing his guest downstairs i could not help watching your face with some anxiety when mister thornton made his confession of having been a shop boy
test-other-1688-142285-0002     /home/w2luser/w2l/audio/LibriSpeech/test-other/1688/142285/1688-142285-0002.flac        2.835   you don't mean that you thought me so silly

==&gt; /home/w2luser/w2l/lists/train-clean-100.lst &lt;==
train-clean-100-103-1240-0000   /home/w2luser/w2l/audio/LibriSpeech/train-clean-100/103/1240/103-1240-0000.flac 14.085  chapter one missus rachel lynde is surprised missus rachel lynde lived just where the avonlea main road dipped down into a little hollow fringed with alders and ladies eardrops and traversed by a brook
train-clean-100-103-1240-0001   /home/w2luser/w2l/audio/LibriSpeech/train-clean-100/103/1240/103-1240-0001.flac 15.945  that had its source away back in the woods of the old cuthbert place it was reputed to be an intricate headlong brook in its earlier course through those woods with dark secrets of pool and cascade but by the time it reached lynde's hollow it was a quiet well conducted little stream
train-clean-100-103-1240-0002   /home/w2luser/w2l/audio/LibriSpeech/train-clean-100/103/1240/103-1240-0002.flac 13.945  for not even a brook could run past missus rachel lynde's door without due regard for decency and decorum it probably was conscious that missus rachel was sitting at her window keeping a sharp eye on everything that passed from brooks and children up

==&gt; /home/w2luser/w2l/lists/train-clean-360.lst &lt;==
train-clean-360-100-121669-0000 /home/w2luser/w2l/audio/LibriSpeech/train-clean-360/100/121669/100-121669-0000.flac     2.04    tom the piper's son
train-clean-360-100-121669-0001 /home/w2luser/w2l/audio/LibriSpeech/train-clean-360/100/121669/100-121669-0001.flac     5.15    the pig was eat and tom was beat and tom ran crying down the street
train-clean-360-100-121669-0002 /home/w2luser/w2l/audio/LibriSpeech/train-clean-360/100/121669/100-121669-0002.flac     12.26   he never did any work except to play the pipes and he played so badly that few pennies ever found their way into his pouch it was whispered around that old barney was not very honest

==&gt; /home/w2luser/w2l/lists/train-other-500.lst &lt;==
train-other-500-1006-135212-0000        /home/w2luser/w2l/audio/LibriSpeech/train-other-500/1006/135212/1006-135212-0000.flac   15.185  coming as it did at a period of exceptional dullness it attracted perhaps rather more attention than it deserved but it offered to the public that mixture of the whimsical and the tragic which is most stimulating to the popular imagination
train-other-500-1006-135212-0001        /home/w2luser/w2l/audio/LibriSpeech/train-other-500/1006/135212/1006-135212-0001.flac   12.114937       interest drooped however when after weeks of fruitless investigation it was found that no final explanation of the facts was forthcoming and the tragedy seemed from that time
train-other-500-1006-135212-0002        /home/w2luser/w2l/audio/LibriSpeech/train-other-500/1006/135212/1006-135212-0002.flac   14.49   it would be as well perhaps that i should refresh their memories as to the singular facts upon which this commentary is founded these facts were briefly as follows at five o'clock on the evening of the eighteenth of march```
		</comment>
		<comment id='5' author='abhinavkulkarni' date='2020-11-12T09:30:09Z'>
		It looks like third column (which corresponds to audio duration) is in seconds but '--minisz' and '--maxisz' are specified in milliseconds.
Could you multiply third column by 1000 for all '.lst' files and it should work.
		</comment>
		<comment id='6' author='abhinavkulkarni' date='2020-11-12T09:30:33Z'>
		&lt;denchmark-link:https://github.com/vineelpratap&gt;@vineelpratap&lt;/denchmark-link&gt;
: Thanks!
Looks like data prep script for tutorial recipe and convnet recipe differ!
		</comment>
	</comments>
</bug>