<bug id='905' author='trangtv57' open_date='2020-11-26T05:00:16Z' closed_time='2020-12-01T06:50:43Z'>
	<summary>Decode does not get best output when using decode in docker problem with word has same spelling (mean same lexicon)?</summary>
	<description>
&lt;denchmark-h:h3&gt;Bug Description&lt;/denchmark-h&gt;

I have done training model streaming conv net in my language. When i'm using decode in docker with just lexicon don't have oov word, everything ok, but when i'm adding lexicon for oov, something when wrong, output decode alway return oov inside not the word like normal, alough score of output word normal alway higher. And i can't figure out what happen.
&lt;denchmark-h:h4&gt;Reproduction Steps&lt;/denchmark-h&gt;

first is the beam dump decoding when not has lexicon of oov.
20200730080731-87ac3ae0-voice_transcript_v2 | 4375.661260 | 4386.166794 | -58.722191 | 8.333333 | trời ơi em a đưa cái cái điện thoại cho nó kêu là ba mươi tám ngàn lận cái cái nó móc ra tờ hai trăm hồi có tiền thối không nói vậy đó
and after adding oov lexicon, output alway replace 'hai' to 'high' or some word has same lexicon like 'hai' such as 'height'
20200730080731-87ac3ae0-voice_transcript_v2 | 4370.939512 | 4386.323524 | -65.691446 | 13.888889 | trời ơi em a đưa cái cái điện thoại cho nó kêu là ba mươi tám ngàn lận cái cái nó móc ra từ high chấm hỏi có tiền thối không nói vậy đó
focus on word has bold. And in this is lexicon of this word in file lexicon:
&lt;denchmark-code&gt;hai    _hai
hai    _h ai
high  _hai
high  _h ai
height _hai
&lt;/denchmark-code&gt;

spelling or lexicon of 2 word is same, but word 'hai' should be use because lm is so small when compare with 'high' and After add 'high' oov to lexicon file, beam dump just output key word oov. I don't know why?.
Can you give me some idea to debug more.
I have test language model of 2 sent, and sure score is right. So problem is just about decoding phase.
Additional info about wp decode:
&lt;denchmark-code&gt;|T|: trời ơi em a ca đưa cái cái điện thoại cho nó kêu là ba mươi tám ngàn lận cái a cái nó móc ra tờ hai trăm hỏi có tiền thối không nói vậy đó
|P|: trời ơi em a a đưa cái cái điện thoại cho nó kêu là ba mươi tám ngàn mà thế cái nó móc ra từ high trăm hồi có tiền thối không nói vậy đó
|t|: t r ờ i _ ơ i _ e m _ a _ c a _ đ ư a _ c á i _ c á i _ đ i ệ n _ t h o ạ i _ c h o _ n ó _ k ê u _ l à _ b a _ m ư ơ i _ t á m _ n g à n _ l ậ n _ c á i _ a _ c á i _ n ó _ m ó c _ r a _ t ờ _ h a i _ t r ă m _ h ỏ i _ c ó _ t i ề n _ t h ố i _ k h ô n g _ n ó i _ v ậ y _ đ ó
|p|: _ t r ờ i _ ơ i _ e m _ a _ a _ đ ư a _ c á i _ c á i _ đ i ệ n _ t h o ạ i _ c h o _ n ó _ k ê u _ l à _ b a _ m ư ơ i _ t á m _ n g à n _ m à _ t h ế _ c á i _ n ó _ m ó c _ r a _ t ừ _ h a i _ t r ă m _ h ồ i _ c ó _ t i ề n _ t h ố i _ k h ô n g _ n ó i _ v ậ y _ đ ó
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Platform and Hardware&lt;/denchmark-h&gt;

All experiment happen in docker.
update: I try to use lastest version of flashlight and wav2letter, the same result.
	</description>
	<comments>
		<comment id='1' author='trangtv57' date='2020-11-28T02:16:16Z'>
		You should always expect language model to return worse score on OOV, so consider

retrain a new language model including most of the OOVs.
retune the decoder parameters (lm weight, word score, etc).

		</comment>
		<comment id='2' author='trangtv57' date='2020-11-28T14:47:50Z'>
		tks &lt;denchmark-link:https://github.com/xuqiantong&gt;@xuqiantong&lt;/denchmark-link&gt;

as i mention in my question, i have logged of decode output and its sure about lm score and word score is right. And first my lm is good enough for declare what phone should be is oov or not.
and i don't know why you think i said lm of oov is alway wórse. i test on special case i know my lm good.
my problem is about 2 word oov and word in dict have same pronunciation as same lexicon so i think trie  built in wav2letter not handle this.
		</comment>
		<comment id='3' author='trangtv57' date='2020-11-30T03:17:33Z'>
		&lt;denchmark-link:https://github.com/tlikhomanenko&gt;@tlikhomanenko&lt;/denchmark-link&gt;

Can you have any help for my question. Because i see you comment in a issue  have some relation, &lt;denchmark-link:https://github.com/facebookresearch/wav2letter/issues/693&gt;#693&lt;/denchmark-link&gt;
 .
		</comment>
		<comment id='4' author='trangtv57' date='2020-12-01T06:50:43Z'>
		I have figured out what is my problem  and how to solve, so i closed the post.
thanks
		</comment>
		<comment id='5' author='trangtv57' date='2020-12-01T06:51:50Z'>
		Here you can see &lt;denchmark-link:https://github.com/facebookresearch/wav2letter/blob/v0.2/src/libraries/decoder/LexiconDecoder.cpp#L112&gt;https://github.com/facebookresearch/wav2letter/blob/v0.2/src/libraries/decoder/LexiconDecoder.cpp#L112&lt;/denchmark-link&gt;
 that we have a loop over all possible words for the same trie node (so labels here will be "hai", "height", "high" for the same path for tokens sequence "_hai"). We have only restrictions that we process max 6 &lt;denchmark-link:https://github.com/facebookresearch/wav2letter/blob/v0.2/src/libraries/decoder/Trie.h#L16&gt;https://github.com/facebookresearch/wav2letter/blob/v0.2/src/libraries/decoder/Trie.h#L16&lt;/denchmark-link&gt;
 words with the same spelling. But first max 6 words with the same spelling will be added in the beam.
		</comment>
		<comment id='6' author='trangtv57' date='2020-12-01T06:52:15Z'>
		&lt;denchmark-link:https://github.com/trangtv57&gt;@trangtv57&lt;/denchmark-link&gt;
 could you post what was the problem?
		</comment>
		<comment id='7' author='trangtv57' date='2020-12-01T06:53:36Z'>
		Seems that you probably have problem with hyps pruning, only there it could happen to remove some hyp which is better with the score you showed above.
		</comment>
		<comment id='8' author='trangtv57' date='2020-12-01T07:38:09Z'>
		Hi &lt;denchmark-link:https://github.com/tlikhomanenko&gt;@tlikhomanenko&lt;/denchmark-link&gt;

problem is i have many oov have same spelling, then my word in dictionary has prune, because your setting, anyway, i think you should add this option to defines.cpp or wiki doc. I can't understand after i try to reading and debug code.
Thanks

Here you can see https://github.com/facebookresearch/wav2letter/blob/v0.2/src/libraries/decoder/LexiconDecoder.cpp#L112 that we have a loop over all possible words for the same trie node (so labels here will be "hai", "height", "high" for the same path for tokens sequence "_hai"). We have only restrictions that we process max 6 https://github.com/facebookresearch/wav2letter/blob/v0.2/src/libraries/decoder/Trie.h#L16 words with the same spelling. But first max 6 words with the same spelling will be added in the beam.

		</comment>
	</comments>
</bug>