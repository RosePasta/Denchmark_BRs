<bug id='807' author='abhinavkulkarni' open_date='2020-08-28T16:48:52Z' closed_time='2020-09-08T20:39:51Z'>
	<summary>Discrepancy between Conv1D implementation of FbGemm vs LibTorch/PyTorch</summary>
	<description>
&lt;denchmark-h:h3&gt;Conv1D logic is implemented differently in fbgemm vs LibTorch/PyTorch/TensorFlow/Keras&lt;/denchmark-h&gt;

TL;DR: FbGemm implementation of Conv1D keeps only one weight matrix overall vs one weight matrix per group
 
 
Conv1D layer takes a N inChannels-dimensional vectors as input and produces M outChannels-dimensional vectors as output. The value of M is determined by N, kernelSize, stride and  padding.
For e.g., in &lt;denchmark-link:https://missinglink.ai/wp-content/uploads/2019/03/1D-convolutional-example_2x.png&gt;this&lt;/denchmark-link&gt;
 picture,
N=9, inChannels=6, kernelSize=2, stride=1, padding=0, M=8
The weight matrix is of the size outChannels x inChannels x kernelSize.
There is an additional groups parameter. When groups &gt; 1, there are groups number of weight matrices. Each matrix is of the size outChannels/groups x inChannels/groups x kernelSize. Essentially each matrix is responsible for convolving groups share of input channels to produce groups share of output channels.
So, after concatenating these groups number of weight matrices, the total size of the final weight matrix is outChannels x inChannels/groups x kernelSize. This helps in reducing the number of parameters (while sacrificing little to nothing on accuracy).
Here is an PyTorch example:
&gt;&gt;&gt; import torch
&gt;&gt;&gt; conv1d = torch.nn.Conv1d(in_channels=4, out_channels=16, kernel_size=3, stride=2, padding=0, groups=2)
&gt;&gt;&gt; conv1d.weight.shape
torch.Size([16, 2, 3])
However, in FbGemm implementation, only 1 copy of the weight matrix is kept overall (instead of one copy per group) and thus the size of the overall weight matrix is  as can be seen &lt;denchmark-link:https://github.com/facebookresearch/wav2letter/blob/v0.2/inference/inference/module/nn/backend/fbgemm/Conv1dFbGemm.cpp#L59&gt;here&lt;/denchmark-link&gt;
.
Is it intentional to keep only one copy of the weight matrix and not different copies per group? Does this discrepancy exist because ArrayFire implementation of Conv1D keeps only one copy of the weight matrix overall (vs one per group)?
Thanks!
	</description>
	<comments>
		<comment id='1' author='abhinavkulkarni' date='2020-08-28T16:49:22Z'>
		Tagging &lt;denchmark-link:https://github.com/tlikhomanenko&gt;@tlikhomanenko&lt;/denchmark-link&gt;

Thanks!
		</comment>
		<comment id='2' author='abhinavkulkarni' date='2020-08-28T18:35:01Z'>
		Hi,
Thanks for mentioning this. Yes, there seems to be a bug in the notation in inference pipeline and needs to be fixed.
However, note that it won't affect the correctness of the current streaming ASR pipeline and we can create  layer with . In other words, we replace  with  here - &lt;denchmark-link:https://github.com/facebookresearch/wav2letter/blob/v0.2/tools/StreamingTDSModelConverter.cpp#L112&gt;https://github.com/facebookresearch/wav2letter/blob/v0.2/tools/StreamingTDSModelConverter.cpp#L112&lt;/denchmark-link&gt;
 .
		</comment>
		<comment id='3' author='abhinavkulkarni' date='2020-08-28T18:41:53Z'>
		&lt;denchmark-link:https://github.com/vineelpratap&gt;@vineelpratap&lt;/denchmark-link&gt;
: Thanks for responding.
As far as I understood, only Linear and Conv1d layers need to be reimplemented in a new backend for the inference pipeline. As long as I correctly mimic the existing FbGemm behavior, it should be possible for me to use my own custom backend, right?
Thanks
		</comment>
		<comment id='4' author='abhinavkulkarni' date='2020-08-28T19:00:36Z'>
		For example, if you are trying to replace the layer Conv1dFbGemm(inChannels, outChannels, kernel, groups), you can create torch.conv1d(inChannels, outChannels/groups, kernel, groups) - they should be equivalent. Note that weight you might have to do .permute to make sure the data ordering is correct (I'm not sure).

only Linear and Conv1d layers need to be reimplemented in a new backend for the inference

Yes, that's true. And you make it work on on ANY device. Note that if you trying to replace  code, you are in fact to trying to replace a matrix multiplication. So, there is no need to bother about channel/group things.
For example, the code &lt;denchmark-link:https://github.com/facebookresearch/wav2letter/blob/v0.2/inference/inference/module/nn/backend/fbgemm/Conv1dFbGemm.cpp#L59&gt;here&lt;/denchmark-link&gt;
 is doing matrix multiplication with  and  (may be with some  involved). So, as long as you can replicate this behavior it should be fine.
		</comment>
		<comment id='5' author='abhinavkulkarni' date='2020-09-08T20:39:51Z'>
		Thanks, &lt;denchmark-link:https://github.com/vineelpratap&gt;@vineelpratap&lt;/denchmark-link&gt;
, I was able to replicate the behavior in . Closing this issue for now.
		</comment>
	</comments>
</bug>