<bug id='937' author='tranmanhdat' open_date='2021-01-14T15:09:56Z' closed_time='2021-01-18T08:26:36Z'>
	<summary>OOM when training stream convnet on custom data</summary>
	<description>
&lt;denchmark-h:h3&gt;Bug Description&lt;/denchmark-h&gt;

Memory does not release when training model
in buff/cache of memory increase when start train and lead to OOM
&lt;denchmark-h:h4&gt;Reproduction Steps&lt;/denchmark-h&gt;

Follow &lt;denchmark-link:https://github.com/facebookresearch/wav2letter/tree/master/recipes/streaming_convnets/librispeech&gt;tutorials&lt;/denchmark-link&gt;
, train on custom data( approximate 500 hours~ 150gb
&lt;denchmark-h:h3&gt;Platform and Hardware&lt;/denchmark-h&gt;

Ubuntu 18.04.5 LTS
Intel(R) Xeon(R) CPU E5-2690 v4@ 2.60GHz
2 GPUs Tesla V100
64 GB RAM
&lt;denchmark-h:h4&gt;Additional Context&lt;/denchmark-h&gt;

Run with docker image cuda lastest, architect same tutorial
my flagsfile
&lt;denchmark-link:https://user-images.githubusercontent.com/30494878/104608805-ea4dc780-56b4-11eb-954c-c193db1bb1b8.png&gt;&lt;/denchmark-link&gt;

memory when run training
&lt;denchmark-link:https://user-images.githubusercontent.com/30494878/104608852-fa65a700-56b4-11eb-91e9-98e7f0fb98cf.png&gt;&lt;/denchmark-link&gt;

my Gpus
&lt;denchmark-link:https://user-images.githubusercontent.com/30494878/104608909-09e4f000-56b5-11eb-8307-a762df723f81.png&gt;&lt;/denchmark-link&gt;

train process
&lt;denchmark-link:https://user-images.githubusercontent.com/30494878/104609037-2b45dc00-56b5-11eb-8d5a-45ac705004a4.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='tranmanhdat' date='2021-01-17T04:15:13Z'>
		What is your longest audio?
		</comment>
		<comment id='2' author='tranmanhdat' date='2021-01-18T08:26:36Z'>
		
What is your longest audio?

approximate 24s, i find out after i release cached then restart docker, bug had gone, maybe docker container didn't release all cache!
		</comment>
	</comments>
</bug>