<bug id='76' author='araffin' open_date='2020-06-29T15:21:30Z' closed_time='2020-06-29T15:58:56Z'>
	<summary>[Bug] Wrong target q in SAC</summary>
	<description>
In SAC, when updating the Q-values:



stable-baselines3/stable_baselines3/sac/sac.py


        Lines 205 to 208
      in
      96b771f






 target_q = th.min(target_q1, target_q2) 



 target_q = replay_data.rewards + (1 - replay_data.dones) * self.gamma * target_q 



 # td error + entropy term 



 q_backup = target_q - ent_coef * next_log_prob.reshape(-1, 1) 





should be (entropy term also conditioned by termination):
target_q = th.min(target_q1, target_q2) - ent_coef * next_log_prob.reshape(-1, 1)
q_backup = replay_data.rewards + (1 - replay_data.dones) * self.gamma * target_q
See original implementation: &lt;denchmark-link:https://github.com/rail-berkeley/softlearning/blob/master/softlearning/algorithms/sac.py#L26&gt;https://github.com/rail-berkeley/softlearning/blob/master/softlearning/algorithms/sac.py#L26&lt;/denchmark-link&gt;

The bug is not present in SB2 (need to double) as it is using the SAC variant with a value function whereas we are using 2 Q-values.
I will push a fix soon.
	</description>
	<comments>
	</comments>
</bug>