<bug id='46' author='PartiallyTyped' open_date='2020-06-08T07:45:55Z' closed_time='2020-06-29T09:49:26Z'>
	<summary>Potential continuity bug in the replay buffer when calling .learn multiple times</summary>
	<description>
agent.learn(N)
agent.learn(N)
# ER isn't reset but the environment is.
This isn't a bug - as is - but the exact behaviour needs to be defined in order to decide what is the correct way to deal with a bug in ER in the DQN branch:
Originally posted by @PartiallyTyped in #28 (comment)
	</description>
	<comments>
		<comment id='1' author='PartiallyTyped' date='2020-06-08T12:34:20Z'>
		What do you propose as a fix?
I would say set done=True to the last observation (index self.pos -1), no?
I don't see a better one yet...
		</comment>
		<comment id='2' author='PartiallyTyped' date='2020-06-08T13:33:32Z'>
		
I would say set done=True to the last observation (index self.pos -1), no?

This messes up the MDP though, it's the same reason we use TimeFeatureWrapper, only in this case, we alter the signal.
		</comment>
		<comment id='3' author='PartiallyTyped' date='2020-06-08T13:38:42Z'>
		
only in this case, we alter the signal.

Well, the user alters the signal, otherwise, he just need to use reset_num_timesteps=False
		</comment>
		<comment id='4' author='PartiallyTyped' date='2020-06-08T13:41:50Z'>
		That's true. I think that we should state that when reset_num_timesteps=True the buffer is emptied/freed. This resolves the problem without a hacky or weird solution. If the user wants to fill in the buffer with older/previous experience, they can do it with callbacks imho and only if they understand the 'risks'.
This also makes it easier to implement &lt;denchmark-link:https://github.com/DLR-RM/stable-baselines3/issues/47&gt;#47&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='5' author='PartiallyTyped' date='2020-06-08T13:48:07Z'>
		
the buffer is emptied/freed.

Sounds quite radical compared to stopping one trajectory early.
		</comment>
		<comment id='6' author='PartiallyTyped' date='2020-06-08T13:49:36Z'>
		I'm also thinking about having 2 replay buffer implementations, one that is simple and easy to read and one that is memory efficient. But I don't like duplicating code...
		</comment>
		<comment id='7' author='PartiallyTyped' date='2020-06-08T13:50:55Z'>
		Maybe for the time being, we can keep the plain buffer until we resolve these issues, then again, a single transition is nothing when we can effectively double the size.
I am neutral about this.
		</comment>
		<comment id='8' author='PartiallyTyped' date='2020-06-10T09:55:45Z'>
		I decided to disable the memory efficient variant by default and add a warning if the user is doing multiple .learn() calls.
I also decided to truncate the trajectory (less radical than clearing the buffer)
See &lt;denchmark-link:https://github.com/DLR-RM/stable-baselines3/commit/8193f5a9e2fab47bf1854e8e8db5061892919aa8&gt;8193f5a&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='PartiallyTyped' date='2020-06-10T10:02:23Z'>
		LGTM! Feel free to close this.
		</comment>
	</comments>
</bug>