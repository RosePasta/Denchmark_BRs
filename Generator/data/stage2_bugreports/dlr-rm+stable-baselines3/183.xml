<bug id='183' author='Wovchena' open_date='2020-10-11T17:58:25Z' closed_time='2020-10-12T21:10:56Z'>
	<summary>Better advantage estimation for OnPolicyAlgorithm</summary>
	<description>
Hi.
Your reimplementation helped me to figure out how a2c works. But while I was studying it, I found some places which can be improved in the reimplementation.

There is a bug in 


stable-baselines3/stable_baselines3/common/on_policy_algorithm.py


         Line 182
      in
      a1e0556






 rollout_buffer.compute_returns_and_advantage(values, dones=dones) 




compute_returns_and_advantage() should take values which correspond to the very last observations as it takes the last dones. But here the function takes values corresponding to previous observations. It should take values corresponding to new_obs, which you need to infer first.



stable-baselines3/stable_baselines3/common/buffers.py


         Line 322
      in
      a1e0556






 if step == self.buffer_size - 1: 




 The if handles only first iteration of the loop. It could be just moved out of the loop.

	</description>
	<comments>
		<comment id='1' author='Wovchena' date='2020-10-11T18:33:31Z'>
		
Very nice catch! These off-by-one errors seem to be very hard to iron out, even when comparing learning curves and all...
Good point. If the first iteration can be moved outside without too much edge-case checking and other mess I think this could be done. Sadly in Python this might not offer a big performance bonus, but if clarity of the code remains I agree this would be net positive.

A PR to fix both would be welcome :)
		</comment>
	</comments>
</bug>