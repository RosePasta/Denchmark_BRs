<bug id='66' author='siferati' open_date='2020-06-17T13:10:28Z' closed_time='2020-06-18T14:21:43Z'>
	<summary>Episode mean reward is not properly logged on tensorboard when using SAC</summary>
	<description>
I'm training an agent on a custom environment using SAC. The environment is wrapped in a Monitor, which is wrapped in a DummyVecEnv, which is wrapped in a VecNormalize, with norm_reward = True.
This is the tensorboard graph for the episode mean reward:



No smoothing
0.9 Smoothing









As you can see, the graph has some weird loops. For example, at around 170k steps or 450k steps.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Edit: Training is conducted in epochs of 50k steps.
Program starts by calling ./start.sh.
start.sh
#!/bin/bash

while [ "$?" -eq 0 ]; do
	python3 main.py
done
main.py
import os.path

from my_custom_env import MyCustomEnv

from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize

class SaveCheckpoint(BaseCallback):
	def __init__(self, save_freq, verbose = 0):
		super(SaveCheckpoint, self).__init__(verbose)
		self.save_freq = save_freq

	def _on_step(self):
		if self.num_timesteps % self.save_freq == 0:
			self.model.save("model.zip")
			self.training_env.save("stats.pkl")

		return True


if __name__ == '__main__':

	# inits
	env = DummyVecEnv([lambda: Monitor(MyCustomEnv())])
	model = None

	# load recent checkpoint
	if os.path.isfile("model.zip") and os.path.isfile("stats.pkl"):
		env = VecNormalize.load("stats.pkl", env)
		env.reset()
		model = SAC.load("model.zip", env)
	else:
		env = VecNormalize(env)
		model = SAC('MlpPolicy', env, verbose = 1, tensorboard_log = ".")

	# replay buffer
	if os.path.isfile("replay_buffer.pkl"):
		model.load_replay_buffer("replay_buffer.pkl")

	# train
	model.learn(50000,
		callback = SaveCheckpoint(10000),
		log_interval = 1,
		reset_num_timesteps = False
	)

	# save replay buffer
	model.save_replay_buffer(".")

	env.close()
&lt;denchmark-code&gt;&gt; pip3 freeze | grep 'stable-baselines3'
stable-baselines3==0.7.0a1
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='siferati' date='2020-06-17T13:13:58Z'>
		Hello,
Please fill the issue template completely.
This is weird as I don't see how timesteps cannot be ordered when using only one environment.
		</comment>
		<comment id='2' author='siferati' date='2020-06-17T13:35:56Z'>
		Updated the post with more detailed info
		</comment>
		<comment id='3' author='siferati' date='2020-06-17T14:01:15Z'>
		In your code, the model saving and stat saving is missing...
I could not (yet) reproduce the bug with the following code:
import os

import gym

from stable_baselines3 import SAC
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize

if __name__ == '__main__':

    # inits
    env = DummyVecEnv([lambda: Monitor(gym.make('Pendulum-v0'))])
   
    # load recent checkpoint
    if os.path.isfile("model.zip") and os.path.isfile("stats.pkl"):
        env = VecNormalize.load("stats.pkl", env)
        env.reset()
        model = SAC.load("model.zip", env)
        print("Loaded")
    else:
        env = VecNormalize(env)
        model = SAC('MlpPolicy', env, verbose = 1, tensorboard_log="/tmp/sb3/",
                    policy_kwargs=dict(net_arch=[64, 64]))

    # replay buffer
    if os.path.isfile("replay_buffer.pkl"):
        model.load_replay_buffer("replay_buffer.pkl")

    # train
    model.learn(2000,
        log_interval=1,
        reset_num_timesteps=False
    )

    model.save('model')
    # save replay buffer
    model.save_replay_buffer("replay_buffer.pkl")
    env.save('stats.pkl')

    env.close()
		</comment>
		<comment id='4' author='siferati' date='2020-06-17T14:15:18Z'>
		
In your code, the model saving and stat saving is missing...

It isn't...? The saving is done by the callback, every 10k steps.
		</comment>
		<comment id='5' author='siferati' date='2020-06-17T14:20:25Z'>
		

In your code, the model saving and stat saving is missing...

It isn't...? The saving is done by the callback, every 10k steps.

True, sorry, i wanted to remove anything that should not be related but now i think this is your problem.
Your are saving checkpoints of models but not the latest one. When you load the last checkpoint, there is a mismatch in the number of timesteps and that causes the weird graph.
		</comment>
		<comment id='6' author='siferati' date='2020-06-17T14:33:01Z'>
		Hm I see. So what you are saying is that the last callback will not be called?
So after the first epoch, I will have saved at 10k steps, 20k, 30k, 40k, but at 50k the training stops and the callback doesn't get called?
		</comment>
		<comment id='7' author='siferati' date='2020-06-18T14:21:43Z'>
		Yup, that seems to have been the problem. Thanks!
		</comment>
	</comments>
</bug>