<bug id='221' author='nbro' open_date='2020-11-15T17:07:53Z' closed_time='2020-11-16T22:43:27Z'>
	<summary>Why would the predict method of the DQN model return an array of multiple indices for the next action?</summary>
	<description>
I am not yet able to reproduce this error with a simple example (given that I am not using a gym environment), but, at some point, the predict method is returning an array as the predicted action, while it does not return anything for the next state/observation, i.e., at some point, the following statement
&lt;denchmark-code&gt;action, _states = model.predict(next_observation)
&lt;/denchmark-code&gt;

returns action = [0 1 0 0] (whose type is numpy.ndarray) and _states = None. Why would this be the case?
I am using
&lt;denchmark-code&gt;from stable_baselines3.dqn.dqn import DQN
from stable_baselines3.dqn.policies import MlpPolicy

model = DQN(MlpPolicy, env, verbose=1, exploration_fraction=0.1, tensorboard_log="some_folder")
model.learn(total_timesteps=10000)
&lt;/denchmark-code&gt;

Note that, before this happening, model.predict correctly returns only one index, which represents only 1 action, but _states is always None (not sure why too).
By looking at your implementation &lt;denchmark-link:https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/dqn/dqn.py#L207&gt;here&lt;/denchmark-link&gt;
, this should happen if . Now, in my case, I don't set the parameter , whose default value is , so  is true, so what is causing the problem is . The thing is: I never experienced this when I was playing with stable-baselines 2. Also, I don't understand your logic in that implementation anyway. Why would  and then sample multiple actions? I guess that if I set , then I will not experience this problem anymore.
Should deterministic always be True after learning?
	</description>
	<comments>
		<comment id='1' author='nbro' date='2020-11-15T17:53:22Z'>
		
Note that, before this happening, model.predict correctly returns only one index, which represents only 1 action, but _states is always None (not sure why too).

state refers to the hidden states for recurrent policies (cf doc) and should not be confused with the environment state, returned by env.reset() or env.step() (written obs for observation in stable-baselines).

returns action = [0 1 0 0]
Also, I don't understand your logic in that implementation anyway. Why would n_batch = observation.shape[0] and then sample multiple actions?

Well, it depends of the shape of the observation. It is made to work with vectorized environments (cf doc).

Should deterministic always be True after learning?

it depends. In continuous control, it is recommended, for Atari games, as the game is deterministic too, it may lead to an infinite game (where the agent neither win, no loses, see &lt;denchmark-link:https://twitter.com/araffin2/status/1164902485749915649&gt;https://twitter.com/araffin2/status/1164902485749915649&lt;/denchmark-link&gt;
).
EDIT: I see what you mean, will push a bug fix soon
		</comment>
		<comment id='2' author='nbro' date='2020-11-15T18:05:43Z'>
		Thanks for reporting the bug, this will be fixed in &lt;denchmark-link:https://github.com/DLR-RM/stable-baselines3/pull/222&gt;#222&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>