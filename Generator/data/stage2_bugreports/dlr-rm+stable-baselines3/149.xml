<bug id='149' author='diditforlulz273' open_date='2020-08-27T10:57:59Z' closed_time='2020-09-01T07:52:32Z'>
	<summary>PPO clip_fraction logging bug</summary>
	<description>
I think I've found a minor bug in your PPO algo realisation.
See, in the main class PPO(OnPolicyAlgorithm), def train() function, after all the loops and main code there is a logging part:
logger.record("train/entropy_loss", np.mean(entropy_losses))
logger.record("train/policy_gradient_loss", np.mean(pg_losses))
logger.record("train/value_loss", np.mean(value_losses))
logger.record("train/approx_kl", np.mean(approx_kl_divs))
logger.record("train/clip_fraction", np.mean(clip_fraction))
logger.record("train/loss", loss.item())
logger.record("train/explained_variance", explained_var)
take a closer look at logging variables. While np.mean is used over lists of accumulated values everywhere, for clip fraction you have np.mean(clip_fraction), NOT clip_fractions. So you always log the fraction from the last rollout data batch only, not avg for the full algo iteration.
I'm pretty certain I'm right. I can make a PR correcting it if you agree.
	</description>
	<comments>
		<comment id='1' author='diditforlulz273' date='2020-08-27T11:01:26Z'>
		Hello,
nice catch.

I can make a PR correcting it if you agree.

please do ;) (don't forget to read the CONTRIBUTING guide first)
		</comment>
	</comments>
</bug>