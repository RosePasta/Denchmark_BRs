<bug id='295' author='ardabbour' open_date='2021-01-19T13:00:20Z' closed_time='2021-01-21T00:42:34Z'>
	<summary>[Bug] Multi env problem with Taxi-v3</summary>
	<description>
&lt;denchmark-h:h3&gt;üêõ Bug&lt;/denchmark-h&gt;

The A2C and PPO algorithms are seeming not working in parallel for the Taxi-v3 environment. Judging by the error message it is a numpy problem. There is no error unless multiprocessing is used. Also, multiprocessing works on other environments such as 'CartPole-v1', so I'm guessing it has to do with the discrete state space.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-h:h4&gt;Minimal Code&lt;/denchmark-h&gt;

from stable_baselines3 import A2C # this can also be PPO, same error
from stable_baselines3.common.env_util import make_vec_env
env = make_vec_env("Taxi-v3", n_envs=4, seed=0)
model = A2C(policy="MlpPolicy", env=env, verbose=True)
model.learn(total_timesteps=10000)
&lt;denchmark-h:h4&gt;Traceback&lt;/denchmark-h&gt;

Using cuda device
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-3-4551c9bb2165&gt; in &lt;module&gt;
      3 env = make_vec_env("Taxi-v3", n_envs=4, seed=0)
      4 model = A2C(policy="MlpPolicy", env=env, verbose=True)
----&gt; 5 model.learn(total_timesteps=10_000)

~/envs/bsh/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py in learn(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)
    181     ) -&gt; "A2C":
    182 
--&gt; 183         return super(A2C, self).learn(
    184             total_timesteps=total_timesteps,
    185             callback=callback,

~/envs/bsh/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py in learn(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)
    220         while self.num_timesteps &lt; total_timesteps:
    221 
--&gt; 222             continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
    223 
    224             if continue_training is False:

~/envs/bsh/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py in collect_rollouts(self, env, callback, rollout_buffer, n_rollout_steps)
    176                 # Reshape in case of discrete action
    177                 actions = actions.reshape(-1, 1)
--&gt; 178             rollout_buffer.add(self._last_obs, actions, rewards, self._last_dones, values, log_probs)
    179             self._last_obs = new_obs
    180             self._last_dones = dones

~/envs/bsh/lib/python3.8/site-packages/stable_baselines3/common/buffers.py in add(self, obs, action, reward, done, value, log_prob)
    350             log_prob = log_prob.reshape(-1, 1)
    351 
--&gt; 352         self.observations[self.pos] = np.array(obs).copy()
    353         self.actions[self.pos] = np.array(action).copy()
    354         self.rewards[self.pos] = np.array(reward).copy()

ValueError: could not broadcast input array from shape (4) into shape (4,1)
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

A model to get trained in parallel using A2C (or PPO).
###¬†System Info

pip inside venv
Graphics card info: GeForce GTX 1050 Ti, Nvidia driver version 460.27.04, CUDA version 11.2
Python version 3.8.7
PyTorch version 1.7.1
Gym version 0.18.0

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

No additional context.
&lt;denchmark-h:h3&gt;Checklist&lt;/denchmark-h&gt;


 I have checked that there is no similar issue in the repo (required)
 I have read the documentation (required)
 I have provided a minimal working example to reproduce the bug (required)

	</description>
	<comments>
		<comment id='1' author='ardabbour' date='2021-01-19T13:37:50Z'>
		Hello,
thanks for raising the issue.
yes, the problem comes from the discrete observation space which is apparently not handled properly in SB3 when using multiple environments.
		</comment>
		<comment id='2' author='ardabbour' date='2021-01-19T14:09:18Z'>
		I pushed a fix in &lt;denchmark-link:https://github.com/DLR-RM/stable-baselines3/pull/296&gt;#296&lt;/denchmark-link&gt;
, should be merged with master soon ;)
		</comment>
	</comments>
</bug>