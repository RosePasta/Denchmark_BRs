<bug id='545' author='mjpost' open_date='2018-09-25T20:32:53Z' closed_time='2019-08-29T12:56:45Z'>
	<summary>Inconsistencies with scoring and inference</summary>
	<description>
There are a few inconsistencies related to inference that have come up in evaluating scoring (&lt;denchmark-link:https://github.com/awslabs/sockeye/pull/538&gt;#538&lt;/denchmark-link&gt;
). However, these are corner cases that could arise in other settings, as well. The problem is:

Length normalization is only applied to completed hypotheses on the beam. However, if the beam hits the maximum sequence length before generating &lt;/s&gt;, it will still return, with the unnormalized score.
This is a general problem for scoring: when retrieving a score from the Sockeye inference CLI (via --output-type translation_with_score), there is no way to know whether &lt;/s&gt; was generated and therefore whether length normalization was applied (since &lt;/s&gt; is stripped before returning to the user).
Sockeye's scores are therefore underspecified.

This is a problem for evaluating scoring. Scoring takes raw text and will therefore always append , just as is done in training. I am running into this problem because sometimes the outputs haven't actually finished but have just hit the maximum output length and are unnormalized. This could be a problem more generally.
I am not sure of the correct solution, but I propose this:

In inference, --maximum-output-length should refer to the hypothesis excluding &lt;/s&gt;. The reasoning is that this flag is a user-facing feature, and users do not see the &lt;/s&gt; since it is stripped off.
Beam search stops at the max output length. If there are unfinished hypotheses, however, the decoder should take one more step and force each the selection of &lt;/s&gt; for all unfinished hypotheses. It will then apply length normalization, too.
Length normalization should be computed including &lt;/s&gt;, since it is generated by the decoder.

This way, the user can be guaranteed that every hypothesis actually finished, and that all obtained scores are comparable.
	</description>
	<comments>
		<comment id='1' author='mjpost' date='2018-09-26T07:14:12Z'>
		What was the rationale originally to not apply length normalization for unfinished hypotheses that hit maximum length?
		</comment>
		<comment id='2' author='mjpost' date='2018-09-26T08:04:12Z'>
		We used to apply length normalization at every time step for all hypotheses, but afaik this was changed when beam pruning was added.
		</comment>
		<comment id='3' author='mjpost' date='2018-09-26T12:40:44Z'>
		Even without this change to pruning, the user calling sockeye.translate wouldn't know whether &lt;/s&gt; had been generated in a sentence that was returned (and thus, wouldn't know what the value of N was in computing the length calculation), because it might just have been a hypothesis that got truncated when it hit the max length. So I think this could all be fixed if we guaranteed that hypotheses returned have had &lt;/s&gt; generated.
		</comment>
		<comment id='4' author='mjpost' date='2018-09-26T12:44:50Z'>
		not sure I follow this logic. I think we should return what the model predicted, optionally truncated because of maximum length. Of course we should make sure that the score comes out normalized, but I don't think we should manually force/add a &lt;/s&gt;.
		</comment>
		<comment id='5' author='mjpost' date='2018-09-26T12:48:50Z'>
		The issue is that length norm includes end of sentence, but that is stripped off before being returned to the user (as it should be). So the user doesn’t know whether the score of the sentence was normalized by the number of words present (N) or by N+1.

matt (from my phone)
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


 Le 26 sept. 2018 à 08:45, Felix Hieber ***@***.***&gt; a écrit :

 not sure I follow this logic. I think we should return what the model predicted, optionally truncated because of maximum length. Of course we should make sure that the score comes out normalized, but I don't think we should manually force/add a &lt;/s&gt;.

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub, or mute the thread.


		</comment>
		<comment id='6' author='mjpost' date='2018-09-26T13:21:30Z'>
		Is that clear? Another option that would retain normalization consistency is to apply normalization only to user-visible words, i.e., excluding &lt;/s&gt;. However, this seems a little strange since the score includes the cost of generating &lt;/s&gt;.
		</comment>
		<comment id='7' author='mjpost' date='2018-09-26T14:30:27Z'>
		Yes its clear now, thanks!
In that case I think that the approach of forcing truncated hypotheses to end with &lt;/s&gt; might be the way to go. I need to think about this a little bit though :)
		</comment>
		<comment id='8' author='mjpost' date='2018-09-26T16:17:37Z'>
		yeah, that's a good point. So I guess we could always take the probability of &lt;/s&gt; as the last token for truncated hypothesis. This is effectively anyway what we do: we force the model to stop.
		</comment>
		<comment id='9' author='mjpost' date='2019-08-29T12:56:45Z'>
		Sockeye 2 addresses this issue, see &lt;denchmark-link:https://github.com/awslabs/sockeye/pull/719&gt;#719&lt;/denchmark-link&gt;
.
		</comment>
	</comments>
</bug>