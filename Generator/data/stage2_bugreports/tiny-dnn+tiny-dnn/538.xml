<bug id='538' author='fastalgo' open_date='2017-02-01T01:26:31Z' closed_time='2017-03-02T19:06:14Z'>
	<summary>change batch size in mnist example got wrong answer</summary>
	<description>
I changed the batch size of mnist.cpp example from 10 to 64.
&lt;denchmark-link:https://github.com/tiny-dnn/tiny-dnn/blob/master/examples/mnist/train.cpp&gt;https://github.com/tiny-dnn/tiny-dnn/blob/master/examples/mnist/train.cpp&lt;/denchmark-link&gt;

Then I got a wrong answer. Why?
	</description>
	<comments>
		<comment id='1' author='fastalgo' date='2017-02-13T09:45:12Z'>
		&lt;denchmark-link:https://github.com/Randl&gt;@Randl&lt;/denchmark-link&gt;
 Could you point me out where to read on the relation between dropout and batch sizes? Thanks!
		</comment>
		<comment id='2' author='fastalgo' date='2017-02-13T10:32:27Z'>
		&lt;denchmark-link:https://github.com/pliptor&gt;@pliptor&lt;/denchmark-link&gt;
 As for this bug, I suppose the proble is in batch and not in dropout, just with droupout it causes crash and with different settings it causes wrong results.
Generally I believe dropout and batch size are not very related.
		</comment>
		<comment id='3' author='fastalgo' date='2017-02-13T10:40:12Z'>
		Thank you. It seems both issues are very important to be solved and I wanted to learn more about them. Let me know if there's anything I could help with.
		</comment>
		<comment id='4' author='fastalgo' date='2017-02-13T10:42:35Z'>
		We need to understand what causes the bug and to localize it first. If you have any insights, feel free to share them.
		</comment>
		<comment id='5' author='fastalgo' date='2017-02-13T10:44:22Z'>
		I tried to look at the dropout code earlier but no clue so far. Will keep trying.
		</comment>
		<comment id='6' author='fastalgo' date='2017-02-13T15:35:27Z'>
		&lt;denchmark-link:https://github.com/Randl&gt;@Randl&lt;/denchmark-link&gt;
 I think it is safer to have a test on the batch size dimension
		</comment>
		<comment id='7' author='fastalgo' date='2017-02-13T16:55:35Z'>
		The learning rate of the optimizer gets multiplied by sqrt(batch dimension) in the mnist code. For a batch size of 64 that's 8. It might be too fast. Let me see.
		</comment>
		<comment id='8' author='fastalgo' date='2017-02-13T17:49:21Z'>
		&lt;denchmark-link:https://github.com/pliptor&gt;@pliptor&lt;/denchmark-link&gt;
 Thanks so much!
		</comment>
		<comment id='9' author='fastalgo' date='2017-02-13T17:52:24Z'>
		fastalgo  My machine is slow so I can't simulate fast I'm getting good results with batch size of 64 by limiting the multiplying factor with std::min(float_t(4),...). The 4 came from the fact that a batch size of 16 was working fine for me. But this is a hack. There might be better things to do here.
		</comment>
		<comment id='10' author='fastalgo' date='2017-02-13T17:54:23Z'>
		&lt;denchmark-link:https://github.com/pliptor&gt;@pliptor&lt;/denchmark-link&gt;
 I got it, have a nice day :-)
		</comment>
		<comment id='11' author='fastalgo' date='2017-02-13T17:54:43Z'>
		&lt;denchmark-link:https://github.com/pliptor&gt;@pliptor&lt;/denchmark-link&gt;
 Well those issues might be not connected after all.
As for limiting learning rate, I don't see something much better than that.
		</comment>
		<comment id='12' author='fastalgo' date='2017-02-13T17:55:40Z'>
		It's a hack but I can submit a patch for mnist if you want.
		</comment>
		<comment id='13' author='fastalgo' date='2017-02-13T17:57:06Z'>
		If you can open PR, we'll discuss it there.
		</comment>
		<comment id='14' author='fastalgo' date='2017-02-13T17:59:29Z'>
		&lt;denchmark-link:https://github.com/pliptor&gt;@pliptor&lt;/denchmark-link&gt;
 Probably, just add it to &lt;denchmark-link:https://github.com/tiny-dnn/tiny-dnn/pull/574&gt;#574&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='fastalgo' date='2017-02-13T18:03:32Z'>
		Good idea.
		</comment>
		<comment id='16' author='fastalgo' date='2017-03-02T15:04:00Z'>
		&lt;denchmark-link:https://github.com/fastalgo&gt;@fastalgo&lt;/denchmark-link&gt;
 Can you check with latest master?
		</comment>
		<comment id='17' author='fastalgo' date='2017-03-02T18:55:21Z'>
		&lt;denchmark-link:https://github.com/Randl&gt;@Randl&lt;/denchmark-link&gt;
 Yes, it is working, thanks!
		</comment>
	</comments>
</bug>