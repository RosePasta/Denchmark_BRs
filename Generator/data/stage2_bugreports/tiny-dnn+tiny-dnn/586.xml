<bug id='586' author='festlv' open_date='2017-02-18T22:53:13Z' closed_time='2017-02-22T19:48:59Z'>
	<summary>Caffe converter silently fails on (some) non-supported layer types</summary>
	<description>
I have been trying out caffe converter example on a custom caffe model.
The prediction vector of the imported model was all zeroes. During debugging of this issue I found out that tiny-dnn network was correctly constructed, but loading weights had silently stopped after encountering BatchNorm layer- it can't be mapped to tiny-dnn layer type for weight loading.
I'd suggest at least changing this line to throw an error in this case, so that unsupported layer types trigger obvious error:
&lt;denchmark-link:https://github.com/tiny-dnn/tiny-dnn/blob/master/tiny_dnn/io/caffe/layer_factory.h#L146&gt;https://github.com/tiny-dnn/tiny-dnn/blob/master/tiny_dnn/io/caffe/layer_factory.h#L146&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='festlv' date='2017-02-18T23:45:20Z'>
		&lt;denchmark-link:https://github.com/festlv&gt;@festlv&lt;/denchmark-link&gt;
 Thanks for reporting this bug! please submit a PR with the pach
		</comment>
		<comment id='2' author='festlv' date='2017-02-18T23:56:15Z'>
		And also covering missing layer in tiny if you need :)
		</comment>
		<comment id='3' author='festlv' date='2017-02-19T09:15:24Z'>
		Submitted a PR.
I do need this missing layer, but I don't know enough about internals of tiny-dnn, caffe and even how exactly batchnorm works, so I won't be able to add it.
		</comment>
		<comment id='4' author='festlv' date='2017-02-22T19:48:59Z'>
		The core of this issue has been fixed in &lt;denchmark-link:https://github.com/tiny-dnn/tiny-dnn/pull/587&gt;#587&lt;/denchmark-link&gt;
.
The missing BatchNorm support is in &lt;denchmark-link:https://github.com/tiny-dnn/tiny-dnn/pull/595&gt;#595&lt;/denchmark-link&gt;
.
I have the following suggestions regarding caffe converter:

List supported layer types in documentation (https://github.com/tiny-dnn/tiny-dnn/blob/master/tiny_dnn/io/caffe/layer_factory_impl.h#L857).
Make the inference in general more robust (maybe a debug mode with assertions). I have been able to proceed through inference although the weights were not loaded for any layers.

		</comment>
	</comments>
</bug>