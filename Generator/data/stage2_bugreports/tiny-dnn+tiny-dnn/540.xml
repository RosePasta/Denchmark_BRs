<bug id='540' author='OleStauning' open_date='2017-02-01T22:04:05Z' closed_time='2017-03-10T20:42:45Z'>
	<summary>SIGSEGV when adding dropout layer</summary>
	<description>
Code:
&lt;denchmark-code&gt;    network&lt;sequential&gt; net;
    adam opt;

    net &lt;&lt; fc&lt;relu&gt;(9*2,40)
        &lt;&lt; fc&lt;relu&gt;(40,10)
        &lt;&lt; dropout(10,0.5)
        &lt;&lt; fc&lt;sigmoid&gt;(10,2);

    int epoch=0;
    int epochs = 50;
    int batch = 50;
    net.fit&lt;mse&gt;(opt, xtrain, ytrain, batch, epochs,
    [](){},
    [&amp;](){
        auto loss = net.get_loss&lt;mse&gt;(xtest, ytest);
        std::cout &lt;&lt; "Loss=" &lt;&lt; loss &lt;&lt; std::endl;
    });
&lt;/denchmark-code&gt;

Error when using valgrind:
&lt;denchmark-code&gt;==7176== Invalid read of size 1
==7176==    at 0x115C35: tiny_dnn::dropout_layer::back_propagation(std::vector&lt;std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt;*, std::allocator&lt;std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt;*&gt; &gt; const&amp;, std::vector&lt;std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt;*, std::allocator&lt;std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt;*&gt; &gt; const&amp;, std::vector&lt;std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt;*, std::allocator&lt;std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt;*&gt; &gt;&amp;, std::vector&lt;std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt;*, std::allocator&lt;std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt;*&gt; &gt;&amp;) (dropout_layer.h:86)
==7176==    by 0x112683: tiny_dnn::layer::backward() (layer.h:515)
==7176==    by 0x11486E: tiny_dnn::sequential::backward(std::vector&lt;std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt;, std::allocator&lt;std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt; &gt; &gt; const&amp;) (nodes.h:285)
==7176==    by 0x13032C: void tiny_dnn::network&lt;tiny_dnn::sequential&gt;::bprop&lt;tiny_dnn::mse&gt;(std::vector&lt;std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt;, std::allocator&lt;std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt; &gt; &gt; const&amp;, std::vector&lt;std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt;, std::allocator&lt;std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt; &gt; &gt; const&amp;, std::vector&lt;std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt;, std::allocator&lt;std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt; &gt; &gt; const&amp;) (network.h:946)
==7176==    by 0x12D112: void tiny_dnn::network&lt;tiny_dnn::sequential&gt;::train_onebatch&lt;tiny_dnn::mse, tiny_dnn::adam&gt;(tiny_dnn::adam&amp;, std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt; const*, std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt; const*, int, int, std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt; const*) (network.h:845)
==7176==    by 0x128B7C: void tiny_dnn::network&lt;tiny_dnn::sequential&gt;::train_once&lt;tiny_dnn::mse, tiny_dnn::adam&gt;(tiny_dnn::adam&amp;, std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt; const*, std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt; const*, int, int, std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt; const*) (network.h:818)
==7176==    by 0x10E407: bool tiny_dnn::network&lt;tiny_dnn::sequential&gt;::fit&lt;tiny_dnn::mse, tiny_dnn::adam, main::{lambda()#1}, main::{lambda()#2}&gt;(tiny_dnn::adam&amp;, std::vector&lt;tiny_dnn::adam&lt;tiny_dnn::adam&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;tiny_dnn::aligned_allocator&gt; &gt;, std::allocator&lt;tiny_dnn::adam&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt; const&amp;, std::vector&lt;tiny_dnn::adam&lt;tiny_dnn::adam&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;tiny_dnn::aligned_allocator&gt; &gt;, std::allocator&lt;tiny_dnn::adam&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt;, unsigned long, int, main::{lambda()#1}, main::{lambda()#2}, bool, int, std::vector&lt;tiny_dnn::adam&lt;tiny_dnn::adam&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;tiny_dnn::aligned_allocator&gt; &gt;, std::allocator&lt;tiny_dnn::adam&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt;) (network.h:784)
==7176==    by 0x10E11B: bool tiny_dnn::network&lt;tiny_dnn::sequential&gt;::fit&lt;tiny_dnn::mse, tiny_dnn::adam, main::{lambda()#1}, main::{lambda()#2}, std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, tiny_dnn::aligned_allocator&gt;(tiny_dnn::adam&amp;, {lambda()#2}&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;tiny_dnn::adam&gt; &gt; const&amp;, {lambda()#2}&lt;tiny_dnn::aligned_allocator, std::allocator&lt;{lambda()#2}&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;tiny_dnn::adam&gt; &gt; const&gt; &gt; const&amp;, unsigned long, int, main::{lambda()#1}, main::{lambda()#2}, bool, int, {lambda()#2}&lt;tiny_dnn::aligned_allocator, std::allocator&lt;{lambda()#2}&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;tiny_dnn::adam&gt; &gt; const&gt; &gt;) (network.h:335)
.....................
==4475== Process terminating with default action of signal 11 (SIGSEGV)
==4475==  Access not within mapped region at address 0x10
==4475==    at 0x12113A: std::_Sp_counted_base&lt;(__gnu_cxx::_Lock_policy)2&gt;::_M_release() (shared_ptr_base.h:150)
==4475==    by 0x11AFCE: std::__shared_count&lt;(__gnu_cxx::_Lock_policy)2&gt;::~__shared_count() (shared_ptr_base.h:662)
==4475==    by 0x114D4F: std::__shared_ptr&lt;tiny_dnn::edge, (__gnu_cxx::_Lock_policy)2&gt;::~__shared_ptr() (shared_ptr_base.h:928)
==4475==    by 0x114D6B: std::shared_ptr&lt;tiny_dnn::edge&gt;::~shared_ptr() (shared_ptr.h:93)
==4475==    by 0x115E54: tiny_dnn::layer::backward() (layer.h:511)
==4475==    by 0x118078: tiny_dnn::sequential::backward(std::vector&lt;std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt;, std::allocator&lt;std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt; &gt; &gt; const&amp;) (nodes.h:285)
==4475==    by 0x136CC6: void tiny_dnn::network&lt;tiny_dnn::sequential&gt;::bprop&lt;tiny_dnn::mse&gt;(std::vector&lt;std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt;, std::allocator&lt;std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt; &gt; &gt; const&amp;, std::vector&lt;std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt;, std::allocator&lt;std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt; &gt; &gt; const&amp;, std::vector&lt;std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt;, std::allocator&lt;std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt; &gt; &gt; const&amp;) (network.h:946)
==4475==    by 0x13309E: void tiny_dnn::network&lt;tiny_dnn::sequential&gt;::train_onebatch&lt;tiny_dnn::mse, tiny_dnn::adam&gt;(tiny_dnn::adam&amp;, std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt; const*, std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt; const*, int, int, std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt; const*) (network.h:845)
==4475==    by 0x12E01C: void tiny_dnn::network&lt;tiny_dnn::sequential&gt;::train_once&lt;tiny_dnn::mse, tiny_dnn::adam&gt;(tiny_dnn::adam&amp;, std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt; const*, std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt; const*, int, int, std::vector&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt; const*) (network.h:818)
==4475==    by 0x110E71: bool tiny_dnn::network&lt;tiny_dnn::sequential&gt;::fit&lt;tiny_dnn::mse, tiny_dnn::adam, main::{lambda()#1}, main::{lambda()#2}&gt;(tiny_dnn::adam&amp;, std::vector&lt;tiny_dnn::adam&lt;tiny_dnn::adam&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;tiny_dnn::aligned_allocator&gt; &gt;, std::allocator&lt;tiny_dnn::adam&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt; const&amp;, std::vector&lt;tiny_dnn::adam&lt;tiny_dnn::adam&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;tiny_dnn::aligned_allocator&gt; &gt;, std::allocator&lt;tiny_dnn::adam&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt;, unsigned long, int, main::{lambda()#1}, main::{lambda()#2}, bool, int, std::vector&lt;tiny_dnn::adam&lt;tiny_dnn::adam&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;tiny_dnn::aligned_allocator&gt; &gt;, std::allocator&lt;tiny_dnn::adam&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt; &gt; &gt;) (network.h:784)
==4475==    by 0x110B85: bool tiny_dnn::network&lt;tiny_dnn::sequential&gt;::fit&lt;tiny_dnn::mse, tiny_dnn::adam, main::{lambda()#1}, main::{lambda()#2}, std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, tiny_dnn::aligned_allocator&gt;(tiny_dnn::adam&amp;, {lambda()#2}&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;tiny_dnn::adam&gt; &gt; const&amp;, {lambda()#2}&lt;tiny_dnn::aligned_allocator, std::allocator&lt;{lambda()#2}&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;tiny_dnn::adam&gt; &gt; const&gt; &gt; const&amp;, unsigned long, int, main::{lambda()#1}, main::{lambda()#2}, bool, int, {lambda()#2}&lt;tiny_dnn::aligned_allocator, std::allocator&lt;{lambda()#2}&lt;std::vector&lt;float, tiny_dnn::aligned_allocator&lt;float, 64ul&gt; &gt;, std::allocator&lt;tiny_dnn::adam&gt; &gt; const&gt; &gt;) (network.h:335)
&lt;/denchmark-code&gt;

The code works fine without the dropout layer. Am I doing something wrong?
Best regards
	</description>
	<comments>
		<comment id='1' author='OleStauning' date='2017-02-01T23:13:36Z'>
		I saw a crash with dropout, but with a different model of course. Also works fine if I comment out dropout. This is on windows
		</comment>
		<comment id='2' author='OleStauning' date='2017-02-24T18:26:07Z'>
		My guess is out_type of dropout_layer is {vector_type::data} and in_type of fully_connected_layer is {vector_type::data, vector_type::weight, vector_type::bias} or {vector_type::data, vector_type::weight}, so they can't be connected.
But if it's really so, network class should report an error about inappropriate layer configuration.
		</comment>
		<comment id='3' author='OleStauning' date='2017-02-24T18:59:03Z'>
		It was my misunderstanding.
		</comment>
		<comment id='4' author='OleStauning' date='2017-02-24T19:31:03Z'>
		In dropout_layer::back_propagation method, in_grad is updated.
for (serial_size_t sample = 0;
      sample &lt; static_cast&lt;serial_size_t&gt;(prev_delta.size()); ++sample) {
  for (serial_size_t i = 0;
        i &lt; static_cast&lt;serial_size_t&gt;(curr_delta.size()); i++) {
    prev_delta[sample][i] = mask_[sample][i] * curr_delta[sample][i];
  }
}
During the process, mask_ member is accesed. A problem is found by adding below assert statement.
  assert(mask_[sample].size() == curr_delta.size());
		</comment>
		<comment id='5' author='OleStauning' date='2017-02-25T01:57:43Z'>
		Posted a fix to the problem.
&lt;denchmark-link:https://github.com/tiny-dnn/tiny-dnn/pull/543#issuecomment-282450220&gt;#543 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='OleStauning' date='2017-03-02T15:03:18Z'>
		&lt;denchmark-link:https://github.com/OleStauning&gt;@OleStauning&lt;/denchmark-link&gt;
 can you please check with current master?
		</comment>
		<comment id='7' author='OleStauning' date='2017-03-10T20:42:45Z'>
		Yes - this have fixed the problem. Thanks.
		</comment>
	</comments>
</bug>