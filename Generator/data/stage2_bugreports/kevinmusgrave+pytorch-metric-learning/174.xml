<bug id='174' author='bobiblazeski' open_date='2020-08-09T21:03:33Z' closed_time='2020-08-31T13:08:03Z'>
	<summary>operation does not have an identity</summary>
	<description>
I'm trying to create embeddings on top of resnet50.
After running metric learning for several hours I'm getting an exception,
operation does not have an identity

GPU available: True, used: True
TPU available: False, using: 0 TPU cores
CUDA_VISIBLE_DEVICES: [0]
| Name        | Type                 | Params
0 | E           | Sequential           | 26 M
1 | miner       | MultiSimilarityMiner | 0
2 | emb_loss_fn | ArcFaceLoss          | 51 K
All dataset keys matched successfully.
Epoch 442: 60%
44/73 [01:20&lt;00:52, 1.82s/it, loss=0.102, v_num=0]
RuntimeError                              Traceback (most recent call last)
 in 
5     #auto_lr_find=True,
6     default_root_dir='./checkpoint')
----&gt; 7 trainer.fit(sculptor)
~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders)
1001
1002         elif self.single_gpu:
-&gt; 1003             results = self.single_gpu_train(model)
1004
1005         elif self.use_tpu:  # pragma: no-cover
~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/distrib_parts.py in single_gpu_train(self, model)
184             self.reinit_scheduler_properties(self.optimizers, self.lr_schedulers)
185
--&gt; 186         results = self.run_pretrain_routine(model)
187         return results
188
~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py in run_pretrain_routine(self, model)
1211
1212         # CORE TRAINING LOOP
-&gt; 1213         self.train()
1214
1215     def test(
~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py in train(self)
368                 # RUN TNG EPOCH
369                 # -----------------
--&gt; 370                 self.run_training_epoch()
371
372                 if self.max_steps and self.max_steps &lt;= self.global_step:
~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_epoch(self)
450             # TRAINING_STEP + TRAINING_STEP_END
451             # ------------------------------------
--&gt; 452             batch_output = self.run_training_batch(batch, batch_idx)
453
454             # only track outputs when user implements training_epoch_end
~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_batch(self, batch, batch_idx)
625                 # calculate loss
626                 # -------------------
--&gt; 627                 opt_closure_result = self.optimizer_closure(
628                     split_batch,
629                     batch_idx,
~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py in optimizer_closure(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)
773                                                                  opt_idx, hiddens)
774             else:
--&gt; 775                 training_step_output = self.training_forward(split_batch, batch_idx, opt_idx,
776                                                              hiddens)
777
~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py in training_forward(self, batch, batch_idx, opt_idx, hiddens)
944             batch = self.transfer_batch_to_gpu(batch, gpu_id)
945             args[0] = batch
--&gt; 946             output = self.model.training_step(*args)
947
948         # TPU support
 in training_step(self, batch, batch_nb)
42         embeddings = self.E(img)
43         hard_pairs = self.miner(embeddings, lbl)
---&gt; 44         emb_loss = self.emb_loss_fn(embeddings, lbl, hard_pairs)
45         #output = self.G(embeddings)
46         #vrt_loss = self.vrt_loss_fn(output, vrt)
~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
720             result = self._slow_forward(*input, **kwargs)
721         else:
--&gt; 722             result = self.forward(*input, **kwargs)
723         for hook in itertools.chain(
724                 _global_forward_hooks.values(),
~/.local/lib/python3.8/site-packages/pytorch_metric_learning/losses/base_metric_loss_function.py in forward(self, embeddings, labels, indices_tuple)
27         c_f.assert_embeddings_and_labels_are_same_size(embeddings, labels)
28         labels = labels.to(embeddings.device)
---&gt; 29         loss_dict = self.compute_loss(embeddings, labels, indices_tuple)
30         self.add_embedding_regularization_to_loss_dict(loss_dict, embeddings)
31         return self.reducer(loss_dict, embeddings, labels)
~/.local/lib/python3.8/site-packages/pytorch_metric_learning/losses/large_margin_softmax_loss.py in compute_loss(self, embeddings, labels, indices_tuple)
83         dtype, device = embeddings.dtype, embeddings.device
84         self.cast_types(dtype, device)
---&gt; 85         miner_weights = lmu.convert_to_weights(indices_tuple, labels, dtype=dtype)
86         mask = self.get_target_mask(embeddings, labels)
87         cosine = self.get_cosine(embeddings)
~/.local/lib/python3.8/site-packages/pytorch_metric_learning/utils/loss_and_miner_utils.py in convert_to_weights(indices_tuple, labels, dtype)
192     indices, counts = torch.unique(torch.cat(indices_tuple, dim=0), return_counts=True)
193     counts = (counts.type(dtype) / torch.sum(counts)) # I have 200 classes
--&gt; 194     weights[indices] = counts / torch.max(counts)
195     return weights
RuntimeError: operation does not have an identity.

Version '0.9.90' with torch  '1.6.0'
My code is as fallows
&lt;denchmark-code&gt;def create_encoder(z_size):
    extractor = models.resnet50(pretrained=True)
    in_features = extractor.fc.in_features
    extractor = torch.nn.Sequential(*list(extractor.children())[:-1])
    for p in extractor.parameters():
        p.requires_grad=False
    mid_features = (in_features  + z_size) // 2
    embedder = torch.nn.Sequential(
        torch.nn.Flatten(),
        torch.nn.Linear(in_features, mid_features),
        torch.nn.LeakyReLU(0.2),
        torch.nn.Linear(mid_features, z_size),
    )
    return torch.nn.Sequential(OrderedDict({
        'extractor': extractor,
        'classifier': embedder,
    }))


class Sculptor(pl.LightningModule):

    def __init__(self, hparams):        
        super(Sculptor, self).__init__()
        self.hparams = AttributeDict(hparams)
        
        self.E = create_encoder(z_size)# Encoder(, use_spectral_norm)        
        self.miner = miners.MultiSimilarityMiner(epsilon=0.1)        
        self.emb_loss_fn = losses.ArcFaceLoss(num_classes=len(ds.trace_keys), embedding_size=z_size)                    
        
    def forward(self, x):
        encodings = self.E(x)
        return self.G(encodings)

    def training_step(self, batch, batch_nb):        
        img, lbl = batch["image"], batch["label"]
        embeddings = self.E(img)
        hard_pairs = self.miner(embeddings, lbl)
        emb_loss = self.emb_loss_fn(embeddings, lbl, hard_pairs)        
        loss =  emb_loss
        return {
            'loss': loss, 
            'log': {
                'train/loss': loss.detach(),
            },
        }    

    def configure_optimizers(self):  
        params = list(self.E.classifier.parameters())        
        return Lookahead(torch.optim.Adam(params, lr=0.0001))                               
    
    
    def train_dataloader(self):
        ds = ImgVertexDataset(img_dirs, trace_file, transform)
        return DataLoader(ds, batch_size=bs, shuffle=True, num_workers=4)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='bobiblazeski' date='2020-08-10T03:09:39Z'>
		Should be fixed in v0.9.91.dev1:
&lt;denchmark-code&gt;pip install pytorch-metric-learning==0.9.91.dev1
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='bobiblazeski' date='2020-08-10T18:35:53Z'>
		Seems that it works, I can't reproduce it anymore on 0.9.91.dev1
		</comment>
	</comments>
</bug>