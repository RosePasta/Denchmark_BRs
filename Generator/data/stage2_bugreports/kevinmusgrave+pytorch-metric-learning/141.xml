<bug id='141' author='KevinMusgrave' open_date='2020-07-16T04:31:27Z' closed_time='2020-07-25T14:17:16Z'>
	<summary>Get rid of self-comparisons in CrossBatchMemory</summary>
	<description>
After the current batch is added to the queue, the tuples are formed using the current batch for anchors and the queue for positives/negatives. But its possible for the anchors and positives to point to the same datapoints, even though they don't appear to be, due to the queue. So there are positive pairs that are just self-comparisons (as noticed here: &lt;denchmark-link:https://github.com/KevinMusgrave/pytorch-metric-learning/issues/138#issuecomment-658469002&gt;#138 (comment)&lt;/denchmark-link&gt;
)
	</description>
	<comments>
		<comment id='1' author='KevinMusgrave' date='2020-07-16T04:31:41Z'>
		Fixed in v0.9.89.dev2
		</comment>
	</comments>
</bug>