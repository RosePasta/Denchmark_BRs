<bug id='24' author='JohnGiorgi' open_date='2020-03-17T02:30:37Z' closed_time='2020-03-17T10:15:00Z'>
	<summary>Loss goes up with CrossBatchMemory?</summary>
	<description>
I am having a strange issue where my loss actually looks like its "flipped" when I wrap a loss function with &lt;denchmark-link:https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#crossbatchmemory&gt;CrossBatchMemory&lt;/denchmark-link&gt;
. My setup is a little contrived but here is the gist of what I call:
from pytorch_metric_learning.losses import NTXentLoss, CrossBatchMemory

loss = NTXentLoss(0.1)
loss = CrossBatchMemory(loss, 128, memory_size=256)
When I don't wrap with CrossBatchMemory loss, my loss goes down and my model trains without issue. When I do wrap it, my loss actually goes up:
&lt;denchmark-link:https://user-images.githubusercontent.com/8917831/76816054-9c86db00-67d5-11ea-99f8-2410891f1c8f.png&gt;&lt;/denchmark-link&gt;

Any ideas? Am I not using CrossBatchMemory as intended? Thanks!
	</description>
	<comments>
		<comment id='1' author='JohnGiorgi' date='2020-03-17T05:37:48Z'>
		&lt;denchmark-link:https://github.com/JohnGiorgi&gt;@JohnGiorgi&lt;/denchmark-link&gt;
 I think I found the bug: The paper says to add the current batch to the memory queue before computing the loss. I'm not doing that. Instead I'm adding it to the memory queue  the loss is computed. So at each iteration, the loss is calculated without considering any of the relationships within the current batch. It only considers relationships between the current batch and the memory queue. The reason I didn't notice this bug before was because I was using a large memory_size, like in the thousands, and it seemed to be working. But I tried 256 like you and it totally fails.
I also found a .to(device) bug in CrossBatchMemory, but that's unrelated to the loss bug you're seeing.
Here are the changes in &lt;denchmark-link:https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/pytorch_metric_learning/losses/cross_batch_memory.py#L18&gt;CrossBatchMemory&lt;/denchmark-link&gt;
 that worked for me:
    labels = labels.to(embeddings.device)                 ### this addresses the .to(device) bug
    self.embedding_memory = self.embedding_memory.to(embeddings.device)
    self.label_memory = self.label_memory.to(labels.device)
    self.add_to_memory(embeddings, labels, batch_size)      ### this addresses the loss bug
    ...
    loss = self.loss(combined_embeddings, combined_labels, indices_tuple)
    self.add_to_memory(embeddings, labels, batch_size)      ### delete this
    return loss
Without this change, I get the same increasing loss bug as you, as well as collapsing accuracy.
With this change:

Loss:
&lt;denchmark-link:https://user-images.githubusercontent.com/2314891/76825993-bc2bfc80-67f1-11ea-8db2-9678f4f4569d.png&gt;&lt;/denchmark-link&gt;

Accuracy:
&lt;denchmark-link:https://user-images.githubusercontent.com/2314891/76826047-db2a8e80-67f1-11ea-8895-8c159127a65d.png&gt;&lt;/denchmark-link&gt;


Loss:
&lt;denchmark-link:https://user-images.githubusercontent.com/2314891/76826066-e7aee700-67f1-11ea-8860-07859fc4f938.png&gt;&lt;/denchmark-link&gt;

Accuracy:
&lt;denchmark-link:https://user-images.githubusercontent.com/2314891/76826073-eb426e00-67f1-11ea-8e19-339b4465d70f.png&gt;&lt;/denchmark-link&gt;

Larger memory size still seems to help.
I'll try to commit and upload soon
		</comment>
		<comment id='2' author='JohnGiorgi' date='2020-03-17T10:15:00Z'>
		&lt;denchmark-link:https://github.com/JohnGiorgi&gt;@JohnGiorgi&lt;/denchmark-link&gt;
 This should be fixed in the latest version 0.9.77.
		</comment>
		<comment id='3' author='JohnGiorgi' date='2020-03-17T19:13:06Z'>
		&lt;denchmark-link:https://github.com/KevinMusgrave&gt;@KevinMusgrave&lt;/denchmark-link&gt;
 Thank you for such a quick response! My loss now heads south as expected.
		</comment>
	</comments>
</bug>