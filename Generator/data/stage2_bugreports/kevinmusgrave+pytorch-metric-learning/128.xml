<bug id='128' author='thinline72' open_date='2020-06-22T18:48:10Z' closed_time='2020-07-25T14:18:35Z'>
	<summary>DistanceWeightedMiner doesn't work with CrossBatchMemory</summary>
	<description>
If I try to use DistanceWeightedMiner with CrossBatchMemory in the following way:
&lt;denchmark-code&gt;miner = miners.DistanceWeightedMiner(cutoff=0.5, nonzero_loss_cutoff=1.4)
metric_loss = losses.MarginLoss(margin=0.2, nu=1.0, beta=1.2, learn_beta=True)

...

# Activate Cross-Batch memory
if epoch_inx + 1 == num_warm_up_epochs:
    metric_loss = losses.CrossBatchMemory(
        loss=metric_loss,
        embedding_size=encoder.encoder_out_dim,
        memory_size=memory_size,
        miner=miner,
    )
    miner = None
&lt;/denchmark-code&gt;

it fails with the following error once CrossBatchMemory is activated:
&lt;denchmark-code&gt;File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_metric_learning/losses/cross_batch_memory.py", line 30, in forward
    indices_tuple = self.create_indices_tuple(batch_size, embeddings, labels, E_mem, L_mem, input_indices_tuple)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_metric_learning/losses/cross_batch_memory.py", line 59, in create_indices_tuple
    indices_tuple = self.miner(embeddings, labels, E_mem, L_mem)
  File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_metric_learning/miners/base_miner.py", line 39, in forward
    mining_output = self.mine(embeddings, labels, ref_emb, ref_labels)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_metric_learning/miners/distance_weighted_miner.py", line 41, in mine
    return lmu.get_random_triplet_indices(labels, weights=np_weights)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_metric_learning/utils/loss_and_miner_utils.py", line 150, in get_random_triplet_indices
    n_idx += c_f.NUMPY_RANDOM.choice(batch_size, k, p=weights[i]).tolist()
  File "mtrand.pyx", line 924, in numpy.random.mtrand.RandomState.choice
ValueError: 'a' and 'p' must have same size
&lt;/denchmark-code&gt;

I use pytorch==1.5, numpy==1.19.0 and pytorch-metric-learning==0.9.88
	</description>
	<comments>
		<comment id='1' author='thinline72' date='2020-07-24T07:03:14Z'>
		I also encountered the same problemã€‚
I think in line 41

should be modified to

&lt;denchmark-link:https://github.com/KevinMusgrave&gt;@KevinMusgrave&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='thinline72' date='2020-07-24T14:15:42Z'>
		&lt;denchmark-link:https://github.com/DengWen0425&gt;@DengWen0425&lt;/denchmark-link&gt;
 Thanks!! I made the change and added a unit test to cover this particular combination of DistanceWeightedMiner and CrossBatchMemory, so it should work now. It's available in v0.9.89.dev3:
&lt;denchmark-code&gt;pip install pytorch-metric-learning==0.9.89.dev3
&lt;/denchmark-code&gt;

FYI, the  is fairly inefficient and might be slow on large batches. I've made a separate issue (&lt;denchmark-link:https://github.com/KevinMusgrave/pytorch-metric-learning/issues/157#issue-665197751&gt;#157 (comment)&lt;/denchmark-link&gt;
) to address this.
		</comment>
	</comments>
</bug>