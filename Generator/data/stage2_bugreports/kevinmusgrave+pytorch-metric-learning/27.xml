<bug id='27' author='JohnGiorgi' open_date='2020-03-20T04:17:11Z' closed_time='2020-03-23T21:38:53Z'>
	<summary>How to use a miner with CrossBatchMemory?</summary>
	<description>
Hi,
I was interested in combining the MaximumLossMiner with the CrossBatchMemory, but cant seem to get it to work, here is what I try:
import torch
from pytorch_metric_learning.losses import CrossBatchMemory, NTXentLoss
from pytorch_metric_learning.miners import MaximumLossMiner

batch_size = 16
embedding_dim = 128

# Generate a dummy batch
anchor_embeddings = torch.randn(batch_size, embedding_dim)
positive_embeddings = torch.randn(batch_size, embedding_dim)
embeddings = torch.cat((anchor_embeddings, positive_embeddings),)
indices = torch.arange(0, anchor_embeddings.size(0))
labels = torch.cat((indices, indices))

# Create loss and miner
_loss = NTXentLoss(0.1)
_miner = MaximumLossMiner(_loss, output_batch_size=16)
loss = CrossBatchMemory(_loss, 128, 1024, _miner)
but trying to propagate through the loss
&lt;denchmark-code&gt;loss(embeddings, labels)
&lt;/denchmark-code&gt;

triggers the following error
&lt;denchmark-code&gt;ValueError                                Traceback (most recent call last)
&lt;ipython-input-13-6d656e3bd79f&gt; in &lt;module&gt;
----&gt; 1 loss(embeddings, labels)

~/miniconda3/envs/t2t/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    530             result = self._slow_forward(*input, **kwargs)
    531         else:
--&gt; 532             result = self.forward(*input, **kwargs)
    533         for hook in self._forward_hooks.values():
    534             hook_result = hook(self, input, result)

~/miniconda3/envs/t2t/lib/python3.7/site-packages/pytorch_metric_learning/losses/cross_batch_memory.py in forward(self, embeddings, labels, input_indices_tuple)
     29         combined_embeddings = torch.cat([embeddings, E_mem], dim=0)
     30         combined_labels = torch.cat([labels, L_mem], dim=0)
---&gt; 31         loss = self.loss(combined_embeddings, combined_labels, indices_tuple)
     32         self.add_to_memory(embeddings, labels, batch_size)
     33         return loss

~/miniconda3/envs/t2t/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    530             result = self._slow_forward(*input, **kwargs)
    531         else:
--&gt; 532             result = self.forward(*input, **kwargs)
    533         for hook in self._forward_hooks.values():
    534             hook_result = hook(self, input, result)

~/miniconda3/envs/t2t/lib/python3.7/site-packages/pytorch_metric_learning/losses/base_metric_loss_function.py in forward(self, embeddings, labels, indices_tuple)
     51         self.avg_embedding_norm = torch.mean(self.embedding_norms)
     52
---&gt; 53         loss = self.compute_loss(embeddings, labels, indices_tuple)
     54         if loss == 0:
     55             loss = torch.sum(embeddings*0)

~/miniconda3/envs/t2t/lib/python3.7/site-packages/pytorch_metric_learning/losses/ntxent_loss.py in compute_loss(self, embeddings, labels, indices_tuple)
     16         cosine_similarity = cosine_similarity / self.temperature
     17
---&gt; 18         a1, p, a2, n = lmu.convert_to_pairs(indices_tuple, labels)
     19
     20         if len(a1) &gt; 0 and len(a2) &gt; 0:

~/miniconda3/envs/t2t/lib/python3.7/site-packages/pytorch_metric_learning/utils/loss_and_miner_utils.py in convert_to_pairs(indices_tuple, labels)
     98         return indices_tuple
     99     else:
--&gt; 100         a, p, n = indices_tuple
    101         return a, p, a, n
    102

ValueError: too many values to unpack (expected 3)
&lt;/denchmark-code&gt;

Any ideas? I figure I am just doing something silly / misusing one of these classes. Thanks in advance!
	</description>
	<comments>
		<comment id='1' author='JohnGiorgi' date='2020-03-21T15:28:59Z'>
		The miner for CrossBatchMemory has to be a TupleMiner, whereas MaximumLossMiner is a SubsetBatchMiner. The difference is that TupleMiners return indices for pairs/triplets, and MaximumLossMiner returns the indices for a subset of an input batch. I'll try to improve the &lt;denchmark-link:https://kevinmusgrave.github.io/pytorch-metric-learning/miners/&gt;documentation&lt;/denchmark-link&gt;
 to make this clear.
As for actually using MaximumLossMiner, there's a few things to note:

I created it basically as an example for SubsetBatchMiner. (Currently it's the only available SubsetBatchMiner.) It's a pretty naive and slow method: it just computes the loss on X randomly selected subsets of the input batch, and returns the subset that has the highest loss. This is slow because it has to compute the loss X times. The idea was that you might want to apply this to a really large batch: first compute embeddings using with torch.no_grad() and then recompute the subset embeddings with gradient information, and finally the loss for the subset.
I looked it at now and noticed that the outputted random subset can include duplicates, because I'm using torch.randint which doesn't have a "replacement" flag. This is something I'll fix in the next release.

If you want to try it out anyway, this should work:
_loss = NTXentLoss(0.1)
_miner = MaximumLossMiner(_loss, output_batch_size=16)
loss = CrossBatchMemory(_loss, 128, 1024)

subset_idx = _miner(embeddings, labels)
subset_embeddings, subset_labels = embeddings[subset_idx], labels[subset_idx]
loss_value = loss(subset_embeddings, subset_labels)
		</comment>
		<comment id='2' author='JohnGiorgi' date='2020-03-21T19:40:36Z'>
		Hmm, I see. Thanks for the guidance!
I don't actually need MaximumLossMiner, I chose it because it seemed the simplest. Sorry for sending you on a wild goose chase.
Given the points you raised, my question is actually: how do I properly use a miner (TupleMiner) with CrossBatchMemory? For example, below I try several TupleMiners, but with each, I run into a problem.
import torch
from pytorch_metric_learning import losses
from pytorch_metric_learning import miners

batch_size = 16
embedding_size = 128

# Generate a dummy batch
anchor_embeddings = torch.randn(batch_size, embedding_size)
positive_embeddings = torch.randn(batch_size, embedding_size)
embeddings = torch.cat((anchor_embeddings, positive_embeddings),)
indices = torch.arange(0, anchor_embeddings.size(0))
labels = torch.cat((indices, indices))

# Three TupleMiners chosen at random
batch_hard_miner = miners.BatchHardMiner(use_similarity=True)
hdc_miner = miners.HDCMiner(filter_percentage=0.9, use_similarity=True)
pair_margin_miner = miners.PairMarginMiner(0.1, 0.1, use_similarity=True)

loss_func = losses.NTXentLoss(0.1)
Triggers a RuntimeError
# Triggers RuntimeError!
cross_batch_loss = losses.CrossBatchMemory(
    loss=loss_func, embedding_size=embedding_size, miner=batch_hard_miner
)
loss = cross_batch_loss(embeddings, labels)
Results in a zero loss?
# Zero loss!
cross_batch_loss = losses.CrossBatchMemory(
    loss=loss_func, embedding_size=embedding_size, miner=hdc_miner
)
assert cross_batch_loss(embeddings, labels).item() == 0
cross_batch_loss = losses.CrossBatchMemory(
    loss=loss_func, embedding_size=embedding_size, miner=pair_margin_miner
)
assert cross_batch_loss(embeddings, labels).item() == 0
However, if I drop the CrossBatchMemory wrapper and use a loss function directly, there are no issues.
# Run (seemingly) without error
miner_output = batch_hard_miner(embeddings, labels)
assert loss_func(embeddings, labels, miner_output).item() != 0
miner_output = hdc_miner(embeddings, labels)
assert loss_func(embeddings, labels, miner_output).item() != 0
miner_output = pair_margin_miner(embeddings, labels)
assert loss_func(embeddings, labels, miner_output).item() != 0
Appreciate any guidance you can give on how to pair CrossBatchMemory with a miner!
		</comment>
		<comment id='3' author='JohnGiorgi' date='2020-03-22T08:32:55Z'>
		&lt;denchmark-link:https://github.com/JohnGiorgi&gt;@JohnGiorgi&lt;/denchmark-link&gt;
 I found the bug that causes BatchHardMiner to fail in CrossBatchMemory. But for HDCMiner and PairMarginMiner, I don't get 0 loss when I run your example.
However, it is possible for PairMarginMiner to result in a loss of 0 if it finds no pairs, because NTXentLoss requires there to be at least 1 positive and 1 negative pair. In your case, a positive margin of 0.1 might be too low, since it will only return positive pairs that have a similarity less than 0.1.
		</comment>
		<comment id='4' author='JohnGiorgi' date='2020-03-23T14:15:40Z'>
		&lt;denchmark-link:https://github.com/JohnGiorgi&gt;@JohnGiorgi&lt;/denchmark-link&gt;
 The bug affecting batch hard miner should be fixed in the latest version 0.9.79, and the other two miners should still work.
		</comment>
		<comment id='5' author='JohnGiorgi' date='2020-03-23T21:38:52Z'>
		&lt;denchmark-link:https://github.com/KevinMusgrave&gt;@KevinMusgrave&lt;/denchmark-link&gt;
 Awesome, thanks again. Really appreciate the quick responses.
You are right about my example having too agressive margins for HDCMiner/PairMarginMiner  -- I get non-zero loss when I adjust them.
		</comment>
	</comments>
</bug>