<bug id='17' author='StefanMojsilovic' open_date='2020-02-27T09:34:00Z' closed_time='2020-03-01T00:48:56Z'>
	<summary>lr_scheduler object has no attribute 'parameters'</summary>
	<description>
Using one of the torch.optim.lr_schedulers object with the example codes produces the following error:
File "/opt/conda/lib/python3.6/site-packages/pytorch_metric_learning/utils/common_functions.py", line 235, in operate_on_dict_of_models
if opt_cond or len([i for i in v.parameters()]) &gt; 0:
AttributeError: 'StepLR' object has no attribute 'parameters'
The error can be quick-fixed by subclassing the desired lr_scheduler and wrapping the state_dict() function by the parameters() function.
However, this will later conflict with the base_trainer when using ReduceLROnPlateau since it will no longer be a part of the torch.optim module.
This can be further fixed by replacing the base_trainer with your own, etc. However, this is not the desired behavior.
	</description>
	<comments>
		<comment id='1' author='StefanMojsilovic' date='2020-02-27T13:51:57Z'>
		I'll get rid of that if-statement, and instead wrap the "save_model" call with a try block. Something like this:
def operate_on_dict_of_models(input_dict, suffix, folder, operation, logging_string):
    for k, v in input_dict.items():
        model_path = modelpath_creator(folder, k, suffix)
        try:
            operation(k, v, model_path)
            logging.info("%s %s"%(logging_string, model_path))
        except:
            logging.warn("Could not %s %s"%(logging_string, model_path))
		</comment>
		<comment id='2' author='StefanMojsilovic' date='2020-03-01T00:48:56Z'>
		&lt;denchmark-link:https://github.com/StefanMojsilovic&gt;@StefanMojsilovic&lt;/denchmark-link&gt;
 This is fixed in the latest version, 0.9.76
		</comment>
	</comments>
</bug>