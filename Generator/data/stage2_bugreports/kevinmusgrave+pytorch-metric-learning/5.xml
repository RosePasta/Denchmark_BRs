<bug id='5' author='s-rog' open_date='2020-02-05T05:33:49Z' closed_time='2020-02-05T11:10:20Z'>
	<summary>Lambda functions and DDP incompatibilities</summary>
	<description>
&lt;denchmark-code&gt;AttributeError: Can't pickle local object 'WeightRegularizerMixin.__init__.&lt;locals&gt;.&lt;lambda&gt;'
&lt;/denchmark-code&gt;

The use of lambda functions causes distributed training to fail due to the use of pickling in fork. I searched for lambdas in the repo and there are only around 10 instances, should be an easy fix?
	</description>
	<comments>
		<comment id='1' author='s-rog' date='2020-02-05T07:36:17Z'>
		Should I change all of the lambdas to function definitions?
		</comment>
		<comment id='2' author='s-rog' date='2020-02-05T08:32:53Z'>
		Yeah, for my use case I changed the lambda function in WeightRegularizerMixin to a regular function and now it works in my multi GPU training setup (I'm using the arcface loss). This would make the library available for those with more substantial/production hardware.
I have not yet done any other testing though.
		</comment>
		<comment id='3' author='s-rog' date='2020-02-05T11:10:20Z'>
		Thanks for pointing this out. In the latest version, all the lambda expressions have been converted to function definitions. Hopefully it works with distributed data parallel now.
		</comment>
		<comment id='4' author='s-rog' date='2020-02-06T00:31:08Z'>
		Cheers, I'll give the update a try soon. Thanks for exposing metric learning in an easy to use API.
		</comment>
		<comment id='5' author='s-rog' date='2020-02-07T01:20:42Z'>
		
&lt;denchmark-link:https://github.com/KevinMusgrave&gt;@KevinMusgrave&lt;/denchmark-link&gt;
 I think nested functions cannot be pickled either.
		</comment>
		<comment id='6' author='s-rog' date='2020-02-07T01:24:25Z'>
		Ah damn! K I'll fix that. At what point do you get the pickle error? When you create the DistributedDataParallel object, or at some other point?
		</comment>
		<comment id='7' author='s-rog' date='2020-02-07T02:38:32Z'>
		I think the error happens when fork is called. When I did my own testing to get it to work I just placed the dummy regularizer outside the class though I guess it can be a class function too, just not nested haha.
		</comment>
		<comment id='8' author='s-rog' date='2020-02-07T02:40:19Z'>
		Are you using the loss function within pytorch lightning? I'd just like a way to test my changes before pushing them.
		</comment>
		<comment id='9' author='s-rog' date='2020-02-07T02:56:41Z'>
		Yeah I'm calling arcface loss in training_step
		</comment>
		<comment id='10' author='s-rog' date='2020-02-07T12:18:58Z'>
		I removed all nested function definitions, so pickling loss functions works now. Hopefully that fixes it! fingers crossed
		</comment>
		<comment id='11' author='s-rog' date='2020-02-10T00:51:48Z'>
		Awesome, I'll test it out soon!
Edit: seems to be working! (after converting all args to keyword args)
		</comment>
	</comments>
</bug>