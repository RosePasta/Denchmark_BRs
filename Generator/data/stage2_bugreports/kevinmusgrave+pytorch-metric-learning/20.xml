<bug id='20' author='glimperg' open_date='2020-02-28T15:24:31Z' closed_time='2020-03-20T11:45:51Z'>
	<summary>Embeddings collapse to one point using BatchHardMiner</summary>
	<description>
I tried to run the example_MetricLossOnly script, only changing the miner to the BatchHardMiner (use_similarity=False, squared_distances=False) and I ran this model for 20 epochs of 500 iterations. Using these settings (on the default CIFAR100 dataset), I get a precision@1 of 0.1774 after the first epoch, and the performance only goes down after more epochs. Using the MultiSimilarityMiner (epsilon=0.1) instead, I get a precision@1 of 0.2579 after one epoch, and it increases after that.
These are the relevant logs:
&lt;denchmark-link:https://github.com/KevinMusgrave/pytorch-metric-learning/files/4267703/accuracies_GlobalEmbeddingSpaceTester_normalized_compared_to_self_VAL.txt&gt;accuracies_GlobalEmbeddingSpaceTester_normalized_compared_to_self_VAL.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/KevinMusgrave/pytorch-metric-learning/files/4267709/loss_histories.txt&gt;loss_histories.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/KevinMusgrave/pytorch-metric-learning/files/4267718/post_gradient_miner_BatchHardMiner.txt&gt;post_gradient_miner_BatchHardMiner.txt&lt;/denchmark-link&gt;

Interestingly, the hardest_triplet_dist, hardest_pos_pair_dist and hardest_neg_pair_dist parameters in the BatchHardMiner all go towards zero. Also, the loss seems to converge to 0.01 (exactly the size of the margin for the triplet loss). This implies that all embeddings collapse to a single point in embedding space.
My question is, do I need different hyperparameters to make the BatchHardMiner work? Or is there an error in the implementation of the BatchHardMiner somewhere? Thanks!
	</description>
	<comments>
		<comment id='1' author='glimperg' date='2020-03-03T22:36:50Z'>
		&lt;denchmark-link:https://github.com/glimperg&gt;@glimperg&lt;/denchmark-link&gt;
 I have a paper deadline this Thursday, so I'll investigate this afterwards
		</comment>
		<comment id='2' author='glimperg' date='2020-03-04T09:02:26Z'>
		Thanks, I have also looked into this a bit, and I am getting the impression that the triplet batch hard loss has trouble converging on many difficult datasets. As the original author of the triplet Batch Hard loss suggests &lt;denchmark-link:https://github.com/VisualComputingInstitute/triplet-reid/issues/30#issuecomment-383604018&gt;here&lt;/denchmark-link&gt;
, the method may only converge for specific learning rates or optimizer settings (such as the epsilon parameter for Adam). So I don't think there's a bug in the BatchHardMiner implementation.
		</comment>
		<comment id='3' author='glimperg' date='2020-03-20T11:45:51Z'>
		&lt;denchmark-link:https://github.com/glimperg&gt;@glimperg&lt;/denchmark-link&gt;
 I finally got around to checking this. It turns out there was a bug in BatchHardMiner, though it wouldn't occur very frequently. The bug is that BatchHardMiner would return a triplet for every sample in the batch, even if that sample had no positives in the batch (i.e. even if the sample is the only one of its kind in the batch).
Note that this bug probably never occurred when you ran the example_MetricLossOnly script, because in that script, batch size = 32 and the MPerClassSampler has m=4, meaning every batch will have 8 classes with 4 samples each.
The bug is fixed in the latest version v0.9.78. I also added tests for BatchHardMiner, which you can find here: &lt;denchmark-link:https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/tests/miners/test_batch_hard_miner.py&gt;https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/tests/miners/test_batch_hard_miner.py&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>