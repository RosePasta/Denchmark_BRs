<bug id='24' author='ygk09' open_date='2018-12-25T10:10:16Z' closed_time='2020-05-18T00:04:30Z'>
	<summary>Problems with model loading in intent extraction</summary>
	<description>
&lt;denchmark-h:h4&gt;Target objective:&lt;/denchmark-h&gt;

After I have run [train_mlt_model.py] according to the Readme.md, a model can be obtained. Save this model by 'model.save' method and reload it as model1. Then the prediction on the validation data of these two models are different.
&lt;denchmark-h:h4&gt;Steps to objective:&lt;/denchmark-h&gt;



Run [train_mlt_model.py] and a model is obtained. This model is stored in the memory.


Save this model on the disk by command: model.save(model_path)


Reload this model from the disk by command:
model1 = MultiTaskIntentModel();
model1.load(model_path)


Use the commands below to evaluate these two model:
predictions = model.predict(test_inputs, batch_size=args.b)
eval = get_conll_scores(predictions, test_y,
{v: k for k, v in dataset.tags_vocab.vocab.items()})
print(eval)
predictions = model1.predict(test_inputs, batch_size=args.b)
eval = get_conll_scores(predictions, test_y,
{v: k for k, v in dataset.tags_vocab.vocab.items()})
print(eval)


The results are different:
model: avg / total      0.820     0.833     0.821      1794
model1:avg / total      0.817     0.826     0.814      1794


So, why the predictions of model and model1 are different ?
	</description>
	<comments>
		<comment id='1' author='ygk09' date='2018-12-26T12:23:17Z'>
		Hi &lt;denchmark-link:https://github.com/ygk09&gt;@ygk09&lt;/denchmark-link&gt;
, thanks for reporting. I'll check the scenario you described and report back.
		</comment>
		<comment id='2' author='ygk09' date='2018-12-26T12:56:41Z'>
		I followed your description and did not succeed in replicating your bug.
Can you share more info?
		</comment>
		<comment id='3' author='ygk09' date='2018-12-27T02:10:16Z'>
		OK. Belows are my procedures:

In PyCharm, run [train_mlt_model.py] in python console mode.
After 5 epochs, the trained model can be obtained in the memory. It is saved on the disk with a name 'model.h5'. Meanwhile, test_inputs and test_y are also stored in the memory.
In the python console, input command:
model1=MultiTaskIntentModel()
and command:
model1.load('model1.h5')
Then we can get two models in the memory: model and model1. The former is obtained after 5 epochs training. The latter is obtained by loading a file, which is obtained through saving the former on the disk.
In the python console, input command:
predictions = model.predict(test_inputs, batch_size=args.b)
eval = get_conll_scores(predictions, test_y,
{v: k for k, v in dataset.tags_vocab.vocab.items()})
print(eval)
and then input:
predictions = model1.predict(test_inputs, batch_size=args.b)
eval = get_conll_scores(predictions, test_y,
{v: k for k, v in dataset.tags_vocab.vocab.items()})
print(eval)
The results of 'print(eval)' are very differnt.
By the way, the parameter 'use_cudnn' is set to be 'True'.

		</comment>
		<comment id='4' author='ygk09' date='2019-01-08T14:24:12Z'>
		I got it.
The issue is a mismatch when training a model using  cudnn LSTM cells and then loading into CPU LSTM cells (there are reports of this issue with Tensorflow).
I will work on a fix so this would be possible, until then please use a single backend for train/inference.
Thanks!
		</comment>
		<comment id='5' author='ygk09' date='2020-05-18T00:04:29Z'>
		Issue marked as stale, closing.
		</comment>
	</comments>
</bug>