<bug id='135' author='kirstin-rhys' open_date='2020-01-16T01:19:19Z' closed_time='2020-05-30T00:04:58Z'>
	<summary>bug: Error exporting QuantizedBert to ONNX</summary>
	<description>
When attempting to export a Q8BERT model to ONNX, TorchScript reports a runtime error.
Since the quantized forward methods embed Tensors with grads from layer instance attributes rather than input parameters, exporting the model has to be done with explicit torch script tracing; however that creates a traced model with lacks fidelity with the python model, as branching behavior is not captured.
However, attempting to create a script model with TorchScript, which should encode that behavior results in a missing config attribute on the BertModel superclass.
To Reproduce
Steps to reproduce the behavior:

Train a Q8Bert model:

&lt;denchmark-code&gt; nlp-train transformer_glue \
            --task_name mrpc \
            --model_name_or_path bert-base-uncased \
            --model_type quant_bert \
            --learning_rate 2e-5 \
            --output_dir $DATA_DIR \
            --data_dir $GLUE_DIR  \
            --evaluate_during_training \
            --do_lower_case \
            --per_gpu_train_batch_size 32 \
            --per_gpu_eval_batch_size 32 \
            --max_seq_length 128
&lt;/denchmark-code&gt;


Load the 8bit model for inference

&lt;denchmark-code&gt;import torch
from nlp_architect.models.transformers.quantized_bert import QuantizedBertForSequenceClassification

model = QuantizedBertForSequenceClassification.from_pretrained(configs.data_dir, from_8bit=True)
device = torch.device("cuda")
model.to(device)
model.eval()

&lt;/denchmark-code&gt;


Attempt to create the script model

&lt;denchmark-code&gt;script_model = torch.jit.script(model)
&lt;/denchmark-code&gt;


This produces the error:

&lt;denchmark-code&gt;Traceback (most recent call last):
  File "convert-torch-q8bert-via-onnx-to-tensorflow.py", line 105, in &lt;module&gt;
    script_model = torch.jit.script(model)
  File "/home/krhys/.local/share/virtualenvs/q8bert-_S0V57VU/lib/python3.7/site-packages/torch/jit/__init__.py", line 1203, in script
    return torch.jit.torch.jit._recursive.recursive_script(obj)
  File "/home/krhys/.local/share/virtualenvs/q8bert-_S0V57VU/lib/python3.7/site-packages/torch/jit/_recursive.py", line 173, in recursive_script
    return copy_to_script_module(mod, overload_stubs + stubs)
  File "/home/krhys/.local/share/virtualenvs/q8bert-_S0V57VU/lib/python3.7/site-packages/torch/jit/_recursive.py", line 95, in copy_to_script_module
    torch.jit._create_methods_from_stubs(script_module, stubs)
  File "/home/krhys/.local/share/virtualenvs/q8bert-_S0V57VU/lib/python3.7/site-packages/torch/jit/__init__.py", line 1423, in _create_methods_from_stubs
    self._c._create_methods(self, defs, rcbs, defaults)
  File "/home/krhys/.local/share/virtualenvs/q8bert-_S0V57VU/lib/python3.7/site-packages/torch/jit/_recursive.py", line 195, in make_strong_submodule
    new_strong_submodule = recursive_script(module)
  File "/home/krhys/.local/share/virtualenvs/q8bert-_S0V57VU/lib/python3.7/site-packages/torch/jit/_recursive.py", line 173, in recursive_script
    return copy_to_script_module(mod, overload_stubs + stubs)
  File "/home/krhys/.local/share/virtualenvs/q8bert-_S0V57VU/lib/python3.7/site-packages/torch/jit/_recursive.py", line 95, in copy_to_script_module
    torch.jit._create_methods_from_stubs(script_module, stubs)
  File "/home/krhys/.local/share/virtualenvs/q8bert-_S0V57VU/lib/python3.7/site-packages/torch/jit/__init__.py", line 1423, in _create_methods_from_stubs
    self._c._create_methods(self, defs, rcbs, defaults)
RuntimeError:
module has no attribute 'config':
at /home/krhys/.local/share/virtualenvs/q8bert-_S0V57VU/lib/python3.7/site-packages/transformers/modeling_bert.py:675:15
        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]
        # ourselves in which case we just need to make it broadcastable to all heads.
        if attention_mask.dim() == 3:
            extended_attention_mask = attention_mask[:, None, :, :]

        # Provided a padding mask of dimensions [batch_size, seq_length]
        # - if the model is a decoder, apply a causal mask in addition to the padding mask
        # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]
        if attention_mask.dim() == 2:
            if self.config.is_decoder:
               ~~~~~~~~~~~ &lt;--- HERE
                batch_size, seq_length = input_shape
                seq_ids = torch.arange(seq_length, device=device)
                causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) &lt;= seq_ids[None, :, None]
                extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]
            else:
                extended_attention_mask = attention_mask[:, None, None, :]

        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
        # masked positions, this operation will create a tensor which is 0.0 for
'__torch__.nlp_architect.models.transformers.quantized_bert.QuantizedBertModel.forward' is being compiled since it was called from '__torch__.nlp_architect.models.transformers.quantized_bert.QuantizedBertForSequenceClassification.forward'
at /home/krhys/.local/share/virtualenvs/q8bert-_S0V57VU/lib/python3.7/site-packages/transformers/modeling_bert.py:1014:8
    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,
                position_ids=None, head_mask=None, inputs_embeds=None, labels=None):

        outputs = self.bert(input_ids,
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...  &lt;--- HERE
                            attention_mask=attention_mask,
                            token_type_ids=token_type_ids,
                            position_ids=position_ids,
                            head_mask=head_mask,
                            inputs_embeds=inputs_embeds)

        pooled_output = outputs[1]

        pooled_output = self.dropout(pooled_output)
&lt;/denchmark-code&gt;

Expected behavior
It should produce a script model without error
**Environment setup: **

OS (Linux/Mac OS): Ubuntu Xenial
Python version: 3.7

Additional context
It may be possible to create pure helper quantized forward functions with explicit arguments which have the @torch.jit.script annotations which are called by the quantized layer forward methods
Note that the HuggingFace transformers Bert model can be exported to ONNX.
	</description>
	<comments>
		<comment id='1' author='kirstin-rhys' date='2020-01-26T23:35:57Z'>
		Hi &lt;denchmark-link:https://github.com/kirstin-rhys&gt;@kirstin-rhys&lt;/denchmark-link&gt;
,
I was able to reproduce this bug. Since the model does have this config.is_decoder attribute I find this bug very curious.
Notice that the quantization ops used in this model are custom made for this library and are not the official PyTorch ops and therefore they are not recognized by the ONNX exporter and/or torchscript.
As of now, we don't have plans to make our quantization ops work with torchscript or ONNX exporter.
Since our release, PyTorch has made its native quantization framework available and replacing our quantization ops with PyTorch ones after training might fix this problem and produce a working exportable model. We would welcome your contribution if you happen to fix this issue.
		</comment>
		<comment id='2' author='kirstin-rhys' date='2020-01-27T04:44:22Z'>
		Hi &lt;denchmark-link:https://github.com/ofirzaf&gt;@ofirzaf&lt;/denchmark-link&gt;
 !
I have been able to export to ONNX through a careful rewrite of the quantized forward layer functions and the use of @torch.jit.script annotations.
I will follow up later
		</comment>
		<comment id='3' author='kirstin-rhys' date='2020-02-04T09:04:30Z'>
		&lt;denchmark-link:https://github.com/kirstin-rhys&gt;@kirstin-rhys&lt;/denchmark-link&gt;
 waiting to hear if everything worked out for you and what was your solution.
		</comment>
		<comment id='4' author='kirstin-rhys' date='2020-02-12T19:19:50Z'>
		It worked out. Sadly the export from onnx to tensorflow has more serious issues.
the basic method was to move the bulk of the quantized forward methods into stateless helper functions via explicit parameter passing with type annotations and @torch.jit.script annotations and manually running the trace with opset 11 and dummy input before exporting
		</comment>
		<comment id='5' author='kirstin-rhys' date='2020-03-18T22:03:43Z'>
		hi &lt;denchmark-link:https://github.com/kirstin-rhys&gt;@kirstin-rhys&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/ofirzaf&gt;@ofirzaf&lt;/denchmark-link&gt;
 , I am running into the same problem of converting a quanitized model to onnx format. for that, i'm using  and the error that i'm getting is

would you be able to help me with that?
		</comment>
		<comment id='6' author='kirstin-rhys' date='2020-03-30T05:32:21Z'>
		
It worked out. Sadly the export from onnx to tensorflow has more serious issues.
the basic method was to move the bulk of the quantized forward methods into stateless helper functions via explicit parameter passing with type annotations and @torch.jit.script annotations and manually running the trace with opset 11 and dummy input before exporting

hi &lt;denchmark-link:https://github.com/kirstin-rhys&gt;@kirstin-rhys&lt;/denchmark-link&gt;
 , I'm also working on exporting this into onnx. I wonder if you would like to share your code or elaborate more on how you get around this problem? Thanks!
		</comment>
		<comment id='7' author='kirstin-rhys' date='2020-05-30T00:04:56Z'>
		Issue marked as stale, closing.
		</comment>
	</comments>
</bug>