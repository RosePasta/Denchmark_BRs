<bug id='319' author='kruus' open_date='2018-09-11T17:33:36Z' closed_time='2018-09-25T21:25:30Z'>
	<summary>ref_eltwise.hpp backward init() may erroneously return "mkldnn_unimplemented"</summary>
	<description>
It looks like ref_eltwise_fwd_t got the init() check correct, but the check in ref_eltwise_bwd_t::init() can cause simple-training-net-c to erroneously return "mkldnn_unimplemented".
A backward check that does not force jit impls to handle easy cases could do like the
forward layer:  put use_generic an implication clause.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

Intel MKL-DNN includes hardware-specific optimizations and may behave
differently on depending on the compiler and build environment. Include
the following information to help reproduce the issue:

CPU i7-6850K
OS version Linux snake10 4.4.0-134-generic #160-Ubuntu SMP Wed Aug 15 14:58:00 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
Compiler version gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
MKLROOT none
CMake version 3.5.1

&lt;denchmark-h:h3&gt;Steps to reproduce&lt;/denchmark-h&gt;

comment out all jit eltwise implementations in cpu_engine.cpp, leaving only the reference
implementation.
&lt;denchmark-h:h3&gt;Actual behavior&lt;/denchmark-h&gt;


With some debug, I see that the backwards relu descriptor fails during simple-training-net-c:

&lt;denchmark-code&gt;mkldnn_verbose,create,lrn,jit:avx2,backward_data,fdata:nChw8c fdiff:nChw8c,alg:lrn_across_channels,mb32ic96ih15iw15,0.103027
 no-eltwise &lt;snip&gt; Inconsistent! [/local/kruus/sx/sx-dnn/src/cpu/ref_eltwise.hpp:68]
one_of(desc()-&gt;prop_kind, forward_training, forward_inference)
 no-eltwise Inconsistent! [/local/kruus/sx/sx-dnn/src/cpu/ref_eltwise.hpp:134]
use_generic &amp;&amp; !one_of(diff_dst_d.ndims(), 4, 5)
 no-eltwise &lt;... other non-eltwise impls&gt;
[/local/kruus/sx/sx-dnn/examples/simple_training_net.c:664]
error: mkldnn_primitive_desc_create(&amp;relu_bwd_pd, &amp;relu_bwd_desc, engine, relu_pd) returns 5
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Simple training net example should run if you have only reference impls available.
&lt;denchmark-h:h3&gt;Fix&lt;/denchmark-h&gt;

It seems that the line use_generic &amp;&amp; !one_of(diff_dst_d.ndims(), 4, 5) should be replaced
for the backwards relu layer by implication(use_generic, one_of(diff_dst_d.ndims(), 4, 5)),
similar to the requirement for the forward layer.
The one-line fix is near the end of src/cpu/ref_eltwise.cpp
	</description>
	<comments>
		<comment id='1' author='kruus' date='2018-09-24T17:20:26Z'>
		Hi,
I'm afraid I don't quite understand the suggested fix. Right now on fwd we check if !implication(use_generic, one_of(src_d.ndims(), 4, 5)) is true to return unimplemented and (use_generic &amp;&amp; !one_of(diff_dst_d.ndims(), 4, 5)) on bwd. I believe those to are equivalent:
!implication(use_generic, one_of(src_d.ndims(), 4, 5)) = !(!use_generic || one_of(src_d.ndims(), 4, 5)) = (use_generic &amp;&amp; !one_of(diff_dst_d.ndims(), 4, 5))
I also can't reproduce the behavior, did you change anything in example as well?
I agree that eltwise ref implementation should always work, so it makes sense to enable generic implementation for all dimensions.
		</comment>
		<comment id='2' author='kruus' date='2018-09-25T21:25:30Z'>
		Ahaa. I had macroized tests to allow MKLDNN_VERBOSE to print out exact cause of unchosen
impls, but got this particular one reversed.  After reading your logical reply, my illogical error
was immediately obvious.
&lt;denchmark-code&gt;-            if (use_generic &amp;&amp; !one_of(diff_dst_d.ndims(), 4, 5))
-                return status::unimplemented;
+            OK_AND(implication(use_generic, one_of(diff_dst_d.ndims(), 4, 5)));
+            if (!ok) return status::unimplemented; 
&lt;/denchmark-code&gt;

Thanks for taking time to look at it.
Wanted: fresh pair of eyes.
		</comment>
	</comments>
</bug>