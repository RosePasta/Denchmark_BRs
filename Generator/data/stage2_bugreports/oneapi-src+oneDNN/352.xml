<bug id='352' author='peteryang1' open_date='2018-11-23T09:54:45Z' closed_time='2018-11-30T04:47:56Z'>
	<summary>get wrong layout when using deconvolution-direct &amp;&amp; group != 1</summary>
	<description>
while I'm calculating a deconv operation, the input_dim is (1,8,1,1) the kernel_dim is (2,4,4,1,1), output_dim is (1,8,1,1), I think the result is incorrect.
if I name the result of first group g1f1, g1f2, g1f3, g1f4 (4 feature maps) , then the second group g2f1, g2f2, g2f3, g2f4, in mkldnn we place the result in order g1f1, g2f1, g1f2, g2f2, g1f3 g2f3, g1f4, g2f4. it seems not correct to place it this way.
In group convolution, when group is not 1, we split the channels into groups, after each group finished calculating, we concat the groups into output. deconv is an operation to do the conv in the opposite way, so we should also concat the channels instead of cross laid.
I have an example , in the op in explained above , I set the weights randomly, bias all zero, and input tensor all one. the model shape is like below:(onnx model)
&lt;denchmark-link:https://user-images.githubusercontent.com/25981102/48937070-f95ae400-ef47-11e8-805f-6e78ace07c72.png&gt;&lt;/denchmark-link&gt;

In pytorch the result is:
&lt;denchmark-link:https://user-images.githubusercontent.com/25981102/48937115-0f68a480-ef48-11e8-97e9-218496ebd0fb.png&gt;&lt;/denchmark-link&gt;

In mkldnn, the result is:
&lt;denchmark-link:https://user-images.githubusercontent.com/25981102/48937353-c9601080-ef48-11e8-992d-f4f9231c9fb9.png&gt;&lt;/denchmark-link&gt;

advises are welcomed! thanks.
	</description>
	<comments>
		<comment id='1' author='peteryang1' date='2018-11-25T12:48:09Z'>
		Hi &lt;denchmark-link:https://github.com/peteryang1&gt;@peteryang1&lt;/denchmark-link&gt;
,
Could you please provide a small reproducer with MKL-DNN calls for the issue? Also you may want to start with benchdnn, which is a standalone Intel MKL-DNN performance and correctness benchmark. Please run it as shown below.
I tried to reproduce it on my side, but failed.
Here is a &lt;denchmark-link:https://gist.github.com/emfomenk/f6ffe2cb2be1aced27c10d12db2f4dc9&gt;github gist&lt;/denchmark-link&gt;
 with pytorch and mkl-dnn code. Both give the same output:
$ # Intel MKL-DNN
$ g++ deconv.cpp -lmkldnn -lgomp -lpthread &amp;&amp; ./a.out
12, 21, -102, -201,

$ # PyTorch
$ python deconv.py
tensor([[[[  12.]], [[  21.]], [[-102.]], [[-201.]]]], grad_fn=&lt;CatBackward&gt;)
Please note that I didn't pass any to deconv in C++ file just to simplify the tests. However benchdnn (which does use any) doesn't report any issues for the sizes you mentioned.
$ xMKLDNN_VERBOSE=1 ./tests/benchdnn/benchdnn --deconv mb1_g2ic8oc8_ih1kh1ph0
0:PASSED __REPRO: g2mb1ic8ih1oc8oh1kh1n"wip"
tests:1 passed:1 skipped:0 mistrusted:0 unimplemented:0 failed:0
		</comment>
		<comment id='2' author='peteryang1' date='2018-11-27T06:30:01Z'>
		Hi, &lt;denchmark-link:https://github.com/emfomenk&gt;@emfomenk&lt;/denchmark-link&gt;
 , I run the code of  &lt;denchmark-link:https://gist.github.com/emfomenk/f6ffe2cb2be1aced27c10d12db2f4dc9&gt;github gist&lt;/denchmark-link&gt;
, it is ok, but if we change the output size to 6, the output of mkldnn is not same with pytorch, the changed code details are:
For deconv.cpp:
  to 
 to 
  to 
 to 
For deconv.py:
 to ,
  to  ,
After those changes, I get the output of mkldnn is : , for pytorch, the output is: .
Can you give me some advice? Thanks.
		</comment>
		<comment id='3' author='peteryang1' date='2018-11-27T08:05:29Z'>
		Hi, &lt;denchmark-link:https://github.com/emfomenk&gt;@emfomenk&lt;/denchmark-link&gt;
 , and thank @zxb1489479870 for your information. I run your code, the result is the same, but my code is not the same as yours and I think your code has something inappropriate.
I found the difference between my code and yours:

I set all the desc's layout mkldnn::memory::format::any, so the weight's layout is set to mkldnn::memory::format::blocked automatically.

const auto conv_transpose_desc = mkldnn::deconvolution_forward::desc(
        mkldnn::prop_kind::forward_inference, _algorithm,
        mkldnn::memory::desc(
            { _input_dims }, mkldnn::memory::data_type::f32,
            mkldnn::memory::format::any),
        mkldnn::memory::desc(
            { _kernel_dims }, mkldnn::memory::data_type::f32,
            mkldnn::memory::format::any),
        mkldnn::memory::desc(
            { _bias_dims }, mkldnn::memory::data_type::f32,
            mkldnn::memory::format::any),
        mkldnn::memory::desc(
            { _output_dims }, mkldnn::memory::data_type::f32,
            mkldnn::memory::format::any),
        { 1,1 },{ 0,0 },{ 0,0 }, mkldnn::padding_kind::zero);

In pytorch, the input and weight are set like below:

the weight is defined like CNHW(IOHW in mkldnn), so I transposed it to NCHW before put then into mkldnn, then because I set them mkldnn::memory::format::any in 1, I use mkldnn::reorder to reorder the weight to the layout it creates; part code like below:

    std::vector&lt;float&gt; kernel_temp(4*8*1*1);
    if (integrated_transpose(4,8,1,1, kernel,cnhw, kernel_temp.data(),nchw) == 0)//transpose weight from cnhw to nchw
    {
        source_layout = nnb_tensor_layout_nchw;
    }
    mkldnn::memory::format source_layout_mkldnn;
    switch (source_layout)
    {
    case nnb_tensor_layout_nchw:
        source_layout_mkldnn = mkldnn::memory::format::goihw;
        break;
    default:
        return EINVAL;
    }
    auto source_kernel_memory = mkldnn::memory(
        { { { kernel_dims },
            mkldnn::memory::data_type::f32,
            source_layout_mkldnn },
          *reinterpret_cast&lt;mkldnn::engine*&gt;(mkldnn_runtime) },
        kernel_temp.data());
    auto target_kernel_memory = mkldnn::memory(
        pdd-&gt;conv_transpose_primitive_desc.weights_primitive_desc(), workspace);
    auto reorder = mkldnn::reorder(source_kernel_memory, target_kernel_memory);
pytorch code is the same as yours.
this is the difference I found, hope to get some advises from you.Thanks!
		</comment>
		<comment id='4' author='peteryang1' date='2018-11-28T13:22:59Z'>
		oK, first of all I am sorry for my example.
By coincidence I chose the symmetric weights and didn't notice the issue. I updated the &lt;denchmark-link:https://gist.github.com/emfomenk/f6ffe2cb2be1aced27c10d12db2f4dc9&gt;gist&lt;/denchmark-link&gt;
.
There are the following changes there:

Weights matrix is not symmetric now {10, 1, 1, 10, ...} --&gt; {10, 2, 1, 10, ...} to reproduce the issue
Weights format can be passed at compile time via -DGWEI_FMT=goihw or -DGWEI_FMT=giohw
I added deconv_with_any.cpp that demonstrates how to run deconvolution in appropriate way (using any at creation) [*]

Original gist used goihw format for weights. However it turned out that PyTorch uses giohw format for weights in deconvolution (in opposite to goihw format for weights in convolution). The reason is deconvolution in PyTorch wants to reuse backward pass of convolution. Thanks @zxb1489479870 for pointing on that.
Once I replaced goihw with giohw all deconv.py, deconv.cpp, and deconv_with_any.cpp gave the same result.
.
The latest version of Intel MKL-DNN doesn't support  and  formats. There are simply no value in the corresponding enum. I prepared a quick patch that adds support for those formats. You can find it &lt;denchmark-link:https://gist.github.com/emfomenk/1c5b78333d0eede05b796f85ccf48192&gt;here&lt;/denchmark-link&gt;
 and apply it against the latest master branch. I hope we will soon promote this patch, and you will be able to use these formats out-of-the-box.
Applying this patch is the recommended way to make deconvolution + reorders work as you want.
There is a way to make Intel MKL-DNN work even without this patch but that would be hacky... so I would suggest not doing that.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

[*] Please note that currently creating deconvolution with any is the only way to get performant primitive. If you force deconvolution to use user-specified format deconvolution would be horribly slow. This is a known issue and will be fixed in future Intel MKL-DNN releases.
		</comment>
		<comment id='5' author='peteryang1' date='2018-11-29T08:33:21Z'>
		after I set the reorder desc to mkldnn::memory::format::giohw, it really works right. it seems your patch does solve this problem. hope to see it merged to the master branch!
I got another small mistake, while I'm building the latest mkldnn, the 25 line of utils.cmake gets an error, seems it's a small bug, deleting this line solves this problem.
the content is:
include("cmake/options.cmake")
but cmake cannot find this file.
thanks again for your help!
		</comment>
		<comment id='6' author='peteryang1' date='2018-11-29T08:40:10Z'>
		Great, thanks for confirming and willing to test the patch!

got another small mistake, while I'm building the latest mkldnn, ...

Hmm, this is super strange 
The file exists, &lt;denchmark-link:https://github.com/intel/mkl-dnn/blob/master/cmake/options.cmake&gt;here it is&lt;/denchmark-link&gt;
. May I ask to clean the build directory completely and try to rebuild from the scratch? Seems like the problem might cause by applying the patch or something like that...
		</comment>
		<comment id='7' author='peteryang1' date='2018-11-29T08:58:49Z'>
		I tried cleaning the cache, it still causes an error. this line is added recently, seems useless, you can check it later, not a big problem.
thank again for your help on the convtranspose.
		</comment>
		<comment id='8' author='peteryang1' date='2018-11-29T09:22:22Z'>
		seems it's my mistake, I rewrite the cmake file to find mkldnn. so the root of cmake is not the root of mkldnn. sorry-.-
		</comment>
		<comment id='9' author='peteryang1' date='2018-11-29T09:24:02Z'>
		No problem. Thanks for confirming that! :)
I will leave the bug open until promote PyTorch formats to master.
		</comment>
	</comments>
</bug>