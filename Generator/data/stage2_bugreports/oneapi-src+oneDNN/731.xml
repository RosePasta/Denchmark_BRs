<bug id='731' author='pinzhenx' open_date='2020-05-19T03:36:40Z' closed_time='2020-05-21T22:46:47Z'>
	<summary>feature request: support strided (non-contiguous) memories</summary>
	<description>
Pytorch uses the same language as DNNL, a dim-stride pair, to describe a plain-format memory. So ideally it's possible to convert all Pytorch tensors to DNNL tensors at zero cost.
Moreover, Pytorch ops support non-contiguous tensors that's been sliced from another, and these sliced tensors may not be contiguous at any dimension, i.e. none of the strides is 1. I'll use "non-contiguous" and "strided" interchangeably below. Under the current DNNL programming model, users are supposed to reorder a strided tensor into a dense one and then pass it to DNNL primitive.
We are now exploring if we can directly pass strided tensors to DNNL and expect it can at least fallback to a ref path. Unfortunately, it failed on a binary primitive with this snippet.
#include &lt;stdio.h&gt;
#include "dnnl.hpp"

using namespace dnnl;
using tag = memory::format_tag;
using dt = memory::data_type;

void dump(const memory&amp; m) {
  printf("[%g", ((float*)m.get_data_handle())[0]);
  for (size_t i = 1; i &lt; m.get_desc().get_size() / sizeof(float); i++)
    printf(", %g", ((float*)m.get_data_handle())[i]);
  printf("]\n");
}

engine eng(engine::kind::cpu, 0);
stream strm(eng);

void test_binary() {
  float src0_buf[12] = {100, 200, 300, 400, 500, 600, 700, 800, 900, 100, 1100, 1200};
  float src1_buf[32] = {1,  2,  3,  4,  5,  6,  7,  8,  9,  10, 11, 12, 13, 14, 15, 16,
                        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32};
  float dst_buf[12] = {0};

  memory src0({{2, 4}, dt::f32, {4, 1}}, eng, src0_buf);
  memory src1({{2, 4}, dt::f32, {16, 4}}, eng, src1_buf);
  memory dst({{2, 4}, dt::f32, {4, 1}}, eng, dst_buf);

  // if we put strided tensor at the first input and set dst format as `any`, 
  // it will corrupt its memory
  auto pd = binary::primitive_desc({algorithm::binary_add, src0.get_desc(), src1.get_desc(), dst.get_desc()}, eng);
  binary(pd).execute(strm, {{DNNL_ARG_SRC_0, src0}, {DNNL_ARG_SRC_1, src1}, {DNNL_ARG_DST, dst}});

  dump(src0);
  dump(src1);
  dump(dst); // [101, 205, 309, 413, 517, 621, 725, 829]
}

int main() {
  test_binary();
  return 0;
}
&lt;denchmark-code&gt;dnnl_verbose,info,oneDNN v1.4.0 (commit 9034f147d87b96000ef8b13a4f37c8b9f8c3e025)
dnnl_verbose,info,cpu,runtime:OpenMP
dnnl_verbose,info,cpu,isa:Intel AVX-512 with AVX512BW, AVX512VL, and AVX512DQ extensions
dnnl_verbose,info,gpu,runtime:none
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:ab:f0 src_f32::blocked:ab:f0 dst_f32::blocked:ab:f0,,alg:binary_add,2x4:2x4 2x4,34.281
[100, 200, 300, 400, 500, 600, 700, 800]
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]
[101, 202, 303, 404, 505, 606, 707, 808]
&lt;/denchmark-code&gt;

According to the discussion in &lt;denchmark-link:https://github.com/oneapi-src/oneDNN/issues/69#issuecomment-630500241&gt;#69 (comment)&lt;/denchmark-link&gt;
, only reorder, sum, concat could correctly handle these strided tensors, and from &lt;denchmark-link:https://github.com/oneapi-src/oneDNN/issues/593#issuecomment-553806623&gt;#593 (comment)&lt;/denchmark-link&gt;
 we knew that not many primitives even support permuted formats. So we want all primitives to have a defined behavior on custom formats, either throw an error or fallback to ref path. Especially for these two classes regardless of DNNL-specific blocked format:


non-contiguous &amp; dense
#elems == the size described by dims&amp;strides.
e.g. memory described by permuted format tags, like ba, nhwc, iohw.


non-contiguous &amp; not dense
#elems &lt; the size described by dims&amp;strides.
e.g. submemory sliced from other memory


	</description>
	<comments>
		<comment id='1' author='pinzhenx' date='2020-05-19T04:56:40Z'>
		Hi &lt;denchmark-link:https://github.com/pinzhenx&gt;@pinzhenx&lt;/denchmark-link&gt;
,
Thanks for the opened issue!
The current described behavior is definitely a bug.

So we want all primitives to have a defined behavior on custom formats, either throw an error or fallback to ref path.

In my opinion, the library indeed should at least fallback to the reference implementation.
But I want to confirm with you that you are indeed fine with the fallback to the reference generic implementations, that are known to be extremely slow. Are you oK with this? Do you want to know that the implementation is "slow" in advance? Which is in most of the cases could be done by checking if pd::impl_info_str() has "ref" sub-string.
		</comment>
		<comment id='2' author='pinzhenx' date='2020-05-19T05:14:05Z'>
		Hi &lt;denchmark-link:https://github.com/pinzhenx&gt;@pinzhenx&lt;/denchmark-link&gt;
, could you try this patch, please?
&lt;denchmark-code&gt;diff --git a/src/cpu/x64/jit_uni_binary.hpp b/src/cpu/x64/jit_uni_binary.hpp
index af1199b3a..f8787f803 100644
--- a/src/cpu/x64/jit_uni_binary.hpp
+++ b/src/cpu/x64/jit_uni_binary.hpp
@@ -78,6 +78,9 @@ struct jit_uni_binary_t : public primitive_t {
                 ok = ok &amp;&amp; bcast_dims[d] == 1;
             if (!ok) return false;

+            ok = ok &amp;&amp; src0_d.is_dense(true) &amp;&amp; src1_d.is_dense(true);
+            if (!ok) return false;
+
             if (src0_d.is_plain() &amp;&amp; src1_d.is_plain()) {
                 const auto &amp;bd0 = src0_d.blocking_desc();
                 const auto &amp;bd1 = src1_d.blocking_desc();
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='pinzhenx' date='2020-05-19T05:56:33Z'>
		This patch does work well.
Another issue, if the src0 is a strided tensor, and dst is of format any, the queried dst is indeed a dense tensor, but the result was still incorrect.
void test_binary_dst_any() {
  float src0_buf[12] = {100, 200, 300, 400, 500, 600, 700, 800, 900, 100, 1100, 1200};
  float src1_buf[32] = {1,  2,  3,  4,  5,  6,  7,  8,  9,  10, 11, 12, 13, 14, 15, 16,
                        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32};
  float dst_buf[12] = {0};

  memory src0({{2, 4}, dt::f32, {4, 1}}, eng, src0_buf);
  memory src1({{2, 4}, dt::f32, {16, 4}}, eng, src1_buf);
  std::swap(src0, src1);    // &lt;--- now src0 is non-contiguous, src1 is dense

  memory::desc dst_desc{{2, 4}, dt::f32, tag::any};

  auto pd = binary::primitive_desc({algorithm::binary_add, src0.get_desc(), src1.get_desc(), dst_desc}, eng);
  memory dst(pd.dst_desc(), eng, dst_buf);
  binary(pd).execute(strm, {{DNNL_ARG_SRC_0, src0}, {DNNL_ARG_SRC_1, src1}, {DNNL_ARG_DST, dst}});

  dump(src0);
  dump(src1);
  dump(dst); // 101, 205, 309, 413, 517, 621, 725, 829
}


dnnl_verbose,info,oneDNN v1.4.0 (commit d310bfe968d4b5165511a68201a71fd995c0d321)
dnnl_verbose,info,cpu,runtime:OpenMP
dnnl_verbose,info,cpu,isa:Intel AVX-512 with AVX512BW, AVX512VL, and AVX512DQ extensions
dnnl_verbose,info,gpu,runtime:none
dnnl_verbose,exec,cpu,binary,ref:any,undef,src_f32::blocked:ab:f0 src_f32::blocked:ab:f0 dst_f32::blocked:ab:f0,,alg:binary_add,2x4:2x4 2x4,6.54688
[1125, 2, 3, 4, 517, 6, 7, 8, 621, 10, 11, 12, 725, 14, 15, 16, 829, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]
[100, 200, 300, 400, 500, 600, 700, 800]
[101, 0, 0, 0, 205, 0, 0, 0]
		</comment>
		<comment id='4' author='pinzhenx' date='2020-05-19T07:22:50Z'>
		
But I want to confirm with you that you are indeed fine with the fallback to the reference generic implementations, that are known to be extremely slow.

Well, that won't do any good for us. I believe we should continue to use the old way of converting it back to dense first.
Thanks for your explanation.
		</comment>
		<comment id='5' author='pinzhenx' date='2020-05-19T19:21:55Z'>
		Hi &lt;denchmark-link:https://github.com/pinzhenx&gt;@pinzhenx&lt;/denchmark-link&gt;
, could you, please, try yet another fix up patch?
&lt;denchmark-code&gt;diff --git a/src/cpu/ref_binary.cpp b/src/cpu/ref_binary.cpp
index 02337be5a..ae5dcefb0 100644
--- a/src/cpu/ref_binary.cpp
+++ b/src/cpu/ref_binary.cpp
@@ -96,6 +96,7 @@ void ref_binary_t&lt;src0_type, src1_type, dst_type&gt;::execute_ref(

     const memory_desc_wrapper src0_d(pd()-&gt;src_md(0));
     const memory_desc_wrapper src1_d(pd()-&gt;src_md(1));
+    const memory_desc_wrapper dst_d(pd()-&gt;dst_md());

     const auto alg = pd()-&gt;desc()-&gt;alg_kind;

@@ -145,7 +146,8 @@ void ref_binary_t&lt;src0_type, src1_type, dst_type&gt;::execute_ref(
     parallel_nd(nelems_A, [&amp;](dim_t i) {
         auto off_A = src0_d.off_l(i);
         auto off_B = is_tensor_op ? src1_d.off_l(i) : map_idx_B(i);
-        perform_op(&amp;dst[off_A], src0[off_A], src1[off_B], params);
+        auto off_C = dst_d.off_l(i);
+        perform_op(&amp;dst[off_C], src0[off_A], src1[off_B], params);
     });
 }
&lt;/denchmark-code&gt;

We've added an item to revisit our testing to better cover strided memory descriptors.
		</comment>
		<comment id='6' author='pinzhenx' date='2020-07-01T16:00:36Z'>
		&lt;denchmark-link:https://github.com/dzarukin&gt;@dzarukin&lt;/denchmark-link&gt;
 same issue in sum
		</comment>
		<comment id='7' author='pinzhenx' date='2020-07-01T18:30:52Z'>
		Hi &lt;denchmark-link:https://github.com/pinzhenx&gt;@pinzhenx&lt;/denchmark-link&gt;
,
Since it's somewhat same but a bit different issue, may I ask you to create another tracker for sum?
And I would really much appreciate a reproducer due to reference sum is reorder based and the issue may hide under the different stone.
Thanks.
		</comment>
		<comment id='8' author='pinzhenx' date='2020-07-02T01:55:40Z'>
		Hi &lt;denchmark-link:https://github.com/dzarukin&gt;@dzarukin&lt;/denchmark-link&gt;
 , thanks for you explanation. Now it seems not to be an issue to me but a kind of UB.
Given two strided src tensors and without a dst desc, it's reasonable for dnnl to choose a strided format for dst, however it's not the behavior of binary. As long as we fixed the dst to dense, sum works well, but that requires us to check density outside. Do you think users should do like that?
&lt;denchmark-code&gt;#include &lt;stdio.h&gt;
#include "dnnl.hpp"

using namespace dnnl;
using tag = memory::format_tag;
using dt = memory::data_type;

engine eng(engine::kind::cpu, 0);

void dump_desc(const memory::desc&amp; mdesc) {
  auto md = mdesc.data;
  printf("\t\t");
  for (int i = 0; i &lt; md.ndims; i++)
    printf("%c\t", 'A' + i);
  printf("\ndims\t\t");
  for (int i = 0; i &lt; md.ndims; i++)
    printf("%ld\t", md.dims[i]);
  printf("\npadded_dims\t");
  for (int i = 0; i &lt; md.ndims; i++)
    printf("%ld\t", md.padded_dims[i]);
  printf("\nstrides\t\t");
  for (int i = 0; i &lt; md.ndims; i++)
    printf("%ld\t", md.format_desc.blocking.strides[i]);
  printf("\n\ninner_idxs\t");
  for (int i = 0; i &lt; md.format_desc.blocking.inner_nblks; i++)
    printf("%ld\t", md.format_desc.blocking.inner_idxs[i]);
  printf("\ninner_blks\t");
  for (int i = 0; i &lt; md.format_desc.blocking.inner_nblks; i++)
    printf("%ld\t", md.format_desc.blocking.inner_blks[i]);
  printf("\n");
}

int main() {
  auto pd = sum::primitive_desc({1.0, 1.0}, {
    {{4, 4}, dt::f32, {100, 10}},  // src0 desc
    {{4, 4}, dt::f32, {100, 10}},  // src1 desc
  }, eng);
  dump_desc(pd.dst_desc());
  return 0;
}
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;                A       B
dims            4       4
padded_dims     4       4
strides         100     10   &lt;-- dst is still a strided tensor

inner_idxs
inner_blks
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='pinzhenx' date='2020-07-02T02:13:52Z'>
		When it comes to other primitives eltwise, there's no way to let it "strided in dense out". So we should do a reorder to make it contiguous, is it correct?
		</comment>
	</comments>
</bug>