<bug id='535' author='eladdidy' open_date='2019-08-14T18:37:32Z' closed_time='2019-08-30T20:08:51Z'>
	<summary>inner product low performance with s8 in compared to f32</summary>
	<description>
I have simple test model with 3 FC, 2 BN + Relu
data type F32 test takes: ~2500 us
data type S8 test takes: ~14000
I would expect INT8 to preform better, can someone explain please?
Attaching code - Just fix the MKLDNNROOT in Make file
Thanks
Elad
&lt;denchmark-link:https://github.com/intel/mkl-dnn/files/3502750/mkl_3fc_test.zip&gt;mkl_3fc_test.zip&lt;/denchmark-link&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CPU: Xeon(R) Platinum 8176 @2.1Ghz
OS: SLES12SP2 4.4.41-11-rt (RT extension)
Compiler:  gcc 4.8.5
mkldnn release 1.0.0

	</description>
	<comments>
		<comment id='1' author='eladdidy' date='2019-08-14T18:55:45Z'>
		Please note that my mkldnn was build with debug, same phenomena with release
		</comment>
		<comment id='2' author='eladdidy' date='2019-08-14T19:15:35Z'>
		Hi &lt;denchmark-link:https://github.com/eladdidy&gt;@eladdidy&lt;/denchmark-link&gt;
,
I check the verbose and see the following (run on Skylake P8180 in release mode):



Primitive
Problem
f32 time (ms)
int8 time (ms)




FC
mb1ic192oc600
0.021
0.014


BatchNorm
mb1ic600
0.012
0.023


FC
mb1ic600oc600
0.056
0.042


BatchNorm
mb1ic600
0.012
0.023


FC
mb1ic600oc256
0.027
0.019



So the INT8 FC is ~1.4x faster than f32 counterpart. That is exactly what is expected on Skylake.
Now the batch norm is a problem. We run somewhat optimized version for f32 (ncsp_bnorm:any), but the reference implementation for int8. That spoils the results.
In general, the assumption is that for inference you don't need batch norm at all, you can embed it to the weights of the previous layer. This is also helpful because you wouldn't lose an accuracy due to type conversion (i.e. the batch-normalization would essentially be done in f32 and not if int8).
Do you think this approach can work for you?
		</comment>
		<comment id='3' author='eladdidy' date='2019-08-14T19:23:10Z'>
		P.S. If the time ratios you see in your (release) setup don't match mine ones, could you please post the verbose output that you have?
		</comment>
		<comment id='4' author='eladdidy' date='2019-08-14T20:10:25Z'>
		Actually, my measurements were done in slightly incorrect environment (my Intel MKL-DNN library was built with Intel MKL, which is mostly for debug purposes). So please disregard the table above. The updated table is:



Primitive
Problem
f32 time (ms)
s8s8 time (ms)
u8s8 time (ms)




FC
mb1ic192oc600
0.009
0.015
0.003


BatchNorm
mb1ic600
0.013
0.023
0.024


FC
mb1ic600oc600
0.046
0.042
0.009


BatchNorm
mb1ic600
0.012
0.025
0.023


FC
mb1ic600oc256
0.019
0.020
0.004


Test FC Time
(program output)
3692
2863
1635




Note s8s8 is the same as original int8 code

So few observations here:

Nothing changes wrt Batch Norm. We will soon promote the implementation for int8 which should improve the situation a bit. But in general, the recommendation is still to try avoiding the BatchNorm altogether in inference.
In addition to s8s8 FC I added u8s8 FC: in the code, I replaced the source data type from s8 to u8. The reason is that Skylake natively supports u8 * s8 --&gt; s32, while s8 * s8 --&gt; s32 should be emulated. The emulation comes with some overhead. For big matrices the overhead is typically negligible, but for small sizes it might be significant. I will need to dig into that more to understand whether this is a bug on our side, or we can do almost nothing to overcome this overhead for these particular sizes (cast @aaraujom in case he has some insights).
Forgetting for a moment the difference between s8s8 and u8s8 you can see that FC for u8s8 is much faster than f32. Theoretical peak says the the difference should be ~1.33x, but here the difference is ~5x. I think that is because for u8s8 case the data fit into L2: ~600 KB of data and 1 MB of L2 cache. For f32 the data takes around 2.5 MB which exceeds the L2 cache.

If we fix s8s8 and will get rid of BatchNorm the speed up of using int8 will be almost 4.6x over f32.



Anyways, I take an AR to see why s8s8 is way worse than u8s8.
		</comment>
		<comment id='5' author='eladdidy' date='2019-08-15T06:58:18Z'>
		Hi,
I'm good with fusing BN into FC and i'll have it by next week.
I'm looking up for the s8s8 fix and will be happy to support dev efforts
Thanks
Elad
		</comment>
	</comments>
</bug>