<bug id='812' author='dcferreira' open_date='2020-06-17T16:41:47Z' closed_time='2020-06-18T07:07:04Z'>
	<summary>--enable-microbatch breaks in text files with "," in it</summary>
	<description>
Describe the bug
When microbatch is enabled, a JSON input is (badly) converted to CSV, and a dataframe is generated from this CSV (I couldn't find any option to avoid this conversion, please let me know if such an option exists).
Therefore, when a text field has the separator character (,) in it, it breaks. I suspect other characters might also break it (", \n).
I'm not 100% sure I'm using this as intended from the client side, so please advise if that is the case.
To Reproduce
Steps to reproduce the behavior:

Make a model, using the following code:

import bentoml
from bentoml.adapters import DataframeInput, DataframeOutput


@bentoml.env(auto_pip_dependencies=True)
class TestSizer(bentoml.BentoService):
    @bentoml.api(input=DataframeInput(input_dtypes={"text": "str"}),
                 output=DataframeOutput())
    def size(self, df):
        if 'text' not in df:
            raise ValueError('The Dataframe needs a column named "text".')
        return df.text.apply(len).to_frame()


if __name__ == '__main__':
    sizer = TestSizer()
    sizer.save()


Launch the server with bentoml serve TestSizer:latest --enable-microbatch.


Use the following script to send some requests:


import pandas as pd
import requests


class TextClient:
    def __init__(self, server_url: str):
        self.server_url = server_url

    def size(self, df: pd.DataFrame) -&gt; pd.DataFrame:
        if 'text' not in df:
            raise ValueError('The given Dataframe requires a column named "html".')
        resp = requests.post(self.server_url, headers={'Content-Type': 'application/json'},
                             data=df.to_json())
        if resp.status_code != 200:
            raise ValueError('Bad reply!')
        return pd.DataFrame(data=resp.json(), index=df.index)


if __name__ == '__main__':
    client = TextClient('http://127.0.0.1:5000/size')
    ex1 = pd.DataFrame({"text": ["this is a text", "and another one"]}, index=["one", "two"])
    ex2 = pd.DataFrame({"text": ["this, is, a, text", "and yet another one"]}, index=["one", "two"])

    print(client.size(ex1))
    print(client.size(ex2))
As described, ex1 is sent fine to the server, and a correct answer comes back; but for ex2 the server breaks with IndexError: list index out of range while trying to parse CSV into a DataFrame.
If the server is running without --enable-microbatch, everything works fine.
Full error message:
&lt;denchmark-code&gt;[2020-06-17 18:29:02,374] ERROR - Exception on /size [POST]
Traceback (most recent call last):
  File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/bentoml/server/bento_api_server.py", line 265, in api_func
    response_body = api.handle_batch_request(request)
  File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/bentoml/service.py", line 132, in handle_batch_request
    responses = self.handler.handle_batch_request(requests, self.func)
  File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/bentoml/adapters/dataframe_input.py", line 298, in handle_batch_request
    datas, content_types
  File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/bentoml/adapters/dataframe_input.py", line 148, in read_dataframes_from_json_n_csv
    df_merged = pd.read_csv(StringIO(df_str_csv), index_col=0)
  File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/pandas/io/parsers.py", line 676, in parser_f
    return _read(filepath_or_buffer, kwds)
  File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/pandas/io/parsers.py", line 454, in _read
    data = parser.read(nrows)
  File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/pandas/io/parsers.py", line 1133, in read
    ret = self._engine.read(nrows)
  File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/pandas/io/parsers.py", line 2078, in read
    values = data.pop(self.index_col[i])
IndexError: list index out of range
127.0.0.1 - - [17/Jun/2020 18:29:02] "POST /size HTTP/1.1" 500 -
[2020-06-17 18:29:02,377] ERROR - Traceback (most recent call last):
  File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/bentoml/marshal/marshal.py", line 207, in request_dispatcher
    resp = await self.batch_handlers[api_name](req)
  File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/bentoml/marshal/dispatcher.py", line 144, in _func
    raise r
  File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/bentoml/marshal/dispatcher.py", line 204, in outbound_call
    outputs = await self.callback(tuple(d for _, d, _ in inputs_info))
  File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/bentoml/marshal/marshal.py", line 110, in _batch_handler_template
    return await func(requests, api_name)
  File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/bentoml/marshal/marshal.py", line 269, in _batch_handler_template
    payload=SimpleResponse(resp.status, resp.headers, raw),
bentoml.exceptions.RemoteException: Bad response status from model server:
500
b'An error has occurred in BentoML user code when handling this request, find the error details in server logs'
&lt;/denchmark-code&gt;

Environment:

OS: Linux
Python Version: 3.7
BentoML Version: tried with 0.8.1 (stable) and also with current master branch

	</description>
	<comments>
		<comment id='1' author='dcferreira' date='2020-06-17T20:08:56Z'>
		&lt;denchmark-link:https://github.com/bojiang&gt;@bojiang&lt;/denchmark-link&gt;
 it probably makes more sense to separate batching JSON and CSV requests?
I think it is ok to add a parameter to DataframeInput to specify the content type to only JSON or CSV if that makes more sense for micro batching implementation, e.g.:
&lt;denchmark-code&gt;@api(input=DataframeInput(http_content_type='CSV'))
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='dcferreira' date='2020-06-18T08:43:28Z'>
		&lt;denchmark-link:https://github.com/dcferreira&gt;@dcferreira&lt;/denchmark-link&gt;
 Thanks for your feedback!

Fixed in PR #814
More cases added to the test of DataframeInput https://github.com/bentoml/BentoML/blob/master/tests/handlers/test_dataframe_handler.py#L153
As I mentioned in doc strings, the conversion before pandas.read_csv/read_json is necessary because it is N(batch size) times faster than an iteration of a batch of JSON inputs with pandas.read_csv. It helps a lot on throughput.

		</comment>
	</comments>
</bug>