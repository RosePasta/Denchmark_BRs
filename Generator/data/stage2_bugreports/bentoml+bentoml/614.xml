<bug id='614' author='smolendawid' open_date='2020-04-29T14:04:18Z' closed_time='2020-05-02T20:16:01Z'>
	<summary>AWS Lambda Deployment: Build function size is over 700MB, max size capable reached</summary>
	<description>
Describe the bug
I'm trying to deploy my model to AWS Lambda.

Failed to create AWS Lambda deployment test-deploy INTERNAL:Build function size is over 700MB, max size capable for AWS Lambda function

The docker that is created in huge though. When I'm investigating it, I can see that /opt/conda is more than 2.3 GB.
My pip dependencies look this way:
&lt;denchmark-code&gt;
pip_dependencies = [
    'bentoml==0.7.3',
    'setuptools',
    'imageio',
    'https://download.pytorch.org/whl/cpu/torch-1.5.0%2Bcpu-cp37-cp37m-linux_x86_64.whl',
    'opencv-python==4.2.0.32',
    'opencv-python-headless==4.2.0.32',
    'pillow==7.0.0',
    'numpy==1.18.1',
    'scikit-image==0.14.2',
    'scipy==1.1.0',
    'lmdb',
    'lxml',
    'natsort',
    'torchvision',
]
&lt;/denchmark-code&gt;

Is there any way I can make docker smaller? Is conda necessary? My PyTorch models in total take ~100mb of disk space. They are saved as torch states, not exported to binary models.
	</description>
	<comments>
		<comment id='1' author='smolendawid' date='2020-04-29T15:12:14Z'>
		I hope it's not a problem I labeled it as a bug but it's neither a feature request, just a request for help.
I can see that PyTorch and some libraries it installs are pretty big. Does it mean I should export it with jit and then load it again in Python somehow?
		</comment>
		<comment id='2' author='smolendawid' date='2020-04-29T15:52:51Z'>
		hi &lt;denchmark-link:https://github.com/smolendawid&gt;@smolendawid&lt;/denchmark-link&gt;
, BentoML's lambda deployment actually does not really deploy the container itself, instead, it runs a docker container that's similar to the AWS Lambda environment, installs all the pip packages and saves all the installed package files to disk. And the docker itself is not used for deployment with Lambda, only the saved PyPI package files are uploaded. BentoML is already trying everything it can to save space and utilize all AWS  Lambda's capacity at the moment. But the model files + all pip dependencies still must be under 700MB.
I will try on my end to see if the dependencies combo you provided can fit within the limit.
		</comment>
		<comment id='3' author='smolendawid' date='2020-04-29T16:24:06Z'>
		&lt;denchmark-link:https://github.com/parano&gt;@parano&lt;/denchmark-link&gt;
 thank you for the support.
My virtualenv grows for more than 250 MB after just pip install 'bentoml==0.7.3'
I am using macOS Catalina 10.15.4 and Python 3.7
		</comment>
		<comment id='4' author='smolendawid' date='2020-04-29T17:09:17Z'>
		&lt;denchmark-link:https://github.com/smolendawid&gt;@smolendawid&lt;/denchmark-link&gt;
 yes, BentoML has a few dependencies that are quite large, such as numpy, pandas. Here are all dependencies installed after installing bentoml:
&lt;denchmark-link:https://user-images.githubusercontent.com/489344/80623086-8d16c680-89fe-11ea-989f-7a5b59b1cacc.png&gt;&lt;/denchmark-link&gt;

We should be able to make pandas an optional dependency and only require it if user uses DataframeHandler. That will save some extra space here.
botocore is already removed in the lambda function when being uploaded by BentoML, because it already comes with the AWS Lambda's python environment.
It looks like even if we just include your model, pytorch, imageio and bentoml, the dependency files will still exceed the limit of AWS Lambda.
You can try letting your API client covert the input image to a NumPy array that your model is expecting, and send the input data via a JSON array instead of an image file. That way I think you won't need imageio, scikit-image on the server-side.
You can also take a look at other deployment options that BentoMl provides: &lt;denchmark-link:https://docs.bentoml.org/en/latest/deployment/index.html&gt;https://docs.bentoml.org/en/latest/deployment/index.html&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='smolendawid' date='2020-04-30T13:52:15Z'>
		And the limit 700MB for Lambda AWS, it's the Lambda limit? I can't found it in the docs anywhere... do you know if it can be configured?
I have some operations required especially using cv2 so I can't really get rid of my deps...
		</comment>
		<comment id='6' author='smolendawid' date='2020-04-30T13:53:00Z'>
		I'm not sure but seems like max Lambda storage is 75 GB:
&lt;denchmark-link:https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html&gt;https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='smolendawid' date='2020-04-30T14:14:32Z'>
		&lt;denchmark-link:https://github.com/smolendawid&gt;@smolendawid&lt;/denchmark-link&gt;
 yes it is an AWS Lambda limit. 75GB is the size of all deployment packages that can be uploaded per AWS region.
For individual lambda deployment, the size limit is the "Deployment package size" on that documentation page, which is 250MB including 5 layers. BentoML automatically setup 5 layers to get the extra storage provided by layers and it also gets an extra 500MB by utilizing the 500MB space in the "/tmp" directory. (BentoML uploads the extra 500MB package size to s3 and downloads them when the function is loaded on Lambda). This gives in a total of 750MB function size in theory, but in practice, the actual storage available in Lambda is just a little over 700MB, and that's why BentoML caps at 700MB.
As far as I know, AWS does not allow its users to increase "The deployment package size", it can not be configured.
		</comment>
		<comment id='8' author='smolendawid' date='2020-05-02T20:16:01Z'>
		I see. Thank you for your support. PyTorch + some image processing library isn't too much in my opinion and often one may need a lot more packages, which I think makes Lambda deployment not so useable.
		</comment>
		<comment id='9' author='smolendawid' date='2020-05-04T04:55:39Z'>
		&lt;denchmark-link:https://github.com/smolendawid&gt;@smolendawid&lt;/denchmark-link&gt;
, indeed AWS lambda is not really suitable for your use case and our documentation should be more clear about those AWS limitations upfront. we are working on documentation that compares all the different deployment platforms, including their strength, cost &amp; limitations. I hope that will be helpful for new BentoML users when they started picking a model serving platform. Thanks for the questions and feedback!
		</comment>
	</comments>
</bug>