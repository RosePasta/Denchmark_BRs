<bug id='1234' author='EricPreventa' open_date='2020-11-09T17:40:58Z' closed_time='2020-11-26T02:55:06Z'>
	<summary>Long running Keras Inference causing timeout, possibly due to not using GEvent</summary>
	<description>
Describe the bug
Running a very deep Keras Inference causes the server to timeout and stop. I process thousands of patches and watching the logs, it's working just fine. RAM is not an issue either. I believe the issue is due to the worker thread is being tied up and not able to communicate, thus gunicorn is killing (and restarting) the container. BentoML uses gEvent for Sagemaker however a little surprised it's not deployed elsewhere. Running this either on GKE or local Docker both encounter the same issue, however it is resolved if the predict step is significantly faster.
Viewing the code of tensorflow serving (shown below), looks like they're also using gevent and this has been spoken about in some of the github issues for tensorflow.
"  options = {
"bind": ":8010",
"timeout": 600,
"workers": 4,
"reload": True,
"spew": True,
"worker_class": "gevent",
}"
Environment:

OS: MACOS 10.15.7
Python Version 3.6.12
BentoML Version 0.9.2

	</description>
	<comments>
		<comment id='1' author='EricPreventa' date='2020-11-09T17:53:33Z'>
		Hi &lt;denchmark-link:https://github.com/EricPreventa&gt;@EricPreventa&lt;/denchmark-link&gt;
  - BentoML's SageMaker deployment implementation is inconsistent with other deployments(Docker, GKE, etc). And we do plan to fix that.  When you test it with local docker, 1) did you enable micro-batching with , 2) have you tried adding a larger timeout value via the CLI argument ? I believe the default timeout is 60 second.
e.g.:
&lt;denchmark-code&gt;bentoml serve-gunicorn YouKerasServiceName:Version --enable-microbatch --timeout=180

docker run -p 5000:5000 your-docker-image-built:latest --enable-microbatch --timeout=180
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='EricPreventa' date='2020-11-09T17:56:03Z'>
		Thank &lt;denchmark-link:https://github.com/parano&gt;@parano&lt;/denchmark-link&gt;
 I'll try that now.I have tried microbatch with and without. Didn't appear to make a difference. But will try extending the timeout with microbatch and see what happens.
Update: Upping the timeout does help, but appears to be just putting a bandaid on a problem instead of really solving it. Not sure how this can be corrected in docker for deployment either. Is there a docker timeout we can set? Wouldn't setting gevent help resolve this issue?
How would I deploy this timeout "hack" to Docker/GKE? Is there any timeout environment variables I can set to enable it inside the container?
		</comment>
		<comment id='3' author='EricPreventa' date='2020-11-09T18:08:16Z'>
		&lt;denchmark-link:https://github.com/ericmand&gt;@ericmand&lt;/denchmark-link&gt;
 did your BentoService code implemented with ? if you did, and your API callback function processes a list of input at a time,  should drastically improve your service's throughput.
You can use the same CLI option when deployed with docker and GKE, as the example I showed above:
&lt;denchmark-code&gt;docker run -p 5000:5000 your-docker-image-built:latest --enable-microbatch --timeout=180
&lt;/denchmark-code&gt;

You can also set the default timeout via an environment variable, e.g.:
&lt;denchmark-code&gt;export BENTOML__API_SERVER__DEFAULT_TIMEOUT=180
&lt;/denchmark-code&gt;

It's not a hack.. just an option for user to config
		</comment>
		<comment id='4' author='EricPreventa' date='2020-11-09T18:15:33Z'>
		
@ericmand did your BentoService code implemented with batch=True? if you did, and your API callback function processes a list of input at a time, --enable-microbatch should drastically improve your service's throughput.
You can use the same CLI option when deployed with docker and GKE, as the example I showed above:
docker run -p 5000:5000 your-docker-image-built:latest --enable-microbatch --timeout=180

You can also set the default timeout via an environment variable, e.g.:
export BENTOML__API_SERVER__DEFAULT_TIMEOUT=180

It's not a hack.. just an option for user to config

The reason why I believe batch=True will not work for me, is because my input is truly only one input field (500x500 image). Inside my predict function I create thousands of patches there and run it through Keras predict.
The reason why I say it's a hack is only because I worry if I set the timeout to be extremely high (this could take an hour to fully finish), what happens if the program truly does crash and needs to be timed out (for deployment/cost savings)? It'll have to wait for that extremely long period of time for it to truly restart.
Tensorflow is using gevent I believe for this same reasoning, then again I'm no expert in servers, my speciality is in machine learning.
		</comment>
		<comment id='5' author='EricPreventa' date='2020-11-09T18:23:11Z'>
		
The reason why I believe batch=True will not work for me, is because my input is truly only one input field (500x500 image). Inside my predict function I create thousands of patches there and run it through Keras predict.

I see, if your predict function already runs vectorized inference then there is no need for using BentoML's micro-batching

The reason why I say it's a hack is only because I worry if I set the timeout to be extremely high (this could take an hour to fully finish), what happens if the program truly does crash and needs to be timed out (for deployment/cost savings)? It'll have to wait for that extremely long period of time for it to truly restart.
Tensorflow is using gevent I believe for this same reasoning, then again I'm no expert in servers, my speciality is in machine learning.

If your prediction function actually runs for an hour on average, then there is really no workaround for it. You should set the value to slightly higher than the average prediction time you need - a proper way is to test the distribution of how long does it take to run your prediction function on your deployment target hardware and then set the timeout accordingly.
		</comment>
		<comment id='6' author='EricPreventa' date='2020-11-09T18:25:35Z'>
		&lt;denchmark-link:https://github.com/parano&gt;@parano&lt;/denchmark-link&gt;
 Thanks for your help. Will set the ENV variable and see if I can get it to run on GKE. I'll keep a look out for trying to find the best approach instead of using a timeout, but for now a timeout should work. What's the preferred place to set the ENV? I've been doing it in the dockerfile. Best practice?
		</comment>
		<comment id='7' author='EricPreventa' date='2020-11-09T18:28:10Z'>
		I believe GKE provides API and UI to set environment variables for your deployment: &lt;denchmark-link:https://cloud.google.com/run/docs/configuring/environment-variables#yaml&gt;https://cloud.google.com/run/docs/configuring/environment-variables#yaml&lt;/denchmark-link&gt;
, I would not recommend doing it via dockerfile.
And I do think setting the timeout is the right solution for this. Happy to discuss more why you don't think it's ideal.
		</comment>
		<comment id='8' author='EricPreventa' date='2020-11-10T05:35:53Z'>
		&lt;denchmark-link:https://github.com/parano&gt;@parano&lt;/denchmark-link&gt;
 Thanks, we can discuss potentially a better solution as I continue to learn more about this path.
The "--timeout" option did work great, however the "export BENTOML__API_SERVER__DEFAULT_TIMEOUT=180" did not. Is there anything else for me to try to set that? I (don't think) there's anyways for me to set the --timeout on my docker once it's deployed on Google Cloud.
		</comment>
		<comment id='9' author='EricPreventa' date='2020-11-12T02:53:19Z'>
		Hi &lt;denchmark-link:https://github.com/ericmand&gt;@ericmand&lt;/denchmark-link&gt;
 - my bad, the correct environment variable name should be .
		</comment>
	</comments>
</bug>