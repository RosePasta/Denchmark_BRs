<bug id='1306' author='xylophone21' open_date='2020-12-05T09:16:39Z' closed_time='2020-12-24T03:06:41Z'>
	<summary>--enable-microbatch got pickle error</summary>
	<description>
Describe the bug
Run demo with --enable-microbatch
serve PyTorchFashionClassifier:latest --enable-microbatch
To Reproduce
serve PyTorchFashionClassifier:latest --enable-microbatch
Expected behavior
run with microbatch
Screenshots/Logs
[2020-12-05 17:18:06,106] DEBUG - Creating local YataiService instance
[2020-12-05 17:18:06,401] DEBUG - Upgrading tables to the latest revision
[2020-12-05 17:18:06,427] INFO - Getting latest version PyTorchFashionClassifier:20201204180721_6A5E98
[2020-12-05 17:18:06,427] INFO - Starting BentoML API server in development mode..
[2020-12-05 17:18:07,866] DEBUG - HTTPSConnectionPool(host='api.amplitude.com', port=443): Max retries exceeded with url: /httpapi (Caused by ConnectTimeoutError(&lt;urllib3.connection.HTTPSConnection object at 0x7fc91aac7100&gt;, 'Connection to api.amplitude.com timed out. (connect timeout=1)'))
[2020-12-05 17:18:09,371] DEBUG - Using BentoML default docker base image 'bentoml/model-server:0.9.2-py38'
[2020-12-05 17:18:09,640] WARNING - BentoML by default does not include spacy and torchvision package when using PytorchModelArtifact. To make sure BentoML bundle those packages if they are required for your model, either import those packages in BentoService definition file or manually add them via @env(pip_packages=['torchvision']) when defining a BentoService
[2020-12-05 17:18:09,642] WARNING - pip package requirement torch already exist
[2020-12-05 17:18:10,654] DEBUG - HTTPSConnectionPool(host='api.amplitude.com', port=443): Max retries exceeded with url: /httpapi (Caused by ConnectTimeoutError(&lt;urllib3.connection.HTTPSConnection object at 0x7fc91cf88fd0&gt;, 'Connection to api.amplitude.com timed out. (connect timeout=1)'))
[2020-12-05 17:18:10,726] INFO - Micro batch enabled for API predict
[2020-12-05 17:18:10,726] INFO - Your system nofile limit is 10240, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.
[2020-12-05 17:18:11,765] DEBUG - HTTPSConnectionPool(host='api.amplitude.com', port=443): Max retries exceeded with url: /httpapi (Caused by ConnectTimeoutError(&lt;urllib3.connection.HTTPSConnection object at 0x7fc91e8d87f0&gt;, 'Connection to api.amplitude.com timed out. (connect timeout=1)'))
Traceback (most recent call last):
File "/Users/lihui/Code/dubhe/bentomlTest/server.py", line 7, in 
bentoml.cli.cli()
File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/click/core.py", line 829, in call
return self.main(*args, **kwargs)
File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/click/core.py", line 782, in main
rv = self.invoke(ctx)
File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/click/core.py", line 1259, in invoke
return _process_result(sub_ctx.command.invoke(sub_ctx))
File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/click/core.py", line 1066, in invoke
return ctx.invoke(self.callback, **ctx.params)
File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/click/core.py", line 610, in invoke
return callback(*args, **kwargs)
File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/bentoml/cli/click_utils.py", line 138, in wrapper
return func(*args, **kwargs)
File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/bentoml/cli/click_utils.py", line 115, in wrapper
return_value = func(*args, **kwargs)
File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/bentoml/cli/click_utils.py", line 99, in wrapper
return func(*args, **kwargs)
File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/bentoml/cli/bento_service.py", line 243, in serve
start_dev_server(saved_bundle_path, port, enable_microbatch, run_with_ngrok)
File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/bentoml/server/init.py", line 76, in start_dev_server
marshal_server.async_start(port=port)
File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/bentoml/marshal/marshal.py", line 298, in async_start
marshal_proc.start()
File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/multiprocessing/process.py", line 121, in start
self._popen = self._Popen(self)
File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/multiprocessing/context.py", line 224, in _Popen
return _default_context.get_context().Process._Popen(process_obj)
File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/multiprocessing/context.py", line 284, in _Popen
return Popen(process_obj)
File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/multiprocessing/popen_spawn_posix.py", line 32, in init
super().init(process_obj)
File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/multiprocessing/popen_fork.py", line 19, in init
self._launch(process_obj)
File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/multiprocessing/popen_spawn_posix.py", line 47, in _launch
reduction.dump(process_obj, fp)
File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/multiprocessing/reduction.py", line 60, in dump
ForkingPickler(file, protocol).dump(obj)
AttributeError: Can't pickle local object 'metrics_patch.._MarshalService'
Process finished with exit code 1
Environment:

OS: MacOS 10.15.7
Python Version Python 3.8

&lt;denchmark-code&gt;{
 "date": "2020-10-15T09:08:27-0700",
 "dirty": false,
 "error": null,
 "full-revisionid": "25c319d8161629d695eaecf6418e50fea6535d6e",
 "version": "0.9.2"
}
&lt;/denchmark-code&gt;

Additional context
No
	</description>
	<comments>
		<comment id='1' author='xylophone21' date='2020-12-08T08:16:14Z'>
		Maybe related: &lt;denchmark-link:https://github.com/bentoml/BentoML/pull/995&gt;#995&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='xylophone21' date='2020-12-08T09:09:42Z'>
		I reproduced this on macOS with Python3.8. I'll draft a new PR like &lt;denchmark-link:https://github.com/bentoml/BentoML/pull/995&gt;#995&lt;/denchmark-link&gt;
. Before the new release, you can use
&lt;denchmark-code&gt;bentoml serve-gunicorn PyTorchFashionClassifier:latest --enable-microbatch --workers 1
&lt;/denchmark-code&gt;

or Python 3.7 instead.
		</comment>
	</comments>
</bug>