<bug id='1040' author='parano' open_date='2020-08-28T05:45:19Z' closed_time='2020-09-21T18:46:12Z'>
	<summary>Prediction log shows encoded payload when enable micro-batching</summary>
	<description>
Describe the bug
Run BentoML API server with --enable-microbatch, the prediction log currently shows the entire batched request payload instead of individual inference task's input and output.
&lt;denchmark-link:https://user-images.githubusercontent.com/489344/91525526-92a1f180-e8b6-11ea-8858-3453ae93c19a.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/489344/91525549-9cc3f000-e8b6-11ea-84f5-301f46cf270b.png&gt;&lt;/denchmark-link&gt;

This issue will be addressed by the ongoing input adapters refactoring: &lt;denchmark-link:https://github.com/bentoml/BentoML/issues/1002&gt;#1002&lt;/denchmark-link&gt;

Expected behavior
Show 1 prediction log item for each prediction request sent from client
	</description>
	<comments>
		<comment id='1' author='parano' date='2020-09-21T18:46:12Z'>
		resolved
		</comment>
	</comments>
</bug>