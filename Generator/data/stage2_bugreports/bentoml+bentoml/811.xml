<bug id='811' author='jondoering' open_date='2020-06-17T13:18:59Z' closed_time='2020-06-18T05:10:25Z'>
	<summary>--enable-microbatch raises error in 0.8.1 with XGboost example</summary>
	<description>
Describe the bug
BentoML throws an errotr when --enable-microbatch.
The model was build using the XGboost Titanic example &lt;denchmark-link:https://nbviewer.jupyter.org/github/bentoml/gallery/blob/master/xgboost/titanic-survival-prediction/xgboost-titanic-survival-prediction.ipynb&gt;https://nbviewer.jupyter.org/github/bentoml/gallery/blob/master/xgboost/titanic-survival-prediction/xgboost-titanic-survival-prediction.ipynb&lt;/denchmark-link&gt;

Screenshots/Logs
[2020-06-16 20:13:03,530] ERROR - Exception on /predict [POST]
Traceback (most recent call last):
File “/Users/Jonathan/anaconda3/envs/clipper/lib/python3.6/site-packages/bentoml/server/bento_api_server.py”, line 265, in api_func
response_body = api.handle_batch_request(request)
File “/Users/Jonathan/anaconda3/envs/clipper/lib/python3.6/site-packages/bentoml/service.py”, line 122, in handle_batch_request
responses = self.handler.handle_batch_request(requests, self.func)
File “/Users/Jonathan/anaconda3/envs/clipper/lib/python3.6/site-packages/bentoml/adapters/dataframe_input.py”, line 302, in handle_batch_request
result_conc = func(df_conc)
File “/Users/Jonathan/anaconda3/envs/clipper/lib/python3.6/site-packages/bentoml/service.py”, line 101, in _wrapped_func
resp = self._func(*args, **kwargs)
File “/Users/jonathan/bentoml/repository/TitanicSurvivalPredictionXgBoost/20200615214647_8709F8/TitanicSurvivalPredictionXgBoost/xgboost_titanic_bento_service.py”, line 14, in predict
data = xgb.DMatrix(data=df[[‘Pclass’, ‘Age’, ‘Fare’, ‘SibSp’, ‘Parch’]])
File “/Users/Jonathan/anaconda3/envs/clipper/lib/python3.6/site-packages/pandas/core/frame.py”, line 2679, in 
return self._getitem_array(key)
File “/Users/Jonathan/anaconda3/envs/clipper/lib/python3.6/site-packages/pandas/core/frame.py”, line 2723, in _getitem_array
indexer = self.loc._convert_to_indexer(key, axis=1)
File “/Users/Jonathan/anaconda3/envs/clipper/lib/python3.6/site-packages/pandas/core/indexing.py”, line 1327, in _convert_to_indexer
.format(mask=objarr[mask]))
KeyError: “[‘Pclass’ ‘Age’ ‘Fare’ ‘SibSp’ ‘Parch’] not in index”
127.0.0.1 - - [16/Jun/2020 20:13:03] “POST /predict HTTP/1.1" 500 -
[2020-06-16 20:13:03,540] ERROR - Traceback (most recent call last):
File “/Users/Jonathan/anaconda3/envs/clipper/lib/python3.6/site-packages/bentoml/marshal/marshal.py”, line 217, in request_dispatcher
resp = await self.batch_handlers&lt;denchmark-link:req&gt;api_name&lt;/denchmark-link&gt;

File “/Users/Jonathan/anaconda3/envs/clipper/lib/python3.6/site-packages/bentoml/marshal/dispatcher.py”, line 144, in _func
raise r
File “/Users/Jonathan/anaconda3/envs/clipper/lib/python3.6/site-packages/bentoml/marshal/dispatcher.py”, line 204, in outbound_call
outputs = await self.callback(tuple(d for _, d, _ in inputs_info))
File “/Users/Jonathan/anaconda3/envs/clipper/lib/python3.6/site-packages/bentoml/marshal/marshal.py”, line 110, in _batch_handler_template
return await func(requests, api_name)
File “/Users/Jonathan/anaconda3/envs/clipper/lib/python3.6/site-packages/bentoml/marshal/marshal.py”, line 279, in _batch_handler_template
payload=SimpleResponse(resp.status, resp.headers, raw),
bentoml.exceptions.RemoteException: Bad response status from model server:
500
b’An error has occurred in BentoML user code when handling this request, find the error details in server logs’ (bearbeitet)
Environment:

OS: MacOS 10.14.6
Python 3.6.5, BentoML-8.1


Issues was already discussed on slack
&lt;denchmark-link:https://bentoml.slack.com/archives/CKRANBHPH/p1592377225457000&gt;https://bentoml.slack.com/archives/CKRANBHPH/p1592377225457000&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='jondoering' date='2020-06-17T23:56:25Z'>
		&lt;denchmark-link:https://github.com/bojiang&gt;@bojiang&lt;/denchmark-link&gt;

After applying this fix the service now fails with following error:
macbooks-mbp-2:bentoml-dev Jonathan$ bentoml serve-gunicorn '/Users/jonathan/bentoml/repository/TitanicSurvivalPredictionXgBoost/20200616225811_5E4522' --enable-microbatch
[2020-06-18 00:51:57,004] INFO - get_gunicorn_num_of_workers: 5, calculated by cpu count
[2020-06-18 00:51:57,023] INFO - Running micro batch service on :5000
[2020-06-18 00:51:57 +0100] [26954] [INFO] Starting gunicorn 20.0.4
[2020-06-18 00:51:57 +0100] [26954] [INFO] Listening at: &lt;denchmark-link:http://0.0.0.0:64494&gt;http://0.0.0.0:64494&lt;/denchmark-link&gt;
 (26954)
[2020-06-18 00:51:57 +0100] [26954] [INFO] Using worker: sync
[2020-06-18 00:51:57 +0100] [26960] [INFO] Booting worker with pid: 26960
[2020-06-18 00:51:57 +0100] [26958] [INFO] Starting gunicorn 20.0.4
[2020-06-18 00:51:57 +0100] [26962] [INFO] Booting worker with pid: 26962
[2020-06-18 00:51:57 +0100] [26958] [INFO] Listening at: &lt;denchmark-link:http://0.0.0.0:5000&gt;http://0.0.0.0:5000&lt;/denchmark-link&gt;
 (26958)
[2020-06-18 00:51:57 +0100] [26958] [INFO] Using worker: aiohttp.worker.GunicornWebWorker
[2020-06-18 00:51:57 +0100] [26963] [INFO] Booting worker with pid: 26963
[2020-06-18 00:51:57,717] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.8.1,  but loading from BentoML version 0.4.9+314.ge56eace
[2020-06-18 00:51:57 +0100] [26963] [ERROR] Exception in worker process
Traceback (most recent call last):
File "/Users/Jonathan/anaconda3/envs/clipper/lib/python3.6/site-packages/gunicorn/arbiter.py", line 583, in spawn_worker
worker.init_process()
File "/Users/Jonathan/anaconda3/envs/clipper/lib/python3.6/site-packages/aiohttp/worker.py", line 52, in init_process
super().init_process()
File "/Users/Jonathan/anaconda3/envs/clipper/lib/python3.6/site-packages/gunicorn/workers/base.py", line 119, in init_process
self.load_wsgi()
File "/Users/Jonathan/anaconda3/envs/clipper/lib/python3.6/site-packages/gunicorn/workers/base.py", line 144, in load_wsgi
self.wsgi = self.app.wsgi()
File "/Users/Jonathan/anaconda3/envs/clipper/lib/python3.6/site-packages/gunicorn/app/base.py", line 67, in wsgi
self.callable = self.load()
File "/Users/Jonathan/anaconda3/envs/clipper/lib/python3.6/site-packages/BentoML-0.4.9+314.ge56eace-py3.6.egg/bentoml/server/marshal_server.py", line 79, in load
outbound_workers=self.outbound_workers,
File "/Users/Jonathan/anaconda3/envs/clipper/lib/python3.6/site-packages/BentoML-0.4.9+314.ge56eace-py3.6.egg/bentoml/marshal/marshal.py", line 44, in 
super(_MarshalService, self).( will be deprecated after BentoML 1.0, use bentoml.adapters.* instead
[2020-06-18 00:51:58,831] WARNING - DataframeHandler will be deprecated after BentoML 1.0, use DataframeInput instead
[00:51:58] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
[00:51:58] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.
[2020-06-18 00:51:59 +0100] [26962] [INFO] Worker exiting (pid: 26962)
[2020-06-18 00:51:59 +0100] [26960] [INFO] Worker exiting (pid: 26960)
[2020-06-18 00:51:59 +0100] [26954] [INFO] Shutting down: Master
[2020-06-18 00:51:59 +0100] [26954] [INFO] Reason: Worker failed to boot.
My config looks like this:
macbooks-mbp-2:bentoml-dev Jonathan$ bentoml config view
[core]
debug = false
usage_tracking = true
bentoml_deploy_version = {BENTOML_VERSION}
default_docker_base_image = bentoml/model-server:{BENTOML_VERSION}
mb_max_latency = 20
[instrument]
default_namespace = BENTOML
prometheus_multiproc_dir = {BENTOML_HOME}/prometheus_multiproc_dir
[logging]
logging_level = INFO
log_format = [%%(asctime)s] %%(levelname)s - %%(message)s
dev_log_format = [%%(asctime)s] {{%%(filename)s:%%(lineno)d}} %%(levelname)s - %%(message)s
base_log_dir = {BENTOML_HOME}/logs/
log_request_image_files = True
prediction_log_filename = prediction.log
prediction_log_json_format = "%%(service_name)s %%(service_version)s %%(api)s %%(request_id)s %%(request)s %%(response)s %%(asctime)s"
feedback_log_filename = feedback.log
feedback_log_json_format = "%%(service_name)s %%(service_version)s %%(request_id)s %%(asctime)s"
yatai_web_server_log_filename = yatai_web_server.log
[tracing]
zipkin_api_url =
[yatai_service]
url =
s3_signature_version = s3v4
repository_base_url = {BENTOML_HOME}/repository/
db_url = sqlite:///{BENTOML_HOME}/storage.db
s3_endpoint_url =
default_namespace = dev
client_certificate_file =
[apiserver]
default_port = 5000
enable_metrics = true
enable_feedback = true
default_timeout = 60
default_image_input_accept_file_extensions = .jpg,.png,.jpeg,.tiff,.webp,.bmp
default_gunicorn_workers_count = -1
batch_request_header = Bentoml-Is-Batch-Request
[marshal_server]
marshal_request_header_flag = BentoML-Is-Merged-Request
default_max_latency = 300
default_max_batch_size = 2000
[cli]
[yatai]
bento_uri_default_expiration = 3000
[tensorflow]
[pytorch]
Any idea?
		</comment>
		<comment id='2' author='jondoering' date='2020-06-18T05:42:32Z'>
		&lt;denchmark-link:https://github.com/jondoering&gt;@jondoering&lt;/denchmark-link&gt;
 Oh I see. This is a special case if you followed master branch these days.
This happened when loading a bundle saved with released bentoml==0.8.1.
		</comment>
		<comment id='3' author='jondoering' date='2020-06-18T12:49:26Z'>
		&lt;denchmark-link:https://github.com/bojiang&gt;@bojiang&lt;/denchmark-link&gt;
 thanks - what is the suggested fix for that - could not find anything in issues.
		</comment>
		<comment id='4' author='jondoering' date='2020-06-18T13:43:21Z'>
		&lt;denchmark-link:https://github.com/jondoering&gt;@jondoering&lt;/denchmark-link&gt;
 The temporary solution is to save the model again with the master version of bentoml. I'll push a fix before next release.
		</comment>
		<comment id='5' author='jondoering' date='2020-06-18T13:48:18Z'>
		This PR &lt;denchmark-link:https://github.com/bentoml/BentoML/pull/818&gt;#818&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>