<bug id='848' author='dcferreira' open_date='2020-06-26T10:10:05Z' closed_time='2020-06-30T16:40:35Z'>
	<summary>"OSError address already in use" when running "bentoml serve" with --enable-microbatch</summary>
	<description>
Describe the bug
The microbatch functionality is broken with the latest update.
From the error message, I'd guess that some service is bound to port 5000, and the microbatch service also tries to bind itself to that port.
To Reproduce
Steps to reproduce the behavior:

Make any bentoml server.
Run it with --enable-microbatch.

Expected behavior
In v0.8.1, a server is launched, and serves the docs in port 5000.
Requests to the endpoint are processed.
In v0.8.2, a server is launched, the docs are served.
However, requests to the endpoint just return 500 error without any message.
The logs don't even show accesses to the endpoint (though they show accesses to the docs page).
Without --enable-microbatch, everything works correctly.
Screenshots/Logs
Logs when starting server, looking at docs, and trying a simple request (which isn't logged, as explained above):
&lt;denchmark-code&gt;▶ bentoml serve TestSizer:latest --enable-microbatch --debug             
[2020-06-26 12:06:14,843] DEBUG - Setting debug mode: ON for current session
[2020-06-26 12:06:14,873] DEBUG - HTTPSConnectionPool(host='api.amplitude.com', port=443): Max retries exceeded with url: /httpapi (Caused by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x7f2df2e28a10&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))
[2020-06-26 12:06:14,878] DEBUG - Creating local YataiService instance
[2020-06-26 12:06:15,044] DEBUG - Upgrading tables to the latest revision
[2020-06-26 12:06:15,052] INFO - Getting latest version TestSizer:20200626120040_ACEF2A
[2020-06-26 12:06:15,055] DEBUG - HTTPSConnectionPool(host='api.amplitude.com', port=443): Max retries exceeded with url: /httpapi (Caused by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x7f2df1dee590&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))
[2020-06-26 12:06:15,083] DEBUG - HTTPSConnectionPool(host='api.amplitude.com', port=443): Max retries exceeded with url: /httpapi (Caused by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x7f2df4f35290&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))
[2020-06-26 12:06:15,106] INFO - Micro batch enabled for API `size`
[2020-06-26 12:06:15,107] INFO - Your system nofile limit is 1048576, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.
[2020-06-26 12:06:15,114] DEBUG - HTTPSConnectionPool(host='api.amplitude.com', port=443): Max retries exceeded with url: /httpapi (Caused by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x7f2df1cd79d0&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))
[2020-06-26 12:06:15,117] INFO - Running micro batch service on :5000
======== Running on http://0.0.0.0:5000 ========
(Press CTRL+C to quit)
[2020-06-26 12:06:15,121] DEBUG - HTTPSConnectionPool(host='api.amplitude.com', port=443): Max retries exceeded with url: /httpapi (Caused by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x7f2df1cec3d0&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))
 * Tip: There are .env or .flaskenv files present. Do "pip install python-dotenv" to use them.
 * Serving Flask app "TestSizer" (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: on
 * Running on http://127.0.0.1:44513/ (Press CTRL+C to quit)
 * Restarting with stat
[2020-06-26 12:06:16,076] DEBUG - Setting debug mode: ON for current session
[2020-06-26 12:06:16,106] DEBUG - HTTPSConnectionPool(host='api.amplitude.com', port=443): Max retries exceeded with url: /httpapi (Caused by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x7fb8b39321d0&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))
[2020-06-26 12:06:16,111] DEBUG - Creating local YataiService instance
[2020-06-26 12:06:16,268] DEBUG - Upgrading tables to the latest revision
[2020-06-26 12:06:16,276] INFO - Getting latest version TestSizer:20200626120040_ACEF2A
[2020-06-26 12:06:16,279] DEBUG - HTTPSConnectionPool(host='api.amplitude.com', port=443): Max retries exceeded with url: /httpapi (Caused by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x7fb8b28de810&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))
[2020-06-26 12:06:16,310] DEBUG - HTTPSConnectionPool(host='api.amplitude.com', port=443): Max retries exceeded with url: /httpapi (Caused by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x7fb8b28b7050&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))
[2020-06-26 12:06:16,332] INFO - Micro batch enabled for API `size`
[2020-06-26 12:06:16,333] INFO - Your system nofile limit is 1048576, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.
[2020-06-26 12:06:16,341] DEBUG - HTTPSConnectionPool(host='api.amplitude.com', port=443): Max retries exceeded with url: /httpapi (Caused by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x7fb8b27c8a90&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))
[2020-06-26 12:06:16,344] INFO - Running micro batch service on :5000
[2020-06-26 12:06:16,349] DEBUG - HTTPSConnectionPool(host='api.amplitude.com', port=443): Max retries exceeded with url: /httpapi (Caused by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x7fb8b27df490&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))
Process Process-1:
 * Tip: There are .env or .flaskenv files present. Do "pip install python-dotenv" to use them.
Traceback (most recent call last):
  File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
    self.run()
  File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/multiprocessing/process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
  File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/bentoml/marshal/marshal.py", line 301, in fork_start_app
    aiohttp.web.run_app(app, port=port)
  File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/aiohttp/web.py", line 433, in run_app
    reuse_port=reuse_port))
  File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/asyncio/base_events.py", line 583, in run_until_complete
    return future.result()
  File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/aiohttp/web.py", line 359, in _run_app
    await site.start()
  File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/aiohttp/web_runner.py", line 104, in start
    reuse_port=self._reuse_port)
  File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/asyncio/base_events.py", line 1385, in create_server
    % (sa, err.strerror.lower())) from None
OSError: [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): address already in use
 * Debugger is active!
 * Debugger PIN: 335-153-800
127.0.0.1 - - [26/Jun/2020 12:06:16] "GET / HTTP/1.1" 200 -
127.0.0.1 - - [26/Jun/2020 12:06:16] "GET /docs.json HTTP/1.1" 200 -
&lt;/denchmark-code&gt;

Environment:

BentoML version: 0.8.2

Additional context
As far as I can tell, this isn't dependent on my code/service, but a general issue for all services.
I'd guess this would have been caught early with &lt;denchmark-link:https://github.com/bentoml/BentoML/issues/805&gt;#805&lt;/denchmark-link&gt;
 .
	</description>
	<comments>
		<comment id='1' author='dcferreira' date='2020-06-26T17:41:27Z'>
		Hi &lt;denchmark-link:https://github.com/dcferreira&gt;@dcferreira&lt;/denchmark-link&gt;
 - I've noticed this issue too, it only appears when you use  command tho, which is meant for development use only.  The  (soon to be renamed as ) should still work for 
		</comment>
		<comment id='2' author='dcferreira' date='2020-06-26T20:27:19Z'>
		For what it's worth, I first noticed it in the docker container, rather than bentoml serve.
		</comment>
		<comment id='3' author='dcferreira' date='2020-06-26T22:29:06Z'>
		Hi &lt;denchmark-link:https://github.com/dcferreira&gt;@dcferreira&lt;/denchmark-link&gt;
, I tried with the API server docker container but can not reproduce it. Did you change the docker container's  to ?
For example, I have built a docker image called parano/test with the IrisClassifier example in the quick start guide, and with the following docker run, I can not reproduce the issue:
docker run -p 3000:5000 parano/test:latest --debug --enable-microbatch
or the equivalent:
docker run -p 3000:5000 parano/test:latest bentoml serve-gunicorn /bento --debug --enable-microbatch
I can, however, reproduce the issue with bentoml serve command:
docker run -p 3000:5000 parano/test:latest bentoml serve /bento --debug --enable-microbatch
[2020-06-26 22:24:54,380] DEBUG - Setting debug mode: ON for current session
[2020-06-26 22:24:55,555] INFO - Micro batch enabled for API `predict`
[2020-06-26 22:24:55,556] INFO - Your system nofile limit is 1048576, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.
[2020-06-26 22:24:55,756] INFO - Running micro batch service on :5000
 * Serving Flask app "IrisClassifier" (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: on
 * Running on http://127.0.0.1:34101/ (Press CTRL+C to quit)
 * Restarting with stat
[2020-06-26 22:24:57,248] DEBUG - Setting debug mode: ON for current session
[2020-06-26 22:24:58,371] INFO - Micro batch enabled for API `predict`
[2020-06-26 22:24:58,372] INFO - Your system nofile limit is 1048576, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.
[2020-06-26 22:24:58,567] INFO - Running micro batch service on :5000
Process Process-1:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/lib/python3.8/site-packages/bentoml/marshal/marshal.py", line 301, in fork_start_app
    aiohttp.web.run_app(app, port=port)
  File "/opt/conda/lib/python3.8/site-packages/aiohttp/web.py", line 419, in run_app
    loop.run_until_complete(_run_app(app,
  File "/opt/conda/lib/python3.8/asyncio/base_events.py", line 616, in run_until_complete
    return future.result()
  File "/opt/conda/lib/python3.8/site-packages/aiohttp/web.py", line 359, in _run_app
    await site.start()
  File "/opt/conda/lib/python3.8/site-packages/aiohttp/web_runner.py", line 100, in start
    self._server = await loop.create_server(  # type: ignore
  File "/opt/conda/lib/python3.8/asyncio/base_events.py", line 1463, in create_server
    raise OSError(err.errno, 'error while attempting '
OSError: [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): address already in use
 * Debugger is active!
 * Debugger PIN: 814-079-434
^C======== Running on http://0.0.0.0:5000 ========
(Press CTRL+C to quit)
Is that the same case on your end?
		</comment>
		<comment id='4' author='dcferreira' date='2020-06-27T07:49:55Z'>
		I quickly checked it again with my example to make sure, and it doesn't work. I can devote some more time to this on Monday.
But for now, I used the example service in &lt;denchmark-link:https://github.com/bentoml/BentoML/issues/812&gt;#812&lt;/denchmark-link&gt;
, and built the docker image .
&lt;denchmark-code&gt;▶ docker run -p 5000:5000 testserver --debug --enable-microbatch                                  
[2020-06-27 07:42:01,638] DEBUG - Setting debug mode: ON for current session
[2020-06-27 07:42:02,796] INFO - get_gunicorn_num_of_workers: 5, calculated by cpu count
[2020-06-27 07:42:02,832] INFO - Running micro batch service on :5000
[2020-06-27 07:42:03,822] DEBUG - Setting up prometheus_multiproc_dir: /root/bentoml/prometheus_multiproc_dir
[2020-06-27 07:42:03,829] DEBUG - Setting up prometheus_multiproc_dir: /root/bentoml/prometheus_multiproc_dir
[2020-06-27 07:42:03 +0000] [15] [INFO] Starting gunicorn 20.0.4
[2020-06-27 07:42:03 +0000] [1] [INFO] Starting gunicorn 20.0.4
[2020-06-27 07:42:03 +0000] [15] [INFO] Listening at: http://0.0.0.0:5000 (15)
[2020-06-27 07:42:03 +0000] [15] [INFO] Using worker: aiohttp.worker.GunicornWebWorker
[2020-06-27 07:42:03 +0000] [1] [INFO] Listening at: http://0.0.0.0:51123 (1)
[2020-06-27 07:42:03 +0000] [1] [INFO] Using worker: sync
[2020-06-27 07:42:03 +0000] [16] [INFO] Booting worker with pid: 16
[2020-06-27 07:42:03 +0000] [17] [INFO] Booting worker with pid: 17
[2020-06-27 07:42:03 +0000] [18] [INFO] Booting worker with pid: 18
[2020-06-27 07:42:03 +0000] [19] [INFO] Booting worker with pid: 19
[2020-06-27 07:42:03 +0000] [20] [INFO] Booting worker with pid: 20
[2020-06-27 07:42:03,993] INFO - Micro batch enabled for API `size`
[2020-06-27 07:42:03,993] INFO - Your system nofile limit is 1048576, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.
[2020-06-27 07:42:04 +0000] [21] [INFO] Booting worker with pid: 21
&lt;/denchmark-code&gt;

Then access the UI on port 5000, and send a request; a 500 error is output, without any record of it in the logs.
You can also try the client code from the same example, with the same result.
Running without --enable-microbatch works as expected.
		</comment>
		<comment id='5' author='dcferreira' date='2020-06-29T16:40:43Z'>
		Thanks again for reporting this &lt;denchmark-link:https://github.com/dcferreira&gt;@dcferreira&lt;/denchmark-link&gt;

I think there are actually two issues here:

DataframeInput has some issue handling micro-batching requests, which caused the 500 error. I can reproduce the same on my local machine with the quickstart sklearn example. cc @bojiang
Running bentoml serve with --enable-microbatch locally will always fail with the address already in use error

		</comment>
		<comment id='6' author='dcferreira' date='2020-06-29T20:20:04Z'>
		Ah ok, then I was unfortunate that the symptoms were the same :P
Thanks for checking this.
If you prefer, I can open a new issue for the DataframeInput.
		</comment>
		<comment id='7' author='dcferreira' date='2020-06-29T21:23:24Z'>
		Thanks &lt;denchmark-link:https://github.com/dcferreira&gt;@dcferreira&lt;/denchmark-link&gt;
 that'd be great! I will look into a fix later today
		</comment>
		<comment id='8' author='dcferreira' date='2020-06-30T06:05:29Z'>
		For port conflict:
ref: &lt;denchmark-link:https://flask.palletsprojects.com/en/1.1.x/server/#in-code&gt;https://flask.palletsprojects.com/en/1.1.x/server/#in-code&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;The reason for this is that due to how the reload mechanism works there are some bizarre side-effects (like executing certain code twice, sometimes crashing without message or dying when a syntax or import error happens).
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>