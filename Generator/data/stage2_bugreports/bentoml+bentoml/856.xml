<bug id='856' author='dcferreira' open_date='2020-06-30T05:12:10Z' closed_time='2020-06-30T06:06:50Z'>
	<summary>DataframeInput can't handle micro-batched input as of v0.8.2</summary>
	<description>
As discussed in &lt;denchmark-link:https://github.com/bentoml/BentoML/issues/848&gt;#848&lt;/denchmark-link&gt;
, there seems to be some problem with , in that it can't handle micro-batching.
To Reproduce
Steps to reproduce the behavior:

Take the example service described in #812 (or any other example that uses DataframeInput)
Serve the server with gunicorn, with --enable-microbatch.

Expected behavior
The size endpoint from the example service should return a reply.
Instead, a 500 error is thrown, and nothing related to it is logged.
Running it without --enable-microbatch works as expected.
Screenshots/Logs
Here are some logs of running a docker container with the example service, and making requests to the size endpoint which returned 500 error:
&lt;denchmark-code&gt;â–¶ docker run -p 5000:5000 testserver --debug --enable-microbatch                                  
[2020-06-27 07:42:01,638] DEBUG - Setting debug mode: ON for current session
[2020-06-27 07:42:02,796] INFO - get_gunicorn_num_of_workers: 5, calculated by cpu count
[2020-06-27 07:42:02,832] INFO - Running micro batch service on :5000
[2020-06-27 07:42:03,822] DEBUG - Setting up prometheus_multiproc_dir: /root/bentoml/prometheus_multiproc_dir
[2020-06-27 07:42:03,829] DEBUG - Setting up prometheus_multiproc_dir: /root/bentoml/prometheus_multiproc_dir
[2020-06-27 07:42:03 +0000] [15] [INFO] Starting gunicorn 20.0.4
[2020-06-27 07:42:03 +0000] [1] [INFO] Starting gunicorn 20.0.4
[2020-06-27 07:42:03 +0000] [15] [INFO] Listening at: http://0.0.0.0:5000 (15)
[2020-06-27 07:42:03 +0000] [15] [INFO] Using worker: aiohttp.worker.GunicornWebWorker
[2020-06-27 07:42:03 +0000] [1] [INFO] Listening at: http://0.0.0.0:51123 (1)
[2020-06-27 07:42:03 +0000] [1] [INFO] Using worker: sync
[2020-06-27 07:42:03 +0000] [16] [INFO] Booting worker with pid: 16
[2020-06-27 07:42:03 +0000] [17] [INFO] Booting worker with pid: 17
[2020-06-27 07:42:03 +0000] [18] [INFO] Booting worker with pid: 18
[2020-06-27 07:42:03 +0000] [19] [INFO] Booting worker with pid: 19
[2020-06-27 07:42:03 +0000] [20] [INFO] Booting worker with pid: 20
[2020-06-27 07:42:03,993] INFO - Micro batch enabled for API `size`
[2020-06-27 07:42:03,993] INFO - Your system nofile limit is 1048576, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.
[2020-06-27 07:42:04 +0000] [21] [INFO] Booting worker with pid: 21
&lt;/denchmark-code&gt;

As mentioned before, no mention of the error appears in the logs.
Environment:

BentoML version: 0.8.2

	</description>
	<comments>
		<comment id='1' author='dcferreira' date='2020-06-30T05:45:36Z'>
		Just discussed with &lt;denchmark-link:https://github.com/bojiang&gt;@bojiang&lt;/denchmark-link&gt;
 - this is a regression introduced recently when we were adding proper JSON input orient support in DataframeInput. A fix is coming out soon &lt;denchmark-link:https://github.com/bentoml/BentoML/pull/857&gt;#857&lt;/denchmark-link&gt;
, we will do some more testing and release a patch to include this fix.
We are also working on an end-to-end test that covers this critical path r(&lt;denchmark-link:https://github.com/bentoml/BentoML/pull/855&gt;#855&lt;/denchmark-link&gt;
), which should help us avoid this type of issue in the future.
		</comment>
	</comments>
</bug>