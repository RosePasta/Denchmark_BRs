<bug id='1032' author='aslom' open_date='2020-08-26T18:47:29Z' closed_time='2020-12-07T22:14:02Z'>
	<summary>IrisClassifier fails to start in OpenShift with Error creating bentoml home directory '/bentoml'</summary>
	<description>
Describe the bug
IrisClassifier fails to start in OpenShift with Error creating bentoml home directory '/bentoml'
&lt;denchmark-code&gt; k get po
NAME                               READY   STATUS             RESTARTS   AGE
iris-classifier-7d4fbc7c58-b826n   0/1     CrashLoopBackOff   2          43s

k logs iris-classifier-7d4fbc7c58-b826n
Traceback (most recent call last):
  File "/opt/conda/lib/python3.8/site-packages/bentoml/configuration/__init__.py", line 94, in load_config
    Path(BENTOML_HOME).mkdir(exist_ok=True)
  File "/opt/conda/lib/python3.8/pathlib.py", line 1284, in mkdir
    self._accessor.mkdir(self, mode)
PermissionError: [Errno 13] Permission denied: '/bentoml'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/bin/bentoml", line 5, in &lt;module&gt;
    from bentoml.cli import cli
  File "/opt/conda/lib/python3.8/site-packages/bentoml/__init__.py", line 25, in &lt;module&gt;
    configure_logging()
  File "/opt/conda/lib/python3.8/site-packages/bentoml/utils/log.py", line 105, in configure_logging
    logging_level = config("logging").get("LOGGING_LEVEL").upper()
  File "/opt/conda/lib/python3.8/site-packages/bentoml/configuration/__init__.py", line 152, in config
    _config = load_config()
  File "/opt/conda/lib/python3.8/site-packages/bentoml/configuration/__init__.py", line 96, in load_config
    raise BentoMLConfigException(
bentoml.exceptions.BentoMLConfigException: Error creating bentoml home directory '/bentoml': Permission denied
&lt;/denchmark-code&gt;

To Reproduce
Steps to reproduce the behavior:

Follow tutorial to build and deploy IrisClassifier to OpenShift - I am using this YAML:

&lt;denchmark-code&gt;apiVersion: v1
kind: Service
metadata:
    labels:
        app: iris-classifier
    name: iris-classifier
spec:
    ports:
    - name: predict
      port: 5000
      targetPort: 5000
    selector:
      app: iris-classifier
    type: LoadBalancer
---
apiVersion: apps/v1
kind: Deployment
metadata:
    labels:
        app: iris-classifier
    name: iris-classifier
spec:
    selector:
        matchLabels:
            app: iris-classifier
    template:
        metadata:
            labels:
                app: iris-classifier
        spec:
            containers:
            - image: aslom/iris-classifier:latest
              imagePullPolicy: Always
              name: iris-classifier
              ports:
              - containerPort: 5000
&lt;/denchmark-code&gt;

Expected behavior
Docker container work but deployment fails
&lt;denchmark-code&gt;docker run aslom/iris-classifier:latest
[2020-08-26 18:40:32,935] INFO - Starting BentoML API server in production mode..
[2020-08-26 18:40:33,359] INFO - get_gunicorn_num_of_workers: 5, calculated by cpu count
[2020-08-26 18:40:33 +0000] [1] [INFO] Starting gunicorn 20.0.4
[2020-08-26 18:40:33 +0000] [1] [INFO] Listening at: http://0.0.0.0:5000 (1)
[2020-08-26 18:40:33 +0000] [1] [INFO] Using worker: sync
[2020-08-26 18:40:33 +0000] [15] [INFO] Booting worker with pid: 15
[2020-08-26 18:40:33 +0000] [16] [INFO] Booting worker with pid: 16
[2020-08-26 18:40:33 +0000] [17] [INFO] Booting worker with pid: 17
[2020-08-26 18:40:33 +0000] [18] [INFO] Booting worker with pid: 18
[2020-08-26 18:40:33 +0000] [19] [INFO] Booting worker with pid: 19
...
&lt;/denchmark-code&gt;

Screenshots/Logs
If applicable, add screenshots, logs or error outputs to help explain your problem.
To give us more information for diagnosing the issue, make sure to enable debug logging:
Add the following lines to your Python code before invoking BentoML:
import bentoml
import logging
bentoml.config().set('core', 'debug', 'true')
bentoml.configure_logging(logging.DEBUG)
And use the --verbose option when running bentoml CLI command, e.g.:
bentoml get IrisClassifier --verbose
Environment:

OS: MacOS 10.15 OCP 4.4
Python/BentoML Version  3.8.5, bentoml, version 0.8.5

Additional context
Add any other context about the problem here.
	</description>
	<comments>
		<comment id='1' author='aslom' date='2020-08-26T19:06:36Z'>
		it may be related to &lt;denchmark-link:https://github.com/bentoml/BentoML/pull/284&gt;#284&lt;/denchmark-link&gt;
 but I am clear what that PR fixed.
		</comment>
		<comment id='2' author='aslom' date='2020-08-26T19:13:34Z'>
		doing more investigation it seems BentoML image is running as root and that has security implications:
&lt;denchmark-link:https://stackoverflow.com/questions/42363105/permission-denied-mkdir-in-container-on-openshift&gt;https://stackoverflow.com/questions/42363105/permission-denied-mkdir-in-container-on-openshift&lt;/denchmark-link&gt;

&lt;denchmark-link:https://serverfault.com/questions/916101/how-do-i-fix-openshift-permission-denied-error&gt;https://serverfault.com/questions/916101/how-do-i-fix-openshift-permission-denied-error&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='aslom' date='2020-08-26T19:50:55Z'>
		Running
&lt;denchmark-code&gt;oc adm policy add-scc-to-user anyuid -z default
securitycontextconstraints.security.openshift.io/anyuid added to: ["system:serviceaccount:bentoml:default"]
&lt;/denchmark-code&gt;

fallowed ot run container:
&lt;denchmark-code&gt;k apply -f iris-classifier-service-deployment.yaml
service/iris-classifier created
deployment.apps/iris-classifier created

k get po
NAME                               READY   STATUS    RESTARTS   AGE
iris-classifier-7d4fbc7c58-8v5d8   1/1     Running   0          3s


k logs iris-classifier-7d4fbc7c58-8v5d8
[2020-08-26 19:48:59,673] INFO - Starting BentoML API server in production mode..
[2020-08-26 19:49:00,023] INFO - get_gunicorn_num_of_workers: 3, calculated by cpu count
[2020-08-26 19:49:00 +0000] [1] [INFO] Starting gunicorn 20.0.4
[2020-08-26 19:49:00 +0000] [1] [INFO] Listening at: http://0.0.0.0:5000 (1)
[2020-08-26 19:49:00 +0000] [1] [INFO] Using worker: sync
[2020-08-26 19:49:00 +0000] [11] [INFO] Booting worker with pid: 11
[2020-08-26 19:49:00 +0000] [12] [INFO] Booting worker with pid: 12
[2020-08-26 19:49:00 +0000] [13] [INFO] Booting worker with pid: 13
[2020-08-26 19:49:00,425] WARNING - bentoml.handlers.* will be deprecated after BentoML 1.0, use bentoml.adapters.* instead
[2020-08-26 19:49:00,426] WARNING - DataframeHandler will be deprecated after BentoML 1.0, use DataframeInput instead
[2020-08-26 19:49:00,452] WARNING - bentoml.handlers.* will be deprecated after BentoML 1.0, use bentoml.adapters.* instead
[2020-08-26 19:49:00,452] WARNING - DataframeHandler will be deprecated after BentoML 1.0, use DataframeInput instead
[2020-08-26 19:49:01,285] WARNING - bentoml.handlers.* will be deprecated after BentoML 1.0, use bentoml.adapters.* instead
[2020-08-26 19:49:01,285] WARNING - DataframeHandler will be deprecated after BentoML 1.0, use DataframeInput instead
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='aslom' date='2020-08-26T19:52:19Z'>
		&lt;denchmark-link:https://github.com/aslom&gt;@aslom&lt;/denchmark-link&gt;
 Thank you for investigating it and make it run with the temporary change on the service's permission!
I will look into improving this with the suggested solution post in the StackOverflow thread.
		</comment>
		<comment id='5' author='aslom' date='2020-08-26T20:52:58Z'>
		Turned out to be a bit of rabbit hole:
The easiest way to test it wothout deploying to k8s is to use uid:guid for docker run:
&lt;denchmark-code&gt; docker run --user 1234:1234 aslom/iris-classifier:latest
&lt;/denchmark-code&gt;

Key requirements:
"Make directories you want to write to group-writable and owned by group id 0 Set the net-bind capability on your executables if they need to bind to ports &lt;1024
&lt;denchmark-link:https://stackoverflow.com/questions/37723401/how-do-you-run-an-openshift-docker-container-as-something-besides-root&gt;https://stackoverflow.com/questions/37723401/how-do-you-run-an-openshift-docker-container-as-something-besides-root&lt;/denchmark-link&gt;

"Being forced to run as an arbitrary user ID does mean that some container images may not run out of the box in OpenShift. This will be the case where images do not adopt security best practices and need to be run as the root user ID even though they have no actual requirement to run as root. Even an image which has been setup to run as a fixed user ID which isn't root may not work.
To ensure container images are portable and will work with container deployment platforms which enforce additional separation between applications by overriding the user ID, images and the applications they contain, should be designed to be able to be run as an arbitrary user ID.
&lt;denchmark-link:https://cookbook.openshift.org/users-and-role-based-access-control/why-do-my-applications-run-as-a-random-user-id.html&gt;https://cookbook.openshift.org/users-and-role-based-access-control/why-do-my-applications-run-as-a-random-user-id.html&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='aslom' date='2020-08-26T20:58:01Z'>
		And here is proof of concept Dockerfile that works with anyuid to run as docker run and in OpenShift:
&lt;denchmark-code&gt;FROM bentoml/model-server:0.8.5

# copy over model files
COPY . /bento
WORKDIR /bento

RUN groupadd -g 2000 bento &amp;&amp; useradd -m -u 2001 -g bento bento

# https://stackoverflow.com/questions/37723401/how-do-you-run-an-openshift-docker-container-as-something-besides-root
# use group root(0) and writable to run under OpenShift anyuser security

RUN chmod -R 775 /bento
RUN chown -R bento:root /bento

RUN mkdir /bentoml
RUN chown -R bento:root /bentoml
RUN chmod -R 777 /bentoml

# otherwise fails
#   File "/opt/conda/lib/python3.8/site-packages/bentoml/configuration/__init__.py", line 94, in load_config
#    Path(BENTOML_HOME).mkdir(exist_ok=True)
#  File "/opt/conda/lib/python3.8/pathlib.py", line 1284, in mkdir
#    self._accessor.mkdir(self, mode)
# PermissionError: [Errno 13] Permission denied: '/bentoml'
RUN chown bento:root /
RUN chmod 755 /

# conda needs to write into /opt/conda
RUN chown -R bento:root /opt/conda
RUN chmod -R 775 /opt/conda

COPY docker-entrypoint.sh /usr/local/bin/
#COPY docker-entrypoint.sh /bento

# Execute permission for docker-entrypoint.sh
RUN chmod +x /usr/local/bin/docker-entrypoint.sh
#RUN chmod +x /bento/docker-entrypoint.sh

USER bento

# Configuring PyPI index
ARG PIP_INDEX_URL=https://pypi.python.org/simple/
ARG PIP_TRUSTED_HOST=pypi.python.org
ENV PIP_INDEX_URL $PIP_INDEX_URL
ENV PIP_TRUSTED_HOST $PIP_TRUSTED_HOST

# Execute permission for bentoml-init.sh
RUN chmod +x /bento/bentoml-init.sh

# Install conda, pip dependencies and run user defined setup script
RUN if [ -f /bento/bentoml-init.sh ]; then bash -c /bento/bentoml-init.sh; fi

# the env var $PORT is required by heroku container runtime
ENV PORT 5000
EXPOSE $PORT


ENTRYPOINT [ "/bento/docker-entrypoint.sh" ]
CMD ["bentoml", "serve-gunicorn", "/bento"]

&lt;/denchmark-code&gt;

Test local:
&lt;denchmark-code&gt;docker run -it --user 1234:1234 aslom/iris-classifier:latest
[2020-08-26 20:52:11,658] INFO - Starting BentoML API server in production mode..
[2020-08-26 20:52:11,879] INFO - get_gunicorn_num_of_workers: 5, calculated by cpu count
[2020-08-26 20:52:11 +0000] [1] [INFO] Starting gunicorn 20.0.4
[2020-08-26 20:52:11 +0000] [1] [INFO] Listening at: http://0.0.0.0:5000 (1)
[2020-08-26 20:52:11 +0000] [1] [INFO] Using worker: sync
[2020-08-26 20:52:11 +0000] [14] [INFO] Booting worker with pid: 14
[2020-08-26 20:52:11 +0000] [15] [INFO] Booting worker with pid: 15
[2020-08-26 20:52:12 +0000] [16] [INFO] Booting worker with pid: 16
[2020-08-26 20:52:12 +0000] [17] [INFO] Booting worker with pid: 17
[2020-08-26 20:52:12 +0000] [18] [INFO] Booting worker with pid: 18
[2020-08-26 20:52:12,265] WARNING - bentoml.handlers.* will be deprecated after BentoML 1.0, use bentoml.adapters.* instead
[2020-08-26 20:52:12,266] WARNING - DataframeHandler will be deprecated after BentoML 1.0, use DataframeInput instead
&lt;/denchmark-code&gt;

and in OCP 4.4 without doing anyuid
&lt;denchmark-code&gt;k get po
NAME                               READY   STATUS    RESTARTS   AGE
iris-classifier-7d4fbc7c58-gwghs   1/1     Running   0          2m9s

k logs iris-classifier-7d4fbc7c58-gwghs
[2020-08-26 20:55:48,516] INFO - Starting BentoML API server in production mode..
[2020-08-26 20:55:48,978] INFO - get_gunicorn_num_of_workers: 3, calculated by cpu count
[2020-08-26 20:55:48 +0000] [1] [INFO] Starting gunicorn 20.0.4
[2020-08-26 20:55:48 +0000] [1] [INFO] Listening at: http://0.0.0.0:5000 (1)
[2020-08-26 20:55:48 +0000] [1] [INFO] Using worker: sync
[2020-08-26 20:55:49 +0000] [11] [INFO] Booting worker with pid: 11
[2020-08-26 20:55:49 +0000] [12] [INFO] Booting worker with pid: 12
[2020-08-26 20:55:49 +0000] [13] [INFO] Booting worker with pid: 13
[2020-08-26 20:55:49,561] WARNING - bentoml.handlers.* will be deprecated after BentoML 1.0, use bentoml.adapters.* instead
[2020-08-26 20:55:49,562] WARNING - DataframeHandler will be deprecated after BentoML 1.0, use DataframeInput instead
[2020-08-26 20:55:49,568] WARNING - bentoml.handlers.* will be deprecated after BentoML 1.0, use bentoml.adapters.* instead
[2020-08-26 20:55:49,569] WARNING - DataframeHandler will be deprecated after BentoML 1.0, use DataframeInput instead
[2020-08-26 20:55:49,604] WARNING - bentoml.handlers.* will be deprecated after BentoML 1.0, use bentoml.adapters.* instead
[2020-08-26 20:55:49,605] WARNING - DataframeHandler will be deprecated after BentoML 1.0, use DataframeInput instead
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='aslom' date='2020-08-26T20:58:41Z'>
		BTW: to debug it is useful to run /bin/bash and see output:
&lt;denchmark-code&gt;apiVersion: v1
kind: Service
metadata:
    labels:
        app: bento-bash
    name: bento-bash
spec:
    ports:
    - name: predict
      port: 5000
      targetPort: 5000
    selector:
      app: bento-bash
    type: LoadBalancer
---
apiVersion: apps/v1
kind: Deployment
metadata:
    labels:
        app: bento-bash
    name: bento-bash
spec:
    selector:
        matchLabels:
            app: bento-bash
    template:
        metadata:
            labels:
                app: bento-bash
        spec:
            containers:
            - image: aslom/iris-classifier:latest
              imagePullPolicy: Always
              name: iris-classifier
              command: ["/bin/bash"]
              args: ["-c", "while true; do echo hello; sleep 10;done"]
              ports:
              - containerPort: 5000
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='aslom' date='2020-08-26T21:00:49Z'>
		&lt;denchmark-link:https://github.com/yubozhao&gt;@yubozhao&lt;/denchmark-link&gt;
 the best solution would be probably to make base image support running anyuid (with chmod and chown as I did) and test it with  (or random uid:guid) in unit tests?
		</comment>
		<comment id='9' author='aslom' date='2020-08-26T22:31:11Z'>
		&lt;denchmark-link:https://github.com/aslom&gt;@aslom&lt;/denchmark-link&gt;
 I think that's a good suggestion. Would you like to open a PR with your changes? I think it would be a good way to recognize your contribution to this issue
		</comment>
		<comment id='10' author='aslom' date='2020-10-14T11:27:36Z'>
		&lt;denchmark-link:https://github.com/yubozhao&gt;@yubozhao&lt;/denchmark-link&gt;
 I have managed to follow &lt;denchmark-link:https://github.com/aslom&gt;@aslom&lt;/denchmark-link&gt;
 solution and get the image to run on openshift and locally. However, whenever i send a post request to it. I either receive connection refused or timeout error.
Any suggestions?
		</comment>
		<comment id='11' author='aslom' date='2020-10-14T20:02:26Z'>
		&lt;denchmark-link:https://github.com/Observe786&gt;@Observe786&lt;/denchmark-link&gt;
 Thanks for letting me know.
I want to make sure, right now the BentoService is working locally (You can make a successful POST request to it and have the prediction result) and you are able to deploy it to an OpenShift cluster, right?
If you can make the prediction locally, it is more likely an issue with the openshift cluster networking. we can troubleshoot from there.
Also, if you don't mind, could you share with me the openshift deployment yaml?
		</comment>
		<comment id='12' author='aslom' date='2020-12-06T23:23:04Z'>
		&lt;denchmark-link:https://github.com/Observe786&gt;@Observe786&lt;/denchmark-link&gt;
 Any updates? Let me know if you resolve the issue, happy to help
&lt;denchmark-link:https://github.com/aslom&gt;@aslom&lt;/denchmark-link&gt;
 I looked at other projects' Openshift deployment.  Deploy to a specified namespace seems like the common practice.
I recommend these steps for deploying Bento Service to an OpenShift cluster.

Create a namespace for BentoService
kubectl create namespace bentoml
Add permission for the namespace that BentoService is deployed in
oc adm policy add-scc-to-group anyuid system:serviceaccounts:bentoml
Deploy services under the namespace from above.

		</comment>
		<comment id='13' author='aslom' date='2020-12-07T22:14:02Z'>
		I am going to close this issue for now.  Feel free to reopen it for dicussion.
		</comment>
		<comment id='14' author='aslom' date='2020-12-08T00:09:26Z'>
		We are also adding an option to allow users to disable logging prediction input &amp; output to log files, which should help resolve this issue as well &lt;denchmark-link:https://github.com/bentoml/BentoML/issues/1009&gt;#1009&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>