<bug id='1180' author='timlod' open_date='2020-10-16T08:58:05Z' closed_time='2020-10-21T09:48:24Z'>
	<summary>SkLearnModelArtifact serving can lead to OSError 24: too many open files</summary>
	<description>

I am trying to  where  includes in my case two large  (around 2GB).
I've run into , and conjecture that the reason is that bentoML uses  to load the artifacts (see &lt;denchmark-link:https://github.com/joblib/joblib/issues/81&gt;joblib/joblib#81&lt;/denchmark-link&gt;
).
To Reproduce

Pack a bento with (several?) large SkLearnModelArtifacts
Try to serve it with a "normal" ulimit -n (I had to set ulimit -n 10000 for this to actually run)
See error

Expected behavior
Any artifact is loaded without trouble (or there is an error when the model is saved, indicating that it is too large/otherwise bad to load).
Environment:

Arch Linux
Python 3.8.5
BentoML-0.9.1

	</description>
	<comments>
		<comment id='1' author='timlod' date='2020-10-20T19:46:48Z'>
		Hi &lt;denchmark-link:https://github.com/timlod&gt;@timlod&lt;/denchmark-link&gt;
 - it looks like there isn't much we can do on BentoML side for this issue, one workaround is to use BentoML's  to package your Scikit learn model: &lt;denchmark-link:https://docs.bentoml.org/en/latest/api/artifacts.html?highlight=PickleArtifact#common-artifacts&gt;https://docs.bentoml.org/en/latest/api/artifacts.html?highlight=PickleArtifact#common-artifacts&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='timlod' date='2020-10-21T09:48:24Z'>
		I see, thanks for the heads-up!
		</comment>
		<comment id='3' author='timlod' date='2020-10-28T10:12:53Z'>
		Actually, it would be possible for BentoML to use joblib.load(model_file_path, mmap_mode=None) - then memory mapping is not used and the error won't occur. How vital is memmapping here?
		</comment>
		<comment id='4' author='timlod' date='2020-10-28T15:52:14Z'>
		
Actually, it would be possible for BentoML to use joblib.load(model_file_path, mmap_mode=None) - then memory mapping is not used and the error won't occur. How vital is memmapping here?

We can potentially make that an option and allow user to turn it off. E.g.
SklearnArtifact("my_model", mmap_mode=None)
		</comment>
	</comments>
</bug>