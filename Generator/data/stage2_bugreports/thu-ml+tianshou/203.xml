<bug id='203' author='zkx741481546' open_date='2020-09-05T10:51:31Z' closed_time='2020-09-06T08:20:17Z'>
	<summary>Why setting policy.train() before collect samples in offline_trainer?</summary>
	<description>

 I have marked all applicable categories:

 exception-raising bug
 RL algorithm bug
 documentation request (i.e. "X is missing from the documentation.")
 new feature request


 I have visited the source website
 I have searched through the issue tracker for duplicates
 I have mentioned version numbers, operating system and environment, where applicable:
import tianshou, torch, sys
print(tianshou.__version__, torch.__version__, sys.version, sys.platform)


I have asked a Batchnorm question earlier, after reading pytorch doc, I think it's more reasonable to set policy.eval() rather than policy.train() before collect samples.
for epoch in range(1, 1 + max_epoch):
        # train
        policy.train()
        if train_fn:
            train_fn(epoch)
        with tqdm.tqdm(total=step_per_epoch, desc=f'Epoch #{epoch}',
                       **tqdm_config) as t:
            while t.n &lt; t.total:
                result = train_collector.collect(n_step=collect_per_step)
                data = {}
                if test_in_train and stop_fn and stop_fn(result['rew']):
                    test_result = test_episode(
                        policy, test_collector, test_fn,
                        epoch, episode_per_test)
Also, I think it should be set to eval mode before test, incase someone using BatchNorm layer or dropout.
	</description>
	<comments>
		<comment id='1' author='zkx741481546' date='2020-09-05T11:00:43Z'>
		
Also, I think it should be set to eval mode before test, incase someone using BatchNorm layer or dropout.

This has been set in test_episode under tianshou/trainer/utils.py

I have asked a Batchnorm question earlier, after reading pytorch doc, I think it's more reasonable to set policy.eval() rather than policy.train() before collect samples.

Indeed, off-policy both trainer should do this , but how about PPO with batch-norm?
I'll make a pr to fix it.
		</comment>
		<comment id='2' author='zkx741481546' date='2020-09-05T11:46:27Z'>
		

Also, I think it should be set to eval mode before test, incase someone using BatchNorm layer or dropout.

This has been set in test_episode under tianshou/trainer/utils.py

I have asked a Batchnorm question earlier, after reading pytorch doc, I think it's more reasonable to set policy.eval() rather than policy.train() before collect samples.

Indeed, off-policy both trainer should do this , but how about PPO with batch-norm?
I'll make a pr to fix it.

Thanks for replying, and I still have a problem. What is the self._action_bias for in DDPGPolicy code? It seems that the action_bias variable is added to action before applying noise.
		</comment>
		<comment id='3' author='zkx741481546' date='2020-09-05T11:59:37Z'>
		
Thanks for replying, and I still have a problem. What is the self._action_bias for in DDPGPolicy code? It seems that the action_bias variable is added to action before applying noise.

You have already asked this in &lt;denchmark-link:https://github.com/thu-ml/tianshou/issues/194&gt;#194&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='zkx741481546' date='2020-09-05T12:35:34Z'>
		&lt;denchmark-link:https://github.com/zkx741481546&gt;@zkx741481546&lt;/denchmark-link&gt;
 please have a look at &lt;denchmark-link:https://github.com/thu-ml/tianshou/pull/204&gt;#204&lt;/denchmark-link&gt;
 and feel free to give any comments!
		</comment>
		<comment id='5' author='zkx741481546' date='2020-09-11T13:17:58Z'>
		I have to revert the code because of the issue mentioned in &lt;denchmark-link:https://github.com/thu-ml/tianshou/issues/211&gt;#211&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>