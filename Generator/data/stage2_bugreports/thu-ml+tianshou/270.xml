<bug id='270' author='hyx1999' open_date='2021-01-10T02:28:37Z' closed_time='2021-01-11T02:19:46Z'>
	<summary>problem of max_grad_norm in PPOPolicy</summary>
	<description>
The parameter "max_grad_norm" is optional in the PPOPolicy parameter list, but in the training phase, PPOPolicy will require that the value of max_grad_norm cannot be None (but the default value of max_grad_norm is None).
	</description>
	<comments>
		<comment id='1' author='hyx1999' date='2021-01-10T08:08:32Z'>
		I don't think so. 


tianshou/tianshou/policy/modelfree/ppo.py


         Line 181
      in
      c6f2648






 if self._max_grad_norm: 





This line checks if max_grad_norm is not None. If this parameter is None, it will directly bypass these lines.
I don't see any other lines that require max_grad_norm not None, could you please elaborate on it?
		</comment>
		<comment id='2' author='hyx1999' date='2021-01-11T02:02:23Z'>
		Sorry, but the ppo algorithm in the tianshou package I installed via pip is different from the code you showed. I tried to update tianshou through pip install tianshou --upgrade and  pip install tianshou --upgrade -i https://pypi.tuna.tsinghua.edu.cn/simple, but it seems that there is no change. maybe I should use pip install git+https://github.com/thu-ml/tianshou.git@master --upgrade?
line 180 to line 182
loss.backward()
nn.utils.clip_grad_norm_(
    list(self.actor.parameters())
    + list(self.critic.parameters()),
    self._max_grad_norm)
self.optim.step()
The version of tianshou I currently use is as follows
&gt;&gt;&gt; import tianshou as ts
&gt;&gt;&gt; ts.__version__
'0.3.0.post1'
&gt;&gt;&gt; 
		</comment>
		<comment id='3' author='hyx1999' date='2021-01-11T02:10:22Z'>
		Okay, it's my bad. I'll post a new release soon (after merge &lt;denchmark-link:https://github.com/thu-ml/tianshou/pull/263&gt;#263&lt;/denchmark-link&gt;
).
		</comment>
	</comments>
</bug>