<bug id='211' author='lanpartis' open_date='2020-09-11T02:09:23Z' closed_time='2020-09-12T00:44:51Z'>
	<summary>Potential bug caused by calling policy.eval() before collecting experience</summary>
	<description>

 I have marked all applicable categories:

 exception-raising bug
 RL algorithm bug
 documentation request (i.e. "X is missing from the documentation.")
 new feature request


 I have visited the source website
 I have searched through the issue tracker for duplicates
 I have mentioned version numbers, operating system and environment, where applicable:
import tianshou, torch, sys
print(tianshou.__version__, torch.__version__, sys.version, sys.platform)


In &lt;denchmark-link:https://github.com/thu-ml/tianshou/commit/8bb8ecba6ea28fdba8f6130755b0ddf493073c84&gt;this commit&lt;/denchmark-link&gt;
 I found policy.eval() was called before collecting experience in both on/off policy trainer, but some algorithms, such as the DDPGPolicy Tianshou provided, use the attribute  to control exploration and exploitation. Wouldn't this change cause serious bug by preventing these algorithms from exploration? Also, I'm not sure if there were any other place that Tianshou uses  to distinguish training and testing.
piece of source code of DDPGPolicy:
def forward(self, batch: Batch,
                state: Optional[Union[dict, Batch, np.ndarray]] = None,
                model: str = 'actor',
                input: str = 'obs',
                explorating: bool = True,
                **kwargs) -&gt; Batch:
        """Compute action over the given batch data.
        :return: A :class:`~tianshou.data.Batch` which has 2 keys:
            * ``act`` the action.
            * ``state`` the hidden state.
        .. seealso::
            Please refer to :meth:`~tianshou.policy.BasePolicy.forward` for
            more detailed explanation.
        """
        model = getattr(self, model)
        obs = getattr(batch, input)
        actions, h = model(obs, state=state, info=batch.info)
        actions += self._action_bias
        if self.training and explorating:
            actions += to_torch_as(self._noise(actions.shape), actions)
        actions = actions.clamp(self._range[0], self._range[1])
        return Batch(act=actions, state=h)
	</description>
	<comments>
		<comment id='1' author='lanpartis' date='2020-09-11T04:50:04Z'>
		I'll open a pr today
		</comment>
	</comments>
</bug>