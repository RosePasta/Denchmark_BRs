<bug id='2379' author='hyz-xmaster' open_date='2020-04-02T11:55:55Z' closed_time='2020-08-13T09:27:55Z'>
	<summary>reppoints_moment_r101_dcn_fpn_2x.pth seems not match reppoints_moment_r101_dcn_fpn_2x.py</summary>
	<description>
Thanks for producing this great toolbox.
Describe the bug
I have tested the  with the model  (&lt;denchmark-link:https://github.com/open-mmlab/mmdetection/tree/master/configs/reppoints&gt;https://github.com/open-mmlab/mmdetection/tree/master/configs/reppoints&lt;/denchmark-link&gt;
) on COCO Val2017, but found that the results (see below) do not match the reported 43.0 mAP.
reppoints_moment_r101_dcn_fpn_2x
Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.056
Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.095
Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.057
Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.012
Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.052
Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.122
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.109
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.161
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.164
Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.023
Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.119
Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.354
BTW, I have also run the reppoints_moment_r101_dcn_fpn_2x_mt and reppoints_moment_r101_fpn_2x, whose results match the reported ones.
reppoints_moment_r101_dcn_fpn_2x_mt
Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.447
Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.656
Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.487
Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.281
Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.495
Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.588
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.358
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.569
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.603
Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.423
Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.651
Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.759
reppoints_moment_r101_fpn_2x
Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.403
Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.610
Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.435
Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.232
Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.446
Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.524
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.334
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.537
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.570
Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.366
Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.614
Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.722
Reproduction


What command or script did you run?
python tools/test.py configs/reppoints/reppoints_moment_r101_dcn_fpn_2x.py checkpoints/reppoints_moment_r101_dcn_fpn_2x.pth --eval bbox


What dataset did you use?
MS COCO Val2017


	</description>
	<comments>
		<comment id='1' author='hyz-xmaster' date='2020-04-03T13:06:54Z'>
		Hi &lt;denchmark-link:https://github.com/hyz-xmaster&gt;@hyz-xmaster&lt;/denchmark-link&gt;
,
Can you first check whether the keys are loaded closely? For example, the full log could benefit a lot.
		</comment>
		<comment id='2' author='hyz-xmaster' date='2020-04-04T00:35:49Z'>
		Hi &lt;denchmark-link:https://github.com/ZwwWayne&gt;@ZwwWayne&lt;/denchmark-link&gt;
,
The log information is as follows:
loading annotations into memory...
Done (t=0.45s)
creating index...
index created!
The model and loaded state dict do not match exactly
missing keys in source state_dict: backbone.layer2.0.conv2.conv_offset.weight, backbone.layer2.0.conv2.conv_offset.bias, backbone.layer2.1.conv2.conv_offset.weight, backbone.layer2.1.conv2.conv_offset.bias, backbone.layer2.2.conv2.conv_offset.weight, backbone.layer2.2.conv2.conv_offset.bias, backbone.layer2.3.conv2.conv_offset.weight, backbone.layer2.3.conv2.conv_offset.bias, backbone.layer3.0.conv2.conv_offset.weight, backbone.layer3.0.conv2.conv_offset.bias, backbone.layer3.1.conv2.conv_offset.weight, backbone.layer3.1.conv2.conv_offset.bias, backbone.layer3.2.conv2.conv_offset.weight, backbone.layer3.2.conv2.conv_offset.bias, backbone.layer3.3.conv2.conv_offset.weight, backbone.layer3.3.conv2.conv_offset.bias, backbone.layer3.4.conv2.conv_offset.weight, backbone.layer3.4.conv2.conv_offset.bias, backbone.layer3.5.conv2.conv_offset.weight, backbone.layer3.5.conv2.conv_offset.bias, backbone.layer3.6.conv2.conv_offset.weight, backbone.layer3.6.conv2.conv_offset.bias, backbone.layer3.7.conv2.conv_offset.weight, backbone.layer3.7.conv2.conv_offset.bias, backbone.layer3.8.conv2.conv_offset.weight, backbone.layer3.8.conv2.conv_offset.bias, backbone.layer3.9.conv2.conv_offset.weight, backbone.layer3.9.conv2.conv_offset.bias, backbone.layer3.10.conv2.conv_offset.weight, backbone.layer3.10.conv2.conv_offset.bias, backbone.layer3.11.conv2.conv_offset.weight, backbone.layer3.11.conv2.conv_offset.bias, backbone.layer3.12.conv2.conv_offset.weight, backbone.layer3.12.conv2.conv_offset.bias, backbone.layer3.13.conv2.conv_offset.weight, backbone.layer3.13.conv2.conv_offset.bias, backbone.layer3.14.conv2.conv_offset.weight, backbone.layer3.14.conv2.conv_offset.bias, backbone.layer3.15.conv2.conv_offset.weight, backbone.layer3.15.conv2.conv_offset.bias, backbone.layer3.16.conv2.conv_offset.weight, backbone.layer3.16.conv2.conv_offset.bias, backbone.layer3.17.conv2.conv_offset.weight, backbone.layer3.17.conv2.conv_offset.bias, backbone.layer3.18.conv2.conv_offset.weight, backbone.layer3.18.conv2.conv_offset.bias, backbone.layer3.19.conv2.conv_offset.weight, backbone.layer3.19.conv2.conv_offset.bias, backbone.layer3.20.conv2.conv_offset.weight, backbone.layer3.20.conv2.conv_offset.bias, backbone.layer3.21.conv2.conv_offset.weight, backbone.layer3.21.conv2.conv_offset.bias, backbone.layer3.22.conv2.conv_offset.weight, backbone.layer3.22.conv2.conv_offset.bias, backbone.layer4.0.conv2.conv_offset.weight, backbone.layer4.0.conv2.conv_offset.bias, backbone.layer4.1.conv2.conv_offset.weight, backbone.layer4.1.conv2.conv_offset.bias, backbone.layer4.2.conv2.conv_offset.weight, backbone.layer4.2.conv2.conv_offset.bias
[&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;] 5000/5000, 11.6 task/s, elapsed: 432s, ETA:     0s
writing results to ./work_dirs/reppoints_moment_r101_dcn_fpn_2x/results.pkl
Evaluating bbox...
Loading and preparing results...
DONE (t=1.82s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type bbox
DONE (t=31.65s).
Accumulating evaluation results...
DONE (t=9.36s).
Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.064
Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.106
Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.064
Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.012
Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.058
Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.137
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.117
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.171
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.174
Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.025
Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.129
Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.372
I think the cause might be that the parameter names of the DCN conv_offset layer in the trained model do not match their counterparts in the resnet.py which are actually with the prefix of  conv2_offset.
One interesting thing is that the mAP score is mysteriously improved from 0.056 to 0.064.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

I have also just tested again on another machine and got the results:
Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.060
Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.101
Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.061
Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010
Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.056
Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.124
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.114
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.168
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.171
Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.023
Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.125
Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369
I guess the reason why the results are different is the random initialization of the unloaded weights.
		</comment>
		<comment id='3' author='hyz-xmaster' date='2020-04-04T07:06:07Z'>
		Hi &lt;denchmark-link:https://github.com/hyz-xmaster&gt;@hyz-xmaster&lt;/denchmark-link&gt;
 ,
What is your version of mmdetection, you might need to upgrade your mmdetection.
		</comment>
		<comment id='4' author='hyz-xmaster' date='2020-04-04T07:21:30Z'>
		Well, I just upgraded mmdetection 2 days ago, so I think it is the latest one.
The version:
&lt;denchmark-code&gt;# GENERATED VERSION FILE
# TIME: Thu Apr  2 18:18:28 2020

__version__ = '1.1.0+d5b1dfa'
short_version = '1.1.0'
&lt;/denchmark-code&gt;

Please see Line 484 &lt;denchmark-link:url&gt;https://github.com/open-mmlab/mmdetection/blob/16a2a2272ac6341bbdaef736fc1710b8f654d002/mmdet/models/backbones/resnet.py#L484&lt;/denchmark-link&gt;
 in resnet.py. It's _offset.
I can actually get correct results for reppoints_moment_r101_dcn_fpn_2x_mt. Can you download the model weights file of reppoints_moment_r101_dcn_fpn_2x.py and run it to see the performance ?
		</comment>
		<comment id='5' author='hyz-xmaster' date='2020-04-04T12:21:31Z'>
		
Due to PR #1894, the DCN works as a conv layer in present, and the offset of DCN should has the key name conv_offset as indicated here.
The bug report is missing keys of XXX.conv_offset.XXX, which means the keys in the checkpoint should have the name XXX.conv_offset.XXX but they do not.
The conv2_offset in the resnet.py is a bug. PR #1894 did not check the initialization after refactoring the conv layers, we will fix it later.
The bug seems to lie in the model checkpoint because if you use reppoints_moment_r101_fpn_2x.py to load it, the keys exactly match and the mAP is 40.6, we will try to find out the reason and fix it.

		</comment>
		<comment id='6' author='hyz-xmaster' date='2020-04-04T13:20:46Z'>
		Yes, I agree that the bug is probably in the checkpoint and thanks for your work.
		</comment>
		<comment id='7' author='hyz-xmaster' date='2020-08-13T09:27:55Z'>
		The is a naming bug in the reppoints checkpoints. This bug have been fixed after we migrated to MMDet V2.0 model zoo. So the issue will be closed. Feel free if you have any further questions.
		</comment>
	</comments>
</bug>