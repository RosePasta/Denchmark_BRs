<bug id='3352' author='171339995' open_date='2020-07-20T06:27:01Z' closed_time='2020-12-22T11:02:57Z'>
	<summary>why memory is always 0 in log ?</summary>
	<description>
2020-07-20 13:02:27,784 - mmdet - INFO - Start running, host: XXXXXX, work_dir: /home/luow/mmdetection/output/tmp
2020-07-20 13:02:27,785 - mmdet - INFO - workflow: [('train', 1)], max: 24 epochs
2020-07-20 13:02:53,527 - mmdet - INFO - Epoch [1][50/6126]	lr: 0.00025, eta: 17:49:11, time: 0.436, data_time: 0.048, memory: 0, loss_rpn_cls: 0.6848, loss_rpn_bbox: 0.0467, loss_cls: 0.2047, acc: 95.7793, loss_bbox: 0.0105, loss: 0.9466
2020-07-20 13:03:23,268 - mmdet - INFO - Epoch [1][100/6126]	lr: 0.00050, eta: 22:38:20, time: 0.673, data_time: 0.082, memory: 0, loss_rpn_cls: 0.4493, loss_rpn_bbox: 0.0533, loss_cls: 0.1425, acc: 98.7930, loss_bbox: 0.0228, loss: 0.6679
2020-07-20 13:04:02,945 - mmdet - INFO - Epoch [1][150/6126]	lr: 0.00075, eta: 1 day, 1:52:50, time: 0.794, data_time: 0.004, memory: 0, loss_rpn_cls: 0.1365, loss_rpn_bbox: 0.0439, loss_cls: 0.1483, acc: 97.0234, loss_bbox: 0.1000, loss: 0.4287
2020-07-20 13:04:43,334 - mmdet - INFO - Epoch [1][200/6126]	lr: 0.00100, eta: 1 day, 3:38:23, time: 0.808, data_time: 0.004, memory: 0, loss_rpn_cls: 0.1102, loss_rpn_bbox: 0.0419, loss_cls: 0.1744, acc: 95.5859, loss_bbox: 0.1597, loss: 0.4862
2020-07-20 13:05:24,305 - mmdet - INFO - Epoch [1][250/6126]	lr: 0.00125, eta: 1 day, 4:47:08, time: 0.819, data_time: 0.004, memory: 0, loss_rpn_cls: 0.1057, loss_rpn_bbox: 0.0453, loss_cls: 0.1839, acc: 95.1465, loss_bbox: 0.1813, loss: 0.5161
2020-07-20 13:06:06,867 - mmdet - INFO - Epoch [1][300/6126]	lr: 0.00150, eta: 1 day, 5:45:44, time: 0.851, data_time: 0.004, memory: 0, loss_rpn_cls: 0.0917, loss_rpn_bbox: 0.0416, loss_cls: 0.1574, acc: 95.5527, loss_bbox: 0.1628, loss: 0.4535
2020-07-20 13:06:49,444 - mmdet - INFO - Epoch [1][350/6126]	lr: 0.00175, eta: 1 day, 6:27:29, time: 0.852, data_time: 0.004, memory: 0, loss_rpn_cls: 0.0770, loss_rpn_bbox: 0.0381, loss_cls: 0.1825, acc: 95.0527, loss_bbox: 0.1809, loss: 0.4784
2020-07-20 13:07:31,353 - mmdet - INFO - Epoch [1][400/6126]	lr: 0.00200, eta: 1 day, 6:54:32, time: 0.838, data_time: 0.004, memory: 0, loss_rpn_cls: 0.0975, loss_rpn_bbox: 0.0485, loss_cls: 0.1670, acc: 95.3633, loss_bbox: 0.1667, loss: 0.4796
2020-07-20 13:08:13,908 - mmdet - INFO - Epoch [1][450/6126]	lr: 0.00225, eta: 1 day, 7:18:55, time: 0.851, data_time: 0.004, memory: 0, loss_rpn_cls: 0.0771, loss_rpn_bbox: 0.0506, loss_cls: 0.1793, acc: 94.5625, loss_bbox: 0.1999, loss: 0.5070
You can see,the data_time is always 0.004 and the memory is always 0,but the process of training is normal.What's wrong ?
	</description>
	<comments>
		<comment id='1' author='171339995' date='2020-07-20T06:57:00Z'>
		my environment info:
&lt;denchmark-h:h2&gt;2020-07-20 13:02:24,029 - mmdet - INFO - Environment info:&lt;/denchmark-h&gt;

sys.platform: linux
Python: 3.7.7 (default, May  7 2020, 21:25:33) [GCC 7.3.0]
CUDA available: True
CUDA_HOME: /usr/local/cuda-10.0
NVCC: Cuda compilation tools, release 10.0, V10.0.130
GPU 0,1,2,3: GeForce RTX 2080 Ti
GCC: gcc (Ubuntu 5.5.0-12ubuntu1) 5.5.0 20171010
PyTorch: 1.4.0
PyTorch compiling details: PyTorch built with:

GCC 7.3
Intel(R) Math Kernel Library Version 2020.0.1 Product Build 20200208 for Intel(R) 64 architecture applications
Intel(R) MKL-DNN v0.21.1 (Git Hash 7d2fd500bc78936d1d648ca713b901012f470dbc)
OpenMP 201511 (a.k.a. OpenMP 4.5)
NNPACK is enabled
CUDA Runtime 10.0
NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
CuDNN 7.6.3
Magma 2.5.1
Build settings: BLAS=MKL, BUILD_NAMEDTENSOR=OFF, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -fopenmp -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Wno-stringop-overflow, DISABLE_NUMA=1, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF,

&lt;denchmark-h:h2&gt;TorchVision: 0.5.0
OpenCV: 4.2.0
MMCV: 0.5.9
MMDetection: 2.1.0+9d2a343
MMDetection Compiler: GCC 5.5
MMDetection CUDA Compiler: 10.0&lt;/denchmark-h&gt;

2020-07-20 13:02:24,030 - mmdet - INFO - Distributed training: False
		</comment>
		<comment id='2' author='171339995' date='2020-08-11T08:21:01Z'>
		It is wired and I can't reproduce this bug. Which config you are training with? What's your run command?
		</comment>
	</comments>
</bug>