<bug id='2970' author='Chrisfsj2051' open_date='2020-06-10T07:11:24Z' closed_time='2020-09-05T07:10:52Z'>
	<summary>Segment Fault in test stage while training with GHM loss</summary>
	<description>

When I use &lt;denchmark-link:https://github.com/open-mmlab/mmdetection/blob/master/configs/ghm/retinanet_ghm_r50_fpn_1x_coco.py&gt;this config&lt;/denchmark-link&gt;
 to train retinanet, everything seems fine in training stage (except for the low speed of gradient descent). However, at the beginning of test stage, I met , which I have never seen in training models with other configs (i.e. without using ghm loss).
Reproduction
&lt;denchmark-code&gt;srun -p mediaf1 --job-name=retinanet_ghm_r50_fpn_1x_coco.py --gres=gpu:8 --ntasks=8 --ntasks-per-node=8 --cpus-per-task=1 --kill-on-bad-exit=1 -x 'SH-IDC2-172-20-20-[65,77]' python -u tools/train.py configs/ghm/retinanet_ghm_r50_fpn_1x_coco.py --work-dir=./mmdet_output/ghm/retinanet_ghm_r50_fpn_1x_coco.py --launcher=slurm
&lt;/denchmark-code&gt;

Also, you can find the full training log &lt;denchmark-link:https://paste.ubuntu.com/p/MfG3WPZYQq/&gt;here&lt;/denchmark-link&gt;
.
Environment

Please run python mmdet/utils/collect_env.py to collect necessary environment infomation and paste it here.

&lt;denchmark-code&gt;
sys.platform: linux
Python: 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) [GCC 7.3.0]
CUDA available: True
CUDA_HOME: /mnt/lustre/share/platform/dep/cuda-9.0-cudnn7.6
NVCC: Cuda compilation tools, release 9.0, V9.0.176
GPU 0,1,2,3,4,5,6,7: GeForce GTX 1080
GCC: gcc (GCC) 5.4.0
PyTorch: 1.3.0
PyTorch compiling details: PyTorch built with:
  - GCC 4.8
  - Intel(R) Math Kernel Library Version 2019.0.4 Product Build 20190411 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v0.20.5 (Git Hash 0125f28c61c1f822fd48570b4c1066f96fcb9b2e)
  - OpenMP 201107 (a.k.a. OpenMP 3.1)
  - NNPACK is enabled
  - CUDA Runtime 9.0
  - NVCC architecture flags: -gencode;arch=compute_52,code=sm_52;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70
  - CuDNN 7.6
  - Magma 2.5.0
  - Build settings: BLAS=MKL, BUILD_NAMEDTENSOR=OFF, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -fopenmp -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Wno-missi
ng-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -
Wno-error=deprecated-declarations -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math, DISABLE_NUMA=1, PERF_WITH
_AVX=1, PERF_WITH_AVX2=1, USE_CUDA=True, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=ON, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

TorchVision: 0.2.2
OpenCV: 4.1.1
MMCV: 0.5.9
MMDetection: 2.0.0+unknown
MMDetection Compiler: GCC 5.4
MMDetection CUDA Compiler: 9.0

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Chrisfsj2051' date='2020-06-12T05:53:04Z'>
		Thanks for your bug report, we reproduced this bug and we will fix it ASAP.
		</comment>
		<comment id='2' author='Chrisfsj2051' date='2020-09-05T07:10:52Z'>
		This issue should have been fixed in &lt;denchmark-link:https://github.com/open-mmlab/mmcv/pull/516&gt;open-mmlab/mmcv#516&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>