<bug id='3052' author='minan19605' open_date='2020-06-17T03:48:05Z' closed_time='2020-09-27T09:51:25Z'>
	<summary>CUDA out of memory when testing</summary>
	<description>
I train an HTC model with ResNeXt 50 successfully, but when I test this model, I have CUDA out of memory issue.  Is there any method to resolve this problem.
The traceback is:
python tools/test.py configs/my_config/htc_x50_32x4d_fpn_dconv_srcatch_coco_linux.py work_dirs/htc/20200618/epoch_2.pth --out work_dirs/htc/20200618/test_result_file.pkl --eval bbox segm
loading annotations into memory...
Done (t=0.09s)
creating index...
index created!
[                                                  ] 0/166, elapsed: 0s, ETA:Traceback (most recent call last):
File "tools/test.py", line 149, in 
main()
File "tools/test.py", line 127, in main
args.show_score_thr)
File "/home/ceyang/mmdetection/mmdet/apis/test.py", line 25, in single_gpu_test
result = model(return_loss=False, rescale=True, **data)
File "/home/ceyang/.conda/envs/dt2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in call
result = self.forward(*input, **kwargs)
File "/home/ceyang/.conda/envs/dt2/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 153, in forward
return self.module(*inputs[0], **kwargs[0])
File "/home/ceyang/.conda/envs/dt2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in call
result = self.forward(*input, **kwargs)
File "/home/ceyang/mmdetection/mmdet/core/fp16/decorators.py", line 49, in new_func
return old_func(*args, **kwargs)
File "/home/ceyang/mmdetection/mmdet/models/detectors/base.py", line 150, in forward
return self.forward_test(img, img_metas, **kwargs)
File "/home/ceyang/mmdetection/mmdet/models/detectors/base.py", line 131, in forward_test
return self.simple_test(imgs[0], img_metas[0], **kwargs)
File "/home/ceyang/mmdetection/mmdet/models/detectors/two_stage.py", line 192, in simple_test
x, proposal_list, img_metas, rescale=rescale)
File "/home/ceyang/mmdetection/mmdet/models/roi_heads/htc_roi_head.py", line 369, in simple_test
ori_shape, scale_factor, rescale)
File "/home/ceyang/mmdetection/mmdet/models/roi_heads/mask_heads/fcn_mask_head.py", line 224, in get_seg_masks
skip_empty=device.type == 'cpu')
File "/home/ceyang/mmdetection/mmdet/models/roi_heads/mask_heads/fcn_mask_head.py", line 304, in _do_paste_mask
grid = torch.stack([gx, gy], dim=3)
RuntimeError: CUDA out of memory. Tried to allocate 2.15 GiB (GPU 0; 7.79 GiB total capacity; 2.92 GiB already allocated; 2.13 GiB free; 4.87 GiB reserved in total by PyTorch)
&lt;denchmark-h:h2&gt;And my system information is as below:&lt;/denchmark-h&gt;

sys.platform: linux
Python: 3.7.4 (default, Aug 13 2019, 20:35:49) [GCC 7.3.0]
CUDA available: True
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 10.2, V10.2.89
GPU 0: GeForce RTX 2070
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.5.0
PyTorch compiling details: PyTorch built with:

GCC 7.3
C++ Version: 201402
Intel(R) Math Kernel Library Version 2019.0.4 Product Build 20190411 for Intel(R) 64 architecture applications
Intel(R) MKL-DNN v0.21.1 (Git Hash 7d2fd500bc78936d1d648ca713b901012f470dbc)
OpenMP 201511 (a.k.a. OpenMP 4.5)
NNPACK is enabled
CPU capability usage: AVX2
CUDA Runtime 10.2
NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
CuDNN 7.6.5
Magma 2.5.2
Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_INTERNAL_THREADPOOL_IMPL -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF,

TorchVision: 0.6.0
OpenCV: 3.4.2
MMCV: 0.5.1
MMDetection: 2.0.0+6603790
MMDetection Compiler: GCC 7.5
MMDetection CUDA Compiler: 10.2
	</description>
	<comments>
		<comment id='1' author='minan19605' date='2020-06-19T03:24:41Z'>
		I met the same problem. When testing, the memory consumption continues to rise.  How can i release the memory?
		</comment>
		<comment id='2' author='minan19605' date='2020-06-21T15:17:01Z'>
		Hi &lt;denchmark-link:https://github.com/minan19605&gt;@minan19605&lt;/denchmark-link&gt;
 @Conrad-hui
Sorry for the late reply.
Have you modified the config?
You may consider using &lt;denchmark-link:https://github.com/open-mmlab/mmdetection/blob/master/.github/ISSUE_TEMPLATE/error-report.md&gt;error report template&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='3' author='minan19605' date='2020-06-24T04:56:46Z'>
		Same error here, no matter how small size I edit, it continue got this error when testing......
		</comment>
		<comment id='4' author='minan19605' date='2020-07-03T10:11:08Z'>
		Im facing similar issue while training on v2.1.0 for a custom dataset. I am training on a custom dataset which is similar to cityscapes. It trained for cityscapes but not for my custom dataset.
I tried with v2.2.0. The training logs show an increase in memory and once it reaches the threshold of any GPU memory. It throws CUDA out of memory. Training ends but the GPU memory is not purged.
UPDATE: I'm training Group Normalization based model
		</comment>
		<comment id='5' author='minan19605' date='2020-07-13T08:25:22Z'>
		I tried with v2.3.0, testing mask_rcnn_r50 for lvis dataset. It was okay to validate while training (per epoch validation) with train.py, but when I try to test with test.py, CUDA Out of Memory occurs after 4~900 steps.
Has this problem been solved?
		</comment>
		<comment id='6' author='minan19605' date='2020-07-14T08:54:58Z'>
		&lt;denchmark-link:https://github.com/open-mmlab/mmdetection/blob/master/mmdet/apis/test.py#L60&gt;https://github.com/open-mmlab/mmdetection/blob/master/mmdet/apis/test.py#L60&lt;/denchmark-link&gt;

I think len(data['img_metas'][0].data) causes memory leakage since it copies its data.
I have solved the problem by changing it to data['img'][0].size(0), but I am not sure this solution can be applied for all cases. I am just using Mask RCNN based models.
		</comment>
		<comment id='7' author='minan19605' date='2020-08-25T04:21:25Z'>
		Hi &lt;denchmark-link:https://github.com/UdonDa&gt;@UdonDa&lt;/denchmark-link&gt;

Sorry for the late reply.
Have you tried ? Does the same problem occur?
Hi &lt;denchmark-link:https://github.com/minan19605&gt;@minan19605&lt;/denchmark-link&gt;
 @Conrad-hui &lt;denchmark-link:https://github.com/jinfagang&gt;@jinfagang&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/deepakksingh&gt;@deepakksingh&lt;/denchmark-link&gt;

I just tried  with  and  . The GPU memory is stable at 2409M usage.
Could you please try again with the latest code?
		</comment>
		<comment id='8' author='minan19605' date='2020-08-27T00:57:02Z'>
		Same problem.
MMCV: 1.0.5
MMDetection: 2.3.0+68d860d
sys.platform: linux
Python: 3.7.7 (default, May  7 2020, 21:25:33) [GCC 7.3.0]
CUDA available: True
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 10.1, V10.1.243
GPU 0: GeForce RTX 2070
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.5.0
PyTorch compiling details: PyTorch built with:

GCC 7.3
C++ Version: 201402
Intel(R) Math Kernel Library Version 2020.0.1 Product Build 20200208 for Intel(R) 64 architecture applications
Intel(R) MKL-DNN v0.21.1 (Git Hash 7d2fd500bc78936d1d648ca713b901012f470dbc)
OpenMP 201511 (a.k.a. OpenMP 4.5)
NNPACK is enabled
CPU capability usage: AVX2
CUDA Runtime 10.1
NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
CuDNN 7.6.3
Magma 2.5.2
Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_INTERNAL_THREADPOOL_IMPL -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF,

TorchVision: 0.6.0a0+82fd1c8
OpenCV: 4.3.0
MMCV: 1.0.5
MMDetection: 2.3.0+68d860d
MMDetection Compiler: GCC 7.5
MMDetection CUDA Compiler: 10.1
creating index...
index created!
[&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                     ] 97/166, 0.6 task/s, elapsed: 155s, ETA:   110sTraceback (most recent call last):
File "tools/test.py", line 153, in 
main()
File "tools/test.py", line 131, in main
args.show_score_thr)
File "/home/minan/mmdet/mmdetection/mmdet/apis/test.py", line 26, in single_gpu_test
result = model(return_loss=False, rescale=True, **data)
File "/home/minan/anaconda3/envs/mmdet/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in call
result = self.forward(*input, **kwargs)
File "/home/minan/anaconda3/envs/mmdet/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 153, in forward
return self.module(*inputs[0], **kwargs[0])
File "/home/minan/anaconda3/envs/mmdet/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in call
result = self.forward(*input, **kwargs)
File "/home/minan/mmdet/mmdetection/mmdet/core/fp16/decorators.py", line 51, in new_func
return old_func(*args, **kwargs)
File "/home/minan/mmdet/mmdetection/mmdet/models/detectors/base.py", line 173, in forward
return self.forward_test(img, img_metas, **kwargs)
File "/home/minan/mmdet/mmdetection/mmdet/models/detectors/base.py", line 153, in forward_test
return self.simple_test(imgs[0], img_metas[0], **kwargs)
File "/home/minan/mmdet/mmdetection/mmdet/models/detectors/two_stage.py", line 199, in simple_test
x, proposal_list, img_metas, rescale=rescale)
File "/home/minan/mmdet/mmdetection/mmdet/models/roi_heads/htc_roi_head.py", line 412, in simple_test
ori_shape, scale_factor, rescale)
File "/home/minan/mmdet/mmdetection/mmdet/models/roi_heads/mask_heads/fcn_mask_head.py", line 225, in get_seg_masks
skip_empty=device.type == 'cpu')
File "/home/minan/mmdet/mmdetection/mmdet/models/roi_heads/mask_heads/fcn_mask_head.py", line 300, in _do_paste_mask
grid = torch.stack([gx, gy], dim=3)
RuntimeError: CUDA out of memory. Tried to allocate 2.15 GiB (GPU 0; 7.80 GiB total capacity; 2.92 GiB already allocated; 2.02 GiB free; 5.08 GiB reserved in total by PyTorch)
		</comment>
		<comment id='9' author='minan19605' date='2020-08-28T07:31:04Z'>
		Similar problem but I'm not running out of GPU memory but out of RAM after several runs of single_gpu_test on test set of arround 1000 elements (trying to do hyperparameters optimizations)
		</comment>
	</comments>
</bug>