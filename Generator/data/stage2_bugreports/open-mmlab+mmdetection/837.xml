<bug id='837' author='GYxiaOH' open_date='2019-06-20T04:18:32Z' closed_time='2019-06-21T04:53:25Z'>
	<summary>when i use ohem.RuntimeError</summary>
	<description>
when i use ohem,it will show
....
File "/search/odin/hongyuan/mmdetection-master/mmdet/core/bbox/samplers/ohem_sampler.py", line 40, in hard_mining
_, topk_loss_inds = loss.topk(num_expected)
RuntimeError: invalid argument 5: k not in range for dimension at /pytorch/aten/src/THC/generic/THCTensorTopK.cu:21
when I use old version of mmdetection, the problem will not apper
	</description>
	<comments>
		<comment id='1' author='GYxiaOH' date='2019-06-20T05:34:15Z'>
		We will fix it soon.
		</comment>
		<comment id='2' author='GYxiaOH' date='2019-06-20T05:35:35Z'>
		&lt;denchmark-link:https://github.com/open-mmlab/mmdetection/issues/824&gt;#824&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='GYxiaOH' date='2019-06-20T07:08:37Z'>
		&lt;denchmark-link:https://github.com/GYxiaOH&gt;@GYxiaOH&lt;/denchmark-link&gt;
  I try to fix it as the following, but I am not sure whether it would fix all bugs.
# mmdet/models/bbox_heads/bbox_head.py

def loss(self,
         cls_score,
         bbox_pred,
         labels,
         label_weights,
         bbox_targets,
         bbox_weights,
         reduce=True):
    losses = dict()
    if cls_score is not None:
        # TODO: when reduce == False
        avg_factor = max(torch.sum(label_weights &gt; 0).float().item(), 1.) if reduce else None
        reduction = 'mean' if reduce else 'none'

        losses['loss_cls'] = self.loss_cls(
            cls_score, labels, label_weights, avg_factor=avg_factor, reduction=reduction)
        losses['acc'] = accuracy(cls_score, labels)
# mmdet/models/losses/cross_entropy_loss.py

@LOSSES.register_module
class CrossEntropyLoss(nn.Module):

    def __init__(self,
                 use_sigmoid=False,
                 use_mask=False,
                 loss_weight=1.0):
        super(CrossEntropyLoss, self).__init__()
        assert (use_sigmoid is False) or (use_mask is False)
        self.use_sigmoid = use_sigmoid
        self.use_mask = use_mask
        self.loss_weight = loss_weight

        if self.use_sigmoid:
            self.cls_criterion = binary_cross_entropy
        elif self.use_mask:
            self.cls_criterion = mask_cross_entropy
        else:
            self.cls_criterion = cross_entropy

    def forward(self, cls_score, label, weight=None, avg_factor=None, reduction='mean',
                **kwargs):
        loss_cls = self.loss_weight * self.cls_criterion(
            cls_score,
            label,
            weight,
            avg_factor=avg_factor,
            reduction=reduction,
            **kwargs)
        return loss_cls
		</comment>
		<comment id='4' author='GYxiaOH' date='2019-06-20T07:19:02Z'>
		
@GYxiaOH I try to fix it as the following, but I am not sure whether it would fix all bugs.
# mmdet/models/bbox_heads/bbox_head.py

def loss(self,
         cls_score,
         bbox_pred,
         labels,
         label_weights,
         bbox_targets,
         bbox_weights,
         reduce=True):
    losses = dict()
    if cls_score is not None:
        # TODO: when reduce == False
        avg_factor = max(torch.sum(label_weights &gt; 0).float().item(), 1.) if reduce else None
        reduction = 'mean' if reduce else 'none'

        losses['loss_cls'] = self.loss_cls(
            cls_score, labels, label_weights, avg_factor=avg_factor, reduction=reduction)
        losses['acc'] = accuracy(cls_score, labels)
# mmdet/models/losses/cross_entropy_loss.py

@LOSSES.register_module
class CrossEntropyLoss(nn.Module):

    def __init__(self,
                 use_sigmoid=False,
                 use_mask=False,
                 loss_weight=1.0):
        super(CrossEntropyLoss, self).__init__()
        assert (use_sigmoid is False) or (use_mask is False)
        self.use_sigmoid = use_sigmoid
        self.use_mask = use_mask
        self.loss_weight = loss_weight

        if self.use_sigmoid:
            self.cls_criterion = binary_cross_entropy
        elif self.use_mask:
            self.cls_criterion = mask_cross_entropy
        else:
            self.cls_criterion = cross_entropy

    def forward(self, cls_score, label, weight=None, avg_factor=None, reduction='mean',
                **kwargs):
        loss_cls = self.loss_weight * self.cls_criterion(
            cls_score,
            label,
            weight,
            avg_factor=avg_factor,
            reduction=reduction,
            **kwargs)
        return loss_cls

Thank you,I will try
		</comment>
		<comment id='5' author='GYxiaOH' date='2019-06-21T04:53:25Z'>
		should have been fixed in &lt;denchmark-link:https://github.com/open-mmlab/mmdetection/pull/839&gt;#839&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='GYxiaOH' date='2020-09-18T13:50:30Z'>
		when I used ohem in faster rcnn，it occurs the problem
File "/home/mrl/Documents/mmdetection/mmdet/core/bbox/samplers/ohem_sampler.py", line 47, in hard_mining
_, topk_loss_inds = loss.topk(num_expected)
RuntimeError: invalid argument 5: k not in range for dimension at /pytorch/aten/src/THC/generic/THCTensorTopK.cu:26
How can i fix it
		</comment>
		<comment id='7' author='GYxiaOH' date='2020-10-13T07:51:05Z'>
		
when I used ohem in faster rcnn，it occurs the problem
File "/home/mrl/Documents/mmdetection/mmdet/core/bbox/samplers/ohem_sampler.py", line 47, in hard_mining
_, topk_loss_inds = loss.topk(num_expected)
RuntimeError: invalid argument 5: k not in range for dimension at /pytorch/aten/src/THC/generic/THCTensorTopK.cu:26
How can i fix it

me too
		</comment>
	</comments>
</bug>