<bug id='1223' author='FilipLangr' open_date='2019-08-20T13:29:36Z' closed_time='2019-09-25T07:35:44Z'>
	<summary>assert len(indices) == self.total_size error during multiple GPU training</summary>
	<description>
I am trying to train my dataset on 8 GPU's. However, after calling ./dist_train.sh this error assertion appeares:
Traceback (most recent call last):
File "./tools/train.py", line 113, in 
main()
File "./tools/train.py", line 109, in main
logger=logger)
File "/mmdetection/mmdet/apis/train.py", line 58, in train_detector
_dist_train(model, dataset, cfg, validate=validate)
File "/mmdetection/mmdet/apis/train.py", line 186, in _dist_train
runner.run(data_loaders, cfg.workflow, cfg.total_epochs)
File "/opt/conda/lib/python3.6/site-packages/mmcv/runner/runner.py", line 358, in run
epoch_runner(data_loaders[i], **kwargs)
File "/opt/conda/lib/python3.6/site-packages/mmcv/runner/runner.py", line 260, in train
for i, data_batch in enumerate(data_loader):
File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 193, in iter    return _DataLoaderIter(self)
File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 493, in init
self._put_indices()
File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 591, in _put_indices
indices = next(self.sample_iter, None)
File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/sampler.py", line 172, in iter
for idx in self.sampler:
File "/mmdetection/mmdet/datasets/loader/sampler.py", line 138, in iter
assert len(indices) == self.total_size
...
in the config I tried various values for imgs_per_gpu and workers_per_gpu, currently it is:
imgs_per_gpu=2, workers_per_gpu=2,
no settings was working though. Single-GPU training works well.
What is the meaning of this assert?
Thanks!
	</description>
	<comments>
		<comment id='1' author='FilipLangr' date='2019-08-20T15:02:02Z'>
		Please follow the Error report issue template.
		</comment>
		<comment id='2' author='FilipLangr' date='2019-08-20T15:13:51Z'>
		
Please follow the Error report issue template.

Here it is, thanks for any help!
Checklist

I have searched related issues but cannot get the expected help.
yes
The bug has not been fixed in the latest version.
yup

Describe the bug
I am trying to train my custom dataset on 8 GPU's. However, after calling ./dist_train.sh the error showed below appeares. In the config I tried more values for imgs_per_gpu and workers_per_gpu (e.g. imgs_per_gpu=2, workers_per_gpu=2), no settings was working though.
Single-GPU training works well.
What is the meaning of the assert in the Traceback? What does not fit? Thanks!
Reproduction

What command or script did you run?

&lt;denchmark-code&gt;./tools/dist_train.sh MY_CONFIG 8 --validate
&lt;/denchmark-code&gt;


Did you make any modifications on the code or config? Did you understand what you have modified?
I modified number of classes, workers_per_gpu, imgs_per_gpu, dataset type and paths to the datasets. No changes in code.
What dataset did you use?
My own dataset of 8 classes converted to COCO format.

Environment

OS: Ubuntu 18.04
GCC 5.4.0
PyTorch version 1.1.0
I built the docker I use on the official pytorch+cuda docker
GPU model: 8xV100
CUDA version: 10.0, CUDNN version: 7.5

Error traceback
&lt;denchmark-code&gt;Traceback (most recent call last):
File "./tools/train.py", line 113, in 
main()
File "./tools/train.py", line 109, in main
logger=logger)
File "/mmdetection/mmdet/apis/train.py", line 58, in train_detector
_dist_train(model, dataset, cfg, validate=validate)
File "/mmdetection/mmdet/apis/train.py", line 186, in _dist_train
runner.run(data_loaders, cfg.workflow, cfg.total_epochs)
File "/opt/conda/lib/python3.6/site-packages/mmcv/runner/runner.py", line 358, in run
epoch_runner(data_loaders[i], **kwargs)
File "/opt/conda/lib/python3.6/site-packages/mmcv/runner/runner.py", line 260, in train
for i, data_batch in enumerate(data_loader):
File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 193, in iter return _DataLoaderIter(self)
File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 493, in init
self._put_indices()
File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 591, in _put_indices
indices = next(self.sample_iter, None)
File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/sampler.py", line 172, in iter
for idx in self.sampler:
File "/mmdetection/mmdet/datasets/loader/sampler.py", line 138, in iter
assert len(indices) == self.total_size
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='FilipLangr' date='2019-08-20T15:20:17Z'>
		One more detail, when I print len(indices) and self.total_size right before the critical assert, it's 9308 and 9312. The size of my training dataset is 9306..
		</comment>
		<comment id='4' author='FilipLangr' date='2019-08-22T10:51:25Z'>
		&lt;denchmark-link:https://github.com/hellock&gt;@hellock&lt;/denchmark-link&gt;
 Any ideas? Recently I found out that when I set the config file to train with 2 GPUs (2 img/gpu), the training (called with ) initiates well. However, training with any more GPUs results in the mentioned assert error. Seems like a bug to me.
		</comment>
		<comment id='5' author='FilipLangr' date='2019-08-22T11:32:27Z'>
		You may count the images with aspect ratio &gt;1 and &lt;1. I suspect that there are only 2 images for one of the two groups.
		</comment>
		<comment id='6' author='FilipLangr' date='2019-08-22T11:58:27Z'>
		
You may count the images with aspect ratio &gt;1 and &lt;1. I suspect that there are only 2 images for one of the two groups.

you are right, there are exactly 2 images with height &gt; width
		</comment>
		<comment id='7' author='FilipLangr' date='2019-08-22T12:08:35Z'>
		The problem lies in &lt;denchmark-link:https://github.com/open-mmlab/mmdetection/blob/master/mmdet/datasets/loader/sampler.py#L135&gt;this line&lt;/denchmark-link&gt;
 where .
		</comment>
		<comment id='8' author='FilipLangr' date='2019-08-23T03:11:45Z'>
		I meet the same issue, how to fix it?
		</comment>
		<comment id='9' author='FilipLangr' date='2019-09-02T15:57:06Z'>
		&lt;denchmark-link:https://github.com/ZhexuanZhou&gt;@ZhexuanZhou&lt;/denchmark-link&gt;

Before bug fixed, simplest way is make your img_per_gpu as power of 2.
e.g. 2, 4, 8, 16 ...
Also, make your gpu number as power of 2.
This works for me.
		</comment>
		<comment id='10' author='FilipLangr' date='2019-09-02T16:15:22Z'>
		
@ZhexuanZhou
Before bug fixed, simplest way is make your img_per_gpu as power of 2.
e.g. 2, 4, 8, 16 ...
Also, make your gpu number as power of 2.
This works for me.

This fix doesn't work for me, as is already mentioned in the bug description

Describe the bug
I am trying to train my custom dataset on 8 GPU's. However, after calling ./dist_train.sh the error showed below appeares. In the config I tried more values for imgs_per_gpu and workers_per_gpu (e.g. imgs_per_gpu=2, workers_per_gpu=2), no settings was working though.

The easiest work-around for me was to comment out the two asserts in the sampler.py :)
		</comment>
		<comment id='11' author='FilipLangr' date='2019-09-02T19:45:34Z'>
		&lt;denchmark-link:https://github.com/FilipLangr&gt;@FilipLangr&lt;/denchmark-link&gt;

Yeah, that might cause some samples lost but not that harmful.
		</comment>
		<comment id='12' author='FilipLangr' date='2019-09-13T07:48:20Z'>
		Hello!
I have situation, when 6 pics, where w&gt;h.
33994 pics, when h&gt;w.
And 2 pics, where h==w.
I have deleted h==w pics and have AssertionError: assert len(indices) == self.total_size anyway :(
		</comment>
		<comment id='13' author='FilipLangr' date='2019-09-13T08:03:31Z'>
		Than I deleted w&gt;h pics and get another error: TypeError: 'NoneType' object is not subscriptable
		</comment>
	</comments>
</bug>