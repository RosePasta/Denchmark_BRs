<bug id='287' author='lostella' open_date='2019-09-05T19:29:33Z' closed_time='2019-09-09T19:27:50Z'>
	<summary>NegativeBinomial not robust enough</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

NegativeBinomial distributions are created when models are configured to use the NegativeBinomialOutput class. If the model happens to output somewhat “very” negative values, then these are softplus’ed to something very close to zero, which breaks the log_prob method of NegativeBinomial because of some logarithm.
This happens sometimes in training models, but it’s rather hard to come up with a MWE.
Either NegativeBinomialOutput or NegativeBinomial (or both) should be made more robust wrt this numerical issue.
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

Not quite a MWE, but this is what happens under the hood.
&lt;denchmark-code&gt;import mxnet as mx
from gluonts.distribution.neg_binomial import NegativeBinomialOutput
network_output = mx.nd.ones(shape=(10,))
distr_output = NegativeBinomialOutput()
args_proj = distr_output.get_args_proj()
args_proj.initialize(init=mx.init.Constant(-1e2))
distr_args = args_proj(network_output)
distr = distr_output.distribution(distr_args)
x = mx.nd.array([1.0])
ll = distr.log_prob(x)
print(ll)
&lt;/denchmark-code&gt;

which gives ll = [ nan ]
	</description>
	<comments>
	</comments>
</bug>