<bug id='721' author='konradsemsch' open_date='2020-03-24T09:03:02Z' closed_time='2020-03-24T17:21:38Z'>
	<summary>m5_gluonts_template.ipynb - M5Evaluator returns all values as NaN</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

I've been trying to reproduce that template, but unfortunately the implementation for returns NaN in all evaluation metrics:
&lt;denchmark-link:https://user-images.githubusercontent.com/15321842/77407447-6a1d4500-6db6-11ea-88b8-6bc37d15e499.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;


Operating system: macOS 10.15.3
Python version: Python 3.7.5
GluonTS version: gluonts==0.4.2

My entire environment details below:
appnope==0.1.0
attrs==19.3.0
aws-roleshell==0.0.1
backcall==0.1.0
bleach==3.1.3
boto3==1.12.27
botocore==1.15.27
certifi==2019.11.28
chardet==3.0.4
cycler==0.10.0
decorator==4.4.2
defusedxml==0.6.0
docutils==0.15.2
entrypoints==0.3
gluonts==0.4.2
graphviz==0.8.4
holidays==0.9.12
idna==2.9
importlib-metadata==1.5.0
ipykernel==5.2.0
ipython==7.13.0
ipython-genutils==0.2.0
ipywidgets==7.5.1
jedi==0.16.0
Jinja2==2.11.1
jmespath==0.9.5
json5==0.9.3
jsonschema==3.2.0
jupyter-client==6.1.0
jupyter-core==4.6.3
jupyterlab==2.0.1
jupyterlab-server==1.0.7
kiwisolver==1.1.0
MarkupSafe==1.1.1
matplotlib==3.2.1
mistune==0.8.4
mxnet==1.6.0
nbconvert==5.6.1
nbformat==5.0.4
notebook==6.0.3
numpy==1.18.2
pandas==0.24.2
pandocfilters==1.4.2
parso==0.6.2
pexpect==4.8.0
pickleshare==0.7.5
prometheus-client==0.7.1
prompt-toolkit==3.0.4
ptyprocess==0.6.0
pydantic==1.4
Pygments==2.6.1
pyparsing==2.4.6
pyrsistent==0.15.7
python-dateutil==2.8.0
pytz==2019.3
pyzmq==19.0.0
requests==2.23.0
s3transfer==0.3.3
Send2Trash==1.5.0
six==1.14.0
terminado==0.8.3
testpath==0.4.4
tornado==6.0.4
tqdm==4.43.0
traitlets==4.3.3
ujson==1.35
urllib3==1.25.8
wcwidth==0.1.9
webencodings==0.5.1
widgetsnbextension==3.5.1
zipp==3.1.0
	</description>
	<comments>
		<comment id='1' author='konradsemsch' date='2020-03-24T11:47:56Z'>
		Hi Konrad! I just re-ran the notebook template using the environment details you provided and don't seem to have any issues with the evaluation. Did you alter the notebook in a meaningful way? Can you confirm that both  and  contain valid values and that forecasts are plotted at the end of the notebook? I've attached &lt;denchmark-link:https://github.com/awslabs/gluon-ts/files/4375010/m5_gluonts_template_exec.pdf&gt;my notebook execution&lt;/denchmark-link&gt;
 for reference.
		</comment>
		<comment id='2' author='konradsemsch' date='2020-03-24T14:31:38Z'>
		Hi &lt;denchmark-link:https://github.com/steverab&gt;@steverab&lt;/denchmark-link&gt;
! First of all apologies for many screenshots below, but I got an error that I need to install xelatex before exporting to pdf and I didn't want to waste more time training the model and making predictions.
I didn't make any other changes to the notebook except for:

input data directory
removing tqdm function when making forecasts

Generally speaking, everything seems to work fine but the summary metrics. I can confirm that both tss and forecasts contain valid values, however, tss also contains NaN as you will see below.
On the other hand plots are produced correctly I guess.
&lt;denchmark-link:https://github.com/awslabs/gluon-ts/files/4375822/Archive.zip&gt;Archive.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='konradsemsch' date='2020-03-24T15:04:04Z'>
		Oh I see what is going on. You set submission=False right before then evaluation, while what you rather need to do is to set submission=False in the second code segment at the very beginning of the notebook. The dataset is sliced differently depending on what submission is set to (this is needed since we don't have access to the true Kaggle labels and therefore need to create our own validation set) which causes the issue in true_divide which is logged in one of your screenshots and which in turn results in NaN values. Can you try running the notebook again while making sure you set submission=False right after the import statements?
		</comment>
		<comment id='4' author='konradsemsch' date='2020-03-24T15:29:17Z'>
		Deaam, now I see that. I completely missed that piece in the beginning - thanks for catching this error! It's still running bit when it's done I will confirm whether it was successful.
I actually also wanted to ask you guys a question regarding building datasets and the way dynamic/static and cat/real columns are treated in Gluon. I already read the Quick Start, as well as Extended Forecating Tutorial (until Datasets) but when I confront the information written there with this template implementation I'm a bit puzzled.
When I read those definitions and then check the column present in the M5 dataset:
&lt;denchmark-link:https://user-images.githubusercontent.com/15321842/77441863-38be6c80-6dea-11ea-8ce6-1557acc4ae9c.png&gt;&lt;/denchmark-link&gt;

I would group all available columns in the following way:
feat_static_cat: 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'
feat_static_real: none
feat_dynamic_cat: 'wm_yr_wk', 'weekday', 'wday', 'month', 'year', 'd',
'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',
'snap_CA', 'snap_TX', 'snap_WI'
feat_dynamic_real: 'sell_price'
With regards to this I would have a couple questions:


in the template those features that I classified as feat_dynamic_cat were mapped to feat_dynamic_real, but aren't they actually categorical features? Or maybe differently, any features directly inferred from the date could be 'real', but aren't events and snaps 'categorical'?


I noticed that event types were converted to binary variables and NaN were replaced by integers. Do all categorical variables need to be converted to integers before plugging them into the algo (for instance in order to use event_name_x)? What about missing values in that context? Perhaps the Transformation part says something about imputation but I haven't got there yet, wanted to clarify this first


DeepAREstimator has no argument use_feat_dynamic_cat - perhaps that's the reason why all categorical features were mapped to real? Does the embedding_dimensionapply only to feat_static_cat?


I was also looking in the documentation/ Github repo for good resources on fine-tuning DeepARE/ Other GluonTS models - could you reference some relevant materials?
		</comment>
		<comment id='5' author='konradsemsch' date='2020-03-24T16:04:47Z'>
		Regarding the metrics, I can confirm that now they are working - thanks!
&lt;denchmark-link:https://user-images.githubusercontent.com/15321842/77448818-8a1e2a00-6df1-11ea-864e-1d885ac1a3f2.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='konradsemsch' date='2020-03-24T16:44:35Z'>
		Glad to hear that this fixed your problem!
Regarding the covariates: The grouping that you came up with sounds reasonable. Unfortunately, most of our models do not support dynamic categorical features at the moment, which is why we currently pass them as real features instead. Also, as already explained in the notebook, some time-varying features will be added by many models automatically as part of their default Transformation chains. And yes, embedding_dimension only applies to feat_static_cat in DeepAR.
Keep in mind that this notebook is just a template, i.e. a starting point which shows how to load and process the data, train a predictor, and evaluate said predictor on a test set or return forecasts to be submitted to Kaggle. We therefore explicitly encourage you to play around with the additional covariates and the general training configuration to improve your performance.
As for fine-tuning, the usual hyper-parameter selection best-practices apply and you should definitely do that to obtain good performance. Some general guidance on this can be found &lt;denchmark-link:https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html&gt;here&lt;/denchmark-link&gt;
, while some references on how to do this for DeepAR specifically can be found &lt;denchmark-link:https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html&gt;here&lt;/denchmark-link&gt;
. Although our SageMaker implementation is slightly different from GluonTS, the most relevant concepts and parameters do carry over.
		</comment>
		<comment id='7' author='konradsemsch' date='2020-03-24T17:07:08Z'>
		
Regarding the covariates: The grouping that you came up with sounds reasonable. Unfortunately, most of our models do not support dynamic categorical features at the moment, which is why we currently pass them as real features instead. Also, as already explained in the notebook, some time-varying features will be added by many models automatically as part of their default Transformation chains. And yes, embedding_dimension only applies to feat_static_cat in DeepAR.

I see now. I think that clarifies all questions I had so far.

Keep in mind that this notebook is just a template, i.e. a starting point which shows how to load and process the data, train a predictor, and evaluate said predictor on a test set or return forecasts to be submitted to Kaggle. We therefore explicitly encourage you to play around with the additional covariates and the general training configuration to improve your performance.

Right, that's perfectly clear and I really appreciate that you guys came up with this, as well as took the time to explain those points! It's much easier now for me to get started once these base concepts are clearer.
Thanks again!
		</comment>
		<comment id='8' author='konradsemsch' date='2020-03-24T17:21:38Z'>
		Sure thing! :)
		</comment>
		<comment id='9' author='konradsemsch' date='2020-03-25T08:51:05Z'>
		Hi &lt;denchmark-link:https://github.com/steverab&gt;@steverab&lt;/denchmark-link&gt;
, after doing a bit more reading I just wanted to ask an additional question regarding Transformations in GluonTs:


you mentioned that e.g. DeepAR builds additional features baed on dates and frequency by itself, but  is my understanding correct that this does not cover additional features that could be calculated using Transformations. These need to be specified additionally right?


regarding missing values: in the tutorial NaN were replaced with a binary feature before constructing the dataset, but I see the AddObservedValuesIndicator  function which could also do it (in the tutorial applied to the target). Is AddObservedValuesIndicator mainly supposed to work with the target or it's also beneficial to use it with other variables which would be then passed with their NaNs into the dataset contructor?


		</comment>
		<comment id='10' author='konradsemsch' date='2020-03-25T09:11:14Z'>
		Ok, actually the source code sheds some light. But what could be done is for instance modify the InstanceSplitter to use also other feature fields and not only  FieldName.FEAT_TIME and               FieldName.OBSERVED_VALUES?
def create_transformation(self) -&gt; Transformation:
        remove_field_names = [FieldName.FEAT_DYNAMIC_CAT]
        if not self.use_feat_static_real:
            remove_field_names.append(FieldName.FEAT_STATIC_REAL)
        if not self.use_feat_dynamic_real:
            remove_field_names.append(FieldName.FEAT_DYNAMIC_REAL)

        return Chain(
            [RemoveFields(field_names=remove_field_names)]
            + (
                [SetField(output_field=FieldName.FEAT_STATIC_CAT, value=[0.0])]
                if not self.use_feat_static_cat
                else []
            )
            + (
                [
                    SetField(
                        output_field=FieldName.FEAT_STATIC_REAL, value=[0.0]
                    )
                ]
                if not self.use_feat_static_real
                else []
            )
            + [
                AsNumpyArray(
                    field=FieldName.FEAT_STATIC_CAT,
                    expected_ndim=1,
                    dtype=self.dtype,
                ),
                AsNumpyArray(
                    field=FieldName.FEAT_STATIC_REAL,
                    expected_ndim=1,
                    dtype=self.dtype,
                ),
                AsNumpyArray(
                    field=FieldName.TARGET,
                    # in the following line, we add 1 for the time dimension
                    expected_ndim=1 + len(self.distr_output.event_shape),
                    dtype=self.dtype,
                ),
                AddObservedValuesIndicator(
                    target_field=FieldName.TARGET,
                    output_field=FieldName.OBSERVED_VALUES,
                    dtype=self.dtype,
                ),
                AddTimeFeatures(
                    start_field=FieldName.START,
                    target_field=FieldName.TARGET,
                    output_field=FieldName.FEAT_TIME,
                    time_features=self.time_features,
                    pred_length=self.prediction_length,
                ),
                AddAgeFeature(
                    target_field=FieldName.TARGET,
                    output_field=FieldName.FEAT_AGE,
                    pred_length=self.prediction_length,
                    log_scale=True,
                    dtype=self.dtype,
                ),
                VstackFeatures(
                    output_field=FieldName.FEAT_TIME,
                    input_fields=[FieldName.FEAT_TIME, FieldName.FEAT_AGE]
                    + (
                        [FieldName.FEAT_DYNAMIC_REAL]
                        if self.use_feat_dynamic_real
                        else []
                    ),
                ),
                InstanceSplitter(
                    target_field=FieldName.TARGET,
                    is_pad_field=FieldName.IS_PAD,
                    start_field=FieldName.START,
                    forecast_start_field=FieldName.FORECAST_START,
                    train_sampler=ExpectedNumInstanceSampler(num_instances=1),
                    past_length=self.history_length,
                    future_length=self.prediction_length,
                    time_series_fields=[
                        FieldName.FEAT_TIME,
                        FieldName.OBSERVED_VALUES,
                    ],
                ),
            ]
        )
		</comment>
		<comment id='11' author='konradsemsch' date='2020-05-22T09:03:42Z'>
		Hi &lt;denchmark-link:https://github.com/steverab&gt;@steverab&lt;/denchmark-link&gt;
! Sorry for bugging you again on this topic but I was hoping you could advise me a bit. I modified this initial implementation in a way that (at least to me) is more understandable using the following approach:

First merge all inputs together and convert to a long format
Then adapt all individual model inputs from this single data source

The advantage of this solution is that all pre-processing steps appear to be a bit more clear and understandable, as well as easier to calculate new features (based on combinations of other features), however, it comes at the expense of processing time (perhaps that could be also optimized). Nevertheless, my problem is that even though it seems to me that all model inputs are exactly the same between my and your notebook, the base model (same parameters &amp; features), your scores roughly 0.9 on the public leaderboard, while mine in high 2.XX... I've been carefully examining all model inputs, their shapes, structures etc. using only a single time-serie and I can't find any differences.
The model doesn't complain when training about anything. Could you please take a quick look at this and advice where the error might be? Perhaps other users will also find my notebook useful in understanding how data input structures should be built.
My environment:
[[source]]
name = "pypi"
url = "https://pypi.org/simple"
verify_ssl = true

[dev-packages]

[packages]
mxnet = "*"
numpy = "*"
pandas = "==0.24.2"
matplotlib = "*"
jupyterlab = "*"
ipywidgets = "*"
xlrd = "*"
gluonts = "==0.5.0"

[requires]
python_version = "3.7"
I've uploaded my notebook as well as one modified calendar input file (the other input files are same located in a 'data_raw' directory). This calendar file modification should not have an impact on performance, it was just helpful to make these modification to get this output data structure.
&lt;denchmark-link:https://github.com/awslabs/gluon-ts/files/4667051/Archive.zip&gt;Archive.zip&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>