<bug id='608' author='jaheba' open_date='2020-02-08T11:08:19Z' closed_time='2020-02-18T10:46:21Z'>
	<summary>infinite weighted quantile loss</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

When the true-target values are zeros, our weighted quantile loss become inf.
This is especially a problem for HPO jobs, where we rely on the mean_wQuantileLoss as our objective metric to access the models performance.
&lt;denchmark-code&gt;[2020-02-08 12:04:19 +0100] [59599] [INFO] #test_score (local, wQuantileLoss[0.1]): inf
[2020-02-08 12:04:19 +0100] [59599] [INFO] #test_score (local, wQuantileLoss[0.2]): inf
[2020-02-08 12:04:19 +0100] [59599] [INFO] #test_score (local, wQuantileLoss[0.3]): inf
[2020-02-08 12:04:19 +0100] [59599] [INFO] #test_score (local, wQuantileLoss[0.4]): inf
[2020-02-08 12:04:19 +0100] [59599] [INFO] #test_score (local, wQuantileLoss[0.5]): inf
[2020-02-08 12:04:19 +0100] [59599] [INFO] #test_score (local, wQuantileLoss[0.6]): inf
[2020-02-08 12:04:19 +0100] [59599] [INFO] #test_score (local, wQuantileLoss[0.7]): inf
[2020-02-08 12:04:19 +0100] [59599] [INFO] #test_score (local, wQuantileLoss[0.8]): inf
[2020-02-08 12:04:19 +0100] [59599] [INFO] #test_score (local, wQuantileLoss[0.9]): inf
[2020-02-08 12:04:19 +0100] [59599] [INFO] #test_score (local, mean_wQuantileLoss): inf
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

The test time-series needs to contain zeros in the prediction-range. The following example triggers the problem until a prediction_length of 4.
{"start": "2020", "target": [2, 1, 0, 0, 0, 0]}
	</description>
	<comments>
		<comment id='1' author='jaheba' date='2020-02-08T11:09:13Z'>
		Here is the code triggering it:



gluon-ts/src/gluonts/evaluation/_base.py


        Lines 321 to 323
      in
      90587ba






 totals[quantile.weighted_loss_name] = np.divide( 



 totals[quantile.loss_name], totals["abs_target_sum"] 



 ) 





		</comment>
		<comment id='2' author='jaheba' date='2020-02-08T11:21:28Z'>
		Looking at tensorflow, they seem to assume a minimum value by which they &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/ca123bc4199a42c65dcc817b6c771fa613171e08/tensorflow/python/keras/losses.py#L796&gt;divide&lt;/denchmark-link&gt;
:
diff = math_ops.abs(
      (y_true - y_pred) / K.clip(math_ops.abs(y_true), K.epsilon(), None))
According to &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/backend/epsilon&gt;this&lt;/denchmark-link&gt;
, epsilon is .
		</comment>
		<comment id='3' author='jaheba' date='2020-02-08T11:44:22Z'>
		This only happens when all true values are 0 (or really close to it). I would propose to just return the unscaled value in that case.
		</comment>
		<comment id='4' author='jaheba' date='2020-02-08T14:31:06Z'>
		I think “inf” is actually the correct and more useful output. Would it maybe be an option to check beforehand whether the test set is only zeros and use a different metric in that case?
		</comment>
		<comment id='5' author='jaheba' date='2020-02-08T14:36:10Z'>
		Another option would be to put the logic you propose in your script and apply it before logging metrics.
		</comment>
		<comment id='6' author='jaheba' date='2020-02-18T10:46:21Z'>
		No fix.
We also emit the unscaled version, which can be used for these cases.
		</comment>
	</comments>
</bug>