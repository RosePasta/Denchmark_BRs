<bug id='94' author='houchangtao' open_date='2019-06-12T00:30:05Z' closed_time='2019-06-20T09:25:59Z'>
	<summary>Dynamical Feature with Different Length will Throw Exception</summary>
	<description>
System: Ubuntu 16.04
Python: 3.6.4
mxnet: 1.4.1
Code to reproduce:
import pandas as pd
url = "https://raw.githubusercontent.com/numenta/NAB/master/data/realTweets/Twitter_volume_AMZN.csv"
df = pd.read_csv(url, header=0, index_col=0)
from gluonts.dataset.common import ListDataset
training_data = [
    {"start": pd.Timestamp(df.index[0], freq='5min'), "target": df.value[:"2015-04-05 00:00:00"], 
     "feat_dynamic_real": pd.to_datetime(df[:"2015-04-05 00:00:00"].index).dayofweek.values},
    {"start": pd.Timestamp(df.index[0], freq='5min'), "target": df.value[:"2015-04-10 00:00:00"], 
     "feat_dynamic_real": pd.to_datetime(df[:"2015-04-10 00:00:00"].index).dayofweek.values}
]
from gluonts.model.deepar import DeepAREstimator
from gluonts.trainer import Trainer
from gluonts.distribution import NegativeBinomialOutput, StudentTOutput
estimator = DeepAREstimator(freq="5min", prediction_length=12, distr_output=NegativeBinomialOutput(),
                            trainer=Trainer(epochs=10))
predictor = estimator.train(training_data=training_data)
Exceptions:
&lt;denchmark-code&gt;KeyError                                  Traceback (most recent call last)
&lt;ipython-input-9-7512b68a283e&gt; in &lt;module&gt;()
      1 estimator = DeepAREstimator(freq="5min", prediction_length=12, distr_output=NegativeBinomialOutput(),
      2                             trainer=Trainer(epochs=10))
----&gt; 3 predictor = estimator.train(training_data=training_data)

/usr/local/lib/python3.6/site-packages/gluonts-0.1.1-py3.6.egg/gluonts/model/estimator.py in train(self, training_data)
    187     def train(self, training_data: Dataset) -&gt; Predictor:
    188 
--&gt; 189         training_transformation, trained_net = self.train_model(training_data)
    190 
    191         # ensure that the prediction network is created within the same MXNet

/usr/local/lib/python3.6/site-packages/gluonts-0.1.1-py3.6.egg/gluonts/model/estimator.py in train_model(self, training_data)
    180             net=trained_net,
    181             input_names=get_hybrid_forward_input_names(trained_net),
--&gt; 182             train_iter=training_data_loader,
    183         )
    184 

/usr/local/lib/python3.6/site-packages/gluonts-0.1.1-py3.6.egg/gluonts/trainer/_base.py in __call__(self, net, input_names, train_iter)
    249 
    250                     with tqdm(train_iter) as it:
--&gt; 251                         for batch_no, data_entry in enumerate(it, start=1):
    252                             if self.halt:
    253                                 break

/usr/local/lib/python3.6/site-packages/tqdm/_tqdm.py in __iter__(self)
   1003                 """), fp_write=getattr(self.fp, 'write', sys.stderr.write))
   1004 
-&gt; 1005             for obj in iterable:
   1006                 yield obj
   1007                 # Update and possibly print the progressbar.

/usr/local/lib/python3.6/site-packages/gluonts-0.1.1-py3.6.egg/gluonts/dataset/loader.py in __iter__(self)
    200             ):
    201                 for batch in self._emit_batches_while_buffer_larger_than(
--&gt; 202                     self.batch_size - 1
    203                 ):
    204                     yield batch

/usr/local/lib/python3.6/site-packages/gluonts-0.1.1-py3.6.egg/gluonts/dataset/loader.py in _emit_batches_while_buffer_larger_than(self, thresh)
    167             self._buffer.shuffle()
    168         while len(self._buffer) &gt; thresh:
--&gt; 169             yield self._buffer.next_batch()
    170 
    171     def _iterate_forever(

/usr/local/lib/python3.6/site-packages/gluonts-0.1.1-py3.6.egg/gluonts/dataset/loader.py in next_batch(self)
     58             #print(np.asarray(v[:n]).dtype)
     59             #print(v[:n])
---&gt; 60             batch[k] = self.stack(v[:n])
     61         #batch = {k: self.stack(v[:n]) for k, v in self._buffers.items()}
     62         for key in self._buffers.keys():

/usr/local/lib/python3.6/site-packages/gluonts-0.1.1-py3.6.egg/gluonts/dataset/loader.py in stack(self, xs)
     70             if data.dtype.kind == 'f':
     71                 data = data.astype(self.float_type)
---&gt; 72             return mx.nd.array(data, dtype=data.dtype, ctx=self.ctx)
     73         elif isinstance(xs[0], mx.nd.NDArray):
     74             return mx.nd.stack(*xs)

/usr/local/lib/python3.6/site-packages/mxnet/ndarray/utils.py in array(source_array, ctx, dtype)
    144         return _sparse_array(source_array, ctx=ctx, dtype=dtype)
    145     else:
--&gt; 146         return _array(source_array, ctx=ctx, dtype=dtype)
    147 
    148 

/usr/local/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py in array(source_array, ctx, dtype)
   2486             except:
   2487                 raise TypeError('source_array must be array like object')
-&gt; 2488     arr = empty(source_array.shape, ctx, dtype)
   2489     arr[:] = source_array
   2490     return arr

/usr/local/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py in empty(shape, ctx, dtype)
   3875     if dtype is None:
   3876         dtype = mx_real_t
-&gt; 3877     return NDArray(handle=_new_alloc_handle(shape, ctx, False, dtype))
   3878 
   3879 

/usr/local/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py in _new_alloc_handle(shape, ctx, delay_alloc, dtype)
    137         ctypes.c_int(ctx.device_id),
    138         ctypes.c_int(int(delay_alloc)),
--&gt; 139         ctypes.c_int(int(_DTYPE_NP_TO_MX[np.dtype(dtype).type])),
    140         ctypes.byref(hdl)))
    141     return hdl

KeyError: &lt;class 'numpy.object_'&gt;
&lt;/denchmark-code&gt;

Error comes from: 


gluon-ts/src/gluonts/dataset/loader.py


         Line 62
      in
      3c6d110






 data = np.asarray(xs) 





	</description>
	<comments>
		<comment id='1' author='houchangtao' date='2019-06-12T09:52:22Z'>
		&lt;denchmark-link:https://github.com/houchangtao&gt;@houchangtao&lt;/denchmark-link&gt;
 One issue with your example is that you're feeding a list directly as a dataset for training.
Using the ListDataset would be appropriate here, but this results in an error as well:
import pandas as pd
url = "https://raw.githubusercontent.com/numenta/NAB/master/data/realTweets/Twitter_volume_AMZN.csv"
df = pd.read_csv(url, header=0, index_col=0)
from gluonts.dataset.common import ListDataset
training_data = ListDataset(
    data_iter=[
        {"start": pd.Timestamp(df.index[0], freq='5min'), "target": df.value[:"2015-04-05 00:00:00"],
         "feat_dynamic_real": pd.to_datetime(df[:"2015-04-05 00:00:00"].index).dayofweek.values},
        {"start": pd.Timestamp(df.index[0], freq='5min'), "target": df.value[:"2015-04-10 00:00:00"],
         "feat_dynamic_real": pd.to_datetime(df[:"2015-04-10 00:00:00"].index).dayofweek.values}
    ],
    freq="5min"
)
from gluonts.model.deepar import DeepAREstimator
from gluonts.trainer import Trainer
from gluonts.distribution import NegativeBinomialOutput
estimator = DeepAREstimator(freq="5min", prediction_length=12, distr_output=NegativeBinomialOutput(),
                            trainer=Trainer(epochs=10))
predictor = estimator.train(training_data=training_data)
Results in:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/Users/stellalo/gluon-ts/temp/run_issue_94.py", line 19, in &lt;module&gt;
    predictor = estimator.train(training_data=training_data)
  File "/Users/stellalo/gluon-ts/src/gluonts/model/estimator.py", line 189, in train
    training_transformation, trained_net = self.train_model(training_data)
  File "/Users/stellalo/gluon-ts/src/gluonts/model/estimator.py", line 182, in train_model
    train_iter=training_data_loader,
  File "/Users/stellalo/gluon-ts/src/gluonts/trainer/_base.py", line 251, in __call__
    for batch_no, data_entry in enumerate(it, start=1):
  File "/Users/stellalo/.virtualenvs/gluonts/lib/python3.6/site-packages/tqdm/_tqdm.py", line 930, in __iter__
    for obj in iterable:
  File "/Users/stellalo/gluon-ts/src/gluonts/dataset/loader.py", line 195, in __iter__
    self.batch_size - 1
  File "/Users/stellalo/gluon-ts/src/gluonts/dataset/loader.py", line 162, in _emit_batches_while_buffer_larger_than
    yield self._buffer.next_batch()
  File "/Users/stellalo/gluon-ts/src/gluonts/dataset/loader.py", line 54, in next_batch
    batch = {k: self.stack(v[:n]) for k, v in self._buffers.items()}
  File "/Users/stellalo/gluon-ts/src/gluonts/dataset/loader.py", line 54, in &lt;dictcomp&gt;
    batch = {k: self.stack(v[:n]) for k, v in self._buffers.items()}
  File "/Users/stellalo/gluon-ts/src/gluonts/dataset/loader.py", line 62, in stack
    data = np.asarray(xs)
  File "/Users/stellalo/.virtualenvs/gluonts/lib/python3.6/site-packages/numpy/core/numeric.py", line 492, in asarray
    return array(a, dtype, copy=False, order=order)
ValueError: could not broadcast input array from shape (10684) into shape (1)

Process finished with exit code 1
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='houchangtao' date='2019-06-12T11:17:21Z'>
		MWE:
from gluonts.dataset.common import ListDataset
from gluonts.model.deepar import DeepAREstimator
from gluonts.trainer import Trainer
training_data = ListDataset(
    data_iter=[
        {"start": "2019-01-01 00:00:00", "target": [1.0, 2.0, 3.0, 4.0],
         "feat_dynamic_real": [1.0, 2.0, 3.0, 4.0]},
        {"start": "2019-01-01 00:00:00", "target": [1.0, 2.0, 3.0, 4.0, 5.0],
         "feat_dynamic_real": [1.0, 2.0, 3.0, 4.0, 5.0]},
    ],
    freq="5min"
)
estimator = DeepAREstimator(freq="5min", prediction_length=2,
                            trainer=Trainer(epochs=10))
predictor = estimator.train(training_data=training_data)
		</comment>
		<comment id='3' author='houchangtao' date='2019-06-12T13:50:26Z'>
		The problem is that the  field is &lt;denchmark-link:https://github.com/awslabs/gluon-ts/blob/cd5df5814797e602ec4a525d89f4bc84b6e5e82f/src/gluonts/dataset/common.py#L258&gt;processed&lt;/denchmark-link&gt;
 as part of the , but not stacked together with other time-dependent fields &lt;denchmark-link:https://github.com/awslabs/gluon-ts/blob/a8c60fac38296268e5b55afb828568021ff23531/src/gluonts/model/deepar/_estimator.py#L189-L192&gt;in the transformation chain&lt;/denchmark-link&gt;
. Therefore &lt;denchmark-link:https://github.com/awslabs/gluon-ts/blob/cd5df5814797e602ec4a525d89f4bc84b6e5e82f/src/gluonts/dataset/loader.py#L51-L58&gt;when a batch of data is formed&lt;/denchmark-link&gt;
 this field is stacked as-is between different entries in the dataset.
As far as I can see, solving this issues means either:

(easier) at the beginning of the transformation, explicitly filtering out any fields which will not be consumed
(harder) completing the transformation chain of DeepAREstimator (and potentially other models), and make it consume all possible fields, possibly defaulting the missing ones (so that no KeyErrors are raised)

		</comment>
		<comment id='4' author='houchangtao' date='2019-06-20T09:25:54Z'>
		&lt;denchmark-link:https://github.com/awslabs/gluon-ts/pull/125&gt;#125&lt;/denchmark-link&gt;
 solves this issues: now features must be explicitly enabled, when constructing the estimator, in order for the model to use them.
		</comment>
	</comments>
</bug>