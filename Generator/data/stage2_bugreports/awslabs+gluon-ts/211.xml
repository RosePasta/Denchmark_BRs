<bug id='211' author='illarum' open_date='2019-07-21T04:09:13Z' closed_time='2019-07-22T14:40:31Z'>
	<summary>Model unexpectedly quits during training</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

DeepAREstimator hangs and freezes during training. Completes 0 training iterations and epochs. Sometimes will freeze entire OS.
I'm able to allocate memory to the GPU via NDArray, as well as converge a CNN MNIST model with normal Gluon. I have tried disabling CUDNN autotune.
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

Code:
&lt;denchmark-code&gt;df = pd.read_csv("test.csv", header=0, index_col=0)
td = ListDataset([{"start": df.index[0], "target": df.value}], freq="1min")

estimator = DeepAREstimator(freq="1min", prediction_length=60, trainer=Trainer(epochs=1, ctx=mx.gpu(0)))
predictors = estimator.train(training_data=td)
&lt;/denchmark-code&gt;

Dataset (test.csv):
&lt;denchmark-code&gt;time           value
1279425840     4.00
1279425900     5.00
...                ...
1551063419  2254.00
1551063479  2257.00
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Error Message&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;  0%|                                                    | 0/50 [00:00&lt;?, ?it/s]
Killed
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;


Operating system: Ubuntu 18.04
Python version: 3.6.8
GluonTS version: 0.2.3
MXNet version: cu101mkl 1.5.0
-GPU: GTX 1070 (responds to nvidia-smi)
-CUDA: Release 10.1

	</description>
	<comments>
		<comment id='1' author='illarum' date='2019-07-21T09:19:55Z'>
		I believe df.value will yield a pd.Series object. Can you try using
df.value.tolist()
instead?
		</comment>
		<comment id='2' author='illarum' date='2019-07-21T15:12:56Z'>
		Doing that still yields the same "error". One interesting thing I noticed is that if I encounter the error in the python interpreter, the entire interpreter quits instead of continuing execution.
		</comment>
		<comment id='3' author='illarum' date='2019-07-21T17:22:34Z'>
		Update: It looks like installing gluonts also installs the normal mxnet package as a dependency. It doesn't use the preexisting mxnet-cu101mkl package I have installed. I think this problem may have something to do with the fact that I now have duplicate packages installed.
		</comment>
		<comment id='4' author='illarum' date='2019-07-21T18:09:06Z'>
		Oh, I had missed that gpu context. Yes, that might be the problem. For now, you should be able to work around it with
&lt;denchmark-code&gt;pip install gluonts 
pip uninstall mxnet 
pip install mxnet-cu101mkl
&lt;/denchmark-code&gt;

In the future we may want to remove mxnet from requirements.txt
		</comment>
		<comment id='5' author='illarum' date='2019-07-21T18:13:43Z'>
		After doing that and resolving a numpy version conflict, the same thing happens. Interestingly, GPU utilization before the crash is around 0%, while CPU is at 100%.
More importantly, the model is killed because, as I have just discovered, it uses 100% of my 32gb of memory. The dataset I'm using is ~80MB.
		</comment>
		<comment id='6' author='illarum' date='2019-07-21T19:18:03Z'>
		Update: it runs on a smaller dataset but encounters the following error:
mxnet.base.MXNetError: vector::_M_range_insert
whenever I try to predict anything in any capacity.
The same error was encountered in a closed bug and had to do with installation (&lt;denchmark-link:https://github.com/awslabs/gluon-ts/issues/150&gt;#150&lt;/denchmark-link&gt;
). The root problem may very well be the dependencies.
		</comment>
		<comment id='7' author='illarum' date='2019-07-22T11:27:45Z'>
		&lt;denchmark-link:https://github.com/RMaron&gt;@RMaron&lt;/denchmark-link&gt;
 are you using ? If so, try downgrading it to , there is some incompatibility with MXNet 1.5.0 which needs to be solved, &lt;denchmark-link:http://ci.mxnet.io/blue/organizations/jenkins/GluonTS-py3-cpu-unittest/detail/PR-214/1/pipeline&gt;see e.g. this CI job here&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='8' author='illarum' date='2019-07-22T11:28:19Z'>
		
@RMaron are you using mxnet==1.5.0?

Yes you are :-)
		</comment>
		<comment id='9' author='illarum' date='2019-07-22T14:40:31Z'>
		That fixed it! Thank you.
To anyone reading this: the Killed error was caused by 100% utilization of memory.
Solution: use a smaller dataset.
mxnet.base.MXNetError: vector::_M_range_insert was due to an incompatibility with MXNET 1.5.0. Solution: downgrade to 1.4.1.
		</comment>
		<comment id='10' author='illarum' date='2019-07-22T15:27:39Z'>
		
To anyone reading this: the Killed error was caused by 100% utilization of memory.
Solution: use a smaller dataset.

This doesn't sound right: how long is df.value.tolist() in your case?
		</comment>
		<comment id='11' author='illarum' date='2019-07-22T15:34:01Z'>
		4,504,326 values. I can watch top as memory usage goes up to 100%, and then it instantly is killed. Not sure if there's some underlying issue but using a smaller dataset worked for me.
		</comment>
	</comments>
</bug>