<bug id='1066' author='elenaehrlich' open_date='2020-09-30T15:40:08Z' closed_time='2020-10-06T14:31:14Z'>
	<summary>Gluon-ts DeepAR not able to learn a Gamma-Gaussian MixtureDistributionOutput() for x_t \in Reals</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

Gluon-ts MixtureDistributionOutput() of Gamma-Gaussian is able to learn the component weights and arguments for real-valued data. Why is Gluon-ts DeepAR not similarly able to learn the component weights and arguments of a Gamma-Gaussian MixtureDistributionOutput() from the same data?
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

from gluonts.mx.distribution.mixture import *
from gluonts.mx.distribution.gaussian import *
from gluonts.mx.distribution.gamma import *
import numpy as np
from gluonts.dataset import common
from gluonts.model import deepar
from gluonts.mx.trainer import Trainer
from gluonts.model.common import Tensor, NPArrayLike
from gluonts.gluonts_tqdm import tqdm
import gluonts
print(f'gluonts {gluonts.__version__}')


##########################################
# Function to learn MLE
def fit_mixture_distribution(x: Tensor, mdo: MixtureDistributionOutput, variate_dimensionality: int = 1, epochs: Optional[int]=1_000):

    args_proj = mdo.get_args_proj()
    args_proj.initialize()
    args_proj.hybridize()

    input = mx.nd.ones((variate_dimensionality, 1))


    trainer = mx.gluon.Trainer(
        args_proj.collect_params(), "sgd", {"learning_rate": 0.02}
    )
    print('trainer.learning_rate',trainer.learning_rate)

    t = tqdm(list(range(epochs)))
    for _ in t:
        with mx.autograd.record():
            distr_args = args_proj(input)
            d = mdo.distribution(distr_args)
            loss = d.loss(x).mean()
        loss.backward()
        loss_value = loss.asnumpy()
        t.set_postfix({"loss": loss_value})
        trainer.step(1)

    distr_args = args_proj(input)
    d = mdo.distribution(distr_args)
    return d

##########################################
# Simulate data 
mx.random.seed(1)
np.random.seed(1)
num_samples = 1000
p1 = 0.2; p2 = 1.0 - p1
mu1 = 0.0; sigma1 = 0.2
alpha = 9.; beta = 2.0
mixture = MixtureDistribution(mx.nd.array([[p1, p2]]), [Gaussian(mx.nd.array([mu1]), mx.nd.array([sigma1])), Gamma(mx.nd.array([alpha]), mx.nd.array([beta]))])
mixture_samples = mixture.sample(num_samples=num_samples)
print('How close is the simulated data to zero? mixture_samples.abs().min() =', mixture_samples.abs().min().asscalar(), '\n')

print('true mixture.components')
for c in mixture.components:
    print(c)


##########################################
print('\nMWE: gluon-ts MixtureDistributionOutput() of Gamma-Gaussian is able to learn the component weights and arguments for real-valued data')
mdo = MixtureDistributionOutput([GaussianOutput(), GammaOutput()])
fit_mixture = fit_mixture_distribution(mixture_samples, mdo, mixture_samples.shape[1], epochs=1_000)
print('fit_mixture.components')
for c in fit_mixture.components:
    print(c)

for ci, c in enumerate(fit_mixture.components):
    for ai, a in enumerate(c.args):
        assert ~np.isnan(a.asnumpy()), \
            f"NaN gradients led to {c}"


##########################################
print('\nWhy is gluon-ts DeepAR not able to learn the component weights and arguments of a Gamma-Gaussian MixtureDistributionOutput() from the same data?')
data = common.ListDataset(
    [
        {
            "start": '2015-02-26 21:42:53',
            "target": mixture_samples.asnumpy().ravel()
        }
    ],
    freq="1min"
)

trainer = Trainer(epochs=3, hybridize=False, learning_rate=0.00001)
print('trainer.learning_rate', trainer.learning_rate)
estimator = deepar.DeepAREstimator(
    freq="1min",
    prediction_length=12,
    trainer=trainer,
    distr_output=mdo,
)
predictor = estimator.train(training_data=data)
&lt;denchmark-h:h2&gt;Error message or code output&lt;/denchmark-h&gt;

(Paste the complete error message, including stack trace, or the undesired output that the above snippet produces.)
&lt;denchmark-code&gt;gluonts 0.5.1.dev102+g88a9832.d20200924
How close is the simulated data to zero? mixture_samples.abs().min() = 0.00029572556 

true mixture.components
gluonts.mx.distribution.gaussian.Gaussian(mu=mxnet.nd.array([0.0], dtype=numpy.float32), sigma=mxnet.nd.array([0.20000000298023224], dtype=numpy.float32))
gluonts.mx.distribution.gamma.Gamma(alpha=mxnet.nd.array([9.0], dtype=numpy.float32), beta=mxnet.nd.array([2.0], dtype=numpy.float32))

MWE: gluon-ts MixtureDistributionOutput() of Gamma-Gaussian is able to learn the component weights and arguments for real-valued data
trainer.learning_rate 0.02
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:05&lt;00:00, 171.16it/s, loss=[1.9887046]]
fit_mixture.components
gluonts.mx.distribution.gaussian.Gaussian(mu=mxnet.nd.array([-0.021975774317979813], dtype=numpy.float32), sigma=mxnet.nd.array([0.1961863487958908], dtype=numpy.float32))
gluonts.mx.distribution.gamma.Gamma(alpha=mxnet.nd.array([3.9501376152038574], dtype=numpy.float32), beta=mxnet.nd.array([0.8736407160758972], dtype=numpy.float32))

gluon-ts DeepAR should be able to learn the component weights and arguments of a Gamma-Gaussian MixtureDistributionOutput() for the same data
trainer.learning_rate 1e-05
learning rate from ``lr_scheduler`` has been overwritten by ``learning_rate`` in optimizer.
  2%|██▌                                                                                                                              | 1/50 [00:00&lt;00:06,  7.65it/s, epoch=1/3, avg_epoch_loss=2.79]Epoch[0] gave nan loss
  2%|██▌                                                                                                                              | 1/50 [00:00&lt;00:11,  4.15it/s, epoch=1/3, avg_epoch_loss=2.79]
Traceback (most recent call last):
  File "example_deepar_fails_mixture_learning.py", line 65, in &lt;module&gt;
    predictor = estimator.train(training_data=data)
  File "/Users/eeehrlic/Documents/projects/capstone/gluon-ts/src/gluonts/model/estimator.py", line 270, in train
    return self.train_model(
  File "/Users/eeehrlic/Documents/projects/capstone/gluon-ts/src/gluonts/model/estimator.py", line 245, in train_model
    self.trainer(
  File "/Users/eeehrlic/Documents/projects/capstone/gluon-ts/src/gluonts/mx/trainer/_base.py", line 376, in __call__
    raise GluonTSUserError(
gluonts.core.exception.GluonTSUserError: Got NaN in first epoch. Try reducing initial learning rate.
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;


Operating system: Mac OSx 10.14.6
Python version: 3.8.5
GluonTS version: 0.5.1.dev102+g88a9832.d20200924
MXNet version: 1.6.0

(Add as much information about your environment as possible, e.g. dependencies versions.)
	</description>
	<comments>
		<comment id='1' author='elenaehrlich' date='2020-10-05T16:43:59Z'>
		Interesting!
This is related to &lt;denchmark-link:https://github.com/awslabs/gluon-ts/issues/519&gt;#519&lt;/denchmark-link&gt;
. PR &lt;denchmark-link:https://github.com/awslabs/gluon-ts/pull/534&gt;#534&lt;/denchmark-link&gt;
 should have fixed that, but it did not for this specific case. I think this is an issue with  failing on the padded zeros which get introduced by the instance splitter.
The current implementation of the Gamma Gaussian Mixture does seem to fail if zeros are in the dataset.
See this simple example:
alpha = mx.nd.array([0.5])
mu =mx.nd.array([1])
sigma =mx.nd.array([1])
a =mx.nd.array([1])
b =mx.nd.array([1])
alpha.attach_grad()
mu.attach_grad()
sigma.attach_grad()
a.attach_grad()
b.attach_grad()

with mx.autograd.record():
    loss = mx.nd.mean(MixtureDistribution(mixture_probs=mx.nd.stack(alpha, 1-alpha, axis =-1), components=[Gaussian(mu=mu, sigma=sigma), Gamma(a,b)]).loss(x=mx.nd.array([0])))
    loss.backward()
    
print(alpha.grad)
print(mu.grad)
print(sigma.grad)
print(a.grad)
print(b.grad)

[-2.]
&lt;NDArray 1 @cpu(0)&gt;

[1.]
&lt;NDArray 1 @cpu(0)&gt;

[0.]
&lt;NDArray 1 @cpu(0)&gt;

[nan]
&lt;NDArray 1 @cpu(0)&gt;

[0.]
&lt;NDArray 1 @cpu(0)&gt;
Nevertheless your example should work, since there are no zeros in the original data.
A quick fix is setting pick_incomplete=False argument of the instance splitter transformation of the DeepAREstimator. That way, no zero-padding is introduced.
To do that, you can create a custom estimator which inherits from the DeepAREstimator and simply copy its create_transformation() method, but set pick_incomplete=False:
from gluonts.model.deepar import DeepAREstimator
import mxnet as mx
import numpy as np

from gluonts.mx.distribution.mixture import *
from gluonts.mx.distribution.gaussian import *
from gluonts.mx.distribution.gamma import *
import numpy as np
from gluonts.dataset import common
from gluonts.mx.trainer import Trainer
from gluonts.transform import (
    AddAgeFeature,
    AddObservedValuesIndicator,
    AddTimeFeatures,
    AsNumpyArray,
    Chain,
    ExpectedNumInstanceSampler,
    InstanceSplitter,
    RemoveFields,
    SetField,
    Transformation,
    VstackFeatures,
)
from gluonts.dataset.field_names import FieldName


mx.random.seed(1)
np.random.seed(1)
num_samples = 1000
p1 = 0.2; p2 = 1.0 - p1
mu1 = 0.0; sigma1 = 0.2
alpha = 9.; beta = 2.0
mixture = MixtureDistribution(mx.nd.array([[p1, p2]]), [Gaussian(mx.nd.array([mu1]), mx.nd.array([sigma1])), Gamma(mx.nd.array([alpha]), mx.nd.array([beta]))])
mixture_samples = mixture.sample(num_samples=num_samples)

data = common.ListDataset(
    [
        {
            "start": '2015-02-26 21:42:53',
            "target": mixture_samples.asnumpy().ravel()
        }
    ],
    freq="1min"
)

trainer = Trainer(epochs=3, hybridize=False, learning_rate=0.00000001)
print('trainer.learning_rate', trainer.learning_rate)

class DeepAREstimator_pi(DeepAREstimator):
    def create_transformation(self):
        remove_field_names = [FieldName.FEAT_DYNAMIC_CAT]
        if not self.use_feat_static_real:
            remove_field_names.append(FieldName.FEAT_STATIC_REAL)
        if not self.use_feat_dynamic_real:
            remove_field_names.append(FieldName.FEAT_DYNAMIC_REAL)

        return Chain(
            [RemoveFields(field_names=remove_field_names)]
            + (
                [SetField(output_field=FieldName.FEAT_STATIC_CAT, value=[0.0])]
                if not self.use_feat_static_cat
                else []
            )
            + (
                [
                    SetField(
                        output_field=FieldName.FEAT_STATIC_REAL, value=[0.0]
                    )
                ]
                if not self.use_feat_static_real
                else []
            )
            + [
                AsNumpyArray(
                    field=FieldName.FEAT_STATIC_CAT,
                    expected_ndim=1,
                    dtype=self.dtype,
                ),
                AsNumpyArray(
                    field=FieldName.FEAT_STATIC_REAL,
                    expected_ndim=1,
                    dtype=self.dtype,
                ),
                AsNumpyArray(
                    field=FieldName.TARGET,
                    # in the following line, we add 1 for the time dimension
                    expected_ndim=1 + len(self.distr_output.event_shape),
                    dtype=self.dtype,
                ),
                AddObservedValuesIndicator(
                    target_field=FieldName.TARGET,
                    output_field=FieldName.OBSERVED_VALUES,
                    dtype=self.dtype,
                    imputation_method=self.imputation_method,
                ),
                AddTimeFeatures(
                    start_field=FieldName.START,
                    target_field=FieldName.TARGET,
                    output_field=FieldName.FEAT_TIME,
                    time_features=self.time_features,
                    pred_length=self.prediction_length,
                ),
                AddAgeFeature(
                    target_field=FieldName.TARGET,
                    output_field=FieldName.FEAT_AGE,
                    pred_length=self.prediction_length,
                    log_scale=True,
                    dtype=self.dtype,
                ),
                VstackFeatures(
                    output_field=FieldName.FEAT_TIME,
                    input_fields=[FieldName.FEAT_TIME, FieldName.FEAT_AGE]
                    + (
                        [FieldName.FEAT_DYNAMIC_REAL]
                        if self.use_feat_dynamic_real
                        else []
                    ),
                ),
                InstanceSplitter(
                    target_field=FieldName.TARGET,
                    is_pad_field=FieldName.IS_PAD,
                    start_field=FieldName.START,
                    forecast_start_field=FieldName.FORECAST_START,
                    train_sampler=ExpectedNumInstanceSampler(num_instances=1),
                    past_length=self.history_length,
                    future_length=self.prediction_length,
                    time_series_fields=[
                        FieldName.FEAT_TIME,
                        FieldName.OBSERVED_VALUES,
                    ],
                    dummy_value=self.distr_output.value_in_support,
                    pick_incomplete = False
                ),
            ]
        )

        
    

estimator = DeepAREstimator_pi(
    freq="1min",
    prediction_length=12,
    trainer=trainer,
    distr_output=MixtureDistributionOutput(distr_outputs=[GaussianOutput(), GammaOutput()]),
)
predictor = estimator.train(training_data=data)
However we should fix in any case:

The MixtureDistribution of a Gamma and a Gaussian should not lead to NaN gradients for x=0.
Even if the output distribution does not support zeros, zero-padding should not lead to NaN gradients.

		</comment>
		<comment id='2' author='elenaehrlich' date='2020-10-06T09:27:12Z'>
		I realized the instance splitter does not always use zero padding. Instead it uses the value_in_support property of the DistributionOutput. The value_in_support() currently defaults to 0. for all MixtureDistributions. I think it makes sense to instead return the value_in_support() of the first component.
This means only this problem remains:

The MixtureDistribution of a Gamma and a Gaussian should not lead to NaN gradients for x=0.

		</comment>
		<comment id='3' author='elenaehrlich' date='2020-10-06T14:31:14Z'>
		Fixed this with &lt;denchmark-link:https://github.com/awslabs/gluon-ts/pull/1078&gt;#1078&lt;/denchmark-link&gt;
.
&lt;denchmark-link:https://github.com/elenaehrlich&gt;@elenaehrlich&lt;/denchmark-link&gt;
 feel free to reopen if you experience any issues.
		</comment>
	</comments>
</bug>