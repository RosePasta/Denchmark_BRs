<bug id='374' author='marvin-hansen' open_date='2019-10-08T14:18:35Z' closed_time='2019-10-08T19:03:06Z'>
	<summary>GPU setup broken</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

Installation of GPU enabled mxnet still runs gluonts code on CPU.
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

$ pip install gluonts
$ pip install mxnet-cu100==1.4.1 # Avoids the 1.5 bug and matches exact CUDA version
import mxnet as mx
print("Number GPU's: " + str(mx.context.num_gpus()))
Returns 0
Essentially, installing mxnet with CUDA supports makes no difference.
The same conda environment with PyTorch finds CUDA.
uninstalling the non-cuda mx package causes an error, and installing it leads to not using the GPU.
&lt;denchmark-h:h2&gt;Error Message&lt;/denchmark-h&gt;

Traceback (most recent call last): File "C:/Users/marvi/PycharmProjects/GNL/Examples/bechmark_models.py", line 10, in &lt;module&gt; from gluonts.dataset.repository.datasets import get_dataset File "C:\Users\marvi\Anaconda3\envs\GNL\lib\site-packages\gluonts\dataset\repository\datasets.py", line 19, in &lt;module&gt; from gluonts.dataset.artificial import ConstantDataset File "C:\Users\marvi\Anaconda3\envs\GNL\lib\site-packages\gluonts\dataset\artificial\__init__.py", line 15, in &lt;module&gt; from ._base import ( File "C:\Users\marvi\Anaconda3\envs\GNL\lib\site-packages\gluonts\dataset\artificial\_base.py", line 24, in &lt;module&gt; from gluonts.dataset.common import ( File "C:\Users\marvi\Anaconda3\envs\GNL\lib\site-packages\gluonts\dataset\common.py", line 41, in &lt;module&gt; from gluonts.dataset.stat import ( File "C:\Users\marvi\Anaconda3\envs\GNL\lib\site-packages\gluonts\dataset\stat.py", line 23, in &lt;module&gt; from gluonts.core.component import validated File "C:\Users\marvi\Anaconda3\envs\GNL\lib\site-packages\gluonts\core\component.py", line 26, in &lt;module&gt; import mxnet as mx File "C:\Users\marvi\Anaconda3\envs\GNL\lib\site-packages\mxnet\__init__.py", line 24, in &lt;module&gt; from .context import Context, current_context, cpu, gpu, cpu_pinned File "C:\Users\marvi\Anaconda3\envs\GNL\lib\site-packages\mxnet\context.py", line 24, in &lt;module&gt; from .base import classproperty, with_metaclass, _MXClassPropertyMetaClass File "C:\Users\marvi\Anaconda3\envs\GNL\lib\site-packages\mxnet\base.py", line 213, in &lt;module&gt; _LIB = _load_lib() File "C:\Users\marvi\Anaconda3\envs\GNL\lib\site-packages\mxnet\base.py", line 204, in _load_lib lib = ctypes.CDLL(lib_path[0], ctypes.RTLD_LOCAL) File "C:\Users\marvi\Anaconda3\envs\GNL\lib\ctypes\__init__.py", line 364, in __init__ self._handle = _dlopen(self._name, mode) OSError: [WinError 126] The specified module could not be found
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;


Operating system: Windows 10
Python version: 3.7
GluonTS version: 0.3

(Add as much information about your environment as possible, e.g. dependencies versions.)
	</description>
	<comments>
		<comment id='1' author='marvin-hansen' date='2019-10-08T14:22:58Z'>
		I think that the problem is that GluonTS checks the presence of GPUs by calling  (&lt;denchmark-link:https://github.com/awslabs/gluon-ts/blob/master/setup.py#L21&gt;here&lt;/denchmark-link&gt;
), and this method doesn't work properly on Windows. Here is an article on  on Windows: &lt;denchmark-link:https://stackoverflow.com/questions/57100015/how-do-i-run-nvidia-smi-on-windows&gt;https://stackoverflow.com/questions/57100015/how-do-i-run-nvidia-smi-on-windows&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='marvin-hansen' date='2019-10-08T15:04:59Z'>
		Hi &lt;denchmark-link:https://github.com/marvin-hansen&gt;@marvin-hansen&lt;/denchmark-link&gt;
,

uninstalling the non-cuda mx package causes an error, and installing it leads to not using the GPU

can you clarify what exactly the expected vs. actual behavior is? Which code is not using the GPU? If mx.context.num_gpus() returns 0, that is most likely an issue with your MXNet installation and not GluonTS specific.
		</comment>
		<comment id='3' author='marvin-hansen' date='2019-10-08T15:23:02Z'>
		&lt;denchmark-link:https://github.com/strawberrypie&gt;@strawberrypie&lt;/denchmark-link&gt;
 nvidia-smi works fine and I can train PyTorch code on GPU.
&lt;denchmark-link:https://github.com/jgasthaus&gt;@jgasthaus&lt;/denchmark-link&gt;
 yes, you are right, it is a mxnet issue. Suggestions or close this issue?
		</comment>
		<comment id='4' author='marvin-hansen' date='2019-10-08T19:03:06Z'>
		The MXNET GPU issue seems to be related to conda / cudatoolkit and indeed unrelated.
		</comment>
	</comments>
</bug>