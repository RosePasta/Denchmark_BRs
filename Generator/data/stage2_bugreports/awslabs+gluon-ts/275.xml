<bug id='275' author='rshyamsundar' open_date='2019-08-30T21:47:25Z' closed_time='2019-09-02T13:47:39Z'>
	<summary>Bug in loading quarterly and yearly time series data</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

It looks like recent changes (&lt;denchmark-link:https://github.com/awslabs/gluon-ts/pull/253&gt;#253&lt;/denchmark-link&gt;
) introduced a bug in loading yearly, quarterly data. The loaded train and test datasets have monthly frequency instead of yearly ("12M") or quarterly ("3M") frequency.
This seems to be the culprit: &lt;denchmark-link:https://github.com/awslabs/gluon-ts/blob/master/src/gluonts/dataset/common.py#L330&gt;https://github.com/awslabs/gluon-ts/blob/master/src/gluonts/dataset/common.py#L330&lt;/denchmark-link&gt;

I actually came across this while doing evaluations for yearly data. Model trained with DeepAREstimator failed during evaluation with the following error. It shows the frequency here correctly in the error message since the estimator gets it from the metadata but the actual train and test datasets have wrong frequency.
&lt;denchmark-code&gt;AssertionError: Cannot extract prediction target since the index of forecast is outside the index of target
Index of forecast: DatetimeIndex(['1752-07-31', '1753-07-31', '1754-07-31', '1755-07-31',
               '1756-07-31', '1757-07-31'],
              dtype='datetime64[ns]', freq='12M')
 Index of target: DatetimeIndex(['1749-12-31', '1750-12-31', '1751-12-31', '1752-12-31',
               '1753-12-31', '1754-12-31', '1755-12-31', '1756-12-31',
               '1757-12-31', '1758-12-31', '1759-12-31', '1760-12-31',
               '1761-12-31', '1762-12-31', '1763-12-31', '1764-12-31',
               '1765-12-31', '1766-12-31', '1767-12-31', '1768-12-31',
               '1769-12-31', '1770-12-31', '1771-12-31', '1772-12-31',
               '1773-12-31', '1774-12-31', '1775-12-31', '1776-12-31',
               '1777-12-31', '1778-12-31', '1779-12-31', '1780-12-31',
               '1781-12-31', '1782-12-31', '1783-12-31', '1784-12-31',
               '1785-12-31'],
              dtype='datetime64[ns]', freq='12M')
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;In [1]: import gluonts                                                                                                                                                                                              

In [2]: from gluonts.dataset.repository.datasets import get_dataset                                                                                                                                                 

In [3]: dataset = get_dataset("m4_yearly", regenerate=False)                                                                                                                                                        

In [4]: next(iter(dataset.test))['start']                                                                                                                                                                           
Out[4]: Timestamp('1749-12-31 00:00:00', freq='M')

In [5]: dataset.metadata.freq
Out[5]: '12M'

In [6]: dataset = get_dataset("m4_quarterly", regenerate=False)                                                                                                                                                     

In [7]: next(iter(dataset.test))['start']                                                                                                                                                                           
Out[7]: Timestamp('1749-12-31 00:00:00', freq='M')
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='rshyamsundar' date='2019-08-30T22:31:09Z'>
		Since  field had wrong frequency &lt;denchmark-link:https://github.com/awslabs/gluon-ts/blob/master/src/gluonts/transform.py#L1081&gt;Forecast start date was wrongly set here&lt;/denchmark-link&gt;
 by shifting 31 months instead of  instead of 31 years in case of m4_yearly data. This explains why the forecast index started at 1752-07-31 as shown in the error message instead of 1780-12-31.
		</comment>
		<comment id='2' author='rshyamsundar' date='2019-08-31T04:28:35Z'>
		It's not clear why ProcessStartField.process needs to have all that logic: we could use some documentation explaining why simply doing
return pd.Timestamp(string, freq=freq)
is not sufficient
		</comment>
		<comment id='3' author='rshyamsundar' date='2019-08-31T11:10:07Z'>
		AFAIK, we do this processing, to align all timestamps within the same period.
E.g. all timestamps in August 2019 should to 2019-08-31:
In [32]: to_offset("M").rollback(pd.Timestamp.now())
Out[32]: Timestamp('2019-08-31 12:57:48.139117')
As we can see, rollback doesn't affect the time information, that's why we call replace on it to reset these values:
In [34]: _32.replace(hour=0, minute=0, second=0, microsecond=0, nanosecond=0)
Out[34]: Timestamp('2019-08-31 00:00:00')
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

That being said, I think 12M still only aligns the data on a monthly bases and not a yearly one:
In [48]: to_offset("12M").rollback(pd.Timestamp.now())
Out[48]: Timestamp('2019-08-31 13:09:39.763214')
vs
In [49]: to_offset("Y").rollback(pd.Timestamp.now())
Out[49]: Timestamp('2018-12-31 13:09:59.602807')
		</comment>
		<comment id='4' author='rshyamsundar' date='2019-08-31T11:48:08Z'>
		
It's not clear why ProcessStartField.process needs to have all that logic: we could use some documentation explaining why simply doing
return pd.Timestamp(string, freq=freq)
is not sufficient

I've tried running the tests with your suggestion and the only tests failing are the ones, which explicitly test for the alignment. Maybe &lt;denchmark-link:https://github.com/vafl&gt;@vafl&lt;/denchmark-link&gt;
 can tell us more, why we need to align the timestamps?
		</comment>
		<comment id='5' author='rshyamsundar' date='2019-08-31T13:56:51Z'>
		The main reason for aligning the time stamps are the time dependent features. These are quite expensive to compute and so we cache the feature values for all time points in a feature matrix:



gluon-ts/src/gluonts/transform.py


         Line 840
      in
      4d044a2






 def _update_cache(self, start: pd.Timestamp, length: int) -&gt; None: 





When we add time features in the transformation, we do a lookup of the start time point and then just index into the feature matrix to return the corresponding part. This cache would not work if time points are not aligned with a regular grid.
Maybe we can improve the alignment logic taking into account the multiple?
		</comment>
	</comments>
</bug>