<bug id='213' author='lws803' open_date='2019-07-22T07:24:10Z' closed_time='2019-08-01T02:15:40Z'>
	<summary>GPU trained model produces runtime error on unpacking predict</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

GPU trained model which is serialised and deserialised produces a runtime error on unpacking using make_evaluation_predictions
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

train_ratio = 0.6
url = "https://raw.githubusercontent.com/numenta/NAB/master/data/realTweets/Twitter_volume_AMZN.csv"
df = pd.read_csv(url, header=0, index_col=0)
data = common.ListDataset([{"start": df.index[0],
                            "target": df.value[:"2015-04-05 00:00:00"]}],
                          freq="5min")
list_values = []
for item in df.value:
    list_values.append(item)

training_data = ListDataset(
    [{"start": df.index[0], "target": list_values[:int(len(list_values)*train_ratio)]}],
    freq = "5min"
)

estimator = deepar.DeepAREstimator(freq="5min", prediction_length=12, trainer=Trainer(ctx='gpu'))
predictor = estimator.train(training_data=training_data)

predictor.serialize(Path("models/"))
predictor = Predictor.deserialize(Path("models/"))

test_data = ListDataset(
    [{"start": df.index[0], "target": list_values[int(len(list_values)*train_ratio):]}],
    freq = "5min"
)

forecast_it, ts_it = make_evaluation_predictions(
    dataset=test_data,  # test dataset
    predictor=predictor,  # predictor,
    num_eval_samples=100
)
print(forecast_it)
forecasts = list(forecast_it) # Breaking point
&lt;denchmark-h:h2&gt;Error Message&lt;/denchmark-h&gt;

RuntimeError: Parameter 'deeparpredictionnetwork0_featureembedder0_cat_0_embedding_weight' was not initialized on context gpu(0). It was only initialized on [cpu(0)].
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;


Operating system: Ubutu 18.04
Python version: Python3
GluonTS version: gluonts==0.2.3

(Add as much information about your environment as possible, e.g. dependencies versions.)
I am also using mxnet-cu100==1.4.1
	</description>
	<comments>
		<comment id='1' author='lws803' date='2019-07-22T15:01:58Z'>
		After digging through the source code, I've found a fix for this. Hmm turns out we wont be able to use what was shown in the tutorials from &lt;denchmark-link:https://gluon-ts.mxnet.io/examples/extended_forecasting_tutorial/extended_tutorial.html#3.3-Saving/Loading-an-existing-model&gt;https://gluon-ts.mxnet.io/examples/extended_forecasting_tutorial/extended_tutorial.html#3.3-Saving/Loading-an-existing-model&lt;/denchmark-link&gt;

from gluonts.model.predictor import Predictor
predictor_deserialized = Predictor.deserialize(Path("/tmp/"))
We have to instead call out deserialise from RepresentableBlockPredictor manually and set the context to 'cpu' which makes sense since we wish to predict and iterate from cpu
predictor = RepresentableBlockPredictor.deserialize(Path("models/"))
predictor.ctx = mx.Context('cpu')
		</comment>
		<comment id='2' author='lws803' date='2019-09-11T15:52:27Z'>
		For the transformer model, predictions on cpu take significantly longer than on gpu.
As discussed in issue 213, trained models can only be loaded into CPU.
&lt;denchmark-link:https://github.com/awslabs/gluon-ts/issues/213&gt;#213&lt;/denchmark-link&gt;

On my test dataset (517 timeseries, each with 720 values) the runtimes are:
25m (cpu)
2m (gpu)
So retraining the model from scratch and doing inference on gpu is faster than loading the saved model from filesystem and doing inference on cpu.
Is it possible to reopen this issue?
		</comment>
		<comment id='3' author='lws803' date='2019-09-15T18:33:11Z'>
		
predictor = RepresentableBlockPredictor.deserialize(Path("models/"))
predictor.ctx = mx.Context('cpu')

I saved a model trained with context="GPU", and had to use this to load it back. However, I am predicting on a large amount of time series and using mx.Context('cpu') is too slow to be practical, but mx.Context('gpu') throws the same error as mentioned by OP.
Any thoughts on how to load back a serialized predictor object and run make_evaluation_predictions using the GPU?
		</comment>
	</comments>
</bug>