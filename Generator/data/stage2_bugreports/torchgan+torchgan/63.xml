<bug id='63' author='avik-pal' open_date='2018-12-07T06:43:26Z' closed_time='2018-12-08T06:39:07Z'>
	<summary>Fix Gradient Visualization</summary>
	<description>
Currently, instead of displaying the gradients norm we are showing the norm of the model parameters.
Problems in fixing this simply:

We are clearing the gradient buffers every time; this makes it impossible to track gradients for different losses.
Printing the gradient wrt only the last executed loss will lead to wrong inference on the part of the user.
Can't make gradients a property of the loss as it makes custom losses a nightmare to implement (we will have to impose a lot of constraints).

	</description>
	<comments>
		<comment id='1' author='avik-pal' date='2018-12-07T18:08:49Z'>
		This can be fixed by storing the gradients of each parameter after going through the train_ops of each loss (as the gradient buffers are zeroed out at the beginning and not at the end). Will submit a PR once &lt;denchmark-link:https://github.com/torchgan/torchgan/pull/62&gt;#62&lt;/denchmark-link&gt;
 is merged.
		</comment>
	</comments>
</bug>