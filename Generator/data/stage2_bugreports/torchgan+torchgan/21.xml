<bug id='21' author='avik-pal' open_date='2018-10-08T08:33:20Z' closed_time='2018-10-08T20:21:56Z'>
	<summary>WGAN Penalty not Working</summary>
	<description>
import torch
import torchvision
from torch.autograd import Variable
from torch.optim import Adam, SGD
import torch.utils.data as data
import torchvision.datasets as dsets
import torchvision.transforms as transforms

from torchgan import *
from torchgan.models import Generator, Discriminator,\
                            SmallDCGANGenerator, SmallDCGANDiscriminator
from torchgan.losses import WassersteinGeneratorLoss, WassersteinDiscriminatorLoss
from torchgan.trainer import Trainer, WGANGP_Trainer

def get_dataset():
    train_dataset = dsets.MNIST(root='/data/avikpal', train=True,
                                transform=transforms.Compose([transforms.Pad((2, 2)),
                                                              transforms.ToTensor(),
                                                              transforms.Normalize(mean = (0.0, 0.0, 0.0), std = (1.0, 1.0, 1.0))]),
                                download=True)
    train_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=True)
    return train_loader

trainer = WGANGP_Trainer(10, SmallDCGANGenerator(out_channels=1), SmallDCGANDiscriminator(in_channels=1), Adam, Adam, LeastSquaresGeneratorLoss, LeastSquaresDiscriminatorLoss, sample_size=64, epochs=5, verbose=5, device=torch.device("cuda:2"), checkpoints="./model/gan2", recon="./images2", optimizer_generator_options={"lr": 0.001}, optimizer_discriminator_options={"lr": 0.01})

trainer(get_dataset)
&lt;denchmark-code&gt;RuntimeError: One of the differentiated Tensors does not require grad
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='avik-pal' date='2018-10-08T20:21:56Z'>
		The problem was not in the definition of the gradient penalty, but the way inputs were provided in the WGAN GP trainer. The gradient in WGAN GP is taken with respect to the input itself, and you were detaching the real images from the computation graph. Since the interpolated input is a linear combination of the real and generated images, autograd was unable to compute the gradient
I've committed the fixes in the trainer branch.
		</comment>
		<comment id='2' author='avik-pal' date='2020-11-03T07:59:15Z'>
		Hi, i got the same problem, could you pls tell me how to solve this.
		</comment>
		<comment id='3' author='avik-pal' date='2020-11-03T09:19:13Z'>
		&lt;denchmark-link:https://github.com/mashibin&gt;@mashibin&lt;/denchmark-link&gt;
 what can you post the stacktrace? The example I posted here shouldn't work at all, given that a lot of work went into the new Trainer.
		</comment>
	</comments>
</bug>