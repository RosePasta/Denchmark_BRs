<bug id='61' author='duanzhihua' open_date='2019-01-21T13:54:33Z' closed_time='2020-04-30T08:33:05Z'>
	<summary>NLP Text categorization support for Chinese corpus</summary>
	<description>
Describe the bug
Load a Chinese text file,The encoding format of the file is GB18030.
run the train.py, Prompt exception：UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb5 in position 2: invalid start byte
To Reproduce
Load a Chinese text file
train.py :
from mindsdb import *
MindsDB().learn(
#from_file="real_estate_description.xlsx", # the path to the file where we can learn from
from_file="shakes.train.numLines.csv", # the path to the file where we can learn from
&lt;denchmark-code&gt;predict='行项目数', # the column we want to learn to predict given all the data in the file（test for text categorization）
model_name='real_estate_desc' # the name of this model
&lt;/denchmark-code&gt;

)
Expected behavior
NLP text categorization support for Chinese corpus
Screenshots
&lt;denchmark-link:&gt;https://github.com/duanzhihua/mindsdb/blob/master/docs/examples/nlp/unicodedecode.png&lt;/denchmark-link&gt;

&lt;denchmark-link:&gt;https://github.com/duanzhihua/mindsdb/blob/master/docs/examples/nlp/ChineseCorpus.png&lt;/denchmark-link&gt;

Desktop (please complete the following information):
OS: Windows 10
	</description>
	<comments>
		<comment id='1' author='duanzhihua' date='2019-01-23T05:21:03Z'>
		Hi &lt;denchmark-link:https://github.com/duanzhihua&gt;@duanzhihua&lt;/denchmark-link&gt;
, thank you for reporting this, do you have time to help me debug this.
		</comment>
		<comment id='2' author='duanzhihua' date='2019-01-24T09:17:22Z'>
		&lt;denchmark-link:https://github.com/torrmal&gt;@torrmal&lt;/denchmark-link&gt;
 Hi, if &lt;denchmark-link:https://github.com/duanzhihua&gt;@duanzhihua&lt;/denchmark-link&gt;
 is too busy to look into it do you mind if I give a try on this issue?
Seems like encoding type for byte string decode function is hardcoded to utf-8 in file_ds.py might not be too hard to fix
		</comment>
		<comment id='3' author='duanzhihua' date='2019-01-24T14:00:28Z'>
		hi, &lt;denchmark-link:https://github.com/torrmal&gt;@torrmal&lt;/denchmark-link&gt;
 ,&lt;denchmark-link:https://github.com/xarus01&gt;@xarus01&lt;/denchmark-link&gt;
 ,Thank you for your prompt and help.Can we modify the code as follows？
&lt;denchmark-code&gt;    try:
        data = StringIO(byte_str.decode('UTF-8'))
    except:
        try:
            data = StringIO(byte_str.decode('GB18030'))
        except:
            logging.error(traceback.format_exc())
            logging.error('Could not load into string')
&lt;/denchmark-code&gt;

If this is changed, the file can be read in, but a new problem arises: The separator of automatic identification of the file is different from what we envisioned.  ',' is used in the requirement, but it is recognized as '*', (file_ds.py  dialect = csv.Sniffer().sniff(data.read(bytes_to_read)))
This causes CSV to be converted to a dataframe exception：
[WARNING] could not check for MindsDB updates
Traceback (most recent call last):
File "D:/PycharmProjects/git_UC_Berkeley_mindsdb/mindsdb-master/train.py", line 9, in 
model_name='real_estate_desc' # the name of this model
File "D:\PycharmProjects\git_UC_Berkeley_mindsdb\mindsdb-master\mindsdb\libs\controllers\mindsdb_controller.py", line 144, in learn
from_ds = getDS(from_data) if from_file is None else getDS(from_file)
File "D:\PycharmProjects\git_UC_Berkeley_mindsdb\mindsdb-master\mindsdb\libs\helpers\multi_data_source.py", line 25, in getDS
from_ds = FileDS(from_data)
File "D:\PycharmProjects\git_UC_Berkeley_mindsdb\mindsdb-master\mindsdb\libs\data_types\data_source.py", line 7, in init
self._setup(*args, **kwargs)
File "D:\PycharmProjects\git_UC_Berkeley_mindsdb\mindsdb-master\mindsdb\libs\data_sources\file_ds.py", line 225, in _setup
self.setDF(pandas.DataFrame(file_list_data, columns=header))
File "C:\Users\lenovo\AppData\Roaming\Python\Python36\site-packages\pandas\core\frame.py", line 387, in init
arrays, columns = _to_arrays(data, columns, dtype=dtype)
File "C:\Users\lenovo\AppData\Roaming\Python\Python36\site-packages\pandas\core\frame.py", line 7475, in _to_arrays
dtype=dtype)
File "C:\Users\lenovo\AppData\Roaming\Python\Python36\site-packages\pandas\core\frame.py", line 7554, in _list_to_arrays
coerce_float=coerce_float)
File "C:\Users\lenovo\AppData\Roaming\Python\Python36\site-packages\pandas\core\frame.py", line 7612, in _convert_object_array
con=len(content)))
AssertionError: 1 columns passed, passed data had 213 columns
What's a good idea?
		</comment>
		<comment id='4' author='duanzhihua' date='2019-01-24T15:41:35Z'>
		hi &lt;denchmark-link:https://github.com/duanzhihua&gt;@duanzhihua&lt;/denchmark-link&gt;
 can you share the file that you are working with so that I can test ideas,
best regards,
Jorge
		</comment>
		<comment id='5' author='duanzhihua' date='2019-01-25T14:05:03Z'>
		hi, &lt;denchmark-link:https://github.com/torrmal&gt;@torrmal&lt;/denchmark-link&gt;
 ，A newpull requests has been submitted.   For a variety of data sources, consider using the encapsulation function of pandas to achieve, okay?
Sincerely yours
zhihua
		</comment>
		<comment id='6' author='duanzhihua' date='2019-02-02T08:04:13Z'>
		Thank you &lt;denchmark-link:https://github.com/duanzhihua&gt;@duanzhihua&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='duanzhihua' date='2019-08-12T23:58:50Z'>
		Better support for different language has now been added, could you try running again &lt;denchmark-link:https://github.com/duanzhihua&gt;@duanzhihua&lt;/denchmark-link&gt;
  ?
		</comment>
		<comment id='8' author='duanzhihua' date='2019-08-13T05:40:21Z'>
		Hi,
   I'm very glad to receive your mail. I'll try running again. Thank you.
   Yours sincerely

   zhihua
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


----------------------------

   *****************************************
   段智华
   中国电信上海公司 监控维护部
   手机： 18918561505
   *****************************************
------------------ 原始邮件 ------------------发件人：George "notifications@github.com"时　间：2019/08/13 07:59:01 星期二收件人：mindsdbmindsdb "mindsdb@noreply.github.com"抄送人：duanzhihua "duanzhihua@189.cn", Mention "mention@noreply.github.com"主　题：Re: [mindsdb/mindsdb] NLP Text categorization support for Chinese corpus (#61)
  Better support for different language has now been added, could you try running again @duanzhihua ?
  —You are receiving this because you were mentioned.Reply to this email directly, view it on GitHub, or mute the thread.

		</comment>
		<comment id='9' author='duanzhihua' date='2019-09-09T03:46:56Z'>
		Any results &lt;denchmark-link:https://github.com/duanzhihua&gt;@duanzhihua&lt;/denchmark-link&gt;
  ?
		</comment>
		<comment id='10' author='duanzhihua' date='2019-09-09T11:05:50Z'>
		Hi,
   there are some problems when Chinese program runs. I will pay attention to them.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


----------------------------

   *****************************************
   段智华
   中国电信上海公司 监控维护部
   手机： 18918561505
   *****************************************
------------------ 原始邮件 ------------------发件人：George "notifications@github.com"时　间：2019/09/09 11:47:10 星期一收件人：mindsdbmindsdb "mindsdb@noreply.github.com"抄送人：duanzhihua "duanzhihua@189.cn", Mention "mention@noreply.github.com"主　题：Re: [mindsdb/mindsdb] NLP Text categorization support for Chinese corpus (#61)
  Any results @duanzhihua ?
  —You are receiving this because you were mentioned.Reply to this email directly, view it on GitHub, or mute the thread.

		</comment>
		<comment id='11' author='duanzhihua' date='2019-11-13T11:09:48Z'>
		Hello：
   Recently, we have tested mindsdb's time series module , and only a few lines of code have been used to realize the prediction of time series. It is very simple to use, and the blog record is as follows.（&lt;denchmark-link:https://duanzhihua.blog.csdn.net/article/details/103053042&gt;https://duanzhihua.blog.csdn.net/article/details/103053042&lt;/denchmark-link&gt;
）
   Now we have a need to predict the excellent rate of network equipment in the communication industry. Based on the data of the past few days, we want to predict the excellent rate of equipment in the next two hours. For the prediction of a single device, we can use N-beats, LSTM and other models to achieve; if we analyze the trend of tens of thousands of devices at the same time, can we use the “group_by ”parameter of mindsdb to achieve grouping? How much computing power does it take? Thank you

  Yours sincerely,
  zhihua
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


---------------------------- 

   *****************************************
   段智华
   中国电信上海公司 监控维护部
   手机： 18918561505
   *****************************************
------------------ 原始邮件 ------------------发件人：George "notifications@github.com"时　间：2019/09/09 11:47:10 星期一收件人：mindsdbmindsdb "mindsdb@noreply.github.com"抄送人：duanzhihua "duanzhihua@189.cn", Mention "mention@noreply.github.com"主　题：Re: [mindsdb/mindsdb] NLP Text categorization support for Chinese corpus (#61) 
  Any results @duanzhihua ?
  —You are receiving this because you were mentioned.Reply to this email directly, view it on GitHub, or mute the thread.

		</comment>
		<comment id='12' author='duanzhihua' date='2020-04-30T08:33:05Z'>
		&lt;denchmark-link:https://github.com/duanzhihua&gt;@duanzhihua&lt;/denchmark-link&gt;

Sorry, I didn't notice your last comment here, in case this is still something you're working on, yes, group_by will essentially separate the time-series data for every device.
For more information on this see: &lt;denchmark-link:https://mindsdb.github.io/mindsdb/docs/advanced-mindsdb#timeseries-predictions&gt;https://mindsdb.github.io/mindsdb/docs/advanced-mindsdb#timeseries-predictions&lt;/denchmark-link&gt;

Will be closing this issue since there's been no update in a while (again, my fault here for not answering sooner, not sure how I missed it), if you still have some problem or questions please feel free to open a new issue and we can pick it up from there :)
		</comment>
	</comments>
</bug>