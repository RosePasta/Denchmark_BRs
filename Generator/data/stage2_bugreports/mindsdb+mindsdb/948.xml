<bug id='948' author='George3d6' open_date='2020-11-18T13:24:03Z' closed_time='2020-11-25T17:09:53Z'>
	<summary>Predictor stuck at status `train` if mindsdb is closed</summary>
	<description>
The reason for this is quite obvious, there is no way to tell if a predictor is actually "training" from the outside, by just looking at the model data.
So if mindsdb_native gets force killed, it will look like training never stopped.
There's 2 ways to check for this in a single machine scenario:

Check if any process called mindsdb_native is running. If not, then invalidate all "training" predictors.
Same thing as 1, but we need to add the specific uuid of the mindsdb_native predictor to the learn call process name in native (not sure how easy this is)
If a mindsdb is started, list all predictors before starting any APIs, if a predictor's status is "training" and mindsdb_native is not running, invalidate it.

	</description>
	<comments>
		<comment id='1' author='George3d6' date='2020-11-24T21:51:29Z'>
		Checking for running processes is harder on windows so... maybe not that easy to implement after all.
		</comment>
		<comment id='2' author='George3d6' date='2020-11-24T21:58:54Z'>
		There's a way to fix the asthetics of this, simply don't list models that are training where training started before mindsdb is started.
		</comment>
		<comment id='3' author='George3d6' date='2020-11-24T21:59:27Z'>
		We still have old models pilling up and occupying space, but we can always fix that later, in mot cases it shouldn't be a big issue.
		</comment>
	</comments>
</bug>