<bug id='494' author='paxcema' open_date='2020-06-13T21:26:08Z' closed_time='2020-06-25T15:32:48Z'>
	<summary>SIGSEGV interrupt during ModelInterface phase</summary>
	<description>
Your Environment

Python version: Python 3.6.9
Operating system: Ubuntu 16.04
Mindsdb version: 1.25.0
Additional info if applicable:

Please describe your issue and how we can replicate it
Hi, I have a fairly small .csv that I'm trying to use like this:
&lt;denchmark-code&gt;p = Predictor(name='pred')
p.learn(to_predict='price', from_data="./train.csv")
&lt;/denchmark-code&gt;

However, the process abruptly ends during the ModelInterface step, emitting an exit code 139 (signal 11 SIGSEGV).
I've traced the error to the from_data_ds.prepare_encoders() method inside learn(). In data_source.py (line 283) when appending to  input_encoder_training_data['targets'], the
encoded output copy.deepcopy(self.encoders[column_name].encode(args[0])) SIGSEGVs when returning self._pytorch_wrapper(ret).
An interesting sidenote is that this only happens in my local machine. Using MindsDB with the same .csv in Google Colab works fine. Any ideas to what could be happening?
	</description>
	<comments>
		<comment id='1' author='paxcema' date='2020-06-15T12:44:05Z'>
		Hi &lt;denchmark-link:https://github.com/paxcema&gt;@paxcema&lt;/denchmark-link&gt;
 , thanks for reporting this. I've tried to reproduce the issue with quite a small dataset (20 - 50 rows) but it looks ok. We had a quite similar issue with the lightwood installation that was raising a failure condition  SEGFAULT error but it was fixed a few months ago. Maybe you can share the dataset (if it is public) so we can try to reproduce this? Other then that, we will try to reproduce this (which I think is quite random) and get back to you with more info.
		</comment>
		<comment id='2' author='paxcema' date='2020-06-15T12:44:18Z'>
		Hmh, stranger, could be some OOM-related errors and I guess those could lead to wired numpy/pandas/torch erros in the background that lead to SIGSEGV.
Could you tell me a few things:

Do you have a GPU + if yes: Which model ? + if no: How much RAM does your machine have ?
What's the size of your dataset (nr of rows, nr of columns, MBs occupied as CSV)
Can you share the data ?
Can you share the last 20-30 lines of logs that appear before you get this error ? Or, even better, the full log of the training session.

		</comment>
		<comment id='3' author='paxcema' date='2020-06-15T18:57:53Z'>
		Thanks for the replies!
&lt;denchmark-link:https://github.com/George3d6&gt;@George3d6&lt;/denchmark-link&gt;


I have a single GTX1070 8GB GPU, and 32 GB of RAM.
It is a (56618, 9) shaped dataset, and the CSV file weights 8MBs.
Sadly, I can't.
Here is the training session log. I realize there are a lot of warnings: we are currently inspecting the dataset further, maybe some anomaly in the data is triggering this.

EDIT: Forgot to mention that I checked and the system does not run OOM in neither RAM nor VRAM.
EDIT 2: Turns out we were using torch1.4.0+cu92 and torchvision0.5.0, after upgrading to torch1.5.0 and torchvision0.6.0 it now works, although it is not clear to us what the exact issue was. Anyway, thanks for the help!
		</comment>
		<comment id='4' author='paxcema' date='2020-06-19T09:17:08Z'>
		&lt;denchmark-link:https://github.com/paxcema&gt;@paxcema&lt;/denchmark-link&gt;
 thanks for the log, reopening the issue since I still think it may be relevant.
Our default requirements are:
&lt;denchmark-code&gt;torchvision &gt;= 0.5.0
torch &gt;= 1.4.0
&lt;/denchmark-code&gt;

So it would work with those, We might have to consider updating the version, based on the log I can see nothing, so it might be worthwhile waiting until we can replicate this.
Your computer has very similar specs to my own, so it's weird that I haven't run into this issue + the logs show the file is small (~50k lines) and the datatype inferred are just numerical and categorical (so no heavy-weight NNs need to be trained to process those). Maybe just some weird thing with a particular version of CUDNN your computer had instaled.
I'm just not sure if I have anything to add at the moment but I think it's worthwhile keeping this on our mind for now.
It's great that you did manage to get it to run though and please let me know if you run into any further issues :)
		</comment>
	</comments>
</bug>