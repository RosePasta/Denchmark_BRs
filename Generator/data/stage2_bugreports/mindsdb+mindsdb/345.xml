<bug id='345' author='George3d6' open_date='2019-12-05T11:51:06Z' closed_time='2019-12-06T15:36:33Z'>
	<summary>Ludwig is unable to properly handle large data frames</summary>
	<description>
&lt;denchmark-link:https://github.com/surendra1472&gt;@surendra1472&lt;/denchmark-link&gt;
 stumbled upon an issue with ludwig+mindsdb not working in a situation where ludwig standalone was and it was impossible to figure out what was happening from the error message.
After investigating locally I'm pretty sure that this is due to the fact that we pass a dataframe to ludwig and Suren was passing a file when training standalone, and ludwig runs into some issues with large dataframes.
Testing locally it seems to handle 5000 rows from said dataframe perfectly file, but crashes at 500,000 rows. Not exactly sure where the limit lies, I think a "safe" way of fixing this would be persisting the data used for training ludwig to a csv under the model storage directory and passing that instead of the dataframe.
Opened a bug report with more details on the ludwig githubL &lt;denchmark-link:https://github.com/ludwig-ai/ludwig/issues/593&gt;ludwig-ai/ludwig#593&lt;/denchmark-link&gt;
 , but I'd assume it might be harder to fix on their end than on our (if they even care about solving it).
	</description>
	<comments>
		<comment id='1' author='George3d6' date='2019-12-06T15:34:45Z'>
		This diagnosis was wrong, it was the mindsdb dataframe splitting code that wasn't working, will do a PR that fixes it soon.
		</comment>
	</comments>
</bug>