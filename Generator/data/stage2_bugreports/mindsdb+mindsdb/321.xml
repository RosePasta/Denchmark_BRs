<bug id='321' author='George3d6' open_date='2019-10-28T15:43:01Z' closed_time='2019-11-16T16:53:05Z'>
	<summary>Consider over-sampling for lightwood or an over-sampling flag</summary>
	<description>
Apparently our approach to lightwood category balancing fails when the difference in categories is too large... which should have been obvious in hindsight, but I'm bad at pytroch :(
See: &lt;denchmark-link:https://discuss.pytorch.org/t/dealing-with-imbalanced-datasets-in-pytorch/22596/22&gt;https://discuss.pytorch.org/t/dealing-with-imbalanced-datasets-in-pytorch/22596/22&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://www.reddit.com/r/MLQuestions/comments/do6vxh/is_there_a_reason_why_oversampling_might_perform/&gt;https://www.reddit.com/r/MLQuestions/comments/do6vxh/is_there_a_reason_why_oversampling_might_perform/&lt;/denchmark-link&gt;

I'm considering a different approach in lightwood (e.g.  a RandomWeightedSampler), but unless I can get that going soon we should manually over-sample for lightwood again.
Either way, we should add a flag to force over-sampling in case whatever implementation a given backend has for given with unbalanced target variable is not satisfactory.
	</description>
	<comments>
		<comment id='1' author='George3d6' date='2019-11-16T16:53:05Z'>
		Implemented a few days ago in both mindsdb and lightwood (Lightwood PR under review currently)
		</comment>
	</comments>
</bug>