<bug id='18' author='jyhong836' open_date='2018-11-15T20:42:19Z' closed_time='2018-11-20T20:00:25Z'>
	<summary>What does the `reset_states` do?</summary>
	<description>
It seems that the method reset_state in metrics resets the stored values. However, I am not sure when it should be used. Is it for resetting states at the end of each epoch?
According to my understanding, the keras-metrics is designed to avoid the incorrect approximation of recall on each batch. Thus, a practical solution is computing the metrics on the end of each epoch independently.
But in the &lt;denchmark-link:https://github.com/netrack/keras-metrics/blob/master/README.md&gt;README.md&lt;/denchmark-link&gt;
, the given example is
import keras
import keras_metrics

model = models.Sequential()
model.add(keras.layers.Dense(1, activation="sigmoid", input_dim=2))
model.add(keras.layers.Dense(1, activation="softmax"))

model.compile(optimizer="sgd",
              loss="binary_crossentropy",
              metrics=[keras_metrics.precision(), keras_metrics.recall()])
which directly pass the keras_metrics.recall() as metrics for batch-based usage. The problem in the demo is that the states may* not be resetted. Therefore, the recall value of each epoch will be dependent on previous epochs.
* I am not sure if the reset_states method is called at the end of each epoch.
	</description>
	<comments>
		<comment id='1' author='jyhong836' date='2018-11-16T08:24:18Z'>
		Hi &lt;denchmark-link:https://github.com/jyhong836&gt;@jyhong836&lt;/denchmark-link&gt;
. The  indeed is called on each epoch, see &lt;denchmark-link:https://github.com/keras-team/keras/blob/2.2.4/keras/engine/training_arrays.py#L145&gt;https://github.com/keras-team/keras/blob/2.2.4/keras/engine/training_arrays.py#L145&lt;/denchmark-link&gt;
.
It seems the only thing with that: metrics should be marked as stateful, while they are not. Thank you for noticing that, I'll prepare appropriate changes.
		</comment>
		<comment id='2' author='jyhong836' date='2018-11-16T13:42:31Z'>
		Thank you for your reply. You are right. I didn't notice the calling in training_arrays.py.
BTW, the unit test case is actually unconvincing. The correctness of the true positive, false negative and etc. values are not tested.
There has been a stateful metric test, i.e., &lt;denchmark-link:https://github.com/keras-team/keras/blob/75a35032e194a2d065b0071a9e786adf6cee83ea/tests/keras/metrics_test.py#L124&gt;test_stateful_metrics&lt;/denchmark-link&gt;
, in the official package but only for binary true positive test. You may refer to that.
Your package is really useful. Thank you for your contribution.
		</comment>
		<comment id='3' author='jyhong836' date='2018-11-17T02:14:56Z'>
		I try to run the unit test in the pull request &lt;denchmark-link:https://github.com/netrack/keras-metrics/pull/19&gt;#19&lt;/denchmark-link&gt;
 . I compare the false_positive value aganist below function:
def ref_false_pos(y_true, y_pred):
    return np.sum(np.logical_and(np.round(y_pred)==1, y_true == 0))
y_pred = model.predict(x)
expected_fp = ref_false_pos(y, y_pred)
The values will not be equal occassionally. Even if I fixed the random seed by  np.random.seed(2334), the inequality still happens occasionally.
Is there any explanation for this stochastics?
		</comment>
		<comment id='4' author='jyhong836' date='2018-11-17T08:09:55Z'>
		&lt;denchmark-link:https://github.com/jyhong836&gt;@jyhong836&lt;/denchmark-link&gt;
, could you, please post an example of run with failing test (output or input data). Unfortunately, I can't reproduce this issue after merging pull request &lt;denchmark-link:https://github.com/netrack/keras-metrics/pull/19&gt;#19&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='5' author='jyhong836' date='2018-11-17T14:49:22Z'>
		&lt;denchmark-link:https://github.com/ybubnov&gt;@ybubnov&lt;/denchmark-link&gt;
 I post my test at &lt;denchmark-link:https://github.com/jyhong836/keras-metrics&gt;jyhong836/keras-metrics&lt;/denchmark-link&gt;
. But I am not sure if you can reproduce the result. I also include a  file. Please put it under your working directory.
Currently, I can reproduce the error on my Macbook, macos 10.14, tensorflow 1.5.0 (cpu version), keras 2.2.4 &amp; 2.1.6. However, I cannot reproduce it on another linux computer, with tensorflow 1.4.0 (GPU version), keras 2.1.6. I am not sure if it is the version issue or the computer issue.
		</comment>
		<comment id='6' author='jyhong836' date='2018-11-19T13:45:57Z'>
		&lt;denchmark-link:https://github.com/jyhong836&gt;@jyhong836&lt;/denchmark-link&gt;
, I've tried to run your tests on my Linux machine and I've managed to reproduce an issue with the tensorflow  version. There is no issue with tensoflow  though.
		</comment>
		<comment id='7' author='jyhong836' date='2018-11-19T14:23:38Z'>
		Tests are failing both when model is loaded from temp_model.hdf5 and after model fitting.
		</comment>
		<comment id='8' author='jyhong836' date='2018-11-19T15:29:04Z'>
		So I guess there is some bug in tensorflow &lt;=1.5.0.
Which tensorflow do you use? GPU or CPU version?
		</comment>
		<comment id='9' author='jyhong836' date='2018-11-20T06:13:02Z'>
		I'm able to reproduce an issue with tensorflow 1.6.0 and 1.7.0 as well. I'm using CPU version (from pip repository, so not optimized for AVX2 and FMA instructions).
		</comment>
		<comment id='10' author='jyhong836' date='2018-11-20T20:00:25Z'>
		I think there is no better solution than upgrading. I will close the issue.
		</comment>
	</comments>
</bug>