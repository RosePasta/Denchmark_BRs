<bug id='230' author='charlesmartin14' open_date='2019-04-18T03:42:43Z' closed_time='2020-04-16T12:46:40Z'>
	<summary>can not load ResNet56 finetuned model for cifar10</summary>
	<description>
I am trying to load the model zoo checkpoint files for the ResNet56 example, as explained in the blog
&lt;denchmark-link:https://nervanasystems.github.io/distiller/model_zoo.html&gt;https://nervanasystems.github.io/distiller/model_zoo.html&lt;/denchmark-link&gt;

I can load the baseline checkpoint file
&lt;denchmark-link:https://s3-us-west-1.amazonaws.com/nndistiller/pruning_filters_for_efficient_convnets/checkpoint.resnet56_cifar_baseline.pth.tar&gt;https://s3-us-west-1.amazonaws.com/nndistiller/pruning_filters_for_efficient_convnets/checkpoint.resnet56_cifar_baseline.pth.tar&lt;/denchmark-link&gt;

but not the finetuned file
&lt;denchmark-link:https://s3-us-west-1.amazonaws.com/nndistiller/pruning_filters_for_efficient_convnets/checkpoint_finetuned.pth.tar&gt;https://s3-us-west-1.amazonaws.com/nndistiller/pruning_filters_for_efficient_convnets/checkpoint_finetuned.pth.tar&lt;/denchmark-link&gt;

from distiller.models import create_model
from distiller.apputils import *
...

cpfiles = {
    'checkpoint.resnet56_cifar_baseline.pth.tar': 92.92,
    'checkpoint_finetuned.pth.tar': 92.96
}

for checkpoint_file, accuracy in cpfiles.items():

    try:
        resnet56_model = create_model(False, 'cifar10', 'resnet56_cifar', parallel=True)
        load_checkpoint(resnet56_model, checkpoint_file);
 
    except Exception as e:
        print("Did you forget to download the checkpoint file?")
        raise e
	</description>
	<comments>
		<comment id='1' author='charlesmartin14' date='2019-04-18T09:35:40Z'>
		Indeed when I try to load the fine-tuned model I get size mismatches. Is this what you're getting as well, or another error?
		</comment>
		<comment id='2' author='charlesmartin14' date='2019-04-18T10:13:36Z'>
		In any case - the reason for the size mismatches is that the checkpoint on AWS, for some reason, does not contain the thinning recipe which is supposed to be saved when thinning is performed during training. So, the model is not thinned during then load process, which means the parameters remain in their original shape. But, the model parameters stored in the checkpoint are after thinning - and so we have a mismatch between the shapes we're loading and the model we try to load them to.
I re-ran the fine-tuning on the baseline checkpoint, please download the proper checkpoint from here:
&lt;denchmark-link:https://github.com/NervanaSystems/distiller/files/3093848/checkpoint_finetuned.pth.tar.gz&gt;checkpoint_finetuned.pth.tar.gz&lt;/denchmark-link&gt;

Note you'll need to gunzip it before loading with Distiller.
Hopefully this solves the error you were seeing. If not, let us know.
		</comment>
		<comment id='3' author='charlesmartin14' date='2019-04-18T14:54:44Z'>
		RuntimeError: Error(s) in loading state_dict for ResNetCifar:
size mismatch for layer1.0.conv1.weight: copying a param with shape torch.Size([7, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).
size mismatch for layer1.0.bn1.weight: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).
size mismatch for layer1.0.bn1.bias: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).
...
		</comment>
		<comment id='4' author='charlesmartin14' date='2019-04-20T16:42:51Z'>
		Yes this works now thank you
		</comment>
		<comment id='5' author='charlesmartin14' date='2019-05-02T13:34:27Z'>
		We need to update the file on AWS.  I'm filing this as a bug.
		</comment>
	</comments>
</bug>