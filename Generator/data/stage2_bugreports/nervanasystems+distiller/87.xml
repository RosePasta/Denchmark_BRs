<bug id='87' author='HKLee2040' open_date='2018-11-29T11:36:23Z' closed_time='2020-02-19T13:40:03Z'>
	<summary>loss.backward() --&amp;gt; RuntimeError</summary>
	<description>
Is there anyone who can help me?
Ubuntu 16.04
command:
time python3 compress_classifier.py --arch resnet20_cifar  ../../../data.cifar10 -p=50 --lr=0.1 --epochs=250 --resume=../cifar10/resnet20/checkpoint_trained_dense.pth.tar --compress=../quantization/preact_resnet20_cifar_pact.yaml  -j=1 --deterministic
Error message:
--- validate (epoch=199)-----------
5000 samples (256 per mini-batch)
==&gt; Top1: 90.300    Top5: 99.700    Loss: 0.297
==&gt; Best Top1: 90.860 on Epoch: 187
Saving checkpoint to: logs/2018.11.29-140224/checkpoint.pth.tar
Training epoch: 45000 samples (256 per mini-batch)
Log file for this run: /media/walker/DATA/work/new_quant/distiller/examples/classifier_compression/logs/2018.11.29-140224/2018.11.29-140224.log
Traceback (most recent call last):
File "compress_classifier.py", line 789, in 
main()
File "compress_classifier.py", line 391, in main
msglogger.info(distiller.masks_sparsity_tbl_summary(model, compression_scheduler))
File "/usr/lib/python3.5/contextlib.py", line 77, in exit
self.gen.throw(type, value, traceback)
File "/media/walker/DATA/work/new_quant/distiller/distiller/data_loggers/collector.py", line 301, in collectors_context
yield collectors_dict
File "compress_classifier.py", line 386, in main
loggers=[tflogger, pylogger], args=args)
File "compress_classifier.py", line 495, in train
loss.backward()
File "/home/walker/.local/lib/python3.5/site-packages/torch/tensor.py", line 93, in backward
torch.autograd.backward(self, gradient, retain_graph, create_graph)
File "/home/walker/.local/lib/python3.5/site-packages/torch/autograd/init.py", line 89, in backward
allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.
real	301m20.430s
user	204m21.640s
sys	99m42.978s
	</description>
	<comments>
		<comment id='1' author='HKLee2040' date='2018-11-29T12:57:04Z'>
		Please try 'loss.backward(retain_graph=True)'
		</comment>
		<comment id='2' author='HKLee2040' date='2018-11-29T23:39:29Z'>
		Hi &lt;denchmark-link:https://github.com/HKLee2040&gt;@HKLee2040&lt;/denchmark-link&gt;
,
Indeed as &lt;denchmark-link:https://github.com/cattpku&gt;@cattpku&lt;/denchmark-link&gt;
 says, changing "compress_classifier.py", line 495 from:
&lt;denchmark-code&gt;loss.backward()
&lt;/denchmark-code&gt;

to:
&lt;denchmark-code&gt;loss.backward(retain_graph=True)
&lt;/denchmark-code&gt;

Will solve the issue.  I don't know why we have this issue pop-up now, so we'll need to investigate before we push a fix.
Note that after running your command line from above (see below) I ran an evaluation execution which fails.
This is your command:
&lt;denchmark-code&gt;time python3 compress_classifier.py --arch resnet20_cifar ../../../data.cifar10 -p=50 --lr=0.1 --epochs=250 --resume=../cifar10/resnet20/checkpoint_trained_dense.pth.tar --compress=../quantization/preact_resnet20_cifar_pact.yaml -j=1 --deterministic
&lt;/denchmark-code&gt;

This is what I ran on the output of the command:
&lt;denchmark-code&gt;time python3 compress_classifier.py --arch resnet20_cifar ../../../data.cifar10 --resume=logs/2018.11.29-222118/checkpoint.pth.tar -e

--------------------------------------------------------
Logging to TensorBoard - remember to execute the server:
&gt; tensorboard --logdir='./logs'
=&gt; loading checkpoint logs/2018.11.29-222118/checkpoint.pth.tar
Checkpoint keys:
optimizer
        compression_sched
        best_top1
        arch
        quantizer_metadata
        epoch
        state_dict
   best top@1: 92.360
Loaded compression schedule from checkpoint (epoch 429)
Loaded quantizer metadata from the checkpoint
Log file for this run: /home/cvds_lab/nzmora/sandbox_5/distiller/examples/classifier_compression/logs/2018.11.30-005540/2018.11.30-005540.log
Traceback (most recent call last):
  File "compress_classifier.py", line 792, in &lt;module&gt;
    main()
  File "compress_classifier.py", line 316, in main
    model, chkpt_file=args.resume)
  File "/home/cvds_lab/nzmora/sandbox_5/distiller/apputils/checkpoint.py", line 116, in load_checkpoint
    quantizer = qmd['type'](model, **qmd['params'])
TypeError: __init__() missing 1 required positional argument: 'optimizer'
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/guyjacob&gt;@guyjacob&lt;/denchmark-link&gt;
 pointed out to me that there is a temporary workaround documented &lt;denchmark-link:https://github.com/NervanaSystems/distiller/issues/59#issuecomment-432546165&gt;here&lt;/denchmark-link&gt;
.  We'll get these two issues fixed next week.
Cheers
Neta
		</comment>
		<comment id='3' author='HKLee2040' date='2018-11-30T11:16:41Z'>
		Thanks!  After changing line 495, the problem is solved.  However, I got another error shown as below:
Training epoch: 45000 samples (256 per mini-batch)
Epoch: [217][   50/  176]    Overall Loss 0.102176    Objective Loss 0.102176    Top1 96.304688    Top5 99.984375    LR 0.100000    Time 5.027114
Epoch: [217][  100/  176]    Overall Loss 0.101807    Objective Loss 0.101807    Top1 96.328125    Top5 99.976562    LR 0.100000    Time 5.022410
Epoch: [217][  150/  176]    Overall Loss 0.100870    Objective Loss 0.100870    Top1 96.364583    Top5 99.976562    LR 0.100000    Time 5.046812
Log file for this run: /media/walker/DATA/work/new_quant/distiller/examples/classifier_compression/logs/2018.11.30-075359/2018.11.30-075359.log
Traceback (most recent call last):
File "compress_classifier.py", line 790, in 
main()
File "compress_classifier.py", line 391, in main
msglogger.info(distiller.masks_sparsity_tbl_summary(model, compression_scheduler))
File "/usr/lib/python3.5/contextlib.py", line 77, in exit
self.gen.throw(type, value, traceback)
File "/media/walker/DATA/work/new_quant/distiller/distiller/data_loggers/collector.py", line 301, in collectors_context
yield collectors_dict
File "compress_classifier.py", line 386, in main
loggers=[tflogger, pylogger], args=args)
File "compress_classifier.py", line 495, in train
loss.backward(retain_graph=True)
File "/home/walker/.local/lib/python3.5/site-packages/torch/tensor.py", line 93, in backward
torch.autograd.backward(self, gradient, retain_graph, create_graph)
File "/home/walker/.local/lib/python3.5/site-packages/torch/autograd/init.py", line 89, in backward
allow_unreachable=True)  # allow_unreachable flag
File "/home/walker/.local/lib/python3.5/site-packages/torch/autograd/function.py", line 76, in apply
return self._forward_cls.backward(self, *args)
File "/media/walker/DATA/work/new_quant/distiller/distiller/quantization/clipped_linear.py", line 63, in backward
grad_input[input.le(0)] = 0
RuntimeError: unspecified launch failure
The schedule is changed:
quantizers:
pact_quantizer:
class: PACTQuantizer
act_clip_init_val: 8.0
bits_activations: 4
bits_weights: 4
#  no override
		</comment>
		<comment id='4' author='HKLee2040' date='2018-11-30T12:14:57Z'>
		Thanks for the update!
I see that you've change the schedule:
&lt;denchmark-code&gt;bits_weights: 3
&lt;/denchmark-code&gt;

is now:
&lt;denchmark-code&gt;bits_weights: 4
&lt;/denchmark-code&gt;

Are there any other changes?
When you have time, can you also send us the first few lines of the log file (they describe your execution environment and sometimes this helps because issues only arise on certain environment configurations, such as having a single GPU, different version of PyTorch, etc.)?
Thanks!
Neta
		</comment>
		<comment id='5' author='HKLee2040' date='2018-11-30T13:04:03Z'>
		No other change.
2018-11-30 07:53:59,249 - Log file for this run: /media/walker/DATA/work/new_quant/distiller/examples/classifier_compression/logs/2018.11.30-075359/2018.11.30-075359.log
2018-11-30 07:53:59,249 - Number of CPUs: 4
2018-11-30 07:53:59,439 - Number of GPUs: 1
2018-11-30 07:53:59,440 - CUDA version: 8.0.61
2018-11-30 07:53:59,440 - CUDNN version: 7102
2018-11-30 07:53:59,441 - Kernel: 4.15.0-39-generic
2018-11-30 07:53:59,441 - OS: Ubuntu 16.04.5 LTS
2018-11-30 07:53:59,441 - Python: 3.5.2 (default, Nov 12 2018, 13:43:14)
[GCC 5.4.0 20160609]
2018-11-30 07:53:59,441 - PyTorch: 0.4.0
2018-11-30 07:53:59,441 - Numpy: 1.14.3
2018-11-30 07:54:01,791 - Git is dirty
2018-11-30 07:54:01,792 - Active Git branch: master
2018-11-30 07:54:01,801 - Git commit: &lt;denchmark-link:https://github.com/IntelLabs/distiller/commit/ea173cb928e274d3f329d562157e4e7d0bb83cfd&gt;ea173cb&lt;/denchmark-link&gt;

2018-11-30 07:54:01,801 - App args: ['compress_classifier.py', '--arch', 'resnet20_cifar', '../../../data.cifar10', '-p=50', '--lr=0.1', '--epochs=250', '--resume=../cifar10/resnet20/checkpoint_trained_dense.pth.tar', '--compress=../quantization/preact_resnet20_cifar_pact.yaml', '-j=1', '--deterministic']
2018-11-30 07:54:01,820 - ==&gt; using cifar10 dataset
2018-11-30 07:54:01,820 - =&gt; creating resnet20_cifar model for CIFAR10
2018-11-30 07:54:15,416 - =&gt; loading checkpoint ../cifar10/resnet20/checkpoint_trained_dense.pth.tar
2018-11-30 07:54:16,453 - Checkpoint keys:
compression_sched
epoch
arch
state_dict
optimizer
best_top1
2018-11-30 07:54:16,454 -    best top@1: 92.540
2018-11-30 07:54:16,454 - Loaded compression schedule from checkpoint (epoch 179)
2018-11-30 07:54:16,455 - =&gt; loaded checkpoint '../cifar10/resnet20/checkpoint_trained_dense.pth.tar' (epoch 179)
2018-11-30 07:54:16,458 - Optimizer Type: &lt;class 'torch.optim.sgd.SGD'&gt;
2018-11-30 07:54:16,459 - Optimizer Args: {'momentum': 0.9, 'lr': 0.1, 'nesterov': False, 'weight_decay': 0.0001, 'dampening': 0}
2018-11-30 07:54:24,914 - Schedule contents:
{
"quantizers": {
"pact_quantizer": {
"class": "PACTQuantizer",
"act_clip_init_val": 8.0,
"bits_activations": 4,
"bits_weights": 4
}
},
"lr_schedulers": {
"training_lr": {
"class": "MultiStepLR",
"milestones": [
60,
120
],
"gammas": 0.1
}
},
"policies": [
{
"quantizer": {
"instance_name": "pact_quantizer"
},
"starting_epoch": 0,
"ending_epoch": 200,
"frequency": 1
},
{
"lr_scheduler": {
"instance_name": "training_lr"
},
"starting_epoch": 0,
"ending_epoch": 121,
"frequency": 1
}
]
}
		</comment>
		<comment id='6' author='HKLee2040' date='2020-02-19T13:40:03Z'>
		Hi,
Closing this issue due to staleness.
		</comment>
	</comments>
</bug>