<bug id='326' author='lehahoang' open_date='2019-07-19T13:55:52Z' closed_time='2019-10-25T08:32:32Z'>
	<summary>Issue of re-evaluation of the quantized checkpoint</summary>
	<description>
Hi,
After successfully quantizing the model, I want to re-evaluate the model by using this command line:
$ python3 compress_classifier.py --arch resnet20_cifar ../data.cifar --qebw 8 --qeba 8 --resume-from quantized_checkpoint.pth.tar --evaluate
However, an error is raised unexpectedly. The log looks like:
&lt;denchmark-code&gt;Log file for this run: /homes/lhoang/distiller/examples/classifier_compression/logs/2019.07.19-154314/2019.07.19-154314.log
=&gt; creating a resnet20_cifar model with the cifar10 dataset

--------------------------------------------------------
Logging to TensorBoard - remember to execute the server:
&gt; tensorboard --logdir='./logs'

=&gt; loading checkpoint quantized_checkpoint.pth.tar
=&gt; Checkpoint contents:
+--------------------+-------------+----------------+
| Key                | Type        | Value          |
|--------------------+-------------+----------------|
| arch               | str         | resnet20_cifar |
| compression_sched  | dict        |                |
| epoch              | int         | 0              |
| extras             | dict        |                |
| quantizer_metadata | dict        |                |
| state_dict         | OrderedDict |                |
+--------------------+-------------+----------------+

=&gt; Checkpoint['extras'] contents:
+----------------+--------+---------+
| Key            | Type   |   Value |
|----------------+--------+---------|
| quantized_top1 | float  |   86.36 |
+----------------+--------+---------+

Loaded compression schedule from checkpoint (epoch 0)
Loaded quantizer metadata from the checkpoint

WARNING:
No stats file passed - Dynamic quantization will be used
At the moment, this mode isn't as fully featured as stats-based quantization, and the accuracy results obtained are likely not as representative of real-world results.
Specifically:
  * Not all modules types are supported in this mode. Unsupported modules will remain in FP32.
  * Optimizations for quantization of layers followed by Relu/Tanh/Sigmoid are only supported when statistics are used.
END WARNING

Weight bits:  8
THIS LINE IS TRULY EXECUTED - CHECKPOINT2
Per-layer quantization parameters saved to logs/2019.07.19-154314/layer_quant_params.yaml

Log file for this run: /homes/lhoang/distiller/examples/classifier_compression/logs/2019.07.19-154314/2019.07.19-154314.log
Traceback (most recent call last):
  File "compress_classifier.py", line 791, in &lt;module&gt;
    main()
  File "compress_classifier.py", line 176, in main
    model, args.resumed_checkpoint_path, model_device=args.device)
  File "/homes/lhoang/distiller/distiller/apputils/checkpoint.py", line 164, in load_checkpoint
    quantizer.prepare_model()
  File "/homes/lhoang/distiller/distiller/quantization/range_linear.py", line 951, in prepare_model
    raise ValueError('PostTrainLinearQuantizer requires dummy input in order to perform certain optimizations')
ValueError: PostTrainLinearQuantizer requires dummy input in order to perform certain optimizations
&lt;/denchmark-code&gt;

It looks like we need to pass "dummy_input" so that it can be executed.
Does it make sense to give the stats file for static quantization during the re-evaluation phase of a quantized model?
By the way, it did not happen when I used the previous source code.
Thank you.
	</description>
	<comments>
		<comment id='1' author='lehahoang' date='2019-07-20T21:53:01Z'>
		Hi &lt;denchmark-link:https://github.com/lehahoang&gt;@lehahoang&lt;/denchmark-link&gt;
 ,
That is indeed a bug that appeared from our last update to PostTrainLinearQuantizer, I will work on this soon enough to fix it.
In the meantime - I would suggest saving the whole model in the checkpoint, instead only its' state_dict like we do in compress_classifier.py, and that would save you the trouble of loading and quantizing.
Cheers,
Lev
		</comment>
		<comment id='2' author='lehahoang' date='2019-07-22T09:43:52Z'>
		Hi &lt;denchmark-link:https://github.com/levzlotnik&gt;@levzlotnik&lt;/denchmark-link&gt;

The above issue does not exist if I use the previous source code.
However, there is another issue of re-evaluating the quantized checkpoint if I use the previous source code.
For Resnet20_cifar model, the classification accuracy of re-evaluating is the same to the case which we do quantization before evaluation.
For VGG-f (f=11, 16...), there is a significant drop of classification accuracy of re-evaluating.
The next part describes what I have done:
Step 1: Training a model
$ CUDA_VISIBLE_DEVICE=0,1 python3 compress_classifier.py --arch vgg11_cifar ../data.cifar --epochs 10
Step 2: Collecting stats for quantization
$ CUDA_VISIBLE_DEVICE=0,1 python3 compress_classifier.py --arch vgg11_cifar ../data.cifar --resume-from logs/2019.07.22-111804/checkpoint.pth.tar --evaluate --qe-calibration 0.1
Step 3: Do quantization
$ CUDA_VISIBLE_DEVICE=0,1 python3 compress_classifier.py --arch vgg11_cifar ../data.cifar --resume-from logs/2019.07.22-111804/checkpoint.pth.tar --evaluate --quantize-eval --qe-stats-file logs/2019.07.22-112206/acts_quantization_stats.yaml --qe-per-channel
==&gt; Top1: 78.590    Top5: 98.570    Loss: 0.615
Step 4: Re-evaluating the checkpoint
 $ CUDA_VISIBLE_DEVICE=0,1 python3 compress_classifier.py --arch vgg11_cifar ../data.cifar --resume-from logs/2019.07.22-112303/quantized_checkpoint.pth.tar --evaluate
==&gt; Top1: 54.680    Top5: 87.620    Loss: 1.376
Again, the difference in classification accuracy does not appear if we are working with Resnet20_cifar model
As I do not really know how it is going under the hood of Distiller, I am a bit confused of the results.
I guess this is a bug of loading quantized checkpoints. Please fix it.
Thanks
		</comment>
		<comment id='3' author='lehahoang' date='2019-07-22T11:14:56Z'>
		Hi &lt;denchmark-link:https://github.com/lehahoang&gt;@lehahoang&lt;/denchmark-link&gt;

The flows under the hood are as follows:

When quantizing "from scratch":

Create FP32 "vanilla" module
Quantize the module according to quantization command line arguments
Evaluate
Save checkpoint with quantized parameters and quantization metadata


When loading a checkpoint:

Create FP32 "vanilla" module
Load checkpoint file
If checkpoint contains quantization metadata:

Quantize module according to the quantization metadata


Load model parameters ("state_dict") from checkpoint
Evaluate



(Note that the 2 flows aren't mutually exclusive, i.e. you could load the checkpoint and then quantize again as if it's "from scratch". Which could lead to unexpected results. But the way you ran it is fine).
As you can see, resuming from "quantized_checkpoint.pth.tar" doesn't really "save" you anything, since we always start with a FP32 model and then quantize it. Admittedly, when I originally added the dumping of the quantized checkpoint, it was more to allow the user to inspect the quantized parameters of the model. Resuming from it was not a use case I was thinking about, since you could just re-run the same quantization "from scratch" and get the exact same results.
Since none of this is really documented, I can definitely see why you'd expect resuming to work.
To summarize:

Regarding the need to pass "dummy_input" - this is indeed a change I made recently, and failed to take care of in the resume flow. We'll fix that, as @levzlotnik said
We'll look into the issue you have with VGG. Even with my "disclaimer" above, I don't see why this would happen. We need to make sure there isn't a more serious issue hiding here
To avoid confusion, we'll make sure loading from a post-train quantization checkpoint just works

For now, if you want to re-create post-train quantization results, just re-run the same command. You can see the command line used for each run in the log file. Look for a line that starts with Command line: near the top of the log file.
		</comment>
		<comment id='4' author='lehahoang' date='2019-07-22T19:42:52Z'>
		Hi &lt;denchmark-link:https://github.com/guyjacob&gt;@guyjacob&lt;/denchmark-link&gt;

Thanks for your instant reply.
As I break down the source code of  "evaluate_model" defined in  compress_classifier.py, I  assume that the models passed to the "test" and dumped to checkpoints in the next code section should be the same. That's why I expect the resuming from quantized checkpoints should work when I load it.
&lt;denchmark-code&gt;     if args.quantize_eval:
        model.cpu()
        quantizer = quantization.PostTrainLinearQuantizer.from_args(model, args)
        quantizer.prepare_model(distiller.get_dummy_input(input_shape=model.input_shape))
        model.to(args.device)

    top1, _, _ = test(test_loader, model, criterion, loggers, activations_collectors, args=args)

    if args.quantize_eval:
        checkpoint_name = 'quantized'
        apputils.save_checkpoint(0, args.arch, model, optimizer=None, scheduler=scheduler,
                                 name='_'.join([args.name, checkpoint_name]) if args.name else checkpoint_name,
                                 dir=msglogger.logdir, extras={'quantized_top1': top1})
&lt;/denchmark-code&gt;

As far as I undersatnd your flow, you save all the parameters like Pytorch does including "the wrapper" module. Loading the saved quantized checkpoint should reproduce the classification accuracy. Please correct me if I am wrong.
I need post-training quantization models for other projects, not only for emulating the quantization as you and your colleagues planed from the start.
Thank you for your strong support.
Best regards,
		</comment>
		<comment id='5' author='lehahoang' date='2019-10-25T08:32:32Z'>
		Closing due to staleness. Please re-open if you need further information or assistance.
		</comment>
	</comments>
</bug>