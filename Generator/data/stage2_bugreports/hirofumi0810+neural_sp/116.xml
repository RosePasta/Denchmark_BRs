<bug id='116' author='cywang97' open_date='2020-07-16T13:54:37Z' closed_time='2020-09-09T07:34:45Z'>
	<summary>bug: bugs when use multi-gpu training</summary>
	<description>
Hi, hiro, I use multi GPU to train an ASR model.  It seems torch.nn.parallel.DataParallel won't scatter the dict input. You should overwrite the scatter function in CustomDataParallel function to make sure each item in the dict can be scattered on different devices.
	</description>
	<comments>
		<comment id='1' author='cywang97' date='2020-07-16T18:51:32Z'>
		&lt;denchmark-link:https://github.com/cywang97&gt;@cywang97&lt;/denchmark-link&gt;
 Thank you for your report. As I usually use a single GPU, I didn't notice that. I will check and fix it.
		</comment>
		<comment id='2' author='cywang97' date='2020-09-02T13:16:10Z'>
		Hi, I met similar problem using multi GPUs.
&lt;denchmark-link:https://user-images.githubusercontent.com/5909894/91987820-1baaa400-ed61-11ea-97b1-be9c20815377.png&gt;&lt;/denchmark-link&gt;

It seems all parameters are only on GPU 0 and inputs on GPU 1.
		</comment>
	</comments>
</bug>