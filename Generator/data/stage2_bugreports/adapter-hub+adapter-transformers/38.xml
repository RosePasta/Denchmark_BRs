<bug id='38' author='prajjwal1' open_date='2020-07-21T08:06:59Z' closed_time='2020-08-29T05:46:12Z'>
	<summary>Trainer doesn't work with multi-gpu setup</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Information&lt;/denchmark-h&gt;

Model I am using (Bert, XLNet ...): bert-base-uncased
Language I am using the model on (English, Chinese ...): English
Adapter setup I am using (if any): Default
The problem arises when using:

 the official example scripts: (give details below): run_glue_wh.py
 my own modified scripts: (give details below)

The tasks I am working on is:

 an official GLUE/SQUaD task: (give the name) MNLI
 my own task or dataset: (give details below)

&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:
HF default Trainer wraps the model in DataParallel if training_args.n_gpu &gt; 1. As a result, it doesn't have the config attribute. When I use your modified Trainer, I am getting the error,  DataParallel object has no attribute 'config'
&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

It should not raise the above error.
&lt;denchmark-h:h2&gt;Environment info&lt;/denchmark-h&gt;

Using the master version of this repo. I had to use CUDA_VISIBLE_DEVICES flag to specify one GPU.
	</description>
	<comments>
		<comment id='1' author='prajjwal1' date='2020-08-18T14:11:02Z'>
		In such models, wrapped up in DataParallel, model.module.config might be right, instead of model.config.
		</comment>
		<comment id='2' author='prajjwal1' date='2020-08-26T09:42:56Z'>
		&lt;denchmark-link:https://github.com/prajjwal1&gt;@prajjwal1&lt;/denchmark-link&gt;
 Thanks for reporting. I think &lt;denchmark-link:https://github.com/Adapter-Hub/adapter-transformers/pull/51&gt;#51&lt;/denchmark-link&gt;
 should fix the specific issue you reported. Please check if this solves your issue.
		</comment>
	</comments>
</bug>