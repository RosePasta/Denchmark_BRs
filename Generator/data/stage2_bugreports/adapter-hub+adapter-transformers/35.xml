<bug id='35' author='xplip' open_date='2020-07-20T14:40:37Z' closed_time='2020-08-17T10:18:09Z'>
	<summary>Run_ner.py by default tries to load prediction head from language adapter directory</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Information&lt;/denchmark-h&gt;

Model I am using (Bert, XLNet ...):
bert-base-multilingual-cased
Language I am using the model on (English, Chinese ...):
Finnish
Adapter setup I am using (if any):
Fi language adapter (fine-tuned the pre-trained one from AdapterHub) &amp; NER task adapter (newly initialized and fine-tuned)
The problem arises when using:

 the official example scripts: (give details below)
 my own modified scripts: (give details below)

Using run_ner.py
The tasks I am working on is:

 an official GLUE/SQUaD task: (give the name)
 my own task or dataset: (give details below)

NER on the FiNER dataset
&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

I execute the run_ner.py script to finetune my adapters, the finetuned adapters are stored in my output_dir
After finetuning, both adapters are stored with a pytorch_model_head.bin each
I run the run_ner.py script again, with do_train=False and do_evaluate=False in order to predict only, load_lang_adapter pointing to the dir where the finetuned language adapter is stored and load_task_adapter pointing to where the finetuned task adapter is stored
The script loads the prediction head weights from the language adapter's pytorch_model_head.bin whereas the task adapter's pytorch_model_head.bin is not used

&lt;denchmark-code&gt;07/20/2020 14:50:17 - INFO - transformers.adapter_model_mixin -   Loading module configuration from best_model/ner/adapter_config.json
07/20/2020 14:50:17 - INFO - transformers.adapter_config -   Adding adapter 'ner' of type 'text_task'.
07/20/2020 14:50:17 - INFO - transformers.adapter_model_mixin -   Loading module weights from best_model/ner/pytorch_adapter.bin
07/20/2020 14:50:17 - INFO - transformers.adapter_model_mixin -   Loading module configuration from best_model/fi/adapter_config.json
07/20/2020 14:50:17 - INFO - transformers.adapter_config -   Adding adapter 'fi' of type 'text_lang'.
07/20/2020 14:50:17 - INFO - transformers.adapter_model_mixin -   Loading module weights from best_model/fi/pytorch_adapter.bin
07/20/2020 14:50:17 - INFO - transformers.adapter_model_mixin -   Loading module configuration from best_model/fi/head_config.json
07/20/2020 14:50:17 - INFO - transformers.adapter_model_mixin -   Loading module weights from best_model/fi/pytorch_model_head.bin
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

My intuition here is that the pytorch_model_head.bin in both the fi lang adapter's and the ner task adapter's directory should be the same since I used both during the training process, but it's unclear to me if that's the case. Since this is an NER script, I would also expect that the head is loaded from the NER task adapter directory. If I wanted to use a different language adapter for Finnish now, I would have to copy the pytorch_model_head.bin from the old language adapter's directory to the new language adapter's directory because the script, by default, tries to load it from there. If it loaded it from the NER adapter's directory instead, this would not be an issue. I'm not aware if there are some additional flags I can set to change this. It may not be a bug really, but it definitely caused some confusion on my side.
&lt;denchmark-h:h2&gt;Environment info&lt;/denchmark-h&gt;


transformers version: 2.11.0
Platform: Darwin-19.5.0-x86_64-i386-64bit
Python version: 3.7.5
PyTorch version (GPU?): 1.5.1 (False)
Tensorflow version (GPU?): 2.2.0 (False)
Using GPU in script?: False
Using distributed or parallel set-up in script?: False

	</description>
	<comments>
		<comment id='1' author='xplip' date='2020-07-20T14:48:12Z'>
		Thinking about it, it would probably not make sense to copy the pytorch_model_head.bin. Rather I would fine-tune again on the new adapter and use the head generated from that finetuning procedure.
Either way it would be good to know if the pytorch_model_head.bin in the NER task adapter's directory is the same as the one in the language adapter's directory
		</comment>
		<comment id='2' author='xplip' date='2020-07-20T15:06:57Z'>
		Sorry for the comment mess. üòÑ  I guess what's mainly caused my confusion was that the language adapter is not actually being finetuned (the weights are fixed here), whereas the NER adapter is actually being finetuned, but then the prediction head is not loaded from the NER adapter directory (regardless of whether that makes a difference).
		</comment>
		<comment id='3' author='xplip' date='2020-07-20T19:53:32Z'>
		Hi &lt;denchmark-link:https://github.com/xplip&gt;@xplip&lt;/denchmark-link&gt;
,

Either way it would be good to know if the pytorch_model_head.bin in the NER task adapter's directory is the same as the one in the language adapter's directory

As you're using a model that only supports one fixed head (BertForTokenClassification etc.), the weights of the head saved in the folder of the language adapter and those saved in the folder of the task adapter should be identical in every case, so it shouldn't make a difference from where they are loaded.

The script loads the prediction head weights from the language adapter's pytorch_model_head.bin whereas the task adapter's pytorch_model_head.bin is not used

That's actually a bug, since prediction heads of task adapters certainly should also be loaded if available. :)
		</comment>
	</comments>
</bug>