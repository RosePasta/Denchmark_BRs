<bug id='30' author='JoPfeiff' open_date='2020-07-13T19:37:34Z' closed_time='2020-08-21T11:18:28Z'>
	<summary>no warning when missing prediction head</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Information&lt;/denchmark-h&gt;

When training e.g. fusion with multiple heads, there is a warning that no prediction head was stored for all adapters.
This is desired and thus no warning should be issued.
Model I am using (Bert, XLNet ...):
any model
Language I am using the model on (English, Chinese ...):
any language
Adapter setup I am using (if any):
The problem arises when using:

 the official example scripts: (give details below)
[ x] my own modified scripts: (give details below)

The tasks I am working on is:

 an official GLUE/SQUaD task: (give the name)
[x ] my own task or dataset: (give details below)

&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

train fusion with multiple heads
when storing the model, warnings are printed for every adapter


&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Environment info&lt;/denchmark-h&gt;


transformers version:
Platform:
Python version:
PyTorch version (GPU?):
Tensorflow version (GPU?):
Using GPU in script?:
Using distributed or parallel set-up in script?:

	</description>
	<comments>
		<comment id='1' author='JoPfeiff' date='2020-07-16T12:25:16Z'>
		Reduced log level for missing heads to "debug".
		</comment>
	</comments>
</bug>