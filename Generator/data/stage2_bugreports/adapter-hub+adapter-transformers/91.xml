<bug id='91' author='rabeehkarimimahabadi' open_date='2020-11-26T08:36:03Z' closed_time='2020-11-30T09:34:15Z'>
	<summary>ImportError: cannot import name 'AutoModelWithHeads' from 'transformers'</summary>
	<description>
Hi
I am trying with this example colab:
&lt;denchmark-link:https://colab.research.google.com/github/Adapter-Hub/website/blob/master/app/static/notebooks/Adapter_Quickstart_Training.ipynb#scrollTo=Lbwb3NRf8mBF&gt;https://colab.research.google.com/github/Adapter-Hub/website/blob/master/app/static/notebooks/Adapter_Quickstart_Training.ipynb#scrollTo=Lbwb3NRf8mBF&lt;/denchmark-link&gt;

getting this error:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "test.py", line 11, in &lt;module&gt;
    from transformers import AutoTokenizer, EvalPrediction, GlueDataset, GlueDataTrainingArguments, AutoModelWithHeads, AdapterType
ImportError: cannot import name 'AutoModelWithHeads' from 'transformers' (/idiap/user/rkarimi/libs/anaconda3/envs/adapter/lib/python3.7/site-packages/transformers/__init__.py)
&lt;/denchmark-code&gt;

versions
&lt;denchmark-code&gt;(adapter) rkarimi@italix17:/idiap/user/rkarimi/dev/internship/seq2seq/adapter-transformers$ conda list | grep transformers
adapter-transformers      1.0.1                     &lt;pip&gt;
transformers              3.5.1                     &lt;pip&gt;
(adapter) rkarimi@italix17:/idiap/user/rkarimi/dev/internship/seq2seq/adapter-transformers$ conda list | grep pytorch
pytorch-lightning         1.0.4                     &lt;pip&gt;
adapter hub from github is installed
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='rabeehkarimimahabadi' date='2020-11-26T08:45:06Z'>
		You can't have transformers and adapter-transformers in your env simultaneously.
Your code is trying to call AutoModelWithHeads which is available in adapter-transformers, but not in transformers.
Try creating a new env, and your bug should be fixed.
		</comment>
		<comment id='2' author='rabeehkarimimahabadi' date='2020-11-26T08:47:09Z'>
		Hi
thanks, I need though transformers 3.5.1, this seems to be using
transformers 1.0.1 which is much lower version, how can I have both of
them? shall I reimplement the library if not working with the last version
of transformers? could you give me some pointers please? thanks
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Nov 26, 2020 at 9:45 AM Jonas Pfeiffer ***@***.***&gt; wrote:
 You can't have transformers and adapter-transformers in your env
 simultaneously.
 Your code is trying to call AutoModelWithHeads which is available in
 adapter-transformers, but not in transformers.
 Try creating a new env, and your bug should be fixed.

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 &lt;#91 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/ARPXHH4BM5KXIDSCIDDG6MLSRYISDANCNFSM4UDNHUSA&gt;
 .



		</comment>
		<comment id='3' author='rabeehkarimimahabadi' date='2020-11-26T08:55:10Z'>
		The pip  1.0.1 is built on  2.11.0  &lt;denchmark-link:https://github.com/Adapter-Hub/adapter-transformers/releases/tag/adapters1.0.1&gt;link&lt;/denchmark-link&gt;

However, the master branch of  is already updated to  3.5.1
You can run   to get that version.
		</comment>
		<comment id='4' author='rabeehkarimimahabadi' date='2020-11-26T08:55:45Z'>
		I wish the library was just some files adapter implementations, and then
let the people add this to their model, this does not work like this, if I
cannot use latest version of huggingface repo, could you assist me please
what to do ?
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Nov 26, 2020 at 9:46 AM Rabeeh Karimi ***@***.***&gt; wrote:
 Hi
 thanks, I need though transformers 3.5.1, this seems to be using
 transformers 1.0.1 which is much lower version, how can I have both of
 them? shall I reimplement the library if not working with the last version
 of transformers? could you give me some pointers please? thanks

 On Thu, Nov 26, 2020 at 9:45 AM Jonas Pfeiffer ***@***.***&gt;
 wrote:

&gt; You can't have transformers and adapter-transformers in your env
&gt; simultaneously.
&gt; Your code is trying to call AutoModelWithHeads which is available in
&gt; adapter-transformers, but not in transformers.
&gt; Try creating a new env, and your bug should be fixed.
&gt;
&gt; —
&gt; You are receiving this because you authored the thread.
&gt; Reply to this email directly, view it on GitHub
&gt; &lt;#91 (comment)&gt;,
&gt; or unsubscribe
&gt; &lt;https://github.com/notifications/unsubscribe-auth/ARPXHH4BM5KXIDSCIDDG6MLSRYISDANCNFSM4UDNHUSA&gt;
&gt; .
&gt;



		</comment>
		<comment id='5' author='rabeehkarimimahabadi' date='2020-11-26T08:56:48Z'>
		well, I am realdy doing that, I am using the Adapter-quickstart-training
and this is already doing that¨
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Nov 26, 2020 at 9:55 AM Jonas Pfeiffer ***@***.***&gt; wrote:
 The pip adapter-transformers 1.0.1 is built on transformers 2.11.0 [
 https://github.com/Adapter-Hub/adapter-transformers/releases/tag/adapters1.0.1
 ]
 However, the master branch of adapter-transformers is already updated to
 transformers 3.5.1
 You can run pip install git+
 https://github.com/Adapter-Hub/adapter-transformers.git to get that
 version.

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 &lt;#91 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/ARPXHH7ABMTN2EFQ6XQROGDSRYJXZANCNFSM4UDNHUSA&gt;
 .



		</comment>
		<comment id='6' author='rabeehkarimimahabadi' date='2020-11-26T08:57:15Z'>
		&lt;denchmark-link:https://colab.sandbox.google.com/github/Adapter-Hub/website/blob/master/app/static/notebooks/Adapter_Quickstart_Training.ipynb&gt;https://colab.sandbox.google.com/github/Adapter-Hub/website/blob/master/app/static/notebooks/Adapter_Quickstart_Training.ipynb&lt;/denchmark-link&gt;

&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Nov 26, 2020 at 9:56 AM Rabeeh Karimi ***@***.***&gt; wrote:
 well, I am realdy doing that, I am using the Adapter-quickstart-training
 and this is already doing that¨

 On Thu, Nov 26, 2020 at 9:55 AM Jonas Pfeiffer ***@***.***&gt;
 wrote:

&gt; The pip adapter-transformers 1.0.1 is built on transformers 2.11.0 [
&gt; https://github.com/Adapter-Hub/adapter-transformers/releases/tag/adapters1.0.1
&gt; ]
&gt; However, the master branch of adapter-transformers is already updated to
&gt; transformers 3.5.1
&gt; You can run pip install git+
&gt; https://github.com/Adapter-Hub/adapter-transformers.git to get that
&gt; version.
&gt;
&gt; —
&gt; You are receiving this because you authored the thread.
&gt; Reply to this email directly, view it on GitHub
&gt; &lt;#91 (comment)&gt;,
&gt; or unsubscribe
&gt; &lt;https://github.com/notifications/unsubscribe-auth/ARPXHH7ABMTN2EFQ6XQROGDSRYJXZANCNFSM4UDNHUSA&gt;
&gt; .
&gt;



		</comment>
		<comment id='7' author='rabeehkarimimahabadi' date='2020-11-26T08:57:52Z'>
		could you direct me which files to reimplement? and give me some
directions, maybe easier if I redo the library for my case, then I am free
to use any versions. thanks
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Nov 26, 2020 at 9:56 AM Rabeeh Karimi ***@***.***&gt; wrote:

 https://colab.sandbox.google.com/github/Adapter-Hub/website/blob/master/app/static/notebooks/Adapter_Quickstart_Training.ipynb

 On Thu, Nov 26, 2020 at 9:56 AM Rabeeh Karimi ***@***.***&gt; wrote:

&gt; well, I am realdy doing that, I am using the Adapter-quickstart-training
&gt; and this is already doing that¨
&gt;
&gt; On Thu, Nov 26, 2020 at 9:55 AM Jonas Pfeiffer ***@***.***&gt;
&gt; wrote:
&gt;
&gt;&gt; The pip adapter-transformers 1.0.1 is built on transformers 2.11.0 [
&gt;&gt; https://github.com/Adapter-Hub/adapter-transformers/releases/tag/adapters1.0.1
&gt;&gt; ]
&gt;&gt; However, the master branch of adapter-transformers is already updated
&gt;&gt; to transformers 3.5.1
&gt;&gt; You can run pip install git+
&gt;&gt; https://github.com/Adapter-Hub/adapter-transformers.git to get that
&gt;&gt; version.
&gt;&gt;
&gt;&gt; —
&gt;&gt; You are receiving this because you authored the thread.
&gt;&gt; Reply to this email directly, view it on GitHub
&gt;&gt; &lt;#91 (comment)&gt;,
&gt;&gt; or unsubscribe
&gt;&gt; &lt;https://github.com/notifications/unsubscribe-auth/ARPXHH7ABMTN2EFQ6XQROGDSRYJXZANCNFSM4UDNHUSA&gt;
&gt;&gt; .
&gt;&gt;
&gt;


		</comment>
		<comment id='8' author='rabeehkarimimahabadi' date='2020-11-26T08:58:26Z'>
		then my first reply is what you need to do.
again, you cannot have transformers and adapter-transformers in your env simultaneously.
adapter-transformers is built on transformers 3.5.1
		</comment>
		<comment id='9' author='rabeehkarimimahabadi' date='2020-11-26T09:01:45Z'>
		As I said I do not have any environment, I am running this colab,
&lt;denchmark-link:https://colab.sandbox.google.com/github/Adapter-Hub/website/blob/master/app/static/notebooks/Adapter_Quickstart_Training.ipynb&gt;https://colab.sandbox.google.com/github/Adapter-Hub/website/blob/master/app/static/notebooks/Adapter_Quickstart_Training.ipynb&lt;/denchmark-link&gt;
,
then if you check version of transformers it is much lower than 3.5.1,
could you point me please where are implementation of adapters, so I just
add them to my codes, without going to the hassel of version issues? thanks
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Nov 26, 2020 at 9:58 AM Jonas Pfeiffer ***@***.***&gt; wrote:
 then my first reply is what you need to do.
 again, you cannot have transformers and adapter-transformers in your env
 simultaneously.
 adapter-transformers *is* built on transformers 3.5.1

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 &lt;#91 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/ARPXHH6U7TYKICJ3U3J32QLSRYKEBANCNFSM4UDNHUSA&gt;
 .



		</comment>
		<comment id='10' author='rabeehkarimimahabadi' date='2020-11-26T09:01:46Z'>
		you can find the models which are covered in our &lt;denchmark-link:https://docs.adapterhub.ml/&gt;documentation&lt;/denchmark-link&gt;
.
we are continuously working on supporting more models.
		</comment>
		<comment id='11' author='rabeehkarimimahabadi' date='2020-11-26T09:05:00Z'>
		sorry my issue is not resolved, could you run this colab please:
&lt;denchmark-link:https://colab.sandbox.google.com/github/Adapter-Hub/website/blob/master/app/static/notebooks/Adapter_Quickstart_Training.ipynb&gt;https://colab.sandbox.google.com/github/Adapter-Hub/website/blob/master/app/static/notebooks/Adapter_Quickstart_Training.ipynb&lt;/denchmark-link&gt;

version of transformers is 1, not 3.5, could you give me one example inside
your code implementing adapter for 1 model like BERT, so I learn how the
implementation works to do it for my codes? thanks
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Nov 26, 2020 at 10:02 AM Rabeeh Karimi ***@***.***&gt; wrote:
 I basically want to use the latest version of transformers, as they update
 every day

 On Thu, Nov 26, 2020 at 10:01 AM Rabeeh Karimi ***@***.***&gt; wrote:

&gt; As I said I do not have any environment, I am running this colab,
&gt; https://colab.sandbox.google.com/github/Adapter-Hub/website/blob/master/app/static/notebooks/Adapter_Quickstart_Training.ipynb,
&gt; then if you check version of transformers it is much lower than 3.5.1,
&gt; could you point me please where are implementation of adapters, so I just
&gt; add them to my codes, without going to the hassel of version issues? thanks
&gt;
&gt; On Thu, Nov 26, 2020 at 9:58 AM Jonas Pfeiffer ***@***.***&gt;
&gt; wrote:
&gt;
&gt;&gt; then my first reply is what you need to do.
&gt;&gt; again, you cannot have transformers and adapter-transformers in your
&gt;&gt; env simultaneously.
&gt;&gt; adapter-transformers *is* built on transformers 3.5.1
&gt;&gt;
&gt;&gt; —
&gt;&gt; You are receiving this because you authored the thread.
&gt;&gt; Reply to this email directly, view it on GitHub
&gt;&gt; &lt;#91 (comment)&gt;,
&gt;&gt; or unsubscribe
&gt;&gt; &lt;https://github.com/notifications/unsubscribe-auth/ARPXHH6U7TYKICJ3U3J32QLSRYKEBANCNFSM4UDNHUSA&gt;
&gt;&gt; .
&gt;&gt;
&gt;


		</comment>
		<comment id='12' author='rabeehkarimimahabadi' date='2020-11-26T09:13:11Z'>
		could you point me please to the minimal implementation details of
adapters?
I would like to use the last huggingface repo version and do not want to
build my codes on top of adapter repo, but just use it with my codes, could
you please assist me how I can implement the adapters, pointing me to the
parts of implementations, thanks
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Nov 26, 2020 at 10:04 AM Rabeeh Karimi ***@***.***&gt; wrote:
 sorry my issue is not resolved, could you run this colab please:

 https://colab.sandbox.google.com/github/Adapter-Hub/website/blob/master/app/static/notebooks/Adapter_Quickstart_Training.ipynb
 version of transformers is 1, not 3.5, could you give me one example
 inside your code implementing adapter for 1 model like BERT, so I learn how
 the implementation works to do it for my codes? thanks

 On Thu, Nov 26, 2020 at 10:02 AM Rabeeh Karimi ***@***.***&gt; wrote:

&gt; I basically want to use the latest version of transformers, as they
&gt; update every day
&gt;
&gt; On Thu, Nov 26, 2020 at 10:01 AM Rabeeh Karimi ***@***.***&gt; wrote:
&gt;
&gt;&gt; As I said I do not have any environment, I am running this colab,
&gt;&gt; https://colab.sandbox.google.com/github/Adapter-Hub/website/blob/master/app/static/notebooks/Adapter_Quickstart_Training.ipynb,
&gt;&gt; then if you check version of transformers it is much lower than 3.5.1,
&gt;&gt; could you point me please where are implementation of adapters, so I just
&gt;&gt; add them to my codes, without going to the hassel of version issues? thanks
&gt;&gt;
&gt;&gt; On Thu, Nov 26, 2020 at 9:58 AM Jonas Pfeiffer ***@***.***&gt;
&gt;&gt; wrote:
&gt;&gt;
&gt;&gt;&gt; then my first reply is what you need to do.
&gt;&gt;&gt; again, you cannot have transformers and adapter-transformers in your
&gt;&gt;&gt; env simultaneously.
&gt;&gt;&gt; adapter-transformers *is* built on transformers 3.5.1
&gt;&gt;&gt;
&gt;&gt;&gt; —
&gt;&gt;&gt; You are receiving this because you authored the thread.
&gt;&gt;&gt; Reply to this email directly, view it on GitHub
&gt;&gt;&gt; &lt;#91 (comment)&gt;,
&gt;&gt;&gt; or unsubscribe
&gt;&gt;&gt; &lt;https://github.com/notifications/unsubscribe-auth/ARPXHH6U7TYKICJ3U3J32QLSRYKEBANCNFSM4UDNHUSA&gt;
&gt;&gt;&gt; .
&gt;&gt;&gt;
&gt;&gt;


		</comment>
		<comment id='13' author='rabeehkarimimahabadi' date='2020-11-26T09:17:07Z'>
		Hi &lt;denchmark-link:https://github.com/rabeehkarimimahabadi&gt;@rabeehkarimimahabadi&lt;/denchmark-link&gt;
,
I just ran the notebook you linked and didn't get the issue you mentioned. As &lt;denchmark-link:https://github.com/JoPfeiff&gt;@JoPfeiff&lt;/denchmark-link&gt;
 pointed out, if you run
&lt;denchmark-code&gt;pip install git+https://github.com/Adapter-Hub/adapter-transformers.git
&lt;/denchmark-code&gt;

you get the latest version based on transformers 3.5.1. It will show 1.0.1 in the list of packages because that's our version number, not Huggingface's version number. Basically, it is transformers 3.5.1 although our version number is different.
You can merge the latest version of the master branch from Huggingface if you need an even newer version.
For the adapter implementations, please refer to this section in the documentation: &lt;denchmark-link:https://docs.adapterhub.ml/extending.html&gt;https://docs.adapterhub.ml/extending.html&lt;/denchmark-link&gt;
 or the relevant files in code: &lt;denchmark-link:https://github.com/Adapter-Hub/adapter-transformers/blob/master/src/transformers/adapter_modeling.py&gt;https://github.com/Adapter-Hub/adapter-transformers/blob/master/src/transformers/adapter_modeling.py&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/Adapter-Hub/adapter-transformers/blob/master/src/transformers/adapter_bert.py&gt;https://github.com/Adapter-Hub/adapter-transformers/blob/master/src/transformers/adapter_bert.py&lt;/denchmark-link&gt;
.
Hope this helps!
		</comment>
		<comment id='14' author='rabeehkarimimahabadi' date='2020-11-26T09:34:54Z'>
		Hi
thanks for the info, I tried to read these, I found it really hard to
understand which parts are for your work, and which one from hugginggface
repo, to extract those and add to my custom model, not inside huggingface repo, could you please be
more specific on the part of implementations of adapters?
thanks.
Best regards
Rabeeh
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;





		</comment>
		<comment id='15' author='rabeehkarimimahabadi' date='2020-11-26T09:43:25Z'>
		There's no standard way to integrate adapters into custom models. If our implementations for bert-based models don't work for you, please have a look at our implementation to get an idea how it might be adapted to your needs. Here are the important parts of the implementation:

adapter_modeling.py implements the single adapter modules
adapter_bert.py integrates the adapter modules into the BERT architecture. Stacking and Fusion of adapters are implemented here. All the integration into the Huggingface model is done via Python mixins. Note how every module of BERT in modeling_bert.py has a corresponding mixin here. When supporting a custom model, you probably want to take this file and make changes to fit your model.
adapter_model_mixin.py implements useful methods for saving and loading adapters. The classes here are explained in https://docs.adapterhub.ml/extending.html
in the actual model class (e.g. modeling_bert.py), we only implement the mixins. No other changes are done here compared to Huggingface.

Those are all the main classes where our changes are implemented.
Unfortunately, we can't give detailed support for implementing custom models. If your model is part of the Huggingface repo, you can open a feature request so we can potentially support it officially. Otherwise, we can try to help out if you have any further specific questions to our implementation.
		</comment>
		<comment id='16' author='rabeehkarimimahabadi' date='2020-11-26T18:00:13Z'>
		Hi &lt;denchmark-link:https://github.com/calpt&gt;@calpt&lt;/denchmark-link&gt;
  and &lt;denchmark-link:https://github.com/JoPfeiff&gt;@JoPfeiff&lt;/denchmark-link&gt;
  thanks for all info. I tried to read the pointers, I still have difficulty understanding the code base and essential part of adapter layers, I was wondering if any of you have time for a short chat of 15 minutes, I would be greatly thankful for your help. I need to understand what is the minimal amount of code needed to add adapters, to understand how to implement them for my model. thanks
		</comment>
		<comment id='17' author='rabeehkarimimahabadi' date='2020-11-30T09:34:15Z'>
		As the discussion has shifted to &lt;denchmark-link:https://github.com/Adapter-Hub/adapter-transformers/issues/92&gt;#92&lt;/denchmark-link&gt;
, closing this thread in favor of &lt;denchmark-link:https://github.com/Adapter-Hub/adapter-transformers/issues/92&gt;#92&lt;/denchmark-link&gt;
 to keep discussion in one place.
		</comment>
	</comments>
</bug>