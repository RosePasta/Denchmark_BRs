<bug id='429' author='smahurkar' open_date='2018-04-03T19:18:34Z' closed_time='2018-08-03T21:46:55Z'>
	<summary>Segmentation Fault for Object Detection on Large Image Dataset</summary>
	<description>
I am using a MacBook Pro. The training for object detection runs fine for up to 225 images. However, when I try to train with 235 images, I get a seg fault:
Segmentation fault: 11
Stack trace returned 10 entries:
[bt] (0) 0   libmxnet.so                         0x000000011386dbdf _ZN5mxnet15segfault_loggerEi + 63
[bt] (1) 1   libsystem_platform.dylib            0x00007fffb37d7b3a _sigtramp + 26
[bt] (2) 2   ???                                 0x0000000000000040 0x0 + 64
[bt] (3) 3   libunity_shared.dylib               0x000000010e22dd67 _ZN4turi13v2_block_impl13block_manager10read_blockENSt3__15tupleIJmmmEEEPPNS0_10block_infoE + 551
[bt] (4) 4   libunity_shared.dylib               0x000000010de59dc7 _ZN4turi23sarray_format_reader_v2INS_13flexible_typeEE21fetch_cache_from_fileEmRNS2_11cache_entryE + 135
[bt] (5) 5   libunity_shared.dylib               0x000000010de57e11 _ZN4turi23sarray_format_reader_v2INS_13flexible_typeEE21fetch_rows_from_cacheEmmRNSt3__16vectorIS1_NS3_9allocatorIS1_EEEE + 561
[bt] (6) 6   libunity_shared.dylib               0x000000010de56f3c _ZN4turi23sarray_format_reader_v2INS_13flexible_typeEE9read_rowsEmmRNSt3__16vectorIS1_NS3_9allocatorIS1_EEEE + 76
[bt] (7) 7   libunity_shared.dylib               0x000000010de54028 _ZN4turi15sarray_iteratorINS_13flexible_typeEEC2EPNS_20sarray_reader_bufferIS1_EEmb + 152
[bt] (8) 8   libunity_shared.dylib               0x000000010de52e14 _ZNK4turi13sarray_readerINS_13flexible_typeEE5beginEm + 292
[bt] (9) 9   libunity_shared.dylib               0x000000010e2a1241 _ZN4turi15sframe_iteratorC2ERKNSt3__16vectorINS1_10shared_ptrINS_13sarray_readerINS_13flexible_typeEEEEENS1_9allocatorIS7_EEEEmb + 241
	</description>
	<comments>
		<comment id='1' author='smahurkar' date='2018-04-06T17:51:39Z'>
		Hi &lt;denchmark-link:https://github.com/smahurkar&gt;@smahurkar&lt;/denchmark-link&gt;
, sorry to hear you are having issues with the detector. I have seen segfaults for some versions of MXNet, so could you give us what version you are running and also what platform (macOS/Linux)? If you don't know, you can find this out by running the following in the terminal:
&lt;denchmark-code&gt;pip freeze | grep -i mxnet
&lt;/denchmark-code&gt;

Thanks!
		</comment>
		<comment id='2' author='smahurkar' date='2018-04-06T19:04:52Z'>
		&lt;denchmark-link:https://github.com/gustavla&gt;@gustavla&lt;/denchmark-link&gt;
 mxnet==0.12.1 was the result of that command.
		</comment>
		<comment id='3' author='smahurkar' date='2018-04-17T03:20:45Z'>
		This issue was solved by reducing the size of the images.
		</comment>
		<comment id='4' author='smahurkar' date='2018-08-03T21:46:55Z'>
		Using a dataset of 800 3000x3000 images, I was able to reproduce a crash on TC 4.3.2 but not at master. I'm guessing this was due to memory issues that were ameliorated by &lt;denchmark-link:https://github.com/apple/turicreate/pull/904&gt;#904&lt;/denchmark-link&gt;
, now that we no longer attempt to sort an SFrame containing raw (decompressed) version of all the images.
Closing this issue for now, but let us know if you have any more difficulties on TC 5.0b3 or later. SFrames containing images also figures to be a focus area for future work in general....
		</comment>
		<comment id='5' author='smahurkar' date='2019-01-02T14:30:28Z'>
		Hi &lt;denchmark-link:https://github.com/smahurkar&gt;@smahurkar&lt;/denchmark-link&gt;
, I'm having the same problem. What specific size did you reduce your images down to? My mxnet is 1.1.0.post0, if that is relevant.
		</comment>
	</comments>
</bug>