<bug id='734' author='vade' open_date='2018-06-21T20:03:01Z' closed_time='2018-08-16T21:40:35Z'>
	<summary>tc.config.set_num_gpus fails on Ubuntu 16.0.4 with multiple GPUs</summary>
	<description>
Hello
Successfully running Turi Create 5.0b1 on Ubuntu with GPU training and putting Core ML models. Very cool.
Our system has 5 GPUs - 2x 1080s and 3x 1070s. Our Turi Create script sets tc.config.set_num_gpus(-1) but we fail to ever see any GPU other than the first in use.
Is this a known issue? is this subject to hardware config?
System: Ubuntu 16.0.4
NVidia Drivers 390.48, Cuda 9.0,
Topology:
&lt;denchmark-code&gt;GPU0	GPU1	GPU2	GPU3	GPU4	CPU Affinity
&lt;/denchmark-code&gt;

GPU0	 X 	PHB	PHB	PHB	PHB	0-3
GPU1	PHB	 X 	PHB	PHB	PHB	0-3
GPU2	PHB	PHB	 X 	PHB	PHB	0-3
GPU3	PHB	PHB	PHB	 X 	PHB	0-3
GPU4	PHB	PHB	PHB	PHB	 X 	0-3
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.48                 Driver Version: 390.48                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1080    Off  | 00000000:01:00.0  On |                  N/A |
|100%   39C    P2    58W / 180W |   1067MiB /  8116MiB |     37%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 1080    Off  | 00000000:02:00.0 Off |                  N/A |
|100%   36C    P8    16W / 180W |    786MiB /  8119MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX 1070    Off  | 00000000:04:00.0 Off |                  N/A |
|100%   30C    P8    19W / 230W |    734MiB /  8119MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  GeForce GTX 1070    Off  | 00000000:05:00.0 Off |                  N/A |
|100%   32C    P8    20W / 230W |    640MiB /  8119MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   4  GeForce GTX 1070    Off  | 00000000:09:00.0 Off |                  N/A |
|100%   29C    P8    20W / 230W |    546MiB /  8119MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
	</description>
	<comments>
		<comment id='1' author='vade' date='2018-06-21T20:08:23Z'>
		This is not a known issue. Its unexpected. We will look into this.
		</comment>
		<comment id='2' author='vade' date='2018-06-21T20:21:08Z'>
		&lt;denchmark-link:https://github.com/srikris&gt;@srikris&lt;/denchmark-link&gt;
 Thanks for the  prompt reply. Please let me know if you need any diagnostics. Happy to help.
		</comment>
		<comment id='3' author='vade' date='2018-06-21T20:51:45Z'>
		Hi. We are seeing the same issue:
&lt;denchmark-link:https://user-images.githubusercontent.com/24554/41744913-7e3b1966-7562-11e8-8cee-ef6f5f401758.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='vade' date='2018-06-21T21:00:20Z'>
		Thanks &lt;denchmark-link:https://github.com/vade&gt;@vade&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/skercher&gt;@skercher&lt;/denchmark-link&gt;
. We'll keep you posted on this.
		</comment>
		<comment id='5' author='vade' date='2018-06-21T23:25:20Z'>
		&lt;denchmark-link:https://github.com/vade&gt;@vade&lt;/denchmark-link&gt;
 Are you using Object detection, Image Classification, or Style transfer?
		</comment>
		<comment id='6' author='vade' date='2018-06-21T23:43:48Z'>
		We are using Image Classifier. :)
		</comment>
		<comment id='7' author='vade' date='2018-06-26T18:30:08Z'>
		&lt;denchmark-link:https://github.com/vade&gt;@vade&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/skercher&gt;@skercher&lt;/denchmark-link&gt;
 - I suspect this is related to batch size. In the latest release we decreased the batch size from 512 to 64. Please try increasing the  parameter of  to  and let us know if this solves the issue.
		</comment>
		<comment id='8' author='vade' date='2018-06-26T19:15:24Z'>
		Thanks &lt;denchmark-link:https://github.com/TobyRoseman&gt;@TobyRoseman&lt;/denchmark-link&gt;
 - will give this a shot. I've mentioned this to our team and will let you know.
		</comment>
		<comment id='9' author='vade' date='2018-06-26T19:50:57Z'>
		Batch size of 512 didn't have any effect. Is the idea that the create function will split batches of &gt; 512 across GPUs? Are weight updates shared across GPUs or are the weights also divided per GPU and updated separately a la AlexNet?
		</comment>
		<comment id='10' author='vade' date='2018-06-27T00:03:15Z'>
		Hi &lt;denchmark-link:https://github.com/TobyRoseman&gt;@TobyRoseman&lt;/denchmark-link&gt;
.  Adding batch size seemed to make the training a bit faster but still didn't utilize all our GPU's.
		</comment>
		<comment id='11' author='vade' date='2018-06-27T20:42:55Z'>
		I am not able to reproduce this problem on Ubuntu/CUDA. All my GPUs are getting used to some degree.
Could someone who is having this issue please let me know two things:
1 - How many GPUs they have.
2 - The output of the following code:
import turicreate as tc
print(tc.toolkits._mxnet_utils.get_mxnet_context())
		</comment>
		<comment id='12' author='vade' date='2018-06-27T20:45:24Z'>
		&lt;denchmark-code&gt;oscar@MayallsObject ~/Oscar % python
Python 2.7.12 (default, Dec  4 2017, 14:50:18) 
[GCC 5.4.0 20160609] on linux2
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; import turicreate as tc
&gt;&gt;&gt; print(tc.toolkits._mxnet_utils.get_mxnet_context())
[gpu(0), gpu(1), gpu(2), gpu(3), gpu(4)]
&gt;&gt;&gt; 
&lt;/denchmark-code&gt;

		</comment>
		<comment id='13' author='vade' date='2018-06-27T20:46:30Z'>
		&lt;denchmark-link:https://github.com/vade&gt;@vade&lt;/denchmark-link&gt;
 - and you have five GPUs?
		</comment>
		<comment id='14' author='vade' date='2018-06-27T20:46:32Z'>
		&lt;denchmark-link:https://github.com/TobyRoseman&gt;@TobyRoseman&lt;/denchmark-link&gt;
 - thanks for the test - in your successful environment can you please note if you are using the release of Turi Create 5b1, what Cuda version and GL driver version and MXNet version you are using just for comparisons sake?
		</comment>
		<comment id='15' author='vade' date='2018-06-27T20:46:50Z'>
		&lt;denchmark-link:https://github.com/tbartelmess&gt;@tbartelmess&lt;/denchmark-link&gt;
 Correct - 2 1080's, 3 1070s.
		</comment>
		<comment id='16' author='vade' date='2018-06-27T20:47:53Z'>
		My Nvidia SMI output from the machine I ran the &gt;&gt;&gt; print(tc.toolkits._mxnet_utils.get_mxnet_context()) code on:
&lt;denchmark-code&gt;oscar@MayallsObject ~/Oscar % nvidia-smi 
Wed Jun 27 16:47:14 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.67                 Driver Version: 390.67                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1080    Off  | 00000000:01:00.0  On |                  N/A |
|100%   40C    P2    58W / 180W |   6844MiB /  8116MiB |     52%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 1080    Off  | 00000000:02:00.0 Off |                  N/A |
|100%   37C    P8    17W / 180W |   7230MiB /  8119MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX 1070    Off  | 00000000:04:00.0 Off |                  N/A |
|100%   33C    P8    20W / 230W |   4768MiB /  8119MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  GeForce GTX 1070    Off  | 00000000:05:00.0 Off |                  N/A |
|100%   34C    P8    20W / 230W |   4768MiB /  8119MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   4  GeForce GTX 1070    Off  | 00000000:09:00.0 Off |                  N/A |
|100%   32C    P8    20W / 230W |   7260MiB /  8119MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
&lt;/denchmark-code&gt;

		</comment>
		<comment id='17' author='vade' date='2018-06-27T20:51:35Z'>
		Additional info if pertinent:
&lt;denchmark-code&gt;scar@MayallsObject ~/Oscar % nvidia-smi topo -m
	GPU0	GPU1	GPU2	GPU3	GPU4	CPU Affinity
GPU0	 X 	PHB	PHB	PHB	PHB	0-3
GPU1	PHB	 X 	PHB	PHB	PHB	0-3
GPU2	PHB	PHB	 X 	PHB	PHB	0-3
GPU3	PHB	PHB	PHB	 X 	PHB	0-3
GPU4	PHB	PHB	PHB	PHB	 X 	0-3

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing a single PCIe switch
  NV#  = Connection traversing a bonded set of # NVLinks

&lt;/denchmark-code&gt;

		</comment>
		<comment id='18' author='vade' date='2018-06-27T21:31:09Z'>
		&lt;denchmark-link:https://github.com/vade&gt;@vade&lt;/denchmark-link&gt;
, My version info -
turicreate: .
MXNet: .
Ubuntu: 16.04
Cuda: release 8.0, V8.0.44
Driver Version: 375.66
How many examples are in your dataset?
For the GPU which is doing the work, what is the typical utilization? For the other GPUs, is there ever any non-zero utilization?
		</comment>
		<comment id='19' author='vade' date='2018-06-27T21:44:11Z'>
		Thanks. Perhaps the issue is we are running a different version of mxNet 1.1.0 against cuda 9.0, with a newer driver. We spent a lot of time trying to get a variety of tool chains to 'agree' on a driver / cuda version since turi isnt the only ML framework we use.
Examples in data sets range from thousands to hundreds of thousands
The GPU typical load for a single run of a batch size 512 is roughly 20 - 30%. Running multiple concurrent training jobs garners up to 60 max Ive seen while monitoring, nary any GPU utilization across any GPU other than the zeroth.
		</comment>
		<comment id='20' author='vade' date='2018-06-27T22:12:38Z'>
		&lt;denchmark-link:https://github.com/TobyRoseman&gt;@TobyRoseman&lt;/denchmark-link&gt;
 Let me know if you need anything else.
Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19)
[GCC 7.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.



import turicreate as tc
print(tc.toolkits._mxnet_utils.get_mxnet_context())
[gpu(0), gpu(1), gpu(2), gpu(3)]



nvidia-smi topo -m
GPU0	GPU1	GPU2	GPU3	CPU Affinity
GPU0	 X 	NV1	NV1	NV2	0-39
GPU1	NV1	 X 	NV2	NV1	0-39
GPU2	NV1	NV2	 X 	NV1	0-39
GPU3	NV2	NV1	NV1	 X 	0-39
		</comment>
		<comment id='21' author='vade' date='2018-06-28T00:13:19Z'>
		Dupe of &lt;denchmark-link:https://github.com/apple/turicreate/issues/741&gt;#741&lt;/denchmark-link&gt;

		</comment>
		<comment id='22' author='vade' date='2018-06-28T01:25:08Z'>
		Can someone explain why this issue was closed? &lt;denchmark-link:https://github.com/afranklin&gt;@afranklin&lt;/denchmark-link&gt;
 ?
		</comment>
		<comment id='23' author='vade' date='2018-06-28T02:14:27Z'>
		&lt;denchmark-link:https://github.com/vade&gt;@vade&lt;/denchmark-link&gt;
 This is a dupe of &lt;denchmark-link:https://github.com/apple/turicreate/issues/741&gt;#741&lt;/denchmark-link&gt;
. I closed the more recent filing instead of the earlier one.
		</comment>
		<comment id='24' author='vade' date='2018-06-28T02:15:56Z'>
		&lt;denchmark-link:https://github.com/afranklin&gt;@afranklin&lt;/denchmark-link&gt;
: On closer inspection. These are not the same issue. &lt;denchmark-link:https://github.com/vade&gt;@vade&lt;/denchmark-link&gt;
 is not able to see multiple GPUs being used while &lt;denchmark-link:https://github.com/apple/turicreate/issues/741&gt;#741&lt;/denchmark-link&gt;
 is about saturating multiple GPUs.
		</comment>
		<comment id='25' author='vade' date='2018-06-28T16:07:50Z'>
		Would a Turi Create engineer kindly comment on whether:


Cuda 9 / 9.x, and newer Nvidia drivers are expected to work with multiple GPUs with respect to Turi Create 5 b 1 on linux?


Any alternate builds of MXNet 1.1.0 expected to work with Turi Create across multiple GPUs with respect to Turi Create 5 b 1 on linux?


Thank you.
		</comment>
		<comment id='26' author='vade' date='2018-06-28T22:20:10Z'>
		&lt;denchmark-link:https://github.com/vade&gt;@vade&lt;/denchmark-link&gt;
 - I would expect it to work with newer versions of Cuda and newer drivers. However that has not been tested.
Please clarify your second question: what do you mean by an "alternate builds of MXNet 1.1.0"? Are you talking about a different release version of MxNet? Or something other than ?
		</comment>
		<comment id='27' author='vade' date='2018-06-28T22:36:14Z'>
		&lt;denchmark-link:https://github.com/TobyRoseman&gt;@TobyRoseman&lt;/denchmark-link&gt;
 for example mxnet-cu90--1.1.0 same version number, but expects cuda 9.
		</comment>
		<comment id='28' author='vade' date='2018-06-28T23:36:11Z'>
		&lt;denchmark-link:https://github.com/vade&gt;@vade&lt;/denchmark-link&gt;
 - thanks for clarifying. I would also expect that to work, but we have not tested it.
Could everyone else having this problem, please let us know which version of Cuda you are using?
		</comment>
		<comment id='29' author='vade' date='2018-08-16T21:40:35Z'>
		I finally got a chance test this on a two GPU system with CUDA 9. Using the most recent version of TuriCreate (5.0b3) I was not able to reproduce this issue; both GPUs were getting used about the same amount. I was also using mxnet-cu90==1.1.0.
Feel free to reopen if this is still and issue with the most recent version of TuriCreate.
		</comment>
		<comment id='30' author='vade' date='2018-08-16T21:47:12Z'>
		Hi Toby - thanks for testing. Can you please document the specific Nvidia driver version and Cuda version (which cuda 9 point release?)
Thanks! Happy to verify with your versions shortly!
cc &lt;denchmark-link:https://github.com/genp&gt;@genp&lt;/denchmark-link&gt;
 ;)
		</comment>
		<comment id='31' author='vade' date='2018-08-16T21:57:10Z'>
		&lt;denchmark-link:https://github.com/vade&gt;@vade&lt;/denchmark-link&gt;
 -
Cuda: 
NVIDIA Driver Version: 
		</comment>
		<comment id='32' author='vade' date='2018-11-30T01:39:52Z'>
		&lt;denchmark-link:https://github.com/TobyRoseman&gt;@TobyRoseman&lt;/denchmark-link&gt;


@vade, My version info -
turicreate: 5.0b1.
MXNet: mxnet-cu80==1.1.0.
Ubuntu: 16.04
Cuda: release 8.0, V8.0.44
Driver Version: 375.66
How many examples are in your dataset?
For the GPU which is doing the work, what is the typical utilization? For the other GPUs, is there ever any non-zero utilization?

The size of example dataset will affect the GPUs utilization？
		</comment>
	</comments>
</bug>