<bug id='608' author='dhgokul' open_date='2018-05-29T14:12:29Z' closed_time='2018-07-27T04:43:40Z'>
	<summary>Turicreate - Training process not proceed after 91 iteration , it loading for more than an hour @ GPU</summary>
	<description>
I have trained around 3.5 k images for single category in GPU version, it took around 10 mins to create Sframe data, after that training process log started, it reached 91 iteration and keep on loading not proceed to next step - PFA
&lt;denchmark-link:https://user-images.githubusercontent.com/5689804/40663991-889a52ac-6377-11e8-8931-aa73fd5c69d1.png&gt;&lt;/denchmark-link&gt;

Total dataset size (3500 images)  - 91 MB, Maxi image size - 80 kb
Machine type : AWS deep learning
Any help appreciated !!
	</description>
	<comments>
		<comment id='1' author='dhgokul' date='2018-05-29T14:14:14Z'>
		If your dataset is licensed permissively, can you share the dataset with us if possible? Its mostly likely something to do with a single data point in your dataset and we'd like to be able to isolate it. We can try and reproduce it. Is most unusual that this happens.
		</comment>
		<comment id='2' author='dhgokul' date='2018-05-29T14:27:45Z'>
		Thanks for your reply srikris, I like to share dataset privately , Can you share emailid or provide option to share
		</comment>
		<comment id='3' author='dhgokul' date='2018-05-29T14:30:57Z'>
		Can you file a report to the Apple Bug reporter (&lt;denchmark-link:https://bugreport.apple.com/&gt;https://bugreport.apple.com/&lt;/denchmark-link&gt;
). Its easier for us to share information in there that is private so we can diagnose better.
		</comment>
		<comment id='5' author='dhgokul' date='2018-05-29T16:12:51Z'>
		&lt;denchmark-link:https://github.com/srikris&gt;@srikris&lt;/denchmark-link&gt;
 Did you get chance to review dataset? Thanks for your response !
		</comment>
		<comment id='6' author='dhgokul' date='2018-06-06T10:56:06Z'>
		&lt;denchmark-link:https://github.com/srikris&gt;@srikris&lt;/denchmark-link&gt;
  Any updates on this issue ?
		</comment>
		<comment id='7' author='dhgokul' date='2018-06-06T13:37:50Z'>
		&lt;denchmark-link:https://github.com/dhgokul&gt;@dhgokul&lt;/denchmark-link&gt;
 sorry for the slow response and thank you for your patience!
We're a bit slower to response this week as we're all at WWDC, supporting Turicreate's developer community.
		</comment>
		<comment id='8' author='dhgokul' date='2018-06-07T16:20:47Z'>
		&lt;denchmark-link:https://github.com/dhgokul&gt;@dhgokul&lt;/denchmark-link&gt;
 Sorry. I can't seem to get access to your dataset. Can you re-share it? It may have expired. Sorry!
		</comment>
		<comment id='9' author='dhgokul' date='2018-06-08T07:04:25Z'>
		&lt;denchmark-link:https://github.com/srikris&gt;@srikris&lt;/denchmark-link&gt;
 I have report this issue in Apple Bug reporter . Please check
&lt;denchmark-link:https://bugreport.apple.com/web/?problemID=40926208&gt;https://bugreport.apple.com/web/?problemID=40926208&lt;/denchmark-link&gt;

Summary :
Area:
CoreML
Summary: Turicreate training is not proceed forward after few iterations(say 91 iteration) @ GPU
Steps to Reproduce: Run turicreate python script in AWS deep learning machine, and it got hanged not proceed forward , But it running in background for an hour
Expected Results: It will create coreML model at the completion of turicreate training process.
Actual Results: Turicreate training is not proceed forward after few iterations(say 91 iteration)
Version/Build: Turi version 4.3.2 (stable version)
cuda 9.0
mxnet 1.1.0
Configuration: AWS deep learning machine (Instance type : g2.2xlarge)
		</comment>
		<comment id='10' author='dhgokul' date='2018-06-08T15:57:59Z'>
		I think around iteration 100 is when the first dataset shuffle would happen given your dataset size, so I think this is what is taking long, causing a freeze.
To unblock you, you may want to try turning this off using an undocumented setting:
tc.object_detector.create(..., _advanced_parameters={'shuffle': False})
Of course, this issue is on us and we will work on finding a better solution. Shuffling is generally good for training, so disabling it could lead to a slightly less accurate model.
		</comment>
		<comment id='11' author='dhgokul' date='2018-06-08T16:51:15Z'>
		&lt;denchmark-link:https://github.com/gustavla&gt;@gustavla&lt;/denchmark-link&gt;
 Thanks will check and let you know . Is there's any specific maximum image dataset for turicreate. How many max images can use for training
		</comment>
		<comment id='12' author='dhgokul' date='2018-06-08T16:53:58Z'>
		There is no hard maximum and we have used much larger datasets on laptops, so this is somewhat mysterious to me. How much RAM does your AWS instance have?
		</comment>
		<comment id='13' author='dhgokul' date='2018-06-08T17:23:52Z'>
		&lt;denchmark-link:https://github.com/gustavla&gt;@gustavla&lt;/denchmark-link&gt;
 Machine configuration : AWS g2.2xlarge, 15.0 GiB, 8 vCPUs, 15gb ram
		</comment>
		<comment id='14' author='dhgokul' date='2018-06-11T14:57:27Z'>
		&lt;denchmark-link:https://github.com/gustavla&gt;@gustavla&lt;/denchmark-link&gt;
 Thanks a lot, it work's really well !
After disable shuffle option in turicreate API ,mAP(mean average precision) value is less when compared to previous model (smaller dataset)
Please suggest any changes required in turicreate API
		</comment>
		<comment id='15' author='dhgokul' date='2018-06-11T16:18:57Z'>
		&lt;denchmark-link:https://github.com/dhgokul&gt;@dhgokul&lt;/denchmark-link&gt;
 Thanks so much for trying this and reporting back. This is really useful information for us.
By default we shuffle the data at the regular intervals for a reason, so turning it off it can unfortunately have negative consequences on evaluation metrics. This is particularly true if your data is sorted in some way already, such as "all cars first, then all bicycles, etc.".
We will work on reducing the memory overhead of sorting, so that you don't have to make this model compromise simply to get it to train without error. Again, apologies for these issues!
		</comment>
		<comment id='16' author='dhgokul' date='2018-06-12T05:48:54Z'>
		&lt;denchmark-link:https://github.com/gustavla&gt;@gustavla&lt;/denchmark-link&gt;
 I am using single category for turi training. If we require shuffle option for that ?
		</comment>
		<comment id='17' author='dhgokul' date='2018-06-12T14:06:51Z'>
		&lt;denchmark-link:https://github.com/gustavla&gt;@gustavla&lt;/denchmark-link&gt;
 After disable shuffle option , Object detection looks not accurate , it detect all objects
Dataset Size(1900) with shuffle = true -&gt; Object detection works @ 75 %
Dataset Size(3755) with shuffle  = false -&gt; It detect all objects , when moving camera
		</comment>
		<comment id='18' author='dhgokul' date='2018-06-12T14:12:09Z'>
		If we disable shuffle option as false, it will created coreML model, but it is not accurate and detect all the objects when compared to model created with shuffle option as true.
If we enable shuffle option, still we get same memory error mentioned below
[14:02:01] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:107: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
2018-06-12 14:02:12  Training    1/1000  Loss  4.299
2018-06-12 14:02:22  Training   10/1000  Loss  4.283
2018-06-12 14:02:32  Training   19/1000  Loss  4.101
2018-06-12 14:02:43  Training   28/1000  Loss  3.865
2018-06-12 14:02:53  Training   37/1000  Loss  3.529
2018-06-12 14:03:03  Training   46/1000  Loss  3.121
2018-06-12 14:03:14  Training   54/1000  Loss  3.017
2018-06-12 14:03:25  Training   63/1000  Loss  2.951
2018-06-12 14:03:35  Training   72/1000  Loss  2.947
2018-06-12 14:03:48  Training   81/1000  Loss  2.941
2018-06-12 14:03:58  Training   91/1000  Loss  2.940
Traceback (most recent call last):
File "turi.py", line 73, in 
model = tc.object_detector.create(train_data, feature='image', annotations='annotations', max_iterations=1000)
File "/home/ubuntu/venv/local/lib/python2.7/site-packages/turicreate/toolkits/object_detector/object_detector.py", line 336, in create
for batch in loader:
File "/home/ubuntu/venv/local/lib/python2.7/site-packages/turicreate/toolkits/object_detector/_sframe_loader.py", line 123, in next
return self._next()
File "/home/ubuntu/venv/local/lib/python2.7/site-packages/turicreate/toolkits/object_detector/_sframe_loader.py", line 232, in _next
self.sframe = self.sframe.sort(_TMP_COL_RANDOM_ORDER)
File "/home/ubuntu/venv/local/lib/python2.7/site-packages/turicreate/data_structures/sframe.py", line 5421, in sort
return SFrame(_proxy=self.proxy.sort(sort_column_names, sort_column_orders))
File "/home/ubuntu/venv/local/lib/python2.7/site-packages/turicreate/cython/context.py", line 49, in exit
raise exc_type(exc_value)
MemoryError: std::bad_alloc
		</comment>
		<comment id='19' author='dhgokul' date='2018-06-19T06:23:56Z'>
		&lt;denchmark-link:https://github.com/gustavla&gt;@gustavla&lt;/denchmark-link&gt;
 Any update on this issue ?
		</comment>
		<comment id='20' author='dhgokul' date='2018-06-25T22:00:59Z'>
		&lt;denchmark-link:https://github.com/dhgokul&gt;@dhgokul&lt;/denchmark-link&gt;
 I tried reproducing this on a VM with limited memory and so far I have been unable to. I will mark this as "need repro" and let some colleagues know so that they can give it a try.
		</comment>
		<comment id='21' author='dhgokul' date='2018-06-27T12:59:15Z'>
		&lt;denchmark-link:https://github.com/gustavla&gt;@gustavla&lt;/denchmark-link&gt;
 Still i am not able to train model in GPU machine. PFB link for dataset
Dataset size is small when compared to previous dataset - 534 images
&lt;denchmark-link:https://bugreport.apple.com/web/?problemID=41519407&gt;https://bugreport.apple.com/web/?problemID=41519407&lt;/denchmark-link&gt;

GPU log : Process keep on increase at initial training process (0  to 90 + percentage) ,after that it not processed
&lt;denchmark-link:https://user-images.githubusercontent.com/5689804/41975123-77d4e762-7a37-11e8-87ba-95d1b558a094.png&gt;&lt;/denchmark-link&gt;

CPU log : Process keep on loading
&lt;denchmark-link:https://user-images.githubusercontent.com/5689804/41975124-78072f06-7a37-11e8-9350-e40484c22b5a.png&gt;&lt;/denchmark-link&gt;

Training log : It keep on loading at 1st iteration
&lt;denchmark-link:https://user-images.githubusercontent.com/5689804/41975125-7838dbc8-7a37-11e8-86f5-58f91eedbe33.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='22' author='dhgokul' date='2018-06-27T13:00:57Z'>
		&lt;denchmark-link:https://github.com/gustavla&gt;@gustavla&lt;/denchmark-link&gt;
  Please check and let me know how to fix this issue, i tried same dataset in CPU machine, got memory error , but my dataset is only 584 images
Traceback (most recent call last):
File "turi.py", line 70, in 
model = tc.object_detector.create(train_data, feature='image', annotations='annotations', max_iterations=10)
File "/home/aximsoft/venv/local/lib/python2.7/site-packages/turicreate/toolkits/object_detector/object_detector.py", line 336, in create
for batch in loader:
File "/home/aximsoft/venv/local/lib/python2.7/site-packages/turicreate/toolkits/object_detector/_sframe_loader.py", line 123, in next
return self._next()
File "/home/aximsoft/venv/local/lib/python2.7/site-packages/turicreate/toolkits/object_detector/_sframe_loader.py", line 145, in _next
row = self.sframe[self.cur_sample]
File "/home/aximsoft/venv/local/lib/python2.7/site-packages/turicreate/data_structures/sframe.py", line 3540, in getitem
val_list = list(SFrame(_proxy = self.proxy.copy_range(lb, 1, ub)))
File "/home/aximsoft/venv/local/lib/python2.7/site-packages/turicreate/data_structures/sframe.py", line 3669, in generator
self.proxy.begin_iterator()
File "turicreate/cython/cy_sframe.pyx", line 231, in turicreate.cython.cy_sframe.UnitySFrameProxy.begin_iterator
File "turicreate/cython/cy_sframe.pyx", line 232, in turicreate.cython.cy_sframe.UnitySFrameProxy.begin_iterator
MemoryError: std::bad_alloc
		</comment>
		<comment id='23' author='dhgokul' date='2018-06-27T13:51:08Z'>
		&lt;denchmark-link:https://github.com/gustavla&gt;@gustavla&lt;/denchmark-link&gt;
  I got error mentioned below @ GPU machine
[13:48:20] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:107: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
Segmentation fault: 11
Stack trace returned 9 entries:
[bt] (0) /home/ubuntu/venv/local/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2a9e78) [0x7f79213dae78]
[bt] (1) /home/ubuntu/venv/local/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2909e8e) [0x7f7923a3ae8e]
[bt] (2) /lib/x86_64-linux-gnu/libc.so.6(+0x354b0) [0x7f79693194b0]
[bt] (3) /lib/x86_64-linux-gnu/libc.so.6(+0x15fe73) [0x7f7969443e73]
[bt] (4) /home/ubuntu/venv/local/lib/python2.7/site-packages/turicreate/cython/../libunity_shared.so(+0xe86455) [0x7f7948018455]
[bt] (5) /home/ubuntu/venv/local/lib/python2.7/site-packages/turicreate/cython/../libunity_shared.so(turi::lambda::lambda_master::bulk_eval(unsigned long, turi::sframe_rows const&amp;, std::vector&lt;turi::flexible_type, std::allocatorturi::flexible_type &gt;&amp;, bool, int)+0x18d) [0x7f7948018dfd]
[bt] (6) /home/ubuntu/venv/local/lib/python2.7/site-packages/turicreate/cython/../libunity_shared.so(+0x7d15a9) [0x7f79479635a9]
[bt] (7) /home/ubuntu/venv/local/lib/python2.7/site-packages/turicreate/cython/../libunity_shared.so(+0x7fdf80) [0x7f794798ff80]
[bt] (8) /home/ubuntu/venv/local/lib/python2.7/site-packages/turicreate/cython/../libunity_shared.so(+0x1a1331f) [0x7f7948ba531f]
		</comment>
		<comment id='24' author='dhgokul' date='2019-06-23T20:00:56Z'>
		Can I safely stop a training process at any time without losing my model? I've been iterating for a few days now and wish to stop as I think my loss values are sufficiently low now (went from over 80 to 0.106.
		</comment>
	</comments>
</bug>