<bug id='1314' author='nickjong' open_date='2019-01-11T21:19:01Z' closed_time='2019-10-02T21:26:33Z'>
	<summary>Object Detector deployed to Vision framework unexpectedly yields confidences at 0.999</summary>
	<description>
I've observed that training and exporting an object detector, and integrating into the Vision sample code &lt;denchmark-link:https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture&gt;here&lt;/denchmark-link&gt;
 yields confidences very close to 1. Training and exporting a model with the same data but without NMS yields confidences that span the range from 0 to 1 (after tweaking the sample code just to output the single highest-confidence box).
Our docs describe NMS as a filtering mechanism; we don't expect any "merging" of confidence scores. So there appears to be an error in either the docs, the CoreML implementation, or the Vision integration.
	</description>
	<comments>
		<comment id='1' author='nickjong' date='2019-09-27T18:37:07Z'>
		It turns out the issue is not due to non-maximum suppression,
instead it is due to the fact that the sample code for Object Detection provided by Apple
(&lt;denchmark-link:https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture&gt;https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture&lt;/denchmark-link&gt;
)
shows a 'normalized' confidence which enforces scores across all classes sum to 1,
which produce confidence near 1 for the predicted class.
If you want to get the original confidence score for the predicted class,
please refer to VNRecognizedObjectObservation.confidence instead.
		</comment>
	</comments>
</bug>