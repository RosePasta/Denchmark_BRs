<bug id='305' author='Ayiruss' open_date='2018-02-24T17:54:04Z' closed_time='2018-07-20T21:30:51Z'>
	<summary>Image similarity export to coreml on a model &amp;gt; 2GB fails</summary>
	<description>

No description provided.

	</description>
	<comments>
		<comment id='1' author='Ayiruss' date='2018-02-24T17:54:51Z'>
		Starting brute force nearest neighbors model training.
[libprotobuf FATAL google/protobuf/wire_format.cc:830] CHECK failed: (output-&gt;ByteCount()) == (expected_endpoint): : Protocol message serialized to a size different from what was originally expected.  Perhaps it was modified by another thread during serialization?
terminate called after throwing an instance of 'google::protobuf::FatalException'
what():  CHECK failed: (output-&gt;ByteCount()) == (expected_endpoint): : Protocol message serialized to a size different from what was originally expected.  Perhaps it was modified by another thread during serialization?
I am facing this issue when I use 400000 images to build the similarity search.
		</comment>
		<comment id='2' author='Ayiruss' date='2018-02-24T17:56:04Z'>
		Thanks for reporting this. Can you share the code snippet that caused this error.
		</comment>
		<comment id='3' author='Ayiruss' date='2018-02-24T17:59:51Z'>
		Thanks for the quick reply.
It's the same snippet which is given in the documentation.
reference_data = tc.image_analysis.load_images('LIM')
reference_data = reference_data.add_row_number()
reference_data.materialize()
model = tc.image_similarity.create(reference_data)
model.export_coreml('sim_model_large.mlmodel')
It fails in tc.image_similarity.create(reference_data)
Is there any limitations in no of images?
		</comment>
		<comment id='4' author='Ayiruss' date='2018-02-24T18:02:52Z'>
		Your error message smells like an issue with the export_coreml. Can you try it without the export_coreML but on a smaller set of data? I suspect you are hitting the 2G limit on protobuf.
		</comment>
		<comment id='5' author='Ayiruss' date='2018-02-24T18:04:03Z'>
		I tried with 1.6GB mlmodel which worked fine. I am just having issue with larger dataset. can't we use the similarity for larger dataset?
		</comment>
		<comment id='6' author='Ayiruss' date='2018-02-24T18:11:12Z'>
		Are you trying to deploy this on your phone? or using it locally? If its the latter, you can do the '.save' instead of '.export_coreml'.
I think we are hitting the 2GB (see &lt;denchmark-link:https://chromium.googlesource.com/external/github.com/google/protobuf/+/HEAD/src/google/protobuf/message_lite.cc#289&gt;https://chromium.googlesource.com/external/github.com/google/protobuf/+/HEAD/src/google/protobuf/message_lite.cc#289&lt;/denchmark-link&gt;
)  limit of a protocol message when exporting to coreml.
This should work fine if you do .save. I'll make a change to the user guide right away to make that clear.
&lt;denchmark-code&gt;model = tc.image_similarity.create(reference_data)
model.save('myModel.mdl')
&lt;/denchmark-code&gt;

We'll try and work out how to serialize the larger model.
		</comment>
		<comment id='7' author='Ayiruss' date='2018-02-24T19:22:47Z'>
		I can able to save the model now.
Thanks a lot!
		</comment>
		<comment id='8' author='Ayiruss' date='2018-02-25T01:46:52Z'>
		I've changed the title to reflect the bug.
		</comment>
		<comment id='9' author='Ayiruss' date='2018-07-20T21:30:51Z'>
		This is a Won't Fix. Protobuf can't support very large messages (&gt;= 2 GB) with decent performance; we max out at about 2 GB on export to Core ML as a result. In real usage, it's unlikely a 2GB+ model will be the right tradeoff between size and usefulness for a macOS or iOS app.
		</comment>
		<comment id='10' author='Ayiruss' date='2020-05-18T00:19:52Z'>
		
开始蛮力最近邻居模型训练。
[libprotobuf重要google / protobuf / wire_format.cc：830]检查失败：（output-&gt; ByteCount（））==（expected_endpoint）：：协议消息序列化为与最初预期大小不同的大小。也许它是在序列化期间被另一个线程修改的？
抛出'google :: protobuf :: FatalException'实例之后调用终止终止
what（）：检查失败：（output-&gt; ByteCount（））==（expected_endpoint）:：协议消息序列化为与最初预期大小不同的大小。也许它是在序列化期间被另一个线程修改的？
当我使用400000张图像建立相似度搜索时，我面临着这个问题。

Hello, have you solved the problem of saving the model?I face the same problem but don't know how to solve it.My training model reached more than 2G.Looking forward to your reply. Thank you
		</comment>
		<comment id='11' author='Ayiruss' date='2020-05-18T00:23:08Z'>
		
Starting brute force nearest neighbors model training.
[libprotobuf FATAL google/protobuf/wire_format.cc:830] CHECK failed: (output-&gt;ByteCount()) == (expected_endpoint): : Protocol message serialized to a size different from what was originally expected. Perhaps it was modified by another thread during serialization?
terminate called after throwing an instance of 'google::protobuf::FatalException'
what(): CHECK failed: (output-&gt;ByteCount()) == (expected_endpoint): : Protocol message serialized to a size different from what was originally expected. Perhaps it was modified by another thread during serialization?
I am facing this issue when I use 400000 images to build the similarity search.

Hello, have you solved the problem of saving the model?I face the same problem but don't know how to solve it.My training model reached more than 2G.Looking forward to your reply. Thank you
		</comment>
	</comments>
</bug>