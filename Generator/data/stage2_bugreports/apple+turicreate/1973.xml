<bug id='1973' author='znation' open_date='2019-05-29T04:43:48Z' closed_time='2019-07-03T22:22:40Z'>
	<summary>Unit test failures in drawing_classifier</summary>
	<description>
Repro steps:
&lt;denchmark-code&gt;pytest test/test_drawing_classifier.py
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;=================================== FAILURES ===================================
____________ DrawingClassifierTest.test_evaluate_with_ground_truth _____________

self = &lt;turicreate.test.test_drawing_classifier.DrawingClassifierTest testMethod=test_evaluate_with_ground_truth&gt;

    def test_evaluate_with_ground_truth(self):
    
        all_metrics = ["accuracy", "auc", "precision", "recall",
                       "f1_score", "log_loss", "confusion_matrix", "roc_curve"]
        for index in range(len(self.models)):
            model = self.models[index]
            sf = self.trains[index]
            individual_run_results = dict()
            for metric in all_metrics:
&gt;               evaluation = model.evaluate(sf, metric=metric)

test_drawing_classifier.py:241: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../toolkits/drawing_classifier/drawing_classifier.py:725: in evaluate
    'recall', 'auc', 'roc_curve', 'confusion_matrix']}
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = &lt;listiterator object at 0x7faeb4655550&gt;

&gt;   evaluation_result = {k: ret[k] for k in ['accuracy', 'f1_score', 'precision',
                                             'recall', 'auc', 'roc_curve', 'confusion_matrix']}
E   KeyError: 'f1_score'

../toolkits/drawing_classifier/drawing_classifier.py:724: KeyError
----------------------------- Captured stdout call -----------------------------
Predicting 11/10
_______ DrawingClassifierFromScratchTest.test_evaluate_with_ground_truth _______

self = &lt;turicreate.test.test_drawing_classifier.DrawingClassifierFromScratchTest testMethod=test_evaluate_with_ground_truth&gt;

    def test_evaluate_with_ground_truth(self):
    
        all_metrics = ["accuracy", "auc", "precision", "recall",
                       "f1_score", "log_loss", "confusion_matrix", "roc_curve"]
        for index in range(len(self.models)):
            model = self.models[index]
            sf = self.trains[index]
            individual_run_results = dict()
            for metric in all_metrics:
&gt;               evaluation = model.evaluate(sf, metric=metric)

test_drawing_classifier.py:241: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../toolkits/drawing_classifier/drawing_classifier.py:725: in evaluate
    'recall', 'auc', 'roc_curve', 'confusion_matrix']}
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = &lt;listiterator object at 0x7faeb45678d0&gt;

&gt;   evaluation_result = {k: ret[k] for k in ['accuracy', 'f1_score', 'precision',
                                             'recall', 'auc', 'roc_curve', 'confusion_matrix']}
E   KeyError: 'f1_score'

../toolkits/drawing_classifier/drawing_classifier.py:724: KeyError
----------------------------- Captured stdout call -----------------------------
Predicting 11/10
______ DrawingClassifierUsingQuickdraw245.test_evaluate_with_ground_truth ______

self = &lt;turicreate.test.test_drawing_classifier.DrawingClassifierUsingQuickdraw245 testMethod=test_evaluate_with_ground_truth&gt;

    def test_evaluate_with_ground_truth(self):
    
        all_metrics = ["accuracy", "auc", "precision", "recall",
                       "f1_score", "log_loss", "confusion_matrix", "roc_curve"]
        for index in range(len(self.models)):
            model = self.models[index]
            sf = self.trains[index]
            individual_run_results = dict()
            for metric in all_metrics:
&gt;               evaluation = model.evaluate(sf, metric=metric)

test_drawing_classifier.py:241: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../toolkits/drawing_classifier/drawing_classifier.py:725: in evaluate
    'recall', 'auc', 'roc_curve', 'confusion_matrix']}
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = &lt;listiterator object at 0x7faeb44e8c50&gt;

&gt;   evaluation_result = {k: ret[k] for k in ['accuracy', 'f1_score', 'precision',
                                             'recall', 'auc', 'roc_curve', 'confusion_matrix']}
E   KeyError: 'f1_score'

../toolkits/drawing_classifier/drawing_classifier.py:724: KeyError
----------------------------- Captured stdout call -----------------------------
Predicting 11/10
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='znation' date='2019-07-02T17:04:33Z'>
		It looks like the test simply isn't in alignment with the toolkit as to what metrics get computed? Let's figure out what's going on here and get these tests back in rotation
		</comment>
	</comments>
</bug>