<bug id='1079' author='baihualinxin' open_date='2018-09-12T01:55:25Z' closed_time='2019-01-15T19:25:10Z'>
	<summary>Using Vision algorithm frame is not accurate</summary>
	<description>
The boundingBox box is not prepared at all，What causes it
Python 2.7
turicreate 5.0b3
mac 10.14 Beta
xcode 10.0 beta 3
iphone：iPhone 8 plus ,IOS 12  beta
Training set use INRIA Annotations for graz-02 (IG02) Cars
Convert the sframe code
&lt;denchmark-code&gt;# Change if applicable
ig02_path = '/Users/dchealth/Desktop/ig02-v1.0-cars'

# Load all images in random order
raw_sf = tc.image_analysis.load_images(ig02_path, recursive=True,
                                       random_order=True)

# Split file names so that we can determine what kind of image each row is
# E.g. bike_005.mask.0.png -&gt; ['bike_005', 'mask']
info = raw_sf['path'].apply(lambda path: os.path.basename(path).split('.')[:2])

# Rename columns to 'name' and 'type'
info = info.unpack().rename({'X.0': 'name', 'X.1': 'type'})

# Add to our main SFrame
raw_sf = raw_sf.add_columns(info)

# Extract label (e.g. 'bike') from name (e.g. 'bike_003')
raw_sf['label'] = raw_sf['name'].apply(lambda name: name.split('_')[0])

# Original path no longer needed
del raw_sf['path']

# Split into images and masks
sf_images = raw_sf[raw_sf['type'] == 'image']
sf_masks = raw_sf[raw_sf['type'] == 'mask']

def mask_to_bbox_coordinates(img):
    """
    Takes a tc.Image of a mask and returns a dictionary representing bounding
    box coordinates: e.g. {'x': 100, 'y': 120, 'width': 80, 'height': 120}
    """
    import numpy as np
    mask = img.pixel_data
    if mask.max() == 0:
        return None
    # Take max along both x and y axis, and find first and last non-zero value
    x0, x1 = np.where(mask.max(0))[0][[0, -1]]
    y0, y1 = np.where(mask.max(1))[0][[0, -1]]

    return {'x': (x0 + x1) / 2, 'width': (x1 - x0),
            'y': (y0 + y1) / 2, 'height': (y1 - y0)}

# Convert masks to bounding boxes (drop masks that did not contain bounding box)
sf_masks['coordinates'] = sf_masks['image'].apply(mask_to_bbox_coordinates)

# There can be empty masks (which returns None), so let's get rid of those
sf_masks = sf_masks.dropna('coordinates')

# Combine label and coordinates into a bounding box dictionary
sf_masks = sf_masks.pack_columns(['label', 'coordinates'],
                                 new_column_name='bbox', dtype=dict)

# Combine bounding boxes of the same 'name' into lists
sf_annotations = sf_masks.groupby('name',
                                 {'annotations': tc.aggregate.CONCAT('bbox')})

# Join annotations with the images. Note, some images do not have annotations,
# but we still want to keep them in the dataset. This is why it is important to
# a LEFT join.
sf = sf_images.join(sf_annotations, on='name', how='left')

# The LEFT join fills missing matches with None, so we replace these with empty
# lists instead using fillna.
sf['annotations'] = sf['annotations'].fillna([])

# Remove unnecessary columns
del sf['type']

# Save SFrame
sf.save('/Users/dchealth/Desktop/ig02-v1.0-cars/ig02car.sframe')
#
&lt;/denchmark-code&gt;

Training code
&lt;denchmark-code&gt;sf = tc.SFrame({'id': range(100)})
sf_train, sf_test = sf.random_split(.9, seed=10)
print(len(sf_train), len(sf_test))
# Load the data
data =  tc.SFrame('/Users/dchealth/Desktop/ig02-v1.0-cars/ig02car.sframe')

# Make a train-test split
train_data, test_data = data.random_split(0.8)

# Create a model
model = tc.object_detector.create(train_data)

# Save predictions to an SArray
predictions = model.predict(test_data)

# Evaluate the model and save the results into a dictionary
metrics = model.evaluate(test_data)

# Save the model for later use in Turi Create
model.save('/Users/dchealth/Desktop/ig02-v1.0-cars/002mymodel.model')

# Export for use in Core ML
model.export_coreml('/Users/dchealth/Desktop/ig02-v1.0-cars/MyCustomObjectDetector002.mlmodel')
&lt;/denchmark-code&gt;

Use this demo to run the mlmodel from training

&lt;denchmark-link:https://user-images.githubusercontent.com/13042809/45397666-b17c2e80-b673-11e8-850d-c86186510411.jpeg&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='baihualinxin' date='2018-09-12T07:18:18Z'>
		May be related to &lt;denchmark-link:https://github.com/apple/turicreate/issues/1016&gt;#1016&lt;/denchmark-link&gt;
. &lt;denchmark-link:https://github.com/shantanuchhabra&gt;@shantanuchhabra&lt;/denchmark-link&gt;
: Please take a look.
		</comment>
		<comment id='2' author='baihualinxin' date='2018-09-18T08:57:53Z'>
		&lt;denchmark-link:https://github.com/shantanuchhabra&gt;@shantanuchhabra&lt;/denchmark-link&gt;
 The problem of training, or the problem of conversion?
		</comment>
		<comment id='3' author='baihualinxin' date='2018-10-20T21:33:43Z'>
		&lt;denchmark-link:https://github.com/baihualinxin&gt;@baihualinxin&lt;/denchmark-link&gt;
  - Any luck figuring this one out? I still can't tell if this is an issue with Turi Create or iOS 12 / Vision Framework / Apple's demo applications.
I've been having these same issues whenever using the models that return VNRecognizedObjectObservations since WWDC 2018 when using Apple's demo apps:
&lt;denchmark-link:https://developer.apple.com/documentation/vision/tracking_multiple_objects_or_rectangles_in_video&gt;https://developer.apple.com/documentation/vision/tracking_multiple_objects_or_rectangles_in_video&lt;/denchmark-link&gt;

&lt;denchmark-link:https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture&gt;https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture&lt;/denchmark-link&gt;

In addition to the bounding boxes being way off, I'm also getting unrealistically high confidence scores (0.999+ in most cases), which also increases the false positives to the point that the model is not usable. When I use the older model type that returns VNCoreMLFeatureValueObservation with demo apps from last year, the bounding boxes are far more accurate and the confidence scores are reasonable/accurate.
At this point, I've started to continue using the older model type from last year and rebuilt several of my object detection apps to accommodate this. I'd much rather using the newer features and code, as it's cleaner and likely won't be phased out as quickly -- but after months of troubleshooting without any breakthrough, I've had to stick with what works.
		</comment>
		<comment id='4' author='baihualinxin' date='2018-10-20T21:54:17Z'>
		I posted on another issue about the inaccurate confidence, I’m also getting .999+ confidence!
		</comment>
		<comment id='5' author='baihualinxin' date='2019-01-10T16:00:54Z'>
		&lt;denchmark-link:https://github.com/shantanuchhabra&gt;@shantanuchhabra&lt;/denchmark-link&gt;
 Can you confirm that this is a dup of &lt;denchmark-link:https://github.com/apple/turicreate/issues/1016&gt;#1016&lt;/denchmark-link&gt;
. A known issue with Vision.
		</comment>
		<comment id='6' author='baihualinxin' date='2019-01-11T21:19:59Z'>
		I believe this is a dupe, although the .999 confidence thing is separate. I filed &lt;denchmark-link:https://github.com/apple/turicreate/issues/1314&gt;#1314&lt;/denchmark-link&gt;
 to track that issue
		</comment>
		<comment id='7' author='baihualinxin' date='2019-01-15T19:25:10Z'>
		I believe this is a dupe of &lt;denchmark-link:https://github.com/apple/turicreate/issues/1016&gt;#1016&lt;/denchmark-link&gt;
.
&lt;denchmark-link:https://github.com/baihualinxin&gt;@baihualinxin&lt;/denchmark-link&gt;
 , can you try adding
&lt;denchmark-code&gt;objectRecognition.imageCropAndScaleOption = .scaleFill
&lt;/denchmark-code&gt;

on your  object like &lt;denchmark-link:https://github.com/gustavla&gt;@gustavla&lt;/denchmark-link&gt;
 recommended in the &lt;denchmark-link:https://github.com/apple/turicreate/issues/1016&gt;#1016&lt;/denchmark-link&gt;
 thread? Feel free to reopen the issue if that doesn't fix it for you!
		</comment>
	</comments>
</bug>