<bug id='267' author='tarakinsuide' open_date='2018-02-13T11:20:22Z' closed_time='2018-06-18T17:38:44Z'>
	<summary>Add batch_size as an option to image_classifier and reduce the default from 512 for memory reasons.</summary>
	<description>
Why is my training file throwing this error?
My code is below:
&lt;denchmark-code&gt;import turicreate as tc

# Load the data
data = tc.SFrame('/home/tarak/PycharmProjects/turicreate1/peacock.sframe')

# Make a train-test split
train_data, test_data = data.random_split(0.8)

# Create a model
model = tc.image_classifier.create(train_data, target='label',  max_iterations=1000)

# Save predictions to an SFrame (class and corresponding class-probabilities)
predictions = model.classify(test_data)

# Evaluate the model and save the results into a dictionary
results = model.evaluate(test_data)
print ("Accuracy         : %s" % results['accuracy'])
print ("Confusion Matrix : \n%s" % results['confusion_matrix'])

# Save the model for later usage in Turi Create
model.save('peacock.model')
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='tarakinsuide' date='2018-02-13T11:21:04Z'>
		[15:58:47] src/nnvm/legacy_json_util.cc:190: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...
[15:58:47] src/nnvm/legacy_json_util.cc:198: Symbol successfully upgraded!
Resizing images...
Performing feature extraction on resized images...
terminate called after throwing an instance of 'std::bad_alloc'
what():  std::bad_alloc
Process finished with exit code 134 (interrupted by signal 6: SIGABRT)
		</comment>
		<comment id='2' author='tarakinsuide' date='2018-02-14T05:07:00Z'>
		Hi &lt;denchmark-link:https://github.com/tarakinsuide&gt;@tarakinsuide&lt;/denchmark-link&gt;
! I'm sorry that you are having this issue. Let's see if you can figure out what's going on.
Can you try firing up python and doing:
&gt;&gt;&gt; import mxnet
Does that produce the same error? I suspect your MXNet installation is broken somehow. Which version do you have (pip freeze | grep -i mxnet)?
		</comment>
		<comment id='3' author='tarakinsuide' date='2018-05-22T12:42:40Z'>
		Hi &lt;denchmark-link:https://github.com/gustavla&gt;@gustavla&lt;/denchmark-link&gt;
. I am facing the same issue when running it on an Ubuntu 18.04 LTS cloud instance. Any idea, when this is going to be fixed?
		</comment>
		<comment id='4' author='tarakinsuide' date='2018-05-22T13:32:54Z'>
		&lt;denchmark-link:https://github.com/askayastha&gt;@askayastha&lt;/denchmark-link&gt;
 Some clarifying questions to narrow down the issue:

How large are your images
How much memory does your machine have?
Are you using the CPU or the GPU?

		</comment>
		<comment id='5' author='tarakinsuide' date='2018-05-22T13:57:22Z'>
		&lt;denchmark-link:https://github.com/srikris&gt;@srikris&lt;/denchmark-link&gt;
 Here's my data and instance configuration:

I am using the Kaggle Cats and Dogs Dataset with around 30 KB per image (approx. 500 X 500 px).
I am using an instance of 4 GB memory.
I am using 2 core Intel(R) Xeon(R) CPU @ 2.30GHz without the GPU.

		</comment>
		<comment id='6' author='tarakinsuide' date='2018-05-22T13:58:08Z'>
		Can you try reducing the batch_size to 32? The default batch size might be larger.
		</comment>
		<comment id='7' author='tarakinsuide' date='2018-05-22T14:11:45Z'>
		&lt;denchmark-link:https://github.com/srikris&gt;@srikris&lt;/denchmark-link&gt;
 I am just getting started with turicreate. Where can we set the batch_size in this framework?
&lt;denchmark-code&gt;# Load the data
data = tc.SFrame('cats-dogs.sframe')

# Make a train-test split
train_data, test_data = data.random_split(0.8)

# Automatically pick the right model based on your data.
# Note: Because the dataset is large, model creation may take hours.
model = tc.image_classifier.create(train_data, target='label')

# Save predictions to an SArray
predictions = model.predict(test_data)

# Evaluate the model and save the results into a dictionary
metrics = model.evaluate(test_data)
print(metrics['accuracy'])

# Save the model for later use in Turi Create
model.save('mymodel.model')

# Export for use in Core ML
model.export_coreml('MyCustomImageClassifier.mlmodel')
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='tarakinsuide' date='2018-05-22T14:20:54Z'>
		Before I suggest a workaround for this, can you try this:
&lt;denchmark-code&gt;model = tc.image_classifier.create(train_data, target = 'label', model = 'squeezenet_v1.1')
&lt;/denchmark-code&gt;

And see if that crashes.
		</comment>
		<comment id='9' author='tarakinsuide' date='2018-05-22T14:35:26Z'>
		&lt;denchmark-link:https://github.com/srikris&gt;@srikris&lt;/denchmark-link&gt;
 Using squeezenet_v1.1 didn't work. Here's the full error log:
Downloading &lt;denchmark-link:https://docs-assets.developer.apple.com/turicreate/models/squeezenet_v1.1-symbol.json&gt;https://docs-assets.developer.apple.com/turicreate/models/squeezenet_v1.1-symbol.json&lt;/denchmark-link&gt;

Download completed: /var/tmp/model_cache/squeezenet_v1.1-symbol.json
Downloading &lt;denchmark-link:https://docs-assets.developer.apple.com/turicreate/models/squeezenet_v1.1-0000.params&gt;https://docs-assets.developer.apple.com/turicreate/models/squeezenet_v1.1-0000.params&lt;/denchmark-link&gt;

Download completed: /var/tmp/model_cache/squeezenet_v1.1-0000.params
[14:23:47] src/nnvm/legacy_json_util.cc:190: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...
[14:23:47] src/nnvm/legacy_json_util.cc:198: Symbol successfully upgraded!
Resizing images...
Performing feature extraction on resized images...
^AcCorrupt JPEG data: 401 extraneous bytes before marker 0xd9
Corrupt JPEG data: 240 extraneous bytes before marker 0xd9
Corrupt JPEG data: 215 extraneous bytes before marker 0xd9
Corrupt JPEG data: 131 extraneous bytes before marker 0xd9
Corrupt JPEG data: 161 extraneous bytes before marker 0xd9
Warning: unknown JFIF revision number 0.00
Corrupt JPEG data: 1406 extraneous bytes before marker 0xd9
Corrupt JPEG data: 258 extraneous bytes before marker 0xd9
Corrupt JPEG data: 102 extraneous bytes before marker 0xd9
Corrupt JPEG data: 2229 extraneous bytes before marker 0xd9
Corrupt JPEG data: 1158 extraneous bytes before marker 0xd9
Corrupt JPEG data: 67 extraneous bytes before marker 0xd9
terminate called after throwing an instance of 'std::bad_alloc'
what():  std::bad_alloc
Aborted (core dumped)
		</comment>
		<comment id='10' author='tarakinsuide' date='2018-05-22T14:42:14Z'>
		&lt;denchmark-link:https://github.com/srikris&gt;@srikris&lt;/denchmark-link&gt;
 Without any modification, the sample code from turicreate docs doesn't get this error in my mid-2012 MacBook Air with far less CPU and memory specs. However, due the size of the dataset I had to cancel the training and run it on a cloud instance.
		</comment>
		<comment id='11' author='tarakinsuide' date='2018-05-22T14:44:48Z'>
		&lt;denchmark-link:https://github.com/askayastha&gt;@askayastha&lt;/denchmark-link&gt;
 Oh. Interesting. That's good to know. I was unable to repro this on a lower end Mac. I thought it was a memory issue but it might be something else. Can you check the mxnet version you have
&lt;denchmark-code&gt;pip freeze | grep mxnet
&lt;/denchmark-code&gt;

		</comment>
		<comment id='12' author='tarakinsuide' date='2018-05-22T14:46:06Z'>
		&lt;denchmark-link:https://github.com/srikris&gt;@srikris&lt;/denchmark-link&gt;
 Here's the mxnet version.
mxnet==1.1.0.post0
		</comment>
		<comment id='13' author='tarakinsuide' date='2018-05-22T14:52:14Z'>
		Lets check if the batch_size is an issue. Here is a quick test (unfortunately, we haven't exposed this as a public parameters, we'll do that right away since it will effect people in low memory settings)
To help us diagnose this, can you try the following:
&lt;denchmark-code&gt;python -c 'import turicreate; print turicreate.image_classifier.__file__'
&lt;/denchmark-code&gt;

It till give you a file  that looks a bit like this /Users/foo/lib/python2.7/site-packages/turicreate/toolkits/image_classifier/__init__.pyc
Now go into the directory but to a different file (replace the line below with the path that you got from the above command)
&lt;denchmark-code&gt;vim /Users/foo/lib/python2.7/site-packages/turicreate/toolkits/image_classifier/image_classifier.py
&lt;/denchmark-code&gt;

and edit 3 lines (around line 119) to include batch_size and set it to 1
&lt;denchmark-code&gt;# Extract features
extracted_features = _tc.SFrame({
target: dataset[target],
 '__image_features__': feature_extractor.extract_features(dataset, feature, batch_size = 1, verbose=verbose),
})
&lt;/denchmark-code&gt;

Now, run your script again. If it still crashes, then we have a different issue.
		</comment>
		<comment id='14' author='tarakinsuide' date='2018-05-22T16:43:39Z'>
		&lt;denchmark-link:https://github.com/srikris&gt;@srikris&lt;/denchmark-link&gt;
 Setting the batch_size to 32 solved the issue. I see that the default batch size was 512. Are there plans to have the batch_size parameter exposed in the public API?
A few observations that I've made during the training is that the default batch_size of 512 fails even on a high end 6 core machine with 5.5 GB memory. What's going on here?
Thanks.
		</comment>
		<comment id='15' author='tarakinsuide' date='2018-05-22T16:44:29Z'>
		Great. We will pull that into the next release thanks.
		</comment>
		<comment id='16' author='tarakinsuide' date='2018-05-22T17:07:28Z'>
		That's really good to know, thanks &lt;denchmark-link:https://github.com/askayastha&gt;@askayastha&lt;/denchmark-link&gt;
. There is an issue tracking this &lt;denchmark-link:https://github.com/apple/turicreate/issues/26&gt;#26&lt;/denchmark-link&gt;
 and I was never convinced it was a memory issue. It looks like it is thanks to your experiment, so that is really useful to know. We'll add this option.
		</comment>
		<comment id='17' author='tarakinsuide' date='2018-05-22T17:22:43Z'>
		&lt;denchmark-link:https://github.com/srikris&gt;@srikris&lt;/denchmark-link&gt;
 The 'bad_alloc' error appeared at the end of the training again. Here's the error log:
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

+-----------+----------+-----------+--------------+-------------------+---------------------+
| Iteration | Passes   | Step size | Elapsed Time | Training-accuracy | Validation-accuracy |
+-----------+----------+-----------+--------------+-------------------+---------------------+
| 1         | 3        | 0.000053  | 2.382877     | 0.980992          | 0.972056            |
| 2         | 6        | 0.500000  | 4.161545     | 0.982057          | 0.981038            |
| 3         | 7        | 0.500000  | 5.036564     | 0.982803          | 0.983034            |
| 4         | 8        | 0.500000  | 5.891734     | 0.983814          | 0.983034            |
| 5         | 9        | 0.500000  | 6.756862     | 0.985571          | 0.981038            |
| 6         | 10       | 0.500000  | 7.628246     | 0.987595          | 0.984032            |
| 7         | 11       | 0.500000  | 8.492141     | 0.966777          | 0.970060            |
| 8         | 13       | 1.000000  | 9.810217     | 0.989032          | 0.988024            |
| 9         | 14       | 1.000000  | 10.678994    | 0.989618          | 0.991018            |
| 10        | 15       | 1.000000  | 11.534363    | 0.990736          | 0.993014            |
+-----------+----------+-----------+--------------+-------------------+---------------------+
TERMINATED: Iteration limit reached.
This model may not be optimal. To improve it, consider increasing max_iterations.
Corrupt JPEG data: 229 extraneous bytes before marker 0xd9
Corrupt JPEG data: 161 extraneous bytes before marker 0xd9
Corrupt JPEG data: 1406 extraneous bytes before marker 0xd9
Corrupt JPEG data: 131 extraneous bytes before marker 0xd9
terminate called after throwing an instance of 'std::bad_alloc'
what():  std::bad_alloc
Aborted (core dumped)
		</comment>
		<comment id='18' author='tarakinsuide' date='2018-05-22T17:24:29Z'>
		Oh. This is now in the predict code. Its the same issue. Can you make the same update to the file in the prediction function:
&lt;denchmark-code&gt;/Users/foo/lib/python2.7/site-packages/turicreate/toolkits/image_classifier/image_classifier.py
&lt;/denchmark-code&gt;

		</comment>
		<comment id='19' author='tarakinsuide' date='2018-06-18T17:38:44Z'>
		&lt;denchmark-link:https://github.com/askayastha&gt;@askayastha&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tarakinsuide&gt;@tarakinsuide&lt;/denchmark-link&gt;
 - with the release of beta 1 for Turi Create 5.0, a  parameter has been added to , , , and . We have also reduced the default value to 64.
		</comment>
	</comments>
</bug>