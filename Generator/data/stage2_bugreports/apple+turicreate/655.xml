<bug id='655' author='kvaughan-ct' open_date='2018-06-12T16:43:45Z' closed_time='2018-07-25T22:18:56Z'>
	<summary>Negative number loss on model training, low confidence in strongly accurate evaluation</summary>
	<description>
Discussed this with several of your engineers last week at WWDC, who asked me to post this here.
General problem domain: Trying to detect lego pieces within an image frame. This would be used in AR to be able to pinpoint where a particular piece is out of an assortment of parts that are spread out. Initially, I am trying to get just a single piece working, though ultimately a couple of hundred different pieces are a more real world scenario.
The model I've trained for a single part exhibits two peculiar characteristics that the engineers I spoke with wanted to explore:

When the model trained, it showed negative loss numbers for much of the training session.
With the trained model, the IMG_1081.JPG test image in the archive is able to be accurately detected but exhibits a confidence value of just 0.2%, while an evaluate() call against the trained data set shows average precisions of 1.0.

An archive with an sframe, model and test images is &lt;denchmark-link:https://www.dropbox.com/s/l1edyncw0bqntts/LegoObjectDetection.zip?dl=0&gt;available here&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='kvaughan-ct' date='2018-06-20T00:33:58Z'>
		Hi &lt;denchmark-link:https://github.com/kvaughan-ct&gt;@kvaughan-ct&lt;/denchmark-link&gt;
 , thanks for posting here, it was really nice talking to you at WWDC! Hope you're doing well! :-)

The negative loss numbers were very bizarre and unexpected -- I was puzzled to see them as well. I tried to train an Object Detector Model using the SFrame you shared, but I'm not able to repro the negative loss numbers. I'm using Turi Create version 5.0b1 on macOS 10.14 with Python 2.7.10. Could you share what version of macOS, Python, and TC you're using so I can do an exact repro?
Furthermore, do you do anything apart from model = tc.object_detector.create(sf, 'annotations', 'image') that I may be missing? Any train/test split?

Looking forward to hearing from you!
		</comment>
		<comment id='2' author='kvaughan-ct' date='2018-07-24T20:20:14Z'>
		Hi &lt;denchmark-link:https://github.com/kvaughan-ct&gt;@kvaughan-ct&lt;/denchmark-link&gt;
 , just wanted to check in on this!
		</comment>
		<comment id='3' author='kvaughan-ct' date='2018-07-25T13:39:11Z'>
		I've not been able to reproduce this either. I'm glad you all saw it in person at least so that I know I wasn't going crazy. My only guess at this point is that I had built cuda on 10.13 and then after upgrading to the beta, didn't get all of it removed before rebuilding on 10.14. Perhaps some mismatched linking caused the issue. I had already cleaned every remnant of cuda that I could find and rebuilt it by the time I was looking at reproduction, and since then have also upgraded to the more recent model MBP. The model is still not very confident, but when I have spare cycles I will continue trying to figure out what sort of data set might help solve this problem, if its solvable at all with current tech.
		</comment>
		<comment id='4' author='kvaughan-ct' date='2018-07-25T22:18:56Z'>
		Thanks for letting us know! Feel free to post if you run into any issues!
Closing this issue since none of us on this thread can repro.
		</comment>
	</comments>
</bug>