<bug id='1016' author='cianiandreadev' open_date='2018-08-21T07:08:18Z' closed_time='2018-11-28T19:05:14Z'>
	<summary>Different behavior between Model and Core ML model</summary>
	<description>
I successfully trained a Object Detection model (default TC YOLO) with TC b3 and exported in CoreML format.
My model has 8000 iterations and 0.8 final loss.
I then validate it with some images using TC and bounding box drawing util and it recognizes them better that I expected!
I then downloaded &lt;denchmark-link:https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture&gt;the sample project for recognizing objects in live capture&lt;/denchmark-link&gt;
 presented by &lt;denchmark-link:https://github.com/znation&gt;@znation&lt;/denchmark-link&gt;
 during the WWDC and replaced the model in the project with my new model.
What it's weird is that the object are no longer recognized. Is NOT a problem of VNDetectedObjectObservation because they are correctly returned, but it seems that the classing and the bounding box does not represent the detected object correctly (different class and wrong bounding box).
I used iOS 12 beta 9 and Xcode 10 beta 6 as developing environment, with an iPad Pro 2017 (or 2016, I don't remember).
From my first test seems this could be a rotation issue, but I don't know if that is the real issue and eventually how to fix it.
Does anybody faced a similar issue or can eventually help me with that?
	</description>
	<comments>
		<comment id='1' author='cianiandreadev' date='2018-08-21T13:57:11Z'>
		I currently found the problem: I think that my model is orientation dependent, while the model for the breakfast-recognition is orientation-independent (in terms of objects).
My model is trained to recognize objects only in one orientation (and doesn't even make sense for me to train for other orientations because I know that the orientation will always be the same).
Now, this is a pure iOS/Vision problem and not related to TC, but if anyone knows how to rotate the CMSampleBuffer before sanding to the Vision framework would be really appreciated a hint.
		</comment>
		<comment id='2' author='cianiandreadev' date='2018-08-21T17:55:49Z'>
		&lt;denchmark-link:https://github.com/cianiandreadev&gt;@cianiandreadev&lt;/denchmark-link&gt;
 - glad you figured the issue out. Thanks for letting us know about the fix.
		</comment>
		<comment id='3' author='cianiandreadev' date='2018-08-21T23:27:22Z'>
		I'm not an expert, but in the sample code there's a spot in VisionObjectRecognitionViewController's captureOutput method where a CVPixelBuffer/CVImageBuffer is retrieved from the CMSampleBuffer. Probably the image can be rotated there.
But the very next line, creating the VNImageRequestHandler, takes an orientation argument...
		</comment>
		<comment id='4' author='cianiandreadev' date='2018-08-22T05:53:18Z'>
		Thanks &lt;denchmark-link:https://github.com/nickjong&gt;@nickjong&lt;/denchmark-link&gt;
 for you reply. Yes, the orientation is indeed passed. I also tried to force the video connection to be in portrait mode using  but in the end I still don't have the correct result.
I can will post you the screenshots, maybe they can help:
&lt;denchmark-link:https://user-images.githubusercontent.com/1215511/44445109-0df1ae00-a5e0-11e8-97ab-2db0f576b6fc.jpg&gt;&lt;/denchmark-link&gt;

In the photo itself you can see the object recognized perfectly with the TC model, on yellow there is instead the CoreML+Vision recognition.
The project I'm using is the breakfast example presented by &lt;denchmark-link:https://github.com/znation&gt;@znation&lt;/denchmark-link&gt;
 with the portrait edit (without it the model wasn't able to recognize the object at all). I also slightly modified the rectangle to just show the border instead of filling it.
Any idea why so much difference between CoreML and TC?
		</comment>
		<comment id='5' author='cianiandreadev' date='2018-08-22T19:55:56Z'>
		Reopening - this difference is not expected. Let's make sure it can be explained by the rotation/orientation (and see if we can fix it that way) before concluding that this isn't a bug.
		</comment>
		<comment id='6' author='cianiandreadev' date='2018-08-23T06:59:56Z'>
		Thanks a lot &lt;denchmark-link:https://github.com/znation&gt;@znation&lt;/denchmark-link&gt;
 for your answer. I double-checked the orientation and I don't think is related to that now.
I just submit a new screenshot with multiple animals if help. The animals are perfectly recognized with very good confidence, then I would say that the model works well.
The red overlay is the detectionOverlay, that I put visible to check if it is right (and it seems to be since the camera is on 640x480 mode).
The code I used is 99% based on the one presented in the WWDC (I referred it with a link in the first post of this issue). I can submit any snippet of it if helps.
&lt;denchmark-link:https://user-images.githubusercontent.com/1215511/44509464-0dc1e300-a6b2-11e8-8dfe-28cbc530ebe9.jpeg&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='cianiandreadev' date='2018-08-23T16:49:44Z'>
		From these screenshots, it looks like you're comparing the results across two scenarios:

running in Turi Create on a Mac, taking a photo as input
running in CoreML (+ Vision) on an iPhone, taking an image from the camera pointed at a screen showing the photo
What happens if, on the phone, you save off the image you're evaluating, load it in Turi Create on your Mac, and then run inference from there?

&lt;denchmark-link:https://github.com/gustavla&gt;@gustavla&lt;/denchmark-link&gt;
 Do we expect our data augmentation to make the model robust to pictures of a screen showing data from the original training distribution? Did you try this use case in your work?
		</comment>
		<comment id='8' author='cianiandreadev' date='2018-08-23T17:06:42Z'>
		&lt;denchmark-link:https://github.com/nickjong&gt;@nickjong&lt;/denchmark-link&gt;
 I already tried your approach indeed and didn't work anyway.
Here an example of a picture taken from the iPhone and passed back to the TC model evaluation:
&lt;denchmark-link:https://user-images.githubusercontent.com/1215511/44540595-9f0f7480-a707-11e8-9c4e-c67c41c45c52.jpg&gt;&lt;/denchmark-link&gt;

If I scan them with CoreML: Same issue.
		</comment>
		<comment id='9' author='cianiandreadev' date='2018-08-27T08:33:34Z'>
		Any news on this issue? Some other ideas/ approaches on how I can try to fix it? I really tried everything I could and still didn't find any solutions üôÅ
		</comment>
		<comment id='10' author='cianiandreadev' date='2018-08-27T08:40:56Z'>
		I have see some of the same issues in my initial tests. I was wondering if it happens because the bounding boxes are drawn after the camera has moved? I.e. in your example &lt;denchmark-link:https://user-images.githubusercontent.com/1215511/44509464-0dc1e300-a6b2-11e8-8dfe-28cbc530ebe9.jpeg&gt;https://user-images.githubusercontent.com/1215511/44509464-0dc1e300-a6b2-11e8-8dfe-28cbc530ebe9.jpeg&lt;/denchmark-link&gt;
 the camera has been moved down a little before the bounding boxes are drawn? I don't have any good examples to show right now...
		</comment>
		<comment id='11' author='cianiandreadev' date='2018-08-27T08:50:10Z'>
		Nope. I try to keep it as steady as possible to avoid this kind of artifacts. I also tried to "manually" center the bb to the animals (by moving the phone) and in the next frame they are moved again down by the evaluation.
&lt;denchmark-link:https://github.com/sejersbol&gt;@sejersbol&lt;/denchmark-link&gt;
  Do you have a working project? Maybe some snippet I can have a look?
		</comment>
		<comment id='12' author='cianiandreadev' date='2018-08-27T09:01:16Z'>
		&lt;denchmark-link:https://github.com/cianiandreadev&gt;@cianiandreadev&lt;/denchmark-link&gt;
 I used the breakfast sample just like you - only changed the mlmodel file. My guess is that the problem must reside with the model itself.
		</comment>
		<comment id='13' author='cianiandreadev' date='2018-08-27T09:09:10Z'>
		&lt;denchmark-link:https://github.com/sejersbol&gt;@sejersbol&lt;/denchmark-link&gt;
 I have the same feeling. But since the TuriCreate version works fluently It must be the conversion between TC and CoreML.
I will try to convert again today.
		</comment>
		<comment id='14' author='cianiandreadev' date='2018-08-27T09:24:21Z'>
		&lt;denchmark-link:https://github.com/cianiandreadev&gt;@cianiandreadev&lt;/denchmark-link&gt;
 Using the sample breakfast project, and I just changed the mlmodel file I get the below screenshots from my initial test model (all about insects ;-)).
&lt;denchmark-link:https://user-images.githubusercontent.com/639304/44652112-5dbee380-a9eb-11e8-80a2-14aeb9fd3ecf.PNG&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/639304/44652133-6ca59600-a9eb-11e8-9e89-95ac65e2c197.PNG&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/639304/44652139-6fa08680-a9eb-11e8-80f4-77decee95c3b.PNG&gt;&lt;/denchmark-link&gt;

It looks like the same problem you have, don't you agree?
		</comment>
		<comment id='15' author='cianiandreadev' date='2018-08-27T11:11:33Z'>
		Absolutely. I think be both have the same issue. &lt;denchmark-link:https://github.com/znation&gt;@znation&lt;/denchmark-link&gt;
 , what is your opinion?
		</comment>
		<comment id='16' author='cianiandreadev' date='2018-08-28T18:25:34Z'>
		&lt;denchmark-link:https://github.com/cianiandreadev&gt;@cianiandreadev&lt;/denchmark-link&gt;
 I agree, it seems like the problem is in the conversion to  for these models. Based on what we are seeing, I suspect there is a bug in the conversion process. Would you be willing to share your Turi Create saved model (comes out as a directory; typically we name them .model or .tcmodel), and your .mlmodel? We can try to debug why the predictions are coming out differently between the two.
		</comment>
		<comment id='17' author='cianiandreadev' date='2018-08-29T07:04:24Z'>
		&lt;denchmark-link:https://github.com/znation&gt;@znation&lt;/denchmark-link&gt;
 Actually I should ask to my company but I think I can easy persuade the peoples-that-counts to share it privately with you. Is that ok? Can you tell me eventually an e-mail I can send the files to?
Thanks for your support.
		</comment>
		<comment id='18' author='cianiandreadev' date='2018-08-29T18:29:15Z'>
		&lt;denchmark-link:https://github.com/cianiandreadev&gt;@cianiandreadev&lt;/denchmark-link&gt;
 That would be great, thanks! My email address is in my &lt;denchmark-link:https://github.com/znation&gt;profile&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='19' author='cianiandreadev' date='2018-09-03T11:59:45Z'>
		&lt;denchmark-link:https://github.com/znation&gt;@znation&lt;/denchmark-link&gt;
 , as requested on Thursday 08-30 I sent you the e-mail with subject "Andrea Ciani - Obj detection issue 1016". Can you kindly tell me if you received it correctly?
Hope it helps. 
Thanks in advance.
		</comment>
		<comment id='20' author='cianiandreadev' date='2018-09-04T04:41:44Z'>
		We are investigating. Thanks!
		</comment>
		<comment id='21' author='cianiandreadev' date='2018-09-12T07:14:08Z'>
		I would like to chime in here. I have a turicreate model in production (via conversion to CoreML) on the App Store, and it works fine on iOS 11 / watchOS 4 and older devices.
However, the same model does not work on iOS 12 / watchOS 5 devices. My model is an LSTM that classifies user activities based on accelerometer in the Apple Watch, and when I run the app in the latest Xcode on a watchOS 5 device, the output is constantly bogus and makes no sense (my hand is still, yet it predicts I am doing an activity that requires a lot of motion).
The same exact model with the same exact build of the app works fine on my watchOS 4 device. I am certain this is a model conversion issue.
(I have also tried this with quantized version of the model that I create in High Sierra, and that model has the same issues.)
I am very concerned about this as my app's CoreML feature will basically stop working when watchOS 5 comes out and all of my users upgrade :(
		</comment>
		<comment id='22' author='cianiandreadev' date='2018-09-12T07:15:25Z'>
		&lt;denchmark-link:https://github.com/swupnil&gt;@swupnil&lt;/denchmark-link&gt;
 I'm opening a separate issue for this. Can you share your model there?
		</comment>
		<comment id='23' author='cianiandreadev' date='2018-09-29T18:11:37Z'>
		Hi &lt;denchmark-link:https://github.com/shantanuchhabra&gt;@shantanuchhabra&lt;/denchmark-link&gt;
, apologies if this is the incorrect place to post this, but I think I am seeing a similar bug.  I am seeing on most models used on iOS 12 that it is cropping part of the image unnecessarily.
In the debugger I captured the input image and output of that image (on our style transfer model).
&lt;denchmark-link:https://user-images.githubusercontent.com/1422280/46249094-57a89200-c3f1-11e8-80fc-65190a4187a1.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/1422280/46249095-57a89200-c3f1-11e8-8e3e-569f5802640e.png&gt;&lt;/denchmark-link&gt;

You can see that image is cut off at the top and bottom and then the output is stretched.
I realize this may not be the correct post, but it seems to be a similar problem.  We do not see this issue when directly feeding the image into the Core ML model (not using the vision framework).
I also filed a bug (problem 44237403).
Thanks in advance!
		</comment>
		<comment id='24' author='cianiandreadev' date='2018-10-04T01:01:19Z'>
		&lt;denchmark-link:https://github.com/ghop02&gt;@ghop02&lt;/denchmark-link&gt;
 Thanks for bringing this to our attention! We're working on your bug report and posted an update on the thread there.
		</comment>
		<comment id='25' author='cianiandreadev' date='2018-10-04T16:03:09Z'>
		Thanks &lt;denchmark-link:https://github.com/shantanuchhabra&gt;@shantanuchhabra&lt;/denchmark-link&gt;
.  I don't actually see an update on that thread, but maybe I'm just looking at it incorrectly
		</comment>
		<comment id='26' author='cianiandreadev' date='2018-10-09T18:07:06Z'>
		I am having the same problem - bug submitted 44742649 with sample code. iOS 11.4.1 is no longer being signed, and iOS 12.0.1 still doesn't work.
		</comment>
		<comment id='27' author='cianiandreadev' date='2018-10-09T21:27:57Z'>
		I have determined that this isn't just a Vision issue. Using our model directly with CoreML produces various results depending on what hardware is used or simulator.
		</comment>
		<comment id='28' author='cianiandreadev' date='2018-10-09T23:08:03Z'>
		Interesting &lt;denchmark-link:https://github.com/kinergy&gt;@kinergy&lt;/denchmark-link&gt;
.  Using the model with CoreML directly produces the expected results.
		</comment>
		<comment id='29' author='cianiandreadev' date='2018-10-11T05:26:22Z'>
		I'm experiencing similar problems and I'm also seeing some weird behavior with confidence. When I get a hit on an object, it always has a confidence of 1.0, which seems a little questionable.
&lt;denchmark-link:https://github.com/gustavla&gt;@gustavla&lt;/denchmark-link&gt;
 I tried the &lt;denchmark-link:https://github.com/apple/turicreate/issues/1016#issuecomment-420674323&gt;snippet you added above&lt;/denchmark-link&gt;
 for trying the ML Model results in the following error:
&lt;denchmark-code&gt;{
    NSLocalizedDescription = "Failed to evaluatue model 0 in pipeline";
    NSUnderlyingError = "Error Domain=com.apple.CoreML Code=0 \"Required input feature not passed to neural network.\" UserInfo={NSLocalizedDescription=Required input feature not passed to neural network.}";
}
&lt;/denchmark-code&gt;

Any idea what the issue could be?
		</comment>
		<comment id='30' author='cianiandreadev' date='2018-10-11T11:43:10Z'>
		This is the output from the same set of 5 images from TC directly, and a via the CoreML model generated by TC.
Classes are: ['hammer', 'mallet', 'saw', 'spanner']
You can see that confidences are not the same between the same images (img1: TC: 53% "mallet", CoreML: 51% "mallet") and entirely different number of detections e.g. img2: TC detected 3 objects  ("hammer", "mallet" and "saw"), and CoreML detected 2 objects ("saw" and "mallet").
I have formatted the output as JSON for readability and easy comparison.
&lt;denchmark-h:h3&gt;Turi Create&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;[
  [],  // img0
  [
  // img1
    {
      "confidence": 0.5330119460790017,
      "type": "rectangle",
      "coordinates": {
        "y": 349.0502166810225,
        "x": 363.1568311292976,
        "width": 47.589577894944455,
        "height": 104.67505821814905
      },
      "label": "mallet"
    }
  ],
  [
  // img2
    {
      "confidence": 0.7227546304968571,
      "type": "rectangle",
      "coordinates": {
        "y": 311.1036779317327,
        "x": 391.10805598230706,
        "width": 95.9827599158653,
        "height": 35.754324839665344
      },
      "label": "mallet"
    },
    {
      "confidence": 0.6302023798550092,
      "type": "rectangle",
      "coordinates": {
        "y": 353.9289310963444,
        "x": 214.99848938061638,
        "width": 69.83673095703125,
        "height": 140.37098517784705
      },
      "label": "saw"
    },
    {
      "confidence": 0.2668375713857364,
      "type": "rectangle",
      "coordinates": {
        "y": 206.30439410686503,
        "x": 230.99165211068106,
        "width": 46.683355478140044,
        "height": 106.4533204298753
      },
      "label": "hammer"
    }
  ],
  [
  // img3
    {
      "confidence": 0.9285530193537053,
      "type": "rectangle",
      "coordinates": {
        "y": 252.4806663563403,
        "x": 212.0289733341525,
        "width": 51.05623685396637,
        "height": 152.18538137582632
      },
      "label": "saw"
    }
  ],
  [
  // img4
    {
      "confidence": 0.9527265307086529,
      "type": "rectangle",
      "coordinates": {
        "y": 300.42011761439545,
        "x": 270.080183376321,
        "width": 124.64950561523443,
        "height": 112.85323509803186
      },
      "label": "saw"
    }
  ]
]
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;CoreML&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;
[
  // img0
  {
    "confidence": "array([], shape=(0, 4), dtype=float64)",
    "coordinates": "array([], shape=(0, 4), dtype=float64)"
  },
  // img1
  {
    "confidence": "array([[0.00092554, 0.51025391, 0.00391006, 0.00431824]])",
    "coordinates": "array([[0.56640625, 0.72851562, 0.07305908, 0.21313477]])"
  },
  // img2
  {
    "confidence": "array([[2.56061554e-04, 3.97443771e-04, 7.51464844e-01, 6.42299652e-04], [1.80339813e-03, 7.26562500e-01, 4.86373901e-03, 1.27887726e-03]])",
    "coordinates": "array([[0.33447266, 0.73681641, 0.10754395, 0.29394531], [0.61083984, 0.64697266, 0.14831543, 0.07611084]])"
  },
  // img3
  {
    "confidence": "array([[0.00162029, 0.00136662, 0.92138672, 0.00336266]])",
    "coordinates": "array([[0.32983398, 0.52539062, 0.07958984, 0.31738281]])"
  },
  // img4
  {
    "confidence": "array([[6.56843185e-05, 2.13980675e-05, 9.49218750e-01, 2.38060951e-04]])",
    "coordinates": "array([[0.42285156, 0.625, 0.1953125 , 0.23803711]])"
  }
]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='31' author='cianiandreadev' date='2018-10-11T16:11:28Z'>
		&lt;denchmark-link:https://github.com/andrewgleave&gt;@andrewgleave&lt;/denchmark-link&gt;
 how were you able to use your CoreML model in Python? I tried using &lt;denchmark-link:https://github.com/gustavla&gt;@gustavla&lt;/denchmark-link&gt;
's suggestions but I got an error. More info in my &lt;denchmark-link:https://github.com/apple/turicreate/issues/1016#issuecomment-428825169&gt;comment above&lt;/denchmark-link&gt;
.
The only thing I'm concerned about is that I built my model with images that have transparency and had to use a &lt;denchmark-link:https://github.com/apple/turicreate/issues/114#issuecomment-352497812&gt;workaround provided by @gustavla&lt;/denchmark-link&gt;
 to get it to work. Now I'm concerned my model is looking for a different field/value, resulting in that crash. This is my first stab at anything ML, so I'm definitely a little out of my comfort zone here!
		</comment>
		<comment id='32' author='cianiandreadev' date='2018-10-11T21:08:05Z'>
		As above. The code is very similar:
&lt;denchmark-code&gt;import glob

from PIL import Image
import coremltools

mlmodel = coremltools.models.MLModel('XYZ.mlmodel')

for path in glob.glob('../shots/*.JPG'):
    img = Image.open(path)
    # resize according to network input image shpae
    resized = img.resize((416, 416), Image.LINEAR)
    print(str(mlmodel.predict({'image': resized})) + '\n')
&lt;/denchmark-code&gt;

Note: the choice of resampling filter affects predictions. I don't think it's enough to account for what I'm seeing above. Maybe I'm wrong, but it no alternative filter gave me comparable results to TC.
		</comment>
		<comment id='33' author='cianiandreadev' date='2018-10-13T13:34:39Z'>
		We‚Äôre seeing this problem too. Core ML model performance improves when you turn the phone sideways to test object detection model. Any known fixed yet?
		</comment>
		<comment id='34' author='cianiandreadev' date='2018-10-15T00:25:28Z'>
		I am experiencing the same problem. Looks like in the iPhone the image is cropped differently. I am using my model for image classification. Using the image picker, it looks like the frame is shifted upwards. Lower part of the image is cropped and there is more of the upper part.
		</comment>
		<comment id='35' author='cianiandreadev' date='2018-10-15T00:34:31Z'>
		If I give a perfectly square image it crops the lower bottom!
		</comment>
		<comment id='36' author='cianiandreadev' date='2018-10-18T18:45:42Z'>
		I have been struggling with this same issue as well when providing my own CoreML model created with TuriCreate 5.1 to the BreakfastFinder. The bounding boxes that are drawn within the app are very offset.
As another user mentioned in this issue, I too am getting ridiculously high confidence scores with the new default Turi CoreML export format -- with all recongized objects coming back at 0.999+ confidence scores, which makes filtering out false positives very difficult. When I export in the older CoreML format and use a different demo app, the confidence scores are at reasonable and expected levels, false positives are easy to filter out, and the bounding boxes work quite a bit better.
If I hold the phone in portrait mode and place the object to be detected at the top of the view, the bounding box is close-ish to the object, but several pixels lower than it should be. The problem gets worse as I move the object lower towards the bottom of the phone. The lower the object is inside the camera's view, the worse the bounding box offset placement is. Holding the phone in landscape mode doesn't really seem to make much of a difference and the offset still varies depending on where the object is detected on screen.
I can't figure out why this offset is so severe or why the confidence scores, even for false positives on just about any 'detected' object is so high when using the default CoreML export options in Turi 5.0+
(see attached GIF)
&lt;denchmark-link:https://user-images.githubusercontent.com/13277778/47176757-072fa080-d2dc-11e8-979c-ce2dfd4c1147.gif&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='37' author='cianiandreadev' date='2018-10-24T00:52:24Z'>
		I am seeing similar behaviour as &lt;denchmark-link:https://github.com/philimanjaro&gt;@philimanjaro&lt;/denchmark-link&gt;
, the bounding boxes always appear to be lower than they should be and this issue exacerbates as the object moves lower in the camera frame. I am also seeing this behaviour running the template breakfast app with the default model and my trained model on a device running 12.0.1
		</comment>
		<comment id='38' author='cianiandreadev' date='2018-11-16T00:42:10Z'>
		Really sorry it has taken so long to track this issue down. This problem can be resolved by adding
objectRecognition.imageCropAndScaleOption = .scaleFill
on your VNCoreMLRequest object. I think different crop-and-scale options should still work reasonably (even though we recommend .scaleFill, since that is how the model is trained), so we are definitely following up on that as well.
		</comment>
		<comment id='39' author='cianiandreadev' date='2018-11-17T14:11:20Z'>
		
Really sorry it has taken so long to tack this issue down. This problem can be resolved by adding
objectRecognition.imageCropAndScaleOption = .scaleFill
on your VNCoreMLRequest object. I think different crop-and-scale options should still work reasonably (even though we recommend .scaleFill, since that is how the model is trained), so we are definitely following up on that as well.

&lt;denchmark-link:https://github.com/gustavla&gt;@gustavla&lt;/denchmark-link&gt;
, that resolved the bounding box issue completely on my end! Thank you for posting that solution.
Like other users, I am also still experiencing the very high confidence scores using the model -- many times clocking in at 0.999+. Is that issue being tracked somewhere else separately? This current issue seems to be tracking the bounding box issue (now resolved!) as well as the confidence score issue when using the model in the iOS app.
Thanks again.
		</comment>
		<comment id='40' author='cianiandreadev' date='2019-05-16T11:20:33Z'>
		Unexpectedly high confidences are being tracking by: &lt;denchmark-link:https://github.com/apple/turicreate/issues/1314&gt;#1314&lt;/denchmark-link&gt;

		</comment>
		<comment id='41' author='cianiandreadev' date='2019-06-07T03:28:27Z'>
		Nice work
		</comment>
		<comment id='42' author='cianiandreadev' date='2019-06-07T03:28:52Z'>
		üéâüéâüéâ
		</comment>
		<comment id='43' author='cianiandreadev' date='2019-07-09T15:45:43Z'>
		Awesome! thank you for the fix!
		</comment>
		<comment id='44' author='cianiandreadev' date='2019-09-25T18:24:55Z'>
		
We‚Äôre seeing this problem too. Core ML model performance improves when you turn the phone sideways to test object detection model. Any known fixed yet?
I have the same problem as you. iPhone running CoreML model in portrait mode cannot detect the object while in landscape mode, it could. But when move the phone horizontally, the offset between bounding box and object become larger.
Could someone explain to me this problem, please?
@philimanjaro @gustavla @yousifKashef

&lt;denchmark-link:https://user-images.githubusercontent.com/7839927/65628707-31b1cf80-dffc-11e9-98be-ee8aa63324d0.PNG&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/7839927/65628710-324a6600-dffc-11e9-872e-5f2bdf7a590d.PNG&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/7839927/65628714-324a6600-dffc-11e9-8361-2a5fec1630bb.PNG&gt;&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>