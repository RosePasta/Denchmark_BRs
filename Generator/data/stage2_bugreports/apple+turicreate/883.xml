<bug id='883' author='FedeGens' open_date='2018-07-24T16:04:42Z' closed_time='2020-03-23T17:11:21Z'>
	<summary>Can't use eGPU in Image Classification</summary>
	<description>
Hi,
The eGPU doesn't seems to be used on my MacBook Pro using an Image Classification model.
During the training of an Object Detection model the GPU is used correctly.
	</description>
	<comments>
		<comment id='1' author='FedeGens' date='2018-07-25T00:49:30Z'>
		I believe this should work. &lt;denchmark-link:https://github.com/FedeGens&gt;@FedeGens&lt;/denchmark-link&gt;
 are you sure the GPU is not getting used at all while creating an image classifier model?
If you don't have many images you may not even notice the GPU getting used while creating an image classifier model. An image classifier gets created in three stages, see the &lt;denchmark-link:https://apple.github.io/turicreate/docs/userguide/image_classifier/how-it-works.html&gt;"tranfer learning" section of this doc&lt;/denchmark-link&gt;
. Only the second stage will use the GPU. Creating an object detection model will utilize the GPU much more.
		</comment>
		<comment id='2' author='FedeGens' date='2018-07-25T09:07:31Z'>
		Thank you for your reply.
The problem is that only the internal GPU is used for the work, when I expected to see the external GPU working on it.
&lt;denchmark-link:https://user-images.githubusercontent.com/22976202/43190649-d6cba60c-8ff9-11e8-95ad-5536bd988278.png&gt;&lt;/denchmark-link&gt;

As you can see from the image, the AMD Radeon Pro 560 (the internal one) is doing all the work, when the RX Vega 64 (the external) is sleeping.
That's the behaviour while creating an image classification model.
&lt;denchmark-link:https://user-images.githubusercontent.com/22976202/43190917-687daafa-8ffa-11e8-812d-46a2f454ccdd.png&gt;&lt;/denchmark-link&gt;

This is what happens when I try to create an object detection model.
I'd like to have the same behaviour with image classification models.
Is there a way to decide which GPU must be used?
I already tried with:
turicreate.config.set_num_gpus(-1)
but there's only 1 GPU working at time.
Thanks.
		</comment>
		<comment id='3' author='FedeGens' date='2018-08-17T21:20:02Z'>
		We've found that this issue relates to another team within Apple - we've reported it to them through our internal tracking system, and are closing the issue here. Thanks for your patience.
		</comment>
		<comment id='4' author='FedeGens' date='2018-08-20T08:08:01Z'>
		Thanks.
How can I know when this will be fixed?
		</comment>
		<comment id='5' author='FedeGens' date='2018-08-20T17:13:28Z'>
		Re-opening it. We will close this when it lands in CoreML.
		</comment>
		<comment id='6' author='FedeGens' date='2018-08-21T07:26:00Z'>
		Thanks again for your support.
		</comment>
		<comment id='7' author='FedeGens' date='2018-09-05T10:35:36Z'>
		Hi,
How did you get it to work with the internal AMD Pro 560? In my case the Turicreate select the Intel UHD graphic when I train a image classification model with 30 000 pictures. I have a MacBook Pro 15" 2018, Mojave beta 10 and Xcode-beta 6. I use Jupyter to run my python code. I have tried to use a eGPU also. Same results as you but internal Intel GPU is running. If I use PlaidML as backend to Keras or Apples Create ML it will use the internal AMD card or eGPU without any problem.
		</comment>
		<comment id='8' author='FedeGens' date='2018-09-05T14:41:16Z'>
		&lt;denchmark-link:https://github.com/aicts&gt;@aicts&lt;/denchmark-link&gt;
 Your issue seems to be a different issue from the one in this post. We seem to be picking the wrong GPU card. I've made a separate issue for this &lt;denchmark-link:https://github.com/apple/turicreate/issues/1063&gt;#1063&lt;/denchmark-link&gt;
. We should be picking the AMD Pro 560 card.
		</comment>
		<comment id='9' author='FedeGens' date='2018-12-17T23:37:28Z'>
		I am having the exact same issue as FedeGens. In image classification, Turi will prefer my 2018 13" Pro's internal Iris graphics over the RX 580 eGPU.
I'm not very well versed on GitHub Issue tracking etiquette, so I apologize if this comment is redundant. I just wanted to make it clear that the issue was not isolated.
		</comment>
		<comment id='10' author='FedeGens' date='2019-01-04T21:44:17Z'>
		Yes, for image classification specifically, Turi Create relies on the Core ML framework in macOS to perform a neural-net-based feature extraction. Core ML doesn't support eGPU in this case. We'll update/close this issue when the situation is resolved.
		</comment>
		<comment id='11' author='FedeGens' date='2019-06-21T15:24:54Z'>
		MacOS 10.15 Catalina now has API allowing us to specify the eGPU here:
&lt;denchmark-link:https://developer.apple.com/documentation/coreml/mlmodelconfiguration/3222915-preferredmetaldevice?language=objc&gt;https://developer.apple.com/documentation/coreml/mlmodelconfiguration/3222915-preferredmetaldevice?language=objc&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='FedeGens' date='2019-11-18T11:06:42Z'>
		Looking for the same function. Still cant use eGPU for image classification training.
		</comment>
		<comment id='13' author='FedeGens' date='2020-10-05T00:19:09Z'>
		Here is a demo repo to recreate the eGPU preference:  &lt;denchmark-link:https://github.com/noahgift/amd-tensorflow-osx&gt;https://github.com/noahgift/amd-tensorflow-osx&lt;/denchmark-link&gt;
.  It does appear that while yes, an eGPU can be used it is preferred.  So if you have a higher-end Mac Pro, and an eGPU also plugged in, you will need to unplug it to get it to prefer the better GPU.
		</comment>
	</comments>
</bug>