<bug id='1162' author='mark-bragg' open_date='2018-10-25T21:43:27Z' closed_time='2020-09-01T20:49:11Z'>
	<summary>Image Preprocessing for Image Similarity Model</summary>
	<description>
I created an image similarity model and used the reference data images to test it out. I tested the turicreate model, and I got back zero distances for reference data images, and the same came back when using this code with the coreml model:
image = tc.image_analysis.resize(reference_data[0]['image'], *reversed(model.input_image_shape))
image = PIL.Image.fromarray(image.pixel_data)
mlmodel.predict({'image':image})
However, when using the model in iOS as a VNCoreMLModel, no reference image test came back with a zero distance, and most of them weren't even the shortest distance, i.e. reference image 0 had a shortest distance to reference id 78.
Since the coreml model works in python, I figured it was a preprocessing issue, so I preprocessed the image myself before passing it to the CoreMLModel. Doing this gave me a consistent output of the reference ids matching the reference images for the shortest distance--yay. The distance still isn't zero, so I have attempted to do whatever I can think of to affect the image to get some difference, but I can't get it any lower than I have already.
Any help is appreciated.
	</description>
	<comments>
		<comment id='1' author='mark-bragg' date='2018-10-30T21:04:37Z'>
		&lt;denchmark-link:https://github.com/nickjong&gt;@nickjong&lt;/denchmark-link&gt;
, any update on this?
		</comment>
		<comment id='2' author='mark-bragg' date='2018-10-31T17:17:40Z'>
		Sorry, no, we haven't had time to investigate this more closely. Although I wonder if it's going to end up being the same underlying cause as &lt;denchmark-link:https://github.com/apple/turicreate/issues/1016&gt;#1016&lt;/denchmark-link&gt;
, where object detectors deploying using the Vision framework are producing unexpected results as well.
Just to confirm, your integration into iOS is along the lines of the sample code in &lt;denchmark-link:https://github.com/apple/turicreate/blob/master/userguide/image_similarity/export-coreml.md&gt;https://github.com/apple/turicreate/blob/master/userguide/image_similarity/export-coreml.md&lt;/denchmark-link&gt;
 ? When you get the correct shortest distances (although still nonzero), how are you preprocessing the image yourself?
&lt;denchmark-link:https://github.com/TobyRoseman&gt;@TobyRoseman&lt;/denchmark-link&gt;
, have you seen anything like this in your work on this toolkit?
		</comment>
		<comment id='3' author='mark-bragg' date='2018-10-31T17:41:59Z'>
		I don't think I've seen any thing like this before, but I'm having a difficult time understanding the exact problem.
&lt;denchmark-link:https://github.com/mark-bragg&gt;@mark-bragg&lt;/denchmark-link&gt;
 - Your Core ML model gives correct results on macOS but gives difference results on iOS, Is that correct?
		</comment>
		<comment id='4' author='mark-bragg' date='2018-10-31T18:05:28Z'>
		&lt;denchmark-link:https://github.com/nickjong&gt;@nickjong&lt;/denchmark-link&gt;
, here is my preprocessing code:
&lt;denchmark-code&gt;+ (CVPixelBufferRef)pixelBufferForImage:(UIImage *)image sideLength:(CGFloat)sideLength {
    UIGraphicsBeginImageContextWithOptions(CGSizeMake(sideLength, sideLength), YES, image.scale);
    [image drawInRect:CGRectMake(0, 0, sideLength, sideLength)];
    UIImage *resizedImage = UIGraphicsGetImageFromCurrentImageContext();
    UIGraphicsEndImageContext();
    
    CFStringRef keys[2] = {kCVPixelBufferCGImageCompatibilityKey, kCVPixelBufferCGBitmapContextCompatibilityKey};
    CFBooleanRef values[2] = {kCFBooleanTrue, kCFBooleanTrue};
    CFDictionaryRef attrs = CFDictionaryCreate(kCFAllocatorDefault, (const void **)keys, (const void **)values, 2, &amp;kCFTypeDictionaryKeyCallBacks, &amp;kCFTypeDictionaryValueCallBacks);
    CVPixelBufferRef buffer;
    CFDictionaryRef realAttrs;
    
    CFMutableArrayRef attrsArray = CFArrayCreateMutable(NULL, 0, NULL);
    CFArrayAppendValue(attrsArray, attrs);
    
    CVPixelBufferCreateResolvedAttributesDictionary(NULL, attrsArray, &amp;realAttrs);
    
    int status = CVPixelBufferCreate(kCFAllocatorDefault, (int)(sideLength), (int)(sideLength), kCVPixelFormatType_32ARGB, realAttrs, &amp;buffer);
    if (status != kCVReturnSuccess) {
        return nil;
    }
    
    CGColorSpaceRef colorSpace = CGColorSpaceCreateWithName(kCGColorSpaceSRGB);
    size_t numComps = CGColorSpaceGetNumberOfComponents(colorSpace);
    size_t bytesPerRow = CVPixelBufferGetBytesPerRow(buffer);
    size_t bitsPerComponent = (8 * (bytesPerRow/sideLength) - 7) / numComps;
    
    CVPixelBufferLockBaseAddress(buffer, kCVPixelBufferLock_ReadOnly);
    void *data = CVPixelBufferGetBaseAddress(buffer);
    
    CGContextRef context = CGBitmapContextCreate(data, sideLength, sideLength, bitsPerComponent, bytesPerRow, colorSpace, kCGImageAlphaNoneSkipFirst);
    
    CGContextTranslateCTM(context, 0, sideLength);
    CGContextScaleCTM(context, 1.0, -1.0);
    
    UIGraphicsPushContext(context);
    [resizedImage drawInRect:CGRectMake(0, 0, sideLength, sideLength)];
    UIGraphicsPopContext();
    CVPixelBufferUnlockBaseAddress(buffer, kCVPixelBufferLock_ReadOnly);
    return buffer;
}
&lt;/denchmark-code&gt;

and yes I followed the Turi export to CoreML instructions.
Side note: I am also using an Object Detection model for what I am developing, and the confidences come back much lower using a video feed in comparison to a single image, I followed this example in implementation: &lt;denchmark-link:https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture?language=objc&gt;https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture?language=objc&lt;/denchmark-link&gt;
.
&lt;denchmark-link:https://github.com/TobyRoseman&gt;@TobyRoseman&lt;/denchmark-link&gt;
, that is correct.
		</comment>
		<comment id='5' author='mark-bragg' date='2018-10-31T18:23:40Z'>
		Just to clarify, you're getting reasonably correct results on macOS using the preprocessing code above, but incorrect results on iOS using Vision to do the preprocessing (ala the code samples)?
		</comment>
		<comment id='6' author='mark-bragg' date='2018-10-31T18:28:01Z'>
		The Objective-C preprocessing code is used in iOS, and I use the Python code in macOS
		</comment>
		<comment id='7' author='mark-bragg' date='2018-10-31T18:43:19Z'>
		Can you try passing in an image of all zeros and see if you get the same results on both macOS and iOS? If that's the case then there is probably an issue with your preprocessing code.
		</comment>
		<comment id='8' author='mark-bragg' date='2019-01-10T16:02:42Z'>
		&lt;denchmark-link:https://github.com/mark-bragg&gt;@mark-bragg&lt;/denchmark-link&gt;
 Any update on this? Did you get a chance to try out the all zeros image on macOS and iOS?
		</comment>
		<comment id='9' author='mark-bragg' date='2019-01-14T17:53:36Z'>
		&lt;denchmark-link:https://github.com/srikris&gt;@srikris&lt;/denchmark-link&gt;
, I have not. The project I am working on has shifted a few times, but I am still using the image similarity model, so I will investigate and have an update by the end of the week.
		</comment>
	</comments>
</bug>