<bug id='741' author='nan-wang' open_date='2020-07-30T05:25:51Z' closed_time='2020-08-05T08:52:17Z'>
	<summary>add retry mechanism for port conflicts</summary>
	<description>
Describe the bug
The zmq ports conflict in some cases.
&lt;denchmark-code&gt;2020-07-29T16:56:40.0807186Z   File "/home/runner/work/jina/jina/jina/peapods/zmq.py", line 142, in init_sockets
2020-07-29T16:56:40.0807474Z     in_sock, in_addr = _init_socket(ctx, self.args.host_in, self.args.port_in, self.args.socket_in,
2020-07-29T16:56:40.0807935Z   File "/home/runner/work/jina/jina/jina/peapods/zmq.py", line 657, in _init_socket
2020-07-29T16:56:40.0808130Z     raise ex
2020-07-29T16:56:40.0808347Z   File "/home/runner/work/jina/jina/jina/peapods/zmq.py", line 654, in _init_socket
2020-07-29T16:56:40.0808832Z     sock.bind('tcp://%s:%d' % (host, port))
2020-07-29T16:56:40.0809098Z   File "zmq/backend/cython/socket.pyx", line 550, in zmq.backend.cython.socket.Socket.bind
2020-07-29T16:56:40.0809479Z   File "zmq/backend/cython/checkrc.pxd", line 26, in zmq.backend.cython.checkrc._check_rc
2020-07-29T16:56:40.0809669Z zmq.error.ZMQError: Address already in use
&lt;/denchmark-code&gt;

Describe how you solve it
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Environment
Screenshots
	</description>
	<comments>
		<comment id='1' author='nan-wang' date='2020-07-30T08:28:11Z'>
		warning, this is not as easy as "change to another port and retry". The Flow has no clue about the port-change at Pea/Pod level and has no way but to re-orchestrate the whole stack.
non-trivial
		</comment>
		<comment id='2' author='nan-wang' date='2020-07-30T15:15:11Z'>
		
warning, this is not as easy as "change to another port and retry". The Flow has no clue about the port-change at Pea/Pod level and has no way but to re-orchestrate the whole stack.
non-trivial

Exactly, we have to rethink about the flow building process.
		</comment>
		<comment id='3' author='nan-wang' date='2020-07-30T19:52:52Z'>
		refactoring Flow just for solving port conflicting problem is over-engineering. i don't see any way to get it done in 2 hours.
need to think other ways
		</comment>
		<comment id='4' author='nan-wang' date='2020-07-31T05:52:16Z'>
		To fix this problem, we need to check the port conflicts. There are two options:

check when defining before running the flow
solution: We scan all the available ports and check if the assigned ports is taken. If yes, a new port will be assigned before building the connections.

pros

No hassle to rebuild the flow


cons

the port might be still taken before running the flow




check when running the flow
solution: We check whether the assigned port is available. If not, a new port will be assigned and the flow will be re-built.

pros

no worry about the port being taken before running the flow. New port will be assigned in the case of conflicts.


cons

the information of the new ports needs to be fed back from the pea level to the flow level
involves flow re-build for every conflict





		</comment>
		<comment id='5' author='nan-wang' date='2020-07-31T05:59:14Z'>
		It is not the right solution technically, but what if we try to add retry mechanism to the tests directly? It may do the jo After all is more a problem of too much concurrent tests right? It is not likely to happen in a prod env.
		</comment>
		<comment id='6' author='nan-wang' date='2020-07-31T06:42:35Z'>
		please notice that random_port already implement your 1

To fix this problem, we need to check the port conflicts. There are two options:


check when defining before running the flow
solution: We scan all the available ports and check if the assigned ports is taken. If yes, a new port will be assigned before building the connections.


pros

No hassle to rebuild the flow



cons

the port might be still taken before running the flow





check when running the flow
solution: We check whether the assigned port is available. If not, a new port will be assigned and the flow will be re-built.


pros

no worry about the port being taken before running the flow. New port will be assigned in the case of conflicts.



cons


the information of the new ports needs to be fed back from the pea level to the flow level


involves flow re-build for every conflict







		</comment>
		<comment id='7' author='nan-wang' date='2020-07-31T08:20:16Z'>
		I agree with this. The ideal case is to have each test case inside a docker container, to avoid collided ports.
Note, we should not use parallelism when doing unit test. The original motivation of moving away from pytest is we want to use the process as a context manager so that when the process ends, it releases all resources. pytest runs all test cases in one process, and this is what we try to avoid.

It is not the right solution technically, but what if we try to add retry mechanism to the tests directly? It may do the jo After all is more a problem of too much concurrent tests right? It is not likely to happen in a prod env.

		</comment>
		<comment id='8' author='nan-wang' date='2020-07-31T08:25:24Z'>
		We have -n 1 option right? I am not familiar with the insights of pytest and unittest. But are we sure that github spins totally isolated environments for every run? Or could it be that the tests for python 3.7 and 3.8 collide?
		</comment>
		<comment id='9' author='nan-wang' date='2020-07-31T11:16:36Z'>
		I'm a pytest newbie, but quick Stackoverflowing shows that it can run each test in a separate process (need to install plugin), see &lt;denchmark-link:https://stackoverflow.com/questions/48234032/run-py-test-test-in-different-process&gt;https://stackoverflow.com/questions/48234032/run-py-test-test-in-different-process&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='nan-wang' date='2020-07-31T11:25:03Z'>
		
I'm a pytest newbie, but quick Stackoverflowing shows that it can run each test in a separate process (need to install plugin), see https://stackoverflow.com/questions/48234032/run-py-test-test-in-different-process

We are using that one actually.
		</comment>
		<comment id='11' author='nan-wang' date='2020-07-31T11:40:56Z'>
		Yeah, should have checked CI workflow. But what's interesting, you have -n 1 option and in this page flag --forked is used instead. Maybe it's worth to try it? ( as I see it's now renamed to --boxed)
		</comment>
		<comment id='12' author='nan-wang' date='2020-07-31T11:43:06Z'>
		I do not know if the problem may come from the 2 runs of 'py37' and py38 working in the same instance. Maybe github actions assigns one container per flow in a matrix?
		</comment>
		<comment id='13' author='nan-wang' date='2020-08-01T13:23:39Z'>
		&lt;denchmark-link:https://docs.github.com/en/actions/reference/virtual-environments-for-github-hosted-runners&gt;https://docs.github.com/en/actions/reference/virtual-environments-for-github-hosted-runners&lt;/denchmark-link&gt;


Each job in a workflow executes in a fresh instance of the virtual machine. All steps in the job execute in the same instance of the virtual machine, allowing the actions in that job to share information using the filesystem.

&lt;denchmark-link:https://github.com/JoanFM&gt;@JoanFM&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='nan-wang' date='2020-08-01T15:44:20Z'>
		
https://docs.github.com/en/actions/reference/virtual-environments-for-github-hosted-runners

Each job in a workflow executes in a fresh instance of the virtual machine. All steps in the job execute in the same instance of the virtual machine, allowing the actions in that job to share information using the filesystem.

@JoanFM

So unittest for py37 and py38 are steps of the unittest job that are run in the same instance in parallel right?
		</comment>
		<comment id='15' author='nan-wang' date='2020-08-02T06:37:33Z'>
		

https://docs.github.com/en/actions/reference/virtual-environments-for-github-hosted-runners

Each job in a workflow executes in a fresh instance of the virtual machine. All steps in the job execute in the same instance of the virtual machine, allowing the actions in that job to share information using the filesystem.

@JoanFM

So unittest for py37 and py38 are steps of the unittest job that are run in the same instance in parallel right?

yes, they are running in parallel.
		</comment>
		<comment id='16' author='nan-wang' date='2020-08-02T06:42:10Z'>
		hi, there. After tracking the record of ports, I think I've found the reason for the port collision, although I couldn't fully fixed in a single PR. To put it simple, the port assigned by socket in the random_port() is possible been taken by other usages before zmq initialization. This is more likely to happen on a machine that open and close port frequently.
Check more details at the PR,
&lt;denchmark-link:https://github.com/jina-ai/jina/pull/750#issuecomment-667625165&gt;#750 (comment)&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>