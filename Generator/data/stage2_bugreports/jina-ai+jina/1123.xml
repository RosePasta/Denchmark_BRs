<bug id='1123' author='nan-wang' open_date='2020-10-22T04:25:22Z' closed_time='2020-10-22T13:54:53Z'>
	<summary>Bug in RecallEvaluator</summary>
	<description>
Describe the bug
Given the relevant docs, [0, 1, 2, 3, 4], and the retrieved docs, [1, 0, 20, 30, 40], the recall@k is as below




expected recall@k
current recall@k




1
0.2
1


2
0.4
1


3
0.4
0.667


4
0.4
0.5


5
0.4
0.4



The Recall@k should be the number of relevant results in the top k retrieved results divided by the number of all the relevant results.

... if the (k + 1)th document retrieved is nonrelevant then recall is the same as for the top k documents ...
from Introduction to Information Retrieval

Describe how you solve it
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Environment
Screenshots
	</description>
	<comments>
		<comment id='1' author='nan-wang' date='2020-10-22T05:12:45Z'>
		Great catch! How did I miss it!. We need to change the RankingEvaluationDriver to pass tuples (id, boolean) and consider the boolean as a flag of being relevant. I think is the same for PrecisionEvaluator.
		</comment>
	</comments>
</bug>