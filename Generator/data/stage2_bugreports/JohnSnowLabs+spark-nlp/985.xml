<bug id='985' author='miloradtrninic' open_date='2020-07-30T09:51:09Z' closed_time='2020-08-06T08:47:39Z'>
	<summary>Can not find the resource to download</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

I have cloned spark nlp starter project and I am trying to run it in the docker environment.
Using Spark 2.4.0
When I do spark submit like this
docker exec spark-master /spark/bin/spark-submit  --executor-memory 1g --driver-memory 1g --class "Main" /deploy/nlptest.jar  i am getting the error java.lang.IllegalArgumentException: requirement failed: Can not find the resource to download please check the name!
Full log:
WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.
2020-07-30 09:29:26 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-07-30 09:29:26 INFO  SparkContext:54 - Running Spark version 2.4.0
2020-07-30 09:29:26 INFO  SparkContext:54 - Submitted application: spark-nlp-starter
2020-07-30 09:29:26 INFO  SecurityManager:54 - Changing view acls to: root
2020-07-30 09:29:26 INFO  SecurityManager:54 - Changing modify acls to: root
2020-07-30 09:29:26 INFO  SecurityManager:54 - Changing view acls groups to:
2020-07-30 09:29:26 INFO  SecurityManager:54 - Changing modify acls groups to:
2020-07-30 09:29:26 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2020-07-30 09:29:27 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 33535.
2020-07-30 09:29:27 INFO  SparkEnv:54 - Registering MapOutputTracker
2020-07-30 09:29:27 INFO  SparkEnv:54 - Registering BlockManagerMaster
2020-07-30 09:29:27 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-07-30 09:29:27 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2020-07-30 09:29:27 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-1623852a-db18-4c99-8af2-6c0d9081c30f
2020-07-30 09:29:27 INFO  MemoryStore:54 - MemoryStore started with capacity 366.3 MB
2020-07-30 09:29:27 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2020-07-30 09:29:27 INFO  log:192 - Logging initialized @2036ms
2020-07-30 09:29:27 INFO  Server:351 - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2020-07-30 09:29:27 INFO  Server:419 - Started @2121ms
2020-07-30 09:29:27 INFO  AbstractConnector:278 - Started ServerConnector@ecf9fb3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-07-30 09:29:27 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1af7f54a{/jobs,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@751e664e{/jobs/json,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@160c3ec1{/jobs/job,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4d0402b{/jobs/job/json,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2fa7ae9{/stages,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7577b641{/stages/json,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3704122f{/stages/stage,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@28a2a3e7{/stages/stage/json,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3f2049b6{/stages/pool,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@10b3df93{/stages/pool/json,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@ea27e34{/storage,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@33a2499c{/storage/json,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@e72dba7{/storage/rdd,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@33c2bd{/storage/rdd/json,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1dfd5f51{/environment,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3c321bdb{/environment/json,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@24855019{/executors,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3abd581e{/executors/json,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4d4d8fcf{/executors/threadDump,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@610db97e{/executors/threadDump/json,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6f0628de{/static,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@42deb43a{/,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1deb2c43{/api,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6f6a7463{/jobs/job/kill,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1bdaa23d{/stages/stage/kill,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
2020-07-30 09:29:27 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at &lt;denchmark-link:http://d3cf4111ba05:4040&gt;http://d3cf4111ba05:4040&lt;/denchmark-link&gt;

2020-07-30 09:29:27 INFO  SparkContext:54 - Added JAR file:/deploy/nlptest.jar at spark://d3cf4111ba05:33535/jars/nlptest.jar with timestamp 1596101367589
2020-07-30 09:29:27 INFO  Executor:54 - Starting executor ID driver on host localhost
2020-07-30 09:29:27 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36075.
2020-07-30 09:29:27 INFO  NettyBlockTransferService:54 - Server created on d3cf4111ba05:36075
2020-07-30 09:29:27 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-07-30 09:29:27 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, d3cf4111ba05, 36075, None)
2020-07-30 09:29:27 INFO  BlockManagerMasterEndpoint:54 - Registering block manager d3cf4111ba05:36075 with 366.3 MB RAM, BlockManagerId(driver, d3cf4111ba05, 36075, None)
2020-07-30 09:29:27 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, d3cf4111ba05, 36075, None)
2020-07-30 09:29:27 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, d3cf4111ba05, 36075, None)
2020-07-30 09:29:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2eb79cbe{/metrics/json,null,AVAILABLE,&lt;denchmark-link:https://github.com/spark&gt;@spark&lt;/denchmark-link&gt;
}
glove_100d download started this may take some time.
2020-07-30 09:29:30 WARN  ApacheUtils:170 - NoSuchMethodError was thrown when disabling normalizeUri. This indicates you are using an old version (&lt; 4.5.8) of Apache http client. It is recommended to use http client version &gt;= 4.5.9 to avoid the breaking change introduced in apache client 4.5.7 and the latency in exception handling. See &lt;denchmark-link:https://github.com/aws/aws-sdk-java/issues/1919&gt;aws/aws-sdk-java#1919&lt;/denchmark-link&gt;
 for more information
2020-07-30 09:29:30 WARN  ApacheUtils:170 - NoSuchMethodError was thrown when disabling normalizeUri. This indicates you are using an old version (&lt; 4.5.8) of Apache http client. It is recommended to use http client version &gt;= 4.5.9 to avoid the breaking change introduced in apache client 4.5.7 and the latency in exception handling. See &lt;denchmark-link:https://github.com/aws/aws-sdk-java/issues/1919&gt;aws/aws-sdk-java#1919&lt;/denchmark-link&gt;
 for more information
2020-07-30 09:29:30 WARN  ApacheUtils:170 - NoSuchMethodError was thrown when disabling normalizeUri. This indicates you are using an old version (&lt; 4.5.8) of Apache http client. It is recommended to use http client version &gt;= 4.5.9 to avoid the breaking change introduced in apache client 4.5.7 and the latency in exception handling. See &lt;denchmark-link:https://github.com/aws/aws-sdk-java/issues/1919&gt;aws/aws-sdk-java#1919&lt;/denchmark-link&gt;
 for more information
2020-07-30 09:29:30 WARN  ApacheUtils:170 - NoSuchMethodError was thrown when disabling normalizeUri. This indicates you are using an old version (&lt; 4.5.8) of Apache http client. It is recommended to use http client version &gt;= 4.5.9 to avoid the breaking change introduced in apache client 4.5.7 and the latency in exception handling. See &lt;denchmark-link:https://github.com/aws/aws-sdk-java/issues/1919&gt;aws/aws-sdk-java#1919&lt;/denchmark-link&gt;
 for more information
Exception in thread "main" java.lang.IllegalArgumentException: requirement failed: Can not find the resource to download please check the name!
at scala.Predef$.require(Predef.scala:224)
at com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.downloadResource(ResourceDownloader.scala:317)
at com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.downloadModel(ResourceDownloader.scala:356)
at com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.downloadModel(ResourceDownloader.scala:351)
at com.johnsnowlabs.nlp.HasPretrained$class.pretrained(HasPretrained.scala:27)
at com.johnsnowlabs.nlp.annotator.package$WordEmbeddingsModel$.com$johnsnowlabs$nlp$embeddings$ReadablePretrainedWordEmbeddings$$super$pretrained(annotator.scala:129)
at com.johnsnowlabs.nlp.embeddings.ReadablePretrainedWordEmbeddings$class.pretrained(WordEmbeddingsModel.scala:87)
at com.johnsnowlabs.nlp.annotator.package$WordEmbeddingsModel$.pretrained(annotator.scala:129)
at com.johnsnowlabs.nlp.annotator.package$WordEmbeddingsModel$.pretrained(annotator.scala:129)
at com.johnsnowlabs.nlp.HasPretrained$class.pretrained(HasPretrained.scala:34)
at com.johnsnowlabs.nlp.annotator.package$WordEmbeddingsModel$.com$johnsnowlabs$nlp$embeddings$ReadablePretrainedWordEmbeddings$$super$pretrained(annotator.scala:129)
at com.johnsnowlabs.nlp.embeddings.ReadablePretrainedWordEmbeddings$class.pretrained(WordEmbeddingsModel.scala:84)
at com.johnsnowlabs.nlp.annotator.package$WordEmbeddingsModel$.pretrained(annotator.scala:129)
at Main$.main(Main.scala:26)
at Main.main(Main.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:849)
at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)
at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
2020-07-30 09:29:31 INFO  SparkContext:54 - Invoking stop() from shutdown hook
2020-07-30 09:29:31 INFO  AbstractConnector:318 - Stopped Spark@ecf9fb3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-07-30 09:29:31 INFO  SparkUI:54 - Stopped Spark web UI at &lt;denchmark-link:http://d3cf4111ba05:4040&gt;http://d3cf4111ba05:4040&lt;/denchmark-link&gt;

2020-07-30 09:29:31 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!
2020-07-30 09:29:31 INFO  MemoryStore:54 - MemoryStore cleared
2020-07-30 09:29:31 INFO  BlockManager:54 - BlockManager stopped
2020-07-30 09:29:31 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped
2020-07-30 09:29:31 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!
2020-07-30 09:29:31 INFO  SparkContext:54 - Successfully stopped SparkContext
2020-07-30 09:29:31 INFO  ShutdownHookManager:54 - Shutdown hook called
2020-07-30 09:29:31 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-b5532f32-a195-453e-9d19-1cefc8b0e501
2020-07-30 09:29:31 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-fd79fd4f-ada9-4c48-8d1c-4aafab33d0c5
&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

Results from nlp starter
&lt;denchmark-h:h2&gt;Current Behavior&lt;/denchmark-h&gt;

Fails on downloading the pretrained model.
&lt;denchmark-h:h2&gt;Steps to Reproduce&lt;/denchmark-h&gt;


Clone started
Run in docker image bde2020/spark-master:2.4.0-hadoop3.1
Sbt assembly to packe
Submit to spark

&lt;denchmark-h:h2&gt;Context&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


Spark NLP version: 2.4.5
Apache NLP version: ??
Java version (java -version): 1.8
Setup and installation (Pypi, Conda, Maven, etc.):
Operating System and version: Linux
Link to your project (if any):

	</description>
	<comments>
		<comment id='1' author='miloradtrninic' date='2020-07-30T09:57:15Z'>
		
Could you please point to what is the starter project?
How did you package that project to get to nlptest.jar
Does your Docker have internet access? Is it behind the proxy/firewall etc. ?

		</comment>
		<comment id='2' author='miloradtrninic' date='2020-07-30T12:48:29Z'>
		
&lt;denchmark-link:https://github.com/maziyarpanahi/spark-nlp-starter&gt;https://github.com/maziyarpanahi/spark-nlp-starter&lt;/denchmark-link&gt;

How did you package that project to get to nlptest.jar?
It is just renamed spark-nlp-starter-assembly-0.1.jar
Does your Docker have internet access? Is it behind the proxy/firewall etc. ?
It has internet access.
		</comment>
		<comment id='3' author='miloradtrninic' date='2020-07-30T14:37:35Z'>
		Thank you. What is the image of your Docker/Dockerfile?
All the steps for is to build and run the Docker with the same starter jar to reproduce this issue would be helpful.
		</comment>
		<comment id='4' author='miloradtrninic' date='2020-07-30T15:10:47Z'>
		Here is the docker compose file
&lt;denchmark-code&gt;spark-master:
  image: bde2020/spark-master:2.4.0-hadoop3.1
  container_name: spark-master
  ports:
    - 8083:8080
    - 4040:4040
  environment: 
    - PYSPARK_PYTHON=python3
    - PYTHONIOENCODING=utf8
  env_file:
    - ../envs/hadoop.env
  volumes:
    - spark_master:/hadoop/dfs/data

spark-worker1:
  image: bde2020/spark-worker:2.4.0-hadoop3.1
  container_name: spark-worker1
  environment:
    - SPARK_MASTER=spark://spark-master:7077
  ports:
    - 8084:8081
  env_file:
    - ../envs/hadoop.env
  depends_on:
    - spark-master


volumes:
hadoop_namenode:
spark_master:
hadoop_datanode1:
hadoop_datanode2:
&lt;/denchmark-code&gt;

Just run the compose and do a simple spark-submit
docker exec spark-master /spark/bin/spark-submit --executor-memory 1g --driver-memory 1g --class "Main" /deploy/nlptest.jar
Edit:
Also delete line .enableHiveSupport()
		</comment>
		<comment id='5' author='miloradtrninic' date='2020-08-01T09:04:13Z'>
		Here is the jar file:
&lt;denchmark-link:https://www.dropbox.com/s/vfh11o3mxxkred2/spark-nlp-starter-assembly-0.1.jar?dl=0&gt;https://www.dropbox.com/s/vfh11o3mxxkred2/spark-nlp-starter-assembly-0.1.jar?dl=0&lt;/denchmark-link&gt;

Please give me at least some feedback, I am stuck here. And when I try to use offline models I get core dumped because of the tensorflow lib.
		</comment>
		<comment id='6' author='miloradtrninic' date='2020-08-03T16:02:59Z'>
		&lt;denchmark-link:https://github.com/miloradtrninic&gt;@miloradtrninic&lt;/denchmark-link&gt;

We reproduce this issue locally and now investigating as to why this happens.
		</comment>
		<comment id='7' author='miloradtrninic' date='2020-08-05T20:55:10Z'>
		&lt;denchmark-link:https://github.com/miloradtrninic&gt;@miloradtrninic&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/maziyarpanahi&gt;@maziyarpanahi&lt;/denchmark-link&gt;
 ok this is a strange error!! the assembly(fat) jar throws the error but if you put the package jar (sbt package) and then do spark-submit it works.. off course you now have to include spark NLP in your spark-submit by either adding the spark-nlp fat jar in jars folder of apache spark or passing param --packages JohnSnowLabs:spark-nlp:2.5.3
so, for now,  as workaround do not work with a fat jar or exclude spark-nlp from assembly jar and provide spark-nlp by appending --packages JohnSnowLabs:spark-nlp:2.5.3 in your spark-submit
spark-submit --executor-memory 1g --driver-memory 1g --packas JohnSnowLabs:spark-nlp:2.5.3 --class "Main" /deploy/nlptest.jar
meanwhile am checking what's the problem with fat assembly jar!!
		</comment>
		<comment id='8' author='miloradtrninic' date='2020-08-05T20:59:10Z'>
		Hi &lt;denchmark-link:https://github.com/rohit-nlp&gt;@rohit-nlp&lt;/denchmark-link&gt;

Yes, I observed the same thing. The same thing is happening to our customized NativeLibrary file we put in org/tensorflow/NativeLibrary in the root of our src. It seems the merge/conflict strategies in the first build.sbt are correct but the second time they are being build it takes the other ones.
		</comment>
		<comment id='9' author='miloradtrninic' date='2020-08-06T08:47:38Z'>
		I confirmed that the issue has been found and will be fixed in the next release. Until then, the workaround is to add provided for spark-nlp dependency and use --packages in the spark-submit.
		</comment>
	</comments>
</bug>