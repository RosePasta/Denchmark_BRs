<bug id='57' author='clayms' open_date='2017-12-10T20:48:20Z' closed_time='2018-01-31T17:00:29Z'>
	<summary>EntityExtractor().setRequireSentences(True) throws error.</summary>
	<description>
EntityExtractor().setRequireSentences(True)
throws this error:
&lt;denchmark-code&gt;Py4JJavaError: An error occurred while calling o461.getParam.
: java.util.NoSuchElementException: Param requireSentences does not exist.
&lt;/denchmark-code&gt;

From the EntityExtractor.scala file, I found the sentences parameter defined on lines 99 to 105, but when I changed lines 191 and 192 in annotator.py from:
&lt;denchmark-code&gt;def setRequireSentences(self, value):
    return self._set(requireSentences=value)
&lt;/denchmark-code&gt;

to:
&lt;denchmark-code&gt;def setRequireSentences(self, value):
    return self._set(sentences=value)
&lt;/denchmark-code&gt;

The error was basically the same:
&lt;denchmark-code&gt;Py4JJavaError: An error occurred while calling o474.getParam.
: java.util.NoSuchElementException: Param sentences does not exist.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='clayms' date='2017-12-14T08:35:43Z'>
		It seems that EntityExtractor works totally incorrect. Will rewrite it in a day.
		</comment>
		<comment id='2' author='clayms' date='2017-12-14T16:27:49Z'>
		&lt;denchmark-link:https://github.com/clayms&gt;@clayms&lt;/denchmark-link&gt;
  Thank you for the report.
I've recreated EntityExtractor in branch &lt;denchmark-link:https://github.com/JohnSnowLabs/spark-nlp/tree/rewrite_entity_extractor&gt;https://github.com/JohnSnowLabs/spark-nlp/tree/rewrite_entity_extractor&lt;/denchmark-link&gt;

Here is an example notebook how to use it &lt;denchmark-link:https://github.com/JohnSnowLabs/spark-nlp/blob/rewrite_entity_extractor/python/example/entities-extractor/extractor.ipynb&gt;https://github.com/JohnSnowLabs/spark-nlp/blob/rewrite_entity_extractor/python/example/entities-extractor/extractor.ipynb&lt;/denchmark-link&gt;

Will release it soon
		</comment>
		<comment id='3' author='clayms' date='2018-01-29T18:56:39Z'>
		I can't have the EntityExtractor() match with any of my entities even though my documents contain those. Here's a simple example I've built in Databricks, as you can see  column is an empty list
&lt;denchmark-link:https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3306898732834076/1537424650694939/3546260386063631/latest.html&gt;https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3306898732834076/1537424650694939/3546260386063631/latest.html&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='clayms' date='2018-01-31T14:13:46Z'>
		&lt;denchmark-link:https://github.com/sofianeh&gt;@sofianeh&lt;/denchmark-link&gt;
 It seems that file /dbfs/testfile.txt is local file not an hdfs file.
Can you try file:///dbfs/testfile.txt instead?
		</comment>
		<comment id='5' author='clayms' date='2018-01-31T16:00:06Z'>
		Already tried with both file:///dbfs/testfile.txt and file:/dbfs/testfile.txt. They end up with this error:
Caused by: java.io.FileNotFoundException: file:/dbfs/testfile.txt (No such file or directory)
		</comment>
		<comment id='6' author='clayms' date='2018-01-31T16:04:22Z'>
		&lt;denchmark-link:https://github.com/sofianeh&gt;@sofianeh&lt;/denchmark-link&gt;
 there is a param called . By default, this param is set to "TXT" which means it only reads from local files without any prefix (we could add parsing of the prefix). To use with prefix and/or to read from hdfs with prefix, you should set this param to "TXTDS"
This will use spark to read such file, and spark allows file:// or hdfs:// prefix. Either that or you remove the file:// prefix (this will usually be faster than TXTDS if the amount of data input is small)
use setEntitiesFormat("TXTDS")
Let us know how it goes! Thanks
		</comment>
		<comment id='7' author='clayms' date='2018-01-31T16:11:27Z'>
		Yes, using TXTDS prevents the error, but I still end up with empty entities. Is there something wrong in my example (content of testfile.txt, my documents, my Pipeline, ...)?
Also, the &lt;denchmark-link:http://nlp.johnsnowlabs.com/components.html&gt;documentation&lt;/denchmark-link&gt;
 mentions a  parameter but it seems no longer working.
		</comment>
		<comment id='8' author='clayms' date='2018-01-31T16:25:56Z'>
		&lt;denchmark-link:https://github.com/sofianeh&gt;@sofianeh&lt;/denchmark-link&gt;
 what is the content of testfile.txt?? and thanks, setMaxLen doesn't exist anymore.
I have just tried this and worked for me (this is one of the unit tests we have there, I have changed it to TXTDS though)
&lt;denchmark-code&gt;    val entityExtractor = new EntityExtractor()
      .setInputCols("sentence", "normalized")
      .setInsideSentences(false)
      .setEntitiesFormat(TXTDS)
      .setEntitiesPath("./src/test/resources/entity-extractor/test-phrases.txt")
      .setOutputCol("entity")
&lt;/denchmark-code&gt;

en this example test-phrases looks like this:
&lt;denchmark-code&gt;dolore magna aliqua
Lorem ipsum dolor sit
laborum
anim id est laborum
&lt;/denchmark-code&gt;

so it basically searches this lines in the text
		</comment>
		<comment id='9' author='clayms' date='2018-01-31T16:27:46Z'>
		testfile.txt is created on the fly within the example notebook I've provided above
		</comment>
		<comment id='10' author='clayms' date='2018-01-31T16:33:32Z'>
		Right, sorry about that. Let me check i'll try the same thing
		</comment>
		<comment id='11' author='clayms' date='2018-01-31T16:45:11Z'>
		&lt;denchmark-link:https://github.com/sofianeh&gt;@sofianeh&lt;/denchmark-link&gt;
 wow. We found the bug. Actually it is not a bug its a bad decision. We are normalizing the input file you provide (testfile.txt) and thus, it lower cases all its content. Hence it is not matching 'Hello World' but it would match 'hello world'
We'll fix this quick. Not normalize the input unless the user uses RecursivePipelines (a new Pipeline object that allows transforming the input with the same Pipeline the user is using)
For now, your pipeline will work if you include a normalizer
&lt;denchmark-code&gt;normalizer = Normalizer()\
  .setInputCols(["token"])\
  .setOutputCol("normal")
&lt;/denchmark-code&gt;

thank you
		</comment>
		<comment id='12' author='clayms' date='2018-01-31T16:50:31Z'>
		Thanks @saifjsl. I will use the workaround you've suggested for now. By the way, setInsideSentences() param is not documented anywhere
		</comment>
		<comment id='13' author='clayms' date='2018-01-31T16:53:45Z'>
		Will update the website asap as well. setInsideSentences decides whether you want to respect or not the sentence bounds. Is set to True by default.
For example, if you search for a piece of text 'This is a sentence. This is another', if InsideSentences is True, this will never match since the target belongs to different sentences. Most of the times it is not the case.
Feel free to share feedback anytime
		</comment>
		<comment id='14' author='clayms' date='2018-01-31T17:33:25Z'>
		I've published my working example on &lt;denchmark-link:https://github.com/sofianeh/databricks-examples/blob/master/python/SparkNLP-entityExtractor-example.py&gt;github&lt;/denchmark-link&gt;
 in case people will need to understand the issue I was having.
		</comment>
	</comments>
</bug>