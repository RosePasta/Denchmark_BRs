<bug id='811' author='maziyarpanahi' open_date='2020-03-04T09:20:17Z' closed_time='2020-03-15T21:19:26Z'>
	<summary>NerDLModel StringIndexOutOfBoundsException exception with IOB/IOB2 is incorrect</summary>
	<description>
When the used CoNLL file has wrong IOB/IOB2 labels the NerDLModel fails with StringIndexOutOfBoundsException exception.
This code may not fail in Python, it only fails in Scala.
import com.johnsnowlabs.nlp.training._
import com.johnsnowlabs.nlp.annotator._
import com.johnsnowlabs.nlp.base._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import org.apache.spark.ml.Pipeline
import com.johnsnowlabs.nlp.training._
import com.johnsnowlabs.nlp.util.io.ResourceHelper
val ss=SparkSession.builder().appName("ner").master("local[4]").getOrCreate()
val training_data1=CoNLL(explodeSentences = false).readDataset( ss,"/home/vakilsearch/Downloads/final_conll.txt")
val emed = new WordEmbeddings().setInputCols("document","token").setOutputCol("embeddings").setStoragePath("/home/vakilsearch/Downloads/embeddings_ner_dl_100.bin","BINARY").setDimension(100).setStorageRef("sample-conll")
val ner = new NerDLApproach().setInputCols("sentence","token","embeddings").setOutputCol("ner").setGraphFolder("/home/vakilsearch/Videos/graph folder").setMaxEpochs(1).setRandomSeed(0).setPo(0.03f).setLr(0.2f).setDropout(0.5f).setLabelColumn("label").setBatchSize(9)
val pipe= new Pipeline().setStages(Array(emed,ner))
val trained= pipe.fit(training_data1)
val document = new DocumentAssembler().setInputCol("text").setOutputCol("document")
val sentenceDetector = new SentenceDetector().setInputCols("document").setOutputCol("sentence")
val token = new Tokenizer().setInputCols("sentence").setOutputCol("token")
val ner_con = new NerConverter().setInputCols("sentence", "token", "ner").setOutputCol("ner_chunk")
val pred_pipe=new Pipeline().setStages(Array(document,sentenceDetector,token, emed,trained,ner_con))
val pred_data=ss.createDataFrame(Seq((1, "CASE NO:1988"))).toDF( "_id", "text").select("text")
val pred_model=pred_pipe.fit(pred_data)
val finall=pred_model.transform(pred_data)
finall.select("ner_chunk", "ner.result").show(truncate=false)
Sample data:
. . O 
This DT B-NP 0
appeal NN I-NP 0
is VBZ B-VP 0
directed VBN I-VP 0
against IN B-PP 0
the DT B-NP 0
final JJ I-NP 0
judgment NN I-NP 0
and CC O 0
order NN B-NP 0
dated VBD B-VP 0
21.12.2007 CD B-NP 0
passed VBN B-VP 0
by IN B-PP 0
the DT B-NP 0
High NNP I-NP I-COURT
Court NNP I-NP I-COURT
of IN B-PP B-COURT
Punjab NNP B-NP I-LOC
&amp; CC I-NP 0
Haryana NNP I-NP I-LOC
at IN B-PP 0
Chandigarh NNP B-NP I-LOC
in IN B-PP 0
RSA NNP B-NP I-CASE_NUMBER
No.1537 NNP I-NP I-CASE_NUMBER
of IN B-PP 0
1993 CD B-NP 0
whereby IN B-PP 0
the DT B-NP 0
Single NNP I-NP 0
Judge NNP I-NP 0
of IN B-PP 0
the DT B-NP 0
High NNP I-NP B-COURT
Court NNP I-NP I-COURT
dismissed VBD B-VP 0
the DT B-NP 0
regular JJ I-NP 0
second JJ I-NP 0
appeal NN I-NP 0
filed VBN B-VP 0
by IN B-PP 0
the DT B-NP 0
appellants NNS I-NP 0
herein NN I-NP 0
and CC O 0
upheld VBD B-VP 0
the DT B-NP 0
judgment/decree NN I-NP 0
1dated VBD B-VP I-CASE_NUMBER
15.04.1993 CD B-NP I-CASE_NUMBER
of IN B-PP 0
the DT B-NP 0
First NNP I-NP 0
Appellate NNP I-NP 0
Court NNP I-NP 0
in IN B-PP 0
C.A NNP B-NP B-CASE_NUMBER
. . O 0
Error:
org.apache.spark.SparkException: Failed to execute user defined function($anonfun$dfAnnotate$1: (array&lt;array&lt;struct&lt;annotatorType:string,begin:int,end:int,result:string,metadata:map&lt;string,string&gt;,embeddings:array&lt;float&gt;&gt;&gt;&gt;) =&gt; array&lt;struct&lt;annotatorType:string,begin:int,end:int,result:string,metadata:map&lt;string,string&gt;,embeddings:array&lt;float&gt;&gt;&gt;)
 at org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1066)
 at org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:152)
 at org.apache.spark.sql.catalyst.expressions.InterpretedProjection.apply(Projection.scala:50)
 at org.apache.spark.sql.catalyst.expressions.InterpretedProjection.apply(Projection.scala:32)
 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
 at scala.collection.immutable.List.foreach(List.scala:381)
 at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)
 at scala.collection.immutable.List.map(List.scala:285)
 at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$apply$24.applyOrElse(Optimizer.scala:1364)
 at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$apply$24.applyOrElse(Optimizer.scala:1359)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:256)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:256)
 at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
 at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:255)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:261)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:261)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)
 at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:261)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:261)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:261)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)
 at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:261)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:261)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:261)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)
 at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:261)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:261)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:261)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)
 at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:261)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
 at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:245)
 at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.apply(Optimizer.scala:1359)
 at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.apply(Optimizer.scala:1358)
 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)
 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)
 at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)
 at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)
 at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)
 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)
 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)
 at scala.collection.immutable.List.foreach(List.scala:381)
 at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)
 at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:66)
 at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:66)
 at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:72)
 at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:68)
 at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:77)
 at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:77)
 at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3359)
 at org.apache.spark.sql.Dataset.head(Dataset.scala:2544)
 at org.apache.spark.sql.Dataset.take(Dataset.scala:2758)
 at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)
 at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)
 at org.apache.spark.sql.Dataset.show(Dataset.scala:747)
 at org.apache.spark.sql.Dataset.show(Dataset.scala:724)
 ... 41 elided
Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: -1
 at java.lang.String.substring(String.java:1931)
 at com.johnsnowlabs.nlp.annotators.ner.NerTagsEncoding$$anonfun$fromIOB$1.apply$mcVI$sp(NerTagsEncoding.scala:52)
 at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:166)
 at com.johnsnowlabs.nlp.annotators.ner.NerTagsEncoding$.fromIOB(NerTagsEncoding.scala:45)
 at com.johnsnowlabs.nlp.annotators.ner.NerConverter$$anonfun$2.apply(NerConverter.scala:30)
 at com.johnsnowlabs.nlp.annotators.ner.NerConverter$$anonfun$2.apply(NerConverter.scala:30)
 at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:252)
 at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:252)
 at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
 at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
 at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:252)
 at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
 at com.johnsnowlabs.nlp.annotators.ner.NerConverter.annotate(NerConverter.scala:30)
 at com.johnsnowlabs.nlp.AnnotatorModel$$anonfun$dfAnnotate$1.apply(AnnotatorModel.scala:35)
 at com.johnsnowlabs.nlp.AnnotatorModel$$anonfun$dfAnnotate$1.apply(AnnotatorModel.scala:34)
 at org.apache.spark.sql.catalyst.expressions.ScalaUDF$$anonfun$2.apply(ScalaUDF.scala:108)
 at org.apache.spark.sql.catalyst.expressions.ScalaUDF$$anonfun$2.apply(ScalaUDF.scala:107)
 at org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1063)
 ... 124 more
	</description>
	<comments>
	</comments>
</bug>