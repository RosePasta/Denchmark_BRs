<bug id='983' author='KatieGoz' open_date='2020-07-28T18:05:42Z' closed_time='2020-09-03T08:22:45Z'>
	<summary>Tokenization Bug With Bigrams in Exception List</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

Bigrams in tokenization exception list appear to be tokenized with subsequent punctuation. (Expected behavior is that the punctuation would be tokenized separately.) Also, when sentence is input into tokenizer (and bigram exception is present) the bigram token is sometimes dropped altogether.
&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

Bigram exceptions should be tokenized without punctuation, and should not be dropped from output regardless of whether document or sentence is input to tokenizer.
&lt;denchmark-h:h2&gt;Current Behavior&lt;/denchmark-h&gt;

For example, if the bigram exception in the tokenizer is "New York" and the input text is "My friend moved to New York. She likes it. Frank visited New York, and didn't like it." Then tokenized output includes "New York." and "New York," when document is the input for tokenizer. When sentence is the input for tokenizer, "New York." is output and "New York," is dropped from output.
&lt;denchmark-h:h2&gt;Possible Solution&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Steps to Reproduce&lt;/denchmark-h&gt;

See: &lt;denchmark-link:https://github.com/kgoz12/JSL_Bugs&gt;https://github.com/kgoz12/JSL_Bugs&lt;/denchmark-link&gt;







&lt;denchmark-h:h2&gt;Context&lt;/denchmark-h&gt;

I was piping tokenized text into context spell checker. Spell checker replacements will not contain punctuation, causing improperly punctuated sentence output.
&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


Spark NLP version: '2.5.4'
Apache NLP version:
Java version (java -version): 8
Setup and installation (Pypi, Conda, Maven, etc.):
Operating System and version: MacOS
Link to your project (if any): https://github.com/kgoz12/JSL_Bugs

	</description>
	<comments>
		<comment id='1' author='KatieGoz' date='2020-08-11T15:08:55Z'>
		I have come across a very similar problem. I'm using R, so my example code probably won't help you much. In my case though the LightPipeline and the PIpeline are both having the problem. I'll try to explain what I'm seeing. I am creating a Spark data frame with one column, text that contains one row with the following text:
No cough/sneezing noted in 1/2 of the cases. No other findings. Eyes: [X] Normal [_] Did not examine [_] Abnormal
then I create a Spark NLP pipeline consisting of document assembler, sentence detector and tokenizer. The document assembler and sentence detector use all default values. For the tokenizer I am specifying "[_]", "[X]" as exceptions using setExceptions. I next fit the pipeline and also create a LightPipeline from it.
If I specify the sentence as the input column for the tokenizer, the tokens "[_]" and "[X]" from the input text are not in the list of the results in either the Pipeline or LightPipeline cases. If I specify document as the input column for the tokenizer then these tokens are present in the results.
Here is the code for the example in case it does help:
&lt;denchmark-code&gt;document_assembler &lt;- nlp_document_assembler(sc, input_col = "text", output_col = "document")
sentence_detector &lt;- nlp_sentence_detector(sc, input_cols = c("document"), output_col = "sentence")
tokenizer &lt;- nlp_tokenizer(sc, input_cols = c("sentence"), output_col = "token", exceptions = c("[X]", "[_]"))

empty_df &lt;- sdf_copy_to(sc, data.frame(text = ""))

pipeline &lt;- ml_pipeline(document_assembler, sentence_detector, tokenizer) %&gt;% 
   ml_fit(empty_df)

lp &lt;- nlp_light_pipeline(pipeline)

text_df &lt;- sdf_copy_to(sc, data.frame(text = c("No cough/sneezing noted in 1/2 of the cases.No other findings. Eyes: [X] Normal [_] Did not examine [_] Abnormal")))

transform_result &lt;- ml_transform(pipeline, text_df)

transform_result %&gt;% 
   mutate(token_exploded = explode(token),
          token = token_exploded.result)

nlp_annotate(lp, "No cough/sneezing noted in 1/2 of the cases.No other findings. Eyes: [X] Normal [_] Did not examine [_] Abnormal")
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='KatieGoz' date='2020-08-21T21:16:58Z'>
		&lt;denchmark-link:https://github.com/dkincaid&gt;@dkincaid&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/KatieGoz&gt;@KatieGoz&lt;/denchmark-link&gt;
 the problem you both present seems to be the same. It's what I called bug#2 in the following description, and will try to get it fixed asap.
&lt;denchmark-h:h1&gt;DocumentAssembler&lt;/denchmark-h&gt;

Roughly speaking, the Tokenizer class performs a 2-step shallow processing,
a) it makes a pass replacing the spaces by a split char, "ↇ", and also protects the "exceptions".
For doing this it creates an internal representation, in which it adds a special char "ↈ", like in "NewↈYork". Following our example,
MyↇfriendↇmovedↇtoↇNewↈYork.ↇSheↇlikesↇit.ↇFrankↇvisitedↇNewↈYork,ↇandↇdidn'tↇlikeↇit.
b) it applies regex rules that contain groups(in the regex parlance).
So, for example we should use a regex like,
.setInfixPatterns(Array("(.*)(,|\.)"))
to separate the punctuation at the end from the words. This works for simple tokens.
Unfortunately, the Tokenizer won't touch a token anymore(won't make it to step b) if it contains a "protection char" so
NewↈYork.
or
NewↈYork,
remain the same. This is bug#1.
&lt;denchmark-h:h1&gt;SentenceDetector&lt;/denchmark-h&gt;

Here we have missing tokens when using the exception feature everywhere except in the first sentence of the document. That's because the Tokenizer relies on index arithmetic which uses indexes into the original document.
When using sentences the indices coming in the annotation still make reference to the original doc, but the text in the annotation is the text from the sentence only.
The only case for which indices of document and sentences match is in the first Sentence. That's why the first occurrence of New York is not dropped.
This is bug#2.
&lt;denchmark-link:https://github.com/maziyarpanahi&gt;@maziyarpanahi&lt;/denchmark-link&gt;
 I will create two PRs for this. Also we can use the description above as documentation for our Tokenizer(wiki may be?).
		</comment>
		<comment id='3' author='KatieGoz' date='2020-08-21T21:19:07Z'>
		Thank you! That is great news that you were able to track it down. We really appreciate the update.
		</comment>
		<comment id='4' author='KatieGoz' date='2020-08-24T11:41:23Z'>
		Thank you for the explanation Alberto! I really appreciate your looking into this!
		</comment>
		<comment id='5' author='KatieGoz' date='2020-09-03T08:22:45Z'>
		This should be fixed in 2.6.0, please feel free to reopen it if it still exists.
		</comment>
	</comments>
</bug>