<bug id='924' author='clabornd' open_date='2020-06-05T22:03:52Z' closed_time='2020-06-06T21:30:42Z'>
	<summary>Column mismatch in BERT NER pretrained pipeline</summary>
	<description>
Trying to get results out of the BERT NER pretrained pipeline, not sure if I am misunderstanding something.  The output of .annotate() returns no NER results, with the same behavior on actual text with named entities.  When i attempt to .transform(mydf) the error message seems to tell me the NER stage is expecting an input column 'embeddings' but the BertEmbeddings stage outputs a 'bert' column.
&lt;denchmark-code&gt;mydf = spark.createDataFrame([Row(text = 'hi this is some text that I made up.'), Row(text = 'this is some more boring text')])

bert_ner = PretrainedPipeline('recognize_entities_bert', lang='en')

# try out an example
text = mydf.take(1)[0]['text']

# Annotate your testing dataset
result = bert_ner.annotate(text)

&gt;&gt;&gt;result['ner']
[]

&gt;&gt;&gt; bert_ner.transform(mydf)

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
     62         try:
---&gt; 63             return f(*a, **kw)
     64         except py4j.protocol.Py4JJavaError as e:

/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    327                     "An error occurred while calling {0}{1}{2}.\n".
--&gt; 328                     format(target_id, ".", name), value)
    329             else:

Py4JJavaError: An error occurred while calling o10400.transform.
: java.lang.IllegalArgumentException: requirement failed: Wrong or missing inputCols annotators in NerDLModel_5a4d924d6e2a.

Current inputCols: sentence,token,embeddings. Dataset's columns:
(column_name=text,is_nlp_annotator=false)
(column_name=document,is_nlp_annotator=true,type=document)
(column_name=sentence,is_nlp_annotator=true,type=document)
(column_name=token,is_nlp_annotator=true,type=token)
(column_name=bert,is_nlp_annotator=true,type=word_embeddings).
Make sure such annotators exist in your pipeline, with the right output names and that they have following annotator types: document, token, word_embeddings
	at scala.Predef$.require(Predef.scala:224)
	at com.johnsnowlabs.nlp.AnnotatorModel._transform(AnnotatorModel.scala:43)
	at com.johnsnowlabs.nlp.AnnotatorModel.transform(AnnotatorModel.scala:79)
	at sun.reflect.GeneratedMethodAccessor602.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)
	at py4j.Gateway.invoke(Gateway.java:295)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:251)
	at java.lang.Thread.run(Thread.java:748)


During handling of the above exception, another exception occurred:

IllegalArgumentException                  Traceback (most recent call last)
&lt;command-1990432813676713&gt; in &lt;module&gt;
----&gt; 1 bert_ner.transform(mydf)

/databricks/python/lib/python3.7/site-packages/sparknlp/pretrained.py in transform(self, data)
    121 
    122     def transform(self, data):
--&gt; 123         return self.model.transform(data)

/databricks/spark/python/pyspark/ml/base.py in transform(self, dataset, params)
    171                 return self.copy(params)._transform(dataset)
    172             else:
--&gt; 173                 return self._transform(dataset)
    174         else:
    175             raise ValueError("Params must be a param map but got %s." % type(params))

/databricks/spark/python/pyspark/ml/pipeline.py in _transform(self, dataset)
    260     def _transform(self, dataset):
    261         for t in self.stages:
--&gt; 262             dataset = t.transform(dataset)
    263         return dataset
    264 

/databricks/spark/python/pyspark/ml/base.py in transform(self, dataset, params)
    171                 return self.copy(params)._transform(dataset)
    172             else:
--&gt; 173                 return self._transform(dataset)
    174         else:
    175             raise ValueError("Params must be a param map but got %s." % type(params))

/databricks/spark/python/pyspark/ml/wrapper.py in _transform(self, dataset)
    310     def _transform(self, dataset):
    311         self._transfer_params_to_java()
--&gt; 312         return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)
    313 
    314 

/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1255         answer = self.gateway_client.send_command(command)
   1256         return_value = get_return_value(
-&gt; 1257             answer, self.gateway_client, self.target_id, self.name)
   1258 
   1259         for temp_arg in temp_args:

/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
     77                 raise QueryExecutionException(s.split(': ', 1)[1], stackTrace)
     78             if s.startswith('java.lang.IllegalArgumentException: '):
---&gt; 79                 raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)
     80             raise
     81     return deco

IllegalArgumentException: "requirement failed: Wrong or missing inputCols annotators in NerDLModel_5a4d924d6e2a.\n\nCurrent inputCols: sentence,token,embeddings. Dataset's columns:\n(column_name=text,is_nlp_annotator=false)\n(column_name=document,is_nlp_annotator=true,type=document)\n(column_name=sentence,is_nlp_annotator=true,type=document)\n(column_name=token,is_nlp_annotator=true,type=token)\n(column_name=bert,is_nlp_annotator=true,type=word_embeddings).\nMake sure such annotators exist in your pipeline, with the right output names and that they have following annotator types: document, token, word_embeddings"
&lt;/denchmark-code&gt;

I am able to get around this by manually getting the 'token' column using another pretrained pipeline (whole separate issue which is probably my misunderstanding) and renaming 'bert' to 'embeddings'.  Wondering also if this is a legitimate workaround, it appears to produce sensible results....any help is appreciated.
&lt;denchmark-code&gt;dl_pipeline = PretrainedPipeline('explain_document_dl', lang='en')
annot_df = dl_pipeline.transform(mydf)

bert_embeddings = BertEmbeddings.pretrained('bert_base_cased')
bert_ner = NerDLModel.pretrained(name='ner_dl_bert', lang='en')

embeddings_df = bert_embeddings.transform(annot_df.select(*['document', 'sentence', 'token']))

ner_df = bert_ner.transform(embeddings_df.withColumnRenamed('bert', 'embeddings'))

&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


Spark NLP version: 2.5
Java version (java -version):  1.8.0_232
Setup and installation (Pypi, Conda, Maven, etc.): Pypi/Maven
Operating System and version: Databricks Runtime V 6.4 (no idea)

	</description>
	<comments>
		<comment id='1' author='clabornd' date='2020-06-06T15:13:38Z'>
		Hi,
Thank you for reporting this, the pipeline was not saved correctly I have reproduced it correctly and uploaded it, so please try it one more time and you should be downloading a new pipeline for 2.5.x, have embeddings as the output of BERT, ner for NER, and entities for the end entities.
What you did at the end is not wrong, in fact, we call that custom pipelines by using multiple pre-trained models and annotators to get the desired pipeline that might not exist in our list. It also gives you more options and flexibility to tune the parameters/inputs/outputs and play around which might not be possible in pre-trained pipelines. So kudos for finding that out on your own! üëè
We have some good examples here: &lt;denchmark-link:https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/tutorials/Certification_Trainings/Public&gt;https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/tutorials/Certification_Trainings/Public&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='clabornd' date='2020-06-06T21:30:42Z'>
		Works!  Thanks for the quick turnaround.
		</comment>
	</comments>
</bug>