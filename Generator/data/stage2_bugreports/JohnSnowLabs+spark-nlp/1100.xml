<bug id='1100' author='tombriles' open_date='2020-10-13T19:03:58Z' closed_time='2020-11-08T12:44:33Z'>
	<summary>Cannot set threshold in python YakeModel annotator to non-Integer</summary>
	<description>
Here is a simple test case:
`import sparknlp
from sparknlp.annotator import *
from sparknlp.base import *
spark = sparknlp.start()
keywords = YakeModel().setInputCols("token").setOutputCol("keyphrases").setThreshold(0.6)`
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

Here's the bug in the code:
python/sparknlp/annotator.py line 2896
    threshold = Param(Params._dummy(), "maxNGrams", "Keyword Score threshold", typeConverter=TypeConverters.toInt)
&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

setThreshold() sets the threshold.
&lt;denchmark-h:h2&gt;Current Behavior&lt;/denchmark-h&gt;

TypeError: Invalid param value given for param "maxNGrams". Could not convert 0.6 to int
&lt;denchmark-h:h2&gt;Possible Solution&lt;/denchmark-h&gt;

Here's the bug in the code:
python/sparknlp/annotator.py line 2896
    threshold = Param(Params._dummy(), "maxNGrams", "Keyword Score threshold", typeConverter=TypeConverters.toInt)
&lt;denchmark-h:h2&gt;Steps to Reproduce&lt;/denchmark-h&gt;


See test case above.

&lt;denchmark-h:h2&gt;Context&lt;/denchmark-h&gt;

Trying to set the threshold.
&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


Spark NLP version: 2.6.2
Apache NLP version: 2.4.5
Java version (java -version): 8
Setup and installation (Pypi, Conda, Maven, etc.): conda
Operating System and version: Databricks
Link to your project (if any): N/A

	</description>
	<comments>
		<comment id='1' author='tombriles' date='2020-10-15T18:07:04Z'>
		Hi &lt;denchmark-link:https://github.com/tombriles&gt;@tombriles&lt;/denchmark-link&gt;

This threshold is actuallythe maximum length of an extracted keyword. It is an integer value and not float. You need to set an integer value.
		</comment>
		<comment id='2' author='tombriles' date='2020-10-15T18:23:51Z'>
		Documentation says:
"setThreshold(float) Each keyword will be given a keyword score greater than 0. (Lower the score better the keyword) Set an upper bound for the keyword score from this method."
&lt;denchmark-link:url&gt;https://nlp.johnsnowlabs.com/docs/en/annotators#yakemodel-keywords-extraction&lt;/denchmark-link&gt;

Also, the scala test at &lt;denchmark-link:url&gt;https://github.com/JohnSnowLabs/spark-nlp/blob/f3779e000c1620d3a9bdbebf59ea0918f5492440/src/test/scala/com/johnsnowlabs/nlp/annotators/keyword/yake/YakeTestSpec.scala&lt;/denchmark-link&gt;
 sets the threshold to 0.6f.
		</comment>
		<comment id='3' author='tombriles' date='2020-10-15T19:05:27Z'>
		I now see where the confusion came from. ;)
		</comment>
		<comment id='4' author='tombriles' date='2020-10-15T19:06:18Z'>
		You are right, the Python API is totally wrong, it says:
&lt;denchmark-code&gt;threshold = Param(Params._dummy(), "maxNGrams", "Keyword Score threshold", typeConverter=TypeConverters.toInt)
&lt;/denchmark-code&gt;

It uses maxNGrams by mistake which made me think it was Int. This will be fixed in the upcoming release.
Thanks for reporting it :)
		</comment>
	</comments>
</bug>