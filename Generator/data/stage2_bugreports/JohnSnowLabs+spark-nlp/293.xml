<bug id='293' author='lisaslyis' open_date='2018-10-25T03:07:44Z' closed_time='2019-08-20T08:39:30Z'>
	<summary>Issues of integration with Zeppelin on local environment</summary>
	<description>
The library is not usable in local Zeppelin 0.8.0 + Spark 2.3.1 environment when the instruction is followed.
Some of the issues might need to be fixed on Zeppelin side.
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;



Spark-nlp python API has issues when the maven artifact is configured in Zeppelin web interface

commons-codec 1.3 seems to have a version conflict issue in Zeppelin pyspark interpreter

The pythonpath needs to be added explicitly for python wrappers (In addition to the real python wrapper files)



SPARK_SUBMIT_OPTIONS doesn't seem to work with spark-nlp artifact when it was set in zeppelin-env.sh

Zeppelin 0.8.0 seems to have an issue with SPARK_SUBMIT_OPTIONS dependencies regarding ivy server/cache when it's run as yarn-client with other default settings. It looks like Zeppelin client has its own way of runtime library resolution.




&lt;denchmark-h:h2&gt;Reproducing steps&lt;/denchmark-h&gt;




Configure com.johnsnowlabs.nlp:spark-nlp_2.11:1.7.2 in Zeppelin spark interpreter setting on web UI
Download python wrapper files and configure spark's pythonpath to refer to them
Run python spark-nlp API on Zeppelin




put export SPARK_SUBMIT_OPTIONS="--packages JohnSnowLabs:spark-nlp:1.7.2 in zeppelin-env.sh
Run scala spark-nlp API on Zeppelin



&lt;denchmark-h:h2&gt;Temporary workaround&lt;/denchmark-h&gt;


Download and pass a fat-jar file directly as a dependency and configure pythonpath for it
Download a jar from remote repo (spark-repo: jar, maven-repo: jar &lt;+ python wrapper from the other place&gt;) using Zeppelin or Spark configuration and set pythonpath for it.

&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


Spark-nlp 1.7.2
Zeppelin 0.8.0
Spark 2.3.1
Yarn-client mode on local cluster

	</description>
	<comments>
		<comment id='1' author='lisaslyis' date='2018-10-25T17:11:58Z'>
		Thanks, we should review the current installation instructions and see what's going on. A user on Slack reported the same issue with the apache.commons.codec path.
		</comment>
		<comment id='2' author='lisaslyis' date='2018-10-25T22:49:56Z'>
		The dependency is as follows
aws-java-sdk 1.7 -&gt; commons-codec 1.3
I guess using a different version of aws-java-sdk or enforcing commons-codec &gt;=1.4 in build-process can be a solution if it doesn't create any compatibility issue.
		</comment>
		<comment id='3' author='lisaslyis' date='2018-10-26T13:58:12Z'>
		Yes I am investigating this. I could reproduce the issue. So far the only thing I learned is that SPARK_SUBMIT_OPTIONS is not reliable in anyway anymore. If I could suggest a package for zeppelin that upgrades commons-codec that should be enough for now, but still testing.
		</comment>
		<comment id='4' author='lisaslyis' date='2018-10-29T16:32:02Z'>
		&lt;denchmark-link:https://github.com/JohnSnowLabs/spark-nlp/pull/296&gt;#296&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='lisaslyis' date='2019-08-20T08:39:30Z'>
		Iâ€™m closing this as the latest releases after2.x are compatible with latest Apache Spark and latest Apache Zeppelin 0.8.2.
		</comment>
	</comments>
</bug>