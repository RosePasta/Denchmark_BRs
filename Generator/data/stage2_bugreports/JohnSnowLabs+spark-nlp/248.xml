<bug id='248' author='iolbr' open_date='2018-07-19T06:13:45Z' closed_time='2019-07-10T07:28:52Z'>
	<summary>Finisher throws Exception when flattening Regex results</summary>
	<description>
When setting up a pipeline with RegexMatcher I got an exception executing the finisher step
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;


Build test datafram
Load Regex pattern
Match
Run finisher

&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

Expect new column in dataframe with results from RegexMatcher
&lt;denchmark-h:h2&gt;Current Behavior&lt;/denchmark-h&gt;

org.apache.spark.SparkException: Failed to execute user defined function(anonfun$flatten$1: (array&lt;struct&lt;annotatorType:string,begin:int,end:int,result:string,metadata:map&lt;string,string&gt;&gt;&gt;) =&gt; string)
at org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1072)
at org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:144)
at org.apache.spark.sql.catalyst.expressions.InterpretedProjection.apply(Projection.scala:48)
at org.apache.spark.sql.catalyst.expressions.InterpretedProjection.apply(Projection.scala:30)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.immutable.List.foreach(List.scala:381)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
at scala.collection.immutable.List.map(List.scala:285)
at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$apply$21.applyOrElse(Optimizer.scala:1078)
at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$apply$21.applyOrElse(Optimizer.scala:1073)
at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:288)
at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:288)
at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:287)
at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:277)
at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.apply(Optimizer.scala:1073)
at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.apply(Optimizer.scala:1072)
at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)
at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)
at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)
at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
at scala.collection.immutable.List.foreach(List.scala:381)
at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:73)
at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:73)
at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:79)
at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:75)
at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:84)
at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:84)
at org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2791)
at org.apache.spark.sql.Dataset.head(Dataset.scala:2112)
at org.apache.spark.sql.Dataset.take(Dataset.scala:2327)
at org.apache.spark.sql.Dataset.showString(Dataset.scala:248)
at org.apache.spark.sql.Dataset.show(Dataset.scala:636)
at org.apache.spark.sql.Dataset.show(Dataset.scala:595)
at org.apache.spark.sql.Dataset.show(Dataset.scala:604)
... 82 elided
Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$dfAnnotate$1: (array&lt;array&lt;struct&lt;annotatorType:string,begin:int,end:int,result:string,metadata:map&lt;string,string&gt;&gt;&gt;&gt;) =&gt; array&lt;struct&lt;annotatorType:string,begin:int,end:int,result:string,metadata:map&lt;string,string&gt;&gt;&gt;)
at org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1072)
at org.apache.spark.sql.catalyst.expressions.ScalaUDF$$anonfun$2.apply(ScalaUDF.scala:90)
at org.apache.spark.sql.catalyst.expressions.ScalaUDF$$anonfun$2.apply(ScalaUDF.scala:88)
at org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1069)
... 133 more
Caused by: java.util.regex.PatternSyntaxException: Unclosed character class near index 0
[
^
at java.util.regex.Pattern.error(Pattern.java:1955)
at java.util.regex.Pattern.clazz(Pattern.java:2548)
at java.util.regex.Pattern.sequence(Pattern.java:2063)
at java.util.regex.Pattern.expr(Pattern.java:1996)
at java.util.regex.Pattern.compile(Pattern.java:1696)
at java.util.regex.Pattern.(Pattern.java:1351)
at java.util.regex.Pattern.compile(Pattern.java:1028)
at scala.util.matching.Regex.(Regex.scala:191)
at scala.collection.immutable.StringLike$class.r(StringLike.scala:255)
at scala.collection.immutable.StringOps.r(StringOps.scala:29)
at scala.collection.immutable.StringLike$class.r(StringLike.scala:244)
at scala.collection.immutable.StringOps.r(StringOps.scala:29)
at com.johnsnowlabs.nlp.util.regex.RegexRule.(RegexRule.scala:12)
at com.johnsnowlabs.nlp.annotators.RegexMatcherModel$$anonfun$com$johnsnowlabs$nlp$annotators$RegexMatcherModel$$matchFactory$1.apply(RegexMatcherModel.scala:50)
at com.johnsnowlabs.nlp.annotators.RegexMatcherModel$$anonfun$com$johnsnowlabs$nlp$annotators$RegexMatcherModel$$matchFactory$1.apply(RegexMatcherModel.scala:50)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
at com.johnsnowlabs.nlp.annotators.RegexMatcherModel.com$johnsnowlabs$nlp$annotators$RegexMatcherModel$$matchFactory$lzycompute(RegexMatcherModel.scala:50)
at com.johnsnowlabs.nlp.annotators.RegexMatcherModel.com$johnsnowlabs$nlp$annotators$RegexMatcherModel$$matchFactory(RegexMatcherModel.scala:48)
at com.johnsnowlabs.nlp.annotators.RegexMatcherModel$$anonfun$annotate$1.apply(RegexMatcherModel.scala:55)
at com.johnsnowlabs.nlp.annotators.RegexMatcherModel$$anonfun$annotate$1.apply(RegexMatcherModel.scala:54)
at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
at com.johnsnowlabs.nlp.annotators.RegexMatcherModel.annotate(RegexMatcherModel.scala:54)
at com.johnsnowlabs.nlp.AnnotatorModel$$anonfun$dfAnnotate$1.apply(AnnotatorModel.scala:43)
at com.johnsnowlabs.nlp.AnnotatorModel$$anonfun$dfAnnotate$1.apply(AnnotatorModel.scala:42)
at org.apache.spark.sql.catalyst.expressions.ScalaUDF$$anonfun$2.apply(ScalaUDF.scala:89)
at org.apache.spark.sql.catalyst.expressions.ScalaUDF$$anonfun$2.apply(ScalaUDF.scala:88)
at org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1069)
... 136 more
ERROR
&lt;denchmark-h:h2&gt;Possible Solution&lt;/denchmark-h&gt;

Looks like an issue when loading pattern from file. My Patternfile contains just a single line:
&lt;denchmark-link:https://github.com/JohnSnowLabs/spark-nlp/files/2208485/regex.txt&gt;regex.txt&lt;/denchmark-link&gt;

[A-Z0-9._%+-]+@[A-Z0-9.-]+.[A-Z]{2,4}|email-address
&lt;denchmark-h:h2&gt;Steps to Reproduce&lt;/denchmark-h&gt;

val test = Seq(
("mail to &lt;denchmark-link:mailto:john.snow@mail.com&gt;john.snow@mail.com&lt;/denchmark-link&gt;
")
).toDF("text")
import com.johnsnowlabs.nlp.annotator._
import com.johnsnowlabs.nlp.base._
import com.johnsnowlabs.util.Benchmark
import org.apache.spark.ml.Pipeline
import com.johnsnowlabs.nlp.util.io.{ExternalResource, ReadAs, ResourceHelper}
val documentAssembler = new DocumentAssembler()
.setInputCol("text")
.setOutputCol("document")
val resource = ExternalResource("../data/regex.txt", ReadAs.LINE_BY_LINE,Map("delimiter"-&gt;"|"))
val mailExtractor = new RegexMatcher()
.setInputCols("document")
.setRules(resource)
.setOutputCol("contact")
val finisher = new Finisher()
.setInputCols("contact")
.setOutputAsArray(false)
.setAnnotationSplitSymbol("@")
.setValueSplitSymbol("#")
val recursivePipeline = new RecursivePipeline()
.setStages(Array(
documentAssembler,
mailExtractor,
finisher
))
recursivePipeline.fit(test).transform(test).show
&lt;denchmark-h:h2&gt;Context&lt;/denchmark-h&gt;

Try to extract mail adresses from text
&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


Version used:Zeppelin notebook with /libs/spark-nlp-assembly-1.6.0.jar
Browser Name and version:
Operating System and version (desktop or mobile):
Link to your project:

	</description>
	<comments>
		<comment id='1' author='iolbr' date='2018-08-22T13:56:35Z'>
		Thank you, we'll take a look!
		</comment>
		<comment id='2' author='iolbr' date='2018-08-22T15:46:29Z'>
		Looks, to me that's happening in the regex matcher, will take a look
		</comment>
		<comment id='3' author='iolbr' date='2018-08-22T21:29:20Z'>
		&lt;denchmark-link:https://github.com/iolbr&gt;@iolbr&lt;/denchmark-link&gt;
 the problem is that "|" is special character for regular expressions, we're not handling that at the moment in the API, but you can work around this with the following change,
val resource = ExternalResource("regex.txt", ReadAs.LINE_BY_LINE,Map("delimiter"-&gt;"\|"))
We will fix this in the next release.
		</comment>
	</comments>
</bug>