<bug id='772' author='mengxr' open_date='2020-02-03T18:44:27Z' closed_time='2020-04-02T18:13:30Z'>
	<summary>typesafe config getting serialized caused "java.io.UTFDataFormatException: encoded string too long"</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

I saw &lt;denchmark-link:https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/main/scala/com/johnsnowlabs/util/ConfigLoader.scala#L21&gt;ConfigLoader.retrieve&lt;/denchmark-link&gt;
 is referenced in class objects that go into a serialized Spark task. This would lead to the "java.io.UTFDataFormatException" issue on some platform when the default (fallback) config is too big.
&lt;denchmark-link:https://github.com/JohnSnowLabs/spark-nlp/issues/745&gt;#745&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/JohnSnowLabs/spark-nlp/issues/691&gt;#691&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/JohnSnowLabs/spark-nlp/issues/679&gt;#679&lt;/denchmark-link&gt;
 might have the same root cause.
cc: &lt;denchmark-link:https://github.com/maziyarpanahi&gt;@maziyarpanahi&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

No serialization error.
&lt;denchmark-h:h2&gt;Current Behavior&lt;/denchmark-h&gt;

Serialization error on some platforms with large default typesafe config.
&lt;denchmark-h:h2&gt;Possible Solution&lt;/denchmark-h&gt;


Do we need to serialize the conf with spark tasks or does it get pulled into the task closure by accident? If it is not needed, we can mark it as @transient.
If it is needed, I think the solution is to map the typesafe config to a narrow scoped config object that contains only relevant configs to avoid serialization issue.

&lt;denchmark-h:h2&gt;Steps to Reproduce&lt;/denchmark-h&gt;

import com.johnsnowlabs.util.ConfigLoader
import java.io._

val bos = new ByteArrayOutputStream()
val oos = new ObjectOutputStream(bos)
oos.writeObject(ConfigLoader.retrieve)
oos.close()
throws the following error on Databricks Runtime 6.3 ML:
&lt;denchmark-code&gt;java.io.UTFDataFormatException: encoded string too long: 67385 bytes
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:364)
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323)
	at com.typesafe.config.impl.SerializedConfigValue.writeValueData(SerializedConfigValue.java:301)
	at com.typesafe.config.impl.SerializedConfigValue.writeValue(SerializedConfigValue.java:375)
	at com.typesafe.config.impl.SerializedConfigValue.writeValueData(SerializedConfigValue.java:315)
	at com.typesafe.config.impl.SerializedConfigValue.writeValue(SerializedConfigValue.java:375)
	at com.typesafe.config.impl.SerializedConfigValue.writeValueData(SerializedConfigValue.java:315)
	at com.typesafe.config.impl.SerializedConfigValue.writeValue(SerializedConfigValue.java:375)
	at com.typesafe.config.impl.SerializedConfigValue.writeValueData(SerializedConfigValue.java:315)
	at com.typesafe.config.impl.SerializedConfigValue.writeValue(SerializedConfigValue.java:375)
	at com.typesafe.config.impl.SerializedConfigValue.writeExternal(SerializedConfigValue.java:441)
	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
&lt;/denchmark-code&gt;

Unfortunately, we have a very large conf:
import com.typesafe.config.ConfigFactory
val config = ConfigFactory.load()
config.getConfig("java").getConfig("class").getString("path").length
output
&lt;denchmark-code&gt;67383
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Context&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


Spark NLP version: 2.3.4
Spark version: 2.4.4 (Databricks Runtime 6.3 ML)
Apache NLP version:
Setup and installation (Pypi, Conda, Maven, etc.):
Operating System and version:
Link to your project (if any):

	</description>
	<comments>
		<comment id='1' author='mengxr' date='2020-02-03T20:06:14Z'>
		&lt;denchmark-link:https://github.com/mengxr&gt;@mengxr&lt;/denchmark-link&gt;
 Thank you so much for debugging this and bringing it to our attention. In case of a large config provided by the user, this could happen and we will investigate this. However, those issues with the YARN cluster-mode I have tested and it works on some annotators and fails with the others. So clearly we have a feature of some sort that is not serializable when the Driver is part of the cluster itself.
This also could result in the same serialization issue which I am glad you brought it up so we can fix them altogether üëç
		</comment>
		<comment id='2' author='mengxr' date='2020-02-23T17:15:35Z'>
		Our team is also facing the same problem. Our Spark configuration is also greater than 75000 bytes and we are also seeing the same issue.
Is there any ETA of a fix for this problem?
		</comment>
		<comment id='3' author='mengxr' date='2020-03-19T18:33:46Z'>
		We have made some progress and made some tests in Cloudera YARN cluster-mode and Databricks. The results are here: &lt;denchmark-link:https://github.com/JohnSnowLabs/spark-nlp/issues/832&gt;#832&lt;/denchmark-link&gt;

Could you please make a fat JAR from this PR and give it a go:
&lt;denchmark-link:https://github.com/JohnSnowLabs/spark-nlp/pull/834&gt;#834&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='mengxr' date='2020-04-02T18:13:30Z'>
		This has been resolved in 2.4.5 release: &lt;denchmark-link:https://github.com/JohnSnowLabs/spark-nlp/releases/tag/2.4.5&gt;https://github.com/JohnSnowLabs/spark-nlp/releases/tag/2.4.5&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>