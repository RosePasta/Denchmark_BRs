<bug id='840' author='maziyarpanahi' open_date='2020-03-27T12:03:14Z' closed_time='2020-04-02T18:16:05Z'>
	<summary>Normalizer should not re-calculate the positions</summary>
	<description>
In Normalizer the re-calculation of token's position results in out of the bound index.
	</description>
	<comments>
		<comment id='1' author='maziyarpanahi' date='2020-04-02T18:16:05Z'>
		This has been resolved in 2.4.5 release: &lt;denchmark-link:https://github.com/JohnSnowLabs/spark-nlp/releases/tag/2.4.5&gt;https://github.com/JohnSnowLabs/spark-nlp/releases/tag/2.4.5&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>