<bug id='666' author='reynoldsm88' open_date='2019-10-28T18:05:29Z' closed_time='2019-10-31T14:24:44Z'>
	<summary>Spark-NLP 2.3.0 upgrade breaks existing pipeline code</summary>
	<description>
After upgrading from version 2.2.2 to 2.3.0 one of our existing jobs breaks due to a ClassCastException when running a Tokenizer that feeds into a Lemmatizer. See error description below.
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

We have an existing job that performs the following operations:
val assembler = new DocumentAssembler().setInputCol( "extracted_text" ).setOutputCol( "document" )

val tokenizer = new Tokenizer()
  .setInputCols( "document" )
  .setOutputCol( "extracted_tokens" )

val lemmatizer = LemmatizerModel
  .read.load( s"${modelDir}/lemma_antbnc" )
  .setInputCols( "extracted_tokens" )
  .setOutputCol( "extracted_lemmas" )
val pipeline = new Pipeline().setStages( Array( assembler, tokenizer, lemmatizer ) )
...
Using Spark-NLP 2.2.2 on top of Apache Spark 2.4.4 (hadoop 2.7.7 or later) this job executes successfully.
&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

We're expecting to get the output of our job, which has been running successfully with the current version of the code for some time.
&lt;denchmark-h:h2&gt;Current Behavior&lt;/denchmark-h&gt;

The job exits with the following exception:
[info]   java.lang.ClassCastException: com.johnsnowlabs.nlp.annotators.LemmatizerModel cannot be cast to com.johnsnowlabs.nlp.annotators.TokenizerModel
Currently, we have only observed this error in our CI/CD builds in a local standalone Spark cluster (we're testing using &lt;denchmark-link:https://github.com/MrPowers/spark-fast-tests&gt;Spark Fast Tests&lt;/denchmark-link&gt;
. We have not yet deployed the job to a real cluster until we understand the issue better.
&lt;denchmark-h:h2&gt;Possible Solution&lt;/denchmark-h&gt;

N/A
&lt;denchmark-h:h2&gt;Steps to Reproduce&lt;/denchmark-h&gt;


Create a job using the code from the description
Implement a Scalatest unit test using spark fast tests to execute the job

&lt;denchmark-h:h2&gt;Context&lt;/denchmark-h&gt;

We just noticed this upon upgrading from 2.2.2 to 2.3.0
&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;

Spark 2.4.4 for Hadoop 2.7.7 or later
Spark NLP 2.3.0
Spark Fast Test 0.20.0-s_2.11
Scala 2.11.12
Openjdk version 1.8.0_202
	</description>
	<comments>
		<comment id='1' author='reynoldsm88' date='2019-10-28T18:05:51Z'>
		If possible I will try to boil this down into a reproducer repository that I can share.
		</comment>
		<comment id='2' author='reynoldsm88' date='2019-10-28T18:41:00Z'>
		Hi &lt;denchmark-link:https://github.com/reynoldsm88&gt;@reynoldsm88&lt;/denchmark-link&gt;

I think this issue is related to this PR we have: &lt;denchmark-link:https://github.com/JohnSnowLabs/spark-nlp/pull/665&gt;#665&lt;/denchmark-link&gt;

It was an error and will be fixed in hotfix  release
		</comment>
		<comment id='3' author='reynoldsm88' date='2019-10-28T18:44:28Z'>
		Ah awesome &lt;denchmark-link:https://github.com/maziyarpanahi&gt;@maziyarpanahi&lt;/denchmark-link&gt;
. Do you have any sense of when you expect the fix to be released?
Just curious for some team planning type stuff.
		</comment>
		<comment id='4' author='reynoldsm88' date='2019-10-28T19:05:06Z'>
		We thought it was just in pretrained and the workaround could hold up for a few days, but now that you reported this bug we did investigate and it was related to the same error.
So it means it's bigger than we thought and we will release the hotfix very soon. I just need to finish my PR for EmbeddingsFinisher transformer in an hour and I expect this all to be done within a few hours.
		</comment>
		<comment id='5' author='reynoldsm88' date='2019-10-28T19:15:12Z'>
		Ah gotcha! Thanks &lt;denchmark-link:https://github.com/maziyarpanahi&gt;@maziyarpanahi&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='reynoldsm88' date='2019-10-31T14:24:44Z'>
		We have released 2.3.1 and I believe this bug has been resolved.
Thanks for reporting it and please do reopen if it's not resolved.
		</comment>
	</comments>
</bug>