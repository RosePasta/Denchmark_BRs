<bug id='306' author='munaAchyuta' open_date='2018-11-13T07:32:18Z' closed_time='2019-03-27T20:00:56Z'>
	<summary>java.lang.NullPointerException issue while calling PerceptronApproach().setCorpus()</summary>
	<description>
Hi,
can anyone tell me why this issue is coming ?
while setting corpus after done with pos-tag this issue is coming.  specifically in this line.
.setCorpus("file:///" + os.getcwd() + "/../../../src/test/resources/anc-pos-corpus-small/","|")
is it permission issue ? i don't think so, bcs other files are able to be created.
python - 2.7
spark - 2.4.0
spark-nlp - latest version
more on issue details :
&lt;denchmark-code&gt;---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
&lt;ipython-input-53-d0ecfbd07161&gt; in &lt;module&gt;()
      2 #import pdb;pdb.set_trace()
      3 print("Start fitting")
----&gt; 4 model = pipeline.fit(data)
      5 print("Fitting is ended")
      6 print (time.time() - start)

/spark/python/pyspark/ml/base.pyc in fit(self, dataset, params)
    130                 return self.copy(params)._fit(dataset)
    131             else:
--&gt; 132                 return self._fit(dataset)
    133         else:
    134             raise ValueError("Params must be either a param map or a list/tuple of param maps, "

/spark/python/pyspark/ml/pipeline.pyc in _fit(self, dataset)
    107                     dataset = stage.transform(dataset)
    108                 else:  # must be an Estimator
--&gt; 109                     model = stage.fit(dataset)
    110                     transformers.append(model)
    111                     if i &lt; indexOfLastEstimator:

/spark/python/pyspark/ml/base.pyc in fit(self, dataset, params)
    130                 return self.copy(params)._fit(dataset)
    131             else:
--&gt; 132                 return self._fit(dataset)
    133         else:
    134             raise ValueError("Params must be either a param map or a list/tuple of param maps, "

/spark/python/pyspark/ml/wrapper.pyc in _fit(self, dataset)
    293 
    294     def _fit(self, dataset):
--&gt; 295         java_model = self._fit_java(dataset)
    296         model = self._create_model(java_model)
    297         return self._copyValues(model)

/spark/python/pyspark/ml/wrapper.pyc in _fit_java(self, dataset)
    290         """
    291         self._transfer_params_to_java()
--&gt; 292         return self._java_obj.fit(dataset._jdf)
    293 
    294     def _fit(self, dataset):

/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1255         answer = self.gateway_client.send_command(command)
   1256         return_value = get_return_value(
-&gt; 1257             answer, self.gateway_client, self.target_id, self.name)
   1258 
   1259         for temp_arg in temp_args:

/spark/python/pyspark/sql/utils.pyc in deco(*a, **kw)
     61     def deco(*a, **kw):
     62         try:
---&gt; 63             return f(*a, **kw)
     64         except py4j.protocol.Py4JJavaError as e:
     65             s = e.java_exception.toString()

/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    326                 raise Py4JJavaError(
    327                     "An error occurred while calling {0}{1}{2}.\n".
--&gt; 328                     format(target_id, ".", name), value)
    329             else:
    330                 raise Py4JError(

Py4JJavaError: An error occurred while calling o1085.fit.
: java.lang.NullPointerException
	at com.johnsnowlabs.nlp.util.io.ResourceHelper$.parseTupleSentences(ResourceHelper.scala:229)
	at com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronApproach.train(PerceptronApproach.scala:110)
	at com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronApproach.train(PerceptronApproach.scala:20)
	at com.johnsnowlabs.nlp.AnnotatorApproach.fit(AnnotatorApproach.scala:33)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
&lt;/denchmark-code&gt;

Thanks.
	</description>
	<comments>
		<comment id='1' author='munaAchyuta' date='2018-11-13T16:57:23Z'>
		hi &lt;denchmark-link:https://github.com/munaAchyuta&gt;@munaAchyuta&lt;/denchmark-link&gt;
, I don't understand how that line could have triggered the exception you pasted. It seems like you're trying to fit the POS model without a dataset or an external file.
Is this your code?,
&lt;denchmark-code&gt;  2 #import pdb;pdb.set_trace()
  3 print("Start fitting")
  4 model = pipeline.fit(data)
  5 print("Fitting is ended")
  6 print (time.time() - start)
&lt;/denchmark-code&gt;

In case it's not, please paste the complete code, at least the part where you set up the pipeline.
thanks!
		</comment>
		<comment id='2' author='munaAchyuta' date='2018-11-14T05:32:12Z'>
		Hi &lt;denchmark-link:https://github.com/albertoandreottiATgmail&gt;@albertoandreottiATgmail&lt;/denchmark-link&gt;
  , thanks for quick reply. FYI, i am using some code samples from spark-nlp repo. more concretely crf-ner code , which is available in 
i was just exploring features through given example test script.
as mentioned above , i'm also using same given dataset (spark-nlp--&gt;src--&gt;test--&gt;resources--&gt;anc-pos-corpus-small).
please find pipeline code .
&lt;denchmark-code&gt;documentAssembler = DocumentAssembler()\
  .setInputCol("text")\
  .setOutputCol("document")

sentenceDetector = SentenceDetector()\
  .setInputCols(["document"])\
  .setOutputCol("sentence")

tokenizer = Tokenizer()\
  .setInputCols(["document"])\
  .setOutputCol("token")

posTagger = PerceptronApproach()\
  .setIterations(3)\
  .setInputCols(["token", "document"])\
  .setOutputCol("pos")\
  .setCorpus("file:///" + os.getcwd() + "/../../../src/test/resources/anc-pos-corpus-small/","|")

nerTagger = NerCrfApproach()\
  .setInputCols(["sentence", "token", "pos"])\
  .setLabelColumn("label")\
  .setOutputCol("ner")\
  .setMinEpochs(1)\
  .setMaxEpochs(1)\
  .setLossEps(1e-3)\
  .setEmbeddingsSource("./glove.6B.100d.txt", 100, 2)\
  .setExternalDataset("./eng_small.train")\
  .setL2(1)\
  .setC0(1250000)\
  .setRandomSeed(0)\
  .setVerbose(2)\
  .setIncludeEmbeddings(False)\
  .setEmbeddingsRef("glove100")

finisher = Finisher() \
    .setInputCols(["ner"]) \
    .setIncludeMetadata(True)

pipeline = Pipeline(
    stages = [
    documentAssembler,
    sentenceDetector,
    tokenizer,
    posTagger,
    #nerTagger,
    #finisher
  ])
&lt;/denchmark-code&gt;

my bad, i don't know proper debugging in spark-nlp &amp; spark. so i was checking issue by making comment.
in pipeline, till  tokenizer it is working. below is the sample output.
&lt;denchmark-code&gt;+------+---------+--------------------+--------------------+--------------------+--------------------+
|itemid|sentiment|                text|            document|            sentence|               token|
+------+---------+--------------------+--------------------+--------------------+--------------------+
|     1|        0|                 ...|[[document, 0, 39...|[[document, 0, 27...|[[token, 0, 1, is...|
|     2|        0|                 ...|[[document, 0, 31...|[[document, 0, 29...|[[token, 0, 0, I,...|
|     3|        1|              omg...|[[document, 0, 22...|[[document, 0, 22...|[[token, 0, 2, om...|
|     4|        0|          .. Omga...|[[document, 0, 12...|[[document, 0, 0,...|[[token, 0, 1, .....|
|     5|        0|         i think ...|[[document, 0, 37...|[[document, 0, 33...|[[token, 0, 0, i,...|
|     6|        0|         or i jus...|[[document, 0, 24...|[[document, 0, 24...|[[token, 0, 1, or...|
|     7|        1|       Juuuuuuuuu...|[[document, 0, 33...|[[document, 0, 33...|[[token, 0, 23, J...|
|     8|        0|       Sunny Agai...|[[document, 0, 39...|[[document, 0, 39...|[[token, 0, 4, Su...|

&lt;/denchmark-code&gt;

Thanks.
		</comment>
		<comment id='3' author='munaAchyuta' date='2018-11-20T19:25:01Z'>
		The error going on here is somewhere related between Spark and py4j.
Spark 2.4.x is very recently out, I haven't tried that version myself yet. Any chance you can try back 2.3.1? Also, please make sure your SPARK_HOME variable is properly set to whatever spark directory you are running against in jupyter.
This issue seems to be pointing out to a corrupted environment setting somewhere. Maybe different Spark installations or environment variables pointing to different roots. Taking a look...
		</comment>
		<comment id='4' author='munaAchyuta' date='2018-11-20T19:36:10Z'>
		Thank you, indeed the fault it is Spark 2.4.x.
So, we're officially not yet supporting it. Documentation updated.
		</comment>
		<comment id='5' author='munaAchyuta' date='2018-11-20T22:57:30Z'>
		I just tested my Scala code that uses already trained POS model by spark-nlp against Spark 2.4.0 and it worked. Is this only for Python?
I'll see if I can test more annotators to see if they all pass in Scala at least.
		</comment>
		<comment id='6' author='munaAchyuta' date='2018-11-21T04:38:05Z'>
		Indeed, the problems seems to come mostly from python. But since Spark 2.4.x changes were noticeable, we can't say we support it until we explicitly tested everything with it. It is safer for now to work with Spark 2.3.2.
		</comment>
		<comment id='7' author='munaAchyuta' date='2019-03-27T20:00:56Z'>
		This has been fixed in new releases of spark-nlp against Spark 2.4.0.
		</comment>
	</comments>
</bug>