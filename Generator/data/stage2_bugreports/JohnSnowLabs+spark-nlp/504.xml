<bug id='504' author='CyborgDroid' open_date='2019-05-13T17:02:06Z' closed_time='2019-05-30T18:59:56Z'>
	<summary>BUG: bert_recognize_entities unable to process dataframes</summary>
	<description>
Data:
&lt;denchmark-code&gt;from sparknlp.pretrained import PretrainedPipeline

data = [
  ('Game of Thrones',1,"In the Seven Kingdoms of Westeros, a soldier of the ancient Night's Watch order survives an attack by supernatural creatures known as the White Walkers, thought until now to be mythical."),
  ('Game of Thrones',1,"In King's Landing, the capital, Jon Arryn, the King's Hand, dies under mysterious circumstances."),
  ('Game of Thrones',3,"Tyrion makes saddle modifications for Bran that will allow the paraplegic boy to ride.")
]

df = spark.createDataFrame(data, ['Series','Episode','text'])
&lt;/denchmark-code&gt;

This works fine:
&lt;denchmark-code&gt;pipeline = PretrainedPipeline('recognize_entities_bert', lang='en')

for row in data:
  print(pipeline.annotate(row[2]))
&lt;/denchmark-code&gt;

But the same data in a dataframe does not:
&lt;denchmark-code&gt;pipeline = PretrainedPipeline('recognize_entities_bert', lang='en')

bert_df = pipeline.annotate(df, 'text')
display(bert_df)
&lt;/denchmark-code&gt;

Output:
&lt;denchmark-code&gt;SparkException: Job aborted due to stage failure: Task 1 in stage 19.0 failed 4 times, most recent failure: Lost task 1.3 in stage 19.0 (TID 51, 10.140.64.9, executor 1): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$dfAnnotate$1: (array&lt;array&lt;struct&lt;annotatorType:string,begin:int,end:int,result:string,metadata:map&lt;string,string&gt;,embeddings:array&lt;float&gt;,sentence_embeddings:array&lt;float&gt;&gt;&gt;&gt;) =&gt; array&lt;struct&lt;annotatorType:string,begin:int,end:int,result:string,metadata:map&lt;string,string&gt;,embeddings:array&lt;float&gt;,sentence_embeddings:array&lt;float&gt;&gt;&gt;)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:634)
	at org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:51)
	at org.apache.spark.sql.execution.collect.Collector$$anonfun$2.apply(Collector.scala:148)
	at org.apache.spark.sql.execution.collect.Collector$$anonfun$2.apply(Collector.scala:147)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:139)
	at org.apache.spark.scheduler.Task.run(Task.scala:112)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:496)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1432)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:502)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: org.tensorflow.TensorFlowException: Op type not registered 'BlockLSTM' in binary running on 0214-174126-heron670-10-140-64-9. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1405)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:227)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:74)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:74)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:108)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:78)
	at com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel.getModelIfNotSet(NerDLModel.scala:38)
	at com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel.tag(NerDLModel.scala:42)
	at com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel.annotate(NerDLModel.scala:99)
	at com.johnsnowlabs.nlp.AnnotatorModel$$anonfun$dfAnnotate$1.apply(AnnotatorModel.scala:35)
	at com.johnsnowlabs.nlp.AnnotatorModel$$anonfun$dfAnnotate$1.apply(AnnotatorModel.scala:34)
	... 15 more
Caused by: org.tensorflow.TensorFlowException: Op type not registered 'BlockLSTM' in binary running on 0214-174126-heron670-10-140-64-9. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.
	at org.tensorflow.Graph.importGraphDef(Native Method)
	at org.tensorflow.Graph.importGraphDef(Graph.java:130)
	at org.tensorflow.Graph.importGraphDef(Graph.java:114)
	at com.johnsnowlabs.ml.tensorflow.TensorflowWrapper$.readGraph(TensorflowWrapper.scala:162)
	at com.johnsnowlabs.ml.tensorflow.TensorflowWrapper$.read(TensorflowWrapper.scala:203)
	at com.johnsnowlabs.ml.tensorflow.TensorflowWrapper.readObject(TensorflowWrapper.scala:144)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2178)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$9.apply(TorrentBroadcast.scala:328)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1432)
	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:329)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1$$anonfun$apply$2.apply(TorrentBroadcast.scala:255)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:231)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1398)
	... 25 more

Driver stacktrace:
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 19.0 failed 4 times, most recent failure: Lost task 1.3 in stage 19.0 (TID 51, 10.140.64.9, executor 1): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$dfAnnotate$1: (array&lt;array&lt;struct&lt;annotatorType:string,begin:int,end:int,result:string,metadata:map&lt;string,string&gt;,embeddings:array&lt;float&gt;,sentence_embeddings:array&lt;float&gt;&gt;&gt;&gt;) =&gt; array&lt;struct&lt;annotatorType:string,begin:int,end:int,result:string,metadata:map&lt;string,string&gt;,embeddings:array&lt;float&gt;,sentence_embeddings:array&lt;float&gt;&gt;&gt;)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:634)
	at org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:51)
	at org.apache.spark.sql.execution.collect.Collector$$anonfun$2.apply(Collector.scala:148)
	at org.apache.spark.sql.execution.collect.Collector$$anonfun$2.apply(Collector.scala:147)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:139)
	at org.apache.spark.scheduler.Task.run(Task.scala:112)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:496)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1432)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:502)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: org.tensorflow.TensorFlowException: Op type not registered 'BlockLSTM' in binary running on 0214-174126-heron670-10-140-64-9. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='CyborgDroid' date='2019-05-13T17:51:00Z'>
		Hi &lt;denchmark-link:https://github.com/CyborgDroid&gt;@CyborgDroid&lt;/denchmark-link&gt;
,
We need more information to narrow down the issue. Please provided these information:
Spark NLP version
Apache Spark version
Operating system
How do you initialize Spark session? (sparknlp.start, SparkSession, etc.)
Thank you.
		</comment>
		<comment id='2' author='CyborgDroid' date='2019-05-13T19:45:05Z'>
		Hi Maziyar,
I tried with a small cluster and a GPU cluster on databricks using JohnSnowLabs:spark-nlp:2.0.3 as shown below. Are you able to get it to run with different settings?
&lt;denchmark-link:https://user-images.githubusercontent.com/8703207/57648812-7deffc00-7594-11e9-9ee5-ef8c7bc253fc.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/8703207/57648811-7deffc00-7594-11e9-8a86-40b8c40330b8.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='CyborgDroid' date='2019-05-13T21:19:36Z'>
		Thanks &lt;denchmark-link:https://github.com/CyborgDroid&gt;@CyborgDroid&lt;/denchmark-link&gt;
, just to be sure this is a Bert related issue or TF graphs loading issue on Databricks, would you mind trying  instead and see what happens? (This is based on NerDL + TF + GloVe with a really good F1 score - 93%)
		</comment>
		<comment id='4' author='CyborgDroid' date='2019-05-14T14:56:13Z'>
		Using entity_recognizer_dl yields the same error.
		</comment>
		<comment id='5' author='CyborgDroid' date='2019-05-14T15:00:01Z'>
		I am sorry for DL with GloVe is a different name: .
It is documented here: &lt;denchmark-link:https://nlp.johnsnowlabs.com/docs/en/models&gt;https://nlp.johnsnowlabs.com/docs/en/models&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='CyborgDroid' date='2019-05-14T15:05:19Z'>
		the entity_recognizer_dl gave the same error with dataframes but it did perform better than bert identifying White Walkers as an ORG when looping through the dictionary and feeding just the text.
		</comment>
		<comment id='7' author='CyborgDroid' date='2019-05-14T15:37:53Z'>
		Does it show the results from a DataFrame but with the error?
		</comment>
		<comment id='8' author='CyborgDroid' date='2019-05-14T21:24:34Z'>
		It does not work with dataframes, only when I do this:
pipeline = PretrainedPipeline('entity_recognizer_dl', lang='en')

for row in data: # data is a list
  print(pipeline.annotate(row[2]))
		</comment>
		<comment id='9' author='CyborgDroid' date='2019-05-28T08:34:19Z'>
		@saifjsl The two pipelines entity_recognizer_dl, recognize_entities_bert, and explain_document_dl are crashing with the same error when it's used by DataFrame in 2.0.4 (not sure if this is isolated just to Python only). Any idea?
		</comment>
		<comment id='10' author='CyborgDroid' date='2019-05-28T16:25:01Z'>
		I will take a look, this is related to tensorflow contrib libraries not being loaded properly in databricks cluster.
		</comment>
		<comment id='11' author='CyborgDroid' date='2019-05-28T18:17:14Z'>
		The following equivalent pipelines do not make use of contrib tensorflow modules:
explain_document_dl_noncontrib
recognize_entities_dl_noncontrib
recognize_entities_bert_noncontrib
Please use this while we resolve the issue, accuracy should be similar, if not slightly worse
		</comment>
		<comment id='12' author='CyborgDroid' date='2019-05-29T20:50:08Z'>
		2.0.5 released. Please confirm.
		</comment>
		<comment id='13' author='CyborgDroid' date='2019-05-30T18:59:56Z'>
		Hi Saif. Thank you, the dl pipelines work now.
		</comment>
		<comment id='14' author='CyborgDroid' date='2019-09-23T05:40:18Z'>
		Hello, I had a problem to be solved. It troubled me lots.
python 3.7
tensorflow 1.13
spark-nlp: 2.2.1
spark: 2.4.0
Using PySpark , win10
The error occured:
pipeline = PretrainedPipeline('onto_recognize_entities_sm',lang="en")
Py4JJavaError: An error occurred while calling z:com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.downloadPipeline.
: java.lang.UnsupportedOperationException: Spark NLP tried to load a Tensorflow Graph using Contrib module, but failed to load it on this system. If you are on Windows, this operation is not supported. Please try a noncontrib model. If not the case, please report this issue. Original error message:
Op type not registered 'BlockLSTM' in binary running on LAPTOP-O2LQN8MT. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) tf.contrib.resampler should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.
at com.johnsnowlabs.ml.tensorflow.TensorflowWrapper$.readGraph(TensorflowWrapper.scala:163)
at com.johnsnowlabs.ml.tensorflow.TensorflowWrapper$.read(TensorflowWrapper.scala:202)
at com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel$class.readTensorflowModel(TensorflowSerializeModel.scala:73)
at com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel$.readTensorflowModel(NerDLModel.scala:139)
at com.johnsnowlabs.nlp.annotators.ner.dl.ReadsNERGraph$class.readNerGraph(NerDLModel.scala:117)
at com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel$.readNerGraph(NerDLModel.scala:139)
at com.johnsnowlabs.nlp.annotators.ner.dl.ReadsNERGraph$$anonfun$2.apply(NerDLModel.scala:121)
at com.johnsnowlabs.nlp.annotators.ner.dl.ReadsNERGraph$$anonfun$2.apply(NerDLModel.scala:121)
at com.johnsnowlabs.nlp.ParamsAndFeaturesReadable$$anonfun$com$johnsnowlabs$nlp$ParamsAndFeaturesReadable$$onRead$1.apply(ParamsAndFeaturesReadable.scala:31)
at com.johnsnowlabs.nlp.ParamsAndFeaturesReadable$$anonfun$com$johnsnowlabs$nlp$ParamsAndFeaturesReadable$$onRead$1.apply(ParamsAndFeaturesReadable.scala:30)
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
at com.johnsnowlabs.nlp.ParamsAndFeaturesReadable$class.com$johnsnowlabs$nlp$ParamsAndFeaturesReadable$$onRead(ParamsAndFeaturesReadable.scala:30)
at com.johnsnowlabs.nlp.ParamsAndFeaturesReadable$$anonfun$read$1.apply(ParamsAndFeaturesReadable.scala:41)
at com.johnsnowlabs.nlp.ParamsAndFeaturesReadable$$anonfun$read$1.apply(ParamsAndFeaturesReadable.scala:41)
at com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:19)
at com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:8)
at org.apache.spark.ml.util.DefaultParamsReader$.loadParamsInstance(ReadWrite.scala:652)
at org.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$4.apply(Pipeline.scala:274)
at org.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$4.apply(Pipeline.scala:272)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
at org.apache.spark.ml.Pipeline$SharedReadWrite$.load(Pipeline.scala:272)
at org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:348)
at org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:342)
at com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.downloadPipeline(ResourceDownloader.scala:360)
at com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.downloadPipeline(ResourceDownloader.scala:354)
at com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader$.downloadPipeline(ResourceDownloader.scala:446)
at com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.downloadPipeline(ResourceDownloader.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
at py4j.Gateway.invoke(Gateway.java:282)
at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
at py4j.commands.CallCommand.execute(CallCommand.java:79)
at py4j.GatewayConnection.run(GatewayConnection.java:238)
at java.lang.Thread.run(Thread.java:748)
I really hope you can help!
Thanks!
		</comment>
		<comment id='15' author='CyborgDroid' date='2019-09-23T06:11:24Z'>
		Hi &lt;denchmark-link:https://github.com/xiemeigongzi88&gt;@xiemeigongzi88&lt;/denchmark-link&gt;

This is not a related place for your issue, but a short answer is that Onto pipelines donâ€™t support Windows, only Linux and macOS. Please use any pipeline that has _noncontrib inthe name if you wish to use it in Windows.
		</comment>
	</comments>
</bug>