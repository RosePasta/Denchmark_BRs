<bug id='745' author='drpraxis' open_date='2020-01-15T18:05:07Z' closed_time='2020-04-02T18:12:55Z'>
	<summary>java.io.UTFDataFormatException: encoded string too long - deploy mode cluster</summary>
	<description>
We are getting "java.io.UTFDataFormatException: encoded string too long" while running "model.transform()" when the job is submitted as --deploy-mode cluster
org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:688) Caused by: java.io.UTFDataFormatException: encoded string too long: 76663 bytes at java.io.DataOutputStream.writeUTF(DataOutputStream.java:364) at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323) at
Spark version: 2.3.0
Spark nlp: 2.2.1
Scla: 2.11
When we run the spark job as "--deploy-mode client" it works fine.
Detailed stack trace:
User class threw exception: org.apache.spark.SparkException: Task not serializable at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:345) at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:335) at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:159) at org.apache.spark.SparkContext.clean(SparkContext.scala:2299) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:844) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:843) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) at org.apache.spark.rdd.RDD.withScope(RDD.scala:363) at org.apache.spark.rdd.RDD.mapPartitionsWithIndex(RDD.scala:843) at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:608) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127) at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247) at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:337) at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38) at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273) at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484) at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484) at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253) at org.apache.spark.sql.Dataset.head(Dataset.scala:2484) at org.apache.spark.sql.Dataset.head(Dataset.scala:2491) at org.apache.spark.sql.Dataset.first(Dataset.scala:2498) at org.apache.spark.ml.feature.VectorAssembler.first$lzycompute$1(VectorAssembler.scala:57) at org.apache.spark.ml.feature.VectorAssembler.org$apache$spark$ml$feature$VectorAssembler$$first$1(VectorAssembler.scala:57) at org.apache.spark.ml.feature.VectorAssembler$$anonfun$2$$anonfun$1.apply$mcI$sp(VectorAssembler.scala:88) at org.apache.spark.ml.feature.VectorAssembler$$anonfun$2$$anonfun$1.apply(VectorAssembler.scala:88) at org.apache.spark.ml.feature.VectorAssembler$$anonfun$2$$anonfun$1.apply(VectorAssembler.scala:88) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.ml.feature.VectorAssembler$$anonfun$2.apply(VectorAssembler.scala:88) at org.apache.spark.ml.feature.VectorAssembler$$anonfun$2.apply(VectorAssembler.scala:58) at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241) at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241) at scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:186) at org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:58) at org.apache.spark.ml.PipelineModel$$anonfun$transform$1.apply(Pipeline.scala:306) at org.apache.spark.ml.PipelineModel$$anonfun$transform$1.apply(Pipeline.scala:306) at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57) at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66) at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:186) at org.apache.spark.ml.PipelineModel.transform(Pipeline.scala:306) at com.tr.ccc.la.dockets.de.inference.functions.EntryPipelineFunctions.prep_and_discover_motion_orders(PipelineFunctions.scala:36) at com.tr.ccc.la.dockets.de.inference.SparkApp$.main(SparkApp.scala:45) at com.tr.ccc.la.dockets.de.inference.SparkApp.main(SparkApp.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:688) Caused by: java.io.UTFDataFormatException: encoded string too long: 76663 bytes at java.io.DataOutputStream.writeUTF(DataOutputStream.java:364) at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323) at com.typesafe.config.impl.SerializedConfigValue.writeValueData(SerializedConfigValue.java:312) at com.typesafe.config.impl.SerializedConfigValue.writeValue(SerializedConfigValue.java:386) at com.typesafe.config.impl.SerializedConfigValue.writeValueData(SerializedConfigValue.java:326) at com.typesafe.config.impl.SerializedConfigValue.writeValue(SerializedConfigValue.java:386) at com.typesafe.config.impl.SerializedConfigValue.writeValueData(SerializedConfigValue.java:326) at com.typesafe.config.impl.SerializedConfigValue.writeValue(SerializedConfigValue.java:386) at com.typesafe.config.impl.SerializedConfigValue.writeValueData(SerializedConfigValue.java:326) at com.typesafe.config.impl.SerializedConfigValue.writeValue(SerializedConfigValue.java:386) at com.typesafe.config.impl.SerializedConfigValue.writeExternal(SerializedConfigValue.java:452) at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348) at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43) at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100) at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:342) ...
	</description>
	<comments>
		<comment id='1' author='drpraxis' date='2020-01-15T18:58:33Z'>
		Yes this is a known issue, currently some annotators are not compatible with cluster mode in YARN.
We need to investigate more.
		</comment>
		<comment id='2' author='drpraxis' date='2020-01-15T19:10:21Z'>
		Thanks &lt;denchmark-link:https://github.com/maziyarpanahi&gt;@maziyarpanahi&lt;/denchmark-link&gt;
 for the quick response. Would you be able to provide rough idea when we can expect more info on this.
Also, are there any shortcuts we can take to get this working in cluster mode while detailed investigation is being performed?
		</comment>
		<comment id='3' author='drpraxis' date='2020-03-19T18:34:06Z'>
		We have made some progress and made some tests in Cloudera YARN cluster-mode and Databricks. The results are here: &lt;denchmark-link:https://github.com/JohnSnowLabs/spark-nlp/issues/832&gt;#832&lt;/denchmark-link&gt;

Could you please make a fat JAR from this PR and give it a go:
&lt;denchmark-link:https://github.com/JohnSnowLabs/spark-nlp/pull/834&gt;#834&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='drpraxis' date='2020-04-02T18:12:55Z'>
		This has been resolved in 2.4.5 release: &lt;denchmark-link:https://github.com/JohnSnowLabs/spark-nlp/releases/tag/2.4.5&gt;https://github.com/JohnSnowLabs/spark-nlp/releases/tag/2.4.5&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>