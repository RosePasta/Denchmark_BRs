<bug id='1154' author='vkocaman' open_date='2020-11-11T16:08:01Z' closed_time='2021-01-04T20:42:05Z'>
	<summary>NerDL eval takes more time than a training epoch</summary>
	<description>
Is your feature request related to a problem? Please describe.
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]
Describe the solution you'd like
A clear and concise description of what you want to happen.
Describe alternatives you've considered
A clear and concise description of any alternative solutions or features you've considered.
Additional context
Add any other context or screenshots about the feature request here.
	</description>
	<comments>
		<comment id='1' author='vkocaman' date='2020-11-11T16:10:12Z'>
		&lt;denchmark-link:https://github.com/maziyarpanahi&gt;@maziyarpanahi&lt;/denchmark-link&gt;
 when we start training, it grabs all the cores while fitting but when it comes to evaluation on val split after each epoch, it drops to a single core (probably due to single row prediction issue). I checked Saif's PR for bathc prediction but it is not there as well.. please see below.. one epoch takes 170 s but eval takes 1413 s.
&lt;denchmark-code&gt;Training started - total epochs: 50 - lr: 0.003 - batch size: 1024 - labels: 3 - chars: 94 - training examples: 49025
Epoch 1/50 started, lr: 0.003, dataset size: 49025
Epoch 1/50 - 179.65s - loss: 530.2455 - batches: 51
Quality on validation dataset (20.0%), validation examples = 9805
time to finish evaluation: 1117.67s
label	 tp	 fp	 fn	 prec	 rec	 f1
B-CHEM	 3598	 1735	 8540	 0.6746672	 0.29642445	 0.41188255
I-CHEM	 414	 28	 14857	 0.9366516	 0.02711021	 0.052695222
tp: 4012 fp: 1763 fn: 23397 labels: 2
Macro-average	 prec: 0.8056594, rec: 0.16176733, f1: 0.2694351
Micro-average	 prec: 0.6947186, rec: 0.14637528, f1: 0.24180327
Epoch 2/50 started, lr: 0.0029850747, dataset size: 49025
Epoch 2/50 - 169.75s - loss: 215.63585 - batches: 51
Quality on validation dataset (20.0%), validation examples = 9805
time to finish evaluation: 1413.23s
label	 tp	 fp	 fn	 prec	 rec	 f1
B-CHEM	 6511	 2401	 5627	 0.73058796	 0.53641456	 0.6186223
I-CHEM	 7750	 1538	 7521	 0.83441	 0.50749785	 0.6311332
tp: 14261 fp: 3939 fn: 13148 labels: 2
Macro-average	 prec: 0.78249896, rec: 0.5219562, f1: 0.62620807
Micro-average	 prec: 0.7835714, rec: 0.52030355, f1: 0.62535906
Epoch 3/50 started, lr: 0.0029702971, dataset size: 49025
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='vkocaman' date='2020-11-11T16:14:14Z'>
		&lt;denchmark-link:https://github.com/vkocaman&gt;@vkocaman&lt;/denchmark-link&gt;
 If what you are testing here is in 2.6.3 then something is not right in the new changes. &lt;denchmark-link:https://github.com/albertoandreottiATgmail&gt;@albertoandreottiATgmail&lt;/denchmark-link&gt;
 please take a look because there shouldn't be any single-core operations regardless of single row per annotation. Also, the time calculation can be off as well if it's not set at the right places.
&lt;denchmark-link:https://github.com/vkocaman&gt;@vkocaman&lt;/denchmark-link&gt;
 Could you please check the same thing in 2.6.2 to be sure this only an issue in 2.6.3 new implementation?
PS: The prediction in the evaluation of 10% cannot take longer than training the 100% of training example, this is a bug
For some references as why: &lt;denchmark-link:https://github.com/JohnSnowLabs/spark-nlp/pull/1085#issuecomment-725538200&gt;#1085 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='vkocaman' date='2020-11-11T20:06:27Z'>
		Hey this is potentially due to the use of spark randomSplit() API. This needs more testing.
Will try a couple of things to see if I can put this to run in a reasonable way.
Thanks for reporting.
		</comment>
		<comment id='4' author='vkocaman' date='2020-11-11T22:01:54Z'>
		Hi &lt;denchmark-link:https://github.com/maziyarpanahi&gt;@maziyarpanahi&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/vkocaman&gt;@vkocaman&lt;/denchmark-link&gt;
 , by single core operation I understand the NER not using all the cores during evaluation, correct? I saw almost complete CPU utilization in my 12 cores laptop.
The process during evaluation is pretty similar in terms of batching, so the same size of batches is used both in training/evaluation. I tested this with the following results,
Training started - total epochs: 30 - lr: 0.001 - batch size: 32 - labels: 8 - chars: 84 - training examples: 11234
Epoch 1/30 started, lr: 0.001, dataset size: 11234
Epoch 1/30 - 236.53s - loss: 1109.013 - batches: 352
Quality on validation dataset (20.0%), validation examples = 2246
time to finish evaluation: 12.50s
Macro-average	 prec: 0.58635503, rec: 0.53799, f1: 0.56113225
Micro-average	 prec: 0.85821867, rec: 0.8457257, f1: 0.8519264
Epoch 2/30 started, lr: 9.950249E-4, dataset size: 11234
Epoch 2/30 - 70.73s - loss: 453.32202 - batches: 352
Quality on validation dataset (20.0%), validation examples = 2246
time to finish evaluation: 6.86s
Macro-average	 prec: 0.5924051, rec: 0.5795302, f1: 0.58589685
Micro-average	 prec: 0.9003517, rec: 0.87683743, f1: 0.888439
Epoch 3/30 started, lr: 9.90099E-4, dataset size: 11234
Epoch 3/30 - 68.35s - loss: 345.98492 - batches: 352
Quality on validation dataset (20.0%), validation examples = 2246
time to finish evaluation: 6.83s
Macro-average	 prec: 0.6153839, rec: 0.5963099, f1: 0.6056968
Micro-average	 prec: 0.9267725, rec: 0.90852004, f1: 0.91755545
Epoch 4/30 started, lr: 9.852217E-4, dataset size: 11234
Epoch 4/30 - 67.38s - loss: 282.5838 - batches: 352
Quality on validation dataset (20.0%), validation examples = 2246
time to finish evaluation: 6.72s
Macro-average	 prec: 0.6214704, rec: 0.60556525, f1: 0.6134147
Micro-average	 prec: 0.9331113, rec: 0.9197945, f1: 0.9264051
Epoch 5/30 started, lr: 9.803922E-4, dataset size: 11234
Epoch 5/30 - 66.62s - loss: 237.84218 - batches: 352
Quality on validation dataset (20.0%), validation examples = 2246
time to finish evaluation: 6.56s
Macro-average	 prec: 0.6189223, rec: 0.6155269, f1: 0.61722
Micro-average	 prec: 0.9361427, rec: 0.9289282, f1: 0.9325215
Epoch 6/30 started, lr: 9.756098E-4, dataset size: 11234
Epoch 6/30 - 67.00s - loss: 203.94446 - batches: 352
Quality on validation dataset (20.0%), validation examples = 2246
time to finish evaluation: 6.65s
Macro-average	 prec: 0.61861926, rec: 0.6136146, f1: 0.61610675
Micro-average	 prec: 0.92885655, rec: 0.9297845, f1: 0.9293203
Epoch 7/30 started, lr: 9.7087387E-4, dataset size: 11234
sometimes at the beginning measurements go high, not sure why, but in other epochs they seem to get more stable.
What I actually found is a problem in the testing dataset which I fixed.
I added that fix to the fix_parameter branch, which is the same code I used for testing this.
Please make sure you're using that code.
		</comment>
		<comment id='5' author='vkocaman' date='2020-11-11T22:56:38Z'>
		&lt;denchmark-link:https://github.com/albertoandreottiATgmail&gt;@albertoandreottiATgmail&lt;/denchmark-link&gt;
 what we see in AWS was different.. what I see from your experiments is what we need.. so I will try that with the fixed code.. thanks for checking this out !
		</comment>
		<comment id='6' author='vkocaman' date='2021-01-04T20:42:05Z'>
		This should be fixed in the 2.7.0 release.
pip install spark-nlp==2.7.0 --upgrade
		</comment>
	</comments>
</bug>