<bug id='189' author='superman24-7' open_date='2018-04-30T17:05:27Z' closed_time='2018-05-04T14:31:02Z'>
	<summary>SentenceDetector Failed to find a default value for useCustomBoundsOnly</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

In attempting to use the Named Entity Recognition Deep Learning annotator (NerDLApproach)  I get the exception
&lt;denchmark-code&gt;Caused by: java.util.NoSuchElementException: Failed to find a default value for useCustomBoundsOnly
	at org.apache.spark.ml.param.Params$$anonfun$getOrDefault$2.apply(params.scala:780)
	at org.apache.spark.ml.param.Params$$anonfun$getOrDefault$2.apply(params.scala:780)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.param.Params$class.getOrDefault(params.scala:779)
	at org.apache.spark.ml.PipelineStage.getOrDefault(Pipeline.scala:42)
	at org.apache.spark.ml.param.Params$class.$(params.scala:786)
	at org.apache.spark.ml.PipelineStage.$(Pipeline.scala:42)
	at com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector.model$lzycompute(SentenceDetector.scala:46)
	at com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector.model(SentenceDetector.scala:45)
	at com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector.tag(SentenceDetector.scala:54)
	at com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector$$anonfun$2.apply(SentenceDetector.scala:66)
	at com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector$$anonfun$2.apply(SentenceDetector.scala:66)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector.annotate(SentenceDetector.scala:66)
	at com.johnsnowlabs.nlp.AnnotatorModel$$anonfun$dfAnnotate$1.apply(AnnotatorModel.scala:43)
	at com.johnsnowlabs.nlp.AnnotatorModel$$anonfun$dfAnnotate$1.apply(AnnotatorModel.scala:42)
	at org.apache.spark.sql.catalyst.expressions.ScalaUDF$$anonfun$2.apply(ScalaUDF.scala:102)
	at org.apache.spark.sql.catalyst.expressions.ScalaUDF$$anonfun$2.apply(ScalaUDF.scala:101)
	at org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1055)
	... 50 more
&lt;/denchmark-code&gt;

This stems from the SentenceDetector that I have included in my pipeline. I have also managed to replicate the issue using the below code:
&lt;denchmark-link:https://github.com/JohnSnowLabs/spark-nlp/blob/84d338d342485abba01e7aa1a94a7b5064c6bd4e/src/test/scala/com/johnsnowlabs/benchmarks/spark/NerDLCoNLL2003.scala&gt;https://github.com/JohnSnowLabs/spark-nlp/blob/84d338d342485abba01e7aa1a94a7b5064c6bd4e/src/test/scala/com/johnsnowlabs/benchmarks/spark/NerDLCoNLL2003.scala&lt;/denchmark-link&gt;

I have attempted to manually set the value as below with the same exception still occuring:
&lt;denchmark-code&gt;    val sentenceDetector = new SentenceDetector()
      .setCustomBounds(Array("\n\n", "\r\n\r\n"))
      .setUseCustomBoundsOnly(false)
      .setInputCols(Array("document"))
      .setOutputCol("sentence")
&lt;/denchmark-code&gt;

This looks like a similar issue to:
&lt;denchmark-link:https://github.com/high-performance-spark/high-performance-spark-examples/issues/89&gt;high-performance-spark/high-performance-spark-examples#89&lt;/denchmark-link&gt;

Looking at the SentenceDetector.scala class default values seem to be set but not for the useCustomBoundsOnly value.
&lt;denchmark-code&gt;  setDefault(
    inputCols -&gt; Array(DOCUMENT),
    useAbbrevations -&gt; false,
    customBounds -&gt; Array.empty[String]
  )
&lt;/denchmark-code&gt;

I would assume we need to modify this to the below:
&lt;denchmark-code&gt;  setDefault(
    inputCols -&gt; Array(DOCUMENT),
    useAbbrevations -&gt; false,
    useCustomBoundsOnly -&gt; false,
    customBounds -&gt; Array.empty[String]
  )
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;


Version used: spark-nlp 1.5.1 apache spark 2.3.0 scala 2.11.12
Operating System and version (desktop or mobile): macOS 10.13.4

	</description>
	<comments>
		<comment id='1' author='superman24-7' date='2018-05-02T12:37:14Z'>
		Hello &lt;denchmark-link:https://github.com/superman24-7&gt;@superman24-7&lt;/denchmark-link&gt;

I am sorry I missed this! You are indeed right and this is not failing in our tests only because we test it in Python, and python has such default value. What calls my attention is why does it still fail after setting it? Going to take a look.
Release 1.5.2 had some flaws so we'll be releasing a hotfix shortly and will very probably fix this.
By curiosity, why did you start with such example, and not use a pretrained NerDL instead?
		</comment>
		<comment id='2' author='superman24-7' date='2018-05-02T13:18:47Z'>
		I have tested and, even though the error is correct, for me it works if I call setUseCustomBoundsOnly(false) on the SentenceDetector.
		</comment>
		<comment id='3' author='superman24-7' date='2018-05-02T13:22:17Z'>
		&lt;denchmark-link:https://github.com/JohnSnowLabs/spark-nlp/pull/192&gt;#192&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='superman24-7' date='2018-05-02T19:53:13Z'>
		Thanks @saifjsl.
After further testing, calling setUseCustomBoundsOnly(false) on the SentenceDetector succeeds with:
&lt;denchmark-code&gt;import com.johnsnowlabs.nlp.RecursivePipeline

val pipeline = new RecursivePipeline()
      .setStages(stages)
&lt;/denchmark-code&gt;

but fails with the exception when declaring the pipeline as:
&lt;denchmark-code&gt;import org.apache.spark.ml.Pipeline  
  
val pipeline = new Pipeline()
      .setStages(stages)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='superman24-7' date='2018-05-02T21:42:24Z'>
		We'll be shortly releasing 1.5.3 with the param default value fix. after this it should work fine.
		</comment>
		<comment id='6' author='superman24-7' date='2018-05-04T14:31:02Z'>
		Should be fixed now
		</comment>
	</comments>
</bug>