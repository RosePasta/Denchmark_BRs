<bug id='1566' author='karlinjf' open_date='2020-04-22T21:32:23Z' closed_time='2020-04-23T18:28:21Z'>
	<summary>Batch being moved to gpu repeatedly with multiple optimizers and single gpu training</summary>
	<description>
If you have multiple optimizers, then transfer_batch_to_gpu winds up getting called once per opt_idx, and the batch is copied each time via copy.copy(batch) in training_forward. Why copy the batch when there is only a single gpu? By removing the copy.copy() my GAN model moves from 8.53it/s to 9.25it/s. Pretty significant speedup.
	</description>
	<comments>
		<comment id='1' author='karlinjf' date='2020-04-22T22:15:11Z'>
		amazing find! mind submitting a PR?
		</comment>
		<comment id='2' author='karlinjf' date='2020-04-22T22:44:28Z'>
		Sure, will do.
		</comment>
	</comments>
</bug>