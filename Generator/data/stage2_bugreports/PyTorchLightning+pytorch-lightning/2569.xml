<bug id='2569' author='YuxianMeng' open_date='2020-07-09T13:45:38Z' closed_time='2020-07-28T21:39:57Z'>
	<summary>logging loss bug when accumulate_grad_batches &amp;gt;= 1</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When  accumulate_grad_batches &gt;= 1, logging loss is divided by accumulate_grad_batches.
For example, when accumulate_grad_batches=1, logging loss is 4; when accumulate_grad_batches=2, loss is 2; accumulate_grad_batches=4, loss is 1
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

logging loss should be similar no matter accumulate_grad_batch is any number.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version (e.g., 1.0): nightly
OS (e.g., Linux): Linux
How you installed PyTorch (conda, pip, source): pip
Build command you used (if compiling from source):
Python version: 3.6
CUDA/cuDNN version: 10.2
GPU models and configuration:
Any other relevant information:

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='YuxianMeng' date='2020-07-28T21:39:57Z'>
		Fixed in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2738&gt;#2738&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>