<bug id='1223' author='lobantseff' open_date='2020-03-24T14:24:01Z' closed_time='2020-05-31T12:31:22Z'>
	<summary>gan.py multi-gpu running problems</summary>
	<description>
Running &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/domain_templates/gan.py&gt;gan.py&lt;/denchmark-link&gt;
 example with Trainer(ngpus=2) causes two types of error:

if Trainer(ngpus=2, distributed_backend='dp')

&lt;denchmark-code&gt;Exception has occurred: AttributeError
'NoneType' object has no attribute 'detach'
  File "/home/user/gan.py", line 146, in training_step
    self.discriminator(self.generated_imgs.detach()), fake)
&lt;/denchmark-code&gt;


if Trainer(ngpus=2, distributed_backend='ddp')


in ./lightling_logs one run creates two folders: version_0 and version_1
Exception caused:
File "/opt/miniconda3/envs/ctln-gan/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 122, in _del_model
os.remove(filepath)
FileNotFoundError: [Errno 2] No such file or directory: '/home/user/pyproj/DCGAN/lightning_logs/version_1/checkpoints/epoch=0.ckpt'

it seems that each subprocess tries to create its own checkpoints and delete not ctrated one.
&lt;denchmark-h:h4&gt;Environment version:&lt;/denchmark-h&gt;

python 3.7.5
pytorch 1.4.0
pytorch-lightning 0.7.1
	</description>
	<comments>
		<comment id='1' author='lobantseff' date='2020-03-24T14:24:41Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='lobantseff' date='2020-03-24T16:49:10Z'>
		The problem is that gan.py example suppose to use buffered values self.generated_images and self.last_img, however during replicating and gathering in 


pytorch-lightning/pytorch_lightning/overrides/data_parallel.py


         Line 64
      in
      22a7264






 replicas = self.replicate(self.module, self.device_ids[:len(inputs)]) 





buffered values are not replicated and not gathered to main LightningModule model
		</comment>
		<comment id='3' author='lobantseff' date='2020-03-27T12:01:55Z'>
		@armavox good catch, mind draft a PR? ðŸ¤–
		</comment>
		<comment id='4' author='lobantseff' date='2020-03-29T22:14:14Z'>
		yep, I'll try
		</comment>
		<comment id='5' author='lobantseff' date='2020-04-14T17:29:01Z'>
		@armavox how is it going?
		</comment>
		<comment id='6' author='lobantseff' date='2020-04-22T09:43:20Z'>
		&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
  I assume to fix it by May
		</comment>
		<comment id='7' author='lobantseff' date='2020-05-28T14:00:50Z'>
		@armavox Any updates on this? Having the same issue...
		</comment>
		<comment id='8' author='lobantseff' date='2020-05-30T15:50:34Z'>
		Made some updates. Sorry for waiting.
There is an official l warning about the use of local (buffered here) variables during the distributed training: &lt;denchmark-link:https://pytorch.org/docs/stable/nn.html#torch.nn.DataParallel&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.DataParallel&lt;/denchmark-link&gt;

So I didn't try to create detours in the Lightning code and fixed only the example to work with dp and ddp.
		</comment>
		<comment id='9' author='lobantseff' date='2020-05-30T15:55:50Z'>
		The problem from point 2 in the heading post seems to be fixed by someone. But the unused folder for parallel experiment still created during ddp training. The problem is in 


pytorch-lightning/pytorch_lightning/trainer/callback_config.py


         Line 66
      in
      fdbbe96






 os.makedirs(ckpt_path, exist_ok=True) 




, which doesn't use rank_zero_only decorator or something else.
I would propose some good fixes, but don't know how to do this elegant.
Thanks for your work!
Best regards, Artem.
		</comment>
	</comments>
</bug>