<bug id='2942' author='ananthsub' open_date='2020-08-13T04:47:43Z' closed_time='2020-08-14T09:37:22Z'>
	<summary>ddp_backend in 0.9.0rc12 fails if no CUDA_VISIBLE_DEVICES found</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

In ddp_backend, training immediately fails if the environment variable CUDA_VISIBLE_DEVICES isn't set. This line should handle the None case gracefully: &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/accelerators/ddp_backend.py#L90&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/accelerators/ddp_backend.py#L90&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Start a run using ddp on CPU. This was discovered using torchelastic to launch
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

This shouldn't crash if the environment variable isn't set. We could default to num_gpus = 0 in this case.
Replacing the line above with something like this could work:
num_gpus = os.environ.get('CUDA_VISIBLE_DEVICES', []).split(',').__len__() 
	</description>
	<comments>
		<comment id='1' author='ananthsub' date='2020-08-13T05:52:19Z'>
		This is a bug in our usage of distributed backend, not an issue with lightning
		</comment>
		<comment id='2' author='ananthsub' date='2020-08-13T06:26:22Z'>
		&lt;denchmark-link:https://github.com/ananthsub&gt;@ananthsub&lt;/denchmark-link&gt;
 good catch, mind send a PR?
		</comment>
		<comment id='3' author='ananthsub' date='2020-08-13T17:30:00Z'>
		&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
  reading more about the accelerator changes, I am confused about the split between ddp and ddp_cpu backends. For example, if the user wants to launch training with torchelastic or a non-slurm launcher, then we use the ddp backend. This will fail if the training is on CPUs. The docs aren't updated with the latest changes either: &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html#distributed-modes&gt;https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html#distributed-modes&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='ananthsub' date='2020-08-13T21:04:16Z'>
		the ddp is GPU based compare to ddp_cpu is CPU only...
		</comment>
		<comment id='5' author='ananthsub' date='2020-08-13T23:21:35Z'>
		&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 in our test case, we launch with these trainer parameters:

and this is the stacktrace we run into:
&lt;denchmark-code&gt;    trainer.fit(self)
  File "pytorch_lightning/trainer/states.py", line 34, in wrapped_fn
    result = fn(self, *args, **kwargs)
  File "pytorch_lightning/trainer/trainer.py", line 1030, in fit
    results = self.accelerator_backend.spawn_ddp_children(model)
  File "pytorch_lightning/accelerators/ddp_backend.py", line 90, in spawn_ddp_children
    num_gpus = os.environ['CUDA_VISIBLE_DEVICES'].split(',').__len__()
  File "/usr/lib/python3.7/os.py", line 679, in __getitem__
    raise KeyError(key) from None
KeyError: 'CUDA_VISIBLE_DEVICES'
&lt;/denchmark-code&gt;

Should this be detected by the config validator earlier?
		</comment>
	</comments>
</bug>