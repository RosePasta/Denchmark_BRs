<bug id='2155' author='pamparana34' open_date='2020-06-11T22:41:34Z' closed_time='2020-06-17T14:32:59Z'>
	<summary>Error with num GPUs when using 0.8.0rc1</summary>
	<description>
I am trying to run my trainer while utilising 4 GPUs on my system.
I launch my trainer as follows:
&lt;denchmark-code&gt;trainer = Trainer(gpus=-1, distributed_backend='ddp')
&lt;/denchmark-code&gt;

This somehow goes into a tizzy and compares about some hyper parameter that was not even added by me. So the error is:
&lt;denchmark-code&gt;train_siamnet.py: error: unrecognized arguments: --gpus 4
&lt;/denchmark-code&gt;

The thing is that I did not pass this argument, so this is something probably added by PL somewhere down the line.
The model that I use and the launch script is as here:
&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2069&gt;#2069&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='pamparana34' date='2020-06-11T23:32:14Z'>
		So it seems one solution is to have the --gpus parameter exposed in the training file and basically ignore it from your end and let PL fill it while calling the script. Is that documented somewhere or is this not something I should be doing?
		</comment>
		<comment id='2' author='pamparana34' date='2020-06-12T20:11:21Z'>
		
So it seems one solution is to have the --gpus parameter exposed in the training file and basically ignore it from your end and let PL fill it while calling the script. Is that documented somewhere or is this not something I should be doing?

I'm also having this issue in 0.8.0rc1! Can you describe more how you fixed this?
		</comment>
		<comment id='3' author='pamparana34' date='2020-06-12T20:14:03Z'>
		The ddp mode now (since 0.8.0) launches your script several times. It expects you to have an argument parser with that gpus argument. It will call your script internally with
python train_siamnet.py --gpus 4, right &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 ?
so the fix is to add this argument to your script and pass it to Trainer.
		</comment>
		<comment id='4' author='pamparana34' date='2020-06-12T20:14:53Z'>
		or use distributed_backend="ddp_spawn" which can be slower.
		</comment>
		<comment id='5' author='pamparana34' date='2020-06-17T14:20:15Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 is this solved in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2212&gt;#2212&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='6' author='pamparana34' date='2020-06-17T14:32:59Z'>
		yes
		</comment>
	</comments>
</bug>