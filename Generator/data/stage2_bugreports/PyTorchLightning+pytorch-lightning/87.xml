<bug id='87' author='AS-researcher6' open_date='2019-08-10T01:35:41Z' closed_time='2019-08-10T12:32:47Z'>
	<summary>Incorrect Implementation for Accumulating Batch Gradients in Trainer</summary>
	<description>
Current Behavior:
If accumulate_grad_batches &gt; default of 1, the Trainer will proceed to take the loss from each batch and run loss.backward() for each batch accumulated, running the optimizer.step() once the desired number of batches has undergone backprop.
Loss averaging is only done for batch_loss_value.
Correct Behavior:
The loss from the output needs to be divided by accumulate_grad_batches before loss.backward() is run, otherwise the overall magnitude of the gradient could be up to N times greater for a simulated batch size N times bigger than the actual.
	</description>
	<comments>
		<comment id='1' author='AS-researcher6' date='2019-08-10T02:05:08Z'>
		&lt;denchmark-link:https://github.com/AS-researcher6&gt;@AS-researcher6&lt;/denchmark-link&gt;
 good find, i see the bug. the division is done once the step is applied, but the division should be on line 926.
Current order:

clip
step
zero_grad
divide accumulated loss by nb accumulated batches

Correct order:

divide accumulated loss by nb accumulated batches
clip
step
zero_grad

Submitting the change.
Mind sanity checking PR &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/88&gt;#88&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='AS-researcher6' date='2019-08-10T03:08:37Z'>
		Gladly! Not sure if you're done fixing things but I don't see the commit where the loss is divided by self.accumulate_grad_batches before the loss.backward(). Otherwise the original version where self.batch_loss_value += loss.item() is good as long as the self.batch_loss_value is not averaged after.
		</comment>
		<comment id='3' author='AS-researcher6' date='2019-08-10T03:29:11Z'>
		look at the PR &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/88&gt;#88&lt;/denchmark-link&gt;

line 928
		</comment>
		<comment id='4' author='AS-researcher6' date='2019-08-10T03:49:58Z'>
		Sorry, I don't think it's quite right yet. Accounting for multiple batches needs to happen in the actual loss itself before backpropagation. Otherwise, its like N accumulated additions of loss.backward() rather than N accumulated additions of (1/N * loss).backward()
After line 898 and before loss.backward() on the 903 if statement, there needs to be a line that's loss = loss / self.accumulate_grad_batches. Line 922 should still be self.batch_loss_value += loss.item(). Line 928 can be removed entirely, since the averaging has already been accounted for.
		</comment>
		<comment id='5' author='AS-researcher6' date='2019-08-10T11:25:07Z'>
		&lt;denchmark-link:https://github.com/AS-researcher6&gt;@AS-researcher6&lt;/denchmark-link&gt;
 i see. updated. Was following the wrong approach before.
How about now?
		</comment>
		<comment id='6' author='AS-researcher6' date='2019-08-10T12:33:43Z'>
		&lt;denchmark-link:https://github.com/AS-researcher6&gt;@AS-researcher6&lt;/denchmark-link&gt;
 merged to master. should be correct now. pls verify
		</comment>
		<comment id='7' author='AS-researcher6' date='2019-08-10T19:44:16Z'>
		All good. Thanks for the fixes!
Working on a SLURM cluster myself so may submit pull requests for expanded Trainer functionality and bring up more things in the future.
		</comment>
	</comments>
</bug>