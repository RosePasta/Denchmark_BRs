<bug id='2683' author='asrafulashiq' open_date='2020-07-24T00:42:02Z' closed_time='2020-08-16T15:19:58Z'>
	<summary>trainer.test not working in ddp</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Testing in ddp is not working in the latest master.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

I am using the gpu_template example from basic_examples in the repo : "python gpu_template.py --gpus 2 --distributed_backend ddp", where, instead of trainer.fit(model), I am using trainer.test(model).
I am getting "RuntimeError: connect() timed out".
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version 1.3.1:
Ubuntu 18.04

	</description>
	<comments>
		<comment id='1' author='asrafulashiq' date='2020-07-24T00:42:52Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='asrafulashiq' date='2020-07-24T14:41:02Z'>
		I'm facing a similar issue to &lt;denchmark-link:https://github.com/asrafulashiq&gt;@asrafulashiq&lt;/denchmark-link&gt;
 . I trained a model using ddp across 4gpus on our institution's slurm cluster. When running trainer.test(model) it seems like the script freezes. Help on this would be greatly appreciated!!  Here is a screenshot of the output of my slurm script for reference.
&lt;denchmark-link:https://user-images.githubusercontent.com/39134049/88403082-0756ad00-cd9a-11ea-8937-cd7d61f607fc.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='asrafulashiq' date='2020-07-26T12:19:44Z'>
		the same issue
		</comment>
		<comment id='4' author='asrafulashiq' date='2020-08-01T12:02:41Z'>
		can confirm, it freezes, but only in ddp. For everyone who faces this issue, use ddp_spawn until this is fixed.
		</comment>
		<comment id='5' author='asrafulashiq' date='2020-08-01T12:19:37Z'>
		I've identified that the process hangs at this line:



pytorch-lightning/pytorch_lightning/core/lightning.py


         Line 958
      in
      f9ccb0f






 torch_distrib.init_process_group(torch_backend, rank=global_rank, world_size=world_size) 





		</comment>
		<comment id='6' author='asrafulashiq' date='2020-08-01T13:19:05Z'>
		Ok, more progress. The issue is caused because the master port is different on each rank. If we set the port manually, problem is solved.
		</comment>
	</comments>
</bug>