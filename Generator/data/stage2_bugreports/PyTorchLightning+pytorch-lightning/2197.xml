<bug id='2197' author='sudarshan85' open_date='2020-06-15T19:55:38Z' closed_time='2020-06-15T21:15:24Z'>
	<summary>OmegaConf/hydra error unsupported `0.8.0rc2`</summary>
	<description>
Hi,
I am using Hydra to setup my configurations variables for my trainer and model parameters. I'm using PL version 0.8.0rc2. When I pass the params to my PL module and set the passed params to self.hparams, I get the following error:
&lt;denchmark-code&gt;ValueError: Unsupported config type of &lt;class 'omegaconf.dictconfig.DictConfig'&gt;.
&lt;/denchmark-code&gt;

I could add more code if that will help.
&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='sudarshan85' date='2020-06-15T19:56:13Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='sudarshan85' date='2020-06-15T20:12:39Z'>
		&lt;denchmark-link:https://github.com/sudarshan85&gt;@sudarshan85&lt;/denchmark-link&gt;
 mind pass a minimal example to reproduce?
		</comment>
		<comment id='3' author='sudarshan85' date='2020-06-15T20:59:03Z'>
		&lt;denchmark-code&gt;#!/usr/bin/env python

import torch
from torch import nn
from torch.nn import functional as F

from argparse import Namespace
from omegaconf import OmegaConf

from pytorch_lightning.core.lightning import LightningModule

class TestModel(LightningModule):
  def __init__(self, hparams):
    super(TestModel, self).__init__()
    self.hparams = hparams
    self.out = nn.Linear(hparams.size,1)
    
  def forward(self, x):
    return self.out(x)
    

if __name__=='__main__':
  hparams = OmegaConf.create({'size': 3})
#   hparams = Namespace(
#     size=3
#   )
  
  model = TestModel(hparams)
  print(model(torch.rand(hparams.size)))
&lt;/denchmark-code&gt;

When using Namespace from argparse, there is no error and its able to print the out value. But when using OmegaConf object, we get the following error:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "./omega_bug.py", line 28, in &lt;module&gt;    model = TestModel(hparams)  File "./omega_bug.py", line 15, in __init__    self.hparams = hparams
  File "/net/vaosl01/opt/NFS/su0/miniconda3/envs/tfool/lib/python3.7/site-packages/torch/nn/modules/module.py", line 638, in __setattr__
    object.__setattr__(self, name, value)  File "/net/vaosl01/opt/NFS/su0/miniconda3/envs/tfool/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py", line 1680, in hparams
    self.save_hyperparameters(hp, frame=inspect.currentframe().f_back.f_back)  File "/net/vaosl01/opt/NFS/su0/miniconda3/envs/tfool/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py", line 1655, in save_hyperparameters    self._set_hparams(hp)  File "/net/vaosl01/opt/NFS/su0/miniconda3/envs/tfool/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py", line 1665, in _set_hparams    raise ValueError(f'Unsupported config type of {type(hp)}.')
ValueError: Unsupported config type of &lt;class 'omegaconf.dictconfig.DictConfig'&gt;.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='sudarshan85' date='2020-06-15T21:08:02Z'>
		&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2139&gt;#2139&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='sudarshan85' date='2020-06-15T21:15:24Z'>
		feel free to reopen if needed üê∞



pytorch-lightning/requirements/extra.txt


         Line 12
      in
      c52497e






 omegaconf&gt;=2.0.0 





		</comment>
		<comment id='6' author='sudarshan85' date='2020-08-03T09:26:55Z'>
		It seems the current Hydra version 0.11.3 doesn't support omegaconf=2.0.0?
I got this error when simply trying to import hydra
&lt;denchmark-code&gt;...
     5 from abc import abstractmethod
      6 import six
----&gt; 7 from omegaconf import DictConfig, ListConfig, Config, MissingMandatoryValue
      8 
      9 from hydra.plugins import Plugin
ImportError: cannot import name 'Config'
&lt;/denchmark-code&gt;

Am I missing something?
What's a good way to pass DictConfig object to Lightning's self.hparams?
EDIT:  will be supported by , using hydra pre-release fixes this. (&lt;denchmark-link:https://github.com/facebookresearch/hydra/releases/tag/hydra-1.0.0rc1&gt;release note)&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>