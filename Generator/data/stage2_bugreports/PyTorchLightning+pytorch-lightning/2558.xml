<bug id='2558' author='pherrusa7' open_date='2020-07-08T21:39:51Z' closed_time='2020-10-05T02:28:34Z'>
	<summary>trainer.test(model) on 0.8.4 with a checkpoint saved in 0.6.0 expects attribute 'checkpoint_callback_best_model_path'</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Save a checkpoint in 0.6.0
Load the model in 0.8.4 (no problem)
model = My_model.load_from_checkpoint(checkpoint_path)
Run trainer.test(model) 
See error

&lt;denchmark-code&gt;---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-3-eb476187792a&gt; in &lt;module&gt;
     84 
     85 
---&gt; 86 trainer.test(model)
     87 print("Test finished!\n")

~/.conda/envs/qwe3/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in test(self, model, test_dataloaders, ckpt_path)
   1240         if model is not None:
   1241             self.model = model
-&gt; 1242             self.fit(model)
   1243 
   1244         # on tpu, .spawn means we don't have a trained model

~/.conda/envs/qwe3/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders)
    977 
    978         elif self.single_gpu:
--&gt; 979             self.single_gpu_train(model)
    980 
    981         elif self.use_tpu:  # pragma: no-cover

~/.conda/envs/qwe3/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_parts.py in single_gpu_train(self, model)
    183             self.reinit_scheduler_properties(self.optimizers, self.lr_schedulers)
    184 
--&gt; 185         self.run_pretrain_routine(model)
    186 
    187     def tpu_train(self, tpu_core_idx, model):

~/.conda/envs/qwe3/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in run_pretrain_routine(self, model)
   1110 
   1111         # restore training and model before hpc call
-&gt; 1112         self.restore_weights(model)
   1113 
   1114         # when testing requested only run test and return

~/.conda/envs/qwe3/lib/python3.6/site-packages/pytorch_lightning/trainer/training_io.py in restore_weights(self, model)
    180         if not did_restore_hpc_weights:
    181             if self.resume_from_checkpoint is not None:
--&gt; 182                 self.restore(self.resume_from_checkpoint, on_gpu=self.on_gpu)
    183 
    184         # wait for all models to restore weights

~/.conda/envs/qwe3/lib/python3.6/site-packages/pytorch_lightning/trainer/training_io.py in restore(self, checkpoint_path, on_gpu)
    312 
    313         # load training state (affects trainer only)
--&gt; 314         self.restore_training_state(checkpoint)
    315 
    316     def dump_checkpoint(self, weights_only: bool = False) -&gt; dict:

~/.conda/envs/qwe3/lib/python3.6/site-packages/pytorch_lightning/trainer/training_io.py in restore_training_state(self, checkpoint)
    427                 )
    428                 checkpoint_callbacks[-1].best_model_score = checkpoint['checkpoint_callback_best']
--&gt; 429             checkpoint_callbacks[-1].best_model_path = checkpoint['checkpoint_callback_best_model_path']
    430 
    431         if early_stopping_callbacks:

KeyError: 'checkpoint_callback_best_model_path'
&lt;/denchmark-code&gt;

But checkpoint's attribute ''checkpoint_callback_best_model_path' doesn't exist in my old PTL version...
&lt;denchmark-h:h3&gt;Temporal Hack&lt;/denchmark-h&gt;



Load the model
checkpoint_path = 'lightning_logs/version_149/checkpoints/_ckpt_epoch_3.ckpt'
checkpoint = torch.load(checkpoint_path)


Add a dummy path to the required argument
checkpoint['checkpoint_callback_best_model_path'] = ''


Save this checkpoint
torch.save(checkpoint, checkpoint_path)


Load from the new checkpoint in 0.8.4 and run trainer.test(model) without any problem


&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version: 1.5.1
OS: Linux
How you installed PyTorch: conda
Python version: 3.6.10
CUDA/cuDNN version: 10.2
GPU models and configuration: Tesla V100-SXM3-32GB

Probably I am misunderstanding something, but this is my quick fix, thanks for your help!
	</description>
	<comments>
		<comment id='1' author='pherrusa7' date='2020-07-08T21:40:48Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='pherrusa7' date='2020-07-08T22:46:35Z'>
		yes, I guess we shall support loading compatibility from the beginning.. &lt;denchmark-link:https://github.com/orgs/PyTorchLightning/teams/core-contributors&gt;@PyTorchLightning/core-contributors&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/pherrusa7&gt;@pherrusa7&lt;/denchmark-link&gt;
 mind send a PR with mapping this as a mirror of existing attribute?
		</comment>
		<comment id='3' author='pherrusa7' date='2020-07-09T13:52:55Z'>
		Dear &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
, I have no time this week, but if the issue is still open in the next weeks I can do it :)
		</comment>
		<comment id='4' author='pherrusa7' date='2020-08-03T20:08:40Z'>
		&lt;denchmark-link:https://github.com/pherrusa7&gt;@pherrusa7&lt;/denchmark-link&gt;
 how about now? :)
		</comment>
		<comment id='5' author='pherrusa7' date='2020-09-16T18:35:17Z'>
		&lt;denchmark-link:https://github.com/pherrusa7&gt;@pherrusa7&lt;/denchmark-link&gt;
 can you verify this is still broken in master?
		</comment>
		<comment id='6' author='pherrusa7' date='2020-09-24T04:53:04Z'>
		It is still broken
		</comment>
		<comment id='7' author='pherrusa7' date='2020-09-29T22:49:31Z'>
		&lt;denchmark-link:https://github.com/dvirginz&gt;@dvirginz&lt;/denchmark-link&gt;

I tested this by saving a checkpoint in 0.6 and loading it with the same code on master branch.
There is no error anymore and  will use the loaded model. I tested on cpu and also multi gpu.
So the latest version should work for you.
Note, it seems that in 0.6 Lightning was not tracking the path to the best model checkpoint. So we can't know which of your old checkpoints is the best. There is also a conversion script in utilities "upgrade_checkpoint.py", this will upgrade old checkpoints to the current format, if needed. I recommend you upgrade all your checkpoints that you intend to use, and work with the latest PL version.
		</comment>
	</comments>
</bug>