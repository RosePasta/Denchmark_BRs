<bug id='3085' author='YinAoXiong' open_date='2020-08-21T03:25:14Z' closed_time='2020-09-01T02:42:44Z'>
	<summary>No log is recorded in debug mode, but manual operation works</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

just as title
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import torch
from torch.nn import functional as F
from torch import nn
from pytorch_lightning.core.lightning import LightningModule
from pytorch_lightning.core.step_result import TrainResult
from pytorch_lightning import loggers
from torch.utils.data import DataLoader, random_split
from torchvision.datasets import MNIST
import os
from torchvision import datasets, transforms
from pytorch_lightning import Trainer
from torch.optim import Adam

class LitMNIST(LightningModule):
    def __init__(self):
        super().__init__()
        self.layer_1 = torch.nn.Linear(28 * 28, 128)
        self.layer_2 = torch.nn.Linear(128, 256)
        self.layer_3 = torch.nn.Linear(256, 10)

    def forward(self, x):
        batch_size, channels, width, height = x.size()
        x = x.view(batch_size, -1)
        x = self.layer_1(x)
        x = torch.relu(x)
        x = self.layer_2(x)
        x = torch.relu(x)
        x = self.layer_3(x)
        x = torch.log_softmax(x, dim=1)
        return x
    def prepare_data(self):
        # download only (not called on every GPU, just the root GPU per node)
        MNIST(os.getcwd(), train=True, download=True)

    def train_dataloader(self):
        transform=transforms.Compose([transforms.ToTensor(),
                                      transforms.Normalize((0.1307,), (0.3081,))])
        mnist_train = MNIST(os.getcwd(), train=True, download=False, transform=transform)
        return DataLoader(mnist_train, batch_size=64)

    def configure_optimizers(self):
        return Adam(self.parameters(), lr=1e-3)

    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = F.nll_loss(logits, y)  # a Breakpoint at here

        # self.logger.log_metrics({'loss':loss},step=self.global_step)

        result = TrainResult(loss)
        result.log('train_loss', loss)

        # add logging
        
        return result

model = LitMNIST()
trainer = Trainer(gpus=1)
trainer.fit(model)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

tensorboard should show my loss curve
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;* CUDA:
        - GPU:
                - GeForce RTX 2080 Ti
                - GeForce RTX 2080 Ti
                - GeForce RTX 2080 Ti
                - GeForce RTX 2080 Ti
        - available:         True
        - version:           10.2
* Packages:
        - numpy:             1.19.1
        - pyTorch_debug:     False
        - pyTorch_version:   1.6.0
        - pytorch-lightning: 0.9.0
        - tensorboard:       2.2.0
        - tqdm:              4.48.2
* System:
        - OS:                Linux
        - architecture:
                - 64bit
                - 
        - processor:         x86_64
        - python:            3.7.7
        - version:           #168-Ubuntu SMP Wed Jan 16 21:00:45 UTC 2019
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

when I use self.logger.log_metrics({'loss':loss},step=self.global_step) it work for me
	</description>
	<comments>
		<comment id='1' author='YinAoXiong' date='2020-08-21T03:25:56Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='YinAoXiong' date='2020-09-25T22:57:53Z'>
		Hi, what exactly is this debug mode?
Given that this issue is closed, I assume you solved the problem.
I ran the code and it generates all the tensorboard logs, so it seems to be fixed in the latest version.
cheers
		</comment>
	</comments>
</bug>