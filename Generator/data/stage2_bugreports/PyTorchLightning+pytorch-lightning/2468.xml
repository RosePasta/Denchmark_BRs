<bug id='2468' author='JanSellner' open_date='2020-07-02T14:25:49Z' closed_time='2020-07-07T17:13:25Z'>
	<summary>Automatic member type conversion breaks weighted loss function</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I use the &lt;denchmark-link:https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss&gt;CrossEntropyLoss&lt;/denchmark-link&gt;
 with weights. These weights are stored as a member variable in the loss function and get automatically casted to float16 when using 16 bit training. This breaks the loss function since the weights must have type 
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

The following code snippet describes the problem:
class MyLightning(pl.LightningModule):
    def __init__(self):
       super(MyLightning, self).__init__()

       self.ce_loss_weighted = nn.CrossEntropyLoss(weight=torch.rand(10))
       # self.ce_loss_weighted.weight.dtype == torch.float32

   def training_step(self, batch):
      # batch['features'].dtype == torch.float16
      prediction = self(batch['features'])
      # prediction.dtype = torch.float32

      # This line breaks because
      # self.ce_loss_weighted.weight.dtype == torch.float16
      loss = self.ce_loss_weighted(prediction, batch['labels'])

Trainer(precision=16, gpus=1).fit(MyLightning())
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

The weights should not get casted to another type. Ideally, this happens intransparent to the user.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version (e.g., 1.0): 1.5
PyTorchLightning: 0.8.4
OS (e.g., Linux): Ubuntu 18.04
Python version: 3.8.3.
CUDA/cuDNN version: 10.2

	</description>
	<comments>
		<comment id='1' author='JanSellner' date='2020-07-02T14:28:45Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='JanSellner' date='2020-07-03T11:29:18Z'>
		&lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/MattPainter01&gt;@MattPainter01&lt;/denchmark-link&gt;
 mind have look?
		</comment>
		<comment id='3' author='JanSellner' date='2020-07-06T15:35:12Z'>
		Are you using apex or native amp?
		</comment>
		<comment id='4' author='JanSellner' date='2020-07-07T08:06:41Z'>
		I am using Nvidia apex (at &lt;denchmark-link:https://github.com/NVIDIA/apex/tree/02a33875970e1b555754dfc4ab85d05595d23764&gt;this commit&lt;/denchmark-link&gt;
).
		</comment>
		<comment id='5' author='JanSellner' date='2020-07-07T11:24:29Z'>
		So I could get this working using native amp, so I guess it is a problem with apex.
Therefore, if you update to latest pytorch, this should solve your problem.
		</comment>
		<comment id='6' author='JanSellner' date='2020-07-07T16:54:36Z'>
		I have just upgraded PyTorch to 1.5.1 and re-installed apex to &lt;denchmark-link:https://github.com/NVIDIA/apex/tree/1ff54b8fed441c39dac181091b44fecdca31a403&gt;this commit&lt;/denchmark-link&gt;
. However, the issue still persists :-(
EDIT: ahh, sorry, now I understand what you mean. When using the current nightly build of PyTorch (pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html), then the error is indeed gone (because PyTorch mixed precision implementation is used). Thanks!
		</comment>
		<comment id='7' author='JanSellner' date='2020-08-14T23:01:17Z'>
		A related problem, probably simpler.  What if I wanted to feed the weights into the constructor?
&lt;denchmark-code&gt;MyLightning(pl.LightningModule)
   def __init__(self, weights):
      self.ce_loss_weighted = nn.CrossEntropyLoss(weight=weights)
&lt;/denchmark-code&gt;

How do I do this?
		</comment>
		<comment id='8' author='JanSellner' date='2020-08-14T23:07:39Z'>
		while creating the instance of lightningmodule
weights = torch.tensor(...)
model = MyLightning(weights=weights)
		</comment>
	</comments>
</bug>