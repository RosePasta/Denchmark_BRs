<bug id='2352' author='thepowerfuldeez' open_date='2020-06-24T21:34:05Z' closed_time='2020-06-25T09:26:30Z'>
	<summary>Shared memory leak with large dataset and num_workers &amp;gt; 0</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Hello! I have large train dataset (135000 samples: image paths and additional data in json format, ~500mb json file). When use such large metadata file, one need to set torch.multiprocessing.set_sharing_strategy('file_system')
When I use num_workers &gt; 0 in DataLoader I obviosly use shared memory through Pytorch multiprocessing. It's roughly 0.5gb * 12 workers = 6gb of shared memory (/dev/shm in df -h).
However, after every epoch this number grows bigger and bigger. On epoch 1 I consume 6gb of shared memory. After 2nd epoch I consume 12gb, after 3rd 18gb and so on. In my case after 6 epoch training stops with OOM exception.
I checked in script shared memory consumption without pytorch-lightning, everything seems fine. It doesn't matter whether I use 4 gpus or only one, same with DDP or DP training.
Sorry, but my data is private and I cannot share details about it.
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Shared memory stays constant around 6gb
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:

GPU:

GeForce RTX 2080 Ti
GeForce RTX 2080 Ti
GeForce RTX 2080 Ti
GeForce RTX 2080 Ti


available:         True
version:           11.0


Packages:

numpy:             1.18.1
pyTorch_debug:     False
pyTorch_version:   1.6.0a0+569c85b
pytorch-lightning: 0.8.1
tensorboard:       2.2.2
tqdm:              4.46.0


System:

OS:                Linux
architecture:

64bit



processor:         x86_64
python:            3.7.7
version:           #92-Ubuntu SMP Fri Feb 28 11:09:48 UTC 2020



	</description>
	<comments>
		<comment id='1' author='thepowerfuldeez' date='2020-06-24T21:34:49Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='thepowerfuldeez' date='2020-06-24T21:36:51Z'>
		With num_workers=0 I have no memory leak, but training is 15 times slower.
		</comment>
		<comment id='3' author='thepowerfuldeez' date='2020-06-25T02:34:16Z'>
		so, using this flag torch.multiprocessing.set_sharing_strategy('file_system') seems to create a memory leak?
can you create a synthetic dataset (random tensors) on Colab that replicate this?
		</comment>
		<comment id='4' author='thepowerfuldeez' date='2020-06-25T09:26:29Z'>
		You are right, the problem is caused by file_system sharing strategy. If I set it to file_allocator (default) I got RuntimeError: received 0 items of ancdata.
from &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/13246&gt;pytorch/pytorch#13246&lt;/denchmark-link&gt;
:
Python Multiprocessing: There is no way of storing arbitrary python objects (even simple lists) in shared memory in Python without triggering copy-on-write behaviour due to the addition of refcounts, everytime something reads from these objects. The refcounts are added memory-page by memory-page, which is why the consumption grows slowly. The processes (workers) will end up having all/most of the memory copied over bit by bit, which is why we get the memory overflow problem.
My data is presented via list of tensors (bboxes and labels for detection, they're variable size) and I use regular list instead of tensor here, hence the problem with multiprocessing.
I fixed my problem for now with setting sharing strategy to 'file_allocator' (default), padding variable sized lists in collate_fn to return tensor instead of list of tensors and ignore 0 label when computing loss.
		</comment>
	</comments>
</bug>