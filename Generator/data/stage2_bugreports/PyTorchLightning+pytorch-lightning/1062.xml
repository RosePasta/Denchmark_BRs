<bug id='1062' author='bkkaggle' open_date='2020-03-06T01:41:01Z' closed_time='2020-03-08T16:46:41Z'>
	<summary>Wandb logger on multiple TPU cores shuts down in the middle of training</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When using the wandb logger with more than one tpu core on colab, wandb stops training in the middle of epoch 2.
See colab notebook: &lt;denchmark-link:https://colab.research.google.com/drive/1aafmyr4MDpZNX-XWRHOAKMaAIGF-5t6c&gt;https://colab.research.google.com/drive/1aafmyr4MDpZNX-XWRHOAKMaAIGF-5t6c&lt;/denchmark-link&gt;

Relevant part of stacktrace:
&lt;denchmark-code&gt;Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 66/66 [00:38&lt;00:00,  2.47it/s, loss=4.260, v_num=2cs3pulm]tcmalloc: large alloc 1125769216 bytes == 0xf9482000 @  0x7fcd9f2c02a4 0x592727 0x4dddf7 0x4ddece 0x4e24ad 0x4e25eb 0x4e1430 0x4e2edb 0x4e284a 0x4e14d8 0x4e2c5b 0x4e27b2 0x4e1430 0x4e2c5b 0x4e307c 0x4e307c 0x4e3024 0x4e34a4 0x4e307c 0x4e3ac6 0x5ebdc2 0x50a94c 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9
tcmalloc: large alloc 1896398848 bytes == 0x17b98e000 @  0x7fcd9f2c02a4 0x592727 0x4dddf7 0x4ddece 0x4e24ad 0x4e25eb 0x4e1430 0x4e2edb 0x4e284a 0x4e14d8 0x4e2c5b 0x4e27b2 0x4e1430 0x4e2c5b 0x4e30d0 0x4e3120 0x4e3024 0x4e34a4 0x4e307c 0x4e3ac6 0x5ebdc2 0x50a94c 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9
tcmalloc: large alloc 3021201408 bytes == 0x232232000 @  0x7fcd9f2c02a4 0x592727 0x4dddf7 0x4ddece 0x4e24ad 0x4e25eb 0x4e1430 0x4e2edb 0x4e284a 0x4e14d8 0x4e2c5b 0x4e27b2 0x4e1430 0x4e2c5b 0x5ec012 0x4e1683 0x4e2c5b 0x4e3120 0x4e3ac6 0x5ebdc2 0x50a94c 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509d48
Epoch 2:  47%|‚ñà‚ñà‚ñä   | 31/66 [00:12&lt;00:05,  6.86it/s, loss=4.159, v_num=2cs3pulm]
Validating:   0%|                                        | 0/35 [00:00&lt;?, ?it/s]
Epoch 2:  48%|‚ñà‚ñà‚ñâ   | 32/66 [00:12&lt;00:08,  3.93it/s, loss=4.159, v_num=2cs3pulm]
Epoch 2:  56%|‚ñà‚ñà‚ñà‚ñé  | 37/66 [00:12&lt;00:05,  5.42it/s, loss=4.159, v_num=2cs3pulm]
Epoch 2:  67%|‚ñà‚ñà‚ñà‚ñà  | 44/66 [00:12&lt;00:02,  7.48it/s, loss=4.159, v_num=2cs3pulm]
Epoch 2:  76%|‚ñà‚ñà‚ñà‚ñà‚ñå | 50/66 [00:12&lt;00:01, 10.14it/s, loss=4.159, v_num=2cs3pulm]
wandb: Waiting for W&amp;B process to finish, PID 1692

Epoch 2:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 57/66 [00:13&lt;00:00, 13.54it/s, loss=4.159, v_num=2cs3pulm]
Epoch 2:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 64/66 [00:13&lt;00:00, 17.75it/s, loss=4.159, v_num=2cs3pulm]wandb: Program ended successfully.

wandb: Waiting for W&amp;B process to finish, PID 1692
Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 66/66 [00:13&lt;00:00, 17.75it/s, loss=4.159, v_num=2cs3pulm]
                                                                                
wandb: Waiting for W&amp;B process to finish, PID 1692

wandb: Waiting for W&amp;B process to finish, PID 1692

wandb: Waiting for W&amp;B process to finish, PID 1692

wandb: Waiting for W&amp;B process to finish, PID 1692

wandb: Waiting for W&amp;B process to finish, PID 1692
wandb: Run summary:
wandb:           _runtime 118.55305814743042
wandb:        global_step 61
wandb:              _step 9
wandb:         _timestamp 1583458583.1040945
wandb:         train_loss 4.152329444885254
wandb:           val_loss 4.094186782836914
wandb:            val_ppl 59.9905891418457
wandb:   adjusted_val_ppl 118.12129974365234
wandb: Syncing files in wandb/run-20200306_013427-2cs3pulm:
wandb:   code/finetune.py
wandb:   epoch=0.ckpt
wandb:   epoch=1.ckpt.part
wandb: plus 7 W&amp;B file(s) and 0 media file(s)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='bkkaggle' date='2020-03-08T16:46:41Z'>
		Looks like this is a problem with not having enough RAM to save checkpoints at the end of each epoch, closing.
		</comment>
		<comment id='2' author='bkkaggle' date='2020-03-08T16:58:24Z'>
		is this colab or kaggle?
		</comment>
		<comment id='3' author='bkkaggle' date='2020-03-09T00:23:24Z'>
		I've checked that this error happens on colab and gcp when the model is too large, but just increasing the amount of RAM in the compute engine instance solves the problem.
		</comment>
	</comments>
</bug>