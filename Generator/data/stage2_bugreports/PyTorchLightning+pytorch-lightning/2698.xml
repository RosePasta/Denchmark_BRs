<bug id='2698' author='ibeltagy' open_date='2020-07-24T23:28:33Z' closed_time='2020-10-19T22:38:44Z'>
	<summary>All TPU cores create tensorboard logs</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

With TPUs, TestTubeLogger writes many empty tensorboard logs, one log per TPU core except one. This confuses tensorboard and prevents it from updating. This is happening because the logger is created before spawning processes then the logger is replicated in each process.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Train any model with ptl.Trainer(logger=TestTubeLogger(), num_tpu_cores=8) then check the tf directory, you will find 1 file with real log and 7 empty files.
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Only the main process creates a tensorboard log.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

pytorch-lightning==0.8.5
	</description>
	<comments>
		<comment id='1' author='ibeltagy' date='2020-07-25T12:09:55Z'>
		&lt;denchmark-link:https://github.com/lezwon&gt;@lezwon&lt;/denchmark-link&gt;
 mind have look?
		</comment>
		<comment id='2' author='ibeltagy' date='2020-07-25T14:23:09Z'>
		WIll do :]
		</comment>
		<comment id='3' author='ibeltagy' date='2020-07-30T09:32:57Z'>
		&lt;denchmark-link:https://github.com/ibeltagy&gt;@ibeltagy&lt;/denchmark-link&gt;
 mind check it with the latest master?
		</comment>
		<comment id='4' author='ibeltagy' date='2020-08-11T01:05:37Z'>
		&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
, couldn't test it because it appears that master is broken. It runs for a few steps then crashes with the following error. The only thing I changed is switching from release v0.8.5 to master, everything else is the same.
&lt;denchmark-code&gt;Epoch 1:   1%|‚ñà‚ñè                                                                                                                                   | 10/1066 [02:07&lt;3:45:12, 12.80s/it, loss=1.645, v_num=0]
Traceback (most recent call last):
  File "scripts/pretrain.py", line 488, in &lt;module&gt;
    main(args)
  File "scripts/pretrain.py", line 482, in main
    trainer.fit(pretrainer)
  File "/home/beltagy/pytorch-lightning/pytorch_lightning/trainer/states.py", line 34, in wrapped_fn
    result = fn(self, *args, **kwargs)
  File "/home/beltagy/pytorch-lightning/pytorch_lightning/trainer/trainer.py", line 1059, in fit
    self.accelerator_backend.train(model)
  File "/home/beltagy/pytorch-lightning/pytorch_lightning/accelerators/tpu_backend.py", line 87, in train
    start_method=self.start_method
  File "/anaconda3/envs/torch-xla-nightly/lib/python3.6/site-packages/torch_xla/distributed/xla_multiprocessing.py", line 387, in spawn
    _start_fn(0, pf_cfg, fn, args)
  File "/anaconda3/envs/torch-xla-nightly/lib/python3.6/site-packages/torch_xla/distributed/xla_multiprocessing.py", line 324, in _start_fn
    fn(gindex, *args)
  File "/home/beltagy/pytorch-lightning/pytorch_lightning/accelerators/tpu_backend.py", line 118, in tpu_train_in_process
    trainer.transfer_distrib_spawn_state_on_fit_end(model, mp_queue, results)
  File "/home/beltagy/pytorch-lightning/pytorch_lightning/trainer/distrib_data_parallel.py", line 417, in transfer_distrib_spawn_state_on_fit_end
    if self.distributed_backend.lower() not in ['ddp_spawn', 'ddp_cpu', 'tpu']:
AttributeError: 'NoneType' object has no attribute 'lower'
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='ibeltagy' date='2020-08-11T09:24:40Z'>
		trying to reproduce with colab tests, but actually I have a problem installing XLA, &lt;denchmark-link:https://github.com/zcain117&gt;@zcain117&lt;/denchmark-link&gt;
 ?
&lt;denchmark-link:https://colab.research.google.com/drive/15E3oo3vPsSvLVufU4I6AK2thcXcg2mdW#scrollTo=BHBz1_AnamN&gt;https://colab.research.google.com/drive/15E3oo3vPsSvLVufU4I6AK2thcXcg2mdW#scrollTo=BHBz1_AnamN&lt;/denchmark-link&gt;
_
		</comment>
		<comment id='6' author='ibeltagy' date='2020-09-22T21:05:04Z'>
		&lt;denchmark-link:https://github.com/lezwon&gt;@lezwon&lt;/denchmark-link&gt;
 mind taking a look?
		</comment>
		<comment id='7' author='ibeltagy' date='2020-09-23T05:19:06Z'>
		I haven't been able to reproduce this issue. Maybe if &lt;denchmark-link:https://github.com/ibeltagy&gt;@ibeltagy&lt;/denchmark-link&gt;
 could share a notebook I could look into it. :)
		</comment>
		<comment id='8' author='ibeltagy' date='2020-09-24T04:18:44Z'>
		Are you testing with num_tpu_cores=8? do you get a single file under the tf directory?
		</comment>
		<comment id='9' author='ibeltagy' date='2020-09-24T08:34:31Z'>
		the test is here, but not counting files



pytorch-lightning/tests/models/test_tpu.py


        Lines 74 to 88
      in
      c94c0a2






 def test_model_tpu_cores_8(tmpdir): 



 """Make sure model trains on TPU.""" 



 trainer_options = dict( 



 default_root_dir=tmpdir, 



 progress_bar_refresh_rate=0, 



 max_epochs=1, 



 tpu_cores=8, 



 limit_train_batches=0.4, 



 limit_val_batches=0.4, 



     ) 



 



 model = EvalModelTemplate() 



 # 8 cores needs a big dataset 



 model.train_dataloader = _serial_train_loader 



 model.val_dataloader = _serial_train_loader 





		</comment>
		<comment id='10' author='ibeltagy' date='2020-09-24T15:53:05Z'>
		The test looks good but it is not capturing the issue that multiple tensorboard log files are created and that these files confuse tensoboard
		</comment>
		<comment id='11' author='ibeltagy' date='2020-09-30T03:17:10Z'>
		
Are you testing with num_tpu_cores=8? do you get a single file under the tf directory?

Yea. I didn't see multiple files.
		</comment>
		<comment id='12' author='ibeltagy' date='2020-10-04T16:10:03Z'>
		&lt;denchmark-link:https://github.com/ibeltagy&gt;@ibeltagy&lt;/denchmark-link&gt;
 mind share a notebook so we can try to reproduce? or see if this issue is still happening on master?
		</comment>
		<comment id='13' author='ibeltagy' date='2020-10-19T22:38:44Z'>
		closing this issue for now. Feel free to reopen with an example so we can reproduce!
		</comment>
	</comments>
</bug>