<bug id='3710' author='vegovs' open_date='2020-09-28T20:12:00Z' closed_time='2020-10-05T03:28:00Z'>
	<summary>UserWarning: "When using EvalResult(early_stop_on=X) or ..." triggers always.</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

The UserWarning:
"When using EvalResult(early_stop_on=X) or TrainResult(early_stop_on=X) the
'monitor' key of EarlyStopping has no effect.
Remove EarlyStopping(monitor='val_early_stop_on) to fix')"
triggers without the EarlyStopping(monitor='val_early_stop_on).
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:
Run as normal with EarlyStopping callback, and no early_stopping_on on either TrainResult or EvalResult:
Example:
Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00&lt;00:00,  1.53it/s, loss=0.750, v_num=20]
Validating: 0it [00:00, ?it/s]
Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02&lt;00:00,  1.39s/it, loss=0.750, v_num=20]/home/vegovs/venvs/seg/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: 
                    When using EvalResult(early_stop_on=X) or TrainResult(early_stop_on=X) the
                    'monitor' key of EarlyStopping has no effect.
                    Remove EarlyStopping(monitor='val_early_stop_on) to fix')
                
  warnings.warn(*args, **kwargs)
Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03&lt;00:00,  1.58s/it, loss=0.750, v_num=20]
Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03&lt;00:00,  1.66s/it, loss=0.750, v_num=20]
Saving latest checkpoint..
INFO:lightning:Saving latest checkpoint..

Process finished with exit code 0
Callback:
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

    early_stopping = EarlyStopping(
        patience=5 if not os.getenv("EARLY_STOP") else int(os.getenv("EARLY_STOP")),
        verbose=True,
    )
EvalResult:
        result = pl.EvalResult(loss, checkpoint_on=loss)
        result.log("val_loss", loss, sync_dist=True)
        if batch_idx == 0:
            rand_idx = randint(0, self.hparams.batch_size - 1)
TrainResult:
        loss = self.loss_funciton(masks_pred, masks)
        result = pl.TrainResult(minimize=loss)
        result.log("train_loss", loss, sync_dist=True)
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Expect no user warning when no monitor value is given to EarlyStopping callback..
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:
- GPU:
- GeForce RTX 2070 SUPER
- available:         True
- version:           10.2
Packages:
- numpy:             1.19.2
- pyTorch_debug:     False
- pyTorch_version:   1.6.0
- pytorch-lightning: 0.9.0
- tqdm:              4.49.0
System:
- OS:                Linux
- architecture:
- 64bit
- ELF
- processor:         x86_64
- python:            3.6.9
- version:           #52~18.04.1-Ubuntu SMP Thu Sep 10 12:50:22 UTC 2020

	</description>
	<comments>
		<comment id='1' author='vegovs' date='2020-10-02T21:31:19Z'>
		Hey! We created a new simpler API for early stopping, simply passing a callback. Please have a look at &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/early_stopping.html&gt;latest docs&lt;/denchmark-link&gt;
 and try our master to see if this fixes the problem!
		</comment>
		<comment id='2' author='vegovs' date='2020-10-05T03:28:00Z'>
		yes, please use the new API!
		</comment>
	</comments>
</bug>