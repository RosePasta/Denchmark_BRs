<bug id='1422' author='bobofzhang' open_date='2020-04-09T09:22:50Z' closed_time='2020-04-09T12:52:16Z'>
	<summary>Not auto add DistributedSampler for DDP training</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

in 0.72, even if we don't set sampler, pytorch_lightning will not add  DistributedSampler for us.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

the reason is in pytorch, if we don't set sampler,  pytorch will add a sampler for us.
in pytorch's dataloader.py:
&lt;denchmark-code&gt;         if sampler is None:  # give default samplers
            if self._dataset_kind == _DatasetKind.Iterable:
                # See NOTE [ Custom Samplers and IterableDataset ]
                sampler = _InfiniteConstantSampler()
            else:  # map-style
                if shuffle:
                    sampler = RandomSampler(dataset)
                else:
                    sampler = SequentialSampler(dataset)
&lt;/denchmark-code&gt;

but in pytorch_lightning we check whether sampler is None to  decide to add sampler
in  data_loading.py funciton auto_add_sampler:
&lt;denchmark-code&gt;        no_sampler_added = dataloader.sampler is None
&lt;/denchmark-code&gt;

because pytorch have default sampler for us, which is not None, pytorch_lighting will not automatically add sampler.
	</description>
	<comments>
	</comments>
</bug>