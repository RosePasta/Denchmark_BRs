<bug id='3260' author='maxjeblick' open_date='2020-08-29T22:09:45Z' closed_time='2020-10-06T17:54:49Z'>
	<summary>auto_scale_batch_size won't reset current_epoch</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When auto_scale_batch_size is enabled, the model is initially trained with varying batch sizes. When training begins, trainer.current_epoch equals 1 instead of 0.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Either observe the progress bar or use a simple callback to track the epoch number, once with  auto_scale_batch_size  enabled and once with  auto_scale_batch_size disabled.
&lt;denchmark-code&gt;from pytorch_lightning import Callback

class PrintCallback(Callback):
    
    def __init__(self):
        self.observed_epochs = []
        
    def on_train_epoch_start(self, trainer, pl_module):
        print(f'Current Epoch: {trainer.current_epoch}')
        self.observed_epochs.append(trainer.current_epoch)

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='maxjeblick' date='2020-08-30T12:50:23Z'>
		since it calls it with various batch_sizes did you find where it sets current_epoch to 0 before checking it on next batch_size?
		</comment>
		<comment id='2' author='maxjeblick' date='2020-08-31T11:43:44Z'>
		The problem is during &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_io.py#L335&gt;model checkpointing&lt;/denchmark-link&gt;
. The checkpoint sets  . That checkpoint will be loaded after having completed the batch size finder. During batch size scaling, the epoch won't be increased.
		</comment>
		<comment id='3' author='maxjeblick' date='2020-09-02T18:48:34Z'>
		Currently blocked until trainer.tune is added.
		</comment>
	</comments>
</bug>