<bug id='1828' author='williamFalcon' open_date='2020-05-14T01:50:05Z' closed_time='2020-05-14T14:34:13Z'>
	<summary>Automatic batch-size scaling is missing properties</summary>
	<description>
&lt;denchmark-code&gt;  File "envs/demo2/lib/python3.7/site-packages/pytorch_lightning/trainer/training_tricks.py", line 267, in _run_power_scaling
    trainer.fit(model)
  File "envs/demo2/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 839, in fit
    self.single_gpu_train(model)
  File "/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py", line 499, in single_gpu_train
    self.run_pretrain_routine(model)
  File "pytorch_lightning/trainer/trainer.py", line 981, in run_pretrain_routine
    False)
  File "evaluation_loop.py", line 326, in _evaluate
    eval_results = model.validation_epoch_end(outputs)
  File "vae.py", line 83, in validation_epoch_end
    self.logger.experiment.add_image('images', grid, 0)
AttributeError: 'NoneType' object has no attribute 'experiment'
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;

Looks like loggers are gone?
	</description>
	<comments>
		<comment id='1' author='williamFalcon' date='2020-05-14T08:37:41Z'>
		Loggers are disabled during the auto scaling, such that all the small runs that are done inside the auto scaling is not logged. This works fine if the user does not explicit call the logger inside their model (i.e. only called by lightning in the background). I am not sure the way around this, of cause the simplest solution would be to ask the user to do something like:
&lt;denchmark-code&gt;if self.logger:
   self.logger.experiment.add_image('images', grid, 0)
&lt;/denchmark-code&gt;

But that is not a good idea...
		</comment>
		<comment id='2' author='williamFalcon' date='2020-05-14T09:19:30Z'>
		Also this problem should also be present for the learning rate finder...
		</comment>
		<comment id='3' author='williamFalcon' date='2020-05-14T12:24:36Z'>
		yeah, canâ€™t ask the user to do that. maybe replace with a logger that has a no op somehow?
		</comment>
		<comment id='4' author='williamFalcon' date='2020-05-14T13:32:35Z'>
		Yes, I think you are right that we just initialize a dummy logger. Something like this should work
&lt;denchmark-code&gt;from pytorch_lightning.loggers import LightningLoggerBase

class DummyExperiment(object):
    def nop(*args, **kw): pass
    def __getattr__(self, _): return self.nop


class DummyLogger(LightningLoggerBase):
    def __init__(self):
        self._experiment = DummyExperiment()
    
    @property
    def experiment(self):
        return self._experiment    
    
    def log_metrics(self, metrics, step):
        pass
    
    def log_hyperparams(self, params):
        pass
    
    @property
    def name(self):
        pass
    
    @property
    def version(self):
        pass
    
logger = DummyLogger()
logger.experiment.add_image('hest', [1,2,3], global_step=3) # has no effect, but still exist
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>