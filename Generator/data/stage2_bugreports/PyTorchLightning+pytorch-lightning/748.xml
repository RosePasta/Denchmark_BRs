<bug id='748' author='bangbangjim' open_date='2020-01-25T07:00:17Z' closed_time='2020-01-26T19:12:40Z'>
	<summary>Fit Error: raised training_step() takes 3 positional arguments but 4 were given when I use truncated_bptt_steps</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Everything works fine when I had truncated_bptt_steps as None, but when I set it to 5. The error mentioned in the title is thrown (see below for Traceback detail):
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "Trainer.py", line 103, in &lt;module&gt;
    trainer.fit(gen)
  File "xxxx/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 695, in fit
    self.single_gpu_train(model)
  File "xxxx/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py", line 441, in single_gpu_train
    self.run_pretrain_routine(model)
  File "xxxx/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 829, in run_pretrain_routine
    self.train()
  File "xxxx/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 332, in train
    self.run_training_epoch()
  File "xxxx/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 386, in run_training_epoch
    output = self.run_training_batch(batch, batch_idx)
  File "xxxx/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 506, in run_training_batch
    loss = optimizer_closure()
  File "xxxx/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 475, in optimizer_closure
    split_batch, batch_idx, opt_idx, self.hiddens)
  File "xxxx/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 593, in training_forward
    output = self.model.training_step(*args)
TypeError: training_step() takes 3 positional arguments but 4 were given
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

My DataSet class
&lt;denchmark-code&gt;class DataSet(data.Dataset):
    def __init__(self, data, token_id2idx):
        '''
        Data and label has to have the same time step size for truncated batch split to work
        'label is the data offset by 1 (because dont need to predict the first CLS token)
        :param data: input ids (B, max_seq)
        :type data: Numpy array
        :param token_id2idx: mapping to convert bert token to idx (which goes from 0 to V-1)
        '''
        self.data = data[:,:-1].copy() # (B, max_seq -1)
        self.labels = data[:, 1:].copy() # (B, max_seq -1)
        assert self.data.shape == self.labels.shape, "data shape and label shape is different, this will cause error during TBPTT"
        for r in range(self.labels.shape[0]):
            for t in range(self.labels.shape[1]):
              self.labels[r, t] = token_id2idx[self.labels[r, t]]
        self.max_seq = self.data.shape[1]

    def __len__(self):
        'Denotes the total number of samples'
        return self.data.shape[0]

    def __getitem__(self, index):
        'Generates one sample of data'
        # Select sample
        X = self.data[index] # (max_seq)
        y = self.labels[index] # (max_seq - 1)
        # y = torch.ones(1).fill_(self.label)
        return X, y
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

PyTorch version: 1.3.1+cu92
Is debug build: No
CUDA used to build PyTorch: 9.2.148
OS: Debian GNU/Linux 9.6 (stretch)
GCC version: (Debian 6.3.0-18+deb9u1) 6.3.0 20170516
CMake version: Could not collect
Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: Tesla K80
Nvidia driver version: 410.72
cuDNN version: Could not collect
Versions of relevant libraries:
[pip] numpy==1.16.4
[pip] numpydoc==0.9.1
[pip] pytorch-ignite==0.1.0
[pip] pytorch-lightning==0.6.0
[pip] pytorch-transformers==1.1.0
[pip] torch==1.3.1+cu92
[pip] torchfile==0.1.0
[pip] torchvision==0.4.2+cu92
[conda] blas                      1.0                         mkl
[conda] mkl                       2019.4                      243
[conda] mkl-service               2.0.2            py37h7b6447c_0
[conda] mkl_fft                   1.0.12           py37ha843d7b_0
[conda] mkl_random                1.0.2            py37hd81dba3_0
[conda] pytorch-ignite            0.1.0                    pypi_0    pypi
[conda] pytorch-lightning         0.6.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.3.1+cu92               pypi_0    pypi
[conda] torchfile                 0.1.0                    pypi_0    pypi
[conda] torchvision               0.4.2+cu92               pypi_0    pypi
	</description>
	<comments>
		<comment id='1' author='bangbangjim' date='2020-01-25T19:36:22Z'>
		Can you show your Lightning Model?
		</comment>
		<comment id='2' author='bangbangjim' date='2020-01-25T19:40:24Z'>
		&lt;denchmark-code&gt;class Generator(pl.LightningModule):

    def __init__(self, vocab_size, idx2token_id, max_seq, model, first_tokens, lr =0.001, folder='testing', hidden_size = 512, embedding_size=768, cls_token_id=101, sep_token_id=102, mask_token_id=103, pad_token_id=0):
        '''
        :params worker: pre-trained worker instance
        :params manager:pre-trained manager instance

        '''
        super(Generator, self).__init__()
        # print ("vocab_size", vocab_size)
        self.folder = folder
        self.max_seq = max_seq
        self.cls_token_id = cls_token_id
        self.sep_token_id = sep_token_id
        self.pad_token_id = pad_token_id
        self.mask_token_id = mask_token_id
        self.vocab_size = vocab_size
        self.input_size = embedding_size
        self.hidden_size = hidden_size
        self.lr = lr
        self.first_tokens = first_tokens
        self.bert = model
        for p in self.bert.parameters():
            p.requires_grad = False
        self.lstm1 = nn.LSTMCell(self.input_size, self.hidden_size)
        self.lstm2 = nn.LSTMCell(self.hidden_size, self.hidden_size)
        # self.lstm3 = nn.LSTMCell(self.hidden_size, self.hidden_size)

        self.fc = nn.Linear(hidden_size, vocab_size)
        self.idx2token_id = idx2token_id
        self.token_id2idx = {v: k for k, v in idx2token_id.items()}
        self.losses = []
        self.loss_fn = nn.NLLLoss(ignore_index=self.token_id2idx[self.pad_token_id])
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def init_hidden(self, batch_size):
        h0 = torch.zeros((batch_size, self.hidden_size))
        c0 = torch.zeros((batch_size, self.hidden_size))
        return h0, c0

    def convert_to_token_ids(self, selected_idx):
        '''
        Convert the index to bert token ids
        @params selected_idx: (B, 1)
        '''
        token_idx = torch.zeros_like(selected_idx) # (B, 1)
        for i in range(selected_idx.shape[0]):
            idx = selected_idx[i, 0].long().item()
            token_idx[i, 0] = self.idx2token_id[idx]
        token_idx = token_idx.long()
        return token_idx

    def forward(self, input_ids, t, last_h1, last_c1, last_h2, last_c2, last_h3, last_c3):
        '''
        Need to add [SEP] token id to the end
        @params input_ids: (B, t)
        @params {int} t: The current time step. Neccessary to find the correct contextualised embedding
        @params last_hidden
        @params last_cell
        '''
        with torch.no_grad():
            batch_size = input_ids.shape[0]
            sep_step =  torch.zeros((batch_size, 1)).fill_(self.sep_token_id).long().to(self.device)

            input_ids = torch.cat([input_ids,sep_step], axis = 1).to(self.device)
            assert input_ids.shape[1] == t+2, f"Unexpected input size at timestep {t}:  {input_ids.shape}"

            hidden_states, _ = self.bert(input_ids)  # (B, S, H)
            embedding = hidden_states[:, t, :]  # (B, H)
        h1, c1 = self.lstm1(embedding, (last_h1, last_c1)) # (B, H), (B, H)
        h2, c2 = self.lstm2(h1, (last_h2, last_c2)) # (B, H), (B, H)
        logits = self.fc(h2)  # (B,  V)
        h3, c3 = last_h3, last_c3
        # h3, c3 = self.lstm3(h2, (last_h3, last_c3)) # (B, H), (B, H)
        # logits = self.fc(h3)  # (B,  V)
        return logits, h1, h2, h3, c1, c2, c3

    def sample(self, batch_size):
        global outputs
        with torch.no_grad():
            h1, c1 = self.init_hidden(batch_size)
            h2, c2 = self.init_hidden(batch_size)
            h3, c3 = self.init_hidden(batch_size)
            h1 = h1.to(self.device)
            c1 = c1.to(self.device)
            h2 = h2.to(self.device)
            c2 = c2.to(self.device)
            h3 = h3.to(self.device)
            c3 = c3.to(self.device)

            cls_step = torch.ones((batch_size, 1)).fill_(self.cls_token_id).long().to(self.device) # (B, 1)
            mask_step = torch.ones(batch_size, self.max_seq - 2).fill_(self.mask_token_id).long().to(self.device)
            first_tokens = self.first_tokens[torch.randint(0, self.first_tokens.size(0), (batch_size,1))].to(self.device) # (B, 1)
            input_ids = torch.cat((cls_step, first_tokens), axis=1)  # (B, max_seq - 1)
            # input_ids = cls_step.clone()
            input_ids = input_ids.to(self.device)
            # outputs = [cls_step]
            for t in range(1, self.max_seq - 1):
                # assert input_ids.shape[1] == t + 1
                params = {
                    'input_ids': input_ids,
                    't': t,
                             "last_h1": h1,
                             "last_c1": c1,
                         "last_h2": h2,
                         "last_c2": c2,
                         "last_h3": h3,
                         "last_c3": c3
                }
                logits, h1, h2, h3, c1, c2, c3 = self.forward(**params)
                probits = torch.softmax(logits, 1)

                selected_idx = torch.argmax(probits, 1) #(B, )

                if t &lt; self.max_seq -2:
                    selected_token_ids = torch.LongTensor([self.idx2token_id[i.item()] for i in selected_idx]).to(self.device).unsqueeze(1) # (B, 1)
                    input_ids = torch.cat([input_ids, selected_token_ids], axis=1).to(self.device)

            samples = [tokenizer.decode(row.cpu().tolist()) for row in input_ids]
            for s in samples:
                print (f"Samples:{s}\n")
            return samples

    def training_step(self, batch, batch_nb):
        real_input_ids, y = batch #input_ids is (B,S - 1), y is (B, S - 1)
        # y = y.to(self.device)
        y = y[:,1:].contiguous().to(self.device) # (B, S-2)

        real_input_ids = real_input_ids.to(self.device)
        batch_size = real_input_ids.shape[0]
        h1, c1 = self.init_hidden(batch_size)
        h2, c2 = self.init_hidden(batch_size)
        h3, c3 = self.init_hidden(batch_size)
        h1 = h1.to(self.device)
        c1 = c1.to(self.device)
        h2 = h2.to(self.device)
        c2 = c2.to(self.device)
        h3 = h3.to(self.device)
        c3 = c3.to(self.device)

        outputs = []
        for t in range(1, self.max_seq - 1):
            # input_ids = real_input_ids[:, t] # Teacher forcing
            params = {
                'input_ids': real_input_ids[:, :t+1],
                't': t,
                "last_h1": h1,
                "last_c1": c1,
                "last_h2": h2,
                "last_c2": c2,
                "last_h3": h3,
                "last_c3": c3
            }
            logits, h1, h2, h3, c1, c2, c3 = self.forward(**params)
            log_probits = torch.log_softmax(logits, dim=1)

            outputs.append(log_probits.unsqueeze(1))
        outputs = torch.cat(outputs, 1) # (B, S -2, V)
        # outputs = outputs[:, 1:, :].contiguous()

        # loss = self.loss_fn(outputs.view(batch_size, -1), y.view(batch_size, -1))
        loss = self.loss_fn(outputs.view(-1, self.vocab_size), y.flatten())

        self.losses.append(loss.item())
        if len(self.losses) % 10 == 0:
            avg_train_loss = np.mean(self.losses)
            self.logger.log_metrics({
                "train_loss": avg_train_loss,
            })
            self.losses = []
        return {'loss': loss}

    def validation_step(self, batch, batch_nb):

        real_input_ids, y = batch #input_ids is (B,S), y is (B, S - 1)
        y = y[:,1:].contiguous().to(self.device)
        # y = y.to(self.device)
        real_input_ids = real_input_ids.to(self.device)

        batch_size = real_input_ids.shape[0]
        with torch.no_grad():

            h1, c1 = self.init_hidden(batch_size)
            h2, c2 = self.init_hidden(batch_size)
            h3, c3 = self.init_hidden(batch_size)
            h1 = h1.to(self.device)
            c1 = c1.to(self.device)
            h2 = h2.to(self.device)
            c2 = c2.to(self.device)
            h3 = h3.to(self.device)
            c3 = c3.to(self.device)
            # Generate input_ids as (B, max_seq) with the stuff in order to generate better contextualised  word embedding
            input_ids = torch.ones(batch_size, self.max_seq - 1).fill_(self.mask_token_id).long()
            input_ids = input_ids.to(self.device)
            outputs = []
            for t in range(1, self.max_seq - 1):
                # input_ids[:, t] = real_input_ids[:, t] # Teacher forcing
                params = {
                    'input_ids': real_input_ids[:, :t+1],
                    't': t,
                             "last_h1": h1,
                             "last_c1": c1,
                         "last_h2": h2,
                         "last_c2": c2,
                         "last_h3": h3,
                         "last_c3": c3
                }
                logits, h1, h2, h3, c1, c2, c3 = self.forward(**params)
                log_probits = torch.log_softmax(logits, dim=1)

                outputs.append(log_probits.unsqueeze(1))
            outputs = torch.cat(outputs, 1) # (B, S -1, V)
            # outputs = outputs[:,1:,:].contiguous()
            loss = self.loss_fn(outputs.view(-1, self.vocab_size), y.flatten())
            return {'val_loss': loss}

    def validation_end(self, outputs):
        """
        Called at the end of validation to aggregate outputs
        :param outputs: list of individual outputs of each validation step
        :return:
        """
        val_loss_mean = 0
        for output in outputs:
            val_loss_mean += output['val_loss']
        val_loss_mean /= len(outputs)
        val_loss_mean = float(val_loss_mean)
        self.logger.log_metrics({
            "val_loss": val_loss_mean,
        })
        tqdm_dict = {'val_loss': val_loss_mean}



        # show val_loss and val_acc in progress bar but only log val_loss
        results = {
            'progress_bar': tqdm_dict,
        }
        samples = self.sample(10)

        return results
    def configure_optimizers(self):
        # REQUIRED
        params = json.load(open("configs/model.json", 'rb'))
        # lr = params['lr']
        return torch.optim.Adam([i for i in self.parameters() if i.requires_grad], lr= self.lr)

    @pl.data_loader
    def train_dataloader(self):
        params = json.load(open("configs/data_loader.json", 'rb'))
        real_data = np.load(f'data/{self.folder}/real_train_data.npy')
        data_loader = util.get_real_data_loader(real_data, self.token_id2idx, params)
        return data_loader


    @pl.data_loader
    def val_dataloader(self):
        params = json.load(open("configs/data_loader.json", 'rb'))
        real_data = np.load(f'data/{self.folder}/real_val_data.npy')
        # real_data = np.load(f'data/{self.folder}/real_train_data.npy')
        data_loader = util.get_real_data_loader(real_data, self.token_id2idx, params)
        return data_loader
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='bangbangjim' date='2020-01-26T11:13:08Z'>
		Are you sure your dataloader is not returning a tuple? Seems like thats where the extra argument is coming from. Trying printing your batch. Lemme know how it goes. Thanks.
		</comment>
		<comment id='4' author='bangbangjim' date='2020-01-26T17:32:49Z'>
		The error is here:
&lt;denchmark-code&gt;def training_step(self, batch, batch_nb):
&lt;/denchmark-code&gt;

When you use truncated_bptt_steps it will pass along the hidden states from the previous step.
You can fix this by adding
&lt;denchmark-code&gt;def training_step(self, batch, batch_nb, hiddens):
&lt;/denchmark-code&gt;

and passing the hiddens into your recurrent cells.
		</comment>
		<comment id='5' author='bangbangjim' date='2020-01-26T19:12:40Z'>
		I see thank you!
		</comment>
		<comment id='6' author='bangbangjim' date='2020-01-26T19:49:13Z'>
		it's not a new feature but also not very well documented. you'll want something like:
&lt;denchmark-code&gt;# Truncated back-propagation through time
def training_step(self, batch, batch_idx, hiddens):
    # hiddens are the hiddens from the previous truncated backprop step
    ...
    out, hiddens = self.lstm(data, hiddens)
    ...

    return {
        "loss": ...,
        "hiddens": hiddens  # remember to detach() this
    }
&lt;/denchmark-code&gt;




pytorch-lightning/pytorch_lightning/trainer/logging.py


         Line 165
      in
      7deec2c






 hiddens = output.get('hiddens') 








pytorch-lightning/pytorch_lightning/trainer/training_loop.py


         Line 584
      in
      b35c472






 # pass hiddens if using tbptt 





		</comment>
	</comments>
</bug>