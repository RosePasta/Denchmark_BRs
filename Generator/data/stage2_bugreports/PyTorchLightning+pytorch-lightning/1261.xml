<bug id='1261' author='Dunrar' open_date='2020-03-27T12:35:53Z' closed_time='2020-04-14T18:22:27Z'>
	<summary>Neptune.ai logger slow, lags behind training</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When running a script which trains multiple models after another, I came across the problem that the neptune.ai logger lags behind my training quite severely ( when the model has finished training, the logger is only about halfway there). This would not be such a big problem for me if the next model would start training while the logger still processes the logs of the previous model, but training continues only after the logger has finished.
I tried logging fewer things, only logging metrics per epoch and disabling hardware monitoring by uninstalling psutil. I don't know if this is really a bug, but even if it is not, is there maybe a way to bypass/mitigate this? Is this maybe related to the windows timing error from &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1186&gt;#1186&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jakubczakon&gt;@jakubczakon&lt;/denchmark-link&gt;
?
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Use the neptune.ai logger (with Windows)
Log a metric

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

The logger should keep up with training or continue logging previous models while another model is already being trained.
	</description>
	<comments>
		<comment id='1' author='Dunrar' date='2020-03-27T13:07:32Z'>
		Hi &lt;denchmark-link:https://github.com/Dunrar&gt;@Dunrar&lt;/denchmark-link&gt;
 are you closing loggers per model explicitly:
neptune_logger_1 = NeptuneLogger(close_after_fit=False)

# your logic

neptune_logger_1.experiment.stop()

neptune_logger_2 = NeptuneLogger(close_after_fit=False)

# your logic

neptune_logger_2.experiment.stop()
Could you elaborate on what do you mean by model here?
		</comment>
		<comment id='2' author='Dunrar' date='2020-03-27T15:36:32Z'>
		Yes, I tried that as well. By model I mean model = MemoryTest(hparams). My code looks like this:
&lt;denchmark-code&gt;agg_losses = []
    for celltype in ['RNN', 'LSTM', 'GRU']:
        setattr(hparams, 'celltype', celltype)
        for layer in range(1, 4):
            setattr(hparams, 'n_layers', layer)
            for cell in range(1, 6):
                setattr(hparams, 'n_cells', cell)
                agg_losses_pred_col = []
                for predict_col in range(1, 6):
                    setattr(hparams, 'predict_col', predict_col)
                    print(' Updated hyperparameters: ', hparams)
                    agg_losses_runs = []
                    for run in range(10):
                        exp_name = 'Type ' + celltype + ', Layers ' + str(layer) + ', Cells ' + str(cell) + ', PredCol ' + str(predict_col) + ', Run ' + str(run)

                        neptune_logger = NeptuneLogger(
                            project_name="dunrar/bachelor-thesis",
                            experiment_name=exp_name,
                            params=vars(hparams),
                            close_after_fit=False
                            # tags=['truncated bptt', 'cyclical learning rates', 'epoch data reload', ...]
                        )
                        if hparams.early_stopping == 'yes':
                            early_stopping = EarlyStopping(
                                monitor='batch/mean_absolute_loss',
                                min_delta=hparams.min_delta,
                                patience=hparams.patience,
                                mode='min'
                            )
                        else:
                            early_stopping = False

                        if hparams.reload_data == 'yes':
                            reload_data = True
                        else:
                            reload_data = False

                        model = MemoryTest(hparams)
                        trainer = pl.Trainer(
                            benchmark=True,
                            checkpoint_callback=False,
                            logger=neptune_logger,
                            reload_dataloaders_every_epoch=reload_data,
                            gpus=hparams.cuda,
                            val_percent_check=0,
                            early_stop_callback=early_stopping,
                            default_save_path=src.settings.LOG_DIR,
                            max_epochs=hparams.epochs
                        )
                        trainer.fit(model)
                        neptune_logger.experiment.stop()
                        agg_losses_runs.append(model.final_loss)
                    agg_losses_pred_col.append(np.mean(agg_losses_runs))
                agg_losses.insert(0, agg_losses_pred_col)
&lt;/denchmark-code&gt;

But even when I only train one model and the model stops/finishes training (CPU load goes down as well) it takes a lot of time until the logger catches up and the console becomes responsive again.
		</comment>
		<comment id='3' author='Dunrar' date='2020-03-27T15:39:51Z'>
		Got it, let me talk to the devs cause I think this should work nicely.
How long does one iteration of this loop take on average?
		</comment>
		<comment id='4' author='Dunrar' date='2020-03-27T16:38:53Z'>
		Hi &lt;denchmark-link:https://github.com/Dunrar&gt;@Dunrar&lt;/denchmark-link&gt;
 , I'm one of the developers of Neptune.ai.
The stop() method waits for some threads to join. One of those threads is responsible for sending channel values and stderr/out.
My current suspicion is that this problem is related to another problem with stderr/out we have on Windows.
Can we check what happens when you do the following (upload_std*=False)?
neptune_logger = NeptuneLogger(
                            project_name="dunrar/bachelor-thesis",
                            experiment_name=exp_name,
                            params=vars(hparams),
                            close_after_fit=False,
                            # tags=['truncated bptt', 'cyclical learning rates', 'epoch data reload', ...],
                            upload_stderr=False,
                            upload_stdout=False
                        )
Does the problem persist then?
		</comment>
		<comment id='5' author='Dunrar' date='2020-03-28T11:29:49Z'>
		Hey &lt;denchmark-link:https://github.com/pitercl&gt;@pitercl&lt;/denchmark-link&gt;
 , that's it, way faster now. It takes care of the other problem as well, of course.
		</comment>
		<comment id='6' author='Dunrar' date='2020-03-31T11:31:51Z'>
		Hi &lt;denchmark-link:https://github.com/Dunrar&gt;@Dunrar&lt;/denchmark-link&gt;
, we're working on a fix. It should be release this week.
		</comment>
		<comment id='7' author='Dunrar' date='2020-04-01T14:56:04Z'>
		Hi &lt;denchmark-link:https://github.com/Dunrar&gt;@Dunrar&lt;/denchmark-link&gt;
, I've released  in version , which should fix the problem with colliding x-s in stdout/stderr.
However, the problem with slow closing of experiments seems to lie elsewhere. I'll look into that as well and see what can be done.
Just wanted to give you a head's up on the progress.
		</comment>
		<comment id='8' author='Dunrar' date='2020-04-01T22:10:57Z'>
		Thank you &lt;denchmark-link:https://github.com/pitercl&gt;@pitercl&lt;/denchmark-link&gt;
, I will check it out tomorrow!  and  already took care of the neptune logger lagging behind the training and it closes with just a short delay. Seems fine to me, I can't compare it to anything though.
Edit: Works now. Even without disabling stderr/out uploading the logger no longer lags behind the training.
Edit 2: No Error but still slow logging/closing, sorry, the number of epochs was just to small to notice it
		</comment>
		<comment id='9' author='Dunrar' date='2020-04-06T20:21:28Z'>
		Yeah, that's expected. I will work on it sometime this week, and hopefully arrive at some improvement.
		</comment>
		<comment id='10' author='Dunrar' date='2020-04-14T17:28:00Z'>
		&lt;denchmark-link:https://github.com/pitercl&gt;@pitercl&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jakubczakon&gt;@jakubczakon&lt;/denchmark-link&gt;
 will you send changes to PL via PR or is it only Neptune task/issue?
		</comment>
		<comment id='11' author='Dunrar' date='2020-04-14T18:12:23Z'>
		Hi &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
, this is a Neptune-only task. I think this can be closed as it's not related to PL directly.
		</comment>
	</comments>
</bug>