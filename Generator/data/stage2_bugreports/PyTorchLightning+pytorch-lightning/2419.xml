<bug id='2419' author='hfwittmann' open_date='2020-06-29T15:18:44Z' closed_time='2020-07-03T19:17:21Z'>
	<summary>TPU MNIST demo hangs in last batch</summary>
	<description>
I am afraid this is not working for me.
_Remark : There have been various posts about this or very similar issues, but as far as I can see they have all been closed.
Example: &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1590&gt;#1590&lt;/denchmark-link&gt;

In fact I posted this exact comment in the following issue, when it was already closed.
&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1403&gt;#1403&lt;/denchmark-link&gt;

I am therefore creating this issue, because I think the closed issue is probably not receiving any attention, which is understandable._
now:
I have tried all the versions, given in the notebook. (The links are given below in NB1)
Additionally, I have also tried it with the version 20200516. That version is given in the official colab TPU MNIST example notebook which does not use pytorch-lightening, ie 20200516. A reference is below in NB2.
The summary of the results are:
"1.5" : wont run at all
"20200325" hangs in the final epoch (with 10 epochs in the 10th, with 3 epochs in the 3rd)
"nightly" crashes with : Exception: process 0 terminated with signal SIGABRT
"20200516" hangs after one epoch
I have tried this several times over the last few days. With the exception of the nightly all these results have always been the same.
NB1:
Locally I am on a Mac, not sure whether this makes a difference.
My terminal gives this
uname -a
Darwin osx-lhind6957 18.7.0 Darwin Kernel Version 18.7.0: Mon Apr 27 20:09:39 PDT 2020; root:xnu-4903.278.35~1/RELEASE_X86_64 x86_64
NB2:
The links for that official colab TPU MNIST example notebook which does not use pytorch lightning are here:
&lt;denchmark-link:https://cloud.google.com/tpu/docs/colabs?hl=de&gt;https://cloud.google.com/tpu/docs/colabs?hl=de&lt;/denchmark-link&gt;

&lt;denchmark-link:https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/mnist-training.ipynb?authuser=1#scrollTo=sPJVqAKyml5W&gt;https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/mnist-training.ipynb?authuser=1#scrollTo=sPJVqAKyml5W&lt;/denchmark-link&gt;

(The official notebook which does not use pytorch lightning has no problem and runs through with 20200516)
	</description>
	<comments>
		<comment id='1' author='hfwittmann' date='2020-06-29T15:19:39Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='hfwittmann' date='2020-06-29T15:39:04Z'>
		Same hangup for the 'MNIST hello world' notebook - computation stops at 72% for the first epoch.
&lt;denchmark-link:https://colab.research.google.com/drive/1F_RNcHzTfFuQf-LeKvSlud6x7jXYkG31#scrollTo=zM15oxCH5lo6&gt;https://colab.research.google.com/drive/1F_RNcHzTfFuQf-LeKvSlud6x7jXYkG31#scrollTo=zM15oxCH5lo6&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='hfwittmann' date='2020-06-30T00:38:07Z'>
		tqdm crashes colab... the code is running but colab is freezing (sometimes you would have noticed you had to restart your browser...). The demos are updated to refresh less frequently.
Set the following to not update the UI so frequently.
&lt;denchmark-code&gt;Trainer(progress_bar_refresh_rate=20)
&lt;/denchmark-code&gt;

Just tested and it works as expected.
Re the HF examples, currently working on updating them since they are out of sync &lt;denchmark-link:https://github.com/hfwittmann&gt;@hfwittmann&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='hfwittmann' date='2020-06-30T08:28:40Z'>
		I am still out of luck, it seems.
None of the combinations I tried work.
Again in the Live Demo book from &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/tpu.html&gt;https://pytorch-lightning.readthedocs.io/en/latest/tpu.html&lt;/denchmark-link&gt;

changing
from
trainer = Trainer(num_tpu_cores=8, progress_bar_refresh_rate=5, max_epochs=10)
to
trainer = Trainer(num_tpu_cores=8, progress_bar_refresh_rate=20, max_epochs=10)
resulted in
"1.5" : hangs in the final epoch
"20200325" hangs in the final epoch
"nightly" crashes with : Exception: process 0 terminated with signal SIGABRT
So there is a change in behavior for "1.5" from "wont run at all" to "hangs in the final epoch"
&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 could you kindly post the version of the workbook that runs through?
		</comment>
		<comment id='5' author='hfwittmann' date='2020-06-30T12:12:20Z'>
		Ok, i think it's something to do with the shutdown sequence for finishing training.
But to clarify, this demo DOES work, but the LAST batch hangs...
&lt;denchmark-link:https://user-images.githubusercontent.com/3640001/86124559-5bc38100-baa9-11ea-90b8-88af45f9f309.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/3640001/86124588-68e07000-baa9-11ea-9e87-50496e8796a9.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='hfwittmann' date='2020-06-30T12:13:06Z'>
		&lt;denchmark-link:https://github.com/lezwon&gt;@lezwon&lt;/denchmark-link&gt;
 want to take a look at this one?
		</comment>
		<comment id='7' author='hfwittmann' date='2020-06-30T12:16:14Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 will have a look at it.
		</comment>
		<comment id='8' author='hfwittmann' date='2020-07-02T08:17:23Z'>
		For what it's worth, it looks like I'm running into the same issue outside of a Colab as well (run from a regular python session).
		</comment>
		<comment id='9' author='hfwittmann' date='2020-07-02T09:20:19Z'>
		I think there's an issue with the lastest xla releases due to which there is a SIGABRT error occurring. It's related to this: &lt;denchmark-link:https://github.com/pytorch/xla/issues/2246&gt;pytorch/xla#2246&lt;/denchmark-link&gt;
. I haven't been able to zero in on it though.
		</comment>
		<comment id='10' author='hfwittmann' date='2020-07-03T19:17:20Z'>
		Fixed on master! also, make sure to use xla nightly
		</comment>
		<comment id='11' author='hfwittmann' date='2020-07-03T19:51:32Z'>
		hi, &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 I think I have my commands confused, to install master from pip would it be ?
&lt;denchmark-code&gt;!pip install https://github.com/PyTorchLightning/pytorch-lightning.git
&lt;/denchmark-code&gt;

		</comment>
		<comment id='12' author='hfwittmann' date='2020-07-03T20:19:52Z'>
		pip install git+&lt;denchmark-link:https://github.com/PytorchLightning/pytorch-lightning.git@master&gt;https://github.com/PytorchLightning/pytorch-lightning.git@master&lt;/denchmark-link&gt;
 --upgrade
		</comment>
		<comment id='13' author='hfwittmann' date='2020-07-03T21:08:47Z'>
		After modifying the notebook to pip install from master, it still seems to be getting stuck at the start of Epoch 2.
		</comment>
		<comment id='14' author='hfwittmann' date='2020-07-03T21:10:26Z'>
		are you using xla nightly? and also increase the refresh rate... otherwise colab will freeze since the progress bar updates to the screen too much
		</comment>
		<comment id='15' author='hfwittmann' date='2020-07-03T21:15:24Z'>
		Yep, it's set to install XLA nightly, and progress_bar_refresh_rate is set to 20.
		</comment>
		<comment id='16' author='hfwittmann' date='2020-07-03T21:27:12Z'>
		Tested it on Kaggle too. It freezes at the beginning of epoch 2. But works when checkpoint_callback=False.
		</comment>
		<comment id='17' author='hfwittmann' date='2020-07-03T22:58:37Z'>
		ummm good to know.
i think itâ€™s something about writing weights? maybe processes are colliding?
		</comment>
		<comment id='18' author='hfwittmann' date='2020-07-04T03:26:33Z'>
		
Tested it on Kaggle too. It freezes at the beginning of epoch 2. But works when checkpoint_callback=False.

Same situation here. I tested on GCP.
		</comment>
		<comment id='19' author='hfwittmann' date='2020-07-04T05:04:47Z'>
		Setting checkpoint_callback=False, I'm back to the demo hanging on the last batch of the last epoch.
		</comment>
		<comment id='20' author='hfwittmann' date='2020-07-04T10:17:06Z'>
		Same here, problems persist.
I tested on colab.

with checkpoint_callback=True, hangs at beginning of 2nd epoch
with checkpoint_callback=False hangs at the end of last epoch

		</comment>
		<comment id='21' author='hfwittmann' date='2020-07-04T14:20:00Z'>
		ok fixed!
Turns out the bug is that without a validation loop the behavior is a bit strange right now.
For the moment, this is fixed on the demo now and will push an upcoming fix for the special case where you don't need a val loop.
Try the demo again!
BTW, you have to wait for all the shutdown stuff to happen at epoch 10... it looks like it's hanging but in reality it's terminating all the distributed stuff.
&lt;denchmark-link:https://user-images.githubusercontent.com/3640001/86514392-f1c81600-bddf-11ea-83bc-6814a78d4810.png&gt;&lt;/denchmark-link&gt;

(the 1 at the bottom means this was successful)
		</comment>
		<comment id='22' author='hfwittmann' date='2020-07-04T19:15:14Z'>
		Thanks and I have no idea how you find the time to work on all this!
		</comment>
	</comments>
</bug>