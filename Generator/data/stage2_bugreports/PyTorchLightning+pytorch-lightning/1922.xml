<bug id='1922' author='jeremyjordan' open_date='2020-05-22T03:10:57Z' closed_time='2020-09-23T14:41:52Z'>
	<summary>Trainer max_steps does not work when reloaded from checkpoint (if value is already exceeded)</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

The trainer flag for max_steps is not respected when you reload the Trainer from a checkpoint.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

This works:
&lt;denchmark-code&gt;import tests.base.utils as tutils
from pytorch_lightning import Trainer, LightningModule
from pytorch_lightning.callbacks import EarlyStopping, LearningRateLogger, ModelCheckpoint
from tests.base import EvalModelTemplate

def test_trainer_max_steps():
    model = EvalModelTemplate()
    trainer = Trainer(max_steps=1)
    trainer.fit(model)
    assert trainer.global_step == 1
&lt;/denchmark-code&gt;

This breaks:
&lt;denchmark-code&gt;def test_trainer_exceed_max_steps_from_checkpoint():
    model = EvalModelTemplate()
    checkpoint_callback = ModelCheckpoint(save_top_k=1)
    trainer = Trainer(checkpoint_callback=checkpoint_callback, max_epochs=4)
    trainer.fit(model)
    step_count = trainer.global_step

    checkpoint_filepath = checkpoint_callback.kth_best_model
    trainer = Trainer(max_steps=1,
                    resume_from_checkpoint=checkpoint_filepath)
    trainer.fit(model)
    # current step exceeds max_steps, training should stop immediately
    assert trainer.global_step == step_count
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Both of the above tests that I've written should pass.
We should also probably warn the user when they pass in a max_steps or max_epochs which exceeds the current state when reloading from a checkpoint.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:
- GPU:
- available:         False
- version:           None
Packages:
- numpy:             1.18.1
- pyTorch_debug:     False
- pyTorch_version:   1.4.0
- pytorch-lightning: 0.7.7-dev
- tensorboard:       2.1.0
- tqdm:              4.42.0
System:
- OS:                Darwin
- architecture:
- 64bit
-
- processor:         i386
- python:            3.7.6
- version:           Darwin Kernel Version 19.4.0: Wed Mar  4 22:28:40 PST 2020; root:xnu-6153.101.6~15/RELEASE_X86_64

	</description>
	<comments>
		<comment id='1' author='jeremyjordan' date='2020-06-08T10:52:45Z'>
		Hey! Can you verify this is still the case?
		</comment>
		<comment id='2' author='jeremyjordan' date='2020-06-10T23:02:04Z'>
		yeah, just change this condition from  to 


&lt;denchmark-link:https://github.com/edenlightning&gt;@edenlightning&lt;/denchmark-link&gt;
 mind send a PR?
		</comment>
		<comment id='3' author='jeremyjordan' date='2020-09-23T14:41:52Z'>
		fixed in force crash when max_epochs &lt; epochs in a checkpoint &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3580&gt;#3580&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>