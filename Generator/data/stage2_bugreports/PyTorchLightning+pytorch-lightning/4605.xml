<bug id='4605' author='aktersnurra' open_date='2020-11-10T14:37:28Z' closed_time='2020-11-11T08:55:25Z'>
	<summary>EarlyStopping BrokenPipeError: [Errno 32] Broken pipe</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

EarlyStopping together with num_workers &gt; 0 yields the following error message for me:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/home/user/.pyenv/versions/3.8.2/lib/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/home/user/.pyenv/versions/3.8.2/lib/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/home/user/.pyenv/versions/3.8.2/lib/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/home/user/.pyenv/versions/3.8.2/lib/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
&lt;/denchmark-code&gt;

I strongly believe this is the case, as setting num_workers=0 or not using the EarlyStopping callbacks removes this error message from showing up.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

This is probably coupled with reading data from a hdf5 file, with an implementation similar to this one:
&lt;denchmark-link:https://discuss.pytorch.org/t/dataloader-when-num-worker-0-there-is-bug/25643/16&gt;https://discuss.pytorch.org/t/dataloader-when-num-worker-0-there-is-bug/25643/16&lt;/denchmark-link&gt;

(It seems like h5 does not have good support multiprocessing handling.)
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version (e.g., 1.0): 1.7.0
OS (e.g., Linux): Ubuntu 20.04
How you installed PyTorch (conda, pip, source): poetry
Python version: 3.8.2
CUDA/cuDNN version: nvidia-cuda-toolkit
GPU models and configuration: 2080 ti

	</description>
	<comments>
		<comment id='1' author='aktersnurra' date='2020-11-10T14:38:17Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='aktersnurra' date='2020-11-10T15:44:07Z'>
		Hey &lt;denchmark-link:https://github.com/aktersnurra&gt;@aktersnurra&lt;/denchmark-link&gt;
 this seems hdf5 specific, would it be possible to get the fullstack trace or reproduce using this?
&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='aktersnurra' date='2020-11-11T08:55:25Z'>
		Hi &lt;denchmark-link:https://github.com/SeanNaren&gt;@SeanNaren&lt;/denchmark-link&gt;
,
I ran the bug_report_model.py, it is as you thought, a hdf5 bug. I suspect it has something to do with the threads reading from the h5 file and early stopping trying to terminate the training that causes the issue. I'll go dig for a solution in the h5 corner.
Thanks for the fast response,
EDIT: Found a solution to the problem,  set swmr to true, i.e.:
&lt;denchmark-code&gt;h5py.File(self.file_path, "r", libver="latest", swmr=True)
&lt;/denchmark-code&gt;

Cheers,
Gustaf
		</comment>
	</comments>
</bug>