<bug id='2238' author='williamFalcon' open_date='2020-06-18T15:02:14Z' closed_time='2020-07-10T01:20:48Z'>
	<summary>when no checkpoints are saved, test fails</summary>
	<description>
Build a model that doesn't save a checkpoint and this crashes.
It should use the last model instead.
&lt;denchmark-code&gt;model = ...
trainer = Trainer(fast_dev_run)
trainer.fit(model)
trainer.test()
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/yukw777&gt;@yukw777&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;torch.serialization._open_file object at 0x7f443d4d88d0&gt;, name = ''
mode = 'rb'

    def __init__(self, name, mode):
&gt;       super(_open_file, self).__init__(open(name, mode))
E       FileNotFoundError: [Errno 2] No such file or directory: ''
&lt;/denchmark-code&gt;

The test for this should be:
&lt;denchmark-code&gt;def test_no_ckpt_test(tmpdir):
    model = EvaluationModel()
    trainer = Trainer(fast_dev_run)
    trainer.fit(model)
    trainer.test()
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='williamFalcon' date='2020-06-18T16:43:31Z'>
		oops, i'll send out a fix soon!
		</comment>
		<comment id='2' author='williamFalcon' date='2020-06-24T17:03:01Z'>
		Hi,
I am currently facing this issue, is there any work around that I could do to resolve this?
		</comment>
		<comment id='3' author='williamFalcon' date='2020-06-24T18:24:33Z'>
		    trainer.test(model)
		</comment>
		<comment id='4' author='williamFalcon' date='2020-06-24T18:40:40Z'>
		&lt;denchmark-link:https://github.com/nischal-sanil&gt;@nischal-sanil&lt;/denchmark-link&gt;
 could you help me understand the issue better by explaining how you don’t have any models saved?
		</comment>
		<comment id='5' author='williamFalcon' date='2020-06-25T05:39:15Z'>
		I was running the Trainer with fast_dev_run set to True, hence there were no checkpoints. Changing  trainer.test() to trainer.test(model) has resolved the issue.
Thanks,
		</comment>
		<comment id='6' author='williamFalcon' date='2020-06-26T01:17:51Z'>
		Hi again,
Consider the following code snippet
&lt;denchmark-code&gt;class Net(LightningModule):
    ...

resnet18 = models.resnet18(pretrained=True)
model = Net(resnet18)

# parameters before training
old_params = model.parameters()

trainer = Trainer(max_steps=10) 
trainer.fit(model)  

# parameters after training 
new_params = model.parameters()
&lt;/denchmark-code&gt;

Where I store the parameters of the model before and after training in old_params and new_params, a check on their equality returns True, suggesting there're no changes to the parameters.
&lt;denchmark-code&gt;def check_params(old,new):
    return all([torch.equal(o,n) for o,n in zip(old,new)])

check_params(old_params,new_params)
## True
&lt;/denchmark-code&gt;

Therefore, while running  the trained model is not used for testing. I had assumed that the  would have updated the parameters to the , but from the above snippet that does not seem to be the case.  And I am not able to figure out if this is an expected behavior of the  or there is a problem with my class definition. If this is the expected behavior of the , then are there any work around for calling  without using a checkpoint because in my case loading from a checkpoint results in an error similar to this issue &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2359&gt;#2359&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='7' author='williamFalcon' date='2020-06-29T18:52:09Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 is it true that if  is , there is no checkpoints saved? I just tried to manually check this in , but  of the trainer still had a valid  even when  is .
&lt;denchmark-link:https://github.com/nischal-sanil&gt;@nischal-sanil&lt;/denchmark-link&gt;
 your code comparing the old parameters to the new parameters is not quite correct. Since the tensors returned by  are "references" to the underlying data, both  and  would point to the same "trained" tensors, hence  would return . You'd have to do something like  to get around that.
		</comment>
		<comment id='8' author='williamFalcon' date='2020-06-29T18:59:52Z'>
		fast_dev_run should not save checkpoints (nor write a logs file). it's like a "compiler"
		</comment>
		<comment id='9' author='williamFalcon' date='2020-06-29T19:55:15Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 hmm maybe there's a bug? I see this:
&lt;denchmark-code&gt;❯ python pl_examples/basic_examples/cpu_template.py --fast_dev_run True
/Users/peteryu/.pyenv/versions/pl/lib/python3.7/site-packages/graphql/type/directives.py:55: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working
  assert isinstance(locations, collections.Iterable), 'Must provide locations for directive.'
Running in fast_dev_run mode: will run a full train, val and test loop using a single batch
GPU available: False, used: False
TPU available: False, using: 0 TPU cores

  | Name      | Type        | Params | In sizes   | Out sizes 
--------------------------------------------------------------------
0 | c_d1      | Linear      | 39 M   | [2, 784]   | [2, 50000]
1 | c_d1_bn   | BatchNorm1d | 100 K  | [2, 50000] | [2, 50000]
2 | c_d1_drop | Dropout     | 0      | [2, 50000] | [2, 50000]
3 | c_d2      | Linear      | 500 K  | [2, 50000] | [2, 10]   
Epoch 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01&lt;00:00,  1.18it/s, loss=2.523, v_num=0]
❯ ls lightning_logs/version_0/checkpoints/epoch=0.ckpt 
lightning_logs/version_0/checkpoints/epoch=0.ckpt
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='williamFalcon' date='2020-07-10T01:20:47Z'>
		added a warning and return {}
		</comment>
	</comments>
</bug>