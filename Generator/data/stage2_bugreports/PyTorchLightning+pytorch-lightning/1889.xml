<bug id='1889' author='HansBambel' open_date='2020-05-19T10:28:37Z' closed_time='2020-05-19T17:16:27Z'>
	<summary>trainer.scale_batch_size() throws exception due to LRScheduler</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I tried finding the biggest possible batch_size for my training, but PL raises a MisconfigurationException saying that my LRScheduler (ReduceLROnPlateau) is conditioned on a metric that is only available after validation_epoch_end. The available metrics are: loss, val_loss.
I assume the LRScheduler requires a metric from the training loop for this to work? Why is this neccessary?
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Have a model with a metric that only exists in validation_epoch_end
Have a LRScheduler which monitors that metric
Use trainer.scale_batch_size
See error

&lt;denchmark-code&gt;File "C:\ProgramData\Anaconda3\envs\ml\lib\site-packages\pytorch_lightning\trainer\training_loop.py", line 779, in update_learning_rates
    raise MisconfigurationException(
pytorch_lightning.utilities.exceptions.MisconfigurationException: ReduceLROnPlateau conditioned on metric meanIoU which is not available. Available metrics are: loss,train_loss. Condition can be set using `monitor` key in lr scheduler dict

&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;trainer = pl.Trainer(gpus=hparams.gpus)
new_batch_size = trainer.scale_batch_size(net, mode='binsearch', init_val=8)
&lt;/denchmark-code&gt;

and in my model:
&lt;denchmark-code&gt;    def configure_optimizers(self):
        opt = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)
        scheduler = {
         'scheduler': optim.lr_scheduler.ReduceLROnPlateau(opt, mode="max", factor=0.5, patience=5),
         'monitor': 'meanIoU',  # Default: val_loss
        }
        return [opt], [scheduler]

    def validation_epoch_end(self, outputs):
        avg_loss = torch.stack([x["val_loss"] for x in outputs]).mean()
        iou_class, mean_iou = self.iou_metric.value()
        mean_iou = torch.tensor(mean_iou)
        self.iou_metric.reset()
        logs = {"val_loss": avg_loss, "meanIoU": mean_iou}
        return {"meanIoU": mean_iou, "log": logs,
                "progress_bar": {"val_loss": avg_loss, "meanIoU": mean_iou}}
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

No Exception and the maximum batch_size for my model.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:

GPU:

GeForce RTX 2070 SUPER


available:         True
version:           10.1


Packages:

numpy:             1.18.1
pyTorch_debug:     False
pyTorch_version:   1.4.0
pytorch-lightning: 0.7.6
tensorboard:       2.1.0
tqdm:              4.45.0


System:

OS:                Windows
architecture:

64bit
WindowsPE


processor:         AMD64 Family 23 Model 113 Stepping 0, AuthenticAMD
python:            3.8.2
version:           10.0.18362



&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='HansBambel' date='2020-05-19T11:52:22Z'>
		Just to be sure, this error only happens when you run the .scale_batch_size(...) method and not when you run .fit(...)?
		</comment>
		<comment id='2' author='HansBambel' date='2020-05-19T13:05:33Z'>
		
Just to be sure, this error only happens when you run the .scale_batch_size(...) method and not when you run .fit(...)?

Correct
		</comment>
		<comment id='3' author='HansBambel' date='2020-05-19T13:14:53Z'>
		The problem here is that learning rates are updated right after the training_epoch has returned, even when we have reached max_steps and should just run the training_teardown



pytorch-lightning/pytorch_lightning/trainer/training_loop.py


        Lines 348 to 355
      in
      ac76dfc






 self.run_training_epoch() 



 



 # update LR schedulers 



 self.update_learning_rates(interval='epoch') 



 



 if self.max_steps and self.max_steps == self.global_step: 



 self.run_training_teardown() 



 return 





We could probably just switch order of the two statements and this will be solved.
		</comment>
		<comment id='4' author='HansBambel' date='2020-05-19T13:18:44Z'>
		Great that you found the reason for that behavior already!
		</comment>
		<comment id='5' author='HansBambel' date='2020-05-19T13:19:50Z'>
		&lt;denchmark-link:https://github.com/HansBambel&gt;@HansBambel&lt;/denchmark-link&gt;
 can you locally try if this works for you, and if it does, would you be up for doing a PR?
		</comment>
		<comment id='6' author='HansBambel' date='2020-05-19T13:46:52Z'>
		
@HansBambel can you locally try if this works for you, and if it does, would you be up for doing a PR?

I'll try it out!
		</comment>
		<comment id='7' author='HansBambel' date='2020-05-19T15:10:37Z'>
		&lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
 it worked! I created a PR &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1891&gt;#1891&lt;/denchmark-link&gt;
 for it.
		</comment>
	</comments>
</bug>