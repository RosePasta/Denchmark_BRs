<bug id='981' author='rmrao' open_date='2020-02-28T23:27:35Z' closed_time='2020-06-01T15:00:35Z'>
	<summary>WandbLogger cannot be used with 'ddp'</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

wandb modifies init such that a child process calling init returns None if the master process has called init. This seems to cause a bug with ddp, and results in rank zero having experiment = None, which crashes the program.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Can be reproduced with the basic MNIST gpu template, simply add a WandbLogger and pass 'ddp' as the distributed backend.
&lt;denchmark-code&gt;-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/rmrao/anaconda3/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 19, in _wrap
    fn(i, *args)
  File "/home/rmrao/anaconda3/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py", line 331, in ddp_train
    self.run_pretrain_routine(model)
  File "/home/rmrao/anaconda3/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 757, in run_pretrain_routine
    self.logger.log_hyperparams(ref_model.hparams)
  File "/home/rmrao/anaconda3/lib/python3.6/site-packages/pytorch_lightning/logging/base.py", line 14, in wrapped_fn
    fn(self, *args, **kwargs)
  File "/home/rmrao/anaconda3/lib/python3.6/site-packages/pytorch_lightning/logging/wandb.py", line 79, in log_hyperparams
    self.experiment.config.update(params)
AttributeError: 'NoneType' object has no attribute 'config'
&lt;/denchmark-code&gt;

This occurs with the latest wandb version and with pytorch-lightning 0.6.
	</description>
	<comments>
		<comment id='1' author='rmrao' date='2020-02-28T23:28:14Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='rmrao' date='2020-02-29T01:08:40Z'>
		Some hacky solutions: calling reinit=True for wandb, adding or this terrible hack:
def init_ddp_connection(self, *args, **kwargs):
    super().init_ddp_connection(*args, **kwargs)

    if torch.distributed.get_rank() == 0:
        import wandb
        wandb.run = None
These both seem to only kind-of work and result in multiple independent calls to wandb.init. I think the ideal solution is that the experiment is only ever initialized on rank zero. However doing this means that wandb cannot be initialized in the master thread at all.
Better than this probably requires some changes to the wandb API.
		</comment>
		<comment id='3' author='rmrao' date='2020-02-29T01:34:42Z'>
		Following up slightly - my hacky solution doesn't really work. It's easy enough though to get the rank zero only solution to work. If this seems like a reasonable solution, let me know and I'll submit a PR.
		</comment>
		<comment id='4' author='rmrao' date='2020-02-29T09:01:53Z'>
		well, have observed some issues with  earlier &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/906&gt;#906&lt;/denchmark-link&gt;
 could you check it?
		</comment>
		<comment id='5' author='rmrao' date='2020-03-04T18:12:34Z'>
		Hmm, I think this is a slightly different issue (I'm running on Ubuntu so I don't think that's the issue). Pickling also works correctly.
		</comment>
		<comment id='6' author='rmrao' date='2020-03-04T18:38:52Z'>
		This particular problem I think stems from this part of the wandb.init(...) code:
def init(...):
    ...
    # If a thread calls wandb.init() it will get the same Run object as
    # the parent. If a child process with distinct memory space calls
    # wandb.init(), it won't get an error, but it will get a result of
    # None.
    # This check ensures that a child process can safely call wandb.init()
    # after a parent has (only the parent will create the Run object).
    # This doesn't protect against the case where the parent doesn't call
    # wandb.init but two children do.
    if run or os.getenv(env.INITED):
        return run
Child processes end up getting None for the wandb run object, which causes logging to fail. There are probably two reasonable and complementary solutions:

The main thread should avoid creating a wandb experiment unless absolutely necessary.

Right now, &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/e586ed47674fd78b158322bb7b14d00aeb912abd/pytorch_lightning/loggers/wandb.py#L63-L69&gt;this&lt;/denchmark-link&gt;
 is the only part of the logging code that the parent thread calls (I assume it's called when pickling):
def __getstate__(self):
    state = self.__dict__.copy()
    # cannot be pickled
    state['_experiment'] = None
    # args needed to reload correct experiment
    state['_id'] = self.experiment.id
    return state
If this is changed to:
def __getstate__(self):
    state = self.__dict__.copy()
    # args needed to reload correct experiment
    if self._experiment is not None:
        state['_id'] = self._experiment.id
    else:
        state['_id'] = None

    # cannot be pickled
    state['_experiment'] = None
    return state
That will ensure that unless the user explicitly logs something or creates the wandb experiment first, then the main thread will not try to create an experiment. Since subsequent logging / saving code is wrapped by the @rank_zero_only decorator, this will generally solve the issue in the base case.
It's also possible that &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/e586ed47674fd78b158322bb7b14d00aeb912abd/pytorch_lightning/loggers/wandb.py#L112-L118&gt;these properties&lt;/denchmark-link&gt;
 are also called by master. Ideally they would be wrapped to not create the experiment unless it had been already created (i.e. experiment should only be created by a function that is wrapped with the  decorator).

If the main thread has created an experiment, rank zero should be passed the re-init argument.

wandb does allow you to reinitialize the experiment. I tried to play around with this a little bit and got some errors, but in theory adding this:
wandb.init(..., reinit=dist.is_available() and dist.is_initialized() and dist.get_rank() == 0)
should force a re-initialization when wandb is already initialzed for rank zero.
		</comment>
		<comment id='7' author='rmrao' date='2020-03-14T23:22:11Z'>
		&lt;denchmark-link:https://github.com/rmrao&gt;@rmrao&lt;/denchmark-link&gt;
 I just made a PR.  It should be safe to always set reinit=True.  I wasn't able to reproduce the specific environment you mentioned.  Can you share some code or a Colab with me to be sure this works ok?
		</comment>
		<comment id='8' author='rmrao' date='2020-03-14T23:40:36Z'>
		I think I had tried setting reinit without fixing other calls to avoid creating the experiment unless necessary. I don‚Äôt think this will produce errors.
I haven‚Äôt experimented with reinit significantly and I‚Äôm not sure what it does in terms of creating a new run in wandb, etc. but I don‚Äôt think I have a setup that explicitly causes an error with reinit.
		</comment>
		<comment id='9' author='rmrao' date='2020-03-15T08:10:27Z'>
		It would be good also add test for logger in ddp mode
		</comment>
		<comment id='10' author='rmrao' date='2020-04-01T18:05:12Z'>
		Has there been any progress on this? What is the fix to enable WandbLogger on 'ddp'?
		</comment>
		<comment id='11' author='rmrao' date='2020-04-28T19:11:53Z'>
		Is there any update to this? I'm also hoping to use wandb with ddp. I've upgraded to the most recent release 0.7.5 but the problem persists
		</comment>
		<comment id='12' author='rmrao' date='2020-04-28T19:18:08Z'>
		&lt;denchmark-link:https://github.com/vanpelt&gt;@vanpelt&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/shawnlewis&gt;@shawnlewis&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='rmrao' date='2020-04-29T20:11:43Z'>
		Can someone provide a small script to reproduce this?  We can work on a fix and include it in the next release.
		</comment>
		<comment id='14' author='rmrao' date='2020-05-01T20:13:44Z'>
		&lt;denchmark-link:https://github.com/julianmack&gt;@julianmack&lt;/denchmark-link&gt;
 try 0.7.3
		</comment>
		<comment id='15' author='rmrao' date='2020-05-07T23:26:06Z'>
		I found a similar bug with the WandB logger where the checkpointing callback expects the logger.name property to not be None



pytorch-lightning/pytorch_lightning/trainer/callback_config.py


        Lines 54 to 59
      in
      3a64260






 ckpt_path = os.path.join( 



 save_dir, 



 self.logger.name, 



 version, 



 "checkpoints" 



 ) 





However, WandB will set the logger.name and logger.version properties to None if experiments are not being used.



pytorch-lightning/pytorch_lightning/loggers/wandb.py


        Lines 124 to 133
      in
      3a64260






 @property 



 def name(self) -&gt; str: 



 # don't create an experiment if we don't have one 



 name = self._experiment.project_name() if self._experiment else None 



 return name 



 



 @property 



 def version(self) -&gt; str: 



 # don't create an experiment if we don't have one 



 return self._experiment.id if self._experiment else None 





		</comment>
		<comment id='16' author='rmrao' date='2020-05-07T23:40:54Z'>
		I worked around this issue by adding the following line to the top of my training job
setattr(WandbLogger, 'name', property(lambda self: self._name))
		</comment>
		<comment id='17' author='rmrao' date='2020-05-16T21:27:59Z'>
		Just to clarify &lt;denchmark-link:https://github.com/parasj&gt;@parasj&lt;/denchmark-link&gt;
's solution, presumably you also have to pass in some  as a keyword arg while constructing the logger.
Unfortunately this means you're now responsible for generating unique names for subsequent runs.
		</comment>
	</comments>
</bug>