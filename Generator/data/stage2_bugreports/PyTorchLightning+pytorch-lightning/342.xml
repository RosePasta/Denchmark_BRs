<bug id='342' author='williamFalcon' open_date='2019-10-09T00:53:29Z' closed_time='2019-10-09T12:21:44Z'>
	<summary>Logger fails at scale</summary>
	<description>
Didn't happen before, but now this is happening:
When running on 1 or 2 nodes the logger works fine.
When running on 2+ nodes nothing logs...
Might be a race condition which dist.barrier() might fix.
&lt;denchmark-link:https://github.com/neggert&gt;@neggert&lt;/denchmark-link&gt;
 any thoughts?
	</description>
	<comments>
		<comment id='1' author='williamFalcon' date='2019-10-09T03:36:09Z'>
		I can't get anything to log other than epoch and gpu details, even with single GPU. Is this coming from trainer.trainer around _process_output()? If someone can point me in the right direction, maybe add in some prints() or extra error messages, I might be able to submit a PR.
		</comment>
		<comment id='2' author='williamFalcon' date='2019-10-09T12:21:44Z'>
		ok, fixed the issue. I was still referencing the logger.experiment object outside of the trainer. This shouldn't be done in any way.
&lt;denchmark-link:https://github.com/jamesjjcondon&gt;@jamesjjcondon&lt;/denchmark-link&gt;
 verify with this fix?
To be clear, the TestTubeLogger works for me in:

single gpu
dp
ddp
ddp2
ddp, ddp2 with 2 nodes
ddp, ddp3 with 2+ nodes

		</comment>
		<comment id='3' author='williamFalcon' date='2019-10-10T11:43:49Z'>
		Yep. working now. Cheers. And I just saw your 9-steps medium article from July. Super helpful. Thanks!
		</comment>
	</comments>
</bug>