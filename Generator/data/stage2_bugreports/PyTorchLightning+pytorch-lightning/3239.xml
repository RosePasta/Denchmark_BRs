<bug id='3239' author='s-rog' open_date='2020-08-28T03:15:04Z' closed_time='2020-08-31T01:18:53Z'>
	<summary>TrainResult log calling reduce_across_time by default (causing error)</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Like the title says, tbptt reduction is being called on trainresult by default and causing crashes.
&lt;denchmark-code&gt;-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 20, in _wrap
    fn(i, *args)
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/accelerators/ddp_spawn_backend.py", line 165, in ddp_train
    results = self.trainer.run_pretrain_routine(model)
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 1237, in run_pretrain_routine
    self.train()
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 396, in train
    self.run_training_epoch()
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 543, in run_training_epoch
    self.run_training_epoch_end(epoch_output, checkpoint_accumulator, early_stopping_accumulator, num_optimizers)
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 672, in run_training_epoch_end
    epoch_log_metrics, epoch_progress_bar_metrics = self.__auto_reduce_results_on_epoch_end(epoch_output)
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 696, in __auto_reduce_results_on_epoch_end
    tbptt_outs = tbptt_outs[0].__class__.reduce_across_time(tbptt_outs)
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/step_result.py", line 392, in reduce_across_time
    result[k] = tbptt_reduce_fx(value)
TypeError: mean(): argument 'input' (position 1) must be Tensor, not list
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;# loss being a scalar tensor
result = pl.TrainResult(loss)
result.log("loss/t", loss, on_step=False, on_epoch=True)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

I'm not sure if calling tbptt reduction by default is intended or an oversight, some clarification would help me start working on a PR!
	</description>
	<comments>
		<comment id='1' author='s-rog' date='2020-08-31T01:18:53Z'>
		I found the source of the issue, will post another issue as this one has an inaccurate description.
		</comment>
	</comments>
</bug>