<bug id='2730' author='zhenhuahu' open_date='2020-07-28T00:29:43Z' closed_time='2020-07-29T20:22:15Z'>
	<summary>Horovod and Native Amp not work</summary>
	<description>
##üêõ Bug
I'm no sure if there is a bug. But when I was tryng to use Horovod as backend to do native amp in PyTorch 1.6, Lightning always points to the function that uses apex amp instead.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Go to 'trainer/distrib_parts.py'
Run '....'
Scroll down to '....'
See error

&lt;denchmark-link:https://user-images.githubusercontent.com/11988890/88603491-12803600-d043-11ea-8b9b-31193cf900e3.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

trainer = pl.Trainer(gpus=1, num_nodes = 1, distributed_backend='horovod', precision = 16)
trainer.fit(generator, train_dataloader=train_loader, val_dataloaders=val_loader)
I tried to run it like: horovodrun  --verbose -np 32 -H server1-0:8,server2-0:8,server3-0:8,server4-0:8     python file.py
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Use native amp when trying to launch Horovod in PyTorch 1.6.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

Please copy and paste the output from our
&lt;denchmark-link:https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py&gt;environment collection script&lt;/denchmark-link&gt;

(or fill out the checklist below manually).
You can get the script and run it with:
&lt;denchmark-code&gt;wget https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py
# For security purposes, please check the contents of collect_env_details.py before running it.
python collect_env_details.py
&lt;/denchmark-code&gt;

CUDA:
- GPU:
- Tesla V100-SXM2-32GB
- Tesla V100-SXM2-32GB
- Tesla V100-SXM2-32GB
- Tesla V100-SXM2-32GB
- Tesla V100-SXM2-32GB
- Tesla V100-SXM2-32GB
- Tesla V100-SXM2-32GB
- Tesla V100-SXM2-32GB
- available:         True
- version:           10.1

Packages:

numpy:             1.19.1
pyTorch_debug:     False
pyTorch_version:   1.6.0.dev20200623+cu101
pytorch-lightning: 0.8.5
tensorboard:       2.3.0
tqdm:              4.48.0


System:

OS:                Linux
architecture:

64bit



processor:         x86_64
python:            3.6.9
version:           #83~16.04.1-Ubuntu SMP Wed Dec 18 04:56:23 UTC 2019




How you installed PyTorch (conda, pip, source): pip
cuDNN version: 7.6.5

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='zhenhuahu' date='2020-07-29T20:22:14Z'>
		Horovod is incompatible with pt 1.6. This is &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2744&gt;disabled&lt;/denchmark-link&gt;
 until horovod issue is fixed.
		</comment>
		<comment id='2' author='zhenhuahu' date='2020-11-16T00:51:16Z'>
		Does horovod work yet with native amp? I'm getting:

AssertionError: optimizer.zero_grad() was called after loss.backward() but before optimizer.step() or optimizer.synchronize(). This is prohibited as it can cause a race condition.

From &lt;denchmark-link:https://horovod.readthedocs.io/en/latest/_modules/horovod/torch/optimizer.html&gt;https://horovod.readthedocs.io/en/latest/_modules/horovod/torch/optimizer.html&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>