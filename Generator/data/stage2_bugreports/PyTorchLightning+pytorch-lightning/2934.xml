<bug id='2934' author='johngrabner' open_date='2020-08-12T15:46:14Z' closed_time='2020-08-15T12:05:59Z'>
	<summary>warm up LR causes crash</summary>
	<description>
My resnet encoder and transformer decoder are not training well.  So trying all kinds of stuff.
Latest attempt to improve is to use a warmup learning rate as described here:
&lt;denchmark-link:url&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/master/docs/source/optimizers.rst&lt;/denchmark-link&gt;

My code is an exact copy:
&lt;denchmark-code&gt;    def optimizer_step(self, epoch_nb, batch_nb, optimizer, optimizer_i, opt_closure):
        if self.trainer.global_step &lt; 500:
            lr_scale = min(1., float(self.trainer.global_step + 1) / 500.)
            for pg in optimizer.param_groups:
                pg['lr'] = lr_scale * self.hparams.learning_rate

                print(f"lr={pg['lr']}")
        optimizer.step()
        optimizer.zero_grad()
&lt;/denchmark-code&gt;

The crash is as follows:
&lt;denchmark-code&gt;Epoch 1:   0%|‚ñç                                                                                                                     | 15/3544 [00:27&lt;1:48:33,  1.85s/it, loss=nan, v_num=19]Traceback (most recent call last):
  File "kiss_transformer.py", line 539, in &lt;module&gt;
    trainer.fit(model, train_loader)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/states.py", line 34, in wrapped_fn
    result = fn(self, *args, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in fit
    self.accelerator_backend.train(model, nprocs=self.num_processes)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_spawn_backend.py", line 42, in train
    mp.spawn(self.ddp_train, nprocs=nprocs, args=(self.mp_queue, model,))
  File "/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 158, in start_processes
    while not context.join():
  File "/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 119, in join
    raise Exception(msg)
Exception: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 20, in _wrap
    fn(i, *args)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_spawn_backend.py", line 154, in ddp_train
    results = self.trainer.run_pretrain_routine(model)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1211, in run_pretrain_routine
    self.train()
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 393, in train
    self.run_training_epoch()
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 490, in run_training_epoch
    batch_output = self.run_training_batch(batch, batch_idx)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 887, in run_training_batch
    grad_norm_dic = self.run_batch_backward_pass(split_batch, batch_idx, opt_idx, optimizer)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 948, in run_batch_backward_pass
    self.call_optimizer_step(optimizer, opt_idx, batch_idx, split_batch)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 986, in call_optimizer_step
    using_native_amp=native_amp)
TypeError: optimizer_step() got an unexpected keyword argument 'using_native_amp'

&lt;/denchmark-code&gt;

I have tried both these versions and same crash:
&lt;denchmark-code&gt;#RUN pip install pytorch-lightning==0.8.5
RUN pip install pytorch-lightning==0.9.0rc12
&lt;/denchmark-code&gt;

Not that I believe these things are related, but my code has the following:
&lt;denchmark-code&gt;        trainer = pl.Trainer(gpus=[0, 1],  accumulate_grad_batches=16, callbacks=[lr_logger]) #  
        trainer.fit(model, train_loader)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='johngrabner' date='2020-08-12T16:23:46Z'>
		same crash even if gpu and accumulate gradient removed.
&lt;denchmark-code&gt;        trainer = pl.Trainer() #  gpus=[0, 1],  accumulate_grad_batches=16, callbacks=[lr_logger]
        trainer.fit(model, train_loader)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='johngrabner' date='2020-08-12T16:27:28Z'>
		swapped optimizer, same crash
&lt;denchmark-code&gt;         optimizer = torch.optim.SGD(self.parameters(), lr=self.hparams.lr)
         #optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='johngrabner' date='2020-08-14T06:12:31Z'>
		This error (TypeError: optimizer_step() got an unexpected keyword argument 'using_native_amp') states that you are missing arguments in when your are defining optimizer_step. From the docs, you can see that the call signature is this:
optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None, on_tpu=False, using_native_amp=False, using_lbfgs=False)
&lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/lightning-module.html#optimizer-step&gt;https://pytorch-lightning.readthedocs.io/en/latest/lightning-module.html#optimizer-step&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='johngrabner' date='2020-08-14T18:04:34Z'>
		ok,
First my version:
print(f"pl__version__ {pl.__version__}")
pl__version__ 0.9.0rc12
Copied example as the docs state.
&lt;denchmark-code&gt;    def optimizer_step(self, current_epoch, batch_idx, optimizer,
                    optimizer_idx, second_order_closure, on_tpu, using_native_amp, using_lbfgs):
        # warm up lr
        if self.trainer.global_step &lt; 500:
            lr_scale = min(1., float(self.trainer.global_step + 1) / 500.)
            for pg in optimizer.param_groups:
                pg['lr'] = lr_scale * self.learning_rate

        # update params
        optimizer.step()
        optimizer.zero_grad()
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/home/john/Documents/GitHub/Offline_Handwriting_Recognition/Solutions/Aug2020_simple_transformer/src/kiss_transformer.py", line 249, in &lt;module&gt;
    trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/states.py", line 34, in wrapped_fn
    result = fn(self, *args, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1017, in fit
    self.accelerator_backend.train(model)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 56, in train
    self.ddp_train(process_idx=self.task_idx, mp_queue=None, model=model)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 219, in ddp_train
    results = self.trainer.run_pretrain_routine(model)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1211, in run_pretrain_routine
    self.train()
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 393, in train
    self.run_training_epoch()
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 490, in run_training_epoch
    batch_output = self.run_training_batch(batch, batch_idx)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 887, in run_training_batch
    grad_norm_dic = self.run_batch_backward_pass(split_batch, batch_idx, opt_idx, optimizer)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 948, in run_batch_backward_pass
    self.call_optimizer_step(optimizer, opt_idx, batch_idx, split_batch)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 986, in call_optimizer_step
    using_native_amp=native_amp)
TypeError: optimizer_step() missing 2 required positional arguments: 'on_tpu' and 'using_lbfgs'
&lt;/denchmark-code&gt;

My trainer
&lt;denchmark-code&gt;trainer = pl.Trainer( gpus=[0, 1],  
                accumulate_grad_batches=16, 
                max_epochs=500, 
                check_val_every_n_epoch=1, 
                distributed_backend='ddp', 
                fast_dev_run=False)
&lt;/denchmark-code&gt;

Can I provide any more info?
		</comment>
		<comment id='5' author='johngrabner' date='2020-08-14T22:40:22Z'>
		try:
optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, second_order_closure, using_native_amp):
or
optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None, on_tpu=False, using_native_amp=False, using_lbfgs=False)
## Add default values to these arguments in the definition as mentioned in docs to avoid this error.
		</comment>
		<comment id='6' author='johngrabner' date='2020-08-14T23:39:25Z'>
		Sorry, I do not follow.
I define a class that includes method def optimizer_step.
&lt;denchmark-code&gt;class JJG_Transformer(pl.LightningModule):

  def __init__(self, weights):
  def training_step(self, batch, batch_idx):
     ....
  def forward(self, batch_im, batch_letter_transformer_input): 
      .....
  def configure_optimizers(self):
      .....
   def optimizer_step(self, current_epoch, batch_idx, optimizer,
                 optimizer_idx, second_order_closure, on_tpu, using_native_amp, using_lbfgs):

&lt;/denchmark-code&gt;

I assume pytorch_ligthning calls this method optimizer_step. Thank you for your help.
		</comment>
		<comment id='7' author='johngrabner' date='2020-08-15T12:04:53Z'>
		You need to change the signature of your method of (optimizer_step) to the one below:
&lt;denchmark-code&gt;class JJG_Transformer(pl.LightningModule):

  def __init__(self, weights):
  def training_step(self, batch, batch_idx):
     ....
  def forward(self, batch_im, batch_letter_transformer_input): 
      .....
  def configure_optimizers(self):
      .....
  def optimizer_step(self, current_epoch, batch_idx, optimizer, 
      optimizer_idx, second_order_closure=None, 
       on_tpu=False, using_native_amp=False, using_lbfgs=False)

&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='johngrabner' date='2020-08-15T12:31:29Z'>
		&lt;denchmark-link:https://github.com/jonashaag&gt;@jonashaag&lt;/denchmark-link&gt;
 just add the defaults

		</comment>
		<comment id='9' author='johngrabner' date='2020-08-15T17:08:52Z'>
		Finally, I get it.
BTW, lightning is awesome.  Thank you for contributing this to society. Tried lighting for Multi GPU. Discovered I got tensorboard for free. Discovered I need batches larger than my GPU supported. No worry, just use accumulate_grad_batches in lightning.  Now I think I need warmup, and lightning is there too.  A side benefit, my code is more structured. Just awesome.
		</comment>
		<comment id='10' author='johngrabner' date='2020-11-06T17:58:17Z'>
		Since the update to &gt; v1.0, this technique (&lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/optimizers.html?highlight=warm-up#step-optimizers-at-arbitrary-intervals&gt;and the instructions in the docs&lt;/denchmark-link&gt;
) breaks. Specifically, I get , since the  call has changed to . I can see how to re-organise  and , but what should one do with ? Default it to ?
		</comment>
		<comment id='11' author='johngrabner' date='2020-12-11T15:19:28Z'>
		Also getting the same error TypeError: optimizer_step() got an unexpected keyword argument 'epoch'. Is the documentation not up to date yet?
BTW, Lightning has been great!
		</comment>
	</comments>
</bug>