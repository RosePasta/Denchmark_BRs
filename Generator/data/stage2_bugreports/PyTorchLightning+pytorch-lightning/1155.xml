<bug id='1155' author='qmeeus' open_date='2020-03-15T13:43:17Z' closed_time='2020-05-03T23:15:57Z'>
	<summary>No validation checks when overfit_pct is set</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When setting the overfit_pct to any value between 0 and 1 (exclusive) in trainer, the validation checks are disabled.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

I have worked on a minimal example to reproduce the bug:
import pytorch_lightning as pl
import torch

class Dataset(torch.utils.data.Dataset):

    def __init__(self, input_dim, output_dim):
        super(Dataset, self).__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim

    def __getitem__(self, idx):
        X = torch.rand(1, self.input_dim)
        y = torch.randint(0, self.output_dim, (1,))
        return X, y

    def __len__(self):
        return 1000

class Model(pl.LightningModule):

    def __init__(self, input_dim, output_dim):
        super(Model, self).__init__()
        self.layer = torch.nn.Linear(input_dim, output_dim)
        self.dataset = Dataset(input_dim, output_dim)

    def forward(self, x, y):
        yhat = torch.softmax(self.layer(x), -1)
        return F.nll_loss(logits, y)

    def train_dataloader(self):
        return torch.utils.data.DataLoader(self.dataset, batch_size=64)

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=1e-3)

    def training_step(self, batch, batch_idx):
        loss = self.forward(*batch)
        return {'loss': loss, 'log': {'loss': loss}}

    def validation_step(self, batch, batch_idx):
        loss = self.forward(*batch)
        return {'val_loss': loss, 'log': {'val_loss': loss}}


if __name__ == '__main__':
    model = Model(100, 10)
    trainer = pl.Trainer(overfit_pct=.01)
    trainer.fit(model)
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Validation checks occur normally
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

PyTorch version: 1.4.0
Is debug build: No
CUDA used to build PyTorch: 10.1

OS: Manjaro Linux
GCC version: (GCC) 8.3.0
CMake version: Could not collect

Python version: 3.7
Is CUDA available: No
CUDA runtime version: 10.2.89
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: /usr/lib/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] numpy==1.18.1
[pip] pytorch-lightning==0.7.1
[pip] torch==1.4.0
[pip] torchvision==0.5.0
[conda] mkl                       2020.0                      166  
[conda] pytorch                   1.4.0           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch
[conda] pytorch-lightning         0.7.1                    pypi_0    pypi
[conda] torchvision               0.5.0                py37_cu101    pytorch
	</description>
	<comments>
		<comment id='1' author='qmeeus' date='2020-03-15T13:43:56Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='qmeeus' date='2020-03-18T21:49:24Z'>
		&lt;denchmark-link:https://github.com/jeffling&gt;@jeffling&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/hadim&gt;@hadim&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 mind check?
		</comment>
		<comment id='3' author='qmeeus' date='2020-03-21T04:01:07Z'>
		, but I had to fix &lt;denchmark-link:https://github.com/qmeeus&gt;@qmeeus&lt;/denchmark-link&gt;
's code sample to make it visible.
The sanity validation checks run, but the validation at the end of the epoch doesn't.
When setting , validation checks work as expected.
Here is the fixed minimal code sample:
&lt;denchmark-code&gt;import pytorch_lightning as pl
import torch
import torch.nn.functional as F


class Dataset(torch.utils.data.Dataset):

    def __init__(self, input_dim, output_dim):
        super(Dataset, self).__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim

    def __getitem__(self, idx):
        X = torch.rand(self.input_dim)
        y = torch.randint(0, self.output_dim, (1,))
        return X, y

    def __len__(self):
        return 1000


class Model(pl.LightningModule):

    def __init__(self, input_dim, output_dim):
        super(Model, self).__init__()
        self.layer = torch.nn.Linear(input_dim, output_dim)
        self.dataset = Dataset(input_dim, output_dim)

    def forward(self, x, y):
        logits = torch.softmax(self.layer(x), -1)
        return F.nll_loss(logits, y.flatten(0))

    def train_dataloader(self):
        return torch.utils.data.DataLoader(self.dataset, batch_size=64)

    def val_dataloader(self):
        return torch.utils.data.DataLoader(self.dataset, batch_size=64)

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=1e-3)

    def training_step(self, batch, batch_idx):
        loss = self.forward(*batch)
        return {'loss': loss, 'log': {'loss': loss}}

    def validation_step(self, batch, batch_idx):
        loss = self.forward(*batch)
        print('see that validation runs only in sanity check')
        return {'val_loss': loss, 'log': {'val_loss': loss}}

    def validation_end(self, outputs):
        loss = torch.stack([output['val_loss'] for output in outputs]).mean()
        return {'val_loss': loss, 'log': {'val_loss': loss}}


if __name__ == '__main__':
    model = Model(100, 10)
    trainer = pl.Trainer(overfit_pct=0.1, max_epochs=10)
    trainer.fit(model)
&lt;/denchmark-code&gt;

For the record, &lt;denchmark-link:https://github.com/qmeeus&gt;@qmeeus&lt;/denchmark-link&gt;
 your code had these issues:

No val_dataloader defined
Wrong shapes returned in dataloader
Wrong shape for nll_loss labels

		</comment>
		<comment id='4' author='qmeeus' date='2020-03-21T04:14:27Z'>
		Actually overfit_pct argument is not documented in the Trainer class. We should fix that and say that setting overfit_pct is the same as setting train_percent_check, val_percent_check and test_percent_check.
		</comment>
		<comment id='5' author='qmeeus' date='2020-03-21T06:26:11Z'>
		False alarm! Turns out it is simply because you chose a too small value for overfit_pct.
Your dataset has size 1000, and dataloader has batch_size 64.
1000 / 64 ~= 15 batches
When you choose overfit_pct = .01, then that gives 15 * 0.01 &lt; 1 batch.
&lt;denchmark-link:https://github.com/qmeeus&gt;@qmeeus&lt;/denchmark-link&gt;
 Please let me know if it isn't clear. I think the behaviour of is correct.
		</comment>
		<comment id='6' author='qmeeus' date='2020-03-21T06:30:40Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 Should we make it so that does not round to 0 batches?
		</comment>
		<comment id='7' author='qmeeus' date='2020-03-21T10:42:11Z'>
		
False alarm! Turns out it is simply because you chose a too small value for overfit_pct.
Your dataset has size 1000, and dataloader has batch_size 64.
1000 / 64 ~= 15 batches
When you choose overfit_pct = .01, then that gives 15 * 0.01 &lt; 1 batch.
@qmeeus Please let me know if it isn't clear. I think the behaviour of overfit_pct is correct.

Awesome, thanks !
		</comment>
	</comments>
</bug>