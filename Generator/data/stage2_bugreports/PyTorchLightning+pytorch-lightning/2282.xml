<bug id='2282' author='soulhi-vz' open_date='2020-06-19T20:39:51Z' closed_time='2020-06-20T23:04:53Z'>
	<summary>optimizer got an empty parameter list</summary>
	<description>
Hi,
Got the following error:
ValueError: optimizer got an empty parameter list with both options below:
def configure_optimizers(self):
# option1 optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)
# option 2
optimizer = torch.optim.Adam(params = list(self.parameters()), lr=self.hparams.lr)
return optimizer
class Autoencoder(pl.LightningModule):
&lt;denchmark-code&gt;def __init__(self, hparams: argparse.Namespace):
    super(Autoencoder,self).__init__() 
    self.hparams = hparams
       
    self_layer_e_1 = nn.Conv1d(hparams.in_channels, hparams.out_channels, hparams.kernel_size)
    self_layer_e_2 = nn.Conv1d(hparams.out_channels,hparams.in_channels,hparams.kernel_size)
    self_layer_d_1 = nn.ConvTranspose1d(hparams.in_channels,hparams.out_channels,hparams.kernel_size)
    self_layer_d_2 = nn.ConvTranspose1d(hparams.out_channels,hparams.in_channels,hparams.kernel_size)
    

def forward(self,x):
    x = self_layer_e_1(x)
    x = nn.ReLu(x)
    x = self_layer_e_2(x)
    encoded = nn.ReLU(x)
    x = self_layer_d_1(encoded)
    x = nn.ReLU(x)
    decoded = self_layer_d_2(x)
    decoded = self.decoder(encoded)
    return self.decoded, self.encoded


def training_step(self, batch, batch_idx):
    x, _ = batch
    decoded, encoded = self.forward(x)
    loss = MSE(x, decoded)
    return loss

def validation_step(self, batch, batch_idx):
    return self._shared_eval(batch, batch_idx, 'val')

def test_step(self, batch, batch_idx):
    return self._shared_eval(batch, batch_idx, 'test')
    
def _shared_eval(self, batch, batch_idx, prefix):
    x, y = batch
    decoded, encoded = self.forward(x)
    loss = F.nll_loss(x, decoded)
    return {f'{prefix}_loss': loss}

def train_dataloader(self):
    return DataLoader(self.CarrierDataset, batch_size=self.hparams.batch_size)

def val_dataloader(self):
    return DataLoader(self.CarrierDataset, batch_size=hparams.batch_size)

def test_dataloader(self):
    return DataLoader(self,CarrierDataset, batch_size=hparams.batch_size)

def configure_optimizers(self):
    #optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)
    optimizer = torch.optim.Adam(params = list(self.parameters()), lr=self.hparams.lr)
    return optimizer
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='soulhi-vz' date='2020-06-19T20:40:31Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='soulhi-vz' date='2020-06-19T21:56:35Z'>
		You need to use self.xxx = nn.Conv2d(a,b,c)  instead of self_xxx = nn.Conv2d(a,b,c)  for nn.Module to register them as parameters, otherwise your module has no paramters, thuse the optimizer gets nothing.
		</comment>
		<comment id='3' author='soulhi-vz' date='2020-06-20T23:04:53Z'>
		It works now. Thanks for the catch !!!!
		</comment>
	</comments>
</bug>