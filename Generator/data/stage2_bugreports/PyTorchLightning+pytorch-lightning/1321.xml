<bug id='1321' author='lkhphuc' open_date='2020-03-31T17:16:51Z' closed_time='2020-04-01T14:21:16Z'>
	<summary>Trainer.add_argparse_args(parser) break the default Tensorboard hparams logging.</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Trainer.add_argparse_args(parser) break the default Tensorboard hparams logging.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:
I pretty much just put together the sample codes in the Hyperparameters section in the docs and it's throw the error.
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;class LitMNIST(pl.LightningModule):
  def __init__(self, hparams):
    super(LitMNIST, self).__init__()
    self.hparams = hparams

    self.layer_1 = torch.nn.Linear(28 * 28, hparams.layer_1_dim)

  def forward(self, x):
    return self.layer_1(x)

  def train_dataloader(self):
    return DataLoader(mydata(), batch_size=self.hparams.batch_size)

  def configure_optimizers(self):
    return Adam(self.parameters(), lr=self.hparams.learning_rate)


def main(args):
    model = LitMNIST(args)
    trainer = pl.Trainer()
    trainer.fit(model)


if __name__ == "__main__":
    parser = ArgumentParser()

    # parametrize the network
    parser.add_argument('--layer_1_dim', type=int, default=128)
    parser.add_argument('--learning_rate', type=float, default=1e-3)

    # add all the available options to the trainer
    parser = pl.Trainer.add_argparse_args(parser)

    args = parser.parse_args()
    main(args)

&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;Traceback (most recent call last):
  File "tmp.py", line 56, in &lt;module&gt;
    main(args)
  File "tmp.py", line 40, in main
    trainer.fit(model)
  File "/Users/phuc/miniconda3/envs/thinc/lib/python3.7/site-packages/pytorch_ligh
tning/trainer/trainer.py", line 630, in fit
    self.run_pretrain_routine(model)
  File "/Users/phuc/miniconda3/envs/thinc/lib/python3.7/site-packages/pytorch_ligh
tning/trainer/trainer.py", line 748, in run_pretrain_routine
    self.logger.log_hyperparams(ref_model.hparams)
  File "/Users/phuc/miniconda3/envs/thinc/lib/python3.7/site-packages/pytorch_ligh
tning/loggers/base.py", line 18, in wrapped_fn
    fn(self, *args, **kwargs)
  File "/Users/phuc/miniconda3/envs/thinc/lib/python3.7/site-packages/pytorch_ligh
tning/loggers/tensorboard.py", line 113, in log_hyperparams
    exp, ssi, sei = hparams(params, {})
  File "/Users/phuc/miniconda3/envs/thinc/lib/python3.7/site-packages/torch/utils/
tensorboard/summary.py", line 156, in hparams
    raise ValueError('value should be one of int, float, str, bool, or torch.Tenso
r')
ValueError: value should be one of int, float, str, bool, or torch.Tensor
&lt;/denchmark-code&gt;

The value it fails at is key callback with value [].
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Trainer.add_argparse_args(parser) should not create trouble for tensorboard hparams logging.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version (e.g., 1.0): 1.3.1
OS (e.g., Linux): Linux
How you installed PyTorch (conda, pip, source): pip
Python version: 3.7.5

	</description>
	<comments>
		<comment id='1' author='lkhphuc' date='2020-04-01T02:30:21Z'>
		Hi, not sure but it looks like this got fixed on master recently. See &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1130&gt;#1130&lt;/denchmark-link&gt;
.
Could you try installing from master and try again?
pip install git+https://github.com/PytorchLightning/pytorch-lightning.git@master --upgrade 
		</comment>
		<comment id='2' author='lkhphuc' date='2020-04-01T14:21:16Z'>
		Yep that fixes it. Thanks.
		</comment>
	</comments>
</bug>