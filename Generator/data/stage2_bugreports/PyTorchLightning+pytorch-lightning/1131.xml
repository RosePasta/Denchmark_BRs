<bug id='1131' author='stathius' open_date='2020-03-12T23:01:45Z' closed_time='2020-03-30T22:43:54Z'>
	<summary>Better message when DataLoader is wrong</summary>
	<description>
On the verge between bug and improvement.
There was a bug in my Validation DataLoader and was returning irrelevant staff. Accidentally the length was 0.  Probably an edge case combination. The error I was getting during the validation sanity check was quite cryptic:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "UNet_WaveProp.py", line 174, in &lt;module&gt;
    trainer.fit(model)
  File "/mnt/RDS/home/code/pytorch-lightning/pytorch_lightning/trainer/trainer.py", line 629, in fit
    self.run_pretrain_routine(model)
  File "/mnt/RDS/home/code/pytorch-lightning/pytorch_lightning/trainer/trainer.py", line 809, in run_pretrain_routine
    False)
  File "/mnt/RDS/home/code/pytorch-lightning/pytorch_lightning/trainer/evaluation_loop.py", line 300, in evaluate
    eval_results = model.validation_epoch_end(outputs)
  File "UNet_WaveProp.py", line 138, in validation_epoch_end
    avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
RuntimeError: stack expects a non-empty TensorList
&lt;/denchmark-code&gt;

I had to go through the code of pytorch-lightning for few hours to understand what was happening.
Maybe a more informative message would make more sense?
One thing would be to check if the DataLoader's size is 0.
What do you think? I could take a stab at a PR.
	</description>
	<comments>
		<comment id='1' author='stathius' date='2020-03-12T23:02:29Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='stathius' date='2020-03-14T01:07:39Z'>
		yeah, better error messing always helps, thx and PR is welcome! :]
		</comment>
	</comments>
</bug>