<bug id='375' author='VismantasD' open_date='2019-10-16T13:07:02Z' closed_time='2019-10-16T13:15:15Z'>
	<summary>training_step is called once for each optimizer</summary>
	<description>
If model returns multiple optimizers, a closure is created for each one of them. This results in training_step called for each optimizer which is wasteful in most cases.
In addition to this, training_step is being called with additional parameter ( optimizer id) - this is not documented and results in an exception which ( calling 2 argument function with 3 parameters )
To Reproduce
Steps to reproduce the behavior:

Create model with two or more optimizers.
training_step should take ( batch, batch_nb) as per documentation and a print statement printing batch_nb
Run trainer.fit
Exception should be thrown re call to training_step with 3 parameters
change training_step to take ( batch, batch_nb, optimizer_idx)
Run trainer.fit
Observe batch_nb printed as many times as there are optimizers.

Expected behavior

In most cases training_step can be called only once - that's efficiency improvement.
Documentation should specify the extra parameter being passed when multiple optimizers are used, to prevent the exception.

	</description>
	<comments>
		<comment id='1' author='VismantasD' date='2019-10-16T13:15:15Z'>
		this is pretty well documented and is the expected behavior.
&lt;denchmark-link:https://williamfalcon.github.io/pytorch-lightning/LightningModule/RequiredTrainerInterface/#training_step&gt;https://williamfalcon.github.io/pytorch-lightning/LightningModule/RequiredTrainerInterface/#training_step&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='VismantasD' date='2019-10-16T14:06:04Z'>
		Ok, I've not noticed the documentation of the idx. But what about calling the training step multiple times? I can see how this could be useful for GANs. In the case of having different optimizers for different parts of the model, this might not be desirable.
		</comment>
		<comment id='3' author='VismantasD' date='2020-02-10T13:41:18Z'>
		I also encountered this problem, where I only want to set different optimizers for different parts of my model, but they should all update the model in each iteration.
Can pytorch_lightening provide a way to achieve this? For example in configure_optimizers we can mark multiple optimizers as a group where they should all update parameters (so behave like a single optimizer in current pytorch_lightening implementation).
		</comment>
		<comment id='4' author='VismantasD' date='2020-04-24T17:10:32Z'>
		
this is pretty well documented and is the expected behavior.
https://williamfalcon.github.io/pytorch-lightning/LightningModule/RequiredTrainerInterface/#training_step

It is giving not found, can you please link the updated docs?
		</comment>
		<comment id='5' author='VismantasD' date='2020-06-05T19:42:11Z'>
		Hi &lt;denchmark-link:https://github.com/L1AN0&gt;@L1AN0&lt;/denchmark-link&gt;
 ,
For doing that you can create a single optimizer with multiple optimizers inside. You can check out this link:

&lt;denchmark-link:https://pytorch.org/docs/master/optim.html#per-parameter-options&gt;Here&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;optim.SGD([{'params': model.base.parameters()},
           {'params': model.classifier.parameters(), 'lr': 1e-3}], 
           lr=1e-2, momentum=0.9)
&lt;/denchmark-code&gt;

2. Multiple Optimizers of Different Type
&lt;denchmark-code&gt;class MultipleOptimizer(object):
    def __init__(*op):
        self.optimizers = op

    def zero_grad(self):
        for op in self.optimizers:
            op.zero_grad()

    def step(self):
        for op in self.optimizers:
            op.step()


opt = MultipleOptimizer(optimizer1(params1, lr=lr1), 
                        optimizer2(params2, lr=lr2))

loss.backward()
opt.zero_grad()
opt.step()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='VismantasD' date='2020-06-06T00:31:42Z'>
		
Hi @L1AN0 ,
For doing that you can create a single optimizer with multiple optimizers inside. You can check out this link:
1. Multiple optimizers of Same type:
Here
optim.SGD([{'params': model.base.parameters()},
           {'params': model.classifier.parameters(), 'lr': 1e-3}], 
           lr=1e-2, momentum=0.9)

2. Multiple Optimizers of Different Type
class MultipleOptimizer(object):
    def __init__(*op):
        self.optimizers = op

    def zero_grad(self):
        for op in self.optimizers:
            op.zero_grad()

    def step(self):
        for op in self.optimizers:
            op.step()


opt = MultipleOptimizer(optimizer1(params1, lr=lr1), 
                        optimizer2(params2, lr=lr2))

loss.backward()
opt.zero_grad()
opt.step()


Thanks. But that won't be sufficient because the MultipleOptimizer class is not an Optimizer. For example, when pytorch lightning is creating a checkpoint, the optimizer will fail because there's no state_dict in MultipleOptimizer.
Sadly, making MultipleOptimizer an appropriate Optimizer is not trivial.
		</comment>
		<comment id='7' author='VismantasD' date='2020-06-10T13:03:45Z'>
		&lt;denchmark-link:https://github.com/L1AN0&gt;@L1AN0&lt;/denchmark-link&gt;
 mind open a new issue if needed?
		</comment>
		<comment id='8' author='VismantasD' date='2020-06-23T09:35:50Z'>
		Also need the feature as &lt;denchmark-link:https://github.com/L1AN0&gt;@L1AN0&lt;/denchmark-link&gt;
 . Same with this need (&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/29#issuecomment-611006872&gt;#29 (comment)&lt;/denchmark-link&gt;
).
Basically, I need to change the optimizer after 5 epochs. If I define two optimizers, the training_step will be executed two times with two different optimizer_idxs. But this is not needed. I only need to call the training_step once for each step since the two stages are not overlapped.
		</comment>
	</comments>
</bug>