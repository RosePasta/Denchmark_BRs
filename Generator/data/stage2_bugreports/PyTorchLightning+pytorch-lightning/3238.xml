<bug id='3238' author='dalmia' open_date='2020-08-28T01:48:55Z' closed_time='2020-09-22T20:47:18Z'>
	<summary>Error using Custom DistributedSampler</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When using a custom DistributedSampler, I get the following error while requesting the train dataloader:
File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/accelerators/tpu_backend.py", line 81, in train
    self.tpu_train_in_process(self.trainer.tpu_id, model, self.trainer, self.mp_queue)
  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/accelerators/tpu_backend.py", line 112, in tpu_train_in_process
    results = trainer.run_pretrain_routine(model)
  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py", line 1239, in run_pretrain_routine
    self.train()
  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py", line 355, in train
    self.reset_train_dataloader(model)
  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/data_loading.py", line 205, in reset_train_dataloader
    self.train_dataloader = self.request_dataloader(model.train_dataloader)
  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/data_loading.py", line 360, in request_dataloader
    dataloader = dataloader_fx()
  File "/content/lib/coreml/coreml/data/data_module.py", line 41, in train_dataloader
    drop_last=False)
  File "/content/lib/coreml/coreml/data/dataloader.py", line 127, in get_dataloader
    'rank': xm.get_ordinal()
  File "/content/lib/coreml/coreml/data/sampler.py", line 139, in __init__
    sampler.dataset, num_replicas, rank, shuffle)
  File "/usr/local/lib/python3.6/dist-packages/torch/utils/data/distributed.py", line 58, in __init__
    rank = dist.get_rank()
  File "/usr/local/lib/python3.6/dist-packages/torch/distributed/distributed_c10d.py", line 598, in get_rank
    _check_default_pg()
  File "/usr/local/lib/python3.6/dist-packages/torch/distributed/distributed_c10d.py", line 210, in _check_default_pg
    "Default process group is not initialized"
AssertionError: Default process group is not initialized
When no custom sampler is used, the code works fine. The  object is created as a wrapper over my actual sampler - the definition can be found &lt;denchmark-link:https://github.com/dalmia/coreml/blob/lightning/coreml/data/sampler.py#L132&gt;here&lt;/denchmark-link&gt;
 and the object creation happens &lt;denchmark-link:https://github.com/dalmia/coreml/blob/lightning/coreml/data/dataloader.py#L116&gt;here&lt;/denchmark-link&gt;
.
A quick Google search led me to &lt;denchmark-link:https://github.com/dalmia/coreml&gt;this&lt;/denchmark-link&gt;
 PyTorch issue where &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/38300#issuecomment-629446169&gt;this&lt;/denchmark-link&gt;
 comment highlighted that the call to  might be missing.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Use &lt;denchmark-link:https://colab.research.google.com/drive/155-MwCKwM-vFFn7rTF7S7aTJSg0bfA8j?usp=sharing&gt;this&lt;/denchmark-link&gt;
 colab notebook.
	</description>
	<comments>
		<comment id='1' author='dalmia' date='2020-08-28T03:34:19Z'>
		come with same issue
		</comment>
		<comment id='2' author='dalmia' date='2020-09-12T05:50:01Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 I do not intend to spam, but is there any fix for this? I found other open issues regarding Custom Distributed Sampler as well and so, figured that this is still relevant. Please have a look!
		</comment>
		<comment id='3' author='dalmia' date='2020-09-13T19:40:53Z'>
		I got this  working
&lt;denchmark-link:https://github.com/huggingface/transformers/blob/master/examples/seq2seq/utils.py#L225&gt;https://github.com/huggingface/transformers/blob/master/examples/seq2seq/utils.py#L225&lt;/denchmark-link&gt;

It required passing -replace_sampler_ddp=False.
		</comment>
		<comment id='4' author='dalmia' date='2020-09-15T16:29:46Z'>
		Hi &lt;denchmark-link:https://github.com/sshleifer&gt;@sshleifer&lt;/denchmark-link&gt;
. I did set  but still got this error. Thanks for pointing me to this example from HuggingFace. I'll try following their example to see if I can get this working!
		</comment>
		<comment id='5' author='dalmia' date='2020-09-20T00:39:46Z'>
		&lt;denchmark-link:https://github.com/dalmia&gt;@dalmia&lt;/denchmark-link&gt;
  EDIT: sorry, I take it back, you provided the link to the code already.
		</comment>
		<comment id='6' author='dalmia' date='2020-09-20T01:13:05Z'>
		&lt;denchmark-link:https://github.com/dalmia&gt;@dalmia&lt;/denchmark-link&gt;

The trainer will call the dataloader before dist.get_rank() is initialized. Therefore the rank is not yet known.
But when creating the sampler, you don't have to provide the rank and num_replicas, you can leave it at the default values and when distributed get's initialized, they will be known.
Furthermore, your distributed sampler also has the following typo:  self.sampler = Sampler (should be sampler)
Here is what worked for me (ddp and ddp_spawn):
class DistributedSamplerWrapper(DistributedSampler):
    def __init__(
            self, sampler,
            num_replicas: Optional[int] = None,
            rank: Optional[int] = None,
            shuffle: bool = True):
        super(DistributedSamplerWrapper, self).__init__(
            sampler.dataset, num_replicas, rank, shuffle)
        # typo:
        # self.sampler = Sampler
        self.sampler = sampler

    def __iter__(self):
        indices = list(self.sampler)
        indices = indices[self.rank:self.total_size:self.num_replicas]
        return iter(indices)

    def __len__(self):
        return len(self.sampler)
and then in dataloader:
sampler = DistributedSamplerWrapper(RandomSampler())  # leave other values default
dataloader = DataLoader(dataset, sampler=sampler, ...)
Hope this helps you.
		</comment>
		<comment id='7' author='dalmia' date='2020-10-11T13:59:07Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 thank you for your answer. I feel so silly for missing the  bug.
However, upon changing it and leaving rank and num_replicas as its default, I still get the same issue as when I am creating the sampler, the  within torch (&lt;denchmark-link:https://github.com/pytorch/pytorch/blob/master/torch/utils/data/distributed.py#L62&gt;https://github.com/pytorch/pytorch/blob/master/torch/utils/data/distributed.py#L62&lt;/denchmark-link&gt;
) calls  if  is None. So, I'm back to the same issue. Shouldn't the sampler be created after distributed gets initialized?
		</comment>
		<comment id='8' author='dalmia' date='2020-10-11T14:27:10Z'>
		It's working now. It was an issue with the way I was passing parameters to my DistributedSampler
		</comment>
		<comment id='9' author='dalmia' date='2020-11-16T02:14:12Z'>
		
It's working now. It was an issue with the way I was passing parameters to my DistributedSampler

Hi, I'm running into this problem and am not clear how you were able to resolve it. Specifically, I'm still hitting the call mentioned above (&lt;denchmark-link:https://github.com/pytorch/pytorch/blob/master/torch/utils/data/distributed.py#L62&gt;https://github.com/pytorch/pytorch/blob/master/torch/utils/data/distributed.py#L62&lt;/denchmark-link&gt;
).
		</comment>
	</comments>
</bug>