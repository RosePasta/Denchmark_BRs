<bug id='3549' author='pbmstrk' open_date='2020-09-18T13:00:13Z' closed_time='2020-09-20T00:00:51Z'>
	<summary>Bug in validation_epoch_end</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

In the &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html?highlight=validation_epoch_end#validation-epoch-end&gt;documentation&lt;/denchmark-link&gt;
,  is described as running at the end of a validation epoch and does not need to necessarily return anything.
When running a slightly modified version of the &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/new_project.html&gt;example&lt;/denchmark-link&gt;
 in the docs,  seemingly runs once on a single batch of validation data and returns an error if nothing is returned.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

A MWE:
&lt;denchmark-code&gt;import os
import torch
import torch.nn.functional as F
from torchvision.datasets import MNIST
from torchvision import transforms
from torch.utils.data import DataLoader
import pytorch_lightning as pl
from torch.utils.data import random_split

class LitModel(pl.LightningModule):

    def __init__(self):
        super().__init__()
        self.layer_1 = torch.nn.Linear(28 * 28, 128)
        self.layer_2 = torch.nn.Linear(128, 10)

    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = self.layer_1(x)
        x = F.relu(x)
        x = self.layer_2(x)
        return x

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)
        return optimizer

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)

        return {'loss': loss}

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        
        return {"val_loss": loss}

    def validation_epoch_end(self, validation_step_outputs):
        print("Length of outputs: {}".format(len(validation_step_outputs)))
        # a dummy return value to avoid error
        return {"val_loss": 0}

model = LitModel()

dataset = MNIST(os.getcwd(), download=True, train=False, transform=transforms.ToTensor())
train, val = random_split(dataset, [9000, 1000])

train_loader = DataLoader(train)
val_loader = DataLoader(val)

# train
trainer = pl.Trainer(progress_bar_refresh_rate=0, max_epochs=5, weights_summary=None, gpus=1)
trainer.fit(model, train_loader, val_loader)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Output&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;GPU available: True, used: True
TPU available: False, using: 0 TPU cores
CUDA_VISIBLE_DEVICES: [0]
/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given
  warnings.warn(*args, **kwargs)
Length of outputs: 1
Length of outputs: 1000
Length of outputs: 1000
Length of outputs: 1000
Length of outputs: 1000
Saving latest checkpoint..
Length of outputs: 1000
&lt;/denchmark-code&gt;

If no return value is given in validation_epoch_end, the following error occurs
&lt;denchmark-code&gt;---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-183-013dcacba267&gt; in &lt;module&gt;()
     10 # train
     11 trainer = pl.Trainer(progress_bar_refresh_rate=0, max_epochs=5, weights_summary=None, gpus=1)
---&gt; 12 trainer.fit(model, train_loader, val_loader)

7 frames
/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/states.py in wrapped_fn(self, *args, **kwargs)
     46             if entering is not None:
     47                 self.state = entering
---&gt; 48             result = fn(self, *args, **kwargs)
     49 
     50             # The INTERRUPTED state can be set inside the run function. To indicate that run was interrupted

/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)
   1071             self.accelerator_backend = GPUBackend(self)
   1072             model = self.accelerator_backend.setup(model)
-&gt; 1073             results = self.accelerator_backend.train(model)
   1074 
   1075         elif self.use_tpu:

/usr/local/lib/python3.6/dist-packages/pytorch_lightning/accelerators/gpu_backend.py in train(self, model)
     49 
     50     def train(self, model):
---&gt; 51         results = self.trainer.run_pretrain_routine(model)
     52         return results
     53 

/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in run_pretrain_routine(self, model)
   1222 
   1223         # run a few val batches before training starts
-&gt; 1224         self._run_sanity_check(ref_model, model)
   1225 
   1226         # clear cache before training

/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in _run_sanity_check(self, ref_model, model)
   1255             num_loaders = len(self.val_dataloaders)
   1256             max_batches = [self.num_sanity_val_steps] * num_loaders
-&gt; 1257             eval_results = self._evaluate(model, self.val_dataloaders, max_batches, False)
   1258 
   1259             # allow no returns from eval

/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/evaluation_loop.py in _evaluate(self, model, dataloaders, max_batches, test_mode)
    397 
    398         # log callback metrics
--&gt; 399         self.__update_callback_metrics(eval_results, using_eval_result)
    400 
    401         # Write predictions to disk if they're available.

/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/evaluation_loop.py in __update_callback_metrics(self, eval_results, using_eval_result)
    429                         flat = {'val_loss': eval_result}
    430                     else:
--&gt; 431                         flat = flatten_dict(eval_result)
    432                     self.callback_metrics.update(flat)
    433             else:

/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/parsing.py in flatten_dict(source, result)
    113         result = {}
    114 
--&gt; 115     for k, v in source.items():
    116         if isinstance(v, dict):
    117             _ = flatten_dict(v, result)

AttributeError: 'NoneType' object has no attribute 'items'
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

The outputs in validation_epoch_end should contain results from all batches. Is a test validation batch running in the background, which could explain why the results of only one batch are included in the first loop.
Return values in  were addressed in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2438&gt;#2438&lt;/denchmark-link&gt;
 (with the same error) and from what I can tell included in the 0.8.4 release - perhaps I am missing something?
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


PyTorch: 1.6.0
pytorch-lightning: 0.9.0

	</description>
	<comments>
		<comment id='1' author='pbmstrk' date='2020-09-18T13:01:10Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='pbmstrk' date='2020-09-18T15:17:40Z'>
		Hi there! What you are seeing in the first output is caused by our "validation sanity check". We run a configurable number of validation steps before training starts to ensure the user doesn't run into any issues after a potentially timely series of training steps. This can be configured (or turned off) with pl.Trainer(num_sanity_val_steps=0).
Regarding returning None from validation_epoch_end, it appears that functionality has recently been removed, I will look into whether that was intentional or not and update the code/docs accordingly.
		</comment>
		<comment id='3' author='pbmstrk' date='2020-09-18T19:10:08Z'>
		&lt;denchmark-link:https://github.com/teddykoker&gt;@teddykoker&lt;/denchmark-link&gt;
 That makes sense. Thanks for the pointer for configuring this!
		</comment>
	</comments>
</bug>