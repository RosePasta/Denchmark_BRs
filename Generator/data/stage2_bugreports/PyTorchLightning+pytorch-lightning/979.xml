<bug id='979' author='neggert' open_date='2020-02-28T22:57:52Z' closed_time='2020-03-03T02:50:38Z'>
	<summary>Failing test: test_running_test_pretrained_model_ddp</summary>
	<description>
I think this is another problem stemming from the fact that we don't have a way to pass data back from torch.multiprocessing.spawn. Needs more investigation.
&lt;denchmark-code&gt;def test_running_test_pretrained_model_ddp(tmpdir):
        """Verify `test()` on pretrained model."""
        ...
        # run test set
        new_trainer = Trainer(**trainer_options)
&gt;       new_trainer.test(pretrained_model)
tests/test_restore_models.py:60:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pytorch_lightning/trainer/trainer.py:1189: in test
    self.run_evaluation(test_mode=True)
pytorch_lightning/trainer/evaluation_loop.py:299: in run_evaluation
    if test_mode and not self.is_overriden('test_step'):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
self = &lt;pytorch_lightning.trainer.trainer.Trainer object at 0x7f845ec23f90&gt;, f_name = 'test_step', model = None
    def is_overriden(self, f_name, model=None):
        if model is None:
            model = self.get_model()
        super_object = LightningModule
        # when code pointers are different, it was overriden
&gt;       is_overriden = getattr(model, f_name).__code__ is not getattr(super_object, f_name).__code__
E       AttributeError: 'NoneType' object has no attribute 'test_step'
pytorch_lightning/trainer/model_hooks.py:20: AttributeError
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='neggert' date='2020-03-01T14:23:30Z'>
		&lt;denchmark-link:https://pytorch.org/docs/stable/notes/multiprocessing.html#reuse-buffers-passed-through-a-queue&gt;https://pytorch.org/docs/stable/notes/multiprocessing.html#reuse-buffers-passed-through-a-queue&lt;/denchmark-link&gt;

torch.multiprocessing is a drop in replacement for Pythonâ€™s python:multiprocessing module. It supports the exact same operations, but extends it, so that all tensors sent through a python:multiprocessing.Queue, will have their data moved into shared memory and will only send a handle to another process.
Looks like we can use python:multiprocessing.Queue?
		</comment>
		<comment id='2' author='neggert' date='2020-03-02T16:15:30Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 Could we check these two commits on GPU - &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/20d15c8023602067eba1605c745448592b709477&gt;20d15c8&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/5dd2afeab108132fb8db32b8b9959c8ea88ed0de&gt;5dd2afe&lt;/denchmark-link&gt;
?
		</comment>
	</comments>
</bug>