<bug id='2372' author='xiadingZ' open_date='2020-06-26T13:35:38Z' closed_time='2020-06-30T14:03:50Z'>
	<summary>training_epoch_end's outputs doesn't have 'loss' key</summary>
	<description>
pytorch-lightning: build from master
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "main.py", line 140, in &lt;module&gt;
    main(hparams)
  File "main.py", line 72, in main
    trainer.fit(model)
  File "/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 881, in fit
    self.ddp_train(task, model)
  File "/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py", line 539, in ddp_train
    self.run_pretrain_routine(model)
  File "/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1091, in run_pretrain_routine
    self.train()
  File "/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 376, in train
    self.run_training_epoch()
  File "/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 510, in run_training_epoch
    self.run_training_epoch_end(epoch_output)
  File "/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 535, in run_training_epoch_end
    epoch_output = model.training_epoch_end(epoch_output)
  File "/mnt/lustre/maxiao1/PVM/models/baseline.py", line 335, in training_epoch_end
    avg_loss = torch.stack([x['loss'] for x in outputs]).mean()
  File "/mnt/lustre/maxiao1/PVM/models/baseline.py", line 335, in &lt;listcomp&gt;
    avg_loss = torch.stack([x['loss'] for x in outputs]).mean()
KeyError: 'loss'
&lt;/denchmark-code&gt;

This is my code:
&lt;denchmark-code&gt;    def training_step(self, batch, batch_idx):
        ...
        return {'loss': loss, "train_acc": acc}

    def training_epoch_end(self, outputs):
        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()
        avg_acc = torch.stack([x['train_acc'] for x in outputs]).mean()
        logs = {'loss': avg_loss, 'train_acc': avg_acc}
        progress_bar = {'train_loss': avg_loss, 'train_acc': avg_acc}
        results = {
            'log': logs,
            'progress_bar': progress_bar
        }
        return results
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='xiadingZ' date='2020-06-26T21:54:01Z'>
		Try: avg_loss = torch.stack([x['batch_loss'] for x in outputs]).mean()
		</comment>
		<comment id='2' author='xiadingZ' date='2020-06-27T01:35:18Z'>
		Thanksï¼Œ it works
but 'train_acc' key doesn't exist, neither do batch_train_acc. How to access other keys returned in training_step?
		</comment>
		<comment id='3' author='xiadingZ' date='2020-06-27T11:35:13Z'>
		As of now in lightning you can access them using x['callback_metrics']['loss'] and x['callback_metrics']['train_acc'], but I think it should be handled in a similar way we do this with validation_epoch_end and test_epoch_end.
		</comment>
		<comment id='4' author='xiadingZ' date='2020-06-29T16:47:24Z'>
		Hi! One hint: for me it works with "loss" under windows but not under ubuntu.
		</comment>
		<comment id='5' author='xiadingZ' date='2020-06-29T17:20:30Z'>
		Weird!! Why is this think platform dependent?? ðŸ¤”
		</comment>
		<comment id='6' author='xiadingZ' date='2020-06-30T07:45:37Z'>
		&lt;denchmark-link:https://github.com/Pet222&gt;@Pet222&lt;/denchmark-link&gt;
 , are u sure that versions on ubuntu and windows are same?
		</comment>
		<comment id='7' author='xiadingZ' date='2020-06-30T10:29:19Z'>
		Hey &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 is this intended behaviour? I was surprised to see this breaking change being introduced with no warning.
If it is intended, why not have consistent behaviour over  and .
If it is not intended, as it seems due to the "bug fix" tag, are you working on it or should I make a PR for this?
		</comment>
		<comment id='8' author='xiadingZ' date='2020-06-30T11:43:59Z'>
		what is the behavior? that the "loss" key is not in training_epoch_end? If so, that's a bug because it should be there
		</comment>
		<comment id='9' author='xiadingZ' date='2020-06-30T11:49:52Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 , on the latest version, the  key was changed to the . I think it was changed &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/0f073819d3e0df8db7602eab489b1bad0fc0949c#diff-c45bd21c331565cbe62aaa12fa43aa0aR717&gt;here&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='xiadingZ' date='2020-06-30T11:50:10Z'>
		Yes, the fact that you need to access it through 'callback metrics'.
Got it!
On Tue, 30 Jun 2020 at 12:44, William Falcon ***@***.***&gt; wrote:
 what is the behavior? that the "loss" key is not in training_epoch_end? If
 so, that's a bug because it should be there

 â€”
 You are receiving this because you commented.
 Reply to this email directly, view it on GitHub
 &lt;&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2372#issuecomment-651740702&gt;#2372 (comment)&lt;/denchmark-link&gt;
&gt;,
 or unsubscribe
 &lt;&lt;denchmark-link:https://github.com/notifications/unsubscribe-auth/ABKWP6XTUJDTEDJ2NZQ3RKTRZHFY5ANCNFSM4OJKX4KQ&gt;https://github.com/notifications/unsubscribe-auth/ABKWP6XTUJDTEDJ2NZQ3RKTRZHFY5ANCNFSM4OJKX4KQ&lt;/denchmark-link&gt;
&gt;
 .

-- 
Best Regards,
Miguel Vera

+351 915 198 452
miguel.coimbra.vera@protonmail.com
Github/Captainvera &lt;&lt;denchmark-link:http://www.github.com/captainvera&gt;http://www.github.com/captainvera&lt;/denchmark-link&gt;
&gt;
		</comment>
		<comment id='11' author='xiadingZ' date='2020-06-30T12:19:01Z'>
		&lt;denchmark-link:https://github.com/captainvera&gt;@captainvera&lt;/denchmark-link&gt;
 would love a PR :)
		</comment>
		<comment id='12' author='xiadingZ' date='2020-06-30T13:24:38Z'>
		&lt;denchmark-link:https://github.com/captainvera&gt;@captainvera&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/xiadingZ&gt;@xiadingZ&lt;/denchmark-link&gt;
 sorry about that! it was a bad bug.
Made a PR &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2428&gt;#2428&lt;/denchmark-link&gt;
  and added tests to make sure this doesn't happen again!
try master now!
weâ€™ll push a new minor again since this is a key bug (and we have a few other key bugs)
		</comment>
		<comment id='13' author='xiadingZ' date='2020-06-30T14:11:11Z'>
		Well, that was fast, thanks!
		</comment>
	</comments>
</bug>