<bug id='1333' author='thinline72' open_date='2020-04-01T21:50:49Z' closed_time='2020-04-01T22:14:12Z'>
	<summary>lr_scheduler.step() should pass epoch variable to be consistent with PyTorch _LRScheduler</summary>
	<description>
PyTorch  base class for schedulers supports passing  parameter to the  &lt;denchmark-link:https://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py#L106&gt;function&lt;/denchmark-link&gt;
.
But when Lightning training loop &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_loop.py#L767&gt;updates learning rates&lt;/denchmark-link&gt;
, it doesn't pass the current epoch index. That leads to a wrong behaviour of those shedulers that relies on  param.
Some schedulers handle that under the hood, like &lt;denchmark-link:https://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py#L898&gt;CosineAnnealingWarmRestarts&lt;/denchmark-link&gt;
 from PyTorch that &lt;denchmark-link:https://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py#L976-L977&gt;increment last epoch index&lt;/denchmark-link&gt;
 if epoch isn't provided. In my case I have a custom scheduler that updates learning rate after each batch via  and it also uses current epoch index. I can create a workaround for sure, but thought it'd be nice to get it fixed in the Pytorch Lightning too. Plus it wasn't obvious for me until I logged lr into tensorboard.
	</description>
	<comments>
		<comment id='1' author='thinline72' date='2020-04-01T21:51:26Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='thinline72' date='2020-04-01T21:59:43Z'>
		Actually,  also handles that by &lt;denchmark-link:https://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py#L139-L141&gt;incrementing the last epoch index&lt;/denchmark-link&gt;
 if epoch isn't provided.
Then it's an issue for all schedulers that update lr after each batch (like with "interval": "step") and rely on self.last_epoch. As it increments self.last_epoch after each batch, it basically becomes last step index instead of last epoch index.
		</comment>
		<comment id='3' author='thinline72' date='2020-04-01T22:14:12Z'>
		Oh, I see that PyTorch has a &lt;denchmark-link:https://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py#L143&gt;deprecation warning&lt;/denchmark-link&gt;
 for the epoch param now. Although they still increment  at each step.
Anyway, probably it doesn't make sense to fix it, I'll close the issue. At least it'll be available in search if someone get stuck like me today :)
		</comment>
	</comments>
</bug>