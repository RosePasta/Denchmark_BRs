<bug id='2636' author='ehsanmok' open_date='2020-07-17T20:49:57Z' closed_time='2020-10-05T11:36:13Z'>
	<summary>nan metric breaking ModelCheckpoint</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Comparing any numbers to float('nan') is False in Python so as a result if a non-loss metric score is nan initially in training, then callback cannot checkpoint any scores after.
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Ignore a nan metric score. This is orthogonal to when grad or weights become nan.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version (e.g., 1.0): 1.5.0
OS (e.g., Linux): Linux
How you installed PyTorch (conda, pip, source): pip
Build command you used (if compiling from source):
Python version: 3.6
CUDA/cuDNN version: 10.0
GPU models and configuration: Tesla V100

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

Previous issue wasn't addressed completely &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1008&gt;#1008&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='ehsanmok' date='2020-07-17T20:50:49Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='ehsanmok' date='2020-09-15T17:43:27Z'>
		&lt;denchmark-link:https://github.com/ehsanmok&gt;@ehsanmok&lt;/denchmark-link&gt;
 mind send a PR? 
		</comment>
		<comment id='3' author='ehsanmok' date='2020-09-26T10:02:44Z'>
		Assign this to me please.
		</comment>
		<comment id='4' author='ehsanmok' date='2020-09-27T07:44:38Z'>
		So I can reproduce this issue &lt;denchmark-link:https://colab.research.google.com/drive/1dbvG_Gth8KvBsNHr5hQ4npIjmf_GAbNS?usp=sharing#scrollTo=ssQ_w97DvdzV&gt;like this&lt;/denchmark-link&gt;
.
I am not exactly clear on what the expected behavior is though. In &lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 's PR for &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1097/files&gt;nan detection and intervention&lt;/denchmark-link&gt;
, training is stopped when loss or weights contain  or infinite values.
What do we want to do:


if a metric that was passed as monitor param to ModelCheckpointgoes nan/inf:

raise an error (like when loss/weights do the same)
raise a warning but continue training and do not save any more checkpoints
raise a warning but continue training and do not save checkpoints when monitor is nan/inf. What if it returns to non nan/inf values (as mentioned in #1008)?



separate from this issue perhaps if any metric goes nan/inf (regardless of whether ModelCheckpoint) is used:

raise a warning / error ? (if the former than this should be addressed in a different issue perhaps)



when do we want to detect nan/infmetrics (whether monitor or any)?

ASAP which would be in the first validation step when it happens (perhaps even in Trainer.run_sanity_check)
on_validation_end (after all batches are processed) and model checkpoint is being saved



		</comment>
	</comments>
</bug>