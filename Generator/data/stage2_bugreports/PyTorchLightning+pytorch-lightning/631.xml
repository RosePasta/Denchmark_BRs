<bug id='631' author='filipeabperes' open_date='2019-12-17T01:16:01Z' closed_time='2020-01-05T19:36:07Z'>
	<summary>num_training_batches rounds down, causing 0 batches count</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

self.num_training_batches is defined using int &lt;denchmark-link:https://github.com/williamFalcon/pytorch-lightning/blob/ca73b70d15bc8db3f57c1fd2d3bf152e6e1d7c4e/pytorch_lightning/trainer/data_loading.py#L52&gt;here&lt;/denchmark-link&gt;
, which rounds it down to 0 when a small training_percent_check or overfit_pct is used, even though at least 1 batch is still processed.
This does not cause any errors in "vanilla" lightning, but crashes any user code that uses the number of batches in a division (for example to get an average of some quantity over batches).
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:
Set the training percentage to a small enough percentage that the number of examples is smaller than the batch size for a given dataset.
This would require a very simple fix, either to use math.ceil() or max(1, self.num_training_batches), depending of how the quantity is expected to behave in the rest of the code.
	</description>
	<comments>
		<comment id='1' author='filipeabperes' date='2019-12-18T13:48:31Z'>
		I think max(1, self.num_training_batches) would be the best because it doesn't change the behaviour for existing code that has num_training_batches &gt;= 1. I'm sure the PL team would appreciate if you made a PR. Is the bug also present for validation and test batches?
		</comment>
		<comment id='2' author='filipeabperes' date='2019-12-27T09:08:53Z'>
		Well, actually it is not the rounding issue. The problem is that now we process num_training_batches + 1 batches instead of num_training_batches if num_training_batches &lt; len(train_dataloader).
		</comment>
	</comments>
</bug>