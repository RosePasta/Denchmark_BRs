<bug id='4260' author='akihironitta' open_date='2020-10-20T15:44:49Z' closed_time='2020-10-26T06:19:35Z'>
	<summary>Tidy up returns from `ddp_train()` of accelerators</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

There are currently inconsistent returns from the method ddp_train of accelerator classes.
For example, the method ddp_train of DDP2Accelerator returns results while ddp_train of DDPSpawnAccelerator returns None. Although I am not familiar with distributed training, it seems that both of the methods should return results (or both should return None)  for consistency.



pytorch-lightning/pytorch_lightning/accelerators/ddp_spawn_accelerator.py


         Line 76
      in
      eddf35a






 def ddp_train(self, process_idx, mp_queue, model, is_master=False, proc_offset=0): 








pytorch-lightning/pytorch_lightning/accelerators/ddp2_accelerator.py


         Line 120
      in
      f37444f






 def ddp_train(self, process_idx, mp_queue, model): 





&lt;denchmark-h:h3&gt;Expected returns&lt;/denchmark-h&gt;

All methods whose names are the same (i.e. ddp_train()) should return the same type of object (e.g. results).
&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

This inconsistency was found while handling &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4232&gt;#4232&lt;/denchmark-link&gt;
.
	</description>
	<comments>
		<comment id='1' author='akihironitta' date='2020-10-24T01:18:33Z'>
		I think this is also where more typehint coverage + Pyre would detect issues based on static analysis
		</comment>
		<comment id='2' author='akihironitta' date='2020-10-24T10:40:57Z'>
		I‚Äôm not sure this is accurate.
sometimes they can‚Äôt really return something or it doesn‚Äôt make sense to.
ddp spawn can‚Äôt return anything :)
the functions are in a subprocess, and you can‚Äôt share objects between the processes unless you use a queue.
so, if you notice, ddpspawn train does not return anything but i think it tries to use a queue to pull out info and put it back on the main process.
in the example you mentioned, at the end of ddp spawn there is no return BUT there is a transfer state function... this function does return a few things but via the correct way which is a queue.
i suggest to watch the video on how ddp works which will explain how forking into processes happens (ie: mp.spawn, and hence the name), and why the use of spawn introduces limitations and renders returns impossible
		</comment>
	</comments>
</bug>