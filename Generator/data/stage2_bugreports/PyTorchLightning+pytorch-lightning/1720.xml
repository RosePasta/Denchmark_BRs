<bug id='1720' author='isvoboda' open_date='2020-05-03T20:20:27Z' closed_time='2020-05-17T12:22:45Z'>
	<summary>Incompatible Trainer.test()</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Trainer.test() stops on missing definitions of train &amp; validation dataloaders methods.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Define LightningModule without train &amp; validation dataloaders
Train such model with data loaders provided to Trainer.fit(...) method
Load model from checkpoint and test it with Trainer.test(...) method

test failes on :
'No `train_dataloader()` method defined. Lightning `Trainer` expects as minimum a'
pytorch_lightning.utilities.exceptions.MisconfigurationException: No `train_dataloader()` method defined. Lightning `Trainer` expects as minimum a `training_step()`, `training_dataloader()` and `configure_optimizers()` to be defined.
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

test_data = data_loader(...)
estimator = MyEstimator.load_from_checkpoint(checkpoint_path=CKPT_PATH)
trainer = pl.Trainer(...)
trainer.test(model=estimator, test_dataloaders=test_data)
&lt;denchmark-h:h3&gt;Expected behaviour&lt;/denchmark-h&gt;

Run the test with provided test data loader, and do not stop on condition related to missing train &amp; validation dataloaders which actually are not mandatory.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;* Packages:
        - numpy:             1.18.4
        - pyTorch_debug:     False
        - pyTorch_version:   1.5.0
        - pytorch-lightning: 0.7.5
        - tensorboard:       2.2.1
        - tqdm:              4.46.0
* System:
        - OS:                Linux
        - architecture:
                - 64bit
                - ELF
        - processor:         
        - python:            3.8.2
        - version:           #1 SMP Wed, 29 Apr 2020 16:23:03 +0000
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='isvoboda' date='2020-05-03T20:21:07Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='isvoboda' date='2020-05-03T20:54:41Z'>
		This issue relates to &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1195&gt;#1195&lt;/denchmark-link&gt;
,  and  should be decoupled. Is there any consensus or proposition of further steps?
		</comment>
		<comment id='3' author='isvoboda' date='2020-05-05T17:54:18Z'>
		Can't we just add
if not self.testing:
    self.check_model_configuration(model)
&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/2a2f303ae91a4a17b1cd8127e5b811e38cb2d978/pytorch_lightning/trainer/trainer.py#L732&gt;here&lt;/denchmark-link&gt;
 because during testing the testing configuraton is already checked &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/2a2f303ae91a4a17b1cd8127e5b811e38cb2d978/pytorch_lightning/trainer/trainer.py#L970&gt;here&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='4' author='isvoboda' date='2020-05-07T15:37:07Z'>
		Also, using  on loaded checkpoint doesn't work without explicitly passing  parameter even if all  methods are defined. &lt;denchmark-link:https://colab.research.google.com/drive/1qtbS48QrNqGW4qXvfEdMdwIcURjBc8ne?usp=sharing&gt;Example&lt;/denchmark-link&gt;
.
This use case isn't documented in the 0.7.5 documentation but evidently was relevant at some point: &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/test_set.html#test-pre-trained-model&gt;https://pytorch-lightning.readthedocs.io/en/latest/test_set.html#test-pre-trained-model&lt;/denchmark-link&gt;
.
I suggest to either:

fix Test set documentation
(preferred) allow to call trainer.test(model) without specifying test_dataloader parameter. After all, what's the point of defining Model.test_dataloader method?

		</comment>
		<comment id='5' author='isvoboda' date='2020-05-07T17:34:34Z'>
		&lt;denchmark-link:https://github.com/iakremnev&gt;@iakremnev&lt;/denchmark-link&gt;
 can you fix the link? It's asking for request access.
		</comment>
		<comment id='6' author='isvoboda' date='2020-05-07T17:54:23Z'>
		&lt;denchmark-link:https://github.com/rohitgr7&gt;@rohitgr7&lt;/denchmark-link&gt;
 oops, fixed it.
		</comment>
		<comment id='7' author='isvoboda' date='2020-05-07T19:16:14Z'>
		Error coming from &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/3a642601e84c3abf1f1b438f9acc932a1f150f7f/pytorch_lightning/trainer/trainer.py#L1045&gt;this line&lt;/denchmark-link&gt;
. Why is it checking whether test_dataloader() returns something or not. If we just check whether it is overridden or not wouldn't that be enough just like it is done in case of  &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/3a642601e84c3abf1f1b438f9acc932a1f150f7f/pytorch_lightning/trainer/trainer.py#L988&gt;here&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='8' author='isvoboda' date='2020-05-07T20:06:00Z'>
		I would suggest
gave_test_loader = isinstance(model.test_dataloader, _PatchDataLoader) or self.is_overridden('test_dataloader', model)
&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/3a642601e84c3abf1f1b438f9acc932a1f150f7f/pytorch_lightning/trainer/trainer.py#L1045&gt;here&lt;/denchmark-link&gt;
, and
if not self.testing:
    self.check_model_configuration(model)
&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/2a2f303ae91a4a17b1cd8127e5b811e38cb2d978/pytorch_lightning/trainer/trainer.py#L732&gt;here&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='isvoboda' date='2020-05-11T20:48:13Z'>
		it is the same as &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1754&gt;#1754&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>