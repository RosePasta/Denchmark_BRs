<bug id='1479' author='VitorGuizilini' open_date='2020-04-13T19:27:22Z' closed_time='2020-06-01T15:00:35Z'>
	<summary>validation_epoch_end behavior with DDP</summary>
	<description>
I might be misunderstanding how PL works, but when using DDP my validation_epoch_end argument still contains batches from single GPUs, and I thought they would be collated from all GPUs.
E.g. My validation dataset has 888 images, but when I validate on 8 GPUs (batch size of 1), I only get 111 batches in validation_epoch_end.
If that's correct, how can I produce metrics that combine information from all GPUs?
	</description>
	<comments>
		<comment id='1' author='VitorGuizilini' date='2020-04-14T09:45:44Z'>
		 validation_step operates on a single batch of data from the validation set.
validation_epoch_end is called at the end of the validation epoch with the outputs of all validation steps.
So I believe your problem lies at validation_step. Can you show us your validation step, ideally the whole model?
		</comment>
		<comment id='2' author='VitorGuizilini' date='2020-04-14T15:20:24Z'>
		I asked around and apparently that is the intended behaviour right now, i.e. validation_epoch_end is per-process, and we cannot access global information for metrics or logging. I was able to solve this by doing the reduce_all myself, with something like this:
&lt;denchmark-code&gt;metrics[key] = metrics[key].to('cuda:{}'.format(self.trainer.proc_rank))
dist.all_reduce(metrics[key])
metrics[key] /= self.trainer.world_size
&lt;/denchmark-code&gt;

Not sure why I had to explicitly cast the tensors to their processes (their devices were all set to -1).
		</comment>
		<comment id='3' author='VitorGuizilini' date='2020-05-05T14:24:24Z'>
		Also want native support of this ability. such as adding an argument to validation_epoch_end , when it's set to True, then metrics returned by validation_epoch_end  will automatically be all_reduce and logged, instead of only metrics on each process
		</comment>
	</comments>
</bug>