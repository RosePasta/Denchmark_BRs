<bug id='638' author='kwanUm' open_date='2019-12-19T12:52:41Z' closed_time='2020-04-07T10:30:47Z'>
	<summary>Pytorch lightning spawns processes after each epoch of training, causing training script to crash unexpectedly</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When running my training script with PTL, I noticed that after the script is running process spawning is happening at the end of every epoch, which reloads the script. This unintended behavior often causes the training to crash if I'm in the middle of changing the code, and essentially locks the script running from any edits to it.
A possible fix for this is to create the Data Loader object only once &lt;denchmark-link:https://fburl.com/1x51d9ul&gt;here&lt;/denchmark-link&gt;
.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Train with PTL using DDP and num_workers &gt; 1, and place a breakpoint at the top of the main file of your project.
	</description>
	<comments>
		<comment id='1' author='kwanUm' date='2020-01-21T02:35:16Z'>
		That link seems to be FB-internal, so I can't see what you're linking to, but you may be able to solve your problem by defining the dataloader as an attribute, then simply returning the existing dataloader in the val_dataloader method.
Early versions of Lightning actually cached the dataloader so that it didn't get re-created every epoch, but there are some use cases that need to create a new DL every time. This way, you have the flexibility to cache it yourself.
		</comment>
		<comment id='2' author='kwanUm' date='2020-01-21T06:50:30Z'>
		Thanks for your reply Nic, I'll go ahead and try that!
&lt;denchmark-link:#&gt;‚Ä¶&lt;/denchmark-link&gt;


On Tue, Jan 21, 2020 at 4:35 AM Nic Eggert ***@***.***&gt; wrote:
 That link seems to be FB-internal, so I can't see what you're linking to,
 but you may be able to solve your problem by defining the dataloader as an
 attribute, then simply returning the existing dataloader in the
 val_dataloader method.

 Early versions of Lightning actually cached the dataloader so that it
 didn't get re-created every epoch, but there are some use cases that need
 to create a new DL every time. This way, you have the flexibility to cache
 it yourself.

 ‚Äî
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 &lt;#638?email_source=notifications&amp;email_token=AB5EBDELPYKS3JKJX6U4VG3Q6ZNOJA5CNFSM4J5FOHPKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEJOJLCA#issuecomment-576492936&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AB5EBDAKGCFLHKW3J6ID2K3Q6ZNOJANCNFSM4J5FOHPA&gt;
 .



		</comment>
		<comment id='3' author='kwanUm' date='2020-01-21T12:34:54Z'>
		&lt;denchmark-link:https://github.com/tullie&gt;@tullie&lt;/denchmark-link&gt;
 want to look at this?
		</comment>
		<comment id='4' author='kwanUm' date='2020-01-21T12:44:20Z'>
		Yep. Will take a look this week
		</comment>
		<comment id='5' author='kwanUm' date='2020-01-31T02:37:46Z'>
		&lt;denchmark-link:https://github.com/kwanUm&gt;@kwanUm&lt;/denchmark-link&gt;
 i've been unable to reproduce the problem.
I'm wondering if you're not tagging your test_dataloader with @data_loader. This will ensure that the dataloader is only constructed once like you suggested. See 


pytorch-lightning/pytorch_lightning/core/decorators.py


         Line 5
      in
      06242c2






 def data_loader(fn): 





If that doesn't help, can you please send me some more code of your setup so I can try and reproduce?
		</comment>
		<comment id='6' author='kwanUm' date='2020-02-01T21:16:21Z'>
		Thanks for trying to check this &lt;denchmark-link:https://github.com/tullie&gt;@tullie&lt;/denchmark-link&gt;
.
I haven't defined a test_dataloader method in my PTL Trainer - only overriden train_dataloader().
I'm sorry for not being able to share the code for you to reproduce it yourself, but here's my train_dataloader code (it's annotated correctly):
&lt;denchmark-code&gt;@pl.data_loader
def train_dataloader(self):
    train_dataset = GANTrainingDataset(...)
    if self.distributed_backend == "ddp":
        data_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset,
                                                                       num_replicas=self.num_of_gpus_running,
                                                                       rank=self.logger.rank)
    else:
        data_sampler = torch.utils.data.RandomSampler(train_dataset)
    return DataLoader(
        train_dataset,
        batch_size=self.batch_size,
        sampler=data_sampler,
        num_workers=self.num_workers,
        drop_last=True,
        pin_memory=True,
    )
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='kwanUm' date='2020-02-03T22:04:15Z'>
		&lt;denchmark-link:https://github.com/kwanUm&gt;@kwanUm&lt;/denchmark-link&gt;
 thanks - I was able to reproduce the problem.
My understanding is that you want DataLoader worker processes to be reused after each epoch. There's an issue and suggested work around for that in the pytorch repo here: &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/15849#issuecomment-573921048&gt;pytorch/pytorch#15849 (comment)&lt;/denchmark-link&gt;

Another suggestion, and what I do, is copy the code base to a separate folder before running distributed training. This ensures any local changes you make won't affect the run.
Let me know if this helps!
		</comment>
		<comment id='8' author='kwanUm' date='2020-04-03T22:22:33Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
		<comment id='9' author='kwanUm' date='2020-04-07T08:07:25Z'>
		Copying the code to another location for each run did the trick for me. Thanks!
		</comment>
		<comment id='10' author='kwanUm' date='2020-04-07T10:30:47Z'>
		awesome!
		</comment>
	</comments>
</bug>