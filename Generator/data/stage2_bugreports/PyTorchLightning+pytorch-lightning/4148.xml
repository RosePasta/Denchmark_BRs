<bug id='4148' author='AljoSt' open_date='2020-10-14T15:11:44Z' closed_time='2020-11-13T10:17:09Z'>
	<summary>missing scaler.scale in manual_backward?</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When using manual_backward and precision=16, an exception is raised:
RuntimeError: unscale_() has already been called on this optimizer since the last update().
As far as I can see, the scaler is actually never used to scale the loss during  and in the example &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/optimizers.html&gt;here&lt;/denchmark-link&gt;
 it is not mentioned that one has to do it before.
Also, I cannot find any 'update' call
colab model to reproduce:
&lt;denchmark-link:https://colab.research.google.com/drive/1n7-NxC0IJ3gYJGZLsoeOtu-oQc3I8UjF?usp=sharing&gt;https://colab.research.google.com/drive/1n7-NxC0IJ3gYJGZLsoeOtu-oQc3I8UjF?usp=sharing&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='AljoSt' date='2020-10-15T09:58:26Z'>
		I am also facing this issue with code identical to the example and 16 bit precision (lightning version is 1.0.1)
		</comment>
		<comment id='2' author='AljoSt' date='2020-10-29T19:54:16Z'>
		might be fixed by &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4311&gt;#4311&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='AljoSt' date='2020-11-03T18:46:31Z'>
		&lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;
 please update status if you made any progress on this
		</comment>
		<comment id='4' author='AljoSt' date='2020-11-04T20:26:49Z'>
		I can give an update here, it seems that within manual backward with native AMP we are missing some calls to the scaler, particular the update call. This happens because we never call optimizer_step in the accelerator when using manual_backward.
To remedy this, you can add this to the bottom of your override training step:
# scale when native amp
if self.trainer.amp_backend == AMPType.NATIVE:
       self.trainer.scaler.update()
such as:
        def training_step(self, batch, batch_idx, optimizer_idx):
            # manual
            (opt_a, opt_b) = self.optimizers()
            loss_1 = self.step(batch[0])

            # make sure there are no grads
            if batch_idx &gt; 0:
                assert torch.all(self.layer.weight.grad == 0)

            self.manual_backward(loss_1, opt_a)
            opt_a.step()
            opt_a.zero_grad()
            assert torch.all(self.layer.weight.grad == 0)

            # fake discriminator
            loss_2 = self.step(batch[0])

            # ensure we forward the correct params to the optimizer
            # without retain_graph we can't do multiple backward passes
            self.manual_backward(loss_2, opt_b, retain_graph=True)
            self.manual_backward(loss_2, opt_a, retain_graph=True)

            assert self.layer.weight.grad is not None
            opt_b.step()
            opt_b.zero_grad()
            assert torch.all(self.layer.weight.grad == 0)
            
            # scale when native amp
            if self.trainer.amp_backend == AMPType.NATIVE:
                 self.trainer.scaler.update()
The work/discussion is going on in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4485&gt;#4485&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='AljoSt' date='2020-11-09T17:38:14Z'>
		&lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='AljoSt' date='2020-11-09T18:35:46Z'>
		Hey the work is still WIP in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4485&gt;#4485&lt;/denchmark-link&gt;
 . We want to make sure we do this correctly and ensure the flow handles edge cases :)
		</comment>
		<comment id='7' author='AljoSt' date='2020-11-13T10:17:09Z'>
		This has been solved in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4485&gt;#4485&lt;/denchmark-link&gt;
 with the addition of manual_optimizer_step:
def training_step(...):
                (opt_a, opt_b) = self.optimizers()
                loss = ...
                # automatically applies scaling, etc...
                self.manual_backward(loss, opt_a)
                # This will force an opt.step() even if accumulate_grad_batches is set.
                self.manual_optimizer_step(opt_a, force_optimizer_step=True, custom_args=4)
manual_optimizer step will ensure appropriate step across hardware, and does the scaler update as well!
		</comment>
	</comments>
</bug>