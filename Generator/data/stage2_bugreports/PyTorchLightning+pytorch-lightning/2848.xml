<bug id='2848' author='c00k1ez' open_date='2020-08-06T09:20:52Z' closed_time='2020-09-02T11:45:24Z'>
	<summary>Cannot pickle custom metric with DDP mode.</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

just run this script.

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import torch
import torch.nn.functional as F
import pytorch_lightning as pl
from pytorch_lightning.metrics import Metric, TensorMetric

class MetricPerplexity(Metric):
    """
    Computes the perplexity of the model.
    """

    def __init__(self, pad_idx: int, *args, **kwargs):
        super().__init__(name='ppl')
        self.pad_idx = pad_idx

    def forward(self, pred: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:
        
        loss = F.cross_entropy(pred, target, reduction='none')
        non_padding = target.ne(self.pad_idx)
        loss = loss.masked_select(non_padding).sum()

        num_words = non_padding.sum()
        ppl = torch.exp(
            torch.min(loss / num_words, torch.tensor([100]).type_as(loss))
        )
        return ppl

class TensorPerplexity(TensorMetric):
    """
    Computes the perplexity of the model.
    """

    def __init__(self, pad_idx: int, *args, **kwargs):
        super().__init__(name='ppl', *args, **kwargs)
        self.pad_idx = pad_idx

    def forward(self, pred: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:
        
        loss = F.cross_entropy(pred, target, reduction='none')
        non_padding = target.ne(self.pad_idx)
        loss = loss.masked_select(non_padding).sum()

        num_words = non_padding.sum()
        ppl = torch.exp(
            torch.min(loss / num_words, torch.tensor([100]).type_as(loss))
        )
        return ppl

class ModelWithMetric(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.lin = torch.nn.Linear(50, 1)
        self.ppl = MetricPerplexity(100)
    
    def training_step(self, batch, batch_nb):
        return {'loss': torch.mean(self(batch))}
    
    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), 0.01)
    
    def forward(self, batch):
        return self.lin(batch[0])

class ModelWithTensorMetric(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.lin = torch.nn.Linear(50, 1)
        self.ppl = TensorPerplexity(100)
    
    def training_step(self, batch, batch_nb):
        return {'loss': torch.mean(self(batch))}
    
    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), 0.01)
    
    def forward(self, batch):
        return self.lin(batch[0])

if __name__ == "__main__":
    m1 = ModelWithMetric()
    m2 = ModelWithTensorMetric()

    t1 = pl.Trainer(distributed_backend='ddp_cpu', fast_dev_run=True)
    t2 = pl.Trainer(distributed_backend='ddp_cpu', fast_dev_run=True)

    loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.rand(10, 50)), batch_size=5)
    t1.fit(m1, train_dataloader=loader)
    print('works well')
    t2.fit(m2, train_dataloader=loader)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Error log&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;AttributeError: Can't pickle local object '_apply_to_outputs.&lt;locals&gt;.decorator_fn.&lt;locals&gt;.new_func'
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

I do not know how to explain this behaviour, I think both of this classes should work well with DDP.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;



OS: MacOS


CUDA:
- GPU:
- available:         False
- version:           None


Packages:
- numpy:             1.18.5
- pyTorch_debug:     False
- pyTorch_version:   1.6.0
- pytorch-lightning: 0.9.0rc9
- tensorboard:       2.2.2
- tqdm:              4.48.0


System:
- OS:                Darwin
- architecture:
- 64bit
- processor:         i386
- python:            3.7.7
- version:           Darwin Kernel Version 19.6.0: Sun Jul  5 00:43:10 PDT 2020


&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='c00k1ez' date='2020-08-07T08:01:18Z'>
		Hi &lt;denchmark-link:https://github.com/c00k1ez&gt;@c00k1ez&lt;/denchmark-link&gt;
 ,
we ware aware of that. This is due to our decorators. I have some local experiments on that, but I was not yet able to get it running completely. Since some of the issues may be resolved with &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2528&gt;#2528&lt;/denchmark-link&gt;
 , I'd like to wait for this one before trying to tackle it.
EDIT: basically this is due to &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/metrics/converters.py#L25&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/metrics/converters.py#L25&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/metrics/converters.py#L50&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/metrics/converters.py#L50&lt;/denchmark-link&gt;
.
If you want to take this over, the way to go is probably to use &lt;denchmark-link:https://docs.python.org/3.8/library/functools.html#functools.wraps&gt;https://docs.python.org/3.8/library/functools.html#functools.wraps&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='c00k1ez' date='2020-08-07T08:14:51Z'>
		Ok üëå
Yeah, thanks for links, I will explore this question üëç
		</comment>
		<comment id='3' author='c00k1ez' date='2020-08-13T09:04:34Z'>
		To check up again on this: seems like &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2528&gt;#2528&lt;/denchmark-link&gt;
 actually resolves this issue. So probably don't investigate much further until we verified this with &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2528&gt;#2528&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='c00k1ez' date='2020-09-02T11:45:23Z'>
		Just confirmed that &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2528&gt;#2528&lt;/denchmark-link&gt;
 actually solved this issue (using the provided code). Closing the issue.
		</comment>
	</comments>
</bug>