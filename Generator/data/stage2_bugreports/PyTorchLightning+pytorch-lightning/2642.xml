<bug id='2642' author='michal2409' open_date='2020-07-18T19:24:11Z' closed_time='2020-08-03T21:46:11Z'>
	<summary>Apex with auto lr finder fails</summary>
	<description>
Hi! I have this error when using together flags precision=16, auto_lr_find=True in Trainer . The error occurs when learning rate finder finish and before training starts. I guess it can happen because of calling amp.initialize for already initialized model i.e the first one it is called when for auto_lr_find and then the second one for the same model for training initialization.
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "train.py", line 136, in &lt;module&gt;
    trainer.fit(model)
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 1003, in fit
    results = self.single_gpu_train(model)
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_parts.py", line 182, in single_gpu_train
    model, optimizers = model.configure_apex(amp, model, self.optimizers, self.amp_level)
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py", line 1006, in configure_apex
    model, optimizers = amp.initialize(model, optimizers, opt_level=amp_level)
  File "/opt/conda/lib/python3.6/site-packages/apex/amp/frontend.py", line 358, in initialize
    return _initialize(models, optimizers, _amp_state.opt_properties, num_losses, cast_model_outputs)
  File "/opt/conda/lib/python3.6/site-packages/apex/amp/_initialize.py", line 171, in _initialize
    check_params_fp32(models)
  File "/opt/conda/lib/python3.6/site-packages/apex/amp/_initialize.py", line 87, in check_params_fp32
    name, param.type()))
  File "/opt/conda/lib/python3.6/site-packages/apex/amp/_amp_state.py", line 32, in warn_or_err
    raise RuntimeError(msg)
RuntimeError: Found param unet.input_block.conv1.0.weight with type torch.cuda.HalfTensor, expected torch.cuda.FloatTensor.
When using amp.initialize, you do not need to call .half() on your model
before passing it, no matter what optimization level you choose.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='michal2409' date='2020-07-18T19:25:13Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='michal2409' date='2020-08-03T21:46:10Z'>
		Same issue as &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2655&gt;#2655&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>