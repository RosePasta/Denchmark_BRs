<bug id='4676' author='quinor' open_date='2020-11-14T22:09:53Z' closed_time='2020-11-16T11:30:09Z'>
	<summary>load_from_checkpoint not working when using save_hyperparameters(conf)</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Following the docs (&lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/hyperparameters.html#lightningmodule-hyperparameters&gt;https://pytorch-lightning.readthedocs.io/en/latest/hyperparameters.html#lightningmodule-hyperparameters&lt;/denchmark-link&gt;
), precisely point 4.:
&lt;denchmark-code&gt;class Example(LightningModule):

    def __init__(self, cfg, *args, **kwargs):
        super().__init__()
        self.save_hyperparameters(cfg)

(...)

Example(dict(key="value", key2="value2"))
&lt;/denchmark-code&gt;

Attempting to load previously saved checkpoint through Example.load_from_checkpoint yields following error:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "./decode.py", line 101, in &lt;module&gt;
    model = DeeplabSearch.load_from_checkpoint(args.checkpoint)
  File "/home/wj359634/venv/lib64/python3.6/site-packages/pytorch_lightning/core/saving.py", line 154, in load_from_checkpoint
    model = cls._load_model_state(checkpoint, strict=strict, **kwargs)
  File "/home/wj359634/venv/lib64/python3.6/site-packages/pytorch_lightning/core/saving.py", line 194, in _load_model_state
    model = cls(**_cls_kwargs)
TypeError: __init__() missing 1 required positional argument: 'cfg'
&lt;/denchmark-code&gt;

I will try to reproduce using the BoringModel once I find some time if someone requests it.
PS
I'll definitely appreciate a hotfix if one comes to someone's mind.
	</description>
	<comments>
		<comment id='1' author='quinor' date='2020-11-14T22:34:51Z'>
		hi, mind shoot a boring model to help us to replicate this issue?
btw, mind share version of PL you are using?
		</comment>
		<comment id='2' author='quinor' date='2020-11-14T23:14:05Z'>
		Right, I have forgotten about the version dump. That's probably important.
I had not updated to the most recent version due to some hotfixes being applied directly to my lightning installation and I did not have time to redo them in the newest one. I looked through the release log and did not find anything related to my issue though.
&lt;denchmark-code&gt;* CUDA:
        - GPU:
                - Tesla V100-PCIE-32GB
        - available:         True
        - version:           10.2
* Packages:
        - numpy:             1.18.4
        - pyTorch_debug:     True
        - pyTorch_version:   1.7.0
        - pytorch-lightning: 1.0.4
        - tqdm:              4.46.0
* System:
        - OS:                Linux
        - architecture:
                - 64bit
                - ELF
        - processor:         x86_64
        - python:            3.6.8
        - version:           #1 SMP Tue Aug 25 17:23:54 UTC 2020
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='quinor' date='2020-11-16T09:51:59Z'>
		Hi &lt;denchmark-link:https://github.com/quinor&gt;@quinor&lt;/denchmark-link&gt;
  ,
I tried to replicate the issue, and it only comes up if the model you save is without self.save_hyperparameters(cfg) ...
Working code below :
&lt;denchmark-code&gt;class MNISTModel(pl.LightningModule):

    def __init__(self, cfg,*args, **kwargs):
        super(MNISTModel, self).__init__()
        self.save_hyperparameters(cfg)
        self.l1 = torch.nn.Linear(28 * 28, 10)

    def forward(self, x):
        return torch.relu(self.l1(x.view(x.size(0), -1)))

    def training_step(self, batch, batch_nb):
        x, y = batch
        loss = F.cross_entropy(self(x), y)
        return loss

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.02)

# Init our model
mnist_model = MNISTModel(dict(key="value", key2="value2"))

# Init DataLoader from MNIST Dataset
train_ds = MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())
train_loader = DataLoader(train_ds, batch_size=32)

# Initialize a trainer
trainer = pl.Trainer(default_root_dir='/tmp/',gpus=1, max_epochs=1, progress_bar_refresh_rate=20)

# Train the model ‚ö°
trainer.fit(mnist_model, train_loader)
trainer.save_checkpoint("mnist.ckpt")
new_model = MNISTModel.load_from_checkpoint(checkpoint_path="mnist.ckpt")

&lt;/denchmark-code&gt;

Code which produces the same issue as what you encountered :
&lt;denchmark-code&gt;
class MNISTModel(pl.LightningModule):
‚Äã
    def __init__(self, cfg,*args, **kwargs):
        super(MNISTModel, self).__init__()
        #self.save_hyperparameters(cfg)
        self.l1 = torch.nn.Linear(28 * 28, 10)
‚Äã
    def forward(self, x):
        return torch.relu(self.l1(x.view(x.size(0), -1)))
‚Äã
    def training_step(self, batch, batch_nb):
        x, y = batch
        loss = F.cross_entropy(self(x), y)
        return loss
‚Äã
    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.02)

# Init our model
mnist_model = MNISTModel(dict(key="value", key2="value2"))

# Init DataLoader from MNIST Dataset
train_ds = MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())
train_loader = DataLoader(train_ds, batch_size=32)

# Initialize a trainer
trainer = pl.Trainer(default_root_dir='/tmp/',gpus=1, max_epochs=1, progress_bar_refresh_rate=20)

# Train the model ‚ö°
trainer.fit(mnist_model, train_loader)
trainer.save_checkpoint("mnist2.ckpt")

new_model = MNISTModel.load_from_checkpoint(checkpoint_path="mnist2.ckpt") 
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
  I'd be happy to help with a quick fix but I'm not sure whether this error needs to be fixed if the parameters to class constructor aren't saved as hyperparameters in the constructor using self.save_hyperparameters(cfg).
If this error needs to be fixed, then what should be the expected behaviour in this case?
		</comment>
		<comment id='4' author='quinor' date='2020-11-16T11:30:09Z'>
		Hmm, I've tried a proper reproduction this time and it does work "in the lab". It still doesn't in the wild (but I've found another suspect, the checkpoints I was trying to load were from slightly different version of the code), thanks &lt;denchmark-link:https://github.com/shaginhekvs&gt;@shaginhekvs&lt;/denchmark-link&gt;
 for proving me wrong. I'm closing the issue for now, will reopen if the issue reappears in controlled condition.
		</comment>
		<comment id='5' author='quinor' date='2020-11-16T12:01:03Z'>
		it could be related to &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4662&gt;#4662&lt;/denchmark-link&gt;
 fix
		</comment>
		<comment id='6' author='quinor' date='2020-12-04T12:08:42Z'>
		now here comes a new problem, if we have a model defined as model=Model(backbone, ...), and we need to pass a network such as resnet to it, then self.save_hyperparameters() will save the resnet and it is time-costing, if we don't use self.save_hyperparameters(), then Model.load_from_checkpoint() will fail to work.
		</comment>
		<comment id='7' author='quinor' date='2020-12-04T13:14:20Z'>
		You can list only arguments to be saved, eventually we can make exclude some arguments...
		</comment>
		<comment id='8' author='quinor' date='2020-12-04T15:30:26Z'>
		I got it!
		</comment>
	</comments>
</bug>