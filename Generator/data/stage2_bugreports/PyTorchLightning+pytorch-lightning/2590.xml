<bug id='2590' author='ShomyLiu' open_date='2020-07-12T04:42:17Z' closed_time='2020-10-03T18:05:33Z'>
	<summary>Memory leaks: process still remain in the back if even the code is finished.</summary>
	<description>
Hi!
I'm new to Lightning, and have experienced one day. However, I found that there are some critical issues, especially in multi-gpu in Memory Leaks:
(1)  Even if the code finished and exited,  the process is still in the background.
(2) After I kill those process manually one by one, there seems still some processes occupying the GPU memories: for example:
&lt;denchmark-link:https://user-images.githubusercontent.com/10215945/87239034-ee1a3c00-c43c-11ea-92d1-03f4394ed095.png&gt;&lt;/denchmark-link&gt;

BTW, there are some other issues in multi-gpu settings.
	</description>
	<comments>
		<comment id='1' author='ShomyLiu' date='2020-07-12T07:50:20Z'>
		The following reproduces can be sure to happen the memory leaks in my code (100%) for your referece:

clone the code from:  https://github.com/ShomyLiu/pytorch_bert_elmo_example
some third-party would need

fire, transformer, and so on
maybe the env should be added export TOKENIZERS_PARALLELISM=False for forbidding the warning information.


go to the data dir ,  download and unzip the dataset in google drive in data/README.md

&lt;denchmark-code&gt;cd data
unzip bert_elmo_glove.weight.zip
&lt;/denchmark-code&gt;


checkout the pl branch ('pl' means pytorch lightning)

&lt;denchmark-code&gt;git checkout pl
&lt;/denchmark-code&gt;


run the code in multi-gpu settings would lead to memory leaks:  for example:

&lt;denchmark-code&gt;python3 main.py train --gpu_id=[0,1] --epochs=5
&lt;/denchmark-code&gt;

Each running will remain a process that would not be exited:
&lt;denchmark-link:https://user-images.githubusercontent.com/10215945/87241532-b9b37980-c456-11ea-9b7a-1b2b73696b29.png&gt;&lt;/denchmark-link&gt;

My enviroment:

python3.6.8
NVIDIA-SMI:  418.39
CUDA: 10.0
pytorch: 1.5.1+cu101
pytorch-lightning: 0.8.5

I'm not sure that the reason from pytorch or lightning.
&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='ShomyLiu' date='2020-07-12T16:22:09Z'>
		Does it also happen with distributed_backend="ddp_spawn"?
		</comment>
		<comment id='3' author='ShomyLiu' date='2020-07-13T00:34:27Z'>
		Hi, I have tried the different backends settings:

distributed_backend="ddp_spawn" with num_workers=0, and in this settings, there is no memory leaks. However, there would be warning:

&lt;denchmark-code&gt; You are using `distributed_backend=ddp_spawn` with num_workers=0. For much faster performance, switch to `distributed_backend=ddp` and set `num_workers&gt;0
&lt;/denchmark-code&gt;


distributed_backend="dp"  this would directly raise an error:

&lt;denchmark-code&gt;RuntimeError: arguments are located on different GPUs
&lt;/denchmark-code&gt;

In addition, if distributed_backend="ddp" and let the code runs over the memory leaks would happen. But if I interrupt the program manually during its running with ctrl-c,  the memory leaks would not happen.   Hope this can help.
		</comment>
		<comment id='4' author='ShomyLiu' date='2020-07-13T01:21:13Z'>
		I found this in the code:
&lt;denchmark-code&gt;def forward(self, x, device):
        self.device = device
&lt;/denchmark-code&gt;

this does not look right. LightningModule also hase a self.device attribute, these calls  could cause the  data left on the wrong device and maybe cause the memory leak?
&lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html#init-tensors-using-type-as-and-register-buffer&gt;https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html#init-tensors-using-type-as-and-register-buffer&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='ShomyLiu' date='2020-07-13T01:38:21Z'>
		Just I have closed another issue about how to put new tensors into the right device  :  &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2585&gt;#2585&lt;/denchmark-link&gt;

Since there are new tensors created in the submodule of  (ie: the Net), so I have passed the device to the  .
&lt;denchmark-code&gt;def forward(self, x, device):
        self.device = device
&lt;/denchmark-code&gt;

Here self is a submodule instead of the main module of pl.LightningModule,  so I think this just a variable including the device information regardless of what the name of the variable is.
		</comment>
		<comment id='6' author='ShomyLiu' date='2020-09-15T20:24:58Z'>
		

run the code in multi-gpu settings would lead to memory leaks:  for example:

python3 main.py train --gpu_id=[0,1] --epochs=5


there is no attribute --gpu_id you shall use --gpus
		</comment>
		<comment id='7' author='ShomyLiu' date='2020-09-16T01:29:30Z'>
		&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
  Hi,   is the argparse in my code, and i will pass the  into the  in .
Yet, maybe this issue has been resolved in the current version,  and I will check again asap
		</comment>
		<comment id='8' author='ShomyLiu' date='2020-09-16T01:35:57Z'>
		I can confirm this is still an issue. I also run into it very often when I kill ddp training. The problem is that the kill signal (like keyboard interrupt for example) is not sent to the children processes in ddp, and they keep running.
I promise I will get back to &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2165&gt;#2165&lt;/denchmark-link&gt;
 soon to fix it.
		</comment>
		<comment id='9' author='ShomyLiu' date='2020-09-16T01:42:00Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
  Thanks for your great effort, and it's indeed a critical issue.
		</comment>
	</comments>
</bug>