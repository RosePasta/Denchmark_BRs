<bug id='2330' author='stllfe' open_date='2020-06-23T14:44:48Z' closed_time='2020-06-25T13:21:43Z'>
	<summary>`use_amp` and multiple optimizers bug</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Faced this issue when tried to use mixed precision with my two-head model which has two pairs of optimizer &amp; scheduler. Without use_amp everything works fine.
With it enabled, I get:
&lt;denchmark-code&gt;TypeError: 'CosineAnnealingLR' object is not subscriptable
&lt;/denchmark-code&gt;

My investigation has ended up here:
    def reinit_scheduler_properties(self, optimizers: list, schedulers: list):
        # Reinitialize optimizer.step properties added by schedulers
        for scheduler in schedulers:
            for optimizer in optimizers:
                scheduler = scheduler['scheduler']  # &lt;===== this place
                # ...
                if scheduler.optimizer == optimizer:
                    # ...
Obviously, next optimizer will get scheduler as an actual non-dict object as it was reassigned on the first iteration...
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Build any LightningModule, which configure_optimizers() method outputs lists of two optimizers and two schedulers. In my case it's something like:

def configure_optimizers(self):
    opt_1 = Adam(params=self.head_1.params(), ...)
    opt_2 = Adam(params=self.head_2.params(), ...)
    
    sch_1 = CosineAnnealingLR(optimizer=opt_1, ...)
    sch_2 = CosineAnnealingLR(optimizer=opt_2, ...)
    return [opt_1, opt_2], [sch_1, sch_2]

Build a Trainer object with  use_amp=True
Call trainer.fit(model)
See error

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;* CUDA:
        - GPU:
                - GeForce GTX 1080 Ti
        - available:         True
        - version:           10.2
* Packages:
        - numpy:             1.18.5
        - pyTorch_debug:     False
        - pyTorch_version:   1.5.0
        - pytorch-lightning: 0.8.1
        - tensorboard:       1.15.0
        - tqdm:              4.45.0
* System:
        - OS:                Linux
        - architecture:
                - 64bit
                -
        - processor:         x86_64
        - python:            3.7.5
        - version:           #107-Ubuntu SMP Thu Jun 4 11:27:52 UTC 2020
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='stllfe' date='2020-06-23T14:45:28Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='stllfe' date='2020-06-24T10:09:24Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 this seems easy to fix (move the  part to the outer loop), and we could also fix issue &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2078&gt;#2078&lt;/denchmark-link&gt;
 while fixing this function
		</comment>
	</comments>
</bug>