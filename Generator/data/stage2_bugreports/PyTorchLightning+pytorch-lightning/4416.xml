<bug id='4416' author='hudeven' open_date='2020-10-28T19:45:11Z' closed_time='2020-11-03T22:02:03Z'>
	<summary>Can't TorchScript LightningModule when using Metric</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Please reproduce using the BoringModel and post here&lt;/denchmark-h&gt;

able to reproduce it in &lt;denchmark-link:https://colab.research.google.com/drive/1MscNHxIc_LIbZxALHbZOAkooNu0TzVly?usp=sharing&gt;https://colab.research.google.com/drive/1MscNHxIc_LIbZxALHbZOAkooNu0TzVly?usp=sharing&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Able to torchscript a Lightning moduel no matter Metric is used or not
It seems hard to make Metric torchscriptable as *args and **kwargs are useful in Python but not supported in torchscript.
As Metric is not needed for inference, I think it should be excluded when calling LightningModule.to_torchscript().
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

Note: Bugs with code are solved faster ! Colab Notebook should be made public !

Colab Notebook: Please copy and paste the output from our environment collection script (or fill out the checklist below manually).

You can get the script and run it with:
&lt;denchmark-code&gt;wget https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py
# For security purposes, please check the contents of collect_env_details.py before running it.
python collect_env_details.py
&lt;/denchmark-code&gt;


CUDA:

GPU:

Tesla T4


available:         True
version:           10.1


Packages:

numpy:             1.18.5
pyTorch_debug:     False
pyTorch_version:   1.6.0+cu101
pytorch-lightning: 0.10.0
tqdm:              4.41.1


System:

OS:                Linux
architecture:

64bit



processor:         x86_64
python:            3.6.9
version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020



&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='hudeven' date='2020-10-28T19:46:33Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='hudeven' date='2020-10-28T20:37:15Z'>
		&lt;denchmark-link:https://github.com/hudeven&gt;@hudeven&lt;/denchmark-link&gt;
 as a workaround can you override  in your LightningModule?
		</comment>
		<comment id='3' author='hudeven' date='2020-10-29T02:36:18Z'>
		
@hudeven as a workaround can you override to_torchscript in your LightningModule?

yeah, it works by overriding to_torchscript() and deleting the metric attributes there
		</comment>
		<comment id='4' author='hudeven' date='2020-10-29T05:05:02Z'>
		A controversial suggestion but could we use another method name instead of forward? I understand using the class name directly is convenient.
Also curious about why the metrics class is an nn.Module? Is it to avoid the pains of syncing across distributed envs instead of using torch.distributed?
		</comment>
		<comment id='5' author='hudeven' date='2020-10-29T05:40:31Z'>
		Having Metrics somehow unique would also be helpful when saving / loading .ckpt. While this is intended behavior (it's a nn.Module), I also run into trouble: &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/4361&gt;#4361&lt;/denchmark-link&gt;

For inference Metrics are not necessary, but if you use them, you need to set strict=False when loading a .ckpt. This creates a risk that actual problems will be ignored (the ones you want loading to fail on).
		</comment>
		<comment id='6' author='hudeven' date='2020-10-29T05:51:22Z'>
		&lt;denchmark-link:https://github.com/hudeven&gt;@hudeven&lt;/denchmark-link&gt;
 I don't know if you need to use TorchScript in combination with , but with  your code does work.
Add this to your script:
def training_step(self, batch, batch_idx):
    # use first batch to create an example input
    if self.example_input_array is None:
        # we only need 1 samples, not a whole batch, but keep the batch dimension
        self.example_input_array = batch[0, :].unsqueeze(dim=0)
And change model.to_torchscript(method='trace')  # method='script'.
This does not solve the actual issue at hand, but can be a workaround for some here.
		</comment>
		<comment id='7' author='hudeven' date='2020-10-29T18:35:25Z'>
		
Also curious about why the metrics class is an nn.Module? Is it to avoid the pains of syncing across distributed envs instead of using torch.distributed?

&lt;denchmark-link:https://github.com/snisarg&gt;@snisarg&lt;/denchmark-link&gt;
, we are using an  for the metrics so that the state of the metric can be passed  different devices, which is necessary if you want to use them in both Lightning or just plain PyTorch. We are still using  to sync the metric states across GPUs.
		</comment>
		<comment id='8' author='hudeven' date='2020-11-02T18:56:00Z'>
		&lt;denchmark-link:https://github.com/NumesSanguis&gt;@NumesSanguis&lt;/denchmark-link&gt;
 thanks for the workaround! I intend to use 'script'. This issue is fixed in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4428&gt;#4428&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>