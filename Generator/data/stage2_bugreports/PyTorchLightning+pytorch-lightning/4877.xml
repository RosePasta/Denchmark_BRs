<bug id='4877' author='yongqianxiao' open_date='2020-11-27T08:54:18Z' closed_time='2020-11-28T12:02:21Z'>
	<summary>RuntimeError: All input tensors must be on the same device. Received cuda:2 and cuda:0</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

RuntimeError: All input tensors must be on the same device. Received cuda:2 and cuda:0
When I try to train with 4 GPUs (it happens if the number of GPUS &gt; 1), there is an error raised. When the validation epoch completed and before train continuously, this error happened.
Besides, this error wouldn't happen when I only use 1 GPU to train. I think this is a bug of pl?
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version : 1.7.0
PyTorch-Lightning version: 1.0.7
OS (e.g., Linux): Ubuntu 18.04
How you installed PyTorch (conda, pip, source): pip
Python version: py3.6.2
CUDA/cuDNN version: installed cudatoolkit with version: 10.0.130
GPU models and configuration: 2080Ti 10GB
Any other relevant information:

File "DCK3.py", line 584, in 
trainer.fit(dck, datamodule=dm)
File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 446, in fit
results = self.accelerator_backend.train()
File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/accelerators/dp_accelerator.py", line 106, in train
results = self.train_or_test()
File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 66, in train_or_test
results = self.trainer.train()
File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 495, in train
self.train_loop.run_training_epoch()
File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 589, in run_training_epoch
self.trainer.run_evaluation(test_mode=False)
File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 611, in run_evaluation
eval_loop_results = self.evaluation_loop.log_epoch_metrics(deprecated_eval_results, epoch_logs, test_mode)
File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 214, in log_epoch_metrics
test_mode
File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/logger_connector.py", line 127, in on_evaluation_epoch_end
self._log_on_evaluation_epoch_end_metrics(epoch_logs)
File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/logger_connector.py", line 195, in _log_on_evaluation_epoch_end_metrics
reduced_epoch_metrics = dl_metrics[0].class.reduce_on_epoch_end(dl_metrics)
File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/core/step_result.py", line 469, in reduce_on_epoch_end
recursive_stack(result)
File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/core/step_result.py", line 608, in recursive_stack
result[k] = collate_tensors(v)
File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/core/step_result.py", line 630, in collate_tensors
return torch.stack(items)
RuntimeError: All input tensors must be on the same device. Received cuda:2 and cuda:3
	</description>
	<comments>
		<comment id='1' author='yongqianxiao' date='2020-11-27T08:55:01Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='yongqianxiao' date='2020-11-27T12:19:09Z'>
		It's a bug of dp training.
SolutionÔºö
1„ÄÅchange to ddp.
2„ÄÅsee question &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4138&gt;#4138&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='yongqianxiao' date='2020-11-27T16:17:16Z'>
		
It's a bug of dp training.
SolutionÔºö
1„ÄÅchange to ddp.
2„ÄÅsee question #4138

Thanks for the help. The second solution works for me. ddp mode is not OK because I train at one computer.
		</comment>
		<comment id='4' author='yongqianxiao' date='2020-11-27T16:21:22Z'>
		

It's a bug of dp training.
SolutionÔºö
1„ÄÅchange to ddp.
2„ÄÅsee question #4138

Thanks for the help. The second solution works for me. ddp mode is not OK because I train at one computer.

DDP can be used in one computer with n gpus. In my experiment, ddp is more effective.
		</comment>
		<comment id='5' author='yongqianxiao' date='2020-11-27T16:33:00Z'>
		


It's a bug of dp training.
SolutionÔºö
1„ÄÅchange to ddp.
2„ÄÅsee question #4138

Thanks for the help. The second solution works for me. ddp mode is not OK because I train at one computer.

DDP can be used in one computer with n gpus. In my experiment, ddp is more effective.

DDP is not worked for me. Errors raised when I changed it to ddp:
Traceback (most recent call last):
  File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/multiprocessing/queues.py", line 241, in _feed
    obj = _ForkingPickler.dumps(obj)
  File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
_pickle.PicklingError: Can't pickle &lt;class 'MemoryError'&gt;: it's not the same object as builtins.MemoryError
Traceback (most recent call last):
  File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 872, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/multiprocessing/queues.py", line 104, in get
    if timeout &lt; 0 or not self._poll(timeout):
  File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/multiprocessing/connection.py", line 414, in _poll
    r = wait([self], timeout)
  File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/multiprocessing/connection.py", line 911, in wait
    ready = selector.select(timeout)
  File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/selectors.py", line 376, in select
    fd_event_list = self._poll.poll(timeout)
  File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 51307) is killed by signal: Killed. 

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/XYQ/DCK_torch/src_torch/DCK3.py", line 584, in &lt;module&gt;
    trainer.fit(dck, datamodule=dm)
  File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 446, in fit
    results = self.accelerator_backend.train()
  File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 148, in train
    results = self.ddp_train(process_idx=self.task_idx, model=model)
  File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 282, in ddp_train
    results = self.train_or_test()
  File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 66, in train_or_test
    results = self.trainer.train()
  File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 495, in train
    self.train_loop.run_training_epoch()
  File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 554, in run_training_epoch
    for batch_idx, (batch, is_last_batch) in train_dataloader:
  File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/profiler/profilers.py", line 82, in profile_iterable
    value = next(iterator)
  File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/data_connector.py", line 46, in _with_is_last
    last = next(it)
  File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 435, in __next__
    data = self._next_data()
  File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1068, in _next_data
    idx, data = self._get_data()
  File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1034, in _get_data
    success, data = self._try_get_data()
  File "/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 885, in _try_get_data
    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e
RuntimeError: DataLoader worker (pid(s) 51307) exited unexpectedly
		</comment>
		<comment id='6' author='yongqianxiao' date='2020-11-28T14:30:27Z'>
		Sorry, i haven't meet such question.If you use the default sampler when using ddp, does this question appear?
If you define you own sampler, you need inherit your sampler class from distirbutedsampler, and set replace_sampler_ddp=False in traier.
		</comment>
		<comment id='7' author='yongqianxiao' date='2020-11-28T15:07:17Z'>
		
Sorry, i haven't meet such question.If you use the default sampler when using ddp, does this question appear?
If you define you own sampler, you need inherit your sampler class from distirbutedsampler, and set replace_sampler_ddp=False in traier.

I customize the dataset but use the default sampler (I didn't give the sampler when I define the Dalaloader).
		</comment>
		<comment id='8' author='yongqianxiao' date='2020-11-28T15:29:09Z'>
		If you don't do some special to sampler,  the pytorch-ligtning will auto add DistributedSampler to your dataloader.
For your question "RuntimeError: DataLoader worker (pid(s) 51307) exited unexpectedly", csdn have some solutions. And i think it is not the bug of pytorch-lightning.
I think a simple solution is to set num_works=0 or 1. I recommend that you can refer to CSDN or stackoverflow, which have many solutions.
		</comment>
		<comment id='9' author='yongqianxiao' date='2020-11-28T15:45:53Z'>
		
If you don't do some special to sampler, the pytorch-ligtning will auto add DistributedSampler to your dataloader.
For your question "RuntimeError: DataLoader worker (pid(s) 51307) exited unexpectedly", csdn have some solutions. And i think it is not the bug of pytorch-lightning.
I think a simple solution is to set num_works=0 or 1. I recommend that you can refer to CSDN or stackoverflow, which have many solutions.

Thanks for continuous replying. I had the setup of num_works=8 and I will try num_works=1 for later.
		</comment>
	</comments>
</bug>