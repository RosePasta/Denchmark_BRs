<bug id='3283' author='tfrizza' open_date='2020-08-31T07:49:55Z' closed_time='2020-09-02T12:48:15Z'>
	<summary>TrainResult erroring with deepcopy on epoch end</summary>
	<description>
When detaching the Result object from the graph the code first calls deepcopy.copy() which throws an error
*** RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.
as the Result object still contains the graphs. It seems like it needs to perform some sort of recursive detach (like the dict implementation in the lines below), since calling detach() on its own will modify training_step_output_for_epoch_end and training_step_output in place.



pytorch-lightning/pytorch_lightning/trainer/training_loop.py


         Line 1038
      in
      f3c63f7






 training_step_output_for_epoch_end = copy(training_step_output) 





N.B. This was run on TPU
	</description>
	<comments>
		<comment id='1' author='tfrizza' date='2020-08-31T07:50:35Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='tfrizza' date='2020-09-02T08:43:36Z'>
		I'm facing the same issue on TPUs. I'm doing result = pl.TrainResult(minimize=loss), and I get that error for the loss. I guess calling .item() or .detach() would solve the logging issue, but of course that wouldn't quite work in general... :/
So I guess Result is just not usable with TPUs ATM and I need to resort to (more) manual logging?
		</comment>
		<comment id='3' author='tfrizza' date='2020-09-02T09:31:49Z'>
		&lt;denchmark-link:https://github.com/harpone&gt;@harpone&lt;/denchmark-link&gt;
 The workaround for me was to use the dictionary approach for logging metrics, i.e. returning . Still not super happy with this since I was hoping to use the Result object to aggregate metrics across TPU cores.
		</comment>
		<comment id='4' author='tfrizza' date='2020-09-02T11:45:23Z'>
		yeah I'm actually now using the trainer's logger, so basically self.logger.log_metrics(loss_dct, step=self.global_step) if batch_idx % 50 == 0 else None, which is actually fine...
I guess using  in validation should work now that &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3252&gt;#3252&lt;/denchmark-link&gt;
 is fixed... need to try that out
		</comment>
		<comment id='5' author='tfrizza' date='2020-09-02T11:47:43Z'>
		&lt;denchmark-link:https://github.com/lezwon&gt;@lezwon&lt;/denchmark-link&gt;
 mind have a look?
		</comment>
		<comment id='6' author='tfrizza' date='2020-09-02T12:16:42Z'>
		This issue should be fixed in master. &lt;denchmark-link:https://github.com/tfrizza&gt;@tfrizza&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/harpone&gt;@harpone&lt;/denchmark-link&gt;
 mind retrying with master?
		</comment>
		<comment id='7' author='tfrizza' date='2020-09-02T12:48:15Z'>
		feel free to re-open if needed üê∞
		</comment>
		<comment id='8' author='tfrizza' date='2020-09-02T13:21:38Z'>
		OK works now on master, both for train and eval (on TPU v3-8). But now I'm getting a total hangup when using gradient clipping... need to check into it further.
		</comment>
		<comment id='9' author='tfrizza' date='2020-09-07T00:08:42Z'>
		Yep seems to be working now, thanks!
		</comment>
	</comments>
</bug>