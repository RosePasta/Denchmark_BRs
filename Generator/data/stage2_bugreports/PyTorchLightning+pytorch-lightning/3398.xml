<bug id='3398' author='EspenHa' open_date='2020-09-08T14:27:20Z' closed_time='2020-09-22T20:35:30Z'>
	<summary>Custom loggers with .save method do not get final epoch metrics in .log_metrics</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Run code sample
Observe that the output for the logger with the save method is missing the print for the final epoch

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

import numpy as np
import pytorch_lightning as pl
import torch
from pytorch_lightning.loggers import LightningLoggerBase
from torch.utils.data import DataLoader, TensorDataset


class MyLogger(LightningLoggerBase):
    def log_metrics(self, metrics, step=None):
        print(f"{self.name} - {step}: {metrics}")

    def log_hyperparams(self, params):
        pass

    @property
    def name(self):
        return "my_logger"

    @property
    def version(self):
        return 1

    @property
    def experiment(self):
        return None


class MyLogger_with_save(LightningLoggerBase):
    def log_metrics(self, metrics, step=None):
        print(f"{self.name} - {step}: {metrics}")

    def log_hyperparams(self, params):
        pass

    @property
    def name(self):
        return "my_logger_with_save"

    @property
    def version(self):
        return 1

    @property
    def experiment(self):
        return None

    def save(self):
        pass


class MyLightningModule(pl.LightningModule):
    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.01)

    def forward(self, x):
        return self.model.forward(x)

    def __init__(self):
        super().__init__()
        self.model = torch.nn.Linear(1, 1)
        self.loss_fn = torch.nn.MSELoss()

    def train_dataloader(self):
        x = torch.from_numpy(np.random.RandomState(0).randn(6, 1).astype(np.float32))
        y = torch.from_numpy(np.random.RandomState(1).randn(6, 1).astype(np.float32))
        return DataLoader(TensorDataset(x, y), batch_size=2)

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self.forward(x)
        loss = self.loss_fn(y, y_hat)
        result = pl.TrainResult(minimize=loss)
        result.log("train_loss", loss, on_epoch=True)
        return result


torch.manual_seed(0)
torch.cuda.manual_seed_all(0)

kwargs = dict(max_epochs=3, progress_bar_refresh_rate=0, weights_summary=None)

trainer_1 = pl.Trainer(logger=MyLogger(), **kwargs)
trainer_1.fit(MyLightningModule())

trainer_2 = pl.Trainer(logger=MyLogger_with_save(), **kwargs)
trainer_2.fit(MyLightningModule())
&lt;denchmark-h:h4&gt;Output&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;GPU available: False, used: False
TPU available: False, using: 0 TPU cores
/home/espen/envs/research/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
my_logger - 3: {'epoch_train_loss': 2.462054967880249, 'epoch': 0}
my_logger - 6: {'epoch_train_loss': 2.417429208755493, 'epoch': 1}
my_logger - 9: {'epoch_train_loss': 2.378046751022339, 'epoch': 2}
Saving latest checkpoint..
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
my_logger_with_save - 3: {'epoch_train_loss': 3.9954922199249268, 'epoch': 0}
my_logger_with_save - 6: {'epoch_train_loss': 3.8402137756347656, 'epoch': 1}
Saving latest checkpoint..
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Expected my_logger_with_save to output something for the third epoch, like my_logger does.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


PL version: master / 0.9.1.rc1 / 701ca8b5496be6ee97529c190c181ef4b1bc6724
PyTorch Version: 1.6.0
How you installed PyTorch: pip
Build command you used (if compiling from source): N/A
Python version: 3.8.5
CUDA/cuDNN version: N/A
GPU models and configuration: N/A
Any other relevant information: N/A

	</description>
	<comments>
		<comment id='1' author='EspenHa' date='2020-09-08T14:35:44Z'>
		Possibly related issue: &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2841&gt;#2841&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='EspenHa' date='2020-09-11T10:18:29Z'>
		Copy of my answer from &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2841&gt;#2841&lt;/denchmark-link&gt;
:
I have found the solution after digging around in the logger code.
Short answer: call super().save() when you define your own save (or don't override it).
Long answer:
First note that logger.log_metrics is not called directly. Instead it is the logger.agg_and_log_metrics (you should not implement this normally, but you can go around the hole aggregation ect by defining this yourself) method that is directly called by lightning which then calls log_metrics after doing some aggregation of metrics. The aggregation is necessary if a metric is tried logged multiple times with the same step. The save method of the base class calls the internal function self._finalize_agg_metrics() that finalizes the aggregating of metrics. Thus, for this hole thing to work, the easy solution is to simply call super().save() in your own implementation of save or directly call the self._finalize_agg_metrics() method in your save.
This is not at all clear from the documentation and we should fix it (easy fix, change  to  in the documentation and make a note that it is important to call).
&lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/loggers.html#make-a-custom-logger&gt;https://pytorch-lightning.readthedocs.io/en/latest/loggers.html#make-a-custom-logger&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='EspenHa' date='2020-09-11T11:26:36Z'>
		While stating this in the documentation is good, I still think it is desirable to avoid the dependency on save being called.
From an outside perspective, it seems counter-intuitive that the presence of a conceptually unrelated method should have an effect on logging.
In addition, this bug could affect many users without them even knowing about it, as it is easy to miss that one logging statement is missing if there is a lot of logging going on.
		</comment>
		<comment id='4' author='EspenHa' date='2020-09-13T15:25:25Z'>
		I agree that it would be good if this dependency did not exist, however as I stated earlier it has to do with the internal aggregation we are doing. It would therefore require major refactor of most loggers to not have this dependency. That is why I think it is a fair comprise to just mention this directly in the documentation (I assume that people read this before implementing their own logger.
		</comment>
	</comments>
</bug>