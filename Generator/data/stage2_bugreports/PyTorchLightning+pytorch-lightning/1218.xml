<bug id='1218' author='Ir1d' open_date='2020-03-23T23:44:27Z' closed_time='2020-05-02T15:36:34Z'>
	<summary>ValueError: only one element tensors can be converted to Python scalars</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

This happens in the training loop.
&lt;denchmark-code&gt;ValueError: only one element tensors can be converted to Python scalars
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

From my observation, I believe this happens when the batch size can't be divided by gpu num. For example on the last batch of each epoch, and when you have 4 gpus but set batch size to 2.
&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

I think it would be nice to use only some of the gpus the user specified, while printing out a msg telling them that the gpus are not specified correctly. Current implementaion simply throws a not friendly error
	</description>
	<comments>
		<comment id='1' author='Ir1d' date='2020-03-24T00:58:13Z'>
		Do I understand correctly, this happens with every Lightning training where batch_size &lt; num_gpus?
Then I would also like to see a warning message like you described and automatically set num_gpus to the batch size.
However, we still have the problem where batch_size &gt; num_gpus for all batches except the last, when we specify drop_last=False in dataloader. What do we do then?
Would the same error occur in torch.nn.DataParallel (without PL)?
		</comment>
		<comment id='2' author='Ir1d' date='2020-03-24T01:11:23Z'>
		the last epoch seems a bit tricky.. I'm not expert on this, but I wonder if its possible to send the data to part of the specified gpus (for example 4 gpus in all, but send 3 batches to first 3 gpu). I remember there's some send_batch_to_gpu function in pl
		</comment>
		<comment id='3' author='Ir1d' date='2020-03-25T14:54:16Z'>
		&lt;denchmark-link:https://github.com/neggert&gt;@neggert&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jeffling&gt;@jeffling&lt;/denchmark-link&gt;
 pls ^^
		</comment>
		<comment id='4' author='Ir1d' date='2020-03-26T04:11:53Z'>
		&lt;denchmark-link:https://github.com/Ir1d&gt;@Ir1d&lt;/denchmark-link&gt;
 btw this person &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1236&gt;#1236&lt;/denchmark-link&gt;
 is getting the same error but doesn't have GPU.
		</comment>
		<comment id='5' author='Ir1d' date='2020-04-16T23:05:33Z'>
		&lt;denchmark-link:https://github.com/Ir1d&gt;@Ir1d&lt;/denchmark-link&gt;
 mind send a PR or provide an example to replicate it?
		</comment>
		<comment id='6' author='Ir1d' date='2020-04-16T23:32:48Z'>
		I can provide an example next week. Currently not sure how to fix this
		</comment>
		<comment id='7' author='Ir1d' date='2020-04-25T04:29:49Z'>
		Hi &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
, I've reproduced the same issue here:
&lt;denchmark-link:https://github.com/Richarizardd/pl_image_classification&gt;https://github.com/Richarizardd/pl_image_classification&lt;/denchmark-link&gt;

Basic image classification with PL using MNIST, CIFAR10, and ImageFolder Datasets from torchvision. If you run the mnist_gpu1.yaml config file, you would get the same issue as &lt;denchmark-link:https://github.com/Ir1d&gt;@Ir1d&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='Ir1d' date='2020-04-26T06:20:25Z'>
		Hi &lt;denchmark-link:https://github.com/Richarizardd&gt;@Richarizardd&lt;/denchmark-link&gt;

I looked at your code and I found that in your validation epoch end, you don't reduce the outputs properly. PL does not do this for you. This is intentional, right &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 ?
So, in your , instead of
for output in outputs:
    metric_total += output[metric_name]
tqdm_dict[metric_name] = metric_total / len(outputs)
you should do the following:
tqdm_dict[metric_name] = torch.stack([output[metric_name] for output in outputs]).mean()
I tested this by adding it to your code and it worked (no error).
As far as I can tell, this is not a bug in PL. However, we could print a better error message.
&lt;denchmark-link:https://github.com/Ir1d&gt;@Ir1d&lt;/denchmark-link&gt;
 you probably had the same mistake.
		</comment>
		<comment id='9' author='Ir1d' date='2020-04-26T06:26:24Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/10709657/80299792-bef31780-87c9-11ea-8f2e-a272514f361b.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='Ir1d' date='2020-04-26T06:31:44Z'>
		In &lt;denchmark-link:https://github.com/Richarizardd&gt;@Richarizardd&lt;/denchmark-link&gt;
's case, the error was thrown in . Could you post your stack trace here so I can check if it's the same?
		</comment>
		<comment id='11' author='Ir1d' date='2020-04-26T06:32:14Z'>
		Your metric reduction looks fine to me.
		</comment>
		<comment id='12' author='Ir1d' date='2020-04-26T06:44:48Z'>
		I tried again and there's still this issue in pl v0.7.3
Here's the whole log for a recent run. I set bs=3 on 4 gpus and set pl use all the 4 gpus, and this is happening for the first batch. (in another case when drop_last is not set and bs=12 on 4 gpus, this is happening for the last batch of the epoch, which seems that it happens when bs &lt; num gpus)
&lt;denchmark-link:https://user-images.githubusercontent.com/10709657/80300053-e4812080-87cb-11ea-9836-9ea527e865ef.png&gt;&lt;/denchmark-link&gt;

I'll try bring a minimal code for reproduction after I finish my midterm tests. Currently my code is available at &lt;denchmark-link:https://github.com/ir1d/AFN&gt;https://github.com/ir1d/AFN&lt;/denchmark-link&gt;
 , but the data is a bit large and might be hard to run.
		</comment>
		<comment id='13' author='Ir1d' date='2020-04-26T07:14:34Z'>
		&lt;denchmark-link:https://github.com/Ir1d&gt;@Ir1d&lt;/denchmark-link&gt;
 found the bug in trainer code. It does not reduce the outputs if output_size of training step does not equal num_gpus. I will make a PR to fix it.
		</comment>
		<comment id='14' author='Ir1d' date='2020-05-02T15:36:33Z'>
		&lt;denchmark-link:https://github.com/Ir1d&gt;@Ir1d&lt;/denchmark-link&gt;
 The fix got merged. Kindly asking you to verify the fix with latest master branch. Closing for now.
		</comment>
	</comments>
</bug>