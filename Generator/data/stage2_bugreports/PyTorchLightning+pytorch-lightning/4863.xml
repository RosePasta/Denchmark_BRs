<bug id='4863' author='carmocca' open_date='2020-11-26T02:43:22Z' closed_time='2020-12-01T09:26:53Z'>
	<summary>trainer.progress_bar_dict values are delayed by one epoch</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

trainer.progress_bar_dict does not contain what was self.log-ed in that epoch's training step, but the previous value.
This also means that there is no value for epoch 0.
&lt;denchmark-h:h3&gt;To Reproduce (and to test)&lt;/denchmark-h&gt;

def test_progress_bar_dict_contains_values_on_train_epoch_end(tmpdir):
    class TestModel(BoringModel):
        def training_step(self, *args):
            self.log("foo", torch.tensor(self.current_epoch), on_step=False, on_epoch=True, prog_bar=True)
            return super().training_step(*args)

        def on_train_epoch_end(self, *_):
            self.epoch_end_called = True
            assert self.trainer.progress_bar_dict["foo"] == self.current_epoch

    trainer = Trainer(
        default_root_dir=tmpdir,
        max_epochs=2,
        limit_train_batches=1,
        limit_val_batches=0,
        checkpoint_callback=False,
        logger=False,
        weights_summary=None,
        progress_bar_refresh_rate=0,
    )
    model = TestModel()
    trainer.fit(model)
    assert model.epoch_end_called
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

trainer.progress_bar_dict contains the values logged in training_step
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

The test fails on master
The commit that introduced this bug is &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/9c8701f2e2c09a984ac0322942c8f742a7cc6bc9&gt;9c8701f&lt;/denchmark-link&gt;
. The previous one works as expected.
I haven't dived into the changes much since it is a large commit. Maybe the author can help &lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='carmocca' date='2020-11-27T02:10:33Z'>
		Okay, the cause of the issue is that trainer.call_hook('on_train_epoch_end', epoch_output) is run before trainer.logger_connector.on_train_epoch_end(). And the progress_bar_dict is updated in the second one.
However, changing the order here


fixes the test above but fails &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/tests/trainer/logging/test_logger_connector.py#L52-L215&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/master/tests/trainer/logging/test_logger_connector.py#L52-L215&lt;/denchmark-link&gt;
 with a 
I will keep debugging...
		</comment>
		<comment id='2' author='carmocca' date='2020-11-27T13:49:08Z'>
		I think the problem is in the reset in call_hook of the model._results as we call after that also self._cache_logged_metrics() which is on empty Result.
Also in the def training_step(...) in training_loop.py we have call_hook which reset the Result and after that collecting the results from model._results which is empty
		</comment>
		<comment id='3' author='carmocca' date='2020-11-28T13:06:41Z'>
		Hey &lt;denchmark-link:https://github.com/carmocca&gt;@carmocca&lt;/denchmark-link&gt;
,
Thanks for reporting this bug. Yes, I needs to be permuted. I will resolve this next week.
Best regards,
T.C
		</comment>
	</comments>
</bug>