<bug id='2653' author='s-rog' open_date='2020-07-21T01:30:04Z' closed_time='2020-09-02T13:36:45Z'>
	<summary>Checkpoints cannot be loaded in non-pl env</summary>
	<description>
## üöÄ Feature
Add an option to save only state_dict for ModelCheckpoint callbacks
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

PL checkpoints cannot be loaded in non-pl envs
&lt;denchmark-h:h3&gt;Motivation&lt;/denchmark-h&gt;

To be able to move trained models and weights into pytorch only environments
&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

Currently  when you do torch.load() on a pl generated checkpoint in an environment without pl, there is a pickling error. For my current use case I have to load the checkpoints in my training environment and save them again with only state_dict for the weights.
See &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2653#issuecomment-681303007&gt;reply below&lt;/denchmark-link&gt;
 for more info
	</description>
	<comments>
		<comment id='1' author='s-rog' date='2020-07-21T18:04:55Z'>
		You can use save_weights_only parameter in ModelCheckpoint to save weights only. Although it will save epoch, global_step and pl_version but that won't be a problem there, I guess. Also can you show the pickling error you are getting?
		</comment>
		<comment id='2' author='s-rog' date='2020-07-22T00:23:59Z'>
		I am using save_weights_only  and that causes a pickling error with module lightning not found (don't have the extact error atm)
		</comment>
		<comment id='3' author='s-rog' date='2020-07-22T04:46:34Z'>
		Can you check when you load that checkpoint manually in pl env, what keys does that file have?
		</comment>
		<comment id='4' author='s-rog' date='2020-08-27T02:20:20Z'>
		Error in non-pl env
&lt;denchmark-code&gt;ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-10-dbc5018f5317&gt; in &lt;module&gt;
----&gt; 1 pretrained_dict = torch.load('../input/weights/test.ckpt', map_location=torch.device('cpu'))

/opt/conda/lib/python3.7/site-packages/torch/serialization.py in load(f, map_location, pickle_module, **pickle_load_args)
    591                     return torch.jit.load(f)
    592                 return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
--&gt; 593         return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
    594 
    595 

/opt/conda/lib/python3.7/site-packages/torch/serialization.py in _legacy_load(f, map_location, pickle_module, **pickle_load_args)
    771     unpickler = pickle_module.Unpickler(f, **pickle_load_args)
    772     unpickler.persistent_load = persistent_load
--&gt; 773     result = unpickler.load()
    774 
    775     deserialized_storage_keys = pickle_module.load(f, **pickle_load_args)

ModuleNotFoundError: No module named 'pytorch_lightning'
&lt;/denchmark-code&gt;

keys in pl env
dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'hparams_name', 'hyper_parameters'])
&lt;denchmark-link:https://github.com/rohitgr7&gt;@rohitgr7&lt;/denchmark-link&gt;
 sorry about the late reply, completely forgot about this issue
Edit:
found the issue, I'll look into a fix
&lt;denchmark-code&gt;for k, v in pretrained_dict.items():
    print(type(k), type(v))

&lt;class 'str'&gt; &lt;class 'int'&gt;
&lt;class 'str'&gt; &lt;class 'int'&gt;
&lt;class 'str'&gt; &lt;class 'str'&gt;
&lt;class 'str'&gt; &lt;class 'collections.OrderedDict'&gt;
&lt;class 'str'&gt; &lt;class 'str'&gt;
&lt;class 'str'&gt; pytorch_lightning.utilities.parsing.AttributeDict
&lt;/denchmark-code&gt;

Edit 2:
I'll submit a PR after refactor week!
		</comment>
		<comment id='5' author='s-rog' date='2020-08-31T03:06:08Z'>
		I got around to testing and can load checkpoints now in non-pl envs. The only change needed was to cast hyper_parameters to dict in dump_checkpoint of  pytorch_lightning/trainer/training_io.py
- checkpoint[LightningModule.CHECKPOINT_HYPER_PARAMS_KEY] = model.hparams
+ checkpoint[LightningModule.CHECKPOINT_HYPER_PARAMS_KEY] = dict(model.hparams)
Thoughts?
		</comment>
		<comment id='6' author='s-rog' date='2020-08-31T10:38:08Z'>
		Yeah this looks good to avoid such error since AttributeDict is a PL thing.
		</comment>
		<comment id='7' author='s-rog' date='2020-08-31T18:04:46Z'>
		&lt;denchmark-link:https://github.com/s-rog&gt;@s-rog&lt;/denchmark-link&gt;
 , I tried on master with  and these are the dict keys I got. No hyperparams.
dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict'])
		</comment>
		<comment id='8' author='s-rog' date='2020-09-01T00:37:25Z'>
		&lt;denchmark-link:https://github.com/rohitgr7&gt;@rohitgr7&lt;/denchmark-link&gt;
 Did the model have ?
If you look at dump_checkpoint() the weights_only arg only controls:
callbacks, optimizer_states, lr_schedulers, native_amp_scaling_state and amp_scaling_state
hparams loggging is only controlled by if model.hparams:
		</comment>
		<comment id='9' author='s-rog' date='2020-09-01T16:56:16Z'>
		ok, yeah my bad :)
		</comment>
	</comments>
</bug>