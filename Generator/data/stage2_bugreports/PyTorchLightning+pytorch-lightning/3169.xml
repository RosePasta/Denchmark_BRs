<bug id='3169' author='VirajBagal' open_date='2020-08-25T17:49:27Z' closed_time='2020-08-26T04:48:31Z'>
	<summary>RuntimeError: Cannot replicate if number of devices (1) is different from 8 Exception in device=TPU:2: Cannot replicate if number of devices (1) is different from 8</summary>
	<description>
I am getting the above (title) error on Kaggle when I am doing
trainer.fit(model, dm)
where trainer = Trainer(tpu_cores = 8, logger = wandblogger, max_epochs = 20)
Following is the trace:

File "/opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py", line 330, in _mp_start_fn
_start_fn(index, pf_cfg, fn, args)
File "/opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py", line 323, in _start_fn
_setup_replication()
Traceback (most recent call last):
File "/opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py", line 330, in _mp_start_fn
_start_fn(index, pf_cfg, fn, args)
Exception in device=TPU:2: Cannot replicate if number of devices (1) is different from 8
File "/opt/conda/lib/python3.7/site-packages/torch_xla/core/xla_model.py", line 287, in xla_replication_devices
format(len(local_devices), len(kind_devices)))
File "/opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py", line 316, in _setup_replication
xm.set_replication(device, [device])
File "/opt/conda/lib/python3.7/site-packages/torch_xla/core/xla_model.py", line 315, in set_replication
replication_devices = xla_replication_devices(devices)
Traceback (most recent call last):
File "/opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py", line 323, in _start_fn
_setup_replication()

I am using Kaggle Kernel. Here is my LightningDataModule.
class ProteinModule(pl.LightningDataModule):
  def __init__(self, config, path, invalid_proteins):
    super().__init__()

    self.path = path
    self.bs = config['bs']
    self.state = config['seed']
    self.invalid = invalid_proteins

  def setup(self, stage=None):

    all_protein_ids = os.listdir(self.path)
    all_protein_ids = [protein for protein in all_protein_ids if protein not in self.invalid]
    train_ids, val_ids = train_test_split(all_protein_ids, test_size = 0.2, random_state = self.state)

    self.traindataset = ProteinDataset(path, train_ids)
    self.valdataset = ProteinDataset(path, val_ids)


  def train_dataloader(self):

    return DataLoader(self.traindataset, batch_size = self.bs, shuffle = True)

  def val_dataloader(self):

    return DataLoader(self.valdataset, batch_size = self.bs, shuffle = False)
	</description>
	<comments>
		<comment id='1' author='VirajBagal' date='2020-08-25T18:24:46Z'>
		Did you call xm.xla_device() somewhere? Or maybe try restarting the kernel and run again.
		</comment>
		<comment id='2' author='VirajBagal' date='2020-08-26T04:47:39Z'>
		Thanks, &lt;denchmark-link:https://github.com/rohitgr7&gt;@rohitgr7&lt;/denchmark-link&gt;
, I was using  in one place. This error is resolved but I have got another error now. I'll open another issue for it.
		</comment>
	</comments>
</bug>