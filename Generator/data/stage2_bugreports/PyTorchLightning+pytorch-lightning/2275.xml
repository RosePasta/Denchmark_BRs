<bug id='2275' author='mstewart141' open_date='2020-06-19T17:18:13Z' closed_time='2020-07-24T20:21:06Z'>
	<summary>`Add ckpt_path option to LightningModule.test()` introduced robustness issues</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Trainer.test(), passing no explicit model, now looks for the best model checkpoint. However, this can fail for models that have __init__ methods that take more than just hparams, which is common.
If I have a pl model with init (kw)args (self, hparams: Namespace, fancy_thing: FancyThing) one runs into problems outlined below.
With the new codepath, Trainer.test() calls down via this path
&lt;denchmark-code&gt;        # if model is not given (None), ckpt_path is given,
        # load the given checkpoint for testing
        if model is None and ckpt_path is not None:
            # ckpt_path is 'best' so load the best model
            if ckpt_path == 'best':
                ckpt_path = self.checkpoint_callback.best_model_path
            model = self.get_model().load_from_checkpoint(ckpt_path)
&lt;/denchmark-code&gt;

Which soon calls down to pytorch_lightning/core/saving.py @ _load_model_state.
This correctly collects init_args_name (here hparams, fancy_thing) but the subsequent logic does not check whether all the requisite params have been found
&lt;denchmark-code&gt;            if args_name == 'kwargs':
                cls_kwargs = {k: v for k, v in model_args.items() if k in init_args_name}
                kwargs.update(**cls_kwargs)
            elif args_name:
                if args_name in init_args_name:
                    kwargs.update({args_name: model_args})
            else:
                args = (model_args, ) + args

        # load the state_dict on the model automatically
        model = cls(*args, **kwargs)
&lt;/denchmark-code&gt;

This means the final line of that snippet fails because fancy_thing is not passed to cls.
Even if it were checked, though, there is not a mechanism here to recover fancy_thing. Lightning might need to e.g. pickle the model init args to rehydrate them later, or provide a user hook specifying how the rehydration here should occur. That said, there may be a simple work around I do not see immediately.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Call Trainer.test() passing no model, where the implicit model has init args that are not just hparams.
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

I've tried to document very clearly above the code flow that triggers this, but please @ me for any more context.
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Rehydration should occur properly, or, as a stopgap, an error should be raised explaining that model must be explicitly be passed to Trainer.test in this case.
&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

Thanks for the great library; really appreciate the team's work!
	</description>
	<comments>
		<comment id='1' author='mstewart141' date='2020-06-19T20:02:20Z'>
		&lt;denchmark-link:https://github.com/yukw777&gt;@yukw777&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='mstewart141' date='2020-06-22T13:36:01Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 it seems like this is related to the hparams deprecation work you guys did? I thought we pickled the init arguments though... why wouldn't  get pickled from the example?
		</comment>
		<comment id='3' author='mstewart141' date='2020-06-24T03:53:04Z'>
		Ahh I think I know what‚Äôs going on.. would it be OK for us to accept extra args and kwargs that‚Äôd be passed into the model init function in test()?
		</comment>
		<comment id='4' author='mstewart141' date='2020-07-24T19:55:12Z'>
		We can close this per &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2423#issuecomment-663707172&gt;#2423 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='mstewart141' date='2020-07-25T08:50:58Z'>
		resolved in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2681&gt;#2681&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>