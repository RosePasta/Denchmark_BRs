<bug id='2438' author='Llannelongue' open_date='2020-07-01T00:46:26Z' closed_time='2020-07-01T11:38:01Z'>
	<summary>`validation_epoch_end` and `test_epoch_end` can't return nothing</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

If validation_epoch_end or test_epoch_end returns nothing (as presented as an option in the documentation), an error occurs.
(Happy to work on a PR to fix this)
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:
Overwrite test_epoch_end and remove return (same for validation_epoch_end
&lt;denchmark-code&gt;File "/.conda/envs/PPI-env/lib/python3.7/site-packages/pytorch_lightning/trainer/logging.py", line 106, in process_output
    for k, v in output.items():
AttributeError: 'NoneType' object has no attribute 'items'
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

import os

import torch
from torch.nn import functional as F
from torch.utils.data import DataLoader
from torchvision.datasets import MNIST
from torchvision import transforms
from pytorch_lightning.core.lightning import LightningModule
from pytorch_lightning import Trainer, seed_everything

class LitModel(LightningModule):

    def __init__(self):
        super().__init__()
        self.l1 = torch.nn.Linear(28 * 28, 10)

    def forward(self, x):
        return torch.relu(self.l1(x.view(x.size(0), -1)))

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        tensorboard_logs = {'train_loss': loss}
        return {'loss': loss, 'log': tensorboard_logs}

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.001)

    def train_dataloader(self):
        dataset = MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())
        loader = DataLoader(dataset, batch_size=32, num_workers=4, shuffle=True)
        return loader

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        return {'val_loss': F.cross_entropy(y_hat, y)}

    def validation_epoch_end(self, outputs):
        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
        tensorboard_logs = {'val_loss': avg_loss}
        # return {'val_loss': avg_loss, 'log': tensorboard_logs}

    def val_dataloader(self):
        # TODO: do a real train/val split
        dataset = MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor())
        loader = DataLoader(dataset, batch_size=32, num_workers=4)
        return loader


def main():
    seed_everything(42)

    model = LitModel()
    # most basic trainer, uses good defaults
    trainer = Trainer(fast_dev_run=True)
    trainer.fit(model)

if __name__ == '__main__':
    main()
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

It should check if there is nothing returned and carry on.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:

GPU:
available:         False
version:           None


Packages:

numpy:             1.17.4
pyTorch_debug:     False
pyTorch_version:   1.5.1
pytorch-lightning: 0.8.3
tensorboard:       2.2.2
tqdm:              4.41.1


System:

OS:                Darwin
architecture:

64bit



processor:         i386
python:            3.7.6
version:           Darwin Kernel Version 19.4.0: Wed Mar  4 22:28:40 PST 2020; root:xnu-6153.101.6~15/RELEASE_X86_64



&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='Llannelongue' date='2020-07-01T03:05:55Z'>
		yes, please submit a PR? you should be allowed to return nothing
		</comment>
		<comment id='2' author='Llannelongue' date='2020-07-01T07:15:11Z'>
		Yes, working on it üëç
		</comment>
	</comments>
</bug>