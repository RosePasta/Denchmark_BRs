<bug id='2804' author='SimonNarendorf' open_date='2020-08-03T06:50:04Z' closed_time='2020-11-12T15:30:22Z'>
	<summary>AttributeError on lr_find with half precision</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ lr_find crash with mixed precision&lt;/denchmark-h&gt;

initial run of lr_find raise AttributeError when run with 16Bit precison
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Init module and trainer with precision=16
Run trainer.lr_find(module)

AttributeError: 'NoneType' object has no attribute 'state_dict'
Stacktrace:
&lt;denchmark-code&gt;/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_io.py in dump_checkpoint(self, weights_only)
    360             # save native amp scaling
    361             if self.use_amp and NATIVE_AMP_AVALAIBLE and not self.use_tpu:
--&gt; 362                 checkpoint['native_amp_scaling_state'] = self.scaler.state_dict()
    363 
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;module = new LightningModule(...)
trainer = Trainer(max_epochs=10, precision=16)
trainer.lr_find(module, min_lr=1e-06, max_lr=1e-01, early_stop_threshold=None)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

You can get the script and run it with Torch 1.6.0+cu101, Torchvision 0.7.0+cu101, CUDA 10.1, Lightning 0.8.5
	</description>
	<comments>
		<comment id='1' author='SimonNarendorf' date='2020-08-03T06:51:02Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='SimonNarendorf' date='2020-08-03T08:43:52Z'>
		Could this be because we don't restore the scaler attribute after the lr_find has finished &lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='3' author='SimonNarendorf' date='2020-08-04T09:47:51Z'>
		This happens because the scaler is setup during the run_pretrain_routine but a checkpoint is tried to save beforehand in the learning rate finder, since we need to restore to this after running the search.
		</comment>
		<comment id='4' author='SimonNarendorf' date='2020-09-16T18:33:24Z'>
		&lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
 whats the fix here?
		</comment>
		<comment id='5' author='SimonNarendorf' date='2020-09-27T15:24:27Z'>
		why not just make lr_find, and scaling batch_size work in a way that it logs the optimized learning_rate / batch_size and add a log for the user to adjust the learning_rate / batch_size manually and reinitialize model / datamodule and call trainer.fit.
		</comment>
		<comment id='6' author='SimonNarendorf' date='2020-10-22T15:45:42Z'>
		&lt;denchmark-link:https://github.com/SimonNarendorf&gt;@SimonNarendorf&lt;/denchmark-link&gt;
 can you upgrade to the latest version to see if issue persists?
		</comment>
		<comment id='7' author='SimonNarendorf' date='2020-11-12T15:30:22Z'>
		Feel free to reopen if needed!
		</comment>
	</comments>
</bug>