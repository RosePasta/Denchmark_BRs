<bug id='4395' author='junwen-austin' open_date='2020-10-27T16:08:05Z' closed_time='2020-11-12T15:59:02Z'>
	<summary>loss from progress bar appears to be sum of loss across all GPUs in Lightning 1.0.3</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

The loss from progress bar during the training appears to be sum of loss across of all GPUs
&lt;denchmark-h:h2&gt;Please reproduce using the BoringModel and post here&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

One can pick any model and dataset and varies the number of GPUs for training. In my example, I have roughly 0.7 loss for 1 GPUs, 1.4 loss for 2 GPUs and 0.28 loss for 4 GPUs for the first few training batches, indicating loss is roughly the sum instead of mean of loss across all GPUs.
I used the standard from the doc:
&lt;denchmark-code&gt;def train_step(self, batch, batch_idx):
    loss = self.forward(batch)
   self.log('loss/train', loss, on_step=True, on_epoch=False, sync_dist=True, sync_dist_op='mean')

&lt;/denchmark-code&gt;

Note: the loss logged in tensorboard appears to be correct (mean of loss)
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

The loss from progress bar during the training should be mean of loss across of all GPUs
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

Note: Bugs with code are solved faster ! Colab Notebook should be made public !


IDE: Please, use our python bug_report_model.py template.


Colab Notebook: Please copy and paste the output from our environment collection script (or fill out the checklist below manually).


You can get the script and run it with:
&lt;denchmark-code&gt;wget https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py
# For security purposes, please check the contents of collect_env_details.py before running it.
python collect_env_details.py
&lt;/denchmark-code&gt;


PyTorch Version (e.g., 1.0):  1.4
OS (e.g., Linux):  Linux
How you installed PyTorch (conda, pip, source):  pip
Build command you used (if compiling from source):
Python version:  3.7
CUDA/cuDNN version:  10.1
GPU models and configuration:
Any other relevant information:   PyTorch Lightning 1.0.3

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='junwen-austin' date='2020-10-27T18:07:47Z'>
		Note: if I don't specify anything, i.e.:
&lt;denchmark-code&gt;self.log('loss/train', loss)
&lt;/denchmark-code&gt;

then the loss from the progress bar appears to be correct. I am using multiple GPUs and not sure what is the difference here.
		</comment>
		<comment id='2' author='junwen-austin' date='2020-10-29T00:29:20Z'>
		Does this behavior happen if you explicitly set prog_bar=True?
Edit:
Doing self.log('loss/train', loss) logs only the value from rank (gpu) 0
		</comment>
		<comment id='3' author='junwen-austin' date='2020-10-29T15:33:03Z'>
		&lt;denchmark-link:https://github.com/s-rog&gt;@s-rog&lt;/denchmark-link&gt;
 Thanks for your comment.
if I do
&lt;denchmark-code&gt;self.log('loss/train', loss, on_step=True, on_epoch=False, sync_dist=True, sync_dist_op='mean', progress_bar=True)
&lt;/denchmark-code&gt;

It looks like it is still doing sum of losses from all GPUs, instead of averaging them.
Also, it appears the documentation is not clear about "doing self.log('loss/train', loss) logs only the value from rank (gpu) 0":
Depending on where log is called from, Lightning auto-determines the correct logging mode for you. But of course you can override the default behavior by manually setting the log() parameters.
&lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/stable/logging.html&gt;https://pytorch-lightning.readthedocs.io/en/stable/logging.html&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='junwen-austin' date='2020-10-29T16:05:46Z'>
		&lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/stable/lightning_module.html#log&gt;https://pytorch-lightning.readthedocs.io/en/stable/lightning_module.html#log&lt;/denchmark-link&gt;

From here you can see by default sync_dist=False but yes the docs from the logging section is not clear and a PR is in order.
The progress bar sum behavior is definitely not intended as well and would require a fix, I'll be taking a closer look when I get a chance.
		</comment>
		<comment id='5' author='junwen-austin' date='2020-10-29T16:37:09Z'>
		&lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;
 I saw that you were working on &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4374&gt;#4374&lt;/denchmark-link&gt;
 perhaps you have an idea of where the source of this bug is?
Upon preliminary inspection the syncing and reduction happens right after calling log() which should mean loggers and progress bar recieve the same value...
		</comment>
		<comment id='6' author='junwen-austin' date='2020-10-30T06:17:38Z'>
		ok the issue is with the default loss logging, not the one you called with self.log
you can test it with the following which works correctly:
&lt;denchmark-code&gt;self.log('0_5', torch.tensor(0.5, device=self.device), on_step=True, on_epoch=False, prog_bar=True, sync_dist=True, sync_dist_op='mean')
# and also sync_dist_op=None for sum
&lt;/denchmark-code&gt;

The reduction behavior for the default loss is sum (sync_dist_op=None) and we need to change it to mean (sync_dist_op='mean')
		</comment>
		<comment id='7' author='junwen-austin' date='2020-11-02T07:54:56Z'>
		Hey &lt;denchmark-link:https://github.com/s-rog&gt;@s-rog&lt;/denchmark-link&gt;
,
Great catch ! Let me have a look at it today.
Best regards,
T.C
		</comment>
		<comment id='8' author='junwen-austin' date='2020-11-02T14:33:07Z'>
		Hey &lt;denchmark-link:https://github.com/junwen-austin&gt;@junwen-austin&lt;/denchmark-link&gt;
,
&lt;denchmark-code&gt;class ExtentedModel(BoringModel):
    
    logged = []
    
    def training_step(self, batch, batch_idx):
        output = self.layer(batch)
        loss = self.loss(batch, output)
        self.log("train_loss", torch.ones(1).to(self.device), on_step=True, on_epoch=True, sync_dist=True, sync_dist_op='mean', prog_bar=True)
        self.logged.append(1)
        return {"loss": loss}    
    
model = ExtentedModel()
model.training_step_end = None
model.training_epoch_end = None

debug = False
if debug:
    distributed_backend = None
    gpus = 0
else:
    distributed_backend = "ddp"
    gpus = 2    

trainer = Trainer(
    max_epochs=1,
    default_root_dir=os.getcwd(),
    limit_train_batches=10,
    limit_test_batches=2,
    limit_val_batches=2,
    distributed_backend=distributed_backend,
    gpus=gpus,
    precision=32,
    )
trainer.fit(model)

print(model.logged)
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;def sync_ddp_if_available(
    result: Union[torch.Tensor], group: Optional[Any] = None, reduce_op: Optional[Union[ReduceOp, str]] = None
) -&gt; torch.Tensor:
    """
    Function to reduce the tensors from several ddp processes to one master process

    Args:
        result: the value to sync and reduce (typically tensor or number)
        group: the process group to gather results from. Defaults to all processes (world)
        reduce_op: the reduction operation. Defaults to sum.
            Can also be a string of 'avg', 'mean' to calculate the mean during reduction.

    Return:
        reduced value
    """

    if torch.distributed.is_available() and torch.distributed.is_initialized():
        divide_by_world_size = False

        if group is None:
            group = torch.distributed.group.WORLD

        if reduce_op is None:
            reduce_op = torch.distributed.ReduceOp.SUM
        elif isinstance(reduce_op, str) and reduce_op in ("avg", "mean"):
            reduce_op = torch.distributed.ReduceOp.SUM
            divide_by_world_size = True

        # sync all processes before reduction
        torch.distributed.barrier(group=group)
        torch.distributed.all_reduce(result, op=reduce_op, group=group, async_op=False)

        if divide_by_world_size:
            print("MEAN")
            result = result / torch.distributed.get_world_size(group)

    return result
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;Epoch 0:   0%|                                                                                                                                                       | 0/12 [00:00&lt;?, ?it/s]
MEAN
MEAN
Epoch 0:   8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                            | 1/12 [00:00&lt;00:00, 114.52it/s, loss=1.177, v_num=11, train_loss_step=1]
MEAN
MEAN
Epoch 0:  17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                    | 2/12 [00:00&lt;00:00, 173.18it/s, loss=1.657, v_num=11, train_loss_step=1]
MEAN
MEAN
Epoch 0:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                           | 3/12 [00:00&lt;00:00, 211.25it/s, loss=1.805, v_num=11, train_loss_step=1]
MEAN
MEAN
Epoch 0:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                   | 4/12 [00:00&lt;00:00, 236.69it/s, loss=1.735, v_num=11, train_loss_step=1]
MEAN
MEAN
Epoch 0:  42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                           | 5/12 [00:00&lt;00:00, 255.45it/s, loss=1.696, v_num=11, train_loss_step=1]
MEAN
MEAN
Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                  | 6/12 [00:00&lt;00:00, 270.00it/s, loss=1.500, v_num=11, train_loss_step=1]
MEAN
MEAN
Epoch 0:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                          | 7/12 [00:00&lt;00:00, 281.26it/s, loss=1.785, v_num=11, train_loss_step=1]
MEAN
MEAN
Epoch 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 8/12 [00:00&lt;00:00, 289.95it/s, loss=1.589, v_num=11, train_loss_step=1]
MEAN
MEAN
Epoch 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 9/12 [00:00&lt;00:00, 297.11it/s, loss=1.463, v_num=11, train_loss_step=1]
MEAN
MEAN
Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00&lt;00:00, 284.59it/s, loss=1.379, v_num=11, train_loss_step=1]
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
&lt;/denchmark-code&gt;

I have tried to reproduce the bug, but it seems we enter divide_by_world_size and perform mean aggregation. Plus, I can see 1 being logged in the progress_bar.
Would you mind reproducing your bug with deterministic behaviour as I did ?
Best regards,
T.C
		</comment>
		<comment id='9' author='junwen-austin' date='2020-11-02T16:14:13Z'>
		&lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;
 I don't have access to my workstation right now but I believe the  prints (2 for 2 gpus) are from the  which are reduced correctly.
The incorrect behavior is from return {"loss": loss} (at least when I tested with return loss) where the value shown in the progress bar is the sum instead of mean across GPUs.
		</comment>
		<comment id='10' author='junwen-austin' date='2020-11-05T10:24:58Z'>
		Hey &lt;denchmark-link:https://github.com/s-rog&gt;@s-rog&lt;/denchmark-link&gt;
,
Thanks for the update. Let me check it out !
Best regards,
T.C
		</comment>
		<comment id='11' author='junwen-austin' date='2020-11-05T15:56:19Z'>
		Hey &lt;denchmark-link:https://github.com/s-rog&gt;@s-rog&lt;/denchmark-link&gt;
,
Could you modify the test above to reproduce the behaviour ? Not sure I fully grasped your answer.
Best regards,
T.C
		</comment>
		<comment id='12' author='junwen-austin' date='2020-11-05T16:40:37Z'>
		&lt;denchmark-code&gt;    def training_step(self, batch, batch_idx):
        return torch.ones(1).to(self.device)
&lt;/denchmark-code&gt;

this should correspond to the loss key on the pbar (the train_loss key that you defined is reduced correctly)
		</comment>
		<comment id='13' author='junwen-austin' date='2020-11-09T15:24:22Z'>
		&lt;denchmark-code&gt;class ExtentedModel(BoringModel):

    def training_step(self, batch, batch_idx):
        output = self.layer(batch)
        loss = self.loss(batch, output)
        loss /= loss.clone().detach()
        return loss


@pytest.mark.skipif(torch.cuda.device_count() &lt; 2, reason="test requires multi-GPU machine")
def test_dpp_reduce_mean_pbar(tmpdir):
    
    os.environ["PL_DEV_DEBUG"] = '1'

    model = ExtentedModel()
    model.training_step_end = None
    model.training_epoch_end = None

    trainer = Trainer(
        max_epochs=1,
        default_root_dir=os.getcwd(),
        limit_train_batches=10,
        limit_test_batches=2,
        limit_val_batches=2,
        distributed_backend="ddp",
        gpus=2,
        precision=32,
        )

    trainer.fit(model)
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;tests/trainer/legacy_deprecate_flow_log_tests/test_trainer_steps_scalar_return.py::test_dpp_reduce_mean_pbar LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2

  | Name  | Type   | Params
---------------------------------
0 | layer | Linear | 66    
Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00&lt;00:00, 367.26it/s, loss=1.000, v_num=22
&lt;/denchmark-code&gt;

I still get loss=1.000. Could you modify this test to reproduce the behaviour ?
Best,
T.C
		</comment>
		<comment id='14' author='junwen-austin' date='2020-11-10T01:31:07Z'>
		&lt;denchmark-code&gt;GPU available: True, used: True
TPU available: False, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2

  | Name  | Type   | Params
---------------------------------
0 | layer | Linear | 66
Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00&lt;00:00, 68.94it/s, loss=2.000, v_num=7, self_log=1]
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;
 This took me a while to figure out but it seems like the bug only shows up if you use  as well (with or without ).
&lt;denchmark-code&gt;class ExtentedModel(BoringModel):
    def training_step(self, batch, batch_idx):
        output = self.layer(batch)
        loss = self.loss(batch, output)
        loss /= loss.clone().detach()
        self.log('self_log', loss, prog_bar=True, sync_dist=True)
        return loss
&lt;/denchmark-code&gt;

		</comment>
		<comment id='15' author='junwen-austin' date='2020-11-11T00:55:06Z'>
		also &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/4479&gt;#4479&lt;/denchmark-link&gt;
 might be related?
		</comment>
	</comments>
</bug>