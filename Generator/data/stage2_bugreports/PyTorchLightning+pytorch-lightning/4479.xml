<bug id='4479' author='tobiascz' open_date='2020-11-02T11:14:58Z' closed_time='2020-11-17T18:28:03Z'>
	<summary>Logging with "self.log" in training_step does not create any outputs in progress bar or external Logger when loss isn't returned</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I think the newly introduced &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html#log&gt;log function&lt;/denchmark-link&gt;
 function does not log properly while being used in the training_step. The same code in validation_step creates the desired results.
&lt;denchmark-code&gt;    def training_step(self, batch, batch_idx):
        output = self.layer(batch)
        loss = self.loss(batch, output)
        self.log("loss", loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)
        self.log("my_metric_train", 1001, prog_bar=True, logger=True, on_step=True, on_epoch=True)
        ##### Doesn't Work #######


    def validation_step(self, batch, batch_idx):
        output = self.layer(batch)
        loss = self.loss(batch, output)
        self.log("val_loss", loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)
        self.log("my_metric_val", 1001, prog_bar=True, logger=True, on_step=True, on_epoch=True)
        ##### Works #######
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Please reproduce using&lt;/denchmark-h&gt;

&lt;denchmark-link:https://gist.github.com/tobiascz/bb2c6de83263eb38181052840062b5ac&gt;https://gist.github.com/tobiascz/bb2c6de83263eb38181052840062b5ac&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Logs created in training_step should show up in the prog_bar and loggers (such as tensorboard logger). Same code in the validation_step creates the desired results.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:

GPU:

Tesla T4


available:         True
version:           10.1


Packages:

numpy:             1.18.5
pyTorch_debug:     False
pyTorch_version:   1.6.0+cu101
pytorch-lightning: 0.10.0
tqdm:              4.41.1


System:

OS:                Linux
architecture:

64bit



processor:         x86_64
python:            3.6.9
version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020
In [ ]:



	</description>
	<comments>
		<comment id='1' author='tobiascz' date='2020-11-02T13:24:58Z'>
		When returning the loss from the training_step it works.
&lt;denchmark-code&gt;def training_step(self, batch, batch_idx):
        output = self.layer(batch)
        loss = self.loss(batch, output)
        self.log("loss", loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)
        self.log("my_metric_train", 1001, prog_bar=True, logger=True, on_step=True, on_epoch=True)
        return loss
        ##### Works #######```
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='tobiascz' date='2020-11-03T09:02:54Z'>
		&lt;denchmark-link:https://github.com/tobiascz&gt;@tobiascz&lt;/denchmark-link&gt;
 I believe it isn't a common issue, but from a design perspective, is it the desired behavior? I would expect the logs to work independently of whether the user returned the loss or not, however, a user warning/error should be raised in the case the training step returned None.
		</comment>
		<comment id='3' author='tobiascz' date='2020-11-03T10:59:39Z'>
		Thanks for pointing this out &lt;denchmark-link:https://github.com/itsikad&gt;@itsikad&lt;/denchmark-link&gt;
.
based on the comment above I am reopening the issue.
&lt;denchmark-code&gt;def training_step(self, batch, batch_idx):
        output = self.layer(batch)
        loss = self.loss(batch, output)
        self.log("loss", loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)
        self.log("my_metric_train", 1001, prog_bar=True, logger=True, on_step=True, on_epoch=True)
        ##### Doesn't Work #######
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;def training_step(self, batch, batch_idx):
        output = self.layer(batch)
        loss = self.loss(batch, output)
        self.log("loss", loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)
        self.log("my_metric_train", 1001, prog_bar=True, logger=True, on_step=True, on_epoch=True)
        return loss
        ##### Works #######
&lt;/denchmark-code&gt;

Expected behavior
logging in the training_step should be independent of optimising the model e.g. returning a loss. Even if the training_step has some issues and does not return a loss, the logging should work as expected.
		</comment>
		<comment id='4' author='tobiascz' date='2020-11-03T15:50:02Z'>
		&lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;
 please take a look. do we want to add a warning here?
		</comment>
		<comment id='5' author='tobiascz' date='2020-11-05T02:55:41Z'>
		I looked into this a bit but haven't found the source of the issue yet. Using this to test:
&lt;denchmark-code&gt;    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        self.log('with_loss', torch.tensor(0.5, device=self.device), on_step=True, on_epoch=False, prog_bar=True, sync_dist=True, sync_dist_op=None)
        return loss
    
    def training_step_end(self, output):
        print(self._results.items())
        return output
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;dict_items([('meta', {'_internal': {'_reduce_on_epoch': False, 'batch_sizes': []}, 'with_loss': {'prog_bar': True, 'logger': True, 'on_step': True, 'on_epoch': False, 'reduce_fx': &lt;built-in method mean of type object at 0x7fdad2bd05a0&gt;, 'value': tensor(0.5000), 'tbptt_reduce_fx': &lt;built-in method mean of type object at 0x7fdad2bd05a0&gt;, 'tbptt_pad_token': 0, 'forked': False, 'dataloader_idx': None}}), ('with_loss', tensor(0.5000))])
Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00&lt;00:00,  8.28it/s, loss=2.302, v_num=18, with_loss=0.5]

dict_items([('meta', {'_internal': {'_reduce_on_epoch': False, 'batch_sizes': []}, 'no___loss': {'prog_bar': True, 'logger': True, 'on_step': True, 'on_epoch': False, 'reduce_fx': &lt;built-in method mean of type object at 0x7f151459e5a0&gt;, 'value': tensor(0.5000), 'tbptt_reduce_fx': &lt;built-in method mean of type object at 0x7f151459e5a0&gt;, 'tbptt_pad_token': 0, 'forked': False, 'dataloader_idx': None}}), ('no___loss', tensor(0.5000))])
Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00&lt;00:00,  8.49it/s, loss=nan, v_num=19]
&lt;/denchmark-code&gt;

The result objects/dictionaries contructed are identical aside from the key I set. Gotta dig deeper...
Edit:
It seems like not returning a loss causes _curr_step_result to be None which bypasses the logging call
		</comment>
		<comment id='6' author='tobiascz' date='2020-11-11T08:14:37Z'>
		Dear &lt;denchmark-link:https://github.com/tobiascz&gt;@tobiascz&lt;/denchmark-link&gt;
,
Thanks for noticing this wrong behaviour. I will look into it asap.
Best regards,
Thomas Chaton.
		</comment>
		<comment id='7' author='tobiascz' date='2020-11-11T08:56:23Z'>
		Dear  &lt;denchmark-link:https://github.com/tobiascz&gt;@tobiascz&lt;/denchmark-link&gt;
,
It seems the bug is currently in the release branch but not master. Let me investigate from there.
&lt;denchmark-h:h1&gt;master branch&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;def test_train_logging_training_step(tmpdir):
    """
    This tests makes sure training_step logged value are properly captured.
    """

    class ExtendedBoringModel(BoringModel):

        def __init__(self):
            super().__init__()
            self.layer = torch.nn.Linear(32, 2)

        def forward(self, x):
            return self.layer(x)

        def loss(self, batch, prediction):
            # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls
            return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))

        def training_step(self, batch, batch_idx):
            output = self.layer(batch)
            loss = self.loss(batch, output)
            self.log("loss", loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)
            self.log("my_metric_train", 1001, prog_bar=True, logger=True, on_step=True, on_epoch=True)

        def validation_step(self, batch, batch_idx):
            output = self.layer(batch)
            loss = self.loss(batch, output)
            self.log("val_loss", loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)
            self.log("my_metric_val", 1001, prog_bar=True, logger=True, on_step=True, on_epoch=True)


        def test_step(self, batch, batch_idx):
            output = self.layer(batch)
            loss = self.loss(batch, output)
            self.log("test_loss", loss, prog_bar=True, logger=True, on_step=False, on_epoch=True)


        def configure_optimizers(self):
            optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)
            lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)
            return [optimizer], [lr_scheduler]

        def train_dataloader(self):
            return torch.utils.data.DataLoader(RandomDataset(32, 64))

        def val_dataloader(self):
            return torch.utils.data.DataLoader(RandomDataset(32, 64))

        def test_dataloader(self):
            return torch.utils.data.DataLoader(RandomDataset(32, 64))

    model = ExtendedBoringModel()
    model.training_step_end = None
    model.training_epoch_end = None

    # Initialize a trainer
    trainer = pl.Trainer(
        max_epochs=1,
        min_epochs=1, 
        limit_train_batches=2,
        limit_val_batches=0,
    )

    # Train the model ‚ö°
    trainer.fit(model)
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://camo.githubusercontent.com/ae248ec6514873dda9dfb212e75f1f6bf53ee7583818bb856c68d835006a4349/68747470733a2f2f696d616765732e7a656e68756275736572636f6e74656e742e636f6d2f3566613032643963373033633136353938613864363833622f32353030663431662d366131302d346562332d613164302d353738646337363635316137&gt;&lt;/denchmark-link&gt;

On 1.0.6. You are entirely right.
&lt;denchmark-link:https://camo.githubusercontent.com/ca14fc5e7b05cc124bdcb43782c5ef6d2a5744d126e65a58096b360ad67c8e53/68747470733a2f2f696d616765732e7a656e68756275736572636f6e74656e742e636f6d2f3566613032643963373033633136353938613864363833622f38643539616530322d323139642d343164322d396364362d323433396364323239353662&gt;&lt;/denchmark-link&gt;

However, it works fine if you return the loss in training_step.
		</comment>
		<comment id='8' author='tobiascz' date='2020-11-11T14:53:52Z'>
		Sometimes there's no unambiguous way of returning a single loss tho, i.e. GAN training.
What I'm doing to bypass the bug rn is this, hopefully there's no impact on the optimization:
(I'm using automatic_optimization=False btw)
    def training_step(self, batch, batch_idx, optimizer_idx):
        ...
        D_loss = ...
        G_loss = ...
        ...
        self.log(D, ...)
        self.log(G, ...)

        return torch.tensor(0)
		</comment>
		<comment id='9' author='tobiascz' date='2020-11-11T15:22:29Z'>
		Hey &lt;denchmark-link:https://github.com/hecoding&gt;@hecoding&lt;/denchmark-link&gt;
,
You might want to have a look at this: &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4618&gt;#4618&lt;/denchmark-link&gt;

Best,
T.C
		</comment>
		<comment id='10' author='tobiascz' date='2020-11-11T17:47:22Z'>
		Hey &lt;denchmark-link:https://github.com/hecoding&gt;@hecoding&lt;/denchmark-link&gt;
,
Do you get this pb when using master ?
Best regards,
Thomas Chaton.
		</comment>
		<comment id='11' author='tobiascz' date='2020-11-11T18:15:41Z'>
		
Dear @tobiascz,
It seems the bug is currently in the release branch but not master. Let me investigate from there.
master branch
def test_train_logging_training_step(tmpdir):
    """
    This tests makes sure training_step logged value are properly captured.
    """

    class ExtendedBoringModel(BoringModel):

        def __init__(self):
            super().__init__()
            self.layer = torch.nn.Linear(32, 2)

        def forward(self, x):
            return self.layer(x)

        def loss(self, batch, prediction):
            # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls
            return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))

        def training_step(self, batch, batch_idx):
            output = self.layer(batch)
            loss = self.loss(batch, output)
            self.log("loss", loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)
            self.log("my_metric_train", 1001, prog_bar=True, logger=True, on_step=True, on_epoch=True)

        def validation_step(self, batch, batch_idx):
            output = self.layer(batch)
            loss = self.loss(batch, output)
            self.log("val_loss", loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)
            self.log("my_metric_val", 1001, prog_bar=True, logger=True, on_step=True, on_epoch=True)


        def test_step(self, batch, batch_idx):
            output = self.layer(batch)
            loss = self.loss(batch, output)
            self.log("test_loss", loss, prog_bar=True, logger=True, on_step=False, on_epoch=True)


        def configure_optimizers(self):
            optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)
            lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)
            return [optimizer], [lr_scheduler]

        def train_dataloader(self):
            return torch.utils.data.DataLoader(RandomDataset(32, 64))

        def val_dataloader(self):
            return torch.utils.data.DataLoader(RandomDataset(32, 64))

        def test_dataloader(self):
            return torch.utils.data.DataLoader(RandomDataset(32, 64))

    model = ExtendedBoringModel()
    model.training_step_end = None
    model.training_epoch_end = None

    # Initialize a trainer
    trainer = pl.Trainer(
        max_epochs=1,
        min_epochs=1, 
        limit_train_batches=2,
        limit_val_batches=0,
    )

    # Train the model ‚ö°
    trainer.fit(model)


On 1.0.6. You are entirely right.

However, it works fine if you return the loss in training_step.

I added this
&lt;denchmark-code&gt;%capture
! pip install git+https://github.com/PyTorchLightning/pytorch-lightning@master
! pip install pytorch-lightning-bolts
&lt;/denchmark-code&gt;

to the code here: &lt;denchmark-link:https://gist.github.com/tobiascz/bb2c6de83263eb38181052840062b5ac&gt;https://gist.github.com/tobiascz/bb2c6de83263eb38181052840062b5ac&lt;/denchmark-link&gt;

But the error is still present.
		</comment>
		<comment id='12' author='tobiascz' date='2020-11-16T20:20:22Z'>
		
Hey @hecoding,
Do you get this pb when using master ?
Best regards,
Thomas Chaton.

Hi &lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;
, sorry busy week. Nope I just ran the code on 1.0.4 from pip. I guess tobias answered your question already. Let me know otherwise.
Good stuff on the lambda closure btw, I'll prolly be using that üëç
Cheers.
		</comment>
		<comment id='13' author='tobiascz' date='2020-11-17T18:28:02Z'>
		This should be fixed on master.
		</comment>
	</comments>
</bug>