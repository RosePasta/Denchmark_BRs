<bug id='4781' author='junwen-austin' open_date='2020-11-20T03:55:48Z' closed_time='2020-12-05T14:49:46Z'>
	<summary>Potential bug in metric when updated with a slice of tensor in DDP</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

when a metric is updated with a slice of tensor as one of the inputs (either pred or target) with multiple GPU with DDP, it throws out an error:
&lt;denchmark-code&gt;RuntimeError: Tensors must be non-overlapping and dense
&lt;/denchmark-code&gt;

Once the slice of the tensor is clone and detach, then it works.
&lt;denchmark-h:h2&gt;Please reproduce using [the BoringModel and post here]&lt;/denchmark-h&gt;

The issue can be reproduced below:
&lt;denchmark-link:https://colab.research.google.com/drive/1yuqwb8BHhmDATp2IUNRNjSXOg3kV73Wp?usp=sharing&gt;https://colab.research.google.com/drive/1yuqwb8BHhmDATp2IUNRNjSXOg3kV73Wp?usp=sharing&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

the metric update should work with a slice of tensor
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

CUDA:
- GPU:
- Tesla P100-PCIE-16GB
- available:         True
- version:           10.1

Packages:

numpy:             1.18.5
pyTorch_debug:     True
pyTorch_version:   1.7.0+cu101
pytorch-lightning: 1.0.7
tqdm:              4.41.1


System:

OS:                Linux
architecture:

64bit



processor:         x86_64
python:            3.6.9
version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020



&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='junwen-austin' date='2020-11-20T05:02:45Z'>
		&lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
 I thought we solved something like this?
		</comment>
		<comment id='2' author='junwen-austin' date='2020-11-20T07:37:18Z'>
		I also though we already fixed this.
&lt;denchmark-link:https://github.com/junwen-austin&gt;@junwen-austin&lt;/denchmark-link&gt;
 when I try to run the colab you linked to, I cannot reproduce. Is this only happening in ddp mode?
		</comment>
		<comment id='3' author='junwen-austin' date='2020-11-20T14:09:53Z'>
		&lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
 Could you try the notebook again with the ddp (which I just updated). The issue was initially identified with ddp but you are right the issue does not replicate without ddp. I'll also update the post once you confirm this issue exists in the ddp model even with 1 GPU. Thanks.
		</comment>
		<comment id='4' author='junwen-austin' date='2020-11-22T10:25:13Z'>
		So I can confirm that this happens when running in ddp mode, however I am not sure if this is a bug. It had nothing to do with detach bug we had some weeks ago.
This happens because all_gather requires all tensors to be contiguous (data is stored in an uninterrupted block of memory). When you take a slice of a tensor, you don¬¥t alter the underlying memory location of data, just how you read it (basically changing the stride). If you try this in your code, you can see for yourself
&lt;denchmark-code&gt;print('output', output.is_contiguous()) # True
print('output slice', output[:,0].is_contiguous()) # False
&lt;/denchmark-code&gt;

the reason why calling output[:,0].detach().clone() works is that clone takes a contiguous copy of the data (the detach() is not nessesary). The more correct solution would be to call:
&lt;denchmark-code&gt;self.metric.update(output[:, 0].contiguous(),  
                   output[:, 1].contiguous())
&lt;/denchmark-code&gt;

We could easily implement calling  inside the lightning metric on all tensors, however I at least want &lt;denchmark-link:https://github.com/ananyahjha93&gt;@ananyahjha93&lt;/denchmark-link&gt;
 opinion on this before moving forward.
		</comment>
		<comment id='5' author='junwen-austin' date='2020-11-22T13:45:10Z'>
		&lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
  thanks for the follow-up and good to know the more correct solution :)
		</comment>
	</comments>
</bug>