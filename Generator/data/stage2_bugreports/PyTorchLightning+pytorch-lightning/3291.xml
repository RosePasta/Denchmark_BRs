<bug id='3291' author='JackCaster' open_date='2020-08-31T15:47:57Z' closed_time='2020-09-22T22:00:25Z'>
	<summary>Checkpoints not working</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Default checkpoint_callback in Trainer() does not work so model's checkpoints are not saved.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:
I first created a simple implementation of a LightningModule. This contains:
&lt;denchmark-code&gt;class Model(pl.LightningModule):
   ...
   def validation_step(self, batch, batch_idx):
      x, y = batch
      y_hat = self(x)
      loss = F.cross_entropy(y_hat, y)
      result = pl.EvalResult()
      result.log('val_loss', loss, prog_bar=True, on_step=False, on_epoch=True)
      return result
   ...
&lt;/denchmark-code&gt;

when I run it with
&lt;denchmark-code&gt;m = Model()
trainer.fit(model) # using default settings
&lt;/denchmark-code&gt;

I get
&lt;denchmark-code&gt;/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: RuntimeWarning: The metric you returned None must be a `torch.Tensor` instance, checkpoint not saved HINT: what is the value of loss in validation_epoch_end()?
  warnings.warn(*args, **kwargs)
/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: RuntimeWarning: Can save best model only with loss available, skipping.
  warnings.warn(*args, **kwargs)
&lt;/denchmark-code&gt;

Following the hint, I then implemented the validation_epoch_end() method. I was surprised I had to, given that most examples in the doc work without it and I assumed it was used when additional control on the validation procedure was needed. Perhaps, I would disable all callbacks by defaults to avoid the need to implement additional methods at first. Anyways, now my model looks like:
&lt;denchmark-code&gt;class Model(pl.LightningModule):
   ...
   def validation_step(self, batch, batch_idx):
      x, y = batch
      y_hat = self(x)
      loss = F.cross_entropy(y_hat, y)
      result = pl.EvalResult()
      result.log('val_loss', loss, prog_bar=True, on_step=False, on_epoch=True)
      return result

   def validation_epoch_end(self, outputs):
      avg_loss = torch.stack([x for x in outputs['val_loss']]).mean()
      return {"val_loss": avg_loss}
   ...
&lt;/denchmark-code&gt;

In the above, I followed &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1153#issuecomment-599792149&gt;#1153 (comment)&lt;/denchmark-link&gt;
 but got:
&lt;denchmark-code&gt;/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/evaluate_loop.py in log_epoch_metrics(self, eval_results)
    148             if isinstance(eval_results, list):
    149                 for eval_result in eval_results:
--&gt; 150                     self.trainer.callback_metrics = eval_result.callback_metrics
    151             else:
    152                 self.trainer.callback_metrics = eval_results.callback_metrics

AttributeError: 'dict' object has no attribute 'callback_metrics'
&lt;/denchmark-code&gt;

Thus, I tried to use the new Result container:
&lt;denchmark-code&gt;class Model(pl.LightningModule):
   ...
   def validation_step(self, batch, batch_idx):
      x, y = batch
      y_hat = self(x)
      loss = F.cross_entropy(y_hat, y)
      result = pl.EvalResult()
      result.log('val_loss', loss, prog_bar=True, on_step=False, on_epoch=True)
      return result

   def validation_epoch_end(self, outputs):
      loss = torch.stack([x for x in outputs['val_loss']]).mean()
      result = pl.EvalResult()
      result.log('val_loss', loss)
      return result
   ...
&lt;/denchmark-code&gt;

When I fit the model again, I get:
&lt;denchmark-code&gt;/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: RuntimeWarning: The metric you returned None must be a `torch.Tensor` instance, checkpoint not saved HINT: what is the value of loss in validation_epoch_end()?
  warnings.warn(*args, **kwargs)
/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: RuntimeWarning: Can save best model only with loss available, skipping.
  warnings.warn(*args, **kwargs)
&lt;/denchmark-code&gt;

If I print the elements of result, I get {'val_loss': tensor(0.6092, device='cuda:0')}, so val_loss is a tensor already.
My last attempt was to define my callback:
&lt;denchmark-code&gt;from pytorch_lightning.callbacks import ModelCheckpoint
checkpoint_callback = ModelCheckpoint(
    monitor='val_loss',
    mode='min',
)

trainer = pl.Trainer(..., checkpoint_callback=checkpoint_callback)
&lt;/denchmark-code&gt;

but unfortunately I still got:
&lt;denchmark-code&gt;/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: RuntimeWarning: The metric you returned None must be a `torch.Tensor` instance, checkpoint not saved HINT: what is the value of val_loss in validation_epoch_end()?
  warnings.warn(*args, **kwargs)
/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: RuntimeWarning: Can save best model only with val_loss available, skipping.
  warnings.warn(*args, **kwargs)
&lt;/denchmark-code&gt;

Would you be able to help?
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

I would expect Pytorch-lightning to work with minimal boilerplate (e.g. only with, training_step, validation_step). Perhaps, checkpoints should be disabled in Trainer() by default. Or, validation_step() could return automatically the aggregated metric by default. Otherwise, we may need a better explanation on how to get the aggregated metrics out the validation step, and make more clear that validation_epoch_end() is needed üëç
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

I am using Google Colab:
&lt;denchmark-code&gt;* CUDA:
	- GPU:
		- Tesla K80
	- available:         True
	- version:           10.1
* Packages:
	- numpy:             1.18.5
	- pyTorch_debug:     False
	- pyTorch_version:   1.6.0+cu101
	- pytorch-lightning: 0.9.1rc1
	- tensorboard:       2.2.0
	- tqdm:              4.41.1
* System:
	- OS:                Linux
	- architecture:
		- 64bit
		- 
	- processor:         x86_64
	- python:            3.6.9
	- version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='JackCaster' date='2020-08-31T15:48:43Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='JackCaster' date='2020-08-31T16:07:49Z'>
		EDIT: Checkpoints work if I set them in the EvalResult like:
&lt;denchmark-code&gt;    def validation_step(self, batch, batch_idx):
      x, y = batch
      y_hat = self(x)
      loss = F.cross_entropy(y_hat, y)
      result = pl.EvalResult(checkpoint_on=loss)
      result.log('val_loss', loss, prog_bar=True, on_step=False, on_epoch=True)
      return result
&lt;/denchmark-code&gt;

but 1) I would like to keep callbacks separate from the module and only in the Trainer(). 2) Will setting the checkpoint in EvalResult interfere with other callbacks set in Trainer()? 3) How can I set up callbacks to monitor custom (aggregate) metrics?
		</comment>
		<comment id='3' author='JackCaster' date='2020-09-10T20:53:40Z'>
		&lt;denchmark-link:https://github.com/JackCaster&gt;@JackCaster&lt;/denchmark-link&gt;
 mind sending a PR with this failing example as minimal test so we can fix it and also prevent this case in the future... 
		</comment>
		<comment id='4' author='JackCaster' date='2020-09-11T22:33:52Z'>
		
@JackCaster mind sending a PR with this failing example as minimal test so we can fix it and also prevent this case in the future... üê∞

Surely I can do that. I am on vacation atm without pc. I will send it sometime next week
		</comment>
		<comment id='5' author='JackCaster' date='2020-09-22T11:09:09Z'>
		ok, so to your points.

Can we keep these metrics separate out of the lightning module? well, the lightningmodule needs to be self contained... that is what defines what to early stop on or not. If you don't do it there then you have to look in the module to figure out what to monitor...

For this reason, we're tying the related monitor metrics to the module.

In fact, imagine your module requires a special callback. You can no longer share your model around and drop into any lightning trainer. But now you ALSO have to tell the person to not forget to init that special callback and do special things for it to work with the module.

It means your lightningmodule leaks abstraction.
		</comment>
		<comment id='6' author='JackCaster' date='2020-09-23T08:31:35Z'>
		Great! I ran a quick test and it works. &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 Do you still need the PR?
I see that now the EarlyStopping callback is disabled by default so we need to initialize ourselves. Instead, ModelCheckpoint is active by default and it will monitor checkpoint_on.
Regarding the , there is an error in the documentation. While  disables it by default 
 in the doc (&lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/trainer.html#early-stop-callback&gt;https://pytorch-lightning.readthedocs.io/en/latest/trainer.html#early-stop-callback&lt;/denchmark-link&gt;
) it reads

None: Same as, if True is specified.
Default: None.

it should be instead:

None: Equivalent to True.
Default: False.

If this an error, shall I open a PR?
		</comment>
		<comment id='7' author='JackCaster' date='2020-09-23T08:44:47Z'>
		
If this an error, shall I open a PR?

yes, please, just be aware that &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 is finishing some final Result refactoring which may collide your you PR :]
		</comment>
		<comment id='8' author='JackCaster' date='2020-09-23T08:50:02Z'>
		

If this an error, shall I open a PR?

yes, please, just be aware that @williamFalcon is finishing some final Result refactoring which may collide your you PR :]

Alright. Perhaps, it is better if I wait a couple of days then.
		</comment>
		<comment id='9' author='JackCaster' date='2020-09-23T11:58:32Z'>
		that PR should be fine. please go ahead
		</comment>
	</comments>
</bug>