<bug id='3302' author='FrancescoSaverioZuppichini' open_date='2020-09-01T09:29:20Z' closed_time='2020-10-29T17:27:54Z'>
	<summary>Unexpected key(s) in state_dict Error when calling Trainer.test</summary>
	<description>
Dear all,
I have a trainer
import torch
from torch.optim.lr_scheduler import ReduceLROnPlateau
from pytorch_lightning import LightningModule
from torch.nn import functional as F
from pytorch_lightning.metrics.functional import accuracy, f1_score, auroc


class TraversabilityModule(LightningModule):

    def __init__(self, model: torch.nn.Module):
        super().__init__()
        self.model = model

    def forward(self, x):
        return self.model(x)

    def get_metrics(self, pred, y):
        return {
            'accuracy': accuracy(pred, y, num_classes=2),
            'f1': f1_score(pred, y, num_classes=2),
            # 'roc_auc': auroc(pred, y)
        }

    def step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        pred = torch.argmax(y_hat, dim=1)
        metrics = self.get_metrics(pred, y)
        loss = F.cross_entropy(y_hat, y)
        return loss, metrics

    def training_step(self, batch, batch_idx):
        loss, metrics = self.step(batch, batch_idx)
        return {'loss': loss, 'log': metrics }

    def validation_step(self, batch, batch_idx):
        loss, metrics = self.step(batch, batch_idx)
        return {'val_loss': loss, 'log': metrics }

    def validation_epoch_end(self, outputs):
        val_loss_mean = torch.stack([x['val_loss'] for x in outputs]).mean()
        acc_mean = torch.stack([x['log']['accuracy'] for x in outputs]).mean()
        f1_mean = torch.stack([x['log']['f1'] for x in outputs]).mean()
        # roc_auc_mean = torch.stack([x['log']['roc_auc'] for x in outputs]).mean()

        return {
            'val_loss': val_loss_mean,
            'val_f1': f1_mean,
            'progress_bar': {'f1': f1_mean},
            'log': {
                'val_accuracy': acc_mean,
                'val_f1': f1_mean,
                # 'roc_auc': roc_auc_mean
                
                }
            }

    def test_step(self, batch, batch_idx):
        loss, metrics = self.step(batch, batch_idx)
        return {'test_loss': loss, 'log': metrics}

    def test_epoch_end(self, outputs):
        val_loss_mean = torch.stack([x['test_loss'] for x in outputs]).mean()
        acc_mean = torch.stack([x['log']['accuracy'] for x in outputs]).mean()
        f1_mean = torch.stack([x['log']['f1'] for x in outputs]).mean()
        # roc_auc_mean = torch.stack([x['log']['roc_auc'] for x in outputs]).mean()

        return {
            'test_loss': val_loss_mean,
            'test_f1': f1_mean,
            'progress_bar': {'f1': f1_mean},
            'log': {
                'test_accuracy': acc_mean,
                'test_f1': f1_mean,
                # 'roc_auc': roc_auc_mean
                
                }
            }

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)
        scheduler = {'scheduler': ReduceLROnPlateau(optimizer, verbose=True),
                     'monitor': 'val_f1'}

        return [optimizer], [scheduler]
And I have a training loop
module = TraversabilityModule(model)

    trainer = pl.Trainer(
        gpus=1,
        max_epochs=params['epoches'],
        logger=comet_logger,
        checkpoint_callback=ModelCheckpoint(
            monitor='val_f1',
            filepath=project.checkpoint_dir / params['model'] / 'model',
            ),
        early_stop_callback=EarlyStopping(monitor='val_f1',
                                          patience=15))
    trainer.fit(module, train_dl, val_dl)
    
    trainer.test(test_dataloaders=test_dl)
I get the following error when I try to validate the test set (at trainer.test)
&lt;denchmark-code&gt;RuntimeError: Error(s) in loading state_dict for TraversabilityModule:
        Unexpected key(s) in state_dict: "model.encoder.gate.0.weight", "model.encoder.gate.1.weight", "model.encoder.gate.1.bias", "model.encoder.gate.1.running_mean", "model.encoder.gate.1.running_var", "model.encoder.gate.1.num_batches_tracked", "model.encoder.layers.0.layer.0.shortcut.0.weight", "model.encoder.layers.0.layer.0.shortcut.1.weight", "model.encoder.layers.0.layer.0.shortcut.1.bias", "model.encoder.layers.0.layer.0.shortcut.1.running_mean", "model.encoder.layers.0.layer.0.shortcut.1.running_var", "model.encoder.layers.0.layer.0.shortcut.1.num_batches_tracked", "model.encoder.layers.0.layer.0.convs.0.weight", "model.encoder.layers.0.layer.0.convs.0.bias", "model.encoder.layers.0.layer.0.convs.0.running_mean", "model.encoder.layers.0.layer.0.convs.0.running_var", "model.encoder.layers.0.layer.0.convs.0.num_batches_tracked", "model.encoder.layers.0.layer.0.convs.2.weight", "model.encoder.layers.0.layer.0.convs.3.weight", "model.encoder.layers.0.layer.0.convs.3.bias", "model.encoder.layers.0.layer.0.convs.3.running_mean", "model.encoder.layers.0.layer.0.convs.3.running_var", "model.encoder.layers.0.layer.0.convs.3.num_batches_tracked", "model.encoder.layers.0.layer.0.convs.5.weight", "model.encoder.layers.0.layer.0.se.se.0.weight", "model.encoder.layers.0.layer.0.se.se.0.bias", "model.encoder.layers.0.layer.0.se.se.2.weight", "model.encoder.layers.0.layer.0.se.se.2.bias", "model.encoder.layers.1.layer.0.shortcut.0.weight", "model.encoder.layers.1.layer.0.shortcut.1.weight", "model.encoder.layers.1.layer.0.shortcut.1.bias", "model.encoder.layers.1.layer.0.shortcut.1.running_mean", "model.encoder.layers.1.layer.0.shortcut.1.running_var", "model.encoder.layers.1.layer.0.shortcut.1.num_batches_tracked", "model.encoder.layers.1.layer.0.convs.0.weight", "model.encoder.layers.1.layer.0.convs.0.bias", "model.encoder.layers.1.layer.0.convs.0.running_mean", "model.encoder.layers.1.layer.0.convs.0.running_var", "model.encoder.layers.1.layer.0.convs.0.num_batches_tracked", "model.encoder.layers.1.layer.0.convs.2.weight", "model.encoder.layers.1.layer.0.convs.3.weight", "model.encoder.layers.1.layer.0.convs.3.bias", "model.encoder.layers.1.layer.0.convs.3.running_mean", "model.encoder.layers.1.layer.0.convs.3.running_var", "model.encoder.layers.1.layer.0.convs.3.num_batches_tracked", "model.encoder.layers.1.layer.0.convs.5.weight", "model.encoder.layers.1.layer.0.se.se.0.weight", "model.encoder.layers.1.layer.0.se.se.0.bias", "model.encoder.layers.1.layer.0.se.se.2.weight", "model.encoder.layers.1.layer.0.se.se.2.bias", "model.encoder.layers.2.layer.0.shortcut.0.weight", "model.encoder.layers.2.layer.0.shortcut.1.weight", "model.encoder.layers.2.layer.0.shortcut.1.bias", "model.encoder.layers.2.layer.0.shortcut.1.running_mean", "model.encoder.layers.2.layer.0.shortcut.1.running_var", "model.encoder.layers.2.layer.0.shortcut.1.num_batches_tracked", "model.encoder.layers.2.layer.0.convs.0.weight", "model.encoder.layers.2.layer.0.convs.0.bias", "model.encoder.layers.2.layer.0.convs.0.running_mean", "model.encoder.layers.2.layer.0.convs.0.running_var", "model.encoder.layers.2.layer.0.convs.0.num_batches_tracked", "model.encoder.layers.2.layer.0.convs.2.weight", "model.encoder.layers.2.layer.0.convs.3.weight", "model.encoder.layers.2.layer.0.convs.3.bias", "model.encoder.layers.2.layer.0.convs.3.running_mean", "model.encoder.layers.2.layer.0.convs.3.running_var", "model.encoder.layers.2.layer.0.convs.3.num_batches_tracked", "model.encoder.layers.2.layer.0.convs.5.weight", "model.encoder.layers.2.layer.0.se.se.0.weight", "model.encoder.layers.2.layer.0.se.se.0.bias", "model.encoder.layers.2.layer.0.se.se.2.weight", "model.encoder.layers.2.layer.0.se.se.2.bias", "model.decoder.decoder.weight", "model.decoder.decoder.bias". 
&lt;/denchmark-code&gt;

They are clearly the weights in my model (a variant of ResNet). The model path exists and the model is stored there.
Thank you.
Best Regards,
Francesco
	</description>
	<comments>
		<comment id='1' author='FrancescoSaverioZuppichini' date='2020-09-02T20:39:08Z'>
		&lt;denchmark-link:https://github.com/ananyahjha93&gt;@ananyahjha93&lt;/denchmark-link&gt;
 mind have a look?
		</comment>
		<comment id='2' author='FrancescoSaverioZuppichini' date='2020-09-05T11:18:49Z'>
		Would it be a good idea to write tests with a real case scenario so this kind of bugs are discovered before pushing to production? It seems impossible to me I am the only one having this problem.
		</comment>
		<comment id='3' author='FrancescoSaverioZuppichini' date='2020-09-08T16:29:36Z'>
		Can you explain what test cases are you referring to?
		</comment>
		<comment id='4' author='FrancescoSaverioZuppichini' date='2020-09-08T17:48:52Z'>
		Hi &lt;denchmark-link:https://github.com/edenlightning&gt;@edenlightning&lt;/denchmark-link&gt;
 , thank you for your reply. Sure, I haven't check how you tested the code but I have only seen that you had only 90% coverage. It would be a great idea to write a test where you actually try to train something so errors like the one I am having will be caught before production.
		</comment>
		<comment id='5' author='FrancescoSaverioZuppichini' date='2020-09-27T08:44:17Z'>
		&lt;denchmark-link:https://github.com/FrancescoSaverioZuppichini&gt;@FrancescoSaverioZuppichini&lt;/denchmark-link&gt;
 still facing the issue? Can you try with master??
		</comment>
		<comment id='6' author='FrancescoSaverioZuppichini' date='2020-10-05T18:25:33Z'>
		&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 can you try to reproduce?
		</comment>
		<comment id='7' author='FrancescoSaverioZuppichini' date='2020-10-05T22:36:40Z'>
		
Hi @edenlightning , thank you for your reply. Sure, I haven't check how you tested the code but I have only seen that you had only 90% coverage. It would be a great idea to write a test where you actually try to train something so errors like the one I am having will be caught before production.

we have quite a high testing, covering all variety of cases as you can see from the wide palette oof EvalModelTemplate... the kind of low coverage number comes from codecov limitation to collect easily data from spawn processes and externally launched scripts :]
&lt;denchmark-link:https://github.com/FrancescoSaverioZuppichini&gt;@FrancescoSaverioZuppichini&lt;/denchmark-link&gt;
 mind share the remaining parts to be able to reproduce your issues such as Model and data loader? or is it fine to use any simple linear model with random data generator as data loader?
		</comment>
		<comment id='8' author='FrancescoSaverioZuppichini' date='2020-10-06T16:58:52Z'>
		Hi, so I think I can reproduce this issue in , but is not reproducible for me when installing from master. The link for a colab notebook reproducing this issue is &lt;denchmark-link:https://colab.research.google.com/drive/1Nf4AHpmMZjwg7YF7Bn9j8RRUvowo0yXh?usp=sharing&gt;here&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='FrancescoSaverioZuppichini' date='2020-10-06T20:21:14Z'>
		&lt;denchmark-link:https://github.com/ktertikas&gt;@ktertikas&lt;/denchmark-link&gt;
 nope still a problem when running the code on master.
But thanks to your reproducible code, I now finally see what the problem is:

CoolSystem takes as argument a PyTorch model
Arguments to the LightningModule are generally considered hyperparameters, but the exception are things like full PyTorch models or unpicklable objects
In your example, the simple model does not get saved to the checkpoint
When it gets loaded, we don't know what to pass to the init.

Solution:
simple_model = SimpleModel()
loaded_model = CoolSystem.load_from_checkpoint(checkpoint_path=ckpt_path, module=simple_model)  
#                                                                           ^-- this arg is important
by passing the missing arg to the load function, it will first init the CoolSystem, then load the state dict on it.
Works fine in your code and I can only assume this was also the case with &lt;denchmark-link:https://github.com/FrancescoSaverioZuppichini&gt;@FrancescoSaverioZuppichini&lt;/denchmark-link&gt;
's code
		</comment>
		<comment id='10' author='FrancescoSaverioZuppichini' date='2020-10-07T10:09:50Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 thanks for the response! In the colab notebook I sent you above I still don't think that your solution works for . In particular, there is a new error () when doing:
simple_model = SimpleModel()
loaded_model = CoolSystem.load_from_checkpoint(checkpoint_path=ckpt_path, model=simple_model)  
#                                                                           ^-- this arg is important
Also I am assuming that the kwarg is model instead of module in the colab notebook case, right?
		</comment>
		<comment id='11' author='FrancescoSaverioZuppichini' date='2020-10-07T11:57:40Z'>
		Hi all, sorry for the late reply, and thank you for all the help. Do you guys still need a minimal working example? I can put something together and share it. You need a normal LightningModule instance with the model (nn.Module instance) as a parameter + field. I ditch PyTorch lighting for a better solution. Nevertheless, I will be more than glad to help out.
&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 I didn't know codecov had that problem, nice to know!
Thank you :)
Best Regards,
Francesco
		</comment>
		<comment id='12' author='FrancescoSaverioZuppichini' date='2020-10-19T22:33:46Z'>
		Please try and reproduce your error using this &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py&gt;model&lt;/denchmark-link&gt;
, and share a colab.
		</comment>
		<comment id='13' author='FrancescoSaverioZuppichini' date='2020-10-29T17:27:54Z'>
		Feel free to reopen with an example, closing this otherwise.
		</comment>
		<comment id='14' author='FrancescoSaverioZuppichini' date='2020-10-30T11:07:12Z'>
		Sure, but I don't think I will ever use again this library
		</comment>
	</comments>
</bug>