<bug id='1844' author='Brechard' open_date='2020-05-15T08:13:14Z' closed_time='2020-05-15T11:25:50Z'>
	<summary>Precision 16 not working</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Setting the flag of precision to 16 does not work. Furthermore, I don't need to set the value of gpus if I have my model as cuda version and precision to 32 it automatically runs in GPU, if I say precision=16 first says that amp and cpu don't work, so I set gpus=[0] and then it gives the following error:
&lt;denchmark-code&gt;I0515 10:03:17.552285 12848 distrib_data_parallel.py:248] GPU available: True, used: True
I0515 10:03:17.552285 12848 distrib_data_parallel.py:296] CUDA_VISIBLE_DEVICES: [0]
I0515 10:03:17.552285 12848 auto_mix_precision.py:52] Using 16bit precision.
Traceback (most recent call last):
  File "C:/Users/rodri/PycharmProjects/TTMelGAN/src/model_dir/train.py", line 146, in &lt;module&gt;
    app.run(train)
  File "C:\Users\rodri\Miniconda3\envs\TTMel\lib\site-packages\absl\app.py", line 299, in run
    _run_main(main, args)
  File "C:\Users\rodri\Miniconda3\envs\TTMel\lib\site-packages\absl\app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "C:/Users/rodri/PycharmProjects/TTMelGAN/src/model_dir/train.py", line 56, in train
    trainer.fit(model)
  File "C:\Users\rodri\Miniconda3\envs\TTMel\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 765, in fit
    self.single_gpu_train(model)
  File "C:\Users\rodri\Miniconda3\envs\TTMel\lib\site-packages\pytorch_lightning\trainer\distrib_parts.py", line 489, in single_gpu_train
    model, optimizers = model.configure_apex(amp, model, self.optimizers, self.amp_level)
NameError: name 'amp' is not defined
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

In the colab lightning demo, in the first example MNIST, set pl.Trainer(gpus=1, precision=16) and you get the same error.
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Training correctly
Since it can be reproduced in Colab I guess that the specs of my laptop or AWS EC2 where I get the same does not matter
	</description>
	<comments>
		<comment id='1' author='Brechard' date='2020-05-15T08:13:52Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='Brechard' date='2020-05-15T09:05:19Z'>
		Could you please specify what lightning version are you using?
		</comment>
		<comment id='3' author='Brechard' date='2020-05-15T09:28:18Z'>
		last one, 0.7.5
		</comment>
		<comment id='4' author='Brechard' date='2020-05-15T09:31:26Z'>
		cool, could you pls verify with 0.7.6.rc4?
		</comment>
		<comment id='5' author='Brechard' date='2020-05-15T09:44:03Z'>
		I got the same thing
		</comment>
		<comment id='6' author='Brechard' date='2020-05-15T10:15:42Z'>
		it seems like you do not have installed APEX, pls run &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/tests/install_AMP.sh&gt;install_AMP.sh&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='Brechard' date='2020-05-15T11:25:44Z'>
		Indeed I had to install Apex, as always it did not work in that way getting pip errors and managed to install it with "conda install -c conda-forge nvidia-apex" in case some has the same issue.
I though that PL was using the apex now integrated in torch so I didn't have to install it separatedly, but after rereading the release notes it will be available with torch 1.6 right?
		</comment>
		<comment id='8' author='Brechard' date='2020-05-15T11:38:12Z'>
		Apex is used for up to pt 1.5
1.6+ uses native apex
		</comment>
		<comment id='9' author='Brechard' date='2020-05-15T11:48:52Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 shall we raise a warning that user tries to use AMP but have not installed any supports, meaning pt &lt;= 1.5 and missing APEX?
		</comment>
		<comment id='10' author='Brechard' date='2020-06-16T18:10:41Z'>
		so how to fix the problem? Installation from .sh Apex didnt solved the problem
		</comment>
		<comment id='11' author='Brechard' date='2020-06-16T18:44:45Z'>
		tbh much easier to upgrade to pt 1.6
		</comment>
	</comments>
</bug>