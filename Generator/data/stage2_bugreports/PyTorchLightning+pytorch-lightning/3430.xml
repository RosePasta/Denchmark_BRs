<bug id='3430' author='griff4692' open_date='2020-09-09T20:36:18Z' closed_time='2020-09-10T16:21:19Z'>
	<summary>Issue with trainer.test in "ddp" distributed mode</summary>
	<description>
Hi -
I have the following pseudo code workflow:

trainer = Trainer(distributed_backend='ddp', ...)
model = new custom LightningModule
trainer.fit(model, ...)
model.freeze()
train.test(model, ...)

The error that I get is this:
AssertionError: DistributedDataParallel is not needed when a module doesn't have any parameter that requires a gradient.
What is the best way to address this?
Thanks very much,
Griffin
	</description>
	<comments>
		<comment id='1' author='griff4692' date='2020-09-09T20:51:45Z'>
		That is a PyTorch AssertionError. Try it without calling model.freeze()?
		</comment>
		<comment id='2' author='griff4692' date='2020-09-16T18:01:22Z'>
		Thanks! this worked
		</comment>
	</comments>
</bug>