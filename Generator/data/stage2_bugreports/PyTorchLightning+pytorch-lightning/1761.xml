<bug id='1761' author='mpaepper' open_date='2020-05-08T17:14:19Z' closed_time='2020-06-01T15:04:43Z'>
	<summary>Memory leaking when using large numpy array in Dataset</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Thank you for the great library! When migrating a larger project, I am running into memory issues, though, so maybe someone can help me out.
So, I have a pretty complicated DataSet which loads plenty of data and buffers the data into the CPU RAM as a numpy array.
I train using ddp and with num_workers = 6 in the dataloader. The training crashes my machine, because of  CPU memory overflow. It works with num_workers = 0, but the higher the num_workers, the higher the memory consumption.
I figured out that this is much worse when using a large numpy array in the Dataset rather than a PyTorch tensor.
Unfortunately, I need numpy arrays, so I am asking you if there is anything I can do?
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

I created a repository to reproduce this. It allows you to train a model on toy data using either a PyTorch tensor or a numpy array in the Dataset.
When running it with the PyTorch tensor the same amount of data uses 5GB of RAM while with Numpy it uses more than 30GB of RAM.
The higher the number of num_workers, the higher the RAM usage - it seems to leak when using numpy?

Clone https://github.com/mpaepper/reproduce_pytorch_lightning_memory_issues
Try the PyTorch tensor with: python minimal.py --num_workers 10
Try the numpy array with: python minimal.py --numpy --num_workers 10
Compare the huge difference in memory consumption

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/mpaepper/reproduce_pytorch_lightning_memory_issues&gt;https://github.com/mpaepper/reproduce_pytorch_lightning_memory_issues&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

I would expect that numpy and PyTorch tensors should behave in the same way when using num_workers &gt; 0, i.e. memory consumption is similar.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:
- GPU:
- GeForce RTX 2080 Ti
- GeForce RTX 2080 Ti
- GeForce RTX 2080 Ti
- GeForce RTX 2080 Ti
- available:         True
- version:           10.1
Packages:
- numpy:             1.16.4
- pyTorch_debug:     False
- pyTorch_version:   1.4.0
- pytorch-lightning: 0.7.5
- tensorboard:       1.14.0
- tqdm:              4.46.0
System:
- OS:                Linux
- architecture:
- 64bit
-
- processor:         x86_64
- python:            3.7.3
- version:           #97-Ubuntu SMP Wed Apr 1 03:25:46 UTC 2020

	</description>
	<comments>
		<comment id='1' author='mpaepper' date='2020-05-08T17:15:07Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='mpaepper' date='2020-05-08T20:04:53Z'>
		I had a similar issue and if I recall defining environmental variable COLAB_GPU forces pytorch lightning to use fork, which might prevent this Nx memory blowup.
&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/trainer.py#L779&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/trainer.py#L779&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='mpaepper' date='2020-05-08T20:12:09Z'>
		Thank you for the answer, but it seems that that option only works for TPU training?
I am training on GPUs.
I tried it out anyways, but it didn't improve my situation. Any other pointers / ideas?
		</comment>
		<comment id='4' author='mpaepper' date='2020-05-11T07:59:10Z'>
		I tried to manually rewrite the PyTorch Lightning code to use fork instead of spawn, but then the error "Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method" comes up:
&lt;denchmark-code&gt;Process Process-1:
Traceback (most recent call last):
  File "/home/xxx/anaconda3/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
    self.run()
  File "/home/xxx/anaconda3/lib/python3.7/multiprocessing/process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
  File "/home/xxx/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py", line 345, in ddp_train
    torch.cuda.set_device(self.root_gpu)
  File "/home/xxx/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py", line 292, in set_device
    torch._C._cuda_setDevice(device)
  File "/home/xxx/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py", line 195, in _lazy_init
    "Cannot re-initialize CUDA in forked subprocess. " + msg)
RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='mpaepper' date='2020-05-11T20:43:29Z'>
		mind be similar to &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1769&gt;#1769&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='mpaepper' date='2020-05-19T10:04:11Z'>
		So for others running into this:
As a workaround, during __init__ I move everything from numpy to PyTorch tensors, so they are stored in RAM -&gt; then the shared memory works. When I use them, I transform them back from PyTorch to numpy (.detach().numpy()).
However, it might fail when you have large amounts of memories stored here, because the file limits of your operating system don't allow you to have enough files open.
Check out ulimit -n (was 1024 for me).
Setting it to a higher limit with ulimit -n 9999 then fixed the error and training works.
However, it still seems too slow. It's only half as fast as it was using Torchbearer before.
The more num_workers I use in the Dataloader, the slower the start of an epoch similar as described here in this issue: &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/discussions/1884&gt;#1884&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='mpaepper' date='2020-06-01T15:04:43Z'>
		&lt;denchmark-link:https://github.com/mpaepper&gt;@mpaepper&lt;/denchmark-link&gt;
 check again?
This should be fixed on master now
		</comment>
		<comment id='8' author='mpaepper' date='2020-06-03T06:12:22Z'>
		Yes, thank you. It's resolved with the recent master additions üëç
		</comment>
	</comments>
</bug>