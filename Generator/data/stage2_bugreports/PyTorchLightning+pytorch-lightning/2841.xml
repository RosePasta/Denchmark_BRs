<bug id='2841' author='wxc-kumar' open_date='2020-08-05T22:29:00Z' closed_time='2020-09-14T08:03:41Z'>
	<summary>Custom Logger Missing 1st Epoch Training Loss Results</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Following the &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/loggers.html#custom-logger&gt;documentation on custom loggers&lt;/denchmark-link&gt;
, I've implemented a simple logger to collect the training and validation epoch losses in a simple dictionary. However, the training loss from the first epoch seems to be missing with a customer logger. See sample output below.
&lt;denchmark-h:h4&gt;lightning.logger.data if epochs == 1&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;{'val_loss': [2.609346866607666], 'epoch': [0]}
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;lightning.logger.data if epochs == 5&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;{'val_loss': [5.344592571258545,
  2.825099468231201,
  2.336381196975708,
  2.0197644233703613,
  1.9344301223754883],
 'epoch': [0, 0, 1, 1, 2, 2, 3, 3, 4],
 'train_loss': [3.918545961380005,
  2.8362650871276855,
  2.284677505493164,
  1.7846769094467163]}
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;


implement simple logger (sample below)
implement simple lightning module with epoch loss logging

&lt;denchmark-h:h4&gt;SimpleLogger&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;from pytorch_lightning.utilities import rank_zero_only
from pytorch_lightning.loggers import LightningLoggerBase

class SimpleLogger(LightningLoggerBase):
    
    def __init__(self):
        super().__init__()
        self.data = {}
    
    def experiment(self):
        return "simplelogger"

    @property
    def name(self):hparams.epochs
        return "simplelogger"

    @property
    def version(self):
        return "simplelogger"
    
    @rank_zero_only
    def log_hyperparams(self, params):
        pass

    @rank_zero_only
    def log_metrics(self, metrics, step):
        for metric, value in metrics.items():
            if metric not in self.data: self.data[metric] = []
            self.data[metric].append(value) # &lt;---- just save the value here
            
    def save(self):
        pass

    @rank_zero_only
    def finalize(self, status):
        pass
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Simple Module&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;class Model(pl.LightningModule):

    def __init__(self, hparams, model):
        super().__init__()
        self.hparams = hparams
        self.model = model

    def forward(self, x):
        x = self.model(x)
        return x

    #using TrainResult
    """ 
    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        
        result = pl.TrainResult(minimize=loss)
        result.log('train_loss', loss, on_step=False, on_epoch=True, logger=True, prog_bar=True) #&lt;----- 
        return result
    """

    # using dict
    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        return {'loss': loss}

    def training_epoch_end(self, training_step_outputs):
        epoch_loss = torch.stack([x['loss'] for x in training_step_outputs]).mean()
        return {
                    'log': {'train_loss': epoch_loss} #&lt;------ no results logged
               }
    
    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        
        result = pl.EvalResult(checkpoint_on=loss)
        result.log('val_loss', loss, on_step=False, on_epoch=True, logger=True, prog_bar=True)
        return result

    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)
        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, self.hparams.lr, steps_per_epoch=self.hparams.steps_per_epoch, epochs=self.hparams.epochs)
        scheduler = {"scheduler": scheduler, "interval" : "step" }
        return [optimizer], [scheduler]
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Train&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;lightning = Model(hparams, models.resnet18())
trainer = pl.Trainer(gpus=1, max_epochs=hparams.epochs, logger=SimpleLogger(), row_log_interval=1)
trainer.fit(lightning, train, val)
lightning.logger.data
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Training epoch logging works correctly for custom loggers
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:
- GPU:
- GeForce RTX 2070
- available:         True
- version:           10.1
Packages:
- numpy:             1.18.5
- pyTorch_debug:     False
- pyTorch_version:   1.5.1
- pytorch-lightning: 0.9.0rc2 // also affects stable v0.8.5
- tensorboard:       2.2.2
- tqdm:              4.46.1
System:
- OS:                Linux
- architecture:
- 64bit
-
- processor:         x86_64
- python:            3.7.7
- version:           #46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020

	</description>
	<comments>
		<comment id='1' author='wxc-kumar' date='2020-08-13T22:46:23Z'>
		Bumping as this issue is over a week old with no acknowledgement. I imagine Custom Loggers are a primary use case and this is currently blocking progress with Lightning. Is there a fix or suggested workaround for this? Am I doing something wrong? I'm temporarily using a logging dictionary stored in the model itself which captures the first epoch log, however this goes against the fundamental principles of Pytorch Lightning with regard to separation of concerns. Tagging some people for attention: &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='wxc-kumar' date='2020-08-13T22:51:38Z'>
		could you try on rc12?
		</comment>
		<comment id='3' author='wxc-kumar' date='2020-08-13T23:04:02Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
, just tried  with TrainResult/EvalResult and the issue was still present.
one epoch
&lt;denchmark-code&gt;{'val_loss': [0.9565523266792297], 'epoch': [0]}
&lt;/denchmark-code&gt;

five epochs
&lt;denchmark-code&gt;{'val_loss': [1.4622362852096558,
  1.0501596927642822,
  0.9127404093742371,
  0.7936658263206482,
  0.7396641969680786],
 'epoch': [0, 0, 1, 1, 2, 2, 3, 3, 4],
 'train_loss': [2.292189836502075,
  1.1138198375701904,
  0.836330235004425,
  0.6153180003166199]}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='wxc-kumar' date='2020-08-19T20:44:22Z'>
		Hello, is there an update on this issue? It's still present in pytorch-lightning==0.9.0rc16. My workaround of using a dictionary embedded in the model does not play well with auto_lr_find or num_sanity_val_steps as these are adding extra values to the dictionary making my logging and model unreliable.
		</comment>
		<comment id='5' author='wxc-kumar' date='2020-09-10T17:20:35Z'>
		&lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
 mind taking a look?
		</comment>
		<comment id='6' author='wxc-kumar' date='2020-09-11T10:17:47Z'>
		I have found the solution after digging around in the logger code.
Short answer: call super().save() when you define your own save (or don't override it).
Long answer:
First note that logger.log_metrics is not called directly. Instead it is the logger.agg_and_log_metrics (you should not implement this normally, but you can go around the hole aggregation ect by defining this yourself) method that is directly called by lightning which then calls log_metrics after doing some aggregation of metrics. The aggregation is necessary if a metric is tried logged multiple times with the same step. The save method of the base class calls the internal function self._finalize_agg_metrics() that finalizes the aggregating of metrics. Thus, for this hole thing to work, the easy solution is to simply call super().save() in your own implementation of save or directly call the self._finalize_agg_metrics() method in your save.
This is not at all clear from the documentation and we should fix it (easy fix, change  to  in the documentation and make a note that it is important to call).
&lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/loggers.html#make-a-custom-logger&gt;https://pytorch-lightning.readthedocs.io/en/latest/loggers.html#make-a-custom-logger&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>