<bug id='4808' author='itsikad' open_date='2020-11-22T15:31:36Z' closed_time='2020-11-22T20:42:10Z'>
	<summary>ModelCheckpoint ignores dirpath may result in permission denied error</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I set my model checkpoint callback as follows:
&lt;denchmark-code&gt;    ckpt_callback = ModelCheckpoint(
        filename='/{epoch}-{val_loss:.2f}',
        monitor='val_loss',
        save_last=True,
        mode='min'
        )
&lt;/denchmark-code&gt;

OR
&lt;denchmark-code&gt;    ckpt_callback = ModelCheckpoint(
        filename='/{epoch}-{val_avg_rmse:.2f}',
        monitor='val_loss',
        save_top_k=k,
        mode='min'
        )
&lt;/denchmark-code&gt;

and initialize the trainer dirpath with default os.getcwd().
In both cases, the a new directory is created in the correct path: proj_root/exp_name/exp_id/checkpoints, yet I get permission denied error since lightning tries to save the checkpoint in /&lt;file_name&gt;.ckpt instead of proj_root/exp_name/exp_id/checkpoints/&lt;file_name&gt;.ckpt, ignoring the dirpath.
Debug leads to inconsistent behavior (as far as I understand from the code/comments) in the following part of model_checkpoint.save_checkpoint():



pytorch-lightning/pytorch_lightning/callbacks/model_checkpoint.py


        Lines 236 to 246
      in
      8601268






 # ie: path/val_loss=0.5.ckpt 



 filepath = self._get_metric_interpolated_filepath_name(monitor_candidates, epoch, global_step) 



 



 # callback supports multiple simultaneous modes 



 # here we call each mode sequentially 



 # Mode 1: save all checkpoints OR only the top k 



 if self.save_top_k: 



 self._save_top_k_checkpoints(monitor_candidates, trainer, pl_module, filepath) 



 



 # Mode 2: save the last checkpoint 



 self._save_last_checkpoint(trainer, pl_module, monitor_candidates, filepath) 






`_get_metric_interpolated_filepath_name()' retrieves only the filename without the dirpath (as the comment above depicts)
self_save_top_k_checkpoints() doesn't add the dirpath internally whereas self._save_last_checkpoint does.
'self.save_top_k' is set to 1 if ModelCheckpoint is initialized with save_top_k=None and 'save_las=True' which means the first if condition is satisfied unless explicitly initializing ModelCheckpoint with `save_top_k=0.

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;


Save checkpoints to expected path.
I think  save_last should be treated in the second if statement (mode 2) instead of the first one (mode 1, save_top_k).

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version (e.g., 1.0): 1.6.0
PyTorch Lightning Version (e.g., 1.0): 1.0.7
OS (e.g., Linux): Ubuntu 18.04
How you installed PyTorch (conda, pip, source): pip
Python version: 3.6.9
CUDA/cuDNN version: 10.2
GPU models and configuration: 2 x RTX2080Ti

	</description>
	<comments>
		<comment id='1' author='itsikad' date='2020-11-22T15:50:46Z'>
		filename='/{epoch}-{val_loss:.2f}' should be filename='{epoch}-{val_loss:.2f}'??
		</comment>
		<comment id='2' author='itsikad' date='2020-11-22T15:56:03Z'>
		 and are not allowed in filename.
&lt;denchmark-link:https://github.com/itsikad&gt;@itsikad&lt;/denchmark-link&gt;
 You mention an inconsistent behaviour, how can it be reproduced (aside from the illegal character in filename)?
		</comment>
		<comment id='3' author='itsikad' date='2020-11-22T16:11:02Z'>
		&lt;denchmark-link:https://github.com/rohitgr7&gt;@rohitgr7&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 Thanks! can't believe I missed this and spent couple of hours debugging it.
This solves the first two points.
the third still occurs. To reproduce just initialize a ModelCheckpoint with save_last=True only (leave save_top_k to default) and observing self.save_top_k in model_checpoint.save_chekcpoint(). Notice that it is set to 1 so both if statements (mode 1 and mode 2 in the code above) are satisfied (checkpoint is saved twice).
		</comment>
		<comment id='4' author='itsikad' date='2020-11-22T19:51:35Z'>
		If save_top_k=None (default), it will be set to 1 only if a monitor is given else it won't be changed. The point here is if someone specifies the monitor key and set save_top_k=None, it won't make sense.
Also, save_last should have nothing to do with the monitor key IMO.
These lines seem a bit wrong to me.



pytorch-lightning/pytorch_lightning/callbacks/model_checkpoint.py


        Lines 253 to 264
      in
      8601268






 if self.monitor is None: 



 # None: save last epoch, -1: save all epochs, 0: nothing is saved 



 if self.save_top_k not in [None, -1, 0]: 



 raise MisconfigurationException( 



 f'ModelCheckpoint(save_top_k={self.save_top_k}, monitor=None) is not a valid' 



 ' configuration. No quantity for top_k to track.' 



         ) 



 if self.save_last: 



 rank_zero_warn( 



 'ModelCheckpoint(save_last=True, monitor=None) is a redundant configuration.' 



 ' You can save the last checkpoint with ModelCheckpoint(save_top_k=None, monitor=None).' 



         ) 





		</comment>
		<comment id='5' author='itsikad' date='2020-11-22T20:21:26Z'>
		&lt;denchmark-link:https://github.com/rohitgr7&gt;@rohitgr7&lt;/denchmark-link&gt;
 regarding save_last, a discussion is already happening here: &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/4335&gt;#4335&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='itsikad' date='2020-11-22T20:30:43Z'>
		thanks &lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='itsikad' date='2020-11-22T20:42:10Z'>
		Thanks &lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/rohitgr7&gt;@rohitgr7&lt;/denchmark-link&gt;
 for your help.
Closing this issue as there's an active discussion on the remaining issue.
		</comment>
	</comments>
</bug>