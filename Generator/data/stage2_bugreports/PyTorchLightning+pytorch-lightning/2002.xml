<bug id='2002' author='pvnieo' open_date='2020-05-29T16:31:18Z' closed_time='2020-05-31T23:06:36Z'>
	<summary>Error using Hydra</summary>
	<description>
Hi all,
Thanks a lot for the awesome library. I'm trying to use Hydra with pytorch-lightning. I'm using the last release of pytorch-lightning.
However, I got the following error after my training step:
&lt;denchmark-code&gt;...
  File "/home/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 241, in on_validation_end
    self._do_check_save(filepath, current, epoch)
  File "/home/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 275, in _do_check_save
    self._save_model(filepath)
  File "/home/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 142, in _save_model
    self.save_function(filepath)
  File "/home/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_io.py", line 260, in save_checkpoint
    checkpoint = self.dump_checkpoint()
  File "/home/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_io.py", line 353, in dump_checkpoint
    raise ValueError(
ValueError: ('The acceptable hparams type is dict or argparse.Namespace,', ' not DictConfig')
Exception ignored in: &lt;function tqdm.__del__ at 0x7f1f9e379a60&gt;
&lt;/denchmark-code&gt;

It says that Dictconfig are not supported for saving, but I saw in a pull request that this problem has been corrected.
Can you point me to direction on how to correct this?
&lt;denchmark-h:h4&gt;Code&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;@hydra.main("config/config.yaml")
def main(cfg=None):
    wrap_tb_logger()
    model = hydra.utils.instantiate(cfg.model, cfg)

    trainer = pl.Trainer(
        gpus=list(cfg.gpus),
        max_epochs=cfg.epochs,
        train_percent_check=0.4
    )

    trainer.fit(model)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;What's your environment?&lt;/denchmark-h&gt;


OS: Linux
Packaging pip
Version 0.7.1

	</description>
	<comments>
		<comment id='1' author='pvnieo' date='2020-05-29T16:31:56Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='pvnieo' date='2020-05-30T12:06:26Z'>
		&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 can you add this fix?
		</comment>
		<comment id='3' author='pvnieo' date='2020-05-31T18:46:15Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 I solved this problem by converting  to a flat dictionary after I initialized my model.
&lt;denchmark-code&gt;import omegaconf

def _to_dot_dict(cfg):
    res = {}
    for k, v in cfg.items():
        if isinstance(v, omegaconf.DictConfig):
            res.update(
                {k + "." + subk: subv for subk, subv in _to_dot_dict(v).items()}
            )
        elif isinstance(v, (str, int, float, bool)):
            res[k] = v

    return res
&lt;/denchmark-code&gt;

I admit this is only a detour waiting for the feature to be released!
		</comment>
		<comment id='4' author='pvnieo' date='2020-05-31T19:09:04Z'>
		&lt;denchmark-link:https://github.com/pvnieo&gt;@pvnieo&lt;/denchmark-link&gt;
 check master?
i pushed a fix last night
		</comment>
		<comment id='5' author='pvnieo' date='2020-05-31T19:31:11Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 Yes thank you, I just tested with the last dev version and It's working!
However, I got this warning, and I don't know where is it called!
&lt;denchmark-code&gt;UserWarning: you called `module.module_arguments` without calling self.auto_collect_arguments()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='pvnieo' date='2020-05-31T19:36:39Z'>
		we can just disable it haha. submit a PR?
		</comment>
		<comment id='7' author='pvnieo' date='2020-05-31T19:40:40Z'>
		Sorry, I submit a PR for what?
		</comment>
		<comment id='8' author='pvnieo' date='2020-05-31T19:50:21Z'>
		disabling the warning
		</comment>
		<comment id='9' author='pvnieo' date='2020-05-31T19:59:38Z'>
		Ok.
I just discovered that (I don't know if it's related the last dev version, but I hadn't this problem before) when I kill my program using Ctrl+C, the process isn't killed and it continue training (before it's says that it's trying "to kill the process gracefully" or sth like this).
I found in &lt;denchmark-link:https://stackoverflow.com/questions/11815947/cannot-kill-python-script-with-ctrl-c/11816038&gt;this&lt;/denchmark-link&gt;
 stack over flow question that it's maybe related to using multiple threads and the killing isn't handled well.
Is this a known issue?
		</comment>
		<comment id='10' author='pvnieo' date='2020-05-31T22:33:56Z'>
		this is fixed in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2029&gt;#2029&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>