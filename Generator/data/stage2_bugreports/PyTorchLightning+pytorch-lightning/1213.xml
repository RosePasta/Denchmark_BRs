<bug id='1213' author='Ir1d' open_date='2020-03-23T11:26:40Z' closed_time='2020-07-10T01:27:31Z'>
	<summary>Testing in dp mode uses only one of the GPUs</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:
Run a test without training
&lt;denchmark-link:https://user-images.githubusercontent.com/10709657/77311956-1706ac00-6d3c-11ea-89fe-3cf2156babe2.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

Modified from the conference-seed repo
trainer = Trainer(
            gpus="-1",
            distributed_backend='dp',
        )
trainer.test(model)
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


pl version: 0.6.0
PyTorch Version (e.g., 1.0): 1.2
OS (e.g., Linux): Ubuntu
How you installed PyTorch (conda, pip, source): pip
Build command you used (if compiling from source):
Python version: 3.6
CUDA/cuDNN version: 10.1
GPU models and configuration:
Any other relevant information:

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='Ir1d' date='2020-03-23T11:29:51Z'>
		ummm yeah, that's a bug. it should run via dp. &lt;denchmark-link:https://github.com/Ir1d&gt;@Ir1d&lt;/denchmark-link&gt;
 want to submit a PR?
&lt;denchmark-link:https://github.com/orgs/PyTorchLightning/teams/core-contributors&gt;@PyTorchLightning/core-contributors&lt;/denchmark-link&gt;
 any one else experience this?
		</comment>
		<comment id='2' author='Ir1d' date='2020-03-23T11:31:21Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 I tried wrapping the model in  like the training loop did, but it tells me that LightningDataParallel object doen't have a . How do I debug this?
		</comment>
		<comment id='3' author='Ir1d' date='2020-03-23T11:40:28Z'>
		you wouldn‚Äôt wrap it yourself ever haha.
the trainer does the wrapping for you.
the trainer needs to be modified to run the test on the correct method when done this way
		</comment>
		<comment id='4' author='Ir1d' date='2020-03-23T11:46:52Z'>
		I was trying to wrap it in evaluate in pytorch_lightning/trainer/evaluation_loop.py . Do you have any idea where to wrap this func?
		</comment>
		<comment id='5' author='Ir1d' date='2020-03-23T11:48:20Z'>
		Anyway, we've find one possible workround here:
After defining a torch model, and before sending it into PL model, wrap it with nn.dataparallel.
		</comment>
		<comment id='6' author='Ir1d' date='2020-03-23T12:09:23Z'>
		evaluate is private... you're not meant to call it directly.
call .test()
		</comment>
		<comment id='7' author='Ir1d' date='2020-03-23T12:15:31Z'>
		lightning does the wrapping by itself...
the fact that this doesn't work, is a bug.
model = MyLightningModule.load_from_checkpoint(...)
trainer = Trainer(
            gpus="-1",
            distributed_backend='dp',
        )
trainer.test(model)
The bug needs to be addressed correctly.
It's weird because we have tests for this... double check that this is really not working for you.
		</comment>
		<comment id='8' author='Ir1d' date='2020-03-23T12:38:02Z'>
		
evaluate is private... you're not meant to call it directly.
call .test()

so let's rename it starting with _ to be clear that it is private from it name
		</comment>
		<comment id='9' author='Ir1d' date='2020-03-23T12:41:39Z'>
		I was calling .test and its not working
		</comment>
		<comment id='10' author='Ir1d' date='2020-03-27T12:30:40Z'>
		&lt;denchmark-link:https://github.com/neggert&gt;@neggert&lt;/denchmark-link&gt;
 may you have look at this multi GPU issue?
		</comment>
		<comment id='11' author='Ir1d' date='2020-06-08T12:41:21Z'>
		&lt;denchmark-link:https://github.com/neggert&gt;@neggert&lt;/denchmark-link&gt;
 ping :)
		</comment>
		<comment id='12' author='Ir1d' date='2020-06-26T13:46:56Z'>
		looking at this with next sprint
		</comment>
		<comment id='13' author='Ir1d' date='2020-07-10T01:27:31Z'>
		fixed! (0.8.5)
		</comment>
	</comments>
</bug>