<bug id='1465' author='Giuseppe5' open_date='2020-04-12T12:08:25Z' closed_time='2020-06-24T12:04:25Z'>
	<summary>Weight Initialization blocks learning</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When trying to perform a custom weight initialization, such as xavier_uniform_ or orthogonal_ from torch.nn.init, the weights are kept fixed and not updated during backpropagation.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Create a simple model, even with just FC layers and ReLU
Initialize the weight with torch.nn.init.xavier_uniform_ or another method
Start Training
Weights are not getting updated

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

I'd expect the initialization not to block the training process
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;packages:
numpy:               1.17.2
pyTorch_debug:       False
pyTorch_version:     1.3.0
pytorch-lightning:   0.7.3
tensorboard:         2.0.0
tqdm:                4.45.0
system:
OS:                  Linux
architecture:
64bit
processor:           x86_64
python:              3.6.9
version:             #163-Ubuntu SMP Mon Sep 24 13:14:43 UTC 2018
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Giuseppe5' date='2020-04-12T12:09:05Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='Giuseppe5' date='2020-04-15T00:16:33Z'>
		Do you have any code to share? I couldn't repro
		</comment>
		<comment id='3' author='Giuseppe5' date='2020-06-24T08:43:07Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
		<comment id='4' author='Giuseppe5' date='2020-06-24T12:04:25Z'>
		Your model/algorithm might just be sensitive to the initialization. I just tried the template model from this repo (gpu_template.py) and tried different initializations in the __init__() of the template model, e.g.,
torch.nn.init.uniform_(self.c_d2.weight, -1, 1)
torch.nn.init.uniform_(self.c_d1.weight, -1, 1)
torch.nn.init.uniform_(self.c_d1.bias, -1, 1)
torch.nn.init.uniform_(self.c_d2.bias, -1, 1)
and it seems all of the methods I tried are worse than the default one used by PyTorch but if I wait long enough, the loss goes down to zero.
Feel free to re-open if you find evidence that this is a problem with PL.
		</comment>
	</comments>
</bug>