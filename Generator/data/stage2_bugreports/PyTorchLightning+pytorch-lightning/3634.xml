<bug id='3634' author='djrmarques' open_date='2020-09-23T22:46:59Z' closed_time='2020-09-25T20:03:31Z'>
	<summary>AttributeError: 'dict' object has no attribute 'callback_metrics' when using validation_epoch_end callbac</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Hello.  I am  trying to setup the early stop callback, and according to the warning that I get I need to use the validation_epoch_end callback. When I do that, I get the following error:
&lt;denchmark-code&gt;AttributeError                            Traceback (most recent call last)

&lt;ipython-input-25-9cbc4363b76a&gt; in &lt;module&gt;()
     10 
     11 # Train the model ‚ö°
---&gt; 12 trainer.fit(model)

10 frames

/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/evaluation_loop.py in __update_callback_metrics(self, eval_results, using_eval_result)
    419             if isinstance(eval_results, list):
    420                 for eval_result in eval_results:
--&gt; 421                     self.callback_metrics = eval_result.callback_metrics
    422             else:
    423                 self.callback_metrics = eval_results.callback_metrics

AttributeError: 'dict' object has no attribute 'callback_metrics'
&lt;/denchmark-code&gt;

class MyNN(pl.LightningModule):
    def __init__(self, input_size=3, seq_len=107, pred_len=68, hidden_size=50, num_layers=1, dropout=0, lr=1e-2):
        super().__init__()
        
        self.pred_len = pred_len
        
        self.lr = lr
        
        self.rnn = nn.LSTM(
            input_size=input_size, 
            hidden_size=hidden_size, 
            num_layers=num_layers, 
            dropout=dropout, 
            bidirectional=True,
            batch_first=True
        )
        
        self.linear = nn.Linear(hidden_size*2, 5)

        self.example_input_array = torch.Tensor(np.zeros(input_size).reshape(1, 1, -1))
    
    def forward(self, X):
        lstm_output, (hidden_state, cell_state) = self.rnn(X)
        
        labels = self.linear(lstm_output[:, :self.pred_len, :])
        
        return labels
    
    def training_step(self, batch, batch_nb):
        x, y = batch
        loss = scoring(self(x.float()), y.float())


        result = pl.TrainResult(minimize=loss)
        result.log('train_loss', loss, logger=True)
        return result
    
    def validation_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x.float())
        loss = scoring(logits, y)

        result = pl.EvalResult(checkpoint_on=loss)
        result.log('val_loss', loss)
        return result

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=self.lr)
    
    def val_dataloader(self):
        return DataLoader(MyValSet(), batch_size=64)
    
    def train_dataloader(self):
        return DataLoader(MyDataset(), batch_size=64, shuffle=True)


    def training_epoch_end(self, outputs):
        #  the function is called after every epoch is completed

        # calculating average loss  
        avg_loss = outputs["train_loss"].mean()

        # creating log dictionary
        tensorboard_logs = {'train_loss': avg_loss}

        epoch_dictionary={
            # required
            'train_loss': avg_loss,
            
            # for logging purposes
            'log': tensorboard_logs}

        return epoch_dictionary

    def validation_epoch_end(self, outputs):
        #  the function is called after every epoch is completed

        # calculating average loss  
        avg_loss = outputs["val_loss"].mean()

        tensorboard_logs = {'val_loss': avg_loss}
        
        epoch_dictionary={
            # required
            'val_loss': avg_loss,
            
            # for logging purposes
            'log': tensorboard_logs}

        return epoch_dictionary
and my training loop is:
LEARNING_RATE = 1e-3
NUM_LAYERS = 2
DROPOUT = 0.1
HIDDEN_SIZE = 100
EPOCHS = 100

# Initialize a trainer
trainer = pl.Trainer(gpus=1, max_epochs=EPOCHS, progress_bar_refresh_rate=20, early_stop_callback=True, auto_lr_find=True)
model = MyNN(num_layers=NUM_LAYERS, dropout=DROPOUT)

# Train the model ‚ö°
trainer.fit(model)
I am using Google Colab, with the following versions:
&lt;denchmark-code&gt;pytorch-lightning==0.9.0
torch==1.6.0+cu101
&lt;/denchmark-code&gt;

Am I doing something wrong, or what is the issue here?
Thank you! :)
	</description>
	<comments>
		<comment id='1' author='djrmarques' date='2020-09-23T22:47:45Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='djrmarques' date='2020-09-24T22:34:42Z'>
		Pretty sure the problem is, you cannot mix results objects in your step methods with dict in your epoch_end methods.
Use either dicts everywhere or results everywhere, but not both. Let me know if that solves your problem.
You mention colab, if you need further help, mind sharing the colab link so we can have a look help you better.
		</comment>
		<comment id='3' author='djrmarques' date='2020-09-24T22:53:24Z'>
		oh, sorry, did not read the first sentence in your message.
For early stopping, use this
    def validation_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x.float())
        loss = scoring(logits, y)

        result = pl.EvalResult(checkpoint_on=loss, early_stop_on=loss)  # &lt;--- add this
        result.log('val_loss', loss)
        return result
There should be no need for the validation_epoch_end, you can savely remove it. the validation loss will be reduced and logged automatically at the end of epoch.
		</comment>
		<comment id='4' author='djrmarques' date='2020-09-25T20:03:31Z'>
		So I ended up putting it all into dictionaries and it worked.
But before that I tried to use the results object in both the validation and train set, and the model was running, but for some reason the train_loss was not logging on tensorboard, but maybe I was doing something wrong. I will leave it for now like this, because I want to finish my model, but after that I will try to set up the results object and see if all goes well.
Thank you!
		</comment>
	</comments>
</bug>