<bug id='3103' author='nick-fung' open_date='2020-08-22T15:13:37Z' closed_time='2020-09-01T18:15:12Z'>
	<summary>Auto scaling batch fails if initial batch size is too large</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

If model.hparams.batch_size is too large and the call to model.fit() fails on the first iteration of binary search scaling, it fails.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Create a large model.
Set model.hparams.batch_size to be a large number, such that it does not fit on your GPU.
See the following error:

 File "/home/nick/anaconda3/envs/ml/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 951, in fit
    self.scale_batch_size(model, mode=self.auto_scale_batch_size)
  File "/home/nick/anaconda3/envs/ml/lib/python3.7/site-packages/pytorch_lightning/trainer/training_tricks.py", line 164, in scale_batch_size
    new_size = _run_binsearch_scaling(self, model, new_size, batch_arg_name, max_trials)
  File "/home/nick/anaconda3/envs/ml/lib/python3.7/site-packages/pytorch_lightning/trainer/training_tricks.py", line 312, in _run_binsearch_scaling
    midval = (high + low) // 2
UnboundLocalError: local variable 'low' referenced before assignment
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

It should handle this initial case properly and perform binary search from with low=0 and high=initial batch size.
If no batch size is possible on with the given memory, it should warn the user.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:
- GPU:
- GeForce GTX 1080 Ti
- available:         True
- version:           10.0.130
Packages:
- numpy:             1.18.1
- pyTorch_debug:     False
- pyTorch_version:   1.3.1
- pytorch-lightning: 0.8.5
- tensorboard:       2.3.0
- tqdm:              4.47.0
System:
- OS:                Linux
- architecture:
- 64bit
- ELF
- processor:         x86_64
- python:            3.7.6
- version:           #58-Ubuntu SMP Fri Jul 10 19:33:51 UTC 2020

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

I'm trying to train a large NLP model. So even very small batch sizes can fail initially.
	</description>
	<comments>
		<comment id='1' author='nick-fung' date='2020-08-22T15:14:13Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='nick-fung' date='2020-08-25T08:30:18Z'>
		&lt;denchmark-link:https://github.com/nick-fung&gt;@nick-fung&lt;/denchmark-link&gt;
 good founding, mind sending a PR with an adjustment? 
cc: &lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/justusschock&gt;@justusschock&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='nick-fung' date='2020-09-01T15:13:40Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='nick-fung' date='2020-09-01T15:33:59Z'>
		this is strange, since the initial batch size should always be set to  which have a default value of 2:


&lt;denchmark-link:https://github.com/nick-fung&gt;@nick-fung&lt;/denchmark-link&gt;
 do you have a colab notebook where I can reproduce the error?
		</comment>
		<comment id='5' author='nick-fung' date='2020-09-01T18:15:12Z'>
		on master it works (gpu_template.py, it uses hparams as in your case), I tested first that I get OOM without running the auto scale. Setting auto_scale_batch_size=True (and on master we need to call trainer.tune() now) it searches through all sizes starting from 2.
from your PR description I see you use 0.8.5, we fixed many things since then, so if you upgrade to 0.9.1rc1 I'm sure it will work.
Closing this for now.
		</comment>
	</comments>
</bug>