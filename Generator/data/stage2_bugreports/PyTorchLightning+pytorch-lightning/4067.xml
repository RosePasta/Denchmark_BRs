<bug id='4067' author='xiadingZ' open_date='2020-10-11T02:48:02Z' closed_time='2020-10-17T14:57:34Z'>
	<summary>checkpoint callback not monitor correct value</summary>
	<description>
pytorch-lightning version 1.0.0rc2
This is my checkpoint callback:
&lt;denchmark-code&gt;    checkpoint_callback = ModelCheckpoint(
        monitor='val_acc',
        mode='max',
    )
&lt;/denchmark-code&gt;

this is my validation_step:
&lt;denchmark-code&gt;        ...
        self.log('val_loss', loss, on_epoch=True, sync_dist=True)
        self.log('val_pacc', part_acc, on_epoch=True, sync_dist=True)
        self.log('val_acc', vid_acc, on_epoch=True, prog_bar=True, sync_dist=True)



        return {'val_loss': loss, "val_pacc": part_acc, "val_acc": vid_acc}
&lt;/denchmark-code&gt;

my tensorboard shows my max val_acc is at epoch 39, but checkpoint saved model is epoch=67.
I run two experiments, but all saved models are at wrong epoch.
Maybe monitor value is not sync_disted, but tensorboard's value is already sync_dist?
	</description>
	<comments>
		<comment id='1' author='xiadingZ' date='2020-10-11T04:11:05Z'>
		Could you verify this with the template model for bug reports here:
&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py&lt;/denchmark-link&gt;

and paste the code here?
You can also set verbose=True in ModelCheckpoint to see the values that the callback receives when saving the checkpoint
		</comment>
		<comment id='2' author='xiadingZ' date='2020-10-15T03:57:45Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 ï¼Œi verify this with the template &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py&gt;model(),&lt;/denchmark-link&gt;
 and my run_test code is
&lt;denchmark-code&gt;    def validation_step(self, batch, batch_idx):
        output = self.layer(batch)
        loss = self.loss(batch, output)
        self.log('val_loss', loss)
        self.log('test_batch_idx', batch_idx)
        return {"x": loss}

# fake data
train_data = torch.utils.data.DataLoader(RandomDataset(32, 64))
val_data = torch.utils.data.DataLoader(RandomDataset(32, 64))
test_data = torch.utils.data.DataLoader(RandomDataset(32, 64))

save_model_callback = callbacks.ModelCheckpoint(
    filepath=os.getcwd() + '/models/{epoch:002d}-{val_loss:.4f}--{test_batch_idx:0.0f}',
    save_last=True,
    save_top_k=1,
    monitor='val_loss',
    mode='min',
    verbose=True,
)

# model
model = TestModel()
trainer = Trainer(
    default_root_dir=os.getcwd(),
    limit_train_batches=1.0,
    limit_val_batches=1.0,
    max_epochs=10,
    weights_summary=None,
    checkpoint_callback=save_model_callback
)
trainer.fit(model, train_data, val_data)
trainer.test(test_dataloaders=test_data)
&lt;/denchmark-code&gt;

but when i check my log, i found out the value test_batch_idx is always 63, like this
&lt;denchmark-link:https://user-images.githubusercontent.com/31505323/96074857-464d5800-0edc-11eb-8187-837ed49bc300.png&gt;&lt;/denchmark-link&gt;

but model's name is correct like this
&lt;denchmark-link:https://user-images.githubusercontent.com/31505323/96075106-e6a37c80-0edc-11eb-83a5-130c695c7da6.png&gt;&lt;/denchmark-link&gt;

I think this the same bug as discussed in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/4141&gt;#4141&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='xiadingZ' date='2020-10-17T14:57:33Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/5495193/96341245-55c5d000-1099-11eb-8f38-9c696fa2f0e6.png&gt;&lt;/denchmark-link&gt;

self.log computes the average of the metric at the end of the epoch. So when you log the test_batch_idx, it will always compute the average of all values 0,1,2,3,4....,n where n is the number of test_steps.
In the attached image you see that when I change to self.log('test_batch_idx', batch_idx, on_step=True), I get individual plots for each epoch, and the batch_idx counter increases as it should.
if you want the step in your filename, I recommend to use self.global_step instead.
Let me know if you have any further questions.
		</comment>
	</comments>
</bug>