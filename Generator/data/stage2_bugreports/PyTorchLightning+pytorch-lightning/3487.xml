<bug id='3487' author='Tim-Chard' open_date='2020-09-13T11:27:42Z' closed_time='2020-09-15T16:41:28Z'>
	<summary>Gradient norms are not logged unless row_log_interval==1</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

In version 0.9 the guards to calculate the gradient norms and then log the metrics can't be satisfied in the same batch unless the row_log_interval  is 1. In most places the guard seems to be (batch_idx + 1) % self.row_log_interval == 0 such as here:



pytorch-lightning/pytorch_lightning/trainer/training_loop.py


        Lines 749 to 757
      in
      b40de54






 def save_train_loop_metrics_to_loggers(self, batch_idx, batch_output): 



 # when metrics should be logged 



 should_log_metrics = (batch_idx + 1) % self.row_log_interval == 0 or self.should_stop 



 if should_log_metrics or self.fast_dev_run: 



 # logs user requested information to logger 



 metrics = batch_output.batch_log_metrics 



 grad_norm_dic = batch_output.grad_norm_dic 



 if len(metrics) &gt; 0 or len(grad_norm_dic) &gt; 0: 



 self.log_metrics(metrics, grad_norm_dic) 





However in run_batch_backward_pass it is batch_idx % self.row_log_interval == 0



pytorch-lightning/pytorch_lightning/trainer/training_loop.py


        Lines 929 to 939
      in
      b40de54






 def run_batch_backward_pass(self, split_batch, batch_idx, opt_idx, optimizer): 



 # ------------------ 



 # GRAD NORMS 



 # ------------------ 



 # track gradient norms when requested 



 grad_norm_dic = {} 



 if batch_idx % self.row_log_interval == 0: 



 if float(self.track_grad_norm) &gt; 0: 



 model = self.get_model() 



 grad_norm_dic = model.grad_norm( 



 self.track_grad_norm) 





&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Run the code sample below (taken from #1527 ).
Confirm that gradients are not being logged in tensorboard.
Change row_log_interval to 1 and rerun the code.
4 Confirm that gradients are now being logged.

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import pytorch_lightning as pl
from torch.utils.data import TensorDataset, DataLoader
from pytorch_lightning.loggers.tensorboard import TensorBoardLogger
from torch.optim import SGD
import torch.nn as nn
import torch


class MWENet(pl.LightningModule):
    def __init__(self):
        super(MWENet, self).__init__()

        self.first = nn.Conv2d(1, 1, 3)
        self.second = nn.Conv2d(1, 1, 3)
        self.loss = nn.L1Loss()

    def train_dataloader(self):
        xs, ys = torch.zeros(16, 1, 10, 10), torch.ones(16, 1, 6, 6)
        ds = TensorDataset(xs, ys)
        return DataLoader(ds)

    def forward(self, xs):
        out = self.first(xs)
        out = self.second(out)
        return out

    def configure_optimizers(self):
        first = SGD(self.first.parameters(), lr=0.01)
        second = SGD(self.second.parameters(), lr=0.01)
        return [second, first]

    def training_step(self, batch, batch_idx, optimizer_idx):
        xs, ys = batch
        out = self.forward(xs)
        return {'loss': self.loss(out, ys)}


net = MWENet()
logger = TensorBoardLogger('tb_logs', name='testing')
trainer = pl.Trainer(
    track_grad_norm=2,
    row_log_interval=2,
    max_epochs=50,
    logger=logger)
trainer.fit(net)

&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Gradients should be logged if track_grad_norm is True
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:
- GPU:
- GeForce GTX 1080 Ti
- available:         True
- version:           10.2
Packages:
- numpy:             1.19.1
- pyTorch_debug:     False
- pyTorch_version:   1.6.0
- pytorch-lightning: 0.9.0
- tensorboard:       2.2.1
- tqdm:              4.48.2
System:
- OS:                Windows
- architecture:
- 64bit
- WindowsPE
- processor:         AMD64 Family 23 Model 113 Stepping 0, AuthenticAMD
- python:            3.7.9
- version:           10.0.19041

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='Tim-Chard' date='2020-09-13T11:28:24Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='Tim-Chard' date='2020-09-13T11:44:15Z'>
		yeah, it's a bug, mind send a PR?
		</comment>
	</comments>
</bug>