<bug id='371' author='hvy' open_date='2019-10-15T07:50:40Z' closed_time='2019-10-18T08:18:06Z'>
	<summary>Number of batches is off-by-one during training</summary>
	<description>
Describe the bug
The number of mini batches iterated over during an epoch seems to be off-by-one during training when passing a train_percent_check &lt; 1.0. More specifically, it trains for one additional iteration.
The operator in this line should probably be changed from  to .
&lt;denchmark-link:https://github.com/williamFalcon/pytorch-lightning/blob/master/pytorch_lightning/trainer/trainer.py#L1130&gt;https://github.com/williamFalcon/pytorch-lightning/blob/master/pytorch_lightning/trainer/trainer.py#L1130&lt;/denchmark-link&gt;

Evaluation by the way seems correct as seen in the following code.
&lt;denchmark-link:https://github.com/williamFalcon/pytorch-lightning/blob/master/pytorch_lightning/trainer/trainer.py#L614&gt;https://github.com/williamFalcon/pytorch-lightning/blob/master/pytorch_lightning/trainer/trainer.py#L614&lt;/denchmark-link&gt;

This leads to unexpected behaviors in e.g. EarlyStopping since the log that it's given is the callback log from the training and not the evaluation (the additional iteration overwrites the evaluation log with the training log).
To Reproduce

Specify some train_percent_check &lt; 1.0 e.g 0.5.
Fit.
Notice that it trains for an additional iteration.

Expected behavior
It should not train for that additional iteration.
	</description>
	<comments>
		<comment id='1' author='hvy' date='2019-10-16T13:58:13Z'>
		are you sure itâ€™s not a rounding error?
If it is indeed an error, want to take a stab at a PR?
		</comment>
	</comments>
</bug>