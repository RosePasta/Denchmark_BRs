<bug id='1381' author='ZhaofengWu' open_date='2020-04-04T23:52:55Z' closed_time='2020-09-30T20:08:49Z'>
	<summary>FP16 x gradient clipping</summary>
	<description>
According to &lt;denchmark-link:https://nvidia.github.io/apex/advanced.html&gt;apex docs&lt;/denchmark-link&gt;
, we should have a separate case for gradient clipping under fp16. But it seems not to be the case in pytorch-lightning. Is that correct? 

	</description>
	<comments>
		<comment id='1' author='ZhaofengWu' date='2020-04-04T23:53:34Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='ZhaofengWu' date='2020-04-05T07:02:10Z'>
		&lt;denchmark-link:https://github.com/neggert&gt;@neggert&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tullie&gt;@tullie&lt;/denchmark-link&gt;
 pls ^^
		</comment>
		<comment id='3' author='ZhaofengWu' date='2020-04-05T10:07:43Z'>
		Should we just fix this when we integrate pytorch mixed precision training &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1337#issue-592324058&gt;#1337 (comment)&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='4' author='ZhaofengWu' date='2020-04-05T10:16:50Z'>
		depends on what will be faster lol
		</comment>
		<comment id='5' author='ZhaofengWu' date='2020-04-05T15:25:04Z'>
		yeah, let's fix with pytorch mixed precision. We need to remove nvidia apex
		</comment>
		<comment id='6' author='ZhaofengWu' date='2020-04-06T18:32:58Z'>
		&lt;denchmark-link:https://pytorch.org/docs/master/notes/amp_examples.html#gradient-clipping&gt;https://pytorch.org/docs/master/notes/amp_examples.html#gradient-clipping&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='ZhaofengWu' date='2020-05-01T14:59:03Z'>
		This is still an issue in 0.7.5 and it's quite critical for training certain recurrent models where mixed precision can cause a 10x reduction in training time.
Can anyone confirm that this is going to be in the 0.7.6 release? We've been hoping it makes 0.7.3, 0.7.4, 0.7.5...
		</comment>
		<comment id='8' author='ZhaofengWu' date='2020-05-01T15:01:13Z'>
		&lt;denchmark-link:https://github.com/samgd&gt;@samgd&lt;/denchmark-link&gt;
 we already added this! you need to install the latest pytorch from nightly or a specific commit and lightning will use native amp.
it’s not us you’re waiting for haha, it’s pytorch 1.6. We’re already compatible with native amp. &lt;denchmark-link:https://github.com/mcarilli&gt;@mcarilli&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='ZhaofengWu' date='2020-05-01T15:04:48Z'>
		Okay, thanks.
Just to confirm this won't work with 0.7.5 and PyTorch 1.5? What does PyTorch 1.6 add over 1.5, I thought the latter already has native amp?
		</comment>
		<comment id='10' author='ZhaofengWu' date='2020-05-01T15:06:43Z'>
		1.6 does not use nvidia apex. it uses its internal version. 1.5 does not have native amp
		</comment>
		<comment id='11' author='ZhaofengWu' date='2020-05-01T15:13:45Z'>
		Awesome, looking forward to it!
		</comment>
		<comment id='12' author='ZhaofengWu' date='2020-05-01T16:25:10Z'>
		
@samgd we already added this! you need to install the latest pytorch from nightly or a specific commit and lightning will use native amp.
it’s not us you’re waiting for haha, it’s pytorch 1.6. We’re already compatible with native amp. @mcarilli

maybe we shall consider to fix it also for lower versions as we still keep back-compatibility...
		</comment>
		<comment id='13' author='ZhaofengWu' date='2020-05-01T21:33:07Z'>
		&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/2950f669834506f8e5845b318b0f25d52d19e331/pytorch_lightning/trainer/training_loop.py#L617&gt;unscale_ &lt;/denchmark-link&gt;
 before clipping was added in the native amp PR, good.  But it might need to be more complicated if there are multiple models and/or optimizers.  Does the call to &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/2950f669834506f8e5845b318b0f25d52d19e331/pytorch_lightning/trainer/training_loop.py#L618&gt;clip_gradients()&lt;/denchmark-link&gt;
 clip just the gradients owned by the optimizer referred to by  at that point, the gradients owned by a particular model, or all the gradients in the network?
		</comment>
		<comment id='14' author='ZhaofengWu' date='2020-07-29T21:13:45Z'>
		&lt;denchmark-link:https://github.com/mcarilli&gt;@mcarilli&lt;/denchmark-link&gt;
 mind checking if the issue is still on master?
		</comment>
		<comment id='15' author='ZhaofengWu' date='2020-09-08T16:18:28Z'>
		&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 still relevant?
		</comment>
		<comment id='16' author='ZhaofengWu' date='2020-09-30T20:08:49Z'>
		Verified clipping works with native fp16
		</comment>
		<comment id='17' author='ZhaofengWu' date='2020-09-30T20:13:57Z'>
		&lt;denchmark-link:https://github.com/teddykoker&gt;@teddykoker&lt;/denchmark-link&gt;
 do we have a test to prevent regressions?
		</comment>
		<comment id='18' author='ZhaofengWu' date='2020-09-30T20:16:22Z'>
		Don't believe so, I can add one
		</comment>
		<comment id='19' author='ZhaofengWu' date='2020-09-30T20:17:01Z'>
		it it's quick.
		</comment>
	</comments>
</bug>