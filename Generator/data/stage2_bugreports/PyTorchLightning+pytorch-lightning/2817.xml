<bug id='2817' author='bradezard131' open_date='2020-08-04T06:18:56Z' closed_time='2020-10-19T22:35:24Z'>
	<summary>Inconsistent handling of steps when using accumulate_grad_batches</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When using accumulate_grad_batches the steps are handled differently in the first epoch from any epoch later. Steps in the first epoch are counted for every batch regardless of accumulate_grad_batches, steps in later epochs are counted as one batch per accumulation (i.e. batch / accumulate_grad_batches). This makes learning rate scheduling, max iters, and logging very confusing
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Train any model using accumulate_grad_batches=2, check_val_every_n_epoch=1 and you'll see the iteration of the first eval is twice the difference between first and second.
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

It should be one or the other, and documented which it is. Doesn't really matter as long as it is consistent
	</description>
	<comments>
		<comment id='1' author='bradezard131' date='2020-08-04T17:21:53Z'>
		can you share a minimal example(colab) to check this issue?
		</comment>
		<comment id='2' author='bradezard131' date='2020-10-19T22:35:22Z'>
		Closing this for now, feel free to reopen if you can reproduce using &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py&gt;our base model example&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>