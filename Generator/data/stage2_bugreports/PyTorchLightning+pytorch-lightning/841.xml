<bug id='841' author='alexandrebone' open_date='2020-02-14T16:15:25Z' closed_time='2020-05-22T11:19:38Z'>
	<summary>Scheduler should be initialized after Apex is configured</summary>
	<description>
&lt;denchmark-h:h3&gt;Description&lt;/denchmark-h&gt;

Using jointly a scheduler and apex-amp throws a UserWarning:

Seems like optimizer.step() has been overridden after learning rate scheduler initialization. Please, make sure to call optimizer.step() before lr_scheduler.step().

As suggested here (&lt;denchmark-link:https://discuss.pytorch.org/t/cyclic-learning-rate-how-to-use/53796&gt;https://discuss.pytorch.org/t/cyclic-learning-rate-how-to-use/53796&lt;/denchmark-link&gt;
), the scheduler should be initialized after Apex is configured.
&lt;denchmark-h:h3&gt;To reproduce&lt;/denchmark-h&gt;

Run the MNIST example with use_amp=True.
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

No scheduler-related user warning.
&lt;denchmark-h:h3&gt;Workaround&lt;/denchmark-h&gt;

The following hack seems to work:
&lt;denchmark-code&gt;    def on_train_start(self):
        self.trainer.lr_schedulers = [optim.lr_scheduler.CosineAnnealingLR(self.trainer.optimizers[0], T_max=10)]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='alexandrebone' date='2020-02-22T23:28:57Z'>
		Good catch, &lt;denchmark-link:https://github.com/alexandrebone&gt;@alexandrebone&lt;/denchmark-link&gt;
 mind you send a PR with the fix? 
		</comment>
		<comment id='2' author='alexandrebone' date='2020-05-06T13:41:06Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
		<comment id='3' author='alexandrebone' date='2020-05-15T16:36:46Z'>
		Any news on this? This bug is still active.
		</comment>
		<comment id='4' author='alexandrebone' date='2020-05-15T16:47:05Z'>
		&lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
 pls ^^
		</comment>
		<comment id='5' author='alexandrebone' date='2020-05-15T17:24:15Z'>
		I will take a look at this. It will not be easy since we construct optimizers and schedulers at the same time currently. But maybe there is a workaround.
		</comment>
		<comment id='6' author='alexandrebone' date='2020-05-18T09:35:59Z'>
		The problem seems to be that the scheduler equips the optimizer.step() method
with a number of fields, that are lost after the optimizer is passed to apex. These
can be reconstructed by calling init method of base scheduler class (which all
schedulers inherence from). Here is working example ('_with_counter' is one field
set by the scheduler) that prevents the UserWarning:
&lt;denchmark-code&gt;import torch

optimizer = torch.optim.Adam(torch.nn.Linear(10,10).parameters(), lr=0.1)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 10, 0.1)

print(hasattr(scheduler.optimizer.step, '_with_counter'))  # &gt;&gt;&gt; True

# This mimics what apex is doing internally
old_step = optimizer.step
def new_step(self, closure=None):
    retval = old_step()
    return retval        
optimizer.step = new_step

print(hasattr(scheduler.optimizer.step, '_with_counter'))  # &gt;&gt;&gt; False

# Call the base lr scheduler class to re-equip the optimizer with 
# fields important for the scheduler 
scheduler.__class__.__mro__[-2].__init__(scheduler, optimizer)

print(hasattr(scheduler.optimizer.step, '_with_counter'))  # &gt;&gt;&gt; True

optimizer.step()
scheduler.step()

&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 is this fine?
		</comment>
		<comment id='7' author='alexandrebone' date='2020-05-18T11:53:48Z'>
		it looks reasonable to me...
cc: &lt;denchmark-link:https://github.com/orgs/PyTorchLightning/teams/core-contributors&gt;@PyTorchLightning/core-contributors&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>