<bug id='555' author='s-rog' open_date='2019-11-28T02:47:18Z' closed_time='2020-01-21T12:51:39Z'>
	<summary>0.5.3 broke DDP vs 0.5.2.1</summary>
	<description>
Using ddp on 0.5.3.2 would cause a gpu worker to crash at an epoch transition (start of epoch 7 for me) and hang the whole training process. I rolled back to 0.5.2.1 (used for my last project) and the issue was gone. Single gpu training works fine on both versions and toggling amp makes no difference. I'm using pytorch 1.3 in NGC 19.10-py3.
Any ideas on how to debug this? since it just hangs without an error code...
Edit:
Should add that I don't have any special hooks/functions running at the start of epochs
	</description>
	<comments>
		<comment id='1' author='s-rog' date='2019-11-28T12:30:02Z'>
		First I would check if this happens with the pl examples in this repository, e.g. the MNIST. If not, it would be good to have code for a minimal example to reproduce it.
		</comment>
		<comment id='2' author='s-rog' date='2019-12-01T03:09:52Z'>
		I'm using ddp with 0.5.3.2 and it is working fine. Is there any error message?
		</comment>
		<comment id='3' author='s-rog' date='2019-12-02T00:54:50Z'>
		Unfortunately no, all I see is a child process crash in htop/nvtop/nvidia-smi while training hangs, I suspect it might have something to do with the recurrent parts of my model, but then I just finished running 100 training runs on 0.5.2.1 with no issues at all... when 0.5.3.2 crashes during the first run
		</comment>
		<comment id='4' author='s-rog' date='2019-12-02T07:54:18Z'>
		Do you get any output if you set the NCCL_DEBUG env variable to INFO?
Are you able to come up with a reproducible code snippet you could share?
		</comment>
		<comment id='5' author='s-rog' date='2019-12-03T01:07:43Z'>
		
NCCL init log
[0] NCCL INFO Bootstrap : Using [0]eth0:10.42.0.4&lt;0&gt;
[0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
[0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed
[0] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_6
[0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed
[0] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_8
[0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed
[0] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_1
[0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed
[0] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_3
[0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed
[0] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_5
[0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed
[0] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_7
[0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed
[0] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_9
[0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed
[0] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
[0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed
[0] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_2
[0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed
[0] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_4
[0] NCCL INFO NET/IB : No device found.
[0] NCCL INFO NET/Socket : Using [0]eth0:10.42.0.4&lt;0&gt;
NCCL version 2.4.8+cuda10.1
[1] NCCL INFO Bootstrap : Using [0]eth0:10.42.0.4&lt;0&gt;
[1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
[1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed
[1] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_6
[1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed
[1] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_8
[1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed
[1] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_1
[1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed
[1] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_3
[1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed
[1] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_5
[1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed
[1] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_7
[1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed
[1] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_9
[1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed
[1] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_0
[1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed
[1] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_2
[1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed
[1] transport/net_ib.cc:117 NCCL WARN NET/IB : Unable to open device mlx5_4
[1] NCCL INFO NET/IB : No device found.
[1] NCCL INFO NET/Socket : Using [0]eth0:10.42.0.4&lt;0&gt;
[0] NCCL INFO Setting affinity for GPU 0 to 0fffff00,000fffff
[0] NCCL INFO NCCL_LL_THRESHOLD set by environment to 0.
[1] NCCL INFO Setting affinity for GPU 1 to 0fffff00,000fffff
[1] NCCL INFO NCCL_LL_THRESHOLD set by environment to 0.
[0] NCCL INFO Channel 00 :    0   1
[0] NCCL INFO Channel 01 :    0   1
[0] NCCL INFO Channel 02 :    0   1
[0] NCCL INFO Channel 03 :    0   1
[0] NCCL INFO Ring 00 : 0[1] -&gt; 1[2] via P2P/IPC
[1] NCCL INFO Ring 00 : 1[2] -&gt; 0[1] via P2P/IPC
[0] NCCL INFO Ring 01 : 0[1] -&gt; 1[2] via P2P/IPC
[1] NCCL INFO Ring 01 : 1[2] -&gt; 0[1] via P2P/IPC
[0] NCCL INFO Ring 02 : 0[1] -&gt; 1[2] via P2P/IPC
[1] NCCL INFO Ring 02 : 1[2] -&gt; 0[1] via P2P/IPC
[0] NCCL INFO Ring 03 : 0[1] -&gt; 1[2] via P2P/IPC
[1] NCCL INFO Ring 03 : 1[2] -&gt; 0[1] via P2P/IPC
[0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled
[0] NCCL INFO comm 0x7f351c002400 rank 0 nranks 2 cudaDev 0 nvmlDev 1 - Init COMPLETE
[1] NCCL INFO comm 0x7f20c4002400 rank 1 nranks 2 cudaDev 1 nvmlDev 2 - Init COMPLETE
[0] NCCL INFO Launch mode Parallel


There were no NCCL log entries during training or when the crash happens, these are the NCCL logs during initialization. This notebook instance has 4 GPUS but my training script was running with 2.
Edit:
I enabled NCCL_DEBUG_SUBSYS=COLL and these were the last NCCL log entries before the worker crashed:
&lt;denchmark-code&gt;[1] NCCL INFO Broadcast: opCount 123b sendbuff 0x7f0cc84de000 recvbuff 0x7f0cc84de000 count 42018 datatype 7 op 0 root 0 comm 0x7f0c98002400 [nranks=2] stream 0x55814780ff80
[1] NCCL INFO Broadcast: opCount 123c sendbuff 0x7f08affffe00 recvbuff 0x7f08affffe00 count 50 datatype 4 op 0 root 0 comm 0x7f0c98002400 [nranks=2] stream 0x55814780ff80
&lt;/denchmark-code&gt;

Edit 2:
I just ran the gpu template in the examples section and encountered the same issue... possibly some interaction specific to the NGC pytorch container? I'll see if I can get the new NGC pytorch 19.11 online since it has a new NCCL version (2.4.8 -&gt; 2.5.6).
Edit 3:
still the same issue with NGC pytorch 19.11, NCCL 2.5.6
Edit 4:
0.5.3 has the same issue as well, which makes 0.5.2.1 the last working version for my configuration
		</comment>
		<comment id='6' author='s-rog' date='2019-12-06T17:51:52Z'>
		Thanks for the info.
As a sanity check, could you run your model on a single GPU that isn't 0? As in use gpus=[1] or whichever GPU crashes. This is to test to make sure this isn't a normal crash because of 0.5.3 that happens on any non-master GPU for your model
		</comment>
		<comment id='7' author='s-rog' date='2019-12-09T00:34:02Z'>
		I ran the example on only gpu 1 and it ran fine (0.5.3)
when running multi-gpu, the crashing gpu seems pretty random
ran it on gpus 2 and 3 individually as well, all worked fine
dp also ran fine with 4 gpus
ddp with gpus 1 and 2 failed with the same behavior
		</comment>
		<comment id='8' author='s-rog' date='2019-12-09T18:45:50Z'>
		&lt;denchmark-link:https://github.com/s-rog&gt;@s-rog&lt;/denchmark-link&gt;
 More questions, thanks so much for spending the time to help debug.
Does it always fail at the same point?
When does it fail?
Is it always the same GPU that fails?
How are you running this? Is it inside docker (you mentioned a container)?
We recently switched to pytorch 1.3 and will be doing some DDP as well. I can see if I can replicate it.
		</comment>
		<comment id='9' author='s-rog' date='2019-12-10T01:02:14Z'>
		It fails around the same epoch
For the example it's the end of the 15th epoch if I recall correctly
The GPU that fails seem pretty random
Yes, in a nvidia/jupyter docker (though I'm running training in terminals instead notebooks)
this was tested on ngc 1.11 which ran pytorch &lt;denchmark-link:https://github.com/pytorch/pytorch/commit/174e1ba3b8ed640c09b5ff927227e765193d0cad&gt;1.4&lt;/denchmark-link&gt;
 though I first observed this behavior on 1.3 in ngc 1.10. I have not tested (I don't think) this on ngc 1.09 which runs pytorch 1.2
		</comment>
		<comment id='10' author='s-rog' date='2019-12-10T20:48:49Z'>
		Could you make sure you have enough RAM to run all of your processes inside said docker container? The docker OOM Killer could be killing a random process inside your container.
You can also disable the OOM killer &lt;denchmark-link:https://docs.docker.com/engine/reference/run/#user-memory-constraints&gt;https://docs.docker.com/engine/reference/run/#user-memory-constraints&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='s-rog' date='2019-12-11T00:35:14Z'>
		This instance is running with 755GB of ram so should be fine haha
Edit:
I ran the example again just to make sure ram didn't somehow explode, which it didn't. Another thing to note is that when a child process/gpu crashes all other processes each hang with 100% CPU thread load.
Edit 2:
Previously when running 0.5.2.1 with another project I experienced something similar where after training for some amount of time, one GPU would start stalling hard and significantly slow down training for a period of time. The vram doesn't get emptied and and the GPU still runs, just barely. Restarting a fresh docker instance doesn't help, I could only wait it out. I was unable to determine if it was a CPU or GPU side issue, but stopping training during this slow period would also result in all other processes each hang with 100% CPU thread load.
		</comment>
		<comment id='12' author='s-rog' date='2019-12-13T07:49:39Z'>
		Another update, just tried running teh example code on pytorch 1.2 (ngc 1.09) and the problem still remains on 0.5.3.2
		</comment>
		<comment id='13' author='s-rog' date='2020-01-06T08:53:10Z'>
		any luck recreating the issue? or is there anything else I can do on my end?
		</comment>
		<comment id='14' author='s-rog' date='2020-01-09T20:07:02Z'>
		Things seem to be working for me... I'll try with the example code soon.
Does this fail if you run it outside of docker?
		</comment>
		<comment id='15' author='s-rog' date='2020-01-10T03:52:22Z'>
		Running bare metal would require quite a lot of setup on my end, I'll see what I can do. I have however just tried running on the bare NGC pytorch image (usually has modifications to fit my workflow) and the issue persists.
Could this be related to running on a dual socket system?
		</comment>
		<comment id='16' author='s-rog' date='2020-01-14T23:21:19Z'>
		&lt;denchmark-link:https://github.com/s-rog&gt;@s-rog&lt;/denchmark-link&gt;
 I'm afraid I'm not familiar with it. Is there a cloud setup on GCP/AWS you think could be close to your setup that I can try replicating with?
		</comment>
		<comment id='17' author='s-rog' date='2020-01-15T05:25:21Z'>
		I looked at GCP/AWS listings but couldn't find any info on sockets. But my setup here is with two of these. 40 cores and 80 threads total.
Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz
Edit: Just tried this on an older image with cuda (non NGC) and the same issue persists
		</comment>
		<comment id='18' author='s-rog' date='2020-01-21T06:27:19Z'>
		I think I have identified the issue: early stopping. I pulled the latest commit and early stopping prints. But I assume it's broken and doesn't stop training correctly, producing the symptoms I described above. &lt;denchmark-link:https://github.com/jeffling&gt;@jeffling&lt;/denchmark-link&gt;

On another note, early stopping should not be on  by default.
Edit:
max_nb_epochs does not stop training in the latest commit, ending training with ctrl-c only kills the process for one GPU, same as early stopping.
		</comment>
		<comment id='19' author='s-rog' date='2020-01-21T12:51:39Z'>
		max_nb_epochs is deprecated I believed. new docs should show this.
		</comment>
		<comment id='20' author='s-rog' date='2020-01-21T12:51:46Z'>
		&lt;denchmark-link:https://github.com/luiscape&gt;@luiscape&lt;/denchmark-link&gt;

		</comment>
		<comment id='21' author='s-rog' date='2020-01-22T00:11:52Z'>
		I see, should have pulled the new examples as well. However, is there any way to make lightning exit training correctly in DDP? (when using early stopping)
		</comment>
		<comment id='22' author='s-rog' date='2020-07-27T22:43:09Z'>
		Is it resolved?
My process is getting frozen when EarlyStopping tries to stop the training while using DDP with 4 GPUS. It seems to stop correctly when calling EarlyStopping on a single GPU with DDP. Also, it works well on DP with 4 GPUS
		</comment>
		<comment id='23' author='s-rog' date='2020-07-28T01:01:55Z'>
		upgrade to 0.8.5
		</comment>
	</comments>
</bug>