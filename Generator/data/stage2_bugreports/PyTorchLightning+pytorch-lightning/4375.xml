<bug id='4375' author='moi90' open_date='2020-10-26T15:15:24Z' closed_time='2020-11-22T12:02:07Z'>
	<summary>LearningRateMonitor produces inconsistent logs with logging_interval="epoch"</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

LearningRateMonitor uses step=trainer.current_epoch whereas in other places, it is always step=trainer.global_step.
This creates inconsistencies and makes the log hard to process:
&lt;denchmark-link:https://colab.research.google.com/drive/1bucI1oGCc_xNsnP_lvBWTz3x_bauH3o7&gt;https://colab.research.google.com/drive/1bucI1oGCc_xNsnP_lvBWTz3x_bauH3o7&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;[{'lr-SGD': 0.1, 'step': 0},
 {'epoch': 0, 'step': 49, 'training_loss': 6.862762529635802e-05},
 {'epoch': 0, 'step': 99, 'training_loss': 4.8670010244222794e-09},
 {'epoch': 0, 'step': 149, 'training_loss': 1.5898393712632242e-13},
 {'epoch': 0, 'step': 199, 'training_loss': 4.135580766728708e-14},
 {'epoch': 0, 'step': 249, 'training_loss': 5.2458037913538647e-14},
 {'epoch': 0, 'step': 299, 'training_loss': 4.879430193227563e-14},
 {'epoch': 0, 'step': 312, 'validation_loss': 4.705802669193945e-14},
 {'lr-SGD': 0.010000000000000002, 'step': 1},
 {'epoch': 1, 'step': 349, 'training_loss': 4.884981308350689e-14},
 {'epoch': 1, 'step': 399, 'training_loss': 4.6407322429331543e-14},
 {'epoch': 1, 'step': 449, 'training_loss': 4.679590048795035e-14},
 {'epoch': 1, 'step': 499, 'training_loss': 4.3798298321462426e-14},
 {'epoch': 1, 'step': 549, 'training_loss': 4.368727601899991e-14},
 {'epoch': 1, 'step': 599, 'training_loss': 4.335420911161236e-14},
 {'epoch': 1, 'step': 625, 'validation_loss': 4.4799982544226416e-14},
 {'lr-SGD': 0.0010000000000000002, 'step': 2},
 {'epoch': 2, 'step': 649, 'training_loss': 4.5352610555937645e-14},
 {'epoch': 2, 'step': 699, 'training_loss': 4.551914400963142e-14},
 {'epoch': 2, 'step': 749, 'training_loss': 4.4853010194856324e-14},
 {'epoch': 2, 'step': 799, 'training_loss': 4.2021941482062175e-14},
 {'epoch': 2, 'step': 849, 'training_loss': 4.651834473179406e-14},
 {'epoch': 2, 'step': 899, 'training_loss': 4.296563105299356e-14},
 {'epoch': 2, 'step': 938, 'validation_loss': 4.4716627725953015e-14},
 {'fake_test_acc': 4.718447854656915e-14, 'step': 939}]
&lt;/denchmark-code&gt;

For lr-SGD rows, step is the epoch, otherwise, step is the global step.
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

I expect step to be used consistently, i.e. it should be always be step=trainer.global_step.
Additionally, LearningRateMonitor could provide a value for epoch=trainer.current_epoch.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

See Colab notebook.
	</description>
	<comments>
	</comments>
</bug>