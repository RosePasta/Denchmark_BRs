<bug id='3117' author='bluesky314' open_date='2020-08-24T10:17:15Z' closed_time='2020-10-03T18:05:32Z'>
	<summary>Distributed Training not starting</summary>
	<description>
My code works and training starts with the progress bar with 1 gpu. However when I switch to 2, either using ddp or ddp_spawn the process gets stuck at:
&lt;denchmark-link:https://user-images.githubusercontent.com/35393972/91033116-800e8a80-e620-11ea-92df-521730301f60.png&gt;&lt;/denchmark-link&gt;

Nothing happens but around 1.5 gb is taken on each gpu(the process takes around 8 on a single gpu)
Here is my code:
&lt;denchmark-code&gt;class LightningModel(pl.LightningModule):

    def __init__(self):
        super(LightningModel, self).__init__()
        # not the best model...
        self.model = U2NET(3, 1)

    def forward(self, x):
        # called with self(x)
        return self.model(x)

    def training_step(self, batch, batch_nb):
        # REQUIRED
        x = batch['image'].float() 
        labels_v=batch['label'].double() 
        
        d0, d1, d2, d3, d4, d5, d6 = self(x)
        d0, d1, d2, d3, d4, d5, d6,loss2, loss = muti_bce_loss_fusion(d0, d1, d2, d3, d4, d5, d6, labels_v)
        
        tensorboard_logs = {'train_loss': loss}
        return {'loss': loss, 'log': tensorboard_logs}


    def configure_optimizers(self):

        opt=torch.optim.Adam(self.model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0) 
#         scheduler=CombineCos(opt,0.2,lr,lr,5e-4,len(self.train_dataloader()),100)
        return opt

    def train_dataloader(self):
        loaders=get_loader([size,size])
        return loaders['train']

def main():
    train_loader = get_loader([size,size])['train']
    model = LightningModel()

    trainer = pl.Trainer(gpus=2,distributed_backend='ddp_spawn')
    trainer.fit(model, train_loader)
  
if __name__ ==  '__main__':
    main()
    

&lt;/denchmark-code&gt;

System
&lt;denchmark-code&gt;Python 3.6 
Pytorch 1.6
Linux
pip installed lightning
CUDA Version: 10.1
Nvidia-V100
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='bluesky314' date='2020-08-25T07:00:06Z'>
		the example seems fine to me, mind share complete example even with dummy data, eg colab notebook?
		</comment>
		<comment id='2' author='bluesky314' date='2020-08-25T16:36:53Z'>
		Colab does not allow ddp I read in the docs. How can I give you a workable example then?
		</comment>
		<comment id='3' author='bluesky314' date='2020-08-25T17:19:30Z'>
		true, so jut share script so we can run it...
		</comment>
		<comment id='4' author='bluesky314' date='2020-09-20T02:12:57Z'>
		&lt;denchmark-link:https://github.com/bluesky314&gt;@bluesky314&lt;/denchmark-link&gt;
  yes please, if you can share the script. I will try to run it and help debug.
how do you launch your script? do you have CUDA_VISIBLE_DEVICES environment variable set?
Also, could you run with NCCL_DEBUG=INFO and post the info that is printed
		</comment>
	</comments>
</bug>