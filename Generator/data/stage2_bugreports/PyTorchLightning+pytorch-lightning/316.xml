<bug id='316' author='ceyzaguirre4' open_date='2019-10-06T03:39:34Z' closed_time='2019-10-08T21:33:34Z'>
	<summary>Custom logger (LightningLoggerBase) doesn't call log_hyperparams and log_metrics receives empty metrics dict</summary>
	<description>
Describe the bug
Custom logger (LightningLoggerBase) doesn't call log_hyperparams and log_metrics receives empty metrics dict.
To Reproduce
from pytorch_lightning.logging import LightningLoggerBase, rank_zero_only

class CustomLogger(LightningLoggerBase):
    def __init__(self, *args, **kwargs):
        super(CustomLogger, self).__init__()
        print("--"*30)
        print("it does at least initialize")

    @rank_zero_only
    def log_hyperparams(self, params):
        print("--"*30)
        print("this will never be printed")

    @rank_zero_only
    def log_metrics(self, metrics, step_num):
        print("--"*30)
        print("the following will be an empty dict {}")
        print(metrics)

custom_logger = CustomLogger()
trainer = Trainer(
        logger=custom_logger,
        max_nb_epochs=10
    )

trainer.fit(model)
Desktop (please complete the following information):

OS: MacOS 10.13.6
pytorch-lightning version 0.5.1

I should add that model is defined correctly
    
def validation_step(self, batch, batch_nb):
    """some processing"""
    return {
        'val_corrects': corrects(y, y_pred).item(),
        'val_ponder_costs': torch.mean(ponder_cost).item(),
        'val_steps': torch.mean(steps).item(),
    }

def validation_end(self, outputs):
    n = 0
    all_corrects = 0
    ponder_costs = 0
    acc_steps = 0

    for output in outputs:
        all_corrects += output['val_corrects']
        ponder_costs += output['val_ponder_costs']
        acc_steps += output['val_steps']

    avgs = (elem / len(outputs) for elem in (all_corrects, ponder_costs, acc_steps))
    
    return {
            'val_corrects': all_corrects / len(outputs),
            'val_ponder_costs': ponder_costs / len(outputs),
            'val_steps': acc_steps / len(outputs),
        }
	</description>
	<comments>
		<comment id='1' author='ceyzaguirre4' date='2019-10-06T16:21:15Z'>
		&lt;denchmark-link:https://github.com/neggert&gt;@neggert&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='ceyzaguirre4' date='2019-10-07T00:30:49Z'>
		&lt;denchmark-link:https://github.com/ceyzaguirre4&gt;@ceyzaguirre4&lt;/denchmark-link&gt;
 install the latest version and try again? i pushed a fix
		</comment>
		<comment id='3' author='ceyzaguirre4' date='2019-10-08T02:30:19Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 The last update fixed the arguments passed to , however, the loggers and  methods are still never called.
(on 0.5.13)
		</comment>
		<comment id='4' author='ceyzaguirre4' date='2019-10-08T11:43:06Z'>
		Got it, &lt;denchmark-link:https://github.com/neggert&gt;@neggert&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='ceyzaguirre4' date='2019-10-08T17:22:35Z'>
		Taking a look
		</comment>
		<comment id='6' author='ceyzaguirre4' date='2019-10-08T17:43:37Z'>
		&lt;denchmark-link:https://github.com/ceyzaguirre4&gt;@ceyzaguirre4&lt;/denchmark-link&gt;
 I can reproduce  not being called, but  seems fine.
Are your hyperparameters stored as an  in ?
Here's the test I wrote. It makes it through up to the assert on logger.finalized.
def test_custom_logger():

    class CustomLogger(LightningLoggerBase):
        def __init__(self):
            super().__init__()
            self.hparams_logged = None
            self.metrics_logged = None
            self.finalized = False

        @rank_zero_only
        def log_hyperparams(self, params):
            self.hparams_logged = params

        @rank_zero_only
        def log_metrics(self, metrics, step_num):
            self.metrics_logged = metrics
        
        @rank_zero_only
        def finalize(self):
            self.finalized = True

    hparams = get_hparams()
    model = LightningTestModel(hparams)

    logger = CustomLogger()

    trainer_options = dict(
        max_nb_epochs=1,
        train_percent_check=0.01,
        logger=logger
    )

    trainer = Trainer(**trainer_options)
    result = trainer.fit(model)
    assert result == 1, "Training failed"
    assert logger.hparams_logged == hparams
    assert logger.metrics_logged != {}
    assert logger.finalized
		</comment>
	</comments>
</bug>