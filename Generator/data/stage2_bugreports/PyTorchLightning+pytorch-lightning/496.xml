<bug id='496' author='jaivardhankapoor' open_date='2019-11-12T04:10:20Z' closed_time='2019-11-13T04:24:07Z'>
	<summary>AssertionError in multi-GPU setting with the `dp` option</summary>
	<description>
Setting
I am training a VAE on a single machine with 2 GPUs, with the dp option. I am running the trainer on a Jupyter notebook.  I am passing a numpy array to the DataLoader and assuming that the loader takes care of putting the input on the selected device.
Issue
Even after passing the validation sanity check, the training loop returns an assertion error on the checking of device of the tensors.
&lt;denchmark-code&gt;---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
&lt;ipython-input-82-1cc2acb79a2d&gt; in &lt;module&gt;
     17 model = VAEModel(hparams=parser)
     18 # summary(model, (2, 16, 16), device='cpu')
---&gt; 19 trainer.fit(model)

~/.conda/envs/env/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model)
    346         # easier to avoid NCCL issues
    347         elif self.use_dp:
--&gt; 348             self.dp_train(model)
    349 
    350         elif self.single_gpu:

~/.conda/envs/env/lib/python3.6/site-packages/pytorch_lightning/trainer/dp_mixin.py in dp_train(self, model)
    102         model = LightningDataParallel(model, device_ids=device_ids)
    103 
--&gt; 104         self.run_pretrain_routine(model)
    105 
    106 

~/.conda/envs/env/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in run_pretrain_routine(self, model)
    469 
    470         # CORE TRAINING LOOP
--&gt; 471         self.train()
    472 
    473     def test(self, model=None):

~/.conda/envs/env/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py in train(self)
     58             # RUN TNG EPOCH
     59             # -----------------
---&gt; 60             self.run_training_epoch()
     61 
     62             # update LR schedulers

~/.conda/envs/env/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py in run_training_epoch(self)
     97             # RUN TRAIN STEP
     98             # ---------------
---&gt; 99             output = self.run_training_batch(batch, batch_nb)
    100             batch_result, grad_norm_dic, batch_step_metrics = output
    101 

~/.conda/envs/env/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py in run_training_batch(self, batch, batch_nb)
    213 
    214                 # calculate loss
--&gt; 215                 loss = optimizer_closure()
    216 
    217                 # nan grads

~/.conda/envs/env/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py in optimizer_closure()
    182                     # forward pass
    183                     output = self.training_forward(
--&gt; 184                         split_batch, batch_nb, opt_idx, self.hiddens)
    185 
    186                     closure_loss = output[0]

~/.conda/envs/env/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py in training_forward(self, batch, batch_nb, opt_idx, hiddens)
    284         # distributed forward
    285         if self.use_ddp or self.use_ddp2 or self.use_dp:
--&gt; 286             output = self.model(*args)
    287 
    288         # single GPU forward

~/.conda/envs/env/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    539             result = self._slow_forward(*input, **kwargs)
    540         else:
--&gt; 541             result = self.forward(*input, **kwargs)
    542         for hook in self._forward_hooks.values():
    543             hook_result = hook(self, input, result)

~/.conda/envs/env/lib/python3.6/site-packages/pytorch_lightning/pt_overrides/override_data_parallel.py in forward(self, *inputs, **kwargs)
     64         replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
     65         outputs = self.parallel_apply(replicas, inputs, kwargs)
---&gt; 66         return self.gather(outputs, self.output_device)
     67 
     68     def parallel_apply(self, replicas, inputs, kwargs):

~/.conda/envs/env/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py in gather(self, outputs, output_device)
    163 
    164     def gather(self, outputs, output_device):
--&gt; 165         return gather(outputs, output_device, dim=self.dim)
    166 
    167 

~/.conda/envs/env/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py in gather(outputs, target_device, dim)
     68     # Setting the function to None clears the refcycle.
     69     try:
---&gt; 70         res = gather_map(outputs)
     71     finally:
     72         gather_map = None

~/.conda/envs/env/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py in gather_map(outputs)
     62                 raise ValueError('All dicts must have the same number of keys')
     63             return type(out)(((k, gather_map([d[k] for d in outputs]))
---&gt; 64                               for k in out))
     65         return type(out)(map(gather_map, zip(*outputs)))
     66 

~/.conda/envs/env/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py in &lt;genexpr&gt;(.0)
     62                 raise ValueError('All dicts must have the same number of keys')
     63             return type(out)(((k, gather_map([d[k] for d in outputs]))
---&gt; 64                               for k in out))
     65         return type(out)(map(gather_map, zip(*outputs)))
     66 

~/.conda/envs/env/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py in gather_map(outputs)
     55             # for i in outputs:
     56             #     print(i.shape, i.device)
---&gt; 57             return Gather.apply(target_device, dim, *outputs)
     58         if out is None:
     59             return None

~/.conda/envs/env/lib/python3.6/site-packages/torch/nn/parallel/_functions.py in forward(ctx, target_device, dim, *inputs)
     52     @staticmethod
     53     def forward(ctx, target_device, dim, *inputs):
---&gt; 54         assert all(map(lambda i: i.is_cuda, inputs))
     55         target_device = _get_device_index(target_device, True)
     56         ctx.target_device = target_device

AssertionError: 
&lt;/denchmark-code&gt;

Attempts to solve the issue

I went into the _functions file where the assertion error is being raised, and on printing the tensors in there, I have noticed that 2 of the tensors are on CPU, hence the error. I can't seem to trace the tensors back to when they were initialized or passed.
I have tried using ddp and ddp2 options, but these simply keep running without any outputs or progress.
I am sure that whenever I am using a stochastic node, such as the reparametrization trick in the VAE, I am sampling the epsilon on the same device as the std vector in the VAE.

Is this simply a bug in pytorch_lightning, or am I doing something wrong?
	</description>
	<comments>
		<comment id='1' author='jaivardhankapoor' date='2019-11-12T10:59:07Z'>
		this stack trace points to a PyTorch code error (the dp and ddp class is just PyTorch under  the hood). Seems that your inputs somehow change. Can you post the forward, trainining_step and dataloader code?
It seems like something in the output of training_step is different. ie: gpu 0 and gpu 1 have different outputs. Likely related to the stochastic nature of what you're doing.
		</comment>
		<comment id='2' author='jaivardhankapoor' date='2019-11-12T15:15:05Z'>
		forward
&lt;denchmark-code&gt;    def forward(self, x):
        x = F.pad(x, pad=(1,1,1,1), mode='circular')
        x_hat, mu, logvar = self.vae(x)
        x_hat = F.normalize(x_hat, p=2, dim=1)


        return x_hat, mu, logvar
&lt;/denchmark-code&gt;

training_step
&lt;denchmark-code&gt;
    def training_step(self, batch, batch_nb):
        x = batch.float()
        x = angle2sincos(x*2*np.pi).permute(0,3,1,2)
        x_hat, mu, logvar = self.forward(x)
        loss_reconstruction, loss_kld, loss_total = self.my_loss(x_hat, x, mu, logvar, batch_size=self.batch_size)
        log_dict = {'train_losses':{'loss_reconstruction': loss_reconstruction, 'loss_kld': loss_kld,
                          'loss_total': loss_total}}
        self.logger.experiment.add_scalar('train_losses/loss_reconstruction',
                                           log_dict['train_losses']['loss_reconstruction'], self.global_step)
        self.logger.experiment.add_scalar('train_losses/loss_kld',
                                           log_dict['train_losses']['loss_kld'], self.global_step)
        self.logger.experiment.add_scalar('train_losses/loss_total',
                                           log_dict['train_losses']['loss_total'], self.global_step)
        return {'loss': loss_total, 'train_step': torch.tensor(self.global_step)}
&lt;/denchmark-code&gt;

loss
&lt;denchmark-code&gt;    def my_loss(self, y_hat, y, mu, logvar, batch_size=1, val=False):
#         bceloss = torch.nn.BCELoss(reduction=self.loss_reduction)
#         CE = bceloss(y, y_hat)
        squareloss = torch.nn.MSELoss(reduction='sum')
#         print("loss:", y_hat.shape, y.shape)
        CE = squareloss(y, y_hat)/batch_size
#         print("loss val:", CE.item())
        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())/batch_size
        
        return CE, KLD, self.beta*KLD + CE
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='jaivardhankapoor' date='2019-11-13T04:24:07Z'>
		An update: I found the error I was looking for. The return dict of training_step was returning a tensor created in-place, which was apparently being created without a device parameter, leading to a CPU tensor. This was causing problems with the assert statement. This was resolved when I removed the tensor from the return dict.
On another note, I wanted to ask if there is a way to combine losses reported to the logger, from different GPUs in the ddp option. When I try to log the errors, the rank_zero_only decorator uses only the 1st GPUs result for logging.
		</comment>
	</comments>
</bug>