<bug id='669' author='ricardorei' open_date='2020-01-08T12:15:36Z' closed_time='2020-01-21T12:09:27Z'>
	<summary>Difference between results during training and testing</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I noticed that the results achieved by my best checkpoint during training dont match the results of calling the trainer.test() function for the same dataset. Note that my LightningModule redirects the test_step and test_end methods to the validation_step and test_end methods, respectively.
    def test_step(self, batch: list, batch_nb: int, *args, **kwargs) -&gt; dict:
        """ Redirects to validation step. """
        return self.validation_step(batch, batch_nb, *args, **kwargs)

    def test_end(self, outputs: list) -&gt; dict:
        """ Redirects to validation end. """
        return self.validation_end(outputs)
Also, if I run one sample of the dev set at each time without using lightning, I get different results.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

To reproduce the error I decided to write a script that trains the model for 1 epoch and then tests the model using trainer.test() and running the test without Lightining.
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

import torch
from comet.data_loader import add_data_args
from comet.models.estimators import FeedForwardEstimator
from pytorch_lightning import Trainer
from test_tube import HyperOptArgumentParser
from comet.optimizers import add_optimizer_args
from comet.schedulers import add_scheduler_args
from tqdm import tqdm
import numpy as np

parser = HyperOptArgumentParser()
parser.add_argument("--loss", default="mse")
parser.add_argument("--disable_progress_bar", default=False)
parser.add_argument("--nr_frozen_epochs", default=0)
parser.add_argument("--pretrained_model", default="")
parser.add_argument("--encoder_learning_rate",default=1e-05)
parser.add_argument("--dropout",default=0.1)
parser.add_argument("--hidden_sizes", default="3072,1536,768")
parser.add_argument("--activations", default="Tanh")
parser.add_argument("--encoder_model", default="LASER")
parser.add_argument("--pool", default="default",)
parser.add_argument("--layer", default="-1")
parser.add_argument("--scalar_mix_dropout", default=0.1)
parser.add_argument("--scalar_mix_layer_norm", default=True)
parser.add_argument("--monitor", default="pearson")
parser.add_argument("--optimizer", default="Adam")
parser.add_argument("--scheduler", default="constant")
parser.add_argument("--batch_size", default=8)
parser.add_argument("--train_percent_check", default=1.0)
parser = add_optimizer_args(parser, 'Adam')
parser = add_scheduler_args(parser, 'constant')
parser = add_data_args(parser)

hparams = parser.parse_args()
model = FeedForwardEstimator(hparams)

trainer = Trainer(
    max_nb_epochs=1,
    min_nb_epochs=1,
    logger=False, 
    gpus=None,
    show_progress_bar=False
)

trainer.fit(model)
print ("training end.")
print ("Starting testing trial 1:")
trainer.test(model)
print ("Starting testing trial 3:")
trainer.test()

model.eval()
print ("Starting manual testing:")
testset = model._retrieve_dataset(hparams, train=False, val=True, test=False)[0]
model_outputs = []
for sample in tqdm(testset):
    model_input, _ = model.prepare_sample([sample])
    prediction = model.forward(**model_input)
    sample["prediction"] = prediction["score"].data.tolist()[0][0]
    model_outputs.append(sample)

predictions = [sample["prediction"] for sample in model_outputs]
targets = [sample["score"] for sample in model_outputs]
metrics = model._compute_metrics(np.array(predictions), np.array(targets))
print (metrics)
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Results:
"pearson": -0.03711935458763926    -&gt; result produced during training in the end of the epoch.
"pearson": -0.03711942892090772     -&gt; result of calling trainer.test()
"pearson": -0.03711942892090772     -&gt; result of calling trainer.test(model)
"pearson": -0.037119481426416974   -&gt; metrics print in the end of the script
&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

When using big transformer models these differences are even bigger.
&lt;denchmark-h:h3&gt;Questions&lt;/denchmark-h&gt;


Has anyone encountered/had the same issue?
Is there any difference between the way the validation loop runs inside the training loop and outside it?

	</description>
	<comments>
		<comment id='1' author='ricardorei' date='2020-01-21T02:11:41Z'>
		All three cases where Lightning calculates the metrics for you are calling the same code internally. Those difference look small enough that I personally would have blamed them on floating point computations, but you said the differences are larger on larger models. We'd need the model and dataset you're using to debug further.
		</comment>
		<comment id='2' author='ricardorei' date='2020-01-21T12:09:27Z'>
		I think your right... this is floating-point differences and when using XLM-R I was getting bigger differences but it was actually my fault. I'm now getting only small differences like the one I show here.
		</comment>
	</comments>
</bug>