<bug id='4768' author='stas00' open_date='2020-11-19T06:35:11Z' closed_time='2020-11-19T06:42:20Z'>
	<summary>pl not respecting total_val_batches</summary>
	<description>
It took me a while to hunt it down,
If I provide:
&lt;denchmark-code&gt;total_train_batches = 10
total_val_batches = 3
val_check_batch = 1
&lt;/denchmark-code&gt;

how do we get to total_batches 40 in tqdm:
Eventually I found the culprit to be this:
&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/callbacks/progress.py#L326-L330&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/callbacks/progress.py#L326-L330&lt;/denchmark-link&gt;

I have a hard time following this logic.
We should have a total of 13 items here, not 40.
Obviously the real numbers are much bigger - so we end with an incredibly huge numbers.
Thanks.
	</description>
	<comments>
		<comment id='1' author='stas00' date='2020-11-19T06:42:19Z'>
		OK, I figured it out - the val_check_batch is very confusing with float and int values. I need to pass val_check_batch = total_train_batches to not rerun validation multiple times. I was under the impression that it was a frequency argument so 0.25 indicated 4 times during eval and 1 - just one time.
		</comment>
	</comments>
</bug>