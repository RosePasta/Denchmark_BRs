<bug id='1461' author='VitorGuizilini' open_date='2020-04-12T00:39:11Z' closed_time='2020-06-01T15:00:35Z'>
	<summary>Leaked semaphores with DDP training</summary>
	<description>
I constantly get this warning when training on an AWS instance (8 GPUs, using DDP). It does not crash, but the training hangs for a few seconds before continuing.
&lt;denchmark-code&gt;/usr/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 3 leaked semaphores to clean up at shutdown
&lt;/denchmark-code&gt;

I can share my docker container if necessary, as it might be an issue with library versions.
	</description>
	<comments>
		<comment id='1' author='VitorGuizilini' date='2020-04-13T20:18:44Z'>
		FYI this behaviour only shows up in the latest master (&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/8544b334e4af9caa060a280146e7d3bb10648332&gt;8544b33&lt;/denchmark-link&gt;
), if I install 0.7.3 it disappears.
		</comment>
		<comment id='2' author='VitorGuizilini' date='2020-04-19T10:43:37Z'>
		@vguizilini  I haven't been able to reproduce this on latest master while running with 8 GPUS using DDP. Are you still getting the warning?
		</comment>
	</comments>
</bug>