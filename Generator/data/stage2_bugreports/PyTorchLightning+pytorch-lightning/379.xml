<bug id='379' author='drazellan' open_date='2019-10-17T09:09:23Z' closed_time='2020-04-16T23:30:18Z'>
	<summary>Early stopping with ddp  bug</summary>
	<description>
Describe the bug
Earley stopping with ddp stalls :
When using distribued mode ddp and early stopping if the stop condition is met in one or more subprocess but not in all subprocess, the corresponding subprocess are stop but the others ones are still running and the training hangs.
with dp mode all is doing fine

I use the example code, with a forked early stopping callback wich stops if val_acc&gt;threshold
I also fix the bug &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/371&gt;#371&lt;/denchmark-link&gt;
 in trainer.py  line 1131 in met_batch_limit = batch_nb &gt; self.nb_training_batches  (i.e change &gt; by &gt;= )
In validation_end() i return :
result = {'progress_bar': tqdm_dict, 'log': tqdm_dict,'val_acc':val_acc_mean}
return result
I use pytorch 1.2
The code :
&lt;denchmark-code&gt;from pytorch_lightning.callbacks import EarlyStopping

class ThresholdStopping(EarlyStopping):
   
    def __init__(self, monitor="val_acc",thresh=0.0,mode="max",verbose=0):
        super(ThresholdStopping, self).__init__()

        self.monitor = monitor
        self.stopped_epoch = 0
        self.thresh=thresh
        self.verbose = verbose

        if mode not in ['auto', 'min', 'max']:
            print('EarlyStopping mode %s is unknown, fallback to auto mode.' % mode)
            mode = 'auto'

        if mode == 'min':
            self.monitor_op = np.less
        elif mode == 'max':
            self.monitor_op = np.greater
        else:
            if 'acc' in self.monitor:
                self.monitor_op = np.greater
            else:
                self.monitor_op = np.less

        self.on_train_begin()

    def on_train_begin(self, logs=None):
        self.stopped_epoch = 0

    def on_epoch_end(self, epoch, logs=None):
        
        current = logs.get(self.monitor)
        stop_training = False

        print("current",current,self.thresh)
        
        if current is None:
            print('Early stopping conditioned on metric `%s` '
                  'which is not available. Available metrics are: %s' %
                  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning)
            stop_training = True
            return stop_training

        if self.monitor_op(current ,self.thresh):
            
            self.stopped_epoch = epoch
            stop_training = True
            self.on_train_end()

        return stop_training

    def on_train_end(self, logs=None):
        if self.stopped_epoch &gt;= 0:
            print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))

def main(hparams):
    """
    Main training routine specific for this project
    :param hparams:
    """
    # ------------------------
    # 1 INIT LIGHTNING MODEL
    # ------------------------
    model = LightningTemplateModel(hparams)

    # ------------------------
    # 2 INIT TRAINER
    # ------------------------
    # trainer = Trainer(max_nb_epochs=1, gpus=[0, 1, 3, 7], distributed_backend='ddp')   

    gpu_param=hparams.gpus
    
    early_stop_callback=ThresholdStopping(monitor='val_acc',thresh=0.85)

    if hparams.distributed_backend=="ddp":
        gpu_param=[0,1,2,3,4, 5, 6, 7]

    trainer = Trainer(
        gpus=gpu_param,
        distributed_backend=hparams.distributed_backend,
        use_amp=hparams.use_16bit,
        early_stop_callback=early_stop_callback,
        min_nb_epochs=-1,
        max_nb_epochs=20,
    )

    # ------------------------
    # 3 START TRAINING
    # ------------------------
    trainer.fit(model)

if __name__ == '__main__':
    # ------------------------
    # TRAINING ARGUMENTS
    # ------------------------
    # these are project-wide arguments

    root_dir = os.path.dirname(os.path.realpath(__file__))
    parent_parser = ArgumentParser(add_help=False)

    # gpu args
    parent_parser.add_argument(
        '--gpus',
        type=int,
        default=2,
        help='how many gpus'
    )
    parent_parser.add_argument(
        '--distributed_backend',
        type=str,
        default='dp',
        help='supports three options dp, ddp, ddp2'
    )
    parent_parser.add_argument(
        '--use_16bit',
        dest='use_16bit',
        action='store_true',
        help='if true uses 16 bit precision'
    )

    # each LightningModule defines arguments relevant to it
    parser = LightningTemplateModel.add_model_specific_args(parent_parser, root_dir)
    hyperparams = parser.parse_args()

    # ---------------------
    # RUN TRAINING
    # ---------------------
    main(hyperparams)
&lt;/denchmark-code&gt;

Expected behavior
All the subprocess should stop

If applicable, add screenshots to help explain your problem.
&lt;denchmark-link:https://user-images.githubusercontent.com/38592055/66994952-70541a80-f0ce-11e9-928c-bbba12e65d49.png&gt;&lt;/denchmark-link&gt;

Desktop (please complete the following information):

OS:  Ubuntu 16.04.4 LTS
Browser [e.g. chrome, safari]
Version : pip version

Additional context
	</description>
	<comments>
		<comment id='1' author='drazellan' date='2019-10-18T13:35:08Z'>
		Great point. Mind submitting a PR? I think this is as simple as closing the dist.group when in ddp or ddp2
		</comment>
		<comment id='2' author='drazellan' date='2019-10-18T17:40:14Z'>
		Hi, could you give more informations to do that?
How a process communicates with the others ones?
i know little about multiprocessing...
with a flags in model class? raise a exception?
		</comment>
		<comment id='3' author='drazellan' date='2019-10-21T09:52:25Z'>
		As this point also impact the checkpoint callback, the best is maybe to reduce all the metrics as in dp or ddp2 mode...so all the processes will stop correctly and the metrics will be computed on the entire dataset...
by the way i saw that the loss values in progress_bar and log keys are not detached from the graph and the val_loss tensor in loss, progress_bar and log are shared, so i think it's better to add .detach().clone() like this :
tqdm_dict = {'train_loss': loss_val.detach().clone()}
log_dict = {'train_loss': loss_val.detach().clone()}
output = OrderedDict({
'loss': loss_val,
'progress_bar': tqdm_dict,
'log': log_dict
})
		</comment>
		<comment id='4' author='drazellan' date='2019-10-21T10:05:14Z'>
		thatâ€™s not necessary because we call .item() on anything passed into those dicts... that detaches the graph and brings things into numpy
		</comment>
		<comment id='5' author='drazellan' date='2019-10-21T10:56:54Z'>
		ok, i see.
Excuses me but i still not fully understand :
In dp or ddp2 mode, the averaging operation done on the loss key  in reduce_distributed_output at the beginning of __process_output (for the callback_metrics) does not affect the graph, as it's done before the call to .item()?
Thanks.
		</comment>
		<comment id='6' author='drazellan' date='2019-10-23T10:19:25Z'>
		if the averaging happens before item then it affects the graph for a brief second. however, that avg then becomes .item() so it has no effect on the the graph because that average is never used anywhere and .item() breaks the link
		</comment>
		<comment id='7' author='drazellan' date='2019-12-04T12:41:12Z'>
		&lt;denchmark-link:https://github.com/jeffling&gt;@jeffling&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/drazellan&gt;@drazellan&lt;/denchmark-link&gt;
 anyone want to submit this PR?
		</comment>
		<comment id='8' author='drazellan' date='2019-12-10T02:59:58Z'>
		I got the same problem and I was stuck thinking that the program was crashing for some reason. Would be great if a message was printed at least when it early stops.
		</comment>
		<comment id='9' author='drazellan' date='2020-01-07T21:57:57Z'>
		This happens whenever there are any random operations used in your procedure. Currently this issue can be avoided by averaging the values across all processes so that each process logs the same value. If you want to synchronize tensor val_loss for example:
import torch.distributed as dist
...
dist.all_reduce(val_loss)
val_loss /= dist.get_world_size()
		</comment>
		<comment id='10' author='drazellan' date='2020-02-17T01:13:58Z'>
		Hi, thanks for this. I'm having a similar issue and not quite following the above.
Background:
I'm using this in my validation-end (because it worked, I couldn't work out another way, I know it needs optimising. (and I have very small batches of very large images)):
`try: # one gpu
&lt;denchmark-code&gt;       avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
       avg_acc = torch.stack([x['val_accuracy'] for x in outputs]).mean()`
&lt;/denchmark-code&gt;

`except: # multi
&lt;denchmark-code&gt;        losses = []
        accuracies = []
        for thing in outputs:
            for x in thing['val_loss']:
                losses.append(x)
            for x in thing['val_accuracy']:
                accuracies.append(x)
        avg_loss = torch.stack(losses).mean()
        avg_acc = torch.stack(accuracies).mean()
&lt;/denchmark-code&gt;

`
&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
  can you point me in the direction to close dist.group? Would that be in core.lightning.py somewhere?
And thanks &lt;denchmark-link:https://github.com/MartinPernus&gt;@MartinPernus&lt;/denchmark-link&gt;
 but where would this go? in training/val step functions?
		</comment>
		<comment id='11' author='drazellan' date='2020-02-17T08:55:05Z'>
		
Hi, thanks for this. I'm having a similar issue and not quite following the above.
Background:
I'm using this in my validation-end (because it worked, I couldn't work out another way, I know it needs optimising. (and I have very small batches of very large images)):
`try: # one gpu
       avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
       avg_acc = torch.stack([x['val_accuracy'] for x in outputs]).mean()`

`except: # multi
        losses = []
        accuracies = []
        for thing in outputs:
            for x in thing['val_loss']:
                losses.append(x)
            for x in thing['val_accuracy']:
                accuracies.append(x)
        avg_loss = torch.stack(losses).mean()
        avg_acc = torch.stack(accuracies).mean()

`
@williamFalcon can you point me in the direction to close dist.group? Would that be in core.lightning.py somewhere?
And thanks @MartinPernus but where would this go? in training/val step functions?

Hi,
it would go in this exact function you posted: when you are calculating the average loss and accuracy in the whole validation set. In your case, you would just write
dist.all_reduce(avg_loss); avg_loss /= dist.get_world_size()
and the same for 'avg_acc'. This way the value will be synchronized across all processes and you won't get early stopping by a subset of processes.
		</comment>
		<comment id='12' author='drazellan' date='2020-02-17T09:04:39Z'>
		Hi
I'm left this issue aside as i have a lot of works in my regular job but thanks for the reply, i completely agree with yours remarks
Nicolas
		</comment>
		<comment id='13' author='drazellan' date='2020-02-17T10:41:15Z'>
		Thanks again. yeah i tried that but it's still hanging. Curious about closing the dist.group but not a massive issue at the moment.
		</comment>
		<comment id='14' author='drazellan' date='2020-03-26T13:56:46Z'>
		&lt;denchmark-link:https://github.com/jamesjjcondon&gt;@jamesjjcondon&lt;/denchmark-link&gt;
 could you please share a minimal example to replicate?
		</comment>
		<comment id='15' author='drazellan' date='2020-04-16T23:30:17Z'>
		&lt;denchmark-link:https://github.com/jamesjjcondon&gt;@jamesjjcondon&lt;/denchmark-link&gt;
 feel free to reopen or send new issue if needed 
		</comment>
		<comment id='16' author='drazellan' date='2020-05-01T10:22:44Z'>
		&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
  Seems to only be an issue in combination with overfit_pc.
`import os
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import EarlyStopping
from argparse import ArgumentParser
from research_seed.mnist.mnist import CoolSystem
def main(hparams):
&lt;denchmark-code&gt;early_stop_callback = EarlyStopping(
        monitor='avg_val_loss',
        min_delta=0.00,
        patience=4,
        verbose=True,
        mode='min',
        )

# init module
model = CoolSystem(hparams)

# most basic trainer, uses good defaults
trainer = Trainer(
    distributed_backend='ddp',
    early_stop_callback=early_stop_callback,
    max_nb_epochs=30,
    overfit_pct=0.2,
    gpus=2,
)
trainer.fit(model)
&lt;/denchmark-code&gt;

if name == 'main':
parser = ArgumentParser(add_help=False)
parser.add_argument('--gpus', type=str, default=-1)
parser.add_argument('--nodes', type=int, default=1)
&lt;denchmark-code&gt;# give the module a chance to add own params
# good practice to define LightningModule speficic params in the module
parser = CoolSystem.add_model_specific_args(parser)

# parse params
hparams = parser.parse_args()

main(hparams)`
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/files/4563512/mnist_trainer_ES_hang.txt&gt;mnist_trainer_ES_hang.txt&lt;/denchmark-link&gt;

(sorry, obviously haven't worked out syntax for posting).
		</comment>
		<comment id='17' author='drazellan' date='2020-05-01T10:26:48Z'>
		Btw, when the above is run and python processes hang on gpu,  gives semaphore warning, as in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1461&gt;#1461&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>