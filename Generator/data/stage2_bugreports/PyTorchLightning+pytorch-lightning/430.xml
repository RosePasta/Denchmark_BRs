<bug id='430' author='kuynzereb' open_date='2019-10-25T07:49:46Z' closed_time='2019-10-30T16:13:40Z'>
	<summary>Incorrect total number of batches</summary>
	<description>
Describe the bug
Sometimes total number of batches is computed wrong.
To Reproduce
Run the following code with current master branch:
&lt;denchmark-code&gt;from time import sleep
import torch
from torch.utils.data import DataLoader, Dataset

import pytorch_lightning as pl


class DummyDataset(Dataset):
    def __init__(self, n):
        super().__init__()
        self.n = n

    def __len__(self):
        return self.n

    def __getitem__(self, idx):
        return torch.rand(10)


class CoolSystem(pl.LightningModule):
    def __init__(self):
        super(CoolSystem, self).__init__()
        self.layer = torch.nn.Linear(10, 10)

    def forward(self, x):
        return self.layer(x)

    def training_step(self, batch, batch_nb):
        sleep(1)
        return {'loss': torch.mean(self.forward(batch) ** 2)}

    def validation_step(self, batch, batch_nb):
        sleep(1)
        return {}

    def validation_end(self, outputs):
        return {}

    def configure_optimizers(self):
        return [torch.optim.Adam(self.layer.parameters())]

    @pl.data_loader
    def train_dataloader(self):
        return DataLoader(DummyDataset(10), batch_size=1)

    @pl.data_loader
    def val_dataloader(self):
        return DataLoader(DummyDataset(5), batch_size=1)

model = CoolSystem()
trainer = pl.Trainer(weights_summary=None, nb_sanity_val_steps=0, early_stop_callback=False,
                     val_percent_check=1.0, val_check_interval=0.5)
trainer.fit(model)
&lt;/denchmark-code&gt;

At first output will look like:
67%|█████▋                    | 10/15 [00:10&lt;00:05,  1.05s/it, batch_nb=4, epoch=0, loss=0.194, v_nb=0]
But at the end of the epoch it will be like:
20it [00:20,  1.07s/it, batch_nb=9, epoch=0, loss=0.212, v_nb=0]
Moreover, if you run
&lt;denchmark-code&gt;trainer = pl.Trainer(weights_summary=None, nb_sanity_val_steps=0, early_stop_callback=False,
                     val_percent_check=1.0, val_check_interval=0.5, check_val_every_n_epoch=10)
&lt;/denchmark-code&gt;

The first epoch will end at the point
67%|█████▋                    | 10/15 [00:09&lt;00:04,  1.01it/s, batch_nb=8, epoch=1, loss=0.069, v_nb=0]
Expected behavior
Correct total number of batches.
Possible solution
Now we have total_batches = nb_training_batches + nb_val_batches, where nb_val_batches is the number of batches of only one validation loop. And the problem arises because actually there can be several validation loops during one training epoch. Moreover there is a parameter check_val_every_n_epoch and thus there can be no validation loops at all.
With this in mind, it looks like the correct formula is:
&lt;denchmark-code&gt;is_val_epoch = (current_epoch + 1) % check_val_every_n_epoch == 0
val_checks_per_epoch = nb_training_batches // val_check_batch if is_val_epoch else 0
total_batches = nb_training_batches + nb_val_batches * val_checks_per_epoch
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='kuynzereb' date='2019-10-25T11:40:49Z'>
		I have the same problem..
		</comment>
		<comment id='2' author='kuynzereb' date='2019-10-29T10:56:26Z'>
		&lt;denchmark-link:https://github.com/kuynzereb&gt;@kuynzereb&lt;/denchmark-link&gt;
 good catch. The solution then is to do what you suggest
total_batches = nb_training_batches + nb_val_batches * val_checks_per_epoch
Do you want to submit that PR? That would be very helpful!
		</comment>
	</comments>
</bug>