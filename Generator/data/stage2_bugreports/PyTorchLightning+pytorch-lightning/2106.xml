<bug id='2106' author='rohitgr7' open_date='2020-06-07T15:33:00Z' closed_time='2020-07-10T01:25:49Z'>
	<summary>tpu_cores=8 not working</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

After &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2016&gt;#2016&lt;/denchmark-link&gt;
 was fixed with PR &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2033&gt;#2033&lt;/denchmark-link&gt;
 the code is running perfectly on single tpu core and a specific tpu core but now not working with 8 tpu cores. After the training is complete getting .
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-link:https://colab.research.google.com/drive/1jPen-P-e4njk_vHUQPsNWrcJ87P5UFxr&gt;Colab notebook&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Should train with 8 tpu cores with no error just like it works in case of a single core.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


pytorch/xla: nightly
pytorch-lightning: master
PyTorch Version (e.g., 1.0): 1.5
OS (e.g., Linux): Linux
How you installed PyTorch (conda, pip, source): pip
Python version: 3.7

	</description>
	<comments>
		<comment id='1' author='rohitgr7' date='2020-06-16T15:06:07Z'>
		
To Reproduce
Colab notebook

I am sorry, I can't run it as it is read-only
&lt;denchmark-link:https://github.com/rohitgr7&gt;@rohitgr7&lt;/denchmark-link&gt;
 mind check tests in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2094&gt;#2094&lt;/denchmark-link&gt;
 as all the test there are passing, see: &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2094#issuecomment-644330374&gt;#2094 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='rohitgr7' date='2020-06-16T16:05:11Z'>
		I'm seeing this issue when I run the notebook. Unsure of why it's happening. It's not consistent either but occurs the majority of times. I ran the code with an old release of lightning too, it still breaks. There is some change somewhere which is breaking it. If I run just the model with plain xmp.spawn , it seems to work fine.
		</comment>
		<comment id='3' author='rohitgr7' date='2020-06-16T16:53:35Z'>
		I have checked this on colab only. Will try it on kaggle-kernels and check if it's happening there too.
		</comment>
		<comment id='4' author='rohitgr7' date='2020-06-26T13:39:58Z'>
		&lt;denchmark-link:https://github.com/lezwon&gt;@lezwon&lt;/denchmark-link&gt;
 is this still an issue? Will reopen if it's still an issue since this is now tested
		</comment>
		<comment id='5' author='rohitgr7' date='2020-06-26T14:17:42Z'>
		I just ran the notebook. The training seems to be working fine, but  fails. Also calling  the second time seems to throw the same error &lt;denchmark-link:https://github.com/rohitgr7&gt;@rohitgr7&lt;/denchmark-link&gt;
 mentioned.
		</comment>
		<comment id='6' author='rohitgr7' date='2020-06-26T19:21:23Z'>
		may we add a test for it so we can later fix it?
		</comment>
		<comment id='7' author='rohitgr7' date='2020-06-27T05:36:07Z'>
		On it. :]
		</comment>
		<comment id='8' author='rohitgr7' date='2020-06-27T14:03:03Z'>
		Checked on Kaggle with master.

When specifying the tpu_id training and testing both are working fineüëå.
When training on 1 core both training and testing completes with SystemExit: 0.
When training on 8 cores training gets stuck and nothing happens unless I restart the kernel and in case of testing it throws RuntimeError: tensorflow/compiler/xla/xla_client/computation_client.cc:272 : Missing XLA configuration whenever I test without training since training never ends in my case.

&lt;denchmark-link:https://github.com/lezwon&gt;@lezwon&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='rohitgr7' date='2020-07-05T22:51:55Z'>
		I tried training a model on 8 tpu cores and checked the model device in forward. The global_rank is varying between 0-7 but the model and batch tensors are always on device 0 or 1.
Can anyone please check if something is wrong in the code??
I have disabled the optimizer step just to check what's happening and returned an arbitrary tensor in forward to maintain the flow of training procedure. Here is the code:
class JigsawBert(pl.LightningModule):
    def __init__(self, hparams, train_df, valid_df, train_tfms, valid_tfms, tokenizer):
        super().__init__()
    
        self.hparams = hparams
        self.train_df, self.valid_df = train_df, valid_df
        self.train_tfms, self.valid_tfms = train_tfms, valid_tfms
        self.tokenizer = tokenizer
        
        self.model = JigsawBertModel(self.hparams.model_path)
    
        self.am_tloss = AverageMeter()
        self.am_vloss = AverageMeter()
        self.am_vauroc = AverageMeter()
    
    def prepare_data(self):
        self.train_ds = JigsawBertDataset(
            df=self.train_df,
            tokenizer=self.tokenizer,
            max_length=self.hparams.max_length,
            tfms=self.train_tfms,
            is_testing=False
        )
        
        self.valid_ds = JigsawBertDataset(
            df=self.valid_df,
            tokenizer=self.tokenizer,
            max_length=self.hparams.max_length,
            tfms=self.valid_tfms,
            is_testing=False
        )
        
    def setup(self, stage):
        pass

    def train_dataloader(self):
        sampler = DistributedSampler(
            self.train_ds,
            num_replicas=xm.xrt_world_size(),
            rank=xm.get_ordinal(),
            shuffle=True
        )
        
        dl = DataLoader(
            self.train_ds,
            batch_size=self.hparams.batch_size,
            num_workers=self.hparams.num_workers,
            sampler=sampler,
            drop_last=True
        )
        
        return dl
    
    def val_dataloader(self):
        sampler = DistributedSampler(
            self.valid_ds,
            num_replicas=xm.xrt_world_size(),
            rank=xm.get_ordinal(),
            shuffle=False
        )
        
        dl = DataLoader(
            self.valid_ds,
            batch_size=self.hparams.batch_size,
            num_workers=self.hparams.num_workers,
            sampler=sampler,
            drop_last=False
        )
        
        return dl
    
    def _compute_loss(self, logits, targets):
        loss_fn = nn.BCEWithLogitsLoss()
        loss = loss_fn(logits, targets)
        return loss
    
    def forward(self, batch):
#         return self.model(batch['input_ids'], batch['token_type_ids'], batch['attn_mask'])
        print(f"Model device: {next(self.model.parameters()).device}",
              f"Input device: {batch['input_ids'].device}",
              f"Input shape: {batch['input_ids'].shape}",
              f"Global rank: {self.trainer.global_rank}")

        return nn.Linear(2, 1)(torch.randn(len(batch['input_ids']), 2)).to(batch['input_ids'].device)
    
    def training_step(self, batch, batch_nb):
        logits = self.forward(batch).view(-1)
        train_loss = self._compute_loss(logits, batch['label'])
        self.am_tloss.update(train_loss.item(), len(batch['input_ids']))
        return {'loss': train_loss}
    
    def training_epoch_end(self, outputs):
        logs = {'avg_train_loss': self.am_tloss.avg}
        self.am_tloss.reset()
        return {'progress_bar': logs, 'log': logs}
    
    def validation_step(self, batch, batch_nb):
        logits = self.forward(batch).view(-1)
        valid_loss = self._compute_loss(logits, batch['label'])
        logits = torch.sigmoid(logits)
        self.am_vloss.update(valid_loss.item(), len(batch['input_ids']))
    
    def validation_epoch_end(self, outputs):
        logs = {
            'avg_valid_loss': self.am_vloss.avg,
            'avg_valid_auroc': self.am_vauroc.avg
        }
        return {'progress_bar': logs, 'log': logs}
    
    def optimizer_step(self, *args, **kwargs):
        pass
    
    def configure_optimizers(self):
        return optim.AdamW(self.parameters(), lr=self.hparams.lr*xm.xrt_world_size())
&lt;denchmark-code&gt;Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 2
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 4
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 1
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 2
Model device: xla:1 Input device: xla:1 Input shape: torch.Size([16, 32]) Global rank: 0
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 1
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 2
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 3
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 7
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 3
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 7
Model device: xla:1 Input device: xla:1 Input shape: torch.Size([16, 32]) Global rank: 0
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 3
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 7
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 3
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 7
Model device: xla:1 Input device: xla:1 Input shape: torch.Size([16, 32]) Global rank: 0
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 6
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 6
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 6
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 5
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 6
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 4
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 5
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 4
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 5
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 4
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 4
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 5
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 1
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 1
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 1
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 1
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 2
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 2
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 2
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 3
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 2
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 3
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 3
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 3
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 6
Model device: xla:1 Input device: xla:1 Input shape: torch.Size([16, 32]) Global rank: 0
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 6
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 4
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 6
Model device: xla:1 Input device: xla:1 Input shape: torch.Size([16, 32]) Global rank: 0
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 4
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 6
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 4
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 7
Model device: xla:1 Input device: xla:1 Input shape: torch.Size([16, 32]) Global rank: 0
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 4
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 7
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 7
Model device: xla:1 Input device: xla:1 Input shape: torch.Size([16, 32]) Global rank: 0
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 7
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 5
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 5
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 3
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 5
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 1
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 5
Model device: xla:0 Input device: xla:0 Input shape: torch.Size([16, 32]) Global rank: 3
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='rohitgr7' date='2020-07-10T01:25:49Z'>
		fixed on 0.8.5 but .test() doesn't work since we can't share weights back to main process. In the meantime, use .test(ckpt_path=PATH)
		</comment>
		<comment id='11' author='rohitgr7' date='2020-07-11T12:50:12Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 Still facing &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2419&gt;#2419&lt;/denchmark-link&gt;
  on the master with 8 tpu cores. With , it's stuck at the beginning of 2nd epoch and with  it stuck at the end of the last epoch (waited for more than 10 minutes for clean up but nothing happens). Tried on both kaggle and colab.
		</comment>
		<comment id='12' author='rohitgr7' date='2020-07-11T13:14:26Z'>
		you need a val loop for now. can you share a colab?
		</comment>
		<comment id='13' author='rohitgr7' date='2020-07-11T13:28:07Z'>
		with a val loop, it's working fine :) but without a val loop it's not. Don't know how to share an editable colab notebook link, but here is the same notebook on kaggle: &lt;denchmark-link:https://www.kaggle.com/rohitgr/lightning-tpu&gt;https://www.kaggle.com/rohitgr/lightning-tpu&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>