<bug id='1201' author='Dunrar' open_date='2020-03-20T22:35:06Z' closed_time='2020-03-31T06:24:27Z'>
	<summary>Early stopping not working on 0.7.1</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Early stopping does not work anymore. When I downgrade from 0.7.1 or the current dev version to 0.6.0 early stopping works again, with the same code.
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;def main(hparams):
    if hparams.early_stopping == 'yes':
        early_stopping = EarlyStopping(
            monitor='batch/mean_absolute_loss',
            min_delta=hparams.min_delta,
            patience=hparams.patience,
            mode='min'
        )
    else:
        early_stopping = False

    model = MemoryTest(hparams)
    trainer = pl.Trainer(
        val_percent_check=0,
        early_stop_callback=early_stopping,
        default_save_path=src.settings.LOG_DIR,
        max_epochs=hparams.epochs
    )

    trainer.fit(model)
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;class MemoryTest(pl.LightningModule):
    # Main Testing Unit for Experiments on Recurrent Cells
    def __init__(self, hp):
        super(MemoryTest, self).__init__()
        self.predict_col = hp.predict_col
        self.n_datasamples = hp.n_datasamples
        self.dataset = hp.dataset
        if self.dataset is 'rand':
            self.seq_len = None
        else:
            self.seq_len = hp.seq_len
        self.hparams = hp
        self.learning_rate = hp.learning_rate
        self.training_losses = []
        self.final_loss = None

        self.model = RecurrentModel(1, hp.n_cells, hp.n_layers, celltype=hp.celltype)

    def forward(self, input, input_len):
        return self.model(input, input_len)

    def training_step(self, batch, batch_idx):
        x, y, input_len = batch
        features_y = self.forward(x, input_len)

        loss = F.mse_loss(features_y, y)
        mean_absolute_loss = F.l1_loss(features_y, y)

        self.training_losses.append(mean_absolute_loss.item())

        neptune_logs = {'batch/train_loss': loss, 'batch/mean_absolute_loss': mean_absolute_loss}
        return {'loss': loss, 'batch/mean_absolute_loss': mean_absolute_loss, 'log': neptune_logs}

    def on_epoch_end(self):
        train_loss_mean = np.mean(self.training_losses)
        self.final_loss = train_loss_mean
        self.training_losses = []  # reset for next epoch

    def configure_optimizers(self):
        return torch.optim.SGD(self.parameters(), lr=self.learning_rate)

    @pl.data_loader
    def train_dataloader(self):
        train_dataset = dg.RandomDataset(self.predict_col, self.n_datasamples)
        if self.dataset == 'rand_fix':
            train_dataset = dg.RandomDatasetFix(self.predict_col, self.n_datasamples, self.seq_len)
        if self.dataset == 'correlated':
            train_dataset = dg.CorrelatedDataset(self.predict_col, self.n_datasamples)
        train_loader = DataLoader(dataset=train_dataset, batch_size=1)
        return train_loader

    @staticmethod
    def add_model_specific_args(parent_parser):
        # MODEL specific
        model_parser = ArgumentParser(parents=[parent_parser])
        model_parser.add_argument('--learning_rate', default=1e-2, type=float)
        model_parser.add_argument('--n_layers', default=1, type=int)
        model_parser.add_argument('--n_cells', default=5, type=int)
        model_parser.add_argument('--celltype', default='LSTM', type=str)

        # training specific (for this model)
        model_parser.add_argument('--epochs', default=500, type=int)
        model_parser.add_argument('--patience', default=5, type=int)
        model_parser.add_argument('--min_delta', default=0.1, type=float)
        model_parser.add_argument('--early_stopping', default='yes', type=str)

        # data specific
        model_parser.add_argument('--n_datasamples', default=1000, type=int)
        model_parser.add_argument('--seq_len', default=10, type=int)
        model_parser.add_argument('--dataset', default='rand', type=str)
        model_parser.add_argument('--predict_col', default=1, type=int)

        return model_parser
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Early-stopping to take effect again.
	</description>
	<comments>
		<comment id='1' author='Dunrar' date='2020-03-20T22:44:23Z'>
		&lt;denchmark-link:https://github.com/Dunrar&gt;@Dunrar&lt;/denchmark-link&gt;
 would you check it on actual master?
		</comment>
		<comment id='2' author='Dunrar' date='2020-03-21T13:22:03Z'>
		&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 do you mean the bleeding edge version via ?
		</comment>
		<comment id='3' author='Dunrar' date='2020-03-21T14:44:39Z'>
		Okay, I tried that but early stopping still does not work
		</comment>
		<comment id='4' author='Dunrar' date='2020-03-21T15:16:18Z'>
		The code sample you provide does not define a validation step/end/dataloader.
I would expect that early stopping does not work without it. How could it?
		</comment>
		<comment id='5' author='Dunrar' date='2020-03-21T15:19:00Z'>
		if no val step is present, it uses the training step for early stopping
		</comment>
		<comment id='6' author='Dunrar' date='2020-03-21T15:20:04Z'>
		oh, my bad! Then I will have a closer look at this issue.
		</comment>
		<comment id='7' author='Dunrar' date='2020-03-24T12:55:29Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 little update. In training_loop.py the line  is to blame. Just deleting the self.disable_validation and is_val_epoch checks solves the problem in my case, but there is probably more to take into consideration.
		</comment>
		<comment id='8' author='Dunrar' date='2020-03-24T13:05:25Z'>
		I also came to that point when I looked at it 2 days ago, will have more time to look at it soon. If I remember correctly, the tests didnt pass and I was tracking down at which point the change was introduced to figure out the reason it is there.
		</comment>
		<comment id='9' author='Dunrar' date='2020-03-31T06:50:43Z'>
		&lt;denchmark-link:https://github.com/Dunrar&gt;@Dunrar&lt;/denchmark-link&gt;
 Thanks for the help. Your suggestion worked and I was able to make a test so that it doesn't break in the future :)
cheers!
		</comment>
		<comment id='10' author='Dunrar' date='2020-03-31T06:54:26Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 Thank you!
		</comment>
	</comments>
</bug>