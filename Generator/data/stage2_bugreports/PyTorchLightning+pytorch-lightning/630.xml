<bug id='630' author='a-y-khan' open_date='2019-12-16T02:47:10Z' closed_time='2020-03-07T00:27:17Z'>
	<summary>Pickle error from Trainer.fit when using MLFlowLogger and distributed data parallel without SLURM</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Trainer.fit fails with a pickle error when the logger is MLFlowLogger, and distributed_backend='ddp' on GPUs but without SLURM.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Instantiate MLFlowLogger in Pytorch 0.5.3.2 with Pytorch 1.3.1 and MLFlow 1.4.0. The execution environment has environment variables MLFLOW_TRACKING_URI, and also MLFLOW_TRACKING_USERNAME and MLFLOW_TRACKING_PASSWORD to connect to the MLflow tracking server with HTTP Basic Authentication. The MLflow tracking server is also v1.4.0.
Instantiate Trainer with MLFlowLogger instance as logger, distributed_backend='ddp' and with the gpus parameter on a machine with NVIDIA GPUs but without SLURM.
Run Trainer.fit

From the error output, it looks like multiprocessing is attempting to pickle the nested function in MLflow function &lt;denchmark-link:https://github.com/mlflow/mlflow/blob/v1.4.0/mlflow/tracking/_tracking_service/utils.py#L81&gt;_get_rest_store&lt;/denchmark-link&gt;
:
&lt;denchmark-code&gt;ayla.khan@gpu12:~/photosynthetic$ python test_mlflow.py
Traceback (most recent call last):
  File "test_mlflow.py", line 71, in &lt;module&gt;
    trainer.fit(model)
  File "/mnt/unihome/home/CORP/ayla.khan/miniconda2/envs/photosynthetic/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 343, in fit
    mp.spawn(self.ddp_train, nprocs=self.num_gpus, args=(model,))
  File "/mnt/unihome/home/CORP/ayla.khan/miniconda2/envs/photosynthetic/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 162, in spawn
    process.start()
  File "/mnt/unihome/home/CORP/ayla.khan/miniconda2/envs/photosynthetic/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/mnt/unihome/home/CORP/ayla.khan/miniconda2/envs/photosynthetic/lib/python3.6/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/mnt/unihome/home/CORP/ayla.khan/miniconda2/envs/photosynthetic/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/mnt/unihome/home/CORP/ayla.khan/miniconda2/envs/photosynthetic/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/mnt/unihome/home/CORP/ayla.khan/miniconda2/envs/photosynthetic/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/mnt/unihome/home/CORP/ayla.khan/miniconda2/envs/photosynthetic/lib/python3.6/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
AttributeError: Can't pickle local object '_get_rest_store.&lt;locals&gt;.get_default_host_creds'
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

Sample code tested with a very simple test model (&lt;denchmark-link:https://gist.github.com/a-y-khan/8693d2b186227561a4baf4d03ce75c34&gt;gist&lt;/denchmark-link&gt;
):
&lt;denchmark-code&gt;test_hparams = Namespace()
model = XORGateModel(test_hparams)

logger = MLFlowLogger(experiment_name='test_lightning_logger',
                                          tracking_uri=os.environ['MLFLOW_TRACKING_URI'])
trainer = pl.Trainer(logger=logger, distributed_backend='ddp', gpus='-1')
trainer.fit(model)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Trainer.fit runs without error.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;(photosynthetic) ayla.khan@gpu12:~/photosynthetic$ python collect_env.py
Collecting environment information...
PyTorch version: 1.3.1
Is debug build: No
CUDA used to build PyTorch: 10.1.243

OS: CentOS Linux 7 (Core)
GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)
CMake version: Could not collect

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration:
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti
GPU 2: GeForce GTX 1080 Ti
GPU 3: GeForce GTX 1080 Ti
GPU 4: GeForce GTX 1080 Ti
GPU 5: GeForce GTX 1080 Ti
GPU 6: GeForce GTX 1080 Ti
GPU 7: GeForce GTX 1080 Ti

Nvidia driver version: 440.33.01
cuDNN version: /usr/local/cuda-10.0/lib64/libcudnn.so.7

Versions of relevant libraries:
[pip] numpy==1.16.4
[pip] pytorch-lightning==0.5.3.2
[pip] pytorch-toolbelt==0.2.1
[pip] torch==1.3.1
[pip] torchsummary==1.5.1
[pip] torchvision==0.4.2
[conda] blas                      1.0                         mkl
[conda] mkl                       2019.4                      243
[conda] mkl-service               2.3.0            py36he904b0f_0
[conda] mkl_fft                   1.0.15           py36ha843d7b_0
[conda] mkl_random                1.1.0            py36hd6b4f25_0
[conda] pytorch                   1.3.1           py3.6_cuda10.1.243_cudnn7.6.3_0    pytorch
[conda] pytorch-lightning         0.5.3.2                  pypi_0    pypi
[conda] pytorch-toolbelt          0.2.1                    pypi_0    pypi
[conda] torchsummary              1.5.1                    pypi_0    pypi
[conda] torchvision               0.4.2                py36_cu101    pytorch
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='a-y-khan' date='2019-12-16T03:20:11Z'>
		Will investigate. We have a test that is supposed to prevent these problems from sneaking back in, but apparently it's not doing it's job.
		</comment>
		<comment id='2' author='a-y-khan' date='2020-01-14T18:32:22Z'>
		I imagine everyone is busy with the build failures - but for the record, I am  having a similar problem. Essentially, I cannot get a logger to work using ddp. It's gving me one of those days when I wonder why I ever wanted to write software ;)
This is Ubuntu 18.04.2LTS, on a 14 core, 7 gpu machine. Python 3.6.8, pytorch 1.3.1, pytorch-lightning 0.5.3.2, Tensorboard 2.1.0. Everything else standard except pillow isis 6.2.2 due to known bug in 7.0.
I am working with a tried and true model and hyperparameters. The model and logging work fine as cpu, gpu, or dp - and ddp if I don't log. But not ddp with logging. I am not using SLURM.
I have tried to get around this several ways: passing a custom logger, not using the logger created by Trainer(), etc. They either fail when called from one of the new processes, with an attribute error in Tensorboard TTDummyFileWriter.get_logdir(), or they fail with a pickle error about thread.locks when being copied to a new process
I will detail these in a bug report if you think they are NOT due to the recent build issues.
But thought you'd want to know ...
s
		</comment>
		<comment id='3' author='a-y-khan' date='2020-02-11T17:39:25Z'>
		&lt;denchmark-link:https://github.com/dbczumar&gt;@dbczumar&lt;/denchmark-link&gt;
,  &lt;denchmark-link:https://github.com/smurching&gt;@smurching&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='4' author='a-y-khan' date='2020-03-03T22:01:41Z'>
		&lt;denchmark-link:https://github.com/neggert&gt;@neggert&lt;/denchmark-link&gt;
 is this fixed now?
		</comment>
		<comment id='5' author='a-y-khan' date='2020-06-17T15:45:55Z'>
		Can this issue be re-opened? I'm currently working with Pytorch-Lightning==0.7.6 and am getting an identical pickle issue when using DDP with the MLFLowLogger.
Reproducing
Using the script the OP gave led to some other errors (mostly to do with lightning version differences), so a new gist to reproduce in Pytorch-Lightning 0.7.6 can be found &lt;denchmark-link:https://gist.github.com/Polyphenolx/39424e5673fc029567f7f3ae3551fffb&gt;here&lt;/denchmark-link&gt;
.
This is easily reproducible in other projects as well.
Error Output
&lt;denchmark-code&gt;/home/user/anaconda3/envs/pytorch/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:23: DeprecationWarning: `logging` package has been renamed to `loggers` since v0.7.0 The deprecated package name will be removed in v0.9.0.
  warnings.warn(*args, **kwargs)
/home/user/anaconda3/envs/pytorch/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:23: DeprecationWarning: `mlflow_logger` module has been renamed to `mlflow` since v0.6.0. The deprecated module name will be removed in v0.8.0.
  warnings.warn(*args, **kwargs)
/home/user/anaconda3/envs/pytorch/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:23: DeprecationWarning: `data_loader` decorator deprecated in v0.7.0. Will be removed v0.9.0
  warnings.warn(*args, **kwargs)
GPU available: True, used: True
No environment variable for node rank defined. Set as 0.
CUDA_VISIBLE_DEVICES: [0,1,2,3]
Traceback (most recent call last):
  File "mlflow_test.py", line 65, in &lt;module&gt;
    trainer.fit(model)
  File "/home/user/anaconda3/envs/pytorch/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 844, in fit
    mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))
  File "/home/user/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/user/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 149, in start_processes
    process.start()
  File "/home/user/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/user/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/user/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/user/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/user/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/user/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
AttributeError: Can't pickle local object '_get_rest_store.&lt;locals&gt;.get_default_host_creds'
&lt;/denchmark-code&gt;

Environment
&lt;denchmark-code&gt;* CUDA:
	- GPU:
		- GeForce GTX 1080 Ti
		- GeForce GTX 1080 Ti
		- GeForce GTX 1080 Ti
		- GeForce GTX 1080 Ti
	- available:         True
	- version:           10.2
* Packages:
	- numpy:             1.18.5
	- pyTorch_debug:     False
	- pyTorch_version:   1.5.0
	- pytorch-lightning: 0.7.6
	- tensorboard:       2.2.2
	- tqdm:              4.46.1
* System:
	- OS:                Linux
	- architecture:
		- 64bit
		- 
	- processor:         x86_64
	- python:            3.6.8
	- version:           #102-Ubuntu SMP Mon May 11 10:07:26 UTC 2020
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='a-y-khan' date='2020-06-17T18:44:05Z'>
		To add to this, it appears to be a greater issue with MLFLow and how their tracking utilities are coded. They use a higher order function that causes issues with pickling in torches DDP backend. I've created an issue on MLFLow git, and submitted a PR to remedy the problem.
In the interim, feel free to implement the fix described in the issue in the MLFlow git as a temporary fix until/if they review/merge mine
		</comment>
		<comment id='7' author='a-y-khan' date='2020-06-23T17:09:05Z'>
		Following up on this: The pickling fix was merged into the master branch of MLFlow a couple days ago (see the bug mention above). Training using DDP is now functional on MLFLow versions installed from master, but it may take them some time to release the fix to PyPi
		</comment>
	</comments>
</bug>