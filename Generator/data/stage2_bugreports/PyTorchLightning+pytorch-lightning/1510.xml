<bug id='1510' author='alexeykarnachev' open_date='2020-04-16T16:25:24Z' closed_time='2020-04-19T20:41:55Z'>
	<summary>Memory (CPU and GPU) leaks during the 1st epoch</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Hello.
This memory leak occurs during the first epoch. If one has a large epoch time (I had &gt; 10 days), the OOM error will come. It's interesting, that in precision=16 mode, it leaks out on the GPU and the CPU both. If we switch amp optimization off (precision=32), the leak goes only on the CPU.
Also, I checked the number of tensors, which are tracked by the garbage collector. And it appeared to be linearly increasing during the first epoch, and then (on the 2nd epoch starts), it falls to the initial value and begins increasing again.
Let me provide the plots:
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Experiment 1: amp_level='O2', precision=16
&lt;denchmark-link:https://user-images.githubusercontent.com/7495098/79478408-0cc28f80-8014-11ea-861b-1b9443de5351.png&gt;&lt;/denchmark-link&gt;


&lt;denchmark-link:https://user-images.githubusercontent.com/7495098/79478523-324f9900-8014-11ea-8e05-c9ef20b1c4d8.png&gt;&lt;/denchmark-link&gt;


&lt;denchmark-link:https://user-images.githubusercontent.com/7495098/79478667-5ad79300-8014-11ea-8727-545230c81649.png&gt;&lt;/denchmark-link&gt;


&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Experiment 2: amp_level=None, precision=None
&lt;denchmark-link:https://user-images.githubusercontent.com/7495098/79478906-a4c07900-8014-11ea-80f6-9284bf86eafe.png&gt;&lt;/denchmark-link&gt;


&lt;denchmark-link:https://user-images.githubusercontent.com/7495098/79478952-b4d85880-8014-11ea-849c-f38c3e1a88ce.png&gt;&lt;/denchmark-link&gt;


&lt;denchmark-link:https://user-images.githubusercontent.com/7495098/79478999-c588ce80-8014-11ea-8755-4d2a2d7451d5.png&gt;&lt;/denchmark-link&gt;


&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

As you can see, both cases have a CPU leak. The "amp"-case also has a GPU leak.
Also, it's clear, that such leaky behavior stops when the 2nd epoch starts.
On these plots, the 2nd epoch starts on the 2nd "saw claw" of the "Num-of-tensors" plot.
Also, there is another observation: the speed of tensors number increasing is 1001. And this is my forward pass method:
    def training_step(self, batch, batch_idx):
        losses = self.forward(batch)
        num_of_tensors = get_num_of_tensors()
        log = {'Num-of-tensors': num_of_tensors, 'Cpu-mem-usg': get_cpu_mem()}

        for i, loss in enumerate(losses):
            log[f'loss{i}'] = loss

        print(num_of_tensors)
        return {'loss': losses[0], 'log': log}
Here I return exactly 1001 tensor: one for loss and 1000 for log.
In my real experiments I had only 3 tensors. It took ~2-3 days to get OOM. But in the current example (see To Reproduce) it will crash much faster.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Execute Code sample (this script has no arguments, so change needed values manually in script).
Go to the tensorboard to check plots.

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-link:https://gist.github.com/alexeykarnachev/47de06b93a717ab0664eded42ed2826a&gt;https://gist.github.com/alexeykarnachev/47de06b93a717ab0664eded42ed2826a&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

The number of tensors, GPU and CPU memory does not increase during the training.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

PyTorch version: 1.4.0
OS: Ubuntu 16.04.6 LTS
Python version: 3.7
Versions of relevant libraries:
[pip] numpy==1.18.1
[pip] pytorch-lightning==0.7.3
[pip] torch==1.4.0
[pip] torchvision==0.5.0
&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

Sorry for so messy flow of the information, but I don't know, how to structure it more clearly.
	</description>
	<comments>
		<comment id='1' author='alexeykarnachev' date='2020-04-16T16:35:20Z'>
		by leak you mean tensors build up during epoch 1? but after that the memory stays constant? ie: there is no more "leak" for epochs &gt;= 2?
		</comment>
		<comment id='2' author='alexeykarnachev' date='2020-04-16T16:41:11Z'>
		Yes, the memory stays constant after 1st epoch ends (although, the number of tensors begins increasing again)
		</comment>
		<comment id='3' author='alexeykarnachev' date='2020-04-16T20:06:55Z'>
		The whole output of a training step is stored.
In your code with every training step, there are new tensors created.
With z log[f'loss{i}'] = loss.item()there is no leak.
I think there is a mistake in optimizer_closure() in the training loop which, returns whole batch output dict. It should be enough to return only callback_metrics instead of the whole batch output.
		</comment>
		<comment id='4' author='alexeykarnachev' date='2020-04-16T20:47:48Z'>
		Yes, I agreed, that with .item() there is no leak because all tensors "disappear in place" (I did not check it, but I believe that it so). But, I suppose, that .item() will slow my code.
On the other hand, .item() is performed anyway by the Trainer itself (before logging), so maybe it's not a big deal to call .item() beforehand. At least as a hotfix solution
		</comment>
		<comment id='5' author='alexeykarnachev' date='2020-04-16T22:47:18Z'>
		Oh, no sorry, just checked: it will be a leak even if we perform log[f'loss{i}'] = loss.item()
Because we still have 'loss': losses[0] part (the actual loss tensor, which needs to be minimized).
So, it will be a leak with speed 1 tensor per step. It's very slow, but the OOM will occur anyway in 6-9 days
		</comment>
		<comment id='6' author='alexeykarnachev' date='2020-04-17T00:33:33Z'>
		can you submit a PR? i thought we took care of all the metrics.
we should also use detach instead of item no? to not slow code down
		</comment>
		<comment id='7' author='alexeykarnachev' date='2020-04-17T06:20:31Z'>
		We take care of it in process_output() but then in optimizer_closure() we return original output_dict again.
We pass then a list of original outputs to the training_epoch_end().
I think w should not do that bc loss, log and progress_bar is handling by us in a proper way so we should return to training_epoch_end only other keys from output_dict and let a user manage it.
		</comment>
		<comment id='8' author='alexeykarnachev' date='2020-04-17T08:41:19Z'>
		What about fp32-mode? There is no leak on the GPU in such a case. What could be the reason?
		</comment>
		<comment id='9' author='alexeykarnachev' date='2020-04-17T11:36:23Z'>
		@AratorField , do you mean this?



pytorch-lightning/pytorch_lightning/trainer/training_loop.py


        Lines 427 to 428
      in
      9b31272






 # bookkeeping 



 outputs = [] 





Here is a list that stores all train step outputs during the epoch.
		</comment>
		<comment id='10' author='alexeykarnachev' date='2020-04-17T11:47:21Z'>
		but we detach everything.



pytorch-lightning/pytorch_lightning/trainer/training_loop.py


         Line 448
      in
      9b31272






 outputs.append(_recursive_detach(batch_output)) 





how could it leak?
		</comment>
		<comment id='11' author='alexeykarnachev' date='2020-04-17T11:55:28Z'>
		Yes, but they (tensors) are still on the GPU after detach. So, in case of long epochs or huge outputs from the training step, the GPU memory will blow after some time.
		</comment>
		<comment id='12' author='alexeykarnachev' date='2020-04-17T12:39:52Z'>
		We can create something like _recursive_item() or remove keys loss, log, progress_bar from batch_output before appending to outputs.
		</comment>
		<comment id='13' author='alexeykarnachev' date='2020-04-17T13:45:17Z'>
		Is it in general a good practice to store values during the epoch? The size of such a bookkeeping list is undetermined in the general case. I mean, that one could have almost an infinite epoch and sooner or later he'll be faced with OOM (GPU or CPU, it does not matter).
		</comment>
		<comment id='14' author='alexeykarnachev' date='2020-04-17T13:59:01Z'>
		the thing is that .item() slows things down.
so we want to detach but not .item().
The tradeoff is that we plug the memory leak but slow things down.
		</comment>
		<comment id='15' author='alexeykarnachev' date='2020-04-17T14:03:27Z'>
		There is no reason to store loss, log and progress_bar for the whole epoch.
Any other key in output_dict could be valuable and has to be stored i.e. for metrics calculating.
		</comment>
		<comment id='16' author='alexeykarnachev' date='2020-04-17T14:11:32Z'>
		Maybe it's possible to introduce a flag, which shows, should we store tensors in this list during an epoch or not.
Or, maybe you can advise me some hot-fix, that I can apply locally. Because now, I can not train even 1 epoch :)
		</comment>
		<comment id='17' author='alexeykarnachev' date='2020-04-17T14:16:33Z'>
		I even have no training_epoch_end method. Maybe, we can check if this method is not determined by the user, we can skip batch results bookkeeping?
		</comment>
		<comment id='18' author='alexeykarnachev' date='2020-04-17T14:28:42Z'>
		


pytorch-lightning/pytorch_lightning/trainer/training_loop.py


         Line 611
      in
      8544b33






 return closure_loss, output_dict 





change it to return closure_loss , callback_metrics
		</comment>
		<comment id='19' author='alexeykarnachev' date='2020-04-17T14:34:48Z'>
		Thank you, I'll patch it locally for now.
		</comment>
	</comments>
</bug>