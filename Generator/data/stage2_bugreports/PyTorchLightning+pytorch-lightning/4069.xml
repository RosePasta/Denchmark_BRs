<bug id='4069' author='peymanpoozesh' open_date='2020-10-11T04:37:14Z' closed_time='2020-10-11T06:28:19Z'>
	<summary>backward callback does not work on pytorch-lightning version 1.0.0rc3</summary>
	<description>
In pytorch-lightning version 0.10 the following code works well. However in pytorch-lightning version 1.0.0rc3, the code does not work
and gives the following error:
TypeError: backward() missing 1 required positional argument: 'optimizer_idx'
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;def configure_optimizers(self):
      optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr)
      return optimizer

def backward(self, trainer, loss, optimizer, optimizer_idx):
        loss.backward(retain_graph=True)

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='peymanpoozesh' date='2020-10-11T06:10:18Z'>
		The new backward hook has arguments: loss, optimizer, optimizer_idx.
You need to remove the trainer argument.
If you need to access trainer, you can do that by self.trainer.



pytorch-lightning/pytorch_lightning/core/lightning.py


         Line 1058
      in
      9558162






 def backward(self, loss: Tensor, optimizer: Optimizer, optimizer_idx: int) -&gt; None: 





		</comment>
	</comments>
</bug>