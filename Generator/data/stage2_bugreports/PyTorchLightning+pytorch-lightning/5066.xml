<bug id='5066' author='lucadiliello' open_date='2020-12-10T19:55:20Z' closed_time='2020-12-10T22:13:39Z'>
	<summary>Incorrect example in `configure_optimizers`</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Example shows to use scheduler key to pass the scheduler in the dict. Correct key is lr_scheduler instead.
The example is:
{
    'scheduler': lr_scheduler, # The LR schduler
    'interval': 'epoch', # The unit of the scheduler's step size
    'frequency': 1, # The frequency of the scheduler
    'reduce_on_plateau': False, # For ReduceLROnPlateau scheduler
    'monitor': 'val_loss', # Metric for ReduceLROnPlateau to monitor
    'strict': True # Whether to crash the training if `monitor` is not found
}
but should be
{
    'lr_scheduler': lr_scheduler, # The LR schduler
    'interval': 'epoch', # The unit of the scheduler's step size
    'frequency': 1, # The frequency of the scheduler
    'reduce_on_plateau': False, # For ReduceLROnPlateau scheduler
    'monitor': 'val_loss', # Metric for ReduceLROnPlateau to monitor
    'strict': True # Whether to crash the training if `monitor` is not found
}
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Just use the wrong scheduler key. No exception will be raised but no scheduler will be used! I think many people, like me, do not know that their scheduler is not running. I would add at least some warning.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version (e.g., 1.7.1):
OS (e.g., Linux): Linux
How you installed PyTorch (conda, pip, source): pip
Build command you used (if compiling from source):
Python version: 3.8.5
CUDA/cuDNN version:
GPU models and configuration:
Any other relevant information:

	</description>
	<comments>
		<comment id='1' author='lucadiliello' date='2020-12-10T21:04:18Z'>
		no it's correct. If you'll use 'lr_scheduler' key, it will raise an exception.
		</comment>
		<comment id='2' author='lucadiliello' date='2020-12-10T21:31:38Z'>
		Double checked on master:

See pytorch_lightning.trainer.optimizers -&gt; TrainerOptimizersMixin.init_optimizers line 55
Using lr_scheduler LRs are logger, using scheduler nothing is logged and I get the warning: RuntimeWarning: You are using LearningRateMonitor callback with models that have no learning rate schedulers. Please see documentation for configure_optimizers method.

		</comment>
		<comment id='3' author='lucadiliello' date='2020-12-10T21:34:11Z'>
		mind reproduce it with &lt;denchmark-link:https://colab.research.google.com/drive/1HvWVVTK8j2Nj52qU4Q4YCyzOm0_aLQF3?usp=sharing&gt;https://colab.research.google.com/drive/1HvWVVTK8j2Nj52qU4Q4YCyzOm0_aLQF3?usp=sharing&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='4' author='lucadiliello' date='2020-12-10T21:53:39Z'>
		it is difficult to reproduce it in Colab since I do not know where the value of learning rates is stored and I cannot check whether it is changing or not. I noticed it from the tensorboard plots.
		</comment>
		<comment id='5' author='lucadiliello' date='2020-12-10T21:56:36Z'>
		can you put your configure_optimizer code here? I'll try with master/
		</comment>
		<comment id='6' author='lucadiliello' date='2020-12-10T22:13:39Z'>
		Closing since the issue was mine misinterpretation of the doc. I was convinced that the given example was about the return of the configure_optimizers method but was wrt the lr_dict instead. Sorry for bothering you.
		</comment>
	</comments>
</bug>