<bug id='616' author='brucemuller' open_date='2019-12-09T12:33:19Z' closed_time='2020-02-11T15:19:41Z'>
	<summary>GPU memory usage/leak varies between experiments/trials with Apex (use_amp=True)</summary>
	<description>
Using use_amp=True with hyper parameter searching results in increased GPU memory usage between experiments. So it'll go from initially 7GB usage to maybe 10GB usage between learning rate trials and then vary between that.
For example:
&lt;denchmark-code&gt;parent_parser = HyperOptArgumentParser(strategy='grid_search', add_help=False)
    
parent_parser.opt_list('--lr', default = 0.01, type=float, tunable=True, options = [0.00005, 0.00006, 0.00007, 0.00008, 0.00009, 0.0001, 0.00011, 0.00012, 0.00013, 0.00014, 0.00015])
    
    hyperparams = parent_parser.parse_args()

    for hparam_trial in hyperparams.trials(20):
        print(hparam_trial)
        main_local(hparam_trial)   
        print("NEXT TRIAL")

def main_local(hparams):

    
    model = Model(hparams)
    expname = "HPSEARCH" +  '_' + str(hparams.lr) + '_' + str(hparams.batch_size)
    tt_logger = TestTubeLogger(save_dir="/",name=expname,debug=False,create_git_tag=False)
    
    trainer = Trainer(logger = tt_logger , use_amp=True, amp_level='O2' , gpus=1,min_nb_epochs=40, max_nb_epochs=41)
      
    torch.cuda.empty_cache()
&lt;/denchmark-code&gt;


PyTorch Version: 1.3.1
Python version: 3.7.5
CUDA/cuDNN version: Driver Version: 440.33.01    CUDA Version: 10.2
Apex: 0.1
Lightning: 0.5.3.2

	</description>
	<comments>
		<comment id='1' author='brucemuller' date='2019-12-09T22:36:27Z'>
		If you put a sleep after the torch.cuda.empty_cache() (or sometime between runs) what is the GPU usage?
Does the memory gradually increase with each run, or does it go back down after the 10gb spike?
		</comment>
		<comment id='2' author='brucemuller' date='2019-12-10T08:33:58Z'>
		&lt;denchmark-link:https://github.com/jeffling&gt;@jeffling&lt;/denchmark-link&gt;
 thanks for your reply. I did some testing with sleep after torch.cuda.empty_cache()
With use_amp = True: GPU memory varies between 7.1gb on the initial run to 10.0gb on another.
After torch.cuda.empty_cache() it varies between 1.7gb and 5.5gb.
With use_amp = False: memory varies between 10.2gb on one run and 11gb on another. After torch.cuda.empty_cache()  the memory varies between 1.6gb and 4.3gb.
In all variations the memory is increasing between trials a few times times and then jumps down and increases gradually again. It does go back down after the maximum spike but then appears to increase again...eventually it runs out of memory altogether.
Any ideas?
		</comment>
		<comment id='3' author='brucemuller' date='2019-12-10T20:33:22Z'>
		&lt;denchmark-link:https://github.com/brucemuller&gt;@brucemuller&lt;/denchmark-link&gt;
  Have you tried directly setting the model to cpu after your run? 
I work with a similar flow and setting that helped with our memory issues.
		</comment>
		<comment id='4' author='brucemuller' date='2019-12-12T10:32:20Z'>
		&lt;denchmark-link:https://github.com/jeffling&gt;@jeffling&lt;/denchmark-link&gt;
 thanks, I'll try that. It looks like it's not a problem with using multi-GPU/ddp but I'll keep investigating
		</comment>
		<comment id='5' author='brucemuller' date='2020-02-11T15:19:40Z'>
		&lt;denchmark-link:https://github.com/jeffling&gt;@jeffling&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/brucemuller&gt;@brucemuller&lt;/denchmark-link&gt;
 is there anything here  we  can  fix to help?  If  so, we can  reopen this.
		</comment>
		<comment id='6' author='brucemuller' date='2020-02-12T09:05:47Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 It's a problem when not using not using distributed backend and using apex for me.
I stopped using apex because it was killing my classification performance, which was the bigger issue.
Do you know why use_amp=true might reduce performance significantly and if there's anything we can do about it?
		</comment>
		<comment id='7' author='brucemuller' date='2020-02-12T12:21:45Z'>
		apex has a few problems... soon itâ€™ll be in pytorch and we can use that. Maybe try different optimization levels?
&lt;denchmark-link:https://github.com/ptrblck&gt;@ptrblck&lt;/denchmark-link&gt;
, any thoughts?
		</comment>
		<comment id='8' author='brucemuller' date='2020-02-12T16:16:09Z'>
		apex.amp wasn't designed to reinitialize models as is done e.g. for a grid search, which might yield to an increased usage of your device memory.
&lt;denchmark-link:https://github.com/brucemuller&gt;@brucemuller&lt;/denchmark-link&gt;
 which  are you using how how significantly is your performance reduced?
		</comment>
	</comments>
</bug>