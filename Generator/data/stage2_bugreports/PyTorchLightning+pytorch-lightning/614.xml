<bug id='614' author='falceeffect' open_date='2019-12-09T11:42:51Z' closed_time='2020-04-09T11:53:27Z'>
	<summary>empty_cache calls in training occupy memory on gpu #0</summary>
	<description>
Training on GPU other than gpu #0 allocates a ~500Mb chunk of memory on gpu #0, the memory is totally unused and should not be allocated at all. Debugging shows that initial allocation happens at this line: &lt;denchmark-link:https://github.com/williamFalcon/pytorch-lightning/blob/2f01c03b38fc16618aa9839d39e0ae5a142c0559/pytorch_lightning/trainer/trainer.py#L517&gt;https://github.com/williamFalcon/pytorch-lightning/blob/2f01c03b38fc16618aa9839d39e0ae5a142c0559/pytorch_lightning/trainer/trainer.py#L517&lt;/denchmark-link&gt;

A bit of research led me to this issue in PyTorch repo: &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/25752&gt;pytorch/pytorch#25752&lt;/denchmark-link&gt;
. This is not the only place where  is called, by the way. Did not check, but other calls probably work the same way.
For now I duct-tape-fixed it for myself by running my script with CUDA_VISIBLE_DEVICES=2 and setting gpus=[0]. Not sure how to fix it properly, though. Would be glad if someone would take a look.
	</description>
	<comments>
		<comment id='1' author='falceeffect' date='2019-12-09T22:40:17Z'>
		This is primarily a pytorch problem, but a fix for us could be to always use torch.cuda.set_device somewhere before our first empty_cache call. I'm not sure what other side effects that might have. Feel free to submit a PR if you'd like
		</comment>
		<comment id='2' author='falceeffect' date='2020-03-05T13:29:14Z'>
		CUDA_VISIBLE_DEVICES=2 did not solve it for me, but adding torch.cuda.set_device(2) did
		</comment>
		<comment id='3' author='falceeffect' date='2020-03-26T15:40:13Z'>
		mind check  &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1094&gt;#1094&lt;/denchmark-link&gt;
? &lt;denchmark-link:https://github.com/jeffling&gt;@jeffling&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/annukkaa&gt;@annukkaa&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/falceeffect&gt;@falceeffect&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='falceeffect' date='2020-04-09T11:53:27Z'>
		feel free to re-open if needed ðŸ¤–
		</comment>
	</comments>
</bug>