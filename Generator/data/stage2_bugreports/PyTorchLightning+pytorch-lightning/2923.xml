<bug id='2923' author='johngrabner' open_date='2020-08-12T01:26:54Z' closed_time='2020-08-14T00:19:58Z'>
	<summary>Documentation</summary>
	<description>
Documentation at &lt;denchmark-link:url&gt;https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.callbacks.lr_logger.html&lt;/denchmark-link&gt;

says:
&lt;denchmark-code&gt;&gt;&gt;&gt; from pytorch_lightning import Trainer
&gt;&gt;&gt; from pytorch_lightning.callbacks import LearningRateLogger
&gt;&gt;&gt; lr_logger = LearningRateLogger(logging_interval='step')
&gt;&gt;&gt; trainer = Trainer(callbacks=[lr_logger])
&lt;/denchmark-code&gt;

The above results in error
Traceback (most recent call last):
&lt;denchmark-code&gt;  File "kiss_transformer.py", line 523, in &lt;module&gt;
    lr_logger = LearningRateLogger(logging_interval='step')
TypeError: __init__() got an unexpected keyword argument 'logging_interval'
&lt;/denchmark-code&gt;

When I drop logging_interval='step'
It logs on tensorboard step 0, but I would like all steps, the same as in &lt;denchmark-link:URL&gt;logging_interval='step'&lt;/denchmark-link&gt;
 example from RedEyed.
&lt;denchmark-code&gt;def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)

        def lr_foo(epoch):
            if epoch &lt; self.hparams.warm_up_step:
                # warm up lr
                lr_scale = 0.1 ** (self.hparams.warm_up_step - epoch)
            else:
                lr_scale = 0.95 ** epoch

            return lr_scale

        scheduler = torch.optim.lr_scheduler.LambdaLR(
            optimizer,
            lr_lambda=lr_foo
        )

        return [optimizer], [scheduler]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='johngrabner' date='2020-08-12T01:27:37Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='johngrabner' date='2020-08-12T05:37:27Z'>
		Hey &lt;denchmark-link:https://github.com/johngrabner&gt;@johngrabner&lt;/denchmark-link&gt;
,
this is from the recent merged PR &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2847&gt;#2847&lt;/denchmark-link&gt;
, this hasn't been released
You could try master for that if you want.
		</comment>
		<comment id='3' author='johngrabner' date='2020-08-14T01:07:20Z'>
		Ok, will give it a shot
		</comment>
	</comments>
</bug>