<bug id='2649' author='discharged-spider' open_date='2020-07-20T18:03:14Z' closed_time='2020-08-05T17:04:51Z'>
	<summary>Integer limit_val_batches does not work with infinite DataLoader.</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Documentation states that you can use limit_val_batches=100 (with integer value) to limit number of batches. However, when using IterableDataset I get the following error:
pytorch_lightning.utilities.exceptions.MisconfigurationException: When using an infinite DataLoader (e.g. with an IterableDataset or when DataLoader does not implement __len__) for limit_val_batches, Trainer(limit_val_batches) must be 0.0 or 1.0.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Just use:
pl.Trainer(val_check_interval=10, limit_val_batches=100)
with infinite DataLoader.
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Do only N (100 in this case) validation steps.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:
- GPU:
- GeForce GTX 1080 Ti
- available:         True
- version:           10.0.130
Packages:
- numpy:             1.17.4
- pyTorch_debug:     False
- pyTorch_version:   1.3.1
- pytorch-lightning: 0.8.5
- tensorboard:       2.2.2
- tqdm:              4.46.1
System:
- OS:                Linux
- architecture:
- 64bit
-
- processor:         x86_64
- python:            3.7.4
- version:           #1 SMP Wed Mar 7 19:03:37 UTC 2018

	</description>
	<comments>
		<comment id='1' author='discharged-spider' date='2020-07-20T18:04:18Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='discharged-spider' date='2020-07-21T18:55:15Z'>
		The bug is here:



pytorch-lightning/pytorch_lightning/trainer/data_loading.py


        Lines 303 to 316
      in
      6d10ac2






 num_batches = len(dataloader) if _has_len(dataloader) else float('inf') 



 self._worker_check(dataloader, f'{mode} dataloader {i}') 



 



 # percent or num_steps 



 limit_eval_batches = getattr(self, f'limit_{mode}_batches') 



 



 if num_batches != float('inf'): 



 self._check_batch_limits(f'limit_{mode}_batches') 



 



 # limit num batches either as a percent or num steps 



 if isinstance(limit_eval_batches, float): 



 num_batches = int(num_batches * limit_eval_batches) 



 else: 



 num_batches = min(len(dataloader), limit_eval_batches) 





		</comment>
	</comments>
</bug>