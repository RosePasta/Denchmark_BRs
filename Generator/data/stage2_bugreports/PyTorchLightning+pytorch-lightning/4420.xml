<bug id='4420' author='ohmeow' open_date='2020-10-29T03:28:08Z' closed_time='2020-11-10T11:37:14Z'>
	<summary>NCCL error using DDP and  PyTorch 1.7</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Getting this error when attempting to use ddp with the "getting started" autoencoder example:
Stack Trace:
&lt;denchmark-code&gt;GPU available: True, used: True
TPU available: False, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2]
initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2
Traceback (most recent call last):
  File "01_getting_started_autoencoder.py", line 66, in &lt;module&gt;
    modle, trainer = cli_main()
  File "01_getting_started_autoencoder.py", line 60, in cli_main
    trainer.fit(model, train_dl)
  File "/home/user/anaconda3/envs/playground-pl/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 440, in fit
Traceback (most recent call last):
  File "/home/user/development/_training/ml/pl-playground/01_getting_started_autoencoder.py", line 66, in &lt;module&gt;
    results = self.accelerator_backend.train()
  File "/home/user/anaconda3/envs/playground-pl/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 138, in train
    results = self.ddp_train(process_idx=self.task_idx, model=model)
  File "/home/user/anaconda3/envs/playground-pl/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 231, in ddp_train
    self.trainer.is_slurm_managing_tasks
  File "/home/user/anaconda3/envs/playground-pl/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 213, in init_ddp_connection
    torch_backend, rank=global_rank, world_size=world_size
  File "/home/user/anaconda3/envs/playground-pl/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py", line 442, in init_process_group
    barrier()
  File "/home/user/anaconda3/envs/playground-pl/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py", line 1947, in barrier
    modle, trainer = cli_main()
  File "/home/user/development/_training/ml/pl-playground/01_getting_started_autoencoder.py", line 60, in cli_main
    trainer.fit(model, train_dl)
  File "/home/user/anaconda3/envs/playground-pl/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 440, in fit
    results = self.accelerator_backend.train()
  File "/home/user/anaconda3/envs/playground-pl/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 138, in train
    work = _default_pg.barrier()
RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:784, invalid usage, NCCL version 2.7.8
    results = self.ddp_train(process_idx=self.task_idx, model=model)
  File "/home/user/anaconda3/envs/playground-pl/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 231, in ddp_train
    self.trainer.is_slurm_managing_tasks
  File "/home/user/anaconda3/envs/playground-pl/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 213, in init_ddp_connection
    torch_backend, rank=global_rank, world_size=world_size
  File "/home/user/anaconda3/envs/playground-pl/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py", line 442, in init_process_group
    barrier()
  File "/home/user/anaconda3/envs/playground-pl/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py", line 1947, in barrier
    work = _default_pg.barrier()
RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:784, invalid usage, NCCL version 2.7.8
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Follow the code in the getting started question with these parameters to Trainer:
&lt;denchmark-code&gt;model = LitAutoEncoder()
trainer = pl.Trainer(gpus='1,2', distributed_backend='ddp')
trainer.fit(model, train_dl)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

For it to train on multiple GPUs :)
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version 1.7:
OS (e.g., Linux): Ubuntu 18.04
How you installed PyTorch (conda, pip, source): pip
Build command you used (if compiling from source): n/a
Python version: 3.7
CUDA/cuDNN version: 10.2/7.6.5
GPU models and configuration: 2 1080Tis
Any other relevant information: n/a

	</description>
	<comments>
		<comment id='1' author='ohmeow' date='2020-10-29T05:44:02Z'>
		Hi, thanks for reporting.
The autoencoder example runs fine for me.
Could you please let me know the Lightning version you are using?
We recently fixed a bug, please use 1.0.4 or newer.
		</comment>
		<comment id='2' author='ohmeow' date='2020-10-29T05:53:24Z'>
		Yah I'm using 1.0.4
Here's the full source for my .py file:
&lt;denchmark-code&gt;import os
import torch
from torch import nn
import torch.nn.functional as F
from torchvision.datasets import MNIST
from torchvision import transforms
from torch.utils.data import DataLoader
import pytorch_lightning as pl
from torch.utils.data import random_split


# define pl module
class LitAutoEncoder(pl.LightningModule):

    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(28*28, 64),
            nn.ReLU(),
            nn.Linear(64, 3)
        )
        self.decoder = nn.Sequential(
            nn.Linear(3, 64),
            nn.ReLU(),
            nn.Linear(64, 28*28)
        )

    def forward(self, x):
        # in lightning, forward defines the prediction/inference actions
        embedding = self.encoder(x)
        return embedding

    def training_step(self, batch, batch_idx):
        # training_step defined the train loop.
        # It is independent of forward
        x, y = batch
        x = x.view(x.size(0), -1)
        z = self.encoder(x)
        x_hat = self.decoder(z)
        loss = F.mse_loss(x_hat, x)

        # Logging to TensorBoard by default
        self.log('train_loss', loss)
        return loss

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)
        return optimizer


# define datasets/dataloaders
dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())
train_dl = DataLoader(dataset)


# train
model = LitAutoEncoder()
trainer = pl.Trainer(gpus='0,1', distributed_backend='ddp')
trainer.fit(model, train_dl)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='ohmeow' date='2020-10-29T06:51:37Z'>
		ok, I can confirm this is only happening on pytorch 1.7
		</comment>
		<comment id='4' author='ohmeow' date='2020-10-29T11:06:10Z'>
		I have the same issue on 1080ti, with V100 GPUs everything works fine.
		</comment>
		<comment id='5' author='ohmeow' date='2020-10-30T01:16:12Z'>
		&lt;denchmark-link:https://github.com/maxjeblick&gt;@maxjeblick&lt;/denchmark-link&gt;
 sounds like a driver issue?
Edit:
Certainly very odd that NCCL is bugging out only with 1080ti GPUs...
		</comment>
		<comment id='6' author='ohmeow' date='2020-10-30T06:41:27Z'>
		I tested the following with our examples:
ddp 1080ti pytorch 1.7: error
ddp 1080ti pytorch 1.6: good
ddp 2080ti pytorch 1.7: good
ddp 2080ti pytorch 1.6: good
so far was not able reproduce with pytorch examples :( need to dig deep
		</comment>
		<comment id='7' author='ohmeow' date='2020-10-31T04:03:09Z'>
		I can confirm the same error using the latest Lightning and PyTorch using Tesla V100s. Does not happen on a single node with 2 GPUs, but once I go to multiple nodes the error happens.
		</comment>
		<comment id='8' author='ohmeow' date='2020-11-04T09:04:59Z'>
		same error with A100 gpus.
RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:784, invalid usage, NCCL version 2.7.8
		</comment>
		<comment id='9' author='ohmeow' date='2020-11-04T14:56:18Z'>
		Have the same issue with 2x2080ti on ubuntu 20.04 using pytorch 1.7 and cuda 11.
downgrading to pytorch 1.6 and cuda 10.2 fixes the issue
		</comment>
		<comment id='10' author='ohmeow' date='2020-11-09T05:48:12Z'>
		Could it be this fix in pytorch?
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/47257&gt;pytorch/pytorch#47257&lt;/denchmark-link&gt;

Exact same error (line number and stack messages).
		</comment>
		<comment id='11' author='ohmeow' date='2020-11-17T13:43:08Z'>
		pytorch closed their issue because this issue exists and you close this issue because their issue exists...
		</comment>
		<comment id='12' author='ohmeow' date='2020-11-18T18:16:06Z'>
		&lt;denchmark-link:https://github.com/julian3xl&gt;@julian3xl&lt;/denchmark-link&gt;
 are you referring to the one I posted? I was under the impression that the fix was merged into pytorch master.
I will check if it's fixed
		</comment>
		<comment id='13' author='ohmeow' date='2020-11-30T15:44:08Z'>
		Have the same issue with single node  2x rtx 3090 on ubuntu 18.04 using pytorch 1.7,   Driver Version: 455.45.01    CUDA Version: 11.1 , pytorch-lightning 1.0.8
		</comment>
		<comment id='14' author='ohmeow' date='2020-12-03T04:11:55Z'>
		I am also hitting this and I am not even using lightning. :-(
		</comment>
		<comment id='15' author='ohmeow' date='2020-12-03T10:03:22Z'>
		&lt;denchmark-link:https://github.com/min-xu-ai&gt;@min-xu-ai&lt;/denchmark-link&gt;
 And can you confirm that in pytorch 1.8 nightly it is fixed?
		</comment>
		<comment id='16' author='ohmeow' date='2020-12-05T01:43:44Z'>
		
@min-xu-ai And can you confirm that in pytorch 1.8 nightly it is fixed?

Great suggestion! I installed latest 1.8.0dev version and it still fails. But the error msg seems to be more helpful than before. &lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 Do you think the underlying error should have been fixed in 1.8.0dev?
&lt;denchmark-code&gt;&gt;       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/home/owen/e/py38_fs/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
E           fn(i, *args)
E         File "/home/owen/git/fairscale/tests/optim/test_oss.py", line 180, in run_test_add_param_group
E           dist_init(rank, world_size, tempfile_name)
E         File "/home/owen/git/fairscale/tests/optim/test_oss.py", line 33, in dist_init
E           dist.init_process_group(init_method=url, backend=backend, rank=rank, world_size=world_size)
E         File "/home/owen/e/py38_fs/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 486, in init_process_group
E           barrier()
E         File "/home/owen/e/py38_fs/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2197, in barrier
E           work = default_pg.barrier()
E       RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:882, invalid usage, NCCL version 2.7.8
E       ncclInvalidUsage: This usually reflects invalid usage of NCCL library (such as too many async ops, too many collectives at once, mixing streams in a group, etc).

../../e/py38_fs/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:166: ProcessRaisedException
&lt;/denchmark-code&gt;

My versions:
&lt;denchmark-code&gt;torch             1.8.0.dev20201204+cu110
torchtext         0.6.0
torchvision       0.9.0.dev20201204+cu110
&lt;/denchmark-code&gt;

		</comment>
		<comment id='17' author='ohmeow' date='2020-12-05T01:58:37Z'>
		Actually, I found out the reason. It seems that my unit test is trying to start a world_size=3 on 2 GPUs. The error msg is definitely hard to parse. It would be nice that dist.init_process_group just check the world_size.
FWIW, gloo backend works fine in this case.
		</comment>
		<comment id='18' author='ohmeow' date='2020-12-09T12:00:36Z'>
		For others who might run into this:
In previous PyTorch Lightning versions, the Trainer received an argument distributed_backend. You now need to rename it to accelerator.
		</comment>
		<comment id='19' author='ohmeow' date='2020-12-23T06:38:44Z'>
		I set this in my script:
CUDA_VISIBLE_DEVICES='0,1,2,3'
Also, I set num_gpus=3
I found that when num_gpus doesn't match the number of visible  GPUs, NCCL error occurs.
When I set num_gpus=4, or CUDA_VISIBLE_DEVICES='0,1,2', it works fine.
		</comment>
		<comment id='20' author='ohmeow' date='2020-12-23T07:15:36Z'>
		&lt;denchmark-link:https://github.com/17Skye17&gt;@17Skye17&lt;/denchmark-link&gt;
 did you mean the Trainer argument  instead of num_gpus? This doesn't reproduce for me using the bugreportmodel.
		</comment>
		<comment id='21' author='ohmeow' date='2020-12-23T07:54:15Z'>
		
@17Skye17 did you mean the Trainer argument gpus instead of num_gpus? This doesn't reproduce for me using the bugreportmodel.

hello, I didn't test the bugreportmodel, I tested torch.nn.Parallel.DistributedDataParallel(model, device_ids, output_device) under this env:
Pytorch1.7,  1080Ti,  Ubuntu16.04,  Cuda10.2.
		</comment>
	</comments>
</bug>