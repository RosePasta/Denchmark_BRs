<bug id='2484' author='wietsedv' open_date='2020-07-03T09:48:35Z' closed_time='2020-08-19T23:01:56Z'>
	<summary>Trainer.scale_batch_size requires model.batch_size instead of model.hparams.batch_size</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Trainer.scale_batch_size only works if a model has the batch_size property and does not work with model.hparams.batch_size even though all documentation points to the reverse.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

All of my hyperparameters are available as model.hparams like suggested in the documentation: (&lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/0.8.4/hyperparameters.html#lightningmodule-hyperparameters&gt;hyperparameters, option 3&lt;/denchmark-link&gt;
.
This means that my  is available as .
This should be fully compatible with the &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/0.8.4/training_tricks.html#auto-scaling-of-batch-size&gt;documented example code&lt;/denchmark-link&gt;
 of  since that code also uses  instead of .
However, when I put my model in Trainer.scale_batch_size, I get the following error:
&lt;denchmark-code&gt;pytorch_lightning.utilities.exceptions.MisconfigurationException: Field batch_size not found in `model.hparams`
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Example code&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;class LitModel(pl.LightningModule):
    def __init__(self, hparams):
        super().__init__()
        self.hparams = args

model = LitModel(args)
trainer = Trainer()
trainer.scale_batch_size(model)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Either  should work with  or the error message, linked documentation examples and docstrings should all change (i.e. &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_tricks.py#L139&gt;here&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_tricks.py#L228&gt;here&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_tricks.py#L233&gt;here&lt;/denchmark-link&gt;
).
(I would prefer the second option. I think that it should work with both model.batch_size and model.hparams.batch_size.)
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


pytorch-lightning         0.8.4

	</description>
	<comments>
		<comment id='1' author='wietsedv' date='2020-07-03T09:49:38Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='wietsedv' date='2020-07-03T10:53:40Z'>
		it seems like a nice first issue, &lt;denchmark-link:https://github.com/wietsedv&gt;@wietsedv&lt;/denchmark-link&gt;
 mind send a PR? 
		</comment>
		<comment id='3' author='wietsedv' date='2020-07-04T05:10:40Z'>
		Appears to be the same with the learning rate parameter.
		</comment>
		<comment id='4' author='wietsedv' date='2020-07-20T19:05:23Z'>
		A clean fix on the user side while waiting for the PR is to actually use self.hparams.batch_size and define self.batch_size as a property of your module:
@property
def batch_size(self):
    return self.hparams.batch_size

@batch_size.setter
def batch_size(self, batch_size):
    self.hparams.batch_size = batch_size
That way you keep your hyper parameters together in case you want to dump them somewhere without having to add specific code.
		</comment>
		<comment id='5' author='wietsedv' date='2020-07-24T15:29:35Z'>
		From &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1896&gt;#1896&lt;/denchmark-link&gt;
 it seems that the problem is rather on the docs side than 's implementation.
 is the correct location to look for the parameter, not .
My above fix is thus also obsolete.
		</comment>
		<comment id='6' author='wietsedv' date='2020-07-24T15:30:51Z'>
		
From #1896 it seems that the problem is rather on the docs side than scale_batch_size()'s implementation.
My above fix is thus also obsolete.

I just tried your fix and it seemed to work :)
		</comment>
		<comment id='7' author='wietsedv' date='2020-07-24T15:37:53Z'>
		Yes it does work, but from what they said in the PR I linked, hparams was just there as a temporary solution, and all hyper parameters are intended to be set as direct instance attributes in __init__.
My fix is obsolete regarding the intended usage of PL.
		</comment>
	</comments>
</bug>