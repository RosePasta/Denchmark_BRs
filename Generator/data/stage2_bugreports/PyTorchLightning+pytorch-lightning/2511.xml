<bug id='2511' author='desmondchchan' open_date='2020-07-05T09:47:45Z' closed_time='2020-07-10T01:18:11Z'>
	<summary>validation_step should be called when trainer.global_step % val_check_interval == 0</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Please take a look at the code sample.
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

import os

import torch
from torch.nn import functional as F
from torch.utils.data import DataLoader
from torchvision.datasets import MNIST
from torchvision import transforms
import pytorch_lightning as pl

class MNISTModel(pl.LightningModule):

    def __init__(self):
        super(MNISTModel, self).__init__()
        self.l1 = torch.nn.Linear(28 * 28, 10)

    def forward(self, x):
        return torch.relu(self.l1(x.view(x.size(0), -1)))

    def training_step(self, batch, batch_nb, using_native_amp=False):
        print("At training_step, global_step=", self.global_step)
        x, y = batch
        loss = F.cross_entropy(self(x), y)
        tensorboard_logs = {'train_loss': loss}
        return {'loss': loss, 'log': tensorboard_logs}

    def validation_step(self, batch, batch_nb):
        print("At validation_step, global_step=", self.global_step)
        x, y = batch
        loss = F.cross_entropy(self(x), y)
        tensorboard_logs = {'val_loss': loss}
        return {'loss': loss, 'log': tensorboard_logs}

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.02)

train_loader = DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)
validation_loader = DataLoader(MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor()), batch_size=32)

mnist_model = MNISTModel()
trainer = pl.Trainer(gpus=-1,
                     limit_val_batches=1,
                     val_check_interval=10,
                     progress_bar_refresh_rate=0
                     )
trainer.fit(mnist_model, train_loader, validation_loader)
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

I set the val_log_interval=10. validation_step is called two times at global_step=0 because of the sanity check and then called again at global_step=9, 19, 29... I think it makes more sense if it is called at global_step=10, 20, 30...
&lt;denchmark-code&gt;At validation_step, global_step= 0
At validation_step, global_step= 0
At training_step, global_step= 0
At training_step, global_step= 1
At training_step, global_step= 2
At training_step, global_step= 3
At training_step, global_step= 4
At training_step, global_step= 5
At training_step, global_step= 6
At training_step, global_step= 7
At training_step, global_step= 8
At training_step, global_step= 9
At validation_step, global_step= 9
At training_step, global_step= 10
At training_step, global_step= 11
At training_step, global_step= 12
At training_step, global_step= 13
At training_step, global_step= 14
At training_step, global_step= 15
At training_step, global_step= 16
At training_step, global_step= 17
At training_step, global_step= 18
At training_step, global_step= 19
At validation_step, global_step= 19
At training_step, global_step= 20
At training_step, global_step= 21
At training_step, global_step= 22
At training_step, global_step= 23
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:
- GPU:
- Tesla T4
- available:         True
- version:           10.2
Packages:
- numpy:             1.18.5
- pyTorch_debug:     False
- pyTorch_version:   1.6.0.dev20200622
- pytorch-lightning: 0.8.0
- tensorboard:       1.15.0
- tqdm:              4.46.1
System:
- OS:                Linux
- architecture:
- 64bit
-
- processor:         x86_64
- python:            3.7.7
- version:           #28~18.04.1-Ubuntu SMP Sat Jun 6 00:0

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='desmondchchan' date='2020-07-05T11:01:53Z'>
		global_step starts from 0, and since val_check is an interval, the current behavious looks fine to me.
		</comment>
		<comment id='2' author='desmondchchan' date='2020-07-05T21:49:02Z'>
		looks fine to me. val_log_interval is just how often logs are sent to the logger, but it has no influence on global step which is always incrementing in training step.
		</comment>
		<comment id='3' author='desmondchchan' date='2020-07-06T01:14:06Z'>
		Sorry I do not agree. The first validation step happens at 0, then 9, then 19. There are only 9 steps between 0 and 9 and 10 steps between 9 and 19.
Also, this val_check_interval is not consistent with row_log_interval. In the above case, if I set both of them to be 10, the logger will log training at step 0, 10, 20, 30,... and log validation at step 0 (two times because of sanity check), 9, 19, 29,... This creates inconsistency.
		</comment>
		<comment id='4' author='desmondchchan' date='2020-07-06T01:17:56Z'>
		Is it correct/consistent if you set num_sanity_val_steps=0?
		</comment>
		<comment id='5' author='desmondchchan' date='2020-07-06T02:46:23Z'>
		With num_sanity_val_steps=0, it still runs validation at 9, 19, 29,... as you mentioned earlier, validation runs every val_check_interval number of training steps.
		</comment>
		<comment id='6' author='desmondchchan' date='2020-07-07T19:52:32Z'>
		
Sorry I do not agree. The first validation step happens at 0, then 9, then 19. There are only 9 steps between 0 and 9 and 10 steps between 9 and 19.

0-9 -&gt; 10 steps
then it will run for 10-19 -&gt; 10 steps again
then it will run for 20-29 -&gt; 10 steps again
...
...
Looks fine.
		</comment>
		<comment id='7' author='desmondchchan' date='2020-07-08T01:22:32Z'>
		I understand that the current logic is validation happens every val_check_interval of training steps. I think it makes more sense if validation happens val_check_interval number of training steps after the last validation. If not, it will be inconsistent with row_log_interval. The training log and validation log is alway off by one step if I set val_check_interval and row_log_interval to be the same. So I think we could count the validation sanity check at step 0 to be a validation step so the validation happens at step 0, then 10, 20, 30. This provides users flexiblility (user can set num_sanity_val_steps=0 or 1) and aligns the training and validation logging.
		</comment>
	</comments>
</bug>