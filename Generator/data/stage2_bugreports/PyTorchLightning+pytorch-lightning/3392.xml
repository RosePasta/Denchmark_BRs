<bug id='3392' author='david-waterworth' open_date='2020-09-07T23:39:29Z' closed_time='2020-09-09T06:46:59Z'>
	<summary>mlflow training loss not reported until end of run</summary>
	<description>
I think I'm logging correctly, this is my training_step
&lt;denchmark-code&gt;    result = pl.TrainResult(loss)
    result.log('loss/train', loss)
    return result
&lt;/denchmark-code&gt;

and validation_step
&lt;denchmark-code&gt;    result = pl.EvalResult(loss)
    result.log('loss/validation', loss)
    return result
&lt;/denchmark-code&gt;

The validation loss is updated in mlflow each epoch, however the training loss isn't displayed until training has finished. Then it's available for every step. This may be a mlflow rather than pytorch-lighting issue - somewhere along the line it seems to be buffered?
&lt;denchmark-link:https://user-images.githubusercontent.com/5028974/92420471-d5b56c00-f1b6-11ea-9296-db075c3dcf87.png&gt;&lt;/denchmark-link&gt;

Versions:
pytorch-lightning==0.9.0
mlflow==1.11.0
Edit: logging TrainResult with on_epoch=True results in the metric appearing in mlflow during training, it's only the default train logging which gets delayed. i.e.
&lt;denchmark-code&gt;    result.log('accuracy/train', acc, on_epoch=True)
&lt;/denchmark-code&gt;

is fine
	</description>
	<comments>
		<comment id='1' author='david-waterworth' date='2020-09-08T08:57:41Z'>
		When using the minimal example provided in the linked issue, and using the default training logging shown above, I don't see the behaviour described.
I can sometimes see a discrepancy between the reported steps for each metrics, but I suspect this is to do with MLFlow and not the PL logger.
&lt;denchmark-link:https://user-images.githubusercontent.com/17157991/92454951-7120fe00-f204-11ea-99f3-7d2ac09b0f5e.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/david-waterworth&gt;@david-waterworth&lt;/denchmark-link&gt;
 could you elaborate on a few points?

When you set up the MLFLowLogger, if your tracking_uri over http: or using file:?
If http, is the tracking server remote?
How long does a model training run typically take?
does this behaviour consistently happen even when refreshing the MLFlow page?

		</comment>
		<comment id='2' author='david-waterworth' date='2020-09-08T10:31:11Z'>
		&lt;denchmark-link:https://github.com/patrickorlando&gt;@patrickorlando&lt;/denchmark-link&gt;

When you set up the MLFLowLogger, if your tracking_uri over http: or using file:?
I'm using file i.e. mlflow = loggers.MLFlowLogger("Transformer")
How long does a model training run typically take?
10-20 minutes
does this behavior consistently happen even when refreshing the MLFlow page?
Yes
		</comment>
		<comment id='3' author='david-waterworth' date='2020-09-08T11:52:02Z'>
		I've tried to reproduce this but cant seem to. I can confirm that the MLFlow logger is logging metrics at the end of each epoch and for me they show up in the MLFlow UI as I refresh the page.
Do you have a working code sample that can reproduce the issue?
		</comment>
		<comment id='4' author='david-waterworth' date='2020-09-08T22:58:39Z'>
		So I can actually see the behaviour you've described, but not when using the minimal example in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/3393&gt;#3393&lt;/denchmark-link&gt;
. I'll try to work out why.
		</comment>
		<comment id='5' author='david-waterworth' date='2020-09-09T00:33:14Z'>
		So I think this is because of the default behaviour of the TrainResult and the way row_log_interval works. And it only appears if the number of batches per epoch is less than row_log_interval
By default TrainResult logs on step and not on epoch.



pytorch-lightning/pytorch_lightning/core/step_result.py


        Lines 510 to 517
      in
      aaf26d7






 def log( 



 self, 



 name, 



 value, 



 prog_bar: bool = False, 



 logger: bool = True, 



 on_step: bool = True, 



 on_epoch: bool = False, 





When logging only per step, the logger connector only logs when the batch_idx is a multiple of row_log_interval. However if you don't have more than row_log_interval batches, the metrics are not logged.



pytorch-lightning/pytorch_lightning/trainer/logger_connector.py


        Lines 229 to 237
      in
      aaf26d7






 def save_train_loop_metrics_to_loggers(self, batch_idx, batch_output): 



 # when metrics should be logged 



 should_log_metrics = (batch_idx + 1) % self.trainer.row_log_interval == 0 or self.trainer.should_stop 



 if should_log_metrics or self.trainer.fast_dev_run: 



 # logs user requested information to logger 



 metrics = batch_output.batch_log_metrics 



 grad_norm_dic = batch_output.grad_norm_dic 



 if len(metrics) &gt; 0 or len(grad_norm_dic) &gt; 0: 



 self.log_metrics(metrics, grad_norm_dic) 





&lt;denchmark-link:https://github.com/david-waterworth&gt;@david-waterworth&lt;/denchmark-link&gt;
 Do you have less than 50 batches per epoch in your model? can you try setting  to be less than the number of train batches to confirm whether the issue is caused by this?
		</comment>
		<comment id='6' author='david-waterworth' date='2020-09-09T00:58:26Z'>
		&lt;denchmark-link:https://github.com/patrickorlando&gt;@patrickorlando&lt;/denchmark-link&gt;
 yes I have 38 batches per epoch. I set  and now the training step metrics are being displayed as they're generated.
		</comment>
		<comment id='7' author='david-waterworth' date='2020-09-09T06:44:45Z'>
		
yes I have 38 batches per epoch. I set row_log_interval=1 and now the training step metrics are being displayed as they're generated.

That makes sense now :)
&lt;denchmark-link:https://github.com/david-waterworth&gt;@david-waterworth&lt;/denchmark-link&gt;
 Should we close this? or is there something left unresolved?
		</comment>
		<comment id='8' author='david-waterworth' date='2020-09-09T06:46:59Z'>
		Thanks for the assistance, no nothing unresolved.
		</comment>
	</comments>
</bug>