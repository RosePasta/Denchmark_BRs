<bug id='1217' author='gunthergl' open_date='2020-03-23T14:30:26Z' closed_time='2020-06-08T12:42:24Z'>
	<summary>tensorboard hyperparameters don't update</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Given two sets of HPARAMS, h_1 and h_2 where h_1 is a strict subset of h_2.

If you run pytorch lightning with parameters h_1, then h_2, the additional parameters from h_2 are not shown in tensorboard
If you run pytorch lightning with parameters h_2, then h_1, the missing parameters from h_1 are shown empty in tensorboard

Case 2 is fine, Case 1 is not.
I already issued this at &lt;denchmark-link:https://github.com/tensorflow/tensorboard/issues/3371&gt;tensorboard &lt;/denchmark-link&gt;
 but was directed back here again.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;


Run code
start tensorboard --logdir=lightning_logs in same directory
Go to HPARAMS in website
See only layer_1_dim

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

import pytorch_lightning as pl
from argparse import ArgumentParser
import torch

class LitMNIST(pl.LightningModule):
    def __init__(self, hparams):
        super(LitMNIST, self).__init__()
        self.hparams = hparams
        self.layer_1 = torch.nn.Linear(28 * 28, self.hparams.layer_1_dim)

    def forward(self, *args, **kwargs):
        pass



if __name__ == '__main__':
    parser = ArgumentParser()
    parser.add_argument('--layer_1_dim', type=int, default=10)
    args = parser.parse_args()

    # print(args)
    ## &gt; Namespace(layer_1_dim=10)
    model = LitMNIST(hparams=args)
    trainer = pl.Trainer()
    try:
        trainer.fit(model)
    except:
        pass

    parser = ArgumentParser()
    parser.add_argument('--layer_1_dim', type=int, default=10)
    parser.add_argument('--another_hyperparameter', type=int, default=10)
    args = parser.parse_args()

    # print(args)
    ## &gt; Namespace(another_hyperparameter=10, layer_1_dim=10)
    model = LitMNIST(hparams=args)
    trainer = pl.Trainer()
    try:
        trainer.fit(model)
    except:
        pass
Change that "solves" the problem: First call net with both parameters
import pytorch_lightning as pl
from argparse import ArgumentParser
import torch

class LitMNIST(pl.LightningModule):
    def __init__(self, hparams):
        super(LitMNIST, self).__init__()
        self.hparams = hparams
        self.layer_1 = torch.nn.Linear(28 * 28, self.hparams.layer_1_dim)

    def forward(self, *args, **kwargs):
        pass



if __name__ == '__main__':
    parser = ArgumentParser()
    parser.add_argument('--layer_1_dim', type=int, default=10)
    parser.add_argument('--another_hyperparameter', type=int, default=10)
    args = parser.parse_args()

    # print(args)
    ## &gt; Namespace(another_hyperparameter=10, layer_1_dim=10)
    model = LitMNIST(hparams=args)
    trainer = pl.Trainer()
    try:
        trainer.fit(model)
    except:
        pass


    parser = ArgumentParser()
    parser.add_argument('--layer_1_dim', type=int, default=10)
    args = parser.parse_args()

    # print(args)
    ## &gt; Namespace(layer_1_dim=10)
    model = LitMNIST(hparams=args)
    trainer = pl.Trainer()
    try:
        trainer.fit(model)
    except:
        pass
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;


Run code
start tensorboard --logdir=lightning_logs in same directory
Go to HPARAMS  in website
See layer_1_dim and another_hyperparameter 

but another_hyperparameter empty in version0



Hackaround:

Run second code. The trick is to call the net with all hyperparameters first, then tensorboard gets  another_hyperparameter 

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version (e.g., 1.0): py3.7_cuda101_cudnn7_0
OS (e.g., Linux): Win10
How you installed PyTorch: conda 
Build command you used (if compiling from source): -
Python version: 3.7.0
CUDA/cuDNN version: 10.1
GPU models and configuration: GTX1650
Any other relevant information:

conda list for pytorch:  pytorch-lightning         0.7.1                    pypi_0    pypi



	</description>
	<comments>
		<comment id='1' author='gunthergl' date='2020-03-23T14:31:05Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='gunthergl' date='2020-03-30T15:38:18Z'>
		&lt;denchmark-link:https://github.com/gunthergl&gt;@gunthergl&lt;/denchmark-link&gt;
 may you consider drafting a PR with fix?
		</comment>
		<comment id='3' author='gunthergl' date='2020-03-30T15:51:11Z'>
		I sadly do not know where exactly this happens plus have very low free capacity atm.
After I have to get my method running first, it's not my top priority to have it fixed right now. As long as i keep track internally what I did it does not matter, it is more much more convenience than crucial right now.
		</comment>
		<comment id='4' author='gunthergl' date='2020-03-30T15:58:47Z'>
		&lt;denchmark-link:https://github.com/jeffling&gt;@jeffling&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/MattPainter01&gt;@MattPainter01&lt;/denchmark-link&gt;
 may you have a look?
		</comment>
		<comment id='5' author='gunthergl' date='2020-04-10T10:31:48Z'>
		I have also seen this issue and assumed like &lt;denchmark-link:https://github.com/gunthergl&gt;@gunthergl&lt;/denchmark-link&gt;
 that it was a tensorboard issue.
I'm working from home at the moment and it's difficult to do any Lightning work from here. &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;

I can look into this properly in a couple of weeks (when we can travel again) if it is not fixed.
		</comment>
		<comment id='6' author='gunthergl' date='2020-05-10T11:47:41Z'>
		Same issues here and I have to delete all the previous logs that have different parameters and restart tensorbord.
		</comment>
		<comment id='7' author='gunthergl' date='2020-05-29T15:31:32Z'>
		I ran into this issue as well, it is unlikely that this is a PyTorch lightning issue.
The following code replicates the problem using only SummaryWriter:
#!/usr/bin/env python3

from torch.utils.tensorboard import SummaryWriter

with SummaryWriter() as w:
    w.add_hparams({"key_A": 10}, {})
with SummaryWriter() as w:
    w.add_hparams({"key_B": 10}, {})
When viewing the Tensorboard summary writer output on http://localhost:6006/#hparams:



Trial_ID
key_A




May29_09-27-46_mbp13/1590766066.254924
10.000


May29_09-27-46_mbp13/1590766066.2567558




		</comment>
		<comment id='8' author='gunthergl' date='2020-06-08T12:42:48Z'>
		Closing this as this is an issue in tensorboard.
		</comment>
	</comments>
</bug>