<bug id='2331' author='soulhi-vz' open_date='2020-06-23T14:48:52Z' closed_time='2020-06-23T21:08:59Z'>
	<summary>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn</summary>
	<description>
Hi,
Got RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
Below details:
&lt;denchmark-code&gt;class Autoencoder(pl.LightningModule):
    
    def __init__(self, hparams: argparse.Namespace):
        super(Autoencoder,self).__init__() 
        self.hparams = hparams
           
        self.layer_e_1 = nn.Conv1d(hparams.in_channels, hparams.out_channels, hparams.kernel_size)
        self.layer_e_2 = nn.Conv1d(hparams.out_channels,hparams.in_channels,hparams.kernel_size)
        self.layer_d_1 = nn.ConvTranspose1d(hparams.in_channels,hparams.out_channels,hparams.kernel_size)
        self.layer_d_2 = nn.ConvTranspose1d(hparams.out_channels,hparams.in_channels,hparams.kernel_size)
        
        Train_x_t_e = CarrierDataset()
    
    def forward(self,x):
        x = self.layer_e_1(x)
        x = F.relu(x)
        x = self.layer_e_2(x)
        encoded = F.relu(x)
        x = self.layer_d_1(encoded)
        x = F.relu(x)
        decoded = self.layer_d_2(x)
        return decoded, encoded
    
    
    def training_step(self, train_batch, batch_idx):
        x, _ = train_batch
        decoded, encoded = self.forward(x)
        mse = nn.MSELoss()
        loss = mse(x, decoded)
        return {'loss': loss}
    
    def validation_step(self,val_batch, batch_idx):
        x, _ = val_batch
        decoded, encoded = self.forward(x)
        mse = nn.MSELoss()
        loss = mse(x, decoded)
        return {'val_loss': loss}
    
    def train_dataloader(self):
        loader = torch.utils.data.DataLoader(
            dataset=Train_x_t_e,
            batch_size=self.hparams.batch_size,
            shuffle=True,
            pin_memory=True)
        return loader

    def val_dataloader(self):
        return DataLoader(dataset=Train_x_t_e, batch_size=self.hparams.batch_size)

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)
        return optimizer

class CarrierDataset(data.Dataset):
    
    def __init__(self):
        Train_x_t_e1 = torch.load('Tensor_100K_1.pt')
        Train_x_t_e2 = torch.load('Tensor_100K_2.pt')
        ag_t = torch.cat((Train_x_t_e1,Train_x_t_e2))
        #df = read_csv(ds)
        self.len = ag_t.shape[0]
        self.ag_t = ag_t
        self.size = sys.getsizeof(ag_t)
        
    def __getitem__(self,index):
        return self.ag_t[index], self.ag_t[index]
    
    def __len__(self):
        return self.len
    
    def __size__(self):
        return self.size    
&lt;/denchmark-code&gt;

Got the following error:
&lt;denchmark-code&gt;RuntimeErrorTraceback (most recent call last)
&lt;ipython-input-277-b675dec29cfe&gt; in &lt;module&gt;
     16 print("Parameters:")
     17 print(args)
---&gt; 18 main(args)

&lt;ipython-input-276-23c135174ba6&gt; in main(hparams)
     19     )
     20 
---&gt; 21     trainer.fit(model)

/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders)
    885             self.optimizers, self.lr_schedulers, self.optimizer_frequencies = self.init_optimizers(model)
    886 
--&gt; 887             self.run_pretrain_routine(model)
    888 
    889         # return 1 when finished

/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_pretrain_routine(self, model)
   1013 
   1014         # CORE TRAINING LOOP
-&gt; 1015         self.train()
   1016 
   1017     def test(

/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py in train(self)
    345                 # RUN TNG EPOCH
    346                 # -----------------
--&gt; 347                 self.run_training_epoch()
    348 
    349                 # update LR schedulers

/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_epoch(self)
    417             # RUN TRAIN STEP
    418             # ---------------
--&gt; 419             _outputs = self.run_training_batch(batch, batch_idx)
    420             batch_result, grad_norm_dic, batch_step_metrics, batch_output = _outputs
    421 

/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_batch(self, batch, batch_idx)
    595 
    596                 # calculate loss
--&gt; 597                 loss, batch_output = optimizer_closure()
    598 
    599                 # check if loss or model weights are nan

/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py in optimizer_closure()
    573                     model_ref = self.get_model()
    574                     with self.profiler.profile('model_backward'):
--&gt; 575                         model_ref.backward(self, closure_loss, optimizer, opt_idx)
    576 
    577                     # track metrics for callbacks

/opt/conda/lib/python3.7/site-packages/pytorch_lightning/core/hooks.py in backward(self, trainer, loss, optimizer, optimizer_idx)
    153                     scaled_loss.backward()
    154         else:
--&gt; 155             loss.backward()

/opt/conda/lib/python3.7/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)
    196                 products. Defaults to ``False``.
    197         """
--&gt; 198         torch.autograd.backward(self, gradient, retain_graph, create_graph)
    199 
    200     def register_hook(self, hook):

/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)
     98     Variable._execution_engine.run_backward(
     99         tensors, grad_tensors, retain_graph, create_graph,
--&gt; 100         allow_unreachable=True)  # allow_unreachable flag
    101 
    102 

RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='soulhi-vz' date='2020-06-23T17:28:50Z'>
		Try: loss = mse(decoded, x).
		</comment>
		<comment id='2' author='soulhi-vz' date='2020-06-23T17:36:49Z'>
		with "loss = mse(decoded, x)"
same issue: RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
		</comment>
		<comment id='3' author='soulhi-vz' date='2020-06-23T17:47:51Z'>
		Can you print decoded.requires_grad after self.forward(x) in training_step? See whether it's True or False.
		</comment>
		<comment id='4' author='soulhi-vz' date='2020-06-23T17:51:56Z'>
		Hi Rohit,
Got:
decoded.requires_grad:
False
		</comment>
		<comment id='5' author='soulhi-vz' date='2020-06-23T18:01:04Z'>
		That means no problem in the loss.backward() as of now. Can you check the same after decoded = self.layer_d_2(x)? Or share a colab notebook?
		</comment>
		<comment id='6' author='soulhi-vz' date='2020-06-23T18:07:37Z'>
		decoded.requires_grad after layer_d_2:
False
decoded.requires_grad:
False
		</comment>
		<comment id='7' author='soulhi-vz' date='2020-06-23T19:38:16Z'>
		decoded.requires_grad should be True. Not sure why it's happening. Mind share a colab notebook??
		</comment>
		<comment id='8' author='soulhi-vz' date='2020-06-23T20:32:06Z'>
		Hi Rohit,
I am not using colab. Here the rest of the code:
def main(hparams) -&gt; None:
# initialize the DQNLightning
model = Autoencoder(hparams)
print("Model")
print(model)
print("hparam")
print(hparams)
#print("params list")
#for parameter in model.parameters():
#    print(parameter)
trainer = pl.Trainer(
fast_dev_run = True
#gpus=10,
#distributed_backend='dp',
#max_epochs=500,
#early_stop_callback=False,
#val_check_interval=100,
#show_progress_bar=False
)
&lt;denchmark-code&gt;trainer.fit(model)
&lt;/denchmark-code&gt;

torch.manual_seed(0)
np.random.seed(0)
&lt;denchmark-h:h1&gt;Pass hyper parameters to the model through the arg parser&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;Create parser&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;argeparse used to write freindly command-line interface&lt;/denchmark-h&gt;

parser = argparse.ArgumentParser()
&lt;denchmark-h:h1&gt;Add arguments&lt;/denchmark-h&gt;

parser.add_argument("--batch_size", type=int, default=1024, help="size of the batches")
parser.add_argument("--lr", type=float, default=1e-2, help="learning rate")
parser.add_argument("--in_channels", type=int, default=17, help="in channels")
parser.add_argument("--out_channels", type=int, default=100, help="out channels")
parser.add_argument("--kernel_size", type=int, default=3, help="kernel size")
&lt;denchmark-h:h1&gt;Parse arguments&lt;/denchmark-h&gt;

args, _ = parser.parse_known_args()
print("Parameters:")
print(args)
main(args)
		</comment>
		<comment id='9' author='soulhi-vz' date='2020-06-23T21:08:59Z'>
		Hi Rohit,
After restarting the kernel, the issue went away. Thanks for your help. Will close the issue now.
/Said
		</comment>
		<comment id='10' author='soulhi-vz' date='2020-11-13T05:40:36Z'>
		I have a similar problem connected to using pl.metrics.functional package for the 'dice_score'. Model was previously training fine with BCELoss, but when I switched to dice_score I get this same error.
Does this mean that PL version of the dice_score is not capable of back-prop and can only be used as a fitness measure and not a loss function?
		</comment>
	</comments>
</bug>