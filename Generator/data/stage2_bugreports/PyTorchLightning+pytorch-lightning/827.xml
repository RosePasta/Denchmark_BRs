<bug id='827' author='srush' open_date='2020-02-12T17:33:30Z' closed_time='2020-03-30T16:05:24Z'>
	<summary>Support/Features for step-based models</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug+Feature&lt;/denchmark-h&gt;

For models like transformer, we utilize step-based learning rates and evaluation.
It would be nice to have several features along this line.


Support for step-based schedulers. Right now we use cannot give the scheduler to lightning, because it calls scheduler.step(epochs=epoch) internally which resets the scheduler.


Fix docs for step based evaluation and checkpointing. It seems like it exists but it is hard to tell which is epochs / steps.


Add a max_steps option for stopping training.


Helper functions for converting between steps and epochs. For instance, from the number of epochs and parallelism, get the steps for configure_optimizers


	</description>
	<comments>
		<comment id='1' author='srush' date='2020-02-13T09:44:08Z'>
		Hi &lt;denchmark-link:https://github.com/srush&gt;@srush&lt;/denchmark-link&gt;

For 1. I have opened issue &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/806&gt;#806&lt;/denchmark-link&gt;
 to discuss about it and I will contribute this soon.
For 3. I added PR &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/728&gt;#728&lt;/denchmark-link&gt;
 that adds max/min steps for training such models.
		</comment>
		<comment id='2' author='srush' date='2020-02-20T13:11:15Z'>
		&lt;denchmark-link:https://github.com/srush&gt;@srush&lt;/denchmark-link&gt;
 good suggestions, would you mind to send a PR for 2 and 4?
		</comment>
		<comment id='3' author='srush' date='2020-02-20T13:14:24Z'>
		we just merged a pr &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/728&gt;#728&lt;/denchmark-link&gt;
 about this
		</comment>
		<comment id='4' author='srush' date='2020-03-30T16:05:24Z'>
		feel free to reopen if needed ü§ñ
		</comment>
	</comments>
</bug>