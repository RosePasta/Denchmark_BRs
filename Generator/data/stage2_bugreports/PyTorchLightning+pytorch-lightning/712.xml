<bug id='712' author='colehurwitz' open_date='2020-01-19T20:18:08Z' closed_time='2020-01-21T13:09:28Z'>
	<summary>Trainer is setting parameters with requires_grad=False to requires_grad=True (bug)</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When training a model that has some parameters where requires_grad=False,  the Trainer  is actually setting requires_grad=True for these parameters and changing them. The bug appears to originate in the TrainerTrainLoopMixin code.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-h:h4&gt;Steps to reproduce the behavior:&lt;/denchmark-h&gt;


Create a model with some parameters which have requires_grad=False
Fit the model using the Trainer
Check to see if the parameters which were set with `requires_grad=False' have changed.

&lt;denchmark-h:h4&gt;Code sample (to reproduce the bug)&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import torch
import numpy as np
import os
from torch.nn import functional as F
from torch.utils.data import DataLoader
import pytorch_lightning as pl

# Make toy dataset
features = torch.from_numpy(np.asarray([[0],[0],[0],[1],[1],[1]])).float()
targets = torch.from_numpy(np.asarray([0,0,0,1,1,1]))
train = torch.utils.data.TensorDataset(features, targets)
train_loader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)


#Define lightning model
class CoolSystem(pl.LightningModule):

    def __init__(self):
        super(CoolSystem, self).__init__()
        self.l1 = torch.nn.Linear(1, 10)
        self.l2 = torch.nn.Linear(10, 2)
        for param in self.l2.parameters():
            param.requires_grad = False
        self.loss_func = torch.nn.CrossEntropyLoss()
   
    def forward(self, x):
        return self.l2(torch.relu(self.l1(x)))

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self.forward(x)
        loss = self.loss_func(y_hat, y)
        tensorboard_logs = {'train_loss': loss}
        return {'loss': loss, 'log': tensorboard_logs}

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.02)

    @pl.data_loader
    def train_dataloader(self):
        return train_loader

# Run the lightning model (check parameter before and after training)

coolsystem = CoolSystem()
print(list(coolsystem.parameters())[3])
trainer = pl.Trainer(min_epochs=10, max_epochs=10, logger=False)    
trainer.fit(coolsystem)
list(coolsystem.parameters())[3]

&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

&lt;denchmark-h:h4&gt;Expected&lt;/denchmark-h&gt;

The parameters with requires_grad == False should not change during training.
&lt;denchmark-h:h4&gt;Actual&lt;/denchmark-h&gt;

The printed out parameter before training has requires_grad == False, but after training with the Trainer, the parameter now has requires_grad == True and has changed values.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version 1.3.1
Linux
PyTorch installed with pip
Python 3.7.1
pytorch-lightning 0.6.0

&lt;denchmark-h:h3&gt;Where I think the issue is!&lt;/denchmark-h&gt;

Here is the code snippet from training_loop.py that I think is causing the issue:
&lt;denchmark-code&gt;class TrainerTrainLoopMixin(ABC):
            .
            .
            .
    def run_training_batch(self, batch, batch_idx):
            .
            .
            .
            # call training_step once per optimizer
            for opt_idx, optimizer in enumerate(self.optimizers):
                # make sure only the gradients of the current optimizer's paramaters are calculated
                # in the training step to prevent dangling gradients in multiple-optimizer setup.
                for param in self.get_model().parameters():
                    param.requires_grad = False
                for group in optimizer.param_groups:
                    for param in group['params']:
                        param.requires_grad = True
&lt;/denchmark-code&gt;

As you can see, the params in the model are all set to  param.requires_grad = True during each training batch!
	</description>
	<comments>
		<comment id='1' author='colehurwitz' date='2020-01-21T03:57:13Z'>
		This behavior was introduced in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/603&gt;#603&lt;/denchmark-link&gt;
. Looks like we may want to revisit some of the changes that PR made. In the meantime, you can work around the problem by not passing parameters with  to your optimizer.
		</comment>
		<comment id='2' author='colehurwitz' date='2020-01-21T03:58:36Z'>
		@aybberk &lt;denchmark-link:https://github.com/jeffling&gt;@jeffling&lt;/denchmark-link&gt;
 ^
		</comment>
		<comment id='3' author='colehurwitz' date='2020-01-21T12:02:35Z'>
		I just disabled that loop for now since I am not working with GANs. thanks for responding and good luck fixing it!
		</comment>
		<comment id='4' author='colehurwitz' date='2020-01-21T12:16:59Z'>
		@aybberk &lt;denchmark-link:https://github.com/jeffling&gt;@jeffling&lt;/denchmark-link&gt;
 yeah. we need to revert this change or make a change to address
		</comment>
		<comment id='5' author='colehurwitz' date='2020-01-21T12:23:57Z'>
		I think &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/603#discussion_r358626166&gt;#603 (comment)&lt;/denchmark-link&gt;
 would also fix that when not working with GANs.
		</comment>
		<comment id='6' author='colehurwitz' date='2020-01-21T12:35:55Z'>
		Unless I am misunderstanding, the solution would be to only do this if there is more than one optimizer. However, normal networks can have more than one optimizer as well and not want this behavior right?
		</comment>
		<comment id='7' author='colehurwitz' date='2020-01-21T12:42:56Z'>
		You are probably correct, but I can't think of a such niche situation right now.
		</comment>
		<comment id='8' author='colehurwitz' date='2020-01-21T12:47:12Z'>
		If you want different learning rates for some parameters than you would use different optimizers, right? For example, if you are making a VAE with a deep encoder and a parametric model for the decoder than you may want different optimizers for the neural network and the learned decoder params. Maybe it is niche, but I have used it before.
		</comment>
		<comment id='9' author='colehurwitz' date='2020-01-21T12:48:21Z'>
		i suggest we do this only if 2 optimizes are present. PR? &lt;denchmark-link:https://github.com/colehurwitz&gt;@colehurwitz&lt;/denchmark-link&gt;
 or @aybberk
		</comment>
		<comment id='10' author='colehurwitz' date='2020-01-21T12:49:59Z'>
		
i suggest we do this only if 2 optimizes are present. PR? @colehurwitz or @aybberk

PR is ready with this suggestion.
		</comment>
		<comment id='11' author='colehurwitz' date='2020-01-21T12:53:54Z'>
		Ok, this fixes my immediate issue at least. Thanks!
		</comment>
		<comment id='12' author='colehurwitz' date='2020-01-21T12:54:02Z'>
		
If you want different learning rates for some parameters than you would use different optimizers, right? For example, if you are making a VAE with a deep encoder and a parametric model for the decoder than you may want different optimizers for the neural network and the learned decoder params. Maybe it is niche, but I have used it before.

You are right, but it would still be implemented in Lightning with iteration between optimizers and this change would not affect the gradients in that setup. Also, I need to mention I think it would not be the best way to implement the situation in Lightning since it automatically iterates between optimizers for same training loop.
edit: You mentioned you used that setup, did you do it in Lightning or something else?
		</comment>
		<comment id='13' author='colehurwitz' date='2020-01-21T13:02:23Z'>
		I did this in regular Pytorch for a recent paper. You bring up some good points though, I did not freeze any parameters so that example may not apply. I do think that if this is specifically a GAN issue though, maybe there is a GAN specific solution? Maybe not though.
Anyways, I appreciate the quick response and the fix should be appropriate for my current work!
		</comment>
		<comment id='14' author='colehurwitz' date='2020-01-21T13:04:17Z'>
		The most structured way to fix it would be, as &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
  mentioned, to save states before regular freeze/unfreeze operation and gradient calculation and load it after gradients are computed. I can work on it but I'm not really sure if it messes up with the distributed training settings since I do not know anything about ddp internals.
		</comment>
		<comment id='15' author='colehurwitz' date='2020-01-21T13:08:51Z'>
		Would it be possible to only freeze and unfreeze variables that have requires_grad=True or is that too expensive to search for every loop?
		</comment>
		<comment id='16' author='colehurwitz' date='2020-01-21T13:08:52Z'>
		it shouldn't mess up the internals @aybberk. and I would do it how &lt;denchmark-link:https://github.com/colehurwitz&gt;@colehurwitz&lt;/denchmark-link&gt;
 mentioned
		</comment>
	</comments>
</bug>