<bug id='2024' author='DKandrew' open_date='2020-05-31T05:21:31Z' closed_time='2020-06-26T13:41:35Z'>
	<summary>A warning that may comes from legacy code</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Receive this warning when running a simple Lightning module, is it related to the recent &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1896&gt;update&lt;/denchmark-link&gt;
? Maybe this warning is related to the past hyperparameters design?
&lt;denchmark-code&gt;[path to]/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: Did not find hyperparameters at model hparams. Saving checkpoint without hyperparameters.
  warnings.warn(*args, **kwargs)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Use the following code sample
&lt;denchmark-code&gt;import os
import torch
from torch.nn import functional as F
from torchvision.datasets import MNIST
from torch.utils.data import DataLoader
from torchvision import transforms
from pytorch_lightning import LightningModule
from pytorch_lightning import Trainer
from pytorch_lightning.loggers import TensorBoardLogger
from torch.utils.tensorboard import SummaryWriter

class MyNet(LightningModule):
    def __init__(self):
        super(MyNet, self).__init__()
        self.l1 = torch.nn.Linear(28 * 28, 10)

    def forward(self, x):
        return torch.relu(self.l1(x.view(x.size(0), -1)))

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.001)

    def train_dataloader(self):
        dataset = MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())
        loader = DataLoader(dataset, batch_size=32, num_workers=4, shuffle=True)
        return loader

    def test_dataloader(self):
        dataset = MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())
        loader = DataLoader(dataset, batch_size=32, num_workers=4, shuffle=False)
        return loader

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        tensorboard_logs = {'train_loss': loss}
        return {'loss': loss, 'log': tensorboard_logs}

    def test_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        tensorboard_logs = {'test_loss': loss}
        return {'loss': loss, 'log': tensorboard_logs}

    def test_epoch_end(self, output):
        with SummaryWriter(self.logger.log_dir) as w:
            for i in range(5):
                w.add_hparams({'lr': 0.1 * i, 'bsize': i}, {'hparam/accuracy': 10 * i, 'hparam/loss': 10 * i})
        return {}


dir_path = "."
tb_logger = TensorBoardLogger(dir_path, name='run2')
model = MyNet()
trainer = Trainer(gpus=1, max_epochs=1, logger=tb_logger)
trainer.fit(model)
trainer.test()

&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Environments&lt;/denchmark-h&gt;


CUDA:

GPU:

Fastest GeForce In The Moon


available:         True
version:           10.2


Packages:

numpy:             1.18.1
pyTorch_debug:     False
pyTorch_version:   1.5.0
pytorch-lightning: 0.7.6
tensorboard:       2.2.1
tqdm:              4.46.0


System:

OS:                Linux
architecture:

64bit
ELF


processor:         x86_64
python:            3.8.2
version:           #102-Ubuntu SMP Mon May 11 10:07:26 UTC 2020



	</description>
	<comments>
		<comment id='1' author='DKandrew' date='2020-05-31T16:11:34Z'>
		Hi there!
It seems to me that the problem you are facing is caused by the version of PyTorch Lightning that you're using. If I'm not mistaken, all the get-rid-of-hparams features were added after the 0.7.6 was released. &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning#bleeding-edge&gt;Try upgrading to the latest version&lt;/denchmark-link&gt;
 of the library, although I'm not completely sure that it's safe to use right now. After the upgrade, I get this mysterious warning first time I run a cell with the code you've wrote. I haven't figured out yet whether it's fine or not.
&lt;denchmark-link:https://user-images.githubusercontent.com/46068804/83356860-048b9d00-a393-11ea-89d2-f1a867382045.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='DKandrew' date='2020-05-31T19:20:27Z'>
		Oh I see. That is possible, so maybe it is because the new features have not integrated into the stable version yet.
		</comment>
		<comment id='3' author='DKandrew' date='2020-05-31T19:28:41Z'>
		it‚Äôs safe. The current change is that the user has to call self.auto_collect_arguments() to save all args to the checkpoint automatically.
we need to decide if we keep it that way or go back to auto-doing it.
&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='DKandrew' date='2020-06-02T08:13:48Z'>
		
If I'm not mistaken, all the get-rid-of-hparams features were added after the 0.7.6 was released.

the parsing init argument is still unreleased change, &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1896&gt;#1896&lt;/denchmark-link&gt;


After the upgrade, I get this mysterious warning first time I run a cell with the code you've wrote. I haven't figured out yet whether it's fine or not.

the same was reported here &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1976&gt;#1976&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>