<bug id='1182' author='ibeltagy' open_date='2020-03-18T15:43:29Z' closed_time='2020-06-06T20:04:41Z'>
	<summary>Restarts ignores `val_check_interval`</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

With val_check_interval != 1, checkpoints are saved in the middle of the epoch, but that location is not saved in the checkpoint, and after a restart, it always begins from the beginning of the last epoch.
To reproduce:

Run any training with val_check_interval=0.2
Kill training after, say, 40% of an epoch
Restart from the saved checkpoint
You will find that it is starting from 0% of that checkpoint

Solution:

Save and load batch_idx
Save and load the dataloader (or just the sampler). According to this https://discuss.pytorch.org/t/how-to-save-dataloader/62813/4, it is just torch.save(dataloader), but I haven't tried.

	</description>
	<comments>
		<comment id='1' author='ibeltagy' date='2020-03-18T21:28:18Z'>
		saving also  sounds reasonable for me...
any other suggestions &lt;denchmark-link:https://github.com/orgs/PyTorchLightning/teams/core-contributors&gt;@PyTorchLightning/core-contributors&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='2' author='ibeltagy' date='2020-03-29T17:59:54Z'>
		This has a potential issue - we won't be able to get the shuffled data in the same order on a second pass. So even if we save the batch index, we still won't be able to do a proper resume. I suppose the best we can do is raise a warning saying that the behaviour may not be repeatable and start from the begininning of the epoch (only way to guarantee that all data is seen at least a certain number of times)  - alternatively, can we guarantee that random dataloader state is restored properly by pickle?
		</comment>
		<comment id='3' author='ibeltagy' date='2020-05-28T19:37:54Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>