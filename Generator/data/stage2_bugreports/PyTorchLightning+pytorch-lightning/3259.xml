<bug id='3259' author='maxjeblick' open_date='2020-08-29T22:01:59Z' closed_time='2020-09-09T08:51:43Z'>
	<summary>Cap batch size by number of training samples when using auto_scale_batch_size</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

The batch size finder sets an unrealistically high batch size if all samples of the training dataset fit into one batch.
&lt;denchmark-code&gt;...
Batch size 8388608 succeeded, trying batch size 16777216
Batch size 16777216 succeeded, trying batch size 33554432
Batch size 33554432 succeeded, trying batch size 67108864
Finished batch size finder, will continue with full run using batch size 67108864
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Run Mnist Example with auto_scale_batch_size=True (one needs to remove hardcoded batch size and set self.batch_size).

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Batch size search space should not be larger than number of available training samples.
	</description>
	<comments>
		<comment id='1' author='maxjeblick' date='2020-08-29T22:02:38Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
	</comments>
</bug>