<bug id='4427' author='maxjeblick' open_date='2020-10-29T15:13:24Z' closed_time='2020-11-02T16:36:49Z'>
	<summary>Gradient accumulation fails with fp16 precision</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Setting accumulate_grad_batches &gt; 1 and precision = 16 causes the following error:
RuntimeError: unscale_() has already been called on this optimizer since the last update().
&lt;denchmark-h:h2&gt;Please reproduce using the BoringModel and post here&lt;/denchmark-h&gt;

&lt;denchmark-link:https://colab.research.google.com/drive/1_7pxqPlpc79k0VYlRdtRXE0JQbhSBWHy?usp=sharing&gt;https://colab.research.google.com/drive/1_7pxqPlpc79k0VYlRdtRXE0JQbhSBWHy?usp=sharing&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:

GPU:

Tesla T4


available:         True
version:           10.1


Packages:

numpy:             1.18.5
pyTorch_debug:     False
pyTorch_version:   1.6.0+cu101
pytorch-lightning: 1.0.4
tqdm:              4.41.1


System:

OS:                Linux
architecture:

64bit



processor:         x86_64
python:            3.6.9
version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020



	</description>
	<comments>
		<comment id='1' author='maxjeblick' date='2020-10-29T16:30:46Z'>
		Thanks for taking this &lt;denchmark-link:https://github.com/ydcjeff&gt;@ydcjeff&lt;/denchmark-link&gt;
!
		</comment>
		<comment id='2' author='maxjeblick' date='2020-10-29T19:54:36Z'>
		might also be fixed by &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4311&gt;#4311&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='maxjeblick' date='2020-10-30T00:14:27Z'>
		I'm only getting this error on 1.0.4 (after I upgraded a few hours ago). I downgraded to 1.0.3 and there is not there.
		</comment>
	</comments>
</bug>