<bug id='3451' author='matthaeusheer' open_date='2020-09-10T20:42:20Z' closed_time='2020-09-23T12:30:00Z'>
	<summary>Very slow training on SLURM cluster</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When switching from my local machine (old and slow 960m laptop GPU) to a SLURM cluster with Titan X GPU's, I see a significant drop in performance from 4.71it/s to 7s/it (!) - so basically training becomes unusably slow.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;


Take a model and submit it as a SLURM job. Happy to provide further details if requested, not sure what is relevant here.

&lt;denchmark-code&gt;2020-09-10 22:11:58,979 : lightning    : INFO       GPU available: True, used: True
2020-09-10 22:11:58,979 : lightning    : INFO       TPU available: False, using: 0 TPU cores
2020-09-10 22:11:58,979 : lightning    : INFO       CUDA_VISIBLE_DEVICES: [0,1]
2020-09-10 22:12:01,698 : lightning    : INFO       Set SLURM handle signals.
Epoch 1:  40%|‚ñà‚ñà‚ñà‚ñâ      | 150/377 [19:11&lt;29:02,  7.68s/it, loss=2.401, v_num=19]
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

The is setup as follows
logger = TensorBoardLogger(str(DATA_DIR_PATH / 'lightning_logs'), name=Path(__file__).stem)
trainer_kwargs = {'logger': logger,
                  'default_root_dir': str(DATA_DIR_PATH / 'lightning_logs'),
                  'val_check_interval': 0.2,  # check (1 / value) * times per train epoch
                  'gpus': 1,
                  # 'distributed_backend': 'ddp',  # checked with ddp enabled, 2 GPUs, multiple workers etc., same slow behaviour
                  'fast_dev_run': False}
trainer = pl.Trainer(**trainer_kwargs)
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


Packages:

numpy:                 1.18.1
pyTorch_debug:   False
pyTorch_version: 1.4.0
pytorch-lightning: 0.8.5
tensorboard:        1.15.0
tqdm:                   4.43.0




PyTorch Version (e.g., 1.0):
OS (e.g., Linux): Linux
How you installed PyTorch (conda, pip, source): conda
Python version: 3.7.9
CUDA/cuDNN version: cudatoolkit 10.0
GPU models and configuration: TitanX (on SLURM cluster)

	</description>
	<comments>
		<comment id='1' author='matthaeusheer' date='2020-09-17T11:17:15Z'>
		I have the same problem on a cluster with LSF. Basic profiling shows most time is spent in the forwards and backwards pass, as well as the optimisation step. Furthermore, checking GPU utilisation appears to be 0% even though pytorch lightning found the gpu device and said it was using it.
		</comment>
		<comment id='2' author='matthaeusheer' date='2020-09-17T12:46:52Z'>
		For me the problem turned out to the setting of the  'distributed_backend' option in the trainer. I had set this to 'None' for testing purposes through the command line. Removing this command line option or changing it back to 'ddp' solved the issue. GPU utilisation is now back up.
		</comment>
		<comment id='3' author='matthaeusheer' date='2020-09-22T18:51:31Z'>
		&lt;denchmark-link:https://github.com/matthaeusheer&gt;@matthaeusheer&lt;/denchmark-link&gt;
 were you able to solve the issue?
		</comment>
		<comment id='4' author='matthaeusheer' date='2020-09-23T12:30:00Z'>
		this looks like user error. glad you got it fixed!
		</comment>
	</comments>
</bug>