<bug id='4295' author='catalys1' open_date='2020-10-21T22:02:06Z' closed_time='2020-11-01T23:46:02Z'>
	<summary>Problems with automatic_optimization=False</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When automatic_optimization = False and terminate_on_nan = True, an exception is raised when checking for nan values. This is due to None being passed in as the value for loss to self.detect_nan_tensors. It looks like the code on master has already changed from what I'm seeing in 1.0.3, so I don't know if this has somehow been fixed or not. The problem seems to be that the AttributeDict returned from train_step has loss=None.
&lt;denchmark-h:h2&gt;Please reproduce using the BoringModel and post here&lt;/denchmark-h&gt;

&lt;denchmark-link:https://colab.research.google.com/drive/1qQmP6BwQk--rBXC7W45y0mn6QK39IPcc&gt;https://colab.research.google.com/drive/1qQmP6BwQk--rBXC7W45y0mn6QK39IPcc&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Don't crash when automatic_optimization = False and terminate_on_nan = True
	</description>
	<comments>
		<comment id='1' author='catalys1' date='2020-10-21T22:02:49Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='catalys1' date='2020-10-21T22:55:33Z'>
		I discovered this because the loss was showing up as nan in the progress bar, and I was trying to figure out why I was getting nan. I dug some more, and it looks like this is itself a bug. I've inspected the loss and network parameters over several steps, and there are no nans. So there seems to be a problem in the logging somewhere, that if you're using automatic_optimization=False you get nan being logged as the loss in the progress bar.
		</comment>
		<comment id='3' author='catalys1' date='2020-11-01T23:46:02Z'>
		Thanks &lt;denchmark-link:https://github.com/catalys1&gt;@catalys1&lt;/denchmark-link&gt;
 you are correct, however recent changes should have resolved this issue since the nan check only runs if using automatic optimization:
&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_loop.py#L779-L789&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_loop.py#L779-L789&lt;/denchmark-link&gt;

In &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/4204&gt;#4204&lt;/denchmark-link&gt;
 we'll make it clearer that you should report values within the training step via the docs :)
		</comment>
		<comment id='4' author='catalys1' date='2020-12-03T23:41:03Z'>
		Hi &lt;denchmark-link:https://github.com/catalys1&gt;@catalys1&lt;/denchmark-link&gt;
,
in def training_step you can maybe overpass the nan issue by updating the running_loss directly:
self.trainer.train_loop.running_loss.append(loss)
in my case, no more nan whenautomatic_optimization=False
		</comment>
	</comments>
</bug>