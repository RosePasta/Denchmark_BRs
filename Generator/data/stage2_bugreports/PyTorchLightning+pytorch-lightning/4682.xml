<bug id='4682' author='jeremyjordan' open_date='2020-11-15T02:36:22Z' closed_time='2020-11-16T16:20:36Z'>
	<summary>tbptt doesn't work with validation step enabled</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Truncated back-propagation through time doesn't work when you have a validation step enabled. The progress bar tries to get a Trainer attribute split_idx before it has been set, leading to the error:
&lt;denchmark-code&gt;AttributeError: 'Trainer' object has no attribute 'split_idx'
&lt;/denchmark-code&gt;

This occurs when we perform a validation sanity check, as &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/61394d543c259ab2db8f423807c02e6d292b27c5/pytorch_lightning/trainer/training_loop.py#L921&gt;the attribute is first set later during the training loop&lt;/denchmark-link&gt;
.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-link:https://colab.research.google.com/drive/1-_lpFVJEQufqMoi7uD4_PtrDpLsbysQy?usp=sharing&gt;https://colab.research.google.com/drive/1-_lpFVJEQufqMoi7uD4_PtrDpLsbysQy?usp=sharing&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;from pytorch_lightning import LightningModule, Trainer
import torch.nn as nn
import torch


class RandomDataset(torch.utils.data.Dataset):
    def __init__(self, n_examples=32, seq_len=16, vocab_size=8):
        self.data = torch.randint(low=0, high=vocab_size, size=(n_examples, seq_len))

    def __getitem__(self, idx):
        x = self.data[idx, :-1]
        y = self.data[idx, 1:]
        return x, y

    def __len__(self):
        return self.data.shape[0]


class RecurrentLangugageModel(LightningModule):
    def __init__(self, vocab_size=8, embedding_dim=32, hidden_dim=32):
        super().__init__()
        self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)
        self.recurrent = nn.LSTM(input_size=embedding_dim, batch_first=True, hidden_size=hidden_dim)
        self.linear = nn.Linear(in_features=hidden_dim, out_features=vocab_size)
        self.criterion = nn.CrossEntropyLoss()

    def forward(self, x):
        embedding = self.embed(x)
        output, _ = self.recurrent(embedding)
        output = self.linear(output)
        return output

    def training_step(self, batch, batch_idx, hiddens):
        x, y = batch
        output = self.forward(x)
        loss = self.criterion(output.permute(0, 2, 1), y)  # (batch, classes, time) x (batch, time)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        output = self.forward(x)
        _ = self.criterion(output.permute(0, 2, 1), y)  # (batch, classes, time) x (batch, time)

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(
            self.parameters(),
            lr=0.001)
        return optimizer


n_examples = 32
seq_len = 16
vocab_size = 8
truncated_bptt_steps = 2
train_dataset = RandomDataset(n_examples=n_examples, seq_len=seq_len, vocab_size=vocab_size)
val_dataset = RandomDataset(n_examples=n_examples, seq_len=seq_len, vocab_size=vocab_size)
train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8)
val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=8)
trainer = Trainer(max_epochs=5, truncated_bptt_steps=truncated_bptt_steps)
model = RecurrentLangugageModel(vocab_size=vocab_size, embedding_dim=32, hidden_dim=32)
trainer.fit(model, train_dataloader=train_dataloader, val_dataloaders=val_dataloader)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

The Trainer shouldn't fail when performing a validation sanity check.
cc &lt;denchmark-link:https://github.com/tullie&gt;@tullie&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 (who worked on this feature according to the git blame)
	</description>
	<comments>
		<comment id='1' author='jeremyjordan' date='2020-11-15T02:39:43Z'>
		There appear to be two easy ways to address this.

We can set the split_idx attribute to None when we initialize the trainer.
We can use getattr() (with a default value) when collecting the split_idx attribute for tqdm.

I tried to mirror the existing  tests &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/tests/models/test_cpu.py#L299&gt;here&lt;/denchmark-link&gt;
 but the model architecture is not flexible enough to support variable sequence lengths. I would argue that we should use a recurrent architecture for  tests.
		</comment>
	</comments>
</bug>