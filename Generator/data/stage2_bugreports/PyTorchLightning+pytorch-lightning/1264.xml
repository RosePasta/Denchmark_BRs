<bug id='1264' author='leemengtaiwan' open_date='2020-03-27T14:14:42Z' closed_time='2020-03-30T22:37:03Z'>
	<summary>Multiple undesired checkpoints created during single epoch</summary>
	<description>
&lt;denchmark-h:h2&gt;🐛 Bug&lt;/denchmark-h&gt;

Thanks for the great project! When I sent custom ModelCheckpoint to Trainer and hoping to get one checkpoint each epoch, the Trainer eventually produced a lot of versioned checkpoints within a single epoch, wasting lots of disk space and were causing confusion. An example is shown as below:
&lt;denchmark-code&gt;checkpoints/
└── gan
    ├── _ckpt_epoch_0.ckpt
    ├── _ckpt_epoch_0_v0.ckpt
    ├── _ckpt_epoch_0_v10.ckpt
    ├── _ckpt_epoch_0_v11.ckpt
    ├── _ckpt_epoch_0_v12.ckpt
    ├── _ckpt_epoch_0_v13.ckpt
    ├── _ckpt_epoch_0_v14.ckpt
    ├── _ckpt_epoch_0_v15.ckpt
    ├── _ckpt_epoch_0_v16.ckpt
    ├── _ckpt_epoch_0_v17.ckpt
    ├── _ckpt_epoch_0_v18.ckpt
    ├── _ckpt_epoch_0_v19.ckpt
    ├── _ckpt_epoch_0_v1.ckpt
    ├── _ckpt_epoch_0_v20.ckpt
    ├── _ckpt_epoch_0_v21.ckpt
&lt;/denchmark-code&gt;

The minimal code I used for training (below I have showed how to reproduce this exactly):
checkpoint_callback = ModelCheckpoint(
        filepath=os.path.join(
            "gan",
            experiment_name
        ),
        save_top_k=-1,
        period=10
    )
    
    trainer = Trainer(
        checkpoint_callback=checkpoint_callback,
		...
    )
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Clone the repo

git clone https://github.com/leemengtaiwan/learnable_ai.git
git checkout 66b4cc9db5d1b9f1e2cf6c190d1172cdf6adbe92

Run the script to train a GAN with MNIST

cd learnable_ai/
python applications/image_generation/train_gan.py\
    --dataset mnist\
    --latent_dim 128\
    --dim 32\
    --channels 1\
    --batch_size 256 \
    --max_epochs 3

See the generated check points under checkpoints:

tree checkpoints/

There should be multiple checkpoints within a single epoch

checkpoints/
└── gan
    ├── _ckpt_epoch_0.ckpt
    ├── _ckpt_epoch_0_v0.ckpt
    ├── _ckpt_epoch_0_v10.ckpt
    ├── _ckpt_epoch_0_v11.ckpt
    ├── _ckpt_epoch_0_v12.ckpt
    ├── _ckpt_epoch_0_v13.ckpt
    ├── _ckpt_epoch_0_v14.ckpt
    ├── _ckpt_epoch_0_v15.ckpt
    ├── _ckpt_epoch_0_v16.ckpt
    ├── _ckpt_epoch_0_v17.ckpt
    ├── _ckpt_epoch_0_v18.ckpt
    ├── _ckpt_epoch_0_v19.ckpt
    ├── _ckpt_epoch_0_v1.ckpt
    ├── _ckpt_epoch_0_v20.ckpt
    ├── _ckpt_epoch_0_v21.ckpt
...
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

One checkpoint for each epoch when using custom ModelCheckpoint for pl.LightningModule which don't simply use val_loss for evaulation metric (thus the default checkpoint functionality on Trainer will not work)
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;cuda:
        GPU:
                Tesla V100-SXM2-16GB
        available:           True
        version:             10.1
packages:
        numpy:               1.18.1
        pyTorch_debug:       False
        pyTorch_version:     1.4.0
        pytorch-lightning:   0.7.1
        tensorboard:         2.2.0
        tqdm:                4.43.0
system:
        OS:                  Linux
        architecture:
                64bit
                ELF
        processor:           x86_64
        python:              3.6.10
        version:             #1 SMP Tue Dec 24 03:25:32 UTC 2019
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;


Related Slack discussion

	</description>
	<comments>
		<comment id='1' author='leemengtaiwan' date='2020-03-27T14:26:25Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='leemengtaiwan' date='2020-03-28T00:08:49Z'>
		it seems that it triggered with step interval instead of epoch interval, continue debug
		</comment>
		<comment id='3' author='leemengtaiwan' date='2020-03-28T18:24:26Z'>
		the issue with checkpoint is that it just specifies the interval of being called assuming ha it is called just once per epoch... but in your case, you turned to fast run mode and the checkpoint is called every batch so the period is correct just it every 10 steps instead of every 10 epochs...
I am going to change the checkpointing to cunt epochs explicitly...
		</comment>
	</comments>
</bug>