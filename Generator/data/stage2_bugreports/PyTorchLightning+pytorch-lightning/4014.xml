<bug id='4014' author='awaelchli' open_date='2020-10-09T01:01:23Z' closed_time='2020-10-30T03:47:38Z'>
	<summary>resume_from_checkpoint loads duplicate ModelCheckpoint</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When reloading Trainer from a checkpoint, the model callback appears twice in the callbacks list.
This ONLY happens if one provides both a checkpoint_callback AND custom callbacks list.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Code below
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

import torch
from torch.utils.data import Dataset
from pytorch_lightning import Trainer, LightningModule, Callback
from pytorch_lightning.callbacks import ModelCheckpoint


class RandomDataset(Dataset):
    def __init__(self, size, length):
        self.len = length
        self.data = torch.randn(length, size)

    def __getitem__(self, index):
        return self.data[index]

    def __len__(self):
        return self.len


class BoringModel(LightningModule):

    def __init__(self):
        super().__init__()
        self.layer = torch.nn.Linear(32, 2)

    def forward(self, x):
        return self.layer(x)

    def loss(self, batch, prediction):
        # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls
        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))

    def training_step(self, batch, batch_idx):
        output = self.layer(batch)
        loss = self.loss(batch, output)
        return {"loss": loss}

    def training_step_end(self, training_step_outputs):
        return training_step_outputs

    def training_epoch_end(self, outputs) -&gt; None:
        torch.stack([x["loss"] for x in outputs]).mean()

    def configure_optimizers(self):
        optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)
        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)
        return [optimizer], [lr_scheduler]


def run_test():
    # fake data
    train_data = torch.utils.data.DataLoader(RandomDataset(32, 64))

    class RandomCallback(Callback):
        pass

    model = BoringModel()
    checkpoint_callback = ModelCheckpoint()
    trainer_args = dict(
        max_epochs=2,
        logger=False,
        progress_bar_refresh_rate=0,
        checkpoint_callback=checkpoint_callback,
        weights_summary=None,
        callbacks=[RandomCallback()]  # try to remove it and see what happens :)
    )
    trainer = Trainer(**trainer_args)
    trainer.fit(model, train_dataloader=train_data)
    print(trainer.callbacks)
    trainer = Trainer(**trainer_args, resume_from_checkpoint=trainer.checkpoint_callback.best_model_path)
    trainer.fit(model, train_dataloader=train_data)
    print(trainer.callbacks)


if __name__ == '__main__':
    run_test()
Produces:
callbacks = [ModelCheckpoint]   # first training
callbacks = [ModelCheckpoint, ModelCheckpoint]  # after resuming
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

callbacks = [ModelCheckpoint]   # first training
callbacks = [ModelCheckpoint]  # after resuming
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

master branch 0.10.0
python 3.7
torch 1.5.1
	</description>
	<comments>
		<comment id='1' author='awaelchli' date='2020-10-29T01:46:24Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 would any callback that's present in both the checkpoint and in the trainer's callbacks list would run into this issue? is this restricted to the checkpoint callback only?
		</comment>
		<comment id='2' author='awaelchli' date='2020-10-29T01:56:59Z'>
		Yes, restricted to this checkpoint only. &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4027&gt;#4027&lt;/denchmark-link&gt;
 is ok but we found that &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4336&gt;#4336&lt;/denchmark-link&gt;
 is a better way to go. We should consistently pass all callbacks to the list, this solves the problem of duplication and complexity/ambiguity when resuming training.
		</comment>
		<comment id='3' author='awaelchli' date='2020-10-29T01:58:43Z'>
		in what instance would we want to load the callback state from a checkpoint and use it? shouldn't we always defer to what's passed to the trainer?
		</comment>
		<comment id='4' author='awaelchli' date='2020-10-29T02:06:13Z'>
		A callback can implement on_load_checkpoint and on_save_checkpoint callback methods. When we resume training, we typically want to resume  using the state we had before, e.g., we want to continue tracking which was the best model, what the early stopping history is, etc. This is a valuable feature imo.
If this is not desired, we have to follow up on this. My current goal is to make the modelcheckpoint be treated like the others, there is no reason to treat it differently.
		</comment>
		<comment id='5' author='awaelchli' date='2020-10-29T02:10:46Z'>
		
My current goal is to make the modelcheckpoint be treated like the others, there is no reason to treat it differently.

üíØ
		</comment>
	</comments>
</bug>