<bug id='4496' author='MohammedAljahdali' open_date='2020-11-03T13:43:15Z' closed_time='2020-11-14T14:32:55Z'>
	<summary>Trainer.test() fail when trys to log_hyperparameter</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

This the error code
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "test.py", line 31, in &lt;module&gt;
    cli_main()
  File "test.py", line 28, in cli_main
    trainer.test(model, test_dataloaders=dm.test_dataloader())
  File "C:\Users\Mohammed\.conda\envs\dl_env\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 710, in test
    results = self.__test_given_model(model, test_dataloaders)
  File "C:\Users\Mohammed\.conda\envs\dl_env\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 775, in __test_given_model
    results = self.fit(model)
  File "C:\Users\Mohammed\.conda\envs\dl_env\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 440, in fit
    results = self.accelerator_backend.train()
  File "C:\Users\Mohammed\.conda\envs\dl_env\lib\site-packages\pytorch_lightning\accelerators\gpu_accelerator.py", line 51, in train
    self.trainer.train_loop.setup_training(model)
  File "C:\Users\Mohammed\.conda\envs\dl_env\lib\site-packages\pytorch_lightning\trainer\training_loop.py", line 135, in setup_training
    self.trainer.logger.log_hyperparams(ref_model.hparams_initial)
  File "C:\Users\Mohammed\.conda\envs\dl_env\lib\site-packages\pytorch_lightning\utilities\distributed.py", line 35, in wrapped_fn
    return fn(*args, **kwargs)
  File "C:\Users\Mohammed\.conda\envs\dl_env\lib\site-packages\pytorch_lightning\loggers\tensorboard.py", line 159, in log_hyperparams
    params = self._flatten_dict(params)
  File "C:\Users\Mohammed\.conda\envs\dl_env\lib\site-packages\pytorch_lightning\loggers\base.py", line 228, in _flatten_dict
    return {delimiter.join(keys): val for *keys, val in _dict_generator(params)}
  File "C:\Users\Mohammed\.conda\envs\dl_env\lib\site-packages\pytorch_lightning\loggers\base.py", line 228, in &lt;dictcomp&gt;
    return {delimiter.join(keys): val for *keys, val in _dict_generator(params)}
TypeError: sequence item 1: expected str instance, numpy.int32 found
&lt;/denchmark-code&gt;

I think this error is caused by something related to the saved hparams
so I tried to print the hparams of my trained model, this is the result of printing my model.hparams:
&lt;denchmark-code&gt;"batch_size":    16
"callback":      False
"ckpt_path":     checkpoints\sgd_cnn_rnn_5\last.ckpt
"decoder":       {1: 'aa', 2: 'la', 3: 'sh', 4: 'ra', 5: 'ya', 6: 'ay', 7: 'da', 8: 'wa', 9: 'sp', 10: 'ta', 11: 'te', 12: 'ka', 13: 'sa', 14: 'gh', 15: 'ee', 16: 'kh', 17: 'na', 18: 'de', 19: 'fa', 20: 'ha', 21: 'ba', 22: 'he', 23: 'hamala', 24: 'to', 25: 'ma', 26: 'th', 27: 'ke', 28: 'se', 29: 'ze', 30: 'aala', 31: 'dh', 32: 'ae', 33: 'za', 34: 'al', 35: 'aela', 36: 'ja', 37: 'hh', 38: 'mala', 39: 'ah', 40: '7', 41: '0', 42: '2', 43: 'jala', 44: 'hala', 45: 'hana', 46: '8', 47: '1', 48: 'khla', 49: '9', 50: '6', 51: 'am', 52: 'ahla'}
"epochs":        400
"factor":        0.5
"height":        32
"hidden_dims":   64
"learning_rate": 0.003
"model_type":    cnn_rnn
"momentum":      0.9
"n_classes":     53
"name":          sgd_cnn_rnn_5_continue
"notes":         None
"optimizer":     sgd
"patience":      5
"rnn_dims":      256
"test_paths":    None
"train_paths":   None
"val_paths":     None
"weight_decay":  0.0001
&lt;/denchmark-code&gt;

This is my testing code
&lt;denchmark-code&gt;parser = ArgumentParser()
parser.add_argument('--test_paths', type=str, default=None, help="comma separate different dirs")
parser.add_argument('--ckpt_path', type=str, required=True)
parser.add_argument('--height', type=int, default=32)
parser.add_argument('--batch_size', type=int, default=16)
args = parser.parse_args()
if args.test_paths:
    test_dirs = args.test_paths.split(',')
    dm = IFN_ENITDataModule(height=args.height, batch_size=args.batch_size, test_dirs=test_dirs)
else:
    dm = IFN_ENITDataModule(height=args.height, batch_size=args.batch_size)
dm.setup("test")
model = ResNetwork.load_from_checkpoint(checkpoint_path=args.ckpt_path, n_classes=dm.num_classes+1, decoder=dm.decoder)
print(model.hparams_initial)
wandb = WandbLogger(project='Word Recognition', name=model.hparams.name, version=model.hparams.name)
trainer = pl.Trainer(gpus=1, precision=16)
trainer.test(model, test_dataloaders=dm.test_dataloader())
&lt;/denchmark-code&gt;

I have pytorch_lightning version 1.0.4
	</description>
	<comments>
		<comment id='1' author='MohammedAljahdali' date='2020-11-14T14:32:55Z'>
		I think storing a dict in hparam causes this issue, however it can be solved by setting logger=False in the trainer.
		</comment>
		<comment id='2' author='MohammedAljahdali' date='2020-12-13T01:05:15Z'>
		It would be nice if by default the logger can store as much as it can, instead of having to disable it when a non-{int, float, long} type is in the hparams.
I think this issue should still remain open.
Here is my particular error with using tensorboard as the logger:
&lt;denchmark-code&gt;  File "examples/gp_regression_example.py", line 398, in &lt;module&gt;
    main()
  File "examples/gp_regression_example.py", line 181, in main
    trainer.fit(model, datamodule=data_module)
  File "/home/acxz/venvs/test-venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 470, in fit
    results = self.accelerator_backend.train()
  File "/home/acxz/venvs/test-venv/lib/python3.8/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py", line 63, in train
    self.trainer.train_loop.setup_training(model)
  File "/home/acxz/venvs/test-venv/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 145, in setup_training
    self.trainer.logger.log_hyperparams(ref_model.hparams_initial)
  File "/home/acxz/venvs/test-venv/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py", line 39, in wrapped_fn
    return fn(*args, **kwargs)
  File "/home/acxz/venvs/test-venv/lib/python3.8/site-packages/pytorch_lightning/loggers/tensorboard.py", line 169, in log_hyperparams
    exp, ssi, sei = hparams(params, metrics)
  File "/home/acxz/venvs/test-venv/lib/python3.8/site-packages/torch/utils/tensorboard/summary.py", line 192, in hparams
    ssi.hparams[k].number_value = v
TypeError: array([ 0.08109958, -1.001376  ], dtype=float32) has type numpy.ndarray, but expected one of: int, long, float
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>