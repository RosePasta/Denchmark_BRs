<bug id='4493' author='xiadingZ' open_date='2020-11-03T07:48:51Z' closed_time='2020-11-04T00:49:57Z'>
	<summary>val and test scores mismatch</summary>
	<description>
In my dataset, file list is the same for val and test, and returned frames are uniformly sampled. In val mode, saved model reports acc as epoch=38-val_loss=0.815-val_acc=0.82275.ckpt. But when I use this model to test, it only has acc 78.99.
I save each videos' test scores in files, and use scores to calculate acc, it's 0.82164.  It's very strange that val/test/manually accs are different. As 0.82275,  0.7899, 0.82164
This is my code:
&lt;denchmark-code&gt;        checkpoint = osp.join(hparams.checkpoint_dir, 'best_model.ckpt')
        hparams_file = osp.join(hparams.checkpoint_dir, 'hparams.yaml')
        model = model.load_from_checkpoint(
            checkpoint_path=checkpoint,
            hparams_file=hparams_file,
            checkpoint_dir = hparams.checkpoint_dir,
            hparam_overrides = {
                'test_flist': hparams.test_flist,
                'pvm_img_backbone': hparams.pvm_img_backbone
            }
        )
        model.eval()
        if 'video' in hparams.model:
            video_datamodule = VideoDataModule(hparams)
            trainer.test(model, datamodule=video_datamodule)
&lt;/denchmark-code&gt;

DataModule:
&lt;denchmark-code&gt;    def val_dataloader(self):
        transform = T.Compose([
            T.GroupResize(self.img_size),
            T.GroupToPILImage(),
            T.GroupToTensor(),
        ])
        dataset = VideoDataset(self.hparams, mode='val', transform=transform)
        return DataLoader(dataset, batch_size=self.batch_size,
                          num_workers=self.num_workers, pin_memory=True)

    def test_dataloader(self):
        transform = T.Compose([
            T.GroupResize(self.img_size),
            T.GroupToPILImage(),
            T.GroupToTensor(),
        ])
        dataset = VideoDataset(self.hparams, mode='val', transform=transform)
        return DataLoader(dataset, batch_size=self.batch_size,
                          num_workers=self.num_workers, pin_memory=True)
&lt;/denchmark-code&gt;

Model:
&lt;denchmark-code&gt;    def validation_step(self, batch, batch_idx):
        ...

        loss_vid = self.loss_fn_video(vidout, video_labels)
        video_labels_hat = torch.argmax(vidout, dim=1).type_as(video_labels)
        # self.val_vid_acc = Accuracy()
        vid_acc = self.val_vid_acc(video_labels_hat, video_labels)

        loss = loss_vid

        self.log('val_loss', loss, on_step=False, on_epoch=True, sync_dist=True)

        self.log('val_acc', vid_acc, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)


    def test_step(self, batch, batch_idx):
        ...
        loss_vid = self.loss_fn_video(vidout, video_labels)
        video_labels_hat = torch.argmax(vidout, dim=1).type_as(video_labels)
        # self.test_vid_acc = Accuracy()
        vid_acc = self.test_vid_acc(video_labels_hat, video_labels)

        return {"test_vacc": vid_acc,  'scores': vidout, 'ids': ids}


    def test_epoch_end(self, outputs):
        avg_vid_acc = self.test_vid_acc.compute()
        ids = torch.cat([x['ids'] for x in outputs], dim=0)
        scores = torch.cat([x['scores'] for x in outputs], dim=0)

        ids = ids.reshape(-1,1).type_as(scores)
        scores = scores.reshape(ids.shape[0], -1)
        scores = torch.cat([ids, scores], dim=-1)

        print("avg vid acc: ", avg_vid_acc)

        path_scores = osp.join(self.hparams.checkpoint_dir, f'img_scores_rank_{self.trainer.global_rank}.pt')
        torch.save(scores.detach().cpu(), path_scores)
&lt;/denchmark-code&gt;

====update
if I  add self.log in test_step, result are still strange
&lt;denchmark-code&gt;    def validation_step(self, batch, batch_idx):
        ...

        loss_vid = self.loss_fn_video(vidout, video_labels)
        video_labels_hat = torch.argmax(vidout, dim=1).type_as(video_labels)
        # self.val_vid_acc = Accuracy()
        vid_acc = self.val_vid_acc(video_labels_hat, video_labels)

        loss = loss_vid

        self.log('val_loss', loss, on_step=False, on_epoch=True, sync_dist=True)

        self.log('val_acc', vid_acc, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)


    def test_step(self, batch, batch_idx):
        ...
        video_labels_hat = torch.argmax(vidout, dim=1).type_as(video_labels)

        vid_acc = self.test_vid_acc(video_labels_hat, video_labels)
        cm =self.test_vid_cm(video_labels_hat, video_labels)


        self.log('test_acc', vid_acc, on_epoch=True, sync_dist=True)
        self.log('cm', cm, on_epoch=True, sync_dist=True)

        return {"test_vacc": vid_acc,  'scores': vidout, 'ids': ids}


    def test_epoch_end(self, outputs):
        avg_vid_acc = self.test_vid_acc.compute()

        cm = self.test_vid_cm.compute()

        ids = torch.cat([x['ids'] for x in outputs], dim=0)

        scores = torch.cat([x['scores'] for x in outputs], dim=0)
&lt;/denchmark-code&gt;

the printed log is
&lt;denchmark-code&gt;avg vid acc:  tensor(0.8244, device='cuda:0')
--------------------------------------------------------------------------------
DATALOADER:0 TEST RESULTS
{'cm': tensor(0.0111, device='cuda:0'),
 'test_acc': tensor(0.8218, device='cuda:0')}
--------------------------------------------------------------------------------
&lt;/denchmark-code&gt;

why the avg_vid_acc= self.test_vid_acc.compute() is different from self.log('test_acc', vid_acc, on_epoch=True, sync_dist=True)? If using self.log will change self.test_vid_acc's internal states, now how can I save ConfusionMetric's result.
As shown in first code, If I don''t use self.log in test_epoch, instead only use compute() in test_epoch_end, TensorMetric's result will be wrong. But if use self.log in test_epoch, it seems that it will change TensorMetirc's stated, so that I can't use compute in test_epoch_end
	</description>
	<comments>
		<comment id='1' author='xiadingZ' date='2020-11-03T09:45:07Z'>
		Dear &lt;denchmark-link:https://github.com/xiadingZ&gt;@xiadingZ&lt;/denchmark-link&gt;
 ,
Thanks for reporting this bug. It seems to be pretty important.
Would you mind to reproducible this bug with the Boring Model ?
Here is the code: &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py&lt;/denchmark-link&gt;

When done, we will take this as high priority.
Best regards,
Thomas Chaton.
		</comment>
		<comment id='2' author='xiadingZ' date='2020-11-03T11:51:46Z'>
		&lt;denchmark-code&gt;import os
import torch
import torch.nn.functional as F
from torchvision.datasets import MNIST
from torchvision import transforms
from torch.utils.data import DataLoader
import pytorch_lightning as pl
from pytorch_lightning.metrics import Accuracy
from torch.utils.data import random_split

import argparse
from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint

class LitModel(pl.LightningModule):

    def __init__(self, hparams, *args, **kwargs):
        super().__init__()
        self.hparams = hparams
        self.l1 = torch.nn.Linear(28 * 28, hparams.c)
        self.test_acc = Accuracy()
        self.val_acc = Accuracy()
        
    def forward(self, x):
        return torch.relu(self.l1(x.view(x.size(0), -1)))

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        return {'loss':loss}
    
    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        label_hat = torch.argmax(y_hat, dim=1).type_as(y)
        
        acc = self.val_acc(label_hat, y)
        self.log('val_acc', acc, on_epoch=True, sync_dist=True)

    def test_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        label_hat = torch.argmax(y_hat, dim=1).type_as(y)
        
        acc = self.test_acc(label_hat, y)
        self.log('test_acc', acc, on_epoch=True, sync_dist=True)

    def test_epoch_end(self, outputs) -&gt; None:
        avg_acc = self.test_acc.compute()
        print('avg_acc: ', avg_acc)
        
    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.0005)
    
    def train_dataloader(self):
        dataset = MNIST('./', download=False, transform=transforms.ToTensor())
        train_loader = DataLoader(dataset, batch_size=128)
        return train_loader

    def val_dataloader(self):
        dataset = MNIST('./', download=False, transform=transforms.ToTensor())
        train_loader = DataLoader(dataset, batch_size=128)
        return train_loader
    
    def test_dataloader(self):
        dataset = MNIST('./', download=False, transform=transforms.ToTensor())
        train_loader = DataLoader(dataset, batch_size=128)
        return train_loader
    
    
def main(hparams):
    MNIST('./', download=True)
    model = LitModel(hparams)
    checkpoint_callback = ModelCheckpoint(
        filename='{epoch}-{val_loss:.3f}-{val_acc:.5f}',
    monitor='val_acc',
    mode='max',
    verbose=True,
    save_last=True
    )
    trainer = pl.Trainer(
        gpus=hparams.gpus,
        num_nodes=hparams.num_nodes,
        distributed_backend='ddp',
        checkpoint_callback=checkpoint_callback,
        max_epochs = hparams.max_epochs
    )
    if hparams.test == 1:
        model = model.load_from_checkpoint(
            checkpoint_path='test_model.ckpt',
            hparams_file='hparams.yaml',
        )
        model.eval()
    else:
        trainer.fit(model)
    trainer.test(model)


if __name__ == '__main__':
    # TRAIN
    parser = argparse.ArgumentParser(conflict_handler="resolve")
    parser = pl.Trainer.add_argparse_args(parser)
    parser.add_argument('--c', type=int, default=10)
    parser.add_argument('--test', type=int, default=0)
    hparams = parser.parse_args()
    hparams.gpus=2
    hparams.num_nodes=1
    hparams.max_epochs=2
    main(hparams)
&lt;/denchmark-code&gt;

I have reprodeced, result is as blow.
&lt;denchmark-code&gt;avg_acc:  tensor(0.6047, device='cuda:0')
avg_acc:  tensor(0.6047, device='cuda:1')
--------------------------------------------------------------------------------
DATALOADER:0 TEST RESULTS
{'test_acc': tensor(0.6188, device='cuda:0'),
 'val_acc': tensor(0.6188, device='cuda:0')}
&lt;/denchmark-code&gt;

pytorch lightning is build from master. this example only show mismatch of self.log and compute. And If I don't use self.log, compute's result is still 0.6047.  It's still an important bug. I need to get the metrics to do my operation in test_epoch_end
I will work on reproducing other bugs in my post.
		</comment>
		<comment id='3' author='xiadingZ' date='2020-11-03T21:32:55Z'>
		ok, I can confirm it
&lt;denchmark-code&gt;avg_acc:  tensor(0.8570, device='cuda:1')
avg_acc:  tensor(0.8570, device='cuda:0')
--------------------------------------------------------------------------------
DATALOADER:0 TEST RESULTS
{'test_acc': tensor(0.8836, device='cuda:0'),
 'val_acc': tensor(0.8836, device='cuda:0')}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='xiadingZ' date='2020-11-03T22:58:28Z'>
		I have not carefully studied all details here, but what sticks out to me is that there are two ways you compute the accuracy, and they can't both be the same in general.
sync_dist=True, if you look at the docs, will average across the gpus. The docs say that sync_dist_op = mean.
Computing the mean per step and then aggregating is not the same as aggregating all stats from all gpus (e.g. in case of accuracy the num_correct and total) and computing the final metric at the end.
To me, this difference you observe there makes total sense. I am fairly confident with my answer here (95%).
ask one of the metrics guys and I am sure they can confirm my statements :)
		</comment>
		<comment id='5' author='xiadingZ' date='2020-11-03T23:02:51Z'>
		... and the correct result in this case is the one computed by the metric class.
		</comment>
		<comment id='6' author='xiadingZ' date='2020-11-03T23:05:08Z'>
		The metrics docs here &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/metrics.html&gt;https://pytorch-lightning.readthedocs.io/en/latest/metrics.html&lt;/denchmark-link&gt;

show the example how log it correctly at the end of epoch.
In that sense, I would advise to remove the bug label and discuss whether there is a need to update the docs to  point out the differences in logging vs. metric computation / aggregation.
		</comment>
		<comment id='7' author='xiadingZ' date='2020-11-04T00:07:26Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 is correct. When you log the output of the metric for each batch, it will take the mean of each one when it reports the value at the end of the epoch. This is not always correct as the mean of a metrics computation over many batches is not always the same as the metrics computation on all of the data. This is why I have personally been against logging epoch level values in the validation step as it can lead to this confusion.
To remedy this issue you can actually log the metric object itself, which will automatically call compute on epoch:
Instead of
acc = self.val_acc(label_hat, y)
self.log('val_acc', acc, on_epoch=True, sync_dist=True)
Do:
self.val_acc(label_hat, y)
self.log('val_acc', self.valid_acc, on_epoch=True)
		</comment>
	</comments>
</bug>