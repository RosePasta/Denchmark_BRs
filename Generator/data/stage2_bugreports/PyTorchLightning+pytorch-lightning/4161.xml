<bug id='4161' author='clearlovewl' open_date='2020-10-15T03:23:12Z' closed_time='2020-10-16T01:04:30Z'>
	<summary>Circulation training increases memory</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;I reproduce using [the BoringModel and post here] https://colab.research.google.com/drive/1HvWVVTK8j2Nj52qU4Q4YCyzOm0_aLQF3?usp=sharing&lt;/denchmark-h&gt;

This is my BoringModel colab link here.
&lt;denchmark-link:https://colab.research.google.com/drive/1PGgSuL3Rm-pWQgBNS75LYE5PQBODueWd?usp=sharing#scrollTo=4Dk6Ykv8lI7X&gt;https://colab.research.google.com/drive/1PGgSuL3Rm-pWQgBNS75LYE5PQBODueWd?usp=sharing#scrollTo=4Dk6Ykv8lI7X&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

codeÔºö
for i in range(0,50): test_x(tmpdir) print(torch.cuda.max_memory_allocated()/ 1024**2,'\n','\n','\n','\n')
The torch.cuda.max_memory_allocated() is increasing.
I  ran out of memory in the second loop in real situation
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:

GPU:

Tesla P4


available:         True
version:           10.1


Packages:

numpy:             1.18.5
pyTorch_debug:     False
pyTorch_version:   1.6.0+cu101
pytorch-lightning: 1.0.1
tqdm:              4.41.1


System:

OS:                Linux
architecture:

64bit



processor:         x86_64
python:            3.6.9
version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020



&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

Wait online for a solution.
	</description>
	<comments>
		<comment id='1' author='clearlovewl' date='2020-10-15T03:23:53Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='clearlovewl' date='2020-10-15T12:29:04Z'>
		please enable access on the colab :)
		</comment>
		<comment id='3' author='clearlovewl' date='2020-10-15T12:33:32Z'>
		&lt;denchmark-link:https://colab.research.google.com/drive/1PGgSuL3Rm-pWQgBNS75LYE5PQBODueWd?usp=sharing&gt;https://colab.research.google.com/drive/1PGgSuL3Rm-pWQgBNS75LYE5PQBODueWd?usp=sharing&lt;/denchmark-link&gt;

this is  the enable access link
		</comment>
		<comment id='4' author='clearlovewl' date='2020-10-16T01:04:30Z'>
		hey, thanks for posting.

You might need to set the seed.
i added clear mem at the loop start.

He have tests that specifically check for memory leaks, so I'm not finding anything on our end. Maybe double check your code? Feel free to try any of our pl_examples as well to see if you notice the same behavior
Finally, the Colab looks pretty constant over time...
&lt;denchmark-link:https://colab.research.google.com/drive/1QTEzsQ32RZ12S2YfARVzlBI8Z9CMyvbH?usp=sharing&gt;https://colab.research.google.com/drive/1QTEzsQ32RZ12S2YfARVzlBI8Z9CMyvbH?usp=sharing&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='clearlovewl' date='2020-10-16T05:39:59Z'>
		Thank you for your kind answer, but I still have problems with my project.
Because of the needs of my project, I need to run the program over and over again to measure the performance of the model. So Each time I give the model a different SEED. This can also lead to memory leaks. This makes me wonder why giving different seedings would cause this problem.
based on your code ÔºåI change seed. colab link :
&lt;denchmark-link:https://colab.research.google.com/drive/1KUz-IFZ8RMK9O2Gd4XSaYbRe8iFFSD09?usp=sharing&gt;https://colab.research.google.com/drive/1KUz-IFZ8RMK9O2Gd4XSaYbRe8iFFSD09?usp=sharing&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>