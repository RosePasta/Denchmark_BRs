<bug id='3668' author='gerardsn' open_date='2020-09-26T15:38:17Z' closed_time='2020-10-06T01:30:42Z'>
	<summary>incorrect batch_sizes when Dataloader returns a dict with multiple tensors.</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Tracked batch sizes in result object are incorrect when a Dataloader returns a dict with multiple tensors.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Create data loader that returns a dict, e.g. batch = {'batchA': tensor_A, 'batchB': tensor_B}.
Both entires have batch size N with N != 2.
For this example a batch size of 2 will be logged since len(batch) == 2.



pytorch-lightning/pytorch_lightning/trainer/evaluation_loop.py


        Lines 147 to 150
      in
      05e5f03






 # track batch size for weighted average 



 is_result_obj = isinstance(output, Result) 



 if is_result_obj: 



 output.track_batch_size(len(batch)) 








pytorch-lightning/pytorch_lightning/trainer/training_loop.py


        Lines 304 to 306
      in
      05e5f03






 # track batch size for weighted average 



 if is_result_obj: 



 training_step_output.track_batch_size(len(split_batch)) 





&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Log correct batch size.
I'm not sure what can be defined as the 'correct' batch size when there are multiple tensors, but I expect that each tensor in the dict has the same batch_size. So, maybe something like:
if is_result_obj:
    if isinstance(batch, dict):
        batch = batch[list(batch.keys())[0]]
    result_obj.track_batch_size(len(batch))
	</description>
	<comments>
		<comment id='1' author='gerardsn' date='2020-10-01T20:33:02Z'>
		I think doing just len(batch) is still wrong since here if the batch is a tuple or some kind of custom batch datatype then len(batch) will be wrong. Considering the basic mnist example too it will give 2 only which is wrong.
		</comment>
		<comment id='2' author='gerardsn' date='2020-10-02T03:17:19Z'>
		This should probably catch most things. Might be a bit much though.
It returns 1 if it fails to determine the batch size to prevent issues with weighted averaging in reduce_on_epoch_end.
if is_result_obj:
    result_obj.track_batch_size(unpack_batchsize(batch))

# maybe add as staticmethod to ResultObj?
def unpack_batchsize(sample):
    """ 
    Recursively unpack sample to find a torch.Tensor.
    returns len(tensor) when found, or 1 when it hits an empty or non iterable.
    """
    if isinstance(sample, torch.Tensor):
        sample = len(sample)
    elif isinstance(sample, dict):
        sample = next(iter(sample.values()), 1)
    elif isinstance(sample, Iterable):
        sample = next(iter(sample), 1)
    else:
        sample = 1  

    if isinstance(sample, int):
        return sample
    return unpack_batchsize(sample)
		</comment>
		<comment id='3' author='gerardsn' date='2020-10-02T03:59:08Z'>
		I suggest adding a function to the LightningModule batch_len_fx which defaults to len if it is not overriden. Anything could be a batch and lightning shouldn't have the responsability of supporting any batch type.
		</comment>
		<comment id='4' author='gerardsn' date='2020-10-02T06:04:56Z'>
		Exactly what I had in mind &lt;denchmark-link:https://github.com/carmocca&gt;@carmocca&lt;/denchmark-link&gt;
. Or maybe simple ask to put  in  itself if ??
.log('some_metric', metric_value, on_epoch=True, batch_size=batch_size)
.log('some_metric', metric_value, on_epoch=False)
		</comment>
		<comment id='5' author='gerardsn' date='2020-10-02T06:07:11Z'>
		Lightning currently defaults to weighted_mean for reduction on epoch end by substituting the reduction method if it is torch.mean:



pytorch-lightning/pytorch_lightning/core/step_result.py


        Lines 389 to 390
      in
      ebc1b23






 if fx == torch.mean: 



 reduced_val = weighted_mean(result[k], batch_sizes) 





If this is the desired behaviour, I think Lightning should at least attempt getting a reasonable estimate for the batch size. In most use cases the dataloader will return multiple tensors, resulting in an incorrect batch estimate if  is the default. (e.g. any supervised method has at least (X, y) in its batch, producing  as mentioned by &lt;denchmark-link:https://github.com/rohitgr7&gt;@rohitgr7&lt;/denchmark-link&gt;
)
This could still be done using batch_len_fx though. On first call, if the method is not overriden, replace the batch_len_fx with a reasonable estimate based on the type of batch. (e.g. len of [tensor, first value in Iterable])
		</comment>
		<comment id='6' author='gerardsn' date='2020-10-02T06:18:16Z'>
		
Exactly what I had in mind @carmocca. Or maybe simple ask to put batch_size in .log itself if on_epoch=True??
.log('some_metric', metric_value, on_epoch=True, batch_size=batch_size)
.log('some_metric', metric_value, on_epoch=False)

this should work too. Probably default to 1 if not provided since len is likely to be wrong.
		</comment>
		<comment id='7' author='gerardsn' date='2020-10-02T20:54:24Z'>
		&lt;denchmark-link:https://github.com/gerardsn&gt;@gerardsn&lt;/denchmark-link&gt;
 I have a problem exactly with this  function.
I'm working with the latest Lightning version from master.



pytorch-lightning/pytorch_lightning/core/step_result.py


         Line 369
      in
      ebc1b23






 def reduce_on_epoch_end(cls, outputs): 





It gets outputs = [{'checkpoint_on': tensor(28.3303, device='cuda:0'), 'val_loss': tensor(28.3303, device='cuda:0'), 'val_precision1': 0.12652068126520682}].
Because I have only one batch per epoch in validation.
Lightning tries to reduce on epoch end.
It feeds
result = tensor([27.8364], device='cuda:0'), weights=tensor([2]), into the weighted_mean function and I get an error here:



pytorch-lightning/pytorch_lightning/core/step_result.py


         Line 897
      in
      ebc1b23






 weights = weights.to(result.device)[:result.size(0)] 





AttributeError: 'list' object has no attribute 'device'
I think it's related to this issue. It would be nice to not reduce anything if it's just one batch per epoch.
		</comment>
		<comment id='8' author='gerardsn' date='2020-10-02T22:15:58Z'>
		&lt;denchmark-link:https://github.com/fogside&gt;@fogside&lt;/denchmark-link&gt;
 in your example result is a tensor so  should not though an error.
		</comment>
		<comment id='9' author='gerardsn' date='2020-10-02T22:17:58Z'>
		
@fogside in your example result is a tensor so result.device should not though an error.

But it's a list with a tensor inside.
		</comment>
		<comment id='10' author='gerardsn' date='2020-10-02T22:21:30Z'>
		
result = tensor([27.8364], device='cuda:0'), weights=tensor([2])

you refering to this right?
		</comment>
		<comment id='11' author='gerardsn' date='2020-10-02T22:29:07Z'>
		

result = tensor([27.8364], device='cuda:0'), weights=tensor([2])

you refering to this right?

Sorry, I just realized, that I was mistaken.
Actually it calls this method twice for some reason.
I added prints at the beginning of weighted_mean function and at the reduce_on_epoch_end (I also changed the number of batches in this example)
&lt;denchmark-code&gt;Result[k] tensor([23.6331, 26.0617, 24.0941, 25.3255], device='cuda:0')
result:  tensor([23.6331, 26.0617, 24.0941, 25.3255], device='cuda:0')
weights:  tensor([2, 2, 2, 2])
Result[k] [0.14285714285714285, 0.06451612903225806, 0.056179775280898875, 0.13793103448275862]
result:  [0.14285714285714285, 0.06451612903225806, 0.056179775280898875, 0.13793103448275862]
weights:  tensor([2, 2, 2, 2])
&lt;/denchmark-code&gt;

And on the second time it gives me the error.
		</comment>
		<comment id='12' author='gerardsn' date='2020-10-02T22:30:45Z'>
		are you logging non-tensor values? maybe doing .item() somewhere in the logs? if not, can you put .log statements here??
		</comment>
		<comment id='13' author='gerardsn' date='2020-10-02T22:33:17Z'>
		
are you logging non-tensor values?

Yes, I was calculating precision in numpy.. Isn't it possible to log non-tensor values?
		</comment>
		<comment id='14' author='gerardsn' date='2020-10-02T22:34:43Z'>
		no.. also to calculate precision or anyother metric you can try pl.metrics package which computes these metrics on the current device itself.
or you can just do torch.tensor(numpy_value) in .log
		</comment>
		<comment id='15' author='gerardsn' date='2020-10-02T22:42:38Z'>
		
no.. also to calculate precision or another metric you can try pl.metrics package which does all of these on the current device itself.

I see. Thank you!
Actually I was trying to work with &lt;denchmark-link:https://github.com/KevinMusgrave/pytorch-metric-learning&gt;pytorch-metric-learning&lt;/denchmark-link&gt;
 and used some function for topk precision estimation from there. But it looks quite tough to merge these 2 frameworks. I see now that topk precision should be calculated in pytorch. Another thing is that I need to have as big batch as possible to get a good topk estimation (it's even better to have the whole val set), that's why I found it hard to make this estimations in the . Maybe I should look into some Callbacks?
But it's not related to this issue.
		</comment>
		<comment id='16' author='gerardsn' date='2020-10-02T22:51:57Z'>
		already working on topk accuracy. Maybe will add topk precision and recall in pl.metrics as well. Can you point me to the implementation of topk precision in pytorch-metric-learning package. It would be helpful. Thanks :)
		</comment>
		<comment id='17' author='gerardsn' date='2020-10-03T08:05:17Z'>
		
already working on topk accuracy. Maybe will add topk precision and recall in pl.metrics as well. Can you point me to the implementation of topk precision in pytorch-metric-learning package. It would be helpful. Thanks :)

It's great!
Sure. I used the class AccuracyCalculator like this
&lt;denchmark-code&gt;accuracy_calculator = AccuracyCalculator(include=("mean_average_precision_at_r"),  k=5)
accuracies = self.accuracy_calculator.get_accuracy(embeddings,
                                                           embeddings,
                                                           labels,
                                                           labels,
                                                           True)

&lt;/denchmark-code&gt;

Implementation:
&lt;denchmark-link:https://github.com/KevinMusgrave/pytorch-metric-learning/blob/10bed5ee8719a543827aa32ea658603c2fcb0130/src/pytorch_metric_learning/utils/accuracy_calculator.py#L45&gt;https://github.com/KevinMusgrave/pytorch-metric-learning/blob/10bed5ee8719a543827aa32ea658603c2fcb0130/src/pytorch_metric_learning/utils/accuracy_calculator.py#L45&lt;/denchmark-link&gt;

		</comment>
		<comment id='18' author='gerardsn' date='2020-10-03T14:32:57Z'>
		So I guess 2 things should be fixed:

Track correct batch_size
Allow non-tensor numeric values in .log(...)

		</comment>
		<comment id='19' author='gerardsn' date='2020-10-05T13:43:15Z'>
		
@gerardsn I have a problem exactly with this weighted_mean function.
I'm working with the latest Lightning version from master.



pytorch-lightning/pytorch_lightning/core/step_result.py


         Line 369
      in
      ebc1b23






 def reduce_on_epoch_end(cls, outputs): 





It gets outputs = [{'checkpoint_on': tensor(28.3303, device='cuda:0'), 'val_loss': tensor(28.3303, device='cuda:0'), 'val_precision1': 0.12652068126520682}].
Because I have only one batch per epoch in validation.
Lightning tries to reduce on epoch end.
It feeds
result = tensor([27.8364], device='cuda:0'), weights=tensor([2]), into the weighted_mean function and I get an error here:



pytorch-lightning/pytorch_lightning/core/step_result.py


         Line 897
      in
      ebc1b23






 weights = weights.to(result.device)[:result.size(0)] 





AttributeError: 'list' object has no attribute 'device'
I think it's related to this issue. It would be nice to not reduce anything if it's just one batch per epoch.

this is fixed on master
		</comment>
		<comment id='20' author='gerardsn' date='2020-10-05T13:44:31Z'>
		ok, making changes to this today.
What do we want as the default behavior? doesn't the custom reduce function solve the problem of custom batches etc?
		</comment>
		<comment id='21' author='gerardsn' date='2020-10-05T14:43:52Z'>
		The batches are not tracked correctly.
		</comment>
	</comments>
</bug>