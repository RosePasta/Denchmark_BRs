<bug id='490' author='ryanwongsa' open_date='2019-11-10T21:41:49Z' closed_time='2019-11-12T03:58:33Z'>
	<summary>Early stopping conditioned on metric `val_loss` isn't recognised when setting the val_check_interval</summary>
	<description>
Describe the bug
Training stops when setting val_check_interval&lt;1.0 in the Trainer class as it doesn't recognise val_loss. I get the following warning at the end of the 3rd epoch:
&lt;denchmark-code&gt;Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,train_loss
&lt;/denchmark-code&gt;

To Reproduce
Steps to reproduce the behavior:

Run the CoolModel example but change the trainer line to
trainer = Trainer(val_check_interval=0.5,default_save_path="test")
Training will stop at the end of the third epoch and the above warning will show.

Expected behavior
Training shouldn't stop and val_loss should be recognised.
Desktop (please complete the following information):

VM: Google Colab
Version 0.5.3.2

Additional context
This doesn't happen with 0.5.2.1 although it looks like something has changed with model saving mechanism since it only seems to save the best model in 0.5.3.2.
EDIT: Also seems to happen when setting train_percent_check&lt;1.0
	</description>
	<comments>
		<comment id='1' author='ryanwongsa' date='2019-11-10T22:10:37Z'>
		can you post your test_end step?
		</comment>
		<comment id='2' author='ryanwongsa' date='2019-11-10T23:53:53Z'>
		I didn't use a test set since it is optional. The default MNIST example in the README will reproduce the behaviour when changing the trainer line to:
&lt;denchmark-code&gt;trainer = Trainer(val_check_interval=0.5,default_save_path="log_dir")
# or
trainer = Trainer(train_percent_check=0.5,default_save_path="log_dir")
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='ryanwongsa' date='2019-11-11T00:56:40Z'>
		sorry, meant validation_end
		</comment>
		<comment id='4' author='ryanwongsa' date='2019-11-11T07:59:32Z'>
		    def validation_end(self, outputs):
        # OPTIONAL
        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
        tensorboard_logs = {'val_loss': avg_loss}
        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}
I tried changing 'avg_val_loss' -&gt; 'val_loss' but the same issue occurs.
		</comment>
		<comment id='5' author='ryanwongsa' date='2019-11-11T11:32:58Z'>
		it should be val_loss
		</comment>
		<comment id='6' author='ryanwongsa' date='2019-11-11T13:21:57Z'>
		I tried it with val_loss too.
&lt;denchmark-code&gt;    def validation_end(self, outputs):
        # OPTIONAL
        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
        tensorboard_logs = {'val_loss': avg_loss}
        return {'val_loss': avg_loss, 'log': tensorboard_logs}
&lt;/denchmark-code&gt;

The issue still occurs.
The issue only doesn't happen when using the default val_check_interval and train_percent_check in the Trainer.
		</comment>
		<comment id='7' author='ryanwongsa' date='2019-11-11T13:38:53Z'>
		ok got it. can you share the stacktrace?
		</comment>
		<comment id='8' author='ryanwongsa' date='2019-11-11T13:44:25Z'>
		There is no error just a warning at the end of epoch 3 and then training stops.
&lt;denchmark-code&gt;Epoch 3: : 1894batch [00:04, 403.95batch/s, batch_nb=18, loss=1.014, v_nb=0] /usr/local/lib/python3.6/dist-packages/pytorch_lightning/callbacks/pt_callbacks.py:128: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,train_loss
  RuntimeWarning)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='ryanwongsa' date='2019-11-11T14:20:06Z'>
		It looks like the problem is that there is only one self.callback_metrics which is sometimes overwritten by self.run_training_batch and sometimes by self.run_evaluation. At the same time, early stopping callback uses self.callback_metrics at the end of the training epoch. And the problem is that there can be no validation run at the last training batch. In that case self.callback_metrics will contain only the metrics from the last training batch.
If it is true, we can just force validation computation at the end of the training epoch.
		</comment>
		<comment id='10' author='ryanwongsa' date='2019-11-11T16:18:28Z'>
		&lt;denchmark-link:https://github.com/kuynzereb&gt;@kuynzereb&lt;/denchmark-link&gt;
 we shouldn't force computation. just partition self.callback_metrics to have
self.callback_metrics['train'] = {}
self.callback_metrics['val'] = {}
self.callback_metrics['test'] = {}
anyone interested in the PR?
		</comment>
		<comment id='11' author='ryanwongsa' date='2019-11-11T18:36:49Z'>
		I created a PR &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/492&gt;#492&lt;/denchmark-link&gt;
 but made a simple change to update self.callback_metrics instead as then it won't require changes to the  callback. It also seems more consistent with how the other logging metrics are updated.
		</comment>
		<comment id='12' author='ryanwongsa' date='2019-12-02T03:01:04Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ryanwongsa&gt;@ryanwongsa&lt;/denchmark-link&gt;

Had this issue been fixed?
I'm facing the same issue after changing my local package along with &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/492&gt;#492&lt;/denchmark-link&gt;
.
BTW, I don't have any validation loops and early stopping callbacks.
		</comment>
		<comment id='13' author='ryanwongsa' date='2020-02-17T03:44:09Z'>
		FYI, I was still having this issue which I traced to not having enough trainer.overfit_pc to check relative to my batch-size and num gpus. validation sanity checks and validation end seemed to get skipped (if I ran without early stopping) thereby not returning my loss metrics dict. solved purely by increasing overfit_pc.
		</comment>
		<comment id='14' author='ryanwongsa' date='2020-04-11T17:32:49Z'>
		I encountered the same problem when I set   in the Trainer.
Validation would only be run after some training epochs, but after the first training epoch, &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/17f58d2e1191d61bc5b2b0cfbf1a42dff714ab8e/pytorch_lightning/callbacks/early_stopping.py#L78-L80&gt;this check is done&lt;/denchmark-link&gt;
:
def check_metrics(self, logs):
    monitor_val = logs.get(self.monitor)
    error_msg = (f'Early stopping conditioned on metric `{self.monitor}`'
                 f' which is not available. Available metrics are:'
                 f' `{"`, `".join(list(logs.keys()))}`')

    if monitor_val is None:
        if self.strict:
            raise RuntimeError(error_msg)
        if self.verbose &gt; 0:
            rank_zero_warn(error_msg, RuntimeWarning)

        return False

    return True
And if the strict parameter is set (as default), the trainer terminates with that exception.
I think the problem is that EarlyStopping checks the presence of a validation metric on_epoch_end rather than on_validation_end. If validation is performed at the end of every epoch this is not a problem, but if one tries to run validation less often it becomes a problem. One could set strict to False, but I think users should get a warning if the validation metric they try to monitor is not present after a validation run.
Instead, a good solution is to make EarlyStopping use on_validation_end instead of on_epoch_end. I believe this was the intention of EarlyStopping from the beginning. I'm opening a PR to discuss this quick fix.
		</comment>
	</comments>
</bug>