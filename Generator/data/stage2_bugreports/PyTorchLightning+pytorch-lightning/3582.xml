<bug id='3582' author='VirajBagal' open_date='2020-09-21T12:39:05Z' closed_time='2020-09-22T04:54:19Z'>
	<summary>Out of memory when trainer.save_checkpoint("example.ckpt")</summary>
	<description>
The sbatch session crashes and I get the following error when I include trainer.save_checkpoint("example.ckpt") in my code.

/var/spool/slurmd/job220424/slurm_script: line 15: 39865 Killed                  python roco_train_mlm_lightning.py --run_name debug --precision 16 --mlm_prob 0.15
slurmstepd: error: Detected 3 oom-kill event(s) in step 220424.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.

Same thing happens when I run the code in notebook on the remoteserver. The kernel dies. Please help. Thank you.
	</description>
	<comments>
		<comment id='1' author='VirajBagal' date='2020-09-21T16:27:02Z'>
		&lt;denchmark-link:https://github.com/VirajBagal&gt;@VirajBagal&lt;/denchmark-link&gt;
 which version of lightning are you using? Currently I am not able to reproduce this with 0.9.0.
		</comment>
		<comment id='2' author='VirajBagal' date='2020-09-22T04:19:41Z'>
		&lt;denchmark-link:https://github.com/ananyahjha93&gt;@ananyahjha93&lt;/denchmark-link&gt;
  Let me give some details.
&lt;denchmark-code&gt;    def validation_step(self, batch, batch_idx):

        loss, acc = self.shared_step(batch, batch_idx)
        result = pl.EvalResult(loss)

        container = {'val_loss': loss, 'val_acc': acc}        
        result.log_dict(container, on_step = True, on_epoch = True, prog_bar = True, logger = True)

        return result
&lt;/denchmark-code&gt;

Now, I want to save checkpoints/modelweights at the end of every validation epoch if my 'total val loss' is smaller than previous 'best val loss'. How do I do that?  The output is following:
&lt;denchmark-code&gt;
&lt;/denchmark-code&gt;

Epoch 0:  87%|████████▋ | 13/15 [00:05&lt;00:00,  2.56it/s, loss=2.698, v_num=08zk, step_train_loss=1.17, step_train_acc=0]
Epoch 0:  93%|█████████▎| 14/15 [00:05&lt;00:00,  2.66it/s, loss=2.698, v_num=08zk, step_train_loss=1.17, step_train_acc=0]
Epoch 0: 100%|██████████| 15/15 [00:05&lt;00:00,  2.58it/s, loss=2.698, v_num=08zk, step_train_loss=1.17, step_train_acc=0, step_val_loss=1.08, step_val_acc=0, epoch_val_loss=1.11, epoch_val_acc=0]
&lt;denchmark-code&gt;&lt;/denchmark-code&gt;

I tried manually ModelCheckpoint as well:
&lt;denchmark-code&gt;checkpoint_callback = ModelCheckpoint(
    filepath=os.getcwd(),
    save_top_k=1,
    verbose=True,
    monitor='epoch_val_loss',
    mode='min',
    prefix=''
)
&lt;/denchmark-code&gt;

But it didnt help. I get the following error:

/home/viraj.bagal/anaconda3/envs/medvqa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning:
When using EvalResult(early_stop_on=X) or TrainResult(early_stop_on=X) the
'monitor' key of ModelCheckpoint has no effect.
Remove ModelCheckpoint(monitor='epoch_val_loss) to fix')
warnings.warn(*args, **kwargs)
/home/viraj.bagal/anaconda3/envs/medvqa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: RuntimeWarning: The metric you returned None must be a torch.Tensor instance, checkpoint not saved HINT: what is the value of epoch_val_loss in validation_epoch_end()?
warnings.warn(*args, **kwargs)
/home/viraj.bagal/anaconda3/envs/medvqa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: RuntimeWarning: Can save best model only with epoch_val_loss available, skipping.
warnings.warn(*args, **kwargs)

		</comment>
		<comment id='3' author='VirajBagal' date='2020-09-22T04:53:48Z'>
		Sorry. Its my mistake. This solved the issue.
 result = pl.EvalResult(checkpoint_on = loss)
		</comment>
	</comments>
</bug>