<bug id='4226' author='maxjeblick' open_date='2020-10-19T09:24:10Z' closed_time='2020-11-10T13:01:21Z'>
	<summary>Batch size finder is not working if batch_size is specified in LightningDataModule</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

The batch size finder won't work if the batch size is specified in the LightningDataModule, only (it is natural to define it there).
An instance of a LightningModule always has the attribute &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/core/lightning.py#L1603&gt;hparams&lt;/denchmark-link&gt;
; the batch_size finder raises a  if  isn't found &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/tuner/batch_size_scaling.py#L70&gt;there&lt;/denchmark-link&gt;
.
&lt;denchmark-h:h2&gt;Please reproduce using the BoringModel and post here&lt;/denchmark-h&gt;

&lt;denchmark-link:https://colab.research.google.com/drive/1gruW3UwitVijzkhcUYIzHlIpoIB1shzt?usp=sharing&gt;https://colab.research.google.com/drive/1gruW3UwitVijzkhcUYIzHlIpoIB1shzt?usp=sharing&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Batch size finder works.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:

GPU:

Tesla T4


available:         True
version:           10.1


Packages:

numpy:             1.18.5
pyTorch_debug:     False
pyTorch_version:   1.6.0+cu101
pytorch-lightning: 0.10.0
tqdm:              4.41.1


System:

OS:                Linux
architecture:

64bit



processor:         x86_64
python:            3.6.9
version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020



	</description>
	<comments>
		<comment id='1' author='maxjeblick' date='2020-10-19T12:41:38Z'>
		That is strange, we have a test for that specific case:



pytorch-lightning/tests/trainer/test_trainer_tricks.py


        Lines 233 to 270
      in
      7c4f80a






 @pytest.mark.skipif(not torch.cuda.is_available(), reason="test requires GPU machine") 



 @pytest.mark.parametrize('use_hparams', [True, False]) 



 def test_auto_scale_batch_size_set_model_attribute(tmpdir, use_hparams): 



 """ Test that new batch size gets written to the correct hyperparameter attribute. """ 



 tutils.reset_seed() 



 



 hparams = EvalModelTemplate.get_default_hparams() 



 before_batch_size = hparams.get('batch_size') 



 



 class HparamsEvalModelTemplate(EvalModelTemplate): 



 



 def dataloader(self, *args, **kwargs): 



 # artificially set batch_size so we can get a dataloader 



 # remove it immediately after, because we want only self.hparams.batch_size 



 setattr(self, "batch_size", before_batch_size) 



 dataloader = super().dataloader(*args, **kwargs) 



 del self.batch_size 



 return dataloader 



 



 datamodule_model = MNISTDataModule(data_dir=tmpdir, batch_size=111)  # this datamodule should get ignored! 



 datamodule_fit = MNISTDataModule(data_dir=tmpdir, batch_size=before_batch_size) 



 



 model_class = HparamsEvalModelTemplate if use_hparams else EvalModelTemplate 



 model = model_class(**hparams) 



 model.datamodule = datamodule_model # unused when another module gets passed to .tune() / .fit() 



 



 trainer = Trainer(default_root_dir=tmpdir, 



 max_epochs=1, 



 auto_scale_batch_size=True, 



 gpus=1) 



 trainer.tune(model, datamodule_fit) 



 after_batch_size = model.hparams.batch_size if use_hparams else model.batch_size 



 assert trainer.datamodule == datamodule_fit 



 assert before_batch_size != after_batch_size 



 assert after_batch_size &lt;= len(trainer.train_dataloader.dataset) 



 assert datamodule_fit.batch_size == after_batch_size 



 # should be left unchanged, since it was not passed to .tune() 



 assert datamodule_model.batch_size == 111 





It must be the logic in lightning_hasattr, that is broken, because it should explicit check if dataloader has a field called batch_size:



pytorch-lightning/pytorch_lightning/utilities/parsing.py


        Lines 177 to 197
      in
      7c4f80a






 def lightning_hasattr(model, attribute): 



 """ Special hasattr for lightning. Checks for attribute in model namespace, 



         the old hparams namespace/dict, and the datamodule. """ 



 trainer = model.trainer 



 



 # Check if attribute in model 



 if hasattr(model, attribute): 



 attr = True 



 # Check if attribute in model.hparams, either namespace or dict 



 elif hasattr(model, 'hparams'): 



 if isinstance(model.hparams, dict): 



 attr = attribute in model.hparams 



 else: 



 attr = hasattr(model.hparams, attribute) 



 # Check if the attribute in datamodule (datamodule gets registered in Trainer) 



 elif trainer is not None and trainer.datamodule is not None and hasattr(trainer.datamodule, attribute): 



 attr = getattr(trainer.datamodule, attribute) 



 else: 



 attr = False 



 



 return attr 





		</comment>
		<comment id='2' author='maxjeblick' date='2020-10-19T14:09:05Z'>
		In the test mentioned above, the batch_size is also specified inside the LightningModule. Both attributes batch_size (inside the module and inside the datamodule) are updated correctly.
The batch size finder fails if batch_size is specified in the datamodule, only.
		</comment>
		<comment id='3' author='maxjeblick' date='2020-10-22T16:10:48Z'>
		&lt;denchmark-link:https://github.com/maxjeblick&gt;@maxjeblick&lt;/denchmark-link&gt;
 would you like to try and submit a PR to fix?
		</comment>
		<comment id='4' author='maxjeblick' date='2020-10-22T16:12:08Z'>
		&lt;denchmark-link:https://github.com/edenlightning&gt;@edenlightning&lt;/denchmark-link&gt;
 Yes I can have a look.
		</comment>
	</comments>
</bug>