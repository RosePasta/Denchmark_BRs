<bug id='2540' author='JeremyAlain' open_date='2020-07-07T14:31:54Z' closed_time='2020-07-24T09:32:00Z'>
	<summary>Possible Bug for multiple optimizers, require_grads=False</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Hey first of all, I really like your repository (great work).
I am using your framework to train two models at the same time, i.e. I followed the GAN example. This means that I have also 2 optimizers. The problem that I am facing is the error:
line 99, in backward allow_unreachable=True)  # allow_unreachable flag RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
I investigated a little and have a pretty good idea what solves the problem. Namely in the file &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_loop.py&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_loop.py&lt;/denchmark-link&gt;

on line 610 we loop over our optimizers (I already saw in previous issues that there was some discussion about how to do this). When we then use one optimizer, we set the parameters of the other model to . However if I look at the way this was implemented, assuming we are currently in the second iteration, i.e. we are optimizing with our second optimizer the following happens:
I set all my parameters to requires_grad=False. I then go to my second optimizer and set all the parameters that are being optimized by it to requires_grad=True. I can then optimize these parameters.
However if I then do the next gradient update over my next batch, the parameters of my first model were still set to requires_grad=False.
One can easily look at this with the following code
&lt;denchmark-code&gt;for param in self.get_model().parameters:
      print(param.requires_grad)
&lt;/denchmark-code&gt;

I am not sure, I could be completely wrong, but i feel like after the last optimizer backpropagates, you forget to reset all the parameters to param.requires_grad.  The reason why I could be wrong, is because somehow I guess in your code exmaple with the GAN's it all worked fine (I didn't test that). But in my case it yields this error.
Another way with which I was able to fix it, is by setting my loss to loss.requires_grad(True)
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

Mac
torch==1.3.0
pytorch-lightning==0.8.3
	</description>
	<comments>
		<comment id='1' author='JeremyAlain' date='2020-07-07T14:32:44Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='JeremyAlain' date='2020-07-07T14:55:34Z'>
		Interesting additional fact,
I just noticed, that if I change my loss to loss.requires_grad(True), my model keeps diverging.
However if I add
&lt;denchmark-code&gt;                for param in self.get_model().parameters():
                    param.requires_grad = True
&lt;/denchmark-code&gt;

to line 666 i.e. after self.batch_loss_value.reset() of the file training_loop.py, then my models learn and I have no problems.
I believe this is due to the fact that my first model is never trained anymore because its parameters are set to requires_grad=False and that also implies that loss.requires_grad(True) is not actually a fix.
		</comment>
		<comment id='3' author='JeremyAlain' date='2020-07-10T00:48:53Z'>
		Have you looked into the GAN in bolts?
&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/master/pl_bolts/models/gans/basic/basic_gan_module.py&gt;https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/master/pl_bolts/models/gans/basic/basic_gan_module.py&lt;/denchmark-link&gt;

If it's still an issue we can reopen
		</comment>
		<comment id='4' author='JeremyAlain' date='2020-07-10T07:55:38Z'>
		Hey thanks for the reply,
I actually looked at the GAN Example of the Colab Notebook &lt;denchmark-link:https://colab.research.google.com/drive/1F_RNcHzTfFuQf-LeKvSlud6x7jXYkG31#scrollTo=TyYOdg8g77P0&gt;https://colab.research.google.com/drive/1F_RNcHzTfFuQf-LeKvSlud6x7jXYkG31#scrollTo=TyYOdg8g77P0&lt;/denchmark-link&gt;
. But I did now also look at the example in the link you sent and also ran it locally. It does not produce the error that I had. However to me at least there is no big difference in the two gan examples and I don't see how it solves my problem, because in the background for the training loop we are still using pytorch-lightning.
Note that I haven't actually implemented a GAN. I am working on Multi-Agent Reinforcement Learning and thus I need to train 2 agent's in each training step. I also updated pytorch-lightning to 0.8.5 and the problem still persists.
Note that this might also be related to this issue: &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2172&gt;#2172&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='JeremyAlain' date='2020-07-10T11:17:24Z'>
		in that repo we also have a full RL section. check the docs out?
		</comment>
		<comment id='6' author='JeremyAlain' date='2020-07-14T08:59:29Z'>
		Yes I looked at these, and my Implementation is based on the example files of &lt;denchmark-link:https://github.com/djbyrne&gt;@djbyrne&lt;/denchmark-link&gt;
. However my problem is not specifically related to RL, its just the fact that I try to train 2 networks that act together in the environment at the same time. Even in the DQN examples in the other repo you have 2 networks, but only 1 is trained and the other is periodically updated to the trained one.
So from my point the problem still persists.
		</comment>
		<comment id='7' author='JeremyAlain' date='2020-07-17T12:12:13Z'>
		Are there any plans to further address this issue or not?
		</comment>
		<comment id='8' author='JeremyAlain' date='2020-07-17T12:14:31Z'>
		if there‚Äôs an issue yeah! could you suggest what to change?
Specify current behavior
Specify expected behavior
in pseudocode
		</comment>
		<comment id='9' author='JeremyAlain' date='2020-07-17T12:27:33Z'>
		Will do :)
		</comment>
		<comment id='10' author='JeremyAlain' date='2020-07-24T09:32:00Z'>
		Hey sorry, I was not able to reproduce the bug with a very simple MNIST example that I tried to create. Which means it might be something more intricate that happens with my current Reinforcement Learning set up. I can't share my research code though and right now don't have time to further investigate. However eventually I will have to figure out a solution or be able to reproduce the issue. I will thus close the issue for now, and when I have time to reinvestigate I will let you know with some pseudo code.
		</comment>
	</comments>
</bug>