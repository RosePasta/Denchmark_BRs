<bug id='271' author='lunixbochs' open_date='2020-11-19T03:45:08Z' closed_time='2020-12-17T23:31:52Z'>
	<summary>mkl-dnn BatchNorm appears to be broken as used by Conformer</summary>
	<description>
Conformer (see &lt;denchmark-link:https://github.com/facebookresearch/flashlight/issues/211&gt;#211&lt;/denchmark-link&gt;
) models train and test fine on CUDA, but if I switch to the CPU backend, I get memory corruption in mkl-dnn. Note that I'm primarily using and reproing this on the v0.2 branch of wav2letter/flashlight, but I don't see any relevant changes in BatchNorm.cpp that aren't also in v0.2, so I think this holds as a current flashlight bug.
Repro 1:

Train a conformer model using CUDA
Run asr_test with the newly trained model and notice it seems to work fine.
Build flashlight with a CPU backend and try running asr_test with the Conformer model.
The model will either crash or output gibberish.

Repro 2:

Build mkl-dnn with -fsanitize=address. This can be added by editing mkl-dnn/CMakeLists.txt and adding these lines near the top:

&lt;denchmark-code&gt;add_definitions(-fsanitize=address -ggdb)
link_libraries(-fsanitize=address)
&lt;/denchmark-code&gt;


Build flashlight with the CPU backend and -fsanitize=address (using a similar CMakeLists.txt edit to mkl-dnn)
Run asr_test on a Conformer model.
Notice ASAN complaining about memory corruption during batch norm.

&lt;denchmark-code&gt;==11322==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x7f2780d03800 at pc 0x558cb4e7effd bp 0x7f2797b179a0 sp 0x7f2797b17990
READ of size 4 at 0x7f2780d03800 thread T22
    #0 0x558cb4e7effc in mkldnn::impl::cpu::ncsp_batch_normalization_fwd_t::execute_forward() const::{lambda(int, int)#2}::operator()(int, int) const (/home/user/build/wav2letter/build/Test+0x1bf2ffc)
    #1 0x558cb4e7f728 in void mkldnn::impl::parallel&lt;mkldnn::impl::cpu::ncsp_batch_normalization_fwd_t::execute_forward() const::{lambda(int, int)#2}&gt;(int, mkldnn::impl::cpu::ncsp_batch_normalization_fwd_t::execute_forward() const::{lambda(int, int)#2}) [clone ._omp_fn.0] (/home/user/build/wav2letter/build/Test+0x1bf3728)
    #2 0x7f27b5f69da6  (/lib/x86_64-linux-gnu/libomp.so.5+0x96da6)
    #3 0x7f27b5f7ec92 in __kmp_invoke_microtask (/lib/x86_64-linux-gnu/libomp.so.5+0xabc92)
    #4 0x7f27b5f139b2  (/lib/x86_64-linux-gnu/libomp.so.5+0x409b2)
    #5 0x7f27b5f125e9  (/lib/x86_64-linux-gnu/libomp.so.5+0x3f5e9)
    #6 0x7f27b5f66139  (/lib/x86_64-linux-gnu/libomp.so.5+0x93139)
    #7 0x7f27b5eb9608 in start_thread /build/glibc-ZN95T4/glibc-2.31/nptl/pthread_create.c:477
    #8 0x7f27b56f4292 in __clone (/lib/x86_64-linux-gnu/libc.so.6+0x122292)

0x7f2780d03800 is located 0 bytes to the right of 237568-byte region [0x7f2780cc9800,0x7f2780d03800)
allocated by thread T0 here:
    #0 0x7f27b891abc8 in malloc (/lib/x86_64-linux-gnu/libasan.so.5+0x10dbc8)
    #1 0x7f27b686421b in cpu::Allocator::nativeAlloc(unsigned long) (/home/user/opt/lib/libafcpu.so.3+0x8a021b)
    #2 0x7f27b76c38a6 in common::DefaultMemoryManager::alloc(bool, unsigned int, long long*, unsigned int) (/home/user/opt/lib/libafcpu.so.3+0x16ff8a6)

Thread T22 created by T0 here:
    #0 0x7f27b8847805 in pthread_create (/lib/x86_64-linux-gnu/libasan.so.5+0x3a805)
    #1 0x7f27b5f65813  (/lib/x86_64-linux-gnu/libomp.so.5+0x92813)

SUMMARY: AddressSanitizer: heap-buffer-overflow (/home/user/build/wav2letter/build/Test+0x1bf2ffc) in mkldnn::impl::cpu::ncsp_batch_normalization_fwd_t::execute_forward() const::{lambda(int, int)#2}::operator()(int, int) const
Shadow bytes around the buggy address:
  0x0fe5701986b0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x0fe5701986c0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x0fe5701986d0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x0fe5701986e0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x0fe5701986f0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
=&gt;0x0fe570198700:[fa]fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0fe570198710: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0fe570198720: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0fe570198730: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0fe570198740: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0fe570198750: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07 
  Heap left redzone:       fa
  Freed heap region:       fd
  Stack left redzone:      f1
  Stack mid redzone:       f2
  Stack right redzone:     f3
  Stack after return:      f5
  Stack use after scope:   f8
  Global redzone:          f9
  Global init order:       f6
  Poisoned by user:        f7
  Container overflow:      fc
  Array cookie:            ac
  Intra object redzone:    bb
  ASan internal:           fe
  Left alloca redzone:     ca
  Right alloca redzone:    cb
  Shadow gap:              cc
==11322==ABORTING
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='lunixbochs' date='2020-11-19T06:16:59Z'>
		I'm confused about this part: &lt;denchmark-link:https://github.com/facebookresearch/flashlight/blob/master/flashlight/fl/autograd/backend/cpu/BatchNorm.cpp#L95-L100&gt;https://github.com/facebookresearch/flashlight/blob/master/flashlight/fl/autograd/backend/cpu/BatchNorm.cpp#L95-L100&lt;/denchmark-link&gt;

If I remove the axes.size() block that edits dims, it stops tripping ASAN (but still doesn't do inference correctly)
&lt;denchmark-code&gt;  std::cout &lt;&lt; "rawdims1 " &lt;&lt; rawDims[0] &lt;&lt; " " &lt;&lt; rawDims[1] &lt;&lt; " " &lt;&lt; rawDims[2] &lt;&lt; " " &lt;&lt; rawDims[3] &lt;&lt; std::endl;
  if (axes.size() &gt; 1) {
    // if norm on multiple axes, we view all axes as on channel axis
    for (auto ax : axes) {
      rawDims[ax] = (ax != kChannelSizeIdx) ? 1 : nfeatures;
    }
  }
  std::cout &lt;&lt; "rawdims2 " &lt;&lt; rawDims[0] &lt;&lt; " " &lt;&lt; rawDims[1] &lt;&lt; " " &lt;&lt; rawDims[2] &lt;&lt; " " &lt;&lt; rawDims[3] &lt;&lt; std::endl;

output:

rawdims1 412 144 1 1
rawdims2 412 1 412 1
&lt;/denchmark-code&gt;

from the top of cpu/BatchNorm.cpp
&lt;denchmark-code&gt;// Flashlight accept HWCN order according to docs
constexpr size_t kHIdx = 0;
constexpr size_t kWIdx = 1;
constexpr size_t kChannelSizeIdx = 2;
constexpr size_t kBatchSizeIdx = 3;
&lt;/denchmark-code&gt;

and later
&lt;denchmark-code&gt;  auto rawDims = std::vector&lt;int&gt;{(int)input.dims(kWIdx),
                                  (int)input.dims(kHIdx),
                                  (int)input.dims(kChannelSizeIdx),
                                  (int)input.dims(kBatchSizeIdx)};
&lt;/denchmark-code&gt;

however swapping W and H doesn't fully fix the issue, maybe another bug?
&lt;denchmark-code&gt;rawdims1 144 412 1 1
rawdims2 144 1 412 1
&lt;/denchmark-code&gt;

but now the output is bad in a different way than before:
&lt;denchmark-code&gt;|P|: ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' t ' v
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='lunixbochs' date='2020-11-21T06:52:33Z'>
		Conformer is landed on master, planning to run benchmarks as you pointed on time slow down and OOM. One of the fixes which now slowdown a bit training is fix of the bug in Layer Norm, but this should not influence so much as you observed. About CPU batchnorm &lt;denchmark-link:https://github.com/jacobkahn&gt;@jacobkahn&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/vineelpratap&gt;@vineelpratap&lt;/denchmark-link&gt;
  any comments on this?
		</comment>
		<comment id='3' author='lunixbochs' date='2020-11-21T18:39:33Z'>
		&lt;denchmark-link:https://github.com/lunixbochs&gt;@lunixbochs&lt;/denchmark-link&gt;
 — the MKL-DNN batch norm implementation has been buggy for some time unfortunately and no one has worked on it for almost a year. We're currently in the process of writing an OpenCL backend that will also run on the CPU which will be much more performant (&lt;denchmark-link:https://github.com/avidov&gt;@avidov&lt;/denchmark-link&gt;
 is working on this) as ArrayFire doesn't thread operations in the CPU backend — only the OpenCL backend has any sort of parallelism.
Sorry that's not the answer you were hoping to hear. I'll consider updating the CPU backend to a newer version of MKL-DNN this week.
		</comment>
		<comment id='4' author='lunixbochs' date='2020-11-30T14:26:39Z'>
		&lt;denchmark-link:https://github.com/lunixbochs&gt;@lunixbochs&lt;/denchmark-link&gt;
 — I'm in the process of updating the CPU backend to use a new version of &lt;denchmark-link:https://github.com/oneapi-src/oneDNN&gt;OneDNN&lt;/denchmark-link&gt;
 and am adding an RNN primitive. I'll comment here when that stuff is committed. Should be done this week.
		</comment>
		<comment id='5' author='lunixbochs' date='2020-12-15T21:16:21Z'>
		&lt;denchmark-link:https://github.com/lunixbochs&gt;@lunixbochs&lt;/denchmark-link&gt;
 would you mind trying again on top of master? I've just added integration with OneDNN v2.0 and it should hopefully fix some issues that we had with several of those primitives. Let me know if/when you have a chance to try that if it improves things.
		</comment>
		<comment id='6' author='lunixbochs' date='2020-12-16T18:08:43Z'>
		Have a good repro. Looking into it.
		</comment>
		<comment id='7' author='lunixbochs' date='2020-12-16T22:34:13Z'>
		I haven't been able to test this yet, as it's very slow for me to train a conformer model on flashlight master and the v0.2 models aren't compatible. I was using a batch size of 24 to train a conformer on v0.2 and I get OOM even with a batch size as small as 4 on master.
		</comment>
		<comment id='8' author='lunixbochs' date='2020-12-17T00:21:59Z'>
		Have a fix for everything, including the memory corruption in Conformer, etc. There's been a bad bug in CPU batchnorm for quite a while that's fixed by a change that should be out shortly. This should also dramatically improve convergence and fix training and inference on the CPU backend throughout.
		</comment>
		<comment id='9' author='lunixbochs' date='2020-12-17T00:22:48Z'>
		To be clear I'm training on GPU, that's a separate regression. Glad you found this issue though!
		</comment>
		<comment id='10' author='lunixbochs' date='2020-12-17T00:23:41Z'>
		Ah — unsure as to what the issue might be on GPU. Do you have an architecture you might be able to share?
		</comment>
		<comment id='11' author='lunixbochs' date='2020-12-17T00:39:22Z'>
		moving back to the &lt;denchmark-link:https://github.com/facebookresearch/flashlight/issues/211&gt;#211&lt;/denchmark-link&gt;
 thread for that issue which has a bit more context
		</comment>
		<comment id='12' author='lunixbochs' date='2020-12-24T22:02:38Z'>
		Just confirmed the batchnorm fix, thanks so much! Conformer inference is working for me on CPU now.
		</comment>
	</comments>
</bug>