<bug id='287' author='tyalamov' open_date='2020-12-01T14:38:34Z' closed_time='2020-12-03T13:51:25Z'>
	<summary>Conformer model decoding</summary>
	<description>
Hi,
I'm trying to train conformer word piece model with architecture shared in this post:
&lt;denchmark-link:https://github.com/facebookresearch/flashlight/issues/211#issuecomment-723714019&gt;#211 (comment)&lt;/denchmark-link&gt;

The training works very well and the results are impressive, but I have issue with decoding. I tried with asr-decoder from both master branch and antares_conformer2 branch. In master branch the decoder crashes with:
terminate called after throwing an instance of 'std::invalid_argument'
what(): CUDNN_STATUS_BAD_PARAM
Aborted (core dumped)
In antares_conformer2 branch it just hangs and doesn't show any result. There are started threads attached to GPUs but the GPUs was on 0% load and the used GPU memory was only 300M.
I tried all posible combinations with and without lexicon, with kenLM and with zeroLM.
In both branches the asr-test exit with:
E1130 14:58:49.129832 87808 Serializer.h:80 Error while loading "/fast/RUN/EXP/conformer/models/model-conformer-fb/002_model_iter_008.bin": Trying to load an unregistered polymorphic type (fl::Conformer).
Make sure your type is registered with CEREAL_REGISTER_TYPE and that the archive you are using was included (and registered with CEREAL_REGISTER_ARCHIVE) prior to calling CEREAL_REGISTER_TYPE.
If your type is already registered and you still see this error, you may need to use CEREAL_REGISTER_DYNAMIC_INIT.
I also tried to train for few epochs letter model with the same arch and the asr-decoder works fine with it. The problem is only with word pieces. I didn't try with words. The asr-test returns the same error with letter model.
The GPUs that I'm using are:
NVIDIA Quadro RTX 8000
(alos tried on 2080Ti with the same result)
The architecture is:
&lt;denchmark-link:https://github.com/facebookresearch/flashlight/issues/211#issuecomment-723714019&gt;#211 (comment)&lt;/denchmark-link&gt;

Configuration for training that I'm using is:
--arch=...
--archdir=...
--tokens=...
--lexicon=...
--train=...
--valid=...
--criterion=ctc
--batchsize=4
--lr=0.1
--lrcrit=0.01
--lr_decay=5
--lr_decay_step=12
--labelsmooth=0.05
--softwstd=4
--momentum=0.95
--maxgradnorm=1.0
--nthread=16
--mfsc=true
--filterbanks=40
--mintsz=200
--maxisz=33000
--enable_distributed=true
--pcttraineval=1
--pctteacherforcing=99
--netoptim=adagrad
--critoptim=adagrad
--adambeta1=0.95
--adambeta2=0.99
--minloglevel=0
--logtostderr
--onorm=target
--sqnorm
--localnrmlleftctx_disabled=300
--runname=...
--rundir=...
--sampletarget=0.01
--single_epoch=false
--itersave=true
--usewordpiece=true
--wordseparator=_
First I tried with this decoder config:
--test=...
--criterion=ctc
--nthread=8
--maxload=-1
--nthread_decoder_am_forward=8
--nthread_decoder=8
--batchsize=4
--lm=...
--lmtype=kenlm
--decodertype=wrd
--beamsize=50
--beamsizetoken=100
--beamthreshold=50
--smearing=max
--lmweight=0.4
--wordscore=0.4
--uselexicon=true
--lexicon=...
Then I start cutting it down to just giving the am and test list. Nothing makes a difference.
	</description>
	<comments>
		<comment id='1' author='tyalamov' date='2020-12-02T03:23:24Z'>
		Did you train word-piece model with codebase in the code committed in the master or with some intermediate stage (cereal issue is because of different versions which are not compatible)? Could you confirm that if you retrain wp model as you do with letters (the same binary) you still have problems with decode.cpp/test.cpp?
		</comment>
		<comment id='2' author='tyalamov' date='2020-12-03T13:51:25Z'>
		Thank you. When I started new trainig with the last master branch commit the decoding works. The problem was in non compatible old commit as you mentioned.
		</comment>
	</comments>
</bug>