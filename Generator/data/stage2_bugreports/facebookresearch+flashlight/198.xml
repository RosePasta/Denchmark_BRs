<bug id='198' author='alealv' open_date='2020-10-11T18:10:56Z' closed_time='2020-10-16T05:21:00Z'>
	<summary>Compile Error: "nvcc fatal   : Unsupported gpu architecture 'compute_30'"</summary>
	<description>
&lt;denchmark-h:h3&gt;Bug Description&lt;/denchmark-h&gt;

After correct configuration
&lt;denchmark-code&gt;cmake .. -GNinja -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/opt/flashlight -DINTEL_MKL_DIR=/opt/intel/mkl -DFL_BUILD_TESTS=OFF -DKENLM_LIB=/opt/kenlm/lib/libkenlm.a -DKENLM_UTIL_LIB=/opt/kenlm/lib/libkenlm_util.a -DKENLM_MODEL_HEADER=/opt/kenlm/include -DGLOG_ROOT_DIR=/opt/glog -DGLOG_INCLUDE_DIR=/opt/glog/include
&lt;/denchmark-code&gt;

When building I get the following error
&lt;denchmark-code&gt;/usr/local/cuda/bin/nvcc -M -D__CUDACC__ /home/aalvarez/Projects/flashlight/app/asr/third_party/warpctc/src/reduce.cu -o /home/aalvarez/Projects/flashlight/build/CMakeFiles/warpctc.dir/app/asr/third_party/warpctc/src/warpctc_generated_reduce.cu.o.NVCC-depend -ccbin /usr/bin/cc -m64 -Dwarpctc_EXPORTS -Xcompiler ,\"-fopenmp\",\"-fPIC\",\"-O3\",\"-DNDEBUG\" -std=c++11 -gencode arch=compute_30,code=sm_30 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_70,code=compute_70 -gencode arch=compute_75,code=compute_75 -DNVCC -I/usr/local/cuda/include -I/home/aalvarez/Projects/flashlight/app/asr/third_party/warpctc/include
nvcc fatal   : Unsupported gpu architecture 'compute_30'
CMake Error at warpctc_generated_reduce.cu.o.Release.cmake:220 (message):
  Error generating
  /home/aalvarez/Projects/flashlight/build/CMakeFiles/warpctc.dir/app/asr/third_party/warpctc/src/./warpctc_generated_reduce.cu.o
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Platform and Hardware&lt;/denchmark-h&gt;

This is my GPU
&lt;denchmark-code&gt;➜ nvidia-smi -L
GPU 0: GeForce GTX 1650 Ti
&lt;/denchmark-code&gt;

OS
&lt;denchmark-code&gt;➜ uname -a
Linux LP-TBC 5.8.14-050814-generic #202010070730 SMP Wed Oct 7 07:35:25 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Additional Context&lt;/denchmark-h&gt;

Here are my CUDA and NVCC version
&lt;denchmark-code&gt;➜ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2020 NVIDIA Corporation
Built on Tue_Sep_15_19:10:02_PDT_2020
Cuda compilation tools, release 11.1, V11.1.74
Build cuda_11.1.TC455_06.29069683_0
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='alealv' date='2020-10-11T18:13:44Z'>
		My guess is that  needs to be updated &lt;denchmark-link:https://github.com/baidu-research/warp-ctc/blob/master/CMakeLists.txt&gt;look here&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='alealv' date='2020-10-11T19:04:08Z'>
		I updated the app/asr/third_party/warpctc/CMakeLists.txt, but the error persists. Some other file must be setting the -gencode arch=compute_30,code=sm_30
Here is the CMakeLists.txt
&lt;denchmark-code&gt;IF (APPLE)
    cmake_minimum_required(VERSION 3.4)
ELSE()
    cmake_minimum_required(VERSION 2.8)
ENDIF()

project(ctc_release)


include_directories(include)

FIND_PACKAGE(CUDA 6.5)
FIND_PACKAGE(Torch)

MESSAGE(STATUS "warpctc: cuda found ${CUDA_FOUND}")
MESSAGE(STATUS "warpctc: Torch found ${Torch_DIR}")

option(WITH_GPU     "compile warp-ctc with CUDA."     ${CUDA_FOUND})
option(WITH_TORCH   "compile warp-ctc with Torch."    ${Torch_FOUND})
option(WITH_OMP     "compile warp-ctc with OpenMP."   ON)
option(BUILD_TESTS  "build warp-ctc unit tests."      ON)
option(BUILD_SHARED "build warp-ctc shared library."  ON)

if(BUILD_SHARED)
    set(WARPCTC_SHARED "SHARED")
else(BUILD_SHARED)
    set(WARPCTC_SHARED "STATIC")
endif(BUILD_SHARED)

if(WIN32)
    set(CMAKE_STATIC_LIBRARY_PREFIX lib)
    set(CMAKE_C_FLAGS_DEBUG   "${CMAKE_C_FLAGS_DEBUG} /bigobj /MTd")
    set(CMAKE_C_FLAGS_RELEASE  "${CMAKE_C_FLAGS_RELEASE} /bigobj /MT")
    set(CMAKE_CXX_FLAGS_DEBUG  "${CMAKE_CXX_FLAGS_DEBUG} /bigobj /MTd")
    set(CMAKE_CXX_FLAGS_RELEASE   "${CMAKE_CXX_FLAGS_RELEASE} /bigobj /MT")
    foreach(flag_var
            CMAKE_CXX_FLAGS CMAKE_CXX_FLAGS_DEBUG CMAKE_CXX_FLAGS_RELEASE)
        if(${flag_var} MATCHES "/MD")
            string(REGEX REPLACE "/MD" "/MT" ${flag_var} "${${flag_var}}")
        endif(${flag_var} MATCHES "/MD")
    endforeach(flag_var)
else(WIN32)
    # Set c++ flags
    set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -O2")
    set(CUDA_NVCC_FLAGS "${CUDA_NVCC_FLAGS} -O2")
endif(WIN32)

if(APPLE)
    set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -std=c++11")
    add_definitions(-DAPPLE)
endif()
if(WITH_OMP AND NOT APPLE)
    set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -fopenmp")
else()
    add_definitions(-DCTC_DISABLE_OMP)
endif()

# need to be at least 30 or __shfl_down in reduce wont compile
IF (CUDA_VERSION VERSION_LESS "11.0")
    set(CUDA_NVCC_FLAGS "${CUDA_NVCC_FLAGS} -gencode arch=compute_30,code=sm_30")
ENDIF()
set(CUDA_NVCC_FLAGS "${CUDA_NVCC_FLAGS} -gencode arch=compute_35,code=sm_35")

set(CUDA_NVCC_FLAGS "${CUDA_NVCC_FLAGS} -gencode arch=compute_50,code=sm_50")
set(CUDA_NVCC_FLAGS "${CUDA_NVCC_FLAGS} -gencode arch=compute_52,code=sm_52")

IF (CUDA_VERSION VERSION_GREATER "7.6")
    set(CUDA_NVCC_FLAGS "${CUDA_NVCC_FLAGS} -gencode arch=compute_60,code=sm_60")
    set(CUDA_NVCC_FLAGS "${CUDA_NVCC_FLAGS} -gencode arch=compute_61,code=sm_61")
    set(CUDA_NVCC_FLAGS "${CUDA_NVCC_FLAGS} -gencode arch=compute_62,code=sm_62")
ENDIF()

IF ((CUDA_VERSION VERSION_GREATER "9.0") OR (CUDA_VERSION VERSION_EQUAL "9.0"))
    set(CUDA_NVCC_FLAGS "${CUDA_NVCC_FLAGS} -gencode arch=compute_70,code=sm_70")
ENDIF()

IF ((CUDA_VERSION VERSION_GREATER "10.0") OR (CUDA_VERSION VERSION_EQUAL "10.0"))
    set(CUDA_NVCC_FLAGS "${CUDA_NVCC_FLAGS} -gencode arch=compute_75,code=sm_75")
ENDIF()

IF ((CUDA_VERSION VERSION_GREATER "11.0") OR (CUDA_VERSION VERSION_EQUAL "11.0"))
    set(CUDA_NVCC_FLAGS "${CUDA_NVCC_FLAGS} -gencode arch=compute_80,code=sm_80")
ENDIF()

IF(NOT APPLE AND NOT WIN32)
    set(CUDA_NVCC_FLAGS "${CUDA_NVCC_FLAGS} --std=c++11")
    if(WITH_OMP)
    set(CUDA_NVCC_FLAGS "${CUDA_NVCC_FLAGS} -Xcompiler -fopenmp")
    endif()
ENDIF()

IF (APPLE)
    EXEC_PROGRAM(uname ARGS -v  OUTPUT_VARIABLE DARWIN_VERSION)
    STRING(REGEX MATCH "[0-9]+" DARWIN_VERSION ${DARWIN_VERSION})
    MESSAGE(STATUS "warpctc: DARWIN_VERSION=${DARWIN_VERSION}")

    #for el capitain have to use rpath

    IF (DARWIN_VERSION LESS 15)
        set(CMAKE_SKIP_RPATH TRUE)
    ENDIF ()

ELSE()
    #always skip for linux
    set(CMAKE_SKIP_RPATH TRUE)
ENDIF()

# windows treat symbolic file as a real file, which is different with unix
# We create a hidden file and compile it instead of origin source file.
function(windows_symbolic TARGET)
    set(oneValueArgs "")
    set(multiValueArgs SRCS PATH DEPS)
    cmake_parse_arguments(windows_symbolic "${options}" "${oneValueArgs}" "${multiValueArgs}" ${ARGN})
    set(final_path ${CMAKE_CURRENT_SOURCE_DIR}/${windows_symbolic_PATH})
    foreach(src ${windows_symbolic_SRCS})
        get_filename_component(src ${src} NAME_WE)
        if (NOT EXISTS ${final_path}/${src}.cpp OR NOT EXISTS ${final_path}/${src}.cu)
            message(FATAL " ${final_path}/${src}.cc and ${final_path}/${src}.cu must exsits, and ${final_path}/${src}.cu must be symbolic file.")
        endif()

        # only copy the xx.cu to .xx.cu when the content are modified
        set(copy_flag 1)
        if (EXISTS ${final_path}/.${src}.cu)
            file(READ ${final_path}/${src}.cpp SOURCE_STR)
            file(READ ${final_path}/.${src}.cu TARGET_STR)
            if (SOURCE_STR STREQUAL TARGET_STR)
                set(copy_flag 0)
            endif()
        endif()
        if (copy_flag)
            add_custom_command(OUTPUT ${final_path}/.${src}.cu
                    COMMAND ${CMAKE_COMMAND} -E remove ${final_path}/.${src}.cu
                    COMMAND ${CMAKE_COMMAND} -E copy "${final_path}/${src}.cpp" "${final_path}/.${src}.cu"
                    COMMENT "create hidden file of ${src}.cu")
        endif(copy_flag)
        add_custom_target(${TARGET} ALL DEPENDS ${final_path}/.${src}.cu)
    endforeach()
endfunction()

IF (WITH_GPU)

    MESSAGE(STATUS "warpctc: Building shared library with GPU support")
    MESSAGE(STATUS "arpctc: NVCC_ARCH_FLAGS" ${CUDA_NVCC_FLAGS})

    if (WIN32)
        SET(CUDA_NVCC_FLAGS "${CUDA_NVCC_FLAGS} -Xcompiler \"/wd 4068 /wd 4244 /wd 4267 /wd 4305 /wd 4819\"")
        windows_symbolic(ctc_entrypoint SRCS ctc_entrypoint.cu PATH src)
        # CUDA_ADD_LIBRARY(warpctc ${WARPCTC_SHARED} src/.ctc_entrypoint.cu src/reduce.cu)
        set(warpctc_SOURCES
              ${CMAKE_CURRENT_LIST_DIR}/src/.ctc_entrypoint.cu
              ${CMAKE_CURRENT_LIST_DIR}/src/reduce.cu PARENT_SCOPE)
    else()
        # CUDA_ADD_LIBRARY(warpctc ${WARPCTC_SHARED} src/ctc_entrypoint.cu src/reduce.cu)
        set(warpctc_SOURCES
              ${CMAKE_CURRENT_LIST_DIR}/src/ctc_entrypoint.cu
              ${CMAKE_CURRENT_LIST_DIR}/src/reduce.cu PARENT_SCOPE)
        endif(WIN32)

    IF (!WITH_TORCH)
        # TARGET_LINK_LIBRARIES(warpctc ${CUDA_curand_LIBRARY})
        set(warpctc_LIBS ${CUDA_curand_LIBRARY} PARENT_SCOPE)
    ENDIF()
    

    # INSTALL(TARGETS warpctc
    #         RUNTIME DESTINATION "bin"
    #         LIBRARY DESTINATION "lib"
    #         ARCHIVE DESTINATION "lib")

    # INSTALL(FILES include/ctc.h DESTINATION "include")

    IF (WITH_TORCH)
        MESSAGE(STATUS "Building Torch Bindings with GPU support")
        INCLUDE_DIRECTORIES(${CUDA_INCLUDE_DIRS} "${CUDA_TOOLKIT_ROOT_DIR}/samples/common/inc")
        INCLUDE_DIRECTORIES(${Torch_INSTALL_INCLUDE} ${Torch_INSTALL_INCLUDE}/TH ${Torch_INSTALL_INCLUDE}/THC)

        TARGET_LINK_LIBRARIES(warpctc luajit luaT THC TH ${CUDA_curand_LIBRARY})
        INSTALL(TARGETS warpctc
                RUNTIME DESTINATION "${Torch_INSTALL_BIN_SUBDIR}"
                LIBRARY DESTINATION "${Torch_INSTALL_LIB_SUBDIR}"
                ARCHIVE DESTINATION "${Torch_INSTALL_LIB_SUBDIR}")

        SET(src torch_binding/binding.cpp torch_binding/utils.c)
        SET(luasrc torch_binding/init.lua)

        ADD_TORCH_PACKAGE(warp_ctc "${src}" "${luasrc}")
        IF (APPLE)
            TARGET_LINK_LIBRARIES(warp_ctc warpctc luajit luaT THC TH ${CUDA_curand_LIBRARY})
        ELSE()
            TARGET_LINK_LIBRARIES(warp_ctc warpctc luajit luaT THC TH ${CUDA_curand_LIBRARY} gomp)
        ENDIF()
    ENDIF()

ELSE()
    MESSAGE(STATUS "Building shared library with no GPU support")

    if (NOT APPLE AND NOT WIN32)
        set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -std=c++11 -O2")
    ENDIF()

    ADD_LIBRARY(warpctc ${WARPCTC_SHARED} src/ctc_entrypoint.cpp)

    if(BUILD_TESTS)
    add_executable(test_cpu tests/test_cpu.cpp )
    TARGET_LINK_LIBRARIES(test_cpu warpctc)
    SET_TARGET_PROPERTIES(test_cpu PROPERTIES COMPILE_FLAGS "${CMAKE_CXX_FLAGS} --std=c++11")
    endif(BUILD_TESTS)

    INSTALL(TARGETS warpctc
            RUNTIME DESTINATION "bin"
            LIBRARY DESTINATION "lib"
            ARCHIVE DESTINATION "lib")

    INSTALL(FILES include/ctc.h DESTINATION "include")

    IF (WITH_TORCH)
        MESSAGE(STATUS "Building Torch Bindings with no GPU support")
        add_definitions(-DTORCH_NOGPU)
        INCLUDE_DIRECTORIES(${Torch_INSTALL_INCLUDE} ${Torch_INSTALL_INCLUDE}/TH)

        TARGET_LINK_LIBRARIES(warpctc luajit luaT TH)

        INSTALL(TARGETS warpctc
                RUNTIME DESTINATION "${Torch_INSTALL_BIN_SUBDIR}"
                LIBRARY DESTINATION "${Torch_INSTALL_LIB_SUBDIR}"
                ARCHIVE DESTINATION "${Torch_INSTALL_LIB_SUBDIR}")

        SET(src torch_binding/binding.cpp torch_binding/utils.c)
        SET(luasrc torch_binding/init.lua)

        ADD_TORCH_PACKAGE(warp_ctc "${src}" "${luasrc}")
        IF (APPLE)
            TARGET_LINK_LIBRARIES(warp_ctc warpctc luajit luaT TH)
        ELSE()
            TARGET_LINK_LIBRARIES(warp_ctc warpctc luajit luaT TH gomp)
        ENDIF()
    ENDIF()

ENDIF()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='alealv' date='2020-10-11T20:02:54Z'>
		I found the error, it is in cmake/select_compute_arch.cmake. It's in these two lines:
&lt;denchmark-code&gt;diff --git a/cmake/select_compute_arch.cmake b/cmake/select_compute_arch.cmake
index 7388d03..de29cae 100644
--- a/cmake/select_compute_arch.cmake
+++ b/cmake/select_compute_arch.cmake
@@ -30,14 +30,14 @@ endif()
 set(CUDA_KNOWN_GPU_ARCHITECTURES  "Fermi" "Kepler" "Maxwell")
 
 # This list will be used for CUDA_ARCH_NAME = Common option (enabled by default)
-set(CUDA_COMMON_GPU_ARCHITECTURES "3.0" "3.5" "5.0")
+# set(CUDA_COMMON_GPU_ARCHITECTURES "3.0" "3.5" "5.0")
 
 if(CUDA_VERSION VERSION_LESS "7.0")
   set(CUDA_LIMIT_GPU_ARCHITECTURE "5.2")
 endif()
 
 # This list is used to filter CUDA archs when autodetecting
-set(CUDA_ALL_GPU_ARCHITECTURES "3.0" "3.2" "3.5" "5.0")
+# set(CUDA_ALL_GPU_ARCHITECTURES "3.0" "3.2" "3.5" "5.0")
 
 if(CUDA_VERSION VERSION_GREATER "7.0" OR CUDA_VERSION VERSION_EQUAL "7.0")
   list(APPEND CUDA_KNOWN_GPU_ARCHITECTURES "Kepler+Tegra" "Kepler+Tesla" "Maxwell+Tegra")
&lt;/denchmark-code&gt;

But even though I comment them, I still got this warning:
&lt;denchmark-code&gt;nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
&lt;/denchmark-code&gt;

And now got
&lt;denchmark-code&gt;/usr/local/cuda/bin/nvcc -M -D__CUDACC__ /home/aalvarez/Projects/flashlight/lib/sequence/criterion/cuda/ViterbiPath.cu -o /home/aalvarez/Projects/flashlight/build/CMakeFiles/fl-libraries-cuda.dir/lib/sequence/criterion/cuda/fl-libraries-cuda_generated_ViterbiPath.cu.o.NVCC-depend -ccbin /usr/bin/cc -m64 -Xcompiler ,\"-O3\",\"-DNDEBUG\" -std=c++11 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -gencode arch=compute_70,code=compute_70 -gencode arch=compute_75,code=compute_75 -DNVCC -I/home/aalvarez/Projects/flashlight/build/CUB/src/CUB -I/home/aalvarez/Projects/flashlight/.. -I/usr/local/cuda/include
nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
In file included from /usr/local/cuda/include/thrust/detail/config/config.h:27,
                 from /usr/local/cuda/include/thrust/detail/config.h:23,
                 from /usr/local/cuda/include/thrust/iterator/iterator_facade.h:35,
                 from /home/aalvarez/Projects/flashlight/build/CUB/src/CUB/cub/device/../iterator/arg_index_input_iterator.cuh:48,
                 from /home/aalvarez/Projects/flashlight/build/CUB/src/CUB/cub/device/device_reduce.cuh:41,
                 from /home/aalvarez/Projects/flashlight/build/CUB/src/CUB/cub/cub.cuh:53,
                 from /home/aalvarez/Projects/flashlight/lib/sequence/criterion/cuda/ViterbiPath.cu:12:
/usr/local/cuda/include/thrust/detail/config/cpp_dialect.h:104:13: warning: Thrust requires C++14. Please pass -std=c++14 to your compiler. Define THRUST_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.
   THRUST_COMPILER_DEPRECATION(C++14, pass -std=c++14 to your compiler);
             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                            
In file included from /usr/local/cuda/include/thrust/system/cuda/detail/execution_policy.h:33,
                 from /usr/local/cuda/include/thrust/iterator/detail/device_system_tag.h:23,
                 from /usr/local/cuda/include/thrust/iterator/detail/iterator_facade_category.h:22,
                 from /usr/local/cuda/include/thrust/iterator/iterator_facade.h:37,
                 from /home/aalvarez/Projects/flashlight/build/CUB/src/CUB/cub/device/../iterator/arg_index_input_iterator.cuh:48,
                 from /home/aalvarez/Projects/flashlight/build/CUB/src/CUB/cub/device/device_reduce.cuh:41,
                 from /home/aalvarez/Projects/flashlight/build/CUB/src/CUB/cub/cub.cuh:53,
                 from /home/aalvarez/Projects/flashlight/lib/sequence/criterion/cuda/ViterbiPath.cu:12:
/usr/local/cuda/include/thrust/system/cuda/config.h:78:2: error: #error The version of CUB in your include path is not compatible with this release of Thrust. CUB is now included in the CUDA Toolkit, so you no longer need to use your own checkout of CUB. Define THRUST_IGNORE_CUB_VERSION_CHECK to ignore this.
 #error The version of CUB in your include path is not compatible with this release of Thrust. CUB is now included in the CUDA Toolkit, so you no longer need to use your own checkout of CUB. Define THRUST_IGNORE_CUB_VERSION_CHECK to ignore this.
  ^~~~~
CMake Error at fl-libraries-cuda_generated_ViterbiPath.cu.o.Release.cmake:220 (message):
  Error generating
  /home/aalvarez/Projects/flashlight/build/CMakeFiles/fl-libraries-cuda.dir/lib/sequence/criterion/cuda/./fl-libraries-cuda_generated_ViterbiPath.cu.o
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='alealv' date='2020-10-13T08:00:06Z'>
		&lt;denchmark-link:https://nvlabs.github.io/cub/&gt;CUB&lt;/denchmark-link&gt;
 is an NVidia library used by the &lt;denchmark-link:https://github.com/facebookresearch/flashlight/blob/master/lib/sequence/criterion/cuda/ViterbiPath.cu#L12&gt;ViterbiPath&lt;/denchmark-link&gt;
 kernel and it  it doesn't work on those architectures. By Googling it seems you can find earlier versions of CUB that were compatible with  and Kepler, but I'm not sure if they'll have what we use from the API (e.g. ). You can give it a try by using an earlier version that didn't deprecate Kepler, modifying this: &lt;denchmark-link:https://github.com/facebookresearch/flashlight/blob/master/cmake/BuildCUB.cmake#L4&gt;https://github.com/facebookresearch/flashlight/blob/master/cmake/BuildCUB.cmake#L4&lt;/denchmark-link&gt;
 to use a different release. Another option is to not build the Criterions on GPU but only on CPU, and use the Criterions on CPU (slower).
		</comment>
		<comment id='5' author='alealv' date='2020-10-13T10:11:24Z'>
		Thanks for the heads up, although yesterday I managed to compile it.
In addition to all the above changes:

I add this line to the CMakeLists.txt

&lt;denchmark-code&gt;add_compile_definitions(THRUST_IGNORE_CUB_VERSION_CHECK)
&lt;/denchmark-code&gt;


After that, I got another error

&lt;denchmark-code&gt;/usr/bin/c++ -DFL_LIBRARIES_USE_CUDA -DFL_LIBRARIES_USE_KENLM -DFL_LIBRARIES_USE_MKL -DKENLM_MAX_ORDER=6 -DNO_CUDNN_DESTROY_HANDLE -DNO_NCCL_COMM_DESTROY_HANDLE -DTHRUST_IGNORE_CUB_VERSION_CHECK -I/usr/local/cuda/include -I/home/aalvarez/Projects/flashlight/build/cereal/src/cereal/include -I/home/aalvarez/Projects/flashlight -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include -I/home/aalvarez/Projects/flashlight/.. -I/opt/intel/mkl/include -I/opt/kenlm/include -isystem /opt/arrayfire/include -fopenmp -O3 -DNDEBUG -fopenmp -pthread -std=gnu++11 -o CMakeFiles/flashlight.dir/flashlight/autograd/backend/cuda/CudnnUtils.cpp.o -c /home/aalvarez/Projects/flashlight/flashlight/autograd/backend/cuda/CudnnUtils.cpp
In file included from /home/aalvarez/Projects/flashlight/flashlight/autograd/backend/cuda/CudnnUtils.cpp:8:
/home/aalvarez/Projects/flashlight/flashlight/autograd/backend/cuda/CudnnUtils.cpp: In constructor ‘fl::RNNDescriptor::RNNDescriptor(af::dtype, int, int, fl::RnnMode, bool, fl::DropoutDescriptor&amp;)’:
/home/aalvarez/Projects/flashlight/flashlight/autograd/backend/cuda/CudnnUtils.cpp:279:19: error: ‘cudnnSetRNNDescriptor’ was not declared in this scope
   CUDNN_CHECK_ERR(cudnnSetRNNDescriptor(
                   ^~~~~~~~~~~~~~~~~~~~~
/home/aalvarez/Projects/flashlight/../flashlight/flashlight/autograd/backend/cuda/CudnnUtils.h:100:52: note: in definition of macro ‘CUDNN_CHECK_ERR’
 #define CUDNN_CHECK_ERR(expr) ::fl::cudnnCheckErr((expr))
                                                    ^~~~
/home/aalvarez/Projects/flashlight/flashlight/autograd/backend/cuda/CudnnUtils.cpp:279:19: note: suggested alternative: ‘cudnnSetLRNDescriptor’
   CUDNN_CHECK_ERR(cudnnSetRNNDescriptor(
                   ^~~~~~~~~~~~~~~~~~~~~
/home/aalvarez/Projects/flashlight/../flashlight/flashlight/autograd/backend/cuda/CudnnUtils.h:100:52: note: in definition of macro ‘CUDNN_CHECK_ERR’
 #define CUDNN_CHECK_ERR(expr) ::fl::cudnnCheckErr((expr))
                                                    ^~~~
&lt;/denchmark-code&gt;

This is because Nvidia changed its &lt;denchmark-link:https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#backward-compatibility&gt;Deprecation Policy&lt;/denchmark-link&gt;
 and introduce a new version of &lt;denchmark-link:https://docs.nvidia.com/deeplearning/cudnn/api/index.html#release-800-preview&gt;cudnnSetRNNDescriptor&lt;/denchmark-link&gt;

I managed to fix it with this patch
&lt;denchmark-code&gt;diff --git a/flashlight/autograd/backend/cuda/CudnnUtils.cpp b/flashlight/autograd/backend/cuda/CudnnUtils.cpp
index 2a76878..4d8c2e8 100644
--- a/flashlight/autograd/backend/cuda/CudnnUtils.cpp
+++ b/flashlight/autograd/backend/cuda/CudnnUtils.cpp
@@ -275,7 +275,7 @@ RNNDescriptor::RNNDescriptor(
   cudnnRNNAlgo_t algo = CUDNN_RNN_ALGO_STANDARD;
   cudnnDataType_t cudnntype = cudnnMapToType(type);
 
-#if CUDNN_VERSION &gt;= 7000
+#if CUDNN_VERSION &gt;= 7000 &amp;&amp; CUDNN_VERSION &lt; 8000 
   CUDNN_CHECK_ERR(cudnnSetRNNDescriptor(
       handle,
       descriptor,
&lt;/denchmark-code&gt;

Although it would be nice to use the &lt;denchmark-link:https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSetRNNDescriptor_v8&gt;new macro&lt;/denchmark-link&gt;

Finally, everything compiles. I will make a PR with the changes
		</comment>
		<comment id='6' author='alealv' date='2020-10-16T05:21:00Z'>
		Thanks for digging the problem and finally solving it! PRs are always welcome! Closing for now, but feel free to send PR and reopen if it is needed.
		</comment>
	</comments>
</bug>