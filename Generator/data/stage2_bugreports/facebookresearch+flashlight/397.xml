<bug id='397' author='abhinavkulkarni' open_date='2021-01-06T13:22:38Z' closed_time='2021-01-08T05:25:47Z'>
	<summary>Unable to perform inference for a long audio</summary>
	<description>
Hi,
I am trying to run model  from this &lt;denchmark-link:https://github.com/facebookresearch/wav2letter/tree/master/recipes/rasr&gt;README&lt;/denchmark-link&gt;
 on the 30s, 60s and 90s, 120s chunks of the same (longer) audio file and I seem to have trouble getting transcriptions for chunks more than 60s in length.
The audio file in question is uploaded &lt;denchmark-link:https://drive.google.com/file/d/1jArUxe62emNTy0An9xI5OeDg44ra5c95/view?usp=sharing&gt;here&lt;/denchmark-link&gt;
. It's a small 3m audio. It's the audio version of &lt;denchmark-link:https://www.youtube.com/watch?v=hX6Cp4vNexc&amp;t=32s&gt;this&lt;/denchmark-link&gt;
 YouTube video.
I chop this audio file into multiples of 30s as follows:
ffmpeg -i cnbc.wav -ar 16000 -ab 256k -ac 1 -t 30s -f flac cnbc-30s.flac
ffmpeg -i cnbc.wav -ar 16000 -ab 256k -ac 1 -t 60s -f flac cnbc-60s.flac
ffmpeg -i cnbc.wav -ar 16000 -ab 256k -ac 1 -t 90s -f flac cnbc-90s.flac
I then run the transcription using following command once for 30s, 60s and 90s audio chunks:
GLOG_logtostderr=1  ./flashlight/build/bin/asr/fl_asr_tutorial_inference_ctc   \
--am_path=./am_transformer_ctc_stride3_letters_300Mparams.bin   \
--tokens_path=./tokens.txt   \
--lexicon_path=./lexicon.txt   \
--lm_path=./lm_common_crawl_small_4gram_prun0-6-15_200kvocab.bin   \
--sample_rate=16000   \
--beam_size=50   \
--beam_size_token=30   \
--beam_threshold=100   \
--lm_weight=1.5   \
--word_score=0   \
--audio_list=audio.lst
where audio.lst contains full path for cnbc-30s.flac, cnbc-60s.flac, etc. for each invokation.
I can get transcription for the 30s and 60s chunks, but not for 90s one.
Why is this the case? I have plenty of RAM (16GB) and the forwarding should work for a mere 90s audio.
Thanks!
&lt;denchmark-h:h3&gt;Platform and Hardware&lt;/denchmark-h&gt;

Ubuntu 20.04, CUDA 10.1
	</description>
	<comments>
		<comment id='1' author='abhinavkulkarni' date='2021-01-07T03:15:20Z'>
		Hi,
Note that we use stride 3 for the acoustic models (as we use letters) compared to stride 8 (for sentence piece based models) which results in more frames being processed. Also, since you are using a large transformer network, memory usage grows quadratically to compute self attention.
So, this doesn't seem unreasonable. Could you try a smaller architecture ?
		</comment>
		<comment id='2' author='abhinavkulkarni' date='2021-01-07T03:30:02Z'>
		Hey &lt;denchmark-link:https://github.com/vineelpratap&gt;@vineelpratap&lt;/denchmark-link&gt;
,
The memory is not a bottleneck, but perhaps results are suffering due to having to attend over too many frames for the larger audio.
Do you have any idea as to how to transcribe a large audio file? Processing 60s chunk independently one at a time is definitely a solution, but how do you carry context from one chunk to the next?
		</comment>
		<comment id='3' author='abhinavkulkarni' date='2021-01-07T03:37:14Z'>
		
The memory is not a bottleneck

Oh okk. I misunderstood the question. Yes, the current transformer are trained to deal with max length up to 35 sec I believe.

how do you carry context from one chunk to the next

This needs a streaming transformer implementation (for example - &lt;denchmark-link:https://arxiv.org/abs/2010.10759&gt;https://arxiv.org/abs/2010.10759&lt;/denchmark-link&gt;
) ... this is not supported at the moment in the code base...
		</comment>
		<comment id='4' author='abhinavkulkarni' date='2021-01-07T11:49:44Z'>
		&lt;denchmark-link:https://github.com/vineelpratap&gt;@vineelpratap&lt;/denchmark-link&gt;
: Do you have any idea how people were transcribing longer audios with Transformer models in the  project? I tried searching for the issues but didn't find anything.
		</comment>
		<comment id='5' author='abhinavkulkarni' date='2021-01-07T17:51:01Z'>
		I don't think we ever supported transcribing longer audios with transformer based models. Because our training data doesn't contain longer audio files, the model may not perform well on longer audios (See some discussion here &lt;denchmark-link:https://github.com/facebookresearch/flashlight/issues/265&gt;#265&lt;/denchmark-link&gt;
  ).
However, I would expect convolution based model not have this issue and they will work fine..
		</comment>
		<comment id='6' author='abhinavkulkarni' date='2021-01-08T09:15:39Z'>
		We used 920 context size for the relative positional embedding in each transformer layer. One possible solution is to use sinusoidal positional embedding (we didn't experiment with it) instead of relative positional embedding. In this case you probably need to train on long audio otherwise will be again misrepresentative on positions (or use some works on limiting the context size of transformers for streaming).
		</comment>
	</comments>
</bug>