<bug id='210' author='zjuturtle' open_date='2020-10-21T07:35:30Z' closed_time='2020-11-02T17:57:12Z'>
	<summary>&amp;lt;unk&amp;gt; cause training crash</summary>
	<description>
&lt;denchmark-h:h3&gt;Bug Description&lt;/denchmark-h&gt;

In app/asr(previously wav2letter++), when I try to train data with "&lt;unk&gt;" token, I'll got crash on &lt;denchmark-link:https://github.com/facebookresearch/flashlight/blob/1b6eb18f1747078368c27fcad81e4bb5e338c7fd/app/asr/data/Utils.cpp#L46&gt;following code&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;//app/asr/data/Utils.cpp

  auto lit = lexicon.find(word);
  if (lit != lexicon.end()) {
    if (lit-&gt;second.size() &gt; 1 &amp;&amp;
        targetSamplePct &gt;
            static_cast&lt;float&gt;(std::rand()) / static_cast&lt;float&gt;(RAND_MAX)) {
      return lit-&gt;second[std::rand() % lit-&gt;second.size()];
    } else {
    // crash here, when encountering word "&lt;unk&gt;". Because lit-&gt;second is empty.
      return lit-&gt;second[0];
    }
&lt;/denchmark-code&gt;

After some study, following is what I got:
In .tokens/.lexicon file generation(use utilities/prepare_librispeech_wp_and_official_lexicon.py from wav2letter++ repo), we explicitly exclude "&lt;unk&gt;", so we do not have "&lt;unk&gt;" in .tokens/.lexicon file
And before training, we first load lexicon file, create a LexiconMap object. In &lt;denchmark-link:https://github.com/facebookresearch/flashlight/blob/1b6eb18f1747078368c27fcad81e4bb5e338c7fd/lib/text/dictionary/Utils.cpp#L54&gt;load operation&lt;/denchmark-link&gt;
, we explicitly add "&lt;unk&gt;" to LexiconMap and set its spelling empty. Why set "&lt;unk&gt;" spelling to empty?  That's the crash reason.
&lt;denchmark-code&gt;  // Insert unknown word. Note kUnkToken="&lt;unk&gt;"
  lexicon[kUnkToken] = {};
&lt;/denchmark-code&gt;

Then we create Dictionary from LexiconMap object, also set Dictionary's defaultIndex to "&lt;unk&gt;" index.
So when encountering "&lt;unk&gt;", it will just crash. I think set "&lt;unk&gt;" spelling to "&lt;unk&gt;"  will solve this problem
	</description>
	<comments>
		<comment id='1' author='zjuturtle' date='2020-10-21T18:02:41Z'>
		&lt;denchmark-link:https://github.com/xuqiantong&gt;@xuqiantong&lt;/denchmark-link&gt;
 do you know why we set the  to empty ?
		</comment>
		<comment id='2' author='zjuturtle' date='2020-10-23T06:30:41Z'>
		Anyone? Got some suggestions or explanation?
		</comment>
		<comment id='3' author='zjuturtle' date='2020-10-27T04:19:16Z'>
		Yea, indeed, your case is reproducible. But could you tell us why do you want to have exactly  token in the transcriptions? Actually, if you look at the logic right below in 


flashlight/app/asr/data/Utils.cpp


        Lines 51 to 66
      in
      1b6eb18






 if (fallback2Ltr) { 



 auto tokens = splitWrd(word); 



 for (const auto&amp; tkn : tokens) { 



 if (dict.contains(tkn)) { 



       res.push_back(tkn); 



     } else if (!skipUnk) { 



 throw std::invalid_argument( 



 "Unknown token '" + tkn + 



 "' when falling back to letter target for the unknown word: " + 



           word); 



     } 



   } 



 } else if (!skipUnk) { 



 throw std::invalid_argument("Unknown word in the lexicon: " + word); 



 } 



 return res; 




, you can see that we support to either ignore those unknown words or fall back to use letters as their spelling.
So, why do you want to include  in targets?
		</comment>
		<comment id='4' author='zjuturtle' date='2020-11-02T07:35:41Z'>
		I got it. So what is your suggestion if we have '' token in training data? Is it OK just ignore it? Or maybe we should take a special strategy about it?
		</comment>
		<comment id='5' author='zjuturtle' date='2020-11-02T17:57:11Z'>
		Seems like &lt;unk&gt; is not showing up in the comments. :p
But yea, I think it is OK to simply ignore those tokens, by either removing them completely or keeping them as they are originally in targets.
Closing for now. Feel free to reopen this issue, when it becomes a major use case.
		</comment>
	</comments>
</bug>