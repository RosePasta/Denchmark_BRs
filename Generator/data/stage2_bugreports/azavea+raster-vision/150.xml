<bug id='150' author='lewfish' open_date='2017-10-27T15:16:16Z' closed_time='2018-02-09T17:34:22Z'>
	<summary>Suspected bug in training data preparation</summary>
	<description>
Previously, we were able to prepare training data, train a model, and make relatively accurate predictions using the old ship dataset. Recently, we ran the entire pipeline on a new ship dataset and were surprised to find that the predictions were extremely inaccurate. I've been running various experiments to isolate the source of the problem and it seems to be coming from the data preparation script. I think this is true because:

If I train a model using the old data zip file (effectively bypassing the data prep stage), the predictions are good
If I prepare a data zip file using the old dataset, the predictions are bad. This is despite the fact that this zip file is based on the same raw data as used in the above bullet point. This makes me think the problem is not in the new raw data itself.

However, despite this evidence, I cannot find anything unexpected about make_train_chips, make_tf_record, or prep_train_data after reviewing the code, looking at git diffs, adding print statements and looking closely at the visualizations of the TFRecord. It's also strange that when training the model on suspected bad data, the learning curves look about the same as when training on good data.
	</description>
	<comments>
		<comment id='1' author='lewfish' date='2017-10-27T15:25:56Z'>
		Here are predictions with old dataset that was prepared with new code:
&lt;denchmark-link:https://user-images.githubusercontent.com/1896461/32111743-60bcacc4-bb09-11e7-8cb0-41b6ecdee353.png&gt;&lt;/denchmark-link&gt;

Here are predictions with old dataset that was prepared on Sept 1 just after commit &lt;denchmark-link:https://github.com/azavea/raster-vision/tree/bc0f35c31041521ffc539b103a388b55ce48ef10&gt;https://github.com/azavea/raster-vision/tree/bc0f35c31041521ffc539b103a388b55ce48ef10&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/1896461/32111800-91bfd8c8-bb09-11e7-8d33-045ef52b1fcb.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='lewfish' date='2017-11-16T17:15:40Z'>
		I ran some experiments using the "new" ships dataset that Joe labeled and prepared the data, trained, and predicted with the current code.  For these experiments, I used two projects Singapore 09-18 (id 357ddf1f-2e5e-4420-8e75-0eed12d2d20f) and Singapore 08-01 (id 7030c078-cf3e-403f-934a-cef039e56b2d). The most interesting result is that training on a single project works reasonable well (for either of the projects), but training on both of them (which should work at least as well considering that I trained til the validation precision plateaued) results in a large number of large false positives. Here are details of the experiments.
&lt;denchmark-h:h2&gt;jm-singapore-test2&lt;/denchmark-h&gt;

Train on Singapore 09-18, predict on Singapore 08-01
&lt;denchmark-link:https://user-images.githubusercontent.com/1896461/32905374-c0a765b6-cac7-11e7-9751-6b3a9b874a0b.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;jm-singapore-test4&lt;/denchmark-h&gt;

Train on Singapore 08-01, predict on Singapore 09-18
&lt;denchmark-link:https://user-images.githubusercontent.com/1896461/32905380-c58fda68-cac7-11e7-8ec6-8bf02a24a931.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;jm-singapore-test3&lt;/denchmark-h&gt;

Train on two projects: Singapore 09-18 and 08-01, predict on Singapore 09-18
&lt;denchmark-link:https://user-images.githubusercontent.com/1896461/32905388-c90ac5c2-cac7-11e7-8355-479a7a222847.png&gt;&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>