<bug id='599' author='lewfish' open_date='2018-12-04T15:17:36Z' closed_time='2018-12-14T16:51:58Z'>
	<summary>Deeplab not respecting replace_model option</summary>
	<description>
If replace_model is False (the default value), Deeplab should resume training from previously saved checkpoints. After my training job was terminated at step 98k (out of 100k), I tried re-running the job, but it started training from scratch.
See
&lt;denchmark-link:https://github.com/azavea/raster-vision/blob/develop/rastervision/backend/tf_deeplab_config.py#L299&gt;https://github.com/azavea/raster-vision/blob/develop/rastervision/backend/tf_deeplab_config.py#L299&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/azavea/raster-vision/blob/develop/rastervision/backend/tf_deeplab.py#L552-L560&gt;https://github.com/azavea/raster-vision/blob/develop/rastervision/backend/tf_deeplab.py#L552-L560&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='lewfish' date='2018-12-14T16:51:58Z'>
		This seems to be working as expected.  The snippet below shows resumption at step 5 of 6 of a truncated Vegas SpaceNet run:
W1214 16:44:41.145881 Reloader tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.
W1214 16:44:41.145881 140683771066112 tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.
W1214 16:44:41.181892 Reloader tf_logging.py:120] Detected out of order event.step likely caused by a TensorFlow restart. Purging 606 expired tensor e
vents from Tensorboard display between the previous step: 5 (timestamp: 1544805818.0605192) and current step: 5 (timestamp: 1544805870.5187938).
W1214 16:44:41.181892 140683771066112 tf_logging.py:120] Detected out of order event.step likely caused by a TensorFlow restart. Purging 606 expired t
ensor events from Tensorboard display between the previous step: 5 (timestamp: 1544805818.0605192) and current step: 5 (timestamp: 1544805870.5187938)
.
Please make sure that train_restart_dir is set to the location where previous checkpoints can be found and that the sync interval has been set to a low-enough value to ensure that checkpoints are actually present.
        backend = rv.BackendConfig.builder(rv.TF_DEEPLAB) \
                                  .with_task(task) \
                                  .with_model_defaults(rv.MOBILENET_V2) \
                                  .with_num_steps(num_steps) \
                                  .with_batch_size(batch_size) \
                                  .with_debug(debug) \
                                  .with_train_options(train_restart_dir='/my/incomplete/run', sync_interval=5) \
                                  .build()
I am going to close this for now.  Can be reopened if there is further difficulty.
		</comment>
	</comments>
</bug>