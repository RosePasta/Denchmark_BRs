<bug id='407' author='miguelgfierro' open_date='2019-09-13T09:26:47Z' closed_time='2019-09-20T16:04:51Z'>
	<summary>[BUG] OOM in arabic tc notebook</summary>
	<description>
&lt;denchmark-h:h3&gt;Description&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;tests/integration/test_notebooks_text_classification.py .F.              [100%]

=================================== FAILURES ===================================
_____________________________ test_tc_dac_bert_ar ______________________________

notebooks = {'automl_with_pipelines_deployment_aks': '/data/home/nlpadmin/myagent/_work/6/s/examples/sentence_similarity/automl_wi...ipynb', 'bert_senteval': '/data/home/nlpadmin/myagent/_work/6/s/examples/sentence_similarity/bert_senteval.ipynb', ...}
tmp = '/tmp/pytest-of-nlpadmin/pytest-556/tmp3__isin9'

    @pytest.mark.gpu
    @pytest.mark.integration
    def test_tc_dac_bert_ar(notebooks, tmp):
        notebook_path = notebooks["tc_dac_bert_ar"]
        pm.execute_notebook(
            notebook_path,
            OUTPUT_NOTEBOOK,
            kernel_name=KERNEL_NAME,
            parameters=dict(
                NUM_GPUS=1,
                DATA_FOLDER=tmp,
                BERT_CACHE_DIR=tmp,
                BATCH_SIZE=32,
                NUM_EPOCHS=1,
                TRAIN_SIZE=0.8,
                NUM_ROWS=15000,
&gt;               RANDOM_STATE=0,
            ),
        )

tests/integration/test_notebooks_text_classification.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/data/anaconda/envs/integration_gpu/lib/python3.6/site-packages/papermill/execute.py:104: in execute_notebook
    raise_for_execution_errors(nb, output_path)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

nb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour...d_time': '2019-09-13T05:14:55.636390', 'duration': 133.063261, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}
output_path = 'output.ipynb'

    def raise_for_execution_errors(nb, output_path):
        """Assigned parameters into the appropriate place in the input notebook
    
        Parameters
        ----------
        nb : NotebookNode
           Executable notebook object
        output_path : str
           Path to write executed notebook
        """
        error = None
        for cell in nb.cells:
            if cell.get("outputs") is None:
                continue
    
            for output in cell.outputs:
                if output.output_type == "error":
                    error = PapermillExecutionError(
                        exec_count=cell.execution_count,
                        source=cell.source,
                        ename=output.ename,
                        evalue=output.evalue,
                        traceback=output.traceback,
                    )
                    break
    
        if error:
            # Write notebook back out with the Error Message at the top of the Notebook.
            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)
            error_msg_cell = nbformat.v4.new_code_cell(
                source="%%html\n" + error_msg,
                outputs=[
                    nbformat.v4.new_output(output_type="display_data", data={"text/html": error_msg})
                ],
                metadata={"inputHidden": True, "hide_input": True},
            )
            nb.cells = [error_msg_cell] + nb.cells
            write_ipynb(nb, output_path)
&gt;           raise error
E           papermill.exceptions.PapermillExecutionError: 
E           ---------------------------------------------------------------------------
E           Exception encountered at "In [14]":
E           ---------------------------------------------------------------------------
E           RuntimeError                              Traceback (most recent call last)
E           &lt;ipython-input-14-4d09f8f74796&gt; in &lt;module&gt;
E                 7         num_epochs=NUM_EPOCHS,
E                 8         batch_size=BATCH_SIZE,
E           ----&gt; 9         verbose=True,
E                10     )    
E                11 print("[Training time: {:.3f} hrs]".format(t.interval / 3600))
E           
E           /data/home/nlpadmin/myagent/_work/6/s/utils_nlp/models/bert/sequence_classification.py in fit(self, token_ids, input_mask, labels, token_type_ids, num_gpus, num_epochs, batch_size, lr, warmup_proportion, verbose)
E               184                     token_type_ids=token_type_ids_batch,
E               185                     attention_mask=mask_batch,
E           --&gt; 186                     labels=None,
E               187                 )
E               188                 loss = loss_func(y_h, y_batch).mean()
E           
E           /data/anaconda/envs/integration_gpu/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
E               545             result = self._slow_forward(*input, **kwargs)
E               546         else:
E           --&gt; 547             result = self.forward(*input, **kwargs)
E               548         for hook in self._forward_hooks.values():
E               549             hook_result = hook(self, input, result)
E           
E           /data/anaconda/envs/integration_gpu/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py in forward(self, input_ids, token_type_ids, attention_mask, labels)
E               987 
E               988     def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):
E           --&gt; 989         _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)
E               990         pooled_output = self.dropout(pooled_output)
E               991         logits = self.classifier(pooled_output)
E           
E           /data/anaconda/envs/integration_gpu/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
E               545             result = self._slow_forward(*input, **kwargs)
E               546         else:
E           --&gt; 547             result = self.forward(*input, **kwargs)
E               548         for hook in self._forward_hooks.values():
E               549             hook_result = hook(self, input, result)
E           
E           /data/anaconda/envs/integration_gpu/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py in forward(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)
E               731         encoded_layers = self.encoder(embedding_output,
E               732                                       extended_attention_mask,
E           --&gt; 733                                       output_all_encoded_layers=output_all_encoded_layers)
E               734         sequence_output = encoded_layers[-1]
E               735         pooled_output = self.pooler(sequence_output)
E           
E           /data/anaconda/envs/integration_gpu/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
E               545             result = self._slow_forward(*input, **kwargs)
E               546         else:
E           --&gt; 547             result = self.forward(*input, **kwargs)
E               548         for hook in self._forward_hooks.values():
E               549             hook_result = hook(self, input, result)
E           
E           /data/anaconda/envs/integration_gpu/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py in forward(self, hidden_states, attention_mask, output_all_encoded_layers)
E               404         all_encoder_layers = []
E               405         for layer_module in self.layer:
E           --&gt; 406             hidden_states = layer_module(hidden_states, attention_mask)
E               407             if output_all_encoded_layers:
E               408                 all_encoder_layers.append(hidden_states)
E           
E           /data/anaconda/envs/integration_gpu/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
E               545             result = self._slow_forward(*input, **kwargs)
E               546         else:
E           --&gt; 547             result = self.forward(*input, **kwargs)
E               548         for hook in self._forward_hooks.values():
E               549             hook_result = hook(self, input, result)
E           
E           /data/anaconda/envs/integration_gpu/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py in forward(self, hidden_states, attention_mask)
E               389 
E               390     def forward(self, hidden_states, attention_mask):
E           --&gt; 391         attention_output = self.attention(hidden_states, attention_mask)
E               392         intermediate_output = self.intermediate(attention_output)
E               393         layer_output = self.output(intermediate_output, attention_output)
E           
E           /data/anaconda/envs/integration_gpu/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
E               545             result = self._slow_forward(*input, **kwargs)
E               546         else:
E           --&gt; 547             result = self.forward(*input, **kwargs)
E               548         for hook in self._forward_hooks.values():
E               549             hook_result = hook(self, input, result)
E           
E           /data/anaconda/envs/integration_gpu/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py in forward(self, input_tensor, attention_mask)
E               347 
E               348     def forward(self, input_tensor, attention_mask):
E           --&gt; 349         self_output = self.self(input_tensor, attention_mask)
E               350         attention_output = self.output(self_output, input_tensor)
E               351         return attention_output
E           
E           /data/anaconda/envs/integration_gpu/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
E               545             result = self._slow_forward(*input, **kwargs)
E               546         else:
E           --&gt; 547             result = self.forward(*input, **kwargs)
E               548         for hook in self._forward_hooks.values():
E               549             hook_result = hook(self, input, result)
E           
E           /data/anaconda/envs/integration_gpu/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py in forward(self, hidden_states, attention_mask)
E               307 
E               308         # Take the dot product between "query" and "key" to get the raw attention scores.
E           --&gt; 309         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
E               310         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
E               311         # Apply the attention mask is (precomputed for all layers in BertModel forward() function)
E           
E           RuntimeError: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 0; 11.17 GiB total capacity; 10.66 GiB already allocated; 5.56 MiB free; 260.39 MiB cached)

&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;How do we replicate the bug?&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior (i.e. solution)&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Other Comments&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='miguelgfierro' date='2019-09-13T09:27:02Z'>
		FYI &lt;denchmark-link:https://github.com/saidbleik&gt;@saidbleik&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>