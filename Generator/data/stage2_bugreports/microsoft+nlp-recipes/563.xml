<bug id='563' author='daden-ms' open_date='2020-02-19T19:06:01Z' closed_time='2020-03-16T13:49:59Z'>
	<summary>[BUG] when fp16 is set true but amp is not available, finetune in transformer base class would still attempt to initialize the model and optimizer</summary>
	<description>
&lt;denchmark-h:h3&gt;Description&lt;/denchmark-h&gt;

line &lt;denchmark-link:https://github.com/microsoft/nlp-recipes/blob/master/utils_nlp/models/transformers/common.py#L174&gt;https://github.com/microsoft/nlp-recipes/blob/master/utils_nlp/models/transformers/common.py#L174&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;How do we replicate the bug?&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior (i.e. solution)&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Other Comments&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='daden-ms' date='2020-03-16T13:49:58Z'>
		fixed in &lt;denchmark-link:https://github.com/microsoft/nlp-recipes/pull/567&gt;#567&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>