<bug id='367' author='poxyu' open_date='2019-12-06T17:18:42Z' closed_time='2019-12-15T10:37:22Z'>
	<summary>[Bug] convert_points_from_homogeneous - NaN gradients in backward pass</summary>
	<description>
I just experienced a NaN-gradient problem while doing a backward pass here:



kornia/kornia/geometry/conversions.py


         Line 99
      in
      4b0ae70






 torch.tensor(1.) / z_vec, 





torch.where works absolutely fine, but if you have zero divisions you find yourself with NaN-gradients for sure ðŸ’©
Here is a toy example:
&lt;denchmark-code&gt;eps = 1e-8

z_vec: torch.Tensor = torch.tensor([4., 6., 0., -3., 1e-9], requires_grad=True)

scale: torch.Tensor = torch.where(
    torch.abs(z_vec) &gt; eps,
    torch.tensor(1.) / z_vec,
    torch.ones_like(z_vec)
)
scale.backward(torch.ones_like(scale))
&lt;/denchmark-code&gt;

And these are z_vec gradients:
tensor([-0.0625, -0.0278,     nan, -0.1111, -0.0000])
For now my little hack is:
&lt;denchmark-code&gt;...
    # we check for points at infinity
    z_vec: torch.Tensor = points[..., -1:]
    if z_vec.requires_grad:
        def z_vec_backward_hook(grad: torch.Tensor) -&gt; torch.Tensor:
            grad[grad != grad] = 0.
            return grad
        z_vec.register_hook(z_vec_backward_hook)
...
&lt;/denchmark-code&gt;

But not sure if it's good enough.
	</description>
	<comments>
		<comment id='1' author='poxyu' date='2019-12-06T22:49:24Z'>
		&lt;denchmark-link:https://github.com/poxyu&gt;@poxyu&lt;/denchmark-link&gt;
 thanks for reporting, I will investigate it
		</comment>
		<comment id='2' author='poxyu' date='2019-12-07T12:53:24Z'>
		with eps equals to 1e-5 seems to work on my computer
		</comment>
		<comment id='3' author='poxyu' date='2019-12-07T14:37:50Z'>
		Even in this toy example?
In my case it happend during homography regression. I initialized my homography module with some "strong" transformation and after several iterations grads became NaN (inside kornia.HomographyWarper).
As I told you there are no problems during forward pass (I tested all homographies manually) and shit happens only in backward pass, when one of the z_vec value becomes inf after division.
		</comment>
		<comment id='4' author='poxyu' date='2019-12-07T14:39:17Z'>
		I will try to give you a real example instead of this toy one.
		</comment>
		<comment id='5' author='poxyu' date='2019-12-07T16:20:27Z'>
		&lt;denchmark-link:https://github.com/edgarriba&gt;@edgarriba&lt;/denchmark-link&gt;
 this is just an example. After about 13-14 iterations  has three NaNs in it:
&lt;denchmark-code&gt;...

class MyHomography(nn.Module):

    def __init__(self, init_homo: torch.Tensor) -&gt; None:
        super().__init__()
        self.homo = nn.Parameter(init_homo.clone().detach())

    def forward(self) -&gt; torch.Tensor:
        return torch.unsqueeze(self.homo, dim=0)


num_iterations = 400
device = torch.device("cuda")

img_src: np.ndarray = cv2.imread("img_src.png", 0)
img_dst: np.ndarray = cv2.imread("img_dst.png", 0)
img_src_t: torch.Tensor = (kornia.image_to_tensor(img_src).float() / 255.).to(device)
img_dst_t: torch.Tensor = (kornia.image_to_tensor(img_dst).float() / 255.).to(device)

init_homo: torch.Tensor = torch.from_numpy(
    np.array([
        [0.0415, 1.2731, -1.1731],
        [-0.9094, 0.5072, 0.4272],
        [0.0762, 1.3981, 1.0646]
    ])
).float()

height, width = img_dst_t.shape[-2:]
warper = kornia.HomographyWarper(height, width)
dst_homo_src = MyHomography(init_homo=init_homo).to(device)

learning_rate = 1e-3
optimizer = optim.Adam(dst_homo_src.parameters(), lr=learning_rate)

for iter_idx in range(num_iterations):
    # warp the reference image to the destiny with current homography
    img_src_to_dst = warper(img_src_t, dst_homo_src())

    # compute the photometric loss
    loss = F.l1_loss(img_src_to_dst, img_dst_t)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

...
&lt;/denchmark-code&gt;

:
&lt;denchmark-link:https://user-images.githubusercontent.com/8268361/70377432-a5cdf500-1914-11ea-9d90-6af09b1eca29.png&gt;&lt;/denchmark-link&gt;

:
&lt;denchmark-link:https://user-images.githubusercontent.com/8268361/70377431-a5cdf500-1914-11ea-8282-27adca9edea5.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='poxyu' date='2019-12-07T16:24:51Z'>
		tell me if you need more information
		</comment>
		<comment id='7' author='poxyu' date='2019-12-07T18:26:42Z'>
		been trying a couple of things so far. First, created this small unit test:
def test_gradcheck_zeros(self, device):                                      
     points_h = torch.tensor([[1., 2., 3., 4., 5.],                           
                              [4., 6., 0., -3., 1e-9]]).t().to(device)        
                                                                              
     # evaluate function gradient                                             
     points_h = tensor_to_gradcheck_var(points_h)  # to var                   
     assert gradcheck(kornia.convert_points_from_homogeneous, (points_h,),    
                      raise_exception=True)                                   
with your fix it doesn't pass
what I've tried is the following,
scale: torch.Tensor = torch.where(            
    torch.abs(z_vec) &gt; eps,                   
    torch.tensor(1.) / (z_vec + eps), -&gt; torch.tensor(1.) / z_vec.clamp(min=eps), 
    torch.tensor(1.) / (z_vec),               
    torch.ones_like(z_vec))
plus reducing the epsilon to eps -&gt; 1e-5 and it passes all the tests except in those where the points has negative values (for the clamp close to positive zero)
the other trick is to directly add an epsilon value to the z_vec which also breaks other tests related to geometry transforms.
Not really sure what should be the trade-off here, but it's an issue that has been here for a while and discussed at the early stage of the library. Check this: &lt;denchmark-link:https://www.google.com/url?q=https://github.com/tensorflow/graphics/issues/17&amp;sa=D&amp;source=hangouts&amp;ust=1575817956956000&amp;usg=AFQjCNF7s96oq3UR0zzNUQ0S1TuA8x7rKw&gt;https://www.google.com/url?q=https://github.com/tensorflow/graphics/issues/17&amp;sa=D&amp;source=hangouts&amp;ust=1575817956956000&amp;usg=AFQjCNF7s96oq3UR0zzNUQ0S1TuA8x7rKw&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='poxyu' date='2019-12-07T18:30:48Z'>
		another thing to consider is a bug in torch.where since should be able to bypass the gradient correctly based on the binary mask generated
		</comment>
		<comment id='9' author='poxyu' date='2019-12-08T01:21:40Z'>
		&lt;denchmark-link:https://discuss.pytorch.org/t/gradients-of-torch-where/26835/2&gt;https://discuss.pytorch.org/t/gradients-of-torch-where/26835/2&lt;/denchmark-link&gt;

my current workaround is:
&lt;denchmark-code&gt;scale: torch.Tensor = torch.where(
    torch.abs(z_vec) &gt; eps,
#        torch.tensor(1.) / z_vec,
    torch.tensor(1.) / z_vec.masked_fill(z_vec == 0., eps),
    torch.ones_like(z_vec))
&lt;/denchmark-code&gt;

thank you, &lt;denchmark-link:https://github.com/edgarriba&gt;@edgarriba&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='poxyu' date='2019-12-08T13:19:47Z'>
		&lt;denchmark-link:https://github.com/poxyu&gt;@poxyu&lt;/denchmark-link&gt;
 great ! check &lt;denchmark-link:https://github.com/kornia/kornia/pull/369&gt;#369&lt;/denchmark-link&gt;
 , I had to increase the epsilon value otherwise the numerical test didn't pass
		</comment>
		<comment id='11' author='poxyu' date='2019-12-09T01:29:25Z'>
		&lt;denchmark-link:https://github.com/edgarriba&gt;@edgarriba&lt;/denchmark-link&gt;
 I love everything about your PR except this big epsilon 
		</comment>
		<comment id='12' author='poxyu' date='2019-12-09T09:51:20Z'>
		well, it's the only way I found to pass the provided test for gradient check and to not break the other existing tests of the whole framework. I've been also playing a bit with the gradcheck tolerance, but it seems that eps 1e-5 is the magic number that makes gradcheck happy :D
		</comment>
		<comment id='13' author='poxyu' date='2019-12-09T11:23:12Z'>
		Guys, what about?
eps=1e-8
mask: torch.Tensor = z_vec.abs() &gt; eps
scale: torch.Tensor = torch.ones_like(z_vec).masked_scatter_(mask, 1.0 / z_vec[mask])
Although it is basically the same :)
&lt;denchmark-link:https://user-images.githubusercontent.com/4803565/70432062-a6829a80-1a7e-11ea-89a0-d19f1f3dc98a.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/4803565/70432574-e5652000-1a7f-11ea-9ad9-68097aa027f1.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='poxyu' date='2019-12-09T12:25:41Z'>
		&lt;denchmark-link:https://github.com/ducha-aiki&gt;@ducha-aiki&lt;/denchmark-link&gt;
 the test I provided above is still failing with your solution with an epsilon smaller than 1e-5
		</comment>
		<comment id='15' author='poxyu' date='2019-12-09T12:43:45Z'>
		Well, actually, gradient is not correct here, as we are putting 1 instead of correct huge number/inf.
So I am in favor of dropping your test and just use &lt;denchmark-link:https://github.com/poxyu&gt;@poxyu&lt;/denchmark-link&gt;
 version, if it works in practice and not generates NaNs
		</comment>
		<comment id='16' author='poxyu' date='2019-12-09T12:51:29Z'>
		sure, then I believe we should somehow provide &lt;denchmark-link:https://github.com/poxyu&gt;@poxyu&lt;/denchmark-link&gt;
's test (or a small version of it) in order to assure correctness of this function since can be a bit critical for others functions that rely on it.
		</comment>
		<comment id='17' author='poxyu' date='2019-12-09T15:37:45Z'>
		&lt;denchmark-link:https://github.com/poxyu&gt;@poxyu&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ducha-aiki&gt;@ducha-aiki&lt;/denchmark-link&gt;
 I'va updated &lt;denchmark-link:https://github.com/kornia/kornia/pull/369&gt;#369&lt;/denchmark-link&gt;
 with a real case test (using random data). Check it out,
		</comment>
		<comment id='18' author='poxyu' date='2019-12-11T03:13:49Z'>
		Similar problem (zero division and NaN gradients) occurs in backward pass here:



kornia/kornia/geometry/conversions.py


         Line 154
      in
      a0bafcd






 theta = torch.sqrt(theta2) 





Super simple toy example:
&lt;denchmark-code&gt;with torch.autograd.detect_anomaly():
    r_vec = torch.tensor([[0., 0., 0.]], requires_grad=True)
    r_mat = kornia.angle_axis_to_rotation_matrix(r_vec)
    r_mat.backward(torch.ones_like(r_mat))
&lt;/denchmark-code&gt;

I guess it has to be a separate issue, hasn't it? ðŸ™‚
P.S. - found it several minutes ago and haven't fixed it myself yet.
		</comment>
		<comment id='19' author='poxyu' date='2019-12-11T07:39:25Z'>
		Yes, please open a separated issue. Will close this once we merge &lt;denchmark-link:https://github.com/kornia/kornia/pull/369&gt;#369&lt;/denchmark-link&gt;

Regarding this new issue, I would give a try at this code from tf graphics,
&lt;denchmark-link:https://github.com/tensorflow/graphics/blob/master/tensorflow_graphics/geometry/transformation/rotation_matrix_3d.py&gt;https://github.com/tensorflow/graphics/blob/master/tensorflow_graphics/geometry/transformation/rotation_matrix_3d.py&lt;/denchmark-link&gt;
 since at least it's more clean
		</comment>
	</comments>
</bug>