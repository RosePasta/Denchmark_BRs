<bug id='382' author='sfikas' open_date='2019-12-19T18:55:48Z' closed_time='2019-12-20T17:05:28Z'>
	<summary>[Bug] conv_soft_argmax2d input range ?</summary>
	<description>
It seems that conv_soft_argmax2d should accept ranges only in 0.0 - 1.0. Otherwise 'NaN' values tend to appear, as per the following example:
&lt;denchmark-link:https://user-images.githubusercontent.com/16489490/71200670-2c81bb00-22a1-11ea-8d36-fa1a26ae0c7d.jpg&gt;&lt;/denchmark-link&gt;

Is this intended behaviour? If not, this is a bug -- If yes, it should be noted on the documentation and a check should also be added in the code, outputing an error if the input range and type is not proper (I am willing to submit the related pull request if this is the case).
	</description>
	<comments>
		<comment id='1' author='sfikas' date='2019-12-20T08:49:44Z'>
		&lt;denchmark-link:https://github.com/sfikas&gt;@sfikas&lt;/denchmark-link&gt;
 I recently found a similar behavior that could come from the same issue. Currently, inside the conv_softmax operator we compute the softmax with a handmade implementation defined as follows:
x_exp = (input / temperature).exp()
See: &lt;denchmark-link:https://github.com/kornia/kornia/blob/master/kornia/geometry/spatial_soft_argmax.py#L391&gt;https://github.com/kornia/kornia/blob/master/kornia/geometry/spatial_soft_argmax.py#L391&lt;/denchmark-link&gt;

However, it turns out that depending on the input data range, the exponential operator con explode or overflow. This could be the case when your previous layer is e.g. a ReLU. For this reason, there is this exponential normalization trick that will make softmax numerically more stable.
See: &lt;denchmark-link:https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/&gt;https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/&lt;/denchmark-link&gt;

In fact, if you dig deeper into PyTorch itself, you will see that they already apply this trick which is totally transparent to the user.
&lt;denchmark-link:https://github.com/pytorch/pytorch/blob/bcb0bb7e0e03b386ad837015faba6b4b16e3bfb9/aten/src/ATen/native/SoftMax.cpp#L44&gt;https://github.com/pytorch/pytorch/blob/bcb0bb7e0e03b386ad837015faba6b4b16e3bfb9/aten/src/ATen/native/SoftMax.cpp#L44&lt;/denchmark-link&gt;

I will fix this issue myself during the day.
		</comment>
		<comment id='2' author='sfikas' date='2019-12-20T08:59:22Z'>
		Thanks! By the way, same goes for conv_soft_argmax3d, which was what I originally wanted to use, and this in turn because it is part of the affine-invariant keypoint detection tools in kornia.feature (class ScaleSpaceDetector etc.)
		</comment>
		<comment id='3' author='sfikas' date='2019-12-20T09:04:31Z'>
		The 'exploding' behaviour of this softmax implementation could (??) be related to the problem reported in &lt;denchmark-link:https://github.com/kornia/kornia/issues/353&gt;#353&lt;/denchmark-link&gt;
  , &lt;denchmark-link:https://github.com/pablovela5620&gt;@pablovela5620&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='sfikas' date='2019-12-20T09:06:08Z'>
		probably. conv_soft_argmax3d is also affected ;D
		</comment>
	</comments>
</bug>