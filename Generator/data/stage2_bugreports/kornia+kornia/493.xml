<bug id='493' author='xen0f0n' open_date='2020-03-23T23:17:32Z' closed_time='2020-10-19T11:33:18Z'>
	<summary>Focal loss sigmoid instead of softmax</summary>
	<description>
As I was trying to figure out whether focal loss takes logits or sigmoid outputs as input, I saw that is passes the input through a softmax.
Shouldn't focal loss use sigmoid instead of softmax? Or at least provide an option to handle both binary and multi-class tasks?
This was also raised &lt;denchmark-link:https://github.com/kuangliu/pytorch-retinanet/issues/6&gt;here&lt;/denchmark-link&gt;
.
	</description>
	<comments>
		<comment id='1' author='xen0f0n' date='2020-03-24T17:53:41Z'>
		&lt;denchmark-link:https://github.com/xen0f0n&gt;@xen0f0n&lt;/denchmark-link&gt;
 thanks for reporting. Sure, we can update with the implementation you suggest. Can you send a small PR updating ?
I leave here the link to the code: &lt;denchmark-link:https://github.com/kuangliu/pytorch-retinanet/blob/master/loss.py#L16&gt;https://github.com/kuangliu/pytorch-retinanet/blob/master/loss.py#L16&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='xen0f0n' date='2020-03-24T17:54:53Z'>
		/cc &lt;denchmark-link:https://github.com/ducha-aiki&gt;@ducha-aiki&lt;/denchmark-link&gt;
 you played around with this loss. Any insight for the suggested update ?
		</comment>
		<comment id='3' author='xen0f0n' date='2020-03-25T14:31:14Z'>
		&lt;denchmark-link:https://github.com/edgarriba&gt;@edgarriba&lt;/denchmark-link&gt;
, sure!
I'm working on it. When I have something that works as intended, I'll get back to you with a PR!
		</comment>
		<comment id='4' author='xen0f0n' date='2020-03-25T15:04:13Z'>
		&lt;denchmark-link:https://github.com/edgarriba&gt;@edgarriba&lt;/denchmark-link&gt;
 sounds great. Looking forward to see that PR :)
		</comment>
		<comment id='5' author='xen0f0n' date='2020-04-01T20:46:58Z'>
		&lt;denchmark-link:https://github.com/edgarriba&gt;@edgarriba&lt;/denchmark-link&gt;

I've just sent a PR (&lt;denchmark-link:https://github.com/kornia/kornia/pull/506&gt;#506&lt;/denchmark-link&gt;
). Although static check fails .... I'm looking into it...
I have some comments related to the focal loss as implemented here.
The original paper &lt;denchmark-link:https://arxiv.org/abs/1708.02002&gt;https://arxiv.org/abs/1708.02002&lt;/denchmark-link&gt;
 leaves room for different interpretations, both for the binary implementation (focal loss was originally proposed for a binary task) and for the multi-class one.
The point in question is parameter alpha. The authors state:
"we note that Î±, the weight assigned to the rare class, also has a stable range [...]"
It seems to me that alpha=0.25 should be assigned to the rare class but I'm not sure what that means for the other class. Does "stable range" mean 0.75? Maybe it doesn't matter, so 1.0?
I went with alpha (.25), (1-alpha) (0.75) and I've assumed that the rare class is coded as label 1.
Moreover, if I understand correctly, the already implemented focal loss (for the multi-class task) doesn't assign alpha to the rare class, but multiplies every class (the "active" one-hot target) with it.
Should we update the docs to explicitly state the decision regarding alpha?
		</comment>
		<comment id='6' author='xen0f0n' date='2020-04-06T17:05:30Z'>
		&lt;denchmark-link:https://github.com/xen0f0n&gt;@xen0f0n&lt;/denchmark-link&gt;
 will review over the week. I need to read in depth the paper and understand some of the maths :)
		</comment>
		<comment id='7' author='xen0f0n' date='2020-10-06T06:00:51Z'>
		Hi!
I was gonna suggest looking at the following implementation of the &lt;denchmark-link:https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py&gt;binary focal loss from facebook&lt;/denchmark-link&gt;
 (also currently used in the &lt;denchmark-link:https://github.com/facebookresearch/detectron2/blob/8999946492ae6930a4b312dbbea952e326a9f1df/detectron2/modeling/meta_arch/retinanet.py&gt;detectron2&lt;/denchmark-link&gt;
).
It's usage of F.binary_cross_entropy_with_logits makes it numerically stable (because of the log-sum-exp trick), and this removes the need to define eps.
		</comment>
		<comment id='8' author='xen0f0n' date='2020-10-07T21:58:49Z'>
		&lt;denchmark-link:https://github.com/ceroytres&gt;@ceroytres&lt;/denchmark-link&gt;
 check: &lt;denchmark-link:https://github.com/kornia/kornia/pull/506&gt;#506&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='xen0f0n' date='2020-10-07T23:40:19Z'>
		&lt;denchmark-link:https://github.com/edgarriba&gt;@edgarriba&lt;/denchmark-link&gt;
 Sorry, I wasn't sure whether to post the suggestion on here or the pull request.
		</comment>
		<comment id='10' author='xen0f0n' date='2020-10-08T06:49:05Z'>
		&lt;denchmark-link:https://github.com/xen0f0n&gt;@xen0f0n&lt;/denchmark-link&gt;
 do you need help with this? &lt;denchmark-link:https://github.com/ceroytres&gt;@ceroytres&lt;/denchmark-link&gt;
 that PR needs a rebase
		</comment>
		<comment id='11' author='xen0f0n' date='2020-10-08T11:43:34Z'>
		&lt;denchmark-link:https://github.com/edgarriba&gt;@edgarriba&lt;/denchmark-link&gt;
 I've just made another PR &lt;denchmark-link:https://github.com/kornia/kornia/pull/699&gt;#699&lt;/denchmark-link&gt;
.
My apologies for the delay and for any confusion caused!
I WILL make another PR for a MulticlassFocalLoss (or SoftmaxFocalLoss) where I'll give the various alpha factors as a Tensor.
		</comment>
	</comments>
</bug>