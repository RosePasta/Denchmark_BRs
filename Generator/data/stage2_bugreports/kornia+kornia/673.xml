<bug id='673' author='likitha-l' open_date='2020-09-10T20:28:26Z' closed_time='2020-12-22T01:32:47Z'>
	<summary>[Bug] Computed gradients on Kornia Augmentation is None</summary>
	<description>
I would like to compute gradients either 1) on the input params 2) or on the internal params of Kornia Transformations. For reference the following snippet generates 'None' gradients on the parameters_aug that is passed to ColorJitter transformation of Kornia.  I am interested in knowing which transformations support computing gradients on its parameters? It looks like HomographyWarper does support computing gradients.
&lt;denchmark-code&gt;class MyAug(nn.Module):
    def __init__(self) -&gt; None:
        super(MyAug, self).__init__()
        self.parameters_aug = nn.Parameter(torch.tensor([0.1,0.1,0.1,0,.1],
                                                        requires_grad=True))
        self.jit = K.ColorJitter(self.parameters_aug[0],
                                 self.parameters_aug[1],
                                 self.parameters_aug[2],
                                 self.parameters_aug[3],
                                 same_on_batch=True,
                                 return_transform=False)
    def forward(self, input):
        input = self.jit(input)
        return input
 
images,labels = next(iter(trainloader))
aug = MyAug()

with torch.enable_grad():
    logits = cnn(aug(images.to(device)))
    loss = criterion(logits,labels.to(device))
    print(loss.item())
    loss.backward()

assert aug.parameters_aug.grad is not None, 'cannot get grads'

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='likitha-l' date='2020-09-10T20:36:45Z'>
		Dear Korina Team,
as &lt;denchmark-link:https://github.com/likitha-l&gt;@likitha-l&lt;/denchmark-link&gt;
  said,  we would like to know if there is a way to receive gradients either:

on the input to the transformation or
on some internals parameters that the transformation has such as aug.jit['_params'] See below.

We would like also to know which transformations, among the ones you have, provide this capability.
Thank you so much for your feedback.
{'training': True,
 '_parameters': OrderedDict(),
 '_buffers': OrderedDict(),
 '_non_persistent_buffers_set': set(),
 '_backward_hooks': OrderedDict(),
 '_forward_hooks': OrderedDict(),
 '_forward_pre_hooks': OrderedDict(),
 '_state_dict_hooks': OrderedDict(),
 '_load_state_dict_pre_hooks': OrderedDict(),
 '_modules': OrderedDict(),
 'return_transform': False,
 'brightness': tensor(0.1000, grad_fn=&lt;SelectBackward&gt;),
 'contrast': tensor(0.1000, grad_fn=&lt;SelectBackward&gt;),
 'saturation': tensor(0.1000, grad_fn=&lt;SelectBackward&gt;),
  'hue': tensor(0., grad_fn=&lt;SelectBackward&gt;),
  'same_on_batch': True,
 '_params': {'brightness_factor': tensor([0.9479, 0.9479, 0.9479, 0.9479, 0.9479, 0.9479, 0.9479, 0.9479, 0.9479,
          0.9479, 0.9479, 0.9479, 0.9479, 0.9479, 0.9479, 0.9479]),
  'contrast_factor': tensor([0.9778, 0.9778, 0.9778, 0.9778, 0.9778, 0.9778, 0.9778, 0.9778, 0.9778,
          0.9778, 0.9778, 0.9778, 0.9778, 0.9778, 0.9778, 0.9778]),
  'hue_factor': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),
   'saturation_factor': tensor([1.0820, 1.0820, 1.0820, 1.0820, 1.0820, 1.0820, 1.0820, 1.0820, 1.0820,
          1.0820, 1.0820, 1.0820, 1.0820, 1.0820, 1.0820, 1.0820]),
  'order': tensor([3, 2, 1, 0])}}```
		</comment>
		<comment id='2' author='likitha-l' date='2020-09-11T01:41:01Z'>
		I have created a &lt;denchmark-link:https://colab.research.google.com/drive/1MfW2zwRfRR5_Rarex9ZhXONEt2U23QYI?usp=sharing&gt;Colab to check the issue here&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='likitha-l' date='2020-09-11T03:35:30Z'>
		Thanks for the issue. The current release does not support it. Please install master by:
! pip install git+https://github.com/kornia/kornia.git
I am still fine-tuning the functions. But here I can show you one example:
torch.manual_seed(42)
jit = K.ColorJitter(
    nn.Parameter(torch.tensor([0.1, 0.1],requires_grad=True)),
    nn.Parameter(torch.tensor([0.1, 0.1],requires_grad=True)),
    nn.Parameter(torch.tensor([0.1, 0.1],requires_grad=True)),
    nn.Parameter(torch.tensor([0.1, 0.1],requires_grad=True)),
    same_on_batch=True,
    return_transform=False
).to(torch.float32)

out = jit(torch.ones(1, 3, 6, 6, dtype=torch.float32))
loss = nn.L1Loss()(out, torch.randn_like(out, dtype=torch.float32) * 100)
optimizer = torch.optim.SGD(jit.parameters(), lr=10)

loss.backward()
optimizer.step()


print([f for f in jit.parameters()])
print(jit.brightness.grad)
&lt;denchmark-code&gt;[Parameter containing:
tensor([0.1131, 0.1980], requires_grad=True), Parameter containing:
tensor([0.1094, 0.2017], requires_grad=True), Parameter containing:
tensor([0.1000, 0.1000], requires_grad=True), Parameter containing:
tensor([0.1000, 0.1000], requires_grad=True)]
tensor([-0.0013, -0.0098])
&lt;/denchmark-code&gt;

Note:
One thing I am not that confident is that the randomness generated from torch.distributions.Uniform will somehow break the gradcheck. Still looking for a workaround. Please advise if you got some ideas.
		</comment>
		<comment id='4' author='likitha-l' date='2020-09-11T16:44:00Z'>
		Thank you &lt;denchmark-link:https://github.com/shijianjian&gt;@shijianjian&lt;/denchmark-link&gt;
.
I updated the &lt;denchmark-link:https://colab.research.google.com/drive/1MfW2zwRfRR5_Rarex9ZhXONEt2U23QYI?usp=sharing&gt;colab&lt;/denchmark-link&gt;
 and indeed now the gradients take values. In my case though I get gradients either  or . Sometimes I get lucky and with different initial input params, I am able to get good gradients but is rare.
From the test, I did yesterday even with the stable version kornia-4.0 I can get the gradients if I structure the code as you did.
Regarding torch.distributions.Uniform, my understanding is that random sampling is not natively differentiable but according to [1] pytorch implements surrogate functions to do it either 1) reinforce method 2) pathwise derivatives, similar to the re-parametrization trick in VAE.  So, from what I understood that should not be the cause of gradient breaking.
[1] &lt;denchmark-link:https://pytorch.org/docs/stable/distributions.html#module-torch.distributions&gt;https://pytorch.org/docs/stable/distributions.html#module-torch.distributions&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='likitha-l' date='2020-09-13T08:05:25Z'>
		&lt;denchmark-link:https://github.com/iacopomasi&gt;@iacopomasi&lt;/denchmark-link&gt;
 The previous version might re-initialize the trained parameters. A bit dangerous I would say.
In my understanding, it should be straightforward if we use the provided reparametrized sampling  function like stated in &lt;denchmark-link:https://pytorch.org/docs/stable/distributions.html#pathwise-derivative&gt;here&lt;/denchmark-link&gt;
. While it not pretty working regarding the proper tracable backpropergation there.
import torch.nn as nn
from torch.autograd import gradcheck

class MyAug(nn.Module):
    def __init__(self) -&gt; None:
        super(MyAug, self).__init__()
        self.p1 = nn.Parameter(torch.tensor([0.01, 0.2],requires_grad=True, dtype=torch.float64))
        self.unif = torch.distributions.Uniform(self.p1[0], self.p1[1])
    def forward(self, input):
        p = self.unif.rsample((1,))
        return input * p

torch.manual_seed(1)
ip = torch.randn(1, requires_grad=True).to(torch.float64)
gradcheck(MyAug(), (ip,))
&lt;denchmark-code&gt;RuntimeError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor([[22444.6191]])
analytical:tensor([[0.0156]])
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='likitha-l' date='2020-10-06T08:11:46Z'>
		&lt;denchmark-link:https://github.com/iacopomasi&gt;@iacopomasi&lt;/denchmark-link&gt;
 To add, I think the  and  comes from the param settings. If you use  for brightness adjustment, it will give you a very black image. I have tested several runs with below code (just higher brightness, contrast and saturations), it shall be fine:
import torch
import torch.nn as nn
import kornia.augmentation as K

jit = K.ColorJitter(
    nn.Parameter(torch.tensor([0.7, 0.7],requires_grad=True)),
    nn.Parameter(torch.tensor([0.7, 0.7],requires_grad=True)),
    nn.Parameter(torch.tensor([0.7, 0.7],requires_grad=True)),
    nn.Parameter(torch.tensor([0.1, 0.1],requires_grad=True)),
    same_on_batch=True,
    return_transform=False
).to(torch.float32)

out = jit(torch.ones(1, 3, 6, 6, dtype=torch.float32))
loss = nn.L1Loss()(out, torch.randn_like(out, dtype=torch.float32) * 100)
optimizer = torch.optim.SGD(jit.parameters(), lr=10)

loss.backward()
optimizer.step()


print([f for f in jit.parameters()])
print(jit.brightness.grad)
		</comment>
		<comment id='7' author='likitha-l' date='2020-10-19T13:24:19Z'>
		&lt;denchmark-link:https://github.com/likitha-l&gt;@likitha-l&lt;/denchmark-link&gt;
 can you check what &lt;denchmark-link:https://github.com/shijianjian&gt;@shijianjian&lt;/denchmark-link&gt;
 proposes ? we tested and should work now. Otherwise, this issue will be closed.
		</comment>
		<comment id='8' author='likitha-l' date='2020-12-18T13:38:35Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions, and happy coding day ðŸ˜Ž
		</comment>
		<comment id='9' author='likitha-l' date='2020-12-21T13:16:18Z'>
		&lt;denchmark-link:https://github.com/likitha-l&gt;@likitha-l&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/shijianjian&gt;@shijianjian&lt;/denchmark-link&gt;
 as far as I remember this was fixed, right ?
		</comment>
		<comment id='10' author='likitha-l' date='2020-12-21T16:04:43Z'>
		Yes. It should have been fixed.
		</comment>
	</comments>
</bug>