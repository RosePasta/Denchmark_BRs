<bug id='503' author='edgarriba' open_date='2020-04-01T09:56:53Z' closed_time='2020-10-15T09:01:58Z'>
	<summary>[Bug] Hardnet do not pass gradcheck</summary>
	<description>
Recently introduced hardnet descriptor (&lt;denchmark-link:https://github.com/kornia/kornia/pull/498&gt;#498&lt;/denchmark-link&gt;
) is not able to pass  tests.
The test is disabled right now and might need some investigation.
Link to the failing tests: 


kornia/test/feature/test_hardnet.py


        Lines 26 to 32
      in
      ee668e3






 @pytest.mark.skip("jacobian not well computed") 



 def test_gradcheck(self, device): 



 patches = torch.rand(2, 1, 32, 32, device=device) 



 patches = utils.tensor_to_gradcheck_var(patches)  # to var 



 hardnet = HardNet().to(patches.device, patches.dtype) 



 assert gradcheck(hardnet, (patches,), eps=1e-4, atol=1e-4, 



 raise_exception=True, ) 





Raised error: &lt;denchmark-link:https://pastebin.com/pKVezb1d&gt;https://pastebin.com/pKVezb1d&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='edgarriba' date='2020-04-01T09:57:45Z'>
		/cc &lt;denchmark-link:https://github.com/DagnyT&gt;@DagnyT&lt;/denchmark-link&gt;
 you might want to check this
		</comment>
		<comment id='2' author='edgarriba' date='2020-04-01T17:17:43Z'>
		I've noticed several things:

doesn't work with dropout.
torch.backends.cudnn.deterministic = True is needed here
the problem stays with _normalize_input method, I assume it might violate this "If any checked tensor in input has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from torch.expand()), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.", but not sure.
Otherwise, without dropout and normalization outside the model, it passes test with gradcheck default params.

		</comment>
		<comment id='3' author='edgarriba' date='2020-04-01T19:01:59Z'>
		&lt;denchmark-link:https://github.com/DagnyT&gt;@DagnyT&lt;/denchmark-link&gt;
 thanks for checking. This are my findings:
&lt;denchmark-link:https://user-images.githubusercontent.com/5157099/78175840-d1dd2b00-745b-11ea-8a63-fb9ccbf6acc0.png&gt;&lt;/denchmark-link&gt;

Now the question is why detach was needed and how do we test this without dropout ?  /cc &lt;denchmark-link:https://github.com/ducha-aiki&gt;@ducha-aiki&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='edgarriba' date='2020-04-01T19:30:44Z'>
		dropout - there might be a beautiful way, hacky way -
hardnet = HardNet()
hardnet.features[18].p=0
detach() is a problem; there was instability during training without it.
		</comment>
		<comment id='5' author='edgarriba' date='2020-04-01T20:00:43Z'>
		Great! Dropout solved.
Have you guys checked this instability with new pytorch versions?
		</comment>
		<comment id='6' author='edgarriba' date='2020-10-08T08:24:59Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions, and happy coding day ðŸ˜Ž
		</comment>
	</comments>
</bug>