<bug id='130' author='Msabih' open_date='2019-05-09T04:23:54Z' closed_time='2020-01-31T11:57:28Z'>
	<summary>[BUG] Using Multi GPU support</summary>
	<description>
While using tgm, I have come across one or two places in the code where the creation of new tensor has to be done on the device in the context otherwise the multi gpu training gives an error.
Should I give a pull request or perhaps you could think of better ways to solve this.
for example in torchgeometry/core/conversions.py in line 97-100, I changed the code as follows
&lt;denchmark-code&gt;device_points = points.device
scale: torch.Tensor = torch.tensor(1.) / torch.clamp(z_vec, eps)
scale = scale.to(device_points)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Msabih' date='2019-05-09T10:46:24Z'>
		&lt;denchmark-link:https://github.com/Msabih&gt;@Msabih&lt;/denchmark-link&gt;
 could you give us more information about your error, a screenshot of what you get, the devices you use, etc. I don't see why multi gpu should be a problem, since  is derived from  and this directly from .
		</comment>
		<comment id='2' author='Msabih' date='2019-05-09T14:05:18Z'>
		I was getting the typical error that these two tensors must be on the same devices. I will rebuild the library and get back to you with the error message. I am using multiple nvidia TX 1080 GPUs.
I didnt read the whole code, just got to the point of error, printed the device names of the tensors and scale was one device: 0 whereas I was using 6 and 7.
		</comment>
		<comment id='3' author='Msabih' date='2019-05-10T23:40:08Z'>
		
@Msabih could you give us more information about your error, a screenshot of what you get, the devices you use, etc. I don't see why multi gpu should be a problem, since scale is derived from z_vec and this directly from points.

&lt;denchmark-link:https://github.com/edgarriba&gt;@edgarriba&lt;/denchmark-link&gt;

/torchgeometry/core/conversions.py", line 98, in convert_points_from_homogeneous
return scale * points[..., :-1]
RuntimeError: binary_op(): expected both inputs to be on same device, but input a is on cuda:0 and in put b is on cuda:6
		</comment>
		<comment id='4' author='Msabih' date='2019-05-11T05:29:56Z'>
		It's interesting. How do you copy your tensors to cuda outside of the functions? Could you provide a small script so that we can reproduce the issue? Which torch version do you use? I'm fine with your fix, but we have been using this function in cuda multi gpu an no errors like this one. Would like to see why it happens first. /cc &lt;denchmark-link:https://github.com/bhack&gt;@bhack&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='Msabih' date='2019-05-12T02:41:59Z'>
		
It's interesting. How do you copy your tensors to cuda outside of the functions? Could you provide a small script so that we can reproduce the issue? Which torch version do you use? I'm fine with your fix, but we have been using this function in cuda multi gpu an no errors like this one. Would like to see why it happens first. /cc @bhack

So I am taking a list of devices and giving it to the DataParallel as argument. Later on images are taken to first GPU for training like images = images.to(self.devices[0])
		</comment>
		<comment id='6' author='Msabih' date='2019-05-12T06:48:52Z'>
		Ok I see. Last, just to fully understand, if you print points, zvec and scale devices what do you get?
		</comment>
		<comment id='7' author='Msabih' date='2019-05-28T11:07:14Z'>
		&lt;denchmark-link:https://github.com/Msabih&gt;@Msabih&lt;/denchmark-link&gt;
 can you provide a unit test script where this can be reproduced ?
		</comment>
		<comment id='8' author='Msabih' date='2019-05-28T11:32:16Z'>
		
@Msabih can you provide a unit test script where this can be reproduced ?

Okay, I will do it in a day as I find time. Just to note, I think this problem will not occur if multiple GPUs were used using DistributedDataParallel rather it would occur only using DataParallel. Because I have switched to using DDP even on single machine with multiple GPUs as it distributes the load equally and is consistent. I did'nt try the perspective transform in that setting but I am pretty sure that this error will not come over there.
However to reproduce this issue on DataParallel, I will make a small test script.
		</comment>
		<comment id='9' author='Msabih' date='2019-05-28T11:36:42Z'>
		awesome. So DDP seems the way to go in general right ?
		</comment>
		<comment id='10' author='Msabih' date='2019-05-28T11:57:41Z'>
		
awesome. So DDP seems the way to go in general right ?

Yes in general that is the case. But users may use DataParallel just because its easier or there could be a genuine use case. One use case is that Distributed Data Parallel takes more memory( its multiplied by number of nodes) if your Data Loader loads all the dataset at once however Data Parallel uses same amount of memory as single process use case. Another usecase might be if the cluster has free GPUs but is limited in terms of CPU Cores.
You might want to read this use case.
&lt;denchmark-link:https://github.com/NVIDIA/apex/issues/269&gt;NVIDIA/apex#269&lt;/denchmark-link&gt;

For this library, pushing scale to the same device would work for all cases, Single, DataParallel and Distributed Data Parallel.
		</comment>
		<comment id='11' author='Msabih' date='2019-06-13T15:04:55Z'>
		&lt;denchmark-link:https://github.com/Msabih&gt;@Msabih&lt;/denchmark-link&gt;
 could you contribute with this patch and a quick test ?
		</comment>
		<comment id='12' author='Msabih' date='2019-09-29T19:01:27Z'>
		&lt;denchmark-link:https://github.com/Msabih&gt;@Msabih&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/bhack&gt;@bhack&lt;/denchmark-link&gt;
 was this solved ? Otherwise I'm closing it.
		</comment>
		<comment id='13' author='Msabih' date='2020-01-31T11:57:28Z'>
		I think this was already solved. Please, reopen in case anyone experience same issue
		</comment>
	</comments>
</bug>