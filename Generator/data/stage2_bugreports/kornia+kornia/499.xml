<bug id='499' author='Eizenborg' open_date='2020-03-27T18:03:24Z' closed_time='2020-10-21T08:36:49Z'>
	<summary>[Bug] tiny bug on _kl_div_2d causes runtime error</summary>
	<description>
Hi all,
Not sure why this happens, but when using js_div_loss_2d (which is awesome, thanks!) I get this Runtime Error:
line 12, in _kl_div_2d q.view(batch * chans, height * width).log(), RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.
which is solved by changing
q.view(batch * chans, height * width).log(), p.view(batch * chans, height * width),
to
q.reshape(batch * chans, height * width).log(), p.reshape(batch * chans, height * width),
on divergence.py, _kl_div_2d, line 12 and 13
	</description>
	<comments>
		<comment id='1' author='Eizenborg' date='2020-04-01T13:53:37Z'>
		&lt;denchmark-link:https://github.com/Eizenborg&gt;@Eizenborg&lt;/denchmark-link&gt;
 is it happening calling  before kornia ?
		</comment>
		<comment id='2' author='Eizenborg' date='2020-10-08T08:25:00Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions, and happy coding day ðŸ˜Ž
		</comment>
		<comment id='3' author='Eizenborg' date='2020-10-19T11:15:56Z'>
		&lt;denchmark-link:https://github.com/anibali&gt;@anibali&lt;/denchmark-link&gt;
 can you check this ?
		</comment>
		<comment id='4' author='Eizenborg' date='2020-10-19T22:54:33Z'>
		&lt;denchmark-link:https://github.com/edgarriba&gt;@edgarriba&lt;/denchmark-link&gt;
 I just wrote a quick test and it does appear that  fails on non-contiguous inputs. What's your policy on this? Are users expected to make the inputs contiguous themselves when required?
    @pytest.mark.parametrize('input,target,expected', [
        (torch.full((1, 1, 2, 4), 0.125), torch.full((1, 1, 2, 4), 0.125), 0.0),
        (torch.full((1, 7, 2, 4), 0.125), torch.full((1, 7, 2, 4), 0.125), 0.0),
        (torch.full((1, 7, 2, 4), 0.125), torch.zeros((1, 7, 2, 4)), 0.346574),
        (torch.zeros((1, 7, 2, 4)), torch.full((1, 7, 2, 4), 0.125), 0.346574),
    ])
    def test_noncontiguous_js(self, device, input, target, expected):
        input = input.to(device).view(input.shape[::-1]).T
        target = target.to(device).view(target.shape[::-1]).T
        # Uncomment the next two lines to make the test pass:
        # input = input.contiguous()
        # target = target.contiguous()
        actual = kornia.losses.js_div_loss_2d(input, target).item()
        assert_allclose(actual, expected)
In my opinion it's probably worthwhile just replacing the calls to view with reshape in _kl_div_2d like OP suggested.
		</comment>
	</comments>
</bug>