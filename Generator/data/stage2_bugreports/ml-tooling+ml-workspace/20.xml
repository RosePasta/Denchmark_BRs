<bug id='20' author='andr0idsensei' open_date='2019-12-17T10:21:45Z' closed_time='2020-01-29T19:18:16Z'>
	<summary>Jupyter Notebook and Jupyter Lab seems to use only one CPU in multi-cpu configs</summary>
	<description>
Describe the bug:
We have a setup where we have some cloud machines with one gpu and multiple cpus, which we use for training some models. In such a context, we've tried running our usual notebooks from the ml-workspace docker image with Jupyter Notebook as well as Jupyter Lab, however we've noticed that during training or multi-process operations, only one CPU on the machine is being used. We've noticed this in netstat as well as using htop.
Expected behaviour:
When running jupyter notebooks that use multiprocess operations, all cpus on the host machine should be used.
Steps to reproduce the issue:
Here is a piece of code we tried to run specifically to test this behaviour:
def f(x):
while True:
x*x
pool = multiprocessing.Pool(3)
pool.map(f,range(100))
Technical details:

Workspace version : 0.8.7
Docker version : 18.09.7
Host Machine OS (Windows/Linux/Mac): Ubuntu 18.04 LTS
Command used to start the workspace : docker run -d -p 8081:8080 --name "ml-workspace" -v ${WORKSPACE_DIR}:/workspace -v /data/:/data --runtime nvidia --env NVIDIA_VISIBLE_DEVICES="all" --env AUTHENTICATE_VIA_JUPYTER="token" --shm-size 512m --restart always $IMAGE
Browser (Chrome/Firefox/Safari):

Possible Fix:
Our suspicion is that maybe there are some restrictions in the setup/run config for Jupyter Notebook.
Additional context:
	</description>
	<comments>
		<comment id='1' author='andr0idsensei' date='2020-01-29T09:30:19Z'>
		&lt;denchmark-link:https://github.com/andr0idsensei&gt;@andr0idsensei&lt;/denchmark-link&gt;
 I just did some evaluation on different machines with your code, but was not able to replicate the issue (atleast with the current build version of workspace: ).
For example, here I tried your code within JupyterLab with two different Pool sizes and both seem to be able to use the available CPUs.
&lt;denchmark-link:https://user-images.githubusercontent.com/2852129/73343849-910d3e00-4281-11ea-88b8-b39b00e42218.png&gt;&lt;/denchmark-link&gt;

Have you checked if other Docker containers have access to the full count of CPUs (maybe the Docker daemon has some limiting configuration)? Have you used a CPU limit on the workspace container?
		</comment>
		<comment id='2' author='andr0idsensei' date='2020-01-29T19:18:16Z'>
		Closing this issue for now since it does not seem to be happening with the latest workspace version. Feel free to reopen the issue if you can still replicate it with the latest version.
		</comment>
		<comment id='3' author='andr0idsensei' date='2020-02-07T08:48:24Z'>
		Thanks Lukas. I haven't checked with the latest version, but I found that it was caused by setting thread affinity via the KMP_AFFINITY env variable. For me it worked if I set the value to empty.
		</comment>
		<comment id='4' author='andr0idsensei' date='2020-02-07T12:55:28Z'>
		&lt;denchmark-link:https://github.com/andr0idsensei&gt;@andr0idsensei&lt;/denchmark-link&gt;
 Thanks, thats very valuable feedback. The  settings where adapted from the recommendations of Intel: &lt;denchmark-link:https://software.intel.com/en-us/articles/maximize-tensorflow-performance-on-cpu-considerations-and-recommendations-for-inference&gt;https://software.intel.com/en-us/articles/maximize-tensorflow-performance-on-cpu-considerations-and-recommendations-for-inference&lt;/denchmark-link&gt;

Unfortunately, I don't have a deep understanding of how KMP_AFFINITY works, but it seems like that for some hardware setups this setting does not actually optimize the performance.
		</comment>
		<comment id='5' author='andr0idsensei' date='2020-02-07T13:19:59Z'>
		I'm a bit confused that the KMP_AFFINITY actually has an effect on the python multiprocessing module. I don't think that the multiprocessing module actually uses any OpenMP code.
		</comment>
		<comment id='6' author='andr0idsensei' date='2020-02-08T18:00:26Z'>
		Actually, I think that we might have messed up the initial multiprocess test somehow, and multiprocessing may not be affected by that (we haven't re-tested that though). The fact that you mentioned for you the bug was not reproducible, made me think a bit more about this. What we actually noticed was that when we used FastAI and Pytorch to train some models, that we usually trained on a multi-cpu and multi-gpu rig, using ml-workspace training was slower and not all CPU's were used. We tried to replicate that with the Pool example in the initial bug report, and at first it seemed to reproduce. However I think, I tried it once before finding out about KMP_AFFINITY, and I couldn't reproduce it. Since I don't think FastAI and Pytorch use multiprocessing (also haven't verified that) it would make sense that they were affected by KMP_AFFINITY.
		</comment>
	</comments>
</bug>