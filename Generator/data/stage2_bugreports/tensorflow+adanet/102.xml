<bug id='102' author='MajesticKhan' open_date='2019-05-04T19:38:48Z' closed_time='2019-05-14T05:10:15Z'>
	<summary>Deserializing Architecture json file</summary>
	<description>
NOTE: I am running Adanet on windows 10 using python 3.5.2
Has anybody ran into this error: TypeError: the JSON object must be str, not 'bytes'?
I am currently debugging the code and this is what I have so far:
gfile.read() outputs  bytes instead of string. This causes an error when _Architecture.deserialize() uses json.loads() which requires a string object.



adanet/adanet/core/estimator.py


        Lines 1145 to 1146
      in
      cb9c718






 with tf.io.gfile.GFile(filename, "rb") as gfile: 



 return _Architecture.deserialize(gfile.read()) 








adanet/adanet/core/architecture.py


        Lines 103 to 119
      in
      cb9c718






 def deserialize(serialized_architecture): 



 """Deserializes a serialized architecture. 



  



     Args: 



       serialized_architecture: String representation of an `_Architecture` 



         obtained by calling `serialize`. 



  



     Returns: 



       A deserialized `_Architecture` instance. 



     """ 



 



 ensemble_arch = json.loads(serialized_architecture) 



 architecture = _Architecture(ensemble_arch["ensemble_candidate_name"]) 



 for subnet in ensemble_arch["subnetworks"]: 



 architecture.add_subnetwork(subnet["iteration_number"], 



 subnet["builder_name"]) 



 return architecture 





	</description>
	<comments>
		<comment id='1' author='MajesticKhan' date='2019-05-04T21:22:25Z'>
		&lt;denchmark-link:https://github.com/MajesticKhan&gt;@MajesticKhan&lt;/denchmark-link&gt;
: We only currently test on Linux machine with CI. If you find a fix, feel free to send a PR and I'll review it.
		</comment>
		<comment id='2' author='MajesticKhan' date='2019-05-04T22:59:06Z'>
		Got it.
Let me dig a little deeper into the issue and start testing out other examples to check if the issue is on my side, if not I will create a PR with the fix. I'll keep you updated through gitter
		</comment>
		<comment id='3' author='MajesticKhan' date='2019-05-07T11:22:33Z'>
		I'm trying to train on using cloud ML engine and I'm getting the same error, I'm not sure it is windows related
		</comment>
		<comment id='4' author='MajesticKhan' date='2019-05-07T14:02:00Z'>
		Okay, the serializing issue is a simple fix, gfile.read().decode(). However, there is another issue when temp_model_dir folder is still being used by an initial model preventing files from being deleted. I hacked my way around this by creating mutiple temp_model_dir folders and not deleting them. After that, it was smooth sailing.
Currently, I'm creating a simple script to test both on windows and linux. After that, I'll figure out how to deal with the folder issue and submit a pr with the fixes.
		</comment>
		<comment id='5' author='MajesticKhan' date='2019-05-08T05:27:26Z'>
		Sorry accidentally clicked on closed.
&lt;denchmark-link:https://github.com/WillianPaiva&gt;@WillianPaiva&lt;/denchmark-link&gt;
 What version of numpy are you using? When I reinstalled Adanet, it reverted back to the previous numpy to 1.15 which prevented me from running a simple script that trains a dnn model. Going back to numpy 1.16.3 allowed me to run the script again
		</comment>
		<comment id='6' author='MajesticKhan' date='2019-05-08T15:52:20Z'>
		
When I reinstalled Adanet, it reverted back to the previous numpy to 1.15

&lt;denchmark-link:https://github.com/MajesticKhan&gt;@MajesticKhan&lt;/denchmark-link&gt;
: I think our dependency requirements were hard-coded to be too strict, so I'll create a commit to loosen those requirements. Did upgrading to numpy 1.16.3 fix any of the other issues you were experiencing?
&lt;denchmark-link:https://github.com/WillianPaiva&gt;@WillianPaiva&lt;/denchmark-link&gt;
: Are you seeing this exact error on Cloud MLE? Are you running as a single process or  distributed training?
		</comment>
		<comment id='7' author='MajesticKhan' date='2019-05-09T04:34:08Z'>
		&lt;denchmark-link:https://github.com/cweill&gt;@cweill&lt;/denchmark-link&gt;
, I reinstalled Adanet and it defaulted to installing numpy 1.15.4 which still caused errors. I would consider strictly upgrading to match the numpy version that Tensorflow 1.13 is using which is numpy-1.16.3
		</comment>
		<comment id='8' author='MajesticKhan' date='2019-05-09T05:38:38Z'>
		&lt;denchmark-link:https://github.com/cweill&gt;@cweill&lt;/denchmark-link&gt;
 This is the script that I will be using to test both on Linux and Windows 10. I copied it from my repository for easier viewing. I will move on to implementing my fixes to Adanet.
#----------------------------------------------------------------Import Libraries
import tensorflow as tf
import pandas as pd
import adanet
tf.logging.set_verbosity("INFO")
#----------------------------------------------------------------


#----------------------------------------------------------------State meta data
Features  = ['SepalLength', 'SepalWidth',
             'PetalLength', 'PetalWidth']
TRAIN_URL = "http://download.tensorflow.org/data/iris_training.csv"
TEST_URL  = "http://download.tensorflow.org/data/iris_test.csv"
#----------------------------------------------------------------


#----------------------------------------------------------------Create feature numeric columns
feature_columns = []
for key in Features:
    feature_columns.append(tf.feature_column.numeric_column(key=key))
#----------------------------------------------------------------


#----------------------------------------------------------------Define dataset input function
def input_fn(file_path, batch_size, epoch, y_name='Species'):

    temp     = pd.read_csv(file_path, names = Features + [y_name], header = 0)
    labels   = temp.pop(y_name)
    features = temp

    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))

    # Shuffle, repeat, and batch the examples.
    dataset = dataset.shuffle(1000).repeat(epoch).batch(batch_size)

    # Return the dataset.
    return dataset
#----------------------------------------------------------------


#----------------------------------------------------------------Create the estimator
config = tf.estimator.RunConfig(
  model_dir              = "Path to Directory",
)

# Define the model head for computing loss and evaluation metrics.
head = tf.contrib.estimator.multi_class_head(n_classes=3)

# Learn to ensemble linear and neural network models.
estimator = adanet.AutoEnsembleEstimator(
    head = head,
    candidate_pool = {
        "linear":
            tf.estimator.LinearEstimator(
                head=head,
                feature_columns=feature_columns),
        "dnn":
            tf.estimator.DNNEstimator(
                head=head,
                feature_columns=feature_columns,
                hidden_units=[10,10])},
    max_iteration_steps=50)
#----------------------------------------------------------------


#----------------------------------------------------------------Execute
# Train the Model.
estimator.train(
    input_fn = lambda: input_fn(TRAIN_URL,10, epoch = None),
    max_steps = 100)

# Evaluate the model.
eval_result = estimator.evaluate(
    input_fn = lambda: input_fn(TEST_URL,10, epoch = 1))
#----------------------------------------------------------------


#----------------------------------------------------------------Sources
# https://github.com/tensorflow/models/tree/master/samples/core/get_started
#----------------------------------------------------------------
		</comment>
		<comment id='9' author='MajesticKhan' date='2019-05-11T01:00:06Z'>
		&lt;denchmark-link:https://github.com/MajesticKhan&gt;@MajesticKhan&lt;/denchmark-link&gt;
 That script looks good to me. Keep us posted. And feel free to send a PR with a fix for reading the architecture JSON file.
		</comment>
		<comment id='10' author='MajesticKhan' date='2019-05-11T03:53:06Z'>
		&lt;denchmark-link:https://github.com/cweill&gt;@cweill&lt;/denchmark-link&gt;
 Working on that pr now...
Two things:
1.) I confirmed that the json issue is affected on linux as well...need to test it on Linux
2.) The numpy version should be 1.16.3, I get the error below when I installed Adanet on a VM from GCE
ImportError: Something is wrong with the numpy installation. While importing we detected an older version of numpy in ['/home/usr/.local/lib/python3.5/site-packages/numpy']. One method of fixing this is to repeatedly uninstall numpy until none is found, then reinstall this version.
		</comment>
		<comment id='11' author='MajesticKhan' date='2019-05-11T04:32:21Z'>
		&lt;denchmark-link:https://github.com/cweill&gt;@cweill&lt;/denchmark-link&gt;
 Submitted the PR...there is an additional issue which is related to the temp_dir_folder, however I will open up a separate issue on that and close this one once the changes are confirmed for deserializing the architecture file.
		</comment>
		<comment id='12' author='MajesticKhan' date='2019-06-05T14:59:20Z'>
		&lt;denchmark-link:https://github.com/MajesticKhan&gt;@MajesticKhan&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/cweill&gt;@cweill&lt;/denchmark-link&gt;

Note: I was running the following tutorial on Ubuntu 16.04, with Tensorflow 1.13, Python 3.5 and numpy 1.16.4.
&lt;denchmark-link:url&gt;https://github.com/tensorflow/adanet/blob/master/adanet/examples/tutorials/customizing_adanet.ipynb&lt;/denchmark-link&gt;

I met the same problem (see the attached file for detailed error log)
The specific code snippet I tried to run is as follows,
&lt;denchmark-code&gt;#@test {"skip": true}
#@title Parameters
LEARNING_RATE = 0.003  #@param {type:"number"}
TRAIN_STEPS = 5000  #@param {type:"integer"}
BATCH_SIZE = 64  #@param {type:"integer"}
ADANET_ITERATIONS = 2  #@param {type:"integer"}

estimator = adanet.Estimator(
    head=head,
    subnetwork_generator=simple_dnn.Generator(
        feature_columns=feature_columns,
        optimizer=tf.train.RMSPropOptimizer(learning_rate=LEARNING_RATE),
        seed=RANDOM_SEED),
    max_iteration_steps=TRAIN_STEPS // ADANET_ITERATIONS,
    evaluator=adanet.Evaluator(
        input_fn=input_fn("train", training=False, batch_size=BATCH_SIZE),
        steps=None),
    config=make_config("simple_dnn"))

results, _ = tf.estimator.train_and_evaluate(
    estimator,
    train_spec=tf.estimator.TrainSpec(
        input_fn=input_fn("train", training=True, batch_size=BATCH_SIZE),
        max_steps=TRAIN_STEPS),
    eval_spec=tf.estimator.EvalSpec(
        input_fn=input_fn("test", training=False, batch_size=BATCH_SIZE),
        steps=None,
        start_delay_secs=1,
        throttle_secs=1,  
    ))
print("Accuracy:", results["accuracy"])
print("Loss:", results["average_loss"])
&lt;/denchmark-code&gt;

It seems to me that updating the numpy version to 1.16.4 cannot work well. I wonder do you have any idea how to deal with this problem?
Detailed Error Log: &lt;denchmark-link:https://github.com/tensorflow/adanet/files/3257720/error.log&gt;error.log&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='MajesticKhan' date='2019-06-05T16:29:00Z'>
		&lt;denchmark-link:https://github.com/LakeCarrot&gt;@LakeCarrot&lt;/denchmark-link&gt;
 ,
Running the latest version of numpy is not the issue. The error that you are seeing is related to ensemble_arch = json.loads(serialized_architecture), please see the first post for further details.
Essentially, the serialized_architecture is in byte format when it should be in string format. However, this implies that you are running python 3.5 or lower as reading the documentation for python 3.6+ shows that json.loads() can take in bytes as inputs.
See the links below:
-&lt;denchmark-link:https://docs.python.org/3.5/library/json.html&gt;json.loads()&lt;/denchmark-link&gt;
 on python 3.5
-&lt;denchmark-link:https://docs.python.org/3.6/library/json.html&gt;json.loads()&lt;/denchmark-link&gt;
 on python 3.6
Credit goes to &lt;denchmark-link:https://stackoverflow.com/questions/42683478/typeerror-the-json-object-must-be-str-not-bytes/42683509&gt;Ikar Pohorsk√Ω&lt;/denchmark-link&gt;
 for spotting the &lt;denchmark-link:https://docs.python.org/3/whatsnew/3.6.html#json&gt;updates to json.loads()&lt;/denchmark-link&gt;
 in python 3.6
I believe &lt;denchmark-link:https://github.com/cweill&gt;@cweill&lt;/denchmark-link&gt;
 will release the latest version with the included update that fixes the serialization issue.
		</comment>
		<comment id='14' author='MajesticKhan' date='2019-06-05T18:25:00Z'>
		&lt;denchmark-link:https://github.com/MajesticKhan&gt;@MajesticKhan&lt;/denchmark-link&gt;
 ,
Thanks a lot for your replay. Yeah. after updating python from 3.5 to 3.6, problem solved. The problem is because of json.loads() function.
		</comment>
	</comments>
</bug>