<bug id='43' author='sdwldchl' open_date='2018-12-06T12:41:17Z' closed_time='2018-12-11T22:55:28Z'>
	<summary>error when running tutorial examples with adanet</summary>
	<description>
I tried to run tutorial examples with adanet, it ended with following error:
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py", line 389, in join
six.reraise(*self._exc_info_to_raise)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py", line 297, in stop_on_exception
yield
File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py", line 795, in run
self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py", line 1195, in _call_model_fn
model_fn_results = self._model_fn(features=features, **kwargs)
File "/usr/local/lib/python2.7/dist-packages/adanet/core/estimator.py", line 1109, in _adanet_model_fn
previous_ensemble_spec=previous_ensemble_spec)
File "/usr/local/lib/python2.7/dist-packages/adanet/core/iteration.py", line 256, in build_iteration
labels=labels)
File "/usr/local/lib/python2.7/dist-packages/adanet/core/ensemble.py", line 515, in append_new_subnetwork
previous_ensemble_spec=ensemble_spec)
File "/usr/local/lib/python2.7/dist-packages/adanet/core/ensemble.py", line 558, in _build_ensemble_spec
summary))
File "/usr/local/lib/python2.7/dist-packages/adanet/core/ensemble.py", line 875, in _adanet_weighted_ensemble_logits
weighted_subnetworks, bias, summary)
File "/usr/local/lib/python2.7/dist-packages/adanet/core/ensemble.py", line 911, in _adanet_weighted_ensemble_logits_helper
ensemble_complexity_regularization)
File "/usr/local/lib/python2.7/dist-packages/adanet/core/tpu_estimator.py", line 43, in _fn
return fn(*args, **kwargs)
File "/usr/local/lib/python2.7/dist-packages/adanet/core/summary.py", line 272, in scalar
collections=[self._TMP_COLLECTION_NAME])
File "/usr/local/lib/python2.7/dist-packages/adanet/core/tpu_estimator.py", line 43, in _fn
return fn(*args, **kwargs)
TypeError: scalar() got an unexpected keyword argument 'collections'
May I have your help? Thanks in advance!
	</description>
	<comments>
		<comment id='1' author='sdwldchl' date='2018-12-06T14:51:14Z'>
		Are you using the new TPUEstimator? Do you have a copy of the tutorial you can share that has this problem?
		</comment>
		<comment id='2' author='sdwldchl' date='2018-12-07T11:23:40Z'>
		In simple_dnn.py, I added two lines of codes to output the iteration_step:
&lt;denchmark-code&gt;def build_subnetwork_train_op(self, subnetwork, loss, var_list, labels,
                                                  iteration_step, summary, previous_ensemble):
"""See adanet.subnetwork.Builder."""
with tf.name_scope(""):       # output iteration_step
  summary.scalar("iteration_step", iteration_step)
#NOTE: The adanet.Estimator increments the global step.
update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
  return self._optimizer.minimize(loss=loss, var_list=var_list)
&lt;/denchmark-code&gt;

In the Customizing AdaNet example, I modified the input_fn function:
&lt;denchmark-code&gt;def input_fn(partition, training, batch_size):
"""Generate an input_fn for the Estimator."""
  def _input_fn():
    if partition == "train":
     dataset = tf.data.Dataset.from_generator(
     generator(x_train, y_train), (tf.float32, tf.int32), ((28, 28), ()))
   else:
    dataset = tf.data.Dataset.from_generator(
    generator(x_test, y_test), (tf.float32, tf.int32), ((28, 28), ()))

# We call repeat after shuffling, rather than before, to prevent separate
# epochs from blending together.
  if training:
    dataset = dataset.shuffle(10 * batch_size, seed=RANDOM_SEED).repeat()

  dataset = dataset.map(preprocess_image).batch(batch_size)
  return dataset.repeat()
return _input_fn
&lt;/denchmark-code&gt;

Also, argument train_distribution was explicitly specified in configuration for estimator:
&lt;denchmark-code&gt;devices = ["/device:GPU:0", "/device:GPU:1"]
distribution = tf.contrib.distribute.MirroredStrategy(devices=devices)
config = tf.estimator.RunConfig(
    save_checkpoints_steps=50000,
    save_summary_steps=50000,
    train_distribute=distribution,
    tf_random_seed=RANDOM_SEED)
&lt;/denchmark-code&gt;

No other changes were made. Thanks in advance.
		</comment>
		<comment id='3' author='sdwldchl' date='2018-12-08T01:43:54Z'>
		
Are you using the new TPUEstimator? Do you have a copy of the tutorial you can share that has this problem?

In simple_dnn.py, I added two lines of codes to output the iteration_step:
&lt;denchmark-code&gt;def build_subnetwork_train_op(self, subnetwork, loss, var_list, labels,
                                                  iteration_step, summary, previous_ensemble):
"""See adanet.subnetwork.Builder."""
with tf.name_scope(""):       # output iteration_step
  summary.scalar("iteration_step", iteration_step)
#NOTE: The adanet.Estimator increments the global step.
update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
  return self._optimizer.minimize(loss=loss, var_list=var_list)
&lt;/denchmark-code&gt;

In the Customizing AdaNet example, I modified the input_fn function:
&lt;denchmark-code&gt;def input_fn(partition, training, batch_size):
"""Generate an input_fn for the Estimator."""
  def _input_fn():
    if partition == "train":
     dataset = tf.data.Dataset.from_generator(
     generator(x_train, y_train), (tf.float32, tf.int32), ((28, 28), ()))
   else:
    dataset = tf.data.Dataset.from_generator(
    generator(x_test, y_test), (tf.float32, tf.int32), ((28, 28), ()))

# We call repeat after shuffling, rather than before, to prevent separate
# epochs from blending together.
  if training:
    dataset = dataset.shuffle(10 * batch_size, seed=RANDOM_SEED).repeat()

  dataset = dataset.map(preprocess_image).batch(batch_size)
  return dataset.repeat()
return _input_fn
&lt;/denchmark-code&gt;

Also, argument train_distribution was explicitly specified in configuration for estimator:
&lt;denchmark-code&gt;devices = ["/device:GPU:0", "/device:GPU:1"]
distribution = tf.contrib.distribute.MirroredStrategy(devices=devices)
config = tf.estimator.RunConfig(
    save_checkpoints_steps=50000,
    save_summary_steps=50000,
    train_distribute=distribution,
    tf_random_seed=RANDOM_SEED)
&lt;/denchmark-code&gt;

No other changes were made. Thanks in advance.
		</comment>
		<comment id='4' author='sdwldchl' date='2018-12-08T16:15:31Z'>
		I see you are using tf.contrib.distribute.MirroredStrategy(devices=devices). I wonder if that could be the issue. I'll have a look.
		</comment>
		<comment id='5' author='sdwldchl' date='2019-02-13T16:33:19Z'>
		All methods of Summary should add an argument , collections=None.
&lt;denchmark-link:https://github.com/tensorflow/adanet/blob/master/adanet/core/summary.py#L38&gt;https://github.com/tensorflow/adanet/blob/master/adanet/core/summary.py#L38&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>