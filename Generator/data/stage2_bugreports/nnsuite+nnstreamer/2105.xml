<bug id='2105' author='ajarthurs' open_date='2020-02-13T21:36:49Z' closed_time='2020-02-23T10:06:02Z'>
	<summary>[Feature request] Support the `int8` tensor-type in TensorFlow Lite models</summary>
	<description>
Is your feature request related to a problem? Please describe.
I cannot run TensorFlow Lite models that contain int8-type tensors via tensor_filter and its TensorFlow Lite framework; however, I have no issue running models containing uint8-type tensors. If the model's input tensor-type is int8, NNStreamer throws the following negotiation error, erroneously reporting the model's input tensor-type to be (null):
&lt;denchmark-code&gt;tensor_filter tensor_filter.c:557:gst_tensor_filter_configure_tensor:&lt;tensorfilter0&gt; The input tensor is not compatible.
0 : int8 [3:513:513:1] | (null) [3:513:513:1] FAILED
&lt;/denchmark-code&gt;

It's clear that NNStreamer does not yet support  tensor types in TensorFlow Lite models. For example, &lt;denchmark-link:https://github.com/nnsuite/nnstreamer/blob/e67091b777989bc0817ffb14d1e8d2cd51a43d15/ext/nnstreamer/tensor_filter/tensor_filter_tensorflow_lite.cc#L307:L333&gt;this function&lt;/denchmark-link&gt;
 does not map  to :
&lt;denchmark-code&gt;// ext/nnstreamer/tensor_filter/tensor_filter_tensorflow_lite.cc
tensor_type
TFLiteInterpreter::getTensorType (TfLiteType tfType)
{
  switch (tfType) {
    case kTfLiteFloat32:
      return _NNS_FLOAT32;
    case kTfLiteUInt8:
      return _NNS_UINT8;
    case kTfLiteInt32:
      return _NNS_INT32;
    case kTfLiteBool:
    /** @todo insert `case kTfLiteInt8:` here */
      return _NNS_INT8;
    case kTfLiteInt64:
      return _NNS_INT64;
    case kTfLiteString:
    default:
      /** @todo Support other types */
      break;
  }

  return _NNS_END;
}
&lt;/denchmark-code&gt;

Describe the solution you'd like
Facilitate TensorFlow Lite models containing int8-type tensors: I already tried patching the mentioned getTensorType function. It resolved the said negotiation error but hangs during playback, so there is more patchwork required.
Describe alternatives you've considered
Strictly uint8-based quantized models, mostly available from TensorFlow's quantized stock models produced from quantization-aware training.

According to &lt;denchmark-link:https://www.tensorflow.org/lite/performance/quantization_spec&gt;TensorFlow Lite's quantizaton specification&lt;/denchmark-link&gt;
, signed 8-bit integers () are preferred over unsigned integers (). Their post-training quantization tool, for example, invariably produces -based models. Although their tool allows the user to specify other I/O types, including , it inserts  operators at the I/O and converts internal tensor-types to .
	</description>
	<comments>
		<comment id='1' author='ajarthurs' date='2020-02-14T01:07:50Z'>
		When we have first developed tensorflow-lite subplugin, we were based on Tensorflow 1.9, which didn't have int8 ( &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/lite/context.h&gt;https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/lite/context.h&lt;/denchmark-link&gt;
 ); thus we allocated nns-int8 for bool (we don't have bool-type in other/tensor type).
However, because modern tensorflow-lite (1.13~1.15) has more types including int8, we need to adjust this.
I'd suggest to add #if/#endif clauses with TensorFlow-Lite version conditions and add new types (and drop bool if it's &gt;= 1.13. we may keep bool if it's &gt;= 1.9 &amp;&amp; &lt;= 1.12).
1.13:
&lt;denchmark-code&gt;typedef enum {
  kTfLiteNoType = 0,
  kTfLiteFloat32 = 1,
  kTfLiteInt32 = 2,
  kTfLiteUInt8 = 3,
  kTfLiteInt64 = 4,
  kTfLiteString = 5,
  kTfLiteBool = 6,
  kTfLiteInt16 = 7,
  kTfLiteComplex64 = 8,
  kTfLiteInt8 = 9,
} TfLiteType;
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='ajarthurs' date='2020-02-14T05:24:51Z'>
		Or... alternatively.. we may add nnstreamer.ini entry:
For example,
&lt;denchmark-code&gt;[tensorflow-lite]
support_bool_with_int8 = false
support_int8_with_int8 = true
# if support_bool_with_int8 == support_int8_with_int8 == true, it's an error and both will be turned off.
&lt;/denchmark-code&gt;

Potential issue with this approach: tf-lite 1.9 will break if support_int8_with_int8 is true. And there is no version-check APIs in older tf-lite.
		</comment>
		<comment id='3' author='ajarthurs' date='2020-02-14T16:15:11Z'>
		&lt;denchmark-link:https://github.com/myungjoo&gt;@myungjoo&lt;/denchmark-link&gt;
 I'm not sure support for -type tensors should be dropped for TF Lite versions 1.13 and up since they are specified in the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/lite/schema/schema.fbs#L31:L43&gt;TF Lite V3 flatbuffers schema&lt;/denchmark-link&gt;
. I noticed the lack of  (or similar) or otherwise  interprets -type tensors as  and not visa-versa. Are you planning on supporting  later after adjusting for ?
		</comment>
		<comment id='4' author='ajarthurs' date='2020-02-17T02:34:26Z'>
		We will be able to support "_NNS_BOOL" if we can define a common data layout across different frameworks. In other words we will need some info on multiple frameworks (including tf/tflite) how they define "BOOL" types and how they pack tensors with "BOOL" types.
In the case of tflite/bool, although I don't remember exactly how/why we defined it, we made "BOOL" equivalent with int8 because it is packed as 8-bit data. If there are other "popular" frameworks that do 1-bit packaging for "BOOL" type, for the size efficiency, we won't be able to simply define "_NNS_BOOL" and let it behave as if it is INT8/UINT8.
As I already have a few clients giving us requirements (none of my clients have yet requested bool) and products with immediate deadlines, we will be progressing slower than before., We do welcome contribution from the public including you :)
		</comment>
		<comment id='5' author='ajarthurs' date='2020-02-17T16:54:19Z'>
		&lt;denchmark-link:https://github.com/myungjoo&gt;@myungjoo&lt;/denchmark-link&gt;
 I also haven't ran across  tensors, at least not for quantized models, so I could take or leave  for sake of testing. But, -type tensors seem unrelated to this issue.
Concerning the "hang" mentioned in my OP when running a quantized TF Lite model with an uint8 I/O wrapper (internal tensors are int8), that turned out to be extremely slow inference; the model eventually finished with some detections. So, the only problem that concerns NNS is allowing for TF Lite models with int8-type I/O; otherwise, I would need to add I/O wrappers to my int8 models.
Next task for me is to investigate the large difference in performance with int8 vs uint8 models, which is a TensorFlow issue.
		</comment>
	</comments>
</bug>