<bug id='921' author='terrytangyuan' open_date='2019-07-16T17:23:25Z' closed_time='2019-08-26T13:15:24Z'>
	<summary>Worker failed but master shows succeeded and completed</summary>
	<description>
Pods:
&lt;denchmark-code&gt;elasticdl-test-mnist-2-master                                     0/1       Completed            0          6m
elasticdl-test-mnist-2-worker-0                                   0/1       Error                0          5m
elasticdl-test-mnist-2-worker-1                                   0/1       Completed            0          5m
&lt;/denchmark-code&gt;

Log from master:
&lt;denchmark-code&gt;I0716 17:17:55.030447 139633362966272 task_dispatcher.py:169] Task:302 completed, 1 remaining tasks
I0716 17:17:55.235365 139628514375424 task_dispatcher.py:169] Task:300 completed, 0 remaining tasks
I0716 17:17:56.480732 139633379751680 k8s_worker_manager.py:133] Got event MODIFIED, phase Succeeded for pod: elasticdl-test-mnist-2-worker-1
I0716 17:17:56.493682 139633379751680 k8s_worker_manager.py:133] Got event MODIFIED, phase Succeeded for pod: elasticdl-test-mnist-2-worker-1
I0716 17:17:59.171561 139651482433344 main.py:242] Stopping RPC server
I0716 17:17:59.174692 139633379751680 k8s_worker_manager.py:133] Got event MODIFIED, phase Running for pod: elasticdl-test-mnist-2-master
I0716 17:17:59.175117 139651482433344 main.py:248] All tasks finished. Keeping TensorBoard service running...
I0716 17:17:59.175363 139651482433344 main.py:251] Master stopped
&lt;/denchmark-code&gt;

Log from worker:
&lt;denchmark-code&gt;I0716 17:17:52.699357 140504608769856 worker.py:220] Loss is 0.162214
I0716 17:17:53.299141 140504608769856 worker.py:220] Loss is 0.234710
I0716 17:17:53.300555 140504608769856 worker.py:252] Receive a new task: 301
I0716 17:17:53.799852 140504608769856 worker.py:220] Loss is 0.192143
I0716 17:17:54.101780 140504608769856 worker.py:220] Loss is 0.082024
I0716 17:17:54.103358 140504608769856 worker.py:252] Receive a new task: 302
I0716 17:17:54.733820 140504608769856 worker.py:220] Loss is 0.170840
I0716 17:17:55.030703 140504608769856 worker.py:220] Loss is 0.080983
Traceback (most recent call last):
  File "/usr/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/elasticdl/python/worker/main.py", line 109, in &lt;module&gt;
    main()
  File "/elasticdl/python/worker/main.py", line 105, in main
    worker.run()
  File "/elasticdl/python/worker/worker.py", line 242, in run
    task = self.get_task()
  File "/elasticdl/python/worker/worker.py", line 75, in get_task
    return self._stub.GetTask(req)
  File "/usr/local/lib/python3.6/dist-packages/grpc/_channel.py", line 565, in __call__
    return _end_unary_response_blocking(state, call, False, None)
  File "/usr/local/lib/python3.6/dist-packages/grpc/_channel.py", line 467, in _end_unary_response_blocking
    raise _Rendezvous(state, None, None, deadline)
grpc._channel._Rendezvous: &lt;_Rendezvous of RPC that terminated with:
	status = StatusCode.UNAVAILABLE
	details = "Socket closed"
	debug_error_string = "{"created":"@1563297480.037507629","description":"Error received from peer ipv4:11.161.60.46:50001","file":"src/core/lib/surface/call.cc","file_line":1052,"grpc_message":"Socket closed","grpc_status":14}"
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='terrytangyuan' date='2019-07-17T05:13:19Z'>
		The master should delete any pending worker pods before the exit.
elasticdl-test-mnist-2-worker-0 cannot start at first because of the lack of resources. After elasticdl-test-mnist-2-worker-1 finishes, there are resources available, but the training job is already done, and the master has exited. Thus  elasticdl-test-mnist-2-worker-0 cannot connect to the master.
		</comment>
		<comment id='2' author='terrytangyuan' date='2019-07-17T11:30:11Z'>
		&lt;denchmark-link:https://github.com/skydoorkai&gt;@skydoorkai&lt;/denchmark-link&gt;
 But from the log, it seems like worker-0 (the failed worker) has already started receiving tasks and calculating the gradients/loss. It should have enough resources since it already started.
		</comment>
		<comment id='3' author='terrytangyuan' date='2019-07-17T14:06:56Z'>
		Then the master exits as there are no more tasks, but the worker pod is still alive, and asking the master for tasks.
		</comment>
		<comment id='4' author='terrytangyuan' date='2019-07-17T14:58:16Z'>
		The master should keep running even when there are no more tasks since we have the following code to keep the TensorBoard running:
&lt;denchmark-code&gt;    logger.info("Stopping RPC server")
    server.stop(0)

    # Keep TensorBoard running when all the tasks are finished
    if tb_service:
        logger.info(
            "All tasks finished. Keeping TensorBoard service running..."
        )
        tb_service.keep_running()
    logger.info("Master stopped")
&lt;/denchmark-code&gt;

One possibility might be that  tb_service.keep_running() is not working as expected, which relies on subprocess.Popen.poll() to check the status of TensorBoard process. If TensorBoard exits already, then the master pod will exit.
One thing I am not sure about is: if it's something with TensorBoard process, why would it exit only when all tasks are completed.
		</comment>
		<comment id='5' author='terrytangyuan' date='2019-07-17T15:27:53Z'>
		Tensorboard is optional.
		</comment>
	</comments>
</bug>