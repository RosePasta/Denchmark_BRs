<bug id='32507' author='ZephyrSails' open_date='2020-01-22T20:52:39Z' closed_time='2020-04-16T03:32:56Z'>
	<summary>"code is too big" error in Conv1d</summary>
	<description>
üêõ I find "code is too big" error when I set the kernel size to a large value, I think this is a bug because this error only happens when we set padding size = 0, but when I set padding size &gt; 0 the error won't throw. This does not seems reasonable to me.
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import torch
import torch.nn as nn

hidden_dim = 256
kernel_size = 769
stride = 64
padding = kernel_size // 2
samples = torch.randn([25, 1, 240640])
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;# Zeros padding mode would work
conv1d_zeros = nn.Conv1d(
            1,
            hidden_dim,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
            padding_mode='zeros',
        )
res1 = conv1d_zeros(samples)
print(res1.size())
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;# result:
torch.Size([25, 256, 3760])
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;# Padding mode other than zeros would crash with "code is too big"
conv1d_reflect = nn.Conv1d(
            1,
            hidden_dim,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
            padding_mode='reflect',
        )
res2 = conv1d_reflect(samples)
print(res2.size())
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;# result:
code is too big
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-100-8976b1b809cc&gt; in &lt;module&gt;
      8             padding_mode='reflect',
      9         )
---&gt; 10 res2 = conv1d_reflect(samples)
     11 print(res2.size())
/mnt/xarfuse/uid-133626/39eb9d9a-ns-4026531840/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    538             result = self._slow_forward(*input, **kwargs)
    539         else:
--&gt; 540             result = self.forward(*input, **kwargs)
    541         for hook in self._forward_hooks.values():
    542             hook_result = hook(self, input, result)
/mnt/xarfuse/uid-133626/39eb9d9a-ns-4026531840/torch/nn/modules/conv.py in forward(self, input)
    203             return F.conv1d(F.pad(input, self._padding_repeated_twice, mode=self.padding_mode),
    204                             self.weight, self.bias, self.stride,
--&gt; 205                             _single(0), self.dilation, self.groups)
    206         return F.conv1d(input, self.weight, self.bias, self.stride,
    207                         self.padding, self.dilation, self.groups)
RuntimeError: code is too big
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;# padding with zero padding size would crash with "code is too big"
conv1d_zeros_no_pad = nn.Conv1d(
            1,
            hidden_dim,
            kernel_size=kernel_size,
            stride=stride,
            padding=0,
            padding_mode='zeros',
        )
res3 = conv1d_zeros_no_pad(samples)
print(res3.size())
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;code is too big
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-101-c6945a8c13d3&gt; in &lt;module&gt;
      8             padding_mode='zeros',
      9         )
---&gt; 10 res3 = conv1d_zeros_no_pad(samples)
     11 print(res3.size())
/mnt/xarfuse/uid-133626/39eb9d9a-ns-4026531840/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    538             result = self._slow_forward(*input, **kwargs)
    539         else:
--&gt; 540             result = self.forward(*input, **kwargs)
    541         for hook in self._forward_hooks.values():
    542             hook_result = hook(self, input, result)
/mnt/xarfuse/uid-133626/39eb9d9a-ns-4026531840/torch/nn/modules/conv.py in forward(self, input)
    205                             _single(0), self.dilation, self.groups)
    206         return F.conv1d(input, self.weight, self.bias, self.stride,
--&gt; 207                         self.padding, self.dilation, self.groups)
    208 
    209 
RuntimeError: code is too big
&lt;/denchmark-code&gt;


PyTorch Version (e.g., '1.5.0a0-fb'):
OS: CentOS Linux
How you installed PyTorch: conda.
Python version: 3.7.5+
CUDA/cuDNN version: N/A
GPU models and configuration: N/A
Any other relevant information: N/A

cc &lt;denchmark-link:https://github.com/ezyang&gt;@ezyang&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/gchanan&gt;@gchanan&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/zou3519&gt;@zou3519&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/gujinghui&gt;@gujinghui&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/PenghuiCheng&gt;@PenghuiCheng&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/XiaobingSuper&gt;@XiaobingSuper&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jianyuh&gt;@jianyuh&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='ZephyrSails' date='2020-01-23T13:39:50Z'>
		Hi,
Have you seen this issue? It seems related: &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/24174&gt;#24174&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='ZephyrSails' date='2020-01-27T18:23:43Z'>
		This is definitely an mkldnn problem
		</comment>
		<comment id='3' author='ZephyrSails' date='2020-01-27T19:59:17Z'>
		Hi &lt;denchmark-link:https://github.com/ZephyrSails&gt;@ZephyrSails&lt;/denchmark-link&gt;
 Could you please run this examples (w/ and w/o padding) with environment variable MKLDNN_VERBOSE=1 and attach the log here?
		</comment>
		<comment id='4' author='ZephyrSails' date='2020-01-30T18:38:16Z'>
		&lt;denchmark-link:https://github.com/albanD&gt;@albanD&lt;/denchmark-link&gt;
 Thanks I have seen &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/24174&gt;#24174&lt;/denchmark-link&gt;
 and that's why I suspect it's related to mlkdnn and avx512.
		</comment>
		<comment id='5' author='ZephyrSails' date='2020-01-30T18:40:56Z'>
		Hi &lt;denchmark-link:https://github.com/tprimak&gt;@tprimak&lt;/denchmark-link&gt;
, I have posted the log with previous code as follow:
&lt;denchmark-h:h2&gt;Zeros padding mode would work&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;mkldnn_verbose,info,Intel(R) MKL-DNN v0.18.1 (Git Hash N/A),Intel(R) Advanced Vector Extensions 512 (Intel(R) AVX-512) with AVX512BW, AVX512VL, and AVX512DQ extensions
mkldnn_verbose,exec,convolution,gemm:blas,forward_training,fsrc:nchw fwei:oihw fbia:x fdst:nchw,alg:convolution_direct,mb25_ic1oc256_ih1oh1kh1sh1dh0ph0_iw240640ow3760kw769sw64dw0pw384,74.446
mkldnn_verbose,exec,reorder,simple:any,undef,in:f32_nchw out:f32_nchw,num:1,25x256x1x3760,6.46094
torch.Size([25, 256, 3760])
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Padding mode other than zeros would crash with "code is too big"&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;Traceback (most recent call last):
  File "&lt;string&gt;", line 37, in &lt;module&gt;
  File "&lt;string&gt;", line 35, in __run
  File "/usr/local/fbcode/platform007/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/local/fbcode/platform007/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/data/users/zhipingxiu/fbsource/fbcode/buck-out/dev/gen/experimental/zhipingxiu/pytorch/circular_padding/test#link-tree/experimental/zhipingxiu/pytorch/circular_padding/test.py", line 36, in &lt;module&gt;
    res2 = conv1d_reflect(samples)
  File "/data/users/zhipingxiu/fbsource/fbcode/buck-out/dev/gen/experimental/zhipingxiu/pytorch/circular_padding/test#link-tree/torch/nn/modules/module.py", line 540, in __call__
    result = self.forward(*input, **kwargs)
  File "/data/users/zhipingxiu/fbsource/fbcode/buck-out/dev/gen/experimental/zhipingxiu/pytorch/circular_padding/test#link-tree/torch/nn/modules/conv.py", line 205, in forward
    _single(0), self.dilation, self.groups)
RuntimeError: code is too big
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;padding with zero padding size would crash with "code is too big"&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;Traceback (most recent call last):
  File "&lt;string&gt;", line 37, in &lt;module&gt;
  File "&lt;string&gt;", line 35, in __run
  File "/usr/local/fbcode/platform007/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/local/fbcode/platform007/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/data/users/zhipingxiu/fbsource/fbcode/buck-out/dev/gen/experimental/zhipingxiu/pytorch/circular_padding/test#link-tree/experimental/zhipingxiu/pytorch/circular_padding/test.py", line 48, in &lt;module&gt;
    res3 = conv1d_zeros_no_pad(samples)
  File "/data/users/zhipingxiu/fbsource/fbcode/buck-out/dev/gen/experimental/zhipingxiu/pytorch/circular_padding/test#link-tree/torch/nn/modules/module.py", line 540, in __call__
    result = self.forward(*input, **kwargs)
  File "/data/users/zhipingxiu/fbsource/fbcode/buck-out/dev/gen/experimental/zhipingxiu/pytorch/circular_padding/test#link-tree/torch/nn/modules/conv.py", line 207, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: code is too big
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='ZephyrSails' date='2020-02-07T01:16:06Z'>
		Hi &lt;denchmark-link:https://github.com/ZephyrSails&gt;@ZephyrSails&lt;/denchmark-link&gt;
, thanks for bring this to us!
This is a legacy issue on  and  on subsequent versions. With latest PyTorch 1.5 code base, you should have  instead of  as you show in the &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/32507#issuecomment-580394799&gt;log&lt;/denchmark-link&gt;
, with  on.
I tested your test script on my machine with latest PyTorch 1.5 code base, and no issue is found. Please notice that the mkl-dnn version is v0.21.1:
(pytorch-mingfei) [mingfeim@mlt-clx065 code_too_big_issue]$ MKLDNN_VERBOSE=2 python test.py
mkldnn_verbose,info,Intel MKL-DNN v0.21.1 (commit 7d2fd500bc78936d1d648ca713b901012f470dbc)
mkldnn_verbose,info,Detected ISA is Intel AVX-512 with Intel DL Boost
mkldnn_verbose,create,convolution,gemm:jit,forward_training,fsrc:nchw fwei:oihw fbia:x fdst:nchw,alg:convolution_direct,mb25_ic1oc256_ih1oh1kh1sh1dh0ph0_iw240640ow3760kw769sw64dw0pw384,0.019043
mkldnn_verbose,exec,convolution,gemm:jit,forward_training,fsrc:nchw fwei:oihw fbia:x fdst:nchw,alg:convolution_direct,mb25_ic1oc256_ih1oh1kh1sh1dh0ph0_iw240640ow3760kw769sw64dw0pw384,31.0591
mkldnn_verbose,create,reorder,simple:any,undef,in:f32_nchw out:f32_nchw,num:1,25x256x1x3760,0.00415039
mkldnn_verbose,exec,reorder,simple:any,undef,in:f32_nchw out:f32_nchw,num:1,25x256x1x3760,2.91602
torch.Size([25, 256, 3760])
mkldnn_verbose,create,convolution,jit:avx2,forward_training,fsrc:nchw fwei:Ohwi8o fbia:x fdst:nChw8c,alg:convolution_direct,mb25_ic1oc256_ih1oh1kh1sh1dh0ph0_iw241408ow3760kw769sw64dw0pw0,0.0549316
mkldnn_verbose,create,reorder,jit:uni,undef,in:f32_oihw out:f32_Ohwi8o,num:1,256x1x1x769,0.0300293
mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_oihw out:f32_Ohwi8o,num:1,256x1x1x769,0.156982
mkldnn_verbose,exec,convolution,jit:avx2,forward_training,fsrc:nchw fwei:Ohwi8o fbia:x fdst:nChw8c,alg:convolution_direct,mb25_ic1oc256_ih1oh1kh1sh1dh0ph0_iw241408ow3760kw769sw64dw0pw0,14.998
mkldnn_verbose,create,reorder,jit:uni,undef,in:f32_nChw8c out:f32_nchw,num:1,25x256x1x3760,0.078125
mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nChw8c out:f32_nchw,num:1,25x256x1x3760,3.01904
mkldnn_verbose,create,convolution,jit:avx2,forward_training,fsrc:nchw fwei:Ohwi8o fbia:x fdst:nChw8c,alg:convolution_direct,mb25_ic1oc256_ih1oh1kh1sh1dh0ph0_iw240640ow3748kw769sw64dw0pw0,0.0549316
mkldnn_verbose,create,reorder,jit:uni,undef,in:f32_oihw out:f32_Ohwi8o,num:1,256x1x1x769,0.0300293
mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_oihw out:f32_Ohwi8o,num:1,256x1x1x769,0.0939941
mkldnn_verbose,exec,convolution,jit:avx2,forward_training,fsrc:nchw fwei:Ohwi8o fbia:x fdst:nChw8c,alg:convolution_direct,mb25_ic1oc256_ih1oh1kh1sh1dh0ph0_iw240640ow3748kw769sw64dw0pw0,15.064
mkldnn_verbose,create,reorder,jit:uni,undef,in:f32_nChw8c out:f32_nchw,num:1,25x256x1x3748,0.0368652
mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nChw8c out:f32_nchw,num:1,25x256x1x3748,3.23999
torch.Size([25, 256, 3748])
torch.Size([25, 256, 3760])
Anyway, it is kind of odd you have mkl-dnn v0.18 on PyTorch 1.5, may i ask how you install your PyTorch?
		</comment>
		<comment id='7' author='ZephyrSails' date='2020-02-07T01:22:24Z'>
		&lt;denchmark-link:https://github.com/mingfeima&gt;@mingfeima&lt;/denchmark-link&gt;
 Thanks for the explanation!
As for PyTorch version, I'm using 1.5.0a0-fb. Off the shelf from facebook internal. I'm not sure how it's synced with MS side update, but I will let our folks know.
		</comment>
		<comment id='8' author='ZephyrSails' date='2020-02-07T22:52:39Z'>
		This might be because our internal copy of mkldnn is out of date. Though I see 0.21.1 in third-party2
		</comment>
		<comment id='9' author='ZephyrSails' date='2020-03-02T22:29:51Z'>
		I can confirm I see the error on pytorch 1.4.0 and mkl 0.21.1 as well
code to reproduce:
&lt;denchmark-code&gt;n_dim = 205794  # 205512 works, 205794 error
out = torch.nn.functional.conv1d(
    torch.zeros([1, 1, n_dim]), torch.zeros([514, 1, 512]), stride=160, padding=0
)
&lt;/denchmark-code&gt;

MKLDNN_VERBOSE:
&lt;denchmark-code&gt;16:46:02 my_test_file.py mkldnn_verbose,info,Intel MKL-DNN v0.21.1 (commit 7d2fd500bc78936d1d648ca713b901012f470dbc)
16:46:02 mkldnn_verbose,info,Detected ISA is Intel AVX-512 with AVX512BW, AVX512VL, and AVX512DQ extensions
16:46:02 mkldnn_verbose,exec,reorder,simple:any,undef,in:f32_oihw out:f32_Ohwi16o,num:1,514x1x1x512,0.526855
16:46:02 mkldnn_verbose,exec,convolution,jit:avx512_common,forward_training,fsrc:nchw fwei:Ohwi16o fbia:undef fdst:nChw16c,alg:convolution_direct,mb1_ic1oc514_ih1oh1kh1sh1dh0ph0_iw205512ow1282kw512sw160dw0pw0,6.52197
16:46:02 mkldnn_verbose,exec,reorder,simple:any,undef,in:f32_nChw16c out:f32_nchw,num:1,1x514x1x1282,0.693115
16:46:02 mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nchw out:f32_nChw16c,num:1,1x64x1x1296,0.0571289
16:46:02 mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_goihw out:f32_Goihw16g,num:1,64x1x1x1x33,0.00292969
16:46:02 mkldnn_verbose,exec,convolution,jit_dw:avx512_common,forward_training,fsrc:nChw16c fwei:Goihw16g fbia:undef fdst:nChw16c,alg:convolution_direct,mb1_g64ic64oc64_ih1oh1kh1sh1dh0ph0_iw1296ow648kw33sw2dw0pw16,0.0981445
16:46:02 mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nChw16c out:f32_nchw,num:1,1x64x1x648,0.0500488
16:46:02 mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nchw out:f32_nChw16c,num:1,1x64x1x648,0.0339355
16:46:02 mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_oihw out:f32_OIhw16i16o,num:1,32x64x1x1,0.00317383
16:46:02 mkldnn_verbose,exec,convolution,jit_1x1:avx512_common,forward_training,fsrc:nChw16c fwei:OIhw16i16o fbia:undef fdst:nChw16c,alg:convolution_direct,mb1_ic64oc32_ih1oh1kh1sh1dh0ph0_iw648ow648kw1sw1dw0pw0,0.0720215
16:46:02 mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nChw16c out:f32_nchw,num:1,1x32x1x648,0.0239258
16:46:02 mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nchw out:f32_nChw16c,num:1,1x32x1x648,0.019043
16:46:02 mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_oihw out:f32_OIhw16i16o,num:1,64x32x1x1,0.00219727
16:46:02 mkldnn_verbose,exec,convolution,jit_1x1:avx512_common,forward_training,fsrc:nChw16c fwei:OIhw16i16o fbia:undef fdst:nChw16c,alg:convolution_direct,mb1_ic32oc64_ih1oh1kh1sh1dh0ph0_iw648ow648kw1sw1dw0pw0,0.0771484
16:46:02 mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nChw16c out:f32_nchw,num:1,1x64x1x648,0.0439453
16:46:02 mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nchw out:f32_nChw16c,num:1,1x64x1x648,0.0341797
16:46:02 mkldnn_verbose,exec,reorder,simple:any,undef,in:f32_oihw out:f32_OIhw16i16o,num:1,35x64x1x1,0.00610352
16:46:02 mkldnn_verbose,exec,convolution,jit_1x1:avx512_common,forward_training,fsrc:nChw16c fwei:OIhw16i16o fbia:x fdst:nChw16c,alg:convolution_direct,mb1_ic64oc35_ih1oh1kh1sh1dh0ph0_iw648ow648kw1sw1dw0pw0,0.0881348
16:46:02 mkldnn_verbose,exec,reorder,simple:any,undef,in:f32_nChw16c out:f32_nchw,num:1,1x35x1x648,0.0310059
16:46:02 mkldnn_verbose,exec,reorder,simple:any,undef,in:f32_oihw out:f32_Ohwi16o,num:1,514x1x1x512,0.424805
16:46:02 mkldnn_verbose,exec,convolution,jit:avx512_common,forward_training,fsrc:nchw fwei:Ohwi16o fbia:undef fdst:nChw16c,alg:convolution_direct,mb1_ic1oc514_ih1oh1kh1sh1dh0ph0_iw205512ow1282kw512sw160dw0pw0,6.52393
16:46:02 mkldnn_verbose,exec,reorder,simple:any,undef,in:f32_nChw16c out:f32_nchw,num:1,1x514x1x1282,0.626953
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='ZephyrSails' date='2020-03-03T01:27:08Z'>
		&lt;denchmark-link:https://github.com/gkucsko&gt;@gkucsko&lt;/denchmark-link&gt;
 thanks for the report, we are closely looking into this.
		</comment>
		<comment id='11' author='ZephyrSails' date='2020-03-19T15:07:17Z'>
		Same issues here on Travis linux builds with pytorch 1.5-nightly and mkl 0.21.1
&lt;denchmark-code&gt;self = Conv1d(10, 150, kernel_size=(36,), stride=(1,))
input = tensor([[[-6.9325e-03, -3.0474e-03, -7.5795e-03,  ..., -3.5221e-03,
          -1.9434e-03, -1.3649e-02],
         [-3....1.0792e-02],
         [-1.4494e-01, -1.4215e-01, -1.1365e-01,  ..., -1.1949e-01,
          -1.2632e-01, -1.3129e-01]]])

    def forward(self, input):
        if self.padding_mode == 'circular':
            expanded_padding = ((self.padding[0] + 1) // 2, self.padding[0] // 2)
            return F.conv1d(F.pad(input, expanded_padding, mode='circular'),
                            self.weight, self.bias, self.stride,
                            _single(0), self.dilation, self.groups)
        return F.conv1d(input, self.weight, self.bias, self.stride,
&gt;                       self.padding, self.dilation, self.groups)

E       RuntimeError: code is too big
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;../../../miniconda/lib/python3.7/site-packages/torch/nn/modules/conv.py:202: RuntimeError

----------------------------- Captured stdout call -----------------------------

mkldnn_verbose,info,Intel MKL-DNN v0.21.1 (commit 7d2fd500bc78936d1d648ca713b901012f470dbc)
mkldnn_verbose,info,Detected ISA is Intel AVX-512 with Intel DL Boost
mkldnn_verbose,exec,reorder,simple:any,undef,in:f32_nchw out:f32_nChw16c,num:1,1x10x1x635,1.04199
mkldnn_verbose,exec,reorder,simple:any,undef,in:f32_oihw out:f32_OIhw16i16o,num:1,150x10x1x8,1.12012
mkldnn_verbose,exec,convolution,jit:avx512_common,forward_training,fsrc:nChw16c fwei:OIhw16i16o fbia:x fdst:nChw16c,alg:convolution_direct,mb1_ic10oc150_ih1oh1kh1sh1dh0ph0_iw635ow628kw8sw1dw0pw0,0.759033
mkldnn_verbose,exec,reorder,simple:any,undef,in:f32_nChw16c out:f32_nchw,num:1,1x150x1x628,0.368896
mkldnn_verbose,exec,reorder,simple:any,undef,in:f32_nchw out:f32_nChw16c,num:1,1x10x1x635,0.0090332
mkldnn_verbose,exec,reorder,simple:any,undef,in:f32_oihw out:f32_OIhw16i16o,num:1,150x10x1x12,0.0310059
mkldnn_verbose,exec,convolution,jit:avx512_common,forward_training,fsrc:nChw16c fwei:OIhw16i16o fbia:x fdst:nChw16c,alg:convolution_direct,mb1_ic10oc150_ih1oh1kh1sh1dh0ph0_iw635ow624kw12sw1dw0pw0,0.344971
...
&lt;/denchmark-code&gt;

		</comment>
		<comment id='12' author='ZephyrSails' date='2020-03-20T00:51:38Z'>
		&lt;denchmark-link:https://github.com/gkucsko&gt;@gkucsko&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/VarIr&gt;@VarIr&lt;/denchmark-link&gt;
 Thanks for the test!
Actually these are a different cases resulting into the same error. These errors are already fixed on DNNL 1.2 (MKL-DNN is renamed as DNNL).
We are updating DNNL(MKL-DNN)  version in &lt;denchmark-link:https://github.com/pytorch/pytorch/pull/32422&gt;#32422&lt;/denchmark-link&gt;
, normally these errors will be cleared when this is settled.
		</comment>
		<comment id='13' author='ZephyrSails' date='2020-04-16T03:32:56Z'>
		Fixed by &lt;denchmark-link:https://github.com/pytorch/pytorch/pull/32422&gt;#32422&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>