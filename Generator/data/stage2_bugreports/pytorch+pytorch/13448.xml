<bug id='13448' author='grey-area' open_date='2018-11-01T13:38:01Z' closed_time='2019-12-09T22:43:30Z'>
	<summary>torch.sigmoid behaves inconsistently for 32- and 64-bit NaN inputs</summary>
	<description>
&lt;denchmark-h:h2&gt;Issue description&lt;/denchmark-h&gt;

When applying torch.sigmoid to a DoubleTensor containing NaNs, the result will contain only NaNs.
When applying torch.sigmoid to a FloatTensor containing NaNs, some values in the resulting tensor will be NaN, and some values will be a small number (4.156e-39). The proportion of each seems to depend deterministically on the size of the input tensor, but for a given input size, the proportion of each will vary from one machine to another.  The NaNs and small valued numbers appear in a periodic pattern in the output tensor.
The bug doesn't manifest if everything is done on GPU.
&lt;denchmark-h:h2&gt;Code example&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Code&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;Ns = [10, 100, 1000, 10000]

for N in Ns:

    nans_32_bit = (torch.zeros(N) / 0)
    nans_64_bit = (nans_32_bit.type(torch.DoubleTensor))

    print('Input: tensor of size N={} containing only NaN'.format(N))

    post_sigmoid_32_bit = torch.sigmoid(nans_32_bit)
    post_sigmoid_64_bit = torch.sigmoid(nans_64_bit)

    proportion_nan_32_bit = torch.mean(torch.isnan(post_sigmoid_32_bit).type(torch.FloatTensor))
    proportion_nan_64_bit = torch.mean(torch.isnan(post_sigmoid_64_bit).type(torch.FloatTensor))

    print('After sigmoid:')
    print('Proportion NaN in result with 32 bit input: {:.3f}'.format(proportion_nan_32_bit))
    print('Proportion NaN in result with 64 bit input: {:.3f}\n'.format(proportion_nan_64_bit))
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Code output&lt;/denchmark-h&gt;

Input: tensor of size N=10 containing only NaN
After sigmoid:
Proportion NaN in result with 32 bit input: 1.000
Proportion NaN in result with 64 bit input: 1.000
Input: tensor of size N=100 containing only NaN
After sigmoid:
Proportion NaN in result with 32 bit input: 0.360
Proportion NaN in result with 64 bit input: 1.000
Input: tensor of size N=1000 containing only NaN
After sigmoid:
Proportion NaN in result with 32 bit input: 0.040
Proportion NaN in result with 64 bit input: 1.000
Input: tensor of size N=10000 containing only NaN
After sigmoid:
Proportion NaN in result with 32 bit input: 0.002
Proportion NaN in result with 64 bit input: 1.000
&lt;denchmark-h:h2&gt;System Info&lt;/denchmark-h&gt;

PyTorch version: 0.4.1
Is debug build: No
CUDA used to build PyTorch: 9.0.176
OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 7.3.0-16ubuntu3) 7.3.0
CMake version: version 3.10.2
Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.1.85
GPU models and configuration: GPU 0: GeForce GTX 1050
Nvidia driver version: 390.59
cuDNN version: 7102
Versions of relevant libraries:
[pip] 18.1
[conda] Could not collect
cc &lt;denchmark-link:https://github.com/ezyang&gt;@ezyang&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/gchanan&gt;@gchanan&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/zou3519&gt;@zou3519&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='grey-area' date='2018-11-01T14:13:51Z'>
		Also get this behaviour, but different proportions:
&lt;denchmark-h:h2&gt;Code Output&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;Input: tensor of size N=10 containing only NaN
After sigmoid:
Proportion NaN in result with 32 bit input: 1.000
Proportion NaN in result with 64 bit input: 1.000

Input: tensor of size N=100 containing only NaN
After sigmoid:
Proportion NaN in result with 32 bit input: 0.040
Proportion NaN in result with 64 bit input: 1.000

Input: tensor of size N=1000 containing only NaN
After sigmoid:
Proportion NaN in result with 32 bit input: 0.008
Proportion NaN in result with 64 bit input: 1.000

Input: tensor of size N=10000 containing only NaN
After sigmoid:
Proportion NaN in result with 32 bit input: 0.002
Proportion NaN in result with 64 bit input: 1.000
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;System info&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;Collecting environment information...
PyTorch version: 0.4.1.post2
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 16.04.5 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: Could not collect

Python version: 3.7
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip] Could not collect
[conda] Could not collect
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='grey-area' date='2018-11-01T21:41:48Z'>
		float32 uses &lt;denchmark-link:https://github.com/pytorch/pytorch/blob/81438f12206fdbfb29953428255e5e9b2d4e6868/aten/src/ATen/native/cpu/avx_mathfun.h#L245&gt;exp256_ps&lt;/denchmark-link&gt;
 to compute a vectorized exponential function. This doesn't propagate NaNs the same way as .
See &lt;denchmark-link:http://gruntthepeon.free.fr/ssemath/&gt;http://gruntthepeon.free.fr/ssemath/&lt;/denchmark-link&gt;

If someone can modify exp256_ps to propagate NaNs without too much overhead, that would be great.
		</comment>
		<comment id='3' author='grey-area' date='2018-11-01T22:01:47Z'>
		&lt;denchmark-link:https://github.com/resistor&gt;@resistor&lt;/denchmark-link&gt;
 do you know of any clever bit-twiddling tricks to propagate NaNs in &lt;denchmark-link:https://github.com/pytorch/pytorch/blob/81438f12206fdbfb29953428255e5e9b2d4e6868/aten/src/ATen/native/cpu/avx_mathfun.h#L245&gt;exp256_ps&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='4' author='grey-area' date='2018-11-01T22:12:17Z'>
		I’m off the grid right now but can take a look when I’m back.  Does exp have the same special cases as pow?
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


 On Nov 1, 2018, at 3:02 PM, Sam Gross ***@***.***&gt; wrote:

 @resistor do you know of any clever bit-twiddling tricks to propagate NaNs in exp256_ps?

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub, or mute the thread.


		</comment>
		<comment id='5' author='grey-area' date='2018-11-05T06:11:02Z'>
		&lt;denchmark-link:https://github.com/colesbury&gt;@colesbury&lt;/denchmark-link&gt;
 I think replacing the min/max operations with properly NaN-propagating ones might suffice?  Actually, just swapping the order of the operands would probably do the trick, exploiting the non-commutivity of SSE's min/max instructions.
That said, I'm not certain offhand that this will correctly handle the other enumerated special cases of exp():

exp(+-0) returns 1.
exp(-infinity) returns +0.
exp(+infinity) returns +infinity.

I haven't traced or tested this function, but correct implementations I've seen in the past usually had to handle at least one of those with special case logic.
		</comment>
		<comment id='6' author='grey-area' date='2018-11-05T16:41:21Z'>
		That makes sense. I think that's how the Sleef implementation propagates NaNs.
I was thinking about this a bit more on Friday and I'm a bit concerned that exp256_ps from avx_mathfun.h isn't great. The implementation from Sleef is superior in most ways: more precise, handles the edge cases properly, and I think it's faster.
Although we link against sleef, we're not using its implementation of exp() because it's not available in a header, so we would incur the full cost of a non-inlineable function call.
We could port the sleef implementation for exp. I tried this and it's mostly mechanical. Of course, it would be better if the sleef implementation were available in a way that could be inlined into call sites.
sleef scalar and vector implementations:
&lt;denchmark-link:https://github.com/shibatch/sleef/blob/master/src/libm/sleefsp.c#L1109-L1129&gt;https://github.com/shibatch/sleef/blob/master/src/libm/sleefsp.c#L1109-L1129&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/shibatch/sleef/blob/master/src/libm/sleefsimdsp.c#L1705-L1727&gt;https://github.com/shibatch/sleef/blob/master/src/libm/sleefsimdsp.c#L1705-L1727&lt;/denchmark-link&gt;

port (incomplete: requires AVX)
&lt;denchmark-link:https://gist.github.com/colesbury/ea25e51bd54ccac17ed8811a18303ae2&gt;https://gist.github.com/colesbury/ea25e51bd54ccac17ed8811a18303ae2&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='grey-area' date='2018-11-05T16:44:09Z'>
		&lt;denchmark-link:https://github.com/colesbury&gt;@colesbury&lt;/denchmark-link&gt;
 we can request the SLEEF author(s) to put it in a header. I remember they are very responsive to our requests.
		</comment>
		<comment id='8' author='grey-area' date='2018-11-05T16:45:38Z'>
		Yes, I'm posting an issue on the Sleef github. It doesn't seem like there will be any simple fix, though.
		</comment>
		<comment id='9' author='grey-area' date='2019-08-22T20:33:00Z'>
		&lt;denchmark-link:https://github.com/colesbury&gt;@colesbury&lt;/denchmark-link&gt;
 can you link the Sleef github issue?
		</comment>
		<comment id='10' author='grey-area' date='2019-08-22T20:41:00Z'>
		&lt;denchmark-link:https://github.com/gchanan&gt;@gchanan&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/shibatch/sleef/issues/230&gt;shibatch/sleef#230&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='grey-area' date='2019-12-06T19:50:35Z'>
		This issue apparently doesn't exist anymore. I just ran the reproducer from the first comment and I got this output:
&lt;denchmark-code&gt;Input: tensor of size N=10 containing only NaN
After sigmoid:
Proportion NaN in result with 32 bit input: 1.000
Proportion NaN in result with 64 bit input: 1.000

Input: tensor of size N=100 containing only NaN
After sigmoid:
Proportion NaN in result with 32 bit input: 1.000
Proportion NaN in result with 64 bit input: 1.000

Input: tensor of size N=1000 containing only NaN
After sigmoid:
Proportion NaN in result with 32 bit input: 1.000
Proportion NaN in result with 64 bit input: 1.000

Input: tensor of size N=10000 containing only NaN
After sigmoid:
Proportion NaN in result with 32 bit input: 1.000
Proportion NaN in result with 64 bit input: 1.000
&lt;/denchmark-code&gt;

		</comment>
		<comment id='12' author='grey-area' date='2019-12-08T21:46:15Z'>
		The sleef issue is not closed. Are you sure you ran your tests on a machine that supports the vectorized instructions necessary to reproduce this issue?
		</comment>
		<comment id='13' author='grey-area' date='2019-12-09T20:18:56Z'>
		&lt;denchmark-link:https://github.com/ezyang&gt;@ezyang&lt;/denchmark-link&gt;
, this machine should at least be running exp256_ps. Here's my environment:
&lt;denchmark-code&gt;PyTorch version: 1.4.0a0+2607772
Is debug build: No
CUDA used to build PyTorch: 10.1

OS: Ubuntu 18.04.3 LTS
GCC version: (crosstool-NG 1.23.0.452-d158) 7.3.0
CMake version: version 3.15.5

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.1.243
GPU models and configuration: 
GPU 0: Quadro RTX 8000
GPU 1: Quadro RTX 8000

Nvidia driver version: 430.50
cuDNN version: /usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.7
&lt;/denchmark-code&gt;

But maybe it acts differently on Quadro RTX 8000 vs GeForce GTX 1050.
		</comment>
		<comment id='14' author='grey-area' date='2019-12-09T22:10:07Z'>
		I also can't reproduce this on a machine that's AVX2 / AVX-512 capable, tried both building from source and a recent nightly.
There are no related changes in  though, see &lt;denchmark-link:https://github.com/pytorch/pytorch/commits/master/aten/src/ATen/native/cpu/avx_mathfun.h&gt;https://github.com/pytorch/pytorch/commits/master/aten/src/ATen/native/cpu/avx_mathfun.h&lt;/denchmark-link&gt;
. May be useful to use  to find if and how this was fixed.
		</comment>
		<comment id='15' author='grey-area' date='2019-12-09T22:10:23Z'>
		Or just add a regression test and see if it passes all CI.
		</comment>
		<comment id='16' author='grey-area' date='2019-12-09T22:43:20Z'>
		sigmoid() doesn't use avx_mathfun.h anymore. &lt;denchmark-link:https://github.com/pytorch/pytorch/pull/7341&gt;#7341&lt;/denchmark-link&gt;
 switched  to use Sleef_expf8_u10 (and Sleef has proper handling of NaN).
The PR that changed Vec256&lt;float&gt;::exp() predates this issue, but wasn't in the stable release (0.4.1) at the time.
Note that the open linked Sleef issue is not about NaN handling. It's about about Sleef functions not getting inlined, which hurts perf (and was the reason we were using avx_mathfun.h in 0.4.1)
		</comment>
	</comments>
</bug>