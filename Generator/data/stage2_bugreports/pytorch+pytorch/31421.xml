<bug id='31421' author='shapovalov' open_date='2019-12-18T14:09:10Z' closed_time='2020-01-31T14:47:07Z'>
	<summary>Matrix multiplication returns wrong result when middle dimension is 0</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

This may be a marginal use case. I expect the result of multiplying N√ó0 by 0√óM matrices be zeros(N, M) as a particular case of matrix multiplication definition. However, bmm and matmul neither fail nor return zero tensor. Most likely, memory initialisation is skipped in that case.
Let‚Äôs discuss whether we consider it a legitimate input, and either fix the result, or raise an exception.
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

Unfortunately, this is a floating bug that depends on the state of the system.
&lt;denchmark-code&gt;t1 = torch.ones((3,2,0)).cuda(0)
t2 = torch.ones((3,0,3)).cuda(0)
res_bmm = torch.bmm(t1, t2)
res_matmul = torch.matmul(t1, t2)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

Either:

res_bmm and res_matmul should always be zeros(3,2,3), regardless of the initial memory state,
or:
exception raised.

&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version (e.g., 1.0): 1.3.0+cu100
OS (e.g., Linux): linux
How you installed PyTorch (conda, pip, source): conda
Build command you used (if compiling from source):
Python version: 3.6
CUDA/cuDNN version: 10.0.130
GPU models and configuration: Quadro GP100

cc &lt;denchmark-link:https://github.com/ezyang&gt;@ezyang&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/gchanan&gt;@gchanan&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/zou3519&gt;@zou3519&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/csarofeen&gt;@csarofeen&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ptrblck&gt;@ptrblck&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ngimel&gt;@ngimel&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='shapovalov' date='2019-12-18T19:52:45Z'>
		I cannot reproduce this on cpu. Where it returns Tensors full of 0s.
What is the error you see on cuda?
		</comment>
		<comment id='2' author='shapovalov' date='2019-12-18T19:55:27Z'>
		I cannot reproduce it even on cuda, even when I make it use initialized memory
&lt;denchmark-code&gt;
In [2]: a=torch.randn(1024*1024, device="cuda")                                                                                  

In [3]: del a                                                                                                                    

In [4]: torch.cuda.memory_allocated()                                                                                  
Out[4]: 0

In [5]: torch.cuda.memory_cached()                                                                                     
Out[5]: 20971520

In [6]: t1=torch.ones((3,2,0)).cuda(0)                                                                                 

In [7]: t2 = torch.ones((3,0,3)).cuda(0)                                                                               

In [8]: torch.cuda.memory_allocated()                                                                                  
Out[8]: 0

In [9]: torch.cuda.memory_cached()                                                                                     
Out[9]: 20971520

In [10]: res_bmm = torch.bmm(t1, t2)                                                                                   

In [11]: torch.cuda.memory_cached()                                                                                    
Out[11]: 23068672

In [12]: res_bmm                                                                                                       
Out[12]: 
tensor([[[0., 0., 0.],
         [0., 0., 0.]],

        [[0., 0., 0.],
         [0., 0., 0.]],

        [[0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0')
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='shapovalov' date='2019-12-18T20:53:49Z'>
		Also would be helpful to show the output that doesn't have 0s.
		</comment>
		<comment id='4' author='shapovalov' date='2019-12-20T15:45:44Z'>
		Thanks for the comments everyone!
I‚Äôve tried to reproduce it standalone by allocating/freeing memory in the loop but failed as well. My hope was that someone here would know the code and could quickly check whether the memory is always initialised in bmm.
The actual line in my code is
&lt;denchmark-code&gt;delta = torch.bmm(alpha[:, None, :], basis[:, 1:, :])
&lt;/denchmark-code&gt;

This is what I get:
&lt;denchmark-code&gt;(Pdb) alpha.size()
torch.Size([10, 0])
(Pdb) basis.size()
torch.Size([10, 1, 43200])
(Pdb) delta
tensor([[[-4.9469e-04,  3.1077e-02, -1.2956e-02,  ..., -5.8799e-03,
           3.2257e-03,  5.6009e-03]],

        [[ 6.5665e-03,  5.5072e-03,  9.1196e-03,  ...,  1.5373e-02,
          -3.3161e-02, -2.4766e-02]],

        [[ 3.9871e-02, -2.9234e-02,  1.2655e-02,  ...,  5.0681e-03,
           1.0401e-02, -1.0340e-03]],

        ...,

        [[ 2.0880e-11, -3.6763e-11, -4.3727e-11,  ...,  1.5785e-10,
           1.2219e-10,  5.8353e-11]],

        [[ 3.3565e-11, -1.8010e-11, -2.1607e-11,  ...,  1.4607e-10,
           1.1281e-10,  5.2787e-11]],

        [[ 2.2027e-11, -3.6393e-11, -3.4783e-11,  ...,  1.4274e-10,
           1.0893e-10,  5.0855e-11]]], device='cuda:0', grad_fn=&lt;BmmBackward&gt;)
&lt;/denchmark-code&gt;

One peculiarity I noticed is that it alternates between zero and non-zero results when running the same line several times.
Let me know if you have any ideas how to narrow it down.
		</comment>
		<comment id='5' author='shapovalov' date='2019-12-26T07:01:13Z'>
		Indeed, with the sizes in the previous snippet I can reproduce it.
&lt;denchmark-code&gt;In [38]: a=torch.randn(10,1,0,device="cuda")                                                                                                    

In [39]: b=torch.randn(10,0,43200, device="cuda")                                                                                               

In [40]: out = torch.randn(10,1,43200).cuda()                                                                                                   

In [41]: torch.bmm(a,b,out=out)                                                                                                                 
Out[41]: 
tensor([[[ 0.1343, -1.8573, -0.4454,  ..., -0.0880, -0.1325,  1.4109]],

        [[ 0.6763, -1.6208,  0.3440,  ...,  0.5403, -1.1779,  0.3506]],

        [[ 0.0802, -0.6159, -0.3801,  ...,  0.9672,  0.4577, -0.7112]],

        ...,

        [[ 0.9164,  2.2342, -2.0074,  ..., -0.6363, -0.9746,  0.4483]],

        [[ 1.1386,  0.9839,  1.8808,  ..., -1.5350,  1.0081,  0.3232]],

        [[ 0.3560, -0.1623, -1.0822,  ..., -0.0443, -0.0282,  0.2387]]],
       device='cuda:0')

&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='shapovalov' date='2019-12-26T07:43:34Z'>
		Looks like cublas bug. With the repro script and .ltrace.conf from this gist (put .ltrace.conf in the root directory)
&lt;denchmark-link:https://gist.github.com/ngimel/780f09d431906a522c4df2da3cd16fbd&gt;https://gist.github.com/ngimel/780f09d431906a522c4df2da3cd16fbd&lt;/denchmark-link&gt;

I'm getting reasonable looking arguments for cublas call, yet output seems not to be overwritten:
&lt;denchmark-code&gt;(pytorch) ngimel@xxxxxxx:~/playground$ ltrace -f -e  'cublasSgemmStridedBatched'  python bmm.py
[pid 23443] --- Called exec() ---
[pid 23444] --- Called exec() ---
[pid 23444] +++ exited (status 0) +++
[pid 23443] --- SIGCHLD (Child exited) ---
[pid 23443] +++ exited (status 0) +++
[pid 23439] --- SIGCHLD (Child exited) ---
[pid 23439] libtorch_cuda.so-&gt;cublasSgemmStridedBatched(0x562bbf906850, 0, 0, 43200, 1, 0, 1.000000, 0, 43200, 43200, 0, 1, 1, 0.000000, 0x7ff76a400000, 43200, 43200, 10) = 0
tensor([[[1., 1., 1.,  ..., 1., 1., 1.]],

        [[1., 1., 1.,  ..., 1., 1., 1.]],

        [[1., 1., 1.,  ..., 1., 1., 1.]],

        ...,

        [[1., 1., 1.,  ..., 1., 1., 1.]],

        [[1., 1., 1.,  ..., 1., 1., 1.]],

        [[1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0')
&lt;/denchmark-code&gt;

We could implement a workaround zeroing the output for this case, actual cublas bugfixes take a long time.
cc &lt;denchmark-link:https://github.com/ptrblck&gt;@ptrblck&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='shapovalov' date='2019-12-26T16:49:14Z'>
		Is the issue linked to the fact that we have both 0-dimensional and 0-stride?
		</comment>
		<comment id='8' author='shapovalov' date='2019-12-26T18:31:28Z'>
		Cublas just does not handle this edge case correctly. In fact, regular matrix multiply with the same sizes produces expected zero result, so the strides are fine.
		</comment>
		<comment id='9' author='shapovalov' date='2019-12-31T13:48:59Z'>
		Thanks for investigating it &lt;denchmark-link:https://github.com/ngimel&gt;@ngimel&lt;/denchmark-link&gt;
 ! I agree memset‚Äôting  with 0s makes sense as a temporary solution.
		</comment>
		<comment id='10' author='shapovalov' date='2020-01-13T19:22:00Z'>
		&lt;denchmark-link:https://github.com/shapovalov&gt;@shapovalov&lt;/denchmark-link&gt;
 This issue should have been fixed in cublas shipping with CUDA10.1.
Could you please install the latest PyTorch with CUDA10.1 and retry your script?
		</comment>
		<comment id='11' author='shapovalov' date='2020-01-30T16:15:05Z'>
		I believe this should be resolved now:
&lt;denchmark-code&gt;‚ùØ python test.py
===== [ res_bmm ] =====
tensor([[[0., 0., 0.],
         [0., 0., 0.]],

        [[0., 0., 0.],
         [0., 0., 0.]],

        [[0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0')

===== [ res_matmul ] =====
tensor([[[0., 0., 0.],
         [0., 0., 0.]],

        [[0., 0., 0.],
         [0., 0., 0.]],

        [[0., 0., 0.],
         [0., 0., 0.]]], device='cuda:0')
&lt;/denchmark-code&gt;


`test.py`
import torch

t1 = torch.ones((3,2,0)).cuda(0)
t2 = torch.ones((3,0,3)).cuda(0)
res_bmm = torch.bmm(t1, t2)
res_matmul = torch.matmul(t1, t2)

print("===== [ res_bmm ] =====")
print(res_bmm)
print()
print("===== [ res_matmul ] =====")
print(res_matmul)

Pytorch version: 1.5.0.dev20200129
CUDA Version: 10.1
		</comment>
		<comment id='12' author='shapovalov' date='2020-01-31T14:47:07Z'>
		Great, thank you!
		</comment>
	</comments>
</bug>