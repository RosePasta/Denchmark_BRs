<bug id='1272' author='ghost(ghost)' open_date='2017-04-17T11:18:09Z' closed_time='2017-09-13T13:04:12Z'>
	<summary>memory leak(cpu, not gpu) in convolution layer</summary>
	<description>
Hello
I have a problem related to memory leak(cpu, not gpu) in ubuntu os.
when I run below code with cudnn 5.1 and 6.0, memory usage increases every epoch.
(memory leak doesn't occur with cudnn 5.0, but cudnn 5.0 is 2x slower than cudnn 5.1 and 6.0)
&lt;denchmark-link:https://gist.github.com/nido4010/6f533f1e55ae1fd38a017533f72dd3e8&gt;https://gist.github.com/nido4010/6f533f1e55ae1fd38a017533f72dd3e8&lt;/denchmark-link&gt;

To manually select convolution algorithm, I revise the code, pytorch-master/torch/csrc/cudnn/Conv.cpp
the revised code is below
&lt;denchmark-link:https://gist.github.com/nido4010/5686db92b1bbf1f252154b42b1cca95c&gt;https://gist.github.com/nido4010/5686db92b1bbf1f252154b42b1cca95c&lt;/denchmark-link&gt;

revision list

comment out "findAlgorithm(state, handle, conv, benchmark, algo);" ( line 240)
cudnnConvolutionFwdAlgo_t fwdAlg =
CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED;( line 320)
cudnnConvolutionBwdDataAlgo_t bwdDataAlg =
CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD_NONFUSED;( line 367)
cudnnConvolutionBwdFilterAlgo_t bwdFilterAlg =
CUDNN_CONVOLUTION_BWD_FILTER_ALGO_WINOGRAD_NONFUSED;(line 395)

I check memory leak, while I change the forward, backward data, backward filter algorithm.
Algorithm list is provided in cudnn user guide.
when I select CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED for forward convolution, which is newly added in cudnn 5.1, memory usage increases by about 90MB every epoch.
when I select CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD_NONFUSED for backward data convolution, which is newly added in cudnn 5.1, memory usage increases by about 120MB every epoch.
when I select CUDNN_CONVOLUTION_BWD_FILTER_ALGO_WINOGRAD_NONFUSED for backward filter convolution, which is newly added in cudnn 5.1, memory usage increases by about 130MB every epoch.
About the other algorithms, memory leak doesn't occur.
I have not yet test the above experiment with cudnn 6.0, but memory leak occur when I compile pytorch with cudnn 6.0.
Is there anybody who experiences memory leak like above??
Experiment setup

OS : linux mint 18.1(ubuntu 16.04.1)
cuda : 8.0
cudnn : 5.1
python : 2.7

	</description>
	<comments>
		<comment id='1' author='ghost(ghost)' date='2017-04-18T10:41:58Z'>
		I also observe memory leaks
torch7+cudnn 5.1 + ubuntu 14.04
		</comment>
		<comment id='2' author='ghost(ghost)' date='2017-04-18T13:25:39Z'>
		cc &lt;denchmark-link:https://github.com/ngimel&gt;@ngimel&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='ghost(ghost)' date='2017-04-21T23:49:48Z'>
		I can't repro with cudnn 6.0.20 (Ubuntu 16.04, cuda 8.0, python 3.5)
		</comment>
		<comment id='4' author='ghost(ghost)' date='2017-04-22T05:31:18Z'>
		&lt;denchmark-link:https://github.com/ngimel&gt;@ngimel&lt;/denchmark-link&gt;

memory leak also occurs in python 3.5.
I observed above memory leak in tensorflow, cntk, and torch7.
Do you use original ubuntu 16.04??
		</comment>
		<comment id='5' author='ghost(ghost)' date='2017-04-23T00:39:38Z'>
		I've used image built from the Dockerfile provided in the repo. A few packages have to be additionally installed to be able to run the example.
		</comment>
		<comment id='6' author='ghost(ghost)' date='2017-04-24T16:19:25Z'>
		I test above example using docker image and memory leak also occurred.
I don't know the exact cause.
		</comment>
		<comment id='7' author='ghost(ghost)' date='2017-04-26T16:17:25Z'>
		&lt;denchmark-link:https://github.com/ngimel&gt;@ngimel&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/apaszke&gt;@apaszke&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/AleximusOrloff&gt;@AleximusOrloff&lt;/denchmark-link&gt;

Finally I solved this problem.
Maybe, the above memory leak is related to nvidia driver.
I replaced driver version 375(default in cuda 8.0) to 367, and memory leak doesn't occur
		</comment>
		<comment id='8' author='ghost(ghost)' date='2017-04-26T19:41:39Z'>
		
I replaced driver version 375(default in cuda 8.0) to 367, and memory leak doesn't occur
I have 1080Ti, so that's not a case.
BTW I discovered that memory leaks only during backpropogandition

		</comment>
		<comment id='9' author='ghost(ghost)' date='2017-05-09T04:35:18Z'>
		&lt;denchmark-link:https://github.com/AleximusOrloff&gt;@AleximusOrloff&lt;/denchmark-link&gt;

I recently change gtx 770 to gtx 1080ti.
I install nvidia driver 378.13, and memory leak doesn't occur
		</comment>
		<comment id='10' author='ghost(ghost)' date='2017-05-25T15:14:24Z'>
		Hi, I am seeing this on TitanX maxwell with both 375.66 and 378.13
I use tensorflow.
I caught the memory leak in gdb but its totally closed source.
#0  __libc_calloc (n=8, elem_size=32) at malloc.c:3170
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/1&gt;#1&lt;/denchmark-link&gt;
  0x00007fbbb885a292 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/2&gt;#2&lt;/denchmark-link&gt;
  0x00007fbbb885a511 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/3&gt;#3&lt;/denchmark-link&gt;
  0x00007fbbb87b9c62 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
&lt;denchmark-link:https://github.com/pytorch/pytorch/pull/4&gt;#4&lt;/denchmark-link&gt;
  0x00007fbbb86eb259 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/5&gt;#5&lt;/denchmark-link&gt;
  0x00007fbbb88f2f1c in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/6&gt;#6&lt;/denchmark-link&gt;
  0x00007fbbb88f33a7 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/7&gt;#7&lt;/denchmark-link&gt;
  0x00007fbbb88f3439 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
&lt;denchmark-link:https://github.com/pytorch/pytorch/pull/8&gt;#8&lt;/denchmark-link&gt;
  0x00007fbbb879dc81 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
&lt;denchmark-link:https://github.com/pytorch/pytorch/pull/9&gt;#9&lt;/denchmark-link&gt;
  0x00007fbbb87a98a8 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/10&gt;#10&lt;/denchmark-link&gt;
 0x00007fbbb88cd876 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
&lt;denchmark-link:https://github.com/pytorch/pytorch/pull/11&gt;#11&lt;/denchmark-link&gt;
 0x00007fbbb86cfd26 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
&lt;denchmark-link:https://github.com/pytorch/pytorch/pull/12&gt;#12&lt;/denchmark-link&gt;
 0x00007fbbb86cff53 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/13&gt;#13&lt;/denchmark-link&gt;
 0x00007fbbb8815300 in cuLaunchKernel () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
&lt;denchmark-link:https://github.com/pytorch/pytorch/pull/14&gt;#14&lt;/denchmark-link&gt;
 0x00007fbbc2349441 in ?? () from /usr/lib/x86_64-linux-gnu/libcudnn.so.5
&lt;denchmark-link:https://github.com/pytorch/pytorch/pull/15&gt;#15&lt;/denchmark-link&gt;
 0x00007fbbc2364fed in ?? () from /usr/lib/x86_64-linux-gnu/libcudnn.so.5
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/16&gt;#16&lt;/denchmark-link&gt;
 0x00007fbbc1fc4c06 in ?? () from /usr/lib/x86_64-linux-gnu/libcudnn.so.5
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/17&gt;#17&lt;/denchmark-link&gt;
 0x00007fbbc1fc7739 in ?? () from /usr/lib/x86_64-linux-gnu/libcudnn.so.5
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/18&gt;#18&lt;/denchmark-link&gt;
 0x00007fbbc1fc84a8 in ?? () from /usr/lib/x86_64-linux-gnu/libcudnn.so.5
&lt;denchmark-link:https://github.com/pytorch/pytorch/pull/19&gt;#19&lt;/denchmark-link&gt;
 0x00007fbbc1fbfb58 in ?? () from /usr/lib/x86_64-linux-gnu/libcudnn.so.5
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/20&gt;#20&lt;/denchmark-link&gt;
 0x00007fbbc1e87041 in cudnnConvolutionBackwardFilter () from /usr/lib/x86_64-linux-gnu/libcudnn.so.5
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/21&gt;#21&lt;/denchmark-link&gt;
 0x00007fbbcc81e2f1 in bool perftools::gputools::cuda::CudnnSupport::DoConvolveBackwardFilterImpl(perftools::gputools::Stream*, int, perftools::gputools::dnn::BatchDescriptor const&amp;, perftools::gputools::DeviceMemory const&amp;, perftools::gputools::dnn::BatchDescriptor const&amp;, perftools::gputools::DeviceMemory, perftools::gputools::dnn::ConvolutionDescriptor const&amp;, perftools::gputools::dnn::FilterDescriptor const&amp;, perftools::gputools::DeviceMemory, perftools::gputools::dnn::AlgorithmConfig const&amp;, perftools::gputools::dnn::ProfileResult*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so
&lt;denchmark-link:https://github.com/pytorch/pytorch/pull/22&gt;#22&lt;/denchmark-link&gt;
 0x00007fbbcc81e894 in perftools::gputools::cuda::CudnnSupport::DoConvolveBackwardFilter(perftools::gputools::Stream*, perftools::gputools::dnn::BatchDescriptor const&amp;, perftools::gputools::DeviceMemory const&amp;, perftools::gputools::dnn::BatchDescriptor const&amp;, perftools::gputools::DeviceMemory, perftools::gputools::dnn::ConvolutionDescriptor const&amp;, perftools::gputools::dnn::FilterDescriptor const&amp;, perftools::gputools::DeviceMemory, perftools::gputools::dnn::AlgorithmConfig const&amp;, perftools::gputools::dnn::ProfileResult*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/23&gt;#23&lt;/denchmark-link&gt;
 0x00007fbbcc86e025 in perftools::gputools::Stream::ThenConvolveBackwardFilterWithAlgorithm(perftools::gputools::dnn::BatchDescriptor const&amp;, perftools::gputools::DeviceMemory const&amp;, perftools::gputools::dnn::BatchDescriptor const&amp;, perftools::gputools::DeviceMemory, perftools::gputools::dnn::ConvolutionDescriptor const&amp;, perftools::gputools::dnn::FilterDescriptor const&amp;, perftools::gputools::DeviceMemory, perftools::gputools::dnn::AlgorithmConfig const&amp;, perftools::gputools::dnn::ProfileResult*) ()
from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so
&lt;denchmark-link:https://github.com/pytorch/pytorch/pull/24&gt;#24&lt;/denchmark-link&gt;
 0x00007fbbcc60b350 in tensorflow::Conv2DSlowBackpropFilterOp&lt;Eigen::GpuDevice, float&gt;::Compute(tensorflow::OpKernelContext*) ()
from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/25&gt;#25&lt;/denchmark-link&gt;
 0x00007fbbcc723a12 in tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) ()
from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/26&gt;#26&lt;/denchmark-link&gt;
 0x00007fbbcc7645e3 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) ()
from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/27&gt;#27&lt;/denchmark-link&gt;
 0x00007fbbcc764e5a in std::_Function_handler&lt;void (), tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(tensorflow::gtl::InlinedVector&lt;tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8&gt; const&amp;, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::{lambda()&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/1&gt;#1&lt;/denchmark-link&gt;
}&gt;::_M_invoke(std::_Any_data const&amp;) ()
from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/28&gt;#28&lt;/denchmark-link&gt;
 0x00007fbbccaa8da0 in std::_Function_handler&lt;void (), Eigen::NonBlockingThreadPoolTempltensorflow::EigenEnvironment::NonBlockingThreadPoolTempl(int, tensorflow::EigenEnvironment)::{lambda()&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/1&gt;#1&lt;/denchmark-link&gt;
}&gt;::_M_invoke(std::_Any_data const&amp;) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/29&gt;#29&lt;/denchmark-link&gt;
 0x00007fbbccaa8050 in std::_Function_handler&lt;void (), tensorflow::EigenEnvironment::CreateThread(std::function&lt;void ()&gt;)::{lambda()&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/1&gt;#1&lt;/denchmark-link&gt;
}&gt;::_M_invoke(std::_Any_data const&amp;) ()
from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/30&gt;#30&lt;/denchmark-link&gt;
 0x00007fbbc976dc80 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/31&gt;#31&lt;/denchmark-link&gt;
 0x00007fbc29d686ba in start_thread (arg=0x7fb9a2ffd700) at pthread_create.c:333
		</comment>
		<comment id='11' author='ghost(ghost)' date='2017-08-15T03:17:50Z'>
		I've been going crazy chasing down a memory leak for the past few days, and I think it's the same issue here.
I'm using a Tesla K80 with Ubuntu 16.04, cuDNN 6, and PyTorch installed from source at &lt;denchmark-link:https://github.com/pytorch/pytorch/commit/60e7966c1f3bb50cd779044a6797159ba33ed4dc&gt;60e7966&lt;/denchmark-link&gt;
.
With 375.66 drivers, CPU memory usage steadily increases during training; however after installing 384.66 the problem went away. On a similarly configured machine with a Tesla P100 GPU, I saw no memory leak with 375.66.
		</comment>
		<comment id='12' author='ghost(ghost)' date='2017-08-15T05:54:03Z'>
		&lt;denchmark-link:https://github.com/jcjohnson&gt;@jcjohnson&lt;/denchmark-link&gt;
 I've had CPU memory leaks associated with NVIDIA drivers + GPUs for years. I've never been able to pinpoint the problem precisely, and it only happens in some unfortunate configurations of things. However, I've stopped seeing them after the release CUDA 7.0 (and I was so glad!).
If you can repro this in a docker container, with a particular GPU + driver version, the NVIDIA team might have some actionable items.
		</comment>
		<comment id='13' author='ghost(ghost)' date='2017-08-15T16:44:21Z'>
		&lt;denchmark-link:https://github.com/soumith&gt;@soumith&lt;/denchmark-link&gt;
 I'm (embarrassingly) not very adept with docker - can you even do this with Docker? If I've understood correctly, with vanilla Docker you must reinstall the NVIDIA driver inside the container and make sure it matches the version of the driver on the host; with nvidia-docker the driver from the host is used inside the container. Either way you need to install the NVIDIA driver on the host before starting the Docker container, so you cannot fully encapsulate the state of the system in a single Dockerfile.
However here are a few minimal shell scripts to reproduce the problem:
&lt;denchmark-link:https://gist.github.com/jcjohnson/4976067b503d8d1ebafb5eb6f38f9aae&gt;https://gist.github.com/jcjohnson/4976067b503d8d1ebafb5eb6f38f9aae&lt;/denchmark-link&gt;

I can reproduce on Google Compute Engine, starting from the public Ubuntu 16.04 image (ubuntu-1604-xenial-v20170811) and creating a machine with 4 vCPU, 16GB of RAM, 64 GB of standard persistent disk, and a single Tesla K80 GPU.
After creating the instance, run either install_driver_375.sh or install_driver_384.sh to install one of the drivers; this will also reboot the machine.
After the machine comes back up, the script run.sh will install PyTorch 0.2 (which ships with its own CUDA and cuDNN) using pip and run the script leak.py which reproduces the problem.
Using 375.66 drivers, CPU memory usage climbs from 1275 MB to 1574 MB over 10k forward / backward iterations, but with 384.66 drivers CPU memory usage stays at 1285 MB.
		</comment>
		<comment id='14' author='ghost(ghost)' date='2017-09-13T13:04:12Z'>
		Spoke to &lt;denchmark-link:https://github.com/ngimel&gt;@ngimel&lt;/denchmark-link&gt;
 about this, and if the issue is fixed in 384.xx, a patch is likely not going to be backported to 375.xx. Closing this because there's not much we can do.
		</comment>
		<comment id='15' author='ghost(ghost)' date='2017-10-12T13:29:30Z'>
		
if the issue is fixed in 384.xx,

I have 384.66, ubuntu 14, python 3.5 and it still leaks inside a docker (it seems that pytorch doesn't free memory when kernel is restarted).

restarting kernel doesn't help
restarting jupyter doesn't help
restartng docker container does help

I'm using latest pytorch/pytorch image + some minimal additional stuff over it.
		</comment>
	</comments>
</bug>