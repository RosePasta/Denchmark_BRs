<bug id='24082' author='rrkarim' open_date='2019-08-09T14:39:58Z' closed_time='2019-12-17T05:47:10Z'>
	<summary>Conv3D/Conv2d performance on Tesla arch. (half precision)</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I'm just testing the new release and observe the increased time consumption for the basic conv2d/conv3d layers. I'm more interested in float16 performance. Here is the simple script:
&lt;denchmark-code&gt;import time
import torch
import torch.nn as nn

dtype = torch.float32
device = torch.device("cuda:0")

mod = nn.Conv3d(64, 64, 3).to(device, dtype=dtype)
input_ = torch.randn((8, 64, 64, 64, 64), device=device, dtype=dtype)

curr = time.time()
torch.cuda.synchronize()

for i in range(100):
    mod(input_)

torch.cuda.synchronize()
print(time.time() - curr)
&lt;/denchmark-code&gt;

It takes 4.002 for float32 and 6.214 for float16.
UPD: testing now with the newest CUDA toolkit, will include the results here.
UPD2: Tested on py3.6_cuda10.0.130_cudnn7.6.2_0, the results are same.

PyTorch Version (e.g., 1.2.0):
OS (e.g., Linux):
How you installed PyTorch (conda):
CUDA/cuDNN version: 7.6.0
GPU models and configuration: RTX 2080 Ti

cc &lt;denchmark-link:https://github.com/ezyang&gt;@ezyang&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/gchanan&gt;@gchanan&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/zou3519&gt;@zou3519&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='rrkarim' date='2019-08-12T22:22:11Z'>
		cc: &lt;denchmark-link:https://github.com/ptrblck&gt;@ptrblck&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/csarofeen&gt;@csarofeen&lt;/denchmark-link&gt;

I wonder if it is nothing more than a CuDNN upgrade screwing up perf (it wont be the first time ;-) ). In this release we are using CuDNN 7.6.2.
&lt;denchmark-link:https://github.com/rrkarim&gt;@rrkarim&lt;/denchmark-link&gt;
 can you please try with ? That will tell us whether it's clearly CuDNN, or if we have to start digging elsewhere.
		</comment>
		<comment id='2' author='rrkarim' date='2019-08-12T22:32:44Z'>
		&lt;denchmark-link:https://github.com/rrkarim&gt;@rrkarim&lt;/denchmark-link&gt;
 Could you swap these lines:
curr = time.time()
torch.cuda.synchronize()
# to
torch.cuda.synchronize()
curr = time.time()
as you are currently also timing the initialization of the layers?
If you are running with torch.backends.cudnn.benchmark = True, could you please add some warmup iterations (10-50) before starting the real profiling?
		</comment>
		<comment id='3' author='rrkarim' date='2019-08-13T09:50:14Z'>
		&lt;denchmark-link:https://github.com/ptrblck&gt;@ptrblck&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/soumith&gt;@soumith&lt;/denchmark-link&gt;
 it is the same.
&lt;denchmark-code&gt;import time
import torch
import torch.nn as nn

torch.backends.cudnn.benchmark=True

dtype = torch.float32
device = torch.device("cuda:0")

mod = nn.Conv3d(64, 64, 3).to(device, dtype=dtype)
input_ = torch.randn((8, 64, 64, 64, 64), device=device, dtype=dtype)

for i in range(40):
    mod(input_)

torch.cuda.synchronize()
curr = time.time()

for i in range(100):
    mod(input_)

torch.cuda.synchronize()
print(time.time() - curr)

&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='rrkarim' date='2019-08-13T09:55:15Z'>
		Setting kernel size to one (mm) I can witness increase in performance.
		</comment>
		<comment id='5' author='rrkarim' date='2019-08-16T15:09:22Z'>
		I've profiled the convolution using &lt;denchmark-link:https://gist.github.com/ptrblck/cd38dbdd0570741aa44f2725ccbc1288&gt;this code&lt;/denchmark-link&gt;
.
Results:
FP32:




cudnn=True, benchmark=True
cudnn=True, benchmark=False
cudnn=False




V100
3.355
3.364
6.198


Titan V
3.551
3.550
6.512


P100
5.483
5.477
8.966



FP16:




cudnn=True, benchmark=True
cudnn=True, benchmark=False
cudnn=False




V100
3.516
3.523
3.200


Titan V
4.089
4.092
3.217


P100
5.456
5.456
12.132



While the FP32 results look reasonable, the FP16 results look indeed fishy.
We'll perform some internal tests and try to debug this issue.
		</comment>
		<comment id='6' author='rrkarim' date='2019-11-23T03:45:56Z'>
		&lt;denchmark-link:https://github.com/rrkarim&gt;@rrkarim&lt;/denchmark-link&gt;
 Could you provide some additional workloads (shapes) which would be important for your use cases?
		</comment>
		<comment id='7' author='rrkarim' date='2019-11-23T12:47:54Z'>
		&lt;denchmark-link:https://github.com/ptrblck&gt;@ptrblck&lt;/denchmark-link&gt;
 not sure about the use case I was trying to work on, since I have posted the issue long time ago and switched to other problems. But the example script above (first message of the thread) is pretty good use case for me. I don't have an access to a Turing arch. for now, therefore I can not test the performance. If everything is solved, you can close the issue.
		</comment>
		<comment id='8' author='rrkarim' date='2019-12-01T04:30:00Z'>
		This issue should be solved in cudnn 7.6.5 as some heuristics were improved and we see these numbers on a V100 for FP16 now:




cudnn=True, benchmark=True
cudnn=True, benchmark=False
cudnn=False




V100
0.7892
0.7894
2.9869


Titan V
0.8612
0.8611
3.4629



&lt;denchmark-link:https://github.com/rrkarim&gt;@rrkarim&lt;/denchmark-link&gt;
 You could build from source using the latest cudnn release to get these improvements.
Also, please let me know, if you see any issues.
		</comment>
		<comment id='9' author='rrkarim' date='2019-12-01T21:08:23Z'>
		&lt;denchmark-link:https://github.com/ptrblck&gt;@ptrblck&lt;/denchmark-link&gt;
 thanks, I will check it ASAP.
		</comment>
		<comment id='10' author='rrkarim' date='2019-12-17T05:47:10Z'>
		Closing, feel free to reopen if the issue persists.
		</comment>
	</comments>
</bug>