<bug id='16610' author='leonardblier' open_date='2019-01-31T16:21:58Z' closed_time='2020-01-27T15:56:49Z'>
	<summary>Potential instability in ConvTranspose2d with cudnn</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Together with &lt;denchmark-link:https://github.com/fmassa&gt;@fmassa&lt;/denchmark-link&gt;
, we observed a potential instability when I run the following snippet with CUDNN. Without CUDNN, it works as expected.
It's also worth noticing that this used to work on Pytorch 0.4.1, and that it works as expected on CPU as well.
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

Run the code. It requires a small pickle file containing one single minibatch of data (replacing it by a random tensor did not reproduce the problem). The file is linked here.
&lt;denchmark-link:https://github.com/pytorch/pytorch/files/2818110/minibatch.pkl.zip&gt;minibatch.pkl.zip&lt;/denchmark-link&gt;

import torch
from torch import optim
import torch.nn as nn
from torch.nn import functional as F
import pickle as pkl

torch.manual_seed(1)


class Decoder(nn.Module):
    """ VAE decoder """
    def __init__(self, img_channels, latent_size):
        super(Decoder, self).__init__()
        self.latent_size = latent_size
        self.img_channels = img_channels

        self.fc1 = nn.Linear(latent_size, 1024)
        self.deconv1 = nn.ConvTranspose2d(1024, 128, 5, stride=2)
        self.deconv2 = nn.ConvTranspose2d(128, 64, 5, stride=2)
        self.deconv3 = nn.ConvTranspose2d(64, 32, 6, stride=2)
        self.deconv4 = nn.ConvTranspose2d(32, img_channels, 6, stride=2)

    def forward(self, x): 
        x = F.relu(self.fc1(x))
        x = x.unsqueeze(-1).unsqueeze(-1)
        x = F.relu(self.deconv1(x))
        x = F.relu(self.deconv2(x))
        x = F.relu(self.deconv3(x))
        reconstruction = torch.sigmoid(self.deconv4(x))
        return reconstruction

torch.backends.cudnn.enabled = False               
cuda = torch.cuda.is_available()
device = torch.device("cuda" if cuda else "cpu")

decoder = Decoder(3, 32).to(device)
decoder.train()
optimizer = optim.Adam(decoder.parameters())

with open('minibatch.pkl', 'rb') as f:
    data = pkl.load(f)

for batch_idx in range(500):
    data = data.to(device)
    mu = torch.randn((32, 32), device=device)
    recon_batch = decoder(mu)
    loss = .5 * (recon_batch - data).pow(2).sum(dim=(1,2,3)).sum()
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if batch_idx % 20 == 0:
        print(f'Loss: {loss.item()/len(data):.6f}')
If the line torch.backends.cudnn.enabled = False is not commented (and thus if I do not use CUDNN), the output is
&lt;denchmark-code&gt;Loss: 412.374939
Loss: 105.106758
Loss: 60.929550
Loss: 56.290668
Loss: 53.807034
Loss: 51.205917
Loss: 50.837688
Loss: 50.152969
Loss: 49.427326
Loss: 49.931602
Loss: 49.682564
Loss: 49.036819
&lt;/denchmark-code&gt;

If I comment that that line (which means that I am using CUDNN), the output is:
&lt;denchmark-code&gt;Loss: 412.365082
Loss: 159.593903
Loss: 139.744904
Loss: 180.208984
Loss: 216.828827
Loss: 235.451126
Loss: 242.588562
Loss: 243.361099
Loss: 248.088272
Loss: 259.658417
Loss: 307.006592
Loss: 329.257568
Loss: 342.992310
Loss: 345.325562
Loss: 391.979645
Loss: 403.900879
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

The two executions of the code (with and without CUDNN) should output approximately the same loss values.
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

PyTorch version: 1.0.0
Is debug build: No
CUDA used to build PyTorch: 9.0.176
OS: Ubuntu 16.04.4 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.5.1
Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 9.2.88
GPU models and configuration:
GPU 0: Quadro GP100
GPU 1: Quadro GP100
Nvidia driver version: 396.51
cuDNN version: Could not collect
Versions of relevant libraries:
[pip] Could not collect
[conda] blas                      1.0                         mkl
[conda] cuda92                    1.0                           0    pytorch
[conda] mkl                       2019.1                      144
[conda] mkl_fft                   1.0.10           py37h14c3975_1    conda-forge
[conda] mkl_random                1.0.2            py37h637b7d7_2    conda-forge
[conda] pytorch                   1.0.0           py3.7_cuda9.0.176_cudnn7.4.1_1    pytorch
[conda] torchvision               0.2.1                      py_2    pytorch
&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='leonardblier' date='2019-01-31T16:25:42Z'>
		Do you know what version of cuDNN this was run with? I see the collect env script failed to figure out this information. Is this our bindist?
		</comment>
		<comment id='2' author='leonardblier' date='2019-01-31T16:27:46Z'>
		cudnn 7.4.1, as shown in the conda binary string for pytorch pytorch 1.0.0 py3.7_cuda9.0.176_cudnn7.4.1_1 pytorch
		</comment>
		<comment id='3' author='leonardblier' date='2019-01-31T18:21:21Z'>
		apparently a  workaround is to use . &lt;denchmark-link:https://github.com/ngimel&gt;@ngimel&lt;/denchmark-link&gt;
 is looking if a fix is possible. We'll try our best to get it into v1.0.1
		</comment>
		<comment id='4' author='leonardblier' date='2019-02-01T14:22:08Z'>
		Thank's &lt;denchmark-link:https://github.com/soumith&gt;@soumith&lt;/denchmark-link&gt;
 , it seems to work like this, and it's faster than without cudnn.
		</comment>
		<comment id='5' author='leonardblier' date='2019-03-20T06:44:25Z'>
		Would you please retry this with cuDNN version 7.5 or greater?
		</comment>
		<comment id='6' author='leonardblier' date='2020-01-27T09:57:51Z'>
		I will try to reproduce with 7.4 and newer versions
		</comment>
		<comment id='7' author='leonardblier' date='2020-01-27T15:56:48Z'>
		This was first worked around in &lt;denchmark-link:https://github.com/pytorch/pytorch/pull/17016&gt;#17016&lt;/denchmark-link&gt;
 and then fixed in later versions of cudnn. Closing this.
		</comment>
	</comments>
</bug>