<bug id='943' author='makarandtapaswi' open_date='2017-03-07T01:34:07Z' closed_time='2017-03-12T17:31:41Z'>
	<summary>nn.Embedding with max_norm acting strange?</summary>
	<description>
Hi,
I was trying to use the max-norm feature of  nn.Embedding and noticed that it seems to change the way indexing works.
So, this is correct:
A = nn.Embedding(20, 5)
print A(Variable(torch.LongTensor([[1, 2], [1, 2]])))
&lt;denchmark-code&gt;Variable containing:
(0 ,.,.) = 
 -0.2591 -0.3065  0.1560 -1.2476 -0.6129
  0.4282  1.8142 -0.3061  0.6822  0.0460

(1 ,.,.) = 
 -0.2591 -0.3065  0.1560 -1.2476 -0.6129
  0.4282  1.8142 -0.3061  0.6822  0.0460
[torch.FloatTensor of size 2x2x5]
&lt;/denchmark-code&gt;

and this happens when using max_norm=1:
A = nn.Embedding(20, 5, max_norm=1)
print A(Variable(torch.LongTensor([[1, 2], [1, 2]])))
&lt;denchmark-code&gt;Variable containing:
(0 ,.,.) = 
 -0.7178 -0.1059  0.4266 -0.2857  0.4581
 -0.5032  0.1199  0.6023 -0.2505  0.5540

(1 ,.,.) = 
 -0.5032  0.1199  0.6023 -0.2505  0.5540
 -0.5032  0.1199  0.6023 -0.2505  0.5540
[torch.FloatTensor of size 2x2x5]
&lt;/denchmark-code&gt;

As seen, the [1, 0]th index seems to load embedding corresponding to 2 instead of 1 as requested.
Am I misunderstanding something?
	</description>
	<comments>
	</comments>
</bug>