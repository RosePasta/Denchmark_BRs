<bug id='15213' author='norahsakal' open_date='2018-12-14T09:45:34Z' closed_time='2019-01-17T20:33:13Z'>
	<summary>Pytorch 1.0.0 with Serverless and AWS Lambda timeout; Error in cpuinfo</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I am running an image classification model with Serverless and AWS Lambda. Receiving following error upon calling the Serverless function;  - I did not misspell this, the spelling is copied from the error message I received and appear with the same spelling here;
&lt;denchmark-link:https://github.com/pytorch/cpuinfo/blob/master/src/linux/processors.c&gt;https://github.com/pytorch/cpuinfo/blob/master/src/linux/processors.c&lt;/denchmark-link&gt;

Suspected it might be something with Pytorch 1.0.0 so I switched back to Pytorch 0.4.1 which made it work.
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:


Run Pytorch in a Lambda with following whl in the requirements.txt;
Pillow==5.3.0
PyYAML==3.13
https://download.pytorch.org/whl/cpu/torch-1.0.0-cp36-cp36m-linux_x86_64.whl
torchvision==0.2.1


Load optional model and predict an image, this error should arise; "Error in cpuinfo: failed to parse the list of present procesors in /sys/devices/system/cpu/present"
and you will also get a timeout from the Lambda, regardless of how long timeout that is chosen; "Process exited before completing request"


No stack trace available, I only receive "Error in cpuinfo: failed to parse the list of present procesors in /sys/devices/system/cpu/present"
and a timeout error; "Process exited before completing request"
&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

Expected image prediction from the model every time a new image is uploaded.
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version (e.g., 1.0): 1.0.0
OS (e.g., Linux): Linux
How you installed PyTorch (conda, pip, source): pip3
Build command you used (if compiling from source):
Python version: 3.6
CUDA/cuDNN version: No CUDA
GPU models and configuration: No GPU
Any other relevant information: Deployed with Serverless and AWS Lambda, 3008 GB memory and 30 second timeout, works with Pytorch 0.4.1

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

Suspected it might be something with Pytorch 1.0.0 so I switched back to Pytorch 0.4.1 which made the image prediction work.
Let me know if I can provide any additional information.
	</description>
	<comments>
		<comment id='1' author='norahsakal' date='2018-12-17T16:44:59Z'>
		once the bug is fixed on the cpuinfo repo, we can update the submodule, and you can get the fix either from our nightly builds, or we'll push it into 1.0.1
		</comment>
		<comment id='2' author='norahsakal' date='2018-12-17T21:13:11Z'>
		Thanks &lt;denchmark-link:https://github.com/soumith&gt;@soumith&lt;/denchmark-link&gt;
!
		</comment>
		<comment id='3' author='norahsakal' date='2018-12-21T02:08:51Z'>
		&lt;denchmark-link:https://github.com/norahsakal&gt;@norahsakal&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/bendidi&gt;@bendidi&lt;/denchmark-link&gt;
: could you test on a nightly or master build of PyTorch and tell us if its fixed? The nightly builds are here: &lt;denchmark-link:https://anaconda.org/pytorch/pytorch-nightly&gt;https://anaconda.org/pytorch/pytorch-nightly&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='norahsakal' date='2018-12-21T11:05:51Z'>
		Hi &lt;denchmark-link:https://github.com/Maratyszcza&gt;@Maratyszcza&lt;/denchmark-link&gt;
 , still get the same Error :
&lt;denchmark-link:https://user-images.githubusercontent.com/22003440/50339729-68603400-0518-11e9-89b7-bfcf6a2140ab.png&gt;&lt;/denchmark-link&gt;

the version of pytorch I'm installing :
Step 8/37 : RUN pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html
 ---&gt; Running in fe6c1bc6796e
Looking in links: https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html
Collecting torch_nightly
  Downloading https://download.pytorch.org/whl/nightly/cpu/torch_nightly-1.0.0.dev20181220-cp36-cp36m-linux_x86_64.whl (88.6MB)
Installing collected packages: torch-nightly
Successfully installed torch-nightly-1.0.0.dev20181220

numpy==1.11
torchvision_nightly==0.2.1

		</comment>
		<comment id='5' author='norahsakal' date='2018-12-21T11:44:01Z'>
		&lt;denchmark-link:https://github.com/bendidi&gt;@bendidi&lt;/denchmark-link&gt;
: cpuinfo reports the error, but now it has a work-around (I should probably downgrade it to a warning). Is anything wrong besides error reported on the log?
		</comment>
		<comment id='6' author='norahsakal' date='2019-01-17T20:33:13Z'>
		fixed via &lt;denchmark-link:https://github.com/pytorch/pytorch/pull/16107&gt;#16107&lt;/denchmark-link&gt;

Patch will be part of v1.0.1 release and also part of tomorrow's nightly builds.
		</comment>
		<comment id='7' author='norahsakal' date='2019-02-25T23:18:17Z'>
		&lt;denchmark-link:https://github.com/norahsakal&gt;@norahsakal&lt;/denchmark-link&gt;
 did you manage to get this to work? Am interested to know how you got your package size under the AWS Lambda limit of 250 MB. Already PyTorch (216 MB) + Numpy (63 MB) is more than the limit
		</comment>
		<comment id='8' author='norahsakal' date='2019-02-26T09:17:47Z'>
		Hi &lt;denchmark-link:https://github.com/mattdawkins&gt;@mattdawkins&lt;/denchmark-link&gt;
, yes cpuinfo errors are fixed  (thx &lt;denchmark-link:https://github.com/Maratyszcza&gt;@Maratyszcza&lt;/denchmark-link&gt;
 )
for the aws limits, one way to do it is to build your env in a docker file, package it in a zip file and save it to s3, and for each lambda call, your script will download the zip packages, unzip it (in /tmp/) and then sys.append(path/to/req),
the docker can be something like this :
&lt;denchmark-code&gt;FROM lambci/lambda:build-python3.6

# install all your python dependencies (i used numpy 1.11 for it's smaller size) 
RUN pip install numpy==1.11.3 
COPY requirements.txt requirements.txt
RUN pip install -r requirements.txt

# install pytorch and torchvision
RUN pip install https://download.pytorch.org/whl/cpu/torch-1.0.1-cp36-cp36m-linux_x86_64.whl
RUN pip install torchvision==0.2.1

# copy site-packages to /deps
RUN mkdir -p /deps/
RUN cp -a -r /var/lang/lib/python3.6/site-packages/. /deps

# some cleaning for smaller footprint
RUN du -sch /deps
RUN find /deps/. -type d -name "tests" -exec rm -rf {} +
RUN rm -rf /deps/{caffe2,wheel,pkg_resources,boto*,aws*,pip,pipenv}
RUN rm /deps/torch/lib/libtorch.so
RUN rm -rf /deps/{*.egg-info,*.dist-info}
RUN find /deps/. -name \*.pyc -delete
RUN du -sch /deps

# zip it 
RUN cd /deps &amp;&amp; zip -9 -q -r /pytorch1.0.1_lambda_deps.zip .

# save it in s3 : 
RUN pip install awscli
ARG AWS_ACCESS_KEY_ID=""
ARG AWS_SECRET_ACCESS_KEY=""
RUN aws s3 cp /pytorch1.0.1_lambda_deps.zip s3://YOUR_BUCKET/pytorch1.0.1_lambda_deps.zip
&lt;/denchmark-code&gt;

in your code it will then be just :
import os
import shutil
import sys
import zipfile
import boto3

s3 = boto3.client('s3')

zip_requirements = "/tmp/pytorch1.0.1_lambda_deps.zip"
pkgdir = '/tmp/sls-py-req'
sys.path.append(pkgdir)
if not os.path.exists(pkgdir):
    tempdir = '/tmp/_temp-sls-py-req'
    if os.path.exists(tempdir):
        shutil.rmtree(tempdir)

    s3.download_file(
                YOUR_BUCKET,"pytorch1.0.1_lambda_deps.zip", zip_requirements)
    zipfile.ZipFile(zip_requirements, 'r').extractall(tempdir)
    os.remove(zip_requirements)
    os.rename(tempdir, pkgdir)  # Atomic
print("Deps extracted successfully !")
personnaly I package pytorch+numpy=1.11.3 directly with the lambda to take advantage of the 250Mb availlable, and download the rest of the requirement as above.
hope it helps !
		</comment>
		<comment id='9' author='norahsakal' date='2019-02-26T10:43:34Z'>
		one problem that I've run into &lt;denchmark-link:https://github.com/soumith&gt;@soumith&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Maratyszcza&gt;@Maratyszcza&lt;/denchmark-link&gt;
 if you have any idea or advice to solve it, is that   the gcc in aws lambda runtime in  while to use cpp extensions with pytorch the minimum version required for gcc is  (cf &lt;denchmark-link:https://github.com/pytorch/pytorch/blob/master/torch/utils/cpp_extension.py#L53&gt;here&lt;/denchmark-link&gt;
 ), I'm trying to run &lt;denchmark-link:https://github.com/facebookresearch/maskrcnn-benchmark&gt;maskrcnn-benchmark&lt;/denchmark-link&gt;
 models in lambda and it uses cpp scripts (if you have any idea &lt;denchmark-link:https://github.com/fmassa&gt;@fmassa&lt;/denchmark-link&gt;
  too) , the error I get is :
Unable to import module 'lambda/inference': /usr/lib64/libstdc++.so.6: version` GLIBCXX_3.4.20' not found (required by /var/task/lambda/lib/maskrcnn_benchmark/_C.cpython-36m-x86_64-linux-gnu.so)
some of the things I tried is to package libstdc++.so.6 with maskrcnn-benchmark and add
extra_compile_args = {"cxx": [
        '-Wl,-rpath,$ORIGIN'
    ]}
to setup of maskrcnn-benchmark , but _C.cpython-36m-x86_64-linux-gnu.so still looks for system libstdc++.so.6 instead of teh one in $ORIGIN
thank you
		</comment>
		<comment id='10' author='norahsakal' date='2019-02-26T18:28:29Z'>
		
@norahsakal did you manage to get this to work? Am interested to know how you got your package size under the AWS Lambda limit of 250 MB. Already PyTorch (216 MB) + Numpy (63 MB) is more than the limit

Hej! &lt;denchmark-link:https://github.com/mattmcclean&gt;@mattmcclean&lt;/denchmark-link&gt;
 I just tried it out, and it works with PyTorch 1.0.0
A trick to reduce the size of the packages (except for the excellent explanation by &lt;denchmark-link:https://github.com/bendidi&gt;@bendidi&lt;/denchmark-link&gt;
) is to use the following in your ;
&lt;denchmark-code&gt;https://download.pytorch.org/whl/cpu/torch-1.0.0-cp36-cp36m-linux_x86_64.whl
&lt;/denchmark-code&gt;

This PyTorch Linux binary is without CUDA and is therefore, &lt;100 MB
Visit; &lt;denchmark-link:https://www.npmjs.com/package/serverless-python-requirements#dealing-with-lambdas-size-limitations&gt;dealing-with-lambdas-size-limitations&lt;/denchmark-link&gt;
 to read about how to compress the libraries, it only requires a minor change to your  code;
&lt;denchmark-code&gt;custom:
  pythonRequirements:
    zip: true
&lt;/denchmark-code&gt;

And then the following at the top of your handler module;
&lt;denchmark-code&gt;try:
  import unzip_requirements
except ImportError:
  pass
&lt;/denchmark-code&gt;

My entire package file is currently 98.11 MB + 86.78 MB and includes all of this;
&lt;denchmark-code&gt;Pillow==5.3.0
PyYAML==3.13
https://download.pytorch.org/whl/cpu/torch-1.0.0-cp36-cp36m-linux_x86_64.whl
torchvision==0.2.1
&lt;/denchmark-code&gt;

For additional PyTorch Linux binaries without CUDA visit; &lt;denchmark-link:https://pytorch.org/get-started/previous-versions/&gt;https://pytorch.org/get-started/previous-versions/&lt;/denchmark-link&gt;

I hope it'll work for you!
		</comment>
		<comment id='11' author='norahsakal' date='2019-02-26T22:01:25Z'>
		Thanks &lt;denchmark-link:https://github.com/norahsakal&gt;@norahsakal&lt;/denchmark-link&gt;
 &amp; &lt;denchmark-link:https://github.com/bendidi&gt;@bendidi&lt;/denchmark-link&gt;
. Got it working  Created a Lambda layer with the zipped   file (with PyTorch + associated libs) and y file so can be reused by other Lambda functions.
Now super easy as I only need the  import unzip_requirements statement at the beginning of the file
		</comment>
		<comment id='12' author='norahsakal' date='2019-02-26T22:07:42Z'>
		&lt;denchmark-link:https://github.com/bendidi&gt;@bendidi&lt;/denchmark-link&gt;
 I'm curious about this line:

for each lambda call, your script will download the zip packages, unzip it (in /tmp/) and then sys.append(path/to/req),

Doesn't that add a lot of overhead, if it's for each user side request?
		</comment>
		<comment id='13' author='norahsakal' date='2019-02-26T22:23:25Z'>
		&lt;denchmark-link:https://github.com/soumith&gt;@soumith&lt;/denchmark-link&gt;
 usually when the lambda is "hot" it will be called just one time per batch-of-requests (as the zip file will stay in /tmp/ and can be reused by subsequent calls), so it is generally just downloaded for the first few parallel calls and reused for subsequent calls when the first ones finish.
Another thing is I try to keep the deps package as small as possible normally (30Mb in my case) and with the download speed in aws Lambda it's quite fast and doesn't impact much the performance (knowing that the inference on an image in my case takes 30-40 seconds, so it's pretty much negligible)
&lt;denchmark-link:https://github.com/mattdawkins&gt;@mattdawkins&lt;/denchmark-link&gt;
 cool ! that was my plan too(after fixing the bug above  ) , it's probably the best way to do it !
		</comment>
		<comment id='14' author='norahsakal' date='2019-02-26T22:24:12Z'>
		&lt;denchmark-link:https://github.com/soumith&gt;@soumith&lt;/denchmark-link&gt;
 no you don't need to do this. I modified mine to bundle a zip inside another zip to get around the 250 MB problem so there is no need to download packages when the Lambda execution context is created. All it needs to do is unzip an 80 MB zip file contained in the Lambda layer using the  script. As long as you do this at the beginning of the script and not in the lambda handler method then it is done only once on execution context creation.
For subsequent invocations there is no penalty and the PyTorch model is already loaded in memory.  I will create a project and share soon..
		</comment>
		<comment id='15' author='norahsakal' date='2019-02-26T22:47:42Z'>
		
Thanks @norahsakal &amp; @bendidi. Got it working üòÑ Created a Lambda layer with the zipped .requirements.zip file (with PyTorch + associated libs) and unzip_requirements.py file so can be reused by other Lambda functions.
Now super easy as I only need the  import unzip_requirements statement at the beginning of the file

I'm glad the solution I suggested worked out for you &lt;denchmark-link:https://github.com/mattmcclean&gt;@mattmcclean&lt;/denchmark-link&gt;

		</comment>
		<comment id='16' author='norahsakal' date='2019-03-04T08:49:56Z'>
		I have made a Docker image that runs a shell script and packages PyTorch 1.0.1 and all the necessary packages that come with it (Pillow, numpy, etc). The total package exceeds the uncompressed size of 250MB but if you use the strip command it will reduce the size to around 230MB and it fits.
You can find the scripts here:
&lt;denchmark-link:https://github.com/Con-Mi/data_science_aws_lambda_packs/tree/master/pytorch1.0.1&gt;https://github.com/Con-Mi/data_science_aws_lambda_packs/tree/master/pytorch1.0.1&lt;/denchmark-link&gt;

		</comment>
		<comment id='17' author='norahsakal' date='2019-03-04T10:00:04Z'>
		Created an example project here showing how to deploy using &lt;denchmark-link:https://github.com/awslabs/serverless-application-model&gt;Serverless Application Model (SAM)&lt;/denchmark-link&gt;
 which deploys your AWS Lambda functions + API GW easily here - &lt;denchmark-link:https://github.com/mattmcclean/sam-pytorch-example&gt;https://github.com/mattmcclean/sam-pytorch-example&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>