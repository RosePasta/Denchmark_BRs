<bug id='13039' author='jendrikjoe' open_date='2018-10-24T08:39:14Z' closed_time='2019-12-07T20:10:21Z'>
	<summary>Crash when reading pandas parquet file after importing pyTorch</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When importing PyTorch and reading a parquet file with pandas the script randomly fails with a segmentation fault. This problem only occurs when importing PyTorch and seems to be related to  &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/11326&gt;#11326&lt;/denchmark-link&gt;
 as &lt;denchmark-link:https://github.com/pytorch/pytorch/pull/12739&gt;#12739&lt;/denchmark-link&gt;
 seems to resolve it (at least I am not able to produce the error anymore).
Steps to reproduce the behavior:
Install PyTorch not effected by &lt;denchmark-link:https://github.com/pytorch/pytorch/pull/12739&gt;#12739&lt;/denchmark-link&gt;
.
A minimal example for producing the error is:
&lt;denchmark-code&gt;import torch
import pandas as pd 

if __name__ == '__main__':
    df = pd.read_parquet('summary.par')
    print('Done')
&lt;/denchmark-code&gt;

The data can be found here: &lt;denchmark-link:https://s3-eu-west-1.amazonaws.com/segfault.torch.public.bucket/summary.par&gt;https://s3-eu-west-1.amazonaws.com/segfault.torch.public.bucket/summary.par&lt;/denchmark-link&gt;

Unluckily I was never able to fetch a stacktrace for this bug. I ran a script to run the code thousands of times in gdb, but it never fails when using gdb, even though it fails around 50% of the times when normally executing it...
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

PyTorch version: 1.0.0a0+17c6d16  --&gt; was 0.4.1 with the error
Is debug build: No
CUDA used to build PyTorch: 10.0.130
OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
CMake version: version 3.11.0
Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: Quadro P5000
Nvidia driver version: 410.48
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.3
/usr/lib/x86_64-linux-gnu/libcudnn_static.a
/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn.so.7.3.1 &lt;-- This is the one used
/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn_static.a
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7.2.1
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn_static.a
Versions of relevant libraries:
[pip] Could not collect  -&gt; Is 18.1
[conda] magma-cuda90              2.3.0                         1    pytorch
[conda] magma-cuda91              2.3.0                         1    pytorch
[conda] magma-cuda92              2.3.0                         1    pytorch
[conda] torch                     1.0.0a0+17c6d16             --&gt; was 0.4.1 with the error
I hope this helps resolving &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/11326&gt;#11326&lt;/denchmark-link&gt;
 and pinpointing down its fault.
Cheers,
Jendrik
	</description>
	<comments>
		<comment id='1' author='jendrikjoe' date='2018-10-29T17:55:50Z'>
		This is hi-pri to repro this. Once we figure out the cause we can reprioritize it.
		</comment>
		<comment id='2' author='jendrikjoe' date='2018-11-12T03:27:25Z'>
		I met the same segment fault error. I use pyarrow engine for pandas to read parquet files.
I found that if I install pyarrow with conda install, it works well (with import torch).
However, if I install pyarrow with pip install, I will see the segment fault error if I import torch and try to read parquet. Note that solely read_parquet works well if not import torch. The problem exists for cpu version too. I tried to use gdb; the backtrace showed some kind of share_ptr problem, which I do not have knowledge to debug that.
It can be reproduced with a virtualenv, python2 or python3 "pip install torch pandas pyarrow".
		</comment>
		<comment id='3' author='jendrikjoe' date='2019-01-09T09:06:54Z'>
		To further simplify the repro example, for me it segfaults on trivial files:
import torch
import pandas as pd

pd.DataFrame({'a': [1]}).to_parquet('crash')
pd.read_parquet('crash')
pyarrow 0.11.1
pytorch 0.4.1 &amp; 1.0.0 &amp; 1.0.0.dev20190108
		</comment>
		<comment id='4' author='jendrikjoe' date='2019-02-21T10:40:18Z'>
		Any news on this? My only workaround is to load data then load pytorch. Things get tricky writing data out to parquet as well, had to revert to CSV/pickled files. I tried the mentionned workaround of installing pyarrow via conda, but that led to the problem described here: &lt;denchmark-link:https://github.com/pandas-dev/pandas/issues/24976&gt;pandas-dev/pandas#24976&lt;/denchmark-link&gt;
.
Tried pyarrow 0.11.0, 0.12.0.
		</comment>
		<comment id='5' author='jendrikjoe' date='2019-02-21T20:24:18Z'>
		For what its worth, feather is not affected by this, and is a decent format (at least better than csv).
		</comment>
		<comment id='6' author='jendrikjoe' date='2019-04-08T13:38:36Z'>
		This issue still continues.
PyTorch: 1.0.1.post2
Python: 3.6.5
Pyarrow: 0.12.0
CentOS7
import torch
import pandas as pd
df = pd.read_parquet('./test.parquet', engine='pyarrow')
Segmentation fault
Note: Pyarrow: 0.13.0 also crashes with keras &amp; tensorflow
import pyarrow
import tensorflow
		</comment>
		<comment id='7' author='jendrikjoe' date='2019-04-09T18:55:20Z'>
		&lt;denchmark-link:https://github.com/selimarikan&gt;@selimarikan&lt;/denchmark-link&gt;
 That's an interesting data point, that pyarrow crashes with tensorflow too. Could this be a packaging problem with pyarrow's pip binaries?
		</comment>
		<comment id='8' author='jendrikjoe' date='2019-04-09T21:16:55Z'>
		&lt;denchmark-link:https://github.com/ezyang&gt;@ezyang&lt;/denchmark-link&gt;
 tensorflow segfault happens with the latest (0.13.0) version of pyarrow but not with previous (0.12.0).
Some people claim that if they use pyarrow conda package, it works. But then pandas cannot find pyarrow.
I had a similar PyTorch segfault issue with some other package (hnswlib), I solved it by building hnswlib locally with relatively newer gcc (4.9) compared to the provided package (4.6).
So probably the problem is clashing libraries that are built with different compiler versions. I guess both PyTorch and pyarrow is loading to global namespace and that causes the issue.
A possible workaround is using fastparquet instead of pyarrow, which is what I am doing at the moment and it works without problems.
import torch
import pandas as pd
df = pd.read_parquet('./test.parquet', engine='fastparquet')
		</comment>
		<comment id='9' author='jendrikjoe' date='2019-04-09T23:02:02Z'>
		this is a packaging issue. There's a long context in the following references:

apache/arrow#3177
https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/TMqRaT-H2bI/overview
https://docs.google.com/document/d/1uYZK2jQtDUPpo3AHe18ZCH1jS9be9s8zR3axLR1SOG0/edit#heading=h.7sjot6x53yvw
https://discuss.python.org/t/the-next-manylinux-specification/1043

		</comment>
		<comment id='10' author='jendrikjoe' date='2019-04-09T23:03:13Z'>
		the known working solution is to use conda for pandas, arrow, pyarrow, pytorch, tensorflow because conda doesn't have this C++ABI problem (conda ships with a standardized environment even for C++ bits)
		</comment>
		<comment id='11' author='jendrikjoe' date='2019-07-12T20:20:18Z'>
		uninstall the pyarrow installed by pip and then reinstall with conda works for me.
		</comment>
		<comment id='12' author='jendrikjoe' date='2019-12-07T20:10:20Z'>
		Here is a complete reproducer including writing a file so one doesn't need to download anything as in the issue description:
&lt;denchmark-code&gt;import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq


df = pd.DataFrame({'one': [-1, np.nan, 2.5],
                   'two': ['foo', 'bar', 'baz'],
                   'three': [True, False, True]},
                   index=list('abc'))

table = pa.Table.from_pandas(df)

# Write a test file
pq.write_table(table, 'example.parquet')

# Read the same file back in
table2 = pq.read_table('example.parquet')

# Also read it in with Pandas (as that's what one of the examples in
# https://github.com/pytorch/pytorch/issues/13039 does
df2 = pd.read_parquet('example.parquet', engine='pyarrow')
&lt;/denchmark-code&gt;

To see the segfault, save the above reproducer as torch_issue_13039.py and do the following:
&lt;denchmark-code&gt;$ conda create -n pytorch-issue13039 python=3.7
$ conda activate pytorch-issue13039
$ pip install pyarrow==0.13.0
$ pip install pandas==0.25.3
$ pip install pytorch==1.3.1
$ python torch_issue_13039.py 
Segmentation fault (core dumped)
&lt;/denchmark-code&gt;

This is quite reproducible, but not 100%. If it doesn't happen, run the last command a few times. For me the segfault happened about 80% of the time.
Pyarrow 0.14.0 wheels still have this issue, but it was fixed for 0.14.1 and remains fixed in 0.15.1 (most recent). To confirm that:
&lt;denchmark-code&gt;$ pip install pyarrow==0.14.1
$ python torch_issue_13039.py  # run multiple times, it won't segfault
&lt;/denchmark-code&gt;

Here is a blog post that described the issue in detail, and what the Pyarrow team did to fix it: &lt;denchmark-link:https://uwekorn.com/2019/09/15/how-we-build-apache-arrows-manylinux-wheels.html&gt;https://uwekorn.com/2019/09/15/how-we-build-apache-arrows-manylinux-wheels.html&lt;/denchmark-link&gt;
. See the  and  sections at the end of that blog post for the current status. In summary, Pyarrow now provides  compliant wheels. As does Tensorflow, which was another "offender" for a long time.
The most reliable way to not run into these issues is indeed using conda, however currently this issue is fixed for wheels.
Since this is fixed and there's nothing to do on the PyTorch side, I'll close this issue.
		</comment>
	</comments>
</bug>