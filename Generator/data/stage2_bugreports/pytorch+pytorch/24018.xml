<bug id='24018' author='yuribd' open_date='2019-08-08T17:07:46Z' closed_time='2019-10-12T21:45:13Z'>
	<summary>'cublas runtime error' for (not so large) *fp16* matrix multiplication</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Sorry if a duplicate of existing bug-report (can't easily find it)
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

torch.zeros((16*2**20 - 512)//2 + 1, 1, dtype=torch.float16, device='cuda:0') @ torch.zeros(1, 2, dtype=torch.float16, device='cuda:0')
&lt;denchmark-code&gt;RuntimeError: cublas runtime error : the GPU program failed to execute at /opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/THC/THCBlas.cu:315
&lt;/denchmark-code&gt;

It also fails if cublasGemmEx called via pytorch extension, with stacktrace:
&lt;denchmark-code&gt;frame #0: c10::Error::Error(c10::SourceLocation, std::string const&amp;) + 0x45 (0x7fe8d5a44dc5 in /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #1: mm_fp16_fp16_acc32(at::Tensor, at::Tensor, at::Tensor) + 0x7dd (0x7fe884d2873d in /mnt/live/research/hffx/SharedCodes/python/xtx/nn/gemm_11/gemm_bind.cpython-37m-x86_64-linux-gnu.so)
frame #2: &lt;unknown function&gt; + 0x199f0 (0x7fe884d389f0 in /mnt/live/research/hffx/SharedCodes/python/xtx/nn/gemm_11/gemm_bind.cpython-37m-x86_64-linux-gnu.so)
frame #3: &lt;unknown function&gt; + 0x19eda (0x7fe884d38eda in /mnt/live/research/hffx/SharedCodes/python/xtx/nn/gemm_11/gemm_bind.cpython-37m-x86_64-linux-gnu.so)
frame #4: &lt;unknown function&gt; + 0x16a79 (0x7fe884d35a79 in /mnt/live/research/hffx/SharedCodes/python/xtx/nn/gemm_11/gemm_bind.cpython-37m-x86_64-linux-gnu.so)
frame #5: _PyMethodDef_RawFastCallKeywords + 0x264 (0x5632b1b5f494 in /opt/anaconda3/bin/python)
frame #6: _PyCFunction_FastCallKeywords + 0x21 (0x5632b1b5f5b1 in /opt/anaconda3/bin/python)
frame #7: _PyEval_EvalFrameDefault + 0x4f32 (0x5632b1bbb5b2 in /opt/anaconda3/bin/python)
frame #8: _PyEval_EvalCodeWithName + 0x2f8 (0x5632b1afba38 in /opt/anaconda3/bin/python)
frame #9: PyEval_EvalCodeEx + 0x44 (0x5632b1afc8a4 in /opt/anaconda3/bin/python)
frame #10: PyEval_EvalCode + 0x1c (0x5632b1afc8cc in /opt/anaconda3/bin/python)
frame #11: &lt;unknown function&gt; + 0x1e140d (0x5632b1bc640d in /opt/anaconda3/bin/python)
frame #12: _PyMethodDef_RawFastCallKeywords + 0xe9 (0x5632b1b5f319 in /opt/anaconda3/bin/python)
frame #13: _PyCFunction_FastCallKeywords + 0x21 (0x5632b1b5f5b1 in /opt/anaconda3/bin/python)
frame #14: _PyEval_EvalFrameDefault + 0x47e3 (0x5632b1bbae63 in /opt/anaconda3/bin/python)
frame #15: _PyEval_EvalCodeWithName + 0x2f8 (0x5632b1afba38 in /opt/anaconda3/bin/python)
frame #16: _PyFunction_FastCallKeywords + 0x325 (0x5632b1b5e765 in /opt/anaconda3/bin/python)
frame #17: _PyEval_EvalFrameDefault + 0x6f5 (0x5632b1bb6d75 in /opt/anaconda3/bin/python)
frame #18: _PyEval_EvalCodeWithName + 0x2f8 (0x5632b1afba38 in /opt/anaconda3/bin/python)
frame #19: _PyFunction_FastCallKeywords + 0x387 (0x5632b1b5e7c7 in /opt/anaconda3/bin/python)
frame #20: _PyEval_EvalFrameDefault + 0x14e3 (0x5632b1bb7b63 in /opt/anaconda3/bin/python)
frame #21: _PyEval_EvalCodeWithName + 0xb99 (0x5632b1afc2d9 in /opt/anaconda3/bin/python)
frame #22: _PyFunction_FastCallKeywords + 0x387 (0x5632b1b5e7c7 in /opt/anaconda3/bin/python)
frame #23: _PyEval_EvalFrameDefault + 0x6f5 (0x5632b1bb6d75 in /opt/anaconda3/bin/python)
frame #24: _PyEval_EvalCodeWithName + 0x2f8 (0x5632b1afba38 in /opt/anaconda3/bin/python)
frame #25: _PyFunction_FastCallKeywords + 0x387 (0x5632b1b5e7c7 in /opt/anaconda3/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x14e3 (0x5632b1bb7b63 in /opt/anaconda3/bin/python)
frame #27: _PyFunction_FastCallKeywords + 0xfb (0x5632b1b5e53b in /opt/anaconda3/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6f5 (0x5632b1bb6d75 in /opt/anaconda3/bin/python)
frame #29: _PyFunction_FastCallKeywords + 0xfb (0x5632b1b5e53b in /opt/anaconda3/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6f5 (0x5632b1bb6d75 in /opt/anaconda3/bin/python)
frame #31: _PyEval_EvalCodeWithName + 0x2f8 (0x5632b1afba38 in /opt/anaconda3/bin/python)
frame #32: _PyFunction_FastCallKeywords + 0x325 (0x5632b1b5e765 in /opt/anaconda3/bin/python)
frame #33: _PyEval_EvalFrameDefault + 0x6f5 (0x5632b1bb6d75 in /opt/anaconda3/bin/python)
frame #34: _PyFunction_FastCallKeywords + 0xfb (0x5632b1b5e53b in /opt/anaconda3/bin/python)
frame #35: _PyEval_EvalFrameDefault + 0x466 (0x5632b1bb6ae6 in /opt/anaconda3/bin/python)
frame #36: _PyFunction_FastCallKeywords + 0xfb (0x5632b1b5e53b in /opt/anaconda3/bin/python)
frame #37: _PyEval_EvalFrameDefault + 0x4b54 (0x5632b1bbb1d4 in /opt/anaconda3/bin/python)
frame #38: _PyEval_EvalCodeWithName + 0x2f8 (0x5632b1afba38 in /opt/anaconda3/bin/python)
frame #39: PyEval_EvalCodeEx + 0x44 (0x5632b1afc8a4 in /opt/anaconda3/bin/python)
frame #40: PyEval_EvalCode + 0x1c (0x5632b1afc8cc in /opt/anaconda3/bin/python)
frame #41: &lt;unknown function&gt; + 0x230f24 (0x5632b1c15f24 in /opt/anaconda3/bin/python)
frame #42: PyRun_FileExFlags + 0xa1 (0x5632b1c20111 in /opt/anaconda3/bin/python)
frame #43: PyRun_SimpleFileExFlags + 0x1c3 (0x5632b1c20303 in /opt/anaconda3/bin/python)
frame #44: &lt;unknown function&gt; + 0x23c41b (0x5632b1c2141b in /opt/anaconda3/bin/python)
frame #45: _Py_UnixMain + 0x3c (0x5632b1c214fc in /opt/anaconda3/bin/python)
frame #46: __libc_start_main + 0xf5 (0x7fe8f24b33d5 in /lib64/libc.so.6)
frame #47: &lt;unknown function&gt; + 0x1e14e2 (0x5632b1bc64e2 in /opt/anaconda3/bin/python)
&lt;/denchmark-code&gt;

It all works as expected for fp32, it also works for fp16 if matrix is slightly smaller:
torch.zeros((16*2**20 - 512)//2, 1, dtype=torch.float16, device='cuda:0') @ torch.zeros(1, 2, dtype=torch.float16, device='cuda:0')
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;


PyTorch: 1.1.0
OS: Linux
Python version: 3.7
CUDA/cuDNN version: 10.0.130
GPU: V-100

	</description>
	<comments>
		<comment id='1' author='yuribd' date='2019-08-08T17:09:13Z'>
		i'm sure &lt;denchmark-link:https://github.com/ngimel&gt;@ngimel&lt;/denchmark-link&gt;
 would know what's the reason :-)
many thanks in advance!
		</comment>
		<comment id='2' author='yuribd' date='2019-08-08T17:32:41Z'>
		Cc &lt;denchmark-link:https://github.com/ptrblck&gt;@ptrblck&lt;/denchmark-link&gt;
, as I'm no longer at Nvidia.
		</comment>
		<comment id='3' author='yuribd' date='2019-08-08T17:44:02Z'>
		Thanks for reporting this issue!
We'll have a look at it. :)
		</comment>
		<comment id='4' author='yuribd' date='2019-08-08T18:27:01Z'>
		Thanks &lt;denchmark-link:https://github.com/ptrblck&gt;@ptrblck&lt;/denchmark-link&gt;
  and &lt;denchmark-link:https://github.com/ngimel&gt;@ngimel&lt;/denchmark-link&gt;
 !
same for pytorch 1.2 (anaconda python 3.7 + 1.2.0-py3.7_cuda10.0.130_cudnn7.6.2_0)
&lt;denchmark-code&gt;RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasGemmEx( handle, opa, opb, m, n, k, &amp;falpha, a, CUDA_R_16F, lda, b, CUDA_R_16F, ldb, &amp;fbeta, c, CUDA_R_16F, ldc, CUDA_R_32F, CUBLAS_GEMM_DFALT_TENSOR_OP)`
``
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='yuribd' date='2019-08-09T18:50:38Z'>
		Wonder if you can reproduce it, or rather something is wrong with environment on my end?
		</comment>
		<comment id='6' author='yuribd' date='2019-08-10T11:40:56Z'>
		&lt;denchmark-link:https://github.com/yuribd&gt;@yuribd&lt;/denchmark-link&gt;
 I can reproduce it and am currently debugging this issue.
		</comment>
		<comment id='7' author='yuribd' date='2019-08-10T17:04:50Z'>
		Thanks, &lt;denchmark-link:https://github.com/ptrblck&gt;@ptrblck&lt;/denchmark-link&gt;
 !
		</comment>
		<comment id='8' author='yuribd' date='2019-08-16T12:05:42Z'>
		&lt;denchmark-link:https://github.com/ptrblck&gt;@ptrblck&lt;/denchmark-link&gt;
 just wonder what workaround you'd recommend in the meantime?
something like this would cause performance regression
k = x.shape[0] // 2
return torch.cat((x[:k] @ w, x[k:] @ w))
and calling cublasGemmStridedBatchedEx feels bit involving... might be you have code for that already?
Thanks!
		</comment>
		<comment id='9' author='yuribd' date='2019-09-04T17:33:05Z'>
		We ran some tests with different setups and could isolate the issue and failing cublas version.
This bug was solved in cublas 10.2.0.186.
The latest public version of cublas is 10.2.1.243 that was released with CUDA 10.1 Update 2.
&lt;denchmark-link:https://github.com/yuribd&gt;@yuribd&lt;/denchmark-link&gt;
 Could you update CUDA to this version, build PyTorch from source, and try to run the code again?
If this issue comes up often, we might want to build the binaries with CUDA 10.1 Update 2.
		</comment>
		<comment id='10' author='yuribd' date='2019-09-04T19:29:15Z'>
		Many thanks &lt;denchmark-link:https://github.com/ptrblck&gt;@ptrblck&lt;/denchmark-link&gt;
 ! Do you think pytorch with fixed CUDA version might be packaged soon for Anaconda? (seems rather major bug to me..)
		</comment>
		<comment id='11' author='yuribd' date='2019-09-04T22:40:33Z'>
		I'm not aware of the current plans to update the CUDA version for the binaries, but &lt;denchmark-link:https://github.com/soumith&gt;@soumith&lt;/denchmark-link&gt;
 might know.
		</comment>
		<comment id='12' author='yuribd' date='2019-09-05T01:13:54Z'>
		hope to move to 10.1 by next release.
		</comment>
		<comment id='13' author='yuribd' date='2019-09-05T09:06:52Z'>
		Many thanks, &lt;denchmark-link:https://github.com/ptrblck&gt;@ptrblck&lt;/denchmark-link&gt;
  and &lt;denchmark-link:https://github.com/soumith&gt;@soumith&lt;/denchmark-link&gt;
!
Sorry, last question on this subject - and what's the ETA for next release?
		</comment>
		<comment id='14' author='yuribd' date='2019-10-12T21:45:13Z'>
		we now have moved to 10.1 and the issue should be fixed.
		</comment>
		<comment id='15' author='yuribd' date='2019-10-14T13:27:53Z'>
		Thanks!
		</comment>
	</comments>
</bug>