<bug id='13276' author='MarcoForte' open_date='2018-10-29T22:41:49Z' closed_time='2019-06-13T07:25:36Z'>
	<summary>Memory error for batched inverse</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

There seems to be a limit of N=256x256 for the number of batches one can invert on the gpu. I don't think this limit corresponds properly to available memory. Because I can run a N=256x256 -1 batched inverse on a gpu with less availiable memory. I have tested tensorflow on google collab and it allows batched inverse of size N=6144x6144.
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import torch
N = 256
x = torch.randn(N*N , 2, 2).cuda()
y = torch.inverse(x)
print(y.cpu().numpy())
&lt;/denchmark-code&gt;

Error:

----&gt; 1 y.cpu().numpy()
RuntimeError: cuda runtime error (9) : invalid configuration argument at /opt/conda/conda-bld/pytorch-nightly_1540802486426/work/aten/src/THC/THCTensorCopy.cu:205

&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

It works if I reduce the batch dimension by one.
x = torch.randn(N*N -1 , 2, 2).cuda() 
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;


PyTorch version: 1.0.0.dev20181029
Is debug build: No
CUDA used to build PyTorch: 8.0.61
OS: Ubuntu 16.04.4 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.2) 5.4.0 20160609
CMake version: version 3.11.0
Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.1.85
GPU models and configuration:
GPU 0: GeForce GTX 1080
GPU 1: GeForce GTX 1080
Nvidia driver version: 387.26
cuDNN version: Probably one of the following:
/usr/local/MATLAB/R2016b/bin/glnxa64/libcudnn.so.4.0.7
/usr/local/cuda-9.1/lib64/libcudnn.so
/usr/local/cuda-9.1/lib64/libcudnn.so.7
/usr/local/cuda-9.1/lib64/libcudnn.so.7.0.5
/usr/local/cuda-9.1/lib64/libcudnn.so.7.1.1
/usr/local/cuda-9.1/lib64/libcudnn_static.a
Versions of relevant libraries:
[pip] Could not collect
[conda] cuda80                    1.0                           0    soumith
[conda] cuda91                    1.0                  h4c16780_0    pytorch
[conda] guided-filter-pytorch     1.1.1                     
[conda] magma-cuda91              2.3.0                         1    pytorch
[conda] pytorch-nightly           1.0.0.dev20181029 py3.6_cuda8.0.61_cudnn7.1.2_0  [cuda80]  pytorch
[conda] torch                     0.4.0                     
[conda] torch                     0.3.1                     

	</description>
	<comments>
		<comment id='1' author='MarcoForte' date='2018-10-29T22:43:50Z'>
		Could you run with CUDA_LAUNCH_BLOCKING=1 and then report back what the error message is?
		</comment>
		<comment id='2' author='MarcoForte' date='2018-10-29T22:51:26Z'>
		Hi, I basically get the same error, more fully

THCudaCheck FAIL file=/opt/conda/conda-bld/pytorch-nightly_1540802486426/work/aten/src/THC/THCTensorCopy.cu line=205 error=9 : invalid configuration argument
Traceback (most recent call last):
File "inverse test.py", line 25, in 
print(y.cpu().numpy())
RuntimeError: cuda runtime error (9) : invalid configuration argument at /opt/conda/conda-bld/pytorch-nightly_1540802486426/work/aten/src/THC/THCTensorCopy.cu:205

		</comment>
		<comment id='3' author='MarcoForte' date='2018-10-30T00:59:16Z'>
		&lt;denchmark-link:https://github.com/zou3519&gt;@zou3519&lt;/denchmark-link&gt;
 could this be MAGMA related?
		</comment>
		<comment id='4' author='MarcoForte' date='2018-10-30T01:23:41Z'>
		I haven't tried to reproduce on my machine yet (I don't have magma at the moment). It's possible that there is some magma version mismatch somewhere, the following lines in the environment look fishy to me:

CUDA used to build PyTorch: 8.0.61
[conda] cuda80 1.0 0 soumith
[conda] cuda91 1.0 h4c16780_0 pytorch
[conda] guided-filter-pytorch 1.1.1
[conda] magma-cuda91 2.3.0 1 pytorch
[conda] pytorch-nightly 1.0.0.dev20181029 py3.6_cuda8.0.61_cudnn7.1.2_0 [cuda80] pytorch

&lt;denchmark-link:https://github.com/MarcoForte&gt;@MarcoForte&lt;/denchmark-link&gt;
 did you build from source or install from binary? The binary you have installed was compiled against CUDA 8.0 and is mismatched with your CUDA version (9.1)
		</comment>
		<comment id='5' author='MarcoForte' date='2018-10-30T07:04:07Z'>
		I am not able to reproduce this. I tested it on a system with the following config:
&lt;denchmark-code&gt;PyTorch version: 1.0.0a0+bb703b1
Is debug build: No
CUDA used to build PyTorch: 9.2.148

OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 7.3.0-16ubuntu3) 7.3.0
CMake version: version 3.10.2

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.2.148
GPU models and configuration: GPU 0: GeForce 940M
Nvidia driver version: 396.54
cuDNN version: Probably one of the following:
/usr/local/cuda-9.2/lib64/libcudnn.so
/usr/local/cuda-9.2/lib64/libcudnn.so.7
/usr/local/cuda-9.2/lib64/libcudnn.so.7.2.1
/usr/local/cuda-9.2/lib64/libcudnn_static.a

Versions of relevant libraries:
[pip] numpy (1.15.0)
[pip] torch (1.0.0a0+bb703b1, /media/vishwak/Official/Projects/pytorch)
[pip] torchvision (0.2.1)
[conda] torch                     1.0.0a0+bb703b1           &lt;pip&gt;
&lt;/denchmark-code&gt;

MAGMA 2.3.0 installed from source.
It's highly likely that it is a MAGMA related issue.
		</comment>
		<comment id='6' author='MarcoForte' date='2018-10-30T14:04:59Z'>
		I previously had installed Pytorch from a binary.
After your comments I installed pytorch from source. But I still get the same error. Here is the updated config:
&lt;denchmark-code&gt;PyTorch version: 1.0.0a0+47c0d88
Is debug build: No
CUDA used to build PyTorch: 9.1.85

OS: Ubuntu 16.04.4 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.2) 5.4.0 20160609
CMake version: version 3.12.3

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.1.85
GPU models and configuration: 
GPU 0: GeForce GTX 1080
GPU 1: GeForce GTX 1080

Nvidia driver version: 387.26
cuDNN version: Probably one of the following:
/usr/local/MATLAB/R2016b/bin/glnxa64/libcudnn.so.4.0.7
/usr/local/cuda-9.1/lib64/libcudnn.so
/usr/local/cuda-9.1/lib64/libcudnn.so.7
/usr/local/cuda-9.1/lib64/libcudnn.so.7.0.5
/usr/local/cuda-9.1/lib64/libcudnn.so.7.1.1
/usr/local/cuda-9.1/lib64/libcudnn_static.a

Versions of relevant libraries:
[pip] Could not collect
[conda] cuda80                    1.0                           0    soumith
[conda] cuda91                    1.0                  h4c16780_0    pytorch
[conda] guided-filter-pytorch     1.1.1                     &lt;pip&gt;
[conda] magma-cuda91              2.3.0                         1    pytorch
[conda] pytorch-nightly           1.0.0.dev20181029 py3.6_cuda8.0.61_cudnn7.1.2_0  [cuda80]  pytorch
[conda] torch                     0.3.1                     &lt;pip&gt;
[conda] torch                     1.0.0a0+47c0d88           &lt;pip&gt;
[conda] torch                     0.4.0                     &lt;pip&gt;
[conda] torch-nightly             1.0.0.dev20181029           &lt;pip&gt;

&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='MarcoForte' date='2018-10-30T19:52:37Z'>
		I have another independent report that this can be reproduced using the magma build that is installed alongside pytorch (magma-cuda92  2.3.0). This seems to be the same as the version you're using, &lt;denchmark-link:https://github.com/vishwakftw&gt;@vishwakftw&lt;/denchmark-link&gt;
. I don't know what is different between the magma we ship with and building magma from source, but if this is indeed a magma problem we should at the very least throw a warning.
		</comment>
		<comment id='8' author='MarcoForte' date='2018-10-31T03:23:23Z'>
		I could reproduce this with CUDA 8, with MAGMA installed from conda. I'll dig deeper into this.
		</comment>
		<comment id='9' author='MarcoForte' date='2018-11-02T09:19:22Z'>
		Although there were no errors in my system, this is what happened with MAGMA installed from source (as is the case with my system):
import torch
N = 256
x = torch.randn(N * N, 2, 2).cuda()
y = torch.inverse(x)
torch.matmul(y, x)  # this is not a batch of identity matrices, in fact y == x
I think we don't have to concern ourselves with the MAGMA install now; I'll recheck the code.
		</comment>
		<comment id='10' author='MarcoForte' date='2018-11-03T05:29:18Z'>
		I have confirmed that it is a MAGMA library issue. I did this by installing MAGMA from scratch and running the test pertaining to getri (testing_(s)dgetri_batched.cpp) with batch size 65536 and matrix dim 2. This is the result:
&lt;denchmark-code&gt;% MAGMA 2.3.0  compiled for CUDA capability &gt;= 6.0, 32-bit magma_int_t, 64-bit pointer.
% CUDA runtime 8000, driver 9010. OpenMP threads 40. 
% device 0: GeForce GTX 1080 Ti, 1582.0 MHz clock, 11178.5 MiB memory, capability 6.1
% device 1: GeForce GTX 1080 Ti, 1582.0 MHz clock, 11178.5 MiB memory, capability 6.1
% device 2: GeForce GTX 1080 Ti, 1582.0 MHz clock, 11178.5 MiB memory, capability 6.1
% device 3: GeForce GTX 1080 Ti, 1582.0 MHz clock, 11178.5 MiB memory, capability 6.1
% Sat Nov  3 10:51:57 2018
% Usage: ./testing/testing_dgetri_batched [options] [-h|--help]

% batchCount   N    CPU Gflop/s (ms)    GPU Gflop/s (ms)   ||I - A*A^{-1}||_1 / (N*cond(A))
%===============================================================================
     65536     2     ---   (  ---  )      0.02 (  59.16)   9.01e+15   failed
&lt;/denchmark-code&gt;

Whereas, for a batch size of 65535 (256 * 256 - 1):
&lt;denchmark-code&gt;% MAGMA 2.3.0  compiled for CUDA capability &gt;= 6.0, 32-bit magma_int_t, 64-bit pointer.
% CUDA runtime 8000, driver 9010. OpenMP threads 40. 
% device 0: GeForce GTX 1080 Ti, 1582.0 MHz clock, 11178.5 MiB memory, capability 6.1
% device 1: GeForce GTX 1080 Ti, 1582.0 MHz clock, 11178.5 MiB memory, capability 6.1
% device 2: GeForce GTX 1080 Ti, 1582.0 MHz clock, 11178.5 MiB memory, capability 6.1
% device 3: GeForce GTX 1080 Ti, 1582.0 MHz clock, 11178.5 MiB memory, capability 6.1
% Sat Nov  3 10:55:07 2018
% Usage: ./testing/testing_dgetri_batched [options] [-h|--help]

% batchCount   N    CPU Gflop/s (ms)    GPU Gflop/s (ms)   ||I - A*A^{-1}||_1 / (N*cond(A))
%===============================================================================
     65535     2     ---   (  ---  )      0.01 (  81.19)   1.14e-16   ok
&lt;/denchmark-code&gt;

Analogous results were found for single precision.
&lt;denchmark-code&gt;% MAGMA 2.3.0  compiled for CUDA capability &gt;= 6.0, 32-bit magma_int_t, 64-bit pointer.
% CUDA runtime 8000, driver 9010. OpenMP threads 40. 
% device 0: GeForce GTX 1080 Ti, 1582.0 MHz clock, 11178.5 MiB memory, capability 6.1
% device 1: GeForce GTX 1080 Ti, 1582.0 MHz clock, 11178.5 MiB memory, capability 6.1
% device 2: GeForce GTX 1080 Ti, 1582.0 MHz clock, 11178.5 MiB memory, capability 6.1
% device 3: GeForce GTX 1080 Ti, 1582.0 MHz clock, 11178.5 MiB memory, capability 6.1
% Sat Nov  3 10:57:57 2018
% Usage: ./testing/testing_sgetri_batched [options] [-h|--help]

% batchCount   N    CPU Gflop/s (ms)    GPU Gflop/s (ms)   ||I - A*A^{-1}||_1 / (N*cond(A))
%===============================================================================
     65536     2     ---   (  ---  )      0.04 (  31.38)   1.68e+07   failed
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;% MAGMA 2.3.0  compiled for CUDA capability &gt;= 6.0, 32-bit magma_int_t, 64-bit pointer.
% CUDA runtime 8000, driver 9010. OpenMP threads 40. 
% device 0: GeForce GTX 1080 Ti, 1582.0 MHz clock, 11178.5 MiB memory, capability 6.1
% device 1: GeForce GTX 1080 Ti, 1582.0 MHz clock, 11178.5 MiB memory, capability 6.1
% device 2: GeForce GTX 1080 Ti, 1582.0 MHz clock, 11178.5 MiB memory, capability 6.1
% device 3: GeForce GTX 1080 Ti, 1582.0 MHz clock, 11178.5 MiB memory, capability 6.1
% Sat Nov  3 10:57:49 2018
% Usage: ./testing/testing_sgetri_batched [options] [-h|--help]

% batchCount   N    CPU Gflop/s (ms)    GPU Gflop/s (ms)   ||I - A*A^{-1}||_1 / (N*cond(A))
%===============================================================================
     65535     2     ---   (  ---  )      0.02 (  44.71)   6.15e-08   ok
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='MarcoForte' date='2018-11-03T05:52:22Z'>
		I have opened an issue in MAGMA's BitBucket repository &lt;denchmark-link:https://bitbucket.org/icl/magma/issues/10/getri_outofplace_batched-fails-when&gt;https://bitbucket.org/icl/magma/issues/10/getri_outofplace_batched-fails-when&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='MarcoForte' date='2018-11-04T06:13:19Z'>
		&lt;denchmark-link:https://github.com/vishwakftw&gt;@vishwakftw&lt;/denchmark-link&gt;
 would Magma 2.4 fix it, instead of 2.3? I'm asking because Magma's batch inverse routines are much faster than CuBLAS's routines for large batch size / large matrix size (according to &lt;denchmark-link:https://github.com/martinarjovsky&gt;@martinarjovsky&lt;/denchmark-link&gt;
 ), so I'd avoid moving to cublas for this.
		</comment>
		<comment id='13' author='MarcoForte' date='2018-11-04T06:16:55Z'>
		Let me try installing MAGMA 2.4 and see if the issue persists. I don't think this has been addressed based on the release notes for 2.4, so I'll check manually.
Thanks for the comment about cuBLAS.
		</comment>
		<comment id='14' author='MarcoForte' date='2018-11-04T09:36:57Z'>
		&lt;denchmark-link:https://github.com/soumith&gt;@soumith&lt;/denchmark-link&gt;
 MAGMA v2.4.0 still has the bug, as suspected. Below are the results:
Double-precision:
&lt;denchmark-code&gt;% MAGMA 2.4.0  compiled for CUDA capability &gt;= 6.0, 32-bit magma_int_t, 64-bit pointer.
% CUDA runtime 8000, driver 9010. OpenMP threads 40. 
% device 0: GeForce GTX 1080 Ti, 1582.0 MHz clock, 11178.5 MiB memory, capability 6.1
% device 1: GeForce GTX 1080 Ti, 1582.0 MHz clock, 11178.5 MiB memory, capability 6.1
% device 2: GeForce GTX 1080 Ti, 1582.0 MHz clock, 11178.5 MiB memory, capability 6.1
% device 3: GeForce GTX 1080 Ti, 1582.0 MHz clock, 11178.5 MiB memory, capability 6.1
% Sun Nov  4 14:03:28 2018
% Usage: ./testing/testing_dgetri_batched [options] [-h|--help]

% batchCount   N    CPU Gflop/s (ms)    GPU Gflop/s (ms)   ||I - A*A^{-1}||_1 / (N*cond(A))
%===============================================================================
     65536     2     ---   (  ---  )     14.69 (   0.08)   9.01e+15   failed
     65535     2     ---   (  ---  )      1.28 (   0.87)   1.45e-16   ok
&lt;/denchmark-code&gt;

Single-precision:
&lt;denchmark-code&gt;% MAGMA 2.4.0  compiled for CUDA capability &gt;= 6.0, 32-bit magma_int_t, 64-bit pointer.
% CUDA runtime 8000, driver 9010. OpenMP threads 40. 
% device 0: GeForce GTX 1080 Ti, 1582.0 MHz clock, 11178.5 MiB memory, capability 6.1
% device 1: GeForce GTX 1080 Ti, 1582.0 MHz clock, 11178.5 MiB memory, capability 6.1
% device 2: GeForce GTX 1080 Ti, 1582.0 MHz clock, 11178.5 MiB memory, capability 6.1
% device 3: GeForce GTX 1080 Ti, 1582.0 MHz clock, 11178.5 MiB memory, capability 6.1
% Sun Nov  4 14:03:58 2018
% Usage: ./testing/testing_sgetri_batched [options] [-h|--help]

% batchCount   N    CPU Gflop/s (ms)    GPU Gflop/s (ms)   ||I - A*A^{-1}||_1 / (N*cond(A))
%===============================================================================
     65536     2     ---   (  ---  )     14.65 (   0.08)   1.68e+07   failed
     65535     2     ---   (  ---  )      1.41 (   0.79)   7.21e-08   ok
&lt;/denchmark-code&gt;

		</comment>
		<comment id='15' author='MarcoForte' date='2018-12-17T15:11:31Z'>
		Any news on this? It seem that this problem is still present in pytorch 1.0.0 (I have just installed the new version using conda and cuda 10.0)
		</comment>
		<comment id='16' author='MarcoForte' date='2018-12-17T15:14:35Z'>
		Hi &lt;denchmark-link:https://github.com/mscipio&gt;@mscipio&lt;/denchmark-link&gt;
, I have confirmed with MAGMA developers that this is an actual issue. A tentative workaround would be to use mini-batching.
		</comment>
		<comment id='17' author='MarcoForte' date='2018-12-17T15:17:48Z'>
		Too bad ... I am just wondering if, in the meantime, it would be possible to try to use cublas.cublasSgetrfBatched + cublas.cublasSgetriBatched from scikit-cuda library ... do you know of any way I can pass the right pointers from torch tensors to these functions?
		</comment>
		<comment id='18' author='MarcoForte' date='2018-12-17T16:00:33Z'>
		So far I solved this way:
def batchedInv(self, batchedTensor):
        if batchedTensor.shape[0] &gt;= 256 * 256 - 1:
            temp = []
            for t in torch.split(batchedTensor, 256 * 256 - 1):
                temp.append(torch.inverse(t))
            return torch.cat(temp)
        else:
            return torch.inverse(batchedTensor)
If you have suggestion for smarter approaches, I am here to learn!
		</comment>
		<comment id='19' author='MarcoForte' date='2019-06-13T07:25:34Z'>
		Closed by &lt;denchmark-link:https://github.com/pytorch/pytorch/commit/4c03ac7ac4d6fe9bc428f20834da053f9701c3c6&gt;4c03ac7&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>