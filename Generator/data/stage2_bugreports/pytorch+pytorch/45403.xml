<bug id='45403' author='mruberry' open_date='2020-09-28T00:21:06Z' closed_time='2020-10-05T17:19:56Z'>
	<summary>test_torch.py/test_inverse_cuda causes illegal memory access on some platforms</summary>
	<description>
Run the test -&gt; illegal memory access on CUDA (on some platforms).
This is indicative of the operation illegally accessing memory.
Likely (almost definitely) caused by &lt;denchmark-link:https://github.com/pytorch/pytorch/pull/42403&gt;#42403&lt;/denchmark-link&gt;
.
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

PyTorch version: 1.7.0a0+db4690c
Is debug build: True
CUDA used to build PyTorch: 10.1
ROCM used to build PyTorch: N/A
OS: Ubuntu 18.04.3 LTS (x86_64)
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
Clang version: 8.0.0 (tags/RELEASE_800/final)
CMake version: version 3.14.0
Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 10.1.105
GPU models and configuration:
GPU 0: Quadro GP100
GPU 1: Quadro GP100
Nvidia driver version: 440.82
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] torch==1.7.0a0
[pip3] torchvision==0.7.0
[conda] blas                      1.0                         mkl
[conda] magma-cuda101             2.5.2                         1    pytorch
[conda] mkl                       2020.1                      217
[conda] mkl-include               2020.1                      217
[conda] mkl-service               2.3.0            py38he904b0f_0
[conda] mkl_fft                   1.2.0            py38h23d657b_0
[conda] mkl_random                1.1.1            py38h0573a6f_0
[conda] numpy                     1.19.1           py38hbc911f0_0
[conda] numpy-base                1.19.1           py38hfa32c7d_0
[conda] torch                     1.7.0a0                   dev_0    
[conda] torchvision               0.7.0                    pypi_0    pypi
cc &lt;denchmark-link:https://github.com/ezyang&gt;@ezyang&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/gchanan&gt;@gchanan&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/zou3519&gt;@zou3519&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/mruberry&gt;@mruberry&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/VitalyFedyunin&gt;@VitalyFedyunin&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/vincentqb&gt;@vincentqb&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/vishwakftw&gt;@vishwakftw&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jianyuh&gt;@jianyuh&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/nikitaved&gt;@nikitaved&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/pearu&gt;@pearu&lt;/denchmark-link&gt;

cc &lt;denchmark-link:https://github.com/xwang233&gt;@xwang233&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='mruberry' date='2020-09-28T01:28:30Z'>
		When running the test in isolation I get:
&lt;denchmark-code&gt;
======================================================================
ERROR: test_inverse_cuda (__main__.TestTorchDeviceTypeCUDA)
----------------------------------------------------------------------
Traceback (most recent call last):
 File "/private/home/mruberry/git/pytorch/torch/testing/_internal/common_utils.py", line 822, in wrapper
  method(*args, **kwargs)
 File "/private/home/mruberry/git/pytorch/torch/testing/_internal/common_utils.py", line 822, in wrapper
  method(*args, **kwargs)
 File "/private/home/mruberry/git/pytorch/torch/testing/_internal/common_device_type.py", line 273, in instantiated_test
  result = test_fn(self, *args)
 File "/private/home/mruberry/git/pytorch/torch/testing/_internal/common_device_type.py", line 508, in dep_fn
  return fn(slf, device, *args, **kwargs)
 File "/private/home/mruberry/git/pytorch/torch/testing/_internal/common_device_type.py", line 508, in dep_fn
  return fn(slf, device, *args, **kwargs)
 File "test/test_torch.py", line 6091, in test_inverse
  matrices_inverse = torch.inverse(matrices)
RuntimeError: cusolver error: 6, when calling `cusolverDnDgetrs( handle, CUBLAS_OP_N, n, nrhs, dA, lda, ipiv, ret, ldb, info)`
----------------------------------------------------------------------
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='mruberry' date='2020-09-28T02:22:53Z'>
		Thanks for the test.
The test at line 6091 of test_torch.py is equivalent to this
import torch
print(torch.__version__)

x = torch.randn(2, 3, 3, dtype=torch.double, device='cuda').permute(0, 2, 1)
y = torch.inverse(x)

print(y)
Since this is a batch 2 matrix, it's handled by cusolver after the PR. Before the PR, it was MAGMA. I tried this with CUDA 11.0 on RTX 2070 Super and there was no error.
For batch size = 2 matrix, after the PR, cusolver matrix inverse kernels are executed in parallel using different CUDA streams. This can greatly improve the performance and should be fine in nearly all cases. Since &lt;denchmark-link:https://github.com/mruberry&gt;@mruberry&lt;/denchmark-link&gt;
 also tried the repro with , then parallel execution is not the problem.
I can only think of the cusolver library function being the problem. Perhaps we need to modify the guard here



pytorch/aten/src/ATen/native/cuda/BatchLinearAlgebraLib.h


        Lines 10 to 13
      in
      d75c402






 #if defined(CUDART_VERSION) &amp;&amp; CUDART_VERSION &gt;= 10000 



 // some cusolver functions doesn't work well on cuda 9.2, cusolver is used on cuda &gt;= 10.0 



 #define USE_CUSOLVER 



 #endif 





If USE_CUSOLVER is not defined, MAGMA implementation will be used.
		</comment>
		<comment id='3' author='mruberry' date='2020-09-28T17:05:45Z'>
		It does not repro with cuda 10.1.243, we should disable cusolver for 10.1.105. &lt;denchmark-link:https://github.com/xwang233&gt;@xwang233&lt;/denchmark-link&gt;
 is there a way to distinguish between patch releases in ifdefs?
		</comment>
		<comment id='4' author='mruberry' date='2020-09-28T17:39:04Z'>
		I think they have different CUDA_VERSION macros. I'll check that.
		</comment>
		<comment id='5' author='mruberry' date='2020-09-28T18:54:47Z'>
		Here are the cusolver header files on my machine
&lt;denchmark-code&gt;$ cd /usr/local

$ ls -lahd cuda-*
drwxr-xr-x. 17 root root 4.0K Sep 28 11:47 cuda-10.1.105
drwxr-xr-x. 17 root root 4.0K Sep 28 11:38 cuda-10.1.168
drwxr-xr-x. 15 root root 4.0K Sep 28 11:13 cuda-10.1.243
drwxr-xr-x. 18 root root 4.0K Mar  5  2020 cuda-10.2
drwxr-xr-x.  7 root root 4.0K Aug 13 13:42 cuda-11.0

$ rg -i cusolver_ver_ cuda-*
cuda-10.2/targets/x86_64-linux/include/cusolver_common.h
76:#define CUSOLVER_VER_MAJOR 10
77:#define CUSOLVER_VER_MINOR 3
78:#define CUSOLVER_VER_PATCH 0
79:#define CUSOLVER_VER_BUILD 89
80:#define CUSOLVER_VERSION (CUSOLVER_VER_MAJOR * 1000 + \
81:                        CUSOLVER_VER_MINOR *  100 + \
82:                        CUSOLVER_VER_PATCH)

cuda-11.0/targets/x86_64-linux/include/cusolver_common.h
76:#define CUSOLVER_VER_MAJOR 10
77:#define CUSOLVER_VER_MINOR 4
78:#define CUSOLVER_VER_PATCH 0
79:#define CUSOLVER_VER_BUILD 191
80:#define CUSOLVER_VERSION (CUSOLVER_VER_MAJOR * 1000 + \
81:                        CUSOLVER_VER_MINOR *  100 + \
82:                        CUSOLVER_VER_PATCH)

cuda-10.1.243/targets/x86_64-linux/include/cusolver_common.h
76:#define CUSOLVER_VER_MAJOR 10
77:#define CUSOLVER_VER_MINOR 2
78:#define CUSOLVER_VER_PATCH 0
79:#define CUSOLVER_VER_BUILD 243
80:#define CUSOLVER_VERSION (CUSOLVER_VER_MAJOR * 1000 + \
81:                        CUSOLVER_VER_MINOR *  100 + \
82:                        CUSOLVER_VER_PATCH)
&lt;/denchmark-code&gt;

To only enable cusolver for cuda &gt;= 10.1.243, we can change the guard to
#if defined(CUDART_VERSION) &amp;&amp; defined(CUSOLVER_VERSION) &amp;&amp; CUSOLVER_VERSION &gt;= 10200
The first CUDART_VERSION is used to guard ROCm.
		</comment>
		<comment id='6' author='mruberry' date='2020-09-28T19:56:51Z'>
		Cool, can you please submit a PR?
		</comment>
		<comment id='7' author='mruberry' date='2020-09-28T19:58:09Z'>
		Sure, thanks!
		</comment>
		<comment id='8' author='mruberry' date='2020-10-05T06:38:18Z'>
		&lt;denchmark-link:https://github.com/xwang233&gt;@xwang233&lt;/denchmark-link&gt;
 we can close this now, right?
		</comment>
		<comment id='9' author='mruberry' date='2020-10-05T07:10:35Z'>
		&lt;denchmark-link:https://github.com/mruberry&gt;@mruberry&lt;/denchmark-link&gt;
 The PR has been merged, so it should solve the problem you saw on 10.1.105. If you have verified that and don't see a crash elsewhere, feel free to close it. Thanks!
		</comment>
		<comment id='10' author='mruberry' date='2020-10-05T17:19:55Z'>
		Verified the test now passes.
		</comment>
	</comments>
</bug>