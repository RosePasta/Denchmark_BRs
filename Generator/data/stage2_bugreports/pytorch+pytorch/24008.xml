<bug id='24008' author='danielcrane' open_date='2019-08-08T08:39:51Z' closed_time='2020-10-10T21:26:40Z'>
	<summary>Performance issues with initial call of MultivariateNormal.log_pdf()</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

After initialising a MultivariateNormal model with scale_tril input, the first .log_prob() is extremely slow.
By running the below repro code (note that it must be run each time in a fresh python instance), I get the following results:
&lt;denchmark-code&gt;[0.32401383699834696, 0.0002588090028439183, 0.00021725300030084327, 0.0002125649989466183, 0.00021101400125189684]
&lt;/denchmark-code&gt;

Note that the time for the first call to .log_prob() is around 1000x slower than the successive calls.

Repro Code

import time
import torch
from torch.distributions.multivariate_normal import MultivariateNormal

mu = torch.ones(2, device='cuda')
sigma = torch.eye(2, device='cuda')
dist = MultivariateNormal(loc=mu, scale_tril=sigma)

runs = 5
times = []
X = torch.rand(1, 2, device='cuda')
for _ in range(runs):
    torch.cuda.synchronize()
    st = time.perf_counter()
    prob = dist.log_prob(X)
    torch.cuda.synchronize()
    times.append(time.perf_counter() - st)
print(times)



When the model is initialised using covariance_matrix as the input, this difference between first and successive call times isn't so pronounced.
The only difference between supplying scale_tril and covariance_matrix is that in the latter case, torch.cholesky() is performed. Somehow performing torch.cholesky() on a CUDA torch object first seems to reduce the calculation time of the first .log_pdf().
In my particular use case, I'm re-initialising models quite frequently, and making only few calls to .log_prob() before discarding the model, and so this first call time being so long is very punishing.
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;


 Environment Information 

PyTorch version: N/A
Is debug build: N/A
CUDA used to build PyTorch: N/A

OS: Pop!_OS 18.10
GCC version: (Ubuntu 8.3.0-6ubuntu1~18.10) 8.3.0
CMake version: version 3.12.1

Python version: 3.7
Is CUDA available: N/A
CUDA runtime version: 10.0.130
GPU models and configuration: GPU 0: GeForce GTX 1070 With Max-Q Design
Nvidia driver version: 410.78
cuDNN version: /usr/lib/cuda-10.0/lib64/libcudnn.so.7.4.1

Versions of relevant libraries:
[pip3] numpy==1.17.0
[pip3] torch==1.1.0
[pip3] torchvision==0.3.0
[conda] blas                      1.0                         mkl  
[conda] mkl                       2019.1                      144  
[conda] mkl-service               1.1.2            py37he904b0f_5  
[conda] mkl_fft                   1.0.6            py37hd81dba3_0  
[conda] mkl_random                1.0.2            py37hd81dba3_0



cc &lt;denchmark-link:https://github.com/vincentqb&gt;@vincentqb&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/fritzo&gt;@fritzo&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/neerajprad&gt;@neerajprad&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/alicanb&gt;@alicanb&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/vishwakftw&gt;@vishwakftw&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/nikitaved&gt;@nikitaved&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/VitalyFedyunin&gt;@VitalyFedyunin&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ngimel&gt;@ngimel&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='danielcrane' date='2019-08-08T08:45:11Z'>
		This issue also seems to stem from _batch_mahalanobis, timing that function alone gives similar results to the above, I guess the first call that uses MAGMA is causing the issue.
		</comment>
		<comment id='2' author='danielcrane' date='2019-08-09T01:46:16Z'>
		After some tests I'm also seeing the same issue when using CPU mode, so it's not restricted to GPU.
		</comment>
		<comment id='3' author='danielcrane' date='2019-08-09T14:35:41Z'>
		It sounds like there is warmup time in the dependencies we use for this computation. I'm not sure how much we can do about this. Can you report an upstream bug to MAGMA?
		</comment>
		<comment id='4' author='danielcrane' date='2020-10-10T21:26:40Z'>
		Closing, warmup issues are unavoidable with lazy initialization (it might be not magma issue, magma itself could be initializing cublas handles).
		</comment>
	</comments>
</bug>