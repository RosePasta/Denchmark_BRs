<bug id='7129' author='yunkaili' open_date='2018-05-01T08:00:28Z' closed_time='2019-04-03T20:00:26Z'>
	<summary>specific batchsize illegal memory access</summary>
	<description>
If you have a question or would like help and support, please ask at our
&lt;denchmark-link:https://discuss.pytorch.org/&gt;forums&lt;/denchmark-link&gt;
.
If you are submitting a feature request, please preface the title with [feature request].
If you are submitting a bug report, please fill in the following details.
&lt;denchmark-h:h2&gt;Issue description&lt;/denchmark-h&gt;

specific batchsize illegal memory access
running with batchsize=2 will cause illegal memory access
while batchsize=4 will not
&lt;denchmark-h:h2&gt;Code example&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import torch
 
  data = torch.autograd.Variable(torch.rand(2,2048,4,8,11)).cuda()
  data_success = torch.autograd.Variable(torch.rand(4,2048,4,8,11)).cuda()

  conv3 = torch.nn.Conv3d(2048, 512, kernel_size=(3,1,1), stride=(1,1,1),padding=(1,0,0),
                                              bias=False).cuda()
 
  out = conv3.forward(data)
  print(out)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;System Info&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;THCudaCheck FAIL file=/pytorch/aten/src/THC/generic/THCTensorCopy.c line=70 error=77 : an illegal memory access was encountered
Traceback (most recent call last):
  File "test.py", line 8, in &lt;module&gt;
    print(out)
  File "/usr/local/lib/python3.5/dist-packages/torch/tensor.py", line 57, in __repr__
    return torch._tensor_str._str(self)
  File "/usr/local/lib/python3.5/dist-packages/torch/_tensor_str.py", line 218, in _str
    fmt, scale, sz = _number_format(self)
  File "/usr/local/lib/python3.5/dist-packages/torch/_tensor_str.py", line 79, in _number_format
    tensor = torch.DoubleTensor(tensor.size()).copy_(tensor).abs_().view(tensor.nelement())
RuntimeError: cuda runtime error (77) : an illegal memory access was encountered at /pytorch/aten/src/THC/generic/THCTensorCopy.c:70
&lt;/denchmark-code&gt;

[environment collection script]
PyTorch version: 0.4.0
Is debug build: No
CUDA used to build PyTorch: 9.0.176
OS: Ubuntu 16.04.4 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
CMake version: version 3.5.1
Python version: 3.5
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: GeForce GTX TITAN X
Nvidia driver version: 390.30
cuDNN version: Probably one of the following:
/usr/local/cuda-8.0/cuda/lib64/libcudnn.so.7.0.5
/usr/local/cuda-8.0/cuda/lib64/libcudnn_static.a
/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.20
/usr/local/cuda-8.0/lib64/libcudnn.so.7.0.5
/usr/local/cuda-8.0/lib64/libcudnn.so.7.old
/usr/local/cuda-8.0/lib64/libcudnn_static.a
/usr/local/cudnn-v50/lib64/libcudnn.so.5.0.5
/usr/local/cudnn-v50/lib64/libcudnn_static.a
/usr/local/cudnn-v60/lib64/libcudnn.so
/usr/local/cudnn-v60/lib64/libcudnn.so.6
/usr/local/cudnn-v60/lib64/libcudnn.so.6.0.20
/usr/local/cudnn-v60/lib64/libcudnn_static.a
Versions of relevant libraries:
[pip3] numpy (1.14.2)
[pip3] torch (0.4.0)
[pip3] torchvision (0.2.0)
[conda] Could not collect

PyTorch or Caffe2: Pytorch
How you installed PyTorch (conda, pip, source): pip
Build command you used (if compiling from source): sudo -HE pip3 install -U pytorch-0.4.0.wheel
OS: ubuntu
PyTorch version: 0.4.0
Python version: 3.5.2
CUDA/cuDNN version:
GPU models and configuration:
GCC version (if compiling from source):
CMake version:
Versions of any other relevant libraries:

	</description>
	<comments>
		<comment id='1' author='yunkaili' date='2018-05-01T10:49:10Z'>
		I can't reproduce this from master on a titanX.
What GPU are you using? Also you may want to run with CUDA_LAUNCH_BLOCKING=1 to get a proper error message.
		</comment>
		<comment id='2' author='yunkaili' date='2018-05-01T20:50:44Z'>
		I can reproduce this on master, on a machine with 2 GPUs. This is the stack trace with CUDA_LAUNCH_BLOCKING=1:
&lt;denchmark-code&gt;$ CUDA_LAUNCH_BLOCKING=1 python batchsize.py
Traceback (most recent call last):
  File "batchsize.py", line 9, in &lt;module&gt;
    out = conv3.forward(data)
  File "/private/home/rzou/pytorch/pytorch/torch/nn/modules/conv.py", line 421, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDNN_STATUS_EXECUTION_FAILED
&lt;/denchmark-code&gt;

not really sure what to make of it
		</comment>
		<comment id='3' author='yunkaili' date='2018-05-01T20:57:24Z'>
		Both batch sizes fail for me. My theory is that you're running out of GPU memory (I saw this post: &lt;denchmark-link:https://discuss.pytorch.org/t/cudnn-status-execution-failed/4441/19&gt;https://discuss.pytorch.org/t/cudnn-status-execution-failed/4441/19&lt;/denchmark-link&gt;
)
		</comment>
		<comment id='4' author='yunkaili' date='2018-05-02T05:36:39Z'>
		I don't think it should OOM.. Neither the input size nor the output size is large.
		</comment>
		<comment id='5' author='yunkaili' date='2018-05-03T07:22:44Z'>
		i think it is related with cudnn.
because if running with cpu, nothing happens.
		</comment>
		<comment id='6' author='yunkaili' date='2018-05-03T07:28:39Z'>
		due to the fact that i am using cuda-8.0
i reinstall the pytorch with pip3 install -U torch torchvision
but the problem remains
		</comment>
		<comment id='7' author='yunkaili' date='2018-05-03T07:44:57Z'>
		using CUDA_LAUNCH_BLOCKING=1
i got the following info
Traceback (most recent call last):
File "test.py", line 9, in 
out = conv3.forward(data)
File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/conv.py", line 421, in forward
self.padding, self.dilation, self.groups)
RuntimeError: CUDNN_STATUS_EXECUTION_FAILED
		</comment>
		<comment id='8' author='yunkaili' date='2018-06-07T14:16:02Z'>
		with CUDA_LAUNCH_BLOCKING=1
python 3.5
pytorch 0.4
Win 10
CUDA 8.0
cudnn 6
I get the following stack trace:

File "C:\Users\owner\AppData\Local\Programs\Python\Python35\lib\site-packages\torch\nn\functional.py", line 1334, in nll_loss
return torch._C._nn.nll_loss2d(input, target, weight, size_average, ignore_index, reduce)
RuntimeError: cuda runtime error (77) : an illegal memory access was encountered at c:\users\administrator\downloads\new-builder\win-wheel\pytorch\aten\src\thcunn\generic/SpatialClassNLLCriterion.cu:131

This code is using CrossEntropyLoss, but another code that uses BCEWithLogits doen't throw any exceptions.
Update: This exception is thrown on the second iteration
		</comment>
		<comment id='9' author='yunkaili' date='2018-06-07T14:33:03Z'>
		&lt;denchmark-link:https://github.com/adizhol&gt;@adizhol&lt;/denchmark-link&gt;
 could you post a reproducing code snippet, possibly in a new issue? The code at the top of this issue doesn't call CrossEntropyLoss.
		</comment>
		<comment id='10' author='yunkaili' date='2018-08-18T08:14:18Z'>
		&lt;denchmark-link:https://github.com/zou3519&gt;@zou3519&lt;/denchmark-link&gt;
 Any updates to this thread? I have a very similar problem with pytorch 0.4 and CUDA 9.0. The Conv3d model works on some batch sizes but crashes on many others. Thanks
		</comment>
		<comment id='11' author='yunkaili' date='2018-08-31T09:19:21Z'>
		Hi all, I met the same situation, this error occurs when I have a large batchsize, and it disappear when I reduce beamsize. Meanwhile I watched GPU memery with nvidia-smi, it's not a problem about OOM.
It's weird beacasue when I debug with pdb, this error occurs later than before.
		</comment>
		<comment id='12' author='yunkaili' date='2018-09-07T04:41:16Z'>
		Any update on this? I get the same error. I am sure it is not a GPU OOM issue. I am getting an error when I am moving the model from CPU to GPU. Below is the stack trace:
&lt;denchmark-code&gt;THCudaCheck FAIL file=/pytorch/aten/src/THC/generic/THCTensorCopy.c line=20 error=77 : an illegal memory access was encountered
 Traceback (most recent call last):
   File "train.py", line 86, in &lt;module&gt;
    main(model_path, config, data_path, log_dir)
   File "train.py", line 26, in main
     model = make_model(model_config, train_dataloader)
   File "/var/storage/shared/i/yash/st/models/__init__.py", line 13, in make_model
     model.cuda() if torch.cuda.is_available() else model.cpu()
   File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 249, in cuda
    return self._apply(lambda t: t.cuda(device))
   File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 176, in _apply
     module._apply(fn)
   File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py", line 111, in _apply
     ret = super(RNNBase, self)._apply(fn)
   File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 182, in _apply
    param.data = fn(param.data)
    File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 249, in &lt;lambda&gt;
     return self._apply(lambda t: t.cuda(device))
RuntimeError: cuda runtime error (77) : an illegal memory access was encountered at /pytorch/aten/src/THC/generic/THCTensorCopy.c:20
&lt;/denchmark-code&gt;

What is super weird is that I get this error with a batch size of 32 and not with 30.
		</comment>
		<comment id='13' author='yunkaili' date='2018-09-07T04:48:50Z'>
		&lt;denchmark-link:https://github.com/yasheshgaur&gt;@yasheshgaur&lt;/denchmark-link&gt;
 I found a workaround actually. By setting torch.backends.cudnn.enabled=False, the problem is gone... I'm still not sure why but I got the idea from &lt;denchmark-link:https://discuss.pytorch.org/t/cudnn-status-execution-failed/4441/9&gt;https://discuss.pytorch.org/t/cudnn-status-execution-failed/4441/9&lt;/denchmark-link&gt;
, and it works for my case...
		</comment>
		<comment id='14' author='yunkaili' date='2018-09-07T05:02:31Z'>
		&lt;denchmark-link:https://github.com/xyang35&gt;@xyang35&lt;/denchmark-link&gt;
 this would mean that I won't be able to use CUDNN. Won't be happy with that. :(
Do you know how much performance loss is expected when we disable cudnn?
		</comment>
		<comment id='15' author='yunkaili' date='2018-10-09T03:44:41Z'>
		I am still facing this problem.
torch.backends.cudnn.enabled=False did not work for me.
Does anyone solve it?
		</comment>
		<comment id='16' author='yunkaili' date='2018-12-03T01:31:37Z'>
		Have you solved it? I meet the same problem
		</comment>
		<comment id='17' author='yunkaili' date='2018-12-26T09:51:13Z'>
		Hi, has someone solved this problem? I have also meet this annoying problem.
		</comment>
		<comment id='18' author='yunkaili' date='2019-01-05T14:25:00Z'>
		Has anybody solved this problem? I met a similar issue. This error is really arbitrary and it happened after uncertain training steps. I am sure it is not due to OOM.
		</comment>
		<comment id='19' author='yunkaili' date='2019-01-07T06:57:44Z'>
		Does not reproduce for me on master (&lt;denchmark-link:https://github.com/pytorch/pytorch/commit/32777231730983aa38bb92624ca0b30fd75fb521&gt;3277723&lt;/denchmark-link&gt;
) with:
&lt;denchmark-code&gt;PyTorch version: 1.1.0a0+3277723
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 16.04.4 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.12.2

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: Tesla M60
GPU 1: Tesla M60

Nvidia driver version: 390.57
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] Could not collect
[conda] blas                      1.0                         mkl  
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] mkl_fft                   1.0.6            py37hd81dba3_0  
[conda] mkl_random                1.0.2            py37hd81dba3_0  
[conda] torch                     1.1.0a0+3277723           &lt;pip&gt;
&lt;/denchmark-code&gt;

For those of you who are seeing this problem, please post your system configuration as collected by &lt;denchmark-link:https://github.com/pytorch/pytorch/blob/master/torch/utils/collect_env.py&gt;https://github.com/pytorch/pytorch/blob/master/torch/utils/collect_env.py&lt;/denchmark-link&gt;
 it will help us get more information about how to debug the problem.
		</comment>
		<comment id='20' author='yunkaili' date='2019-03-02T19:56:15Z'>
		I have narrowed it down to the last line of
&lt;denchmark-code&gt;class RemapByInds(nn.Module):
    def __init__(self, null_val=0):
        super(RemapByInds, self).__init__()
        self.null_val = null_val

    def forward(self, x, inds):
        time_steps_x = torch.ones_like(x[:, 0, :]).cumsum(dim=1) - 1
        batch_num_x = torch.ones_like(x[:, 0, :]).cumsum(dim=0) - 1
        out = torch.zeros_like(x) + self.null_val
        out_inds = torch.cat((time_steps_x[inds[:, 0], inds[:, 1]].view(-1, 1),
                              batch_num_x[inds[:, 0], inds[:, 1]].view(-1, 1)), dim=1).type(torch.long)
        out[out_inds[:, 0], :, out_inds[:, 1]] = x[inds[:, 0], :, inds[:, 1]]
        return out
    
&lt;/denchmark-code&gt;

x is a 3d tensor ( batch x features x timesteps ) float32, inds is a 2d ( mx2 )  tensor int64
		</comment>
		<comment id='21' author='yunkaili' date='2019-03-11T19:15:09Z'>
		&lt;denchmark-link:https://github.com/danFromTelAviv&gt;@danFromTelAviv&lt;/denchmark-link&gt;
 the way to reproduce illegal memory access is highly dependent on the value of , because  might be referring to an out of bounds value.
In such a case, we do want to give you a device assert, rather than "illegal memory access". Can you provide example snippets of code to construct the  and  and reproduce the error?
		</comment>
		<comment id='22' author='yunkaili' date='2019-03-12T08:19:49Z'>
		&lt;denchmark-link:https://github.com/soumith&gt;@soumith&lt;/denchmark-link&gt;
 thanks. I actually got it to work eventually.
in my case inds were calculated with nonzeros on a tensor of the same shape as x... so it would have to explicitly be within range.
For the benefit of other users:
If my memory is accurate I think the issue was when inds was empty. I used a regularization loss to push inds away from being empty and that got my network to train without issues.
		</comment>
		<comment id='23' author='yunkaili' date='2019-03-27T11:06:57Z'>
		I am facing a very similar problem (possibly the same described here), but with the number of output channels of a 2D convolution. I have found a simple way to reproduce the error. Please indicate if I should open a new issue.
Notice that it works with 40 output channels, it does not work with 41:
&lt;denchmark-code&gt;Python 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) 
[GCC 7.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; import torch
&gt;&gt;&gt; from torch import nn
&gt;&gt;&gt; device = torch.device("cuda")
&gt;&gt;&gt; ones = torch.ones(32, 32, 57, 35).to(device)
&gt;&gt;&gt; n = nn.Conv2d(32, 40, 5, 2).to(device)
&gt;&gt;&gt; out = n(ones)
&gt;&gt;&gt; n = nn.Conv2d(32, 41, 5, 2).to(device)
&gt;&gt;&gt; out = n(ones)
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "/home/vkmetzsc/.conda/envs/env36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/vkmetzsc/.conda/envs/env36/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 320, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED
&lt;/denchmark-code&gt;

The output of collect_env.py is:
&lt;denchmark-code&gt;Collecting environment information...
PyTorch version: 1.0.1.post2
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 16.04.6 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: GeForce RTX 2080 Ti
Nvidia driver version: 410.48
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.10
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.5.0

Versions of relevant libraries:
[pip] numpy==1.15.4
[pip] numpydoc==0.8.0
[pip] torch==1.0.1.post2
[pip] torchvision==0.2.1
[conda] Could not collect
&lt;/denchmark-code&gt;

		</comment>
		<comment id='24' author='yunkaili' date='2019-03-30T22:37:46Z'>
		&lt;denchmark-link:https://github.com/Virgiliok&gt;@Virgiliok&lt;/denchmark-link&gt;
 CUDA 9 doesn't work with RTX cards. Could you try a CUDA 10 build?
		</comment>
		<comment id='25' author='yunkaili' date='2019-04-03T09:18:00Z'>
		Thank you, &lt;denchmark-link:https://github.com/SsnL&gt;@SsnL&lt;/denchmark-link&gt;
. Using a CUDA 10 build solved my problem.
		</comment>
		<comment id='26' author='yunkaili' date='2019-04-03T20:00:26Z'>
		I'm going to go ahead and close this as a CUDA/CuDNN bug that is fixed in the latest version. If you are still seeing this problem with CUDA 10, please file a new bug.
		</comment>
	</comments>
</bug>