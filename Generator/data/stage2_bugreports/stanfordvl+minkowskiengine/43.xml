<bug id='43' author='HughRunyan' open_date='2019-11-01T16:28:21Z' closed_time='2019-12-12T23:30:28Z'>
	<summary>RuntimeError: CUDA error: invalid configuration argument in pytorch batch normalization on GPUs with VRAM&amp;gt;16G</summary>
	<description>
I'm getting this error on a Tesla V100 (CUDA 10.0) card I don't see on 1080Ti (CUDA 9.0) with same code.
&lt;denchmark-code&gt;&lt;ipython-input-10-0312a913726b&gt; in &lt;module&gt;
     17         #batch['y']=batch['y'].cuda()
     18         labels = torch.cat(batch[2]).long().cuda()
---&gt; 19         predictions = unet(input)
     20         loss = criterion(predictions.F, labels)
     21         #loss = criterion(predictions, labels)

~/miniconda3/envs/torch10/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    539             result = self._slow_forward(*input, **kwargs)
    540         else:
--&gt; 541             result = self.forward(*input, **kwargs)
    542         for hook in self._forward_hooks.values():
    543             hook_result = hook(self, input, result)

/chei-ml/plots/fr3/2013/all_labels/examples/minkunet.py in forward(self, x)
    122     def forward(self, x):
    123         out = self.conv0p1s1(x)
--&gt; 124         out = self.bn0(out)
    125         out_p1 = self.relu(out)
    126 

~/miniconda3/envs/torch10/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    539             result = self._slow_forward(*input, **kwargs)
    540         else:
--&gt; 541             result = self.forward(*input, **kwargs)
    542         for hook in self._forward_hooks.values():
    543             hook_result = hook(self, input, result)

~/miniconda3/envs/torch10/lib/python3.7/site-packages/MinkowskiEngine-0.2.8-py3.7-linux-x86_64.egg/MinkowskiEngine/MinkowskiNormalization.py in forward(self, input)
     56 
     57     def forward(self, input):
---&gt; 58         output = self.bn(input.F)
     59         return SparseTensor(
     60             output,

~/miniconda3/envs/torch10/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    539             result = self._slow_forward(*input, **kwargs)
    540         else:
--&gt; 541             result = self.forward(*input, **kwargs)
    542         for hook in self._forward_hooks.values():
    543             hook_result = hook(self, input, result)

~/miniconda3/envs/torch10/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py in forward(self, input)
     70             # TODO: if statement only here to tell the jit to skip emitting this when it is None
     71             if self.num_batches_tracked is not None:
---&gt; 72                 self.num_batches_tracked += 1
     73                 if self.momentum is None:  # use cumulative moving average
     74                     exponential_average_factor = 1.0 / float(self.num_batches_tracked)

RuntimeError: CUDA error: invalid configuration argument
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='HughRunyan' date='2019-11-01T18:56:29Z'>
		So, as you can see. This error is coming from /torch/nn/modules/batchnorm.py
As stated in the MinkowskiNet paper, &lt;denchmark-link:https://arxiv.org/abs/1904.08755&gt;https://arxiv.org/abs/1904.08755&lt;/denchmark-link&gt;
, we are using the pytorch's batchnormalization function. This is a pretty shallow wrapper &lt;denchmark-link:https://github.com/StanfordVL/MinkowskiEngine/blob/master/MinkowskiEngine/MinkowskiNormalization.py#L58&gt;https://github.com/StanfordVL/MinkowskiEngine/blob/master/MinkowskiEngine/MinkowskiNormalization.py#L58&lt;/denchmark-link&gt;
.
So that means that the CUDA error can only happen inside the pytorch's cuda code since it doesn't have the CUDA code.
For the cause of error, I am not exactly sure, but I vaguely suspect this is the CUDA thread configuration such as grid size specification is wrong in pytorch. Also, this error only happens when the number of points in the sparse tensor exceeds a certain point. Also, this happened on bn0 which handles &gt; 1.5M points which doesn't happen on 1080. I didn't have much time to dig deeper into the torch cuda code to see what is causing this, but it seems to happen very rarely.
One way to “handle” this error is &lt;denchmark-link:https://github.com/chrischoy/SpatioTemporalSegmentation/blob/master/lib/train.py#L72&gt;https://github.com/chrischoy/SpatioTemporalSegmentation/blob/master/lib/train.py#L72&lt;/denchmark-link&gt;

I’ll try pytorch v1.3 and see if this is fixed.
		</comment>
	</comments>
</bug>