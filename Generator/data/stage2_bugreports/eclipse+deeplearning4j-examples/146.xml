<bug id='146' author='mannipa' open_date='2016-05-25T16:02:50Z' closed_time='2016-06-10T16:12:48Z'>
	<summary>Error when run from nd4j-cuda-7.5 backend</summary>
	<description>
I run dl4j-0.4-examples(0.4-rc3.9) on cuda 7.5, but turn to an error: Please ensure that you have an nd4j backend on your classpath
(1) I had edited pom.xml and replaced "nd4j-native" to "nd4j-cuda-7.5". The sample code is in my test repo "&lt;denchmark-link:https://github.com/hellojensens/dl4j-0.4-examples.git&gt;https://github.com/hellojensens/dl4j-0.4-examples.git&lt;/denchmark-link&gt;
"
&lt;nd4j.backend&gt;nd4j-cuda-7.5&lt;/nd4j.backend&gt;
(2) Run from IntelliJ
(3) Error: Please ensure that you have an nd4j backend on your classpath.
My system is OS X 10.11.1 with CUDA 7.5 installed.
Thanks!
	</description>
	<comments>
		<comment id='1' author='mannipa' date='2016-06-10T13:13:21Z'>
		I'm experiencing the same thing:
Cuda 7.5 installed
Linux mint 17.3.
Cuda and NVIDIA driver are installed correctly
		</comment>
		<comment id='2' author='mannipa' date='2016-06-10T13:24:40Z'>
		First: can you check that nvcc is actually on your path environment variable?
		</comment>
		<comment id='3' author='mannipa' date='2016-06-10T13:30:39Z'>
		Note here: Java has to be able to see it on the PATH. So if you have added Cuda to you PATH variable via .bashrc, then it isn't visible to things like IntelliJ that you have started from the Desktop.
Define it in your .profile file, and logout and back in. That should make your changes to the PATH variable visible to things that aren't started from the shell.
		</comment>
		<comment id='4' author='mannipa' date='2016-06-10T14:42:01Z'>
		Treo got it right: eclipse did not read the PATH variable that was modified in .bashrc
I placed this line in .profile and the change was propagated into eclipse:
PATH="/usr/local/cuda/bin:$PATH"
HOWEVER I also placed this line in .profile but the variable IS NOT created and not available in eclipse!
export LD_LIBRARY_PATH=/usr/local/cuda/lib64
Anyways, I started eclipse from a terminal and both variables were available in eclipse but when I start I get lots of errors like this:
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:4278 code=13(cudaErrorInvalidSymbol) "result"
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:4278 code=13(cudaErrorInvalidSymbol) "result"
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:4278 code=13(cudaErrorInvalidSymbol) "result"
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:4278 code=13(cudaErrorInvalidSymbol) "result"
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:4278 code=13(cudaErrorInvalidSymbol) "result"
		</comment>
		<comment id='5' author='mannipa' date='2016-06-10T14:58:33Z'>
		Now next question: which CUDA version do you actually have on your system?
P.S. i mean full CUDA Toolkit version
P.P.S. and what CUDA device you have there?
		</comment>
		<comment id='6' author='mannipa' date='2016-06-10T15:10:22Z'>
		CUDA 7.5
$ nvidia-smi
+------------------------------------------------------+
| NVIDIA-SMI 352.93     Driver Version: 352.93         |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GT 435M     Off  | 0000:02:00.0     N/A |                  N/A |
| N/A   55C    P0    N/A /  N/A |    204MiB /  2047MiB |     N/A      Default |
+-------------------------------+----------------------+----------------------+
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0                  Not Supported                                         |
+-----------------------------------------------------------------------------+
nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2015 NVIDIA Corporation
Built on Tue_Aug_11_14:27:32_CDT_2015
Cuda compilation tools, release 7.5, V7.5.17
		</comment>
		<comment id='7' author='mannipa' date='2016-06-10T15:21:49Z'>
		Can i also have deviceQuery output?
		</comment>
		<comment id='8' author='mannipa' date='2016-06-10T15:23:50Z'>
		CUDA Device Query (Runtime API) version (CUDART static linking)
Detected 1 CUDA Capable device(s)
Device 0: "GeForce GT 435M"
CUDA Driver Version / Runtime Version          7.5 / 7.5
CUDA Capability Major/Minor version number:    2.1
Total amount of global memory:                 2047 MBytes (2146631680 bytes)
( 2) Multiprocessors, ( 48) CUDA Cores/MP:     96 CUDA Cores
GPU Max Clock rate:                            1300 MHz (1.30 GHz)
Memory Clock rate:                             800 Mhz
Memory Bus Width:                              128-bit
L2 Cache Size:                                 131072 bytes
Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65535), 3D=(2048, 2048, 2048)
Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers
Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers
Total amount of constant memory:               65536 bytes
Total amount of shared memory per block:       49152 bytes
Total number of registers available per block: 32768
Warp size:                                     32
Maximum number of threads per multiprocessor:  1536
Maximum number of threads per block:           1024
Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
Max dimension size of a grid size    (x,y,z): (65535, 65535, 65535)
Maximum memory pitch:                          2147483647 bytes
Texture alignment:                             512 bytes
Concurrent copy and kernel execution:          Yes with 1 copy engine(s)
Run time limit on kernels:                     Yes
Integrated GPU sharing Host Memory:            No
Support host page-locked memory mapping:       Yes
Alignment requirement for Surfaces:            Yes
Device has ECC support:                        Disabled
Device supports Unified Addressing (UVA):      Yes
Device PCI Domain ID / Bus ID / location ID:   0 / 2 / 0
Compute Mode:
&lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;
deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 7.5, CUDA Runtime Version = 7.5, NumDevs = 1, Device0 = GeForce GT 435M
Result = PASS
		</comment>
		<comment id='9' author='mannipa' date='2016-06-10T15:25:41Z'>
		CUDA Capability Major/Minor version number: 2.1
I'm afraid that's an issue, we don't support cc below 3.0 :(
		</comment>
		<comment id='10' author='mannipa' date='2016-06-10T15:27:16Z'>
		How do I get version 3.0 or higher? Is it a HW problem?
		</comment>
		<comment id='11' author='mannipa' date='2016-06-10T15:27:55Z'>
		That's EXACTLY hardware problem. Your gpu is just too old.
Hypothetically it might be possible to compile and run on it, but i have no access to such hardware to test it, and i don't see any sense for it - it's just slow.
		</comment>
		<comment id='12' author='mannipa' date='2016-06-10T15:37:43Z'>
		Bummer.
I have a newer laptop but it runs windows 7. it might be painful to get it to work there.
That guy has GeForceGT 740M. Would that work?
		</comment>
		<comment id='13' author='mannipa' date='2016-06-10T15:49:47Z'>
		Yes, 740m is cc 3.0, so that should work.
But don't expect too much there, CUDA isn't a magic, and on slow devices - it is slow as well :)
		</comment>
		<comment id='14' author='mannipa' date='2016-06-10T15:51:22Z'>
		Thanks
		</comment>
		<comment id='15' author='mannipa' date='2016-11-07T16:02:59Z'>
		I got similar error  when running newest deeplearning4j-examples codes on Windows 10:
&lt;denchmark-code&gt;CUDA error at C:/projects/skymind_deploy/libnd4j/blas/cuda/NativeOps.cu:5334 code=13(cudaErrorInvalidSymbol) "result" 
&lt;/denchmark-code&gt;

This is my  deviceQuery output, should I update CUDA to 8.0 ?
&lt;denchmark-code&gt;CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 1 CUDA Capable device(s)

Device 0: "GeForce GTX 1060 6GB"
  CUDA Driver Version / Runtime Version          8.0 / 7.5
  CUDA Capability Major/Minor version number:    6.1
  Total amount of global memory:                 6144 MBytes (6442450944 bytes)
MapSMtoCores for SM 6.1 is undefined.  Default to use 128 Cores/SM
MapSMtoCores for SM 6.1 is undefined.  Default to use 128 Cores/SM
  (10) Multiprocessors, (128) CUDA Cores/MP:     1280 CUDA Cores
  GPU Max Clock rate:                            1785 MHz (1.78 GHz)
  Memory Clock rate:                             4004 Mhz
  Memory Bus Width:                              192-bit
  L2 Cache Size:                                 1572864 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  2048
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)
  Run time limit on kernels:                     Yes
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Disabled
  CUDA Device Driver Mode (TCC or WDDM):         WDDM (Windows Display Driver Model)
  Device supports Unified Addressing (UVA):      Yes
  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0
  Compute Mode:
     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, CUDA Runtime Version = 7.5, NumDevs = 1, Device0 = GeForce GTX 1060 6GB
Result = PASS
&lt;/denchmark-code&gt;

Amazing! After updating CUDA to 8.0.44 (even without CUDA 7.5 uninstallation). Everything works fine !
		</comment>
		<comment id='16' author='mannipa' date='2016-11-07T16:19:58Z'>
		Since your device supports cuda 8.0 only, yes - you should use nd4j-cuda-8.0 backend.
		</comment>
		<comment id='17' author='mannipa' date='2017-04-27T12:15:03Z'>
		I am trying to get it to work on Windows, since that is my only machine with an Nvidia GPU(GTX 960). I updated the pom.xml, installed CUDA 8 and added C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\bin to PATH but I am still getting "Caused by: java.lang.RuntimeException: org.nd4j.linalg.factory.Nd4jBackend$NoAvailableBackendException: Please ensure that you have an nd4j backend on your classpath. Please see: &lt;denchmark-link:http://nd4j.org/getstarted.html&gt;http://nd4j.org/getstarted.html&lt;/denchmark-link&gt;
"
		</comment>
		<comment id='18' author='mannipa' date='2017-04-27T12:15:53Z'>
		Mind jumping in to gitter? We need a ton more info from you. &lt;denchmark-link:https://gitter.im/deeplearning4j/deeplearning4j&gt;https://gitter.im/deeplearning4j/deeplearning4j&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>