<bug id='719' author='arnoegw' open_date='2021-01-08T17:17:19Z' closed_time='2021-01-15T15:15:15Z'>
	<summary>SavedModel built with BERT preprocessing models keeps using Hub's original vocab file path</summary>
	<description>
From &lt;denchmark-link:https://stackoverflow.com/questions/65511729&gt;https://stackoverflow.com/questions/65511729&lt;/denchmark-link&gt;
 by mattdeak:
Saving a Keras Model that involves hub.KerasLayer("&lt;denchmark-link:https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&gt;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&lt;/denchmark-link&gt;
") or ".../1" creates a SavedModel that keeps referring to vocab.txt for table initialization at the location from which the TF Hub model was loaded, not from the actual location of the new SavedModel.
I can reproduce the problem, but more analysis is needed.
	</description>
	<comments>
		<comment id='1' author='arnoegw' date='2021-01-12T08:07:52Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/46293&gt;https://github.com/tensorflow/tensorflow/issues/46293&lt;/denchmark-link&gt;

Having the same issue here.
		</comment>
		<comment id='2' author='arnoegw' date='2021-01-12T08:44:53Z'>
		Is there any way to bypass this problem for now? I kind of blocked here by this issue. I've created Docker container to mitigate this issue for now but we have to deploy our SavedModel to cloud and my solution won't really help.
&lt;denchmark-code&gt;FROM tensorflow/serving:2.4.0

COPY ./modelname/ /models/modelname/1
COPY ./modelnameassets/vocab.txt /tmp/tfhub_modules/09bd4e665682e6f03bc72fbcff7a68bf6879910e/assets/vocab.txt

ENV MODEL_NAME=modelname
&lt;/denchmark-code&gt;

Maybe there's a way to manually patch the vocab.txt path in the model before saving it?
		</comment>
		<comment id='3' author='arnoegw' date='2021-01-12T17:44:55Z'>
		I'm hoping to publish new preprocessing models within the next few days that avoid this issue, please stay tuned.
		</comment>
		<comment id='4' author='arnoegw' date='2021-01-15T15:27:56Z'>
		The problem should be fixed with &lt;denchmark-link:https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3&gt;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3&lt;/denchmark-link&gt;
. (The model content is live, but documentation needs some time to propagate to the tfhub.dev UI. Other docs will be updated soon.)
My analysis of the root cause is in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/46456&gt;tensorflow/tensorflow#46456&lt;/denchmark-link&gt;
. The updated models were built with the workaround from commit &lt;denchmark-link:https://github.com/tensorflow/models/commit/d03930565a70997b10e1d252c2d521587716af18&gt;tensorflow/models@d039305&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>