<bug id='91' author='snarb' open_date='2018-06-27T14:26:59Z' closed_time='2019-02-28T17:55:32Z'>
	<summary>flip_left_right True causes retrain.py to hand in the midle of the training</summary>
	<description>
Tested on the 20k * 3 classes dataset. Without flip_left_right True all works fine. With flip_left_right True training is hanging in the process. Last message "INFO:tensorflow:2018-06-27 14:23:49.600421: Step 310: Train accuracy = 78.0%". Tested several times.
	</description>
	<comments>
		<comment id='1' author='snarb' date='2018-06-27T14:40:35Z'>
		&lt;denchmark-link:https://github.com/arnoegw&gt;@arnoegw&lt;/denchmark-link&gt;
 might have a suggestion here
If I understand the situation correctly, training (and eval?) runs successfully for some number of steps, then the training terminates? It's a bit hard to say what is the issue here without knowing additional details. What is your training setup (os, py version, gpu/cpu, etc)? How does resource usage looks during training, how much memory/cpu is being used?  How does cpu/memory usage looks like as the training progresses?
		</comment>
		<comment id='2' author='snarb' date='2018-06-27T14:45:37Z'>
		Yes, training is running successfully for some number of step(different sometimes 310 sometimes 40) then it hangs for ever, no crash. This is ubuntu 16. Tf 1.8 inside the Jupiter. Master version of the hub I have dropped this setting and have no problems now. Just a feedback. I think some kind of leak. If you are interesing I can do some check.
		</comment>
		<comment id='3' author='snarb' date='2018-06-27T14:50:46Z'>
		That (memory leak) was my initial guess as well. One option to validate is
try to profile using TF profiler (
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/README.md&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/README.md&lt;/denchmark-link&gt;
)
and if it is indeed a memory leak in some TF op then file a bug report with
TF.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Wed, Jun 27, 2018 at 4:45 PM Pavel Konovalov ***@***.***&gt; wrote:
 Yes, training is running successfully for some number of step(different
 sometimes 310 sometimes 40). This is ubuntu 16. Tf 1.8 inside the Jupiter.
 Master version of the hub I have dropped this setting and have no problems
 now. Just a feedback. I think some kind of leak.

 —
 You are receiving this because you commented.


 Reply to this email directly, view it on GitHub
 &lt;#91 (comment)&gt;, or mute
 the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AbunTAxkmNL0yZOQbbWY6cjCISURzA4yks5uA5qTgaJpZM4U50d_&gt;
 .



		</comment>
		<comment id='4' author='snarb' date='2019-02-28T17:55:32Z'>
		Closing as innactive.
		</comment>
	</comments>
</bug>