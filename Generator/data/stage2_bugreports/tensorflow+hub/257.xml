<bug id='257' author='dav-ell' open_date='2019-03-19T15:38:48Z' closed_time='2019-03-21T10:41:52Z'>
	<summary>Semantic similarity gets slower in a for loop</summary>
	<description>
I have a function like this
&lt;denchmark-code&gt;def sentences_to_vectors(sentences, embed=None):
    if embed == None:
        embed = hub.Module("https://tfhub.dev/google/universal-sentence-encoder-large/3")
    
    with tf.Session() as session:
        session.run([tf.global_variables_initializer(), tf.tables_initializer()])
        message_embeddings = session.run(embed(sentences))

        return message_embeddings
&lt;/denchmark-code&gt;

And I run it like this:
&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow_hub as hub

embed = hub.Module("https://tfhub.dev/google/universal-sentence-encoder-large/3")
# arr_of_arr_of_strings = [ [ "hello world", "hi" ], [ "foo bar", "giraffe" ], ... ]
for arr_of_strings in arr_of_arr_of_strings:
    vectors = sentences_to_vectors(arr_of_strings, embed=embed)
&lt;/denchmark-code&gt;

This works great for the first few iterations (about 12-13 seconds with len(arr_of_strings) == 2500). However, by iteration 50, the time to run sentences_to_vectors is over one minute.
I assume I'm doing something wrong here (some state I need to reset between calls), but it's not immediately clear to me. Anyone know how to fix this, or if this has to do with the TF-Hub module?
	</description>
	<comments>
		<comment id='1' author='dav-ell' date='2019-03-19T16:07:33Z'>
		I was able to fix this by changing the function to
&lt;denchmark-code&gt;def sentences_to_vectors(sentences):
    embed = hub.Module("https://tfhub.dev/google/universal-sentence-encoder-large/3")
    with tf.Session() as session:
        session.run([tf.global_variables_initializer(), tf.tables_initializer()])
        message_embeddings = session.run(embed(sentences))

    tf.reset_default_graph()
    return message_embeddings
&lt;/denchmark-code&gt;

It appears that running the embedder module across multiple sessions resulted in a steadily increasing graph size for TF to compute. The fix simply loads the module and clears it every iteration. There may be a better way to do this that doesn't require re-loading the module every time (which may also reduce some of the computational advantages TF can use). I'm leaving this issue open for now in case someone has a better solution.
		</comment>
		<comment id='2' author='dav-ell' date='2019-03-20T20:21:21Z'>
		Thanks for sharing the work-around. We will leave this open for better solutions.
		</comment>
		<comment id='3' author='dav-ell' date='2019-03-21T10:41:48Z'>
		Hi,
this is still building the graph every time a request comes. Please take a look at: &lt;denchmark-link:https://github.com/tensorflow/hub/blob/master/docs/common_issues.md#running-inference-on-a-pre-initialized-module&gt;https://github.com/tensorflow/hub/blob/master/docs/common_issues.md#running-inference-on-a-pre-initialized-module&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='dav-ell' date='2020-08-17T11:30:16Z'>
		Hello, did anyone get this embedding working efficiently in TF2.0? I am trying to get embedding of input texts in a loop and it is still very slow, taking ~ 5 sec for each input.
		</comment>
	</comments>
</bug>