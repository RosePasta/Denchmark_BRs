<bug id='540' author='hrk21' open_date='2020-03-21T11:53:58Z' closed_time='2020-03-24T10:18:12Z'>
	<summary>No gradient defined error on bert_multi_cased_L-12_H-768_A-12</summary>
	<description>
Setting trainable=True option while loading the model using hub.KerasLayers, triggers the error below while calling model.fit.
NotFoundError: [Derived]No gradient defined for op: StatefulPartitionedCall
[[{{node Func/_4}}]]
[[PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/StatefulPartitionedCall_grad/SymbolicGradient]] [Op:__inference_distributed_function_48070]
Function call stack:
distributed_function
	</description>
	<comments>
		<comment id='1' author='hrk21' date='2020-03-23T06:24:01Z'>
		In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!
		</comment>
		<comment id='2' author='hrk21' date='2020-03-23T15:04:53Z'>
		The model:
&lt;denchmark-link:https://user-images.githubusercontent.com/10379732/77325162-a494cc80-6d39-11ea-9aaf-1af93258e527.png&gt;&lt;/denchmark-link&gt;

Fit call:
&lt;denchmark-link:https://user-images.githubusercontent.com/10379732/77329936-7d8dc900-6d40-11ea-90e3-eb6f4716ca92.png&gt;&lt;/denchmark-link&gt;

Error from cell output:
&lt;denchmark-link:https://user-images.githubusercontent.com/10379732/77330661-7dda9400-6d41-11ea-81ea-64203786c1e6.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/10379732/77330444-30f6bd80-6d41-11ea-9226-e6c024c42605.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/10379732/77330529-4cfa5f00-6d41-11ea-9e70-79a78a92a338.png&gt;&lt;/denchmark-link&gt;

This issue is also posted here:
&lt;denchmark-link:https://github.com/google-research/bert/issues/1012&gt;google-research/bert#1012&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='hrk21' date='2020-03-23T15:27:14Z'>
		This error is triggered regardless of the TF / TF-HUB Version
		</comment>
		<comment id='4' author='hrk21' date='2020-03-24T10:18:09Z'>
		Hi &lt;denchmark-link:https://github.com/hrk21&gt;@hrk21&lt;/denchmark-link&gt;
, thank you for your report. I can reproduce the problem for TF 1.15.0 (with tf.enable_v2_behavior()) but I cannot reproduce the problem with TF 2.1.0 (which is currently the default on colab.research.google.com). Based on that, I am tentatively closing this issue.
If you have a complete, self-contained reproduction for TF2.1 or TF2.2rc including pip install commands in a notebook on &lt;denchmark-link:https://colab.research.google.com&gt;https://colab.research.google.com&lt;/denchmark-link&gt;
 or as a complete Python program (as text, not screenshot), please provide it and reopen this issue.
		</comment>
		<comment id='5' author='hrk21' date='2020-03-25T20:30:54Z'>
		&lt;denchmark-link:https://github.com/arnoegw&gt;@arnoegw&lt;/denchmark-link&gt;
  I get the same error when eager execution is desabled with TF 2.1.0
		</comment>
		<comment id='6' author='hrk21' date='2020-05-16T13:55:48Z'>
		[I had a similar issue. Not sure if my solution applies. ]
I realized my hub module has several tags. Trying a more appropriate tag did have the gradient op.
Here's a code sample that you could quickly try:
For example, my module has tag '"A"' and tag '"B"'. For tag "B", the gradient op is defined, then the following should work:
with tf.GradientTape() as tape:
  some_input = tf.random.uniform(shape=(13, 224, 224, 3))

  model = hub.KerasLayer("/path/to/hub", tags='B')

  some_output = model(some_input)
  loss = tf.reduce_mean((some_output)**2)

grad = tape.gradient(loss, im)
print(grad)
Running the same with tag '"A"' will fail with the same error.
		</comment>
	</comments>
</bug>