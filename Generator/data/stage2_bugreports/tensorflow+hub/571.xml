<bug id='571' author='anurag89sharma' open_date='2020-04-29T06:41:49Z' closed_time='2020-05-11T08:40:25Z'>
	<summary>Using large vocabulary in SentencePiece Library to get sentence embeddings using USE Lite model degrades the embeddings</summary>
	<description>
I am currently working on using the Universal Sentence Encoder Lite model for getting text embeddings for our corpus to find similar items. The &lt;denchmark-link:https://tfhub.dev/google/universal-sentence-encoder-lite/2&gt;documentations &lt;/denchmark-link&gt;
 says to process the sentences using the &lt;denchmark-link:https://github.com/google/sentencepiece&gt;SentencePiece Library&lt;/denchmark-link&gt;
 before passing them to the model.
The default model of the SentencePiece library has a vocabulary size of 8K. Using this I am able to get good results. I then tried to use a larger vocabulary of size 30K from SentencePiece to see if the quality improves. I found out that the quality of the embeddings degrades.
I used annoy library to build the index of the embedding vectors to get the K nearest matching items. The results I got with the embeddings from the larger vocabulary is giving very bad results (that have no significance to the input text).
Wanted to know if the USE Lite model is specifically trained on SentencePiece 8k vocabulary.
Thank you.
	</description>
	<comments>
		<comment id='1' author='anurag89sharma' date='2020-05-11T08:40:23Z'>
		This is expected (beyond USE Lite): you can't just change the vocab of a pre-trained text embedding. The pre-training depends on the particular ids assigned by the original vocab.
		</comment>
	</comments>
</bug>