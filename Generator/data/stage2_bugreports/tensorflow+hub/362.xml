<bug id='362' author='jmiller656' open_date='2019-09-10T14:57:20Z' closed_time='2020-07-24T11:31:48Z'>
	<summary>Bug: Nondeterminism with USE; last element in batch is not equal</summary>
	<description>
Hey there ðŸ‘‹
Ran into an issue with tensorflow-hub on my machine. I was Encoding the same string several times with universal sentence encoder v1 and found that if I encode the same string multiple times, the last item in the batch will be slightly different. Here's a script I made to reproduce the problem:
&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow_hub as hub
import numpy as np

url = 'https://tfhub.dev/google/universal-sentence-encoder/1'
encoder = hub.Module(url, trainable=False)
sess = tf.Session()
sess.run([tf.global_variables_initializer(), tf.tables_initializer()])
messages = tf.placeholder(dtype=tf.string, shape=[None])
pre_embeddings = encoder(messages)
samples = ["N/A", "N/A", "N/A", "N/A", "N/A", "N/A", "N/A", "N/A", "N/A"]
emb = sess.run(pre_embeddings, feed_dict={messages: samples})

for i,j in zip(range(len(samples)-1), range(1,len(samples))):
    if not np.array_equal(emb[i], emb[j]):
        print(emb[i][:10])
        print(emb[j][:10])
        print(i, j)
&lt;/denchmark-code&gt;

here's the output I get:
&lt;denchmark-code&gt;[-0.04345907  0.07182628 -0.04540685  0.00242239  0.04771155  0.00699149
 -0.0530593   0.00340921 -0.08327182  0.0748596 ]
[-0.04345907  0.07182629 -0.04540686  0.00242239  0.04771155  0.00699149
 -0.05305931  0.00340921 -0.08327183  0.0748596 ]
7 8
&lt;/denchmark-code&gt;

You can see that the second item of each vector is slightly off, as well as a few others.
Here are the tensorflow and tf-hub versions I'm using:
&lt;denchmark-code&gt;tensorflow              1.12.0   
tensorflow-estimator    1.14.0   
tensorflow-hub          0.1.1   
&lt;/denchmark-code&gt;

I'm running on macOS Mojave 10.14.3
	</description>
	<comments>
		<comment id='1' author='jmiller656' date='2019-09-10T16:27:50Z'>
		&lt;denchmark-link:https://github.com/jmiller656&gt;@jmiller656&lt;/denchmark-link&gt;
,
This is a known issue for version 1, we fixed it in the second version. Please switch to v2. Thanks!
		</comment>
		<comment id='2' author='jmiller656' date='2019-09-10T17:24:22Z'>
		Hmm, doesn't look like switching to v2 fixes it. Ran the following and still got the same output as above:
&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow_hub as hub
import numpy as np

url = 'https://tfhub.dev/google/universal-sentence-encoder/2'
encoder = hub.Module(url, trainable=False)
sess = tf.Session()
sess.run([tf.global_variables_initializer(), tf.tables_initializer()])
messages = tf.placeholder(dtype=tf.string, shape=[None])
pre_embeddings = encoder(messages)
samples = ["N/A", "N/A", "N/A", "N/A", "N/A", "N/A", "N/A", "N/A", "N/A"]
emb = sess.run(pre_embeddings, feed_dict={messages: samples})

for i,j in zip(range(len(samples)-1), range(1,len(samples))):
    if not np.array_equal(emb[i], emb[j]):
        print(emb[i][:10])
        print(emb[j][:10])
        print(i, j)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='jmiller656' date='2020-07-24T11:31:48Z'>
		&lt;denchmark-link:https://github.com/jmiller656&gt;@jmiller656&lt;/denchmark-link&gt;
 - are you still facing this issue? If so, please reopen this bug and we'll follow up.
		</comment>
	</comments>
</bug>