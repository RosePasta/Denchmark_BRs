<bug id='447' author='acmilannesta' open_date='2019-12-15T02:59:13Z' closed_time='2020-01-06T09:24:47Z'>
	<summary>Fail to wrap albert output layer using tfhub.module</summary>
	<description>
I used albert_base_v2 and tfhub on python3.6 and tensorflow 1.15.0.
I tried to wrap the albert output layer with a pooling layer. And it gives the following erros:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
FailedPreconditionError                   Traceback (most recent call last)
&lt;ipython-input-6-d725e62752e5&gt; in &lt;module&gt;()
----&gt; 1 dense = keras.layers.Lambda(lambda x: x[:, 0])(albert_outputs)

7 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in __call__(self, *args, **kwargs)
   1470         ret = tf_session.TF_SessionRunCallable(self._session._session,
   1471                                                self._handle, args,
-&gt; 1472                                                run_metadata_ptr)
   1473         if run_metadata:
   1474           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

FailedPreconditionError: 2 root error(s) found.
  (0) Failed precondition: Error while reading resource variable module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias)
	 [[{{node module_apply_tokens/bert/encoder/transformer/group_0_11/layer_11/inner_group_0/ffn_1/intermediate/output/dense/add/ReadVariableOp}}]]
  (1) Failed precondition: Error while reading resource variable module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/module/bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias)
	 [[{{node module_apply_tokens/bert/encoder/transformer/group_0_11/layer_11/inner_group_0/ffn_1/intermediate/output/dense/add/ReadVariableOp}}]]
	 [[module_apply_tokens/bert/encoder/transformer/group_0_11/layer_11/inner_group_0/ffn_1/intermediate/output/dense/add/ReadVariableOp/_1]]
0 successful operations.
0 derived errors ignored.
&lt;/denchmark-code&gt;

Here is my code:
&lt;denchmark-code&gt;    from tensorflow import keras
    import tensorflow_hub as hub
    q_in = keras.layers.Input(shape=(None,), dtype=tf.int32, name="q_input_word_ids")
    q2_in = keras.layers.Input(shape=(None,), dtype=tf.int32, name="q_input_masks")
    q3_in = keras.layers.Input(shape=(None,), dtype=tf.int32, name="q_segment_ids")

    albert_module = hub.Module('https://tfhub.dev/google/albert_base/2', trainable=True)
    albert_inputs = dict(input_ids=q_in, input_mask=q2_in, segment_ids=q3_in)
    albert_outputs = albert_module(albert_inputs, signature="tokens", as_dict=True)["sequence_output"]
&lt;/denchmark-code&gt;

Up till now everything runs fine. But when I run the following, it gives the error above.
&lt;denchmark-code&gt;    pooled= keras.layers.GlobalAveragePooling1D()(albert_outputs)
&lt;/denchmark-code&gt;

Also I tried wrap it in tf.Session, but it didn't work:
&lt;denchmark-code&gt;with tf.Session() as session:
    keras.backend.set_session(session)
    session.run(tf.global_variables_initializer())  
    session.run(tf.tables_initializer())
    dense =  keras.layers.GlobalAveragePooling1D()(albert_outputs)
&lt;/denchmark-code&gt;

Any helps are appreciated!
	</description>
	<comments>
		<comment id='1' author='acmilannesta' date='2019-12-16T05:37:27Z'>
		&lt;denchmark-link:https://github.com/acmilannesta&gt;@acmilannesta&lt;/denchmark-link&gt;
,
Can you please let us know the TF Hub Version you are using. Thanks!
		</comment>
		<comment id='2' author='acmilannesta' date='2019-12-16T12:43:19Z'>
		I'm using tfhub 0.7.0.

rmothukuru &lt;notifications@github.com&gt; 于2019年12月16日周一 上午12:37写道：
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


 @acmilannesta &lt;https://github.com/acmilannesta&gt;,
 Can you please let us know the TF Hub Version you are using. Thanks!

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#447?email_source=notifications&amp;email_token=ALL6NUUNKT5JU226PBSFHILQY4HZRA5CNFSM4J256DYKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEG5SLNI#issuecomment-565913013&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/ALL6NUQHNOJH35SYEGXRIJ3QY4HZRANCNFSM4J256DYA&gt;
 .



		</comment>
		<comment id='3' author='acmilannesta' date='2019-12-17T09:41:23Z'>
		Hi &lt;denchmark-link:https://github.com/acmilannesta&gt;@acmilannesta&lt;/denchmark-link&gt;
, you'r using the old  class as if if were a Keras Layer type. That's not supported; please review &lt;denchmark-link:https://www.tensorflow.org/hub/migration_tf2&gt;tensorflow.org/hub/migration_tf2&lt;/denchmark-link&gt;
 and its links to the new and old APIs.
&lt;denchmark-link:https://tfhub.dev/google/albert_base/2&gt;https://tfhub.dev/google/albert_base/2&lt;/denchmark-link&gt;
 comes in the old  format. With tensorflow-hub 0.7.0, this can also be used in the new  API, but not with .
		</comment>
		<comment id='4' author='acmilannesta' date='2020-01-06T09:24:44Z'>
		I haven't heard back, so I suppose this has been solved.
		</comment>
	</comments>
</bug>