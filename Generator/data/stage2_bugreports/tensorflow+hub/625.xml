<bug id='625' author='GokulNC' open_date='2020-06-25T13:40:54Z' closed_time='2020-07-15T14:46:36Z'>
	<summary>How does pre-trained ELMo produce embeddings for characters of different languages? (out-of-vocabulary alphabets)</summary>
	<description>
Here's the code from my notebook:
%tensorflow_version 1.x
import tensorflow as tf
import tensorflow_hub as hub

elmo = hub.Module("https://tfhub.dev/google/elmo/2", trainable=True)
tf.logging.set_verbosity(tf.logging.ERROR)

def elmo_vectors(x):
    embeddings = elmo(x, signature="default", as_dict=True)["elmo"]
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(tf.tables_initializer())
        return sess.run(embeddings)
Output for non-English language: (Hindi in this example)
words = ['गोकुल']
v = elmo_vectors(words)
print(v.shape) # (1,1,1024)
print(v[0][0])
# Output: [ 0.3731584   0.5700774  -0.48072845 ... -0.1241736   0.5961436 -0.6986947 ]
The &lt;denchmark-link:https://tfhub.dev/google/elmo/2&gt;documentation of the pre-trained ELMo on Tensorflow Hub&lt;/denchmark-link&gt;
 shows that it was trained only on the English language.
That is, the dataset from 1 billion word benchmark is based on monolingual English data. (&lt;denchmark-link:https://opensource.google/projects/lm-benchmark&gt;Source&lt;/denchmark-link&gt;
)
So, how/why am I getting embeddings for non-English vocabulary words from ELMo using the TF Hub model?
Please explain.
	</description>
	<comments>
		<comment id='1' author='GokulNC' date='2020-06-26T06:07:14Z'>
		Could reproduce the issue. Please find the &lt;denchmark-link:https://colab.research.google.com/gist/rmothukuru/7f19a91e109fe06344fc9029b76bf6f6/hub_625.ipynb&gt;Github Gist&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='GokulNC' date='2020-07-01T12:40:42Z'>
		Update -- I'm following up with the publisher. Will report back once I have more information.
		</comment>
		<comment id='3' author='GokulNC' date='2020-07-15T14:46:36Z'>
		You can see the original Elmo code here: &lt;denchmark-link:https://github.com/allenai/bilm-tf/blob/master/bilm/data.py#L159&gt;https://github.com/allenai/bilm-tf/blob/master/bilm/data.py#L159&lt;/denchmark-link&gt;

The Elmo model uses byte sequence encoding up to a certain length for which the bytes for hindi characters are included - however the embeddings produced for non-English text may not be very meaningful.
Additionally, many models (not necessarily Elmo) include out-of-vocabulary (OOV) buckets which map tokens not in the vocab to embeddings trained on low-frequency tokens via a hash function, to fixed random embeddings, or to vectors of zeros. That way models can always provide an output instead of crashing.
Hope that clarifies!
		</comment>
	</comments>
</bug>