<bug id='443' author='swecooo' open_date='2019-12-12T11:34:20Z' closed_time='2020-04-01T09:08:39Z'>
	<summary>Universal sentence encoder lite 2 empty string</summary>
	<description>
First of all, I apologize if I'm posting this into an inappropriate place but I see other issues about Universal sentence encoder in this project.
Secondly I know that embedding empty strings is not a standard use case of USE but I'm posting this since the behavior is not consistent.
&lt;denchmark-h:h3&gt;Description&lt;/denchmark-h&gt;

I'm trying to embed texts in batches. Some of the texts can be empty. There are three cases that can occur:

Empty string is not at the last position of the batch: we get embeddings for all the input strings including the empty string.
Empty string at the last position of the batch: the output tensor doesn't include the embedding for this string, i.e. the output tensor shape is smaller than input tensor shape.
Only empty string in the batch: ValueError raised.

&lt;denchmark-h:h3&gt;Runnable example&lt;/denchmark-h&gt;

Initialization:
import os
import sentencepiece as spm
import tensorflow as tf
import tensorflow_hub as hub

def process_to_IDs_in_sparse_format(sp, sentences):
    # An utility method that processes sentences with the sentence piece processor
    # 'sp' and returns the results in tf.SparseTensor-similar format:
    # (values, indices, dense_shape)
    ids = [sp.EncodeAsIds(x) for x in sentences]
    max_len = max(len(x) for x in ids)
    dense_shape = (len(ids), max_len)
    values = [item for sublist in ids for item in sublist]
    indices = [[row, col] for row in range(len(ids)) for col in range(len(ids[row]))]
    return (values, indices, dense_shape)


os.environ['TFHUB_CACHE_DIR'] = './models'
module = hub.Module("https://tfhub.dev/google/universal-sentence-encoder-lite/2")
sp = spm.SentencePieceProcessor()
sp.Load("models/539544f0a997d91c327c23285ea00c37588d92cc/assets/universal_encoder_8k_spm.model")
Empty string not last:
sentences = [
    "",
    "The quick brown fox jumps over the lazy dog."
]
values, indices, dense_shape = process_to_IDs_in_sparse_format(sp, sentences)

with tf.Session() as session:
    session.run(tf.group([tf.global_variables_initializer(), tf.tables_initializer()]))
    message_embeddings = session.run(
        embeddings,
        feed_dict={input_placeholder.values: values,
                   input_placeholder.indices: indices,
                   input_placeholder.dense_shape: dense_shape})

print(sentences)
print(message_embeddings.shape)
# (2, 512)
print(message_embeddings)
# [[ 0.00235466 -0.03487525  0.01310202 ... -0.05458128 -0.0276587
#    0.03307254]
#  [-0.01353392 -0.04256385  0.03684283 ... -0.02522222  0.02741303
#    0.01377513]]
Empty string last:
sentences = [
    "The quick brown fox jumps over the lazy dog.",
    ""
]

input_placeholder = tf.sparse_placeholder(tf.int64, shape=[None, None])
embeddings = module(
    inputs=dict(
        values=input_placeholder.values,
        indices=input_placeholder.indices,
        dense_shape=input_placeholder.dense_shape))

values, indices, dense_shape = process_to_IDs_in_sparse_format(sp, sentences)

with tf.Session() as session:
    session.run(tf.group([tf.global_variables_initializer(), tf.tables_initializer()]))
    message_embeddings = session.run(
        embeddings,
        feed_dict={input_placeholder.values: values,
                   input_placeholder.indices: indices,
                   input_placeholder.dense_shape: dense_shape})

print(sentences)
print(message_embeddings.shape)
# (1, 512)
print(message_embeddings)
# [[-0.01353392 -0.04256385  0.03684283 ... -0.02522222  0.02741303  0.01377513]]
Only empty string:
sentences = [""]
values, indices, dense_shape = process_to_IDs_in_sparse_format(sp, sentences)

with tf.Session() as session:
    session.run(tf.group([tf.global_variables_initializer(), tf.tables_initializer()]))
    message_embeddings = session.run(
        embeddings,
        feed_dict={input_placeholder.values: values,
                   input_placeholder.indices: indices,
                   input_placeholder.dense_shape: dense_shape})

# ValueError: Cannot feed value of shape (0,) for Tensor 'Placeholder_2:0', which has shape '(?, 2)'
	</description>
	<comments>
		<comment id='1' author='swecooo' date='2020-03-28T20:11:46Z'>
		Hi this seems to be a problem with the SentencePiece library actually. By looking at the source code of the sentence piece library it seems like empty lines are removed before they are passed into the SentencePiece model. Here is a link to the trainer_interface code &lt;denchmark-link:https://github.com/google/sentencepiece/blob/0f6db9477090688f3f8368fa9598b332b7d3c6dc/src/trainer_interface.cc#L325&gt;line 325&lt;/denchmark-link&gt;
.
The Lite version of the Universal sentence encoder requires the ID generated by the SentencePiece generator as one of the arguments and looking at the different IDs generated by the generator it seems like the generator does not return any ID for whitespaces or empty strings.
print(sp.EncodeAsIds("Hello"))
 # [341, 4125]
print(sp.EncodeAsIds("H"))
 # [341]
print(sp.EncodeAsIds("."))
 # [30, 6]
print(sp.EncodeAsIds(""))
 # []
Now trying to recreate your findings these are what I found out.
When an empty string is passed
messages = [""]

values, indices, dense_shape = process_to_IDs_in_sparse_format(sp, messages)
print(values)
# []
print(indices)
# []
print(dense_shape)
# (1, 0)

feed_dict = {input_placeholder.values: values,
                input_placeholder.indices: indices,
                input_placeholder.dense_shape: dense_shape}

with tf.Session() as session:
  session.run([tf.global_variables_initializer(), tf.tables_initializer()])
  message_embeddings = session.run(
      encodings,
      feed_dict={input_placeholder.values: values,
                input_placeholder.indices: indices,
                input_placeholder.dense_shape: dense_shape})
  
print(message_embeddings.shape)
# ValueError: Cannot feed value of shape (0,) for Tensor 'Placeholder_5:0', which has shape '(?, 2)'
The model expects to receive an index value of size (?,2) like [0, 1] but instead it was given [] hence the ValueError
When an empty string is passed in the start
messages = ["",
            "hello"]

values, indices, dense_shape = process_to_IDs_in_sparse_format(sp, messages)
print(values)
# [341, 4125]
print(indices)
# [[1, 0], [1, 1]]
print(dense_shape)
# (2, 2)

feed_dict = {input_placeholder.values: values,
                input_placeholder.indices: indices,
                input_placeholder.dense_shape: dense_shape}

with tf.Session() as session:
  session.run([tf.global_variables_initializer(), tf.tables_initializer()])
  message_embeddings = session.run(
      encodings,
      feed_dict={input_placeholder.values: values,
                input_placeholder.indices: indices,
                input_placeholder.dense_shape: dense_shape})
  
print(message_embeddings.shape)
# (2, 512)
Here notice the output of values and indices, The output of indices is [[1, 0], [1, 1]] which means that the 2 values [341, 4125] belong to string present in messages at index 1. ie "Hello" not "". But there are 2 embeddings created one for "" and one for "Hello".
When an empty string is passed in the middle
messages = ["Hello",
            "",
            "Hello"]

values, indices, dense_shape = process_to_IDs_in_sparse_format(sp, messages)
print(values)
# [341, 4125, 341, 4125]
print(indices)
# [[0, 0], [0, 1], [2, 0], [2, 1]]
print(dense_shape)
# (3, 2)

feed_dict = {input_placeholder.values: values,
                input_placeholder.indices: indices,
                input_placeholder.dense_shape: dense_shape}

with tf.Session() as session:
  session.run([tf.global_variables_initializer(), tf.tables_initializer()])
  message_embeddings = session.run(
      encodings,
      feed_dict={input_placeholder.values: values,
                input_placeholder.indices: indices,
                input_placeholder.dense_shape: dense_shape})
  
print(message_embeddings.shape)
# (3, 512)
Same here, the values belong to the strings from the messages list at the 0th and 2nd index.
There are three embeddings created and comparing the embedding for created for "" with the previous example embedding for "", they seem to be similar because they have very close cosine similarity. Therefore it seems clear that the model can create embeddings for empty strings.
When an empty string is passed in the end.
messages = ["Hello",
            ""]

values, indices, dense_shape = process_to_IDs_in_sparse_format(sp, messages)
print(values)
# [341, 4125]
print(indices)
# [[0, 0], [0, 1]]
print(dense_shape)
# (2, 2)

feed_dict = {input_placeholder.values: values,
                input_placeholder.indices: indices,
                input_placeholder.dense_shape: dense_shape}

with tf.Session() as session:
  session.run([tf.global_variables_initializer(), tf.tables_initializer()])
  message_embeddings = session.run(
      encodings,
      feed_dict={input_placeholder.values: values,
                input_placeholder.indices: indices,
                input_placeholder.dense_shape: dense_shape})
  
print(message_embeddings.shape)
# (1, 512)
Over here only 1 embedding is created for the string "Hello"
This is not an official explanation but my thinking is that the empty strings in the start are converted to embeddings to maintain the same number of output as the Input but the empty string in the last gets removed because according to some conventions empty line at the end denotes the end of file.
The best thing for you to do however is to just remove empty strings before embedding them.
		</comment>
		<comment id='2' author='swecooo' date='2020-03-28T20:14:22Z'>
		You can however embed empty strings in the non-lite version of the universal sentence encoder.
If you feel like this issue has been solved, please close the issue :)
		</comment>
		<comment id='3' author='swecooo' date='2020-03-31T20:34:22Z'>
		&lt;denchmark-link:https://github.com/swecooo&gt;@swecooo&lt;/denchmark-link&gt;
 As mentioned in the above comment, please remove the empty strings before embedding and use it. Please respond if you think otherwise. Thanks!
		</comment>
		<comment id='4' author='swecooo' date='2020-04-01T09:08:39Z'>
		Thank you for your time and explanation. :)

the (sentencepiece) generator does not return any ID for whitespaces or empty strings.

I completely understand that this is an edge case. I agree with removing empty strings and string containing only whitespaces from the data.
I just thought that the behavior should be consistent to avoid different results for different ordering of the strings but this gets resolved when we remote those invalid strings.
		</comment>
	</comments>
</bug>