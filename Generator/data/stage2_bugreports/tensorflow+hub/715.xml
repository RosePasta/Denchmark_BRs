<bug id='715' author='Doyel' open_date='2020-12-18T04:32:52Z' closed_time='2021-01-06T15:17:19Z'>
	<summary>Input Layer can't seem to detect batches of RaggedTensors</summary>
	<description>
In &lt;denchmark-link:https://www.tensorflow.org/tutorials/text/solve_glue_tasks_using_bert_on_tpu&gt;this&lt;/denchmark-link&gt;
 tensorflow tutorial, there is a function - 'make_bert_preprocess_model' that returns a model mapping string features to BERT inputs.
My current project requires me to get offsets of tokens. The recommended &lt;denchmark-link:https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1&gt;preprocessing model&lt;/denchmark-link&gt;
 does not allow me to access token offsets (either ways, the current BertTokenizer.tokenize_with_offsets method has a bug for which I have already logged an issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/45684&gt;#45684&lt;/denchmark-link&gt;
).
As a workaround, I had made some modifications in 'make_bert_preprocess_model' that took as input batches of RaggedTensors and returned features formatted into BERT inputs.
So my code was running till yesterday. It suddenly started complaining from today morning. On googling a bit, I realized that you guys have released a newer version of the &lt;denchmark-link:https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&gt;preprocessing model&lt;/denchmark-link&gt;
. So how do I make my code run again?
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes and No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab


Please note that I have used BertTokenizer in the gist file for representation purpose only! The Colab gist file can be found &lt;denchmark-link:https://colab.research.google.com/gist/Doyel/6693f9b226e3577db9b10e630ddd3b47/untitled0.ipynb&gt;here&lt;/denchmark-link&gt;

Error obtained:
ValueError: Layer model expects 2 input(s), but it received 4 input tensors. Inputs received: [&lt;tf.RaggedTensor [[[1996], [2417], [6546]]]&gt;, &lt;tf.RaggedTensor [[[8044], [1999], [3712]]]&gt;, &lt;tf.RaggedTensor [[[1037], [2611], [1999], [2630], [4377]]]&gt;, &lt;tf.RaggedTensor [[[1037], [3336]]]&gt;]
	</description>
	<comments>
		<comment id='1' author='Doyel' date='2020-12-18T13:15:34Z'>
		Was able to reproduce the issue with &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/164273b852241e5abd5b851d5739b681/45825.ipynb&gt;TF v2.4&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/ee05ca98408589a3c0e23385ff48830d/45825-tf-nightly.ipynb&gt;TF-nightly&lt;/denchmark-link&gt;
.
Whereas with &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/dd81bd7eb58aed293dd5eaab534aecb3/45825-2-3.ipynb#scrollTo=a0t1yFlc760X&gt;TF v2.3&lt;/denchmark-link&gt;
, I am facing an error stating . Please check the linked gist for reference. Thanks!
		</comment>
		<comment id='2' author='Doyel' date='2021-01-06T14:42:50Z'>
		First off: yes, there is a bug with TF.text's BertTokenizer.tokenize_with_offsets(), so we're not exposing that in the SavedModels on TF Hub for now, but I do acknowledge the need for offsets in a variety of advanced use-cases.
		</comment>
		<comment id='3' author='Doyel' date='2021-01-06T15:04:24Z'>
		As far as I can see, the problem in your &lt;denchmark-link:https://colab.research.google.com/gist/Doyel/6693f9b226e3577db9b10e630ddd3b47/untitled0.ipynb&gt;colab&lt;/denchmark-link&gt;
 is with constructing the dict of features: you have a list of dict of list of RaggedTensor, what you need is a dict of RaggedTensor.
The code starts to work for me when I replace the final cell with
bert_tokenizer = text.BertTokenizer(vocab_lookup_table=vocab_file, lower_case=lower_case)
b1_tokenids = bert_tokenizer.tokenize(['The red flower'])
b2_tokenids = bert_tokenizer.tokenize(['A girl in blue dress'])
b3_tokenids = bert_tokenizer.tokenize(['Clouds in sky'])
b4_tokenids = bert_tokenizer.tokenize(['A baby'])
test_text = {'my_input1': tf.concat([b1_tokenids, b3_tokenids], axis=0),
             'my_input2': tf.concat([b2_tokenids, b4_tokenids], axis=0)}

test_preprocess_model = make_bert_preprocess_model(['my_input1', 'my_input2'])
test_preprocess_model(test_text)
The result is a dict of tensors suitable for feeding into the BERT encoder.
		</comment>
		<comment id='4' author='Doyel' date='2021-01-06T15:17:00Z'>
		Finally: It's true that there was an update of the BERT and ALBERT preprocessing models to version /2, but a new model version on tfhub.dev does not change the result of hub.load(), because the URL passed in there always specifies a fixed version.
Maybe you were affected by Colab's upgrade to TF2.4: TF2.4 has a bug that wasn't present for 2.3 and is already fixed for 2.5. It affects the particular way in which a BERT preprocessing model may get used from within Dataset.map() with a Cloud TPU, see the model Changelog for details. I recommend to just update to preprocessing model version /2 and retry.
		</comment>
		<comment id='5' author='Doyel' date='2021-01-06T17:49:59Z'>
		&lt;denchmark-link:https://github.com/arnoegw&gt;@arnoegw&lt;/denchmark-link&gt;

Thank you for the recommendation!
		</comment>
	</comments>
</bug>