<bug id='469' author='uchua' open_date='2020-01-08T18:59:00Z' closed_time='2020-01-10T16:22:33Z'>
	<summary>Compiling model including hub.KerasLayer fails in distribution strategy scope.</summary>
	<description>
When trying to compile a model that includes a hub.KerasLayer I get the following error:
&lt;denchmark-code&gt;ValueError: Variable (&lt;tf.Variable 'bert/embeddings/word_embeddings:0' shape=(119547, 768) dtype=float32&gt;) was not created in the distribution strategy scope of (&lt;tensorflow.python.distribute.tpu_strategy.TPUStrategy object at 0x7fb17c83f3c8&gt;). It is most likely due to not all layers or the model or optimizer being created outside the distribution strategy scope. Try to make sure your code looks similar to the following.
with strategy.scope():
  model=_create_model()
  model.compile(...)
&lt;/denchmark-code&gt;

Code used to build and compile model:
with tpu_strategy.scope():
    in_id = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name="input_ids", dtype=np.int32)
    in_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name="input_masks", dtype=np.int32)
    in_segment = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name="segment_ids", dtype=np.int32)
    bert_inputs = {"input_ids": in_id, "input_mask": in_mask, "segment_ids": in_segment}

    bert_output = hub.KerasLayer(BERT_MODEL_HUB, trainable=True, signature="tokens", output_key="pooled_output")(bert_inputs)

    dense = tf.keras.layers.Dense(256, input_shape=(768,), activation='relu')(bert_output)
    pred = tf.keras.layers.Dense(len(unique_labels), activation='sigmoid')(dense)

    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
        loss=tf.keras.losses.binary_crossentropy,
        metrics=["accuracy"]
    )
This is the model I'm loading in the KerasLayer: &lt;denchmark-link:https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12/1&gt;https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12/1&lt;/denchmark-link&gt;

The entire model is created within the scope of the distribution strategy, so I'm not sure what the cause of the error would be, other than a bug.
	</description>
	<comments>
		<comment id='1' author='uchua' date='2020-01-09T08:55:18Z'>
		Yes, this may well be due to a bug in TensorFlow 2.0. Sorry about that. Please try a recent tf-nightly PIP package and reopen if the issue persists.
		</comment>
		<comment id='2' author='uchua' date='2020-01-09T09:01:09Z'>
		Also, you're using a TF1-style hub.Module. Try &lt;denchmark-link:https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/1&gt;https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/1&lt;/denchmark-link&gt;
 instead (see &lt;denchmark-link:https://www.tensorflow.org/hub/tf1_hub_module&gt;https://www.tensorflow.org/hub/tf1_hub_module&lt;/denchmark-link&gt;
 vs &lt;denchmark-link:https://www.tensorflow.org/hub/tf2_saved_model&gt;https://www.tensorflow.org/hub/tf2_saved_model&lt;/denchmark-link&gt;
).
		</comment>
		<comment id='3' author='uchua' date='2020-01-09T18:02:04Z'>
		Edit:
Apparently tf-nightly did install correctly, but when you run tf.version.GIT_VERSION it tells you it's version 1.12.
Strangely enough, installing tf-nightly gave me TensorFlow version 1.12. I was able to install TensorFlow v2.1.0-rc2-17-ge5bf8de by running pip install tensorflow-gpu==2.1, but then I get these errors when trying to create the TPU distribution strategy:
&lt;denchmark-code&gt;UnimplementedError                        Traceback (most recent call last)
&lt;ipython-input-25-6596c0485615&gt; in &lt;module&gt;
      7 
      8     resolver = tf.distribute.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)
----&gt; 9     tf.config.experimental_connect_to_host(resolver.master())
     10     tf.tpu.experimental.initialize_tpu_system(resolver)
     11     tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)

~/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/remote.py in connect_to_remote_host(remote_host, job_name)
     75       {job_name: [_strip_prefix(host, _GRPC_PREFIX) for host in remote_hosts]})
     76 
---&gt; 77   connect_to_cluster(cluster_spec)
     78 
     79 

~/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/remote.py in connect_to_cluster(cluster_spec_or_resolver, job_name, task_index, protocol, make_master_device_default)
    140     context.set_server_def(server_def)
    141   else:
--&gt; 142     context.update_server_def(server_def)
    143 
    144   if make_master_device_default and isinstance(

~/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/context.py in update_server_def(server_def)
   2017 
   2018 def update_server_def(server_def):
-&gt; 2019   context().update_server_def(server_def)
   2020 
   2021 

~/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/context.py in update_server_def(self, server_def, keep_alive_secs)
    592       pywrap_tensorflow.TFE_ContextUpdateServerDef(self._context_handle,
    593                                                    keep_alive_secs,
--&gt; 594                                                    server_def_str)
    595       self._initialize_logical_devices()
    596 

UnimplementedError: 
Additional GRPC error information:
{"created":"@1578592686.165655670","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"","grpc_status":12}
&lt;/denchmark-code&gt;

This is the code for creating the strategy:
TPU_ADDRESS = "grpc://" + "10.0.0.2:8470"

with tf.compat.v1.Session(TPU_ADDRESS) as session:
    print('TPU devices:')
    pprint.pprint(session.list_devices())

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)
tf.config.experimental_connect_to_host(resolver.master())
tf.tpu.experimental.initialize_tpu_system(resolver)
tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)
So it looks like there might be a bug in TensorFlow 2.1? Or should I be creating the TPU distribution strategy differently?
		</comment>
		<comment id='4' author='uchua' date='2020-01-10T08:35:18Z'>
		Please help me investigate further. Do I guess correctly that your code runs on a Colab notebook hosted on colab.research.google.com? Please share a Colab notebook with me to reproduce the issue with TF 2.1 (possibly prerelease) and BERT from a TF2-style SavedModel, such as &lt;denchmark-link:https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/1&gt;https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/1&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='5' author='uchua' date='2020-01-10T16:22:33Z'>
		&lt;denchmark-link:https://github.com/arnoegw&gt;@arnoegw&lt;/denchmark-link&gt;
 I'm using Colab and a Google Cloud TPU Instance. The error I previously posted appears to be caused by  already having been ran. I still encounter another error however while trying to create the TPU Strategy, but it appears to be an issue with TensorFlow, not TensorFlow Hub.
		</comment>
		<comment id='6' author='uchua' date='2020-12-06T06:40:15Z'>
		Using TF-2.3, still got the same problem. I use distribute.MirroredStrategy().scope(). Any idea?
		</comment>
		<comment id='7' author='uchua' date='2020-12-21T10:03:08Z'>
		
Using TF-2.3, still got the same problem. I use distribute.MirroredStrategy().scope(). Any idea?

I also have the same issue. Please help!
		</comment>
	</comments>
</bug>