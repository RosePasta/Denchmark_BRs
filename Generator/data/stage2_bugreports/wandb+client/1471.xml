<bug id='1471' author='stathius' open_date='2020-11-06T14:17:50Z' closed_time='2020-12-08T11:35:36Z'>
	<summary>Weights are not logged when model is watched</summary>
	<description>
The gradients of the model are indeed logged but not the parameters.
Using:
pytorch-lightning==1.0.4
torch==1.7.0
torchvision==0.8.1
wandb==0.10.9
mac os x 10.15.7
&lt;denchmark-code&gt;        logger = pl.loggers.WandbLogger(project=hparams.project_name, save_dir=save_dir)
        logger.log_hyperparams(vars(hparams))
        logger.watch(model, log='all', log_freq=1)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='stathius' date='2020-11-06T14:17:54Z'>
		Issue-Label Bot is automatically applying the label bug to this issue, with a confidence of 0.96. Please mark this comment with üëç or üëé to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://mlbot.net/data/wandb/client&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='2' author='stathius' date='2020-11-06T14:41:15Z'>
		Hi &lt;denchmark-link:https://github.com/stathius&gt;@stathius&lt;/denchmark-link&gt;
 , thanks for reporting this issue. We're looking into this.
		</comment>
		<comment id='3' author='stathius' date='2020-11-09T07:51:44Z'>
		I believe by default it is supposed to log only gradients, but you can change it by using argument log.
Please see the documentation (&lt;denchmark-link:https://docs.wandb.com/library/integrations/pytorch#options&gt;here&lt;/denchmark-link&gt;
) for more details.
		</comment>
		<comment id='4' author='stathius' date='2020-11-09T11:30:30Z'>
		I am using the log='all' argument as per the snippet I provide above.
&lt;denchmark-link:#&gt;‚Ä¶&lt;/denchmark-link&gt;


On Mon, Nov 9, 2020 at 9:51 AM Artyom Hakobyan ***@***.***&gt; wrote:
 I believe by default it is supposed to log only gradients, but you can
 change it by using argument log.
 Please see the documentation (here
 &lt;https://docs.wandb.com/library/integrations/pytorch#options&gt;) for more
 details.

 ‚Äî
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#1471 (comment)&gt;, or
 unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AAPQ4BNOLJNXG76SKXAEQOTSO6NR3ANCNFSM4TMXLVWQ&gt;
 .



		</comment>
		<comment id='5' author='stathius' date='2020-11-11T03:19:35Z'>
		Hm, I tried to replicate this by setting log='all' in &lt;denchmark-link:https://github.com/wandb/examples/blob/master/examples/pytorch/pytorch-cnn-mnist/main.py&gt;https://github.com/wandb/examples/blob/master/examples/pytorch/pytorch-cnn-mnist/main.py&lt;/denchmark-link&gt;
 and it seemed to work as intended, logging both parameters and gradients - could you try that with your setup
		</comment>
		<comment id='6' author='stathius' date='2020-12-08T05:10:26Z'>
		Hey &lt;denchmark-link:https://github.com/stathius&gt;@stathius&lt;/denchmark-link&gt;

Did you happen to find the solution to the issue?
It would be great for us if we could close the stale threads if the solution has already been achieved. 
		</comment>
		<comment id='7' author='stathius' date='2020-12-08T10:55:45Z'>
		Hi &lt;denchmark-link:https://github.com/ariG23498&gt;@ariG23498&lt;/denchmark-link&gt;
 not really but it's fine to close it.
		</comment>
	</comments>
</bug>