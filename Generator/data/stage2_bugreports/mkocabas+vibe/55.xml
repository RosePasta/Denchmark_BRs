<bug id='55' author='AlbertoSabater' open_date='2020-04-07T14:26:45Z' closed_time='2020-04-25T08:23:44Z'>
	<summary>Minimun requirements for --run-simplify</summary>
	<description>
Hi! I would like to know which are the minimum requirements for the --run_smplify. I have no problem when running the demo with bbox tracking, but when I use --tracking_method pose --run_smplify I get the following error when on the smplfy step, related with a lack of GPU memory.
&lt;denchmark-code&gt;Running "ffmpeg -i /mnt/hdd/....png"
Images saved to "/tmp/..."
Input video number of frames 7841
Executing build/examples/openpose/openpose.bin --model_pose BODY_21A --tracking 1 --render_pose 0 --video /mnt/hdd/datasets_hdd/data_laura/Dados/Ines_10/1129/video1575040423.57.avi.mp4 --write_json /tmp/video1575040423.57.avi.mp4_posetrack --display 0
Starting OpenPose demo...
Configuring OpenPose...
Starting thread(s)...
Auto-detecting all available GPUs... Detected 1 GPU(s), using 1 of them starting at GPU 0.
RUNNING PoseExtractorCaffeStaf::PoseExtractorCaffeStaf
OpenPose demo successfully finished. Total time: 167.414414 seconds.
=&gt; loaded pretrained model from 'data/vibe_data/spin_model_checkpoint.pth.tar'
Performance of pretrained model on 3DPW: 56.56075477600098
Loaded pretrained weights from "data/vibe_data/vibe_model_wo_3dpw.pth.tar"
Running VIBE on each tracklet...
  0%|                                                                                                                                                              | 0/4 [00:00&lt;?, ?it/s]Traceback (most recent call last):
  File "demo.py", line 383, in &lt;module&gt;
    main(args)
  File "demo.py", line 192, in main
    pose2aa=False,
  File "/home/asabater/projects_lisbon/VIBE/lib/utils/demo_utils.py", line 130, in smplify_runner
    gt_keypoints_2d_orig).mean(dim=-1)
  File "/home/asabater/projects_lisbon/VIBE/lib/smplify/temporal_smplify.py", line 243, in get_fitting_loss
    betas=betas, return_full_pose=True)
  File "/home/asabater/anaconda3/envs/vibe-env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/asabater/projects_lisbon/VIBE/lib/models/spin.py", line 485, in forward
    smpl_output = super(SMPL, self).forward(*args, **kwargs)
  File "/home/asabater/anaconda3/envs/vibe-env/lib/python3.7/site-packages/smplx/body_models.py", line 376, in forward
    self.lbs_weights, pose2rot=pose2rot, dtype=self.dtype)
  File "/home/asabater/anaconda3/envs/vibe-env/lib/python3.7/site-packages/smplx/lbs.py", line 212, in lbs
    T = torch.matmul(W, A.view(batch_size, num_joints, 16)) \
RuntimeError: CUDA out of memory. Tried to allocate 3.12 GiB (GPU 0; 10.76 GiB total capacity; 7.41 GiB already allocated; 1.58 GiB free; 7.73 GiB reserved in total by PyTorch)
Exception ignored in: &lt;function tqdm.__del__ at 0x7f2cf94c0710&gt;
Traceback (most recent call last):
  File "/home/asabater/anaconda3/envs/vibe-env/lib/python3.7/site-packages/tqdm/_tqdm.py", line 931, in __del__
    self.close()
  File "/home/asabater/anaconda3/envs/vibe-env/lib/python3.7/site-packages/tqdm/_tqdm.py", line 1133, in close
    self._decr_instances(self)
  File "/home/asabater/anaconda3/envs/vibe-env/lib/python3.7/site-packages/tqdm/_tqdm.py", line 496, in _decr_instances
    cls.monitor.exit()
  File "/home/asabater/anaconda3/envs/vibe-env/lib/python3.7/site-packages/tqdm/_monitor.py", line 52, in exit
    self.join()
  File "/home/asabater/anaconda3/envs/vibe-env/lib/python3.7/threading.py", line 1041, in join
    raise RuntimeError("cannot join current thread")
RuntimeError: cannot join current thread
&lt;/denchmark-code&gt;

I am working with Ubuntu 18.04 with 32 GB of RAM, and an NVIDIA 2080Ti with 11GB of memory.
Is there any way to reduce memory consumption by tuning some parameters?
How important the smplify step for the final result?
Thanks in advance.
	</description>
	<comments>
		<comment id='1' author='AlbertoSabater' date='2020-04-25T08:23:44Z'>
		
I am working with Ubuntu 18.04 with 32 GB of RAM, and an NVIDIA 2080Ti with 11GB of memory.

This configuration is more than enough. I also have 2080ti GPU on my machine and it works without any problem. It seems most of your CUDA memory is already allocated by another job. Rerunning VIBE after removing that job should work. If you can't remove that job, you may also adjust the batch sizes of tracking and VIBE accordingly. Check the &lt;denchmark-link:docs/demo.md&gt;demo&lt;/denchmark-link&gt;
 readme for that.
		</comment>
		<comment id='2' author='AlbertoSabater' date='2020-04-27T14:13:17Z'>
		No, the problem is that SIMPLify takes as input all the predictions within a video. So, this data is converted to Tensors, that require GPU memory. If the predictions correspond to short videos, as in the demo example, there is no problem. But if the video is too long, much more GPU memory is needed.
To solve this, I ran SIMPLify on batches of data to reduce the memory requirements. This is the code I modified in the demo script:
&lt;denchmark-code&gt;        # ========= [Optional] run Temporal SMPLify to refine the results ========= #
        if args.run_smplify and args.tracking_method == 'pose':
            norm_joints2d = np.concatenate(norm_joints2d, axis=0)
            norm_joints2d = convert_kps(norm_joints2d, src='staf', dst='spin')
            norm_joints2d = torch.from_numpy(norm_joints2d).float().to(device)
 
            pred_verts = pred_verts.cpu()
            pred_cam = pred_cam.cpu()
            pred_pose = pred_pose.cpu()
            pred_betas = pred_betas.cpu()
            pred_joints3d = pred_joints3d.cpu()
            
            
            SMPLify_batch = 256
            for i in np.arange(0, len(frames), SMPLify_batch):
                init = i
                end = i + SMPLify_batch
                
                # pred_verts_batch = pred_verts[init:end].to(device)
                pred_pose_batch = pred_pose[init:end].to(device)
                pred_betas_batch = pred_betas[init:end].to(device)
                pred_cam_batch = pred_cam[init:end].to(device).to(device)
                norm_joints2d_batch = norm_joints2d[init:end].to(device)
                # pred_joints3d_batch = pred_joints3d[init:end].to(device)

                # Run Temporal SMPLify
                update, new_opt_vertices, new_opt_cam, new_opt_pose, new_opt_betas, \
                new_opt_joints3d, new_opt_joint_loss, opt_joint_loss = smplify_runner(
                    pred_rotmat=pred_pose_batch,
                    pred_betas=pred_betas_batch,
                    pred_cam=pred_cam_batch,
                    j2d=norm_joints2d_batch,
                    device=device,
                    batch_size=norm_joints2d_batch.shape[0],
                    pose2aa=False,
                )

            
                print(f'Update ratio after Temporal SMPLify: {update.sum()} / {norm_joints2d_batch.shape[0]} / {norm_joints2d.shape[0]}')
                 
                pred_verts[init:end][update] = new_opt_vertices[update]
                pred_cam[init:end][update] = new_opt_cam[update]
                pred_pose[init:end][update] = new_opt_pose[update]
                pred_betas[init:end][update] = new_opt_betas[update]
                pred_joints3d[init:end][update] = new_opt_joints3d[update]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='AlbertoSabater' date='2020-04-27T18:28:39Z'>
		Oh, you are right. Actually I never tested smplify on longer videos. I would be happy to merge if you can open a PR with those changes. Thanks!
		</comment>
	</comments>
</bug>