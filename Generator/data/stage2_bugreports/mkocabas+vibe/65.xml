<bug id='65' author='jaygit123' open_date='2020-04-24T15:31:27Z' closed_time='2020-04-25T08:18:21Z'>
	<summary>output pickle file (vibe_output.pkl) seems to be invalid</summary>
	<description>
Hi &lt;denchmark-link:https://github.com/mkocabas&gt;@mkocabas&lt;/denchmark-link&gt;
,
Thanks for this awesome implementation of VIBE. Works very good on video input.
But for some reason the output pickle file (vibe_output.pkl) seems to be invalid and is unusable.
'KeyError: 1' is thrown when I try to get keys in it.
NOTE: I am using --tracking_method as 'pose' and use STAF extension of OpenPose (BODY_25).
Environment details

Google Colab notebook with Ubuntu 18.04.3 LTS
Python version 3.6.9
Pytorch - version 1.4.0 and cuDNN version : 7603
Error noticed while trying to read the output pickle file (vibe_output.pkl) is given below:

Error

Track ids: dict_keys([-1])


VIBE output file content:
KeyError                                  Traceback (most recent call last)
 in ()
4
5 print('VIBE output file content:', end='\n\n')
----&gt; 6 for k,v in output[1].items():
7   if k != 'joints2d':
8     print(k, v.shape)
KeyError: 1

You can check the generated pkl file and mp4 files in this location:
&lt;denchmark-link:https://drive.google.com/file/d/1-15ghwv4lcEqGl1LjuzgxlmHj820wzTh/view?usp=sharing&gt;VIBE-output-pkl-error.zip&lt;/denchmark-link&gt;

I noticed someone else also had mentioned that the generated pickle file is unusable.
Could you please help resolving this issues please?
NOTE:
This is how I am trying to read the pkl file:
&lt;denchmark-code&gt;# Inspect the output file content
import joblib
output = joblib.load('/content/VIBE/output/output/vibe_output.pkl')
print('Track ids:', output.keys(), end='\n\n')

print('VIBE output file content:', end='\n\n')
for k,v in output[1].items():
  if k != 'joints2d': 
    print(k, v.shape)
&lt;/denchmark-code&gt;

Thanks &amp; Regards,
Jay
	</description>
	<comments>
		<comment id='1' author='jaygit123' date='2020-04-24T15:51:52Z'>
		Hi &lt;denchmark-link:https://github.com/jaygit123&gt;@jaygit123&lt;/denchmark-link&gt;
,
Thanks for your interest. I just tested the demo on google colab and it worked as expected. Could you try once again?
		</comment>
		<comment id='2' author='jaygit123' date='2020-04-24T16:33:08Z'>
		Hi &lt;denchmark-link:https://github.com/mkocabas&gt;@mkocabas&lt;/denchmark-link&gt;
 ,
Thanks for awesome quick response. I have checked once again and still getting the same error with respect to pkl file. Not sure what's wrong.
BTW, I have just made these minor changes to 'VIBE/lib/utils/pose_tracker.py' file.

added params --face &amp; --hand to the params list of openpose.bin command
changed value of param --model_pose from BODY_21A to BODY_25
With the above changes, the resultant call of openpose is like the below:


openpose.bin --video /content/output.mp4 --write_json /tmp/output.mp4_posetrack --display 0 --model_pose BODY_25 --face --hand --hand_detector 0 --render_pose 0

Could the above changes cause generated pkl file to be unusable?
One more question (not related to pkl file)...please answer.
Even though openpose gives 2D keypoints of face and hands (along with body keypoints), I see they are not used in the final 3D model (facial expression and fingers do not change as per the pose in video). Is there a way to enable facial expression and fingers movements to sync with that of video input? Please share some pointers.
Regards,
Jay
		</comment>
		<comment id='3' author='jaygit123' date='2020-04-24T19:31:48Z'>
		I think the difference you mention in open pose is not the only change required, it requires changing the body model too.
Mesh's facial expression and fingers are related to the usage of SMPLX body model, as it includes face and fingers 3d keypoints:
&lt;denchmark-link:https://user-images.githubusercontent.com/11877852/80248596-17d57b00-8625-11ea-99c0-987d71d3d93e.png&gt;&lt;/denchmark-link&gt;

Currently VIBE uses SMPL:


SMPL key points are shown in: &lt;denchmark-link:https://ikvision.github.io/assets/images/Joint_regression_frame.png&gt;SMPL_mesh&lt;/denchmark-link&gt;

One should integrate SMPLX model for training and prediction flow
from smplx.body_models import  SMPLX
		</comment>
		<comment id='4' author='jaygit123' date='2020-04-25T08:18:21Z'>
		
added params --face &amp; --hand to the params list of openpose.bin command

Even though you add face and hand, you would not be able to use them since VIBE does not support SMPL-X body model. Also, staf pose tracker does not support the tracking of face and hand keypoints.

changed value of param --model_pose from BODY_21A to BODY_25

You should not do that, because pose tracking is only supported through BODY_21A model. BODY_25 model will not give you tracking info.
I suppose the above modifications are the main cause for your problem.

Is there a way to enable facial expression and fingers movements to sync with that of video input?

VIBE currently does not support SMPL-X models which includes face and hands as &lt;denchmark-link:https://github.com/ikvision&gt;@ikvision&lt;/denchmark-link&gt;
 mentioned. And it is not trivial to implement for the time being.
		</comment>
		<comment id='5' author='jaygit123' date='2020-04-25T09:59:44Z'>
		Hello &lt;denchmark-link:https://github.com/mkocabas&gt;@mkocabas&lt;/denchmark-link&gt;
,
Thanks for the detailed response. Got it.
Hello &lt;denchmark-link:https://github.com/ikvision&gt;@ikvision&lt;/denchmark-link&gt;
,
Thanks for your inputs. I shall explore implementing SMPL-X. If you know any repo that already has implemented this (SMPL-X with openpose body, face &amp; hands keypoints) kindly point me to it.
		</comment>
	</comments>
</bug>