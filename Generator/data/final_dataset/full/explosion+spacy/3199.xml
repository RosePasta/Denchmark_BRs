<bug_data>
<bug id='3199' author='sotnyk' open_date='2019-01-26T10:20:10Z' closed_time='2019-02-08T17:33:33Z'>
 	<summary>TypeError for sentences' noun_chunks if "it_core_news_sm" is used</summary>
 	<description>
 &lt;denchmark-h:h2&gt;How to reproduce the behaviour&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;import spacy
 from spacy.lang.it.examples import sentences as sents_it
 from spacy.lang.fr.examples import sentences as sents_fr
 
 fr_nlp = spacy.load('fr_core_news_sm')
 fr_doc = fr_nlp(sents_fr[0])
 
 it_nlp = spacy.load('it_core_news_sm')
 it_doc = it_nlp(sents_it[0])
 
 print(list(list(fr_doc.sents)[0].noun_chunks))
 print(list(list(it_doc.sents)[0].noun_chunks))
 &lt;/denchmark-code&gt;
 
 For French (also have been checked small English and German vocabularies) we get:
 
 [de dollard]
 
 For Italian:
 &lt;denchmark-code&gt;TypeError                                 Traceback (most recent call last)
 &lt;ipython-input-5-03cc3443c5ec&gt; in &lt;module&gt;
 ----&gt; 1 list(list(it_doc.sents)[0].noun_chunks)
 
 span.pyx in __get__()
 
 TypeError: 'NoneType' object is not callable
 &lt;/denchmark-code&gt;
 
 Additional issue:
 it_doc.noun_chunks simple returns empty list without any elements. But one or more noun chunks is expected.
 &lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;
 
 &lt;denchmark-h:h2&gt;Info about spaCy&lt;/denchmark-h&gt;
 
 
 spaCy version: 2.0.16
 Platform: Windows-10-10.0.17763-SP0
 Python version: 3.6.8
 
 	</description>
 	<comments>
 		<comment id='1' author='sotnyk' date='2019-01-26T13:01:04Z'>
 		Thanks for the report â€“ I really thought we had this fixed in the nightly, but I was able to reproduce the error there as well.
 The problem is that Italian doesn't have a noun chunk iterator and spaCy doesn't fail gracefully here. For Doc objects, I think we already fixed this â€“ but the sentences in doc.sents are Span objects, and it seems like Span.noun_chunks still defaults to None for the noun chunks iterator (instead of just returning no noun chunks).
 		</comment>
 		<comment id='2' author='sotnyk' date='2019-01-27T17:45:11Z'>
 		&lt;denchmark-link:https://github.com/ines&gt;@ines&lt;/denchmark-link&gt;
  , Thank you for explanation.
 So, I should implement noun chunks recognition in Italian texts "by hand", using POS tagging feature.
 		</comment>
 		<comment id='3' author='sotnyk' date='2019-01-30T21:37:42Z'>
 		If you do end up implementing Italian noun chunks, that'd be really cool and we'd definitely appreciate a pull request ðŸ™‚ (Maybe you can take inspiration from one of the other languages, like Spanish?)
 In the meantime, fixing Span.noun_chunks like Doc.noun_chunks (if no noun chunks are available) should also solve this and prevent spaCy from raising an error.
 		</comment>
 		<comment id='4' author='sotnyk' date='2019-03-10T17:43:35Z'>
 		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
 		</comment>
 	</comments>
 </bug>
<commit id='ea07f3022e618a19341105c5fe06c4995739ef4e' author='Ines Montani' date='2019-02-08 18:33:16+01:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='ADD' old_name='None' new_name='spacy\tests\regression\test_issue3199.py'>
 		<file_info nloc='7' complexity='1' token_count='62'></file_info>
 	</modification>
 	<modification change_type='MODIFY' old_name='spacy\tokens\span.pyx' new_name='spacy\tokens\span.pyx'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>406,407,408</added_lines>
 			<deleted_lines>406,407</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
