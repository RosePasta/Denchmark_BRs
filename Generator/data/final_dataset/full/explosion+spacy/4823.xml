<bug_data>
<bug id='4823' author='kormilitzin' open_date='2019-12-19T17:04:25Z' closed_time='2019-12-21T12:07:26Z'>
 	<summary>CLI spacy train fails with large amount of data</summary>
 	<description>
 I am training NER model with 7 categories and the data set contains 200K examples (texts) with average 60K annotated spans per category. However spacy train fails if I use all data. When I randomly subsample, then it works normally. The error I receive when use all data:
 $ python -m spacy train en ....
 
 Training pipeline: ['ner']
 Starting with blank model 'en'
 Counting training words (limit=0)
 Traceback (most recent call last):
 File "/usr/lib/python3.6/runpy.py", line 193, in _run_module_as_main
 "main", mod_spec)
 File "/usr/lib/python3.6/runpy.py", line 85, in _run_code
 exec(code, run_globals)
 File "/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/spacy/main.py", line 33, in 
 plac.call(commands[command], sys.argv[1:])
 File "/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/plac_core.py", line 367, in call
 cmd, result = parser.consume(arglist)
 File "/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/plac_core.py", line 232, in consume
 return cmd, self.func(*(args + varargs + extraopts), **kwargs)
 File "/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/spacy/cli/train.py", line 230, in train
 corpus = GoldCorpus(train_path, dev_path, limit=n_examples)
 File "gold.pyx", line 224, in spacy.gold.GoldCorpus.init
 File "gold.pyx", line 235, in spacy.gold.GoldCorpus.write_msgpack
 File "gold.pyx", line 280, in read_tuples
 File "gold.pyx", line 545, in read_json_file
 File "gold.pyx", line 592, in _json_iterate
 OverflowError: value too large to convert to int
 
 Is there any way to overcome this problem? Thanks.
 	</description>
 	<comments>
 		<comment id='1' author='kormilitzin' date='2019-12-20T08:53:56Z'>
 		That does look like spaCy is crashing on the large training file. Could you provide a little more information to help us look into this:
 
 the exact command you ran
 which spaCy version you're using (from source or installed with pip / conda ? which version number ?)
 how large (in MB or GB) your training file is on disk
 
 		</comment>
 		<comment id='2' author='kormilitzin' date='2019-12-21T08:40:16Z'>
 		This is a duplicate of &lt;denchmark-link:https://github.com/explosion/spaCy/issues/4703&gt;#4703&lt;/denchmark-link&gt;
 . I guess we should add a useful warning and there's really no reason not to change it to .
 		</comment>
 		<comment id='3' author='kormilitzin' date='2019-12-21T12:07:26Z'>
 		Merging this with &lt;denchmark-link:https://github.com/explosion/spaCy/issues/4703&gt;#4703&lt;/denchmark-link&gt;
 !
 		</comment>
 		<comment id='4' author='kormilitzin' date='2020-01-24T12:01:43Z'>
 		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
 		</comment>
 	</comments>
 </bug>
<commit id='732142bf2825be61824d453009cc0cea130c3b4b' author='Sofie Van Landeghem' date='2019-12-21 21:12:19+01:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='spacy\errors.py' new_name='spacy\errors.py'>
 		<file_info nloc='587' complexity='23' token_count='1852'></file_info>
 		<modified_lines>
 			<added_lines>108,109,110,111</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='spacy\gold.pyx' new_name='spacy\gold.pyx'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>16,560,561,562,563,569,577</added_lines>
 			<deleted_lines>16,565,573</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
