<bug_data>
<bug id='624' author='fmfn' open_date='2016-11-11T22:46:06Z' closed_time='2016-11-25T11:45:04Z'>
 	<summary>KeyError when adding special tokens</summary>
 	<description>
 I am trying to run the example of adding special tokens to the tokenizer and getting the following keyerror:
 &lt;denchmark-code&gt;&lt;ipython-input-3-3c4362d5406f&gt; in &lt;module&gt;()
       8             POS: u'VERB'},
       9         {
 ---&gt; 10             ORTH: u'me'}])
      11 assert [w.text for w in nlp(u'gimme that')] == [u'gim', u'me', u'that']
      12 assert [w.lemma_ for w in nlp(u'gimme that')] == [u'give', u'-PRON-', u'that']
 
 /Users/&lt;user&gt;/venvs/general/lib/python3.5/site-packages/spacy/tokenizer.pyx in spacy.tokenizer.Tokenizer.add_special_case (spacy/tokenizer.cpp:8460)()
 
 /Users/&lt;user&gt;/venvs/general/lib/python3.5/site-packages/spacy/vocab.pyx in spacy.vocab.Vocab.make_fused_token (spacy/vocab.cpp:7879)()
 
 KeyError: 'F'
 &lt;/denchmark-code&gt;
 
 The code used is the following:
 &lt;denchmark-code&gt;import spacy
 from spacy.attrs import ORTH, POS, LEMMA
 
 nlp = spacy.load("en", parser=False)
 
 assert [w.text for w in nlp(u'gimme that')] == [u'gimme', u'that']
 nlp.tokenizer.add_special_case(u'gimme',
     [
         {
             ORTH: u'gim',
             LEMMA: u'give',
             POS: u'VERB'},
         {
             ORTH: u'me'}])
 assert [w.text for w in nlp(u'gimme that')] == [u'gim', u'me', u'that']
 assert [w.lemma_ for w in nlp(u'gimme that')] == [u'give', u'-PRON-', u'that']
 &lt;/denchmark-code&gt;
 
 Am I missing something here?
 System info:
 
 MacOS
 python3.5.2
 spacy 1.2.0
 
 	</description>
 	<comments>
 		<comment id='1' author='fmfn' date='2016-11-11T22:49:25Z'>
 		Sorry about this â€” the docs got a bit ahead of the code here. The docs describe how the feature should work, and will work shortly (I'll probably fix it over the weekend).
 At the moment you can use the key "F" instead of ORTH, "L" instead of LEMMA, and "pos" instead of POS.
 		</comment>
 		<comment id='2' author='fmfn' date='2016-11-11T22:54:10Z'>
 		Nice!
 I got it to work by passing 'F' and working backwards, after I traced the make_fused_token method. But "L" and "P" were extra hidden.
 Thanks for the lightning reply and superb work.
 		</comment>
 		<comment id='3' author='fmfn' date='2016-11-11T23:01:13Z'>
 		After changing it to:
 &lt;denchmark-code&gt;nlp.tokenizer.add_special_case(
     u'gimme',
     [
         {
             "F": u'gim',
             "L": u'give',
             "pos": u'VERB'
         },
         {
             "F": u'me',
         }
     ]
 )
 &lt;/denchmark-code&gt;
 
 I get:
 &lt;denchmark-code&gt;KeyError                                  Traceback (most recent call last)
 &lt;ipython-input-6-df7b9eb25a34&gt; in &lt;module&gt;()
       8         },
       9         {
 ---&gt; 10             "F": u'me',
      11         }
      12     ]
 
 /Users/&lt;&gt;/venvs/general/lib/python3.5/site-packages/spacy/tokenizer.pyx in spacy.tokenizer.Tokenizer.add_special_case (spacy/tokenizer.cpp:8460)()
 
 /Users/&lt;&gt;/venvs/general/lib/python3.5/site-packages/spacy/vocab.pyx in spacy.vocab.Vocab.make_fused_token (spacy/vocab.cpp:7907)()
 
 /Users/&lt;&gt;/venvs/general/lib/python3.5/site-packages/spacy/morphology.pyx in spacy.morphology.Morphology.assign_tag (spacy/morphology.cpp:3919)()
 
 KeyError: 97
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='4' author='fmfn' date='2016-11-25T11:45:04Z'>
 		This should now be fixed on master. Thanks for your patience.
 		</comment>
 		<comment id='5' author='fmfn' date='2018-05-09T02:38:23Z'>
 		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
 		</comment>
 	</comments>
 </bug>
<commit id='1e0f566d9549a117cb273ec8ca996e01ba982f08' author='Matthew Honnibal' date='2016-11-25 12:43:24+01:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='spacy\vocab.pyx' new_name='spacy\vocab.pyx'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>23,24,341,343,344,345,346,347,348</added_lines>
 			<deleted_lines>340,341,342,343,344,345,346,347,348</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
