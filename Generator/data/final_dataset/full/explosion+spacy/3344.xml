<bug_data>
<bug id='3344' author='RonRademaker' open_date='2019-02-27T16:39:52Z' closed_time='2019-03-10T15:01:59Z'>
 	<summary>parser.pipe() skips examples when batch_size &amp;lt; 4</summary>
 	<description>
 &lt;denchmark-h:h2&gt;How to reproduce the behaviour&lt;/denchmark-h&gt;
 
 Hi,
 When evaluating the space EntityRecognizer I'm running into performance issues. I use Â±30 custom entities with about Â±70000 examples. When running this in batches on a V100 GPU (batch size of Â±16000) one epoch takes about 1 minute and 20 seconds. After that I want to evaluate both my training and test set to gain insight in what's happening. Obviously, this means I need to run all examples through the model, I use the nlp.pipe(samples) structure to do this with a large batch size and no limits on n_threads. Still, this process takes about 8 minutes on the same machine. I think this is very very weird as I don't see how it makes sense that a forward prop with a backprop can be a magnitude faster than just a forward prop. This is why I classify this a but, sorry if it's not and I'm doing something stupid.
 Example code:
 &lt;denchmark-code&gt;counter = 0
 for doc in nlp.pipe(texts, batch_size=16384):
     counter = counter + 1 # Noop to make sure the inside of my loop is not the bottleneck
 &lt;/denchmark-code&gt;
 
 I tried other batch sizes (anything from 24 to 218) but found no significant difference. Also tried explicit n_threads, but it gives similar results. Using nlp.pipe over looping over example by example did speed up by a factor 2.
 &lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;
 
 
 Operating System: Ubuntu (AWS deep learning AMI, p3.2xlarge)
 Python Version Used: 3.6.5
 spaCy Version Used: 2.0.18
 Environment Information: Tesla V100 GPU with 16GB RAM
 
 	</description>
 	<comments>
 		<comment id='1' author='RonRademaker' date='2019-02-27T20:10:48Z'>
 		Can you try again on spacy-nightly? There's some weirdness with the batch sizing in the v2.0 versions. v2.1 should be out soon.
 		</comment>
 		<comment id='2' author='RonRademaker' date='2019-02-28T10:15:17Z'>
 		Thanks, I tried the nightly version but it looks like entity training broke down somewhere. Testing with the script as documented (I added one sentence that is 100% the same as a training example) on &lt;denchmark-link:https://spacy.netlify.com/usage/training#ner&gt;https://spacy.netlify.com/usage/training#ner&lt;/denchmark-link&gt;
  I get:
 &lt;denchmark-code&gt;Created blank 'en' model
 Losses {'ner': 29.85830795764923}
 Losses {'ner': 22.974769711494446}
 Losses {'ner': 15.957626104354858}
 Losses {'ner': 9.803914964199066}
 Losses {'ner': 8.14788521034643}
 Losses {'ner': 8.616836511762813}
 Losses {'ner': 7.976333131315187}
 Losses {'ner': 7.303826485993341}
 Losses {'ner': 4.836598809459247}
 Losses {'ner': 3.989627659902908}
 Entities in 'Do you like horses?'
 Entities in 'they pretend to care about your feelings, those horses'
 &lt;/denchmark-code&gt;
 
 When training on my actual data it looks like spacy is not learning anything (I did not change the code, and it was learning things before), i.e. it recognizes no entities (0 recall). I use a custom train script for training that was based on the example from the docs. My training data is in the format as used in that script (tuples with text and entities in the text with start and end offsets), is it possible to convert that format to the json the spacy train cli accepts so I can try with that command as well?
 		</comment>
 		<comment id='3' author='RonRademaker' date='2019-02-28T12:45:49Z'>
 		By the way, it does look like evaluation is now a matter of seconds ðŸŽ‰ (but with 0 recall....)
 		</comment>
 		<comment id='4' author='RonRademaker' date='2019-02-28T13:12:50Z'>
 		The links are pointing to the master branch I think, which will be correct on launch. It should work with the version of the script on develop.
 		</comment>
 		<comment id='5' author='RonRademaker' date='2019-02-28T15:27:56Z'>
 		Thanks, that script does work for me so obviously I'm doing something wrong. I can't really find any real differences that would explain it though, debugging I did something that strikes me as very odd:
 After each epoch I do:
 &lt;denchmark-code&gt;print(nlp.get_pipe('ner').model._mem.weights)
 &lt;/denchmark-code&gt;
 
 And it consistently gives me an empty list [] which seems to me that the model has no parameters (so it can't predict or learn anything). The model was created with blank
 &lt;denchmark-code&gt;print(nlp.get_pipe('ner').model._mem._mem)
 &lt;/denchmark-code&gt;
 
 Consistently gives all zeros:
 &lt;denchmark-code&gt;[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
   0. 0. 0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
   0. 0. 0. 0. 0. 0. 0. 0.]]
 &lt;/denchmark-code&gt;
 
 Any ideas where I can look next?
 		</comment>
 		<comment id='6' author='RonRademaker' date='2019-02-28T17:35:15Z'>
 		Well, the example has very little data, so the learning is pretty delicate. I think the way the batching is different on the master branch, it just happens to not converge well.
 As for the weights: try model.upper._mem. The parser model has three layers that hold the weights. See the spacy/syntax/_parser_model.pyx file for details.
 		</comment>
 		<comment id='7' author='RonRademaker' date='2019-03-01T08:37:31Z'>
 		Thanks, that allowed me to make sure that weights were updating. I also found the issue, it was not with training but with evaluating in batches:
 Works as expected:
 &lt;denchmark-code&gt;for text in samples:
     doc = nlp(text)
     # evaluate doc.ents
 &lt;/denchmark-code&gt;
 
 Zero recall:
 &lt;denchmark-code&gt;for doc in nlp.pipe(texts, batch_size=1):
     # evaluate doc.ents
 &lt;/denchmark-code&gt;
 
 Zero recall:
 &lt;denchmark-code&gt;for doc in nlp.pipe(texts, batch_size=2):
     # evaluate doc.ents
 &lt;/denchmark-code&gt;
 
 Works as expected:
 &lt;denchmark-code&gt;for doc in nlp.pipe(texts, batch_size=4):
     # evaluate doc.ents
 &lt;/denchmark-code&gt;
 
 Increasing the batch size further things keep working.
 		</comment>
 		<comment id='8' author='RonRademaker' date='2019-03-04T16:33:42Z'>
 		Looks suspicious for sure! Thanks for reporting.
 		</comment>
 		<comment id='9' author='RonRademaker' date='2019-03-10T15:01:59Z'>
 		Yeah the batching bug was a regression introduced quite recently, when I fixed a constant in the batch sizing. I changed it to take batch_size // 4 as a value for a sub-batch, but this breaks for n &lt; 4. Fixed now.
 		</comment>
 		<comment id='10' author='RonRademaker' date='2019-04-09T15:38:43Z'>
 		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
 		</comment>
 	</comments>
 </bug>
<commit id='7461e5e055bafab323faa6d7f0160162b655b523' author='Matthew Honnibal' date='2019-03-10 16:01:34+01:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='spacy\syntax\nn_parser.pyx' new_name='spacy\syntax\nn_parser.pyx'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>224</added_lines>
 			<deleted_lines>224</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
