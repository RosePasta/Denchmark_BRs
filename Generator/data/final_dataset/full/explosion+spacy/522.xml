<bug_data>
<bug id='522' author='xuanyiguang' open_date='2016-10-11T19:09:31Z' closed_time='2016-10-23T13:28:49Z'>
 	<summary>vector_norm and similarity value incorrect</summary>
 	<description>
 Somehow vector_norm is incorrectly calculated.
 &lt;denchmark-code&gt;import spacy
 import numpy as np
 nlp = spacy.load("en")
 # using u"apples" just as an example
 apples = nlp.vocab[u"apples"]
 print apples.vector_norm
 # prints 1.4142135381698608, or sqrt(2)
 print np.sqrt(np.dot(apples.vector, apples.vector))
 # prints 1.0
 &lt;/denchmark-code&gt;
 
 Then vector_norm is used in similarity, which always returns a value that is always half of the correct value.
 &lt;denchmark-code&gt;def similarity(self, other):
     if self.vector_norm == 0 or other.vector_norm == 0:
         return 0.0
     return numpy.dot(self.vector, other.vector) / (self.vector_norm * other.vector_norm)
 &lt;/denchmark-code&gt;
 
 It is OK if the use case is to rank similarity scores for synonyms. But the cosine similarity score itself is incorrect.
 	</description>
 	<comments>
 		<comment id='1' author='xuanyiguang' date='2016-10-11T19:11:44Z'>
 		Thanks! Will figure this out.
 		</comment>
 		<comment id='2' author='xuanyiguang' date='2016-10-23T13:03:07Z'>
 		I think this is fixed in 1.0, but this bug makes me uneasy because I don't feel like I really understand what was wrong. I haven't had time to test 0.101.0 yet, but: you say the cosine was always half? I can't figure out why that should be...
 What I've come up with is that this calculation looks unreliable:
         for orth, lex_addr in self._by_orth.items():
             lex = &lt;LexemeC*&gt;lex_addr
             if lex.lower &lt; vectors.size():
                 lex.vector = vectors[lex.lower]
                 for i in range(vec_len):
                     lex.l2_norm += (lex.vector[i] * lex.vector[i])
                 lex.l2_norm = math.sqrt(lex.l2_norm)
             else:
                 lex.vector = EMPTY_VEC
 The lex.l2_norm value is possibly uninitialised, and so there may be a problem there. Passing a 32 bit float to the Python function math.sqrt is also suspicious. But if this was the problem, the results should have been "unreliable, always wrong". Always half?? Unsettling!
 		</comment>
 		<comment id='3' author='xuanyiguang' date='2016-10-23T13:28:49Z'>
 		Got it now.
 The previous default vectors were already normalized. This led to a value of lex.l2_norm = 1 being stored in the lexemes.bin file. This was then read back out into the LexemeC struct when the vocabulary was deserialised.
 Later, I added the capability to load custom word vectors, which meant the L2 norm had to be calculated. However, I didn't initialised the value of lex.l2_norm to 0 before computing the new norm. Since the default vectors were normalised, the initial value was always 1, and the eventual norm was sqrt(1+1). This explains why the similarity was consistently half.
 No tests checked the exact value returned by the similarity function. They only sanity-checked relative values. This has since been addressed.
 		</comment>
 		<comment id='4' author='xuanyiguang' date='2018-05-09T07:39:15Z'>
 		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
 		</comment>
 	</comments>
 </bug>
<commit id='2c3a67b693da506ca4b523743969ee6757e98b22' author='Matthew Honnibal' date='2016-10-23 14:49:31+02:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='spacy\tokens\doc.pyx' new_name='spacy\tokens\doc.pyx'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>4,254,256,258,259</added_lines>
 			<deleted_lines>9,255,257,258</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='spacy\tokens\span.pyx' new_name='spacy\tokens\span.pyx'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>6,139,141,143,144</added_lines>
 			<deleted_lines>6,140,142,143</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
