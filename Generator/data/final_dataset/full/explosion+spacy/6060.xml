<bug_data>
<bug id='6060' author='kinghuang' open_date='2020-09-13T02:30:33Z' closed_time='2020-09-22T19:53:34Z'>
 	<summary>Calling retokenize after setting custom token norm applies norm to wrong token</summary>
 	<description>
 Setting a custom norm on a token then retokenizing causes the custom norm to be applied to both the original token (which has now moved) and whatever token is now at the original token's index (due to retokenization).
 &lt;denchmark-h:h2&gt;How to reproduce the behaviour&lt;/denchmark-h&gt;
 
 import spacy
 
 nlp = spacy.load("en")
 doc = nlp("The quick brownfoxjumpsoverthe lazy dog w/ white spots.")
 
 # Set custom norm on the w/ token.
 doc[5].norm_ = "with"
 
 # Retokenize to split out the words in the token at doc[2].
 token = doc[2]
 with doc.retokenize() as retokenizer:
   retokenizer.split(token, ["brown", "fox", "jumps", "over", "the"], heads=[(token, idx) for idx in range(5)])
 
 # The new token at index 5 is "over". But, its norm is incorrectly set to "with".
 # The previous "w/" token is now at index 9, and still has the custom norm set earlier.
 assert doc[9].text  == "w/"   # OK
 assert doc[9].norm_ == "with" # OK
 assert doc[5].text  == "over" # OK
 assert doc[5].norm_ == "over" # Error - The new "over" token has the "w/" token's custom norm.
 &lt;denchmark-h:h2&gt;Info about spaCy&lt;/denchmark-h&gt;
 
 
 spaCy version: 2.3.2
 Platform: Linux-4.19.76-linuxkit-x86_64-with-glibc2.2.5
 Python version: 3.8.5
 Models: en
 
 	</description>
 	<comments>
 		<comment id='1' author='kinghuang' date='2020-09-13T17:47:39Z'>
 		Thanks for the report! Definitely looks like a bug, we'll look into it.
 		</comment>
 	</comments>
 </bug>
<commit id='e4acb286582477caaf5486833781c5802374d171' author='Adriane Boyd' date='2020-09-22 21:53:33+02:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='spacy\tests\doc\test_retokenize_split.py' new_name='spacy\tests\doc\test_retokenize_split.py'>
 		<file_info nloc='170' complexity='12' token_count='1827'></file_info>
 		<method name='test_doc_retokenizer_split_norm' parameters='en_vocab'>
 				<method_info nloc='11' complexity='2' token_count='117' nesting_level='0' start_line='203' end_line='219'></method_info>
 			<added_lines>203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>201,202</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='spacy\tokens\_retokenize.pyx' new_name='spacy\tokens\_retokenize.pyx'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>358</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
