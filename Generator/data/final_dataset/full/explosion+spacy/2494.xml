<bug_data>
<bug id='2494' author='arlinajsk' open_date='2018-06-29T15:14:53Z' closed_time='2018-07-06T10:41:07Z'>
 	<summary>Problem with deserializing custom Tokenizer on windows</summary>
 	<description>
 &lt;denchmark-h:h2&gt;How to reproduce the behaviour&lt;/denchmark-h&gt;
 
 I customize the tokenizer by simply changing the infix:
 infix_re = re.compile(r'''[-~/]''')
 nlp.tokenizer = Tokenizer(nlp.vocab, infix_finditer = infix_re.finditer )
 When I try to save the model to disk, I get this error, complaining about the prefix (which I don't want to set, just wanted to modify the infix bebavior)
 nlp.to_disk( path )
 File "ext\vc12_win32\lib\python2.7\site-packages\spacy\language.py", line 607, in to_disk
 File "ext\vc12_win32\lib\python2.7\site-packages\spacy\util.py", line 514, in to_disk
 File "ext\vc12_win32\lib\python2.7\site-packages\spacy\language.py", line 595, in 
 File "spacy\tokenizer.pyx", line 358, in spacy.tokenizer.Tokenizer.to_disk (spacy/tokenizer.cpp:7862)
 with path.open('wb') as file_:
 File "spacy\tokenizer.pyx", line 359, in spacy.tokenizer.Tokenizer.to_disk (spacy/tokenizer.cpp:7789)
 file_.write(self.to_bytes(**exclude))
 File "spacy\tokenizer.pyx", line 388, in spacy.tokenizer.Tokenizer.to_bytes (spacy/tokenizer.cpp:8959)
 return util.to_bytes(serializers, exclude)
 File "ext\vc12_win32\lib\python2.7\site-packages\spacy\util.py", line 496, in to_bytes
 File "spacy\tokenizer.pyx", line 382, in spacy.tokenizer.Tokenizer.to_bytes.lambda2 (spacy/tokenizer.cpp:8386)
 ('prefix_search', lambda: self.prefix_search.self.pattern),
 AttributeError: 'NoneType' object has no attribute 'self'
 Since the tokenizer is working for me, I just thought I'll disable it while saving the model:
 nlp.to_disk( path, disable = ( 'tokenizer', ) )
 Now when I try to load it back, I got this error:
 nlp2 = spacy.load( output_dir )
 File "ext\vc12_win32\lib\python2.7\site-packages\spacy_init_.py", line 19, in load
 File "ext\vc12_win32\lib\python2.7\site-packages\spacy\util.py", line 119, in load_model
 File "ext\vc12_win32\lib\python2.7\site-packages\spacy\util.py", line 159, in load_model_from_path
 File "ext\vc12_win32\lib\python2.7\site-packages\spacy\language.py", line 638, in from_disk
 File "ext\vc12_win32\lib\python2.7\site-packages\spacy\util.py", line 522, in from_disk
 File "ext\vc12_win32\lib\python2.7\site-packages\spacy\language.py", line 626, in 
 File "spacy\tokenizer.pyx", line 371, in spacy.tokenizer.Tokenizer.from_disk (spacy/tokenizer.cpp:8195)
 self.from_bytes(bytes_data, **exclude)
 File "spacy\tokenizer.pyx", line 406, in spacy.tokenizer.Tokenizer.from_bytes (spacy/tokenizer.cpp:9890)
 msg = util.from_bytes(bytes_data, deserializers, exclude)
 File "ext\vc12_win32\lib\python2.7\site-packages\spacy\util.py", line 501, in from_bytes
 File "ext3\noarch\pylib\site-packages\msgpack_numpy.py", line 187, in unpackb
 File "msgpack/_unpacker.pyx", line 211, in msgpack._unpacker.unpackb
 UnpackValueError: Unpack failed: error = 0
 Things work fine without the custom tokenizer behavior all together (the custom tokenizer works and the NER is identified correctly)
 &lt;denchmark-h:h2&gt;Your Environment&lt;/denchmark-h&gt;
 
 
 Operating System: Windows 7
 Python Version Used: 2.7
 spaCy Version Used: 2.0.9 / 2.0.10
 Environment Information:
 
 	</description>
 	<comments>
 		<comment id='1' author='arlinajsk' date='2018-06-29T17:06:26Z'>
 		Thanks for the report â€“ a similar issue actually  &lt;denchmark-link:https://support.prodi.gy/t/how-to-save-a-custom-tokenizer/661/4&gt;came up&lt;/denchmark-link&gt;
 , so we've been working on the tokenizer serialization (see the  branch in &lt;denchmark-link:https://github.com/explosion/spaCy/commit/f08c871adf6f126c2ea7112804c813b977bcb167&gt;f08c871&lt;/denchmark-link&gt;
  and see &lt;denchmark-link:https://github.com/explosion/spaCy/commit/526be4082329d16ecf7b1fa40b81f2008396a325&gt;526be40&lt;/denchmark-link&gt;
 ).
 Basically, the problem is that the prefix, suffix, infix and token match rules default to None, which works fine while you're using the tokenizer. But when the tokenizer is saved, spaCy falsely assumes that they're all functions, so it doesn't serialize correctly (and without the tokenizer, spaCy complains when you try to load the model back in).
 In your case, you want to use custom infix rules and the default prefix and suffix rules, right? If so, you could do the following as a workaround:
 def custom_tokenizer(nlp):
     infix_re = re.compile(r'''[-~/]''')
     return Tokenizer(nlp.vocab,
                      prefix_search=nlp.tokenizer.prefix_search,
                      suffix_search=nlp.tokenizer.suffix_search,
                      token_match=nlp.tokenizer.token_match,
                      infix_finditer=infix_re.finditer)
 
 nlp.tokenizer = custom_tokenizer(nlp)
 This will set all other tokenizer rules to the defaults of your nlp object.
 		</comment>
 		<comment id='2' author='arlinajsk' date='2018-06-29T17:22:19Z'>
 		The workaround works! thank you &lt;denchmark-link:https://github.com/ines&gt;@ines&lt;/denchmark-link&gt;
 !
 		</comment>
 		<comment id='3' author='arlinajsk' date='2018-08-05T10:52:33Z'>
 		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
 		</comment>
 	</comments>
 </bug>
<commit id='38e07ade4c2e940567c4d6f54674a9fcedf920c6' author='ines' date='2018-07-06 12:40:51+02:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='spacy\tests\serialize\test_serialize_tokenizer.py' new_name='spacy\tests\serialize\test_serialize_tokenizer.py'>
 		<file_info nloc='31' complexity='6' token_count='254'></file_info>
 		<method name='test_serialize_custom_tokenizer' parameters='en_vocab,en_tokenizer'>
 				<method_info nloc='4' complexity='1' token_count='38' nesting_level='0' start_line='17' end_line='22'></method_info>
 			<added_lines>17,18,19,20,21,22</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>5,23,24</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
