<bug_data>
<bug id='115' author='dabadee' open_date='2019-01-01T19:03:40Z' closed_time='2019-01-21T15:09:21Z'>
 	<summary>NNZ (Sparse) turn to 0 on Quantizing</summary>
 	<description>
 I am doing the following
 
 
 Train simplenet on cifar
 python3 compress_classifier.py --arch simplenet_cifar ../../../data.cifar10 -p 30 -j=1 --lr=0.01 --epochs 100
 
 
 Prune using AGP
 python3 compress_classifier.py -a simplenet_cifar ../../../data.cifar10 -p 30 -j=1 --lr=0.01 --compress=agp_simplenet75.yaml --resume logs/2019.01.01-134735/checkpoint.pth.tar --epochs 25
 
 
 Quantize using QuantAwareTrainRangeLinearQuantizer
 python3 compress_classifier.py -a simplenet_cifar ../../../data.cifar10 -p 30 -j=1 --lr=0.001 --compress=quant.yaml --resume logs/2019.01.01-183251/checkpoint.pth.tar --epochs 20
 
 
 On the very first step both conv layers' weights and their associated float_weight, scale, and zero point layers become 100% sparse with 0 NNZ sparse values.
 Is it a bug or am I going in wrong direction ?
 My yaml files are as follows
 agp_simplenet75.yaml
 &lt;denchmark-code&gt;version: 1
 pruners:
   fc1_pruner:
     class: 'AutomatedGradualPruner'
     initial_sparsity : 0.05
     final_sparsity: 0.75
     weights: ['module.fc1.weight']
 
   fc2_pruner:
     class: 'AutomatedGradualPruner'
     initial_sparsity : 0.05
     final_sparsity: 0.75
     weights: ['module.fc2.weight']
  
   fc3_pruner:
     class: 'AutomatedGradualPruner'
     initial_sparsity : 0.05
     final_sparsity: 0.75
     weights: ['module.fc3.weight']
 
   conv1_pruner:
     class: 'AutomatedGradualPruner'
     initial_sparsity : 0.03
     final_sparsity: 0.75
     weights: ['module.conv1.weight']
 
   conv2_pruner:
     class: 'AutomatedGradualPruner'
     initial_sparsity : 0.03
     final_sparsity: 0.75
     weights: ['module.conv2.weight']
 
 
 lr_schedulers:
   # Learning rate decay scheduler
    pruning_lr:
      class: ExponentialLR
      gamma: 0.9
 
 
 policies:
   - pruner:
       instance_name : 'conv1_pruner'
     starting_epoch: 100
     ending_epoch: 110
     frequency: 2
 
   - pruner:
       instance_name : 'conv2_pruner'
     starting_epoch: 100
     ending_epoch: 110
     frequency: 2
 
   - pruner:
       instance_name : 'fc1_pruner'
     starting_epoch: 100
     ending_epoch: 120
     frequency: 2
 
   - pruner:
       instance_name : 'fc2_pruner'
     starting_epoch: 100
     ending_epoch: 120
     frequency: 2
 
   - pruner:
       instance_name: 'fc3_pruner'
     starting_epoch: 100
     ending_epoch: 120
     frequency: 2
 
   - lr_scheduler:
       instance_name: pruning_lr
     starting_epoch: 101
     ending_epoch: 180
     frequency: 1
 &lt;/denchmark-code&gt;
 
 quant.yaml
 &lt;denchmark-code&gt;quantizers:
   linear_quantizer:
     class: QuantAwareTrainRangeLinearQuantizer
     bits_activations: 8
     bits_weights: 8
     mode: 'ASYMMETRIC_UNSIGNED'  # Can try "SYMMETRIC" as well
     ema_decay: 0.999   # Decay value for exponential moving average tracking of activation ranges
     per_channel_wts: True
 
 policies:
     - quantizer:
         instance_name: linear_quantizer
       # For now putting a large range here, which should cover both training from scratch or resuming from some
       # pre-trained checkpoint at some unknown epoch
       starting_epoch: 0
       ending_epoch: 300
       frequency: 1
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='dabadee' date='2019-01-09T15:44:46Z'>
 		can any one help with this ? or just give me some pointers to look at
 		</comment>
 		<comment id='2' author='dabadee' date='2019-01-10T15:11:26Z'>
 		It is a bug indeed. After pruning, we have entire channels with value 0, and this isn't handled properly. The calculated scale factor for these channels is inf, and in turn the quantized values are nan, and the end result is what you see.
 I'll be pushing several quantization related changes in the coming days, so will fix this along with those.
 Apologies for taking so long to respond.
 		</comment>
 		<comment id='3' author='dabadee' date='2019-01-21T15:09:10Z'>
 		Should be fixed by &lt;denchmark-link:https://github.com/NervanaSystems/distiller/commit/10ce938cf010f2cbc4865f13efdd10d7bd5f63f9&gt;this commit&lt;/denchmark-link&gt;
 .
 Also, note that the  model you were experimenting with called ReLU via the functional PyTorch API. However, for the  to be able to detect layers and replace them with a quantized implementation, they must be instantiated as modules. The way it was, only the weights in  were being quantized, and the activations were left untouched. I also pushed a &lt;denchmark-link:https://github.com/NervanaSystems/distiller/commit/24381169f2bb70c953cfc765ae342d98ef8a3153&gt;commit&lt;/denchmark-link&gt;
  modifying the model accordingly.
 Closing this, please re-open if there are anymore issues.
 		</comment>
 		<comment id='4' author='dabadee' date='2019-01-21T15:16:15Z'>
 		great, I will check it out today
 		</comment>
 	</comments>
 </bug>
<commit id='10ce938cf010f2cbc4865f13efdd10d7bd5f63f9' author='Guy Jacob' date='2019-01-21 16:48:06+02:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='0.11578947368421053'></dmm_unit>
 	<modification change_type='MODIFY' old_name='distiller\quantization\q_utils.py' new_name='distiller\quantization\q_utils.py'>
 		<file_info nloc='97' complexity='27' token_count='836'></file_info>
 		<method name='symmetric_linear_quantization_params' parameters='num_bits,saturation_val'>
 				<method_info nloc='11' complexity='3' token_count='79' nesting_level='0' start_line='30' end_line='48'></method_info>
 			<added_lines>31,32,33,34,35,38,39,40,41,42,43,44,45,46,47</added_lines>
 			<deleted_lines>34,35,37,38,39,40</deleted_lines>
 		</method>
 		<method name='_prep_saturation_val_tensor' parameters='sat_val'>
 				<method_info nloc='8' complexity='3' token_count='63' nesting_level='0' start_line='20' end_line='27'></method_info>
 			<added_lines>20,21,22,23,24,25,26,27</added_lines>
 			<deleted_lines>23,24,25,26,27</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>28,29,53,54,55,56,57,58,59,60,61,62,63,64,66,67,68,69,70,71,72,73,74,75,76,77,79,82,83</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='tests\test_quant_utils.py'>
 		<file_info nloc='92' complexity='2' token_count='1285'></file_info>
 	</modification>
 </commit>
</bug_data>
