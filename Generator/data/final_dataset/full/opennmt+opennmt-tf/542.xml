<bug_data>
<bug id='542' author='guillaumekln' open_date='2019-11-07T10:39:42Z' closed_time='2019-11-07T12:43:45Z'>
 	<summary>experimental_distribute_dataset does not always split the batches</summary>
 	<description>
 In &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-tf/commit/4f8189b7a84c23f1fdd33db67a1e25579065d042&gt;4f8189b&lt;/denchmark-link&gt;
 , we scaled the batch size by the number of devices based on  documentation:
 
 We will assume that the input dataset is batched by the global batch size
 
 However, the method does not always split the input batches. This was at least found when using batch_type: tokens even though the first dimension is divisible by the number of replicas.
 	</description>
 	<comments>
 	</comments>
 </bug>
<commit id='bbf48f4ca8ffd2804b36019f217d07e3704a560e' author='Guillaume Klein' date='2019-11-07 13:43:44+01:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='opennmt\runner.py' new_name='opennmt\runner.py'>
 		<file_info nloc='334' complexity='37' token_count='2214'></file_info>
 		<method name='train' parameters='self,num_devices,with_eval,checkpoint_path'>
 				<method_info nloc='60' complexity='6' token_count='384' nesting_level='1' start_line='127' end_line='203'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>158</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='opennmt\training.py' new_name='opennmt\training.py'>
 		<file_info nloc='167' complexity='19' token_count='1279'></file_info>
 		<modified_lines>
 			<added_lines>70,71,72,73,74,75,76</added_lines>
 			<deleted_lines>70</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
