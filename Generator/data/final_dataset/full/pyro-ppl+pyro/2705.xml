<bug_data>
<bug id='2705' author='fritzo' open_date='2020-12-03T13:27:00Z' closed_time='2020-12-04T04:37:33Z'>
 	<summary>PyroModule.requires_grad_(False).__call__() issues unwanted warning</summary>
 	<description>
 After training a PyroModule model, I'd like to detach it and use it elsewhere in a "frozen" state. One obvious way to do so would be to call my_module.requires_grad_(False) to detach all parameters. Doing so results in the desired behavior, but has the side effect of triggering the following warning (many many times):
 &lt;denchmark-code&gt;/Users/fobermey/github/pyro-ppl/pyro/pyro/primitives.py:361: UserWarning: linear.bias was not registered in the param store because requires_grad=False
   " requires_grad=False")
 &lt;/denchmark-code&gt;
 
 It is unclear to me how to resolve this. Should we provide a detach_() method for modules that converts all PyroParams to Tensors? Should we simply remove that warning if it occurs in a PyroModule? Should we be calling some other method like my_module.train(False)?
 	</description>
 	<comments>
 	</comments>
 </bug>
<commit id='a707d96c36490fbe65d1a22bb36d82ae957b1158' author='Fritz Obermeyer' date='2020-12-03 22:37:33-06:00'>
 	<dmm_unit complexity='0.0' interfacing='0.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pyro\primitives.py' new_name='pyro\primitives.py'>
 		<file_info nloc='268' complexity='35' token_count='992'></file_info>
 		<method name='module' parameters='name,nn_module,update_module_params'>
 				<method_info nloc='33' complexity='12' token_count='212' nesting_level='0' start_line='324' end_line='380'></method_info>
 			<added_lines>359,360,361,362</added_lines>
 			<deleted_lines>359,360,361</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
