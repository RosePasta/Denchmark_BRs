<bug_data>
<bug id='1374' author='thematrixduo' open_date='2018-09-10T13:17:49Z' closed_time='2018-09-14T18:02:14Z'>
 	<summary>Low count accuracy of AIR model</summary>
 	<description>
 Hi,
 I have been experimenting with the Attend Infer Repeat model. I downloaded the code from examples and run the code with the args provided in the tutorial (&lt;denchmark-link:http://pyro.ai/examples/air.html&gt;http://pyro.ai/examples/air.html&lt;/denchmark-link&gt;
 ), which is:
 python main.py -n 200000 -blr 0.1 --z-pres-prior 0.01 --scale-prior-sd 0.2 --predict-net 200 --bl-predict-net 200 --decoder-output-use-sigmoid --decoder-output-bias -2 --seed 287710
 But the highest count accuracy I get is around 76%. Could someone let me know if the args are not correct? I did not change any part of the code.
 Also I am using pyro version 0.2.1 and pytorch version 0.4.0
 Many thanks
 	</description>
 	<comments>
 		<comment id='1' author='thematrixduo' date='2018-09-10T18:21:34Z'>
 		Hi, did you let inference run to completion or are you stopping it after a few epochs?  Are you talking about count accuracy on the training images (as in the tutorial) or some unseen test images?
 cc &lt;denchmark-link:https://github.com/null-a&gt;@null-a&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='thematrixduo' date='2018-09-10T18:37:40Z'>
 		
 Could someone let me know if the args are not correct?
 
 The command line args in the tutorial are the ones I used, and it looks like you're using the same.
 As well as answers to eb8680's questions, I'd be interested in hearing what other values for count accuracy you obtained, if that's possible. Thanks.
 		</comment>
 		<comment id='3' author='thematrixduo' date='2018-09-11T11:03:13Z'>
 		&lt;denchmark-link:https://github.com/eb8680&gt;@eb8680&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/null-a&gt;@null-a&lt;/denchmark-link&gt;
 
 I let the inference run till the end (I believe it is specified by -n 200000). I am using the count accuracy function provided in 'main.py'. I just specified the args '--eval-every' to 1000. I have attached a plot of the count accuracy obtained. This plot looks very different from the plot given in the tutorial.
 &lt;denchmark-link:https://user-images.githubusercontent.com/8089252/45356239-a5806600-b5ba-11e8-9bb2-e2b819eaf611.jpg&gt;&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='4' author='thematrixduo' date='2018-09-11T12:44:21Z'>
 		&lt;denchmark-link:https://github.com/thematrixduo&gt;@thematrixduo&lt;/denchmark-link&gt;
  How many times have you run inference? What other values for final count accuracy have you obtained? Setting the random seed doesn't make this deterministic, so some variance is expected. If you're seeing consistently poor results, I'll try running it myself and see if I can spot anything odd going on. Thanks.
 		</comment>
 		<comment id='5' author='thematrixduo' date='2018-09-11T16:09:20Z'>
 		&lt;denchmark-link:https://github.com/null-a&gt;@null-a&lt;/denchmark-link&gt;
  I have tried at least 5 times but for none of them did the count accuracy go above 80%. I have attached another plot.
 &lt;denchmark-link:https://user-images.githubusercontent.com/8089252/45372635-6a475c80-b5e5-11e8-8d74-7c4a8d3b9cf6.jpg&gt;&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='6' author='thematrixduo' date='2018-09-12T09:24:12Z'>
 		&lt;denchmark-link:https://github.com/thematrixduo&gt;@thematrixduo&lt;/denchmark-link&gt;
  Great, thanks for the info. That sounds worse than I would expect, so I'll take a look. To begin, I guess I'll confirm I'm getting similar results to you, and then I'll go back and try running against the Pyro commit recorded in the tutorial.
 		</comment>
 		<comment id='7' author='thematrixduo' date='2018-09-12T15:10:07Z'>
 		&lt;denchmark-link:https://github.com/thematrixduo&gt;@thematrixduo&lt;/denchmark-link&gt;
  It looks like performance may have degraded after &lt;denchmark-link:https://github.com/pyro-ppl/pyro/commit/c99ea6732b342ef9a52430c8a2567b0387912236&gt;c99ea67&lt;/denchmark-link&gt;
 . If you apply the following patch (to v0.2.1 say) and then re-try I think you will see performance similar to that reported in the tutorial.
 diff --git a/examples/air/main.py b/examples/air/main.py
 index df89df2..09bdf25 100644
 --- a/examples/air/main.py
 +++ b/examples/air/main.py
 @@ -195,7 +195,7 @@ def main(**kwargs):
          vis.images(draw_many(x, tensor_to_objs(latents_to_tensor(z))))
  
      def per_param_optim_args(module_name, param_name):
 -        lr = args.baseline_learning_rate if 'bl_' in param_name else args.learning_rate
 +        lr = args.baseline_learning_rate if 'bl_' in param_name or 'bl_' in module_name else args.learning_rate
          return {'lr': lr}
  
      svi = SVI(air.model, air.guide,
 If we confirm this fixes the problem, then I'll open a PR to apply the patch to dev. (BTW, I've not been using a fixed random seed when I run this locally, and I seem to achieve OK results consistently.)
 		</comment>
 		<comment id='8' author='thematrixduo' date='2018-09-12T17:47:47Z'>
 		&lt;denchmark-link:https://github.com/null-a&gt;@null-a&lt;/denchmark-link&gt;
  good catch!
 		</comment>
 		<comment id='9' author='thematrixduo' date='2018-09-13T14:05:45Z'>
 		&lt;denchmark-link:https://github.com/null-a&gt;@null-a&lt;/denchmark-link&gt;
  Thanks for the help!
 		</comment>
 		<comment id='10' author='thematrixduo' date='2018-09-14T19:18:11Z'>
 		
 @null-a Thanks for the help!
 
 &lt;denchmark-link:https://github.com/thematrixduo&gt;@thematrixduo&lt;/denchmark-link&gt;
  No problem, thanks for opening the issue.
 		</comment>
 	</comments>
 </bug>
<commit id='b03be07f31aeaf11f3526453874b396b69c7e854' author='null-a' date='2018-09-14 11:02:09-07:00'>
 	<dmm_unit complexity='0.6666666666666666' interfacing='1.0' size='0.6666666666666666'></dmm_unit>
 	<modification change_type='MODIFY' old_name='examples\air\air.py' new_name='examples\air\air.py'>
 		<file_info nloc='233' complexity='23' token_count='2316'></file_info>
 		<method name='baseline_step' parameters='self,prev,inputs'>
 				<method_info nloc='15' complexity='4' token_count='142' nesting_level='1' start_line='260' end_line='286'></method_info>
 			<added_lines>285</added_lines>
 			<deleted_lines>285</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='examples\air\main.py' new_name='examples\air\main.py'>
 		<file_info nloc='252' complexity='40' token_count='2205'></file_info>
 		<method name='main' parameters='kwargs'>
 				<method_info nloc='81' complexity='23' token_count='660' nesting_level='0' start_line='124' end_line='243'></method_info>
 			<added_lines>197,198,199,201</added_lines>
 			<deleted_lines>198</deleted_lines>
 		</method>
 		<method name='main.isBaselineParam' parameters='module_name,param_name'>
 				<method_info nloc='2' complexity='2' token_count='15' nesting_level='1' start_line='197' end_line='198'></method_info>
 			<added_lines>197,198</added_lines>
 			<deleted_lines>198</deleted_lines>
 		</method>
 		<method name='main.per_param_optim_args' parameters='module_name,param_name'>
 				<method_info nloc='3' complexity='2' token_count='29' nesting_level='1' start_line='200' end_line='202'></method_info>
 			<added_lines>201</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
