<bug_data>
<bug id='531' author='fritzo' open_date='2017-11-08T04:10:24Z' closed_time='2017-11-08T23:39:37Z'>
 	<summary>site["scale"] is double counted at non-reparameterized sites</summary>
 	<description>
 &lt;denchmark-h:h2&gt;The Problem&lt;/denchmark-h&gt;
 
 Consider the surrogate elbo at a nonreparameterized site (as &lt;denchmark-link:https://github.com/uber/pyro/blob/ece8ccc167695f81aa7ec1fed74a70c8fc15254b/pyro/infer/trace_elbo.py#L150&gt;computed in&lt;/denchmark-link&gt;
  )
 log_p = model_site["log_pdf"]
 log_q = guide_site["log_pdf"]
 surrogate_elbo = log_p + (log_p - log_q).detach() * log_q
 Now suppose this site occurs in an  where we're subsampling 1% of the data, so that a  sets . Then  &lt;denchmark-link:https://github.com/uber/pyro/blob/ece8ccc167695f81aa7ec1fed74a70c8fc15254b/pyro/poutine/trace.py#L71&gt;will scale&lt;/denchmark-link&gt;
  both  and  by 100. Therefore the 's first term  will be scaled by 100, but the second term  will be scaled by 100 * 100.
 &lt;denchmark-h:h2&gt;Scope&lt;/denchmark-h&gt;
 
 This should only affect models in one of the following scenarios:
 
 The model has both global and local sites, and the local sites are non-reparameterized and subsampled.
 The model has local sites that are non-reparameterized and subsampled and includes model parameters at that site (not only guide parameters).
 
 &lt;denchmark-h:h2&gt;Possible Solutions&lt;/denchmark-h&gt;
 
 
 Handle site["scale"] separately, outside of Trace.log_pdf(). This scale is already wired through Trace_ELBO and TraceGraph_ELBO as weight.
 Pros: Avoids being too clever. Plays well with scale that varies across a batch (as in BranchPoutine).
 Cons: May require special treatment of site["scale"] in many places outside of Trace_ELBO and TraceGraph_ELBO.
 Keep the existing logic of Trace.log_pdf(), and remove a factor of site["scale"] from the (log_p - log_q).detach() * log_q term.
 Pros: Minimally invasive.
 Cons: Seems like a hack. Does not play well with scales that vary across a batch.
 
 	</description>
 	<comments>
 		<comment id='1' author='fritzo' date='2017-11-08T08:17:53Z'>
 		
 but the second term (log_p - log_q).detach() * log_q will be scaled by 100 * 100.
 
 Is this really a bug?
 log_p_scaled is a good estimate of log_p_true, similarly for log_q. If you want to compute an estimate of (log_p_true - log_q_true) * log_q_true then (log_p_scaled - log_q_scaled) * log_q_scaled gives you that.
 If you get lucky and log_p_scaled = log_p_true (likewise for q) then you'll compute the exact thing. This wouldn't be the case if you pulled out an extra scaling factor as proposed here?
 		</comment>
 		<comment id='2' author='fritzo' date='2017-11-08T16:48:35Z'>
 		&lt;denchmark-link:https://github.com/null-a&gt;@null-a&lt;/denchmark-link&gt;
  Yes I think you're right that this works as intended in . Thanks for explaining!
 I'm still a little confused about , which for an entirely local model &lt;denchmark-link:https://github.com/uber/pyro/blob/ece8ccc167695f81aa7ec1fed74a70c8fc15254b/pyro/infer/tracegraph_elbo.py#L276&gt;computes&lt;/denchmark-link&gt;
   independently for each datum in a minibatch. I would expect the  values to be scaled by , whereas in practice it is the  and  that are scaled, so that  is scaled . Do you think it's possible that  is correct, but  is incorrect?
 		</comment>
 		<comment id='3' author='fritzo' date='2017-11-08T17:24:22Z'>
 		i'm afraid &lt;denchmark-link:https://github.com/fritzo&gt;@fritzo&lt;/denchmark-link&gt;
  is right. there's an implicit rao-blackwellization that happens when you subsample which makes it a bit confusing. we'll fix it.
 		</comment>
 	</comments>
 </bug>
<commit id='b94f06a2257bf362208c28048544deb297aa5e24' author='Fritz Obermeyer' date='2017-11-08 15:39:36-08:00'>
 	<dmm_unit complexity='0.9722222222222222' interfacing='0.9722222222222222' size='0.3611111111111111'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pyro\infer\trace_elbo.py' new_name='pyro\infer\trace_elbo.py'>
 		<file_info nloc='116' complexity='34' token_count='956'></file_info>
 		<method name='loss_and_grads' parameters='self,model,guide,args,kwargs'>
 				<method_info nloc='40' complexity='14' token_count='307' nesting_level='1' start_line='123' end_line='180'></method_info>
 			<added_lines>138,139,140,141,142,144,145,147,151,152</added_lines>
 			<deleted_lines>138,139,140,141,142,144,146,150,151</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pyro\infer\tracegraph_elbo.py' new_name='pyro\infer\tracegraph_elbo.py'>
 		<file_info nloc='204' complexity='57' token_count='1517'></file_info>
 		<method name='loss_and_grads' parameters='self,model,guide,args,kwargs'>
 				<method_info nloc='137' complexity='46' token_count='1079' nesting_level='1' start_line='91' end_line='298'></method_info>
 			<added_lines>269,270,271,276,278</added_lines>
 			<deleted_lines>273,274,276,277</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='tests\infer\test_gradient.py'>
 		<file_info nloc='78' complexity='11' token_count='935'></file_info>
 	</modification>
 	<modification change_type='DELETE' old_name='tests\infer\test_gradient_step.py' new_name='None'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 	</modification>
 </commit>
</bug_data>
