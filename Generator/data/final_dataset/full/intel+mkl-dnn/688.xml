<bug_data>
<bug id='688' author='redhairdragon' open_date='2020-04-07T10:07:47Z' closed_time='2020-04-17T21:39:24Z'>
 	<summary>GRU Primitive Creation Error.</summary>
 	<description>
 Hi, I am creating a really simple GRU function. It keeps showing
 "terminate called after throwing an instance of 'dnnl::error'
 what():  could not create a descriptor for an LBR GRU forward propagation primitive
 Aborted (core dumped) "
 Can anyone help me? I know this can be stupid. But I just cannot figure out why.
 &lt;denchmark-code&gt;#include &lt;algorithm&gt;
 #include &lt;cmath&gt;
 #include &lt;cstring&gt;
 #include &lt;iostream&gt;
 #include &lt;numeric&gt;
 #include &lt;string&gt;
 #include &lt;vector&gt;
 #include "dnnl.hpp"
 
 using namespace dnnl;
 using std::cout;
 using std::endl;
 
 using tag = memory::format_tag;
 using dt = memory::data_type;
 using dim_t = dnnl::memory::dim;
 
 const memory::dim N = CHUNK_SIZE,  // batch size
     T = 1,                   // time steps
     C = 600,                         // channels
     G = 3,                         // gates (GRU has 6-&gt;3 U, 3 W)
     L = 1,                 // layers
     D = 1;                         // directions
 
 int main(int argc, char** argv) {
     // Create execution dnnl::engine.
     engine engine(engine::kind::cpu, 0);
     // Create dnnl::stream.
     stream engine_stream(engine);
 
     memory::dims src_dims = {T, N, 2 * C};
     memory::dims hx_dims = {L, D, N, C};
     memory::dims w_dims = {L, D, 2 * C, G, C};
     memory::dims u_dims = {L, D, C, G, C};
     memory::dims dst_dims = {T, N, C};
 
     auto src_layer_md = memory::desc(src_dims, dt::f32, tag::any);
     auto src_iter_md = memory::desc();
     // auto src_iter_md = memory::desc(hx_dims, dt::f32, tag::ldnc);
     auto gru_weights_layer_md = memory::desc(w_dims, dt::f32, tag::any);
     auto gru_weights_iter_md = memory::desc(u_dims, dt::f32, tag::any);
     auto bias_md = memory::desc();
     auto dst_layer_md = memory::desc(dst_dims, dt::f32, tag::any);
     auto dst_iter_md = memory::desc();
     // auto dst_iter_md = memory::desc(hx_dims, dt::f32, tag::any);
 
 
     auto gru_desc = gru_forward::desc(
         prop_kind::forward_training, 
         rnn_direction::unidirectional_left2right,
         src_layer_md, 
         src_iter_md, 
         gru_weights_layer_md, 
         gru_weights_iter_md,
         bias_md, 
         dst_layer_md, 
         dst_iter_md);
 
     // Create primitive descriptor.
     auto gru_pd = gru_forward::primitive_desc(gru_desc, engine);
 }
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='redhairdragon' date='2020-04-07T14:54:14Z'>
 		Hi &lt;denchmark-link:https://github.com/redhairdragon&gt;@redhairdragon&lt;/denchmark-link&gt;
 ,
 It seems we have a bug in the documentation: the bias is not expected to be zero. The fix is the following:
 -    auto bias_md = memory::desc();
 +    auto bias_md = memory::desc({L, D, G, C}, dt::f32, tag::ldgo);
 I tend to think this is a bug in the documentation and not in the implementation, as for training we assume that bias is a mandatory learnable parameter. However, from the usability perspective, I think nothing prevents us to make bias an optional parameter. &lt;denchmark-link:https://github.com/redhairdragon&gt;@redhairdragon&lt;/denchmark-link&gt;
 , could you please comment on what is your use-case? In the final application do you need bias or as in the example here you want it to be zero? Is it inference or training?
 		</comment>
 		<comment id='2' author='redhairdragon' date='2020-04-07T17:25:10Z'>
 		
 Hi @redhairdragon,
 It seems we have a bug in the documentation: the bias is not expected to be zero. The fix is the following:
 -    auto bias_md = memory::desc();
 +    auto bias_md = memory::desc({L, D, G, C}, dt::f32, tag::ldgo);
 I tend to think this is a bug in the documentation and not in the implementation, as for training we assume that bias is a mandatory learnable parameter. However, from the usability perspective, I think nothing prevents us to make bias an optional parameter. @redhairdragon, could you please comment on what is your use-case? In the final application do you need bias or as in the example here you want it to be zero? Is it inference or training?
 
 Thank you very much. I was just experimenting with the API and I saw bias is optional so I just ignored it.
 		</comment>
 		<comment id='3' author='redhairdragon' date='2020-04-07T17:33:51Z'>
 		Thanks for the update!
 I will keep this issue opened, as we have to at least fix the documentation.
 		</comment>
 	</comments>
 </bug>
<commit id='1e3f73a86f26dc019cd8fc41a96aeaae2d83ecff' author='Fomenko, Evarist M' date='2020-04-17 13:55:45-07:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='doc\primitives\rnn.md' new_name='doc\primitives\rnn.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>373,374,375,376,377,381,382,383</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
