<bug_data>
<bug id='1798' author='michael20at' open_date='2019-03-07T19:56:50Z' closed_time='2019-09-16T18:00:21Z'>
 	<summary>Unable to shuffle if the fields are not of same length</summary>
 	<description>
 Hm, tried ml-agents 0.7 with my new 2080ti (runs great on normal Tensorflow),
 updated Unity to the newest version 2018.3.7f1, training of 3D Balls example via cmd starts fine, but after a few episode (sometimes only a few, sometimes after 9) I get
 "Unable to shuffle if the fields are not of same length"!
 What could be the problem?
 	</description>
 	<comments>
 		<comment id='1' author='michael20at' date='2019-03-09T08:29:21Z'>
 		Hm, it kinda works if I increase the buffer size by ten in the config file, but I still don't know why?
 		</comment>
 		<comment id='2' author='michael20at' date='2019-04-02T01:41:53Z'>
 		This is kind of weird and unexpected, I don't see other people raising this issue, so I guess maybe delete and rerun and try to follow the basic guide step by step might help.
 		</comment>
 		<comment id='3' author='michael20at' date='2019-04-02T17:05:21Z'>
 		I also experienced this issue.
 I use ml-agents 0.6 on 1080ti.
          batch_size:     1536   beta:   0.006   buffer_size:    15360 epsilon:        0.17 gamma:  0.995 hidden_units:   512 lambd:  0.9 learning_rate:  0.00012 max_steps:      5.0e15 normalize:      True num_epoch:      4 num_layers:     4 time_horizon:   768 summary_freq:   2000 use_recurrent:  False use_curiosity:  False
 I saw this phenomenon when I took agent' s life time too long.
 So I guess agent should be Done() before buffer_size.
 		</comment>
 		<comment id='4' author='michael20at' date='2019-04-03T22:49:07Z'>
 		Are you using a Numpy version greater than 1.14.1 by any chance? Newer versions of Numpy are known to have similar issues.
 		</comment>
 		<comment id='5' author='michael20at' date='2019-04-23T06:08:52Z'>
 		I have the same issue.
 I use ml-agents 0.8.1 on 2070 card which only support CUDA 10.
 CUDA 10 needs TensorFlow &gt;=1.13.0.
 And Numpy 1.14.1 is not support TensorFlow&gt;=1.13.0
 		</comment>
 		<comment id='6' author='michael20at' date='2019-04-23T14:53:00Z'>
 		Same issue. Using Ubuntu Bionic, numpy v 1.16.2
 Trying to track it down, seems a numpy array (advantages) changes shape from (N,) to (1,N) when the ppy trainer.py subtracts the mean from it, about line 325.
 I hacked out a thing to bypass policy updates when that happens. Walker seems to be training now. My 5 yr old approves of the walker. It got up to a reward of 300 so far.
 		</comment>
 		<comment id='7' author='michael20at' date='2019-06-03T13:54:51Z'>
 		This doesnt seem to happen when using Numpy 1.14.5 and tensorflow 1.7 as mentioned in &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/blob/37d139af636e4a2351751fbf0f2fca5a9ed7457f/ml-agents/setup.py#L33&gt;setup.py&lt;/denchmark-link&gt;
 .
 I'm still not sure what the issue is but at least one doesn't get the bug.
 		</comment>
 		<comment id='8' author='michael20at' date='2019-07-12T07:12:59Z'>
 		It should be a compatibility problem with higher version of Tensorflow or Numpy.
 In my situation (py 3.7 &amp;&amp; tf 1.13 &amp;&amp; np 1.16), simply adding parameter  to all the calls of  in &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents/mlagents/trainers/buffer.py&gt;ml-agents/mlagents/trainers/buffer.py&lt;/denchmark-link&gt;
  could fix this problem.
 But I dont know if that would cause any potential problems.
 		</comment>
 		<comment id='9' author='michael20at' date='2019-07-12T17:03:06Z'>
 		The fields in the buffer should be all numerical, so I don't see an issue with adding the dtype. We'll definitely track this as we upgrade to newer versions of Numpy.
 		</comment>
 		<comment id='10' author='michael20at' date='2019-07-17T17:42:25Z'>
 		Hi &lt;denchmark-link:https://github.com/EfveZombie&gt;@EfveZombie&lt;/denchmark-link&gt;
 , we found the root cause of this issue and fixed it on the latest  branch. Let us know if it fixes your issue. Thanks!
 		</comment>
 		<comment id='11' author='michael20at' date='2019-09-16T18:00:21Z'>
 		Closing this issue as it was addressed in a recent release of ML-Agents.
 		</comment>
 	</comments>
 </bug>
<commit id='c025086818949707dc5cf47a47b10b86e3d42937' author='Ervin T' date='2019-07-17 10:37:34-07:00'>
 	<dmm_unit complexity='1.0' interfacing='0.0' size='0.25'></dmm_unit>
 	<modification change_type='MODIFY' old_name='ml-agents\mlagents\trainers\ppo\policy.py' new_name='ml-agents\mlagents\trainers\ppo\policy.py'>
 		<file_info nloc='210' complexity='29' token_count='1557'></file_info>
 		<method name='get_value_estimates' parameters='self,BrainInfo,int,bool'>
 				<method_info nloc='2' complexity='1' token_count='15' nesting_level='1' start_line='195' end_line='196'></method_info>
 			<added_lines>195,196</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='get_value_estimates' parameters='self,brain_info,idx'>
 				<method_info nloc='18' complexity='7' token_count='196' nesting_level='1' start_line='193' end_line='217'></method_info>
 			<added_lines>195,196,197,202,206,207,208,209,210,211,212</added_lines>
 			<deleted_lines>193,201,217</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>3,4,228,229</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='ml-agents\mlagents\trainers\ppo\trainer.py' new_name='ml-agents\mlagents\trainers\ppo\trainer.py'>
 		<file_info nloc='407' complexity='31' token_count='2649'></file_info>
 		<method name='get_gae' parameters='rewards,value_estimates,value_next,gamma,lambd'>
 				<method_info nloc='5' complexity='1' token_count='70' nesting_level='0' start_line='503' end_line='516'></method_info>
 			<added_lines>513</added_lines>
 			<deleted_lines>510</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>349,350,351,352,353,354</added_lines>
 			<deleted_lines>349,350,351</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='ml-agents\mlagents\trainers\tests\test_ppo.py' new_name='ml-agents\mlagents\trainers\tests\test_ppo.py'>
 		<file_info nloc='312' complexity='13' token_count='2091'></file_info>
 		<method name='test_ppo_get_value_estimates' parameters='mock_communicator,mock_launcher,dummy_config'>
 				<method_info nloc='24' complexity='3' token_count='181' nesting_level='0' start_line='71' end_line='97'></method_info>
 			<added_lines>71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>69,70,98,99</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
