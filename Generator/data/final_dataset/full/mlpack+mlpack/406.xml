<bug_data>
<bug id='406' author='rcurtin' open_date='2015-02-19T22:13:00Z' closed_time='2015-04-27T00:34:19Z'>
 	<summary>CF should avoid calculating full matrix when providing recommendations</summary>
 	<description>
 Right now, the first line of CF::GetRecommendations() reads
 &lt;denchmark-code&gt;rating = w * h
 &lt;/denchmark-code&gt;
 
 which has the issue that the RAM on the system must be equal to the number of items vs. the number of recommendations.  Then, we run tree-based kNN on the rating matrix, which is of high dimension, which will be slow:
 &lt;denchmark-code&gt;// Calculate the neighborhood of the queried users.
 // This should be a templatized option.
 neighbor::AllkNN a(rating, query);
 arma::mat resultingDistances; // Temporary storage.
 a.Search(numUsersForSimilarity, neighborhood, resultingDistances);
 &lt;/denchmark-code&gt;
 
 But this isn't necessary.  Note that what we are trying to do is find the most similar users (columns), but we have decomposed the input matrix X = W * H.  (H is the matrix that holds the user preferences, depending on how you look at it.)
 Now, some quick linear algebra gives us that X.col(i) = W * H.col(i).  But remember, we are looking for the nearest neighbors of X.col(i), so this is equivalent to the nearest neighbors of H.col(i).  Why aren't we searching for the nearest neighbors in the H matrix, then?
 A patch for this ticket should also include some information on the speedup obtained (in either a test program or the cf executable), and verification that the module provides the same results (perhaps through the already written tests).
 	</description>
 	<comments>
 		<comment id='1' author='rcurtin' date='2015-02-24T23:18:33Z'>
 		My linear algebra is wrong.  X.col(i) = W * H.col(i), but it is not true that d(X.col(i), X.col(j)) = d(H.col(i), H.col(j)) unless we make some assumptions about W, which we can't do.  Oops.  Self-assigning until I figure out how this can be done.
 		</comment>
 		<comment id='2' author='rcurtin' date='2015-03-25T22:11:02Z'>
 		d(X.col(i), X.col(j)) = d(W H.col(i), W H.col(j)).
 For the L2 distance (which is fine for now), we can show that this is the Mahalanobis distance with M^{-1} = W^T W.  Decompose M^{-1} = L L^T (Cholesky decomposition), and then multiply H by L^T to obtain H' (this takes O(r^2 n) time).  Then, once this is done,
 d(X.col(i), X.col(j)) = d(H'.col(i), H'.col(j))
 and each distance calculation takes only O(r) time.  We can use simple nearest neighbor search out of the box on H', then.
 		</comment>
 	</comments>
 </bug>
<commit id='fbdc8d44bcdb3a9f174b6f9a8839ced81b20f98f' author='ryan' date='2015-04-26 20:34:14-04:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='src\mlpack\methods\cf\cf_impl.hpp' new_name='src\mlpack\methods\cf\cf_impl.hpp'>
 		<file_info nloc='161' complexity='22' token_count='1230'></file_info>
 		<method name='mlpack::cf::CF&lt;FactorizerType&gt;::GetRecommendations' parameters='numRecs,recommendations,users'>
 				<method_info nloc='48' complexity='10' token_count='413' nesting_level='2' start_line='118' end_line='200'></method_info>
 			<added_lines>122,123,124,125,126,127,128,129,130,136,140,147,159,160,161,162,163,164,165,166,175</added_lines>
 			<deleted_lines>122,123,129,133,140,144,145,146,147,148,149,150,151,152,153,154,155,156,157,174</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
