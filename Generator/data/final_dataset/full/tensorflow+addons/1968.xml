<bug_data>
<bug id='1968' author='avnx' open_date='2020-07-03T18:10:08Z' closed_time='2020-07-11T05:25:47Z'>
 	<summary>Error with mixed precision</summary>
 	<description>
 System information
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS
 TensorFlow version and how it was installed (source or binary):  https://github.com/horovod/horovod/blob/master/Dockerfile.gpu (changed TENSORFLOW_VERSION=2.2.0 and NCCL_VERSION=2.5.6-1+cuda10.1)
 TensorFlow-Addons version and how it was installed (source or binary): nightly (0.11.0-dev)
 Python version: 3.6.9
 Is GPU used? (yes/no): yes
 
 Code doesn't work when applying mixed precision.
 Am I doing something wrong?
 Code to reproduce the issue
 &lt;denchmark-code&gt;import tensorflow as tf
 import tensorflow_addons as tfa
 from tensorflow.keras.mixed_precision import experimental as mixed_precision
 
 policy = mixed_precision.Policy('mixed_float16')
 mixed_precision.set_policy(policy)
 
 embedding_layer = tf.keras.layers.Embedding(37,
                                             37,
                                             name='embedding')
 
 rnn_cell = tf.keras.layers.LSTMCell(10)
 attention_mechanism = tfa.seq2seq.BahdanauAttention(units=10, name='BahdanauAtt')
 attention_wrapper = tfa.seq2seq.AttentionWrapper(rnn_cell,
                                                  attention_mechanism,
                                                  attention_layer_size=10,
                                                  name='Att_wrapper')
 mlp = tf.keras.layers.Dense(14, name='output_mlp')
 decoder_cell = tfa.seq2seq.BasicDecoder(attention_wrapper,
                                         output_layer=mlp,
                                         sampler=tfa.seq2seq.ScheduledEmbeddingTrainingSampler(0.1,
                                                                                               embedding_layer),
                                         name='train_decoder')
 
 @tf.function
 def step(inputs, encoder_outputs):
     
     attention_mechanism.setup_memory(encoder_outputs)
     decoder_initial_state = attention_wrapper.get_initial_state(batch_size=32,
                                                                 dtype=encoder_outputs.dtype)
 
     embeddings = embedding_layer(inputs)
 
     outputs = decoder_cell(embeddings,
                        initial_state=decoder_initial_state,
                        training=True)
     return outputs
 
 encoder_outputs = tf.random.uniform([32, 17, 256], dtype=tf.float16)
 inputs = tf.random.uniform([32, 27], maxval=37, dtype=tf.int32)
 step(inputs, encoder_outputs)
 
 
 &lt;/denchmark-code&gt;
 
 Other info / logs
 &lt;denchmark-code&gt;/usr/local/lib/python3.6/dist-packages/tensorflow_addons/seq2seq/decoder.py:163 call  *
         self,
     /usr/local/lib/python3.6/dist-packages/typeguard/__init__.py:262 wrapper  *
         retval = func(*args, **kwargs)
     /usr/local/lib/python3.6/dist-packages/tensorflow_addons/seq2seq/decoder.py:397 body  *
         (next_outputs, decoder_state, next_inputs, decoder_finished) = decoder.step(
     /usr/local/lib/python3.6/dist-packages/tensorflow_addons/seq2seq/basic_decoder.py:134 step  *
         cell_outputs, cell_state = self.cell(inputs, state, training=training)
     /usr/local/lib/python3.6/dist-packages/tensorflow_addons/seq2seq/attention_wrapper.py:2052 call  *
         attention, alignments, next_attention_state = self._attention_fn(
     /usr/local/lib/python3.6/dist-packages/tensorflow_addons/seq2seq/attention_wrapper.py:1595 _compute_attention  *
         context_ = tf.matmul(expanded_alignments, attention_mechanism.values)
     /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:180 wrapper  **
         return target(*args, **kwargs)
     /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:2946 matmul
         a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)
     /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:1526 batch_mat_mul_v2
         "BatchMatMulV2", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)
     /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:506 _apply_op_helper
         inferred_from[input_arg.type_attr]))
 
     TypeError: Input 'y' of 'BatchMatMulV2' Op has type float16 that does not match type float32 of argument 'x'.
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='avnx' date='2020-07-08T17:07:23Z'>
 		The first issue is that we are forcing  at &lt;denchmark-link:https://github.com/tensorflow/addons/blob/master/tensorflow_addons/seq2seq/attention_wrapper.py#L559-L560&gt;https://github.com/tensorflow/addons/blob/master/tensorflow_addons/seq2seq/attention_wrapper.py#L559-L560&lt;/denchmark-link&gt;
 .
 Then we have a similar issue at &lt;denchmark-link:https://github.com/tensorflow/addons/blob/master/tensorflow_addons/seq2seq/sampler.py#L394-L400&gt;https://github.com/tensorflow/addons/blob/master/tensorflow_addons/seq2seq/sampler.py#L394-L400&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='avnx' date='2020-07-08T17:48:51Z'>
 		Yes, we should fix this. The seq2seq layers were designed before the new dtype policy.
 While working on a fix, I find it a bit unexpected that tf.keras.layers.Embedding does not return float16 here.
 		</comment>
 	</comments>
 </bug>
<commit id='8bad1e45f4ddfb0f774955de93b43f45a8f8c960' author='Guillaume Klein' date='2020-07-10 22:25:46-07:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow_addons\conftest.py' new_name='tensorflow_addons\conftest.py'>
 		<file_info nloc='14' complexity='0' token_count='33'></file_info>
 		<modified_lines>
 			<added_lines>4</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow_addons\seq2seq\attention_wrapper.py' new_name='tensorflow_addons\seq2seq\attention_wrapper.py'>
 		<file_info nloc='1225' complexity='147' token_count='6262'></file_info>
 		<modified_lines>
 			<added_lines>1722,1728</added_lines>
 			<deleted_lines>113,114,559,560,738,739,1096,1097,1277,1278,1737</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow_addons\seq2seq\sampler.py' new_name='tensorflow_addons\seq2seq\sampler.py'>
 		<file_info nloc='551' complexity='85' token_count='3536'></file_info>
 		<method name='next_inputs' parameters='self,time,outputs,state,sample_ids'>
 				<method_info nloc='8' complexity='1' token_count='75' nesting_level='1' start_line='381' end_line='407'></method_info>
 			<added_lines>393,394,395</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='next_inputs.maybe_sample' parameters=''>
 				<method_info nloc='17' complexity='1' token_count='128' nesting_level='2' start_line='386' end_line='403'></method_info>
 			<added_lines>393,394,395</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow_addons\seq2seq\tests\attention_wrapper_test.py' new_name='tensorflow_addons\seq2seq\tests\attention_wrapper_test.py'>
 		<file_info nloc='772' complexity='38' token_count='6173'></file_info>
 		<method name='test_bahdanau_not_normalized' parameters=''>
 				<method_info nloc='37' complexity='1' token_count='286' nesting_level='0' start_line='542' end_line='579'></method_info>
 			<added_lines>544,549,555,556,559,561,563,568</added_lines>
 			<deleted_lines>544,550,551,554,556,558,563</deleted_lines>
 		</method>
 		<method name='test_luong_not_normalized' parameters=''>
 				<method_info nloc='30' complexity='1' token_count='240' nesting_level='0' start_line='618' end_line='649'></method_info>
 			<added_lines>620,625,631,632,635,637,639</added_lines>
 			<deleted_lines>618,624,625,628,630,632</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>401,404,405,406,433,541,617</added_lines>
 			<deleted_lines>403,430</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow_addons\seq2seq\tests\decoder_test.py' new_name='tensorflow_addons\seq2seq\tests\decoder_test.py'>
 		<file_info nloc='117' complexity='9' token_count='880'></file_info>
 		<method name='test_dynamic_decode_rnn_with_scheduled_embedding_training_sampler' parameters=''>
 				<method_info nloc='22' complexity='1' token_count='167' nesting_level='0' start_line='133' end_line='157'></method_info>
 			<added_lines>133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>130,131,132</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow_addons\utils\test_utils.py' new_name='tensorflow_addons\utils\test_utils.py'>
 		<file_info nloc='150' complexity='40' token_count='1035'></file_info>
 		<method name='run_with_mixed_precision_policy' parameters='request'>
 				<method_info nloc='6' complexity='3' token_count='52' nesting_level='0' start_line='106' end_line='111'></method_info>
 			<added_lines>106,107,108,109,110,111</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>24,25,105,112,113</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
