<bug_data>
<bug id='461' author='guillaumekln' open_date='2019-08-29T16:57:43Z' closed_time='2019-09-10T01:02:54Z'>
 	<summary>Setting a custom attention_layer fails with AttentionMechanism without memory</summary>
 	<description>
 System information
 
 Have I written custom code: Yes
 OS Platform and Distribution: Ubuntu 16.04
 TensorFlow installed from: binary
 TensorFlow version: 2.0.0rc0
 TensorFlow Addons installed from: PyPi
 TensorFlow Addons version: 0.5.0.dev20190829
 Python version and type: 3.6
 
 Describe the bug
 When creating an AttentionMechanism without a memory and then creating an AttentionWrapper with a custom attention_layer, an error is raised.
 Describe the expected behavior
 No error should be raised.
 Code to reproduce the issue
 import tensorflow as tf
 import tensorflow_addons as tfa
 
 units = 32
 attention_mechanism = tfa.seq2seq.LuongAttention(units)
 cell = tf.keras.layers.LSTMCell(units)
 attention_layer = tf.keras.layers.Dense(
     units, use_bias=False, activation=tf.math.tanh)
 attention_wrapper = tfa.seq2seq.AttentionWrapper(
     cell, attention_mechanism, attention_layer=attention_layer)
 Other info / logs
 &lt;denchmark-code&gt;  File "/lib/python3.6/site-packages/tensorflow_addons/seq2seq/attention_wrapper.py", line 1698, in &lt;genexpr&gt;
     ])[-1]) for layer, mechanism in zip(
 AttributeError: 'NoneType' object has no attribute 'shape'
 &lt;/denchmark-code&gt;
 
 The code tries to compute the attention layer output shape and for that, it uses attention_mechanism.values which is None at the time the AttentionWrapper constructor is called.
 	</description>
 	<comments>
 		<comment id='1' author='guillaumekln' date='2019-08-30T06:03:43Z'>
 		&lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
  Qianli, could you take a look? Thanks.
 		</comment>
 		<comment id='2' author='guillaumekln' date='2019-08-30T12:24:16Z'>
 		This one is a bit tricky. A possible fix is to lazily compute . However, in the usage introduced by &lt;denchmark-link:https://github.com/tensorflow/addons/pull/354&gt;#354&lt;/denchmark-link&gt;
 , it will not work until  is called. We could just raise an exception in that case saying that the user should call this method first.
 		</comment>
 	</comments>
 </bug>
<commit id='6633c43afb6bdd66b43933a2221847b4d273bc87' author='Guillaume Klein' date='2019-09-09 21:02:53-04:00'>
 	<dmm_unit complexity='1.0' interfacing='0.9782608695652174' size='0.2391304347826087'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow_addons\seq2seq\attention_wrapper.py' new_name='tensorflow_addons\seq2seq\attention_wrapper.py'>
 		<file_info nloc='1088' complexity='137' token_count='6371'></file_info>
 		<method name='state_size' parameters='self'>
 				<method_info nloc='12' complexity='5' token_count='93' nesting_level='1' start_line='1802' end_line='1819'></method_info>
 			<added_lines>1812</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_batch_size_checks' parameters='self,batch_size,error_message'>
 				<method_info nloc='9' complexity='2' token_count='41' nesting_level='1' start_line='1744' end_line='1752'></method_info>
 			<added_lines>1745</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_attention_mechanisms_checks' parameters='self'>
 				<method_info nloc='8' complexity='3' token_count='27' nesting_level='1' start_line='1735' end_line='1742'></method_info>
 			<added_lines>1735,1736,1737,1738,1739,1740,1741,1742</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='get_initial_state' parameters='self,inputs,batch_size,dtype'>
 				<method_info nloc='44' complexity='7' token_count='267' nesting_level='1' start_line='1821' end_line='1884'></method_info>
 			<added_lines>1871</added_lines>
 			<deleted_lines>1844</deleted_lines>
 		</method>
 		<method name='output_size' parameters='self'>
 				<method_info nloc='5' complexity='2' token_count='24' nesting_level='1' start_line='1795' end_line='1799'></method_info>
 			<added_lines>1797</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_get_attention_layer_size' parameters='self'>
 				<method_info nloc='16' complexity='5' token_count='102' nesting_level='1' start_line='1754' end_line='1772'></method_info>
 			<added_lines>1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772</added_lines>
 			<deleted_lines>1770</deleted_lines>
 		</method>
 		<method name='memory_initialized' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='10' nesting_level='1' start_line='142' end_line='146'></method_info>
 			<added_lines>142,143,144,145,146</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>141,147,1705,1743,1773</added_lines>
 			<deleted_lines>1683,1693,1694,1695,1696,1697,1698,1699,1702,1703,1704,1785</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow_addons\seq2seq\attention_wrapper_test.py' new_name='tensorflow_addons\seq2seq\attention_wrapper_test.py'>
 		<file_info nloc='733' complexity='31' token_count='5996'></file_info>
 		<method name='testCustomAttentionLayer' parameters='self'>
 				<method_info nloc='19' complexity='1' token_count='199' nesting_level='1' start_line='251' end_line='271'></method_info>
 			<added_lines>251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>272</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
