<bug_data>
<bug id='1194' author='georgesterpu' open_date='2020-03-02T00:26:21Z' closed_time='2020-03-06T09:24:12Z'>
 	<summary>Error when using a mask in RNN + AttentionWrapper cell</summary>
 	<description>
 System information
 
 OS Platform and Distribution:  manjaro linux testing
 TensorFlow version: pypi tf-nightly 2.2.0.dev20200228
 TensorFlow-Addons version: pypi tfa-nightly 0.9.0.dev20200228
 Python version: 3.7.6
 Is GPU used?: no
 
 Describe the bug
 A ValueError is raised when using a mask in a RNN layer taking a RNN cell wrapped with an AttentionWrapper.
 Code to reproduce the issue
 import tensorflow_addons as tfa
 import tensorflow as tf
 
 bs = 4
 cell = tf.keras.layers.LSTMCell(3)
 mechanism = tfa.seq2seq.LuongAttention(units=3)
 cell = tfa.seq2seq.AttentionWrapper(cell, mechanism)
 layer = tf.keras.layers.RNN(cell)
 
 
 @tf.function
 def test(masked=False):
     seq_len = tf.random.uniform(shape=(), minval=2, maxval=10, dtype=tf.int32)
     data = tf.ones(shape=(bs, seq_len, 3))
     mechanism.setup_memory(data)
 
     if masked:
         mask = tf.sequence_mask(lengths=bs * [seq_len])
     else:
         mask = None
 
     return layer(data, mask=mask)
 
 
 test(masked=False); print('Test 1 passed')
 test(masked=True); print('Test 2 passed')
 Other info / logs
 Test 1 passed
 Traceback (most recent call last):
   File "/run/media/zenbook/work/phd/work/__/pb1.py", line 26, in &lt;module&gt;
     test(masked=True); print('Test 2 passed')
   File "/home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 580, in __call__
     result = self._call(*args, **kwds)
   File "/home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 611, in _call
     return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
   File "/home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 2419, in __call__
     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
   File "/home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 2777, in _maybe_define_function
     graph_function = self._create_graph_function(args, kwargs)
   File "/home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 2667, in _create_graph_function
     capture_by_value=self._capture_by_value),
   File "/home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py", line 981, in func_graph_from_py_func
     func_outputs = python_func(*func_args, **func_kwargs)
   File "/home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 441, in wrapped_fn
     return weak_wrapped_fn().__wrapped__(*args, **kwds)
   File "/home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py", line 968, in wrapper
     raise e.ag_error_metadata.to_exception(e)
 ValueError: in user code:
 
     /run/media/zenbook/work/phd/work/__/pb1.py:22 test  *
         return layer(data, mask=mask)
     /home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py:652 __call__  **
         return super(RNN, self).__call__(inputs, **kwargs)
     /home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:926 __call__
         outputs = call_fn(cast_inputs, *args, **kwargs)
     /home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py:793 call
         zero_output_for_mask=self.zero_output_for_mask)
     /home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:4205 rnn
         **while_loop_kwargs)
     /home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:2688 while_loop
         back_prop=back_prop)
     /home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/ops/while_v2.py:229 while_loop
         len_orig_loop_vars], expand_composites=True))
     /home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/ops/while_v2.py:1143 _check_shapes_compat
         "specify a less-specific shape." % (input_t.name, shape, t.shape))
 
     ValueError: Input tensor 'rnn/AttentionWrapperZeroState/zeros_3:0' enters the loop with shape (), but has shape (4, 1) after one iteration. To allow the shape to vary across iterations, use the `shape_invariants` argument of tf.while_loop to specify a less-specific shape.
 
 
 Process finished with exit code 1
 	</description>
 	<comments>
 		<comment id='1' author='georgesterpu' date='2020-03-02T14:19:38Z'>
 		This is because the Keras RNN assumes that each tensor in the state has a batch dimension. However, the time field in AttentionWrapperState is just a scalar tensor.
 I think we could work around not having this time field. &lt;denchmark-link:https://github.com/georgesterpu&gt;@georgesterpu&lt;/denchmark-link&gt;
  Do you want to look into that?
 		</comment>
 		<comment id='2' author='georgesterpu' date='2020-03-02T15:36:18Z'>
 		Thanks, &lt;denchmark-link:https://github.com/guillaumekln&gt;@guillaumekln&lt;/denchmark-link&gt;
 .
 Assuming that the problem comes from the  field -  uses it to write the current alignment slice to a  when the  flag is True. I can see several possibilities, such as:
 
 pass the current loop index from outside (the time parameter of the _step function declared inside keras.backend.rnn ?
 manage the loop counter as a member of the AttentionWrapper class
 extend the TensorArray class to support an append operation, since AttentionWrapper never leapfrogs over indices and only writes sequentially anyway.
 
 Unfortunately, as a final year student, I lack the throughput to carry on with a more detailed investigation - this is why I created the issue in the first place ☺️
 		</comment>
 		<comment id='3' author='georgesterpu' date='2020-03-02T16:50:37Z'>
 		Thanks looking into workarounds. Actually, as alignment_history has a dynamic size and we only append to it, we can do something like:
 ta = ta.write(ta.size(), elem)
 
 Unfortunately, as a final year student, I lack the throughput to carry on with a more detailed investigation - this is why I created the issue in the first place relaxed
 
 No worries! Thank you for finding these issues. I can make a PR as I already have a fix locally.
 		</comment>
 		<comment id='4' author='georgesterpu' date='2020-03-02T17:30:36Z'>
 		Also, any chance that
 4) the shape_invariants parameter of the while_loop in keras.backend.rnn could be configured conveniently to work with an AttentionWrapperState ?
 		</comment>
 		<comment id='5' author='georgesterpu' date='2020-03-02T17:40:12Z'>
 		If you dive into the K.rnn code you'll find that it applies a tf.where on each state tensor, including our time scalar. This is the issue. See the relevant code here:
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/keras/backend.py#L4070&gt;https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/keras/backend.py#L4070&lt;/denchmark-link&gt;
 
 compared to the Addons decoder code that ignores scalar tensors:
 
 
 
 addons/tensorflow_addons/seq2seq/decoder.py
 
 
          Line 455
       in
       fcaa672
 
 
 
 
 
 
  # TensorArrays and scalar states get passed through. 
 
 
 
 
 
 		</comment>
 		<comment id='6' author='georgesterpu' date='2020-03-06T17:26:51Z'>
 		&lt;denchmark-link:https://github.com/guillaumekln&gt;@guillaumekln&lt;/denchmark-link&gt;
  thanks a lot !
 There is a corner case where this doesn't work: when setting  in the  constructor. . Crashing under both eager and graph modes.
 		</comment>
 		<comment id='7' author='georgesterpu' date='2020-03-06T17:28:09Z'>
 		import tensorflow_addons as tfa
 import tensorflow as tf
 import os
 
 os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
 
 cell = tf.keras.layers.LSTMCell(3)
 mechanism = tfa.seq2seq.LuongAttention(units=3)
 cell = tfa.seq2seq.AttentionWrapper(cell, mechanism, alignment_history=True)
 layer = tf.keras.layers.RNN(cell)
 
 
 # @tf.function
 def mytest():
     var_len = tf.random.uniform(shape=(), minval=2, maxval=10, dtype=tf.int32)
     lengths = tf.random.uniform(shape=(var_len,), minval=1, maxval=var_len + 1, dtype=tf.int32)
     data = tf.ones(shape=(var_len, var_len, 3))
     mask = tf.sequence_mask(lengths, maxlen=var_len)
     mechanism.setup_memory(data, lengths)
 
     return layer(data, mask=mask)
 
 mytest()
 		</comment>
 		<comment id='8' author='georgesterpu' date='2020-03-06T17:43:17Z'>
 		It seems the Keras RNN backend does not accept TensorArray in the inputs. &lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
  Is there any plans for Keras to support TensorArray in cell states?
 		</comment>
 	</comments>
 </bug>
<commit id='8305b1b16bfbaf1ff6009a96487117f916fabfe5' author='Guillaume Klein' date='2020-03-06 10:24:11+01:00'>
 	<dmm_unit complexity='0.0' interfacing='0.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow_addons\seq2seq\attention_wrapper.py' new_name='tensorflow_addons\seq2seq\attention_wrapper.py'>
 		<file_info nloc='1275' complexity='147' token_count='6478'></file_info>
 		<method name='state_size' parameters='self'>
 				<method_info nloc='16' complexity='5' token_count='94' nesting_level='1' start_line='1909' end_line='1930'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>1918</deleted_lines>
 		</method>
 		<method name='call' parameters='self,inputs,state,kwargs'>
 				<method_info nloc='68' complexity='9' token_count='357' nesting_level='1' start_line='2001' end_line='2109'></method_info>
 			<added_lines>2085,2086,2087</added_lines>
 			<deleted_lines>2089,2101</deleted_lines>
 		</method>
 		<method name='get_initial_state' parameters='self,inputs,batch_size,dtype'>
 				<method_info nloc='52' complexity='7' token_count='274' nesting_level='1' start_line='1932' end_line='2003'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>1986</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>1392,1406</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow_addons\seq2seq\attention_wrapper_test.py' new_name='tensorflow_addons\seq2seq\attention_wrapper_test.py'>
 		<file_info nloc='870' complexity='37' token_count='6600'></file_info>
 		<method name='testLuongScaled' parameters='self'>
 				<method_info nloc='35' complexity='1' token_count='260' nesting_level='1' start_line='743' end_line='779'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>763</deleted_lines>
 		</method>
 		<method name='testLuongMonotonicNotNormalized' parameters='self'>
 				<method_info nloc='38' complexity='1' token_count='284' nesting_level='1' start_line='903' end_line='942'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>922</deleted_lines>
 		</method>
 		<method name='test_attention_state_with_variable_length_input' parameters='self'>
 				<method_info nloc='13' complexity='1' token_count='151' nesting_level='1' start_line='993' end_line='1008'></method_info>
 			<added_lines>999,1000,1001,1003,1008</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='testBahdanauNormalized' parameters='self'>
 				<method_info nloc='33' complexity='1' token_count='263' nesting_level='1' start_line='671' end_line='705'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>689</deleted_lines>
 		</method>
 		<method name='testBahdanauNotNormalized' parameters='self'>
 				<method_info nloc='42' complexity='1' token_count='312' nesting_level='1' start_line='627' end_line='669'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>648</deleted_lines>
 		</method>
 		<method name='testBahdanauMonotonicNotNormalized' parameters='self'>
 				<method_info nloc='40' complexity='1' token_count='296' nesting_level='1' start_line='818' end_line='859'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>838</deleted_lines>
 		</method>
 		<method name='testLuongNotNormalized' parameters='self'>
 				<method_info nloc='33' complexity='1' token_count='249' nesting_level='1' start_line='707' end_line='741'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>726</deleted_lines>
 		</method>
 		<method name='testBahdanauMonotonicNormalized' parameters='self'>
 				<method_info nloc='40' complexity='1' token_count='299' nesting_level='1' start_line='861' end_line='901'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>880</deleted_lines>
 		</method>
 		<method name='testNotUseAttentionLayer' parameters='self'>
 				<method_info nloc='34' complexity='1' token_count='262' nesting_level='1' start_line='781' end_line='816'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>799</deleted_lines>
 		</method>
 		<method name='testLuongMonotonicScaled' parameters='self'>
 				<method_info nloc='40' complexity='1' token_count='295' nesting_level='1' start_line='944' end_line='985'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>964</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>479</added_lines>
 			<deleted_lines>479,1013</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
