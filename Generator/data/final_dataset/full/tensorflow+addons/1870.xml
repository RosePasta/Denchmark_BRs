<bug_data>
<bug id='1870' author='avnx' open_date='2020-05-21T19:11:52Z' closed_time='2020-05-24T05:31:26Z'>
 	<summary>Error in types description of tfa.seq2seq.AttentionWrapper</summary>
 	<description>
 System information
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LT
 TensorFlow version and how it was installed (source or binary): 2.2.0
 TensorFlow-Addons version and how it was installed (source or binary): 0.10.0
 Python version: 3.6.9
 Is GPU used? (yes/no): yes
 
 As I noticed you added type description to classes.
 But I found that at least in my case there is a conflict of types in description with real mission of arguments.
 According to docs attention_mechanism and attention_layer_size in tfa.seq2seq.AttentionWrapper could be lists (for MultiHead attention case) or single instances. But in types you specified that attention_mechanism and attention_layer_size must be tf.keras.layers.Layer and  Optional[FloatTensorLike] respectively.
 &lt;denchmark-link:https://github.com/tensorflow/addons/blob/v0.9.1/tensorflow_addons/seq2seq/attention_wrapper.py#L1621&gt;https://github.com/tensorflow/addons/blob/v0.9.1/tensorflow_addons/seq2seq/attention_wrapper.py#L1621&lt;/denchmark-link&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='avnx' date='2020-05-22T07:58:36Z'>
 		Thanks for reporting. The description is correct but not the types annotation. The PR referenced above should fix this.
 		</comment>
 	</comments>
 </bug>
<commit id='64ade2170c404c7c6962bbe5e84634fee2095b29' author='Guillaume Klein' date='2020-05-23 22:31:25-07:00'>
 	<dmm_unit complexity='1.0' interfacing='0.8181818181818182' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow_addons\seq2seq\attention_wrapper.py' new_name='tensorflow_addons\seq2seq\attention_wrapper.py'>
 		<file_info nloc='1281' complexity='147' token_count='6539'></file_info>
 		<method name='__init__' parameters='self,Layer,Layer,None,bool,None,bool,None,None,None,None,kwargs'>
 				<method_info nloc='13' complexity='1' token_count='98' nesting_level='1' start_line='1618' end_line='1630'></method_info>
 			<added_lines>1622,1623,1629,1630</added_lines>
 			<deleted_lines>1621,1622,1628</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,Layer,Layer,Number,None,bool,None,bool,None,None,Layer,None,None,kwargs'>
 				<method_info nloc='15' complexity='1' token_count='134' nesting_level='1' start_line='1619' end_line='1633'></method_info>
 			<added_lines>1622,1623,1629,1630,1631</added_lines>
 			<deleted_lines>1621,1622,1628</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>31,35,1707</added_lines>
 			<deleted_lines>34,1704</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow_addons\seq2seq\tests\attention_wrapper_test.py' new_name='tensorflow_addons\seq2seq\tests\attention_wrapper_test.py'>
 		<file_info nloc='775' complexity='38' token_count='6185'></file_info>
 		<method name='test_attention_wrapper_with_multiple_attention_mechanisms' parameters=''>
 				<method_info nloc='9' complexity='1' token_count='88' nesting_level='0' start_line='937' end_line='946'></method_info>
 			<added_lines>937,938,939,940,941,942,943,944,945,946</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>935,936</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
