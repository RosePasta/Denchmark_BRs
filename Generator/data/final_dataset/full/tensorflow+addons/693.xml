<bug_data>
<bug id='693' author='danijar' open_date='2019-11-12T09:50:49Z' closed_time='2019-12-26T18:10:20Z'>
 	<summary>Lost gradient after clipping in focal loss?</summary>
 	<description>
 
 
 
 addons/tensorflow_addons/losses/focal_loss.py
 
 
          Line 132
       in
       776b751
 
 
 
 
 
 
  y_pred = K.clip(y_pred, K.epsilon(), 1. - K.epsilon()) 
 
 
 
 
 
 	</description>
 	<comments>
 		<comment id='1' author='danijar' date='2019-11-12T17:06:39Z'>
 		cc &lt;denchmark-link:https://github.com/AakashKumarNain&gt;@AakashKumarNain&lt;/denchmark-link&gt;
  code owner
 		</comment>
 		<comment id='2' author='danijar' date='2019-11-12T17:19:08Z'>
 		&lt;denchmark-link:https://github.com/danijar&gt;@danijar&lt;/denchmark-link&gt;
  any sample code to reproduce?
 If it is the case though, then I suspect that epsilon added to y_pred in this line
 p_t = (y_true * y_pred) + ((1 - y_true) * (1 - y_pred))
 should solve the issue. Need to test it though.
 		</comment>
 		<comment id='3' author='danijar' date='2019-11-14T05:15:22Z'>
 		I just read through the code and wanted to highlight a potential problem. Looking at K.clip, it doesn't seem to provide a gradient for values that have been clipped. In this is indeed the case, a fix would be
 clipped = K.clip(y_pred, K.epsilon(), 1. - K.epsilon())
 y_pred = y_pred - tf.stop_gradient(y_pred) + tf.stop_gradient(clipped)
 		</comment>
 		<comment id='4' author='danijar' date='2019-11-14T06:51:32Z'>
 		Thank you &lt;denchmark-link:https://github.com/danijar&gt;@danijar&lt;/denchmark-link&gt;
  for looking into this and providing a fix. I will look into it once again and we will make the necessary code changes. Thank you again
 &lt;denchmark-link:https://github.com/WindQAQ&gt;@WindQAQ&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/facaiy&gt;@facaiy&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/seanpmorgan&gt;@seanpmorgan&lt;/denchmark-link&gt;
  sounds good?
 		</comment>
 		<comment id='5' author='danijar' date='2019-11-20T19:14:09Z'>
 		&lt;denchmark-link:https://github.com/AakashKumarNain&gt;@AakashKumarNain&lt;/denchmark-link&gt;
  Sorry for the late response. I'm fine with this. Maybe provide a testcase to verify the patch is needed!
 		</comment>
 		<comment id='6' author='danijar' date='2019-12-10T19:10:26Z'>
 		Sorry for the delay. I was on vacation. I will check and if needed, fix this during this weekend
 		</comment>
 		<comment id='7' author='danijar' date='2019-12-12T10:49:23Z'>
 		K.clip is just clip by value. It doesn't seem to affect the gradients, hence no fix is needed IMO. Though looking for any additional inputs
 		</comment>
 		<comment id='8' author='danijar' date='2019-12-12T18:12:29Z'>
 		What do you mean? Clip by value is not differentiable.
 		</comment>
 		<comment id='9' author='danijar' date='2019-12-12T20:26:01Z'>
 		The same thing is being used here in  calculations.
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/backend.py#L4558-L4588&gt;https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/backend.py#L4558-L4588&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='10' author='danijar' date='2019-12-12T21:38:59Z'>
 		&lt;denchmark-link:https://github.com/danijar&gt;@danijar&lt;/denchmark-link&gt;
  I understand the confusion.
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/f7f4049ded8ad2bfbaecde1ff2d42460cc9fa762/tensorflow/python/ops/clip_ops.py#L38&gt;tf.clip_by_value&lt;/denchmark-link&gt;
  uses  and  currently (ignoring the TODO).
 Both  and  have their gradients defined &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/606a542c81df033d493d25e32b9f175b947fcbbb/tensorflow/core/ops/math_grad.cc#L617&gt;here&lt;/denchmark-link&gt;
 . So yes, as &lt;denchmark-link:https://github.com/AakashKumarNain&gt;@AakashKumarNain&lt;/denchmark-link&gt;
  mentioned, using  shouldn't be a problem for differentiation.
 As for the gradient, it'll be 0 outside the range of min_value and max_value.
 		</comment>
 		<comment id='11' author='danijar' date='2019-12-15T18:40:34Z'>
 		Exactly. Is it supposed to be 0 outside of the range?
 		</comment>
 		<comment id='12' author='danijar' date='2019-12-15T20:24:29Z'>
 		&lt;denchmark-link:https://github.com/danijar&gt;@danijar&lt;/denchmark-link&gt;
  My bad, you're right. A clip is used in BCE before taking a  for numerical stability. That won't be needed for focal loss.
 Funnily enough in the tf.keras implementation of  BCE, that does both a clip followed by an output + eps. I think the addition of eps after the clip won't be needed.
 		</comment>
 		<comment id='13' author='danijar' date='2019-12-16T04:12:18Z'>
 		My main point was that the gradient should probably be patched back onto the clipped tensor. Anyway, I'm not a user of the module.
 		</comment>
 		<comment id='14' author='danijar' date='2019-12-17T16:20:18Z'>
 		We will patch it.
 		</comment>
 	</comments>
 </bug>
<commit id='732b18fa55c299e1abad8ba7ac281e55ff5f7b3e' author='Aakash Kumar Nain' date='2019-12-26 23:28:04+05:18'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow_addons\losses\focal_loss.py' new_name='tensorflow_addons\losses\focal_loss.py'>
 		<file_info nloc='105' complexity='4' token_count='474'></file_info>
 		<modified_lines>
 			<added_lines>45,46,47,113,115,123,124,125,126,127,129,130,134,136,138,139,140,144,148,151</added_lines>
 			<deleted_lines>45,46,47,113,115,123,125,126,130,132,134,135,136,140,144,147,148</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow_addons\losses\focal_loss_test.py' new_name='tensorflow_addons\losses\focal_loss_test.py'>
 		<file_info nloc='72' complexity='5' token_count='661'></file_info>
 		<method name='test_without_logits' parameters='self'>
 				<method_info nloc='20' complexity='1' token_count='213' nesting_level='1' start_line='84' end_line='111'></method_info>
 			<added_lines>96,97,110</added_lines>
 			<deleted_lines>94,107</deleted_lines>
 		</method>
 		<method name='test_with_logits' parameters='self'>
 				<method_info nloc='26' complexity='1' token_count='255' nesting_level='1' start_line='47' end_line='81'></method_info>
 			<added_lines>62,63,64,65,80</added_lines>
 			<deleted_lines>62,63,78</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
