<bug_data>
<bug id='2094' author='BinyanHu' open_date='2020-08-15T14:10:33Z' closed_time='2020-09-01T17:48:03Z'>
 	<summary>Weights of Inner Optimizers Not Saved</summary>
 	<description>
 System information
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 &amp; Windows 10
 TensorFlow version and how it was installed (source or binary): 2.3.0 from source
 TensorFlow-Addons version and how it was installed (source or binary): 0.11.1 from source
 Python version: 3.7
 Is GPU used? (yes/no): yes
 
 Describe the bug
 Resume a training process needs the restoration of the optimizer states to continue training RIGHT from the previous state without any loss of accuracy. Currently, the keras interface of saving model keras.Model.save_weights  checkpoints both the network parameters and the optimizer weights. However, when an optimizer is wrapped inside another, its weights can not be saved by this mean.
 For example, when I was trying to use the Ranger optimizer, which is constructed by wrapping RAdam with Lookahead:
 &lt;denchmark-code&gt;optimizer = tfa.optimizers.Lookahead(
     tfa.optimizers.RectifiedAdam()
 )
 &lt;/denchmark-code&gt;
 
 I noticed a performance drop on resuming training. I found that the weights of the inner RAdam were not saved into the checkpoint. (I checked the .index file in the checkpoint folder and there are no variable names like "m" and "v", only "slow", which is the weights of Lookahead). Therefore, after loading the weights from file and restart fitting, the weights of RAdam are randomly reinitialized. This could because the weights of the inner optimizer are not automatically tracked.
 Experiments
 I trained the two LeNets on the FashionMNIST dataset. All the configurations are the same except for the optimizers. Both training are interrupted in the middle and then resumed.
 &lt;denchmark-link:https://user-images.githubusercontent.com/26186061/90313678-f46e6d00-df40-11ea-9000-3fa8e429818d.png&gt;&lt;/denchmark-link&gt;
 
 
 Note the "bump" of the Ranger curve caused by the reinitialization of RAdam weights. Apparently, the weights of the inner optimizer are not correctly saved.
 	</description>
 	<comments>
 		<comment id='1' author='BinyanHu' date='2020-08-15T14:56:56Z'>
 		Can you prepare a minimal PR with a new test to cover your case?
 		</comment>
 		<comment id='2' author='BinyanHu' date='2020-08-15T15:03:00Z'>
 		So that we could check if It is similar to &lt;denchmark-link:https://github.com/tensorflow/addons/issues/1911&gt;#1911&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='3' author='BinyanHu' date='2020-08-15T16:40:28Z'>
 		
 So that we could check if It is similar to #1911
 
 Our issues are similar. But the real problem is on the missing weights of the inner RAdam optimizer.
 First, I reran my program with status.assert_consumed(), and the errors are as follows:
 &lt;denchmark-code&gt;AssertionError:
 Unresolved object in checkpoint (root).optimizer.iter: attributes {
   name: "VARIABLE_VALUE"
   full_name: "iter"
   checkpoint_key: "optimizer/iter/.ATTRIBUTES/VARIABLE_VALUE"
 }
 &lt;/denchmark-code&gt;
 
 Same as &lt;denchmark-link:https://github.com/tensorflow/addons/issues/1911&gt;#1911&lt;/denchmark-link&gt;
 . This is because the variable  is not yet created by the time when we load the weights. If we do  after fitting the model, the error goes away. The following warnings of all "slow" of the network parameters are not used in that issue are caused merely by non-training mode does not require loading the optimizer states, which is not a problem. In all, calling  right after loading weights does not reveal the problem.
 Second, learning rate warmup could help RAdam to re-accumulate the mean and variance statistics with small steps rather than "messing up" the network weights in the first few steps on resuming. This can, to some extent, alleviate the missing of the RAdam weights, but is definitely not the correct solution.
 Plus, I just checked the sizes of the checkpoint files: Ranger 3381kb and RAdam 5070kb. With an extra slot "slow", the size of the Ranger checkpoint should not be smaller, indicating that the weights of RAdam are missing.
 I think the reason is evident here. If a PR is still needed, how should the test be conducted? Would saving and loading a model with a Lookahead-wrapped optimizer with slots be enough to demonstrate the problem?
 		</comment>
 		<comment id='4' author='BinyanHu' date='2020-08-15T17:10:18Z'>
 		Lookahead test has no serializzation test currently.
 So I think that you can add a small one and let It to fail in &lt;denchmark-link:https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/tests/lookahead_test.py&gt;https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/tests/lookahead_test.py&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='5' author='BinyanHu' date='2020-08-15T17:17:08Z'>
 		Check if some of the original author tests could be useful &lt;denchmark-link:https://github.com/CyberZHG/keras-lookahead/blob/master/tests/test_optimizers.py&gt;https://github.com/CyberZHG/keras-lookahead/blob/master/tests/test_optimizers.py&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='6' author='BinyanHu' date='2020-08-15T17:18:50Z'>
 		/cc &lt;denchmark-link:https://github.com/CyberZHG&gt;@CyberZHG&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='7' author='BinyanHu' date='2020-08-15T17:23:11Z'>
 		Also check that you are recovering custom objects on load e.g. custom_objects={ 'RAdam': RAdam, 'Lookahead': Lookahead, })
 		</comment>
 		<comment id='8' author='BinyanHu' date='2020-08-16T15:51:41Z'>
 		Hi &lt;denchmark-link:https://github.com/BinyanHu&gt;@BinyanHu&lt;/denchmark-link&gt;
 , thanks for investigating this. Can you provide the minimal code snippet to reproduce the issue, e.g. the way you save the model? Thank you!
 		</comment>
 		<comment id='9' author='BinyanHu' date='2020-08-16T16:09:20Z'>
 		&lt;denchmark-code&gt;AssertionError:
 Unresolved object in checkpoint (root).optimizer.iter: attributes {
   name: "VARIABLE_VALUE"
   full_name: "iter"
   checkpoint_key: "optimizer/iter/.ATTRIBUTES/VARIABLE_VALUE"
 }
 
 &lt;/denchmark-code&gt;
 
 I think this is because of the fact that the value you pass to your optimizer is float, it gives this warning. You can use tf.Variable() for that.
 On the other hand, I feel this is the real issue here
 &lt;denchmark-code&gt;Plus, I just checked the sizes of the checkpoint files: Ranger 3381kb and RAdam 5070kb. With an extra slot "slow", the size of the Ranger checkpoint should not be smaller, indicating that the weights of RAdam are missing.
 &lt;/denchmark-code&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='2bf57f8383cd84aa145905aa01a30f59824feb2b' author='bhack' date='2020-09-01 10:48:02-07:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow_addons\optimizers\average_wrapper.py' new_name='tensorflow_addons\optimizers\average_wrapper.py'>
 		<file_info nloc='99' complexity='21' token_count='673'></file_info>
 		<modified_lines>
 			<added_lines>49</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow_addons\optimizers\lookahead.py' new_name='tensorflow_addons\optimizers\lookahead.py'>
 		<file_info nloc='154' complexity='17' token_count='830'></file_info>
 		<modified_lines>
 			<added_lines>83</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
