<bug_data>
<bug id='1900' author='jimthompson5802' open_date='2020-05-31T01:19:30Z' closed_time='2020-06-02T23:17:03Z'>
 	<summary>BeamSearchDecoder with non LSTM cells raises ValueError exception</summary>
 	<description>
 System information
 
 OS Platform and Distribution: Tensorflow Docker Container based on image tensorflow/tensorflow:2.2.0-jupyter
 TensorFlow version and how it was installed (source or binary): 2.2.0
 TensorFlow-Addons version and how it was installed (source or binary): tfa-nightly  0.11.0.dev20200520032238
 Python version: 3.6.9
 Is GPU used? (yes/no): no
 
 Describe the bug
 Working with &lt;denchmark-link:https://github.com/w4nderlust&gt;@w4nderlust&lt;/denchmark-link&gt;
  on this PR &lt;denchmark-link:https://github.com/ludwig-ai/ludwig/pull/699&gt;ludwig-ai/ludwig#699&lt;/denchmark-link&gt;
   we encountered an issue with seq2seq beam search.  If we use  or  with beam search and no Atttention we see this error:
 &lt;denchmark-code&gt;ValueError: The two structures don't have the same nested structure.
 First structure: type=list str=[&lt;tf.Tensor: shape=(384, 256), dtype=float32, numpy=
 array([[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)&gt;]
 Second structure: type=int str=256
 More specifically: Substructure "type=list str=[&lt;tf.Tensor: shape=(384, 256), dtype=float32, numpy=
 array([[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)&gt;]" is a sequence, while substructure "type=int str=256" is not
 
 &lt;/denchmark-code&gt;
 
 No error occurs when we use LSTMCell with beam search and no Attention.
 Refer this &lt;denchmark-link:https://github.com/uber/ludwig/pull/699#issuecomment-636403380&gt;posting&lt;/denchmark-link&gt;
  for more context.
 Code to reproduce the issue
 Minimal Working example (NOTE: updated minimal working example to simplify code)
 &lt;denchmark-code&gt;import numpy as np
 import tensorflow as tf
 from tensorflow import keras
 import tensorflow_addons as tfa
 
 # ValueError exception raised for both SimpleRNNCell and GRUCell
 
 #DECODER_CELL_TYPE = keras.layers.SimpleRNNCell  # Raises ValueError Exception
 #DECODER_CELL_TYPE = keras.layers.GRUCell        # Raises ValueError Exception
 DECODER_CELL_TYPE = keras.layers.LSTMCell        # Does not raise ValueError Exception
 
 VOCAB_SIZE = 100
 EMBED_SIZE = 10
 RNN_UNITS = 256
 INPUT_SEQUENCE_SIZE = 10
 OUTPUT_SEQUENCE_SIZE = 15
 BATCH_SIZE = 128
 BEAM_WIDTH = 3
 SIMULATE_LSTM_ENCODER = False
 
 #============== simulate output from encoder ========================
 encoder_outputs = tf.zeros([BATCH_SIZE, RNN_UNITS], dtype=tf.float32)
 if SIMULATE_LSTM_ENCODER:
     encoder_state = [tf.zeros([BATCH_SIZE, RNN_UNITS], dtype=tf.float32),
                      tf.zeros([BATCH_SIZE, RNN_UNITS], dtype=tf.float32)]
 else:
     encoder_state = [tf.zeros([BATCH_SIZE, RNN_UNITS], dtype=tf.float32)]
 
 
 
 # ================ Setup Decoder =====================
 embeddings_dec = keras.layers.Embedding(VOCAB_SIZE, EMBED_SIZE)
 decoder_cell = DECODER_CELL_TYPE(RNN_UNITS)
 output_layer = keras.layers.Dense(VOCAB_SIZE)
 
 GO_SYMBOL = VOCAB_SIZE - 1
 END_SYMBOL = 0
 batch_size = BATCH_SIZE
 encoder_sequence_length = tf.convert_to_tensor(
     np.array([INPUT_SEQUENCE_SIZE] * BATCH_SIZE),
     tf.int32
 )
 
 decoder_input = tf.expand_dims(
     [GO_SYMBOL] * batch_size, 1)
 start_tokens = tf.fill([batch_size], GO_SYMBOL)
 end_token = END_SYMBOL
 
 decoder_emb_inp = embeddings_dec(decoder_input)
 
 if DECODER_CELL_TYPE._keras_api_names[0] != 'keras.layers.LSTMCell':
     # adjust for non LSTMCell decoder
     encoder_state = [encoder_state[0]]
 
 
 #================= setup for beam search ==================
 decoder_initial_state = decoder_cell.get_initial_state(
     batch_size=BATCH_SIZE,
     dtype=tf.float32
 )
 
 if not isinstance(decoder_initial_state, list):
     decoder_initial_state = [decoder_initial_state]
 
 tiled_decoder_initial_state = tfa.seq2seq.tile_batch(
     decoder_initial_state,
     multiplier=BEAM_WIDTH
 )
 
 decoder = tfa.seq2seq.beam_search_decoder.BeamSearchDecoder(
     cell=decoder_cell,
     beam_width=BEAM_WIDTH,
     output_layer=output_layer
 )
 
 #================= perform beam search  =====================
 decoder_embedding_matrix = embeddings_dec.variables[0]
 
 (
     first_finished,
     first_inputs,
     first_state
 ) = decoder.initialize(
     decoder_embedding_matrix,
     start_tokens=start_tokens,
     end_token=end_token,
     initial_state=tiled_decoder_initial_state
 )
 
 inputs = first_inputs
 state = first_state
 
 for j in range(OUTPUT_SEQUENCE_SIZE):
     outputs, next_state, next_inputs, finished = decoder.step(
         j, inputs, state, training=False)
     inputs = next_inputs
     state = next_state
 
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='jimthompson5802' date='2020-05-31T01:22:16Z'>
 		&lt;denchmark-link:https://github.com/dynamicwebpaige&gt;@dynamicwebpaige&lt;/denchmark-link&gt;
  this is another blocker, similar to the similar problem that was solved before.
 		</comment>
 		<comment id='2' author='jimthompson5802' date='2020-06-02T23:28:25Z'>
 		Thank you for the quick resolution!
 		</comment>
 		<comment id='3' author='jimthompson5802' date='2020-06-03T02:49:30Z'>
 		Thank you as well.  I can confirm the fix addresses the issue we encountered.
 		</comment>
 	</comments>
 </bug>
<commit id='fe5adac35a601cdae3cd6247070cfb4d8fe242bc' author='Guillaume Klein' date='2020-06-02 16:17:02-07:00'>
 	<dmm_unit complexity='0.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow_addons\seq2seq\beam_search_decoder.py' new_name='tensorflow_addons\seq2seq\beam_search_decoder.py'>
 		<file_info nloc='754' complexity='80' token_count='4305'></file_info>
 		<method name='step' parameters='self,time,inputs,state,training,name'>
 				<method_info nloc='47' complexity='3' token_count='301' nesting_level='1' start_line='537' end_line='602'></method_info>
 			<added_lines>572,573,574</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow_addons\seq2seq\tests\beam_search_decoder_test.py' new_name='tensorflow_addons\seq2seq\tests\beam_search_decoder_test.py'>
 		<file_info nloc='526' complexity='24' token_count='4564'></file_info>
 		<method name='test_dynamic_decode_rnn' parameters='cell_class,time_major,has_attention,with_alignment_history'>
 				<method_info nloc='2' complexity='1' token_count='9' nesting_level='0' start_line='530' end_line='531'></method_info>
 			<added_lines>530,531</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_dynamic_decode_rnn' parameters='time_major,has_attention,with_alignment_history'>
 				<method_info nloc='90' complexity='4' token_count='549' nesting_level='0' start_line='527' end_line='628'></method_info>
 			<added_lines>527,528,529,530,531,532,550</added_lines>
 			<deleted_lines>527,545</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
