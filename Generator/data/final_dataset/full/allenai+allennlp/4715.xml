<bug_data>
<bug id='4715' author='vikigenius' open_date='2020-10-07T17:23:47Z' closed_time='2020-10-08T05:22:20Z'>
 	<summary>Rouge metric incorrectly computed with distributed training</summary>
 	<description>
 &lt;denchmark-h:h2&gt;Checklist&lt;/denchmark-h&gt;
 
 
  I have verified that the issue exists against the master branch of AllenNLP.
  I have read the relevant section in the contribution guide on reporting bugs.
  I have checked the issues list for similar or identical bug reports.
  I have checked the pull requests list for existing proposed fixes.
  I have checked the CHANGELOG and the commit log to find out if the bug was already fixed in the master branch.
  I have included in the "Description" section below a traceback from any exceptions related to this bug.
  I have included in the "Related issues or possible duplicates" section beloew all related issues and possible duplicate issues (If there are none, check this box anyway).
  I have included in the "Environment" section below the name of the operating system and Python version that I was using when I discovered this bug.
  I have included in the "Environment" section below the output of pip freeze.
  I have included in the "Steps to reproduce" section below a minimally reproducible example.
 
 &lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;
 
 Training the same model with and without distributed training produces drastically different metrics and incorrect metrics with distributed.
 In fact distributed training on 8 GPUs gives me scores &gt; 1 which is obviously incorrect.
 
 Multi GPU metrics:
 
   "validation_ROUGE-1_R": 3.7029976844787598,
   "validation_ROUGE-2_R": 1.9398850351572037,
   "validation_ROUGE-1_P": 4.3116472363471985,
   "validation_ROUGE-2_P": 2.3042131861050925,
   "validation_ROUGE-1_F1": 3.819778541723887,
   "validation_ROUGE-2_F1": 2.039711813131968,
   "validation_ROUGE-L": 3.579911470413208,
 
 
 
 
 Single GPU metrics:
 
   "validation_ROUGE-1_R": 0.49840065422148055,
   "validation_ROUGE-2_R": 0.28016004520356264,
   "validation_ROUGE-1_P": 0.5445739355913551,
   "validation_ROUGE-2_P": 0.3068941746277846,
   "validation_ROUGE-1_F1": 0.5003892512410818,
   "validation_ROUGE-2_F1": 0.28298027227105127,
   "validation_ROUGE-L": 0.4661126545409945,
 
 
 
 &lt;denchmark-h:h2&gt;Related issues or possible duplicates&lt;/denchmark-h&gt;
 
 
 #4050 is a possibly related issue
 
 &lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;
 
 OS: GNU/Linux
 Python version: 3.8.5
 
 Output of pip freeze:
 
 alabaster==0.7.12
 alembic==1.4.3
 -e git+https://github.com/allenai/allennlp.git@edcb6d3466d2c4263f1e6a5731c6ace5358f47e8#egg=allennlp
 -e git+https://github.com/allenai/allennlp-models.git@a330876f95cfec99f0ab724fbc0237f7d1f3288c#egg=allennlp_models
 apache-airflow==1.10.12
 apispec==1.3.3
 argcomplete==1.12.0
 argon2-cffi==20.1.0
 astor==0.8.1
 async-generator==1.10
 attrs==19.3.0
 Babel==2.8.0
 backcall==0.2.0
 bandit==1.6.2
 bert-serving-client==1.10.0
 bleach==3.2.1
 blis==0.4.1
 boto3==1.15.4
 botocore==1.18.4
 cached-property==1.5.2
 catalogue==1.0.0
 cattrs==1.0.0
 certifi==2020.4.5.2
 cffi==1.14.3
 chardet==3.0.4
 click==7.1.2
 colorama==0.4.3
 colorlog==4.0.2
 configparser==3.5.3
 conllu==4.2
 coverage==5.1
 croniter==0.3.34
 cymem==2.0.3
 darglint==1.4.1
 datasets==1.0.1
 decorator==4.4.2
 defusedxml==0.6.0
 dictdiffer==0.8.1
 dill==0.3.2
 dnspython==2.0.0
 doc8==0.8.1
 docutils==0.16
 dparse==0.5.1
 email-validator==1.1.1
 entrypoints==0.3
 eradicate==1.0
 filelock==3.0.12
 flake8==3.8.3
 flake8-bandit==2.1.2
 flake8-broken-line==0.2.0
 flake8-bugbear==19.8.0
 flake8-commas==2.0.0
 flake8-comprehensions==3.2.3
 flake8-debugger==3.2.1
 flake8-docstrings==1.5.0
 flake8-eradicate==0.3.0
 flake8-isort==3.0.1
 flake8-plugin-utils==1.3.1
 flake8-polyfill==1.0.2
 flake8-pytest-style==1.3.0
 flake8-quotes==2.1.2
 flake8-rst-docstrings==0.0.12
 flake8-string-format==0.2.3
 Flask==1.1.2
 Flask-Admin==1.5.4
 Flask-AppBuilder==2.3.0
 Flask-Babel==1.0.0
 Flask-Caching==1.3.3
 Flask-JWT-Extended==3.24.1
 Flask-Login==0.4.1
 Flask-OpenID==1.2.5
 Flask-SQLAlchemy==2.4.4
 flask-swagger==0.2.14
 Flask-WTF==0.14.3
 ftfy==5.8
 funcsigs==1.0.2
 future==0.18.2
 gitdb==4.0.5
 GitPython==3.1.3
 graphviz==0.14.1
 gunicorn==20.0.4
 h5py==2.10.0
 idna==2.9
 imagesize==1.2.0
 importlib-metadata==1.6.1
 ipykernel==5.3.4
 ipython==7.16.1
 ipython-genutils==0.2.0
 ipywidgets==7.5.1
 iso8601==0.1.13
 isort==4.3.21
 itsdangerous==1.1.0
 jedi==0.17.2
 Jinja2==2.11.2
 jmespath==0.10.0
 joblib==0.16.0
 json-merge-patch==0.2
 jsonnet==0.16.0
 jsonpickle==1.4.1
 jsonschema==3.2.0
 jupyter==1.0.0
 jupyter-client==6.1.7
 jupyter-console==6.2.0
 jupyter-core==4.6.3
 jupyterlab-pygments==0.1.1
 lazy-object-proxy==1.5.1
 lockfile==0.12.2
 m2r==0.2.1
 Mako==1.1.3
 Markdown==2.6.11
 MarkupSafe==1.1.1
 marshmallow==3.6.1
 marshmallow-enum==1.5.1
 marshmallow-polyfield==5.9
 marshmallow-sqlalchemy==0.23.1
 mccabe==0.6.1
 mistune==0.8.4
 more-itertools==8.4.0
 murmurhash==1.0.2
 mypy==0.782
 mypy-extensions==0.4.3
 natsort==7.0.1
 nbclient==0.5.0
 nbconvert==6.0.3
 nbformat==5.0.7
 nest-asyncio==1.4.0
 nitpick==0.22.2
 nltk==3.5
 notebook==6.1.4
 numpy==1.19.2
 overrides==3.1.0
 packaging==20.4
 pandas==0.25.3
 pandocfilters==1.4.2
 parso==0.7.1
 pbr==5.4.5
 pendulum==1.4.4
 pep8-naming==0.9.1
 pexpect==4.8.0
 pickleshare==0.7.5
 plac==1.1.3
 pluggy==0.13.1
 preshed==3.0.2
 prison==0.1.3
 prometheus-client==0.8.0
 prompt-toolkit==3.0.3
 protobuf==3.13.0
 psutil==5.7.2
 ptyprocess==0.6.0
 py==1.8.1
 py-rouge==1.1
 py4j==0.10.9
 pyarrow==1.0.1
 pycodestyle==2.6.0
 pycparser==2.20
 pydocstyle==5.0.2
 pyflakes==2.2.0
 Pygments==2.6.1
 PyJWT==1.7.1
 pyparsing==2.4.7
 pyrsistent==0.17.3
 pyspark==3.0.1
 pytest==5.4.3
 pytest-cov==2.10.1
 pytest-randomly==3.4.1
 python-daemon==2.2.4
 python-dateutil==2.8.1
 python-editor==1.0.4
 python-nvd3==0.15.0
 python-openid==2.2.5
 python-slugify==4.0.0
 pytz==2020.1
 pytzdata==2020.1
 PyYAML==5.3.1
 pyzmq==19.0.2
 qtconsole==4.7.7
 QtPy==1.9.0
 regex==2020.9.27
 requests==2.23.0
 restructuredtext-lint==1.3.1
 ruamel.yaml==0.16.10
 ruamel.yaml.clib==0.2.0
 s3transfer==0.3.3
 sacremoses==0.0.43
 safety==1.9.0
 scikit-learn==0.23.2
 scipy==1.5.2
 Send2Trash==1.5.0
 sentencepiece==0.1.91
 setproctitle==1.1.10
 six==1.15.0
 smmap==3.0.4
 snowballstemmer==2.0.0
 sortedcontainers==2.2.2
 spacy==2.3.2
 Sphinx==2.4.4
 sphinx-autodoc-typehints==1.10.3
 sphinxcontrib-applehelp==1.0.2
 sphinxcontrib-devhelp==1.0.2
 sphinxcontrib-htmlhelp==1.0.3
 sphinxcontrib-jsmath==1.0.1
 sphinxcontrib-qthelp==1.0.3
 sphinxcontrib-serializinghtml==1.1.4
 SQLAlchemy==1.3.19
 SQLAlchemy-JSONField==0.9.0
 SQLAlchemy-Utils==0.36.8
 srsly==1.0.2
 stevedore==2.0.0
 tabulate==0.8.7
 tenacity==4.12.0
 tensorboardX==2.1
 terminado==0.8.3
 testfixtures==6.14.1
 testpath==0.4.4
 text-unidecode==1.3
 thinc==7.4.1
 threadpoolctl==2.1.0
 thrift==0.13.0
 tokenizers==0.8.1rc2
 toml==0.10.0
 tomlkit==0.7.0
 torch==1.6.0
 tornado==6.0.4
 tqdm==4.49.0
 traitlets==4.3.3
 transformers==3.3.1
 typed-ast==1.4.1
 typing-extensions==3.7.4.2
 tzlocal==1.5.1
 unicodecsv==0.14.1
 urllib3==1.25.9
 wasabi==0.8.0
 wcwidth==0.2.4
 webencodings==0.5.1
 wemake-python-styleguide==0.14.1
 Werkzeug==0.16.1
 widgetsnbextension==3.5.1
 word2number==1.1
 WTForms==2.3.3
 xxhash==2.0.0
 zipp==3.1.0
 zope.deprecation==4.4.0
 
 
 
 &lt;denchmark-h:h2&gt;Steps to reproduce&lt;/denchmark-h&gt;
 
 Train BART twice, once with distributed training on multiple GPUs and the other on a single GPU without distributed training and compare the metrics.json file.
 	</description>
 	<comments>
 		<comment id='1' author='vikigenius' date='2020-10-07T20:37:30Z'>
 		Thanks &lt;denchmark-link:https://github.com/vikigenius&gt;@vikigenius&lt;/denchmark-link&gt;
 , I believe the issue is that  is not being aggregated across GPUs. Would you be interested in making a PR to fix it? If not, one of us will probably be able to get to it within the next few days.
 		</comment>
 		<comment id='2' author='vikigenius' date='2020-10-07T21:05:33Z'>
 		&lt;denchmark-link:https://github.com/epwalsh&gt;@epwalsh&lt;/denchmark-link&gt;
  Yeah, I will look into creating a PR. I don't have a lot of experience with distributed training yet. But I am excited to learn more.
 		</comment>
 	</comments>
 </bug>
<commit id='bc6f15accc2392c42d97e2bdee7bcb47a664f597' author='Vikash' date='2020-10-08 05:22:19+00:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>73</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='allennlp\training\metrics\rouge.py' new_name='allennlp\training\metrics\rouge.py'>
 		<file_info nloc='203' complexity='18' token_count='1201'></file_info>
 		<modified_lines>
 			<added_lines>209,210,211,212,213,214,215</added_lines>
 			<deleted_lines>209</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
