<bug_data>
<bug id='4757' author='mklimasz' open_date='2020-10-28T07:46:07Z' closed_time='2020-10-28T19:38:48Z'>
 	<summary>PretrainedTransformerTokenizer fails when disabling "fast" tokenizer</summary>
 	<description>
 &lt;denchmark-h:h2&gt;Checklist&lt;/denchmark-h&gt;
 
 
  I have verified that the issue exists against the master branch of AllenNLP.
  I have read the relevant section in the contribution guide on reporting bugs.
  I have checked the issues list for similar or identical bug reports.
  I have checked the pull requests list for existing proposed fixes.
  I have checked the CHANGELOG and the commit log to find out if the bug was already fixed in the master branch.
  I have included in the "Description" section below a traceback from any exceptions related to this bug.
  I have included in the "Related issues or possible duplicates" section beloew all related issues and possible duplicate issues (If there are none, check this box anyway).
  I have included in the "Environment" section below the name of the operating system and Python version that I was using when I discovered this bug.
  I have included in the "Environment" section below the output of pip freeze.
  I have included in the "Steps to reproduce" section below a minimally reproducible example.
 
 &lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;
 
 Tested on 1.2.0rc1 and master
 Intra work tokenizer doesn't work when we deliberately set use fast tokenizer to false (not sure if it's new transformers change).
 I think that setting return_token_type_ids to None instead of False is solution here.
 
 Python traceback:
 
 Traceback (most recent call last):
   File "bug_example.py", line 4, in &lt;module&gt;
     tokenizer_kwargs={"use_fast": False}).intra_word_tokenize(["My", "text", "will"])
   File "X/venv/lib/python3.6/site-packages/allennlp-1.2.0rc1-py3.6.egg/allennlp/data/tokenizers/pretrained_transformer_tokenizer.py", line 387, in intra_word_tokenize
     tokens, offsets = self._intra_word_tokenize(string_tokens)
   File "X/venv/lib/python3.6/site-packages/allennlp-1.2.0rc1-py3.6.egg/allennlp/data/tokenizers/pretrained_transformer_tokenizer.py", line 354, in _intra_word_tokenize
     return_token_type_ids=False,
   File "X/venv/lib/python3.6/site-packages/transformers-3.4.0-py3.6.egg/transformers/tokenization_utils_base.py", line 2229, in encode_plus
     **kwargs,
   File "X/venv/lib/python3.6/site-packages/transformers-3.4.0-py3.6.egg/transformers/tokenization_utils.py", line 490, in _encode_plus
     verbose=verbose,
   File "X/venv/lib/python3.6/site-packages/transformers-3.4.0-py3.6.egg/transformers/tokenization_utils_base.py", line 2617, in prepare_for_model
     "Asking to return token_type_ids while setting add_special_tokens to False "
 ValueError: Asking to return token_type_ids while setting add_special_tokens to False results in an undefined behavior. Please set add_special_tokens to True or set return_token_type_ids to None.
 
 
 
 
 &lt;denchmark-h:h2&gt;Related issues or possible duplicates&lt;/denchmark-h&gt;
 
 Not to my knowledge
 &lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;
 
 OS:
 Ubuntu 18.04 LTS
 Python version:
 3.6.9 and 3.8.0
 
 Output of pip freeze:
 
 absl-py==0.9.0
 allennlp==1.2.0rc1
 attrs==20.2.0
 blis==0.4.1
 boto3==1.16.5
 botocore==1.19.5
 cached-property==1.5.2
 cachetools==4.1.1
 catalogue==1.0.0
 certifi==2020.6.20
 chardet==3.0.4
 click==7.1.2
 conllu==2.3.2
 cymem==2.0.3
 dataclasses==0.5
 dataclasses-json==0.5.2
 en-core-web-sm==2.3.1
 filelock==3.0.12
 future==0.18.2
 google-auth==1.22.1
 google-auth-oauthlib==0.4.1
 grpcio==1.33.1
 h5py==3.0.0rc1
 idna==2.10
 importlib-metadata==2.0.0
 iniconfig==1.1.1
 jmespath==0.10.0
 joblib==0.14.1
 jsonnet==0.15.0
 jsonpickle==1.4.1
 Markdown==3.3.3
 marshmallow==3.8.0
 marshmallow-enum==1.5.1
 murmurhash==1.0.2
 mypy-extensions==0.4.3
 nltk==3.5
 numpy==1.19.2
 oauthlib==3.1.0
 overrides==3.1.0
 packaging==20.4
 pkg-resources==0.0.0
 plac==1.1.3
 pluggy==0.13.1
 preshed==3.0.2
 protobuf==4.0.0rc2
 py==1.9.0
 pyasn1==0.4.8
 pyasn1-modules==0.2.8
 pyparsing==3.0.0a2
 pytest==6.1.1
 python-dateutil==2.8.1
 regex==2020.10.23
 requests==2.23.0
 requests-oauthlib==1.3.0
 rsa==4.6
 s3transfer==0.3.3
 sacremoses==0.0.43
 scikit-learn==0.23.2
 scipy==1.5.3
 sentencepiece==0.1.94
 six==1.15.0
 spacy==2.3.2
 srsly==1.0.2
 stringcase==1.2.0
 tensorboard==2.1.0
 tensorboardX==2.1
 thinc==7.4.1
 threadpoolctl==2.1.0
 tokenizers==0.9.2
 toml==0.10.1
 torch==1.6.0
 tqdm==4.43.0
 transformers==3.4.0
 typing-extensions==3.7.4.3
 typing-inspect==0.6.0
 urllib3==1.25.11
 wasabi==0.8.0
 Werkzeug==1.0.1
 zipp==3.4.0
 
 
 
 
 &lt;denchmark-h:h2&gt;Steps to reproduce&lt;/denchmark-h&gt;
 
 
 Example source:
 
 from allennlp.data.tokenizers import PretrainedTransformerTokenizer
 
 PretrainedTransformerTokenizer("bert-base-cased",
                                tokenizer_kwargs={"use_fast": False}).intra_word_tokenize(["My", "text", "will"])
 
 
 
 	</description>
 	<comments>
 		<comment id='1' author='mklimasz' date='2020-10-28T19:38:48Z'>
 		&lt;denchmark-link:https://github.com/epwalsh&gt;@epwalsh&lt;/denchmark-link&gt;
  thanks for the merge. I'm closing this issue as solved.
 		</comment>
 	</comments>
 </bug>
<commit id='baca7545252196245f8288b35de8fa78e381e086' author='Mateusz Klimaszewski' date='2020-10-28 09:26:14-07:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>109</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='allennlp\data\tokenizers\pretrained_transformer_tokenizer.py' new_name='allennlp\data\tokenizers\pretrained_transformer_tokenizer.py'>
 		<file_info nloc='377' complexity='22' token_count='1879'></file_info>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>354</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
