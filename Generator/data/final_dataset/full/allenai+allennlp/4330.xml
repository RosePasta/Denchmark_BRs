<bug_data>
<bug id='4330' author='epwalsh' open_date='2020-06-05T22:18:19Z' closed_time='2020-07-09T18:50:13Z'>
 	<summary>Transformer tokenizers cause deadlocks when dataset reader is lazy and dataloader num_workers &amp;gt; 0</summary>
 	<description>
 &lt;denchmark-h:h2&gt;Checklist&lt;/denchmark-h&gt;
 
 
  I have verified that the issue exists against the master branch of AllenNLP.
  I have read the relevant section in the contribution guide on reporting bugs.
  I have checked the issues list for similar or identical bug reports.
  I have checked the pull requests list for existing proposed fixes.
  I have checked the CHANGELOG and the commit log to find out if the bug was already fixed in the master branch.
  I have included in the "Description" section below a traceback from any exceptions related to this bug.
  I have included in the "Related issues or possible duplicates" section beloew all related issues and possible duplicate issues (If there are none, check this box anyway).
  I have included in the "Environment" section below the name of the operating system and Python version that I was using when I discovered this bug.
  I have included in the "Environment" section below the output of pip freeze.
  I have included in the "Steps to reproduce" section below a minimally reproducible example.
 
 &lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;
 
 Dataset readers that use a PretrainedTransformerTokenizer can cause a deadlock when used lazily and with num_workers &gt; 0 in the Dataloader.
 
 Python traceback:
 
 ...
 2020-06-05 15:04:14,980 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1
 2020-06-05 15:04:14,980 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False
 2020-06-05 15:04:14,980 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False
 2020-06-05 15:04:14,980 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38
 2020-06-05 15:04:14,982 - WARNING - allennlp.training.trainer - You provided a validation dataset but patience was set to None, meaning that early stopping is disabled
 2020-06-05 15:04:14,982 - INFO - allennlp.training.trainer - Beginning training.
 2020-06-05 15:04:14,982 - INFO - allennlp.training.trainer - Epoch 0/9
 2020-06-05 15:04:14,982 - INFO - allennlp.training.trainer - Worker 0 memory usage MB: 3119.212
 2020-06-05 15:04:14,989 - INFO - allennlp.common.file_utils - checking cache for https://allennlp.s3.amazonaws.com/datasets/snli/snli_1.0_dev.jsonl at /home/epwalsh/.allennlp/cache/144d0e8739288dbcde80c238baf94dde44c4ed58da59c92cf48ccce5b649574a.07ccd1720fbe07137833cd627398fb7e2687bdcbad963d71cae8c97a37f76687
 2020-06-05 15:04:14,989 - INFO - allennlp.common.file_utils - waiting to acquire lock on /home/epwalsh/.allennlp/cache/144d0e8739288dbcde80c238baf94dde44c4ed58da59c92cf48ccce5b649574a.07ccd1720fbe07137833cd627398fb7e2687bdcbad963d71cae8c97a37f76687
 2020-06-05 15:04:14,989 - INFO - filelock - Lock 140643240348696 acquired on /home/epwalsh/.allennlp/cache/144d0e8739288dbcde80c238baf94dde44c4ed58da59c92cf48ccce5b649574a.07ccd1720fbe07137833cd627398fb7e2687bdcbad963d71cae8c97a37f76687.lock
 2020-06-05 15:04:14,989 - INFO - allennlp.common.file_utils - cache of https://allennlp.s3.amazonaws.com/datasets/snli/snli_1.0_dev.jsonl is up-to-date
 2020-06-05 15:04:14,989 - INFO - filelock - Lock 140643240348696 released on /home/epwalsh/.allennlp/cache/144d0e8739288dbcde80c238baf94dde44c4ed58da59c92cf48ccce5b649574a.07ccd1720fbe07137833cd627398fb7e2687bdcbad963d71cae8c97a37f76687.lock
 2020-06-05 15:04:14,990 - INFO - allennlp_models.pair_classification.dataset_readers.snli - Reading SNLI instances from jsonl dataset at: /home/epwalsh/.allennlp/cache/144d0e8739288dbcde80c238baf94dde44c4ed58da59c92cf48ccce5b649574a.07ccd1720fbe07137833cd627398fb7e2687bdcbad963d71cae8c97a37f76687
 2020-06-05 15:04:15,010 - INFO - allennlp.common.file_utils - checking cache for https://allennlp.s3.amazonaws.com/datasets/snli/snli_1.0_test.jsonl at /home/epwalsh/.allennlp/cache/321a276d553780889b3d4276a1ce372c54c405f8e57917cdfa993feb5a0783a2.02752124e6570073d50876f7f66a32191d9787dcb992c56e79f558cebf5faf92
 2020-06-05 15:04:15,011 - INFO - allennlp.common.file_utils - waiting to acquire lock on /home/epwalsh/.allennlp/cache/321a276d553780889b3d4276a1ce372c54c405f8e57917cdfa993feb5a0783a2.02752124e6570073d50876f7f66a32191d9787dcb992c56e79f558cebf5faf92
 2020-06-05 15:04:15,011 - INFO - filelock - Lock 140642731300176 acquired on /home/epwalsh/.allennlp/cache/321a276d553780889b3d4276a1ce372c54c405f8e57917cdfa993feb5a0783a2.02752124e6570073d50876f7f66a32191d9787dcb992c56e79f558cebf5faf92.lock
 2020-06-05 15:04:15,011 - INFO - allennlp.common.file_utils - cache of https://allennlp.s3.amazonaws.com/datasets/snli/snli_1.0_test.jsonl is up-to-date
 2020-06-05 15:04:15,011 - INFO - filelock - Lock 140642731300176 released on /home/epwalsh/.allennlp/cache/321a276d553780889b3d4276a1ce372c54c405f8e57917cdfa993feb5a0783a2.02752124e6570073d50876f7f66a32191d9787dcb992c56e79f558cebf5faf92.lock
 2020-06-05 15:04:15,011 - INFO - allennlp_models.pair_classification.dataset_readers.snli - Reading SNLI instances from jsonl dataset at: /home/epwalsh/.allennlp/cache/321a276d553780889b3d4276a1ce372c54c405f8e57917cdfa993feb5a0783a2.02752124e6570073d50876f7f66a32191d9787dcb992c56e79f558cebf5faf92
 2020-06-05 15:04:15,026 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 29
 2020-06-05 15:04:15,031 - INFO - allennlp.training.trainer - Training
 2020-06-05 15:04:15,133 - INFO - allennlp.common.file_utils - checking cache for https://allennlp.s3.amazonaws.com/datasets/snli/snli_1.0_train.jsonl at /home/epwalsh/.allennlp/cache/e00019e4314663e308d583b0fce3b28548b1935143669947b5642779f366dce0.a6d9333199c8569c358962009242171183a893a532b6a5267c91fd3ea0ca6484
 2020-06-05 15:04:15,133 - INFO - allennlp.common.file_utils - waiting to acquire lock on /home/epwalsh/.allennlp/cache/e00019e4314663e308d583b0fce3b28548b1935143669947b5642779f366dce0.a6d9333199c8569c358962009242171183a893a532b6a5267c91fd3ea0ca6484
 2020-06-05 15:04:15,134 - INFO - filelock - Lock 140642730869648 acquired on /home/epwalsh/.allennlp/cache/e00019e4314663e308d583b0fce3b28548b1935143669947b5642779f366dce0.a6d9333199c8569c358962009242171183a893a532b6a5267c91fd3ea0ca6484.lock
 2020-06-05 15:04:15,134 - INFO - allennlp.common.file_utils - cache of https://allennlp.s3.amazonaws.com/datasets/snli/snli_1.0_train.jsonl is up-to-date
 2020-06-05 15:04:15,134 - INFO - filelock - Lock 140642730869648 released on /home/epwalsh/.allennlp/cache/e00019e4314663e308d583b0fce3b28548b1935143669947b5642779f366dce0.a6d9333199c8569c358962009242171183a893a532b6a5267c91fd3ea0ca6484.lock
 2020-06-05 15:04:15,134 - INFO - allennlp_models.pair_classification.dataset_readers.snli - Reading SNLI instances from jsonl dataset at: /home/epwalsh/.allennlp/cache/e00019e4314663e308d583b0fce3b28548b1935143669947b5642779f366dce0.a6d9333199c8569c358962009242171183a893a532b6a5267c91fd3ea0ca6484
 # hangs forever here
 
 
 
 &lt;denchmark-h:h2&gt;Related issues or possible duplicates&lt;/denchmark-h&gt;
 
 
 I'm pretty sure this has to do with huggingface/tokenizers#187
 
 &lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;
 
 OS: Ubuntu 18.04
 Python version: 3.6.5
 
 Output of pip freeze:
 
 aiohttp==3.6.2
 alabaster==0.7.11
 -e git+git@github.com:epwalsh/allennlp.git@902d36a520dd75fd82ebaa014799ed8fa6d02e2e#egg=allennlp
 -e git+git@github.com:allenai/allennlp-beaker.git@d3afc6e23ae22ea434aff6b9296f9d6e17fc2b45#egg=allennlp_beaker
 -e git+git@github.com:allenai/allennlp-models.git@efe66bc086c95a78c7779933b82e1382b63a3ee1#egg=allennlp_models
 apex==0.1
 appdirs==1.4.3
 argh==0.26.2
 asn1crypto==0.24.0
 aspy.yaml==1.3.0
 astroid==2.3.0
 async-timeout==3.0.0
 atomicwrites==1.1.5
 attrs==19.3.0
 aws-xray-sdk==0.95
 awscli==1.18.40
 Babel==2.6.0
 backcall==0.1.0
 black==19.10b0
 bleach==2.1.3
 blis==0.4.1
 boto==2.49.0
 boto3==1.13.16
 botocore==1.16.16
 catalogue==1.0.0
 cattrs==0.9.0
 certifi==2020.4.5.1
 cffi==1.11.5
 cfgv==2.0.1
 chardet==3.0.4
 click==7.1.2
 click-completion==0.5.0
 click-spinner==0.1.10
 codecov==2.1.3
 colorama==0.3.9
 conllu==3.0
 cookies==2.2.1
 coverage==5.1
 coveralls==1.5.1
 crayons==0.1.2
 cryptography==2.3.1
 cycler==0.10.0
 cymem==2.0.3
 cytoolz==0.9.0.1
 dash==1.5.1
 dash-auth==1.3.2
 dash-bootstrap-components==0.7.2
 dash-core-components==1.4.0
 dash-daq==0.1.4
 dash-html-components==1.0.1
 dash-renderer==1.2.0
 dash-table==4.5.0
 dataclasses==0.7
 decorator==4.3.0
 dill==0.2.8.2
 docker==3.5.0
 docker-pycreds==0.3.0
 docopt==0.6.2
 docutils==0.15.2
 ecdsa==0.13
 editdistance==0.4
 en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz
 entrypoints==0.3
 filelock==3.0.12
 flake8==3.8.1
 flaky==3.6.1
 Flask==1.0.2
 Flask-Caching==1.7.2
 Flask-Compress==1.4.0
 Flask-Cors==3.0.7
 Flask-Login==0.4.1
 Flask-SeaSurf==0.2.2
 ftfy==5.5.0
 future==0.18.2
 gevent==1.3.6
 greenlet==0.4.14
 gunicorn==19.9.0
 h5py==2.10.0
 hide-code==0.5.2
 html5lib==1.0.1
 identify==1.4.7
 idna==2.9
 idna-ssl==1.1.0
 imagesize==1.0.0
 importlib-metadata==1.6.0
 importlib-resources==1.0.2
 inotify==0.2.10
 ipykernel==4.8.2
 ipython==6.5.0
 ipython-genutils==0.2.0
 ipywidgets==7.4.0
 isort==4.3.4
 itsdangerous==0.24
 jedi==0.16.0
 jeepney==0.4.3
 Jinja2==2.11.1
 jmespath==0.10.0
 joblib==0.15.1
 jsondiff==1.1.1
 jsonnet==0.16.0
 jsonpickle==1.4.1
 jsonschema==2.6.0
 jupyter==1.0.0
 jupyter-client==5.2.3
 jupyter-console==5.2.0
 jupyter-contrib-core==0.3.3
 jupyter-core==4.4.0
 jupyter-nbextensions-configurator==0.4.0
 keyring==21.2.1
 kiwisolver==1.0.1
 lazy-object-proxy==1.3.1                                                                                                                                                                                  
 livereload==2.5.2
 lunr==0.5.6
 Markdown==3.2.1
 markdown-include==0.5.1
 MarkupSafe==1.0
 mathy-pydoc==0.6.7
 matplotlib==3.2.1
 mccabe==0.6.1
 mistune==0.8.3
 mkdocs==1.1
 mkdocs-material==5.2.0
 mkdocs-material-extensions==1.0
 mock==2.0.0
 more-itertools==8.3.0
 moto==1.3.4
 msgpack==0.5.6
 msgpack-numpy==0.4.3.1
 multidict==4.7.6
 murmurhash==1.0.2
 mypy==0.770
 mypy-extensions==0.4.3
 nbconvert==5.3.1
 nbformat==4.4.0
 neovim==0.2.6
 -e git+git@github.com:epwalsh/nlp-models.git@23232ea470503e5a6453aca2bb9a22b84de6848d#egg=nlpete
 nltk==3.5
 nodeenv==1.3.3
 nose==1.3.7
 notebook==5.6.0
 nr.collections==0.0.1
 nr.databind==0.0.4
 nr.databind.core==0.0.14
 nr.databind.json==0.0.9
 nr.interface==0.0.2
 nr.metaclass==0.0.5
 nr.parsing.date==0.1.0
 nr.pylang.utils==0.0.2
 nr.stream==0.0.3
 numpy==1.18.4
 numpydoc==0.8.0
 nvidia-ml-py3==7.352.0
 overrides==3.0.0
 packaging==20.4
 pandocfilters==1.4.2
 parsimonious==0.8.0
 parso==0.6.2
 pathspec==0.7.0
 pathtools==0.1.2
 pbr==4.2.0
 pdfkit==0.6.1
 pexpect==4.6.0
 pickleshare==0.7.4
 pkg-resources==0.0.0
 pkginfo==1.4.2
 plac==1.1.3
 plotly==4.2.1                                                                                                                                                                                               [50/1861]
 pluggy==0.13.1
 port-for==0.3.1
 pre-commit==2.3.0
 preshed==3.0.2
 prometheus-client==0.3.1
 prompt-toolkit==1.0.15
 protobuf==3.12.1
 ptyprocess==0.6.0
 py==1.8.1
 py-rouge==1.1
 py3nvml==0.2.5
 pyaml==17.12.1
 pyasn1==0.4.4
 pycodestyle==2.6.0
 pycparser==2.18
 pycryptodome==3.6.5
 pycycle==0.0.8
 pydoc-markdown @ git+https://github.com/NiklasRosenstein/pydoc-markdown.git@f0bf8af1db4f11581c19d206d4ed1ab34b4854c1
 pydocstyle==5.0.2
 pyflakes==2.2.0
 Pygments==2.5.2
 pylint==2.4.1
 pymdown-extensions==7.0
 pypandoc==1.4
 pyparsing==2.4.7
 pytest==5.4.2
 pytest-cov==2.8.1
 python-dateutil==2.8.1
 python-jose==2.0.2
 pytorch-pretrained-bert==0.6.1
 pytz==2017.3
 PyYAML==5.3
 pyzmq==17.1.2
 qtconsole==4.3.1
 readme-renderer==26.0
 regex==2020.5.14
 registrable==0.0.1
 requests==2.23.0
 requests-toolbelt==0.8.0
 responses==0.10.14
 retrying==1.3.3
 rsa==3.4.2
 ruamel.yaml==0.16.10
 ruamel.yaml.clib==0.2.0
 s3transfer==0.3.3
 sacremoses==0.0.43
 scikit-learn==0.23.1
 scipy==1.4.1
 SecretStorage==3.1.2
 semantic-version==2.8.5
 Send2Trash==1.5.0
 sentencepiece==0.1.91
 shellingham==1.2.8
 simplegeneric==0.8.1
 six==1.15.0
 snowballstemmer==1.2.1
 spacy==2.2.4
 Sphinx==2.2.0
 sphinx-autobuild==0.7.1
 sphinx-rtd-theme==0.4.1
 sphinxcontrib-applehelp==1.0.1
 sphinxcontrib-devhelp==1.0.1
 sphinxcontrib-htmlhelp==1.0.2
 sphinxcontrib-jsmath==1.0.1
 sphinxcontrib-qthelp==1.0.2
 sphinxcontrib-serializinghtml==1.1.3
 sqlparse==0.2.4
 srsly==1.0.2
 tensorboardX==2.0
 terminado==0.8.1
 testpath==0.3.1
 thinc==7.4.0
 threadpoolctl==2.0.0
 tokenizers==0.7.0
 toml==0.10.0
 toolz==0.9.0
 torch==1.5.0
 tornado==5.1
 tqdm==4.46.1
 traitlets==4.3.2
 transformers==2.9.1
 twine==3.1.1
 typed-ast==1.4.0
 typing==3.7.4.1
 typing-extensions==3.7.4
 ua-parser==0.8.0
 ujson==1.35
 Unidecode==1.0.22
 urllib3==1.25.9
 virtualenv==16.7.5
 wasabi==0.6.0
 watchdog==0.10.2
 wcwidth==0.1.9
 webencodings==0.5.1
 websocket-client==0.49.0
 Werkzeug==0.14.1
 widgetsnbextension==3.4.0
 word2number==1.1
 wrapt==1.10.11
 xmltodict==0.11.0
 yarl==1.2.6
 zipp==3.1.0
 
 
 
 &lt;denchmark-h:h2&gt;Steps to reproduce&lt;/denchmark-h&gt;
 
 Run allennlp train on this config:
 
 Example source:
 
 local transformer_model = "roberta-large";
 local transformer_dim = 1024;
 local cls_is_last_token = false;
 
 {
   "dataset_reader":{
     "type": "snli",
     "lazy": true,
     "tokenizer": {
       "type": "pretrained_transformer",
       "model_name": transformer_model,
       "add_special_tokens": false
     },
     "token_indexers": {
       "tokens": {
         "type": "pretrained_transformer",
         "model_name": transformer_model,
         "max_length": 40
       }
     }
   },
   "train_data_path": "https://allennlp.s3.amazonaws.com/datasets/snli/snli_1.0_train.jsonl",
   "validation_data_path": "https://allennlp.s3.amazonaws.com/datasets/snli/snli_1.0_dev.jsonl",
   "test_data_path": "https://allennlp.s3.amazonaws.com/datasets/snli/snli_1.0_test.jsonl",
   "model": {
     "type": "basic_classifier",
     "text_field_embedder": {
       "token_embedders": {
         "tokens": {
           "type": "pretrained_transformer",
           "model_name": transformer_model,
           "max_length": 512
         }
       }
     },
     "seq2vec_encoder": {
        "type": "cls_pooler",
        "embedding_dim": transformer_dim,
        "cls_is_last_token": cls_is_last_token
     },
     "feedforward": {
       "input_dim": transformer_dim,
       "num_layers": 1,
       "hidden_dims": transformer_dim,
       "activations": "tanh"
     },
     "dropout": 0.1,
     "namespace": "tags"
   },
   "data_loader": {
     "batch_size" : 8,
     "num_workers": true,
   },
   "trainer": {
     "num_epochs": 10,
     "cuda_device" : -1,
     "validation_metric": "+accuracy",
     "learning_rate_scheduler": {
       "type": "slanted_triangular",
       "cut_frac": 0.06
     },
     "optimizer": {
       "type": "huggingface_adamw",
       "lr": 2e-5,
       "weight_decay": 0.1,
     }
   }
 }
 
 
 	</description>
 	<comments>
 		<comment id='1' author='epwalsh' date='2020-06-05T22:45:06Z'>
 		Our thinking is that we would have a very hard time resolving this issue ourselves, and it'll take work from huggingface's tokenizers repo to really fix this.  The suggested workaround is to just always avoid setting num_workers when using a huggingface tokenizer.
 		</comment>
 		<comment id='2' author='epwalsh' date='2020-06-08T18:28:55Z'>
 		Here's an even simpler repro example btw:
 from allennlp.data import DataLoader, Vocabulary
 from allennlp.data.tokenizers import PretrainedTransformerTokenizer
 from allennlp.data.token_indexers import PretrainedTransformerIndexer
 from allennlp_models.pair_classification.dataset_readers import SnliReader
 
 reader = SnliReader(
     lazy=True,
     max_instances=10,
     tokenizer=PretrainedTransformerTokenizer(
         model_name="roberta-large", add_special_tokens=False
     ),
     token_indexers={
         "tokens": PretrainedTransformerIndexer(
             model_name="roberta-large", max_length=40
         )
     },
 )
 
 print("initialized dataset")
 ds = reader.read("https://allennlp.s3.amazonaws.com/datasets/snli/snli_1.0_dev.jsonl")
 
 print("creating vocab")
 vocab = Vocabulary.from_instances(ds)
 
 ds.index_with(vocab)
 
 print("reading instances")
 # this will hang on the next line unless you remove the `num_workers=1` bit
 for instance in DataLoader(ds, num_workers=1):
     print(instance)
 print("done")
 I believe this confirms the issue is from &lt;denchmark-link:https://github.com/huggingface/tokenizers/issues/187&gt;huggingface/tokenizers#187&lt;/denchmark-link&gt;
 .
 		</comment>
 		<comment id='3' author='epwalsh' date='2020-06-15T22:27:12Z'>
 		There is a PR open now with tokenizers that provides a work-around for this: &lt;denchmark-link:https://github.com/huggingface/tokenizers/pull/306&gt;huggingface/tokenizers#306&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='4' author='epwalsh' date='2020-06-29T16:53:13Z'>
 		This is now fixed in the latest release of tokenizers (version 0.8.0), but we'll have to wait until it makes it upstream into a transformers release.
 		</comment>
 		<comment id='5' author='epwalsh' date='2020-07-06T02:21:41Z'>
 		I'm facing the same issue; is the solution to just wait for huggingface's release that will be incorporated here?
 Does this mean that AllenNLP 1.0 cannot be used for any code that uses the PretrainedTransformerTokenizer?
 		</comment>
 		<comment id='6' author='epwalsh' date='2020-07-06T04:54:15Z'>
 		Just don't set num_workers &gt; 0 with a lazy dataset.
 		</comment>
 		<comment id='7' author='epwalsh' date='2020-07-06T04:55:41Z'>
 		Someone from the group tried num_workers = 0 and it didn't work. I will try and report.
 		</comment>
 		<comment id='8' author='epwalsh' date='2020-07-06T04:58:21Z'>
 		We have lots of models that train just fine with transformers, so it definitely is not a global problem. Make sure you have an up to date release, also.
 		</comment>
 	</comments>
 </bug>
<commit id='b9a91646bd97942609b545794e889b16ba8e05a5' author='dependabot-preview[bot]' date='2020-07-09 11:50:12-07:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>27,47</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='allennlp\data\tokenizers\pretrained_transformer_tokenizer.py' new_name='allennlp\data\tokenizers\pretrained_transformer_tokenizer.py'>
 		<file_info nloc='382' complexity='17' token_count='1837'></file_info>
 		<modified_lines>
 			<added_lines>131,133</added_lines>
 			<deleted_lines>131,133,188</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='setup.py' new_name='setup.py'>
 		<file_info nloc='59' complexity='0' token_count='187'></file_info>
 		<modified_lines>
 			<added_lines>67</added_lines>
 			<deleted_lines>67</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\data\token_indexers\pretrained_transformer_indexer_test.py' new_name='tests\data\token_indexers\pretrained_transformer_indexer_test.py'>
 		<file_info nloc='169' complexity='16' token_count='1193'></file_info>
 		<method name='test_as_array_produces_token_sequence_roberta' parameters='self'>
 				<method_info nloc='12' complexity='1' token_count='79' nesting_level='1' start_line='54' end_line='66'></method_info>
 			<added_lines>58</added_lines>
 			<deleted_lines>58</deleted_lines>
 		</method>
 		<method name='test_as_array_produces_token_sequence_roberta_sentence_pair' parameters='self'>
 				<method_info nloc='16' complexity='1' token_count='96' nesting_level='1' start_line='68' end_line='83'></method_info>
 			<added_lines>74,83</added_lines>
 			<deleted_lines>74,83</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\data\tokenizers\pretrained_transformer_tokenizer_test.py' new_name='tests\data\tokenizers\pretrained_transformer_tokenizer_test.py'>
 		<file_info nloc='283' complexity='32' token_count='1432'></file_info>
 		<method name='test_token_idx_bert_cased' parameters='self'>
 				<method_info nloc='24' complexity='3' token_count='111' nesting_level='1' start_line='117' end_line='140'></method_info>
 			<added_lines>123,124,125,134</added_lines>
 			<deleted_lines>123,132</deleted_lines>
 		</method>
 		<method name='test_splits_roberta' parameters='self'>
 				<method_info nloc='17' complexity='2' token_count='58' nesting_level='1' start_line='10' end_line='27'></method_info>
 			<added_lines>16</added_lines>
 			<deleted_lines>16</deleted_lines>
 		</method>
 		<method name='test_token_idx_roberta' parameters='self'>
 				<method_info nloc='22' complexity='3' token_count='103' nesting_level='1' start_line='142' end_line='163'></method_info>
 			<added_lines>146</added_lines>
 			<deleted_lines>144</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
