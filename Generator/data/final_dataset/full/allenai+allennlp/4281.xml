<bug_data>
<bug id='4281' author='arthurdeschamps' open_date='2020-05-25T10:35:28Z' closed_time='2020-06-03T00:04:23Z'>
 	<summary>"TypeError: not a sequence" on simple coreference resolution</summary>
 	<description>
 Describe the bug
 I get a TypeError: not a sequence when trying to predict this simple string: "Besides its prominence in sports, Notre Dame is also a large, four-year, highly residential research University, and is consistently ranked among the top twenty universities in the United States  and as a major global university."
 Full stacktrace:
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "/home/arthur/question-generation/models/sg_dqg.py", line 156, in &lt;module&gt;
     preprocess(args.ds)
   File "/home/arthur/question-generation/models/sg_dqg.py", line 124, in preprocess
     coreferences = coreference_resolution(evidences_list)
   File "/home/arthur/question-generation/models/sg_dqg.py", line 74, in coreference_resolution
     document="Besides its prominence in sports, Notre Dame is also a large, four-year, highly residential research University, and is consistently ranked among the top twenty universities in the United States  and as a major global university."
   File "/home/arthur/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/allennlp_models/coref/coref_predictor.py", line 65, in predict
     return self.predict_json({"document": document})
   File "/home/arthur/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/allennlp/predictors/predictor.py", line 48, in predict_json
     return self.predict_instance(instance)
   File "/home/arthur/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/allennlp/predictors/predictor.py", line 171, in predict_instance
     outputs = self._model.forward_on_instance(instance)
   File "/home/arthur/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/allennlp/models/model.py", line 142, in forward_on_instance
     return self.forward_on_instances([instance])[0]
   File "/home/arthur/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/allennlp/models/model.py", line 167, in forward_on_instances
     model_input = util.move_to_device(dataset.as_tensor_dict(), cuda_device)
   File "/home/arthur/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/allennlp/data/batch.py", line 139, in as_tensor_dict
     for field, tensors in instance.as_tensor_dict(lengths_to_use).items():
   File "/home/arthur/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/allennlp/data/instance.py", line 99, in as_tensor_dict
     tensors[field_name] = field.as_tensor(padding_lengths[field_name])
   File "/home/arthur/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/allennlp/data/fields/text_field.py", line 103, in as_tensor
     self._indexed_tokens[indexer_name], indexer_lengths[indexer_name]
   File "/home/arthur/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/allennlp/data/token_indexers/pretrained_transformer_mismatched_indexer.py", line 96, in as_padded_tensor_dict
     offsets_tokens, offsets_padding_lengths, default_value=lambda: (0, 0)
 TypeError: not a sequence
 &lt;/denchmark-code&gt;
 
 To Reproduce
 Run this simple piece of code :
 &lt;denchmark-code&gt;predictor = Predictor.from_path("https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz")
 predictor.predict(
 document="Besides its prominence in sports, Notre Dame is also a large, four-year, highly residential research University, and is consistently ranked among the top twenty universities in the United States  and as a major global university."
 )
 &lt;/denchmark-code&gt;
 
 Expected behavior
 The function should return the proper resolved coreferences.
 System (please complete the following information):
 
 OS: Linux
 Python version: 3.7.7
 AllenNLP version: 1.0.0rc4
 PyTorch version: 1.5.0
 
 	</description>
 	<comments>
 		<comment id='1' author='arthurdeschamps' date='2020-05-26T17:38:47Z'>
 		Hi &lt;denchmark-link:https://github.com/arthurdeschamps&gt;@arthurdeschamps&lt;/denchmark-link&gt;
 , I did some debugging and a found that the root of the issue comes from there being some  values in the  list from your stacktrace, which causes  to fail when trying to turn this list into a tensor because it doesn't know how to handle the  values. These  values come from this line:
 &lt;denchmark-link:https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/pretrained_transformer_tokenizer.py#L360&gt;https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/pretrained_transformer_tokenizer.py#L360&lt;/denchmark-link&gt;
 
 &lt;denchmark-link:https://github.com/dirkgr&gt;@dirkgr&lt;/denchmark-link&gt;
  any ideas how we should fix this?
 		</comment>
 		<comment id='2' author='arthurdeschamps' date='2020-05-26T20:49:13Z'>
 		This is something that has to be handled in the model (or in the original tokenizer).
 Recall: _intra_word_tokenize takes an existing tokenization of a string, and cuts the tokens into word pieces suitable for a transformer. What should happen when the existing tokenization produces tokens that have zero word pieces? I can't answer that in general. It depends on your case.
 If your answer is "That should never happen.", then look at the cases where it happens anyways and find out why. Maybe the original tokenizer produces tokens that are nothing but spaces? Zero-length tokens? But sometimes there is a legitimate reason for this, and you need to handle it somehow downstream.
 		</comment>
 		<comment id='3' author='arthurdeschamps' date='2020-05-26T20:58:09Z'>
 		&lt;denchmark-link:https://github.com/dirkgr&gt;@dirkgr&lt;/denchmark-link&gt;
  in any case, I think we need to improve our error message here. Seems to me like  should raise an exception when a token results in zero word pieces?
 		</comment>
 		<comment id='4' author='arthurdeschamps' date='2020-05-26T21:34:54Z'>
 		&lt;denchmark-link:https://github.com/arthurdeschamps&gt;@arthurdeschamps&lt;/denchmark-link&gt;
  is this case you have extra whitespace in your document:
 &lt;denchmark-code&gt;...he United States  and as...
                    ^^
 &lt;/denchmark-code&gt;
 
 Nevertheless, the error message should be improved here. See &lt;denchmark-link:https://github.com/allenai/allennlp/pull/4291&gt;#4291&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='5' author='arthurdeschamps' date='2020-05-26T21:52:43Z'>
 		
 Seems to me like _intra_word_tokenize should raise an exception when a token results in zero word pieces?
 
 Sometimes this is a case you want to handle specifically. If we throw an exception, you could never deal with that case.
 		</comment>
 		<comment id='6' author='arthurdeschamps' date='2020-05-26T22:15:55Z'>
 		
 Sometimes this is a case you want to handle specifically. If we throw an exception, you could never deal with that case.
 
 My issue with not throwing an exception is that there are several places that rely on this function returning all non-Nonetype offsets. This issue brought to light one of these places.
 mypy should really have caught this before in our CI checks because this is a typing error on our part.
 That said, the other option would be to raise an exception downstream in all of the places that use these offsets when  is encountered. I didn't go that route with &lt;denchmark-link:https://github.com/allenai/allennlp/pull/4291&gt;#4291&lt;/denchmark-link&gt;
  because I couldn't find a single example where a  offset was actually handled.
 		</comment>
 		<comment id='7' author='arthurdeschamps' date='2020-05-26T23:45:53Z'>
 		What are all of those places that expect it?
 		</comment>
 		<comment id='8' author='arthurdeschamps' date='2020-05-26T23:49:01Z'>
 		
 What are all of those places that expect it?
 
 
 https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/pretrained_transformer_mismatched_indexer.py#L96
 https://github.com/allenai/allennlp-models/blob/master/allennlp_models/coref/util.py#L105
 
 		</comment>
 		<comment id='9' author='arthurdeschamps' date='2020-05-26T23:50:09Z'>
 		Can we fix it by adding a fixed embedding to the mismatched embedder for these cases, which gets substituted when there is nothing else to add?
 I'm looking at the other case right now.
 		</comment>
 		<comment id='10' author='arthurdeschamps' date='2020-05-26T23:53:52Z'>
 		For the coref case, it would be pretty messed up if the mention starts or ends in a token that has no word pieces. I'd be OK just letting it crash in that case, or just throwing an exception right there.
 We're losing a little bit of generality if we do this. The proper fix would be to scan forward from the start token until we find one that's not None, and scan backwards from the end token. But I think the case where that's necessary is quite rare, and possibly not worth the complexity.
 		</comment>
 		<comment id='11' author='arthurdeschamps' date='2020-05-27T02:50:19Z'>
 		Might be relevant: &lt;denchmark-link:https://github.com/allenai/allennlp/issues/3779&gt;#3779&lt;/denchmark-link&gt;
 , esp. the part where it was re-opened. Specifically see &lt;denchmark-link:https://github.com/allenai/allennlp/pull/3808/files#r381036253&gt;https://github.com/allenai/allennlp/pull/3808/files#r381036253&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='12' author='arthurdeschamps' date='2020-05-27T02:53:25Z'>
 		
 
 
 allennlp/tests/data/tokenizers/pretrained_transformer_tokenizer_test.py
 
 
          Line 225
       in
       8ff47d3
 
 
 
 
 
 
  def test_intra_word_tokenize_whitespaces(self): 
 
 
 
 
  was supposed to test this behavior.
 		</comment>
 		<comment id='13' author='arthurdeschamps' date='2020-05-29T01:14:47Z'>
 		I proposed a fix at &lt;denchmark-link:https://github.com/allenai/allennlp/pull/4301&gt;#4301&lt;/denchmark-link&gt;
 .
 		</comment>
 		<comment id='14' author='arthurdeschamps' date='2020-06-03T00:05:57Z'>
 		&lt;denchmark-link:https://github.com/allenai/allennlp/pull/4301&gt;#4301&lt;/denchmark-link&gt;
  fixes this issue, in the sense that you don't get an exception anymore. But instead it puts a zero vector into your embeddings. If the model isn't trained for that possibility, anything could happen. There is no guarantee it will give good performance.
 		</comment>
 		<comment id='15' author='arthurdeschamps' date='2020-07-10T10:36:17Z'>
 		I found that upgrading to huggingface transformers 3.0.0 facilitates this error
 		</comment>
 		<comment id='16' author='arthurdeschamps' date='2020-07-10T12:04:39Z'>
 		What do you mean by "facilitates"?
 		</comment>
 		<comment id='17' author='arthurdeschamps' date='2020-07-10T18:07:54Z'>
 		Sorry I should have given you more to go off - really big fan of you guys!
 I upgraded to transformers==3.0.1 without changing any code and got the same type error. When I reverted versions of transformers&lt;3.0.0 the same code worked fine.
 		</comment>
 		<comment id='18' author='arthurdeschamps' date='2020-07-13T09:52:23Z'>
 		I just tried running the code from the issue description at the top on the latest master of allennlp and allennlp-models, and transformers==3.0.2. I had no problems. What are you running?
 
 really big fan of you guys!
 
 Thanks! We appreciate it :-)
 		</comment>
 		<comment id='19' author='arthurdeschamps' date='2020-07-13T09:53:09Z'>
 		Actually, if you could put repro steps into the description of a new issue, that would be great. Then we don't have to have a discussion on a closed issue, which might get lost easily.
 		</comment>
 		<comment id='20' author='arthurdeschamps' date='2020-07-14T07:51:52Z'>
 		That's strange. I think transformers introduced a few tokenizer fixes between 3.0.1 and 3.0.2, but might also be something else. I can try repro this in the next week or two as have some hard deadlines approaching and have not recorded all the steps I took to fix this error as well as I could have...
 		</comment>
 	</comments>
 </bug>
<commit id='fc47bf6ae5c0df6d473103d459b75fa7edbdd979' author='Dirk Groeneveld' date='2020-06-02 17:04:22-07:00'>
 	<dmm_unit complexity='0.6571428571428571' interfacing='1.0' size='0.02857142857142857'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>19</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='allennlp\data\token_indexers\pretrained_transformer_mismatched_indexer.py' new_name='allennlp\data\token_indexers\pretrained_transformer_mismatched_indexer.py'>
 		<file_info nloc='94' complexity='15' token_count='527'></file_info>
 		<method name='tokens_to_indices' parameters='self,Vocabulary'>
 				<method_info nloc='12' complexity='6' token_count='133' nesting_level='1' start_line='62' end_line='79'></method_info>
 			<added_lines>66,67,68,69,70</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='allennlp\modules\token_embedders\pretrained_transformer_mismatched_embedder.py' new_name='allennlp\modules\token_embedders\pretrained_transformer_mismatched_embedder.py'>
 		<file_info nloc='75' complexity='3' token_count='238'></file_info>
 		<modified_lines>
 			<added_lines>89,90,91</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\modules\token_embedders\pretrained_transformer_mismatched_embedder_test.py' new_name='tests\modules\token_embedders\pretrained_transformer_mismatched_embedder_test.py'>
 		<file_info nloc='113' complexity='9' token_count='1036'></file_info>
 		<method name='test_long_sequence_splitting_end_to_end' parameters='self'>
 				<method_info nloc='37' complexity='3' token_count='343' nesting_level='1' start_line='56' end_line='101'></method_info>
 			<added_lines>101</added_lines>
 			<deleted_lines>98</deleted_lines>
 		</method>
 		<method name='test_token_without_wordpieces' parameters='self'>
 				<method_info nloc='34' complexity='3' token_count='324' nesting_level='1' start_line='102' end_line='141'></method_info>
 			<added_lines>102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
