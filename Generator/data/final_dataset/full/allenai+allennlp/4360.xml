<bug_data>
<bug id='4360' author='koren-v' open_date='2020-06-15T14:10:01Z' closed_time='2020-11-02T22:21:23Z'>
 	<summary>Can't get correct output using .saliency_interpret_from_json() from Interpret</summary>
 	<description>
 Hi, I'm trying to get the importance of each word in a sentence while sentiment classification but always get the same result for different inputs. The Code:
 &lt;denchmark-code&gt;from allennlp.predictors.predictor import Predictor
 import allennlp_models.classification
 from allennlp.interpret.saliency_interpreters import (
     SaliencyInterpreter,
     SimpleGradient,
     SmoothGradient,
     IntegratedGradient,
 )
 
 predictor = Predictor.from_path("https://storage.googleapis.com/allennlp-public-models/sst-roberta-large-2020.06.08.tar.gz")
 simple_grad = SimpleGradient(predictor)
 
 input_json = {'sentence' : "a very well-made, funny and entertaining picture."}
 
 simple_grad.saliency_interpret_from_json(input_json)
 &lt;/denchmark-code&gt;
 
 gives:
 &lt;denchmark-code&gt;{'instance_1': {'grad_input_1': [1.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0]}}
 &lt;/denchmark-code&gt;
 
 So what can be the reason of this problem?
 	</description>
 	<comments>
 		<comment id='1' author='koren-v' date='2020-06-15T16:00:04Z'>
 		No idea what's causing the issue yet, but I have confirmed that this is a bug.  I get the correct output with rc3, but it's broken with rc6.
 		</comment>
 		<comment id='2' author='koren-v' date='2020-06-15T17:55:44Z'>
 		&lt;denchmark-link:https://github.com/matt-gardner&gt;@matt-gardner&lt;/denchmark-link&gt;
  Thanks for your answer.
 		</comment>
 		<comment id='3' author='koren-v' date='2020-06-15T19:37:32Z'>
 		&lt;denchmark-link:https://github.com/matt-gardner&gt;@matt-gardner&lt;/denchmark-link&gt;
  Is it possible way to load this model in rc3? Because using the same code with this version gives:
 &lt;denchmark-code&gt;ConfigurationError: sst_tokens not in acceptable choices for validation_dataset_reader.type: ['conll2003', 'interleaving', 'sequence_tagging', 'sharded', 'babi', 'text_classification_json']. You should either use the --include-package flag to make sure the correct module is loaded, or use a fully qualified class name in your config file like {"model": "my_module.models.MyModel"} to have it imported automatically.
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='4' author='koren-v' date='2020-06-15T19:59:58Z'>
 		Yeah, just include this line in your script: from allennlp_models import sentiment.
 		</comment>
 		<comment id='5' author='koren-v' date='2020-06-30T21:54:05Z'>
 		Possibly related: &lt;denchmark-link:https://github.com/allenai/allennlp-models/pull/85&gt;allenai/allennlp-models#85&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='6' author='koren-v' date='2020-08-18T16:18:34Z'>
 		&lt;denchmark-link:https://github.com/matt-gardner&gt;@matt-gardner&lt;/denchmark-link&gt;
  this is just a friendly ping to make sure you haven't forgotten about this issue 
 		</comment>
 		<comment id='7' author='koren-v' date='2020-08-18T19:42:51Z'>
 		I thought the issue might have been resolved with recent changes to the SST model / tokenization stuff.  But I just ran the model on master (both this and the models repo), and had the same issue.  This is still not resolved.
 We're giving a tutorial at EMNLP on interpreting predictions, and this should definitely be fixed before then.  I'm putting this into the 1.2 milestone, but only so we don't forget about it; it really should be in a 1.3 or a 2.1 milestone, or something, but those don't exist yet.
 		</comment>
 		<comment id='8' author='koren-v' date='2020-10-09T23:10:37Z'>
 		Just because it hasn't been mentioned yet: The problem is that it takes the gradients at the top of the transformer, not the bottom, when using the mismatched embedder. Since the pooler takes only the first token ([CLS]), all the gradients are there.
 		</comment>
 		<comment id='9' author='koren-v' date='2020-10-15T21:40:37Z'>
 		&lt;denchmark-link:https://github.com/matt-gardner&gt;@matt-gardner&lt;/denchmark-link&gt;
 , do you have an idea of when you will get to this?
 		</comment>
 		<comment id='10' author='koren-v' date='2020-10-15T22:15:59Z'>
 		I have some thoughts on how to fix this.  If I'm lucky, I can maybe get to it tomorrow; if not, I'm not sure, but I might be able to make it happen next week.
 		</comment>
 		<comment id='11' author='koren-v' date='2020-11-02T23:19:09Z'>
 		I couldn't get the demo to work end-to-end, but I have confirmed that the example from in here works now as expected.
 		</comment>
 		<comment id='12' author='koren-v' date='2020-11-07T00:01:14Z'>
 		&lt;denchmark-link:https://github.com/dirkgr&gt;@dirkgr&lt;/denchmark-link&gt;
  can you share which example works?  i am still having trouble getting this working.  i tried 1.0.0rc3 (as suggested way up above) and see this error:  
 this still seems broken.
 i tried 1.1.0, 1.2.0, and master and i see the incorrect array [1.0, 0.0, 0.0, ...].
 here is my code:  &lt;denchmark-link:https://github.com/data-science-on-aws/workshop/blob/1958164/07_train/wip/99_AllenNLP_RoBERTa_Prediction.ipynb&gt;https://github.com/data-science-on-aws/workshop/blob/1958164/07_train/wip/99_AllenNLP_RoBERTa_Prediction.ipynb&lt;/denchmark-link&gt;
 
 any help would be appreciated!  hoping to get a working demo of this soon.
 should we re-open this?
 		</comment>
 		<comment id='13' author='koren-v' date='2020-11-07T01:08:07Z'>
 		You have to install  from both allennlp and allennlp-models. Then copy-and-paste the code from the description above. When I do this, I see reasonable numbers. I also made a test for this here: &lt;denchmark-link:https://github.com/allenai/allennlp-models/pull/163&gt;allenai/allennlp-models#163&lt;/denchmark-link&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='b7cec515448f46daec9a76551d0a3342ef51b27b' author='Matt Gardner' date='2020-11-02 14:21:23-08:00'>
 	<dmm_unit complexity='0.9464285714285714' interfacing='0.35714285714285715' size='0.875'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>8,11,12,13,14,15,16,17,18,41</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='allennlp\interpret\saliency_interpreters\integrated_gradient.py' new_name='allennlp\interpret\saliency_interpreters\integrated_gradient.py'>
 		<file_info nloc='70' complexity='16' token_count='539'></file_info>
 		<method name='_integrate_gradients' parameters='self,Instance'>
 				<method_info nloc='28' complexity='7' token_count='212' nesting_level='1' start_line='72' end_line='116'></method_info>
 			<added_lines>79,80,87,89,92,93,108,109</added_lines>
 			<deleted_lines>78,81</deleted_lines>
 		</method>
 		<method name='_register_forward_hook' parameters='self,int,List'>
 				<method_info nloc='5' complexity='1' token_count='38' nesting_level='1' start_line='41' end_line='61'></method_info>
 			<added_lines>42,54,59,60,61</added_lines>
 			<deleted_lines>41,53,58,60,61</deleted_lines>
 		</method>
 		<method name='_register_forward_hook.forward_hook' parameters='module,inputs,output'>
 				<method_info nloc='4' complexity='2' token_count='43' nesting_level='2' start_line='50' end_line='56'></method_info>
 			<added_lines>54</added_lines>
 			<deleted_lines>53</deleted_lines>
 		</method>
 		<method name='_register_hooks' parameters='self,int,List,List'>
 				<method_info nloc='9' complexity='1' token_count='74' nesting_level='1' start_line='42' end_line='70'></method_info>
 			<added_lines>42,54,59,60,61,62,63,64,65,67,68,69,70</added_lines>
 			<deleted_lines>53,58,60,61,70</deleted_lines>
 		</method>
 		<method name='_register_hooks.get_token_offsets' parameters='module,inputs,outputs'>
 				<method_info nloc='4' complexity='2' token_count='29' nesting_level='2' start_line='59' end_line='62'></method_info>
 			<added_lines>59,60,61,62</added_lines>
 			<deleted_lines>60,61</deleted_lines>
 		</method>
 		<method name='_register_hooks.forward_hook' parameters='module,inputs,output'>
 				<method_info nloc='4' complexity='2' token_count='39' nesting_level='2' start_line='51' end_line='57'></method_info>
 			<added_lines>54</added_lines>
 			<deleted_lines>53</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>5</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='allennlp\interpret\saliency_interpreters\saliency_interpreter.py' new_name='allennlp\interpret\saliency_interpreters\saliency_interpreter.py'>
 		<file_info nloc='49' complexity='3' token_count='229'></file_info>
 		<method name='_aggregate_token_embeddings' parameters=''>
 				<method_info nloc='2' complexity='1' token_count='19' nesting_level='1' start_line='41' end_line='42'></method_info>
 			<added_lines>41,42</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1,2,3,4,5,8,39,40,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='allennlp\interpret\saliency_interpreters\simple_gradient.py' new_name='allennlp\interpret\saliency_interpreters\simple_gradient.py'>
 		<file_info nloc='50' complexity='9' token_count='391'></file_info>
 		<method name='_register_hooks.get_token_offsets' parameters='module,inputs,outputs'>
 				<method_info nloc='4' complexity='2' token_count='29' nesting_level='2' start_line='67' end_line='70'></method_info>
 			<added_lines>67,68,69,70</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='saliency_interpret_from_json' parameters='self,JsonDict'>
 				<method_info nloc='25' complexity='5' token_count='214' nesting_level='1' start_line='18' end_line='55'></method_info>
 			<added_lines>27,28,29,30,32,34,35,39,40,41,49</added_lines>
 			<deleted_lines>24,25,26,30,32,43,51</deleted_lines>
 		</method>
 		<method name='_register_forward_hook' parameters='self,List'>
 				<method_info nloc='5' complexity='1' token_count='34' nesting_level='1' start_line='51' end_line='64'></method_info>
 			<added_lines>57</added_lines>
 			<deleted_lines>51,59,61,62,64</deleted_lines>
 		</method>
 		<method name='_register_hooks.forward_hook' parameters='module,inputs,output'>
 				<method_info nloc='2' complexity='1' token_count='28' nesting_level='2' start_line='64' end_line='65'></method_info>
 			<added_lines>65</added_lines>
 			<deleted_lines>64</deleted_lines>
 		</method>
 		<method name='_register_hooks' parameters='self,List,List'>
 				<method_info nloc='9' complexity='1' token_count='70' nesting_level='1' start_line='57' end_line='78'></method_info>
 			<added_lines>57,65,67,68,69,70,72,73,74,75,76,77,78</added_lines>
 			<deleted_lines>59,61,62,64</deleted_lines>
 		</method>
 		<method name='_register_forward_hook.forward_hook' parameters='module,inputs,output'>
 				<method_info nloc='2' complexity='1' token_count='32' nesting_level='2' start_line='58' end_line='59'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>59</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>5</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='allennlp\nn\util.py' new_name='allennlp\nn\util.py'>
 		<file_info nloc='1534' complexity='153' token_count='7645'></file_info>
 		<method name='get_token_offsets_from_text_field_inputs' parameters=''>
 				<method_info nloc='2' complexity='1' token_count='9' nesting_level='0' start_line='1808' end_line='1809'></method_info>
 			<added_lines>1808,1809</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='find_text_field_embedder' parameters='Module'>
 				<method_info nloc='11' complexity='3' token_count='52' nesting_level='0' start_line='1748' end_line='1759'></method_info>
 			<added_lines>1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='find_embedding_layer' parameters='Module'>
 				<method_info nloc='33' complexity='11' token_count='188' nesting_level='0' start_line='1762' end_line='1805'></method_info>
 			<added_lines>1774,1782,1783,1784,1785,1786,1787</added_lines>
 			<deleted_lines>1766,1767,1768,1769,1770,1771,1772,1773,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1760,1761,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='allennlp\predictors\predictor.py' new_name='allennlp\predictors\predictor.py'>
 		<file_info nloc='288' complexity='33' token_count='1317'></file_info>
 		<method name='_register_embedding_gradient_hooks.get_token_offsets' parameters='module,inputs,outputs'>
 				<method_info nloc='4' complexity='2' token_count='31' nesting_level='2' start_line='178' end_line='181'></method_info>
 			<added_lines>178,179,180,181</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,Model,DatasetReader,bool'>
 				<method_info nloc='7' complexity='2' token_count='71' nesting_level='1' start_line='31' end_line='37'></method_info>
 			<added_lines>37</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_register_embedding_gradient_hooks.hook_layers' parameters='module,grad_in,grad_out'>
 				<method_info nloc='12' complexity='2' token_count='110' nesting_level='2' start_line='153' end_line='176'></method_info>
 			<added_lines>154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_register_embedding_gradient_hooks' parameters='self,embedding_gradients'>
 				<method_info nloc='9' complexity='1' token_count='60' nesting_level='1' start_line='138' end_line='188'></method_info>
 			<added_lines>140,141,146,147,148,149,150,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,183,184,185,187,188</added_lines>
 			<deleted_lines>145,147,149,150</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>2,9,22,23</added_lines>
 			<deleted_lines>135,136,137</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
