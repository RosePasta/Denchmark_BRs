<bug_data>
<bug id='4839' author='dcfidalgo' open_date='2020-12-04T16:17:48Z' closed_time='2020-12-16T02:09:45Z'>
 	<summary>Superfluous warning when extending the vocab in the `Embedding`</summary>
 	<description>
 &lt;denchmark-h:h2&gt;Checklist&lt;/denchmark-h&gt;
 
 
  I have verified that the issue exists against the master branch of AllenNLP.
  I have read the relevant section in the contribution guide on reporting bugs.
  I have checked the issues list for similar or identical bug reports.
  I have checked the pull requests list for existing proposed fixes.
  I have checked the CHANGELOG and the commit log to find out if the bug was already fixed in the master branch.
  I have included in the "Description" section below a traceback from any exceptions related to this bug.
  I have included in the "Related issues or possible duplicates" section beloew all related issues and possible duplicate issues (If there are none, check this box anyway).
  I have included in the "Environment" section below the name of the operating system and Python version that I was using when I discovered this bug.
  I have included in the "Environment" section below the output of pip freeze.
  I have included in the "Steps to reproduce" section below a minimally reproducible example.
 
 &lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;
 
 If one creates an allennlp.modules.token_embedders.embedding.Embedding without a pretrained_file, you still get a warning when extending the vocab that no pretrained_file is found. I would expect the warning only to trigger if one specified a pretrained_file when creating the Embedding or when an extension_pretrained_file is passed on to Embedding.extend_vocab.
 I would be more than happy to provide a PR if you think this is actually and issue and should be addressed.
 
 Python traceback:
 
 WARNING:root:Embedding at model_path, None cannot locate the pretrained_file.  If you are fine-tuning and want to use using pretrained_file for embedding extension, please pass the mapping by --embedding-sources argument.
 
 
 
 
 &lt;denchmark-h:h2&gt;Related issues or possible duplicates&lt;/denchmark-h&gt;
 
 
 None
 
 &lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;
 
 OS: Ubuntu 20.04
 Python version: 3.8.0
 
 Output of pip freeze:
 
 attrs==20.3.0
 blis==0.7.3
 boto3==1.16.29
 botocore==1.19.29
 catalogue==1.0.0
 certifi==2020.11.8
 chardet==3.0.4
 click==7.1.2
 cymem==2.0.4
 dataclasses==0.6
 filelock==3.0.12
 future==0.18.2
 h5py==3.1.0
 idna==2.10
 iniconfig==1.1.1
 jmespath==0.10.0
 joblib==0.17.0
 jsonnet==0.17.0
 jsonpickle==1.4.2
 murmurhash==1.0.4
 nltk==3.5
 numpy==1.19.4
 overrides==3.1.0
 packaging==20.7
 plac==1.1.3
 pluggy==0.13.1
 preshed==3.0.4
 protobuf==3.14.0
 py==1.9.0
 pyparsing==2.4.7
 pytest==6.1.2
 python-dateutil==2.8.1
 regex==2020.11.13
 requests==2.25.0
 s3transfer==0.3.3
 sacremoses==0.0.43
 scikit-learn==0.23.2
 scipy==1.5.4
 sentencepiece==0.1.91
 six==1.15.0
 spacy==2.3.4
 srsly==1.0.4
 tensorboardX==2.1
 thinc==7.4.3
 threadpoolctl==2.1.0
 tokenizers==0.9.3
 toml==0.10.2
 torch==1.7.0
 tqdm==4.54.0
 transformers==3.5.1
 typing-extensions==3.7.4.3
 urllib3==1.26.2
 wasabi==0.8.0
 
 
 
 &lt;denchmark-h:h2&gt;Steps to reproduce&lt;/denchmark-h&gt;
 
 
 Example source:
 
 from allennlp.data import Token, Instance, Vocabulary
 from allennlp.data.fields import TextField
 from allennlp.data.token_indexers import SingleIdTokenIndexer
 from allennlp.modules.token_embedders.embedding import Embedding
 
 instance = Instance({"token": TextField([Token("test")], {"tokens": SingleIdTokenIndexer()})})
 vocab = Vocabulary.from_instances([instance])
 
 instance2 = Instance({"token": TextField([Token("this")], {"tokens": SingleIdTokenIndexer()})})
 vocab2 = Vocabulary.from_instances([instance, instance2])
 
 embedder = Embedding(1, vocab=vocab)
 embedder.extend_vocab(vocab2)
 
 
 	</description>
 	<comments>
 		<comment id='1' author='dcfidalgo' date='2020-12-04T17:43:12Z'>
 		&lt;denchmark-link:https://github.com/dcfidalgo&gt;@dcfidalgo&lt;/denchmark-link&gt;
  Hi, thanks for catching this! You are most welcome to submit a PR!
 		</comment>
 	</comments>
 </bug>
<commit id='832901e8cb00ab0f415649bb2e52ffbab4a5ae5d' author='David Fidalgo' date='2020-12-15 18:09:44-08:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>40</added_lines>
 			<deleted_lines>40</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='allennlp\modules\token_embedders\embedding.py' new_name='allennlp\modules\token_embedders\embedding.py'>
 		<file_info nloc='507' complexity='33' token_count='2228'></file_info>
 		<modified_lines>
 			<added_lines>256,288,289,290,292,293,294,295,296,297,298,299</added_lines>
 			<deleted_lines>256,288,289,290,291,292,293,294,295,296,297,299,300</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
