<bug_data>
<bug id='430' author='chunfuchen' open_date='2017-09-26T16:10:09Z' closed_time='2017-09-26T16:26:44Z'>
 	<summary>Distributed Learning on ResNet</summary>
 	<description>
 I follow the instruction to setup distributed learning environment and modify the code in imagenet-resnet.py for distribution learning.
 &lt;denchmark-link:http://tensorpack.readthedocs.io/en/latest/modules/train.html#tensorpack.train.DistributedTrainerReplicated&gt;http://tensorpack.readthedocs.io/en/latest/modules/train.html#tensorpack.train.DistributedTrainerReplicated&lt;/denchmark-link&gt;
 
 I encounter the issue when the worker is trying to build the graph model. May I direct use imagenet_resnet_utils.py without modification? Or the changes are needed? (Details are provided below)
 Any suggestion is appreciated.
 Here is error trace:
 &lt;denchmark-code&gt;[0926 11:56:44 @distributed.py:73] My role in the cluster: job=worker, task=0
 [0926 11:56:44 @input_source.py:235] Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...
 [0926 11:56:44 @multigpu.py:87] Building graph for training tower 0...
 [0926 11:56:44 @registry.py:121] tower0/conv0 input: [None, 3, 224, 224]
 Traceback (most recent call last):
   File "examples/ResNet/imagenet-resnet-dist.py", line 169, in &lt;module&gt;
     DistributedTrainerReplicated(config, server).train()
   File "/mnt/nvme1/chenrich/Developer/tensorpack-0.5.0/tensorpack/train/distributed.py", line 92, in __init__
     super(DistributedTrainerReplicated, self).__init__(config)
   File "/mnt/nvme1/chenrich/Developer/tensorpack-0.5.0/tensorpack/train/base.py", line 93, in __init__
     self._setup()   # subclass will setup the graph and InputSource
   File "/mnt/nvme1/chenrich/Developer/tensorpack-0.5.0/tensorpack/train/distributed.py", line 215, in _setup
     use_vs=[True] * self.config.nr_tower)  # open vs at each tower
   File "/mnt/nvme1/chenrich/Developer/tensorpack-0.5.0/tensorpack/train/multigpu.py", line 94, in build_on_multi_tower
     ret.append(func())
   File "/mnt/nvme1/chenrich/Developer/tensorpack-0.5.0/tensorpack/train/distributed.py", line 213, in &lt;lambda&gt;
     self.model, self._input_source),
   File "/mnt/nvme1/chenrich/Developer/tensorpack-0.5.0/tensorpack/train/multigpu.py", line 113, in _build_graph_get_grads
     model.build_graph(input)
   File "/mnt/nvme1/chenrich/Developer/tensorpack-0.5.0/tensorpack/graph_builder/model_desc.py", line 122, in build_graph
     self._build_graph(inputs)
   File "examples/ResNet/imagenet-resnet-dist.py", line 67, in _build_graph
     preresnet_group if self.preact else resnet_group, self.block_func)
   File "/mnt/nvme1/chenrich/Developer/tensorpack-0.5.0/examples/ResNet/imagenet_resnet_utils.py", line 204, in resnet_backbone
     .Conv2D('conv0', 64, 7, stride=2, nl=BNReLU)
   File "/mnt/nvme1/chenrich/Developer/tensorpack-0.5.0/tensorpack/models/linearwrap.py", line 48, in f
     ret = layer(name, self._t, *args, **kwargs)
   File "/mnt/nvme1/chenrich/Developer/tensorpack-0.5.0/tensorpack/models/registry.py", line 124, in wrapped_func
     outputs = func(*args, **actual_args)
   File "/mnt/nvme1/chenrich/Developer/tensorpack-0.5.0/tensorpack/models/conv2d.py", line 61, in Conv2D
     W = tf.get_variable('W', filter_shape, initializer=W_init)
   File "/home/chenrich/.tensorflowEnvPy3Alt/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py", line 1065, in get_variable
     use_resource=use_resource, custom_getter=custom_getter)
   File "/home/chenrich/.tensorflowEnvPy3Alt/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py", line 962, in get_variable
     use_resource=use_resource, custom_getter=custom_getter)
   File "/home/chenrich/.tensorflowEnvPy3Alt/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py", line 360, in get_variable
     validate_shape=validate_shape, use_resource=use_resource)
   File "/home/chenrich/.tensorflowEnvPy3Alt/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py", line 1405, in wrapped_custom_getter
     *args, **kwargs)
   File "/mnt/nvme1/chenrich/Developer/tensorpack-0.5.0/tensorpack/train/utility.py", line 36, in __call__
     return getter(name, *args, **kwargs)
   File "/mnt/nvme1/chenrich/Developer/tensorpack-0.5.0/tensorpack/train/utility.py", line 33, in __call__
     collections.remove(tf.GraphKeys.GLOBAL_VARIABLES)
 KeyError: 'variables'
 &lt;/denchmark-code&gt;
 
 I launch my codes as:
 
 
 The modified code is available at
 &lt;denchmark-link:https://gist.github.com/chunfuchen/d8f6db7183ba94ac416948029c366387&gt;https://gist.github.com/chunfuchen/d8f6db7183ba94ac416948029c366387&lt;/denchmark-link&gt;
 
 The modifications are:
 
 TrainCfg: wrapper dataflow into data. (L117)
 
 return TrainConfig(
         model=model,
         data=QueueInput(dataset_train),
         # dataflow=dataset_train,
         callbacks=callbacks,
         steps_per_epoch=5000,
         max_epoch=110,
         nr_tower=nr_tower
     )
 
 Setup Distributed Learners. (Line 162)
 
 cluster_spec = tf.train.ClusterSpec({
             'ps': ['localhost:2222', 'localhost:2232'],
             'worker': ['localhost:2223', 'localhost:2233']
         })
         server = tf.train.Server(
             cluster_spec, job_name=args.job, 
             task_index=args.task, config=get_default_sess_config())
         DistributedTrainerReplicated(config, server).train()```
 Note: I was testing on single machine but eventually I will deploy on a cluster.
 	</description>
 	<comments>
 	</comments>
 </bug>
<commit id='c8028236d8e061eb23da98770761370acfe76bce' author='Yuxin Wu' date='2017-09-26 09:26:27-07:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorpack\train\distributed.py' new_name='tensorpack\train\distributed.py'>
 		<file_info nloc='231' complexity='42' token_count='1664'></file_info>
 		<method name='_setup' parameters='self'>
 				<method_info nloc='38' complexity='5' token_count='323' nesting_level='1' start_line='194' end_line='242'></method_info>
 			<added_lines>207,208,209,210,211,212,213,214</added_lines>
 			<deleted_lines>208,209,210,211,212,213,214,215,216</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>16</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
