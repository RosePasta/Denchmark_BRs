<bug_data>
<bug id='1040' author='elmirador' open_date='2019-01-09T10:57:57Z' closed_time='2019-01-10T07:05:48Z'>
 	<summary>Bug Report: BilinearUpSample() only works when channels=1</summary>
 	<description>
 &lt;denchmark-h:h3&gt;1. What you did:&lt;/denchmark-h&gt;
 
 I wrote a piece of code to test the behavior of BilinearUpSample() layer:
 &lt;denchmark-link:https://gist.github.com/elmirador/4816836ca5b35481a43b07109f6dae63&gt;https://gist.github.com/elmirador/4816836ca5b35481a43b07109f6dae63&lt;/denchmark-link&gt;
 
 The layer's code is directly copied from models/pool.py from 
 
 
 tensorpack/tensorpack/models/pool.py
 
 
         Lines 145 to 196
       in
       7bf5581
 
 
 
 
 
 
  def BilinearUpSample(x, shape): 
 
 
 
  """ 
 
 
 
      Deterministic bilinearly-upsample the input images. 
 
 
 
      It is implemented by deconvolution with "BilinearFiller" in Caffe. 
 
 
 
      It is aimed to mimic caffe behavior. 
 
 
 
   
 
 
 
      Args: 
 
 
 
          x (tf.Tensor): a NHWC tensor 
 
 
 
          shape (int): the upsample factor 
 
 
 
   
 
 
 
      Returns: 
 
 
 
          tf.Tensor: a NHWC tensor. 
 
 
 
      """ 
 
 
 
  log_deprecated("BilinearUpsample", "Please implement it in your own code instead!", "2019-03-01") 
 
 
 
  inp_shape = x.shape.as_list() 
 
 
 
  ch = inp_shape[3] 
 
 
 
  assert ch is not None 
 
 
 
  
 
 
 
  shape = int(shape) 
 
 
 
  filter_shape = 2 * shape 
 
 
 
  
 
 
 
  def bilinear_conv_filler(s): 
 
 
 
  """ 
 
 
 
          s: width, height of the conv filter 
 
 
 
          https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/include/caffe/filler.hpp#L219-L268 
 
 
 
          """ 
 
 
 
  f = np.ceil(float(s) / 2) 
 
 
 
  c = float(2 * f - 1 - f % 2) / (2 * f) 
 
 
 
  ret = np.zeros((s, s), dtype='float32') 
 
 
 
  for x in range(s): 
 
 
 
  for y in range(s): 
 
 
 
  ret[x, y] = (1 - abs(x / f - c)) * (1 - abs(y / f - c)) 
 
 
 
  return ret 
 
 
 
  w = bilinear_conv_filler(filter_shape) 
 
 
 
  w = np.repeat(w, ch * ch).reshape((filter_shape, filter_shape, ch, ch)) 
 
 
 
  
 
 
 
  weight_var = tf.constant(w, tf.float32, 
 
 
 
  shape=(filter_shape, filter_shape, ch, ch), 
 
 
 
  name='bilinear_upsample_filter') 
 
 
 
  x = tf.pad(x, [[0, 0], [shape - 1, shape - 1], [shape - 1, shape - 1], [0, 0]], mode='SYMMETRIC') 
 
 
 
  out_shape = tf.shape(x) * tf.constant([1, shape, shape, 1], tf.int32) 
 
 
 
  deconv = tf.nn.conv2d_transpose(x, weight_var, out_shape, 
 
 
 
                                      [1, shape, shape, 1], 'SAME') 
 
 
 
  edge = shape * (shape - 1) 
 
 
 
  deconv = deconv[:, edge:-edge, edge:-edge, :] 
 
 
 
  
 
 
 
  if inp_shape[1]: 
 
 
 
  inp_shape[1] *= shape 
 
 
 
  if inp_shape[2]: 
 
 
 
  inp_shape[2] *= shape 
 
 
 
  deconv.set_shape(inp_shape) 
 
 
 
  return deconv 
 
 
 
 
 
 &lt;denchmark-h:h3&gt;2. What you observed:&lt;/denchmark-h&gt;
 
 Original Image:
 &lt;denchmark-link:https://user-images.githubusercontent.com/3897907/50889932-2437c380-1434-11e9-92ff-25babf24f31d.png&gt;&lt;/denchmark-link&gt;
 
 Upsampled Image (in color):
 &lt;denchmark-link:https://user-images.githubusercontent.com/3897907/50890247-c6f04200-1434-11e9-983c-5db985f61fdd.png&gt;&lt;/denchmark-link&gt;
 
 Read the original image as grayscale, then upsample:
 &lt;denchmark-link:https://user-images.githubusercontent.com/3897907/50890657-9230ba80-1435-11e9-870d-ce8c59c4b9f0.png&gt;&lt;/denchmark-link&gt;
 
 The reason for this behavior is that conv2d_tranpose() is a convolution. It convolutes on all channels instead of one single channel while bilinear upsampling is a channel-wise operation. To fix it just apply the convolution channel-wise.
 I do think it's a good idea to use Caffe's version of bilinear upsampling instead of the one in TF. Sad to see it's going to be deprecated.
 The CaffeBilinearUpSample() in the HED example should also be corrected.
 &lt;denchmark-h:h3&gt;3. Your environment:&lt;/denchmark-h&gt;
 
 
 Python version: 3.5
 TF version: 1.4.0
 Tensorpack version: 0.9.0.1-master (ac9ac2a)
 
 	</description>
 	<comments>
 		<comment id='1' author='elmirador' date='2019-01-09T17:49:41Z'>
 		Thanks a lot for finding this!
 A great example of what I wrote here: &lt;denchmark-link:https://tensorpack.readthedocs.io/tutorial/symbolic.html#use-other-symbolic-libraries&gt;https://tensorpack.readthedocs.io/tutorial/symbolic.html#use-other-symbolic-libraries&lt;/denchmark-link&gt;
 
 
 It’s best to not trust others’ layers!
 For non-standard layers that’s not included in TensorFlow or Tensorpack, it’s best to implement them yourself. Non-standard layers often do not have a mathematical definition that people all agree on, and different people can implement it differently. Also, deep learning models on github often have bugs, especially when there is no reproduced experiments with the code.
 For your own good, it’s best to implement the layers yourself. This is also why Tensorpack does not contain non-standard layers.
 
  was made un-public and removed from documentation (&lt;denchmark-link:https://tensorpack.readthedocs.io/modules/models.html&gt;https://tensorpack.readthedocs.io/modules/models.html&lt;/denchmark-link&gt;
 ) a while ago because it's a non-standard layer and we should not provide an implementation for it. But it's good to know the bug!
 		</comment>
 		<comment id='2' author='elmirador' date='2019-01-09T18:13:33Z'>
 		New implementation:
 def BilinearUpSample(x, shape):
     """
     Deterministic bilinearly-upsample the input images.
     It is implemented by deconvolution with "BilinearFiller" in Caffe.
     It is aimed to mimic caffe behavior.
     Args:
         x (tf.Tensor): a NHWC tensor
         shape (int): the upsample factor
     Returns:
         tf.Tensor: a NHWC tensor.
     """
     #log_deprecated("BilinearUpsample", "Please implement it in your own code instead!", "2019-03-01")
     inp_shape = x.shape.as_list()
     ch = inp_shape[3]
     assert ch is not None
 
     shape = int(shape)
     filter_shape = 2 * shape
 
     def bilinear_conv_filler(s):
         """
         s: width, height of the conv filter
         https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/include/caffe/filler.hpp#L219-L268
         """
         f = np.ceil(float(s) / 2)
         c = float(2 * f - 1 - f % 2) / (2 * f)
         ret = np.zeros((s, s), dtype='float32')
         for x in range(s):
             for y in range(s):
                 ret[x, y] = (1 - abs(x / f - c)) * (1 - abs(y / f - c))
         return ret
     w = bilinear_conv_filler(filter_shape)
     w = np.repeat(w, ch * 1).reshape((filter_shape, filter_shape, ch, 1))
 
     weight_var = tf.constant(w, tf.float32,
                              shape=(filter_shape, filter_shape, ch, 1),
                              name='bilinear_upsample_filter')
     x = tf.pad(x, [[0, 0], [shape - 1, shape - 1], [shape - 1, shape - 1], [0, 0]], mode='SYMMETRIC')
     out_shape = tf.shape(x) * tf.constant([1, shape, shape, 1], tf.int32)
 
     @tf.custom_gradient
     def depthwise_deconv(x):
         ret = tf.nn.depthwise_conv2d_native_backprop_input(
             out_shape, weight_var, x, [1, shape, shape, 1], padding='SAME')
         def grad(dy):
             return tf.nn.depthwise_conv2d(dy, weight_var, [1, shape, shape, 1], padding='SAME')
         return ret, grad
 
     deconv = depthwise_deconv(x)
 
     edge = shape * (shape - 1)
     deconv = deconv[:, edge:-edge, edge:-edge, :]
 
     if inp_shape[1]:
         inp_shape[1] *= shape
     if inp_shape[2]:
         inp_shape[2] *= shape
     deconv.set_shape(inp_shape)
     return deconv
 I haven't verified the backward is correct, but forward seems right now.
 		</comment>
 		<comment id='3' author='elmirador' date='2019-01-10T06:16:33Z'>
 		
 Thanks a lot for finding this!
 A great example of what I wrote here: https://tensorpack.readthedocs.io/tutorial/symbolic.html#use-other-symbolic-libraries
 
 It’s best to not trust others’ layers!
 For non-standard layers that’s not included in TensorFlow or Tensorpack, it’s best to implement them yourself. Non-standard layers often do not have a mathematical definition that people all agree on, and different people can implement it differently. Also, deep learning models on github often have bugs, especially when there is no reproduced experiments with the code.
 For your own good, it’s best to implement the layers yourself. This is also why Tensorpack does not contain non-standard layers.
 
 BilinearUpSample was made un-public and removed from documentation (https://tensorpack.readthedocs.io/modules/models.html) a while ago because it's a non-standard layer and we should not provide an implementation for it. But it's good to know the bug!
 
 I see, but some of the non-standard layers like upsampling are pretty common. A separate non-standard layer codebase might be helpful though.
 The reason I hesitate to use TF's implementation is that the image.resize_* functions are messy (at least in resize_area) and might cause some weird things to happen. tf.image.resize_bilinear works fine though.
 		</comment>
 		<comment id='4' author='elmirador' date='2019-01-10T06:27:01Z'>
 		
 A separate non-standard layer codebase might be helpful though.
 
 Might be helpful as a reference. But it will have the same issue: personally I would not trust any non-standard layers written by others.
 tf.image.resize_bilinear has its own issues as well and it certainly is not equivalent to this layer in tensorpack during upsampling. That's why they're called non-standard.
 		</comment>
 		<comment id='5' author='elmirador' date='2019-01-10T07:03:28Z'>
 		
 Might be helpful as a reference. But it will have the same issue: personally I would not trust any non-standard layers written by others.
 
 I agree. Someone should at least read the code before using it.
 
 tf.image.resize_bilinear has its own issues as well and it certainly is not equivalent to this layer in tensorpack during upsampling. That's why they're called non-standard.
 
 Got it. I'll dig harder on TF's code.
 All in all, pretty happy to contribute. I love tensorpack much more than keras personally lol
 		</comment>
 		<comment id='6' author='elmirador' date='2019-01-10T07:05:47Z'>
 		Also, just found that the HED example uses the layer with channel=1 only, so there is nothing to fix. But I'll add some notes to the code.
 Closing as resolved. Thanks again!
 		</comment>
 	</comments>
 </bug>
<commit id='0b2ab4ae4536a074d9159c73f86f6fed52e512fc' author='Yuxin Wu' date='2019-01-09 23:23:02-08:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='README.md' new_name='README.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>66,67</added_lines>
 			<deleted_lines>66,67</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='docs\tutorial\extend\dataflow.md' new_name='docs\tutorial\extend\dataflow.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>4,5,6</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='examples\HED\hed.py' new_name='examples\HED\hed.py'>
 		<file_info nloc='241' complexity='32' token_count='2297'></file_info>
 		<method name='run' parameters='model_path,image_path,output'>
 				<method_info nloc='22' complexity='5' token_count='183' nesting_level='0' start_line='272' end_line='293'></method_info>
 			<added_lines>290</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='CaffeBilinearUpSample' parameters='x,shape'>
 				<method_info nloc='24' complexity='3' token_count='265' nesting_level='0' start_line='48' end_line='98'></method_info>
 			<added_lines>55,59,62,63,86,87,89,91,95,96</added_lines>
 			<deleted_lines>54,58,61,62,85,86,88,90,92,93</deleted_lines>
 		</method>
 		<method name='build_graph' parameters='self,image,edgemap'>
 				<method_info nloc='52' complexity='2' token_count='599' nesting_level='1' start_line='106' end_line='176'></method_info>
 			<added_lines>108,118,121,122,150,151,152,153,156,168,169,170,171,173,174,175,176</added_lines>
 			<deleted_lines>116,119,147,148,149,150,164,165,166,167,168,170,171,172,173</deleted_lines>
 		</method>
 		<method name='build_graph.branch' parameters='name,l,up'>
 				<method_info nloc='9' complexity='2' token_count='75' nesting_level='2' start_line='111' end_line='119'></method_info>
 			<added_lines>118</added_lines>
 			<deleted_lines>116,119</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>18</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorpack\dataflow\base.py' new_name='tensorpack\dataflow\base.py'>
 		<file_info nloc='74' complexity='18' token_count='342'></file_info>
 		<method name='__len__' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='10' nesting_level='1' start_line='89' end_line='120'></method_info>
 			<added_lines>100,101,102</added_lines>
 			<deleted_lines>100,101,102</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorpack\models\conv2d.py' new_name='tensorpack\models\conv2d.py'>
 		<file_info nloc='165' complexity='2' token_count='935'></file_info>
 		<modified_lines>
 			<added_lines>45</added_lines>
 			<deleted_lines>45</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
