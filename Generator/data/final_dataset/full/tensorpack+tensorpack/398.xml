<bug_data>
<bug id='398' author='vfdev-5' open_date='2017-08-30T19:10:06Z' closed_time='2017-08-30T19:28:14Z'>
 	<summary>Issue with k-folds training (cross-validation)</summary>
 	<description>
 Hi,
 I try to train k models on k-folded data. Code is &lt;denchmark-link:https://gist.github.com/vfdev-5/05648eaf8ffb71a6f2dd0ee932e22dea&gt;here&lt;/denchmark-link&gt;
 . Training on the 1st fold runs correctly and on the next fold I have the following error:
 &lt;denchmark-code&gt;ValueError: Tensor("input:0", shape=(?, 30, 30, 3), dtype=float32) must be from the same graph as Tensor("QueueInput/input_queue:0", shape=(), dtype=resource).
 &lt;/denchmark-code&gt;
 
 Could you suggest how to do such k-folds training within your frame work (probably, I would have a similar graph problem with pure TF).
 Thank you
 	</description>
 	<comments>
 		<comment id='1' author='vfdev-5' date='2017-08-30T19:11:57Z'>
 		I think you need to clear the graph with tf.reset_default_graph() before starting a new training.
 		</comment>
 		<comment id='2' author='vfdev-5' date='2017-08-30T19:13:35Z'>
 		Yes, I do this &lt;denchmark-link:https://gist.github.com/vfdev-5/05648eaf8ffb71a6f2dd0ee932e22dea#file-cifar-convnet-cross-validation-py-L186&gt;here&lt;/denchmark-link&gt;
 . However, it is not enough
 		</comment>
 		<comment id='3' author='vfdev-5' date='2017-08-30T19:28:47Z'>
 		Should be fixed now. BTW, because you created a new graph each time, you probably don't need a reset.
 		</comment>
 		<comment id='4' author='vfdev-5' date='2017-08-30T19:30:17Z'>
 		Okay, I see. Thank you !
 		</comment>
 		<comment id='5' author='vfdev-5' date='2017-08-30T21:54:11Z'>
 		I rerun my code with the latest tensorpack and the bug disappeared üëç
 However, at the end there is an IndexError that is thrown after the messge Train is finished!. Here is the traceback:
 &lt;denchmark-code&gt;[0830 21:45:55 @base.py:242] Training has finished!
 Traceback (most recent call last):
   File "cifar-convnet-cross-validation.py", line 190, in &lt;module&gt;
     trainer.train()
   File "/usr/lib/python3.5/contextlib.py", line 66, in __exit__
     next(self.gen)
   File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py", line 3815, in get_controller
     if self.stack[-1] is not default:
 IndexError: list index out of range
 [0830 21:45:55 @prefetch.py:174] [Prefetch Master] Context terminated.
 Prefetch process exited.
 [0830 21:45:55 @input_source.py:203] EnqueueThread QueueInput/input_queue Exited.
 Prefetch process exited.
 Prefetch process exited.
 Prefetch process exited.
 Prefetch process exited.
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-link:https://github.com/ppwwyyxx&gt;@ppwwyyxx&lt;/denchmark-link&gt;
   could you hint how to fix this? Thanks
 		</comment>
 		<comment id='6' author='vfdev-5' date='2017-08-30T22:04:17Z'>
 		Using reset_default_graph under tf.Graph() is already forbidden in latest TensorFlow (I'm using nightly).
 Using this and it will be fine:
 for val_fold_index in range(n_folds):
     with tf.Graph().as_default():
       # no reset
 Or this:
 for val_fold_index in range(n_folds):
     # no new graph
     tf.reset_default_graph()
 		</comment>
 		<comment id='7' author='vfdev-5' date='2019-04-13T15:22:34Z'>
 		Hi yx,
 My question is similar and I'm using examples/FasterRCNN with my own dataset (600 images).
 The model easily overfitting at around 50 epoch (100 steps per epoch).
 &lt;denchmark-link:https://user-images.githubusercontent.com/12379916/56081626-fce10000-5e41-11e9-9cb7-1ab7e7ee7e00.png&gt;&lt;/denchmark-link&gt;
 
 And frcnn loss keep going down.
 &lt;denchmark-link:https://user-images.githubusercontent.com/12379916/56081712-f30bcc80-5e42-11e9-9aec-8d4778628c47.png&gt;&lt;/denchmark-link&gt;
 
 I'm wondering if we got any solutions like cross validation/drop out/early stop in tensorpack for FRCNN?
 Can I assume that we don't need those methods for large dataset like COCO?
 		</comment>
 		<comment id='8' author='vfdev-5' date='2019-04-13T17:40:30Z'>
 		You can implement early stop with tensorpack callbacks.
 		</comment>
 	</comments>
 </bug>
<commit id='3ed43ab4ad4309e710a41ea071af96b166c63475' author='Yuxin Wu' date='2017-08-30 12:28:09-07:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='docs\tutorial\callback.md' new_name='docs\tutorial\callback.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>18,19,20,23,35,55,59</added_lines>
 			<deleted_lines>18,21,33,53,57</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorpack\graph_builder\model_desc.py' new_name='tensorpack\graph_builder\model_desc.py'>
 		<file_info nloc='84' complexity='19' token_count='508'></file_info>
 		<method name='__new__' parameters='cls,type,shape,name'>
 				<method_info nloc='5' complexity='1' token_count='44' nesting_level='1' start_line='28' end_line='38'></method_info>
 			<added_lines>37</added_lines>
 			<deleted_lines>28,29</deleted_lines>
 		</method>
 		<method name='build_placeholder' parameters='self,prefix'>
 				<method_info nloc='8' complexity='3' token_count='60' nesting_level='1' start_line='59' end_line='75'></method_info>
 			<added_lines>74</added_lines>
 			<deleted_lines>75</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>77</added_lines>
 			<deleted_lines>78</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
