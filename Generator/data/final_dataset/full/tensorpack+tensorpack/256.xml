<bug_data>
<bug id='256' author='msbauer' open_date='2017-05-08T09:19:44Z' closed_time='2017-05-08T16:24:44Z'>
 	<summary>OfflinePredictor broken on the current master</summary>
 	<description>
 Hi,
 I don't fully understand what the problem is exactly, so I hope I can describe it in an understandable way.
 Basically, I have a trained network on which I want to run an OfflinePredictor several times (b/c I have several batches of data that I want to predict on). So far, this all worked nicely.
 On the current master the OfflinePredictor runs once without problems and on the second attempt I get the following error message:
 &lt;denchmark-code&gt;[0508 11:02:52 @cifar_dataset.py:36] Found cifar100 data in /..../tensorpack_data/cifar100_data.
 before calling 
 [0508 11:02:52 @common.py:128] conv1_1 input: [None, 32, 32, 3]
 [0508 11:02:52 @common.py:136] conv1_1 output: [None, 32, 32, 64]
 [0508 11:02:52 @common.py:128] conv1_2 input: [None, 32, 32, 64]
 [0508 11:02:52 @common.py:136] conv1_2 output: [None, 32, 32, 64]
 [0508 11:02:52 @common.py:128] pool1 input: [None, 32, 32, 64]
 [0508 11:02:52 @common.py:136] pool1 output: [None, 16, 16, 64]
 [0508 11:02:52 @common.py:128] conv2_1 input: [None, 16, 16, 64]
 [0508 11:02:52 @common.py:136] conv2_1 output: [None, 16, 16, 128]
 [0508 11:02:52 @common.py:128] conv2_2 input: [None, 16, 16, 128]
 [0508 11:02:52 @common.py:136] conv2_2 output: [None, 16, 16, 128]
 [0508 11:02:52 @common.py:128] pool2 input: [None, 16, 16, 128]
 [0508 11:02:52 @common.py:136] pool2 output: [None, 8, 8, 128]
 [0508 11:02:52 @common.py:128] conv3_1 input: [None, 8, 8, 128]
 [0508 11:02:52 @common.py:136] conv3_1 output: [None, 8, 8, 256]
 [0508 11:02:52 @common.py:128] conv3_2 input: [None, 8, 8, 256]
 [0508 11:02:52 @common.py:136] conv3_2 output: [None, 8, 8, 256]
 [0508 11:02:52 @common.py:128] pool3 input: [None, 8, 8, 256]
 [0508 11:02:52 @common.py:136] pool3 output: [None, 4, 4, 256]
 [0508 11:02:52 @common.py:128] conv4_1 input: [None, 4, 4, 256]
 [0508 11:02:52 @common.py:136] conv4_1 output: [None, 4, 4, 512]
 [0508 11:02:52 @common.py:128] conv4_2 input: [None, 4, 4, 512]
 [0508 11:02:53 @common.py:136] conv4_2 output: [None, 4, 4, 512]
 [0508 11:02:53 @common.py:128] pool4 input: [None, 4, 4, 512]
 [0508 11:02:53 @common.py:136] pool4 output: [None, 2, 2, 512]
 [0508 11:02:53 @common.py:128] fc6 input: [None, 2, 2, 512]
 [0508 11:02:53 @common.py:136] fc6 output: [None, 1024]
 [0508 11:02:53 @common.py:128] fc7 input: [None, 1024]
 [0508 11:02:53 @common.py:136] fc7 output: [None, 512]
 [0508 11:02:53 @common.py:128] fc8 input: [None, 512]
 [0508 11:02:53 @common.py:136] fc8 output: [None, 90]
 [0508 11:02:53 @regularize.py:18] Apply regularizer for fc6/W:0
 [0508 11:02:53 @regularize.py:18] Apply regularizer for fc7/W:0
 [0508 11:02:53 @regularize.py:18] Apply regularizer for fc8/W:0
 2017-05-08 11:02:53.069260: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
 2017-05-08 11:02:53.069280: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
 2017-05-08 11:02:53.069288: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
 2017-05-08 11:02:53.069294: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
 2017-05-08 11:02:53.069300: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
 [0508 11:02:53 @sessinit.py:128] WRN Variable beta1_power:0 in checkpoint not found in the graph!
 [0508 11:02:53 @sessinit.py:128] WRN Variable beta2_power:0 in checkpoint not found in the graph!
 [0508 11:02:53 @sessinit.py:128] WRN Variable input_queue_size/EMA:0 in checkpoint not found in the graph!
 [0508 11:02:53 @sessinit.py:128] WRN Variable learning_rate:0 in checkpoint not found in the graph!
 [0508 11:02:53 @sessinit.py:100] Restoring checkpoint from /XXXX/model-39000 ...
 INFO:tensorflow:Restoring parameters from /XXXX/model-39000
 INFO:tensorflow:Restoring parameters from /XXXX/model-39000
 after calling
 (256,)
 [0 6 7 7 0 8 2 5 9 3]
 before calling 
 Traceback (most recent call last):
   File "cifar_laplace.py", line 243, in &lt;module&gt;
     run_experiment(cifar_set_parameters.get_set_parameters(set_id=int(args.set)))
   File "cifar_laplace.py", line 108, in run_experiment
     x_train, y_train = utils.get_batch_cifar(checkpoint_file, 100, classes_kshot, 'train', set_id=set_id)
   File "..../oneshot_code/utils.py", line 111, in get_batch_cifar
     acts = cifar_convnet.network_activation(dp[0], file_name, n_split=n_split)
   File "..../cifar_convnet.py", line 186, in network_activation
     output_names=['fc_activation']
   File "..../tensorpack/predict/base.py", line 141, in __init__
     config.model.build_graph(input_placehdrs)
   File "..../tensorpack/models/model_desc.py", line 116, in build_graph
     self._build_graph(model_inputs)
   File "..../cifar_convnet.py", line 92, in _build_graph
     add_moving_summary(tf.reduce_mean(wrong, name='train_error'))
   File "..../tensorpack/tfutils/summary.py", line 162, in add_moving_summary
     avg_maintain_op = averager.apply(v)
   File "..../tensorflow/python/training/moving_averages.py", line 387, in apply
     (1.0 + num_updates) / (10.0 + num_updates))
   File "..../tensorflow/python/ops/gen_math_ops.py", line 1392, in minimum
     result = _op_def_lib.apply_op("Minimum", x=x, y=y, name=name)
   File "..../tensorflow/python/framework/op_def_library.py", line 331, in apply_op
     g = ops._get_graph_from_inputs(_Flatten(keywords.values()))
   File "..../tensorflow/python/framework/ops.py", line 3912, in _get_graph_from_inputs
     _assert_same_graph(original_graph_element, graph_element)
   File "..../tensorflow/python/framework/ops.py", line 3851, in _assert_same_graph
     "%s must be from the same graph as %s." % (item, original_item))
 ValueError: Tensor("EMA/decay:0", shape=(), dtype=float32) must be from the same graph as Tensor("truediv:0", shape=(), dtype=float32).
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='msbauer' date='2017-05-08T13:34:39Z'>
 		also seeing this.
 in image2image.py after
 GANTrainer(config).train()
 i added:
 model_file = os.path.join(logger.LOG_DIR, 'checkpoint')
 sample(args.data, model_file)
 this used to work. but after recent changes to master i am getting:
 Traceback (most recent call last):
 File "Image2Image.py", line 230, in 
 sample(args.data, model_file)
 File "Image2Image.py", line 199, in sample
 pred = SimpleDatasetPredictor(pred, ds)
 File "C:\Anaconda3\lib\site-packages\tensorpack\predict\dataset.py", line 65, in init
 self.predictor = OfflinePredictor(config)
 File "C:\Anaconda3\lib\site-packages\tensorpack\predict\base.py", line 141, in init
 config.model.build_graph(input_placehdrs)
 File "C:\Anaconda3\lib\site-packages\tensorpack\models\model_desc.py", line 116, in build_graph
 self._build_graph(model_inputs)
 File "Image2Image.py", line 118, in _build_graph
 self.build_losses(real_pred, fake_pred)
 File "C:\Users\eyaler\Dropbox\python\tensorpack\examples\GAN\GAN.py", line 54, in build_losses
 add_moving_summary(self.g_loss, self.d_loss, d_accuracy, g_accuracy)
 File "C:\Anaconda3\lib\site-packages\tensorpack\tfutils\summary.py", line 162, in add_moving_summary
 avg_maintain_op = averager.apply(v)
 File "C:\Anaconda3\lib\site-packages\tensorflow\python\training\moving_averages.py", line 387, in apply
 (1.0 + num_updates) / (10.0 + num_updates))
 File "C:\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_math_ops.py", line 1392, in minimum
 result = _op_def_lib.apply_op("Minimum", x=x, y=y, name=name)
 File "C:\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py", line 331, in apply_op
 g = ops._get_graph_from_inputs(_Flatten(keywords.values()))
 File "C:\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py", line 3912, in _get_graph_from_inputs
 _assert_same_graph(original_graph_element, graph_element)
 File "C:\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py", line 3851, in _assert_same_graph
 "%s must be from the same graph as %s." % (item, original_item))
 ValueError: Tensor("truediv_2:0", shape=(), dtype=float32) must be from the same graph as Tensor("EMA/decay:0", shape=(), dtype=float32).
 		</comment>
 		<comment id='2' author='msbauer' date='2017-05-08T16:25:12Z'>
 		Thanks for reporting. Should be fixed now.
 		</comment>
 	</comments>
 </bug>
<commit id='f9fc329b7b12064c5d59c4781cef265e36810f24' author='Yuxin Wu' date='2017-05-08 09:24:39-07:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorpack\predict\base.py' new_name='tensorpack\predict\base.py'>
 		<file_info nloc='121' complexity='22' token_count='754'></file_info>
 		<method name='__init__' parameters='self,config'>
 				<method_info nloc='12' complexity='1' token_count='105' nesting_level='1' start_line='132' end_line='149'></method_info>
 			<added_lines>140</added_lines>
 			<deleted_lines>140</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
