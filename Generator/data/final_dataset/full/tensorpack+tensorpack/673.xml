<bug_data>
<bug id='673' author='vfdev-5' open_date='2018-02-25T22:58:10Z' closed_time='2018-03-02T08:03:23Z'>
 	<summary>Infinite loop with MultiProcessMapDataZMQ</summary>
 	<description>
 There is an infinite loop with the following code:
 &lt;denchmark-code&gt;from tensorpack.dataflow import MultiProcessMapDataZMQ, FakeData
 
 ds = FakeData(((50, 50, 3), (1,)))
 
 import numpy as np
 
 def proc(dp):
     img = np.random.randint(0, 255, size=(1012, 1012, 3))
     img[img &lt; 120] = 0
     return dp
 
 mp_ds = MultiProcessMapDataZMQ(ds, map_func=proc, nr_proc=10, strict=True)
 
 mp_ds.reset_state()
 
 for i, (_, _) in enumerate(mp_ds.get_data()):
     if i % 100 == 0:
         print(i, end=" . ", flush=True)
 &lt;/denchmark-code&gt;
 
 Important part is  and small size of . If change size to (512, 512, 3), then loop ends properly. Probably, there is something with buffer filling and dequeing...
 Execution stucks &lt;denchmark-link:https://github.com/ppwwyyxx/tensorpack/blob/master/tensorpack/dataflow/parallel_map.py#L81&gt;here&lt;/denchmark-link&gt;
 .
 	</description>
 	<comments>
 		<comment id='1' author='vfdev-5' date='2018-02-25T23:01:22Z'>
 		Cannot reproduce. Please update
 		</comment>
 		<comment id='2' author='vfdev-5' date='2018-02-25T23:11:44Z'>
 		Actually, I have already updated to the master. Try a smaller size, for example (10, 10, 3) in FakeData
 		</comment>
 		<comment id='3' author='vfdev-5' date='2018-02-26T07:29:27Z'>
 		&lt;denchmark-link:https://github.com/ppwwyyxx&gt;@ppwwyyxx&lt;/denchmark-link&gt;
  could you reproduce the issue ?
 		</comment>
 		<comment id='4' author='vfdev-5' date='2018-02-26T07:47:00Z'>
 		I can not. Maybe upgrade/reinstall msgpack?
 		</comment>
 		<comment id='5' author='vfdev-5' date='2018-02-26T08:01:23Z'>
 		Could you try please this code too:
 from tensorpack.dataflow import MultiProcessMapDataZMQ, RNGDataFlow, MultiThreadMapData
 
 
 dataset = [("file_%i" % i, i % 3) for i in range(500)]
 
 class TestDataset(RNGDataFlow):
     
     def __init__(self, dataset):
         super(TestDataset, self).__init__()
         self.dataset = dataset
         
     def size(self):
         return len(self.dataset)
         
     def get_data(self):
         for k in range(len(self.dataset)):
             yield self.dataset[k]            
 
 test_dataset = TestDataset(dataset)
 test_dataset.size()
 
 def data_transform(dp):
     return dp
 
 mp_test_dataset = MultiProcessMapDataZMQ(test_dataset, nr_proc=10, map_func=data_transform, strict=True)
 
 mp_test_dataset.reset_state()
 
 
 for i, (fp, _) in enumerate(mp_test_dataset.get_data()):
     if i % 100 == 0:
         print(i, end=" . ", flush=True)
 Okay, I'll try to upgrade msgpack, which version do you use ?
 Version of msgpack I use is msgpack (0.5.6)
 		</comment>
 		<comment id='6' author='vfdev-5' date='2018-02-26T08:03:17Z'>
 		The code successfully exits. I use the latest version of msgpack &amp; msgpack-numpy. Maybe also try upgrading pyzmq.
 		</comment>
 		<comment id='7' author='vfdev-5' date='2018-02-26T08:08:12Z'>
 		Strange configuration I have. Here is a list of versions of some of packages I have:
 &lt;denchmark-code&gt;msgpack (0.5.6)
 msgpack-numpy (0.4.3)
 msgpack-python (0.5.5)
 numpy (1.14.1)
 pickleshare (0.7.4)
 pip (9.0.1)
 pyzmq (17.0.0)
 tensorpack (0.8.1)
 &lt;/denchmark-code&gt;
 
 on Ubuntu 16.04
 		</comment>
 		<comment id='8' author='vfdev-5' date='2018-02-26T08:13:39Z'>
 		&lt;denchmark-link:https://github.com/msgpack/msgpack-python#pypi-package-name&gt;https://github.com/msgpack/msgpack-python#pypi-package-name&lt;/denchmark-link&gt;
 
 Maybe related.
 		</comment>
 		<comment id='9' author='vfdev-5' date='2018-02-26T09:29:29Z'>
 		I could reproduce the issue using this docker file. Could you try it please?
 &lt;denchmark-link:https://github.com/ppwwyyxx/tensorpack/files/1757877/Dockerfile.txt&gt;Dockerfile.txt&lt;/denchmark-link&gt;
 
 &lt;denchmark-link:https://github.com/ppwwyyxx/tensorpack/files/1757878/test_tensorpack_inf_loop.txt&gt;test_tensorpack_inf_loop.txt&lt;/denchmark-link&gt;
 
 Please remove .txt extensions from Dockerfile and replace it for  by . Github does not allow include py and dockerfile as is.
 		</comment>
 		<comment id='10' author='vfdev-5' date='2018-02-27T00:43:23Z'>
 		I can reproduce the issue in docker. Looks like I'm using zmq socket in a wrong way where messages can get lost if the sockets aren't created with a desired order.
 		</comment>
 		<comment id='11' author='vfdev-5' date='2018-03-02T09:58:30Z'>
 		Awesome ! ðŸŽ‰
 		</comment>
 	</comments>
 </bug>
<commit id='760b924a2fc99249483cad6979c63a3f1054a57f' author='Yuxin Wu' date='2018-03-02 00:02:11-08:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorpack\dataflow\parallel_map.py' new_name='tensorpack\dataflow\parallel_map.py'>
 		<file_info nloc='312' complexity='72' token_count='1933'></file_info>
 		<method name='_reset_once' parameters='self'>
 				<method_info nloc='16' complexity='3' token_count='168' nesting_level='1' start_line='256' end_line='274'></method_info>
 			<added_lines>258</added_lines>
 			<deleted_lines>259</deleted_lines>
 		</method>
 		<method name='_send' parameters='self,dp'>
 				<method_info nloc='3' complexity='1' token_count='30' nesting_level='1' start_line='279' end_line='281'></method_info>
 			<added_lines>280</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='run' parameters='self'>
 				<method_info nloc='10' complexity='2' token_count='89' nesting_level='2' start_line='227' end_line='237'></method_info>
 			<added_lines>229</added_lines>
 			<deleted_lines>230</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>5,275,282,283,284</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
