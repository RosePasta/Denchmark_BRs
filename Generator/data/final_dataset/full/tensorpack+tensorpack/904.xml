<bug_data>
<bug id='904' author='thuzhf' open_date='2018-09-21T10:04:49Z' closed_time='2018-09-21T20:14:52Z'>
 	<summary>AttributeError: 'TrainLoop' object has no attribute 'starting_epoch'</summary>
 	<description>
 The problem occurred when L318 below is executed if stats is not None (e.g. when you restore training from a log dir). The full error message is attached in the end.
 
 
 
 tensorpack/tensorpack/callbacks/monitor.py
 
 
         Lines 309 to 318
       in
       36b05bb
 
 
 
 
 
 
  def _setup_graph(self): 
 
 
 
  stats = JSONWriter.load_existing_json() 
 
 
 
  self._fname = os.path.join(logger.get_logger_dir(), JSONWriter.FILENAME) 
 
 
 
  if stats is not None: 
 
 
 
  try: 
 
 
 
  epoch = stats[-1]['epoch_num'] + 1 
 
 
 
  except Exception: 
 
 
 
  epoch = None 
 
 
 
  
 
 
 
  starting_epoch = self.trainer.loop.starting_epoch 
 
 
 
 
 
 It can be seen self.starting_epoch is defined in L48 below.
 
 
 
 tensorpack/tensorpack/train/base.py
 
 
         Lines 44 to 48
       in
       36b05bb
 
 
 
 
 
 
  def config(self, steps_per_epoch, starting_epoch, max_epoch): 
 
 
 
  """ 
 
 
 
          Configure the loop given the settings. 
 
 
 
          """ 
 
 
 
  self.starting_epoch = int(starting_epoch) 
 
 
 
 
 
 And config() is called in L241 below in the main_loop() function.
 
 
 
 tensorpack/tensorpack/train/base.py
 
 
         Lines 233 to 241
       in
       36b05bb
 
 
 
 
 
 
  def main_loop(self, steps_per_epoch, starting_epoch, max_epoch): 
 
 
 
  """ 
 
 
 
          Run the main training loop. 
 
 
 
   
 
 
 
          Args: 
 
 
 
              steps_per_epoch, starting_epoch, max_epoch (int): 
 
 
 
          """ 
 
 
 
  with self.sess.as_default(): 
 
 
 
  self.loop.config(steps_per_epoch, starting_epoch, max_epoch) 
 
 
 
 
 
 And setup_callbacks() (L289 below) is executed before main_loop() (L291 below).
 
 
 
 tensorpack/tensorpack/train/base.py
 
 
         Lines 274 to 291
       in
       36b05bb
 
 
 
 
 
 
  def train(self, 
 
 
 
  callbacks, monitors, 
 
 
 
  session_creator, session_init, 
 
 
 
  steps_per_epoch, starting_epoch=1, max_epoch=9999999): 
 
 
 
  """ 
 
 
 
          Implemented by three lines: 
 
 
 
   
 
 
 
          .. code-block:: python 
 
 
 
   
 
 
 
              self.setup_callbacks(callbacks, monitors) 
 
 
 
              self.initialize(session_creator, session_init) 
 
 
 
              self.main_loop(steps_per_epoch, starting_epoch, max_epoch) 
 
 
 
   
 
 
 
          You can call those methods by yourself to have better control on details if needed. 
 
 
 
          """ 
 
 
 
  self.setup_callbacks(callbacks, monitors) 
 
 
 
  self.initialize(session_creator, session_init) 
 
 
 
  self.main_loop(steps_per_epoch, starting_epoch, max_epoch) 
 
 
 
 
 
 And L318 in the first file (monitor.py) will be finally executed in the calling of setup_callbacks() above. Actually you already have load_existing_epoch_number  in L298 below.
 
 
 
 tensorpack/tensorpack/callbacks/monitor.py
 
 
          Line 298
       in
       36b05bb
 
 
 
 
 
 
  def load_existing_epoch_number(): 
 
 
 
 
 
 And the above function is only called in the following class:
 
 
 
 tensorpack/tensorpack/train/config.py
 
 
         Lines 177 to 240
       in
       36b05bb
 
 
 
 
 
 
  class AutoResumeTrainConfig(TrainConfig): 
 
 
 
  """ 
 
 
 
      Same as :class:`TrainConfig`, but does the following to automatically 
 
 
 
      resume from training: 
 
 
 
   
 
 
 
      1. If a checkpoint was found in :meth:`logger.get_logger_dir()`, set 
 
 
 
         `session_init` option to load it. 
 
 
 
      2. If a JSON history was found in :meth:`logger.get_logger_dir()`, try to 
 
 
 
         load the epoch number from it and set the `starting_epoch` option to 
 
 
 
         continue training. 
 
 
 
   
 
 
 
      You can choose to let the above two option to either overwrite or 
 
 
 
      not overwrite user-provided arguments, as explained below. 
 
 
 
      """ 
 
 
 
  def __init__(self, always_resume=True, **kwargs): 
 
 
 
  """ 
 
 
 
          Args: 
 
 
 
              always_resume (bool): If False, user-provided arguments 
 
 
 
                  `session_init` and `starting_epoch` will take priority. 
 
 
 
                  Otherwise, resume will take priority. 
 
 
 
              kwargs: same as in :class:`TrainConfig`. 
 
 
 
   
 
 
 
          Note: 
 
 
 
              The main goal of this class is to let a training job to resume 
 
 
 
              without changing any line of code or command line arguments. 
 
 
 
              So it's useful to let resume take priority over user-provided arguments sometimes: 
 
 
 
   
 
 
 
              If your training starts from a pre-trained model, 
 
 
 
              you would want it to use user-provided model loader at the 
 
 
 
              beginning, but a "resume" model loader when the job was 
 
 
 
              interrupted and restarted. 
 
 
 
          """ 
 
 
 
  if always_resume or 'session_init' not in kwargs: 
 
 
 
  sessinit = self._get_sessinit_resume() 
 
 
 
  if sessinit is not None: 
 
 
 
  path = sessinit.path 
 
 
 
  if 'session_init' in kwargs: 
 
 
 
  logger.info("Found checkpoint at {}. " 
 
 
 
  "session_init arguments will be overwritten.".format(path)) 
 
 
 
  else: 
 
 
 
  logger.info("Will load checkpoint at {}.".format(path)) 
 
 
 
  kwargs['session_init'] = sessinit 
 
 
 
  
 
 
 
  if always_resume or 'starting_epoch' not in kwargs: 
 
 
 
  last_epoch = self._get_last_epoch() 
 
 
 
  if last_epoch is not None: 
 
 
 
  now_epoch = last_epoch + 1 
 
 
 
  logger.info("Found history statistics from JSON. " 
 
 
 
  "Overwrite the starting epoch to epoch #{}.".format(now_epoch)) 
 
 
 
  kwargs['starting_epoch'] = now_epoch 
 
 
 
  
 
 
 
  super(AutoResumeTrainConfig, self).__init__(**kwargs) 
 
 
 
  
 
 
 
  def _get_sessinit_resume(self): 
 
 
 
  logdir = logger.get_logger_dir() 
 
 
 
  if not logdir: 
 
 
 
  return None 
 
 
 
  path = os.path.join(logdir, 'checkpoint') 
 
 
 
  if not tf.gfile.Exists(path): 
 
 
 
  return None 
 
 
 
  return SaverRestore(path) 
 
 
 
  
 
 
 
  def _get_last_epoch(self): 
 
 
 
  return JSONWriter.load_existing_epoch_number() 
 
 
 
 
 
 Even if I changed TrainConfig() to AutoResumeTrainConfig(), the same problem still occurred due to the same reason. Although self.starting_epoch is also defined below, it is the config's attribute instead of the train.loop's:
 
 
 
 tensorpack/tensorpack/train/config.py
 
 
          Line 146
       in
       7b8728f
 
 
 
 
 
 
  self.starting_epoch = int(starting_epoch) 
 
 
 
 
 
 I think some easy workarounds will be:
 
 Defining starting_epoch earlier as the train.loop's attribute (that way AutoResumeTrainConfig() will probably work).
 Changing L318 in the first file (monitor.py) to use function load_existing_epoch_number() (that way maybe will make AutoResumeTrainConfig() useless, but this is handy because users can always use TrainConfig()).
 
 However, to be consistent with the logic of your Trainer.train() function (including 3 functions), it seems it needs more consideration on how to fix this.
 The following is the full error message:
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "main.py", line 307, in &lt;module&gt;
     main()
   File "main.py", line 303, in main
     launch_train_with_config(config, trainer)
   File "/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/train/interface.py", line 95, in launch_train_with_config
     extra_callbacks=config.extra_callbacks)
   File "/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/train/base.py", line 319, in train_with_defaults
     steps_per_epoch, starting_epoch, max_epoch)
   File "/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/train/base.py", line 289, in train
     self.setup_callbacks(callbacks, monitors)
   File "/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/utils/argtools.py", line 181, in wrapper
     return func(*args, **kwargs)
   File "/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/train/base.py", line 189, in setup_callbacks
     self._callbacks.setup_graph(weakref.proxy(self))
   File "/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/callbacks/base.py", line 52, in setup_graph
     self._setup_graph()
   File "/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/callbacks/group.py", line 70, in _setup_graph
     cb.setup_graph(self.trainer)
   File "/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/callbacks/monitor.py", line 55, in setup_graph
     self._setup_graph()
   File "/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/callbacks/monitor.py", line 318, in _setup_graph
     starting_epoch = self.trainer.loop.starting_epoch
 AttributeError: 'TrainLoop' object has no attribute 'starting_epoch'
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='thuzhf' date='2018-09-21T15:07:13Z'>
 		This is an issue I fixed before but forgot to take a note about it. Unfortunately it was introduced again lately..
 I will think about how to properly address this.
 		</comment>
 	</comments>
 </bug>
<commit id='9ce85aae9f63919087cfdae76238d094ab5c35f5' author='Yuxin Wu' date='2018-09-21 13:14:26-07:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorpack\callbacks\inference.py' new_name='tensorpack\callbacks\inference.py'>
 		<file_info nloc='122' complexity='31' token_count='668'></file_info>
 		<modified_lines>
 			<added_lines>81,82,83</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorpack\callbacks\monitor.py' new_name='tensorpack\callbacks\monitor.py'>
 		<file_info nloc='325' complexity='96' token_count='2101'></file_info>
 		<method name='_setup_graph' parameters='self'>
 				<method_info nloc='4' complexity='1' token_count='23' nesting_level='1' start_line='310' end_line='313'></method_info>
 			<added_lines>311,312,313</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_before_train' parameters='self'>
 				<method_info nloc='24' complexity='5' token_count='188' nesting_level='1' start_line='315' end_line='345'></method_info>
 			<added_lines>315,324,325,332,335,340,344</added_lines>
 			<deleted_lines>324,327,328,333,336,337,338,339,340,341,343,344</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>309,314</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorpack\dataflow\dataset\cifar.py' new_name='tensorpack\dataflow\dataset\cifar.py'>
 		<file_info nloc='131' complexity='38' token_count='1010'></file_info>
 		<method name='__init__' parameters='self,train_or_test,shuffle,dir'>
 				<method_info nloc='2' complexity='1' token_count='33' nesting_level='1' start_line='152' end_line='158'></method_info>
 			<added_lines>155</added_lines>
 			<deleted_lines>152,156</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,train_or_test,shuffle,dir,cifar_classnum'>
 				<method_info nloc='20' complexity='6' token_count='155' nesting_level='1' start_line='88' end_line='107'></method_info>
 			<added_lines>88,107</added_lines>
 			<deleted_lines>88</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,train_or_test,shuffle,dir,cifar_classnum'>
 				<method_info nloc='22' complexity='7' token_count='165' nesting_level='1' start_line='88' end_line='110'></method_info>
 			<added_lines>88,107,108,109</added_lines>
 			<deleted_lines>88</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,train_or_test,shuffle,dir'>
 				<method_info nloc='2' complexity='1' token_count='33' nesting_level='1' start_line='155' end_line='161'></method_info>
 			<added_lines>155,159</added_lines>
 			<deleted_lines>156</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>166</added_lines>
 			<deleted_lines>163</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorpack\train\config.py' new_name='tensorpack\train\config.py'>
 		<file_info nloc='146' complexity='23' token_count='855'></file_info>
 		<method name='__init__' parameters='self,always_resume,kwargs'>
 				<method_info nloc='19' complexity='8' token_count='131' nesting_level='1' start_line='191' end_line='228'></method_info>
 			<added_lines>225</added_lines>
 			<deleted_lines>225</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
