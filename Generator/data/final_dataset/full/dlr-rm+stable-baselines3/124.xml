<bug_data>
<bug id='124' author='rk37' open_date='2020-07-25T10:07:07Z' closed_time='2020-07-25T20:35:49Z'>
 	<summary>AttributeError occurred when use "bias=False" linear layer in custom FeaturesExtractor</summary>
 	<description>
 Describe the bug
 When use custom  FeaturesExtractor for common.policies.ActorCriticPolicy, AttributeError occurred : 'NoneType' object has no attribute 'data'.
 test codes
 import sys
 import numpy as np
 import gym
 import torch
 import torch.nn as nn
 
 import stable_baselines3
 from stable_baselines3 import PPO
 from stable_baselines3.ppo import MlpPolicy
 from stable_baselines3.common.cmd_util import make_vec_env
 
 # Parallel environments
 env = make_vec_env('CartPole-v1', n_envs=4)
 
 from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
 
 class CustomFeaturesExtractor(BaseFeaturesExtractor):
         def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 512):
             super(CustomFeaturesExtractor, self).__init__(observation_space, features_dim)
             self.linear = nn.Sequential(torch.nn.Flatten(),
                                         # disable bias
                                         torch.nn.Linear(np.prod(observation_space.shape), features_dim, bias=False),
                                         torch.nn.ReLU())
             pass
         
         def forward(self, observations: torch.Tensor) -&gt; torch.Tensor:
             return self.linear(observations)
             pass
 
 from stable_baselines3.common.policies import ActorCriticPolicy
 
 class CustomPolicy(ActorCriticPolicy):
     def __init__(self, *args, **kwargs):
         super(CustomPolicy, self).__init__(*args, **kwargs, features_extractor_class = CustomFeaturesExtractor)
         pass
 
 model = PPO(CustomPolicy, env, verbose=1)
 model.learn(total_timesteps=25000)
 stack traces
 Using cuda device
 ---------------------------------------------------------------------------
 AttributeError                            Traceback (most recent call last)
 &lt;ipython-input-9-3bcc7e758863&gt; in &lt;module&gt;
 ----&gt; 1 model = PPO(CustomPolicy, env, verbose=1)
       2 model.learn(total_timesteps=25000)
 
 ~/anaconda3/envs/tf2rl/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py in __init__(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, target_kl, tensorboard_log, create_eval_env, policy_kwargs, verbose, seed, device, _init_setup_model)
     118 
     119         if _init_setup_model:
 --&gt; 120             self._setup_model()
     121 
     122     def _setup_model(self) -&gt; None:
 
 ~/anaconda3/envs/tf2rl/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py in _setup_model(self)
     121 
     122     def _setup_model(self) -&gt; None:
 --&gt; 123         super(PPO, self)._setup_model()
     124 
     125         # Initialize schedules for policy/value clipping
 
 ~/anaconda3/envs/tf2rl/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py in _setup_model(self)
     112             n_envs=self.n_envs,
     113         )
 --&gt; 114         self.policy = self.policy_class(
     115             self.observation_space,
     116             self.action_space,
 
 &lt;ipython-input-8-d7d248355231&gt; in __init__(self, *args, **kwargs)
       3 class CustomPolicy(ActorCriticPolicy):
       4     def __init__(self, *args, **kwargs):
 ----&gt; 5         super(CustomPolicy, self).__init__(*args, **kwargs, features_extractor_class = CustomFeaturesExtractor)
       6         pass
 
 ~/anaconda3/envs/tf2rl/lib/python3.8/site-packages/stable_baselines3/common/policies.py in __init__(self, observation_space, action_space, lr_schedule, net_arch, device, activation_fn, ortho_init, use_sde, log_std_init, full_std, sde_net_arch, use_expln, squash_output, features_extractor_class, features_extractor_kwargs, normalize_images, optimizer_class, optimizer_kwargs)
     399         self.action_dist = make_proba_distribution(action_space, use_sde=use_sde, dist_kwargs=dist_kwargs)
     400 
 --&gt; 401         self._build(lr_schedule)
     402 
     403     def _get_data(self) -&gt; Dict[str, Any]:
 
 ~/anaconda3/envs/tf2rl/lib/python3.8/site-packages/stable_baselines3/common/policies.py in _build(self, lr_schedule)
     490             }
     491             for module, gain in module_gains.items():
 --&gt; 492                 module.apply(partial(self.init_weights, gain=gain))
     493 
     494         # Setup optimizer with initial learning rate
 
 ~/anaconda3/envs/tf2rl/lib/python3.8/site-packages/torch/nn/modules/module.py in apply(self, fn)
     287         """
     288         for module in self.children():
 --&gt; 289             module.apply(fn)
     290         fn(self)
     291         return self
 
 ~/anaconda3/envs/tf2rl/lib/python3.8/site-packages/torch/nn/modules/module.py in apply(self, fn)
     287         """
     288         for module in self.children():
 --&gt; 289             module.apply(fn)
     290         fn(self)
     291         return self
 
 ~/anaconda3/envs/tf2rl/lib/python3.8/site-packages/torch/nn/modules/module.py in apply(self, fn)
     288         for module in self.children():
     289             module.apply(fn)
 --&gt; 290         fn(self)
     291         return self
     292 
 
 ~/anaconda3/envs/tf2rl/lib/python3.8/site-packages/stable_baselines3/common/policies.py in init_weights(module, gain)
     189         if isinstance(module, (nn.Linear, nn.Conv2d)):
     190             nn.init.orthogonal_(module.weight, gain=gain)
 --&gt; 191             module.bias.data.fill_(0.0)
     192 
     193     @abstractmethod
 
 AttributeError: 'NoneType' object has no attribute 'data'
 
 System Info
 
 GPU nvidia gtx 1070
 Python 3.8.3
 PyTorch 1.5.1
 Gym 0.17.2
 Stable-Baselines 3 0.8.0a5
 
 	</description>
 	<comments>
 		<comment id='1' author='rk37' date='2020-07-25T10:11:39Z'>
 		Hello,
 It seems that a check is missing during initialization of the weights. We would appreciate a PR that solves this issue ;)
 In the meatime, you can deactivate orthogonal initialization by passing ortho_init=False to the policy.
 		</comment>
 	</comments>
 </bug>
<commit id='bd2aae0c27d238a6a2c2f3a4c54066dfb30bd49b' author='rk37' date='2020-07-25 22:35:48+02:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='docs\misc\changelog.rst' new_name='docs\misc\changelog.rst'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>34,360</added_lines>
 			<deleted_lines>359</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='stable_baselines3\common\policies.py' new_name='stable_baselines3\common\policies.py'>
 		<file_info nloc='691' complexity='59' token_count='3381'></file_info>
 		<method name='init_weights' parameters='Module,float'>
 				<method_info nloc='8' complexity='3' token_count='68' nesting_level='1' start_line='185' end_line='192'></method_info>
 			<added_lines>191,192</added_lines>
 			<deleted_lines>191</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
