<bug_data>
<bug id='265' author='decodyng' open_date='2020-12-19T00:17:08Z' closed_time='2021-01-11T16:03:32Z'>
 	<summary>[Bug] Silent NaNs in PPO Loss Calculation if n_steps=1 and n_envs=1</summary>
 	<description>
 &lt;denchmark-h:h3&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 This is somewhere between a bug and a request for more informative errors:
 When n_steps and n_envs are both set to 1, the batch returned by a rollout buffer &lt;denchmark-link:https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/ppo/ppo.py#L160&gt;here&lt;/denchmark-link&gt;
  will be of length 1. This will make the  calculation return  values, since the normalization step involves calculating the standard deviation of advantages, which is undefined for a single element.
 I recognize that this small of a setting is definitely an edge case (I ran into it during testing, when we were setting all values quite low for speed reasons), so I'm not sure it makes sense to have logic for this case, but at minimum, I think it would be beneficial to have some kind of explicit warning that checks if actions or advantages have a single element, so that there's a clear indication of the source of the issue, rather than having to follow a breadcrumb trail of nans from some higher abstraction level of code
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;from stable_baselines3 import PPO
 
 env = gym.make('CartPole-v1')
 model = PPO('MlpPolicy', env, verbose=1, n_steps=1)
 model.learn(total_timesteps=10)
 &lt;/denchmark-code&gt;
 
 This will fail with an unclear error :
 &lt;denchmark-code&gt;RuntimeError: invalid multinomial distribution (encountering probability entry &lt; 0)
 &lt;/denchmark-code&gt;
 
 If you put a debugger or insert logging statements at ppo.py:170, you'll be able to see that (1) len(advantages) = 1 and consequently (2) advantages.std() = nan, which first arises as a visible bug when you try to collect an on-policy rollout after your first training step, since the nan values in loss propagate into nan parameter values.
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 Either (1) explicit support for training on effective batches of size 1, or (2) a clearer and earlier error when you attempt to construct an algorithm object with n_steps=1 and n_envs=1, informing the user that the case isn't supported.
 ###¬†System Info
 Describe the characteristic of your environment:
 
 Describe how the library was installed (pip, docker, source, ...): Cloned from fork of current master, installed via pip
 GPU models and configuration: N/A
 Python version: 3.7.0
 PyTorch version: 1.7.1
 Gym version: 0.17.3
 Versions of any other relevant libraries: N/A
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 Add any other context about the problem here.
 &lt;denchmark-h:h3&gt;Checklist&lt;/denchmark-h&gt;
 
 
  I have checked that there is no similar issue in the repo (required)
  I have read the documentation (required)
  I have provided a minimal working example to reproduce the bug (required)
 
 	</description>
 	<comments>
 		<comment id='1' author='decodyng' date='2020-12-19T17:03:37Z'>
 		Hello,
 nice catch =)
 Even though this case is unlikely to happen (using PPO with n_steps=1 and n_envs=1), I agree we should have a better error message, an assert should do the trick ;)
 We would happy if you could submit a PR that solves this issue ;)
 		</comment>
 		<comment id='2' author='decodyng' date='2020-12-21T18:45:36Z'>
 		Sound good; I already have a fork with that basic assert, and would be happy to merge that in.
 One subtler thing that I'd be curious for the maintainers' perspective on: my impression from the testing I did in uncovering this bug is that the effective batch size used in an update is min(batch_size, n_envs*n_steps).
 First off, I'm curious if that is also your understanding of how you expect things to work. Secondly, if that impression is correct, would you be open to also adding a Warning if n_envs*n_steps is less than a specified batch size, since that might mean the intended batch size isn't actually being used for updates?
 		</comment>
 		<comment id='3' author='decodyng' date='2020-12-21T19:01:16Z'>
 		
 One subtler thing that I'd be curious for the maintainers' perspective on: my impression from the testing I did in uncovering this bug is that the effective batch size used in an update is min(batch_size, n_envs*n_steps).
 
 What makes you think that?
 The only thing that can happen when using the rollout buffer with PPO, is that the last minibatch will be truncated (so the last minibatch size will be:
 total_transitions = n_envs * n_steps
 n_minibatches = total_transitions // batch_size # number of minibatches with correct batch size
 
 # the last minibatch size will be smaller in that case
 if total_transitions - n_minibatches * batch_size &gt; 0:
     last_minibatch_size = total_transitions - n_minibatches * batch_size
 but it would probably be good to add a warning too.
 		</comment>
 		<comment id='4' author='decodyng' date='2020-12-21T19:20:25Z'>
 		Here's an example demonstrating the thing I think I'm seeing here:
 &lt;denchmark-code&gt;import gym
 from stable_baselines3 import PPO
 
 env = gym.make('CartPole-v1')
 model = PPO('MlpPolicy', env, verbose=1, n_steps=14, batch_size=16)
 model.learn(total_timesteps=100)
 &lt;/denchmark-code&gt;
 
 If I put a debug statement at ppo.py:183 to print len(advantages) (which I think is the effective batch size, since the loss is calculated and gradient update is done within that loop), I'm seeing that debug line always return a value of 14 (=14 steps*1 env) rather than 16 (the specified batch size), for all batches, rather than just the final batch.
 		</comment>
 		<comment id='5' author='decodyng' date='2020-12-21T19:24:36Z'>
 		Ah, reading over your comment again, I now think we're saying the same thing here, except you're framing it as the last minibatch getting truncated, and in the situation I'm describing, you can't pull even a single minibatch from the amount of data present in n_steps*n_envs, so all batches are truncated
 		</comment>
 		<comment id='6' author='decodyng' date='2020-12-21T19:25:55Z'>
 		If I set the batch_size to instead be 8, I see what you're referring to (where the effective batch size alternates between 8 and 6, to add up to 14)
 		</comment>
 		<comment id='7' author='decodyng' date='2020-12-21T19:31:34Z'>
 		Given this, I could see two opportunities to warn:
 
 The thing I was thinking of, where you've set your steps and envs poorly such that every batch will end up truncated
 Warning when n_steps*n_envs % batch_size !=0 in general, and just noting that the final batch in each buffer will end up truncated. This seems less important than (1) especially when the number of batches per buffer is large, but still potentially worth noting. (Obviously (1) is a subset situation of (2), but I think it'd be worth explicitly calling out since I think it's a larger problem when all batches will be truncated)
 
 We could also try to merge these into some kind of aggregated error, where we pre-calculate how often a truncated batch will happen, and explicitly say "k of every K batches will be truncated to a value of x", which might nicely cover both cases, and hopefully give people pause when they see "10 of every 10 batches will be truncated," or something like that
 This is a situation that, I think if you understand all the relevant parameters well, you hopefully wouldn't run into, but I think the nuance of how envs, steps, timesteps, and batch_size work together here is subtle enough that it's worth covering cases like these.
 		</comment>
 		<comment id='8' author='decodyng' date='2020-12-22T10:21:32Z'>
 		Just warn at model creation that the last minibatch size will have a size smaller than batch_size (and we can print those two) because n_envs * n_steps is not a multiple of batch_size
 		</comment>
 	</comments>
 </bug>
<commit id='b1aee717729e84d0e5a05e37324a6b1521db6e1b' author='Cody Wild' date='2021-01-11 17:03:32+01:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='docs\misc\changelog.rst' new_name='docs\misc\changelog.rst'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>38,39,40,54,542</added_lines>
 			<deleted_lines>538</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='stable_baselines3\ppo\ppo.py' new_name='stable_baselines3\ppo\ppo.py'>
 		<file_info nloc='234' complexity='17' token_count='1363'></file_info>
 		<modified_lines>
 			<added_lines>1,32,33,34,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138</added_lines>
 			<deleted_lines>31,118</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\test_utils.py' new_name='tests\test_utils.py'>
 		<file_info nloc='248' complexity='33' token_count='2191'></file_info>
 		<method name='test_ppo_warnings' parameters=''>
 				<method_info nloc='5' complexity='1' token_count='45' nesting_level='0' start_line='343' end_line='353'></method_info>
 			<added_lines>343,344,345,346,347,348,349,350,351,352,353</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_vec_env_monitor_kwargs' parameters=''>
 				<method_info nloc='14' complexity='1' token_count='141' nesting_level='0' start_line='73' end_line='89'></method_info>
 			<added_lines>83,84,85,86,87,88</added_lines>
 			<deleted_lines>83</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>9,341,342</added_lines>
 			<deleted_lines>9</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
