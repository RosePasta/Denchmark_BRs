<bug_data>
<bug id='659' author='DmitryUlyanov' open_date='2016-03-26T06:42:19Z' closed_time='2016-10-20T21:52:30Z'>
 	<summary>Torch + LMDB, not enough memory</summary>
 	<description>
 Hello, I am trying to use lmdb dataset that I created with DIGITS and torch defined model. The dataset has around 300k objects. The loading is interrupted by the error:
 &lt;denchmark-code&gt;2016-03-26 09:30:27 [INFO ] creating data readers
 2016-03-26 09:30:27 [INFO ] opening LMDB database: /place/home/dmitry/git/DIGITS/digits/jobs/20160325-003220-9caf/train_db
 2016-03-26 09:30:27 [INFO ] opening LMDB database: /place/home/dmitry/git/DIGITS/digits/jobs/20160325-003220-9caf/train_db
 2016-03-26 09:30:27 [INFO ] opening LMDB database: /place/home/dmitry/git/DIGITS/digits/jobs/20160325-003220-9caf/train_db
 2016-03-26 09:30:27 [INFO ] opening LMDB database: /place/home/dmitry/git/DIGITS/digits/jobs/20160325-003220-9caf/train_db
 2016-03-26 09:32:08 [FAIL] /home/dmitry/torch/install/share/lua/5.1/threads/threads.lua:264:
 [thread 4 callback] /place/home/dmitry/git/DIGITS/tools/torch/data.lua:326: not enough memory
 &lt;/denchmark-code&gt;
 
 I have tried to install different versions of LMDB lib, but it does not help. Thank you in advance.
 	</description>
 	<comments>
 		<comment id='1' author='DmitryUlyanov' date='2016-03-27T15:59:16Z'>
 		Thanks for the report &lt;denchmark-link:https://github.com/DmitryUlyanov&gt;@DmitryUlyanov&lt;/denchmark-link&gt;
 . And sorry to hear that you met this issue! I will try and reduce the memory footprint of DIGITS's data loader for Torch. In the meantime you might want to check whether reducing the number of loader threads helps with the memory issue. The default is 4 threads and this is coming from this &lt;denchmark-link:https://github.com/NVIDIA/DIGITS/blob/master/tools/torch/main.lua#L221&gt;line&lt;/denchmark-link&gt;
  (for the training set) and this &lt;denchmark-link:https://github.com/NVIDIA/DIGITS/blob/master/tools/torch/main.lua#L235&gt;line&lt;/denchmark-link&gt;
  (for the validation set).
 		</comment>
 		<comment id='2' author='DmitryUlyanov' date='2016-03-31T18:21:33Z'>
 		Hi &lt;denchmark-link:https://github.com/DmitryUlyanov&gt;@DmitryUlyanov&lt;/denchmark-link&gt;
  I notice that right after opening LMDB files, each thread allocates large amounts of memory in order to store keys to all entries. This takes in the order of 500MB worth of memory per thread per million keys. Did it help to reduce the number of threads? How much spare memory do you have on your system?
 		</comment>
 		<comment id='3' author='DmitryUlyanov' date='2016-03-31T22:43:16Z'>
 		Hello, thank you for the investigation. I did not have time to try to reduce the number of workers yet. I have 12GB GPU, AlexNet with 128 batch, which I assume needs ~4 GB. The server has 256GB RAM, so problem is not there probably.
 I tried to run MNIST example and it worked good for me. I will try reduce the number of workers in a while and report. Thanks!
 		</comment>
 		<comment id='4' author='DmitryUlyanov' date='2016-04-15T19:46:45Z'>
 		Just to bump this up, i added a variable to &lt;denchmark-link:https://github.com/NVIDIA/DIGITS/blob/master/tools/torch/main.lua#L221&gt;https://github.com/NVIDIA/DIGITS/blob/master/tools/torch/main.lua#L221&lt;/denchmark-link&gt;
  and &lt;denchmark-link:https://github.com/NVIDIA/DIGITS/blob/master/tools/torch/main.lua#L235&gt;https://github.com/NVIDIA/DIGITS/blob/master/tools/torch/main.lua#L235&lt;/denchmark-link&gt;
  so that the amount of used threads are now  instead of 4. I am running this with a Titan X; and it is using  for ImageNet, 1.3 million images of 256x256. LMDB size 140Gb. It seems to be working finally. &lt;denchmark-link:https://github.com/gheinrich&gt;@gheinrich&lt;/denchmark-link&gt;
  how did you come up with the number of 500MB per thread per million keys? If i have  of memory on my Titan, that does not compute. Thanks mate, and thanks for the thread-tip!
 Update: After waiting an hour, it doesnt want to train at all. GPU utilizaiton jumps up ever half a minute from a silent 0% to 50%+ for a split second and then back.
 |   3  GeForce GTX TIT...  Off  | 0000:04:00.0     Off |                  N/A |
 | 26%   65C    P2    85W / 250W |   5170MiB / 12287MiB |            Default |
 		</comment>
 		<comment id='5' author='DmitryUlyanov' date='2016-04-15T20:45:13Z'>
 		Hi Tim, I was referring to CPU memory footprint for storing LMDB keys. The GPU memory is mostly a function of batch size and the number of parameters in your model and this doesn't depend on the number of data loader threads.
 Today I have resumed working on this issue. I am trying to enable data loading from LMDB without storing keys at all, which would improve CPU memory consumption a lot. But I have troubles doing it in a multi-threaded environment. I'll keep you posted! Thanks for your patience!
 		</comment>
 		<comment id='6' author='DmitryUlyanov' date='2016-04-15T20:47:57Z'>
 		But as &lt;denchmark-link:https://github.com/DmitryUlyanov&gt;@DmitryUlyanov&lt;/denchmark-link&gt;
  noted with only 300k images and 256Gb CPU RAM and I have found out (by making it work with 1 thread), it does not seem to be related to the CPU's RAM.
 		</comment>
 		<comment id='7' author='DmitryUlyanov' date='2016-04-15T21:02:20Z'>
 		The memory error fires before we load the model into GPU. There might be restrictions in place on &lt;denchmark-link:https://github.com/DmitryUlyanov&gt;@DmitryUlyanov&lt;/denchmark-link&gt;
  's system that limit the maximum amount of memory that a single process is able to use so the error might fire well before we hit the hard 256GB limit. By all means, if you find anything please let us know. Thanks in advance!
 		</comment>
 		<comment id='8' author='DmitryUlyanov' date='2016-04-16T20:48:53Z'>
 		It turns out that Luajit limits heap memory to 2GB. So I am proposing a change to move LMDB keys from a Lua table to a tds.Vec (where items are allocated off the C heap and don't suffer from the Luajit 2GB heap limit).
 I was able to train 1 epoch of Alexnet on 10 million 256x256 RGB images. It took 4h47m on a Titan X and GPU utilization was consistently over 95%.
 &lt;denchmark-link:https://github.com/DmitryUlyanov&gt;@DmitryUlyanov&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/TimZaman&gt;@TimZaman&lt;/denchmark-link&gt;
  can you try PR &lt;denchmark-link:https://github.com/NVIDIA/DIGITS/pull/686&gt;#686&lt;/denchmark-link&gt;
 ?
 A further improvement would be to share LMDB keys between threads but I'd like to see if &lt;denchmark-link:https://github.com/NVIDIA/DIGITS/pull/686&gt;#686&lt;/denchmark-link&gt;
  is working for you first. Thanks!
 		</comment>
 		<comment id='9' author='DmitryUlyanov' date='2016-04-16T23:47:16Z'>
 		1M images, 256x256, batch size 256, vanilla AlexNet, Titan X, 4 loader threads. and your pull request: now works. On Caffe it took around an hour per epoch. This seems to be taking 2 hours per epoch. But I have a  1200% luajit CPU usage now. GPU utilisation is still swinging a lot between 0 and 100 (that could be the 8tb hdd i am using).
 		</comment>
 		<comment id='10' author='DmitryUlyanov' date='2016-04-17T08:43:41Z'>
 		Thank you very much for testing this Tim! Does the code look sensible to you? I wouldn't mind another pair of eyes to review this!
 I've created &lt;denchmark-link:https://github.com/NVIDIA/DIGITS/issues/687&gt;#687&lt;/denchmark-link&gt;
  to see if we can tackle the perf issue.
 		</comment>
 		<comment id='11' author='DmitryUlyanov' date='2016-10-20T21:52:30Z'>
 		&lt;denchmark-link:https://github.com/gheinrich&gt;@gheinrich&lt;/denchmark-link&gt;
 :  I'm experiencing this exact error () during model training with torch.  I see you've closed this issue, but I'm still not sure what I can do to address the problem.  My dataset is a large lmdb set created by DIGITS.
 I tried setting OMP_NUM_THREADS=1 and even setting batch_size=1.  The memory limits on the GPU and CPU do not grow very large, according to DIGITS.
 Will you please point me in the right direction here?  I can provide additional information, if needed.
 		</comment>
 		<comment id='12' author='DmitryUlyanov' date='2016-10-20T22:04:34Z'>
 		&lt;denchmark-link:https://github.com/michaelholm-ce&gt;@michaelholm-ce&lt;/denchmark-link&gt;
  do you have  installed as a lua package?
 		</comment>
 		<comment id='13' author='DmitryUlyanov' date='2016-10-20T22:10:47Z'>
 		&lt;denchmark-link:https://github.com/TimZaman&gt;@TimZaman&lt;/denchmark-link&gt;
 :  I did not, but now do, and am trying it.  Thank you.
 		</comment>
 	</comments>
 </bug>
<commit id='bc647b48b2ec356eb49dd3e73693b27542d950bb' author='Greg Heinrich' date='2016-04-16 22:52:10+02:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='docs\BuildTorch.md' new_name='docs\BuildTorch.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>66</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='scripts\travis\install-torch.sh' new_name='scripts\travis\install-torch.sh'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>24</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tools\torch\data.lua' new_name='tools\torch\data.lua'>
 		<file_info nloc='472' complexity='91' token_count='3172'></file_info>
 		<method name='DBSource' parameters=''>
 				<method_info nloc='11' complexity='2' token_count='61' nesting_level='0' start_line='324' end_line='334'></method_info>
 			<added_lines>325</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>14,15</added_lines>
 			<deleted_lines>323</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
