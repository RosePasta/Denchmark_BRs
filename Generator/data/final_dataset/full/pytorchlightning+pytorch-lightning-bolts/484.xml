<bug_data>
<bug id='484' author='briankosw' open_date='2020-12-26T01:34:37Z' closed_time='2021-01-18T10:24:26Z'>
 	<summary>VOCDetectionDataModule fails to load</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 I tried to use VOCDetectionDataModule, and it seems that I fails to properly load the dataset because of the problems with multiprocessing. See the below minimal repro.
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 Steps to reproduce the behavior:
 &gt;&gt;&gt; from pl_bolts.datamodules import VOCDetectionDataModule
 &gt;&gt;&gt; datamodule = VOCDetectionDataModule(data_dir=data_dir, num_workers=2)
 &gt;&gt;&gt; datamodule.prepare_data()
 &gt;&gt;&gt; train_loader = datamodule.train_dataloader(batch_size=1)
 &gt;&gt;&gt; next(iter(train_loader))
 ---------------------------------------------------------------------------
 AttributeError                            Traceback (most recent call last)
 &lt;ipython-input-22-0fd2476fd2ff&gt; in &lt;module&gt;
 ----&gt; 1 next(iter(train_loader))
 
 ~/anaconda3/envs/lightning-bolts/lib/python3.8/site-packages/torch/utils/data/dataloader.py in __iter__(self)
     350             return self._iterator
     351         else:
 --&gt; 352             return self._get_iterator()
     353
     354     @property
 
 ~/anaconda3/envs/lightning-bolts/lib/python3.8/site-packages/torch/utils/data/dataloader.py in _get_iterator(self)
     292             return _SingleProcessDataLoaderIter(self)
     293         else:
 --&gt; 294             return _MultiProcessingDataLoaderIter(self)
     295
     296     @property
 
 ~/anaconda3/envs/lightning-bolts/lib/python3.8/site-packages/torch/utils/data/dataloader.py in __init__(self, loader)
     799             #     before it starts, and __del__ tries to join but will get:
     800             #     AssertionError: can only join a started process.
 --&gt; 801             w.start()
     802             self._index_queues.append(index_queue)
     803             self._workers.append(w)
 
 ~/anaconda3/envs/lightning-bolts/lib/python3.8/multiprocessing/process.py in start(self)
     119                'daemonic processes are not allowed to have children'
     120         _cleanup()
 --&gt; 121         self._popen = self._Popen(self)
     122         self._sentinel = self._popen.sentinel
     123         # Avoid a refcycle if the target function holds an indirect
 
 ~/anaconda3/envs/lightning-bolts/lib/python3.8/multiprocessing/context.py in _Popen(process_obj)
     222     @staticmethod
     223     def _Popen(process_obj):
 --&gt; 224         return _default_context.get_context().Process._Popen(process_obj)
     225
     226 class DefaultContext(BaseContext):
 
 ~/anaconda3/envs/lightning-bolts/lib/python3.8/multiprocessing/context.py in _Popen(process_obj)
     282         def _Popen(process_obj):
     283             from .popen_spawn_posix import Popen
 --&gt; 284             return Popen(process_obj)
     285
     286     class ForkServerProcess(process.BaseProcess):
 
 ~/anaconda3/envs/lightning-bolts/lib/python3.8/multiprocessing/popen_spawn_posix.py in __init__(self, process_obj)
      30     def __init__(self, process_obj):
      31         self._fds = []
 ---&gt; 32         super().__init__(process_obj)
      33
      34     def duplicate_for_child(self, fd):
 
 ~/anaconda3/envs/lightning-bolts/lib/python3.8/multiprocessing/popen_fork.py in __init__(self, process_obj)
      17         self.returncode = None
      18         self.finalizer = None
 ---&gt; 19         self._launch(process_obj)
      20
      21     def duplicate_for_child(self, fd):
 
 ~/anaconda3/envs/lightning-bolts/lib/python3.8/multiprocessing/popen_spawn_posix.py in _launch(self, process_obj)
      45         try:
      46             reduction.dump(prep_data, fp)
 ---&gt; 47             reduction.dump(process_obj, fp)
      48         finally:
      49             set_spawning_popen(None)
 
 ~/anaconda3/envs/lightning-bolts/lib/python3.8/multiprocessing/reduction.py in dump(obj, file, protocol)
      58 def dump(obj, file, protocol=None):
      59     '''Replacement for pickle.dump() using ForkingPickler.'''
 ---&gt; 60     ForkingPickler(file, protocol).dump(obj)
      61
      62 #
 
 AttributeError: Can't pickle local object 'VOCDetectionDataModule._default_transforms.&lt;locals&gt;.&lt;lambda&gt;'
 If num_workers=0, then the above repro works perfectly fine. It's when num_workers&gt;0 that the repro fails.
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 
 PyTorch Version (e.g., 1.0): 1.7
 OS (e.g., Linux): macOS
 How you installed PyTorch (conda, pip, source): pip
 Build command you used (if compiling from source):
 Python version: 3.8.6
 CUDA/cuDNN version: N/A
 GPU models and configuration: N/A
 Any other relevant information:
 PyTorch Lightning: 1.1.2
 PyTorch Lightning Bolts: 0.2.5
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='briankosw' date='2020-12-26T03:06:05Z'>
 		If we run in terminal like python -m train ... with num_workers&gt;0, this will also be fine. But if we are working in Jupyter or IPython, it seems that we must set num_workers=0. Maybe this is not a bug?
 		</comment>
 		<comment id='2' author='briankosw' date='2020-12-26T03:48:44Z'>
 		I get the error when running in the terminal, so I'm not sure what's wrong. I manually reproduced this error by attempting to pickle the lambda function:
 import pickle
 import torchvision.transforms as transforms
 transform = lambda image, target: (transforms.ToTensor()(image), target)
 pickle.dumps(transform)
 I get the following error:
 &lt;denchmark-code&gt;---------------------------------------------------------------------------
 PicklingError                             Traceback (most recent call last)
 &lt;ipython-input-4-a283fa50ceb5&gt; in &lt;module&gt;
 ----&gt; 1 pickle.dumps(transform)
 
 PicklingError: Can't pickle &lt;function &lt;lambda&gt; at 0x7fe6b6b3f430&gt;: attribute lookup &lt;lambda&gt; on __main__ failed
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='3' author='briankosw' date='2020-12-27T15:25:19Z'>
 		Are torchvision transforms (especially lambda) pickleable ? I'm not sure.. They are JIT scriptable since 0.8.1 +. .
 Have a look at &lt;denchmark-link:https://stackoverflow.com/questions/25348532/can-python-pickle-lambda-functions&gt;this&lt;/denchmark-link&gt;
  thread.
 Posting a thread from the above link.
 &lt;denchmark-code&gt;what worked for me (windows 10, python 3.7) was to pass a function instead of a lambda function:
 
 def merge(x):
     return Image.merge("RGB", x.split()[::-1])
 
 transforms.Lambda(merge)
 
 instead of:
 
 transforms.Lambda(lambda x: Image.merge("RGB", x.split()[::-1]))
 &lt;/denchmark-code&gt;
 
 Also IIRC this bug occurs on Windows especially, we cannot pickle lambda objects. I really don't remember the thread but read somewhere in PyTorch discussions.
 Another &lt;denchmark-link:https://stackoverflow.com/questions/64347217/error-pickle-picklingerror-cant-pickle-function-lambda-at-0x0000002f2175b&gt;thread&lt;/denchmark-link&gt;
  that points this (Almost exactly what we are doing here).
 I think this is MacOS + Windows issue, I have tried to reproduce over linux as below.
 I ran the following on Linux, Ubuntu 20.04, torch 1.7.1, torchvision 0.8.2 and could not get any error.
 &lt;denchmark-code&gt;import pickle
 import torchvision.transforms as transforms
 import torch
 
 if __name__ == "main":
     image = torch.randn(3, 224, 224)
     target = torch.randn(1, 10)
     transform = lambda image, target: (transforms.ToTensor()(image), target)
     pickle.dumps(transform)
 
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='4' author='briankosw' date='2020-12-27T15:55:20Z'>
 		Hm it does seem that you're absolutely on the money, since I ran your script on my computer (macOS) and I get the pickle error still. I'll try to narrow down the root cause of failure so that we can properly address this. I do think that the VOC detection data module implementation can avoid using  as well as be improved in general in certain ways. What do you think &lt;denchmark-link:https://github.com/oke-aditya&gt;@oke-aditya&lt;/denchmark-link&gt;
 ?
 		</comment>
 		<comment id='5' author='briankosw' date='2020-12-27T16:11:28Z'>
 		Let's see, but I tihnk we can avoid lamda I think from &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/master/pl_bolts/datamodules/vocdetection_datamodule.py#L204&gt;line&lt;/denchmark-link&gt;
 
 Let's hear from &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
  and &lt;denchmark-link:https://github.com/akihironitta&gt;@akihironitta&lt;/denchmark-link&gt;
  .
 		</comment>
 		<comment id='6' author='briankosw' date='2021-01-06T08:05:33Z'>
 		I saw a similar issue yesterday in PL example with Windows, and strange that it was such for a very long time and suddenly appeared yesterday... see: &lt;denchmark-link:https://stackoverflow.com/a/59680818/4521646&gt;https://stackoverflow.com/a/59680818/4521646&lt;/denchmark-link&gt;
 
 
 Let's see, but I tihnk we can avoid lamda I think from line
 
 yes, let's replace the lambda function :] &lt;denchmark-link:https://github.com/briankosw&gt;@briankosw&lt;/denchmark-link&gt;
  want to take it over?
 		</comment>
 	</comments>
 </bug>
<commit id='67ebfe0ab6914187c013251fff490b30b24faa1a' author='Brian Ko' date='2021-01-18 10:13:54+01:00'>
 	<dmm_unit complexity='0.0' interfacing='0.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pl_bolts\datamodules\vocdetection_datamodule.py' new_name='pl_bolts\datamodules\vocdetection_datamodule.py'>
 		<file_info nloc='151' complexity='20' token_count='894'></file_info>
 		<method name='__init__' parameters='self,transforms,image_transforms'>
 				<method_info nloc='3' complexity='1' token_count='21' nesting_level='1' start_line='20' end_line='22'></method_info>
 			<added_lines>20,22</added_lines>
 			<deleted_lines>20</deleted_lines>
 		</method>
 		<method name='train_dataloader' parameters='self,batch_size,transforms'>
 				<method_info nloc='17' complexity='4' token_count='113' nesting_level='1' start_line='149' end_line='173'></method_info>
 			<added_lines>152,160,161,162</added_lines>
 			<deleted_lines>149,157,158,159,160,161,162</deleted_lines>
 		</method>
 		<method name='__call__' parameters='self,image,target'>
 				<method_info nloc='6' complexity='3' token_count='43' nesting_level='1' start_line='24' end_line='29'></method_info>
 			<added_lines>27,28</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='train_dataloader' parameters='self,batch_size,image_transforms'>
 				<method_info nloc='15' complexity='3' token_count='103' nesting_level='1' start_line='152' end_line='173'></method_info>
 			<added_lines>152,160,161,162</added_lines>
 			<deleted_lines>157,158,159,160,161,162</deleted_lines>
 		</method>
 		<method name='_default_transforms' parameters='self'>
 				<method_info nloc='9' complexity='2' token_count='79' nesting_level='1' start_line='198' end_line='206'></method_info>
 			<added_lines>200,201,202,203,204,205,206</added_lines>
 			<deleted_lines>202,203,204,205,206</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,transforms'>
 				<method_info nloc='2' complexity='1' token_count='12' nesting_level='1' start_line='20' end_line='21'></method_info>
 			<added_lines>20</added_lines>
 			<deleted_lines>20</deleted_lines>
 		</method>
 		<method name='val_dataloader' parameters='self,batch_size,transforms'>
 				<method_info nloc='17' complexity='4' token_count='111' nesting_level='1' start_line='175' end_line='198'></method_info>
 			<added_lines>175,183,184,185</added_lines>
 			<deleted_lines>175,183,184,185,186,187</deleted_lines>
 		</method>
 		<method name='val_dataloader' parameters='self,batch_size,image_transforms'>
 				<method_info nloc='15' complexity='3' token_count='101' nesting_level='1' start_line='175' end_line='196'></method_info>
 			<added_lines>175,183,184,185</added_lines>
 			<deleted_lines>175,183,184,185,186,187</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>9</added_lines>
 			<deleted_lines>9,207,208,209,210,211</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
