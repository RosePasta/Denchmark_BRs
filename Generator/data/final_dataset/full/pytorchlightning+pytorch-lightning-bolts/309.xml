<bug_data>
<bug id='309' author='FrankXinqi' open_date='2020-10-25T17:05:41Z' closed_time='2020-11-17T19:36:33Z'>
 	<summary>How to use SSLOnlineEvaluator callback</summary>
 	<description>
 &lt;denchmark-h:h2&gt;‚ùì Questions and Help&lt;/denchmark-h&gt;
 
 &lt;denchmark-h:h3&gt;Before asking:&lt;/denchmark-h&gt;
 
 Lightning Forum Posted Question: &lt;denchmark-link:https://forums.pytorchlightning.ai/t/how-to-use-sslonlineevaluator/309&gt;https://forums.pytorchlightning.ai/t/how-to-use-sslonlineevaluator/309&lt;/denchmark-link&gt;
 
 &lt;denchmark-h:h4&gt;What is your question?&lt;/denchmark-h&gt;
 
 I study the code provided here SSL SimCLR on colab, and implemented a similar code.
 I have modified the datamodule to load mine, and I can run the code. However, if I use SSLOnlineEvaluator. I cannot make it work. If I specify the callbacks for model, the error shows:
 TypeError: on_train_batch_end() takes 6 positional arguments but 7 were given
 &lt;denchmark-h:h4&gt;Code&lt;/denchmark-h&gt;
 
 I have the following code:
 &lt;denchmark-code&gt;def to_device(batch, device):
     (img1, _), y = batch
     img1 = img1.to(device)
     y = y.to(device)
     return img1, y
 
 online_finetuner = SSLOnlineEvaluator(z_dim=2048 * 2 * 2, num_classes = 7)
 online_finetuner.to_device = to_device
 
 lr_logger = LearningRateMonitor()
 
 callbacks = [online_finetuner, lr_logger]
 
 # pick data
 rafdb_height=100
 batch_size=64
 
 # data
 dm = RAFDBDataModule(num_workers=0)
 dm.train_transforms = SimCLRTrainDataTransform(input_height=rafdb_height)
 dm.val_transforms = SimCLREvalDataTransform(input_height=rafdb_height)
 dm.test_transforms = SimCLREvalDataTransform(input_height=rafdb_height)
 
 # model
 model = SimCLR(num_samples=dm.num_samples, batch_size=batch_size)
 
 # fit
 trainer = pl.Trainer(gpus=1, max_epochs=20, callbacks=callbacks)
 trainer.fit(model, dm)
 &lt;/denchmark-code&gt;
 
 Full Output information:
 &lt;denchmark-code&gt;GPU available: True, used: True
 TPU available: False, using: 0 TPU cores
 LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
 
   | Name                 | Type         | Params
 ------------------------------------------------------
 0 | encoder              | ResNet       | 25 M  
 1 | projection           | Projection   | 4 M   
 2 | non_linear_evaluator | SSLEvaluator | 8 M   
 Epoch 0:   0%|          | 0/383 [00:00&lt;?, ?it/s] Traceback (most recent call last):
   File "H:/Project/PycharmProject/FER_Long-tailed/train_SimCLR.py", line 236, in &lt;module&gt;
     trainer.fit(model, dm)
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 440, in fit
     results = self.accelerator_backend.train()
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\accelerators\gpu_accelerator.py", line 54, in train
     results = self.train_or_test()
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\accelerators\accelerator.py", line 66, in train_or_test
     results = self.trainer.train()
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 483, in train
     self.train_loop.run_training_epoch()
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\trainer\training_loop.py", line 557, in run_training_epoch
     self.on_train_batch_end(epoch_output, epoch_end_outputs, batch, batch_idx, dataloader_idx)
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\trainer\training_loop.py", line 249, in on_train_batch_end
     self.trainer.call_hook('on_train_batch_end', epoch_end_outputs, batch, batch_idx, dataloader_idx)
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 823, in call_hook
     trainer_hook(*args, **kwargs)
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\trainer\callback_hook.py", line 147, in on_train_batch_end
     callback.on_train_batch_end(self, self.get_model(), outputs, batch, batch_idx, dataloader_idx)
 TypeError: on_train_batch_end() takes 6 positional arguments but 7 were given
 Epoch 0:   0%|          | 0/383 [00:02&lt;?, ?it/s]
 
 Process finished with exit code 1
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h4&gt;What have you tried?&lt;/denchmark-h&gt;
 
 Try to put OnlineEvaluator as the only callback, but it does not work as well.
 &lt;denchmark-h:h4&gt;What's your environment?&lt;/denchmark-h&gt;
 
 
 OS: Win10
 Packaging [e.g. pip, conda]
 Version:
 python                    3.8.6
 pytorch                   1.6.0
 pytorch-lightning         1.0.2                      py_0    conda-forge
 pytorch-lightning-bolts   0.2.5                    pypi_0    pypi
 
 	</description>
 	<comments>
 		<comment id='1' author='FrankXinqi' date='2020-10-25T17:54:00Z'>
 		I believe this PR &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/pull/277&gt;#277&lt;/denchmark-link&gt;
  fixed this
 		</comment>
 		<comment id='2' author='FrankXinqi' date='2020-10-26T01:44:56Z'>
 		Cool. I see that an 'output' arg was added. I will install the latest version from GitHub.
 Just an offset question: If I do not want to install the blots package again by
 pip install git+https://github.com/PytorchLightning/pytorch-lightning-bolts.git@master --upgrade, instead I download the package from GitHub, put it in my project folder, and just call any function by its relative path. Would this work with this kind of big project?
 		</comment>
 		<comment id='3' author='FrankXinqi' date='2020-10-26T02:02:40Z'>
 		I never tried that but you should be able to make it work. You probably have to add it to the python path or similar to have the imports working.
 We don't recommend this, because you won't get any updates and have to manually repeat the process if you need to get  new version.
 		</comment>
 		<comment id='4' author='FrankXinqi' date='2020-10-26T02:12:37Z'>
 		Cool. There is a new bug, when I upgrade to the master version.  The network maltiplication has a dimension not match as
 &lt;denchmark-code&gt;  File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\torch\nn\functional.py", line 1676, in linear
     output = input.matmul(weight.t())
 RuntimeError: mat1 dim 1 must match mat2 dim 0
 &lt;/denchmark-code&gt;
 
 The output information
 &lt;denchmark-code&gt;GPU available: True, used: True
 TPU available: False, using: 0 TPU cores
 LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
 
   | Name                 | Type         | Params
 ------------------------------------------------------
 0 | encoder              | ResNet       | 25 M  
 1 | projection           | Projection   | 4 M   
 2 | non_linear_evaluator | SSLEvaluator | 8 M   
 Epoch 0:   0%|          | 0/383 [00:00&lt;?, ?it/s] Traceback (most recent call last):
   File "H:/Project/PycharmProject/FER_Long-tailed/train_SimCLR.py", line 276, in &lt;module&gt;
     trainer.fit(model, dm)
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 440, in fit
     results = self.accelerator_backend.train()
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\accelerators\gpu_accelerator.py", line 54, in train
     results = self.train_or_test()
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\accelerators\accelerator.py", line 66, in train_or_test
     results = self.trainer.train()
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 483, in train
     self.train_loop.run_training_epoch()
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\trainer\training_loop.py", line 557, in run_training_epoch
     self.on_train_batch_end(epoch_output, epoch_end_outputs, batch, batch_idx, dataloader_idx)
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\trainer\training_loop.py", line 249, in on_train_batch_end
     self.trainer.call_hook('on_train_batch_end', epoch_end_outputs, batch, batch_idx, dataloader_idx)
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 823, in call_hook
     trainer_hook(*args, **kwargs)
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\trainer\callback_hook.py", line 147, in on_train_batch_end
     callback.on_train_batch_end(self, self.get_model(), outputs, batch, batch_idx, dataloader_idx)
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pl_bolts\callbacks\ssl_online.py", line 85, in on_train_batch_end
     mlp_preds = pl_module.non_linear_evaluator(representations)
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\torch\nn\modules\module.py", line 722, in _call_impl
     result = self.forward(*input, **kwargs)
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pl_bolts\models\self_supervised\evaluator.py", line 30, in forward
     logits = self.block_forward(x)
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\torch\nn\modules\module.py", line 722, in _call_impl
     result = self.forward(*input, **kwargs)
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\torch\nn\modules\container.py", line 117, in forward
     input = module(input)
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\torch\nn\modules\module.py", line 722, in _call_impl
     result = self.forward(*input, **kwargs)
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\torch\nn\modules\linear.py", line 91, in forward
     return F.linear(input, self.weight, self.bias)
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\torch\nn\functional.py", line 1676, in linear
     output = input.matmul(weight.t())
 RuntimeError: mat1 dim 1 must match mat2 dim 0
 Epoch 0:   0%|          | 0/383 [00:00&lt;?, ?it/s]
 &lt;/denchmark-code&gt;
 
 This bug only happens if I add callback. I works fine for a few time(not sure if it is step, iterations ...), but then it gives input.shape=32,100352, weight.t().shape=8192,1024.
 		</comment>
 		<comment id='5' author='FrankXinqi' date='2020-10-26T02:29:55Z'>
 		The above mentioned issue is with my customed datamodule. Here is a different issue with official CIFAR10.
 The issue seems to relate with CUDA, not sure if the problem is on my side.  Just to make it clear, all codes run well without callbacks passed in.
 &lt;denchmark-code&gt;GPU available: True, used: True
 TPU available: False, using: 0 TPU cores
 LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
 Files already downloaded and verified
 Files already downloaded and verified
 
   | Name                 | Type         | Params
 ------------------------------------------------------
 0 | encoder              | ResNet       | 25 M  
 1 | projection           | Projection   | 4 M   
 2 | non_linear_evaluator | SSLEvaluator | 8 M   
 Epoch 0:   0%|          | 0/1562 [00:00&lt;?, ?it/s] C:/cb/pytorch_1000000000000/work/aten/src/THCUNN/ClassNLLCriterion.cu:108: block: [0,0,0], thread: [0,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
 C:/cb/pytorch_1000000000000/work/aten/src/THCUNN/ClassNLLCriterion.cu:108: block: [0,0,0], thread: [1,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
 C:/cb/pytorch_1000000000000/work/aten/src/THCUNN/ClassNLLCriterion.cu:108: block: [0,0,0], thread: [4,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
 C:/cb/pytorch_1000000000000/work/aten/src/THCUNN/ClassNLLCriterion.cu:108: block: [0,0,0], thread: [8,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
 C:/cb/pytorch_1000000000000/work/aten/src/THCUNN/ClassNLLCriterion.cu:108: block: [0,0,0], thread: [9,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
 C:/cb/pytorch_1000000000000/work/aten/src/THCUNN/ClassNLLCriterion.cu:108: block: [0,0,0], thread: [11,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
 C:/cb/pytorch_1000000000000/work/aten/src/THCUNN/ClassNLLCriterion.cu:108: block: [0,0,0], thread: [12,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
 C:/cb/pytorch_1000000000000/work/aten/src/THCUNN/ClassNLLCriterion.cu:108: block: [0,0,0], thread: [14,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
 C:/cb/pytorch_1000000000000/work/aten/src/THCUNN/ClassNLLCriterion.cu:108: block: [0,0,0], thread: [15,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
 C:/cb/pytorch_1000000000000/work/aten/src/THCUNN/ClassNLLCriterion.cu:108: block: [0,0,0], thread: [20,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
 C:/cb/pytorch_1000000000000/work/aten/src/THCUNN/ClassNLLCriterion.cu:108: block: [0,0,0], thread: [24,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
 C:/cb/pytorch_1000000000000/work/aten/src/THCUNN/ClassNLLCriterion.cu:108: block: [0,0,0], thread: [26,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
 C:/cb/pytorch_1000000000000/work/aten/src/THCUNN/ClassNLLCriterion.cu:108: block: [0,0,0], thread: [27,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
 C:/cb/pytorch_1000000000000/work/aten/src/THCUNN/ClassNLLCriterion.cu:108: block: [0,0,0], thread: [31,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
 Traceback (most recent call last):
   File "H:/Project/PycharmProject/FER_Long-tailed/example/SimCLR.py", line 36, in &lt;module&gt;
     trainer.fit(model, dm)
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 440, in fit
     results = self.accelerator_backend.train()
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\accelerators\gpu_accelerator.py", line 54, in train
     results = self.train_or_test()
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\accelerators\accelerator.py", line 66, in train_or_test
     results = self.trainer.train()
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 483, in train
     self.train_loop.run_training_epoch()
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\trainer\training_loop.py", line 557, in run_training_epoch
     self.on_train_batch_end(epoch_output, epoch_end_outputs, batch, batch_idx, dataloader_idx)
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\trainer\training_loop.py", line 249, in on_train_batch_end
     self.trainer.call_hook('on_train_batch_end', epoch_end_outputs, batch, batch_idx, dataloader_idx)
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 823, in call_hook
     trainer_hook(*args, **kwargs)
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\trainer\callback_hook.py", line 147, in on_train_batch_end
     callback.on_train_batch_end(self, self.get_model(), outputs, batch, batch_idx, dataloader_idx)
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pl_bolts\callbacks\ssl_online.py", line 95, in on_train_batch_end
     acc = accuracy(mlp_preds, y, num_classes=trainer.datamodule.num_classes)
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\metrics\functional\classification.py", line 286, in accuracy
     tps, fps, tns, fns, sups = stat_scores_multiple_classes(
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\metrics\functional\classification.py", line 197, in stat_scores_multiple_classes
     num_classes = get_num_classes(pred=pred, target=target, num_classes=num_classes)
   File "F:\Program\Anaconda3\envs\pytorch-lightning\lib\site-packages\pytorch_lightning\metrics\functional\classification.py", line 96, in get_num_classes
     num_target_classes = int(target.max().detach().item() + 1)
 RuntimeError: CUDA error: device-side assert triggered
 Epoch 0:   0%|          | 0/1562 [00:00&lt;?, ?it/s]
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='6' author='FrankXinqi' date='2020-11-16T06:53:41Z'>
 		&lt;denchmark-link:https://github.com/FrankXinqi&gt;@FrankXinqi&lt;/denchmark-link&gt;
  what is the number of classes being passed to SSLOnlineEvaluator. What looks to me is the case when num_classes being passed is different than the class labels found in target tensor.
 		</comment>
 		<comment id='7' author='FrankXinqi' date='2020-11-16T06:55:54Z'>
 		&lt;denchmark-link:https://github.com/FrankXinqi&gt;@FrankXinqi&lt;/denchmark-link&gt;
  for the first issue, I am merging a fix which should solve this issue. The output of resnet now is a flattened 2048 dimensional feature vector which is taken by the online evaluation callback.
 		</comment>
 		<comment id='8' author='FrankXinqi' date='2020-11-16T07:42:56Z'>
 		
 @FrankXinqi what is the number of classes being passed to SSLOnlineEvaluator. What looks to me is the case when num_classes being passed is different than the class labels found in target tensor.
 
 My cutomized dataset has 7 output classes. For CIFAR10, it put is as 10.  However, neither my own dataset or pl CIFAR10 do not work.
 		</comment>
 		<comment id='9' author='FrankXinqi' date='2020-11-16T07:45:52Z'>
 		
 @FrankXinqi for the first issue, I am merging a fix which should solve this issue. The output of resnet now is a flattened 2048 dimensional feature vector which is taken by the online evaluation callback.
 
 Many thanks. For the first issue, do you mean this issue "RuntimeError: mat1 dim 1 must match mat2 dim 0"?  If so, it may take some time to merge your PR to the master branch. Would you mind pointing out the solution, so I can fix it locally?
 Thx.
 		</comment>
 		<comment id='10' author='FrankXinqi' date='2020-11-16T08:43:52Z'>
 		&lt;denchmark-link:https://github.com/FrankXinqi&gt;@FrankXinqi&lt;/denchmark-link&gt;
  changed quite a few things around, like a whole revamp of simclr, difficult to point to a single change.
 &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/pull/329&gt;#329&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='11' author='FrankXinqi' date='2020-11-16T10:24:39Z'>
 		
 @FrankXinqi changed quite a few things around, like a whole revamp of simclr, difficult to point to a single change.
 #329
 
 Thanks. Seems like there was a merge 14 days ago, but not recently. Wanna to know if I reinstall the lastest one from Github, will the problem be fixed?
 		</comment>
 		<comment id='12' author='FrankXinqi' date='2020-11-16T17:40:26Z'>
 		&lt;denchmark-link:https://github.com/FrankXinqi&gt;@FrankXinqi&lt;/denchmark-link&gt;
 
 &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/tree/fix/simclr&gt;https://github.com/PyTorchLightning/pytorch-lightning-bolts/tree/fix/simclr&lt;/denchmark-link&gt;
 
 This is the branch that will be merged. The PR will close this issue because we have removed all the complexity of SSLOnlineEvaluator callback. Feel free to open new issues based on the new code base if something doesn't work still.
 		</comment>
 		<comment id='13' author='FrankXinqi' date='2020-11-17T02:00:31Z'>
 		Thanks. I will try it later and response to this issue still, if there are other bugs met.
 		</comment>
 	</comments>
 </bug>
<commit id='77ff983085cc4889b9cb5a439ec3eb2b2536599c' author='Ananya Harsh Jha' date='2020-11-17 14:36:31-05:00'>
 	<dmm_unit complexity='0.45901639344262296' interfacing='0.907103825136612' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='docs\source\self_supervised_models.rst' new_name='docs\source\self_supervised_models.rst'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>277,293,314,317,318,319,320,324,325,326,327,328,329,330,331,332,333,334,335,336,434</added_lines>
 			<deleted_lines>292,313,316,317,318,319,323,324,325,326,327,425</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pl_bolts\callbacks\ssl_online.py' new_name='pl_bolts\callbacks\ssl_online.py'>
 		<file_info nloc='85' complexity='7' token_count='540'></file_info>
 		<method name='__init__' parameters='self,str,float,None,int,int'>
 				<method_info nloc='7' complexity='1' token_count='37' nesting_level='1' start_line='27' end_line='33'></method_info>
 			<added_lines>29</added_lines>
 			<deleted_lines>33</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,float,None,int,int,str'>
 				<method_info nloc='7' complexity='1' token_count='38' nesting_level='1' start_line='27' end_line='33'></method_info>
 			<added_lines>29</added_lines>
 			<deleted_lines>33</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>37</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pl_bolts\models\self_supervised\byol\byol_module.py' new_name='pl_bolts\models\self_supervised\byol\byol_module.py'>
 		<file_info nloc='163' complexity='13' token_count='1042'></file_info>
 		<method name='cli_main' parameters=''>
 				<method_info nloc='33' complexity='4' token_count='286' nesting_level='0' start_line='178' end_line='228'></method_info>
 			<added_lines>224</added_lines>
 			<deleted_lines>223,224,225,226,227,228</deleted_lines>
 		</method>
 		<method name='cli_main.to_device' parameters='batch,device'>
 				<method_info nloc='5' complexity='1' token_count='36' nesting_level='1' start_line='223' end_line='227'></method_info>
 			<added_lines>224</added_lines>
 			<deleted_lines>223,224,225,226,227</deleted_lines>
 		</method>
 		<method name='shared_step' parameters='self,batch,batch_idx'>
 				<method_info nloc='12' complexity='1' token_count='119' nesting_level='1' start_line='107' end_line='126'></method_info>
 			<added_lines>108</added_lines>
 			<deleted_lines>108</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>230,231</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pl_bolts\models\self_supervised\byol\models.py' new_name='pl_bolts\models\self_supervised\byol\models.py'>
 		<file_info nloc='28' complexity='5' token_count='222'></file_info>
 		<method name='__init__' parameters='self,encoder'>
 				<method_info nloc='8' complexity='2' token_count='63' nesting_level='1' start_line='23' end_line='35'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>30,31</deleted_lines>
 		</method>
 		<method name='forward' parameters='self,x'>
 				<method_info nloc='7' complexity='1' token_count='64' nesting_level='1' start_line='37' end_line='43'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>39,40</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pl_bolts\models\self_supervised\resnets.py' new_name='pl_bolts\models\self_supervised\resnets.py'>
 		<file_info nloc='307' complexity='27' token_count='2042'></file_info>
 		<method name='__init__' parameters='self,inplanes,planes,stride,downsample,groups,base_width,dilation,norm_layer'>
 				<method_info nloc='2' complexity='1' token_count='33' nesting_level='1' start_line='135' end_line='136'></method_info>
 			<added_lines>135,136</added_lines>
 			<deleted_lines>135,136</deleted_lines>
 		</method>
 		<method name='resnet101' parameters='bool,bool,kwargs'>
 				<method_info nloc='9' complexity='1' token_count='44' nesting_level='0' start_line='301' end_line='309'></method_info>
 			<added_lines>309</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,block,layers,num_classes,zero_init_residual,groups,width_per_group,replace_stride_with_dilation,norm_layer,return_all_feature_maps,first_conv,maxpool1'>
 				<method_info nloc='13' complexity='1' token_count='44' nesting_level='1' start_line='134' end_line='146'></method_info>
 			<added_lines>134,135,136,137,138,139,140,141,142,143,144,145,146</added_lines>
 			<deleted_lines>134,135,136,137</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,block,layers,num_classes,zero_init_residual,groups,width_per_group,replace_stride_with_dilation,norm_layer,return_all_feature_maps'>
 				<method_info nloc='3' complexity='1' token_count='37' nesting_level='1' start_line='177' end_line='179'></method_info>
 			<added_lines>177,178,179</added_lines>
 			<deleted_lines>177,178,179</deleted_lines>
 		</method>
 		<method name='forward' parameters='self,x'>
 				<method_info nloc='19' complexity='2' token_count='144' nesting_level='1' start_line='234' end_line='256'></method_info>
 			<added_lines>253,254,255</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='resnet50_bn' parameters='bool,bool,kwargs'>
 				<method_info nloc='9' complexity='1' token_count='44' nesting_level='0' start_line='317' end_line='325'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>317,318,319,320,321,322,323,324,325</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1,28,147,165,166,167,168,169,170,171,172,173,174,180,181,182</added_lines>
 			<deleted_lines>15,28,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,197,198,201,326,327,336</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pl_bolts\models\self_supervised\simclr\simclr_finetuner.py' new_name='pl_bolts\models\self_supervised\simclr\simclr_finetuner.py'>
 		<file_info nloc='142' complexity='5' token_count='935'></file_info>
 		<method name='cli_main' parameters=''>
 				<method_info nloc='129' complexity='5' token_count='868' nesting_level='0' start_line='16' end_line='162'></method_info>
 			<added_lines>17,22,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,41,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,66,67,68,70,71,72,73,74,75,78,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,96,97,98,99,100,101,102,103,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161</added_lines>
 			<deleted_lines>16,17,19,20,22,23,25,27,28,29,30,31,34,37,39,40,41,43,44,45,46,47,49,50,51,52</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>6,7,8,9,10,11,12,13</added_lines>
 			<deleted_lines>6,7,11</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pl_bolts\models\self_supervised\simclr\simclr_module.py' new_name='pl_bolts\models\self_supervised\simclr\simclr_module.py'>
 		<file_info nloc='360' complexity='40' token_count='2718'></file_info>
 		<method name='__init__' parameters='self,input_dim,hidden_dim,output_dim'>
 				<method_info nloc='10' complexity='1' token_count='92' nesting_level='1' start_line='50' end_line='60'></method_info>
 			<added_lines>57</added_lines>
 			<deleted_lines>57,58,59,60</deleted_lines>
 		</method>
 		<method name='nt_xent_loss' parameters='self,out_1,out_2,temperature,eps'>
 				<method_info nloc='18' complexity='3' token_count='227' nesting_level='1' start_line='279' end_line='316'></method_info>
 			<added_lines>279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,int,int,int,float,float,float,kwargs'>
 				<method_info nloc='8' complexity='1' token_count='46' nesting_level='1' start_line='57' end_line='64'></method_info>
 			<added_lines>57</added_lines>
 			<deleted_lines>57,58,59,60,61,62,63,64</deleted_lines>
 		</method>
 		<method name='shared_step' parameters='self,batch,batch_idx'>
 				<method_info nloc='11' complexity='2' token_count='90' nesting_level='1' start_line='168' end_line='190'></method_info>
 			<added_lines>168,170,171,173,174,175,177,178,179,181,183,186,188</added_lines>
 			<deleted_lines>168,169,170,171,172,173,174,175,177,178,179,180,182,183,184,185,186,188</deleted_lines>
 		</method>
 		<method name='optimizer_step' parameters='self,int,int,Optimizer,int,None,bool,bool,bool'>
 				<method_info nloc='10' complexity='1' token_count='47' nesting_level='1' start_line='246' end_line='255'></method_info>
 			<added_lines>246,247,248,249,250,251,252,253,254,255</added_lines>
 			<deleted_lines>246,247,249,250,251,252,253</deleted_lines>
 		</method>
 		<method name='init_encoder' parameters='self'>
 				<method_info nloc='10' complexity='1' token_count='43' nesting_level='1' start_line='83' end_line='94'></method_info>
 			<added_lines>83,84,85,86,87,88,89,90,91</added_lines>
 			<deleted_lines>83,84,85,86,87,88,89,90,91,92,94</deleted_lines>
 		</method>
 		<method name='setup' parameters='self,stage'>
 				<method_info nloc='3' complexity='1' token_count='30' nesting_level='1' start_line='113' end_line='115'></method_info>
 			<added_lines>113,114,115</added_lines>
 			<deleted_lines>113,114,115</deleted_lines>
 		</method>
 		<method name='add_model_specific_args' parameters='parent_parser'>
 				<method_info nloc='30' complexity='1' token_count='466' nesting_level='1' start_line='319' end_line='357'></method_info>
 			<added_lines>321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='cli_main' parameters=''>
 				<method_info nloc='99' complexity='11' token_count='606' nesting_level='0' start_line='360' end_line='483'></method_info>
 			<added_lines>362,363,371,372,373,374,375,376,382,383,384,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,460,461,462,463,464,465,466,467,468,469,471,472,473,474,475,476,477,478,479,480,481</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='configure_optimizers' parameters='self'>
 				<method_info nloc='28' complexity='5' token_count='124' nesting_level='1' start_line='214' end_line='244'></method_info>
 			<added_lines>214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244</added_lines>
 			<deleted_lines>214,221,225,226,227,232,233,234,235,236,237,239,240</deleted_lines>
 		</method>
 		<method name='exclude_from_wt_decay' parameters='self,named_params,weight_decay,skip_list'>
 				<method_info nloc='14' complexity='5' token_count='88' nesting_level='1' start_line='197' end_line='212'></method_info>
 			<added_lines>197,198,199,201,202,203,204,205,206,207,209,210,211,212</added_lines>
 			<deleted_lines>197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212</deleted_lines>
 		</method>
 		<method name='forward' parameters='self,x'>
 				<method_info nloc='2' complexity='1' token_count='18' nesting_level='1' start_line='161' end_line='163'></method_info>
 			<added_lines>161,162,163</added_lines>
 			<deleted_lines>163</deleted_lines>
 		</method>
 		<method name='init_model' parameters='self'>
 				<method_info nloc='8' complexity='3' token_count='44' nesting_level='1' start_line='151' end_line='159'></method_info>
 			<added_lines>151,152,153,154,155,157,158</added_lines>
 			<deleted_lines>151,152,153,154,157,159</deleted_lines>
 		</method>
 		<method name='validation_step' parameters='self,batch,batch_idx'>
 				<method_info nloc='4' complexity='1' token_count='35' nesting_level='1' start_line='191' end_line='195'></method_info>
 			<added_lines>192,194</added_lines>
 			<deleted_lines>195</deleted_lines>
 		</method>
 		<method name='training_step' parameters='self,batch,batch_idx'>
 				<method_info nloc='4' complexity='1' token_count='35' nesting_level='1' start_line='185' end_line='189'></method_info>
 			<added_lines>186,188</added_lines>
 			<deleted_lines>185,186,188</deleted_lines>
 		</method>
 		<method name='shared_step' parameters='self,batch'>
 				<method_info nloc='11' complexity='2' token_count='78' nesting_level='1' start_line='165' end_line='183'></method_info>
 			<added_lines>165,166,167,168,170,171,173,174,175,177,178,179,181,183</added_lines>
 			<deleted_lines>165,168,169,170,171,172,173,174,175,177,178,179,180,182,183</deleted_lines>
 		</method>
 		<method name='forward' parameters='ctx,tensor'>
 				<method_info nloc='8' complexity='2' token_count='62' nesting_level='1' start_line='27' end_line='37'></method_info>
 			<added_lines>27,28,30,31,32,34,35,37</added_lines>
 			<deleted_lines>27,29,30,31,32,33</deleted_lines>
 		</method>
 		<method name='cli_main.to_device' parameters='batch,device'>
 				<method_info nloc='5' complexity='1' token_count='36' nesting_level='1' start_line='258' end_line='262'></method_info>
 			<added_lines>258,259,260,261,262</added_lines>
 			<deleted_lines>258,259,260,261,262</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self'>
 				<method_info nloc='4' complexity='1' token_count='34' nesting_level='1' start_line='24' end_line='27'></method_info>
 			<added_lines>25,26,27</added_lines>
 			<deleted_lines>24,25,26,27</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,int,int,int,str,int,str,int,int,int,int,float,bool,bool,str,bool,bool,float,float,float,float,kwargs'>
 				<method_info nloc='23' complexity='1' token_count='126' nesting_level='1' start_line='68' end_line='90'></method_info>
 			<added_lines>68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90</added_lines>
 			<deleted_lines>77,78,79,80,81,82,83,84,85,86,87,88,89,90</deleted_lines>
 		</method>
 		<method name='backward' parameters='ctx,grad_output'>
 				<method_info nloc='6' complexity='1' token_count='67' nesting_level='1' start_line='40' end_line='46'></method_info>
 			<added_lines>40,41,42,43,44,45,46</added_lines>
 			<deleted_lines>44,45,46</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1,2,4,6,8,9,10,11,12,14,16,17,18,19,20,21,22,38,39,104,105,106,107,108,109,110,111,112,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,136,137,138,140,141,142,143,144,145,146,147,149,245,256,257,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278</added_lines>
 			<deleted_lines>5,6,8,10,11,12,13,15,16,17,18,19,20,23,96,97,98,100,101,102,103,104,105,106,108,109,110,111,117,118,119,120,121,122,125,127,128,129,131,132,133,134,135,136,137,139,140,141,142,143,145,147,148,149,196,213,245,257,264,265,267</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pl_bolts\models\self_supervised\simclr\transforms.py' new_name='pl_bolts\models\self_supervised\simclr\transforms.py'>
 		<file_info nloc='168' complexity='8' token_count='852'></file_info>
 		<method name='__init__' parameters='self,kernel_size,p,min,max'>
 				<method_info nloc='5' complexity='1' token_count='45' nesting_level='1' start_line='191' end_line='197'></method_info>
 			<added_lines>191,194,197</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__call__' parameters='self,sample'>
 				<method_info nloc='5' complexity='1' token_count='35' nesting_level='1' start_line='95' end_line='101'></method_info>
 			<added_lines>97,101</added_lines>
 			<deleted_lines>97,98,99,100</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,int,float,float'>
 				<method_info nloc='8' complexity='2' token_count='49' nesting_level='1' start_line='108' end_line='117'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>108,109,110,111,112,113</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,int,bool,float,normalize'>
 				<method_info nloc='6' complexity='1' token_count='26' nesting_level='1' start_line='40' end_line='45'></method_info>
 			<added_lines>40,41,42,43,44,45</added_lines>
 			<deleted_lines>40</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,int,int'>
 				<method_info nloc='15' complexity='2' token_count='150' nesting_level='1' start_line='40' end_line='55'></method_info>
 			<added_lines>40,41,42,43,44,45,46,47,53,55</added_lines>
 			<deleted_lines>40,46,48,49,50,51,52,53,54,55</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,int,float,normalize,bool'>
 				<method_info nloc='6' complexity='1' token_count='26' nesting_level='1' start_line='145' end_line='150'></method_info>
 			<added_lines>145,146,147,148,149,150</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,103,104,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,143,144,151,152,153,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,186,190,205</added_lines>
 			<deleted_lines>61,64,82,83,84,85,86,88,90,91,92,93,94,104,105,106,107,125</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pl_bolts\models\self_supervised\swav\swav_finetuner.py' new_name='pl_bolts\models\self_supervised\swav\swav_finetuner.py'>
 		<file_info nloc='114' complexity='4' token_count='801'></file_info>
 		<method name='cli_main' parameters=''>
 				<method_info nloc='105' complexity='4' token_count='738' nesting_level='0' start_line='12' end_line='133'></method_info>
 			<added_lines>60,61,62,63,64</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pl_bolts\models\self_supervised\swav\swav_module.py' new_name='pl_bolts\models\self_supervised\swav\swav_module.py'>
 		<file_info nloc='455' complexity='56' token_count='3695'></file_info>
 		<method name='__init__' parameters='self,int,int,int,str,int,str,int,int,int,int,int,int,float,int,int,str,int,list,list,bool,bool,str,bool,bool,float,float,float,float,float,kwargs'>
 				<method_info nloc='32' complexity='1' token_count='190' nesting_level='1' start_line='27' end_line='58'></method_info>
 			<added_lines>33</added_lines>
 			<deleted_lines>30</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,int,int,int,int,str,str,int,int,int,int,int,int,float,int,int,str,int,list,list,bool,bool,str,bool,bool,float,float,float,float,float,kwargs'>
 				<method_info nloc='32' complexity='1' token_count='188' nesting_level='1' start_line='27' end_line='58'></method_info>
 			<added_lines>33</added_lines>
 			<deleted_lines>30</deleted_lines>
 		</method>
 		<method name='add_model_specific_args' parameters='parent_parser'>
 				<method_info nloc='52' complexity='1' token_count='730' nesting_level='1' start_line='414' end_line='477'></method_info>
 			<added_lines>447</added_lines>
 			<deleted_lines>447</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\models\self_supervised\test_models.py' new_name='tests\models\self_supervised\test_models.py'>
 		<file_info nloc='101' complexity='6' token_count='843'></file_info>
 		<method name='test_simclr' parameters='tmpdir,datadir'>
 				<method_info nloc='10' complexity='1' token_count='109' nesting_level='0' start_line='76' end_line='88'></method_info>
 			<added_lines>83</added_lines>
 			<deleted_lines>83</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\models\self_supervised\test_scripts.py' new_name='tests\models\self_supervised\test_scripts.py'>
 		<file_info nloc='57' complexity='12' token_count='412'></file_info>
 		<modified_lines>
 			<added_lines>48,49</added_lines>
 			<deleted_lines>48</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
