<bug_data>
<bug id='851' author='lokeshkvn' open_date='2020-06-22T17:46:12Z' closed_time='2020-07-06T04:09:19Z'>
 	<summary>OneCycleLRWithWarmup does not starts with init_lr</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug Report&lt;/denchmark-h&gt;
 
 OneCycleLRWithWarmup starts ahead of initial LR (does not start with init_lr)
 If the number of steps is 6 and warmup_fraction = 0.5 and init_lr = 1e-3 and lr_range=(2e-3,1e-3),
 The lr starts with 1.36-3 excutes two steps and starts cooling down from 2e-3
 &lt;denchmark-h:h3&gt;How To Reproduce&lt;/denchmark-h&gt;
 
 Steps to reproduce the behavior:
 
 Go to https://colab.research.google.com/github/catalyst-team/catalyst/blob/master/examples/notebooks/segmentation-tutorial.ipynb#scrollTo=OG6_pgmkxk_b
 
 
 &lt;denchmark-code&gt;batch_size = 8
 loaders = get_loaders(
     images=ALL_IMAGES[:200],
     masks=ALL_MASKS[:200],
     random_state=SEED,
     train_transforms_fn=train_transforms,
     valid_transforms_fn=valid_transforms,
     batch_size=batch_size
 ) 
 &lt;/denchmark-code&gt;
 
 
 set epochs to 10
 replace scheduler with
 
 &lt;denchmark-code&gt;scheduler = OneCycleLRWithWarmup(optimizer, num_steps = 6,lr_range=(2e-3,1e-3), init_lr = 1e-3, warmup_fraction = 0.5)
 &lt;/denchmark-code&gt;
 
 
 Run
 
 &lt;denchmark-h:h4&gt;Screenshots&lt;/denchmark-h&gt;
 
 &lt;denchmark-link:https://user-images.githubusercontent.com/16884938/85318756-20e2ab80-b4f3-11ea-8589-40039d7a056f.png&gt;&lt;/denchmark-link&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 OneCycleLRWithWarmup should start with lr = init_lr
 (Pardon me if I am wrong, please advise me if this is the correct behavior of OneCycleLRWithWarmup as I am new to DL)
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 Google Colab
 	</description>
 	<comments>
 		<comment id='1' author='lokeshkvn' date='2020-06-22T17:46:48Z'>
 		Hi! Thank you for your contribution! Great first issue!
 		</comment>
 		<comment id='2' author='lokeshkvn' date='2020-06-22T18:26:11Z'>
 		hi &lt;denchmark-link:https://github.com/lokeshkvn&gt;@lokeshkvn&lt;/denchmark-link&gt;
  , thanks for the issue!
 Due to a bunch of current tasks (we are working on huge codestyle and docs update),
 could you please help us and provide full-featured example for reproduce the issue?
 here is the template:
 import torch
 from torch.utils.data import DataLoader, TensorDataset
 from catalyst.dl import SupervisedRunner
 
 # data
 num_samples, num_features = int(1e4), int(1e1)
 X, y = torch.rand(num_samples, num_features), torch.rand(num_samples)
 dataset = TensorDataset(X, y)
 loader = DataLoader(dataset, batch_size=32, num_workers=1)
 loaders = {"train": loader, "valid": loader}
 
 # model, criterion, optimizer, scheduler
 model = torch.nn.Linear(num_features, 1)
 criterion = torch.nn.MSELoss()
 optimizer = torch.optim.Adam(model.parameters())
 scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [3, 6])
 
 # model training
 runner = SupervisedRunner()
 runner.train(
     model=model,
     criterion=criterion,
     optimizer=optimizer,
     scheduler=scheduler,
     loaders=loaders,
     logdir="./logdir",
     num_epochs=8,
     verbose=True,
 )
 		</comment>
 		<comment id='3' author='lokeshkvn' date='2020-06-22T18:42:32Z'>
 		Sure &lt;denchmark-link:https://github.com/Scitator&gt;@Scitator&lt;/denchmark-link&gt;
  I will update you with the full-feature example.
 		</comment>
 		<comment id='4' author='lokeshkvn' date='2020-06-22T18:57:53Z'>
 		Hi &lt;denchmark-link:https://github.com/Scitator&gt;@Scitator&lt;/denchmark-link&gt;
 
 Here is the code example to reproduce:
 import torch
 from torch.utils.data import DataLoader, TensorDataset
 from catalyst.dl import SupervisedRunner
 from catalyst.contrib.nn.schedulers.onecycle import OneCycleLRWithWarmup
 
 # data
 num_samples, num_features = int(1e4), int(1e1)
 X, y = torch.rand(num_samples, num_features), torch.rand(num_samples)
 dataset = TensorDataset(X, y)
 loader = DataLoader(dataset, batch_size=32, num_workers=1)
 loaders = {"train": loader, "valid": loader}
 
 # model, criterion, optimizer, scheduler
 model = torch.nn.Linear(num_features, 1)
 criterion = torch.nn.MSELoss()
 optimizer = torch.optim.Adam(model.parameters())
 scheduler = OneCycleLRWithWarmup(optimizer, num_steps = 6,lr_range=(2e-3,1e-3), init_lr = 1e-3, warmup_fraction = 0.5)
 
 # model training
 runner = SupervisedRunner()
 runner.train(
     model=model,
     criterion=criterion,
     optimizer=optimizer,
     scheduler=scheduler,
     loaders=loaders,
     logdir="./logdir",
     num_epochs=8,
     verbose=True,
 )
 Then run tensorboard
 &lt;denchmark-code&gt;load_ext tensorboard 
 tensorboard --logdir 'logdir/
 &lt;/denchmark-code&gt;
 
 Below is the lr plot I get using tensorboard:
 &lt;denchmark-link:https://user-images.githubusercontent.com/16884938/85324867-34930f80-b4fd-11ea-94fb-da62c801df94.png&gt;&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='5' author='lokeshkvn' date='2020-06-23T20:34:13Z'>
 		Dear &lt;denchmark-link:https://github.com/lokeshkvn&gt;@lokeshkvn&lt;/denchmark-link&gt;
  ,
 looks like I found the solution,
 You need to change this line
 &lt;denchmark-link:https://github.com/catalyst-team/catalyst/blob/master/catalyst/core/callbacks/scheduler.py#L121&gt;https://github.com/catalyst-team/catalyst/blob/master/catalyst/core/callbacks/scheduler.py#L121&lt;/denchmark-link&gt;
 
 to
 self._scheduler.recalculate(
     loader_len=runner.loader_len, current_step=runner.epoch - 1
 )
 Would you like to make a PR with this fix?
 		</comment>
 		<comment id='6' author='lokeshkvn' date='2020-06-24T02:58:24Z'>
 		Thanks I will make a PR soon with the above fix. üòÉ
 		</comment>
 	</comments>
 </bug>
<commit id='135ed6f82c133dd0595e56c562fa75b3cd806078' author='Sergey Kolesnikov' date='2020-07-05 22:24:55+03:00'>
 	<dmm_unit complexity='1.0' interfacing='0.6' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>44</added_lines>
 			<deleted_lines>44</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='bin\tests\check_dl_cv.sh' new_name='bin\tests\check_dl_cv.sh'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>36,37,38,352,353,382,383,413,414,443,444</added_lines>
 			<deleted_lines>36,37,351,380,381,411,440,441</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='catalyst\core\callbacks\scheduler.py' new_name='catalyst\core\callbacks\scheduler.py'>
 		<file_info nloc='197' complexity='44' token_count='998'></file_info>
 		<method name='on_loader_start' parameters='self,IRunner'>
 				<method_info nloc='14' complexity='4' token_count='54' nesting_level='1' start_line='110' end_line='123'></method_info>
 			<added_lines>122</added_lines>
 			<deleted_lines>122</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='tests\_tests_scripts\core_runner_onecyle_lr_scheduler.py'>
 		<file_info nloc='54' complexity='4' token_count='353'></file_info>
 	</modification>
 </commit>
</bug_data>
