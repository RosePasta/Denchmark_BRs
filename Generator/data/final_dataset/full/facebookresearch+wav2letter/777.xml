<bug_data>
<bug id='777' author='gopesh97' open_date='2020-08-07T09:44:36Z' closed_time='2020-08-15T05:43:38Z'>
 	<summary>[Argument to train CONVLM]</summary>
 	<description>
 &lt;denchmark-h:h3&gt;Bug Description&lt;/denchmark-h&gt;
 
 Refer to this.
 &lt;denchmark-link:https://github.com/facebookresearch/wav2letter/tree/master/recipes/models/lexicon_free/librispeech#reproduce-language-models-training--evaluation&gt;https://github.com/facebookresearch/wav2letter/tree/master/recipes/models/lexicon_free/librispeech#reproduce-language-models-training--evaluation&lt;/denchmark-link&gt;
 
 This is the command given to train a convlm from fairseq.
 When you run it, throws an error stating one the argument is wrong, i.e., keep-interval-update
 &lt;denchmark-code&gt;python3 [FAIRSEQ]/train.py [MODEL_DST]/decoder/fairseq_word_data \
 --save-dir [MODEL_DST]/decoder/convlm_models/word_14B \
 --task=language_modeling \
 --arch=fconv_lm --fp16 --max-epoch=48 --optimizer=nag \
 --lr=0.5 --lr-scheduler=fixed --decoder-embed-dim=128 --clip-norm=0.1 \
 --decoder-layers='[(512, 5)] + [(128, 1, 0), (128, 5, 0), (512, 1, 3)] * 3 + [(512, 1, 0), (512, 5, 0), (1024, 1, 3)] * 3 + [(1024, 1, 0), (1024, 5, 0), (2048, 1, 3)] * 6 + [(1024, 1, 0), (1024, 5, 0), (4096, 1, 3)]' \
 --dropout=0.1 --weight-decay=1e-07 \
 --max-tokens=1024 --tokens-per-sample=1024 --sample-break-mode=none \
 --criterion=adaptive_loss --adaptive-softmax-cutoff='10000,50000,200000' --seed=42 \
 --log-format=json --log-interval=100 \
 --save-interval-updates=10000 --keep-interval-update=10 \
 --ddp-backend="no_c10d" --distributed-world-size=8 &gt; [MODEL_DST]/decoder/convlm_models/word_14B/train.log
 
 &lt;/denchmark-code&gt;
 
 Upon searching the valid arguments from fairseq, turns out this argument is keep-interval-updates.
 So this change needs to be done so that anyone else won't face this issue.
 &lt;denchmark-h:h3&gt;Platform and Hardware&lt;/denchmark-h&gt;
 
 I am using the wav2letter/wav2letter:cuda-latest docker.
 	</description>
 	<comments>
 		<comment id='1' author='gopesh97' date='2020-08-07T21:54:31Z'>
 		Thanks for the pointing to this!
 Will fix this typo in the README.
 		</comment>
 	</comments>
 </bug>
<commit id='55e9ebc233a001a21c4033aa2a8a60dbf1fe62ec' author='Tatiana Likhomanenko' date='2020-08-12 13:10:01-07:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='recipes\models\lexicon_free\librispeech\README.md' new_name='recipes\models\lexicon_free\librispeech\README.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>62,78,93</added_lines>
 			<deleted_lines>62,78,93</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='recipes\models\lexicon_free\wsj\README.md' new_name='recipes\models\lexicon_free\wsj\README.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>56,72,87</added_lines>
 			<deleted_lines>56,72,87</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='recipes\models\sota\2019\lm_corpus_and_PL_generation\README.md' new_name='recipes\models\sota\2019\lm_corpus_and_PL_generation\README.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>18</added_lines>
 			<deleted_lines>18</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
