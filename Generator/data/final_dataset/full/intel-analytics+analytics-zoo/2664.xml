<bug_data>
<bug id='2664' author='hkvision' open_date='2020-08-03T08:37:32Z' closed_time='2020-09-08T05:29:59Z'>
 	<summary>py4j.protocol.Py4JJavaError: An error occurred while calling o61.csv. : java.lang.IllegalArgumentException: Illegal pattern component: XXX</summary>
 	<description>
 &lt;denchmark-link:https://github.com/intel-analytics/analytics-zoo/pull/2628&gt;#2628&lt;/denchmark-link&gt;
 
 This PR breaks spark.read.csv/json, with the following error:
 &lt;denchmark-code&gt;py4j.protocol.Py4JJavaError: An error occurred while calling o61.csv.
 : java.lang.IllegalArgumentException: Illegal pattern component: XXX
 	at org.apache.commons.lang3.time.FastDatePrinter.parsePattern(FastDatePrinter.java:282)
 	at org.apache.commons.lang3.time.FastDatePrinter.init(FastDatePrinter.java:149)
 	at org.apache.commons.lang3.time.FastDatePrinter.&lt;init&gt;(FastDatePrinter.java:142)
 	at org.apache.commons.lang3.time.FastDateFormat.&lt;init&gt;(FastDateFormat.java:384)
 	at org.apache.commons.lang3.time.FastDateFormat.&lt;init&gt;(FastDateFormat.java:369)
 	at org.apache.commons.lang3.time.FastDateFormat$1.createInstance(FastDateFormat.java:91)
 	at org.apache.commons.lang3.time.FastDateFormat$1.createInstance(FastDateFormat.java:88)
 	at org.apache.commons.lang3.time.FormatCache.getInstance(FormatCache.java:82)
 	at org.apache.commons.lang3.time.FastDateFormat.getInstance(FastDateFormat.java:165)
 	at org.apache.spark.sql.execution.datasources.csv.CSVOptions.&lt;init&gt;(CSVOptions.scala:139)
 	at org.apache.spark.sql.execution.datasources.csv.CSVOptions.&lt;init&gt;(CSVOptions.scala:41)
 	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:58)
 	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)
 	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)
 	at scala.Option.orElse(Option.scala:289)
 	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:179)
 	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373)
 	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)
 	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
 	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:615)
 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 	at java.lang.reflect.Method.invoke(Method.java:498)
 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
 	at py4j.Gateway.invoke(Gateway.java:282)
 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
 	at py4j.GatewayConnection.run(GatewayConnection.java:238)
 	at java.lang.Thread.run(Thread.java:748)
 &lt;/denchmark-code&gt;
 
 Either revert this PR if the conflict can be resolved or refer to here: &lt;denchmark-link:https://stackoverflow.com/questions/46429616/spark-2-2-illegal-pattern-component-xxx-java-lang-illegalargumentexception-ill&gt;https://stackoverflow.com/questions/46429616/spark-2-2-illegal-pattern-component-xxx-java-lang-illegalargumentexception-ill&lt;/denchmark-link&gt;
 
 Error log: &lt;denchmark-link:http://10.239.47.210:18888/job/ZOO-NB-Pip-ExampleTests-py36/84/console&gt;http://10.239.47.210:18888/job/ZOO-NB-Pip-ExampleTests-py36/84/console&lt;/denchmark-link&gt;
 
 Related issue: &lt;denchmark-link:https://github.com/intel-analytics/analytics-zoo/issues/2522&gt;#2522&lt;/denchmark-link&gt;
 
 cc &lt;denchmark-link:https://github.com/jenniew&gt;@jenniew&lt;/denchmark-link&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='hkvision' date='2020-08-03T08:38:16Z'>
 		Also, for Spark backend, if we are running on a hadoop cluster, we still need to add  for local file prefixes? &lt;denchmark-link:https://github.com/jenniew&gt;@jenniew&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='hkvision' date='2020-08-03T17:14:13Z'>
 		No need to add file:// in yarn.
 		</comment>
 		<comment id='3' author='hkvision' date='2020-08-04T01:36:16Z'>
 		But it raises error:
 &lt;denchmark-code&gt;&gt;&gt;&gt; data_shard = zoo.orca.data.pandas.read_csv("/opt/work/client/kai/nyc_taxi.csv")
 
 Traceback (most recent call last):
   File "&lt;stdin&gt;", line 1, in &lt;module&gt;
   File "/opt/work/client/anaconda3/envs/tianchi/lib/python3.7/site-packages/zoo/orca/data/pandas/preprocessing.py", line 33, in read_csv
     return read_file_spark(file_path, "csv", **kwargs)
   File "/opt/work/client/anaconda3/envs/tianchi/lib/python3.7/site-packages/zoo/orca/data/pandas/preprocessing.py", line 146, in read_file_spark
     df = spark.read.csv(file_path, **kwargs)
   File "/opt/work/client/anaconda3/envs/tianchi/lib/python3.7/site-packages/pyspark/sql/readwriter.py", line 472, in csv
     return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))
   File "/opt/work/client/anaconda3/envs/tianchi/lib/python3.7/site-packages/py4j/java_gateway.py", line 1257, in __call__
     answer, self.gateway_client, self.target_id, self.name)
   File "/opt/work/client/anaconda3/envs/tianchi/lib/python3.7/site-packages/pyspark/sql/utils.py", line 69, in deco
     raise AnalysisException(s.split(': ', 1)[1], stackTrace)
 pyspark.sql.utils.AnalysisException: 'Path does not exist: hdfs://Almaren-Node-003:9000/opt/work/client/kai/nyc_taxi.csv;'
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='4' author='hkvision' date='2020-08-04T07:26:27Z'>
 		spark.read.csv/json defaults to use hadoop fs.
 		</comment>
 		<comment id='5' author='hkvision' date='2020-08-11T04:28:46Z'>
 		This error also happens on YARN cluster. Need to fix it asap.
 		</comment>
 		<comment id='6' author='hkvision' date='2020-08-11T15:36:47Z'>
 		This is because bigdl and analytics-zoo pip using different spark versions to build?
 		</comment>
 		<comment id='7' author='hkvision' date='2020-08-11T19:04:07Z'>
 		what error? "Path does not exist: hdfs://Almaren-Node-003:9000/opt/work/client/kai/nyc_taxi.csv" is because we use spark to read csv and spark uses hadoop fs to read file. I think we don't need to fix it.
 For  java.lang.IllegalArgumentException: Illegal pattern component: XXX, the error is inconsistent commons-lang3 version between spark and hadoop.  We could either put our bigdl/analytics-zoo jars before hadoop jars, or add option("timestampFormat", "yyyy/MM/dd HH:mm:ss ZZ") to fix it. My previous fix is putting our jars at the front of driver and executor classpath, but &lt;denchmark-link:https://github.com/intel-analytics/analytics-zoo/pull/2628&gt;#2628&lt;/denchmark-link&gt;
  changes that order of classpath. I think now we may need to add this option("timestampFormat", "yyyy/MM/dd HH:mm:ss ZZ") to fix it.
 		</comment>
 	</comments>
 </bug>
<commit id='98e0205fa8275e7617cb357517c61801c99376da' author='Yang Wang' date='2020-09-08 13:29:58+08:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pyzoo\zoo\util\engine.py' new_name='pyzoo\zoo\util\engine.py'>
 		<file_info nloc='109' complexity='41' token_count='771'></file_info>
 		<method name='__prepare_analytics_zoo_env.append_path' parameters='env_var_name,path'>
 				<method_info nloc='7' complexity='3' token_count='58' nesting_level='1' start_line='72' end_line='78'></method_info>
 			<added_lines>76</added_lines>
 			<deleted_lines>76</deleted_lines>
 		</method>
 		<method name='__prepare_analytics_zoo_env' parameters=''>
 				<method_info nloc='25' complexity='13' token_count='228' nesting_level='0' start_line='66' end_line='102'></method_info>
 			<added_lines>76</added_lines>
 			<deleted_lines>76</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
