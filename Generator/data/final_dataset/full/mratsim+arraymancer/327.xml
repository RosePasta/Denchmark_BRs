<bug_data>
<bug id='327' author='mratsim' open_date='2018-12-09T13:07:23Z' closed_time='2018-12-16T11:16:06Z'>
 	<summary>Custom NN types - "undeclared field 'grad'</summary>
 	<description>
 When creating a custom NN type if it doesn't use generics you run into issues with the autograd backward method:
 For example
 type ShakespeareNet = object
   # Embedding weight = Encoder
   encoder_w: Variable[Tensor[float32]]
 
   # GRU RNN = Internal representation
   gru_W3s0, gru_W3sN, gru_U3s, gru_bW3s, bU3s: Variable[Tensor[float32]]
 
   # Linear layer weight = Decoder
   decoder_w: Variable[Tensor[float32]]
 Will trigger an error here:
 
 
 
 Arraymancer/src/autograd/gates_basic.nim
 
 
         Lines 30 to 31
       in
       89cd3f4
 
 
 
 
 
 
  method backward*[TT](self: AddGate[TT], payload: Payload[TT]): SmallDiffs[TT] {.noInit, inline.}= 
 
 
 
  let gradient = payload.variable.grad 
 
 
 
 
 
 &lt;denchmark-code&gt;yourfile.nim(71, 22) template/generic instantiation of `backward` from here
 ../src/autograd/gates_basic.nim(32, 34) Error: undeclared field: 'grad'
 &lt;/denchmark-code&gt;
 
 This happens when the generic types and its method can be materialised early.
 The workaround is to use a generic type ...
 type ShakespeareNet[TT] = object
   # Embedding weight = Encoder
   encoder_w: Variable[TT]
 
   # GRU RNN = Internal representation
   gru_W3s0, gru_W3sN, gru_U3s, gru_bW3s, bU3s: Variable[TT]
 
   # Linear layer weight = Decoder
   decoder_w: Variable[TT]
 Renaming the payload field from variable to something else to avoid collision might also help.
 Removing  altogether would also be a solution (&lt;denchmark-link:https://github.com/mratsim/Arraymancer/issues/302&gt;#302&lt;/denchmark-link&gt;
 )
 	</description>
 	<comments>
 		<comment id='1' author='mratsim' date='2018-12-13T00:18:35Z'>
 		Trying to change the following
 type
   Sgd*[TT] = object
     ## Stochastic gradient descent
     params: seq[Variable[TT]]
     lr: float32 # Learning rate.
 into
 type
   Sgd*[T] = object
     ## Stochastic gradient descent
     params: seq[Variable[Tensor[T]]]
     lr: T # Learning rate.
 also triggers the same issue at the same location.
 Changing payload.variable field to a name that is not used at all didn't fix the issue:
 
 
 
 Arraymancer/src/autograd/ag_data_structure.nim
 
 
         Lines 81 to 86
       in
       3ae364d
 
 
 
 
 
 
  PayloadKind* = enum 
 
 
 
    pkVar, pkSeq 
 
 
 
  Payload*[TT] = object 
 
 
 
  case kind*: PayloadKind 
 
 
 
  of pkVar: variable*: Variable[TT] 
 
 
 
  of pkSeq: sequence*: seq[Variable[TT]] 
 
 
 
 
 
 		</comment>
 		<comment id='2' author='mratsim' date='2018-12-16T11:16:06Z'>
 		fixed by &lt;denchmark-link:https://github.com/mratsim/Arraymancer/pull/333&gt;#333&lt;/denchmark-link&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='edf22b13800e7154db49a845868ee4778f9fb42c' author='mratsim' date='2018-12-16 00:25:46+01:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='src\nn\optimizers\optimizers.nim' new_name='src\nn\optimizers\optimizers.nim'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>19,21,22</added_lines>
 			<deleted_lines>19,21,22</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
