<bug_data>
<bug id='183' author='mratsim' open_date='2017-12-22T23:22:15Z' closed_time='2018-05-01T10:49:31Z'>
 	<summary>MNIST OpenMP accuracy 25% less than single threaded</summary>
 	<description>
 Compiling the &lt;denchmark-link:https://github.com/mratsim/Arraymancer/blob/8a92d825ae436e868c82709ade7553a8a6716816/examples/ex02_handwritten_digits_recognition.nim&gt;MNIST example&lt;/denchmark-link&gt;
  with OpenMP leads to much worse accuracy for no apparent reason.
 After the first epoch
 Random seed 1337: Single-thread: 76.2%, OpenMP 51%
 Random seed 42: Single-thread: 83.2%, OpenMP 57.5%
 	</description>
 	<comments>
 		<comment id='1' author='mratsim' date='2017-12-23T08:43:25Z'>
 		Added &lt;denchmark-link:https://github.com/mratsim/Arraymancer/blob/061b2d07f8904db5c14a662f020fbcfb7e4cbac3/examples/ex02_handwritten_digits_recognition.nim&gt;loss tracking&lt;/denchmark-link&gt;
  to MNIST example.
 Log:
 &lt;denchmark-h:h2&gt;No OpenMP&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;Epoch is: 0
 Batch id: 0
 Loss is:  132.9124755859375
 Epoch is: 0
 Batch id: 200
 Loss is:  2.301989078521729
 Epoch is: 0
 Batch id: 400
 Loss is:  1.155071973800659
 Epoch is: 0
 Batch id: 600
 Loss is:  1.043337464332581
 Epoch is: 0
 Batch id: 800
 Loss is:  0.58299720287323
 Epoch is: 0
 Batch id: 1000
 Loss is:  0.5417937040328979
 Epoch is: 0
 Batch id: 1200
 Loss is:  0.6955615282058716
 Epoch is: 0
 Batch id: 1400
 Loss is:  0.4742314517498016
 Epoch is: 0
 Batch id: 1600
 Loss is:  0.3307125866413116
 Epoch is: 0
 Batch id: 1800
 Loss is:  0.6455222368240356
 
 Epoch #0 done. Testing accuracy
 Accuracy: 83.24999999999999%
 Loss:     0.5828457295894622
 
 
 Epoch is: 1
 Batch id: 0
 Loss is:  0.5344035029411316
 Epoch is: 1
 Batch id: 200
 Loss is:  0.4455387890338898
 Epoch is: 1
 Batch id: 400
 Loss is:  0.1642555445432663
 Epoch is: 1
 Batch id: 600
 Loss is:  0.5191419124603271
 Epoch is: 1
 Batch id: 800
 Loss is:  0.2091695368289948
 Epoch is: 1
 Batch id: 1000
 Loss is:  0.2661008834838867
 Epoch is: 1
 Batch id: 1200
 Loss is:  0.405451238155365
 Epoch is: 1
 Batch id: 1400
 Loss is:  0.1397259384393692
 Epoch is: 1
 Batch id: 1600
 Loss is:  0.526863694190979
 Epoch is: 1
 Batch id: 1800
 Loss is:  0.5916416645050049
 
 Epoch #1 done. Testing accuracy
 Accuracy: 88.49000000000001%
 Loss:     0.3582650691270828
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h2&gt;openMP&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;Epoch is: 0
 Batch id: 0
 Loss is:  132.9124603271484
 Epoch is: 0
 Batch id: 200
 Loss is:  2.315118074417114
 Epoch is: 0
 Batch id: 400
 Loss is:  1.185595512390137
 Epoch is: 0
 Batch id: 600
 Loss is:  0.9815522432327271
 Epoch is: 0
 Batch id: 800
 Loss is:  0.6072715520858765
 Epoch is: 0
 Batch id: 1000
 Loss is:  0.6047156453132629
 Epoch is: 0
 Batch id: 1200
 Loss is:  0.7644815444946289
 Epoch is: 0
 Batch id: 1400
 Loss is:  0.4162880778312683
 Epoch is: 0
 Batch id: 1600
 Loss is:  0.3775918483734131
 Epoch is: 0
 Batch id: 1800
 Loss is:  0.5572935938835144
 
 Epoch #0 done. Testing accuracy
 Accuracy: 57.50999999999999%
 Loss:     0.5141938149929046
 
 
 Epoch is: 1
 Batch id: 0
 Loss is:  0.4935077428817749
 Epoch is: 1
 Batch id: 200
 Loss is:  0.4984779953956604
 Epoch is: 1
 Batch id: 400
 Loss is:  0.1299190074205399
 Epoch is: 1
 Batch id: 600
 Loss is:  0.4471000134944916
 Epoch is: 1
 Batch id: 800
 Loss is:  0.240534171462059
 Epoch is: 1
 Batch id: 1000
 Loss is:  0.2717002034187317
 Epoch is: 1
 Batch id: 1200
 Loss is:  0.374997079372406
 Epoch is: 1
 Batch id: 1400
 Loss is:  0.112208716571331
 Epoch is: 1
 Batch id: 1600
 Loss is:  0.6015350818634033
 Epoch is: 1
 Batch id: 1800
 Loss is:  0.5499600172042847
 
 Epoch #1 done. Testing accuracy
 Accuracy: 60.31999999999999%
 Loss:     0.3561002880334854
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='2' author='mratsim' date='2017-12-23T09:05:30Z'>
 		After disabling OpenMP in im2col, maxpool2D, softmax, accuracy_score with no change, the most likely candidate is argmax (which is also used in maxpool2D)
 		</comment>
 		<comment id='3' author='mratsim' date='2018-05-01T10:49:31Z'>
 		Issue was the parallel version of Argmax, I suppose their was a race in the parallel "fold_enumerateAxis_inline" template.
 It has been deleted along with all the 3 extra procs needed to implement argmax in a functional manner. New version doesn't exhibit the issue. It is not parallel but as it uses less temporaries (passing from a tensor of tuples to a tuple of tensors), it might even be faster.
 Result on MNIST:
 Without OpenMP
 &lt;denchmark-code&gt;Epoch is: 0
 Batch id: 0
 Loss is:  194.3991851806641
 Epoch is: 0
 Batch id: 200
 Loss is:  2.60599946975708
 Epoch is: 0
 Batch id: 400
 Loss is:  1.708131313323975
 Epoch is: 0
 Batch id: 600
 Loss is:  1.061241149902344
 Epoch is: 0
 Batch id: 800
 Loss is:  0.8607467412948608
 Epoch is: 0
 Batch id: 1000
 Loss is:  0.9292868375778198
 Epoch is: 0
 Batch id: 1200
 Loss is:  0.6178927421569824
 Epoch is: 0
 Batch id: 1400
 Loss is:  0.4008050560951233
 Epoch is: 0
 Batch id: 1600
 Loss is:  0.2450754344463348
 Epoch is: 0
 Batch id: 1800
 Loss is:  0.3787734508514404
 
 Epoch #0 done. Testing accuracy
 Accuracy: 84.24999999999999%
 Loss:     0.4853884726762772
 
 
 Epoch is: 1
 Batch id: 0
 Loss is:  0.8319419622421265
 Epoch is: 1
 Batch id: 200
 Loss is:  0.3116425573825836
 Epoch is: 1
 Batch id: 400
 Loss is:  0.232885867357254
 Epoch is: 1
 Batch id: 600
 Loss is:  0.3877259492874146
 Epoch is: 1
 Batch id: 800
 Loss is:  0.3621436357498169
 Epoch is: 1
 Batch id: 1000
 Loss is:  0.5054937601089478
 Epoch is: 1
 Batch id: 1200
 Loss is:  0.4431287050247192
 Epoch is: 1
 Batch id: 1400
 Loss is:  0.2153264284133911
 Epoch is: 1
 Batch id: 1600
 Loss is:  0.1401071697473526
 Epoch is: 1
 Batch id: 1800
 Loss is:  0.3415909707546234
 
 Epoch #1 done. Testing accuracy
 Accuracy: 87.91%
 Loss:     0.3657706841826439
 &lt;/denchmark-code&gt;
 
 With OpenMP
 &lt;denchmark-code&gt;Epoch is: 0
 Batch id: 0
 Loss is:  194.3992156982422
 Epoch is: 0
 Batch id: 200
 Loss is:  2.586019992828369
 Epoch is: 0
 Batch id: 400
 Loss is:  1.865354180335999
 Epoch is: 0
 Batch id: 600
 Loss is:  1.339139461517334
 Epoch is: 0
 Batch id: 800
 Loss is:  0.9255489110946655
 Epoch is: 0
 Batch id: 1000
 Loss is:  0.8845529556274414
 Epoch is: 0
 Batch id: 1200
 Loss is:  0.5737345814704895
 Epoch is: 0
 Batch id: 1400
 Loss is:  0.4271677136421204
 Epoch is: 0
 Batch id: 1600
 Loss is:  0.3307865262031555
 Epoch is: 0
 Batch id: 1800
 Loss is:  0.3299965560436249
 
 Epoch #0 done. Testing accuracy
 Accuracy: 81.76000000000001%
 Loss:     0.6043894469738007
 
 
 Epoch is: 1
 Batch id: 0
 Loss is:  0.9704902172088623
 Epoch is: 1
 Batch id: 200
 Loss is:  0.2412533462047577
 Epoch is: 1
 Batch id: 400
 Loss is:  0.2090668380260468
 Epoch is: 1
 Batch id: 600
 Loss is:  0.5054131746292114
 Epoch is: 1
 Batch id: 800
 Loss is:  0.4721413254737854
 Epoch is: 1
 Batch id: 1000
 Loss is:  0.5908526182174683
 Epoch is: 1
 Batch id: 1200
 Loss is:  0.3866634964942932
 Epoch is: 1
 Batch id: 1400
 Loss is:  0.1679804921150208
 Epoch is: 1
 Batch id: 1600
 Loss is:  0.3216101229190826
 Epoch is: 1
 Batch id: 1800
 Loss is:  0.2575283646583557
 
 Epoch #1 done. Testing accuracy
 Accuracy: 90.58999999999999%
 Loss:     0.2933134615421295
 &lt;/denchmark-code&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='3d1377214453198382b3f7e8d896b379ba02f00e' author='mratsim' date='2018-05-01 12:37:12+02:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='src\tensor\accessors.nim' new_name='src\tensor\accessors.nim'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>335</added_lines>
 			<deleted_lines>335</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='src\tensor\aggregate.nim' new_name='src\tensor\aggregate.nim'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>15,21,151,152,153,155,156,157,158,159,160,162,163,164,165,166</added_lines>
 			<deleted_lines>15,150,152,153,154,155,156,157,158,159,160,161,163,164,165</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='src\tensor\higher_order_foldreduce.nim' new_name='src\tensor\higher_order_foldreduce.nim'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>56</added_lines>
 			<deleted_lines>56,63,64,65,66,67,68,69,70,71,72,73,74</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='DELETE' old_name='src\tensor\private\p_aggregate.nim' new_name='None'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\tensor\test_aggregate.nim' new_name='tests\tensor\test_aggregate.nim'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>106,107,108,109,110,111,112,113</added_lines>
 			<deleted_lines>106</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
