<bug_data>
<bug id='278' author='jasonliu747' open_date='2020-07-29T03:18:26Z' closed_time='2020-08-22T12:27:01Z'>
 	<summary>Different memory consumption among GPUs</summary>
 	<description>
 Describe the bug
 When running a training, either on single or multiple nodes, the memory of first GPU card will always be consumed more than others. Looks like it only happened after upgrading BytePS version to v0.2.4.
 To Reproduce
 Steps to reproduce the behavior:
 &lt;denchmark-code&gt;apiVersion: "kubeflow.org/v1beta1"
 kind: "MXJob"
 metadata:
   name: "byteps-mxnet-job"
 spec:
   jobMode: MXTrain
   mxReplicaSpecs:
     Scheduler:
       replicas: 1
       restartPolicy: Never
       template:
         spec:
           containers:
             - name: mxnet
               image: bytepsimage/mxnet:v0.2.4
               command: ["bpslaunch"]
     Server:
       replicas: 1
       restartPolicy: Never
       template:
         spec:
           containers:
             - name: mxnet
               image: bytepsimage/mxnet:v0.2.4
               command: ["bpslaunch"]
               resources:
                 requests:
                   cpu: 4
     Worker:
       replicas: 2
       restartPolicy: Never
       template:
         spec:
           containers:
             - name: mxnet
               image: bytepsimage/mxnet:v0.2.4
               command: ["bpslaunch"]
               args: ["python3", "/usr/local/byteps/example/mxnet/train_imagenet_byteps.py", "--benchmark", "1", "--batch-size=32", "--num-epochs=1"]
               volumeMounts:
               - mountPath: /dev/shm
                 name: dshm
               resources:
                 limits:
                   nvidia.com/gpu: 8
           volumes:
           - name: dshm
             emptyDir:
               medium: Memory
 &lt;/denchmark-code&gt;
 
 
 &lt;denchmark-link:https://user-images.githubusercontent.com/24452340/88752517-b7f2f100-d18c-11ea-8729-4dd6102498b9.png&gt;&lt;/denchmark-link&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='jasonliu747' date='2020-07-30T01:39:17Z'>
 		I am not sure why PID 35529-35546 also uses memory on GPU0. Is your  identical with this &lt;denchmark-link:https://github.com/bytedance/byteps/blob/master/example/mxnet/train_imagenet_byteps.py&gt;example&lt;/denchmark-link&gt;
 ?
 
 Looks like it only happened after upgrading BytePS version to v0.2.4.
 
 If it means v0.2.3 has no such issue, it looks weird. Because &lt;denchmark-link:https://github.com/bytedance/byteps/commit/c16efff5aa5722738978c93da4ca86b2f8c00634&gt;c16efff&lt;/denchmark-link&gt;
  and &lt;denchmark-link:https://github.com/bytedance/byteps/commit/9e3b46d3c5508f0cc1b19540564c124f3a2fc0eb&gt;9e3b46d&lt;/denchmark-link&gt;
  do not seem to be relevant to this problem..
 		</comment>
 		<comment id='2' author='jasonliu747' date='2020-07-30T02:43:06Z'>
 		
 I am not sure why PID 35529-35546 also uses memory on GPU0. Is your train_imagenet_byteps.py identical with this example?
 
 Yes, it's totally identical.
 
 If it means v0.2.3 has no such issue, it looks weird.
 
 I'm not sure which version caused this issue. The last version I used before v0.2.4 was v0.2.0.  And I just double checked, there is indeed no such issue in v0.2.0.
 &lt;denchmark-link:https://user-images.githubusercontent.com/24452340/88874323-62cce300-d251-11ea-9430-6aa230818811.png&gt;&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='3' author='jasonliu747' date='2020-07-30T04:03:05Z'>
 		
 I am not sure why PID 35529-35546 also uses memory on GPU0. Is your train_imagenet_byteps.py identical with this example?
 
 Looks like it only happened after upgrading BytePS version to v0.2.4.
 
 If it means v0.2.3 has no such issue, it looks weird. Because c16efff and 9e3b46d do not seem to be relevant to this problem..
 
 Are u sure? I have been experiencing this problem since I started using byteps.
 		</comment>
 		<comment id='4' author='jasonliu747' date='2020-07-30T10:19:13Z'>
 		It happens when training IMAGENET. Interestingly, &lt;denchmark-link:https://github.com/vycezhong/byteps/blob/gradient_compression/example/mxnet/train_gluon_imagenet_byteps_gc.py&gt;my script&lt;/denchmark-link&gt;
  using  also suffers this.
 		</comment>
 		<comment id='5' author='jasonliu747' date='2020-07-30T10:46:15Z'>
 		Yes, I can reproduce this with v0.2.4 on MXNet.. But PyTorch and TF benchmarks are fine. So I think the problem is in the MXNet example code (or maybe the plugins). We should investigate this.
 		</comment>
 		<comment id='6' author='jasonliu747' date='2020-08-19T08:02:35Z'>
 		&lt;denchmark-link:https://github.com/ymjiang&gt;@ymjiang&lt;/denchmark-link&gt;
  any update on this issue?
 		</comment>
 		<comment id='7' author='jasonliu747' date='2020-08-22T07:07:38Z'>
 		&lt;denchmark-link:https://github.com/jasonliu747&gt;@jasonliu747&lt;/denchmark-link&gt;
   It comes from . After setting  the problem is gone.
 		</comment>
 		<comment id='8' author='jasonliu747' date='2020-08-22T12:27:57Z'>
 		Fixed by &lt;denchmark-link:https://github.com/vycezhong&gt;@vycezhong&lt;/denchmark-link&gt;
 . Closing this.
 		</comment>
 	</comments>
 </bug>
<commit id='c5a10545e8c2c8a75ff29be943befe57c81f8a1e' author='Yuchen Zhong' date='2020-08-22 20:27:00+08:00'>
 	<dmm_unit complexity='0.5' interfacing='0.5' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='example\mxnet\common\data_byteps.py' new_name='example\mxnet\common\data_byteps.py'>
 		<file_info nloc='147' complexity='21' token_count='1180'></file_info>
 		<method name='get_rec_iter' parameters='args,rank'>
 				<method_info nloc='60' complexity='7' token_count='404' nesting_level='0' start_line='111' end_line='170'></method_info>
 			<added_lines>119,121,149,150,168,169</added_lines>
 			<deleted_lines>119,121,149,167</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='example\mxnet\common\fit_byteps.py' new_name='example\mxnet\common\fit_byteps.py'>
 		<file_info nloc='243' complexity='61' token_count='1950'></file_info>
 		<method name='fit' parameters='args,network,data_loader,kwargs'>
 				<method_info nloc='135' complexity='39' token_count='1049' nesting_level='0' start_line='142' end_line='329'></method_info>
 			<added_lines>161</added_lines>
 			<deleted_lines>161</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='example\mxnet\train_gluon_imagenet_byteps_gc.py' new_name='example\mxnet\train_gluon_imagenet_byteps_gc.py'>
 		<file_info nloc='469' complexity='72' token_count='3667'></file_info>
 		<method name='main' parameters=''>
 				<method_info nloc='106' complexity='20' token_count='740' nesting_level='0' start_line='134' end_line='546'></method_info>
 			<added_lines>273,274,292,293</added_lines>
 			<deleted_lines>273,291</deleted_lines>
 		</method>
 		<method name='main.get_data_rec' parameters='rec_train,rec_train_idx,rec_val,rec_val_idx,batch_size,num_workers'>
 				<method_info nloc='59' complexity='2' token_count='382' nesting_level='1' start_line='228' end_line='295'></method_info>
 			<added_lines>273,274,292,293</added_lines>
 			<deleted_lines>273,291</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
