<bug_data>
<bug id='1255' author='cristianmtr' open_date='2020-11-09T16:08:40Z' closed_time='2020-11-11T11:03:36Z'>
 	<summary>clarify meaning of 'score'</summary>
 	<description>
 Describe the bug
 From a discussion with &lt;denchmark-link:https://github.com/JoanFM&gt;@JoanFM&lt;/denchmark-link&gt;
  , it seems like there is a bit of confusion around the  field.
 
 In the context of chunk comparisons (at the Indexer level) it means distance. Thus, smaller is better
 In the context of documents (as parents of chunks) it means similarity. Or at least, in the context of MinRanker (https://github.com/jina-ai/jina-hub/blob/f92276c1c4a38ef9ac34a16c4065b5d360805600/rankers/MinRanker/__init__.py#L12), larger is better.
 
 Thus there is nothing wrong with the output of the below example (from multires lyrics example):
 &lt;denchmark-link:https://user-images.githubusercontent.com/8330330/98240250-6a470b00-1f69-11eb-986c-fc09ade9074a.png&gt;&lt;/denchmark-link&gt;
 
 However, this is confusing. The problem, to me, is a matter of documentation / conflicting definitions for the score field.
 As originally discussed in &lt;denchmark-link:https://github.com/jina-ai/jina/issues/1224&gt;#1224&lt;/denchmark-link&gt;
 
 Describe how you solve it
 Possible solutions:
 
 documentation
 changing the name it by document / chunk. Document would be "similarity" (?), while chunk should be "distance". AFAIK, protobuf represents them with the same object, so that wouldn't work
 adding another field to the list of match and list of chunk that represents the method use to generate the score field.
 
 Something like
  "method": "MinRanker"  # or "Indexer
 in the output
 &lt;denchmark-link:https://user-images.githubusercontent.com/8330330/98564389-9a194a00-22ac-11eb-8395-8e5d0bcb0dba.png&gt;&lt;/denchmark-link&gt;
 
 Environment
 &lt;denchmark-code&gt;jina                          0.7.8
 jina-proto                    0.0.75
 jina-vcs-tag                  (unset)
 libzmq                        4.3.2
 pyzmq                         1.19.4
 protobuf                      3.13.0
 proto-backend                 cpp
 grpcio                        1.33.2
 ruamel.yaml                   0.16.12
 python                        3.8.5
 platform                      Linux
 platform-release              5.4.0-52-generic
 platform-version              #57-Ubuntu SMP Thu Oct 15 10:57:00 UTC 2020
 architecture                  x86_64
 processor                     x86_64
 jina-resources                /home/cristian/code/jina/jina/resources
 JINA_ARRAY_QUANT              (unset)
 JINA_BINARY_DELIMITER         (unset)
 JINA_CONTRIB_MODULE           (unset)
 JINA_CONTRIB_MODULE_IS_LOADING(unset)
 JINA_CONTROL_PORT             (unset)
 JINA_DB_COLLECTION            (unset)
 JINA_DB_HOSTNAME              (unset)
 JINA_DB_NAME                  (unset)
 JINA_DB_PASSWORD              (unset)
 JINA_DB_USERNAME              (unset)
 JINA_DEFAULT_HOST             (unset)
 JINA_DISABLE_UVLOOP           (unset)
 JINA_EXECUTOR_WORKDIR         (unset)
 JINA_FULL_CLI                 (unset)
 JINA_IPC_SOCK_TMP             (unset)
 JINA_LOG_CONFIG               (unset)
 JINA_LOG_NO_COLOR             (unset)
 JINA_POD_NAME                 (unset)
 JINA_PROFILING                (unset)
 JINA_RANDOM_PORTS             (unset)
 JINA_SOCKET_HWM               (unset)
 JINA_TEST_GPU                 (unset)
 JINA_TEST_PRETRAINED          (unset)
 JINA_VCS_VERSION              (unset)
 JINA_WARN_UNNAMED             (unset)
 &lt;/denchmark-code&gt;
 
 Screenshots
 	</description>
 	<comments>
 		<comment id='1' author='cristianmtr' date='2020-11-09T16:31:03Z'>
 		I think something similar to what you describe of the method used to calculate the score is currently in place. The op_name in the NamedScore
 &lt;denchmark-code&gt;message NamedScore {
     float value = 1; // value
     string op_name = 2; // the name of the operator/score function
     string description = 3; // text description of the score
     repeated NamedScore operands = 4; // the score can be nested
     string ref_id = 5; // the score is computed between doc `id` and `ref_id`
 }
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='2' author='cristianmtr' date='2020-11-09T17:24:48Z'>
 		yes, our protobuf already support it. Unfortunately, most of our rankers didn't leverage the rich semantic and only add value without description. The ranker creator should write that semantic into the description just like writing comment. It's a best practice thing when extending with Jina. I'd suggest enforcing this best practice from now.
 		</comment>
 		<comment id='3' author='cristianmtr' date='2020-11-09T17:28:05Z'>
 		I do think is already being added &lt;denchmark-link:https://github.com/cristianmtr&gt;@cristianmtr&lt;/denchmark-link&gt;
  , can you check if you see it inside the  object inside  at any level?
 		</comment>
 		<comment id='4' author='cristianmtr' date='2020-11-09T17:35:35Z'>
 		just fyi, you can simply add r.score.description = exec.__doc__ in the RankDriver at jina/drivers/rank.py:178 to make it more descriptive.
 But then every score now carries a long description, could be less network-efficient. This is the tradeoff you anyway have to take. Or you don't make the decision and add an option and let the user decide.
 		</comment>
 		<comment id='5' author='cristianmtr' date='2020-11-10T07:57:36Z'>
 		Couldn't we maybe find a way to make them all fit one direction? Either all are lower is better or all are higher is better?
 		</comment>
 		<comment id='6' author='cristianmtr' date='2020-11-10T08:47:42Z'>
 		unfortunately, no. Not universally everywhere. Same goes with tf, pytorch, where score is always task-specific, you as the user say what is best, loss or distance is at low-level, it is smaller the better by definition.
 		</comment>
 		<comment id='7' author='cristianmtr' date='2020-11-10T09:24:11Z'>
 		&lt;denchmark-link:https://github.com/cristianmtr&gt;@cristianmtr&lt;/denchmark-link&gt;
  maybe we can add some part in the documentation having a little discussion on .
 		</comment>
 		<comment id='8' author='cristianmtr' date='2020-11-10T09:25:12Z'>
 		Yeah, I guess we can do that. Is there a known list of rankers/sorters and whether they are lower or higher is better?
 		</comment>
 		<comment id='9' author='cristianmtr' date='2020-11-10T09:26:05Z'>
 		
 Yeah, I guess we can do that. Is there a known list of rankers/sorters and whether they are lower or higher is better?
 
 this list will evolve, so I would keep it conceptual
 		</comment>
 		<comment id='10' author='cristianmtr' date='2020-11-10T10:55:40Z'>
 		I will try to write up something after reading through the code.
 Another possible approach: since all the classes that run any of the scoring / measure know their metric (right?), we could perhaps have them invert it so as to match one specific direction (lower or higher).
 E.g. NumpyIndexer knows it uses cosine dist. It measures distances between vectors. It gets the lowest nr as top result, right? We could then do 1 - score to map it so that higher is better.
 It might be problematic in the cases where we don't know the upper bound of a scoring/ranking function (not sure which examples might fit here)
 		</comment>
 	</comments>
 </bug>
<commit id='6a84ca2f143e61e64d21978bc01671e97d551524' author='cristian' date='2020-11-11 12:03:35+01:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='docs\chapters\flow\index.md' new_name='docs\chapters\flow\index.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556</added_lines>
 			<deleted_lines>532</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
