<bug_data>
<bug id='54' author='TomNong' open_date='2019-06-22T18:49:56Z' closed_time='2019-06-25T23:31:40Z'>
 	<summary>AttentionRNNDecoder param initial_state wrapped in AttentionWrapperState</summary>
 	<description>
 Hi &lt;denchmark-link:https://github.com/gpengzhi&gt;@gpengzhi&lt;/denchmark-link&gt;
  , we noticed that it would be too heavy to wrap the AttentionRNNDecoder forward method parameter "initial_state" into AttentionWrapperState. The position is here: &lt;denchmark-link:https://github.com/asyml/texar-pytorch/blob/master/texar/modules/decoders/rnn_decoders.py#L518&gt;https://github.com/asyml/texar-pytorch/blob/master/texar/modules/decoders/rnn_decoders.py#L518&lt;/denchmark-link&gt;
 . Would you consider making it as a regular tensor? Thanks!
 	</description>
 	<comments>
 		<comment id='1' author='TomNong' date='2019-06-22T18:57:29Z'>
 		Good point. &lt;denchmark-link:https://github.com/TomNong&gt;@TomNong&lt;/denchmark-link&gt;
  I think we could simplify this part. Let me think about the code refactoring on the part.
 		</comment>
 		<comment id='2' author='TomNong' date='2019-06-22T18:58:05Z'>
 		In the TF version, the user only needs to provide the actual cell state (not the wrapper state) as initial_state, and the state is wrapped into an AttentionWrapperState in AttentionRNNDecoder.initialize. We could follow similar logic here.
 Note that this breaks the type annotations in the method signature, as initial_state is no longer of type Optional[State] where State is the first type argument of RNNDecoder. Consider changing the annotation of initial_state to Optional[RNNCellBase] which allows arbitrary cell types. mypy should not report errors since the signature is already # type: ignore'd.
 		</comment>
 	</comments>
 </bug>
<commit id='567aaa1589d08bfed086467f258d17d1138a5b51' author='Pengzhi Gao' date='2019-06-25 16:59:34-04:00'>
 	<dmm_unit complexity='1.0' interfacing='0.9166666666666666' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='examples\seq2seq_attn\seq2seq_attn.py' new_name='examples\seq2seq_attn\seq2seq_attn.py'>
 		<file_info nloc='131' complexity='14' token_count='941'></file_info>
 		<method name='main._train_epoch' parameters=''>
 				<method_info nloc='11' complexity='3' token_count='65' nesting_level='1' start_line='132' end_line='143'></method_info>
 			<added_lines>139</added_lines>
 			<deleted_lines>139</deleted_lines>
 		</method>
 		<method name='main' parameters=''>
 				<method_info nloc='21' complexity='2' token_count='169' nesting_level='0' start_line='119' end_line='182'></method_info>
 			<added_lines>139</added_lines>
 			<deleted_lines>139</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='texar\core\attention_mechanism.py' new_name='texar\core\attention_mechanism.py'>
 		<file_info nloc='795' complexity='25' token_count='2933'></file_info>
 		<modified_lines>
 			<added_lines>34,928</added_lines>
 			<deleted_lines>34,928</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='texar\modules\decoders\rnn_decoders.py' new_name='texar\modules\decoders\rnn_decoders.py'>
 		<file_info nloc='598' complexity='11' token_count='1378'></file_info>
 		<method name='forward' parameters='self,Tensor,None,None,None,None,None,None,bool,None,kwargs'>
 				<method_info nloc='13' complexity='1' token_count='112' nesting_level='1' start_line='539' end_line='551'></method_info>
 			<added_lines>539,540,541,542,543,544,545,546,547,548,549</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='forward' parameters='self,Tensor,None,None,None,None,None,None,bool,None,kwargs'>
 				<method_info nloc='12' complexity='1' token_count='104' nesting_level='1' start_line='513' end_line='524'></method_info>
 			<added_lines>513,514</added_lines>
 			<deleted_lines>513,514,515,516,517,518,519,520,521,522</deleted_lines>
 		</method>
 		<method name='_get_batch_size_fn' parameters='x'>
 				<method_info nloc='4' complexity='2' token_count='25' nesting_level='3' start_line='503' end_line='506'></method_info>
 			<added_lines>503,504,505,506</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='initialize' parameters='self,Helper'>
 				<method_info nloc='7' complexity='1' token_count='59' nesting_level='1' start_line='489' end_line='495'></method_info>
 			<added_lines>489,490,491,492,493,494,495</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>30,496,497,498,499,500,501,502,507,508,509,510,511,512,648,649</added_lines>
 			<deleted_lines>30,621</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
