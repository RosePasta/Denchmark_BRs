<bug_data>
<bug id='313' author='tanyuqian' open_date='2020-07-06T03:03:45Z' closed_time='2020-07-21T20:04:11Z'>
 	<summary>A bug in GPT2Tokenizer.</summary>
 	<description>
 GPT2Tokenizer fails to recover a sentence "BART is a seq2seq model." with encoded ids of it. The output sentence is "BART is a seqseq model.". It should be related to numbers' processing.
 A script to show the bug is here: &lt;denchmark-link:https://github.com/tanyuqian/texar-pytorch/blob/master/examples/bart/gpt2_tokenizer_bug.py&gt;https://github.com/tanyuqian/texar-pytorch/blob/master/examples/bart/gpt2_tokenizer_bug.py&lt;/denchmark-link&gt;
 
 	</description>
 	<comments>
 	</comments>
 </bug>
<commit id='4d08a8c2d92f67ca54723a849a34beaf40a59cf6' author='Pengzhi Gao' date='2020-07-21 16:04:10-04:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='texar\torch\data\tokenizers\gpt2_tokenizer.py' new_name='texar\torch\data\tokenizers\gpt2_tokenizer.py'>
 		<file_info nloc='329' complexity='28' token_count='1535'></file_info>
 		<modified_lines>
 			<added_lines>129,143</added_lines>
 			<deleted_lines>129,143,144</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
