<bug_data>
<bug id='392' author='lalitpagaria' open_date='2020-09-17T16:25:43Z' closed_time='2020-09-21T11:37:55Z'>
 	<summary>Faiss document store create duplicate vector_ids</summary>
 	<description>
 Describe the bug
 Faiss document store will generate duplicate vector ids when number of documents in write_documents and update_embeddings functions is greater than configured index_buffer_size.
 Error message
 No error message
 Expected behavior
 All written documents should have unique vector_ids
 Additional context
 Using enumerate index as vector_id causing this issue.
 &lt;denchmark-code&gt;for vector_id, doc in enumerate(document_objects[i: i + self.index_buffer_size]): 
 &lt;/denchmark-code&gt;
 
 My &lt;denchmark-link:https://github.com/deepset-ai/haystack/pull/385&gt;PR&lt;/denchmark-link&gt;
  will fix this issue, because of refactoring of vector_is generation logic.
 To Reproduce
 Following test data will reproduce this issue
 &lt;denchmark-code&gt;documents = [
         {"name": "name_1", "text": "text_1", "embedding": np.random.rand(768).astype(np.float32)},
         {"name": "name_2", "text": "text_2", "embedding": np.random.rand(768).astype(np.float32)},
         {"name": "name_3", "text": "text_3", "embedding": np.random.rand(768).astype(np.float32)},
     ]
 
     document_store = FAISSDocumentStore(sql_url="sqlite:///haystack_test_faiss.db", index_buffer_size=len(documents) - 1)
     document_store.delete_all_documents()
     document_store.write_documents(documents)
     documents_indexed = document_store.get_all_documents()
 
     # test if number of documents is correct
     assert len(documents_indexed) == len(documents)
 
     # test if two docs have same vector_is assigned
     vector_ids = set()
     for i, doc in enumerate(documents_indexed):
         vector_ids.add(doc.meta["vector_id"])
     assert len(vector_ids) == len(documents)
 &lt;/denchmark-code&gt;
 
 System:
 All system
 	</description>
 	<comments>
 		<comment id='1' author='lalitpagaria' date='2020-09-17T16:26:34Z'>
 		It will be fixed by this PR &lt;denchmark-link:https://github.com/deepset-ai/haystack/pull/385&gt;#385&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='lalitpagaria' date='2020-09-18T06:32:14Z'>
 		Hey &lt;denchmark-link:https://github.com/lalitpagaria&gt;@lalitpagaria&lt;/denchmark-link&gt;
  ,
 Good catch! I can reproduce this bug. I saw &lt;denchmark-link:https://github.com/deepset-ai/haystack/pull/385&gt;#385&lt;/denchmark-link&gt;
 , but I think for this bug we can have a simpler solution. I will therefore create a separate PR to not mix it with the bigger changes of &lt;denchmark-link:https://github.com/deepset-ai/haystack/pull/385&gt;#385&lt;/denchmark-link&gt;
 . We will need a more careful review and discussion there.
 		</comment>
 		<comment id='3' author='lalitpagaria' date='2020-09-21T11:37:55Z'>
 		Fixed by &lt;denchmark-link:https://github.com/deepset-ai/haystack/pull/395&gt;#395&lt;/denchmark-link&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='4c503158a7555becf5b67473ce0bcd55413b5544' author='Malte Pietsch' date='2020-09-18 12:52:22+02:00'>
 	<dmm_unit complexity='0.8823529411764706' interfacing='0.9411764705882353' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='haystack\document_store\faiss.py' new_name='haystack\document_store\faiss.py'>
 		<file_info nloc='139' complexity='32' token_count='1140'></file_info>
 		<method name='write_documents' parameters='self,None'>
 				<method_info nloc='24' complexity='12' token_count='235' nesting_level='1' start_line='52' end_line='82'></method_info>
 			<added_lines>66,73,77,81</added_lines>
 			<deleted_lines>72</deleted_lines>
 		</method>
 		<method name='update_embeddings' parameters='self,BaseRetriever,None'>
 				<method_info nloc='24' complexity='8' token_count='237' nesting_level='1' start_line='106' end_line='144'></method_info>
 			<added_lines>127,130,131,135,136,137,138,139</added_lines>
 			<deleted_lines>126,130,131,132,133,134</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>169</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='test\test_faiss.py' new_name='test\test_faiss.py'>
 		<file_info nloc='52' complexity='10' token_count='590'></file_info>
 		<method name='test_faiss_write_docs' parameters='document_store,index_buffer_size'>
 				<method_info nloc='19' complexity='5' token_count='233' nesting_level='0' start_line='9' end_line='41'></method_info>
 			<added_lines>9,16,17,25,26,27,28,29,30,31,32,41</added_lines>
 			<deleted_lines>28</deleted_lines>
 		</method>
 		<method name='test_faiss_update_docs' parameters='document_store,index_buffer_size'>
 				<method_info nloc='25' complexity='5' token_count='273' nesting_level='0' start_line='45' end_line='83'></method_info>
 			<added_lines>45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_faiss_indexing' parameters='document_store'>
 				<method_info nloc='14' complexity='2' token_count='160' nesting_level='0' start_line='6' end_line='28'></method_info>
 			<added_lines>8,9,16,17,25,26,27,28</added_lines>
 			<deleted_lines>6,28</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>3,5,42,43,44,84,85</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
