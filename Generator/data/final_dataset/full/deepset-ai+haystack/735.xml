<bug_data>
<bug id='735' author='brandenchan' open_date='2021-01-14T10:51:37Z' closed_time='2021-01-22T09:06:38Z'>
 	<summary>Ram Issue in Tutorial 9 (DPR Training) Colab</summary>
 	<description>
 When running the DPR Training tutorial in Colab, the download of the training dataset seems to run fine, but the program crashes before starting to download the dev file due to running out of RAM.
 # Download original DPR data
 # WARNING: the train set is 7.4GB and the dev set is 800MB
 doc_dir = "data/dpr_training/"
 s3_url_train = "https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-train.json.gz"
 s3_url_dev = "https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-dev.json.gz"
 fetch_archive_from_http(s3_url_train, output_dir=doc_dir + "train/")
 fetch_archive_from_http(s3_url_dev, output_dir=doc_dir + "dev/")
 We need to find some way to reduce this RAM consumption. It likely has something to do with the way the files are being uncompressed
 def fetch_archive_from_http(url: str, output_dir: str, proxies: Optional[dict] = None):
             ...
             elif url[-3:] == ".gz":
                 json_bytes = gzip.open(temp_file.name).read()
                 filename = url.split("/")[-1].replace(".gz", "")
                 output_filename = Path(output_dir) / filename
                 output = open(output_filename, "wb")
                 output.write(json_bytes)
             else:
                 ...
         return True
 	</description>
 	<comments>
 		<comment id='1' author='brandenchan' date='2021-01-14T11:04:57Z'>
 		How about our tutorial just uses small files so people can quickly go through the code + execution, and we have the links for the large datafiles for interested users as comments?
 		</comment>
 		<comment id='2' author='brandenchan' date='2021-01-14T14:32:42Z'>
 		Actually following line read whole file in memory also decompress it in memory as well.
 &lt;denchmark-code&gt;json_bytes = gzip.open(temp_file.name).read()
 &lt;/denchmark-code&gt;
 
 Python 3 support buffered reading automatically so if change it to as follows memory utilization will improve -
 &lt;denchmark-code&gt;            elif url[-3:] == ".gz":
                 filename = url.split("/")[-1].replace(".gz", "")
                 output_filename = Path(output_dir) / filename
                 with gzip.open(temp_file.name) as f, open(output_filename, "wb") as output:
                         for line in f:
                                output.write(line)
 &lt;/denchmark-code&gt;
 
 I have not tested above snippet, I will do tonight. If this not work fine there is another solution to read file in &lt;denchmark-link:https://stackoverflow.com/questions/34632521/downloading-a-large-file-in-chunks-with-gzip-encoding-python-3-4&gt;chunks&lt;/denchmark-link&gt;
 . But I don't think the would be necessary as gzip already support buffered IO.
 One more point we can directly uncompress file from url instead of download to temp file and uncompressing it. Python compression libs have streaming support. Refer this &lt;denchmark-link:https://github.com/deepset-ai/haystack/issues/709#issuecomment-755654431&gt;#709 (comment)&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='3' author='brandenchan' date='2021-01-14T14:51:31Z'>
 		Hey &lt;denchmark-link:https://github.com/lalitpagaria&gt;@lalitpagaria&lt;/denchmark-link&gt;
  thanks so much for the suggestion! I actually tested it and it solved the problem. I haven't removed the tempfile code but I did integrate your snippet in &lt;denchmark-link:https://github.com/deepset-ai/haystack/pull/737&gt;#737&lt;/denchmark-link&gt;
 .
 		</comment>
 		<comment id='4' author='brandenchan' date='2021-01-14T15:09:57Z'>
 		&lt;denchmark-link:https://github.com/brandenchan&gt;@brandenchan&lt;/denchmark-link&gt;
  glad to know it. Thanks for testing it. 
 		</comment>
 		<comment id='5' author='brandenchan' date='2021-01-21T19:43:28Z'>
 		&lt;denchmark-link:https://github.com/tholor&gt;@tholor&lt;/denchmark-link&gt;
  This can be closed now
 		</comment>
 	</comments>
 </bug>
<commit id='725c03220f74f4ab6a9411840a630e5690b3bd29' author='Branden Chan' date='2021-01-21 09:57:55+01:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='haystack\preprocessor\utils.py' new_name='haystack\preprocessor\utils.py'>
 		<file_info nloc='304' complexity='52' token_count='1971'></file_info>
 		<method name='fetch_archive_from_http' parameters='str,str,None'>
 				<method_info nloc='32' complexity='7' token_count='250' nesting_level='0' start_line='328' end_line='376'></method_info>
 			<added_lines>369,370,371</added_lines>
 			<deleted_lines>367,370,371</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
