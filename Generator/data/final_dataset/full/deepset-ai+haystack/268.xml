<bug_data>
<bug id='268' author='AC-29' open_date='2020-07-27T22:38:07Z' closed_time='2020-09-22T14:24:28Z'>
 	<summary>Multiprocessing pool issue</summary>
 	<description>
 Hi there!
 I am currently trying to fine tune haystack using a json file of 500 questions and answers that I created using Hyastack Online Annotation Tool. However, when running reader.train(data_dir=train_data, train_filename="answers_500.json", use_gpu=True, n_epochs=1, save_dir="my_model") I get the following error message:
 Preprocessing Dataset data\answers_500.json:   0%|          | 0/13 [03:52&lt;?, ? Dicts/s]
 multiprocessing.pool.RemoteTraceback:
 """
 Traceback (most recent call last):
 File "C:\ProgramData\Anaconda3\envs\General\lib\multiprocessing\pool.py", line 121, in worker
 result = (True, func(_args, *_kwds))
 File "C:\ProgramData\Anaconda3\envs\General\lib\site-packages\farm\data_handler\data_silo.py", line 124, in _dataset_from_chunk
 dataset = processor.dataset_from_dicts(dicts=dicts, indices=indices)
 File "C:\ProgramData\Anaconda3\envs\General\lib\site-packages\farm\data_handler\processor.py", line 1144, in dataset_from_dicts
 dataset, tensor_names = self._create_dataset(keep_baskets=False)
 File "C:\ProgramData\Anaconda3\envs\General\lib\site-packages\farm\data_handler\processor.py", line 308, in _create_dataset
 features_flat.extend(sample.features)
 TypeError: 'NoneType' object is not iterable
 """
 The above exception was the direct cause of the following exception:
 Traceback (most recent call last):
 File "C:/Users/haystack_finetuning.py", line 25, in 
 main()
 File "C:/Users/haystack_finetuning.py", line 14, in main
 reader.train(data_dir=train_data, train_filename="answers_500.json", use_gpu=True, n_epochs=1, save_dir="my_model")
 File "c:\users\haystack\haystack\reader\farm.py", line 180, in train
 data_silo = DataSilo(processor=processor, batch_size=batch_size, distributed=False)
 File "C:\ProgramData\Anaconda3\envs\General\lib\site-packages\farm\data_handler\data_silo.py", line 105, in init
 self._load_data()
 File "C:\ProgramData\Anaconda3\envs\General\lib\site-packages\farm\data_handler\data_silo.py", line 207, in _load_data
 self.data["train"], self.tensor_names = self._get_dataset(train_file)
 File "C:\ProgramData\Anaconda3\envs\General\lib\site-packages\farm\data_handler\data_silo.py", line 176, in _get_dataset
 for dataset, tensor_names in results:
 File "C:\ProgramData\Anaconda3\envs\General\lib\multiprocessing\pool.py", line 748, in next
 raise value
 TypeError: 'NoneType' object is not iterable
 I have been diving into the code trying to understand the problem and for some reason the variable "results" from data_silo.py appears to be empty causing the aforementioned error, however I do not fully understand why. I have been doing some tests and it works fine with, for instance, the SQUAD json file.  Is there anything I am missing? Should anything be done with the json file that is obtained after expporting the QnA from the Annotation tool?
 Thank you in advance
 â”†Issue is synchronized with this &lt;denchmark-link:https://deepset-ai.atlassian.net/browse/HS-34&gt;Jira Task&lt;/denchmark-link&gt;
  by &lt;denchmark-link:https://www.unito.io/learn-more&gt;Unito&lt;/denchmark-link&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='AC-29' date='2020-07-28T07:10:48Z'>
 		Hi &lt;denchmark-link:https://github.com/AC-29&gt;@AC-29&lt;/denchmark-link&gt;
  ,
 Can you please check if you have documents without any labels in your json? This could potentially trigger such an exception.
 		</comment>
 		<comment id='2' author='AC-29' date='2020-07-28T08:26:11Z'>
 		Hi &lt;denchmark-link:https://github.com/tholor&gt;@tholor&lt;/denchmark-link&gt;
  ,
 Thanks for your answer. I assume that by documents without any labels you mean documents without corresponding questions and answers. If that is the case, I had some documents without any questions. I just removed those documents from the annotation tool and exported the json file again but unfortunately it is still not working. I still get the same error.
 		</comment>
 		<comment id='3' author='AC-29' date='2020-07-28T16:35:48Z'>
 		Hi &lt;denchmark-link:https://github.com/tholor&gt;@tholor&lt;/denchmark-link&gt;
  ,
 Thanks for following up on my question and suggesting to configure a multiprocessing flag for reader.train(). I have continued on reading through the code. Something I omitted on my first comment is that I am receiving an error before the one I mentioned which, in the beginning I though it was just a warning but I am not sure anymore. I can see the messages "ERROR - farm.data_handler.processor -   Could not convert this sample to features: " and " ERROR - farm.data_handler.processor -  Answer using start/end indices is 'XXXXX' while gold label text is  'XXXXXXXX'. This is happening because in the function _check_valid_answer from processor.py, answer_indices and answer_text are different. Basically, what I am finding is that in some cases the answer_indices point to the "correct" text of the answer, for instance "This is the answer to your question.". However, for some reason the answer_text (which I believe is the answer we highlight in the Annotation Tool while generating Questions-Answers pairs) may contain some special characters which are not considered by the indices. Following on the previous example, this means answer_text would be for instance "\n\nThis is the answer to your questio". They have the same length but the strings are different, since the indices point to the proper text whereas the answer_text contains special characters. From this I am wondering whether this Error could affect somehow by creating sample attributes or features as None (therefore triggering the error shown in the first comment). Also, what could be the reason for having different answer_indices and answer_text strings?
 Thank you again.
 		</comment>
 		<comment id='4' author='AC-29' date='2020-07-29T16:46:24Z'>
 		Hi &lt;denchmark-link:https://github.com/AC-29&gt;@AC-29&lt;/denchmark-link&gt;
  ,
 Thanks for the additional info. This misalignment of answer text and offsets is actually a bug in our annotation tool that happens in some circumstances (special characters play a role here). We are working on a fix, but also created a little gist that should fix your issue in the shorterm on the existing labels. It prints and corrects most, if not all, of the problematic cases: &lt;denchmark-link:https://gist.github.com/Timoeller/be6dfd8e34cdcd84fdca4c4aa72f42fc&gt;https://gist.github.com/Timoeller/be6dfd8e34cdcd84fdca4c4aa72f42fc&lt;/denchmark-link&gt;
 
 Nevertheless, training should not stop if some QA pairs contain mistakes. Have you already tried running training with disabled multiprocessing? You can do that with the new arg introduced in &lt;denchmark-link:https://github.com/deepset-ai/haystack/pull/271&gt;#271&lt;/denchmark-link&gt;
  :
 &lt;denchmark-code&gt;reader.train(..., num_processes=1)
 &lt;/denchmark-code&gt;
 
 Hope this helps!
 		</comment>
 		<comment id='5' author='AC-29' date='2020-07-30T15:18:18Z'>
 		HI &lt;denchmark-link:https://github.com/tholor&gt;@tholor&lt;/denchmark-link&gt;
  ,
 Thank you very much. The gist you provided apparently fix most of the errors I was getting with farm.data_handler.processor. Regarding reader.train() unfortunately when using num_processes = 1 I still get the error:
 Preprocessing Dataset data\answers_500_v4_corrected.json:   0%|          | 0/13 [05:40&lt;?, ? Dicts/s]
 Traceback (most recent call last):
 File "C:/Users/haystack_finetuning.py", line 32, in 
 main()
 File "C:/Users/haystack_finetuning.py", line 22, in main
 reader.train(data_dir=train_data, train_filename="answers_500_v4_corrected.json", use_gpu=True, n_epochs=1, save_dir="my_model", num_processes=1)
 File "c:\users\haystack\reader\farm.py", line 186, in train
 data_silo = DataSilo(processor=processor, batch_size=batch_size, distributed=False, max_processes=num_processes)
 File "C:\ProgramData\Anaconda3\envs\General\lib\site-packages\farm\data_handler\data_silo.py", line 105, in init
 self._load_data()
 File "C:\ProgramData\Anaconda3\envs\General\lib\site-packages\farm\data_handler\data_silo.py", line 207, in _load_data
 self.data["train"], self.tensor_names = self._get_dataset(train_file)
 File "C:\ProgramData\Anaconda3\envs\General\lib\site-packages\farm\data_handler\data_silo.py", line 176, in _get_dataset
 for dataset, tensor_names in results:
 File "C:\ProgramData\Anaconda3\envs\General\lib\site-packages\farm\data_handler\data_silo.py", line 124, in _dataset_from_chunk
 dataset = processor.dataset_from_dicts(dicts=dicts, indices=indices)
 File "C:\ProgramData\Anaconda3\envs\General\lib\site-packages\farm\data_handler\processor.py", line 1144, in dataset_from_dicts
 dataset, tensor_names = self._create_dataset(keep_baskets=False)
 File "C:\ProgramData\Anaconda3\envs\General\lib\site-packages\farm\data_handler\processor.py", line 308, in _create_dataset
 features_flat.extend(sample.features)
 TypeError: 'NoneType' object is not iterable
 It seems like rolling back to previous commits of haystack solved it and has allowed me to fine tune the model. The last commit that allowed me to fine tune without any issues is commit &lt;denchmark-link:https://github.com/deepset-ai/haystack/commit/17c1b84c21a95747809abd278f4db8c7bc7d6396&gt;17c1b84&lt;/denchmark-link&gt;
  from July 14th. The next commit (&lt;denchmark-link:https://github.com/deepset-ai/haystack/commit/99a6a340478ca6fb70137f6811a1af282796debc&gt;99a6a34&lt;/denchmark-link&gt;
  from July 14th as well) already triggers the error (as well as the newest versions up to now).
 		</comment>
 		<comment id='6' author='AC-29' date='2020-08-03T15:31:00Z'>
 		Thanks for locating the commit. There we Upgraded to a new FARM version where a lot of QA processing was refactored.
 
 Possibly you did this already, but just to be sure: did you update to the new FARM version in your environment as well?
 I think the problem originates in some quite specific input data. Could you maybe share the QA data you created with us so we can replicate the issue? if so, send to timo.moeller [at] deepset.ai
 
 		</comment>
 		<comment id='7' author='AC-29' date='2020-08-05T16:13:13Z'>
 		Hey &lt;denchmark-link:https://github.com/AC-29&gt;@AC-29&lt;/denchmark-link&gt;
   we found more problems with the annotation tool export as mentioned in &lt;denchmark-link:https://github.com/deepset-ai/haystack/issues/287&gt;#287&lt;/denchmark-link&gt;
 .
 We adjusted the hotfix gist. Now it not only corrects the offsets, but also handles unanswerable questions. Please give it another try with the &lt;denchmark-link:https://gist.github.com/Timoeller/0ab74574600efe753d46125a09d809f5&gt;updated gist&lt;/denchmark-link&gt;
 .
 At the same time we are of course improving the tool + QA data handling in FARM...
 		</comment>
 		<comment id='8' author='AC-29' date='2020-09-22T14:24:27Z'>
 		Most cases leading to this error should be fixed by now in the annotation tool and new labels should therefore be correct.
 For old ones, we recommend using the above gist.
 Closing now.
 		</comment>
 	</comments>
 </bug>
<commit id='abec1be722ef74747184f8c9824c567b1f1322b1' author='Malte Pietsch' date='2020-07-29 16:28:23+02:00'>
 	<dmm_unit complexity='1.0' interfacing='0.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='haystack\reader\farm.py' new_name='haystack\reader\farm.py'>
 		<file_info nloc='285' complexity='22' token_count='1940'></file_info>
 		<method name='train' parameters='self,str,str,None,None,None,int,int,float,None,float,1,int,None,0'>
 				<method_info nloc='16' complexity='1' token_count='110' nesting_level='1' start_line='106' end_line='121'></method_info>
 			<added_lines>121</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>4,144,145,152,153,154,186</added_lines>
 			<deleted_lines>145,180</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
