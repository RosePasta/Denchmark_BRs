<bug_data>
<bug id='399' author='braaannigan' open_date='2020-04-01T13:19:12Z' closed_time='2020-05-06T20:24:15Z'>
 	<summary>LightGBM error special JSON characters in feature name</summary>
 	<description>
 Hi - great library everyone!
 I'm doing tabular prediction with news article data.  The LightGBM model doesn't run with the following error:
 &lt;denchmark-code&gt;LightGBMError: Do not support special JSON characters in feature name.
 Do not support special JSON characters in feature name.
 &lt;/denchmark-code&gt;
 
 The upstream issue in LightGBM is here:&lt;denchmark-link:https://github.com/microsoft/LightGBM/issues/2455&gt;microsoft/LightGBM#2455&lt;/denchmark-link&gt;
 
 Basically they check for special json characters .e.g [],{}": in features names and throw an error if found. Could autogluon check for these characters and remove any offending features?
 Liam
 	</description>
 	<comments>
 		<comment id='1' author='braaannigan' date='2020-04-01T18:14:07Z'>
 		Good catch, thanks for the info! Just to clarify, LightGBM fails in this scenario, but the rest of the models train successfully and you are able to get a result from AutoGluon?
 If so, I think I can look into fixing this, the question is if I can get it to still use the features rather than dropping them entirely (I think I can by clever renaming).
 		</comment>
 		<comment id='2' author='braaannigan' date='2020-04-01T18:46:12Z'>
 		Yep, everything else works fine and I get a result at the end
 		</comment>
 		<comment id='3' author='braaannigan' date='2020-04-02T19:19:54Z'>
 		To aid in testing to ensure I am covering all of the failure cases, could you provide a sample training data of the news article data you are experiencing the problem with or the location which you obtained the data? If you wish to keep it private that is fine, but it could help accelerating the fix on my end.
 		</comment>
 		<comment id='4' author='braaannigan' date='2020-04-03T06:16:15Z'>
 		Sure - first 3 rows of csv are enough to trigger it - see attached zip.
 &lt;denchmark-link:https://github.com/awslabs/autogluon/files/4425329/err.tar.gz&gt;err.tar.gz&lt;/denchmark-link&gt;
 
 Is there a way to access errors in the underlying models using the %debug in ipython?
 		</comment>
 		<comment id='5' author='braaannigan' date='2020-05-04T08:21:59Z'>
 		Hi,
 I have encountred the same problem. Fixed it before the training begins with a regular expression and lambda function (pandas). Object named "data" must be a dataframe.
 import re
 data = data.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))
 Could provide data/column headers if needed.
 		</comment>
 		<comment id='6' author='braaannigan' date='2020-05-04T11:37:32Z'>
 		Thanks for the info &lt;denchmark-link:https://github.com/oguzhangur96&gt;@oguzhangur96&lt;/denchmark-link&gt;
 ! Right now the main difficulty here is to ensure that column names are always converted when entering the model (whether for feature importances, fit, inference) and also inversely converted back to the original upon leaving. This seems to be something that LightGBM itself should handle, and I don't have a good understanding of why they haven't done so.
 Furthermore, it isn't sufficient to only remove the special characters, because we have to ensure no two columns have the same name. One simple way is to just rename columns 0-n as '0', '1', ...'n-1', 'n', but then this operation would also have to be done on all of the 99% of problems where this is not required, particularly for online-inference of single rows. I have to first benchmark inference times in these situations to ensure our online-inference speed isn't significantly slowed due to this fix.
 		</comment>
 		<comment id='7' author='braaannigan' date='2020-05-06T20:24:15Z'>
 		&lt;denchmark-link:https://github.com/braaannigan&gt;@braaannigan&lt;/denchmark-link&gt;
  , &lt;denchmark-link:https://github.com/oguzhangur96&gt;@oguzhangur96&lt;/denchmark-link&gt;
 
 &lt;denchmark-link:https://github.com/awslabs/autogluon/pull/451&gt;#451&lt;/denchmark-link&gt;
  is merged and should fix this issue (Just in time for our upcoming 0.0.7 release!). I have tested this on various NLP problems which previously caused crashes, and it successfully trains.
 Thanks again for highlighting this issue!
 Best,
 Nick
 		</comment>
 		<comment id='8' author='braaannigan' date='2020-05-07T14:33:35Z'>
 		Great - looking forward to the next release!
 		</comment>
 		<comment id='9' author='braaannigan' date='2020-06-04T10:52:01Z'>
 		how can i fix this error "LightGBMError: Do not support special JSON characters in feature name."
 		</comment>
 		<comment id='10' author='braaannigan' date='2020-06-04T18:20:35Z'>
 		&lt;denchmark-link:https://github.com/ind-kum&gt;@ind-kum&lt;/denchmark-link&gt;
  Please check your AutoGluon version and upgrade to 0.0.10 if you haven't. This should fix the issue.
 		</comment>
 	</comments>
 </bug>
<commit id='cd5f6050bc9f33838b5ce3a5849df82ac63b61bd' author='Nick Erickson' date='2020-05-06 13:17:34-07:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='autogluon\utils\tabular\ml\models\abstract\abstract_model.py' new_name='autogluon\utils\tabular\ml\models\abstract\abstract_model.py'>
 		<file_info nloc='407' complexity='111' token_count='3249'></file_info>
 		<method name='compute_feature_importance' parameters='self,X,y,features_to_use,preprocess,subsample_size,silent,kwargs'>
 				<method_info nloc='22' complexity='11' token_count='244' nesting_level='1' start_line='241' end_line='270'></method_info>
 			<added_lines>253,255</added_lines>
 			<deleted_lines>253,255</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='autogluon\utils\tabular\ml\models\lgb\lgb_model.py' new_name='autogluon\utils\tabular\ml\models\lgb\lgb_model.py'>
 		<file_info nloc='252' complexity='72' token_count='2143'></file_info>
 		<method name='compute_feature_importance' parameters='self,kwargs'>
 				<method_info nloc='6' complexity='4' token_count='68' nesting_level='1' start_line='280' end_line='285'></method_info>
 			<added_lines>280,281,282,283,284,285</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='get_model_feature_importance' parameters='self,use_original_feature_names'>
 				<method_info nloc='8' complexity='6' token_count='100' nesting_level='1' start_line='298' end_line='305'></method_info>
 			<added_lines>298,302,303,304</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='get_model_feature_importance' parameters='self'>
 				<method_info nloc='5' complexity='2' token_count='45' nesting_level='1' start_line='270' end_line='274'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>270</deleted_lines>
 		</method>
 		<method name='generate_datasets' parameters='self,DataFrame,Series,params,X_test,Y_test,dataset_train,dataset_val,save'>
 				<method_info nloc='14' complexity='9' token_count='198' nesting_level='1' start_line='167' end_line='186'></method_info>
 			<added_lines>174</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='preprocess' parameters='self,X,is_train'>
 				<method_info nloc='15' complexity='7' token_count='120' nesting_level='1' start_line='149' end_line='165'></method_info>
 			<added_lines>149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165</added_lines>
 			<deleted_lines>154</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>5,38,166,279,286</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
