<bug_data>
<bug id='305' author='ellehoej' open_date='2018-10-29T13:45:38Z' closed_time='2018-11-09T19:49:21Z'>
 	<summary>DeepExplainer doesn't work with keras concatenate layers</summary>
 	<description>
 Hi,
 Thanks for some great work.
 I'm getting an error (see below) when using DeepExplainer (SHAP 0.24, TF 1.8, Keras 2.2.4) on a keras model that uses "concatenate" through the Keras functional API.
 The model is simply:
 &lt;denchmark-code&gt;x_input = Input(shape=(days_input, n_features_input))
 
 x_d = Flatten()(x_input)
 
 x_d1 = Dense(256, use_bias=False)(x_d)
 x_d1 = BatchNormalization()(x_d1)
 x_dl = PReLU()(x_d1)
 x_d1 = Dropout(0.5)(x_d1)
 
 x_d3 = Dense(128, use_bias=False)(x_d1)
 x_d3 = BatchNormalization()(x_d3)
 x_d3 = PReLU()(x_d3)
 x_d3 = Dropout(0.5)(x_d3)
 
 x_d3 = concatenate([x_d1, x_d3])
 
 dense = Dense(1,activation='sigmoid')
 
 x = dense(x_d3)
 
 model = Model(inputs=x_input, outputs=x)
 model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])
 &lt;/denchmark-code&gt;
 
 Generating the DeepExplainer works:
 &lt;denchmark-code&gt;e = shap.DeepExplainer(model, train_X[:20])
 &lt;/denchmark-code&gt;
 
 But when generating shap_values:
 &lt;denchmark-code&gt;shap_values = e.shap_values(test_X)
 &lt;/denchmark-code&gt;
 
 I get the following error:
 &lt;denchmark-code&gt;InvalidArgumentError                      Traceback (most recent call last)
 C:\Anaconda\envs\keras_updated\lib\site-packages\tensorflow\python\framework\ops.py in get_attr(self, name)
    2326         with c_api_util.tf_buffer() as buf:
 -&gt; 2327           c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)
    2328           data = c_api.TF_GetBuffer(buf)
 
 InvalidArgumentError: Operation 'concatenate_1/concat' has no attr named '_XlaCompile'.
 
 During handling of the above exception, another exception occurred:
 
 ValueError                                Traceback (most recent call last)
 C:\Anaconda\envs\keras_updated\lib\site-packages\tensorflow\python\ops\gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)
     379     try:
 --&gt; 380       xla_compile = op.get_attr("_XlaCompile")
     381       xla_separate_compiled_gradients = op.get_attr(
 
 C:\Anaconda\envs\keras_updated\lib\site-packages\tensorflow\python\framework\ops.py in get_attr(self, name)
    2330         # Convert to ValueError for backwards compatibility.
 -&gt; 2331         raise ValueError(str(e))
    2332       x = attr_value_pb2.AttrValue()
 
 ValueError: Operation 'concatenate_1/concat' has no attr named '_XlaCompile'.
 
 During handling of the above exception, another exception occurred:
 
 AssertionError                            Traceback (most recent call last)
 &lt;ipython-input-23-9f62380f1037&gt; in &lt;module&gt;()
 ----&gt; 1 shap_values = e.shap_values(test_X)
 
 C:\Anaconda\envs\keras_updated\lib\site-packages\shap\explainers\deep.py in shap_values(self, X, ranked_outputs, output_rank_order)
     290                 # run attribution computation graph
     291                 feature_ind = model_output_ranks[j,i]
 --&gt; 292                 sample_phis = self.run(self.phi_symbolic(feature_ind), self.model_inputs, joint_input)
     293 
     294                 # assign the attributions to the right part of the output arrays
 
 C:\Anaconda\envs\keras_updated\lib\site-packages\shap\explainers\deep.py in phi_symbolic(self, i)
     202             try:
     203                 out = self.model_output[:,i] if self.multi_output else self.model_output
 --&gt; 204                 self.phi_symbolics[i] = tf.gradients(out, self.model_inputs)
     205 
     206             finally:
 
 C:\Anaconda\envs\keras_updated\lib\site-packages\tensorflow\python\ops\gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)
     492   with ops.get_default_graph()._lock:  # pylint: disable=protected-access
     493     return _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops,
 --&gt; 494                             gate_gradients, aggregation_method, stop_gradients)
     495 
     496 
 
 C:\Anaconda\envs\keras_updated\lib\site-packages\tensorflow\python\ops\gradients_impl.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)
     634                 # functions.
     635                 in_grads = _MaybeCompile(grad_scope, op, func_call,
 --&gt; 636                                          lambda: grad_fn(op, *out_grads))
     637               else:
     638                 # For function call ops, we add a 'SymbolicGradient'
 
 C:\Anaconda\envs\keras_updated\lib\site-packages\tensorflow\python\ops\gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)
     383       xla_scope = op.get_attr("_XlaScope").decode()
     384     except ValueError:
 --&gt; 385       return grad_fn()  # Exit early
     386 
     387   if not xla_compile:
 
 C:\Anaconda\envs\keras_updated\lib\site-packages\tensorflow\python\ops\gradients_impl.py in &lt;lambda&gt;()
     634                 # functions.
     635                 in_grads = _MaybeCompile(grad_scope, op, func_call,
 --&gt; 636                                          lambda: grad_fn(op, *out_grads))
     637               else:
     638                 # For function call ops, we add a 'SymbolicGradient'
 
 C:\Anaconda\envs\keras_updated\lib\site-packages\shap\explainers\deep.py in custom_grad(self, op, *grads)
     315         """ Passes a gradient op creation request to the correct handler.
     316         """
 --&gt; 317         return op_handlers[op.type](self, op, *grads)
     318 
     319 
 
 C:\Anaconda\envs\keras_updated\lib\site-packages\shap\explainers\deep.py in handler(explainer, op, *grads)
     515 def linearity_1d(input_ind):
     516     def handler(explainer, op, *grads):
 --&gt; 517         return linearity_1d_handler(input_ind, explainer, op, *grads)
     518     return handler
     519 
 
 C:\Anaconda\envs\keras_updated\lib\site-packages\shap\explainers\deep.py in linearity_1d_handler(input_ind, explainer, op, *grads)
     522     for i in range(len(op.inputs)):
     523         if i != input_ind:
 --&gt; 524             assert not explainer._variable_inputs(op)[i], str(i) + "th input to " + op.name + " cannot vary!"
     525 
     526     return explainer.orig_grads[op.type](op, *grads)
 
 AssertionError: 1th input to concatenate_1/concat cannot vary!
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='ellehoej' date='2018-11-02T23:42:17Z'>
 		The 1th input is the second (axis) argument so it looks like you have an axis that depends on your inputs...is that supposed to be true? Typically people don't choose the axis they concat with based on the model inputs.
 		</comment>
 		<comment id='2' author='ellehoej' date='2018-11-05T09:25:14Z'>
 		I think you misread the notation around  &lt;denchmark-link:https://github.com/slundberg&gt;@slundberg&lt;/denchmark-link&gt;
  .
 I get the same error using the following network:
 &lt;denchmark-code&gt;# Define inputs
 input_features = Input(shape=(n_features_input,))
 input_embeddings = Input(shape=(1,))
 
 # Define embeddings
 x_embeddings = Embedding(input_dim=n_stations, output_dim=embedding_size)(input_embeddings)
 x_embeddings = Reshape((embedding_size, ))(x_embeddings)
 
 # Concatenate
 x = Concatenate(axis=1)([input_features, x_embeddings])
 
 # Layers
 x = Dense(512, kernel_initializer='normal', activation='relu')(x)
 x = Dense(256, kernel_initializer='normal', activation='relu')(x)
 output = Dense(1, activation='linear')(x)
 
 # Compile model
 model = Model(
     inputs=[input_features, input_stations, input_vis_codes], 
     outputs=output)
 model.compile(
     loss='mean_squared_error', 
     optimizer=keras.optimizers.Adam(lr=learning_rate),
     metrics=['mse'])
 &lt;/denchmark-code&gt;
 
 The final error line (same as &lt;denchmark-link:https://github.com/ellehoej&gt;@ellehoej&lt;/denchmark-link&gt;
  ):
 &lt;denchmark-code&gt;AssertionError: 1th input to concatenate_10/concat cannot vary!
 &lt;/denchmark-code&gt;
 
 Is the Concatenate layer just not supported in shap?
 I am using
 
 Tensorflow 1.8 backend
 Keras 2.2.4
 Python 3.6
 
 		</comment>
 		<comment id='3' author='ellehoej' date='2018-11-08T18:31:16Z'>
 		There must be an issue with the assertion then, causing Concatenate to
 break. I will look into it as soon as I can, it could be that the assertion
 is checking the wrong input.
 &lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;
 
 
 On Mon, Nov 5, 2018 at 1:25 AM Lasse Regin Nielsen ***@***.***&gt; wrote:
  I think you misread the notation @slundberg &lt;https://github.com/slundberg&gt;
  .
 
  # Define inputs
  input_features = Input(shape=(n_features_input,))
  input_embeddings = Input(shape=(1,))
 
  # Define embeddings
  x_embeddings = Embedding(input_dim=n_stations, output_dim=embedding_size)(input_embeddings)
  x_embeddings = Reshape((embedding_size, ))(x_embeddings)
 
  # Concatenate
  x = Concatenate(axis=1)([input_features, x_embeddings])
 
  # Layers
  x = Dense(512, kernel_initializer='normal', activation='relu')(x)
  x = Dense(256, kernel_initializer='normal', activation='relu')(x)
  output = Dense(1, activation='linear')(x)
 
  # Compile model
  model = Model(
      inputs=[input_features, input_stations, input_vis_codes],
      outputs=output)
  model.compile(
      loss='mean_squared_error',
      optimizer=keras.optimizers.Adam(lr=learning_rate),
      metrics=['mse'])
 
  and I get the same error as @ellehoej &lt;https://github.com/ellehoej&gt;
 
  AssertionError: 1th input to concatenate_10/concat cannot vary!
 
  Is the Concatenate layer just not supported in shap?
 
  —
  You are receiving this because you were mentioned.
 
 
  Reply to this email directly, view it on GitHub
  &lt;#305 (comment)&gt;, or mute
  the thread
  &lt;https://github.com/notifications/unsubscribe-auth/ADkTxW8ACQmBfb2X8txlMw19ZeaQf1M7ks5usAP8gaJpZM4X_NKx&gt;
  .
 
 
 
 		</comment>
 		<comment id='4' author='ellehoej' date='2018-11-08T18:32:24Z'>
 		Thank you very much &lt;denchmark-link:https://github.com/slundberg&gt;@slundberg&lt;/denchmark-link&gt;
  ! Let me know if you need anything from me.
 		</comment>
 		<comment id='5' author='ellehoej' date='2018-11-09T17:06:10Z'>
 		&lt;denchmark-link:https://github.com/LasseRegin&gt;@LasseRegin&lt;/denchmark-link&gt;
  do you happen to already have a complete notebook that demonstrates the problem? If not I can build one.
 		</comment>
 		<comment id='6' author='ellehoej' date='2018-11-09T17:30:30Z'>
 		I created a new (simplified) Notebook demonstrating the problem. It can be found here:
 &lt;denchmark-link:https://gist.github.com/LasseRegin/a3de5fea9e6c3a499df52973574ed405&gt;https://gist.github.com/LasseRegin/a3de5fea9e6c3a499df52973574ed405&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='7' author='ellehoej' date='2018-11-09T17:31:46Z'>
 		Hi &lt;denchmark-link:https://github.com/slundberg&gt;@slundberg&lt;/denchmark-link&gt;
 ,
 Unfortunately we are not able to share the data,  but here's another example:
 &lt;denchmark-code&gt;#Define input from training set.
 n_features_input = train_X_features.shape[1]
 x_features = Input(shape=(n_features_input,), name ='Features_input')
 
 
 x_1 = Dense(256, use_bias=False)(x_features)
 x_1 = BatchNormalization()(x_1)
 x_l = PReLU()(x_1)
 x_1 = Dropout(0.5)(x_1)
 
 
 x_2 = Dense(256, use_bias=False)(x_1)
 x_2 = BatchNormalization()(x_2)
 x_2 = PReLU()(x_2)
 x_2 = Dropout(0.5)(x_2)
 
 x_3 = concatenate([x_1, x_2])
 
 dense = Dense(1,activation='sigmoid')
 x = dense(x_3)
 
 model = Model(inputs=x_features, outputs=x)
 
 adam = keras.optimizers.Adam(lr=3e-4)
 model.compile(loss='mean_squared_error', optimizer=adam, metrics=['mse'])
 model.summary()
 &lt;/denchmark-code&gt;
 
 The model summary is then:
 &lt;denchmark-code&gt;__________________________________________________________________________________________________
 Layer (type)                    Output Shape         Param #     Connected to                     
 ==================================================================================================
 Features_input (InputLayer)     (None, 64)           0                                            
 __________________________________________________________________________________________________
 dense_1 (Dense)                 (None, 256)          16384       Features_input[0][0]             
 __________________________________________________________________________________________________
 batch_normalization_1 (BatchNor (None, 256)          1024        dense_1[0][0]                    
 __________________________________________________________________________________________________
 dropout_1 (Dropout)             (None, 256)          0           batch_normalization_1[0][0]      
 __________________________________________________________________________________________________
 dense_2 (Dense)                 (None, 256)          65536       dropout_1[0][0]                  
 __________________________________________________________________________________________________
 batch_normalization_2 (BatchNor (None, 256)          1024        dense_2[0][0]                    
 __________________________________________________________________________________________________
 p_re_lu_2 (PReLU)               (None, 256)          256         batch_normalization_2[0][0]      
 __________________________________________________________________________________________________
 dropout_2 (Dropout)             (None, 256)          0           p_re_lu_2[0][0]                  
 __________________________________________________________________________________________________
 concatenate_1 (Concatenate)     (None, 512)          0           dropout_1[0][0]                  
                                                                  dropout_2[0][0]                  
 __________________________________________________________________________________________________
 dense_3 (Dense)                 (None, 1)            513         concatenate_1[0][0]              
 ==================================================================================================
 Total params: 84,737
 Trainable params: 83,713
 Non-trainable params: 1,024
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-code&gt;background = SHAP_train_X_scaled[::100]
 explainer = shap.DeepExplainer(model, background)
 X_shap = SHAP_train_X_scaled[::1000].values
 shap_values = explainer.shap_values(X_shap)
 &lt;/denchmark-code&gt;
 
 Gives us the error.
 Best regards
 Mads
 		</comment>
 		<comment id='8' author='ellehoej' date='2018-11-09T17:33:19Z'>
 		Excellent, &lt;denchmark-link:https://github.com/LasseRegin&gt;@LasseRegin&lt;/denchmark-link&gt;
 !
 		</comment>
 		<comment id='9' author='ellehoej' date='2018-11-09T19:50:07Z'>
 		Thanks &lt;denchmark-link:https://github.com/LasseRegin&gt;@LasseRegin&lt;/denchmark-link&gt;
  for the setup! I found and fixed the problem.
 		</comment>
 		<comment id='10' author='ellehoej' date='2018-11-09T21:17:21Z'>
 		That is great &lt;denchmark-link:https://github.com/slundberg&gt;@slundberg&lt;/denchmark-link&gt;
  ! Thank you very much!
 		</comment>
 		<comment id='11' author='ellehoej' date='2018-11-12T07:47:24Z'>
 		Thanks for the quick fix and for a great package in general &lt;denchmark-link:https://github.com/slundberg&gt;@slundberg&lt;/denchmark-link&gt;
  , it works in our end as well now.
 		</comment>
 	</comments>
 </bug>
<commit id='890d85b49228c061a0d46ad76e0bbaa6b506d015' author='Scott Lundberg' date='2018-11-09 11:49:13-08:00'>
 	<dmm_unit complexity='0.9090909090909091' interfacing='0.2727272727272727' size='0.9090909090909091'></dmm_unit>
 	<modification change_type='MODIFY' old_name='shap\__init__.py' new_name='shap\__init__.py'>
 		<file_info nloc='14' complexity='0' token_count='92'></file_info>
 		<modified_lines>
 			<added_lines>3</added_lines>
 			<deleted_lines>3</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='shap\explainers\deep\__init__.py' new_name='shap\explainers\deep\__init__.py'>
 		<file_info nloc='35' complexity='7' token_count='162'></file_info>
 		<method name='__init__' parameters='self,model,data,session,learning_phase_flags'>
 				<method_info nloc='19' complexity='6' token_count='106' nesting_level='1' start_line='18' end_line='82'></method_info>
 			<added_lines>82</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>83</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='shap\explainers\deep\deep_tf.py' new_name='shap\explainers\deep\deep_tf.py'>
 		<file_info nloc='401' complexity='152' token_count='4094'></file_info>
 		<method name='linearity_with_excluded' parameters='input_inds'>
 				<method_info nloc='3' complexity='1' token_count='9' nesting_level='0' start_line='484' end_line='487'></method_info>
 			<added_lines>484,485,486,487</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='linearity_with_excluded_handler' parameters='input_inds,explainer,op,grads'>
 				<method_info nloc='5' complexity='4' token_count='80' nesting_level='0' start_line='489' end_line='494'></method_info>
 			<added_lines>489,490,491,492,493,494</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='linearity_with_excluded.handler' parameters='explainer,op,grads'>
 				<method_info nloc='2' complexity='1' token_count='22' nesting_level='1' start_line='485' end_line='486'></method_info>
 			<added_lines>485,486</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='linearity_1d_handler' parameters='input_ind,explainer,op,grads'>
 				<method_info nloc='5' complexity='3' token_count='69' nesting_level='0' start_line='477' end_line='482'></method_info>
 			<added_lines>478</added_lines>
 			<deleted_lines>478</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>488,495,543</added_lines>
 			<deleted_lines>531</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
