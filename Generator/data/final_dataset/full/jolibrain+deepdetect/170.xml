<bug_data>
<bug id='170' author='EBazarov' open_date='2016-09-05T20:53:36Z' closed_time='2016-09-06T10:47:59Z'>
 	<summary>Accuracy score from DeDe and computed using sklearn</summary>
 	<description>
 I have a difficulty of understanding metrics from DeDe. When I'm providing Train and Test data set, DeDe compute metrics during training, and at the end provide final results. But when I predict same test set, using "predict API" and then compute accuracy metric, I had results that are not the same at all. I'm using SVM connector, and for testing purpose decided to use public dataset agnews.
 You can find my code and results in attached html report witch extracted from jupyter notebook.
 Maybe it's not problem of metrics, but problem of wrong using DeDe API, I'm confusing, because  difference is huge.
 &lt;denchmark-link:https://github.com/beniz/deepdetect/files/455607/DeDe_SVM_agnews_test_acc.zip&gt;DeDe_SVM_agnews_test_acc.zip&lt;/denchmark-link&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='EBazarov' date='2016-09-06T05:09:21Z'>
 		I believe your python code reproduced below is not doing what I suppose you want it to do:
 def read_dede_results(dic_results):
     pred_1_prob = []
     for pred in dic_results['body']['predictions']:
         pred_1_prob.append(pred['classes'][1]['prob'])
     y_pred_proba = np.array(pred_1_prob).astype(np.float)
     y_pred = np.around(np.array(pred_1_prob))
     return y_pred_proba, y_pred
 
 the top category and probability should be acquired from pred['classes'][0], not sure why you are using the second value instead;
 the label should be acquired from pred['classes'][0]['cat'], instead you are rounding all probabilities to 0
 
 In the end my understanding is that you are computing the accuracy of a classifier that returns label 0 everywhere.
 		</comment>
 		<comment id='2' author='EBazarov' date='2016-09-06T05:23:16Z'>
 		I think I get you, you mean that predicted classes sorted in results, that's mean you have at first place best label.
 Sorry my mistake, I will rerun the code. But the problem that before I have used API with 'best':1, and extracted predictions in a little bit more intelligent way, but also had differents.
 Thanks, I will do one m
 		</comment>
 		<comment id='3' author='EBazarov' date='2016-09-06T05:23:53Z'>
 		I will do test one more time ASAP
 		</comment>
 		<comment id='4' author='EBazarov' date='2016-09-06T07:02:41Z'>
 		Ok, this is a new results. I have extracted prediction using pred['classes'][0]['cat'], so now I think it should be rigth.
 Still have difference between accuracy from DeDe and computed using sklearn.
 &lt;denchmark-link:https://github.com/beniz/deepdetect/files/456096/DeDe_SVM_agnews_test_acc_bug_fix.zip&gt;DeDe_SVM_agnews_test_acc_bug_fix.zip&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='5' author='EBazarov' date='2016-09-06T08:00:57Z'>
 		AGnews has 4 balanced classes, you are only declaring two to dd, with nclasses=2. You can easily observe this since the predictions are then only 0 or 1. The probability that a 4 balanced classes classifier with 90% accuracy outputs only 0 and 1 is null. From there you can immediately infer that you have a class specification problem...
 		</comment>
 		<comment id='6' author='EBazarov' date='2016-09-06T08:03:48Z'>
 		At top of a notebook file I have declared that I will use only "Sport" and "Business" data for simplicity.
 So that's why I am using nclasses = 2
 		</comment>
 		<comment id='7' author='EBazarov' date='2016-09-06T08:06:49Z'>
 		It is even worse then, it is impossible to have a binary classifier do 38% accuracy, since reversing the classes would yield 62% accuracy... Just saw it is 53%, which is again suspicious because random.
 		</comment>
 		<comment id='8' author='EBazarov' date='2016-09-06T09:28:02Z'>
 		OK, now I can reproduce through another mean, thanks.
 		</comment>
 		<comment id='9' author='EBazarov' date='2016-09-06T09:58:02Z'>
 		I am able to reproduce this behavior with the following code
 accuracy calculated by DD: ~90%
 accuracy calculated from DD probabilities: ~64%
 EDIT: Thanks to &lt;denchmark-link:https://github.com/beniz&gt;@beniz&lt;/denchmark-link&gt;
 , adding 'measure': ['f1'] during prediction phase is enough to reproduce the bug. I modified the code accordingly
 
 Prediction on test set:
 {'measure': {'accp': 0.6539, 'precision': 0.653899999999346, 'recall': 0.7724241736769586, 'f1': 0.7082375472400635}, 'time': 4605.0}
 Metrics during training phase:
 {'precision': 0.8962499999991037, 'mcll': 0.2626906841772865, 'iteration': 900.0, 'accp': 0.89625, 'auc': 0.95724583, 'recall': 0.8964958313795168, 'train_loss': 0.2949938476085663, 'acc': 0.89625, 'f1': 0.8963728988344203}
 
 import time
 import numpy as np
 from dd_client import DD
 
 sname = "AccuracyComparison"
 description = "Reproduce accuracy measure"
 
 # change to your model path
 repository = "/path/to/model"
 
 # path to data (binary classification data)
 input_data = ["X_train.svm", "X_test.svm"]
 
 
 # Service creation
 dd = DD('localhost')
 dd.set_return_format(dd.RETURN_PYTHON)
 model = {'templates': '../templates/caffe', 'repository': repository}
 mllib = 'caffe'
 
 service_parameters_input = {'connector': 'svm'}
 service_parameters_mllib = {"db": True, "template": "mlp", "nclasses": 2, "layers": [128,128], "activation": 'relu'}
 service_parameters_output = {}
 
 json_dump = dd.put_service(sname, model, description, mllib, service_parameters_input,
                                 service_parameters_mllib, service_parameters_output)
 
 
 time.sleep(1)
 
 
 # Training model
 train_parameters_input = {'db': True},
 train_parameters_output = {"measure": ['mcll','auc','accp']},
 train_parameters_mllib = {'gpu': True,
                                'solver': {'iterations': 1000, 'test_interval': 100, 'base_lr': 0.01,
                                           'solver_type': 'adam'},
                                'net': {'batch_size': 256}}
 
 dd.post_train(sname, input_data,
               train_parameters_input,
               train_parameters_mllib,
               train_parameters_output, async=True)
 
 train_status = ''
 while True:
     train_status = dd.get_train(sname, job=1, timeout=2)
     if train_status['head']['status'] == 'running':
         print(train_status['body']['measure'])
     else:
         print(train_status)
         break
 
 # Predicting test set
 predict_parameters_input = {},
 predict_parameters_output = {"best": 2, 'measure': ['f1']},
 predict_parameters_mllib = {'gpu': True, 'gpuid': 0}
 
 json_dump = dd.post_predict(sname, [input_data[1]], predict_parameters_input, predict_parameters_mllib,
                             predict_parameters_output)
 print(json_dump['body'])
 		</comment>
 		<comment id='10' author='EBazarov' date='2016-09-06T10:02:49Z'>
 		Thanks, you can actually reproduce by simply adding measure:['f1'] to the predict call in the output object.
 		</comment>
 		<comment id='11' author='EBazarov' date='2016-09-06T10:47:59Z'>
 		Fixed. Some would appreciate the hellish effect of a misplaced -1... So using /predict along with measure allows you to get the accuracy on test set whenever that is needed outside of the training loop.
 		</comment>
 	</comments>
 </bug>
<commit id='73c54d4a6275218634639bd61559ae8a5e399265' author='Emmanuel Benazera' date='2016-09-06 12:47:17+02:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='src\svminputfileconn.cc' new_name='src\svminputfileconn.cc'>
 		<file_info nloc='165' complexity='33' token_count='1090'></file_info>
 		<method name='dd::SVMInputFileConn::deserialize_vocab' parameters='required'>
 				<method_info nloc='24' complexity='6' token_count='162' nesting_level='1' start_line='196' end_line='219'></method_info>
 			<added_lines>212</added_lines>
 			<deleted_lines>212</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
