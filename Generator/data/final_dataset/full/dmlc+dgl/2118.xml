<bug_data>
<bug id='2118' author='Junyoungpark' open_date='2020-08-27T12:20:44Z' closed_time='2020-08-30T01:27:29Z'>
 	<summary>Maybe a bug on reading-out node and edge features?</summary>
 	<description>
 Hi,
 I found a sketchy issue while I do the node readout and edge readout and do the backpropgation.
 &lt;denchmark-code&gt;import dgl
 import torch
 
 if __name__ == '__main__':
     src, dst = (0, 1, 2), (2, 1, 0)
     g = dgl.graph((src, dst))
     g.ndata['feat'] = torch.ones(3, 2)
     g.edata['feat'] = torch.ones(3, 3)
 
     nh_enc = torch.nn.Linear(2, 2)
     g.ndata['h'] = nh_enc(g.ndata['feat'])
 
     eh_enc = torch.nn.Linear(3, 3)
     g.edata['h'] = eh_enc(g.edata['feat'])
 
     n_readout = dgl.readout_nodes(g, 'h', op='sum')
     e_readout = dgl.readout_edges(g, 'h', op='sum')
 
     inpt = torch.cat([n_readout, e_readout], dim=-1)
 
     m = torch.nn.Linear(5, 1)
 
     out = m(inpt)
     out.backward()
 
 &lt;/denchmark-code&gt;
 
 on ubuntu 18.04 + dgl-cu101 0.5.0 + torch 1.5.1
 The above code give me an error as follows:
 Using backend: pytorch
 Traceback (most recent call last):
 File "/home/silab9/dev/ScheduleNet/check_dgl_readouts.py", line 24, in 
 out.backward()
 File "/home/silab9/.pyenv/versions/gpu_torch/lib/python3.8/site-packages/torch/tensor.py", line 198, in backward
 torch.autograd.backward(self, gradient, retain_graph, create_graph)
 File "/home/silab9/.pyenv/versions/gpu_torch/lib/python3.8/site-packages/torch/autograd/init.py", line 98, in backward
 Variable._execution_engine.run_backward(
 RuntimeError: [21:16:12] /opt/dgl/src/array/kernel.cc:39: Check failed: arrays[i].IsContiguous(): Expect U_data to be a contiguous tensor
 Stack trace:
 [bt] (0) /home/silab9/.pyenv/versions/gpu_torch/lib/python3.8/site-packages/dgl/libdgl.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x22) [0x7fd30ca89e02]
 [bt] (1) /home/silab9/.pyenv/versions/gpu_torch/lib/python3.8/site-packages/dgl/libdgl.so(+0x8f91fa) [0x7fd30cbbe1fa]
 [bt] (2) /home/silab9/.pyenv/versions/gpu_torch/lib/python3.8/site-packages/dgl/libdgl.so(+0x8fec24) [0x7fd30cbc3c24]
 [bt] (3) /home/silab9/.pyenv/versions/gpu_torch/lib/python3.8/site-packages/dgl/libdgl.so(DGLFuncCall+0x52) [0x7fd30d190232]
 [bt] (4) /home/silab9/.pyenv/versions/gpu_torch/lib/python3.8/site-packages/dgl/_ffi/_cy3/core.cpython-38-x86_64-linux-gnu.so(+0x19f87) [0x7fd3d24fbf87]
 [bt] (5) /home/silab9/.pyenv/versions/gpu_torch/lib/python3.8/site-packages/dgl/_ffi/_cy3/core.cpython-38-x86_64-linux-gnu.so(+0x1a5fe) [0x7fd3d24fc5fe]
 [bt] (6) /home/silab9/.pyenv/versions/gpu_torch/bin/python(_PyObject_MakeTpCall+0x91) [0x5629f25dbbd1]
 [bt] (7) /home/silab9/.pyenv/versions/gpu_torch/bin/python(_PyEval_EvalFrameDefault+0x6264) [0x5629f25c9f24]
 [bt] (8) /home/silab9/.pyenv/versions/gpu_torch/bin/python(+0x5dac7) [0x5629f25c2ac7]
 But when I do only backprop on either one of node or edge readouts, It works perfectly fine.
 &lt;denchmark-code&gt;    src, dst = (0, 1, 2), (2, 1, 0)
     g = dgl.graph((src, dst))
     g.ndata['feat'] = torch.ones(3, 2)
     g.edata['feat'] = torch.ones(3, 3)
 
     nh_enc = torch.nn.Linear(2, 2)
     g.ndata['h'] = nh_enc(g.ndata['feat'])
 
     eh_enc = torch.nn.Linear(3, 3)
     g.edata['h'] = eh_enc(g.edata['feat'])
 
     n_readout = dgl.readout_nodes(g, 'h', op='sum')
 
     inpt = n_readout
 
     m = torch.nn.Linear(2, 1)
 
     out = m(inpt)
     out.backward()
 &lt;/denchmark-code&gt;
 
 Is this behavior is a bug or my mistake?
 Thanks,
 Jun
 	</description>
 	<comments>
 		<comment id='1' author='Junyoungpark' date='2020-08-27T13:33:05Z'>
 		Thanks for reporting this, I'll take a look.
 		</comment>
 		<comment id='2' author='Junyoungpark' date='2020-08-29T15:36:53Z'>
 		The problem seems to be related to backward function of GSpMM.
 When I print the shape and stride of dZ at the beginning of GSpMM.backward() I found the shape is (1, 3) but the stride is (5, 1), which doesn't make sense.
 May need more time to figure out the reason.
 		</comment>
 		<comment id='3' author='Junyoungpark' date='2020-08-29T16:15:49Z'>
 		OK I think I know what is going on here.  Probably it's how PyTorch and NumPy etc handles strides of a view of an array with size-1 axes.
 Normally, the strides on an axis of a contiguous array should be a cumulative product of the sizes of the succeeding axes:
 torch.randn((2, 3, 4)).stride()    # (12, 4, 1)
 DGL determines whether an array is contiguous by this rule.
 However, this is not true for views of tensors which have size 1 on the first few axes:
 torch.randn((2, 5))[:, :3].stride()    # (5, 1)
 torch.randn((2, 5))[:, :3].contiguous().stride()    # (3, 1)
 torch.randn((1, 5))[:, :3].stride()    # (5, 1)
 torch.randn((1, 5))[:, :3].contiguous().stride()    # (5, 1) ???
 In fact, we can also observe the same phenomenon in NumPy arrays:
 np.ascontiguousarray(np.random.randn(1, 5)[:, :3]).strides    # (40, 8)
 As per the bug, the reason is that the input of GSpMM.backward() in this case is actually a view of a 1xN tensor, which falls into the corner case above.
 We should change the implementation of IsContiguous() accordingly.
 		</comment>
 		<comment id='4' author='Junyoungpark' date='2020-08-30T01:27:29Z'>
 		This should be fixed in &lt;denchmark-link:https://github.com/dmlc/dgl/pull/2127&gt;#2127&lt;/denchmark-link&gt;
  .
 		</comment>
 	</comments>
 </bug>
<commit id='7816c5a2493e205809e32c333b62ce7e7be634d8' author='Zihao Ye' date='2020-08-30 09:26:03+08:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='src\runtime\ndarray.cc' new_name='src\runtime\ndarray.cc'>
 		<file_info nloc='416' complexity='59' token_count='3178'></file_info>
 		<method name='dgl::runtime::NDArray::IsContiguous' parameters=''>
 				<method_info nloc='15' complexity='5' token_count='102' nesting_level='2' start_line='136' end_line='152'></method_info>
 			<added_lines>140,141,142,143,144,145,146,147,148,149,151</added_lines>
 			<deleted_lines>140,141,142,143,145</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
