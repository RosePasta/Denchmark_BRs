<bug_data>
<bug id='192' author='aymericdamien' open_date='2016-07-10T12:23:26Z' closed_time='2016-07-11T08:39:12Z'>
 	<summary>Dropout in RNNs</summary>
 	<description>
 It seems there is a bug where dropout in RNNs are still applied at prediction time, investigating if it is coming from tflearn or tensorflow.
 	</description>
 	<comments>
 		<comment id='1' author='aymericdamien' date='2016-07-10T19:22:45Z'>
 		I did a quick test and ran the sentiment analysis example &lt;denchmark-link:https://github.com/tflearn/tflearn/blob/master/examples/nlp/lstm.py&gt;1&lt;/denchmark-link&gt;
  and Kera's equivalent [2](using Tensorflow as a backend instead of Theano). Even with dropout, the Keras code doesn't have the same problem, which would suggest the bug is in tflearn.
 		</comment>
 		<comment id='2' author='aymericdamien' date='2016-07-11T08:40:33Z'>
 		Actually the error was that the TF wrapper always apply dropout, so it has to be modified with TFLearn 'is_training' op condition, to only apply dropout at training time.
 		</comment>
 	</comments>
 </bug>
<commit id='b3117f14930ec2b71f1f06f74183e2b6f940dddf' author='aymericdamien' date='2016-07-11 16:36:37+08:00'>
 	<dmm_unit complexity='1.0' interfacing='0.16666666666666666' size='0.25'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tflearn\layers\recurrent.py' new_name='tflearn\layers\recurrent.py'>
 		<file_info nloc='403' complexity='34' token_count='3275'></file_info>
 		<method name='state_size' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='11' nesting_level='1' start_line='636' end_line='637'></method_info>
 			<added_lines>636,637</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,cell,input_keep_prob,output_keep_prob,seed'>
 				<method_info nloc='2' complexity='1' token_count='23' nesting_level='1' start_line='600' end_line='601'></method_info>
 			<added_lines>600,601</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='output_size' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='11' nesting_level='1' start_line='640' end_line='641'></method_info>
 			<added_lines>640,641</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__call__' parameters='self,inputs,state,scope'>
 				<method_info nloc='18' complexity='5' token_count='141' nesting_level='1' start_line='643' end_line='663'></method_info>
 			<added_lines>643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>20,56,597,598,599,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,638,639,642,664,665</added_lines>
 			<deleted_lines>55,56</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
