<bug_data>
<bug id='70' author='JulianGerhard21' open_date='2019-08-28T06:14:45Z' closed_time='2019-08-30T15:18:37Z'>
 	<summary>'NERProcessor' has no attribute 'tokenizer'</summary>
 	<description>
 Python: 3.6.8
 OS: Windows 10
 FARM: 0.2.0
 Hi guys,
 if I want to run FARM to do some NER finetuning, I tried both approaches:
 experiments = load_experiments("conll_de_config.json")
 and the stick-your-blocks-together one. Both of them ended with the following error:
 &lt;denchmark-code&gt;08/28/2019 07:47:35 - INFO - farm.utils -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
 08/28/2019 07:47:35 - INFO - pytorch_transformers.tokenization_utils -   loading file https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased-vocab.txt from cache at C:\Users\JulianGerhard\.cache\torch\pytorch_transformers\da299cdd121a3d71e1626f2908dda0d02658f42e925a3d6abd8273ec08cf41a6.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d
 08/28/2019 07:47:35 - INFO - farm.data_handler.data_silo -   
 Loading data into the data silo ... 
               ______
                |o  |   !
    __          |:`_|---'-.
   |__|______.-/ _ \-----.|       
  (o)(o)------'\ _ /     ( )      
  
 08/28/2019 07:47:35 - INFO - farm.data_handler.data_silo -   Loading train set from: data/conll03-de\train.txt 
 08/28/2019 07:47:35 - INFO - farm.data_handler.utils -    Couldn't find data/conll03-de\train.txt locally. Trying to download ...
 08/28/2019 07:47:35 - INFO - farm.data_handler.utils -   downloading and extracting file C:\Users\JulianGerhard\PycharmProjects\word_embeddings\ner_finetuning\data\conll03-de to dir 
 08/28/2019 07:47:39 - INFO - farm.data_handler.processor -   Got ya 8 parallel workers to fill the baskets with samples (chunksize = 1000)...
   0%|          | 0/24000 [00:00&lt;?, ?it/s]multiprocessing.pool.RemoteTraceback: 
 """
 Traceback (most recent call last):
   File "C:\Users\JulianGerhard\AppData\Local\Programs\Python\Python36\lib\multiprocessing\pool.py", line 119, in worker
     result = (True, func(*args, **kwds))
   File "C:\Users\JulianGerhard\AppData\Local\Programs\Python\Python36\lib\multiprocessing\pool.py", line 44, in mapstar
     return list(map(*args))
   File "c:\users\juliangerhard\pycharmprojects\farm\farm\data_handler\processor.py", line 310, in _multiproc_sample
     samples = cls._dict_to_samples(dict=basket.raw, all_dicts=all_dicts)
   File "c:\users\juliangerhard\pycharmprojects\farm\farm\data_handler\processor.py", line 555, in _dict_to_samples
     tokenized = tokenize_with_metadata(dict["text"], cls.tokenizer, cls.max_seq_len)
 AttributeError: type object 'NERProcessor' has no attribute 'tokenizer'
 """
 
 The above exception was the direct cause of the following exception:
 
 Traceback (most recent call last):
   File "C:/Users/JulianGerhard/PycharmProjects/word_embeddings/ner_finetuning/farm_experimen t.py", line 6, in &lt;module&gt;
     run_experiment(experiments[0])
   File "c:\users\juliangerhard\pycharmprojects\farm\farm\experiment.py", line 87, in run_experiment
     distributed=distributed,
   File "c:\users\juliangerhard\pycharmprojects\farm\farm\data_handler\data_silo.py", line 39, in __init__
     self._load_data()
   File "c:\users\juliangerhard\pycharmprojects\farm\farm\data_handler\data_silo.py", line 47, in _load_data
     self.data["train"], self.tensor_names = self.processor.dataset_from_file(train_file)
   File "c:\users\juliangerhard\pycharmprojects\farm\farm\data_handler\processor.py", line 366, in dataset_from_file
     self._init_samples_in_baskets()
   File "c:\users\juliangerhard\pycharmprojects\farm\farm\data_handler\processor.py", line 304, in _init_samples_in_baskets
     zip(samples, self.baskets), total=len(self.baskets)
   File "C:\Users\JulianGerhard\AppData\Local\Programs\Python\Python36\lib\site-packages\tqdm\_tqdm.py", line 1034, in __iter__
     for obj in iterable:
   File "C:\Users\JulianGerhard\AppData\Local\Programs\Python\Python36\lib\multiprocessing\pool.py", line 320, in &lt;genexpr&gt;
     return (item for chunk in result for item in chunk)
   File "C:\Users\JulianGerhard\AppData\Local\Programs\Python\Python36\lib\multiprocessing\pool.py", line 735, in next
     raise value
 AttributeError: type object 'NERProcessor' has no attribute 'tokenizer'
   0%|          | 0/24000 [00:06&lt;?, ?it/s]
 
 &lt;/denchmark-code&gt;
 
 I installed twice - from repo and with pip.
 After investigating this shortly (I need to leave), I think it has something to do with your NERProcessor's classmethod implementation. I can confirm that the tokenizer is set properly:
  &lt;farm.modeling.tokenization.BertTokenizer object at 0x00000238DE551080&gt;
 and the cls instance, _dict_to_samples is receiving is of type:
 &lt;class 'farm.data_handler.processor.NERProcessor'&gt;
 Any ideas?
 Regards
 	</description>
 	<comments>
 		<comment id='1' author='JulianGerhard21' date='2019-08-28T06:58:19Z'>
 		I just did a quick check and couldn't reproduce this issue on my linux machine. Both modes seem to work for NER.
 I suspect your error is due to how Windows handles multiprocessing (see &lt;denchmark-link:https://stackoverflow.com/questions/42148344/python-multiprocessing-linux-windows-difference&gt;https://stackoverflow.com/questions/42148344/python-multiprocessing-linux-windows-difference&lt;/denchmark-link&gt;
 ). Since the tokenizer is a class attribute of Processor and not an instance attribute, this might cause problems in inidividual processes.
 We will investigate this further ...
 		</comment>
 		<comment id='2' author='JulianGerhard21' date='2019-08-30T15:18:37Z'>
 		Hi &lt;denchmark-link:https://github.com/JulianGerhard21&gt;@JulianGerhard21&lt;/denchmark-link&gt;
 . This issue has been resolved by &lt;denchmark-link:https://github.com/deepset-ai/FARM/pull/76&gt;#76&lt;/denchmark-link&gt;
 .
 		</comment>
 	</comments>
 </bug>
<commit id='ab0226d62a3f4a95c679d04b7cbeb6f904f648f8' author='Tanay Soni' date='2019-08-30 17:17:28+02:00'>
 	<dmm_unit complexity='0.0' interfacing='0.9629629629629629' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='farm\data_handler\processor.py' new_name='farm\data_handler\processor.py'>
 		<file_info nloc='753' complexity='91' token_count='4276'></file_info>
 		<method name='_init_samples_in_baskets' parameters='self'>
 				<method_info nloc='29' complexity='7' token_count='205' nesting_level='1' start_line='289' end_line='321'></method_info>
 			<added_lines>290,291,292,293,295,296,297,298,299,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321</added_lines>
 			<deleted_lines>289,290,292,293,299,300,301,302,303,304,306,307,308,309,319,320,321</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,tokenizer,max_seq_len,label_list,metrics,train_filename,dev_filename,test_filename,dev_split,data_dir,label_dtype,multiprocessing_chunk_size,max_processes,share_all_baskets_for_multiprocessing,use_multiprocessing'>
 				<method_info nloc='16' complexity='1' token_count='43' nesting_level='1' start_line='59' end_line='74'></method_info>
 			<added_lines>74</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_featurize_samples' parameters='self'>
 				<method_info nloc='29' complexity='7' token_count='197' nesting_level='1' start_line='331' end_line='362'></method_info>
 			<added_lines>332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>13,120,121,122,123,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018</added_lines>
 			<deleted_lines>42,285,286,288,322,323,324,325,326,327,328,329,959,960,961,962,963,964,980,981,982,983,984,985</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
