<bug_data>
<bug id='553' author='Ganeshpadmanaban' open_date='2020-09-23T08:09:58Z' closed_time='2020-10-04T11:05:00Z'>
 	<summary>Converting FARM classification model to hugging face model</summary>
 	<description>
 Describe the bug
 I'm not able to save a trained farm-classification model to hugging face format since I want to move to the other framework for more specific operations.
 Error message
 &lt;denchmark-code&gt;---------------------------------------------------------------------------
 RuntimeError                              Traceback (most recent call last)
 &lt;ipython-input-83-b2a730b6ac24&gt; in &lt;module&gt;
 ----&gt; 1 convert_to_transformers()
 
 &lt;ipython-input-82-8ab35f02f804&gt; in convert_to_transformers()
      12 
      13     # convert to transformers
 ---&gt; 14     transformer_model = model.convert_to_transformers()
      15 
      16     # save it (note: transformers use str instead of Path objects)
 
     555             setattr(transformers_model, transformers_model.base_model_prefix, self.language_model.model)
     556             transformers_model.classifier.load_state_dict(
 --&gt; 557                 self.prediction_heads[0].feed_forward.feed_forward[0].state_dict())
     558         elif self.prediction_heads[0].model_type == "token_classification":
     559             # add more info to config
 
    1043         if len(error_msgs) &gt; 0:
    1044             raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
 -&gt; 1045                                self.__class__.__name__, "\n\t".join(error_msgs)))
    1046         return _IncompatibleKeys(missing_keys, unexpected_keys)
    1047 
 
 RuntimeError: Error(s) in loading state_dict for Linear:
 	size mismatch for weight: copying a param with shape torch.Size([2, 768]) from checkpoint, the shape in current model is torch.Size([2, 4096]).
 &lt;/denchmark-code&gt;
 
 To Reproduce
 
 Load language model "distilbert-base-german-cased"
 Add prediction head (TextClassificationHead)
 Save model
 Convert to huggingface using https://github.com/deepset-ai/FARM/blob/master/examples/conversion_huggingface_models.py
 
 System:
 
 OS: Ubuntu 18.04
 GPU/CPU: i7
 FARM version: 0.4.8
 
 	</description>
 	<comments>
 		<comment id='1' author='Ganeshpadmanaban' date='2020-09-29T09:41:29Z'>
 		Since you are doing classification did you also try the conversion_huggingface_models_classification.py script directly beneath the example you linked?
 		</comment>
 		<comment id='2' author='Ganeshpadmanaban' date='2020-10-01T09:00:01Z'>
 		
 Since you are doing classification did you also try the conversion_huggingface_models_classification.py script directly beneath the example you linked?
 
 Oh yeah, i tried conversion_huggingface_models_classification.py and the output is from that actually. Possibly architecture for distrilbert is not compatible either. I'll investigate further this weekend and update.
 		</comment>
 		<comment id='3' author='Ganeshpadmanaban' date='2020-10-01T09:26:09Z'>
 		Ok, strange. Can you give more details to what model you are converting? Can you post the actual code you used for converting?
 Judging from the error message there is a language model size mismatch: 768 is the embedding dimension of base models, 4096 is the embedding dimension of some larger models.
 		</comment>
 		<comment id='4' author='Ganeshpadmanaban' date='2020-10-01T09:53:34Z'>
 		&lt;denchmark-code&gt;import os
 import logging
 from pathlib import Path
 import pandas as pd
 
 from farm.data_handler.data_silo import DataSilo
 from farm.data_handler.processor import Processor
 from farm.data_handler.processor import TextClassificationProcessor
 from farm.modeling.optimization import initialize_optimizer
 from farm.infer import Inferencer
 from farm.modeling.adaptive_model import AdaptiveModel
 from farm.modeling.language_model import LanguageModel
 from farm.modeling.prediction_head import TextClassificationHead
 from farm.modeling.tokenization import Tokenizer
 from farm.train import Trainer
 from farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings
 
 from transformers.pipelines import pipeline
 
 def create_data():
 
     # create data
     if not os.path.exists('dataset'):
         os.makedirs('dataset')
 
     dataset = pd.DataFrame(columns=["text", "label"])
     dataset["text"] = ["happy", "bad", "awesome!", "okay", "nothing", "whatever", "great", "good", "fine", "goody", "hallo!", "mhh"]
     dataset["label"] = ["POS", "NEG", "POS", "NEG", "NEG", "NEG", "POS", "POS", "POS", "POS", "POS", "NEG"]
 
     dataset.to_csv("dataset/train.tsv", index=False, sep="\t")
 
     # create data
     dataset = pd.DataFrame(columns=["text", "label"])
     dataset["text"] = ["whatever", "great", "good", "fine"]
     dataset["label"] = ["NEG", "POS", "POS", "POS"]
 
 
     dataset.to_csv("dataset/test.tsv", index=False, sep="\t")
 
 def create_model():
 
     ##########################
     ########## Settings
     ##########################
     set_all_seeds(seed=42)
     n_epochs = 1
     batch_size = 4
     evaluate_every = 100
     lang_model = "distilbert-base-german-cased"
     do_lower_case = False
     use_amp = None
 
     device, n_gpu = initialize_device_settings(use_cuda=True, use_amp=use_amp)
 
     # 1.Create a tokenizer
     tokenizer = Tokenizer.load(pretrained_model_name_or_path=lang_model, do_lower_case=do_lower_case)
 
     # 2. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset
     # Here we load GermEval 2018 Data automaticaly if it is not available.
     # GermEval 2018 only has train.tsv and test.tsv dataset - no dev.tsv
 
     label_list = ["POS", "NEG"]
     metric = "f1_macro"
 
     processor = TextClassificationProcessor(tokenizer=tokenizer,
                                             max_seq_len=32,
                                             data_dir=Path("dataset/"),
                                             label_list=label_list,
                                             metric=metric,
                                             label_column_name="label"
                                             )
 
     # 3. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and calculates a
     #    few descriptive statistics of our datasets
     data_silo = DataSilo(
         processor=processor,
         batch_size=batch_size)
 
     # 4. Create an AdaptiveModel
     # a) which consists of a pretrained language model as a basis
     language_model = LanguageModel.load(lang_model)
     # b) and a prediction head on top that is suited for our task =&gt; Text classification
     prediction_head = TextClassificationHead(
         class_weights=data_silo.calculate_class_weights(task_name="text_classification"),
         num_labels=len(label_list))
 
     model = AdaptiveModel(
         language_model=language_model,
         prediction_heads=[prediction_head],
         embeds_dropout_prob=0.1,
         lm_output_types=["per_sequence"],
         device=device)
 
     # 5. Create an optimizer
     model, optimizer, lr_schedule = initialize_optimizer(
         model=model,
         learning_rate=3e-5,
         device=device,
         n_batches=len(data_silo.loaders["train"]),
         n_epochs=n_epochs,
         use_amp=use_amp)
 
     # 6. Feed everything to the Trainer, which keeps care of growing our model into powerful plant and evaluates it from time to time
     trainer = Trainer(
         model=model,
         optimizer=optimizer,
         data_silo=data_silo,
         epochs=n_epochs,
         n_gpu=n_gpu,
         lr_schedule=lr_schedule,
         evaluate_every=evaluate_every,
         device=device)
 
     # 7. Let it grow
     trainer.train()
 
     # 8. Hooray! You have a model. Store it:
     save_dir = Path("saved_models/test-model")
     model.save(save_dir)
     processor.save(save_dir)
 
 # ##############################################
 # ###  From FARM -&gt; Transformers
 # ##############################################
 def convert_to_transformers():
     farm_input_dir = Path("saved_models/test-model")
     transformers_output_dir = "saved_models/test-model-out"
     #
     # # # load from FARM format
     model = AdaptiveModel.load(farm_input_dir, device="cpu")
     processor = Processor.load_from_dir(farm_input_dir)
     model.connect_heads_with_processor(processor.tasks)
 
     # convert to transformers
     transformer_model = model.convert_to_transformers()
 
     # save it (note: transformers use str instead of Path objects)
     Path(transformers_output_dir).mkdir(parents=True, exist_ok=True)
     transformer_model.save_pretrained(transformers_output_dir)
     processor.tokenizer.save_pretrained(transformers_output_dir)
 
 
 if __name__ == "__main__":
 	create_data()
 	create_model()
 	convert_to_transformers()
 &lt;/denchmark-code&gt;
 
 Here you go :)
 		</comment>
 		<comment id='5' author='Ganeshpadmanaban' date='2020-10-04T11:07:02Z'>
 		Thanks &lt;denchmark-link:https://github.com/Ganeshpadmanaban&gt;@Ganeshpadmanaban&lt;/denchmark-link&gt;
  for the detailed code. I could reproduce your issue.
 I found and fixed the bug on FARM side. When you look at the PR you will see that the wrong config type was loaded.
 I tested your code again and it works now. Could you verify it on your side as well, please?
 		</comment>
 	</comments>
 </bug>
<commit id='c106bca84f36a44ef890e43a1e3da6171e4a8bd8' author='Timo Moeller' date='2020-10-04 13:04:59+02:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='farm\modeling\language_model.py' new_name='farm\modeling\language_model.py'>
 		<file_info nloc='808' complexity='128' token_count='5024'></file_info>
 		<method name='load' parameters='cls,pretrained_model_name_or_path,language,kwargs'>
 				<method_info nloc='22' complexity='3' token_count='176' nesting_level='1' start_line='745' end_line='786'></method_info>
 			<added_lines>767</added_lines>
 			<deleted_lines>767</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
