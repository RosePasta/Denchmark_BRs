<bug_data>
<bug id='533' author='himanshurawlani' open_date='2020-09-10T17:26:55Z' closed_time='2020-09-24T18:33:54Z'>
 	<summary>Error in conversion of BertForMaskedLM with BertLMHead to HuggingFace transformers</summary>
 	<description>
 
 I started with the &lt;denchmark-link:https://github.com/deepset-ai/FARM/blob/master/examples/lm_finetuning.py&gt;lm_finetuning.py&lt;/denchmark-link&gt;
  script to pre-train BERT on a domain-specific text corpus. After completing training, the saved artifacts included the following:
 
 language_model.bin
 language_model_config.json (BertForMaskedLM)
 prediction_head_0.bin
 prediction_head_0_config.json (BertLMHead)
 prediction_head_1.bin
 prediction_head_1_config.json (NextSentenceHead)
 processor_config.json
 special_tokens_map.json
 tokenizer_config.json
 vocab.txt
 
 I know FARM currently doesn't support converting an AdaptiveModel with two prediction heads to a HuggingFace transformers model. However, after going through the &lt;denchmark-link:https://github.com/deepset-ai/FARM/blob/d7b5cb61f062436789aa9081a85ade4f56e2db6f/farm/modeling/adaptive_model.py#L305&gt;AdaptiveModel.load()&lt;/denchmark-link&gt;
  classmethod used in the model conversion &lt;denchmark-link:https://github.com/deepset-ai/FARM/blob/master/examples/conversion_huggingface_models.py&gt;script&lt;/denchmark-link&gt;
 , I figured out that removing the  (NextSentenceHead) and  (PyTorch model) files should result in successful conversion to HuggingFace model as the conversion script would consider it as an AdaptiveModel with single PredictionHead.
 Error message
 After removing the files and running the conversion script, I get an error saying:
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "conversion_huggingface_models.py", line 88, in &lt;module&gt;
     convert_to_transformers("./farm_saved_models/bert-english-lm", 
   File "conversion_huggingface_models.py", line 46, in convert_to_transformers
     transformer_model = model.convert_to_transformers()
   File "/home/himanshu/.conda/envs/tf2/lib/python3.8/site-packages/farm/modeling/adaptive_model.py", line 509, in convert_to_transformers
     elif len(self.prediction_heads[0].layer_dims) != 2:
   File "/home/himanshu/.conda/envs/tf2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 771, in __getattr__
     raise ModuleAttributeError("'{}' object has no attribute '{}'".format(
 torch.nn.modules.module.ModuleAttributeError: 'BertLMHead' object has no attribute 'layer_dims'
 &lt;/denchmark-code&gt;
 
 I made some changes to &lt;denchmark-link:https://github.com/deepset-ai/FARM/blob/d7b5cb61f062436789aa9081a85ade4f56e2db6f/farm/modeling/prediction_head.py#L714&gt;BertLMHead&lt;/denchmark-link&gt;
  class by referring to HuggingFace's &lt;denchmark-link:https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L540&gt;BertLMPredictionHead&lt;/denchmark-link&gt;
  and added the following lines to () method:
 &lt;denchmark-code&gt;self.layer_dims = [hidden_size, vocab_size]
 self.decoder.bias = self.bias
 &lt;/denchmark-code&gt;
 
 I also updated the () function by changing the &lt;denchmark-link:https://github.com/deepset-ai/FARM/blob/d7b5cb61f062436789aa9081a85ade4f56e2db6f/farm/modeling/prediction_head.py#L807&gt;line&lt;/denchmark-link&gt;
 :
 &lt;denchmark-code&gt;lm_logits = self.decoder(hidden_states) + self.bias
 &lt;/denchmark-code&gt;
 
 to
 &lt;denchmark-code&gt;lm_logits = self.decoder(hidden_states)
 &lt;/denchmark-code&gt;
 
 Expected behavior
 After making the above changes and re-training the model, the conversion script ran successfully and I was able to import the model into HuggingFace pipelines with the following code:
 &lt;denchmark-code&gt;from transformers import pipeline
 
 fill_mask = pipeline(
     "fill-mask",
     model="./bert-english-lm",
     tokenizer="./bert-english-lm"
 )
 &lt;/denchmark-code&gt;
 
 Correct me if I'm wrong, I expected the conversion script to run successfully after removing the NextSentenceHead files. I'm not sure if this a bug or a feature request as I'm new to FARM. However, I was able to solve the issue with a few changes as described above.
 System:
 
 OS: Ubuntu 18.04.4 LTS (Bionic Beaver)
 CPU: Intel(R) Xeon(R) Platinum 8168 CPU @ 2.70GHz
 FARM version: 0.4.7
 
 	</description>
 	<comments>
 		<comment id='1' author='himanshurawlani' date='2020-09-14T11:03:11Z'>
 		Hey &lt;denchmark-link:https://github.com/himanshurawlani&gt;@himanshurawlani&lt;/denchmark-link&gt;
  thanks for the detailed report about what you did.
 Your method of removing the PH files, adjusting the BertLMHead and conversion seem reasonable but I did not test them myself.
 Did you test the outcome and does the transformers model predict meaningful words when you fill the masks?
 		</comment>
 		<comment id='2' author='himanshurawlani' date='2020-09-22T16:31:55Z'>
 		Related to &lt;denchmark-link:https://github.com/deepset-ai/FARM/issues/517&gt;#517&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='3' author='himanshurawlani' date='2020-09-22T21:09:30Z'>
 		
 Hey @himanshurawlani thanks for the detailed report about what you did.
 Your method of removing the PH files, adjusting the BertLMHead and conversion seem reasonable but I did not test them myself.
 Did you test the outcome and does the transformers model predict meaningful words when you fill the masks?
 
 Yes, I've tested it and it works perfectly.
 		</comment>
 		<comment id='4' author='himanshurawlani' date='2020-09-23T09:20:54Z'>
 		Hey, I just realized something. Do we need the change in the predictionhead forward function (removal of bias addition) for the conversion?
 I think we just need the additional parameters layer dims and decoder.bias for the conversion to work, because the actual forward code is happening in transformers pipeline.
 Is that correct?
 If so, we could integrate this functionality easy within FARM. How about creating a PR with
 
 these additional params in FARMs BertLMHead (maybe make a comment that they are only used for conversion to transformers).
 adding a function in https://github.com/deepset-ai/FARM/blob/master/examples/conversion_huggingface_models.py that describes what you did to make the conversion work (delete NSP head weights and config)
 
 		</comment>
 		<comment id='5' author='himanshurawlani' date='2020-09-23T10:01:55Z'>
 		I think we need to change the prediction head forward function (by removing bias addition). In the code, we initialize the linear layer with bias=False and then assign the bias separately. If we do not remove the bias addition, in forward function, then it will be added twice during FARM's training.
 &lt;denchmark-code&gt;# The output weights are the same as the input embeddings, but there is
 # an output-only bias for each token.
 self.decoder = nn.Linear(hidden_size,
                                  vocab_size,
                                  bias=False)
 self.bias = nn.Parameter(torch.zeros(vocab_size))
 
 # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`
 self.decoder.bias = self.bias
 &lt;/denchmark-code&gt;
 
 This is exactly what is done in HuggingFace's &lt;denchmark-link:https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L540&gt;BertLMPredictionHead&lt;/denchmark-link&gt;
 .
 We should be easily able to integrate this within FARM. I'll work on creating a PR, following are the changes I see:
 
 Adding additional params in FARM's BertLMHead class as described in the issue.
 We can modify the conversion script to delete NSP head weights and config but what if someone needs it for testing or for continuing training? I was thinking, we can modify AdaptiveModel.convert_to_transformers() to accept len(self.prediction_heads) == 2 if one of the head is self.prediction_heads[x].model_type == "language_modelling" and use that to initialize ph_state_dict variable.
 
 Let me know if this is possible or will it have cascading effects on other modules.
 		</comment>
 		<comment id='6' author='himanshurawlani' date='2020-09-23T10:20:52Z'>
 		Ah ok, I understand now what you mean: if we set the self.decoder.bias parameter in FARM the bias is also added when the decoder (a linear pytorch layer) is called. Therefore we need to remove the explicit bias addition.
 &lt;denchmark-link:https://github.com/tholor&gt;@tholor&lt;/denchmark-link&gt;
  do you think changing how bias is used inside the BertLMHead will have an impact on other parts of FARM? I am slightly worried about the AWS sagemaker algo...
 Another solution would be to add this self.decoder.bias just before conversion to HF. Then we do not need to change the forward pass. Do you think that is an option &lt;denchmark-link:https://github.com/himanshurawlani&gt;@himanshurawlani&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='7' author='himanshurawlani' date='2020-09-23T11:44:06Z'>
 		&lt;denchmark-link:https://github.com/Timoeller&gt;@Timoeller&lt;/denchmark-link&gt;
  I tried your suggested alternative and that works too (I got meaningful predictions for [MASK]). We can assign the bias, in , as follows:
 &lt;denchmark-code&gt;self.prediction_heads[0].decoder.bias = self.prediction_heads[0].bias
 &lt;/denchmark-code&gt;
 
 We do not need to change the forward pass in this case.
 		</comment>
 		<comment id='8' author='himanshurawlani' date='2020-09-23T17:08:38Z'>
 		Nice, thanks for testing. Would you be interested in creating a PR with your method? We would be happy to include it in the library. I linked a similar issue, so it seems more people would be happy about this functionality as well.
 		</comment>
 		<comment id='9' author='himanshurawlani' date='2020-09-23T17:34:25Z'>
 		Sure &lt;denchmark-link:https://github.com/Timoeller&gt;@Timoeller&lt;/denchmark-link&gt;
 , I'll create a PR soon.
 		</comment>
 		<comment id='10' author='himanshurawlani' date='2020-09-24T18:33:54Z'>
 		Thanks &lt;denchmark-link:https://github.com/Timoeller&gt;@Timoeller&lt;/denchmark-link&gt;
  for approving the fix. I'll close this issue.
 		</comment>
 	</comments>
 </bug>
<commit id='23d317748b9a77b7ad434f943ebca5cde7c0233a' author='Himanshu Rawlani' date='2020-09-24 20:25:57+02:00'>
 	<dmm_unit complexity='0.3333333333333333' interfacing='0.6666666666666666' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='farm\modeling\adaptive_model.py' new_name='farm\modeling\adaptive_model.py'>
 		<file_info nloc='513' complexity='114' token_count='3675'></file_info>
 		<method name='convert_to_transformers' parameters='self'>
 				<method_info nloc='53' complexity='14' token_count='635' nesting_level='1' start_line='505' end_line='581'></method_info>
 			<added_lines>506,507,508,509,530,531,532</added_lines>
 			<deleted_lines>506,533,534</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='farm\modeling\prediction_head.py' new_name='farm\modeling\prediction_head.py'>
 		<file_info nloc='932' complexity='232' token_count='6945'></file_info>
 		<method name='__init__' parameters='self,hidden_size,vocab_size,hidden_act,task_name,kwargs'>
 				<method_info nloc='19' complexity='1' token_count='167' nesting_level='1' start_line='715' end_line='745'></method_info>
 			<added_lines>723,724</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
