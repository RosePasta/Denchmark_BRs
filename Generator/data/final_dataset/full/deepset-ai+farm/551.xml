<bug_data>
<bug id='551' author='ftesser' open_date='2020-09-21T13:32:33Z' closed_time='2020-10-08T05:26:01Z'>
 	<summary>Natural Questions Inference return just one prediction result if there are more than one questions</summary>
 	<description>
 Describe the bug
 Given a QA_input dictionary with more than one question e.g.:
  QA_input = [
         {
             "qas": ["Did GameTrailers rated Twilight Princess as one of the best games ever created?", "Did GameTrailers rated Twilight Princess as one of the worst games ever created?"],
             "context":  "Twilight Princess was released to universal critical acclaim and commercial success. It received perfect scores from major publications such as 1UP.com, Computer and Video Games, Electronic Gaming Monthly, Game Informer, GamesRadar, and GameSpy. On the review aggregators GameRankings and Metacritic, Twilight Princess has average scores of 95% and 95 for the Wii version and scores of 95% and 96 for the GameCube version. GameTrailers in their review called it one of the greatest games ever created."
         }
     ]
 the returned inference object:
 result = model.inference_from_dicts(dicts=QA_input, return_json=False)
 is a list that contains just one QApred object:
 [&lt;farm.modeling.predictions.QAPred object at 0x7f50dcb73e20&gt;]
 that contains just the results for the first question.
 Expected behavior
 Do return a list of QAPred objects of the same size of the number questions listed in qas (like the "normal" Squad model does).
 Additional context
 I understand that probably the reason of that is because the original NQ dataset contains just a question for each document, but I think that the API user it expect to receive answers for each question (or at least to be warned that Natural Questions Inference return just one prediction also of there are more questions).
 To Reproduce
 Modify  the QA_input in examples/natural_questions.py adding one or more questions (like above).
 Then run examples/natural_questions.py (to save time: the same error is obtained  running the script just from point 9 (
 
 
 FARM/examples/natural_questions.py
 
 
          Line 125
       in
       c6871b6
 
 
 
 
 
 
  # 9. Since training on the whole NQ corpus requires substantial compute resources we trained and uploaded a model on s3 
 
 
 
 
 )
 System:
 
 OS:
 GPU/CPU:
 FARM version:
 
 	</description>
 	<comments>
 		<comment id='1' author='ftesser' date='2020-09-21T14:02:22Z'>
 		Hey thanks for raising this issue, agreed this is unexpected behaviour but I would not necessarily call it a bug. We are currently refactoring the input dicts and will possibly handle this case differently.
 As you suggested: a warning message would be nice for now. Would you like to create a PR with it?
 		</comment>
 		<comment id='2' author='ftesser' date='2020-09-23T09:43:22Z'>
 		Hello &lt;denchmark-link:https://github.com/Timoeller&gt;@Timoeller&lt;/denchmark-link&gt;
 , yes I will make a PR for a warning message.
 But I really like to have this feature in FARM, I am going to explain why:
 In some scenarios, given a document, it is usual to make several questions for the same document, making just one call of:
 result = model.inference_from_dicts(dicts=QA_input, return_json=False)
 is more efficient because the tokenization (and feature computation?) of the document is made just one time for document.
 		</comment>
 		<comment id='3' author='ftesser' date='2020-09-23T09:47:27Z'>
 		This is a good idea we also were trying to implement,
 but at least the feature computation has to be handled on one question + one document level, since question and doc text is concatenated when put into the language model.
 The time it takes to tokenize a doc is really negligible, especially with fast tokenizers. So tokenizing twice to have a more clear structure and readable code is something we decided to go for. Do you agree with this?
 		</comment>
 		<comment id='4' author='ftesser' date='2020-09-23T10:09:37Z'>
 		OK, you are right about feature computation (my fault). I just experiments some differences in performances between squadprocessor and naturalquestionprocessor, and I was thinking about the re-tokenization.
 I will make a try with fast tokenizers.
 I agree that the structure is more clear with the constraint of [one doc + one question], and if the tokenization time is negligible I agree that is simple to manage that structure.
 		</comment>
 		<comment id='5' author='ftesser' date='2020-10-05T20:33:01Z'>
 		Please &lt;denchmark-link:https://github.com/Timoeller&gt;@Timoeller&lt;/denchmark-link&gt;
  see this last PR &lt;denchmark-link:https://github.com/deepset-ai/FARM/pull/565&gt;#565&lt;/denchmark-link&gt;
 .
 		</comment>
 		<comment id='6' author='ftesser' date='2020-10-07T17:15:05Z'>
 		Thanks for adding the warning. Is it ok with you if we close this issue now while still working on a solution for including multiple questions in NQ inferencing?
 		</comment>
 		<comment id='7' author='ftesser' date='2020-10-08T05:26:01Z'>
 		Yes, it is ok. Thanks!
 		</comment>
 	</comments>
 </bug>
<commit id='2ea428f81d82de8edf2c5f92fe885980ddd48384' author='ftesser' date='2020-10-07 19:13:08+02:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='farm\infer.py' new_name='farm\infer.py'>
 		<file_info nloc='353' complexity='46' token_count='2252'></file_info>
 		<modified_lines>
 			<added_lines>13,658,659,660,661</added_lines>
 			<deleted_lines>13</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
