<bug_data>
<bug id='298' author='cregouby' open_date='2020-03-27T17:10:46Z' closed_time='2020-04-01T12:19:54Z'>
 	<summary>[data_handler/utils.py] read_docs_from_txt() is not robust to sentence made of Out-of-vocabulary characters</summary>
 	<description>
 Describe the bug
 When a text file with one sentence per line is read by read_docs_from_txt() with the sentence being made only on UTF-8  characters, pipeline end-up silently with a tokeniser exception. Such sentences may occur when text comes out of an automatic extraction workflow.
 Error message
 &lt;denchmark-code&gt;      .--.        _____                       _
     .'_\/_'.     / ____|                     | |
     '. /\ .'    | (___   __ _ _ __ ___  _ __ | | ___
       "||"       \___ \ / _` | '_ ` _ \| '_ \| |/ _ \
        || /\     ____) | (_| | | | | | | |_) | |  __/
     /\ ||//\)   |_____/ \__,_|_| |_| |_| .__/|_|\___|
    (/\||/                             |_|
 ______\||/___________________________________________
 
 ID: train-2283-0
 Clear Text:
         doc_id: 02789c9e-a975-3e0a-be08-bd6d8fa1f864
         sent_id: 2
         text: ��������������������������������������������������������������������������������������������������������������������������������
 Tokenized:
         tokens: []
         offsets: []
         start_of_word: []
 Features:
         None
 &lt;/denchmark-code&gt;
 
 Expected behavior
 You could maybe add an explicit error message, and a fallback to [UNK] token
 To Reproduce
 Create a syntetic dataset with sentences made of utf-8 space characters
 System:
 
 OS: Linux
 GPU/CPU: GPU
 FARM version: 4ab81bd
 
 	</description>
 	<comments>
 		<comment id='1' author='cregouby' date='2020-04-01T09:52:26Z'>
 		Hi, this pull request (&lt;denchmark-link:https://github.com/deepset-ai/FARM/pull/308&gt;#308&lt;/denchmark-link&gt;
 ) should fix the issue. Could you try it out and let us know if it solves it?
 		</comment>
 	</comments>
 </bug>
<commit id='347ca82f9ad446bca3967e5e14421d0fe4c4f07a' author='Branden Chan' date='2020-04-01 14:19:53+02:00'>
 	<dmm_unit complexity='0.0' interfacing='0.09090909090909091' size='0.15151515151515152'></dmm_unit>
 	<modification change_type='MODIFY' old_name='farm\data_handler\processor.py' new_name='farm\data_handler\processor.py'>
 		<file_info nloc='808' complexity='126' token_count='4753'></file_info>
 		<method name='_dict_to_samples' parameters='self,dictionary,all_dicts'>
 				<method_info nloc='54' complexity='8' token_count='362' nesting_level='1' start_line='804' end_line='866'></method_info>
 			<added_lines>826,827,828,829,830,831,832,833,834,835,856,857</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_init_samples_in_baskets' parameters='self'>
 				<method_info nloc='13' complexity='9' token_count='124' nesting_level='1' start_line='270' end_line='282'></method_info>
 			<added_lines>280,281,282</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_dict_to_samples' parameters='self,dict,kwargs'>
 				<method_info nloc='10' complexity='3' token_count='113' nesting_level='1' start_line='481' end_line='492'></method_info>
 			<added_lines>483,484,485,486,487</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>525,526,527,528,529,530,531,532,533,534,700,701,702,703,1179,1180,1181,1182</added_lines>
 			<deleted_lines>480</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
