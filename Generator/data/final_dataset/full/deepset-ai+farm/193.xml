<bug_data>
<bug id='193' author='Hahny' open_date='2020-01-16T11:29:21Z' closed_time='2020-01-19T19:08:01Z'>
 	<summary>Error during dev set evaluation in lm_finetuning.py when adding a custom vocab via  tokenizer.add_tokens</summary>
 	<description>
 Hello and thanks again for your work and help!
 When i add custom tokens during an LM finetuning an index out of range error is thrown during the evaluation of the dev set. The error does not appear if you run the training without an dev set (setting dev_filename=False in the  BertStyleLMProcessor).
 If no dev and test set is specified, the model can later be used for a downstream task with the new custom vocab. Error seems only to appear when a dev and test set is specified like in the example. I am using the latest master branch.
 The reason i want a dev set in the LM finetuning is to save some "checkpoints" via early stopping during the LM fine tuning and test them on a downstream task.
 Here is the error and after that the whole script:
 &lt;denchmark-code&gt;Train epoch 1/1 (Cur. train loss: 0.6664):  18%|█▊        | 30/170 [00:48&lt;03:43,  1.60s/it]
 Evaluating:   0%|          | 0/319 [00:00&lt;?, ?it/s]
 ---------------------------------------------------------------------------
 IndexError                                Traceback (most recent call last)
 &lt;ipython-input-3-f9d4de447982&gt; in &lt;module&gt;()
       1 # 7. Let it grow! Watch the tracked metrics live on the public mlflow server: https://public-mlflow.deepset.ai
 ----&gt; 2 model = trainer.train(model)
 
 ~/bertclassifier/FARM/farm/train.py in train(self, model)
     224                     ):
     225                         evalnr += 1
 --&gt; 226                         result = self.evaluator_dev.eval(model)
     227                         self.evaluator_dev.log_results(result, "Dev", self.global_step)
     228                         if self.early_stopping:
 
 ~/bertclassifier/FARM/farm/eval.py in eval(self, model, return_preds_and_labels)
      71                 losses_per_head = model.logits_to_loss_per_head(logits=logits, **batch)
      72                 preds = model.logits_to_preds(logits=logits, **batch)
 ---&gt; 73                 labels = model.prepare_labels(**batch)
      74 
      75             # stack results of all batches per prediction head
 
 ~/bertclassifier/FARM/farm/modeling/adaptive_model.py in prepare_labels(self, **kwargs)
     170         #     all_labels.append(labels)
     171         for head in self.prediction_heads:
 --&gt; 172             labels = head.prepare_labels(**kwargs)
     173             all_labels.append(labels)
     174         return all_labels
 
 ~/bertclassifier/FARM/farm/modeling/prediction_head.py in prepare_labels(self, **kwargs)
     662         # we have a batch of sequences here. we need to convert for each token in each sequence.
     663         for ids_for_sequence in label_ids:
 --&gt; 664             labels.append([self.label_list[int(x)] for x in ids_for_sequence if int(x) != -1])
     665         return labels
     666 
 
 ~/bertclassifier/FARM/farm/modeling/prediction_head.py in &lt;listcomp&gt;(.0)
     662         # we have a batch of sequences here. we need to convert for each token in each sequence.
     663         for ids_for_sequence in label_ids:
 --&gt; 664             labels.append([self.label_list[int(x)] for x in ids_for_sequence if int(x) != -1])
     665         return labels
     666 
 
 IndexError: list index out of range
 &lt;/denchmark-code&gt;
 
 Script:
 &lt;denchmark-code&gt;import logging
 
 import torch
 
 from farm.data_handler.data_silo import DataSilo
 from farm.data_handler.processor import BertStyleLMProcessor
 from farm.modeling.adaptive_model import AdaptiveModel
 from farm.modeling.language_model import LanguageModel
 from farm.modeling.prediction_head import BertLMHead, NextSentenceHead
 from farm.modeling.tokenization import Tokenizer
 from farm.train import Trainer
 from farm.modeling.optimization import initialize_optimizer
 
 from farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings
 
 logging.basicConfig(
     format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
     datefmt="%m/%d/%Y %H:%M:%S",
     level=logging.INFO,
 )
 
 set_all_seeds(seed=42)
 ml_logger = MLFlowLogger(tracking_uri="https://public-mlflow.deepset.ai/")
 ml_logger.init_experiment(
     experiment_name="Public_FARM", run_name="Run_minimal_example_lm"
 )
 
 device, n_gpu = "cuda",1
 n_epochs = 1
 batch_size = 32
 evaluate_every = 30
 lang_model = "bert-base-cased"
 
 
 # 1.Create a tokenizer
 tokenizer = Tokenizer.load(
     pretrained_model_name_or_path=lang_model, do_lower_case=False
 )
 
 special_tokens = ['regression', 'logistic']
 tokenizer.add_tokens(list(special_tokens))
 
 # 2. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset
 processor = BertStyleLMProcessor(
     data_dir="../data/lm_finetune_nips", tokenizer=tokenizer, max_seq_len=128, max_docs=30
 )
 # 3. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and calculates a few descriptive statistics of our datasets
 data_silo = DataSilo(processor=processor, batch_size=batch_size, max_multiprocessing_chunksize=20)
 
 language_model = LanguageModel.load(lang_model,n_added_tokens=len(tokenizer.added_tokens_decoder))
 lm_prediction_head = BertLMHead.load(lang_model,n_added_tokens=len(tokenizer.added_tokens_decoder))
 next_sentence_head = NextSentenceHead.load(lang_model)
 
 model = AdaptiveModel(
     language_model=language_model,
     prediction_heads=[lm_prediction_head, next_sentence_head],
     embeds_dropout_prob=0.1,
     lm_output_types=["per_token", "per_sequence"],
     device=device,
 )
 
 # 5. Create an optimizer
 model, optimizer, lr_schedule = initialize_optimizer(
     model=model,
     learning_rate=2e-5,
     device=device,
     n_batches=len(data_silo.loaders["train"]),
     n_epochs=n_epochs,
 )
 
 # 6. Feed everything to the Trainer, which keeps care of growing our model into powerful plant and evaluates it from time to time
 trainer = Trainer(
     optimizer=optimizer,
     data_silo=data_silo,
     epochs=n_epochs,
     n_gpu=n_gpu,
     lr_schedule=lr_schedule,
     evaluate_every=evaluate_every,
     device=device,
 )
 
 # 7. Let it grow! Watch the tracked metrics live on the public mlflow server: https://public-mlflow.deepset.ai
 model = trainer.train(model)
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='Hahny' date='2020-01-17T14:03:53Z'>
 		Hey thanks for a well documented issue. I was able to replicate it. I believe the issue has to do with the initialization of the BertStyleLMProcessor (processor.py). The vocab list there is not updated with the added tokens. I have opened  &lt;denchmark-link:https://github.com/deepset-ai/FARM/pull/197&gt;#197&lt;/denchmark-link&gt;
  to deal with this. Let me know if this solves it.
 		</comment>
 		<comment id='2' author='Hahny' date='2020-01-19T19:08:01Z'>
 		Thanks, i had written my own checking points according to my needs, Therefore, i did not test your update.
 		</comment>
 	</comments>
 </bug>
<commit id='1f6e08260670ddb5b2af5d5a783ec2f9a9ebf91b' author='Branden Chan' date='2020-01-17 18:15:06+01:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='farm\data_handler\processor.py' new_name='farm\data_handler\processor.py'>
 		<file_info nloc='705' complexity='105' token_count='4169'></file_info>
 		<method name='get_added_tokens' parameters='self'>
 				<method_info nloc='4' complexity='2' token_count='43' nesting_level='1' start_line='730' end_line='733'></method_info>
 			<added_lines>730,731,732,733</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>725,726,734</added_lines>
 			<deleted_lines>725,726</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
