<bug_data>
<bug id='113' author='tholor' open_date='2019-10-11T15:31:43Z' closed_time='2019-10-14T17:11:19Z'>
 	<summary>Empty datasets due to random_split_ConcatDataset</summary>
 	<description>
 Describe the bug
 DataSilo is Crashing during the attempt of splitting a dev set from a "small" train set.
 It seems that random_split_ConcatDataset() doesn't work if there's only a single chunk (= 1 dataset). idx_dataset is 0 in that case and thus creates an empty ConcatDataset for train
 Error message
 Error that was thrown (if available)
 &lt;denchmark-code&gt;10/11/2019 17:12:47 - INFO - farm.data_handler.data_silo -   Loading dev set as a slice of train set
 Traceback (most recent call last):
   File ".../train.py", line 436, in &lt;module&gt;
     augmentation=True)
   File ".../train.py", line 348, in continue_finetuning
     data_silo = DataSilo(processor=processor, batch_size=batch_size, multiprocessing_chunk_size=2000)
   File "/.../farm/data_handler/data_silo.py", line 49, in __init__
     self._load_data()
   File ".../farm/data_handler/data_silo.py", line 104, in _load_data
     self._create_dev_from_train()
   File ".../farm/data_handler/data_silo.py", line 175, in _create_dev_from_train
     train_dataset, dev_dataset = self.random_split_ConcatDataset(self.data["train"], lengths=[n_train, n_dev])
   File ".../farm/data_handler/data_silo.py", line 200, in random_split_ConcatDataset
     train = ConcatDataset(ds.datasets[:idx_dataset])
   File ".../torch/utils/data/dataset.py", line 68, in __init__
     assert len(datasets) &gt; 0, 'datasets should not be an empty iterable'
 AssertionError: datasets should not be an empty iterable
 &lt;/denchmark-code&gt;
 
 Expected behavior
 A portion of the train set should be splitted apart into a dev set
 
 Related function:
 &lt;denchmark-link:https://github.com/deepset-ai/FARM/blob/master/farm/data_handler/data_silo.py#L186&gt;https://github.com/deepset-ai/FARM/blob/master/farm/data_handler/data_silo.py#L186&lt;/denchmark-link&gt;
 
 &lt;denchmark-code&gt;    def random_split_ConcatDataset(self, ds, lengths):
 ...
         if sum(lengths) != len(ds):
             raise ValueError("Sum of input lengths does not equal the length of the input dataset!")
 
         idx_dataset = np.where(np.array(ds.cumulative_sizes) &gt; lengths[0])[0][0]
 
         train = ConcatDataset(ds.datasets[:idx_dataset])
         test = ConcatDataset(ds.datasets[idx_dataset:])
         return train, test
 &lt;/denchmark-code&gt;
 
 To Reproduce
 
 Provide no dev_filename but instead set dev_split
 Have a trainfile that has less samples that multiprocessing_chunksize
 
 System:
 
 OS: Ubuntu 18.04
 GPU/CPU: CPU
 FARM version: 0.2.1
 
 	</description>
 	<comments>
 		<comment id='1' author='tholor' date='2019-10-14T17:11:19Z'>
 		Resolved by &lt;denchmark-link:https://github.com/deepset-ai/FARM/pull/114&gt;#114&lt;/denchmark-link&gt;
 
 Closing this now
 		</comment>
 	</comments>
 </bug>
<commit id='432ffb136a7c260b883e55c6c134a7e49ddaf53c' author='Timo Moeller' date='2019-10-14 18:54:48+02:00'>
 	<dmm_unit complexity='1.0' interfacing='0.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='farm\__init__.py' new_name='farm\__init__.py'>
 		<file_info nloc='12' complexity='0' token_count='76'></file_info>
 		<modified_lines>
 			<added_lines>18,19</added_lines>
 			<deleted_lines>18</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='farm\data_handler\data_silo.py' new_name='farm\data_handler\data_silo.py'>
 		<file_info nloc='204' complexity='33' token_count='1603'></file_info>
 		<method name='_create_dev_from_train' parameters='self'>
 				<method_info nloc='12' complexity='2' token_count='102' nesting_level='1' start_line='173' end_line='186'></method_info>
 			<added_lines>182</added_lines>
 			<deleted_lines>173,174,180</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,processor,batch_size,distributed'>
 				<method_info nloc='8' complexity='1' token_count='50' nesting_level='1' start_line='32' end_line='48'></method_info>
 			<added_lines>32</added_lines>
 			<deleted_lines>32,47</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,processor,batch_size,distributed,multiprocessing_chunk_size'>
 				<method_info nloc='9' complexity='1' token_count='59' nesting_level='1' start_line='32' end_line='49'></method_info>
 			<added_lines>32</added_lines>
 			<deleted_lines>32,47</deleted_lines>
 		</method>
 		<method name='random_split_ConcatDataset' parameters='self,ds,lengths'>
 				<method_info nloc='9' complexity='2' token_count='89' nesting_level='1' start_line='188' end_line='206'></method_info>
 			<added_lines>201,202</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_get_dataset' parameters='self,filename'>
 				<method_info nloc='29' complexity='7' token_count='253' nesting_level='1' start_line='57' end_line='97'></method_info>
 			<added_lines>65,66,67,68,69,70,71,72,73,74,77,80,82,86,94</added_lines>
 			<deleted_lines>66,67,70,73,75,79,87</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>167,171,172</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='farm\data_handler\processor.py' new_name='farm\data_handler\processor.py'>
 		<file_info nloc='599' complexity='81' token_count='3433'></file_info>
 		<method name='_dict_to_samples' parameters='self,dictionary,all_dicts'>
 				<method_info nloc='22' complexity='2' token_count='132' nesting_level='1' start_line='624' end_line='645'></method_info>
 			<added_lines>625</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='farm\eval.py' new_name='farm\eval.py'>
 		<file_info nloc='108' complexity='25' token_count='762'></file_info>
 		<method name='eval' parameters='self,model'>
 				<method_info nloc='57' complexity='15' token_count='448' nesting_level='1' start_line='46' end_line='134'></method_info>
 			<added_lines>93,118,129</added_lines>
 			<deleted_lines>117,118,119,120,121</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='farm\infer.py' new_name='farm\infer.py'>
 		<file_info nloc='190' complexity='25' token_count='1186'></file_info>
 		<method name='__init__' parameters='self,model,processor,batch_size,gpu,name,return_class_probs'>
 				<method_info nloc='8' complexity='1' token_count='23' nesting_level='1' start_line='45' end_line='52'></method_info>
 			<added_lines>52</added_lines>
 			<deleted_lines>51,52</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,model,processor,batch_size,gpu,name,return_class_probs,multiprocessing_chunk_size'>
 				<method_info nloc='9' complexity='1' token_count='28' nesting_level='1' start_line='44' end_line='52'></method_info>
 			<added_lines>52</added_lines>
 			<deleted_lines>51,52</deleted_lines>
 		</method>
 		<method name='load' parameters='cls,load_dir,batch_size,gpu,embedder_only,return_class_probs,multiprocessing_chunk_size'>
 				<method_info nloc='8' complexity='1' token_count='26' nesting_level='1' start_line='97' end_line='104'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>104</deleted_lines>
 		</method>
 		<method name='inference_from_dicts' parameters='self,dicts,rest_api_schema'>
 				<method_info nloc='30' complexity='5' token_count='224' nesting_level='1' start_line='143' end_line='193'></method_info>
 			<added_lines>162,163,164,165,166,167,168,169,170,171,174,177,179,183,191</added_lines>
 			<deleted_lines>167,168,171,174,176,180,188</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>5</added_lines>
 			<deleted_lines>91,118,119,140</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
