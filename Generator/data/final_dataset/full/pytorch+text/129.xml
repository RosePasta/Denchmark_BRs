<bug_data>
<bug id='129' author='jihunchoi' open_date='2017-09-25T13:16:50Z' closed_time='2017-10-10T05:38:18Z'>
 	<summary>Unintuitive behavior of Iterator when sort is False</summary>
 	<description>
 Currently, the following line is executed regardless of sort value.
 
 
 
 text/torchtext/data/iterator.py
 
 
          Line 162
       in
       2980f1b
 
 
 
 
 
 
  minibatch.reverse() 
 
 
 
 
 
 It could result in a counter-intuitive behavior when sort is False, since one would probably expect that the order of data is kept intact when sort is False.
 I think this should be executed only when sort is True.
 Is it by design, or a bug?
 	</description>
 	<comments>
 		<comment id='1' author='jihunchoi' date='2017-09-30T13:26:16Z'>
 		Hmm it seems that it can't be solved simply --- I think currently sorting is kind of tangled in the implementation.
 For example when BucketIterator is used, sorting is done within a batch, whether sort is True or not.
 (But with the plain Iterator, sort-within-batch does not occur thus we can't use packed_padded_sequence unless we perform some explicit sorting.)
 I think just calling "reverse" function in the iterator cannot completely solve the issue wrt "packed sequence". Currently it works only with BucketIterator.
 I suggest the following modification:
 
 "sort" flag is used only for indicating "sort the entire data".
 Add the additional "sort_batch" (or more appropriate name) flag to let the iterator sort the batch.
 Sort order (increasing or decreasing) should be designated only by sort_key function of a dataset. Implanting "reverse" operation directly into the core Dataset implementation does not seem to be a good way to do this.
 
 For example, if one wants to sort the batch, not the entire data, in the decreasing order of length, they should set sort=False, sort_batch=True, and sort_key=lambda ex: -len(ex.text).
 		</comment>
 		<comment id='2' author='jihunchoi' date='2017-10-02T07:09:12Z'>
 		I agree with most of that, but I still think it's helpful for the order within a batch to default to the opposite of the provided sort_key order (which is used directly as the order between batches). That packed sequences are sorted in decreasing order is essentially a cuDNN implementation detail, while reversing the batch later is difficult to do performantly in the absence of negative stride support in TH/THC core and I think it's strange to ask users to manually provide a reversed sort_key if they want to use packed sequences (since I see sort_key as largely a dataset property that says by which field's length/combination of field lengths a dataset is most naturally sorted).
 Ideally I want to satisfy everyone, but this sorting question has been causing issues in OpenNMT for a few weeks, so I think we should come to a decision and release 0.2 to pypi? Personally I'd lean towards a) making the change outlined in the OP and b) also adding a separate flag to allow sorting the batch even when the data isn't otherwise sorted, but retaining the . Any thoughts &lt;denchmark-link:https://github.com/nelson-liu&gt;@nelson-liu&lt;/denchmark-link&gt;
 ?
 		</comment>
 		<comment id='3' author='jihunchoi' date='2017-10-08T06:33:36Z'>
 		Whoops, this seems to have skipped my inbox. I agree with the conclusion that seems to have been reached (make change described in OP, add flag to allow sorting batch when data isn't otherwise sorted but retaining the reverse)...
 		</comment>
 		<comment id='4' author='jihunchoi' date='2017-10-10T05:57:26Z'>
 		I think the solution is cool. In default settings it works nicely with packed sequences, and it's highly customizable! (+ solves the OpenNMT issue)
 		</comment>
 	</comments>
 </bug>
<commit id='c7389b1e4a2897c8c173f33c3132bfefe6c3125c' author='jekbradbury' date='2017-10-09 22:38:17-07:00'>
 	<dmm_unit complexity='0.0' interfacing='0.8' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='torchtext\data\iterator.py' new_name='torchtext\data\iterator.py'>
 		<file_info nloc='233' complexity='44' token_count='1355'></file_info>
 		<method name='__init__' parameters='self,dataset,batch_size,sort_key,device,batch_size_fn,count,count,train,repeat,shuffle,sort,sort_within_batch'>
 				<method_info nloc='4' complexity='1' token_count='48' nesting_level='1' start_line='74' end_line='77'></method_info>
 			<added_lines>76,77</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,dataset,batch_size,sort_key,device,batch_size_fn,count,count,train,repeat,shuffle,sort'>
 				<method_info nloc='3' complexity='1' token_count='44' nesting_level='1' start_line='70' end_line='72'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>72</deleted_lines>
 		</method>
 		<method name='__iter__' parameters='self'>
 				<method_info nloc='17' complexity='7' token_count='99' nesting_level='1' start_line='160' end_line='180'></method_info>
 			<added_lines>169,170,171,172,173,174,175,176</added_lines>
 			<deleted_lines>160,161,162</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>66,67,68,69,84,85,86,87</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
