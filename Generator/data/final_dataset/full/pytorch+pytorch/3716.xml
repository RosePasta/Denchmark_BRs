<bug_data>
<bug id='3716' author='avmgithub' open_date='2017-11-15T16:28:51Z' closed_time='2020-05-26T16:09:17Z'>
 	<summary>[ppc64le] test_torch.py test failing in test_qr</summary>
 	<description>
 Running on Ubuntu 16.04 ppc64le , when running test_torch.py  test fails in test_qr.  Test only fails when running on Virtual Machine.
 $ python -m unittest test_torch.TestTorch.test_qr
 check_qr 1
 check_qr 2
 check_qr 3
 check big matrix 4
 F
 FAIL: test_qr (test_torch.TestTorch)
 Traceback (most recent call last):
 File "/home/ubuntu/builder/jenkins/pytorch/pytorch/test/common.py", line 53, in wrapper
 fn(*args, **kwargs)
 File "/home/ubuntu/builder/jenkins/pytorch/pytorch/test/test_torch.py", line 2127, in test_qr
 self.assertEqual(a, a_qr, prec=1e-3)
 File "/home/ubuntu/builder/jenkins/pytorch/pytorch/test/common.py", line 204, in assertEqual
 assertTensorsEqual(x, y)
 File "/home/ubuntu/builder/jenkins/pytorch/pytorch/test/common.py", line 196, in assertTensorsEqual
 self.assertLessEqual(max_err, prec, message)
 AssertionError: 5.990425851987622 not less than or equal to 0.001 :
 &lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;
 
 Ran 1 test in 0.457s
 FAILED (failures=1)
 This only fails when running on a virtual machine.  Baremetal systems do not seem to exhibit the failure. Also this particular line seems to affect the failure:
 a = torch.randn(1000, 1000)  after the comment # check big matrix
 If I change this to a = torch.randn(130, 130),   in most cases the test passes but there are random failures.
 If someone can please point me to where the torch.randn  code is , I'll see if I can figure this out.
 	</description>
 	<comments>
 		<comment id='1' author='avmgithub' date='2017-11-16T04:47:06Z'>
 		the torch.randn code eventually links to
 &lt;denchmark-link:https://github.com/pytorch/pytorch/blob/master/aten/src/TH/generic/THTensorRandom.c#L80&gt;https://github.com/pytorch/pytorch/blob/master/aten/src/TH/generic/THTensorRandom.c#L80&lt;/denchmark-link&gt;
 
 &lt;denchmark-link:https://github.com/pytorch/pytorch/blob/master/aten/src/TH/THRandom.c#L253&gt;https://github.com/pytorch/pytorch/blob/master/aten/src/TH/THRandom.c#L253&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='avmgithub' date='2017-12-08T18:12:42Z'>
 		Hi,
 I found that the problem is in the LAPACK implementation in OpenBLAS. OpenBLAS 0.2.19 has issues detecting the correct number of CPUs and this leads to some threading issues as far as I can understand. This happens on Virtual Machines and also on other systems where there are CPU limitations. As of today Conda provides OpenBLAS 0.2.19 on ppc64le. RHEL 7.4 ships with OpenBLAS 0.2.20 and Ubuntu 16.04 ships with OpenBLAS 0.2.18. I tested both RHEL 7.4 and Ubuntu 16.04 and it works fine with them.
 The PR in OpenBLAS that fixes the problem is &lt;denchmark-link:https://github.com/xianyi/OpenBLAS/pull/981&gt;xianyi/OpenBLAS#981&lt;/denchmark-link&gt;
 .
 Edit: To summarize: the problem is in OpenBLAS 0.2.19, earlier and later versions work fine.
 		</comment>
 		<comment id='3' author='avmgithub' date='2018-05-09T09:21:52Z'>
 		I see a similar issue with pytorch v0.4.0, I am using OpenBLAS v0.2.20.
 &lt;denchmark-code&gt;======================================================================
 FAIL: test_qr (test_torch.TestTorch)
 ----------------------------------------------------------------------
 Traceback (most recent call last):
   File "common.py", line 66, in wrapper
     fn(*args, **kwargs)
   File "test_torch.py", line 3133, in test_qr
     self.assertEqual(a, a_qr, prec=1e-3)
   File "common.py", line 243, in assertEqual
     assertTensorsEqual(x, y)
   File "common.py", line 235, in assertTensorsEqual
     self.assertLessEqual(max_err, prec, message)
 AssertionError: tensor(6.6219) not less than or equal to 0.001
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='4' author='avmgithub' date='2018-05-09T11:53:50Z'>
 		I'm also using openBlas 0.2.20.
 $ rpm -qa | grep blas
 libopenblas-0.2.20-2528.e2e928f.ppc64le
 Are you using the version that comes from the OS  or from PowerAI
 I do not see the problem on my Power9 RH7.4
 cpuinfo
 processor       : 159
 cpu             : POWER9 (raw), altivec supported
 clock           : 1983.000000MHz
 revision        : 2.1 (pvr 004e 1201)
 $ uname -a
 Linux p95a28.pbm.ihost.com 4.11.0-44.5.1.el7a.ppc64le &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/1&gt;#1&lt;/denchmark-link&gt;
  SMP Thu Jan 18 02:18:03 EST 2018 ppc64le ppc64le ppc64le GNU/Linux
 		</comment>
 		<comment id='5' author='avmgithub' date='2018-05-09T15:24:00Z'>
 		We're using a PowerAI build of OpenBLAS 0.2.20 (a bit newer than yours and not published yet libopenblas-0.2.20-2593.b59753e.ppc64le).
 Some particulars in case this rings any bells with anyone...
 We can force the failure with a simple test, based on the failing test_qr testcase:
 &lt;denchmark-code&gt;#!/usr/bin/env python
 
 import torch
 
 for sz in [ 127, 128, 129, 130, 500 ]:
         print("Size = {}".format(sz))
         for i in range(10):
                 a = torch.randn(sz, sz)
                 q, r = torch.qr(a)
                 a_qr = torch.mm(q, r)
                 m = float(max(max(x) for x in a - a_qr))
                 print("max diff = {0:.6f}{1}".format(m, " FAIL!" if m &gt; 0.001 else ""))
 &lt;/denchmark-code&gt;
 
 The failure doesn't seem to occur with matrix sizes of 128x128 or smaller, but will occur with increasing frequency as the matrix size grows. With 129x129, it fails maybe 1 in 10; 500x500 around 9 in 10.
 Not clear whether the problem is coming from torch.qr() or torch.mm() (or both). But presumably one of them is getting into some code that chooses a different algorithm at the 128x128 size cutoff (maybe based on particulars of the build or runtime environment).
 Doesn't fail for us on our Power8 machines but does on Power9, with same OpenBLAS and PyTorch binaries.
 		</comment>
 		<comment id='6' author='avmgithub' date='2018-05-09T21:51:23Z'>
 		Problem doesn't seem to reproduce with environment variable OMP_NUM_THREADS=1 set, so it's presumably an issue with OpenBLAS multi-threading that shows up on Power9.
 		</comment>
 		<comment id='7' author='avmgithub' date='2018-06-29T13:37:13Z'>
 		I am able to reproduce this issue with the following snippet:
 &lt;denchmark-code&gt;import torch
 a = torch.randn(1000,1000)
 q,r = torch.qr(a)
 m,tau = torch.geqrf(a)
 a_qr = torch.orgqr(m,tau)
 m_triu = torch.triu(m)
 y = float(max(max(x) for x in q - a_qr))
 z = float(max(max(x) for x in r - m_triu))
 
 print("Q max diff = {0:.6f}{1}".format(y, " FAIL!" if y &gt; 0.001 else ""))
 print("R max diff = {0:.6f}{1}".format(z, " FAIL!" if z &gt; 0.001 else ""))
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='8' author='avmgithub' date='2020-05-26T16:09:16Z'>
 		Given &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/3716#issuecomment-350332992&gt;#3716 (comment)&lt;/denchmark-link&gt;
  I am optimistically closing this as fixed.
 		</comment>
 	</comments>
 </bug>
<commit id='ae534dc9780ed5e327cc5c0a9d4685293a5573d3' author='ycao' date='2020-05-08 17:56:36-07:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='test\jit\test_builtins.py' new_name='test\jit\test_builtins.py'>
 		<file_info nloc='101' complexity='25' token_count='698'></file_info>
 		<method name='test_del_multiple_operands.del_list_multiple_operands' parameters='x'>
 				<method_info nloc='3' complexity='1' token_count='17' nesting_level='3' start_line='82' end_line='85'></method_info>
 			<added_lines>82,83,84,85</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_del_multiple_operands.del_dict_multiple_operands' parameters='x'>
 				<method_info nloc='3' complexity='1' token_count='17' nesting_level='3' start_line='90' end_line='93'></method_info>
 			<added_lines>90,91,92,93</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_del_multiple_operands' parameters='self'>
 				<method_info nloc='9' complexity='1' token_count='53' nesting_level='1' start_line='77' end_line='93'></method_info>
 			<added_lines>77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>94</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='torch\jit\frontend.py' new_name='torch\jit\frontend.py'>
 		<file_info nloc='568' complexity='147' token_count='4993'></file_info>
 		<method name='build_Delete' parameters='ctx,stmt'>
 				<method_info nloc='7' complexity='2' token_count='62' nesting_level='1' start_line='287' end_line='293'></method_info>
 			<added_lines>288,289,290,291,292</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
