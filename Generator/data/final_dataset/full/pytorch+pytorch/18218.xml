<bug_data>
<bug id='18218' author='fracting' open_date='2019-03-20T11:50:36Z' closed_time='2019-05-31T01:38:51Z'>
 	<summary>MKL-DNN causes wrong results on Xeon</summary>
 	<description>
 Editorial note: We think this is an MKL-DNN bug. See &lt;denchmark-link:https://github.com/oneapi-src/oneDNN/issues/431&gt;oneapi-src/oneDNN#431&lt;/denchmark-link&gt;
 
 &lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;
 
 Update: an updated test program to demo the problem is at &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/18218#issuecomment-475453948&gt;#18218 (comment)&lt;/denchmark-link&gt;
 
 --
 Hi, I have a simple python program as below:
 &lt;denchmark-code&gt;  1 import torch
   2
   3 #torch.set_flush_denormal(False)
   4 torch.set_printoptions(profile="full")
   5 torch.set_printoptions(precision=65)
   6
   7 conv2 = torch.load("conv2.pth")
   8 print("haha conv2")
   9 for param in conv2.parameters():
  10     print(param.size())
  11     #print(param.data)
  12
  13 input = torch.load("input.pth")
  14 print("haha input")
  15 print(input[0].size())
  16 #print(input[0])
  17
  18 output = conv2(input[0])
  19 print("haha output")
  20 print(output.size())
  21 print(output)
 &lt;/denchmark-code&gt;
 
 I ran the same code (CPU only, no GPU) on different machines:
 machine A with Intel(R) Core(TM) i7-4870HQ CPU @ 2.50GHz
 machine B with Intel(R) Core(TM) i9-8950HK CPU @ 2.90GHz
 machine C with Intel(R) Xeon(R) Platinum 8124M CPU @ 3.00GHz
 While I was loading the same input.pth and the same conv2.pth, the output from machine C is difference with the other machines, the output from machine A are the same as the output from machine B. I tried the same docker image on different machines, so probably it's not a software problem.
 I don't have permission to call PTRACE_TRACEME on machine C, so ltrace wouldn't help in this case.
 Any one could tell me how do I know which low level C library function does pytorch call when computing conv2d?
 I tried objdump -T on libmkldnn.so.0 and set breakpoint on mkldnn_dilated_convolution_forward_desc_init  and mkldnn_convolution_forward_desc_init but it didn't stop, so probably I was guessing the wrong function.
 Any hints are greatly appreciated!
 	</description>
 	<comments>
 		<comment id='1' author='fracting' date='2019-03-20T11:57:09Z'>
 		&lt;denchmark-link:https://github.com/pytorch/pytorch/files/2987978/conv2_input.zip&gt;conv2_input.zip&lt;/denchmark-link&gt;
 
 Attached are the data files to reproduce this problem.
 &lt;denchmark-code&gt;$ md5 conv2.pth
 MD5 (conv2.pth) = 7ba2908fb86852affa164d77e97a037f
 $ md5 input.pth
 MD5 (input.pth) = 59b814700fa1dcbbca1f56bce568ba37
 &lt;/denchmark-code&gt;
 
 Last few lines of good output are:
 &lt;denchmark-code&gt;1646785            -3.38981270790100097656250000000000000000000000000000000000000000000e+00,
 1646786            -3.34102034568786621093750000000000000000000000000000000000000000000e+00,
 1646787            -1.77292987704277038574218750000000000000000000000000000000000000000e-01]]]],
 1646788        grad_fn=&lt;MkldnnConvolutionBackward&gt;)
 &lt;/denchmark-code&gt;
 
 Last few lines of bad output are:
 &lt;denchmark-code&gt;1646785            -3.38981294631958007812500000000000000000000000000000000000000000000e+00,
 1646786            -3.34102010726928710937500000000000000000000000000000000000000000000e+00,
 1646787            -1.77292868494987487792968750000000000000000000000000000000000000000e-01]]]],
 1646788        grad_fn=&lt;MkldnnConvolutionBackward&gt;)
 &lt;/denchmark-code&gt;
 
 Note the first line in good snip ends with 56250000* while the first line in bad snip ends with 81250000*
 		</comment>
 		<comment id='2' author='fracting' date='2019-03-20T13:56:37Z'>
 		Different CPUs may use different conv algorithms. Also parallelized floating point arithmetic is not deterministic.
 		</comment>
 		<comment id='3' author='fracting' date='2019-03-20T15:59:37Z'>
 		as &lt;denchmark-link:https://github.com/SsnL&gt;@SsnL&lt;/denchmark-link&gt;
  said, they are the same to a particular machine precision. Beyond that, different CPU models, thread count, and other factors might give you different results (but same upto machine precision)
 		</comment>
 		<comment id='4' author='fracting' date='2019-03-20T22:23:33Z'>
 		Thanks &lt;denchmark-link:https://github.com/SsnL&gt;@SsnL&lt;/denchmark-link&gt;
  and &lt;denchmark-link:https://github.com/soumith&gt;@soumith&lt;/denchmark-link&gt;
  , that was a very important lesson to learn!
 Could I ask a follow up question?
 We have a deep learning classification model (PyTorch) which predicts the same results for same validation set on 3 different machines, however its prediction on the Intel(R) Xeon(R) Platinum 8124M CPU @ 3.00GHz machine significantly bias to one category. For example, when all other machines outputs bedroom category, the Xeon machine outputs shop, when all other machines outputs backyard category, the Xeon machine still outputs shop.
 Do you have any suggestions how to track this kind of issue?
 Is there any well known floating number computation test suites which I can run on different machines and compare the result? Or is the PyTorch test suite useful for this kind of diagnosing?
 Thank you again!
 		</comment>
 		<comment id='5' author='fracting' date='2019-03-21T05:13:09Z'>
 		&lt;denchmark-link:https://github.com/fracting&gt;@fracting&lt;/denchmark-link&gt;
  It could be a problem with mkldnn. Could you try building from source w/o mkldnn and see if the xeon machine outputs a different prediction?
 		</comment>
 		<comment id='6' author='fracting' date='2019-03-21T07:29:27Z'>
 		Hi &lt;denchmark-link:https://github.com/SsnL&gt;@SsnL&lt;/denchmark-link&gt;
 
 Thanks for the hints! I compile PyTorch from scratch, either with or without mkldnn (by exporting USE_MKLDNN="FALSE")
 I can confirm the model outputs bad prediction with mkldnn, but outputs good prediction without mkldnn. (Both on Xeon)
 		</comment>
 		<comment id='7' author='fracting' date='2019-03-21T15:31:06Z'>
 		&lt;denchmark-link:https://github.com/soumith&gt;@soumith&lt;/denchmark-link&gt;
  This is probably worth taking a look at.
 &lt;denchmark-link:https://github.com/fracting&gt;@fracting&lt;/denchmark-link&gt;
  Could you update the issue description to include your new findings? A more detailed approach to reproduce the problem using the network &amp; weights would be great. Thanks!
 		</comment>
 		<comment id='8' author='fracting' date='2019-03-21T16:34:30Z'>
 		this is high-priority, if there's any way to reproduce your issue, we will look and fix mkl-dnn.
 cc: &lt;denchmark-link:https://github.com/mingfeima&gt;@mingfeima&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='9' author='fracting' date='2019-03-21T23:36:37Z'>
 		cc &lt;denchmark-link:https://github.com/jgong5&gt;@jgong5&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='10' author='fracting' date='2019-03-22T00:33:58Z'>
 		&lt;denchmark-link:https://github.com/soumith&gt;@soumith&lt;/denchmark-link&gt;
  we will double check this immediately and reply to you soon.
 		</comment>
 		<comment id='11' author='fracting' date='2019-03-22T00:39:58Z'>
 		&lt;denchmark-link:https://github.com/pytorch/pytorch/files/2994453/demo2.tar.gz&gt;demo2.tar.gz&lt;/denchmark-link&gt;
 
 Hi, I can reduce the problem with the attached demo code and data.
 The code loads a conv2 layer from conv2.pth, loads a demo input from input.pth, then computes the current output, comparing with output.i7.pth and output.xeon.pth
 &lt;denchmark-code&gt;.
 ├── conv2.pth
 ├── demo2.py
 ├── input.pth
 ├── output.i7.pth
 └── output.xeon.pth
 
 $ python demo2.py
 &lt;/denchmark-code&gt;
 
 According to my test, the cosine similarity between i7 output and xeon output is pretty low:
 &lt;denchmark-code&gt;tensor([[[ 2.350216172635555e-02,  1.297117769718170e-01,
            1.381154656410217e-01,  6.210334133356810e-03,
            3.263642266392708e-02,  1.101085636764765e-02,
            2.132313139736652e-02,  5.432983487844467e-02,
 &lt;/denchmark-code&gt;
 
 Env:
 Python3.6, with pytorch installed from latest conda
 Please let me know if you can reproduce. I'm glad to reduce it to C code if you can assist. Thanks!
 		</comment>
 		<comment id='12' author='fracting' date='2019-03-22T00:56:58Z'>
 		&lt;denchmark-link:https://github.com/fracting&gt;@fracting&lt;/denchmark-link&gt;
  i see your post on &lt;denchmark-link:https://github.com/intel/mkl-dnn/issues/431&gt;mkl-dnn&lt;/denchmark-link&gt;
  github, the info for me should be good enough to kick off debugging.
 I will let you know if i need additional info.
 		</comment>
 		<comment id='13' author='fracting' date='2019-03-22T00:57:34Z'>
 		&lt;denchmark-link:https://github.com/fracting&gt;@fracting&lt;/denchmark-link&gt;
   Between output.i7.pth and output.xeon.pth, both of them are processed on MKL-DNN?
 		</comment>
 		<comment id='14' author='fracting' date='2019-03-22T01:00:04Z'>
 		
 Hi, @Jianhui-Li , yes, both output.i7.pth and output.xeon.pth are computed with official conda pytorch package, which has MKL-DNN by default.
 Also torch._C.has_mkl are True on both environment.
 
 Update:
 Hi &lt;denchmark-link:https://github.com/Jianhui-Li&gt;@Jianhui-Li&lt;/denchmark-link&gt;
  , the output.i7.pth was from Mac OS X, which has  torch._C.has_mkl flag as True but I'm not confident if it is equivalent to the Linux version after reading
 &lt;denchmark-link:https://github.com/oneapi-src/oneDNN/issues/10&gt;oneapi-src/oneDNN#10&lt;/denchmark-link&gt;
 
 I'll test on Linux and come back later.
 Update2:
 I can confirm output from my i7+OSX (host machine) is identical with output from my i7+Linux (docker container running on exactly the same host machine).
 Hi &lt;denchmark-link:https://github.com/mingfeima&gt;@mingfeima&lt;/denchmark-link&gt;
  thanks for working on it! Let me know what else info is needed.
 		</comment>
 		<comment id='15' author='fracting' date='2019-05-31T01:38:48Z'>
 		Retested on a Xeon machine with both pytorch-1.0.1 and pytorch-1.1.0 from official conda binary packages, confirming fixed in pytorch-1.1.0.
 &lt;denchmark-code&gt;$ cat /proc/cpuinfo | grep flags
 flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single kaiser fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f rdseed adx smap clflushopt clwb avx512cd xsaveopt xsavec xgetbv1 ida arat pku
 &lt;/denchmark-code&gt;
 
 Closing the issue, thank you for all the great work!
 		</comment>
 	</comments>
 </bug>
<commit id='d17aa723730d9d72d032983abedf54735d86e345' author='Qian Hong' date='2019-05-30 11:36:07-07:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='test\test_nn.py' new_name='test\test_nn.py'>
 		<file_info nloc='6970' complexity='1163' token_count='76905'></file_info>
 		<method name='test_Conv2d_groups_nobias_v2' parameters='self'>
 				<method_info nloc='27' complexity='4' token_count='415' nesting_level='1' start_line='4781' end_line='4810'></method_info>
 			<added_lines>4781,4782,4783,4784,4785,4786,4787,4788,4789,4790,4791,4792,4793,4794,4795,4796,4797,4798,4799,4800,4801,4802,4803,4804,4805,4806,4807,4808,4809,4810</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>4777,4778,4779,4780,4811</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
