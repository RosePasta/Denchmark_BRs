<bug_data>
<bug id='11294' author='anqixu' open_date='2020-10-09T03:39:24Z' closed_time='2020-10-12T20:48:45Z'>
 	<summary>[rllib] SAC (pytorch) log_alpha not optimizing with GPU enabled</summary>
 	<description>
 &lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;
 
 The entropy regularization coefficient alpha (implemented as sac_model.log_alpha tensor) is not being updated by sac_policy.alpha_optim.
 Verify by running the attached script, load tensorboard, and confirm that the alpha value does not change over training iterations.
 &lt;denchmark-h:h3&gt;Suspected cause&lt;/denchmark-h&gt;
 
 I believe the culprit is due to an incorrect order of execution:
 
 sac_model.log_alpha is initially created as a tensor on CPU device
 sac_policy.alpha_optim is initialized with the sac_model.log_alpha param
 but then sac_model.log_alpha is CLONED to the CUDA device, and this copy is used during backprop
 
 To verify, put a debugger breakpoint &lt;denchmark-link:https://github.com/ray-project/ray/blob/b450cb030a5d316f04d4f6322e152335853d1a88/rllib/policy/torch_policy.py#L402&gt;after opt.step()&lt;/denchmark-link&gt;
 , then verify that  does not match :
 self._optimizers[3].param_groups[0]["params"][0]
 # DEBUGGER OUTPUT: tensor([-0.0003], requires_grad=True)
 
 self.model.log_alpha
 # DEBUGGER OUTPUT: tensor([0.], device='cuda:0', grad_fn=&lt;CopyBackwards&gt;)
 
 self._optimizers[3].param_groups[0]["params"][0] is self.model.log_alpha
 # DEBUGGER OUTPUT: False
 Unfortunately, I don't know RLLib internals enough to know how to properly solve this issue.
 &lt;denchmark-h:h3&gt;Related issue: unable to disable GPU usage in RLLib configs&lt;/denchmark-h&gt;
 
 According to &lt;denchmark-link:https://docs.ray.io/en/master/rllib-algorithms.html#sac&gt;SAC configs&lt;/denchmark-link&gt;
 , we should be able to disable GPU usage by the learner/driver by setting . I also set  just in case. But Ray still puts tensors onto CUDA. I don't know why this is the case.
 To sidestep this issue, we can force Ray to start without GPUs: ray.init(num_gpus=0, ...).
 Ray version and other system information (Python version, TensorFlow version, OS):
 
 Tested with ray 1.0.0 (via pip) and nightly (ray-1.1.0.dev0)
 Python 3.7.5 (default, Nov  7 2019, 10:50:52) [GCC 8.3.0] on linux
 Ubuntu 18.04: Linux MIMIC-5530-UB1804 5.4.0-48-generic #52~18.04.1-Ubuntu SMP Thu Sep 10 12:50:22 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
 
 &lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;
 
 Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
 &lt;denchmark-link:https://github.com/ray-project/ray/files/5352388/torch_sac_alpha_unchanged.py.txt&gt;torch_sac_alpha_unchanged.py.txt&lt;/denchmark-link&gt;
 
 If we cannot run your script, we cannot fix your issue.
 
  I have verified my script runs in a clean environment and reproduces the issue.
  I have verified the issue also occurs with the latest wheels.
 
 	</description>
 	<comments>
 		<comment id='1' author='anqixu' date='2020-10-09T06:33:22Z'>
 		For a quick fix (PR to follow today):
 Could try creating the sale.log_alpha property inside sac_torch_model.py like so?:
 &lt;denchmark-code&gt;        log_alpha = nn.Parameter(
             torch.from_numpy(np.array([np.log(initial_alpha)])).float())
         self.register_parameter("log_alpha", log_alpha)
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='2' author='anqixu' date='2020-10-09T06:41:24Z'>
 		PR: &lt;denchmark-link:https://github.com/ray-project/ray/pull/11298&gt;#11298&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='3' author='anqixu' date='2020-10-09T07:57:04Z'>
 		&lt;denchmark-link:https://github.com/anqixu&gt;@anqixu&lt;/denchmark-link&gt;
 
 Thanks for filing this issue! :)
 		</comment>
 	</comments>
 </bug>
<commit id='f5e2cda68ae6035793686780c952d2083d98cf54' author='Sven Mika' date='2020-10-12 13:48:44-07:00'>
 	<dmm_unit complexity='0.1111111111111111' interfacing='0.8888888888888888' size='0.1111111111111111'></dmm_unit>
 	<modification change_type='MODIFY' old_name='rllib\agents\sac\sac_torch_model.py' new_name='rllib\agents\sac\sac_torch_model.py'>
 		<file_info nloc='193' complexity='10' token_count='843'></file_info>
 		<modified_lines>
 			<added_lines>134,135,136</added_lines>
 			<deleted_lines>134,135,136,137</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\agents\sac\tests\test_sac.py' new_name='rllib\agents\sac\tests\test_sac.py'>
 		<file_info nloc='439' complexity='58' token_count='3228'></file_info>
 		<method name='_translate_weights_to_torch' parameters='self,weights_dict,map_'>
 				<method_info nloc='9' complexity='5' token_count='77' nesting_level='1' start_line='518' end_line='526'></method_info>
 			<added_lines>521,522,524</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_sac_loss_function' parameters='self'>
 				<method_info nloc='248' complexity='43' token_count='1760' nesting_level='1' start_line='74' end_line='379'></method_info>
 			<added_lines>112,134,191,264,288,297,359,360,361,362,374,375,376,377</added_lines>
 			<deleted_lines>189,262,286,295,357,369</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>513,515</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
