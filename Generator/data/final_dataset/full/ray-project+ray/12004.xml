<bug_data>
<bug id='12004' author='PidgeyBE' open_date='2020-11-13T09:19:22Z' closed_time='2020-11-24T20:16:02Z'>
 	<summary>[autoscaler] Make NODE_START_WAIT_S configurable (for docker container nodes)</summary>
 	<description>
 &lt;denchmark-h:h3&gt;Describe your feature request&lt;/denchmark-h&gt;
 
 Currently the time it can take to start a new node in the autoscaler is limited to 300 seconds: 
 
 
 ray/python/ray/autoscaler/_private/command_runner.py
 
 
          Line 36
       in
       3b56a1a
 
 
 
 
 
 
  NODE_START_WAIT_S = 300 
 
 
 
 
 
 In some setups, where starting and configuring new nodes takes a few minutes and downloading docker images also takes some time, it can happen 300s is too short.
 So it would be nice if this constant would be configurable...
 	</description>
 	<comments>
 		<comment id='1' author='PidgeyBE' date='2020-11-13T21:58:38Z'>
 		cc &lt;denchmark-link:https://github.com/wuisawesome&gt;@wuisawesome&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='PidgeyBE' date='2020-11-14T00:05:42Z'>
 		This deadline should only apply to physical start of the instance (time until first SSH connection can be established). Post-setup steps like downloading images are after this step.
 		</comment>
 		<comment id='3' author='PidgeyBE' date='2020-11-14T00:32:41Z'>
 		&lt;denchmark-link:https://github.com/PidgeyBE&gt;@PidgeyBE&lt;/denchmark-link&gt;
  I guess one question here is what error message(s) are you seeing that lead you to believe this constant is the problem?
 		</comment>
 		<comment id='4' author='PidgeyBE' date='2020-11-14T17:48:41Z'>
 		&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
  Indeed that's correct
 &lt;denchmark-link:https://github.com/wuisawesome&gt;@wuisawesome&lt;/denchmark-link&gt;
  I dont remember exactly, I had to help out in a debugging sessions because our DevOps team was experiencing issues with it. The problem is we are using ray autoscaling on kubernetes and e.g. in google cloud we have an additional system which scales physical nodes when there is no space for additional pods anymore.
 So when Ray autoscaling decides to start a new GPU worker for example, first a new Google Cloud VM has to be started, NVIDIA drivers have to be installed etc, then the docker image for the worker pod has to be downloaded into the newly created VM and only after this the Pod becomes "ready". This whole proces (before SSHing into the pod becomes possible) takes longer than 300s.
 Currently our issue is resolved by patching the ray code (replacing  with ), but being able to configure it would be a lot cleaner... :)
 		</comment>
 	</comments>
 </bug>
<commit id='589555455548bca0aeaf434cdeb83f3bf4999446' author='Eric Liang' date='2020-11-24 12:16:01-08:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\_private\command_runner.py' new_name='python\ray\autoscaler\_private\command_runner.py'>
 		<file_info nloc='634' complexity='104' token_count='3918'></file_info>
 		<modified_lines>
 			<added_lines>17,18</added_lines>
 			<deleted_lines>17,36</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\_private\constants.py' new_name='python\ray\autoscaler\_private\constants.py'>
 		<file_info nloc='21' complexity='2' token_count='117'></file_info>
 		<modified_lines>
 			<added_lines>15,16,17</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
