<bug_data>
<bug id='2647' author='whikwon' open_date='2018-08-14T04:10:52Z' closed_time='2018-08-17T01:04:32Z'>
 	<summary>[rllib] Ape-X multiple VMs training issue</summary>
 	<description>
 &lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;
 
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04
 Ray installed from (source or binary): source
 Ray version: 0.5.0
 Python version: 3.6.3
 Exact command to reproduce:
 
 &lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;
 
 I made a cluster using 3 Azure VMs and ran the basic code of Ape-X DDPG.
 As soon as I execute the code, All CPUs start and stop immediately with no errors.
 Actually, when I ran the same code in one VM, it worked.
 &lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;
 
 import ray
 import ray.rllib.agents.ddpg as ddpg
 
 ray.init(&lt;head-ip:port&gt;)
 agent = ddpg.ApexDDPGAgent(config={'num_workers': 145,}, env='Pendulum-v0')
 
 for i in range(1000):
     result = agent.train()
     print('result: {}'.format(result))
 
     if i % 100 == 0:
         checkpoint = agent.save()
 	</description>
 	<comments>
 		<comment id='1' author='whikwon' date='2018-08-14T04:25:37Z'>
 		Hi &lt;denchmark-link:https://github.com/whikwon&gt;@whikwon&lt;/denchmark-link&gt;
 ,
 Can you verify that the ray cluster has actually been created? You can do this by posting the output of ray.global_state.client_table().
 Also, can you clarify by what you mean when the CPUs start and stop?
 This would help debug the issue. Thanks!
 		</comment>
 		<comment id='2' author='whikwon' date='2018-08-14T04:42:32Z'>
 		&lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
  Thanks for the quick response.
 I think cluster has been created. (picture below)
 &lt;denchmark-link:https://user-images.githubusercontent.com/30768596/44072013-68fe7248-9fc7-11e8-977e-6273fe661a40.png&gt;&lt;/denchmark-link&gt;
 
 I'm monitoring cpu usage using  top in console. when i execute the code above, all cpu go to 100% in a second and being dead.
 Anyway, all cpu don't work at all but code i executed doesn't generate any error.
 		</comment>
 		<comment id='3' author='whikwon' date='2018-08-14T04:50:41Z'>
 		No problem - does the code simply return? Or does it hang?
 It's possible that it's starting and waiting for actors (which may take some time initially).
 Also, it would be good to set OMP_NUM_THREADS as an environment variable, or else you could hit some OS thread limit.
 One place to check for errors is to tail /tmp/raylogs/worker*.
 		</comment>
 		<comment id='4' author='whikwon' date='2018-08-14T05:01:27Z'>
 		Just hang. I thought it would take a while but status doesn't change.
 &lt;denchmark-link:https://user-images.githubusercontent.com/30768596/44072525-4c76660a-9fca-11e8-9e70-bb0af21dd9f2.png&gt;&lt;/denchmark-link&gt;
 
 where should i have to give the OMP_NUM_THREADS?
 		</comment>
 		<comment id='5' author='whikwon' date='2018-08-14T05:05:24Z'>
 		Perhaps run something like `export OMP_NUM_THREADS=2` on each machine
 before initializing Ray (or after ray stop and initializing Ray again).
 &lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;
 
 
 On Mon, Aug 13, 2018 at 10:01 PM Whi Kwon ***@***.***&gt; wrote:
  Just hang. I thought it would take a while but status doesn't change.
 
  [image: image]
  &lt;https://user-images.githubusercontent.com/30768596/44072525-4c76660a-9fca-11e8-9e70-bb0af21dd9f2.png&gt;
 
  where should i have to give the OMP_NUM_THREADS?
 
  —
  You are receiving this because you were mentioned.
  Reply to this email directly, view it on GitHub
  &lt;#2647 (comment)&gt;,
  or mute the thread
  &lt;https://github.com/notifications/unsubscribe-auth/AEUc5R2Uxy8s-6KkbBKO3Q_CNMNf7xhxks5uQlmpgaJpZM4V7wzd&gt;
  .
 
 
 
 		</comment>
 		<comment id='6' author='whikwon' date='2018-08-14T05:12:16Z'>
 		Thanks. I tried but it doesn't work. OMG
 		</comment>
 		<comment id='7' author='whikwon' date='2018-08-14T19:59:39Z'>
 		Sorry to hear. What are the contents of the worker logs? tail /tmp/raylogs/worker*
 		</comment>
 		<comment id='8' author='whikwon' date='2018-08-14T21:56:35Z'>
 		It is possible you are running into this issue: &lt;denchmark-link:https://github.com/ray-project/ray/issues/2541&gt;#2541&lt;/denchmark-link&gt;
 
 One work around is to avoid launching that many workers, so if you have N virtual CPUs, use N/2 workers to avoid completely filling up the cluster. I usually find no performance benefit from using more than N/2 workers anyways, since model infererence is very floating point intensive and doesn't benefit from hyperthreading.
 		</comment>
 		<comment id='9' author='whikwon' date='2018-08-15T05:12:13Z'>
 		&lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
  Nothing special, tensorflow and gym warning.. something like that.
 		</comment>
 		<comment id='10' author='whikwon' date='2018-08-15T05:13:55Z'>
 		&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
  i had no problem running N virtual CPUs in one Azure Virtual Machine.
 Now, I'm using M virtual and tried M*N/2 workers but same situation happens. (No warning, No CPU usage. Just hanging.)
 		</comment>
 		<comment id='11' author='whikwon' date='2018-08-15T07:01:15Z'>
 		Do you get the hang with a very small number of workers, similar or less than that for a single machine?
 Trying to figure out if this is a multi node issue.
 		</comment>
 		<comment id='12' author='whikwon' date='2018-08-15T07:17:08Z'>
 		Oh btw, I would also switch to running the agent with Tune (see rllib training api doc for usage). That usually takes care of subtle resource allocation issues, and will print out the current usage as well.
 		</comment>
 		<comment id='13' author='whikwon' date='2018-08-15T20:41:33Z'>
 		&lt;denchmark-link:https://github.com/ray-project/ray/pull/2661&gt;#2661&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='14' author='whikwon' date='2018-08-17T01:04:32Z'>
 		Should be fixed -- feel free to reopen if it happens still.
 		</comment>
 	</comments>
 </bug>
<commit id='6670880f03bc6856342b6304cbf72fe4a8d78e4c' author='Eric Liang' date='2018-08-16 18:03:50-07:00'>
 	<dmm_unit complexity='0.0' interfacing='1.0' size='0.8421052631578947'></dmm_unit>
 	<modification change_type='MODIFY' old_name='python\ray\rllib\agents\dqn\dqn.py' new_name='python\ray\rllib\agents\dqn\dqn.py'>
 		<file_info nloc='174' complexity='26' token_count='1184'></file_info>
 		<method name='_init' parameters='self'>
 				<method_info nloc='27' complexity='6' token_count='205' nesting_level='1' start_line='122' end_line='163'></method_info>
 			<added_lines>140,141,142,143,144,145,146,147,148,149,150,151,152,153,157,158,159,160</added_lines>
 			<deleted_lines>140,141,142,143,144</deleted_lines>
 		</method>
 		<method name='_init.create_remote_evaluators' parameters=''>
 				<method_info nloc='7' complexity='1' token_count='44' nesting_level='2' start_line='141' end_line='147'></method_info>
 			<added_lines>141,142,143,144,145,146,147</added_lines>
 			<deleted_lines>141,142,143,144</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\rllib\optimizers\async_replay_optimizer.py' new_name='python\ray\rllib\optimizers\async_replay_optimizer.py'>
 		<file_info nloc='247' complexity='31' token_count='1618'></file_info>
 		<method name='step' parameters='self'>
 				<method_info nloc='14' complexity='3' token_count='108' nesting_level='1' start_line='214' end_line='227'></method_info>
 			<added_lines>215</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='set_evaluators' parameters='self,remote_evaluators'>
 				<method_info nloc='8' complexity='3' token_count='68' nesting_level='1' start_line='205' end_line='212'></method_info>
 			<added_lines>205,206</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>30,201,202,203,204</added_lines>
 			<deleted_lines>30,178</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='test\jenkins_tests\run_multi_node_tests.sh' new_name='test\jenkins_tests\run_multi_node_tests.sh'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>274,275,276,277,278,279,280,281,282,283,284,285,286,287,319</added_lines>
 			<deleted_lines>117,118,119,120,121,122,123,124,125,126,127,128,129,130,319</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
