<bug_data>
<bug id='7258' author='waldroje' open_date='2020-02-21T03:17:24Z' closed_time='2020-04-23T02:16:31Z'>
 	<summary>[tune][rllib] "node manager has mistakenly been marked dead by the monitor" - from long running PBT</summary>
 	<description>
 &lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;
 
 I was running PBT trials, and I thought after &lt;denchmark-link:https://github.com/ray-project/ray/pull/7201&gt;#7201&lt;/denchmark-link&gt;
  it might succeed, but after running for a decent amount of time, it crashed with the following.
 &lt;denchmark-code&gt;== Status == 
 Memory usage on this node: 58.9/184.7 GiB
 PopulationBasedTraining: 15 checkpoints, 10 perturbs
 Resources requested: 65/144 CPUs, 0.0/0 GPUs, 0.0/294.97 GiB heap, 0.0/43.65 GiB objects
 Result logdir: /home/ubuntu/ray_results/2020/2/20/220836984702/ip-172-31-5-2/front-time
 Number of trials: 8 (7 PAUSED, 1 RUNNING)
 +------------------------------+----------+------------------+--------+------------------+-------------+-----------+
 | Trial name                   | status   | loc              |   iter |   total time (s) |   timesteps |    reward |
 |------------------------------+----------+------------------+--------+------------------+-------------+-----------|
 | IMPALA_make_fpc_env_8b193446 | PAUSED   |                  |     50 |          2789.58 |     4183168 |  0.930425 |
 | IMPALA_make_fpc_env_8b193447 | PAUSED   |                  |     50 |          2977.83 |     4131712 | -0.544352 |
 | IMPALA_make_fpc_env_8b193448 | PAUSED   |                  |     50 |          2197.65 |     4217344 | -0.161039 |
 | IMPALA_make_fpc_env_8b193449 | PAUSED   |                  |     50 |          1814.8  |     5104000 |  0.203119 |
 | IMPALA_make_fpc_env_8b19344a | PAUSED   |                  |     50 |          2787.9  |     4180736 | -0.470465 |
 | IMPALA_make_fpc_env_8b19344b | PAUSED   |                  |     50 |          3216.9  |     4077312 | -7.46704  |
 | IMPALA_make_fpc_env_8b19344c | RUNNING  | 172.31.5.2:66208 |     49 |          2022.33 |     4590464 | -0.598401 |
 | IMPALA_make_fpc_env_8b19344d | PAUSED   |                  |     40 |          1456.58 |     4065408 | -2.61389  |
 +------------------------------+----------+------------------+--------+------------------+-------------+-----------+                                                                                                                                                                                                                                                                                                                                                                                                    
 E0221 00:59:27.506393 21075 task_manager.cc:165] Task failed: IOError: sent task to dead actor: Type=ACTOR_TASK, Language=PYTHON, function_descriptor={type=PythonFunctionDescriptor, module_name=ray.rllib.agents.trainer_template, class_name=IMPALA, function_name=stop, function_hash=}, task_id=53576960b66fdad15bba91dc0100, job_id=0100 , num_args=0, num_returns=2, actor_task_spec={actor_id=5bba91dc0100, actor_caller_id=ffffffffffffffffffffffff0100, actor_counter=11}                                    
 E0221 00:59:45.873986 21150 task_manager.cc:147] 3 retries left for task fffffffffffffffff9c5fe5b0100, attempting to resubmit.                                          
 E0221 00:59:45.874063 21150 core_worker.cc:196] Will resubmit task after a 5 second delay: Type=ACTOR_CREATION_TASK, Language=PYTHON, function_descriptor={type=PythonF unctionDescriptor, module_name=ray.rllib.agents.trainer_template, class_name=IMPALA, function_name=__init__, function_hash=230012600}, task_id=fffffffffffffffff9c5fe5b 0100, job_id=0100, num_args=4, num_returns=1, actor_creation_task_spec={actor_id=f9c5fe5b0100, max_reconstructions=0, is_direct_call=1, max_concurrency=1, is_asyncio_a ctor=0, is_detached=0}                                                                                                                                                  
 F0221 00:59:51.171756 21150 direct_task_transport.cc:153] Lost connection with local raylet. 
 Error: IOError: 14: failed to connect to all addresses                    
  *** Check failure stack trace: *** 
 @     0x7feef24a5d8d  google::LogMessage::Fail()
 @     0x7feef24a71fc  google::LogMessage::SendToLog()
 @     0x7feef24a5a69  google::LogMessage::Flush()
 @     0x7feef24a5c81  google::LogMessage::~LogMessage()
 @     0x7feef2252fa9  ray::RayLog::~RayLog()
 @     0x7feef213e349  ray::CoreWorkerDirectTaskSubmitter::RetryLeaseRequest()
 @     0x7feef2140b07 _ZNSt17_Function_handlerIFvRKN3ray6StatusERKNS0_3rpc23RequestWorkerLeaseReplyEEZNS0_29CoreWorkerDirectTaskSubmitter24RequestNewWorkerIfNeededERKSt5tupleIIiSt6vectorINS0_8ObjectIDESaISC_EENS0_7ActorIDEEEPKNS4_7AddressEEUlS3_S7_E_E9_M_invokeERKSt9_Any_dataS3_S7_
 @     0x7feef2177ea5  ray::rpc::ClientCallImpl&lt;&gt;::OnReplyReceived()                                                                                                     @     0x7feef2106993 _ZN5boost4asio6detail18completion_handlerIZN3ray3rpc17ClientCallManager29PollEventsFromCompletionQueueEiEUlvE_E11do_completeEPvPNS1_19schedul er_operationERKNS_6system10error_codeEm
 @     0x7feef2104e35  boost::asio::detail::scheduler::run()
 @     0x7feef210a3b3  ray::CoreWorker::RunIOService()
 @     0x7fee4ce789e0  (unknown)
 @     0x7feef6db56db  start_thread
 @     0x7feef70ee88f  clone
 Aborted (core dumped)
 --
 &lt;/denchmark-code&gt;
 
 Any insight? I'm running 2 x c5.18xlarge, no gpus, as my custom model is not very deep, and I've run this successfully in the past, definitely on 0,7,7,, but have generally been getting failure ever since 0.8.0,
 A couple items from logs... working up from bottom in terms of recent output.
 raylet.out
 &lt;denchmark-code&gt;F0221 00:59:33.108211  4371 node_manager.cc:472]  Check failed: node_id != self_node_id_ Exiting because this node manager has mistakenly been marked dead by the monitor.
 --
 
 &lt;/denchmark-code&gt;
 
 python-driver
 &lt;denchmark-code&gt;I0221 00:58:42.699936 21150 direct_actor_transport.cc:101] Failing pending tasks for actor a63d60130100
 I0221 00:58:42.699982 21150 core_worker.cc:897] received notification on actor, state=2, actor_id: a63d60130100, ip address: 172.31.5.2, port: 42913, worker_id: 4537e1 998f60fa3e5576a978b514b23fa9ac5713, raylet_id: c32ef7252b22fde98d0adc211081baa441a44b37
 I0221 00:58:42.723261 21150 direct_task_transport.cc:34] Connected to 172.31.5.2:38085
 I0221 00:58:47.063997 21150 core_worker.cc:897] received notification on actor, state=0, actor_id: 7982f88e0100, ip address: 172.31.5.2, port: 38085, worker_id: 47caa0 115cb69ec7c8bb3f7ffd21c1b0925dbaed, raylet_id: c32ef7252b22fde98d0adc211081baa441a44b37
 I0221 00:59:18.804953 21150 direct_actor_transport.cc:101] Failing pending tasks for actor 7982f88e0100
 I0221 00:59:18.804987 21150 core_worker.cc:897] received notification on actor, state=2, actor_id: 7982f88e0100, ip address: 172.31.5.2, port: 38085, worker_id: 47caa0 115cb69ec7c8bb3f7ffd21c1b0925dbaed, raylet_id: c32ef7252b22fde98d0adc211081baa441a44b37
 I0221 00:59:18.805001 21150 direct_actor_transport.cc:101] Failing pending tasks for actor 5bba91dc0100
 I0221 00:59:18.805004 21150 core_worker.cc:897] received notification on actor, state=2, actor_id: 5bba91dc0100, ip address: 172.31.5.2, port: 37067, worker_id: 9c5f44 9e201ec97bb8ccc771ff560db5a463ef56, raylet_id: c32ef7252b22fde98d0adc211081baa441a44b37
 E0221 00:59:19.567294 21075 task_manager.cc:165] Task failed: IOError: sent task to dead actor: Type=ACTOR_TASK, Language=PYTHON, function_descriptor={type=PythonFunct ionDescriptor, module_name=ray.rllib.agents.trainer_template, class_name=IMPALA, function_name=save_to_object, function_hash=}, task_id=4703b247b808e1c97982f88e0100, j ob_id=0100, num_args=0, num_returns=2, actor_task_spec={actor_id=7982f88e0100, actor_caller_id=ffffffffffffffffffffffff0100, actor_counter=3}
 E0221 00:59:19.571149 21075 task_manager.cc:165] Task failed: IOError: sent task to dead actor: Type=ACTOR_TASK, Language=PYTHON, function_descriptor={type=PythonFunct ionDescriptor, module_name=ray.rllib.agents.trainer_template, class_name=IMPALA, function_name=stop, function_hash=}, task_id=4d47f048bdbaeda87982f88e0100, job_id=0100 , num_args=0, num_returns=2, actor_task_spec={actor_id=7982f88e0100, actor_caller_id=ffffffffffffffffffffffff0100, actor_counter=4}
 W0221 00:59:20.355378 21075 plasma_store_provider.cc:66] Trying to put an object that already existed in plasma: ab2b067417969e8c4756862d010000c801000000.
 W0221 00:59:21.368199 21075 plasma_store_provider.cc:66] Trying to put an object that already existed in plasma: 35ba21f43e8607d08778027a010000c801000000.
 W0221 00:59:22.377468 21075 plasma_store_provider.cc:66] Trying to put an object that already existed in plasma: bdd16c3b71214fe0686a406a010000c801000000.
 W0221 00:59:23.761042 21075 plasma_store_provider.cc:66] Trying to put an object that already existed in plasma: acd84b982f24b7602203123e010000c801000000.
 W0221 00:59:25.134455 21075 plasma_store_provider.cc:66] Trying to put an object that already existed in plasma: d271658bf2072ba67132dc3a010000c801000000.
 W0221 00:59:26.955770 21075 plasma_store_provider.cc:66] Trying to put an object that already existed in plasma: 3ab66d48d45f6d4ebc823686010000c801000000.
 W0221 00:59:27.456938 21075 plasma_store_provider.cc:66] Trying to put an object that already existed in plasma: 525ad1491b5cce6f34efea21010000c801000000.
 E0221 00:59:27.506393 21075 task_manager.cc:165] Task failed: IOError: sent task to dead actor: Type=ACTOR_TASK, Language=PYTHON, function_descriptor={type=PythonFunct ionDescriptor, module_name=ray.rllib.agents.trainer_template, class_name=IMPALA, function_name=stop, function_hash=}, task_id=53576960b66fdad15bba91dc0100, job_id=0100 , num_args=0, num_returns=2, actor_task_spec={actor_id=5bba91dc0100, actor_caller_id=ffffffffffffffffffffffff0100, actor_counter=11}
 I0221 00:59:31.302026 21150 direct_task_transport.cc:34] Connected to 172.31.5.2:41377
 E0221 00:59:45.873986 21150 task_manager.cc:147] 3 retries left for task fffffffffffffffff9c5fe5b0100, attempting to resubmit.
 E0221 00:59:45.874063 21150 core_worker.cc:196] Will resubmit task after a 5 second delay: Type=ACTOR_CREATION_TASK, Language=PYTHON, function_descriptor={type=PythonF unctionDescriptor, module_name=ray.rllib.agents.trainer_template, class_name=IMPALA, function_name=__init__, function_hash=230012600}, task_id=fffffffffffffffff9c5fe5b 0100, job_id=0100, num_args=4, num_returns=1, actor_creation_task_spec={actor_id=f9c5fe5b0100, max_reconstructions=0, is_direct_call=1, max_concurrency=1, is_asyncio_a ctor=0, is_detached=0}
 I0221 00:59:45.874127 21150 raylet_client.cc:364] Error returning worker: IOError: 14: failed to connect to all addresses
 F0221 00:59:51.171756 21150 direct_task_transport.cc:153] Lost connection with local raylet. Error: IOError: 14: failed to connect to all addresses
 --
 &lt;/denchmark-code&gt;
 
 Happy to attach more info if you think it will help... let me know.  I can't provide scripts as it's all off custom models/envs...
 Ray version and other system information (Python version, TensorFlow version, OS):
 ray 0.9.0, python 3.6.8, tensorflow 2.1, aws ubuntu 18.04
 &lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;
 
 Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
 If we cannot run your script, we cannot fix your issue.
 
  I have verified my script runs in a clean environment and reproduces the issue.
 [x ] I have verified the issue also occurs with the latest wheels.
 
 	</description>
 	<comments>
 		<comment id='1' author='waldroje' date='2020-02-21T18:18:08Z'>
 		cc &lt;denchmark-link:https://github.com/edoakes&gt;@edoakes&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='waldroje' date='2020-02-21T19:25:01Z'>
 		&lt;denchmark-link:https://github.com/waldroje&gt;@waldroje&lt;/denchmark-link&gt;
  can you make a reproduction script with a standard gym env?
 		</comment>
 		<comment id='3' author='waldroje' date='2020-02-21T19:57:00Z'>
 		i'll try and see if I can replicate the behavior... will revert
 		</comment>
 		<comment id='4' author='waldroje' date='2020-02-25T12:34:34Z'>
 		So.. I'm going back to square one, and going to build up from simple cases to more complex.... I have installed 0.8.2,
 I created a simple pbt with a toy gym env that I can use in more complex cases... but I'm getting errors on what appears to be checkpoint intervals on the most basic case.  It seems the fetch_result is returning a checkpoint string rather than a result dictionary
 Do you get the same behavior?
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "/home/svc-tai-dev/virt/algo_36/lib64/python3.6/site-packages/ray/tune/trial_runner.py", line 536, in _process_trial_save
     trial.on_checkpoint(trial.saving_to)
   File "/home/svc-tai-dev/virt/algo_36/lib64/python3.6/site-packages/ray/tune/trial.py", line 400, in on_checkpoint
     if not os.path.exists(checkpoint.value):
   File "/usr/lib64/python3.6/genericpath.py", line 19, in exists
     os.stat(path)
 TypeError: stat: path should be string, bytes, os.PathLike or integer, not dict
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-link:https://github.com/ray-project/ray/files/4249964/pbt_dqn_test.zip&gt;pbt_dqn_test.zip&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='5' author='waldroje' date='2020-02-29T17:16:35Z'>
 		I tested the attached script from my previous post on the latest wheel and it still errors... any ideas on this?
 		</comment>
 		<comment id='6' author='waldroje' date='2020-02-29T17:58:59Z'>
 		OK thanks a lot for the script! Let me try running this; I think this could be an bug with how we handle checkpoints.
 cc &lt;denchmark-link:https://github.com/ujvl&gt;@ujvl&lt;/denchmark-link&gt;
 
 BTW, can you join the Ray slack so we can communicate faster?
 		</comment>
 		<comment id='7' author='waldroje' date='2020-02-29T18:43:40Z'>
 		fwiw... i see the problem in trial_runner._process_trial_save where
 checkpoint_value = self.trial_executor.fetch_result(trial) 
 where checkpoint_value is supposed to be a path... but sometimes comes back as trial results... which then cascades into the errors...
 yes, I will get up on Slack...
 		</comment>
 		<comment id='8' author='waldroje' date='2020-03-01T10:57:43Z'>
 		looks like an edge case where the checkpoint interval and perturbation interval coincide, causing PBT to restart the trial (resulting in a train call dispatch) and tune to dispatch a save call in the same step. which violates the invariant that only 1 future per trial is queued.
 imo we should consider refactoring pbt in a way that isn't directly interacting with the executor. like at most it should be calling back into trial_runner
 as a more immediate fix we could just check the type of the return value before figuring out how to process it (ie before calling process_{save/train/restore}). or avoid calling save altogether if there's a future for the trial currently queued.
 i prefer the former but &lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
  lmk if you have any other ideas i can push a quick fix
 		</comment>
 		<comment id='9' author='waldroje' date='2020-03-01T23:57:37Z'>
 		Shouldn't the immediate fix be that PBT has a way to call "save" without affecting the event loop?
 		</comment>
 		<comment id='10' author='waldroje' date='2020-03-02T00:30:04Z'>
 		pbt shouldn't be responsible for FT (memory checkpoints already don't affect the event loop). if you mean give PBT a way to start/restore trials without affecting the event loop then i agree
 		</comment>
 		<comment id='11' author='waldroje' date='2020-03-21T15:04:20Z'>
 		Is there a quick fix on this that you can suggest which would enable me to use pbt on the current master?
 		</comment>
 		<comment id='12' author='waldroje' date='2020-04-22T03:06:23Z'>
 		We just merged a fix - so the second issue in the thread should be resolved. &lt;denchmark-link:https://github.com/waldroje&gt;@waldroje&lt;/denchmark-link&gt;
  are you still seeing this issue?
 		</comment>
 		<comment id='13' author='waldroje' date='2020-04-22T03:19:51Z'>
 		I saw that come through and merged it in and kicked off a run earlier… so far no problems, will let you know if it finishes w/ no problems.
 
 On Apr 21, 2020, at 11:06 PM, Richard Liaw &lt;notifications@github.com&gt; wrote:
 
 
 We just merged a fix - so the second issue in the thread should be resolved. &lt;denchmark-link:https://github.com/waldroje&gt;@waldroje&lt;/denchmark-link&gt;
  &lt;&lt;denchmark-link:https://github.com/waldroje&gt;https://github.com/waldroje&lt;/denchmark-link&gt;
 &gt; are you still seeing this issue?
 
 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub &lt;&lt;denchmark-link:https://github.com/ray-project/ray/issues/7258#issuecomment-617521371&gt;#7258 (comment)&lt;/denchmark-link&gt;
 &gt;, or unsubscribe &lt;&lt;denchmark-link:https://github.com/notifications/unsubscribe-auth/AHSHWMPA2V6FJJWNGFVGD73RNZNLZANCNFSM4KY3LNVA&gt;https://github.com/notifications/unsubscribe-auth/AHSHWMPA2V6FJJWNGFVGD73RNZNLZANCNFSM4KY3LNVA&lt;/denchmark-link&gt;
 &gt;.
 		</comment>
 		<comment id='14' author='waldroje' date='2020-04-22T04:04:06Z'>
 		Awesome! sorry for the delay.
 &lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;
 
 
 On Tue, Apr 21, 2020 at 8:20 PM waldroje ***@***.***&gt; wrote:
  I saw that come through and merged it in and kicked off a run earlier… so
  far no problems, will let you know if it finishes w/ no problems.
 
  On Apr 21, 2020, at 11:06 PM, Richard Liaw ***@***.***&gt;
  wrote:
 
 
  We just merged a fix - so the second issue in the thread should be
  resolved. @waldroje &lt;https://github.com/waldroje&gt; are you still seeing
  this issue?
 
  —
  You are receiving this because you were mentioned.
  Reply to this email directly, view it on GitHub &lt;
  #7258 (comment)&gt;,
  or unsubscribe &lt;
  https://github.com/notifications/unsubscribe-auth/AHSHWMPA2V6FJJWNGFVGD73RNZNLZANCNFSM4KY3LNVA
  &gt;.
 
 
  —
  You are receiving this because you were mentioned.
  Reply to this email directly, view it on GitHub
  &lt;#7258 (comment)&gt;,
  or unsubscribe
  &lt;https://github.com/notifications/unsubscribe-auth/ABCRZZP6X62Q2CUW45B3MM3RNZO6HANCNFSM4KY3LNVA&gt;
  .
 
 
 
 		</comment>
 		<comment id='15' author='waldroje' date='2020-04-23T01:31:52Z'>
 		No issues after running all day… much appreciated!
 &lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;
 
 
 On Apr 22, 2020, at 12:04 AM, Richard Liaw ***@***.***&gt; wrote:
 
  Awesome! sorry for the delay.
 
  On Tue, Apr 21, 2020 at 8:20 PM waldroje ***@***.***&gt; wrote:
 
  &gt; I saw that come through and merged it in and kicked off a run earlier… so
  &gt; far no problems, will let you know if it finishes w/ no problems.
  &gt;
  &gt; On Apr 21, 2020, at 11:06 PM, Richard Liaw ***@***.***&gt;
  &gt; wrote:
  &gt;
  &gt;
  &gt; We just merged a fix - so the second issue in the thread should be
  &gt; resolved. @waldroje &lt;https://github.com/waldroje&gt; are you still seeing
  &gt; this issue?
  &gt;
  &gt; —
  &gt; You are receiving this because you were mentioned.
  &gt; Reply to this email directly, view it on GitHub &lt;
  &gt; #7258 (comment)&gt;,
  &gt; or unsubscribe &lt;
  &gt; https://github.com/notifications/unsubscribe-auth/AHSHWMPA2V6FJJWNGFVGD73RNZNLZANCNFSM4KY3LNVA
  &gt; &gt;.
  &gt;
  &gt;
  &gt; —
  &gt; You are receiving this because you were mentioned.
  &gt; Reply to this email directly, view it on GitHub
  &gt; &lt;#7258 (comment)&gt;,
  &gt; or unsubscribe
  &gt; &lt;https://github.com/notifications/unsubscribe-auth/ABCRZZP6X62Q2CUW45B3MM3RNZO6HANCNFSM4KY3LNVA&gt;
  &gt; .
  &gt;
  —
  You are receiving this because you were mentioned.
  Reply to this email directly, view it on GitHub, or unsubscribe.
 
 
 
 		</comment>
 		<comment id='16' author='waldroje' date='2020-04-23T02:16:30Z'>
 		Amazing! thanks for the long long wait.
 		</comment>
 	</comments>
 </bug>
<commit id='708dff6d8f7dd6f7919e06c1845f1fea0cca5b89' author='Ujval Misra' date='2020-04-20 15:10:36-07:00'>
 	<dmm_unit complexity='0.9647058823529412' interfacing='0.8470588235294118' size='0.4235294117647059'></dmm_unit>
 	<modification change_type='MODIFY' old_name='python\ray\tune\durable_trainable.py' new_name='python\ray\tune\durable_trainable.py'>
 		<file_info nloc='60' complexity='9' token_count='303'></file_info>
 		<method name='save' parameters='self,checkpoint_dir'>
 				<method_info nloc='9' complexity='3' token_count='72' nesting_level='1' start_line='41' end_line='64'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>60</deleted_lines>
 		</method>
 		<method name='delete_checkpoint' parameters='self,checkpoint_path'>
 				<method_info nloc='10' complexity='2' token_count='58' nesting_level='1' start_line='80' end_line='94'></method_info>
 			<added_lines>86,87,88,89,90,91,92,93</added_lines>
 			<deleted_lines>85,86</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1,7,8</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\tune\examples\pbt_example.py' new_name='python\ray\tune\examples\pbt_example.py'>
 		<file_info nloc='97' complexity='8' token_count='467'></file_info>
 		<modified_lines>
 			<added_lines>111</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\tune\ray_trial_executor.py' new_name='python\ray\tune\ray_trial_executor.py'>
 		<file_info nloc='498' complexity='115' token_count='3296'></file_info>
 		<method name='_start_trial' parameters='self,trial,checkpoint,runner,train'>
 				<method_info nloc='14' complexity='7' token_count='133' nesting_level='1' start_line='216' end_line='244'></method_info>
 			<added_lines>216,226,243</added_lines>
 			<deleted_lines>216,242</deleted_lines>
 		</method>
 		<method name='reset_trial' parameters='self,trial,new_config,new_experiment_tag'>
 				<method_info nloc='15' complexity='2' token_count='75' nesting_level='1' start_line='342' end_line='366'></method_info>
 			<added_lines>347,348</added_lines>
 			<deleted_lines>345,346,347,348</deleted_lines>
 		</method>
 		<method name='_start_trial' parameters='self,trial,checkpoint,runner'>
 				<method_info nloc='14' complexity='6' token_count='127' nesting_level='1' start_line='216' end_line='243'></method_info>
 			<added_lines>216,226,243</added_lines>
 			<deleted_lines>216,242</deleted_lines>
 		</method>
 		<method name='restore' parameters='self,trial,checkpoint'>
 				<method_info nloc='32' complexity='8' token_count='201' nesting_level='1' start_line='636' end_line='684'></method_info>
 			<added_lines>636,644,684</added_lines>
 			<deleted_lines>636,683,684</deleted_lines>
 		</method>
 		<method name='restore' parameters='self,trial,checkpoint,block'>
 				<method_info nloc='35' complexity='9' token_count='216' nesting_level='1' start_line='636' end_line='689'></method_info>
 			<added_lines>636,644,684,685,686,687,688,689</added_lines>
 			<deleted_lines>636,683,684</deleted_lines>
 		</method>
 		<method name='start_trial' parameters='self,trial,checkpoint'>
 				<method_info nloc='16' complexity='3' token_count='106' nesting_level='1' start_line='281' end_line='305'></method_info>
 			<added_lines>282,291,295</added_lines>
 			<deleted_lines>281,293</deleted_lines>
 		</method>
 		<method name='start_trial' parameters='self,trial,checkpoint,train'>
 				<method_info nloc='16' complexity='3' token_count='114' nesting_level='1' start_line='282' end_line='307'></method_info>
 			<added_lines>282,291,295</added_lines>
 			<deleted_lines>293</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\tune\schedulers\pbt.py' new_name='python\ray\tune\schedulers\pbt.py'>
 		<file_info nloc='314' complexity='47' token_count='1623'></file_info>
 		<method name='choose_trial_to_run' parameters='self,trial_runner'>
 				<method_info nloc='9' complexity='5' token_count='79' nesting_level='1' start_line='364' end_line='379'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>369,371</deleted_lines>
 		</method>
 		<method name='_exploit' parameters='self,trial_executor,trial,trial_to_clone'>
 				<method_info nloc='32' complexity='4' token_count='199' nesting_level='1' start_line='299' end_line='343'></method_info>
 			<added_lines>326,327,328,329,330,332,333,338,339</added_lines>
 			<deleted_lines>304,306,330,335</deleted_lines>
 		</method>
 		<method name='_quantiles' parameters='self'>
 				<method_info nloc='15' complexity='6' token_count='137' nesting_level='1' start_line='341' end_line='362'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>345,347</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>273</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\tune\tests\test_trial_scheduler.py' new_name='python\ray\tune\tests\test_trial_scheduler.py'>
 		<file_info nloc='1129' complexity='181' token_count='9773'></file_info>
 		<method name='testCheckpointDict._train' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='13' nesting_level='3' start_line='1177' end_line='1178'></method_info>
 			<added_lines>1177,1178</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='testCheckpointDict._setup' parameters='self,config'>
 				<method_info nloc='2' complexity='1' token_count='16' nesting_level='3' start_line='1174' end_line='1175'></method_info>
 			<added_lines>1174,1175</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='testCheckpointDict._restore' parameters='self,state'>
 				<method_info nloc='2' complexity='1' token_count='12' nesting_level='3' start_line='1183' end_line='1184'></method_info>
 			<added_lines>1183,1184</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='testCheckpointing._train' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='13' nesting_level='3' start_line='1142' end_line='1143'></method_info>
 			<added_lines>1142,1143</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='testCheckpointDict._save' parameters='self,path'>
 				<method_info nloc='2' complexity='1' token_count='11' nesting_level='3' start_line='1180' end_line='1181'></method_info>
 			<added_lines>1180,1181</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='start_trial' parameters='self,trial,checkpoint_obj'>
 				<method_info nloc='4' complexity='1' token_count='30' nesting_level='1' start_line='189' end_line='192'></method_info>
 			<added_lines>190</added_lines>
 			<deleted_lines>189</deleted_lines>
 		</method>
 		<method name='basicSetup' parameters='self,resample_prob,explore,perturbation_interval,log_config,hyperparams,hyperparam_mutations,step_once'>
 				<method_info nloc='8' complexity='1' token_count='35' nesting_level='1' start_line='1114' end_line='1121'></method_info>
 			<added_lines>1114,1115,1116,1117,1118,1119,1120,1121</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='testCheckpointing._save' parameters='self,path'>
 				<method_info nloc='5' complexity='1' token_count='30' nesting_level='3' start_line='1145' end_line='1149'></method_info>
 			<added_lines>1145,1146,1147,1148,1149</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='setUp' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='13' nesting_level='1' start_line='1107' end_line='1108'></method_info>
 			<added_lines>1107,1108</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='testCheckpointDict' parameters='self'>
 				<method_info nloc='23' complexity='2' token_count='113' nesting_level='1' start_line='1170' end_line='1203'></method_info>
 			<added_lines>1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='testCheckpointing' parameters='self'>
 				<method_info nloc='21' complexity='2' token_count='109' nesting_level='1' start_line='1138' end_line='1168'></method_info>
 			<added_lines>1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='restore' parameters='self,trial,checkpoint,block'>
 				<method_info nloc='2' complexity='1' token_count='16' nesting_level='1' start_line='200' end_line='201'></method_info>
 			<added_lines>200</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='restore' parameters='self,trial,checkpoint'>
 				<method_info nloc='2' complexity='1' token_count='12' nesting_level='1' start_line='199' end_line='200'></method_info>
 			<added_lines>200</added_lines>
 			<deleted_lines>199</deleted_lines>
 		</method>
 		<method name='start_trial' parameters='self,trial,checkpoint_obj,train'>
 				<method_info nloc='4' complexity='1' token_count='34' nesting_level='1' start_line='190' end_line='193'></method_info>
 			<added_lines>190</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='tearDown' parameters='self'>
 				<method_info nloc='3' complexity='1' token_count='13' nesting_level='1' start_line='1110' end_line='1112'></method_info>
 			<added_lines>1110,1111,1112</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>12,1106,1109,1113,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1169,1204,1205</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\tune\trainable.py' new_name='python\ray\tune\trainable.py'>
 		<file_info nloc='328' complexity='62' token_count='2081'></file_info>
 		<method name='default_resource_request' parameters='cls,config'>
 				<method_info nloc='2' complexity='1' token_count='10' nesting_level='1' start_line='186' end_line='205'></method_info>
 			<added_lines>189</added_lines>
 			<deleted_lines>189</deleted_lines>
 		</method>
 		<method name='training_iteration' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='10' nesting_level='1' start_line='559' end_line='566'></method_info>
 			<added_lines>559,560,561,562,563,564,565,566</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>558,567</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\tune\trial.py' new_name='python\ray\tune\trial.py'>
 		<file_info nloc='420' complexity='94' token_count='2474'></file_info>
 		<modified_lines>
 			<added_lines>111,194</added_lines>
 			<deleted_lines>111,194,195</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\tune\trial_executor.py' new_name='python\ray\tune\trial_executor.py'>
 		<file_info nloc='113' complexity='36' token_count='686'></file_info>
 		<method name='restore' parameters='self,trial,checkpoint'>
 				<method_info nloc='3' complexity='1' token_count='18' nesting_level='1' start_line='214' end_line='228'></method_info>
 			<added_lines>215,224</added_lines>
 			<deleted_lines>214</deleted_lines>
 		</method>
 		<method name='start_trial' parameters='self,trial,checkpoint'>
 				<method_info nloc='3' complexity='1' token_count='18' nesting_level='1' start_line='75' end_line='84'></method_info>
 			<added_lines>75,82</added_lines>
 			<deleted_lines>75</deleted_lines>
 		</method>
 		<method name='start_trial' parameters='self,trial,checkpoint,train'>
 				<method_info nloc='3' complexity='1' token_count='22' nesting_level='1' start_line='75' end_line='85'></method_info>
 			<added_lines>75,82</added_lines>
 			<deleted_lines>75</deleted_lines>
 		</method>
 		<method name='restore' parameters='self,trial,checkpoint,block'>
 				<method_info nloc='3' complexity='1' token_count='22' nesting_level='1' start_line='215' end_line='230'></method_info>
 			<added_lines>215,224</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
