<bug_data>
<bug id='12466' author='liside' open_date='2020-11-28T04:06:29Z' closed_time='2020-12-20T05:46:34Z'>
 	<summary>[SGD] Inconsistent timeout units used in SGD</summary>
 	<description>
 &lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;
 
 Inconsistent timeout units used in SGD, making it unable to time out properly.
 TorchTrainer in RaySGD takes a parameter timeout_s:
 &lt;denchmark-code&gt;timeout_s (float): Seconds before the torch process group times out. Useful when machines are unreliable.
 &lt;/denchmark-code&gt;
 
 In the remote mode, TorchTrainer wraps timedelta around the given timeout in seconds to initialize workers. As per Python doc, timedelta turns . If one gives 10.0 to TorchTrainer, timedelta will turn it into 10 days, causing it nearly impossible to time out. See: &lt;denchmark-link:https://github.com/ray-project/ray/blob/master/python/ray/util/sgd/torch/worker_group.py#L178&gt;https://github.com/ray-project/ray/blob/master/python/ray/util/sgd/torch/worker_group.py#L178&lt;/denchmark-link&gt;
 
 Ray version and other system information (Python version, TensorFlow version, OS):
 Ray v1.0.0, PyTorch v1.7.0, Ubuntu v18.04
 &lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;
 
 Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
 Any tutorial code using remote workers would reproduce this bug.
 If we cannot run your script, we cannot fix your issue.
 
  I have verified my script runs in a clean environment and reproduces the issue.
  I have verified the issue also occurs with the latest wheels.
 
 	</description>
 	<comments>
 		<comment id='1' author='liside' date='2020-11-28T18:11:17Z'>
 		cc &lt;denchmark-link:https://github.com/amogkam&gt;@amogkam&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='liside' date='2020-11-29T05:11:55Z'>
 		This is a good catch &lt;denchmark-link:https://github.com/liside&gt;@liside&lt;/denchmark-link&gt;
 ! Should be fixed in &lt;denchmark-link:https://github.com/ray-project/ray/pull/12477&gt;#12477&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='3' author='liside' date='2020-11-29T05:52:20Z'>
 		You might want to change this line as well: &lt;denchmark-link:https://github.com/ray-project/ray/blob/master/python/ray/util/sgd/torch/worker_group.py#L470&gt;https://github.com/ray-project/ray/blob/master/python/ray/util/sgd/torch/worker_group.py#L470&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='4' author='liside' date='2020-11-29T06:12:11Z'>
 		Oh right another good catch ;)
 		</comment>
 	</comments>
 </bug>
<commit id='51139ed37c5a64a87c916e6a6675385f9b700535' author='Amog Kamsetty' date='2020-12-19 21:46:33-08:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='python\ray\util\sgd\torch\constants.py' new_name='python\ray\util\sgd\torch\constants.py'>
 		<file_info nloc='11' complexity='0' token_count='41'></file_info>
 		<modified_lines>
 			<added_lines>9</added_lines>
 			<deleted_lines>9</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\util\sgd\torch\torch_trainer.py' new_name='python\ray\util\sgd\torch\torch_trainer.py'>
 		<file_info nloc='497' complexity='56' token_count='2098'></file_info>
 		<method name='__init__' parameters='self,training_operator_cls,initialization_hook,config,num_workers,num_cpus_per_worker,use_gpu,backend,wrap_ddp,timeout_s,use_fp16,use_tqdm,add_dist_sampler,scheduler_step_freq,use_local,num_replicas,batch_size,model_creator,data_creator,optimizer_creator,scheduler_creator,loss_creator,serialize_data_creation,data_loader_args,apex_args'>
 				<method_info nloc='27' complexity='1' token_count='100' nesting_level='1' start_line='137' end_line='164'></method_info>
 			<added_lines>148</added_lines>
 			<deleted_lines>146</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,training_operator_cls,initialization_hook,config,num_workers,num_cpus_per_worker,use_gpu,backend,wrap_ddp,timeout_s,use_fp16,use_tqdm,add_dist_sampler,scheduler_step_freq,use_local,num_replicas,batch_size,model_creator,data_creator,optimizer_creator,scheduler_creator,loss_creator,serialize_data_creation,data_loader_args,apex_args'>
 				<method_info nloc='27' complexity='1' token_count='100' nesting_level='1' start_line='135' end_line='162'></method_info>
 			<added_lines>148</added_lines>
 			<deleted_lines>146</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>108,109,110,235,236,237</added_lines>
 			<deleted_lines>108</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\util\sgd\torch\worker_group.py' new_name='python\ray\util\sgd\torch\worker_group.py'>
 		<file_info nloc='403' complexity='103' token_count='2623'></file_info>
 		<method name='_setup_process_group' parameters='self,address,world_size,starting_rank'>
 				<method_info nloc='10' complexity='2' token_count='62' nesting_level='1' start_line='153' end_line='181'></method_info>
 			<added_lines>178</added_lines>
 			<deleted_lines>178</deleted_lines>
 		</method>
 		<method name='start_workers' parameters='self,num_workers'>
 				<method_info nloc='35' complexity='4' token_count='232' nesting_level='1' start_line='438' end_line='481'></method_info>
 			<added_lines>470</added_lines>
 			<deleted_lines>470</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
