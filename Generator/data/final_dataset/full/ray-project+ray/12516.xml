<bug_data>
<bug id='12516' author='hybug' open_date='2020-12-01T03:18:30Z' closed_time='2020-12-09T13:20:15Z'>
 	<summary>[rllib] Trainer.compute_action Error with Dict type observation inputs</summary>
 	<description>
 &lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;
 
 Python: 3.8.5,
 TensorFlow: tensorflow-gpu 2.0.0
 Ray: ray 1.0.1 &amp; ray 0.8.6
 I want to reproduce the code of this blog, but I got an error.  &lt;denchmark-link:https://towardsdatascience.com/action-masking-with-rllib-5e4bec5e7505&gt;Action Masking with RLlib&lt;/denchmark-link&gt;
 
 &lt;denchmark-h:h3&gt;Training Script&lt;/denchmark-h&gt;
 
 Here is the code script. use pip install or_gym first
 &lt;denchmark-code&gt;from or_gym.utils import create_env
 from gym import spaces
 from ray.rllib.utils import try_import_tf
 from ray.rllib.models.tf.fcnet import FullyConnectedNetwork
 from ray.rllib.models.tf.tf_modelv2 import TFModelV2
 from ray.rllib.models import ModelCatalog
 from ray import tune
 from ray.rllib import agents
 import ray
 import or_gym
 import numpy as np
 env = or_gym.make('Knapsack-v0')
 
 print("Max weight capacity:\t{}kg".format(env.max_weight))
 print("Number of items:\t{}".format(env.N))
 
 env_config = {'N': 5,
               'max_weight': 15,
               'item_weights': np.array([1, 12, 2, 1, 4]),
               'item_values': np.array([2, 4, 2, 1, 10]),
               'mask': True}
 env = or_gym.make('Knapsack-v0', env_config=env_config)
 print("Max weight capacity:\t{}kg".format(env.max_weight))
 print("Number of items:\t{}".format(env.N))
 
 tf = try_import_tf()
 # tf.compat.v1.disable_eager_execution()
 
 
 class KP0ActionMaskModel(TFModelV2):
 
     def __init__(self, obs_space, action_space, num_outputs,
                  model_config, name, true_obs_shape=(11,),
                  action_embed_size=5, *args, **kwargs):
 
         super(KP0ActionMaskModel, self).__init__(obs_space,
                                                  action_space, num_outputs, model_config, name,
                                                  *args, **kwargs)
 
         self.action_embed_model = FullyConnectedNetwork(
             spaces.Box(0, 1, shape=true_obs_shape),
             action_space, action_embed_size,
             model_config, name + "_action_embedding")
         self.register_variables(self.action_embed_model.variables())
 
     def forward(self, input_dict, state, seq_lens):
         avail_actions = input_dict["obs"]["avail_actions"]
         action_mask = input_dict["obs"]["action_mask"]
         action_embedding, _ = self.action_embed_model({
             "obs": input_dict["obs"]["state"]})
         intent_vector = tf.expand_dims(action_embedding, 1)
         action_logits = tf.math.reduce_sum(avail_actions * intent_vector,
                                            axis=1)
         inf_mask = tf.math.maximum(tf.math.log(action_mask), tf.float32.min)
         return action_logits + inf_mask, state
 
     def value_function(self):
         return self.action_embed_model.value_function()
 
 
 ModelCatalog.register_custom_model('kp_mask', KP0ActionMaskModel)
 
 
 def register_env(env_name, env_config={}):
     env = create_env(env_name)
     tune.register_env(env_name, lambda env_name: env(
         env_name, env_config=env_config))
 
 
 register_env('Knapsack-v0', env_config=env_config)
 
 
 ray.init(ignore_reinit_error=True)
 trainer_config = {
     "model": {
         "custom_model": "kp_mask"
         },
     "env_config": env_config
      }
 trainer = agents.ppo.PPOTrainer(env='Knapsack-v0', config=trainer_config)
 
 env = trainer.env_creator('Knapsack-v0')
 state = env.state
 state['action_mask'][0] = 0
 
 
 actions = np.array([trainer.compute_action(state) for i in range(10)])
 
 print(actions)
 &lt;/denchmark-code&gt;
 
 This script works fine in Ray0.8.7, but in Ray1.0.1 rasie Error. Because trainer.compute_action() canâ€™t deal with dict type input
 &lt;denchmark-h:h3&gt;Error&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "/data2/huangcq/miniconda3/envs/majenv/lib/python3.8/site-packages/ray/rllib/models/preprocessors.py", line 60, in check_shape
     if not self._obs_space.contains(observation):
   File "/data2/huangcq/miniconda3/envs/majenv/lib/python3.8/site-packages/gym/spaces/box.py", line 128, in contains
     return x.shape == self.shape and np.all(x &gt;= self.low) and np.all(x &lt;= self.high)
 AttributeError: 'dict' object has no attribute 'shape'
 
 During handling of the above exception, another exception occurred:
 
 Traceback (most recent call last):
   File "/notebooks/projects/hanyu/ReferProject/MahjongFastPK/test.py", line 96, in &lt;module&gt;
     actions = np.array([trainer.compute_action(state) for i in range(10)])
   File "/notebooks/projects/hanyu/ReferProject/MahjongFastPK/test.py", line 96, in &lt;listcomp&gt;
     actions = np.array([trainer.compute_action(state) for i in range(10)])
   File "/data2/huangcq/miniconda3/envs/majenv/lib/python3.8/site-packages/ray/rllib/agents/trainer.py", line 819, in compute_action
     preprocessed = self.workers.local_worker().preprocessors[
   File "/data2/huangcq/miniconda3/envs/majenv/lib/python3.8/site-packages/ray/rllib/models/preprocessors.py", line 166, in transform
     self.check_shape(observation)
   File "/data2/huangcq/miniconda3/envs/majenv/lib/python3.8/site-packages/ray/rllib/models/preprocessors.py", line 66, in check_shape
     raise ValueError(
 ValueError: ('Observation for a Box/MultiBinary/MultiDiscrete space should be an np.array, not a Python list.', {'action_mask': array([0, 1, 1, 1, 1]), 'avail_actions': array([1., 1., 1., 1., 1.]), 'state': array([ 1, 12,  2,  1,  4,  2,  4,  2,  1, 10,  0])})
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Question&lt;/denchmark-h&gt;
 
 The problem is code works fine in Ray 0.8.6, bug in Ray 1.0.1 raise the ValueError.
 So, what should i do to use compute_action() dealing with Dict type input in Ray 1.0.1?
 Thanks for any help!
 	</description>
 	<comments>
 		<comment id='1' author='hybug' date='2020-12-01T03:26:05Z'>
 		&lt;denchmark-link:https://github.com/sven1977&gt;@sven1977&lt;/denchmark-link&gt;
  Any suggestion will be helpful
 		</comment>
 		<comment id='2' author='hybug' date='2020-12-02T08:04:33Z'>
 		sven1977 ask me to assign this issue to him in discuss.ray.io, but i dont know how to assing issues. Could you help me assign this issues? &lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
   Thanks!
 		</comment>
 		<comment id='3' author='hybug' date='2020-12-04T08:39:46Z'>
 		Update: Ray 1.0.1 Raise Error, And Ray 1.0.0 works well &lt;denchmark-link:https://github.com/sven1977&gt;@sven1977&lt;/denchmark-link&gt;
 
 P.S.
 if use Ray 1.0.0 change tf = try_import_tf() to tf1, tf, tfv = try_import_tf() to aviod tf.expand_dims() Error
 		</comment>
 		<comment id='4' author='hybug' date='2020-12-09T08:16:23Z'>
 		Taking a look right now ...
 		</comment>
 		<comment id='5' author='hybug' date='2020-12-09T08:16:32Z'>
 		Thanks for filing this &lt;denchmark-link:https://github.com/hybug&gt;@hybug&lt;/denchmark-link&gt;
  !
 		</comment>
 		<comment id='6' author='hybug' date='2020-12-09T10:03:24Z'>
 		Ok, here is the fix PR: &lt;denchmark-link:https://github.com/ray-project/ray/pull/12706&gt;#12706&lt;/denchmark-link&gt;
 
 As a workaround, could you change the following in your rllib/evaluation/worker_set.py (look for the  if-block)?
 &lt;denchmark-code&gt;            # If num_workers &gt; 0, get the action_spaces and observation_spaces
             # to not be forced to create an Env on the driver.
             if self._remote_workers:
                 remote_spaces = ray.get(self.remote_workers(
                 )[0].foreach_policy.remote(
                     lambda p, pid: (pid, p.observation_space, p.action_space)))
                 spaces = {
                     e[0]: (getattr(e[1], "original_space", e[1]), e[2])
                     for e in remote_spaces
                 }
             ...
 &lt;/denchmark-code&gt;
 
 The problem was that the local-worker (driver) was using the already preprocessed space (got it from the remote-worker) to build its own policy/preprocessot stack. That's why the necessary DictFlatteningPreprocessor was never built and your Trainer did not do any preprocessing (on your input dict's observation) prior to sending the data to the Policy.
 		</comment>
 		<comment id='7' author='hybug' date='2020-12-09T13:20:15Z'>
 		Wonderful solution! Thanks to your selfless contribution!
 		</comment>
 	</comments>
 </bug>
<commit id='ea25482f6a4467e8cc3aa6543d83da47543b44b6' author='Sven Mika' date='2020-12-09 11:49:21-08:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='rllib\agents\trainer_template.py' new_name='rllib\agents\trainer_template.py'>
 		<file_info nloc='159' complexity='10' token_count='745'></file_info>
 		<modified_lines>
 			<added_lines>68</added_lines>
 			<deleted_lines>68</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\evaluation\worker_set.py' new_name='rllib\evaluation\worker_set.py'>
 		<file_info nloc='302' complexity='26' token_count='1866'></file_info>
 		<modified_lines>
 			<added_lines>82,83,84,85</added_lines>
 			<deleted_lines>82</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
