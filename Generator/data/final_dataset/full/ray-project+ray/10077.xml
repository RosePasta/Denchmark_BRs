<bug_data>
<bug id='10077' author='richardrl' open_date='2020-08-13T00:18:05Z' closed_time='2020-08-14T02:38:33Z'>
 	<summary>Docker + AWS fails with no such container error</summary>
 	<description>
 Latest dev version of ray
 &lt;denchmark-code&gt;(vanilla_ray_venv) richard@richard-desktop:~/improbable/vanillas/ray/python/ray/autoscaler/aws$ ray up aws_gpu_dummy.yaml 
 2020-08-12 20:12:39,383	INFO config.py:268 -- _configure_iam_role: Role not specified for head node, using arn:aws:iam::179622923911:instance-profile/ray-autoscaler-v1
 2020-08-12 20:12:39,612	INFO config.py:346 -- _configure_key_pair: KeyName not specified for nodes, using ray-autoscaler_us-east-1
 2020-08-12 20:12:39,745	INFO config.py:407 -- _configure_subnet: SubnetIds not specified for head node, using [('subnet-f737f791', 'us-east-1a')]
 2020-08-12 20:12:39,746	INFO config.py:417 -- _configure_subnet: SubnetId not specified for workers, using [('subnet-f737f791', 'us-east-1a')]
 2020-08-12 20:12:40,358	INFO config.py:590 -- _create_security_group: Created new security group ray-autoscaler-richard_cluster_gpu_dummy (sg-0061ca6aff182c1bf)
 2020-08-12 20:12:40,739	INFO config.py:444 -- _configure_security_group: SecurityGroupIds not specified for head node, using ray-autoscaler-richard_cluster_gpu_dummy (sg-0061ca6aff182c1bf)
 2020-08-12 20:12:40,739	INFO config.py:454 -- _configure_security_group: SecurityGroupIds not specified for workers, using ray-autoscaler-richard_cluster_gpu_dummy (sg-0061ca6aff182c1bf)
 This will create a new cluster [y/N]: y
 2020-08-12 20:12:42,619	INFO commands.py:531 -- get_or_create_head_node: Launching new head node...
 2020-08-12 20:12:42,620	INFO node_provider.py:326 -- NodeProvider: calling create_instances with subnet-f737f791 (count=1).
 2020-08-12 20:12:44,032	INFO node_provider.py:354 -- NodeProvider: Created instance [id=i-0729c7a86355d5ff8, name=pending, info=pending]
 2020-08-12 20:12:44,223	INFO commands.py:570 -- get_or_create_head_node: Updating files on head node...
 2020-08-12 20:12:44,320	INFO command_runner.py:331 -- NodeUpdater: i-0729c7a86355d5ff8: Waiting for IP...
 2020-08-12 20:12:54,409	INFO command_runner.py:331 -- NodeUpdater: i-0729c7a86355d5ff8: Waiting for IP...
 2020-08-12 20:12:54,534	INFO log_timer.py:27 -- NodeUpdater: i-0729c7a86355d5ff8: Got IP  [LogTimer=10310ms]
 2020-08-12 20:12:54,534	INFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; command -v docker'
 Warning: Permanently added '3.226.253.119' (ECDSA) to the list of known hosts.
 /usr/bin/docker
 Shared connection to 3.226.253.119 closed.
 2020-08-12 20:14:04,587	INFO updater.py:71 -- NodeUpdater: i-0729c7a86355d5ff8: Updating to 6b5fc8ee8c5dcdf3cfabe0bf90ba4e844f65a7c9
 2020-08-12 20:14:04,587	INFO updater.py:180 -- NodeUpdater: i-0729c7a86355d5ff8: Waiting for remote shell...
 2020-08-12 20:14:04,587	INFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; docker exec -it  pytorch_docker /bin/bash -c '"'"'bash --login -c -i '"'"'"'"'"'"'"'"'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; uptime'"'"'"'"'"'"'"'"''"'"' '
 2020-08-12 20:14:04,950	INFO log_timer.py:27 -- AWSNodeProvider: Set tag ray-node-status=waiting-for-ssh on ['i-0729c7a86355d5ff8']  [LogTimer=361ms]
                              Error: No such container: pytorch_docker
 Shared connection to 3.226.253.119 closed.
 2020-08-12 20:14:21,222	INFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; docker exec -it  pytorch_docker /bin/bash -c '"'"'bash --login -c -i '"'"'"'"'"'"'"'"'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; uptime'"'"'"'"'"'"'"'"''"'"' '
 Error: No such container: pytorch_docker
 Shared connection to 3.226.253.119 closed.
 2020-08-12 20:14:26,417	INFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; docker exec -it  pytorch_docker /bin/bash -c '"'"'bash --login -c -i '"'"'"'"'"'"'"'"'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; uptime'"'"'"'"'"'"'"'"''"'"' '
 Error: No such container: pytorch_docker
 Shared connection to 3.226.253.119 closed.
 2020-08-12 20:14:31,610	INFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; docker exec -it  pytorch_docker /bin/bash -c '"'"'bash --login -c -i '"'"'"'"'"'"'"'"'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; uptime'"'"'"'"'"'"'"'"''"'"' '
 Error: No such container: pytorch_docker
 Shared connection to 3.226.253.119 closed.
 2020-08-12 20:14:36,798	INFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; docker exec -it  pytorch_docker /bin/bash -c '"'"'bash --login -c -i '"'"'"'"'"'"'"'"'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; uptime'"'"'"'"'"'"'"'"''"'"' '
 Error: No such container: pytorch_docker
 Shared connection to 3.226.253.119 closed.
 2020-08-12 20:14:41,986	INFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; docker exec -it  pytorch_docker /bin/bash -c '"'"'bash --login -c -i '"'"'"'"'"'"'"'"'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; uptime'"'"'"'"'"'"'"'"''"'"' '
 Error: No such container: pytorch_docker
 Shared connection to 3.226.253.119 closed.
 2020-08-12 20:14:47,170	INFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; docker exec -it  pytorch_docker /bin/bash -c '"'"'bash --login -c -i '"'"'"'"'"'"'"'"'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; uptime'"'"'"'"'"'"'"'"''"'"' '
 Error: No such container: pytorch_docker
 Shared connection to 3.226.253.119 closed.
 2020-08-12 20:14:52,358	INFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; docker exec -it  pytorch_docker /bin/bash -c '"'"'bash --login -c -i '"'"'"'"'"'"'"'"'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; uptime'"'"'"'"'"'"'"'"''"'"' '
 Error: No such container: pytorch_docker
 Shared connection to 3.226.253.119 closed.
 2020-08-12 20:14:57,554	INFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; docker exec -it  pytorch_docker /bin/bash -c '"'"'bash --login -c -i '"'"'"'"'"'"'"'"'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; uptime'"'"'"'"'"'"'"'"''"'"' '
 Error: No such container: pytorch_docker
 Shared connection to 3.226.253.119 closed.
 2020-08-12 20:15:02,750	INFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; docker exec -it  pytorch_docker /bin/bash -c '"'"'bash --login -c -i '"'"'"'"'"'"'"'"'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; uptime'"'"'"'"'"'"'"'"''"'"' '
 Error: No such container: pytorch_docker
 Shared connection to 3.226.253.119 closed.
 2020-08-12 20:15:07,938	INFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; docker exec -it  pytorch_docker /bin/bash -c '"'"'bash --login -c -i '"'"'"'"'"'"'"'"'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; uptime'"'"'"'"'"'"'"'"''"'"' '
 Error: No such container: pytorch_docker
 Shared connection to 3.226.253.119 closed.
 2020-08-12 20:15:13,126	INFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; docker exec -it  pytorch_docker /bin/bash -c '"'"'bash --login -c -i '"'"'"'"'"'"'"'"'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; uptime'"'"'"'"'"'"'"'"''"'"' '
 Error: No such container: pytorch_docker
 Shared connection to 3.226.253.119 closed.
 2020-08-12 20:15:18,307	INFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; docker exec -it  pytorch_docker /bin/bash -c '"'"'bash --login -c -i '"'"'"'"'"'"'"'"'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; uptime'"'"'"'"'"'"'"'"''"'"' '
 Error: No such container: pytorch_docker
 Shared connection to 3.226.253.119 closed.
 2020-08-12 20:15:23,494	INFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; docker exec -it  pytorch_docker /bin/bash -c '"'"'bash --login -c -i '"'"'"'"'"'"'"'"'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; uptime'"'"'"'"'"'"'"'"''"'"' '
 Error: No such container: pytorch_docker
 Shared connection to 3.226.253.119 closed.
 Shared connection to 3.226.253.119 closed.
 2020-08-12 20:19:01,502	INFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; docker exec -it  pytorch_docker /bin/bash -c '"'"'bash --login -c -i '"'"'"'"'"'"'"'"'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; uptime'"'"'"'"'"'"'"'"''"'"' '
 Error: No such container: pytorch_docker
 Shared connection to 3.226.253.119 closed.
 2020-08-12 20:19:06,689	INFO log_timer.py:27 -- NodeUpdater: i-0729c7a86355d5ff8: Got remote shell  [LogTimer=302102ms]
 2020-08-12 20:19:06,690	INFO log_timer.py:27 -- NodeUpdater: i-0729c7a86355d5ff8: Applied config 6b5fc8ee8c5dcdf3cfabe0bf90ba4e844f65a7c9  [LogTimer=302103ms]
 2020-08-12 20:19:06,690	ERROR updater.py:88 -- NodeUpdater: i-0729c7a86355d5ff8: Error executing: Unable to connect to node
 
 Exception in thread Thread-2:
 Traceback (most recent call last):
   File "/usr/lib/python3.6/threading.py", line 916, in _bootstrap_inner
     self.run()
   File "/home/richard/improbable/vanillas/ray/python/ray/autoscaler/updater.py", line 76, in run
     self.do_update()
   File "/home/richard/improbable/vanillas/ray/python/ray/autoscaler/updater.py", line 232, in do_update
     self.wait_ready(deadline)
   File "/home/richard/improbable/vanillas/ray/python/ray/autoscaler/updater.py", line 224, in wait_ready
     assert False, "Unable to connect to node"
 AssertionError: Unable to connect to node
 
 2020-08-12 20:19:06,962	ERROR commands.py:650 -- get_or_create_head_node: Updating 3.226.253.119 failed
 2020-08-12 20:19:07,002	INFO log_timer.py:27 -- AWSNodeProvider: Set tag ray-node-status=update-failed on ['i-0729c7a86355d5ff8']  [LogTimer=312ms]
 
 &lt;/denchmark-code&gt;
 
 YAML file for repo attached.
 &lt;denchmark-code&gt;# An unique identifier for the head node and workers of this cluster.
 cluster_name: richard_cluster_gpu_dummy
 
 # The minimum number of workers nodes to launch in addition to the head
 # node. This number should be &gt;= 0.
 min_workers: 1
 
 # The maximum number of workers nodes to launch in addition to the head
 # node. This takes precedence over min_workers.
 max_workers: 5
 
 # The initial number of worker nodes to launch in addition to the head
 # node. When the cluster is first brought up (or when it is refreshed with a
 # subsequent `ray up`) this number of nodes will be started.
 initial_workers: 1
 
 # Whether or not to autoscale aggressively. If this is enabled, if at any point
 #   we would start more workers, we start at least enough to bring us to
 #   initial_workers.
 autoscaling_mode: default
 
 # This executes all commands on all nodes in the docker efcontainer,
 # and opens all the necessary ports to support the Ray cluster.
 # Empty string means disabled.
 docker:
   image: "pytorch/pytorch:latest" # e.g., tensorflow/tensorflow:1.5.0-py3
   container_name: "pytorch_docker" # e.g. ray_docker
   # If true, pulls latest version of image. Otherwise, `docker run` will only pull the image
   # if no cached version is present.
   pull_before_run: True
   run_options: []
 #    - $([ -d /proc/driver ] &amp;&amp; echo -n --runtime-nvidia) # Use the nvidia runtime only if nvidia gpu's are installed
   worker_run_options:
     - --runtime=nvidia  # Extra options to pass into "docker run"
 
   # Example of running a GPU head with CPU workers
   # head_image: "tensorflow/tensorflow:1.13.1-py3"
   # head_run_options:
   #     - --runtime=nvidia
 
   # worker_image: "ubuntu:18.04"
   # worker_run_options: []
 
 # The autoscaler will scale up the cluster to this target fraction of resource
 # usage. For example, if a cluster of 10 nodes is 100% busy and
 # target_utilization is 0.8, it would resize the cluster to 13. This fraction
 # can be decreased to increase the aggressiveness of upscaling.
 # This value must be less than 1.0 for scaling to happen.
 target_utilization_fraction: 0.8
 
 # If a node is idle for this many minutes, it will be removed.
 idle_timeout_minutes: 5
 
 # Cloud-provider specific configuration.
 provider:
   type: aws
   region: us-east-1
   # Availability zone(s), comma-separated, that nodes may be launched in.
   # Nodes are currently spread between zones by a round-robin approach,
   # however this implementation detail should not be relied upon.
   availability_zone: us-east-1a, us-east-1b
   cache_stopped_nodes: False
 
 # How Ray will authenticate with newly launched nodes.
 auth:
   ssh_user: ubuntu
 # By default Ray creates a new private keypair, but you can also use your own.
 # If you do so, make sure to also set "KeyName" in the head and worker node
 # configurations below.
 #    ssh_private_key: /path/to/your/key.pem
 
 # Provider-specific config for the head node, e.g. instance type. By default
 # Ray will auto-configure unspecified fields such as SubnetId and KeyName.
 # For more documentation on available fields, see:
 # http://boto3.readthedocs.io/en/latest/reference/services/ec2.html#EC2.ServiceResource.create_instances
 head_node:
   InstanceType: c4.2xlarge
   ImageId: ami-043f9aeaf108ebc37 # Deep Learning AMI (Ubuntu) Version 24.3
 #   You can provision additional disk space with a conf as follows
   BlockDeviceMappings:
     - DeviceName: /dev/sda1
       Ebs:
         VolumeSize: 100
 
   # Additional options in the boto docs.
 
 # Provider-specific config for worker nodes, e.g. instance type. By default
 # Ray will auto-configure unspecified fields such as SubnetId and KeyName.
 # For more documentation on available fields, see:
 # http://boto3.readthedocs.io/en/latest/reference/services/ec2.html#EC2.ServiceResource.create_instances
 worker_nodes:
   InstanceType: p3.2xlarge
   ImageId: ami-043f9aeaf108ebc37 # Deep Learning AMI (Ubuntu) Version 24.3
 
   # Run workers on spot by default. Comment this out to use on-demand.
   InstanceMarketOptions:
     MarketType: spot
     # Additional options can be found in the boto docs, e.g.
     #   SpotOptions:
     #       MaxPrice: MAX_HOURLY_PRICE
 
   # Additional options in the boto docs.
 
 # Files or directories to copy to the head and worker nodes. The format is a
 # dictionary from REMOTE_PATH: LOCAL_PATH, e.g.
 file_mounts: {
 
 }
 
 
 
 # List of commands that will be run before `setup_commands`. If docker is
 # enabled, these commands will run outside the container and before docker
 # is setup.
 initialization_commands: []
 
 # List of shell commands to run to set up nodes.
 setup_commands:
   # Note: if you're developing Ray, you probably want to create an AMI that
   # has your Ray repo pre-cloned. Then, you can replace the pip installs
   # below with a git checkout &lt;your_sha&gt; (and possibly a recompile).
   - pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp37-cp37m-manylinux1_x86_64.whl
 #  - pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp37-cp37m-manylinux1_x86_64.whl
   # Consider uncommenting these if you also want to run apt-get commands during setup
   # - sudo pkill -9 apt-get || true
   # - sudo pkill -9 dpkg || true
   # - sudo dpkg --configure -a
 
 # Custom commands that will be run on the head node after common setup.
 head_setup_commands:
   - pip install boto3  # 1.4.8 adds InstanceMarketOptions
 
 # Custom commands that will be run on worker nodes after common setup.
 worker_setup_commands:
   - pip install boto3  # 1.4.8 adds InstanceMarketOptions
 
 # Command to start ray on the head node. You don't need to change this.
 head_start_ray_commands:
   - ray stop
   - ulimit -n 65536; ray start --num-cpus=0 --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml
 
 # Command to start ray on worker nodes. You don't need to change this.
 worker_start_ray_commands:
   - ray stop
   - ulimit -n 65536; ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='richardrl' date='2020-08-14T02:38:31Z'>
 		&lt;denchmark-link:https://github.com/richardrl&gt;@richardrl&lt;/denchmark-link&gt;
  It looks like the issue is resolved, so I will close it. Please reopen it if I am wrong :)!
 		</comment>
 	</comments>
 </bug>
<commit id='a252aa29da9feb2faec62a59c9cac3a1c2b72e34' author='Ian Rodney' date='2020-08-13 20:35:06-07:00'>
 	<dmm_unit complexity='0.0' interfacing='0.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\command_runner.py' new_name='python\ray\autoscaler\command_runner.py'>
 		<file_info nloc='490' complexity='62' token_count='2841'></file_info>
 		<method name='run_rsync_up' parameters='self,source,target'>
 				<method_info nloc='11' complexity='3' token_count='82' nesting_level='1' start_line='558' end_line='569'></method_info>
 			<added_lines>559</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\updater.py' new_name='python\ray\autoscaler\updater.py'>
 		<file_info nloc='289' complexity='46' token_count='1818'></file_info>
 		<method name='sync_file_mounts.do_sync' parameters='remote_path,local_path,allow_non_existing_paths'>
 				<method_info nloc='18' complexity='7' token_count='143' nesting_level='2' start_line='131' end_line='156'></method_info>
 			<added_lines>148,149,150</added_lines>
 			<deleted_lines>148,149</deleted_lines>
 		</method>
 		<method name='wait_ready' parameters='self,deadline'>
 				<method_info nloc='38' complexity='6' token_count='239' nesting_level='1' start_line='176' end_line='225'></method_info>
 			<added_lines>192</added_lines>
 			<deleted_lines>191</deleted_lines>
 		</method>
 		<method name='sync_file_mounts' parameters='self,sync_cmd,step_numbers,2'>
 				<method_info nloc='23' complexity='5' token_count='141' nesting_level='1' start_line='121' end_line='174'></method_info>
 			<added_lines>148,149,150</added_lines>
 			<deleted_lines>148,149</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
