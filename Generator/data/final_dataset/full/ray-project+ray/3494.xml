<bug_data>
<bug id='3494' author='ericl' open_date='2018-12-07T21:04:44Z' closed_time='2019-01-05T06:30:35Z'>
 	<summary>[rllib] Model self loss isn't included in all algorithms</summary>
 	<description>
 &lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;
 
 &lt;denchmark-link:https://groups.google.com/forum/#!topic/ray-dev/dk0erEEnkFY&gt;https://groups.google.com/forum/#!topic/ray-dev/dk0erEEnkFY&lt;/denchmark-link&gt;
 
 In DQN, DDPG, IMPALA, and A3C, the gradients() function for the tf policy graph is overriden, but does not include model.loss().
 The fix is to reference self._loss in gradients(), since that is always the total loss passed to TFPolicyGraph.
 We should also add a simple test that the model self loss is improving for each algorithm (for example by just adding custom model with a free TF variable that gets sent to zero).
 	</description>
 	<comments>
 		<comment id='1' author='ericl' date='2018-12-07T21:33:33Z'>
 		&lt;denchmark-link:https://github.com/megankawakami&gt;@megankawakami&lt;/denchmark-link&gt;
  it would be great if you could look at this.
 		</comment>
 		<comment id='2' author='ericl' date='2018-12-08T22:18:12Z'>
 		Does it mean that for example A3C work wrongly?
 		</comment>
 		<comment id='3' author='ericl' date='2018-12-08T22:32:51Z'>
 		Only if you are using self supervised losses.
 		</comment>
 		<comment id='4' author='ericl' date='2018-12-09T03:58:01Z'>
 		I got it , thanks.
 		</comment>
 	</comments>
 </bug>
<commit id='03fe760616a70e7255f944168265b52efed27c6e' author='Eric Liang' date='2019-01-04 22:30:35-08:00'>
 	<dmm_unit complexity='0.2' interfacing='0.0' size='0.2'></dmm_unit>
 	<modification change_type='MODIFY' old_name='doc\source\rllib-offline.rst' new_name='doc\source\rllib-offline.rst'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>1,2</added_lines>
 			<deleted_lines>1,2</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='doc\source\rllib.rst' new_name='doc\source\rllib.rst'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>86,87,92,93,100,101</added_lines>
 			<deleted_lines>86,87,92,93,100,101</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\rllib\agents\a3c\a3c_tf_policy_graph.py' new_name='python\ray\rllib\agents\a3c\a3c_tf_policy_graph.py'>
 		<file_info nloc='141' complexity='11' token_count='1153'></file_info>
 		<method name='gradients' parameters='self,optimizer'>
 				<method_info nloc='5' complexity='1' token_count='57' nesting_level='1' start_line='144' end_line='148'></method_info>
 			<added_lines>145</added_lines>
 			<deleted_lines>145</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\rllib\agents\ddpg\ddpg_policy_graph.py' new_name='python\ray\rllib\agents\ddpg\ddpg_policy_graph.py'>
 		<file_info nloc='428' complexity='48' token_count='3282'></file_info>
 		<method name='__init__' parameters='self,observation_space,action_space,config'>
 				<method_info nloc='157' complexity='19' token_count='1464' nesting_level='1' start_line='183' end_line='374'></method_info>
 			<added_lines>209,210,243,250,269,270,277,315,316,317,318,319,320,363</added_lines>
 			<deleted_lines>210,211,244,251,270,271,278,358</deleted_lines>
 		</method>
 		<method name='_build_p_network' parameters='self,obs,obs_space'>
 				<method_info nloc='9' complexity='1' token_count='70' nesting_level='1' start_line='455' end_line='463'></method_info>
 			<added_lines>456,462,463</added_lines>
 			<deleted_lines>457</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>43</added_lines>
 			<deleted_lines>180,181,451</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\rllib\agents\dqn\dqn_policy_graph.py' new_name='python\ray\rllib\agents\dqn\dqn_policy_graph.py'>
 		<file_info nloc='450' complexity='42' token_count='3568'></file_info>
 		<method name='gradients' parameters='self,optimizer'>
 				<method_info nloc='12' complexity='4' token_count='87' nesting_level='1' start_line='402' end_line='413'></method_info>
 			<added_lines>406</added_lines>
 			<deleted_lines>406</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\rllib\agents\impala\vtrace_policy_graph.py' new_name='python\ray\rllib\agents\impala\vtrace_policy_graph.py'>
 		<file_info nloc='229' complexity='12' token_count='1729'></file_info>
 		<method name='gradients' parameters='self,optimizer'>
 				<method_info nloc='5' complexity='1' token_count='57' nesting_level='1' start_line='265' end_line='269'></method_info>
 			<added_lines>266</added_lines>
 			<deleted_lines>266</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\rllib\tuned_examples\pendulum-ddpg.yaml' new_name='python\ray\rllib\tuned_examples\pendulum-ddpg.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>7</added_lines>
 			<deleted_lines>7</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
