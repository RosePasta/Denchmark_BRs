<bug_data>
<bug id='12531' author='ZumbiAzul' open_date='2020-12-01T12:58:36Z' closed_time='2020-12-18T09:31:41Z'>
 	<summary>[tune] Fails to reproduce a basic tune tutorial example</summary>
 	<description>
 Hello, I am trying out RayTune for the first time, and currently I am going through the  &lt;denchmark-link:https://docs.ray.io/en/latest/tune/tutorials/tune-tutorial.html#tune-tutorial&gt;(basic Tune tutorial webpage)&lt;/denchmark-link&gt;
 
 The code I am trying to run is the following:
 &lt;denchmark-code&gt;import numpy as np
 import torch
 import torch.optim as optim
 import torch.nn as nn
 from torchvision import datasets, transforms
 from torch.utils.data import DataLoader
 import torch.nn.functional as F
 
 from ray import tune
 from ray.tune.suggest.hyperopt import HyperOptSearch
 from ray.tune.schedulers import ASHAScheduler
 
 from hyperopt import hp
 
 import os
 
 #%% ConvNet model
 class ConvNet(nn.Module):
     def __init__(self):
         super(ConvNet, self).__init__()
         # In this example, we don't change the model architecture
         # due to simplicity.
         self.conv1 = nn.Conv2d(1, 3, kernel_size=3)
         self.fc = nn.Linear(192, 10)
 
     def forward(self, x):
         x = F.relu(F.max_pool2d(self.conv1(x), 3))
         x = x.view(-1, 192)
         x = self.fc(x)
         return F.log_softmax(x, dim=1)
     
 #%% Train and test functions
 EPOCH_SIZE = 512
 TEST_SIZE = 256
 
 def train(model, optimizer, train_loader):
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
     model.train()
     for batch_idx, (data, target) in enumerate(train_loader):
         # We set this just for the example to run quickly.
         if batch_idx * len(data) &gt; EPOCH_SIZE:
             return
         data, target = data.to(device), target.to(device)
         optimizer.zero_grad()
         output = model(data)
         loss = F.nll_loss(output, target)
         loss.backward()
         optimizer.step()
 
 
 def test(model, data_loader):
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
     model.eval()
     correct = 0
     total = 0
     with torch.no_grad():
         for batch_idx, (data, target) in enumerate(data_loader):
             # We set this just for the example to run quickly.
             if batch_idx * len(data) &gt; TEST_SIZE:
                 break
             data, target = data.to(device), target.to(device)
             outputs = model(data)
             _, predicted = torch.max(outputs.data, 1)
             total += target.size(0)
             correct += (predicted == target).sum().item()
 
     return correct / total
 
 #%% Train MNIST function
 
 def train_mnist(config):
     # Data Setup
     mnist_transforms = transforms.Compose(
         [transforms.ToTensor(),
          transforms.Normalize((0.1307, ), (0.3081, ))])
 
     train_loader = DataLoader(
         datasets.MNIST("~/data", train=True, download=True, transform=mnist_transforms),
         batch_size=64,
         shuffle=True)
     test_loader = DataLoader(
         datasets.MNIST("~/data", train=False, transform=mnist_transforms),
         batch_size=64,
         shuffle=True)
 
     model = ConvNet()
     optimizer = optim.SGD(
         model.parameters(), lr=config["lr"], momentum=config["momentum"])
     for i in range(10):
         train(model, optimizer, train_loader)
         acc = test(model, test_loader)
 
         # Send the current training result back to Tune
         tune.report(mean_accuracy=acc)
 
         if i % 5 == 0:
             # This saves the model to the trial directory
             torch.save(model.state_dict(), "./model.pth")
 
 #%% Defining a search space
 
 # Letâ€™s run 1 trial by calling tune.run and randomly sample from a uniform distribution for learning rate and momentum.
 search_space = {
     "lr": tune.sample_from(lambda spec: 10**(-10 * np.random.rand())),
     "momentum": tune.uniform(0.1, 0.9)
 }
 
 # Uncomment this to enable distributed execution
 # `ray.init(address="auto")`
 
 # Download the dataset first
 datasets.MNIST("~/data", train=True, download=True)
 
 analysis = tune.run(train_mnist, config=search_space, resources_per_trial={'gpu': 1})
 
 # tune.run returns an Analysis object. You can use this to plot the performance of this trial.
 dfs = analysis.trial_dataframes
 [d.mean_accuracy.plot() for d in dfs.values()]
 
 #%% Evaluate your model
 
 df = analysis.results_df
 logdir = analysis.get_best_logdir("mean_accuracy", mode="max")
 state_dict = torch.load(os.path.join(logdir, "model.pth"))
 
 model = ConvNet()
 model.load_state_dict(state_dict)
 &lt;/denchmark-code&gt;
 
 The error message is the following:
 &lt;denchmark-code&gt;&lt;IPython.core.display.HTML object&gt;
 2020-12-01 13:52:06,559	ERROR trial_runner.py:793 -- Trial train_mnist_0303d_00000: Error processing event.
 Traceback (most recent call last):
   File "C:\Users\Admin\.conda\envs\pytorch_env\lib\site-packages\ray\tune\trial_runner.py", line 726, in _process_trial
     result = self.trial_executor.fetch_result(trial)
   File "C:\Users\Admin\.conda\envs\pytorch_env\lib\site-packages\ray\tune\ray_trial_executor.py", line 489, in fetch_result
     result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)
   File "C:\Users\Admin\.conda\envs\pytorch_env\lib\site-packages\ray\worker.py", line 1452, in get
     raise value.as_instanceof_cause()
 ray.exceptions.RayTaskError(TuneError): ray::ImplicitFunc.train() (pid=16872, ip=130.235.28.9)
   File "python\ray\_raylet.pyx", line 482, in ray._raylet.execute_task
   File "python\ray\_raylet.pyx", line 436, in ray._raylet.execute_task.function_executor
   File "C:\Users\Admin\.conda\envs\pytorch_env\lib\site-packages\ray\function_manager.py", line 553, in actor_method_executor
     return method(actor, *args, **kwargs)
   File "C:\Users\Admin\.conda\envs\pytorch_env\lib\site-packages\ray\tune\trainable.py", line 336, in train
     result = self.step()
   File "C:\Users\Admin\.conda\envs\pytorch_env\lib\site-packages\ray\tune\function_runner.py", line 366, in step
     self._report_thread_runner_error(block=True)
   File "C:\Users\Admin\.conda\envs\pytorch_env\lib\site-packages\ray\tune\function_runner.py", line 512, in _report_thread_runner_error
     raise TuneError(("Trial raised an exception. Traceback:\n{}"
 ray.tune.error.TuneError: Trial raised an exception. Traceback:
 ray::ImplicitFunc.train() (pid=16872, ip=130.235.28.9)
   File "C:\Users\Admin\.conda\envs\pytorch_env\lib\site-packages\ray\tune\function_runner.py", line 248, in run
     self._entrypoint()
   File "C:\Users\Admin\.conda\envs\pytorch_env\lib\site-packages\ray\tune\function_runner.py", line 315, in entrypoint
     return self._trainable_func(self.config, self._status_reporter,
   File "C:\Users\Admin\.conda\envs\pytorch_env\lib\site-packages\ray\tune\function_runner.py", line 575, in _trainable_func
     output = fn()
   File "D:\Jupiter_playground\raytune_playground.py", line 90, in train_mnist
     train(model, optimizer, train_loader)
   File "D:\Jupiter_playground\raytune_playground.py", line 45, in train
     output = model(data)
   File "C:\Users\Admin\.conda\envs\pytorch_env\lib\site-packages\torch\nn\modules\module.py", line 722, in _call_impl
     result = self.forward(*input, **kwargs)
   File "D:\Jupiter_playground\raytune_playground.py", line 27, in forward
     x = F.relu(F.max_pool2d(self.conv1(x), 3))
   File "C:\Users\Admin\.conda\envs\pytorch_env\lib\site-packages\torch\nn\modules\module.py", line 722, in _call_impl
     result = self.forward(*input, **kwargs)
   File "C:\Users\Admin\.conda\envs\pytorch_env\lib\site-packages\torch\nn\modules\conv.py", line 419, in forward
     return self._conv_forward(input, self.weight)
   File "C:\Users\Admin\.conda\envs\pytorch_env\lib\site-packages\torch\nn\modules\conv.py", line 415, in _conv_forward
     return F.conv2d(input, weight, self.bias, self.stride,
 RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same
 &lt;IPython.core.display.HTML object&gt;
 Traceback (most recent call last):
 
   File "D:\Jupiter_playground\raytune_playground.py", line 114, in &lt;module&gt;
     analysis = tune.run(train_mnist, config=search_space, resources_per_trial={'gpu': 1})
 
   File "C:\Users\Admin\.conda\envs\pytorch_env\lib\site-packages\ray\tune\tune.py", line 434, in run
     raise TuneError("Trials did not complete", incomplete_trials)
 
 TuneError: ('Trials did not complete', [train_mnist_0303d_00000])
 &lt;/denchmark-code&gt;
 
 I made sure that all necessary packages are installed into the Conda environment. Could someone help me figure out how to resolve the issue?
 	</description>
 	<comments>
 		<comment id='1' author='ZumbiAzul' date='2020-12-13T09:43:41Z'>
 		Looks like you need to call model.cuda()
 		</comment>
 		<comment id='2' author='ZumbiAzul' date='2020-12-13T09:44:00Z'>
 		Or rather; model.to(device)
 		</comment>
 		<comment id='3' author='ZumbiAzul' date='2020-12-14T15:38:21Z'>
 		Thanks, &lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
  That was indeed the solution. Could you please tell, how you figured it out, because I am not sure how obvious it is mentioned in the error?
 		</comment>
 		<comment id='4' author='ZumbiAzul' date='2020-12-16T10:39:44Z'>
 		Hi &lt;denchmark-link:https://github.com/ZumbiAzul&gt;@ZumbiAzul&lt;/denchmark-link&gt;
 , this can be seen here:
 &lt;denchmark-code&gt;RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same
 &lt;/denchmark-code&gt;
 
 which indicates that the input is a cuda tensor and the model is not (otherwise it would also be a torch.cuda.FloatTensor). By moving the model to the GPU the model weights will become torch.cuda.FloatTensor, too.
 		</comment>
 		<comment id='5' author='ZumbiAzul' date='2020-12-16T10:56:07Z'>
 		We're updating the docs in &lt;denchmark-link:https://github.com/ray-project/ray/pull/12914&gt;#12914&lt;/denchmark-link&gt;
  so this doesn't happen in the future. Thanks for pointing this issue out to us!
 		</comment>
 	</comments>
 </bug>
<commit id='426f8a8d15449c5e78efc4604e2b0fd4a8577fb8' author='Kai Fricke' date='2020-12-18 01:31:40-08:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='python\ray\tune\tests\tutorial.py' new_name='python\ray\tune\tests\tutorial.py'>
 		<file_info nloc='109' complexity='14' token_count='965'></file_info>
 		<method name='train_mnist' parameters='config'>
 				<method_info nloc='23' complexity='4' token_count='202' nesting_level='0' start_line='81' end_line='112'></method_info>
 			<added_lines>96,97,99,100</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>168,169,170,171,172</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
