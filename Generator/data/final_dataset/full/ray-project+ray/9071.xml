<bug_data>
<bug id='9071' author='msloma144' open_date='2020-06-21T17:25:08Z' closed_time='2021-01-19T13:21:27Z'>
 	<summary>[rllib] State shapes incorrect using custom model (TorchModelV2, TFModelV2) (PPO)</summary>
 	<description>
 &lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;
 
 It seems that the states being passed to TorchModelV2 and TFModelV2 are incorrect, as the shapes don't seem to match up. Please see the stack traces below. Note that I am using PPO. Also, I do not want to use the RecurrentNetwork as I need more control than that provides.
 Ray version and other system information (Python version, TensorFlow version, OS):
 Python 3.8.3, ray: 0.9.0.dev0, torch: 1.5.1, tensorflow: 2.2.0, WSL Ubuntu 18.04.4 LTS
 &lt;denchmark-h:h3&gt;Keras/Tensorflow Error&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;2020-06-21 13:09:32,571	ERROR trial_runner.py:524 -- Trial PPO_TestingGym_f28cf_00000: Error processing event.
 Traceback (most recent call last):
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/tune/trial_runner.py", line 472, in _process_trial
     result = self.trial_executor.fetch_result(trial)
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py", line 430, in fetch_result
     result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/worker.py", line 1478, in get
     raise value.as_instanceof_cause()
 ray.exceptions.RayTaskError(InvalidArgumentError): ray::PPO.train() (pid=20426, ip=192.168.2.105)
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/tensorflow/python/client/session.py", line 1349, in _run_fn
     return self._call_tf_sessionrun(options, feed_dict, fetch_list,
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/tensorflow/python/client/session.py", line 1441, in _call_tf_sessionrun
     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,
 tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [120,64] vs. [6,64]
 	 [[{{node default_policy_1/tower_1/model_1/lstm/while/lstm_cell/add_6}}]]
 
 During handling of the above exception, another exception occurred:
 
 ray::PPO.train() (pid=20426, ip=192.168.2.105)
   File "python/ray/_raylet.pyx", line 443, in ray._raylet.execute_task
   File "python/ray/_raylet.pyx", line 446, in ray._raylet.execute_task
   File "python/ray/_raylet.pyx", line 447, in ray._raylet.execute_task
   File "python/ray/_raylet.pyx", line 401, in ray._raylet.execute_task.function_executor
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/trainer.py", line 520, in train
     raise e
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/trainer.py", line 506, in train
     result = Trainable.train(self)
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/tune/trainable.py", line 317, in train
     result = self._train()
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py", line 137, in _train
     return self._train_exec_impl()
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py", line 175, in _train_exec_impl
     res = next(self.train_exec_impl)
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 731, in __next__
     return next(self.built_iterator)
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 744, in apply_foreach
     for item in it:
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 744, in apply_foreach
     for item in it:
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 814, in apply_filter
     for item in it:
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 814, in apply_filter
     for item in it:
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 744, in apply_foreach
     for item in it:
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 744, in apply_foreach
     for item in it:
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 752, in apply_foreach
     result = fn(item)
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/execution/train_ops.py", line 204, in __call__
     batch_fetches = optimizer.optimize(
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/execution/multi_gpu_impl.py", line 257, in optimize
     return sess.run(fetches, feed_dict=feed_dict)
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/tensorflow/python/client/session.py", line 957, in run
     result = self._run(None, fetches, feed_dict, options_ptr,
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/tensorflow/python/client/session.py", line 1180, in _run
     results = self._do_run(handle, final_targets, final_fetches,
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/tensorflow/python/client/session.py", line 1358, in _do_run
     return self._do_call(_run_fn, feeds, fetches, targets, options,
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/tensorflow/python/client/session.py", line 1384, in _do_call
     raise type(e)(node_def, op, message)
 tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [120,64] vs. [6,64]
 	 [[node default_policy_1/tower_1/model_1/lstm/while/lstm_cell/add_6 (defined at mnt/c/Users/user/Desktop/RLlib_Issue/rnn_model.py:81) ]]
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Pytorch Error&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;2020-06-21 13:14:26,130	ERROR trial_runner.py:524 -- Trial PPO_TestingGym_a4dcc_00000: Error processing event.
 Traceback (most recent call last):
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/tune/trial_runner.py", line 472, in _process_trial
     result = self.trial_executor.fetch_result(trial)
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py", line 430, in fetch_result
     result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/worker.py", line 1478, in get
     raise value.as_instanceof_cause()
 ray.exceptions.RayTaskError(RuntimeError): ray::PPO.train() (pid=21085, ip=192.168.2.105)
   File "python/ray/_raylet.pyx", line 447, in ray._raylet.execute_task
   File "python/ray/_raylet.pyx", line 401, in ray._raylet.execute_task.function_executor
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/trainer.py", line 520, in train
     raise e
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/trainer.py", line 506, in train
     result = Trainable.train(self)
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/tune/trainable.py", line 317, in train
     result = self._train()
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py", line 137, in _train
     return self._train_exec_impl()
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py", line 175, in _train_exec_impl
     res = next(self.train_exec_impl)
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 731, in __next__
     return next(self.built_iterator)
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 744, in apply_foreach
     for item in it:
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 744, in apply_foreach
     for item in it:
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 814, in apply_filter
     for item in it:
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 814, in apply_filter
     for item in it:
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 744, in apply_foreach
     for item in it:
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 744, in apply_foreach
     for item in it:
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 752, in apply_foreach
     result = fn(item)
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/execution/train_ops.py", line 62, in __call__
     info = do_minibatch_sgd(
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/utils/sgd.py", line 114, in do_minibatch_sgd
     batch_fetches = (local_worker.learn_on_batch(
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py", line 737, in learn_on_batch
     info_out[pid] = policy.learn_on_batch(batch)
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/policy/torch_policy.py", line 242, in learn_on_batch
     self._loss(self, self.model, self.dist_class, train_batch))
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/ppo/ppo_torch_policy.py", line 113, in ppo_surrogate_loss
     logits, state = model.from_batch(train_batch)
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/models/modelv2.py", line 224, in from_batch
     return self.__call__(input_dict, states, train_batch.get("seq_lens"))
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/models/modelv2.py", line 181, in __call__
     res = self.forward(restored, state or [], seq_lens)
   File "/mnt/c/Users/user/Desktop/RLlib_Issue/rnn_model.py", line 166, in forward
     self._features, [h, c] = self.lstm(x, [torch.unsqueeze(state[0], 0),
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/torch/nn/modules/module.py", line 550, in __call__
     result = self.forward(*input, **kwargs)
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/torch/nn/modules/rnn.py", line 567, in forward
     self.check_forward_args(input, hx, batch_sizes)
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/torch/nn/modules/rnn.py", line 522, in check_forward_args
     self.check_hidden_size(hidden[0], expected_hidden_size,
   File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/torch/nn/modules/rnn.py", line 187, in check_hidden_size
     raise RuntimeError(msg.format(expected_hidden_size, tuple(hx.size())))
 RuntimeError: Expected hidden[0] size (1, 140, 256), got (1, 7, 256)
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;
 
 Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
 If we cannot run your script, we cannot fix your issue.
 
 [ X ] I have verified my script runs in a clean environment and reproduces the issue. (created new conda environment and installed all packages from scratch using pip)
 [ X ] I have verified the issue also occurs with the latest wheels.
 
 &lt;denchmark-h:h4&gt;rllib_ppo_agent.py&lt;/denchmark-h&gt;
 
 from testing_gym import TestingGym
 from ray.rllib.models import ModelCatalog
 from ray.tune.registry import register_env
 from rnn_model import TorchRNNModel, RNNModel
 from ray import tune
 
 timesteps = 5
 
 
 def env_creator(env_config):
     env = TestingGym()
     return env  # return an env instance
 
 
 if __name__ == "__main__":
     register_env("TestingGym", env_creator)
     # also have had issues with TF models
     ModelCatalog.register_custom_model("torch_model", TorchRNNModel)
     ModelCatalog.register_custom_model("keras_model",  RNNModel)
 
     tune.run(
         "A2C",
         stop={"episodes_total": 500},
         checkpoint_at_end=True,
         checkpoint_freq=100,
         config={
             "env": "TestingGym",
             "num_workers": 14,
             "env_config": {},
             "lr": 0.000001,
             "framework": "torch",
             "model": {
                 "custom_model_config":
                     {
                         "timesteps": timesteps
                     },
                 "fcnet_hiddens": [256, 256, 256, 256],
                 "custom_model": "torch_model",
             }
         },
         local_dir="./results", )
 &lt;denchmark-h:h4&gt;rnn_model.py&lt;/denchmark-h&gt;
 
 import numpy as np
 
 from ray.rllib.models.modelv2 import ModelV2
 from ray.rllib.models.preprocessors import get_preprocessor
 from ray.rllib.models.tf.tf_modelv2 import TFModelV2
 from ray.rllib.utils.annotations import override
 from ray.rllib.utils.framework import try_import_tf, try_import_torch
 from ray.rllib.models.torch.torch_modelv2 import TorchModelV2
 
 tf = try_import_tf()
 torch, nn = try_import_torch()
 
 
 class RNNModel(TFModelV2):
     """Example of using the Keras functional API to define a RNN model."""
 
     def __init__(self,
                  obs_space,
                  action_space,
                  num_outputs,
                  model_config,
                  name,
                  hiddens_size=256,
                  cell_size=64,
                  timesteps=5):
         super(RNNModel, self).__init__(obs_space, action_space, num_outputs,
                                        model_config, name)
         self.obs_space = obs_space
         self.cell_size = cell_size
         self.timesteps = timesteps
 
         print(f"OBS SPACE: {obs_space.shape}")
         # Define input layers
         input_layer = tf.keras.layers.Input(
             shape=(timesteps, int(obs_space.shape[0]/self.timesteps)), name="inputs")
 
         state_in_h = tf.keras.layers.Input(shape=(cell_size, ), name="h")
         state_in_c = tf.keras.layers.Input(shape=(cell_size, ), name="c")
         #seq_in = tf.keras.layers.Input(shape=(), name="seq_in", dtype=tf.int32)
 
         # Preprocess observation with a hidden layer and send to LSTM cell
         dense1 = tf.keras.layers.Dense(
             hiddens_size, activation=tf.nn.sigmoid, name="dense1")(input_layer)
         lstm_out, state_h, state_c = tf.keras.layers.LSTM(
             cell_size,
             #return_sequences=True,
             return_state=True, name="lstm")(
                 inputs=dense1,
                 #mask=tf.sequence_mask(seq_in),
                 initial_state=[state_in_h, state_in_c])
         #flats = tf.keras.layers.Flatten()(lstm_out)
         # Postprocess LSTM output with another hidden layer and compute values
 
         _ = lstm_out
         for units in model_config["fcnet_hiddens"]:
             _ = tf.keras.layers.Dense(
                 units,
                 activation=tf.keras.activations.sigmoid)(_)
 
         logits = tf.keras.layers.Dense(
             self.num_outputs,
             activation=tf.keras.activations.linear,
             name="logits")(_)
         values = tf.keras.layers.Dense(
             1, activation=None, name="values")(_)
 
         # Create the RNN model
         self.rnn_model = tf.keras.Model(
             inputs=[input_layer, state_in_h, state_in_c],
             outputs=[logits, values, state_h, state_c])
         self.register_variables(self.rnn_model.variables)
         self.rnn_model.summary()
 
     @override(TFModelV2)
     def forward(self, inputs, state, seq_lens):
         print("forward")
         print(f"INPUTS: {state}")
         inputs = inputs['obs']
         inputs = tf.reshape(tensor=inputs, shape=[-1, self.timesteps, int(self.obs_space.shape[0]/self.timesteps)])
 
         model_out, self._value_out, h, c = self.rnn_model([inputs,] + state)
         return model_out, [h, c]
 
     @override(ModelV2)
     def get_initial_state(self):
         return [
             np.zeros(self.cell_size, np.float32),
             np.zeros(self.cell_size, np.float32),
         ]
 
     @override(ModelV2)
     def value_function(self):
         return tf.reshape(self._value_out, [-1])
 
 
 class TorchRNNModel(TorchModelV2, nn.Module):
     def __init__(self,
                  obs_space,
                  action_space,
                  num_outputs,
                  model_config,
                  name,
                  fc_size=64,
                  lstm_state_size=256,
                  num_symbols=5,
                  timesteps=5):
         super().__init__(obs_space, action_space, num_outputs, model_config,
                          name)
         nn.Module.__init__(self)
         self.timesteps = timesteps
         self.num_symbols = num_symbols
 
         self.obs_size = get_preprocessor(obs_space)(obs_space).size
         print(f"RNN Obs Size: {self.obs_size}")
         self.obs_size = int(self.obs_size/self.timesteps)
         self.fc_size = fc_size
         self.lstm_state_size = lstm_state_size
 
         # Build the Module from fc + LSTM + 2xfc (action + value outs).
         self.fc1 = nn.Linear(self.obs_size, self.fc_size)
         self.lstm = nn.LSTM(self.fc_size, self.lstm_state_size, batch_first=True)
         self.action_branch = nn.Linear(self.lstm_state_size, num_outputs)
         self.value_branch = nn.Linear(self.lstm_state_size, 1)
         # Holds the current "base" output (before logits layer).
         self._features = None
 
     @override(ModelV2)
     def get_initial_state(self):
         # Place hidden states on same device as model.
         h = [
             self.fc1.weight.new(1, self.lstm_state_size).zero_().squeeze(0),
             self.fc1.weight.new(1, self.lstm_state_size).zero_().squeeze(0)
         ]
         print(f"Inital State: {h[0].shape},  {h[1].shape}")
         return h
 
     @override(ModelV2)
     def value_function(self):
         assert self._features is not None, "must call forward() first"
         return torch.reshape(self.value_branch(self._features), [-1])
 
     @override(ModelV2)
     def forward(self, inputs, state, seq_lens):
         """
         Feeds `inputs` (B x T x ..) through the Gru Unit.
 
         Returns the resulting outputs as a sequence (B x T x ...).
         Values are stored in self._cur_value in simple (B) shape (where B
         contains both the B and T dims!).
 
         Returns:
             NN Outputs (B x T x ...) as sequence.
             The state batches as a List of two items (c- and h-states).
         """
         print("forward")
         #print(f"INPUTS: {state}")
         inputs = inputs['obs']
         # if not isinstance(inputs, tuple):
         inputs = torch.reshape(input=inputs, shape=(-1, self.timesteps, int(self.obs_size)))
         print(f"inputs shape: {inputs.shape}")
         print(f"state sizes: h {torch.unsqueeze(state[0], 0).shape}, c {torch.unsqueeze(state[1], 0).shape}")
         # embedding_input = inputs[:, :, :self.num_symbols]
         # inputs = inputs[:, :, self.num_symbols:]
 
         x = nn.functional.relu(self.fc1(inputs))
         self._features, [h, c] = self.lstm(x, [torch.unsqueeze(state[0], 0),
                                                torch.unsqueeze(state[1], 0)])
         print(f"state size after: h {h.shape}, c {c.shape}")
         print(f"LSTM shape: {self._features.shape}")
         self._features = self._features[:, -1, :]
         print(f"LSTM shape After: {self._features.shape}")
         action_out = self.action_branch(self._features)
         print(f"action shape: {action_out.shape}")
 
         return action_out, [torch.squeeze(h, 0), torch.squeeze(c, 0)]
 &lt;denchmark-h:h4&gt;testing_gym.py&lt;/denchmark-h&gt;
 
 import gym
 from gym import error, spaces, utils
 from gym.utils import seeding
 import numpy as np
 import sys
 
 
 class TestingGym(gym.Env):
     metadata = {'render.modes': ['human']}
 
     def __init__(self, timesteps=5):
         self.timesteps = timesteps
 
         super(TestingGym, self).__init__()
 
         self.reward_range = (-sys.float_info.max-1, sys.float_info.max)
 
         self.action_space = spaces.Box(low=np.array([0, 0]), high=np.array([4, 1]), dtype=np.float16)
 
         self.done_counter = 0
         self.obs_length = 15
         self.observation_space = spaces.Box(low=-sys.float_info.max-1, high=sys.float_info.max, shape=(self.timesteps * self.obs_length,), dtype=np.float32)
 
     def _initial_observation(self):
         curr_obs = np.random.random((self.timesteps, self.obs_length))
         curr_obs = curr_obs.reshape((self.timesteps * self.obs_length,))
         print(f"Obs Length: {curr_obs.shape}")
         return curr_obs
 
     def step(self, action):
         self.done_counter += 1
 
         curr_obs = np.random.random((self.timesteps, self.obs_length))
         curr_obs = curr_obs.reshape((self.timesteps * self.obs_length,))
 
         if self.done_counter &gt; 1000:
             done = True
         else:
             done = False
 
         print(f"Obs Length: {curr_obs.shape}")
         return curr_obs, 1, done, {}
 
     def reset(self):
         self.done_counter = 0
 
         return self._initial_observation()
 	</description>
 	<comments>
 		<comment id='1' author='msloma144' date='2020-12-26T02:42:22Z'>
 		Thank you for sharing this, as I am facing the same challenge in recent days using TorchRNNModel with TorchModelV2.  I believe it happens on my experiment after workers finish collecting experiences and batch shape changes.
 Ray version and other system information (Python version, TensorFlow version, OS):
 Python 3.7.6, ray: 1.1.0, torch: 1.7.0a0+b7147fe, WSL Ubuntu 18.04.4 LTS
 RuntimeError: Expected hidden[0] size (1, 1, 128), got (1, 32, 128)
 		</comment>
 		<comment id='2' author='msloma144' date='2020-12-30T00:23:57Z'>
 		Is there a workaround to get the correct shape the first time around?  It seems very connected to this thread #&lt;denchmark-link:https://github.com/ray-project/ray/issues/12509&gt;#12509&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='3' author='msloma144' date='2020-12-30T01:51:17Z'>
 		Strange, I'm getting a different error on your torch script, related to the states list being empty.
 The reason is that A3C uses a buggy ValueFunction mixin, which assumes that no RNN is being used. I'll fix this now and re-try. ...
 		</comment>
 		<comment id='4' author='msloma144' date='2020-12-30T02:08:52Z'>
 		Thanks for looking at this Sven.  My results are the same when using A3C/A2C.  If we change it over to PPO then we get the state shape error.
 &lt;denchmark-code&gt;state sizes: h torch.Size([1, 32, 256]), c torch.Size([1, 32, 256])
 state size after: h torch.Size([1, 32, 256]), c torch.Size([1, 32, 256])
 LSTM shape: torch.Size([32, 5, 256])
 LSTM shape After: torch.Size([32, 256])
 action shape: torch.Size([32, 4])
 * forward pass #1
 inputs shape: torch.Size([1, 5, 15])
 state sizes: h torch.Size([1, 1, 256]), c torch.Size([1, 1, 256])
 state size after: h torch.Size([1, 1, 256]), c torch.Size([1, 1, 256])
 LSTM shape: torch.Size([1, 5, 256])
 LSTM shape After: torch.Size([1, 256])
 action shape: torch.Size([1, 4])
 * forward pass #2
 inputs shape: torch.Size([32, 5, 15])
 state sizes: h torch.Size([1, 4, 256]), c torch.Size([1, 4, 256])
 &lt;/denchmark-code&gt;
 
 RuntimeError: Expected hidden[0] size (1, 32, 256), got [1, 4, 256]
 		</comment>
 		<comment id='5' author='msloma144' date='2020-12-30T03:56:53Z'>
 		Ok, getting the same error now after having fixed these legacy A2C issues. ....
 		</comment>
 	</comments>
 </bug>
<commit id='2e3655e8a9b1e37fa6e29f11db02a6b53cfb5928' author='Sven Mika' date='2021-01-19 14:22:36+01:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='rllib\agents\a3c\a3c_tf_policy.py' new_name='rllib\agents\a3c\a3c_tf_policy.py'>
 		<file_info nloc='87' complexity='9' token_count='645'></file_info>
 		<method name='setup_mixins' parameters='policy,obs_space,action_space,config'>
 				<method_info nloc='3' complexity='1' token_count='39' nesting_level='0' start_line='98' end_line='100'></method_info>
 			<added_lines>99</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='postprocess_advantages' parameters='policy,sample_batch,other_agent_batches,episode'>
 				<method_info nloc='4' complexity='1' token_count='15' nesting_level='0' start_line='17' end_line='20'></method_info>
 			<added_lines>17,18,19,20</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__.value' parameters='ob,prev_action,prev_reward,state'>
 				<method_info nloc='9' complexity='2' token_count='106' nesting_level='2' start_line='75' end_line='83'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>75,76,77,78,79,80,81,82,83</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self'>
 				<method_info nloc='4' complexity='1' token_count='21' nesting_level='1' start_line='73' end_line='85'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>73,74,75,76,77,78,79,80,81,82,83,84,85</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>4,6,10,12,21,22,23,24,25,26,27,28,29,30,31,110</added_lines>
 			<deleted_lines>5,10,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,72,86,87,118,129</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\agents\a3c\a3c_torch_policy.py' new_name='rllib\agents\a3c\a3c_torch_policy.py'>
 		<file_info nloc='77' complexity='6' token_count='503'></file_info>
 		<method name='_value' parameters='self,obs'>
 				<method_info nloc='3' complexity='1' token_count='51' nesting_level='1' start_line='82' end_line='84'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>82,83,84</deleted_lines>
 		</method>
 		<method name='add_advantages' parameters='policy,sample_batch,other_agent_batches,episode'>
 				<method_info nloc='4' complexity='1' token_count='15' nesting_level='0' start_line='18' end_line='21'></method_info>
 			<added_lines>18,19,20,21</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='setup_mixins' parameters='Policy,Space,Space,TrainerConfigDict'>
 				<method_info nloc='3' complexity='1' token_count='29' nesting_level='0' start_line='70' end_line='72'></method_info>
 			<added_lines>70,71,72</added_lines>
 			<deleted_lines>70,71,72</deleted_lines>
 		</method>
 		<method name='apply_grad_clipping' parameters='policy,optimizer,loss'>
 				<method_info nloc='13' complexity='5' token_count='99' nesting_level='0' start_line='60' end_line='74'></method_info>
 			<added_lines>70,71,72,73,74</added_lines>
 			<deleted_lines>60,61,62,63,64,65,66,67,68,69,70,71,72,73,74</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1,2,4,5,7,10,12,13,22,23,24,25,26,27,28,29,30,31,32,75,76,77,78,79,80,81,90,94</added_lines>
 			<deleted_lines>2,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,75,76,81,93</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\agents\cql\cql_torch_policy.py' new_name='rllib\agents\cql\cql_torch_policy.py'>
 		<file_info nloc='242' complexity='8' token_count='1960'></file_info>
 		<modified_lines>
 			<added_lines>24,25</added_lines>
 			<deleted_lines>11,25</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\agents\ddpg\ddpg_torch_policy.py' new_name='rllib\agents\ddpg\ddpg_torch_policy.py'>
 		<file_info nloc='194' complexity='29' token_count='1489'></file_info>
 		<modified_lines>
 			<added_lines>12</added_lines>
 			<deleted_lines>4,13</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\agents\dqn\dqn_tf_policy.py' new_name='rllib\agents\dqn\dqn_tf_policy.py'>
 		<file_info nloc='347' complexity='21' token_count='2470'></file_info>
 		<modified_lines>
 			<added_lines>304,305,306,307,308</added_lines>
 			<deleted_lines>304,305,306,307,308,309,310,311,312,313,314</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\agents\dqn\dqn_torch_policy.py' new_name='rllib\agents\dqn\dqn_torch_policy.py'>
 		<file_info nloc='320' complexity='13' token_count='2180'></file_info>
 		<modified_lines>
 			<added_lines>22,23</added_lines>
 			<deleted_lines>7,23,24,25</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\agents\dreamer\dreamer_torch_policy.py' new_name='rllib\agents\dreamer\dreamer_torch_policy.py'>
 		<file_info nloc='174' complexity='14' token_count='1553'></file_info>
 		<modified_lines>
 			<added_lines>8</added_lines>
 			<deleted_lines>4</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\agents\impala\impala.py' new_name='rllib\agents\impala\impala.py'>
 		<file_info nloc='191' complexity='24' token_count='1219'></file_info>
 		<method name='get_policy_class' parameters='config'>
 				<method_info nloc='16' complexity='4' token_count='77' nesting_level='0' start_line='148' end_line='163'></method_info>
 			<added_lines>162</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>4</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\agents\impala\vtrace_torch_policy.py' new_name='rllib\agents\impala\vtrace_torch_policy.py'>
 		<file_info nloc='183' complexity='18' token_count='1354'></file_info>
 		<modified_lines>
 			<added_lines>13,14</added_lines>
 			<deleted_lines>6,14,15</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\agents\maml\maml_tf_policy.py' new_name='rllib\agents\maml\maml_tf_policy.py'>
 		<file_info nloc='365' complexity='36' token_count='2487'></file_info>
 		<modified_lines>
 			<added_lines>4,5,6,7,425</added_lines>
 			<deleted_lines>4,5,6,7,425</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\agents\maml\maml_torch_policy.py' new_name='rllib\agents\maml\maml_torch_policy.py'>
 		<file_info nloc='306' complexity='23' token_count='2081'></file_info>
 		<modified_lines>
 			<added_lines>4,5,8,12,358</added_lines>
 			<deleted_lines>4,7,8,11,358</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\agents\mbmpo\mbmpo_torch_policy.py' new_name='rllib\agents\mbmpo\mbmpo_torch_policy.py'>
 		<file_info nloc='75' complexity='1' token_count='412'></file_info>
 		<modified_lines>
 			<added_lines>8,10,17,88</added_lines>
 			<deleted_lines>6,9,10,88</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\agents\ppo\appo_tf_policy.py' new_name='rllib\agents\ppo\appo_tf_policy.py'>
 		<file_info nloc='344' complexity='16' token_count='2014'></file_info>
 		<modified_lines>
 			<added_lines>17,18,341,342</added_lines>
 			<deleted_lines>16,18,341,342</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\agents\ppo\appo_torch_policy.py' new_name='rllib\agents\ppo\appo_torch_policy.py'>
 		<file_info nloc='244' complexity='12' token_count='1724'></file_info>
 		<modified_lines>
 			<added_lines>29,30</added_lines>
 			<deleted_lines>13,30,31</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\agents\ppo\ppo_tf_policy.py' new_name='rllib\agents\ppo\ppo_tf_policy.py'>
 		<file_info nloc='277' complexity='19' token_count='1520'></file_info>
 		<method name='postprocess_ppo_gae' parameters='Policy,SampleBatch,AgentID,None,None'>
 				<method_info nloc='5' complexity='1' token_count='36' nesting_level='0' start_line='330' end_line='334'></method_info>
 			<added_lines>330,331,332,333,334</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>11,335,336,337,338,339,340,341,342,343,344,345,352</added_lines>
 			<deleted_lines>11,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,401</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\agents\ppo\ppo_torch_policy.py' new_name='rllib\agents\ppo\ppo_torch_policy.py'>
 		<file_info nloc='232' complexity='16' token_count='1226'></file_info>
 		<modified_lines>
 			<added_lines>10,11,12,21,22,281</added_lines>
 			<deleted_lines>10,11,12,13,22,23,282</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\agents\ppo\tests\test_ppo.py' new_name='rllib\agents\ppo\tests\test_ppo.py'>
 		<file_info nloc='322' complexity='53' token_count='2634'></file_info>
 		<method name='test_ppo_loss_function' parameters='self'>
 				<method_info nloc='81' complexity='18' token_count='587' nesting_level='1' start_line='226' end_line='321'></method_info>
 			<added_lines>256,257,258</added_lines>
 			<deleted_lines>258,259,260,261,262</deleted_lines>
 		</method>
 		<method name='test_ppo_free_log_std' parameters='self'>
 				<method_info nloc='33' complexity='8' token_count='234' nesting_level='1' start_line='177' end_line='224'></method_info>
 			<added_lines>216,217</added_lines>
 			<deleted_lines>215,216,217,218,219</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>8,9,10,11,12,13</added_lines>
 			<deleted_lines>8,9,10,11,12</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\agents\sac\sac.py' new_name='rllib\agents\sac\sac.py'>
 		<file_info nloc='105' complexity='6' token_count='417'></file_info>
 		<modified_lines>
 			<added_lines>76,106</added_lines>
 			<deleted_lines>76,77,107,108,109</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\agents\sac\sac_torch_policy.py' new_name='rllib\agents\sac\sac_torch_policy.py'>
 		<file_info nloc='400' complexity='15' token_count='2191'></file_info>
 		<modified_lines>
 			<added_lines>25</added_lines>
 			<deleted_lines>12,26</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\contrib\maddpg\maddpg_policy.py' new_name='rllib\contrib\maddpg\maddpg_policy.py'>
 		<file_info nloc='303' complexity='36' token_count='2170'></file_info>
 		<method name='gradients' parameters='self,optimizer,loss'>
 				<method_info nloc='7' complexity='2' token_count='68' nesting_level='1' start_line='267' end_line='273'></method_info>
 			<added_lines>268,269,270,271,272</added_lines>
 			<deleted_lines>268,269,270,271,272,273</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>274,275,276,277,278</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\evaluation\postprocessing.py' new_name='rllib\evaluation\postprocessing.py'>
 		<file_info nloc='116' complexity='3' token_count='623'></file_info>
 		<method name='compute_gae_for_sample_batch' parameters='Policy,SampleBatch,AgentID,None,None'>
 				<method_info nloc='5' complexity='1' token_count='36' nesting_level='0' start_line='84' end_line='88'></method_info>
 			<added_lines>84,85,86,87,88</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='discount_cumsum' parameters='ndarray,float'>
 				<method_info nloc='13' complexity='1' token_count='53' nesting_level='0' start_line='149' end_line='161'></method_info>
 			<added_lines>149,150,151,152,153,154,155,156,157,158,159,160,161</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>3,4,5,6,9,82,83,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148</added_lines>
 			<deleted_lines>5,6,7,8,9,10,11,12,13,14,15,16,17,18,19</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\tuned_examples\sac\mspacman-sac.yaml' new_name='rllib\tuned_examples\sac\mspacman-sac.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>17,18,20,21</added_lines>
 			<deleted_lines>17,18,20,21</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\utils\tf_ops.py' new_name='rllib\utils\tf_ops.py'>
 		<file_info nloc='129' complexity='42' token_count='1058'></file_info>
 		<method name='minimize_and_clip' parameters='optimizer,objective,var_list,clip_val'>
 				<method_info nloc='11' complexity='6' token_count='110' nesting_level='0' start_line='89' end_line='106'></method_info>
 			<added_lines>95,105,106</added_lines>
 			<deleted_lines>95,105,106</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>107,108</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\utils\torch_ops.py' new_name='rllib\utils\torch_ops.py'>
 		<file_info nloc='109' complexity='42' token_count='1052'></file_info>
 		<method name='apply_grad_clipping' parameters='policy,optimizer,loss'>
 				<method_info nloc='13' complexity='5' token_count='100' nesting_level='0' start_line='17' end_line='38'></method_info>
 			<added_lines>17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>39,40</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
