<bug_data>
<bug id='10372' author='ebilgin' open_date='2020-08-27T19:51:28Z' closed_time='2020-09-22T06:03:07Z'>
 	<summary>KeyError: 'action_logp' while consuming offline data [rllib]</summary>
 	<description>
 I am trying to consume offline data using the example &lt;denchmark-link:https://github.com/ray-project/ray/blob/master/rllib/examples/saving_experiences.py&gt;here&lt;/denchmark-link&gt;
 . I have the following code:
 &lt;denchmark-code&gt;from ray.tune.logger import pretty_print
 from ray.rllib.agents.dqn.dqn import DEFAULT_CONFIG
 from ray.rllib.agents.dqn.dqn import DQNTrainer
 import numpy as np
 
 config = DEFAULT_CONFIG.copy()
 config["env"] = "CartPole-v0"
 config["input"] = {"demo-out": 0.3, "sampler": 0.7}
 config["explore"] = False
 trainer = DQNTrainer(config=config)
 best_eps_length_avg = np.PINF
 while True:
     results = trainer.train()
     print(pretty_print(results))
 &lt;/denchmark-code&gt;
 
 I am getting the following error:
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "/home/enes/ws/code/arl/mt/test/consume_experiences.py", line 16, in &lt;module&gt;
     results = trainer.train()
   File "/home/enes/ws/envs/rlws/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 522, in train
     raise e
   File "/home/enes/ws/envs/rlws/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 508, in train
     result = Trainable.train(self)
   File "/home/enes/ws/envs/rlws/lib/python3.7/site-packages/ray/tune/trainable.py", line 332, in train
     result = self.step()
   File "/home/enes/ws/envs/rlws/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py", line 110, in step
     res = next(self.train_exec_impl)
   File "/home/enes/ws/envs/rlws/lib/python3.7/site-packages/ray/util/iter.py", line 758, in __next__
     return next(self.built_iterator)
   File "/home/enes/ws/envs/rlws/lib/python3.7/site-packages/ray/util/iter.py", line 785, in apply_foreach
     for item in it:
   File "/home/enes/ws/envs/rlws/lib/python3.7/site-packages/ray/util/iter.py", line 845, in apply_filter
     for item in it:
   File "/home/enes/ws/envs/rlws/lib/python3.7/site-packages/ray/util/iter.py", line 845, in apply_filter
     for item in it:
   File "/home/enes/ws/envs/rlws/lib/python3.7/site-packages/ray/util/iter.py", line 785, in apply_foreach
     for item in it:
   File "/home/enes/ws/envs/rlws/lib/python3.7/site-packages/ray/util/iter.py", line 845, in apply_filter
     for item in it:
   File "/home/enes/ws/envs/rlws/lib/python3.7/site-packages/ray/util/iter.py", line 1078, in build_union
     item = next(it)
   File "/home/enes/ws/envs/rlws/lib/python3.7/site-packages/ray/util/iter.py", line 758, in __next__
     return next(self.built_iterator)
   File "/home/enes/ws/envs/rlws/lib/python3.7/site-packages/ray/util/iter.py", line 785, in apply_foreach
     for item in it:
   File "/home/enes/ws/envs/rlws/lib/python3.7/site-packages/ray/util/iter.py", line 785, in apply_foreach
     for item in it:
   File "/home/enes/ws/envs/rlws/lib/python3.7/site-packages/ray/util/iter.py", line 785, in apply_foreach
     for item in it:
   [Previous line repeated 2 more times]
   File "/home/enes/ws/envs/rlws/lib/python3.7/site-packages/ray/rllib/execution/replay_ops.py", line 89, in gen_replay
     item = local_buffer.replay()
   File "/home/enes/ws/envs/rlws/lib/python3.7/site-packages/ray/rllib/execution/replay_buffer.py", line 331, in replay
     beta=self.prioritized_replay_beta)
   File "/home/enes/ws/envs/rlws/lib/python3.7/site-packages/ray/rllib/execution/replay_buffer.py", line 173, in sample
     batch = self._encode_sample(idxes)
   File "/home/enes/ws/envs/rlws/lib/python3.7/site-packages/ray/rllib/execution/replay_buffer.py", line 64, in _encode_sample
     out = SampleBatch.concat_samples([self._storage[i] for i in idxes])
   File "/home/enes/ws/envs/rlws/lib/python3.7/site-packages/ray/rllib/policy/sample_batch.py", line 93, in concat_samples
     out[k] = concat_aligned([s[k] for s in samples])
   File "/home/enes/ws/envs/rlws/lib/python3.7/site-packages/ray/rllib/policy/sample_batch.py", line 93, in &lt;listcomp&gt;
     out[k] = concat_aligned([s[k] for s in samples])
   File "/home/enes/ws/envs/rlws/lib/python3.7/site-packages/ray/rllib/policy/sample_batch.py", line 294, in __getitem__
     return self.data[key]
 KeyError: 'action_logp'
 &lt;/denchmark-code&gt;
 
 I am using ray 0.8.7 on Ubuntu 18.04 LTS. Tensorflow 2.3.0.
 	</description>
 	<comments>
 		<comment id='1' author='ebilgin' date='2020-08-29T05:35:13Z'>
 		It seems like somehow we're trying to read that key, though action_prob should be enough. As a workaround, you can try also writing action_logp (calling add_values(action_logp=np.log(action_prob))).
 		</comment>
 	</comments>
 </bug>
<commit id='daa03ba6e69b1ff3a2965dda12fc4904f21137af' author='Eric Liang' date='2020-09-21 23:03:06-07:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='rllib\examples\saving_experiences.py' new_name='rllib\examples\saving_experiences.py'>
 		<file_info nloc='43' complexity='0' token_count='258'></file_info>
 		<modified_lines>
 			<added_lines>45</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\execution\__init__.py' new_name='rllib\execution\__init__.py'>
 		<file_info nloc='37' complexity='0' token_count='153'></file_info>
 		<modified_lines>
 			<added_lines>1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\execution\concurrency_ops.py' new_name='rllib\execution\concurrency_ops.py'>
 		<file_info nloc='76' complexity='11' token_count='403'></file_info>
 		<modified_lines>
 			<added_lines>16,17,18,19,21,22,24,25,26,30</added_lines>
 			<deleted_lines>16,17,18,19,20,22,23,25,26,27</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\execution\rollout_ops.py' new_name='rllib\execution\rollout_ops.py'>
 		<file_info nloc='202' complexity='25' token_count='858'></file_info>
 		<modified_lines>
 			<added_lines>30,31,32,33,34,35,36</added_lines>
 			<deleted_lines>30,31,32,33,34,35,36,37</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
