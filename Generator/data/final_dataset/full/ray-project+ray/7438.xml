<bug_data>
<bug id='7438' author='soundway' open_date='2020-03-04T03:08:17Z' closed_time='2020-03-04T09:48:08Z'>
 	<summary>[rllib] Evaluation doesn’t work properly for PyTorch.</summary>
 	<description>
 This is not a contribution.
 Ray version: 0.8.1
 Python version: 3.6.8
 Pytorch version: 1.4
 OS: Ubuntu 18.04 Docker
 Training with Pytorch with evaluation enabled doesn’t work properly as it ends up calling tensorflow code. Here's a script that reproduces the problem:
 import gym
 from gym.spaces import Box
 from ray import tune
 
 class DummyEnv(gym.Env):
     def __init__(self, config):
         self.action_space = Box(0.0, 1.0, shape=(1,))
         self.observation_space = Box(0.0, 1.0, shape=(1, ))
 
     def reset(self):
         self.num_steps = 0
         return [0.0]
 
     def step(self, action):
         self.num_steps += 1
         return [0.0], 1.0, True if self.num_steps == 30 else False, {}
 
 tune.run(
     "PPO",
     config={
         "env": DummyEnv,
         "use_pytorch": True,
         "num_workers": 1,
         "evaluation_interval": 1,
         "evaluation_config": {"seed": 1},
     }
 )
 This will result in the following exception:
 &lt;denchmark-code&gt;ray.exceptions.RayTaskError(AssertionError): ray::PPO.train() (pid=33990)
   File "python/ray/_raylet.pyx", line 452, in ray._raylet.execute_task
   File "python/ray/_raylet.pyx", line 430, in ray._raylet.execute_task.function_executor
   File "/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer.py", line 513, in train
     evaluation_metrics = self._evaluate()
   File "/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer.py", line 685, in _evaluate
     lambda w: w.restore(ray.get(weights)))
   File "/usr/local/lib/python3.6/dist-packages/ray/rllib/evaluation/worker_set.py", line 110, in foreach_worker
     local_result = [func(self.local_worker())]
   File "/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer.py", line 685, in &lt;lambda&gt;
     lambda w: w.restore(ray.get(weights)))
   File "/usr/local/lib/python3.6/dist-packages/ray/rllib/evaluation/rollout_worker.py", line 764, in restore
     self.policy_map[pid].set_state(state)
   File "/usr/local/lib/python3.6/dist-packages/ray/rllib/policy/policy.py", line 320, in set_state
     self.set_weights(state)
   File "/usr/local/lib/python3.6/dist-packages/ray/rllib/policy/tf_policy.py", line 290, in set_weights
     return self._variables.set_weights(weights)
   File "/usr/local/lib/python3.6/dist-packages/ray/experimental/tf_utils.py", line 185, in set_weights
     assert assign_list, ("No variables in the input matched those in the "
 AssertionError: No variables in the input matched those in the network. Possible cause: Two networks were defined in the same TensorFlow graph. To fix this, place each network definition in its own tf.Graph.
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='soundway' date='2020-03-04T09:34:54Z'>
 		This is a bug (seems to have been there for a while). Fixing this now ...
 		</comment>
 		<comment id='2' author='soundway' date='2020-03-04T09:48:08Z'>
 		Thanks for filing this! Please use this PR here for a fix (will be merged into master in the next days):
 &lt;denchmark-link:https://github.com/ray-project/ray/pull/7443&gt;#7443&lt;/denchmark-link&gt;
 
 The problem was that the default policy (always TF) would be used for evaluation workers, no matter the setting in config[use_pytorch].
 		</comment>
 	</comments>
 </bug>
<commit id='c38224d8e53cefad62481d911f0efe90d2d74775' author='Eric Liang' date='2020-03-04 12:53:04-08:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='rllib\agents\trainer_template.py' new_name='rllib\agents\trainer_template.py'>
 		<file_info nloc='123' complexity='32' token_count='728'></file_info>
 		<method name='_init' parameters='self,config,env_creator'>
 				<method_info nloc='34' complexity='11' token_count='202' nesting_level='2' start_line='95' end_line='135'></method_info>
 			<added_lines>98,103,104,105,106,107,110,111,113,114,116,117</added_lines>
 			<deleted_lines>99,100,101,102,106,108</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77</added_lines>
 			<deleted_lines>41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
