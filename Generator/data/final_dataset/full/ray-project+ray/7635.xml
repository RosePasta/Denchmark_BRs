<bug_data>
<bug id='7635' author='fominskayage' open_date='2020-03-17T10:29:02Z' closed_time='2020-03-26T11:00:49Z'>
 	<summary>[rllib] DQN module 'tensorflow._api.v1.compat.v1.initializers' has no attribute 'GlorotUniform'</summary>
 	<description>
 &lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;
 
 I run rllib train --run=DQN --env=CartPole-v0 --config '{"n_step": 5, "v_min": -10.0, "v_max": 10.0, "noisy": true, "num_atoms": 10, "dueling": true, "double_q": true}'
 At first I get
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "/usr/local/lib/python3.7/site-packages/ray/tune/trial_runner.py", line 459, in _process_trial
     result = self.trial_executor.fetch_result(trial)
   File "/usr/local/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py", line 377, in fetch_result
     result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
   File "/usr/local/lib/python3.7/site-packages/ray/worker.py", line 1504, in get
     raise value.as_instanceof_cause()
 ray.exceptions.RayTaskError(AttributeError): ray::DQN.__init__() (pid=96308, ip=192.168.120.74)
   File "python/ray/_raylet.pyx", line 437, in ray._raylet.execute_task
   File "python/ray/_raylet.pyx", line 449, in ray._raylet.execute_task
   File "python/ray/_raylet.pyx", line 450, in ray._raylet.execute_task
   File "python/ray/_raylet.pyx", line 452, in ray._raylet.execute_task
   File "python/ray/_raylet.pyx", line 430, in ray._raylet.execute_task.function_executor
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py", line 86, in __init__
     Trainer.__init__(self, config, env, logger_creator)
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 447, in __init__
     super().__init__(config, logger_creator)
   File "/usr/local/lib/python3.7/site-packages/ray/tune/trainable.py", line 172, in __init__
     self._setup(copy.deepcopy(self.config))
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 591, in _setup
     self._init(self.config, self.env_creator)
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py", line 105, in _init
     self.config["num_workers"])
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 658, in _make_workers
     logdir=self.logdir)
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/evaluation/worker_set.py", line 60, in __init__
     RolloutWorker, env_creator, policy, 0, self._local_config)
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/evaluation/worker_set.py", line 262, in _make_worker
     _fake_sampler=config.get("_fake_sampler", False))
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py", line 355, in __init__
     self._build_policy_map(policy_dict, policy_config)
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py", line 820, in _build_policy_map
     policy_map[name] = cls(obs_space, act_space, merged_conf)
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/policy/tf_policy_template.py", line 138, in __init__
     obs_include_prev_action_reward=obs_include_prev_action_reward)
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/policy/dynamic_tf_policy.py", line 137, in __init__
     self.model = make_model(self, obs_space, action_space, config)
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/agents/dqn/dqn_policy.py", line 183, in build_q_model
     parameter_noise=config["parameter_noise"])
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/models/catalog.py", line 349, in get_model_v2
     name, **model_kwargs)
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/agents/dqn/distributional_q_model.py", line 185, in __init__
     q_out = build_action_value_in_scope(self.model_out)
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/agents/dqn/distributional_q_model.py", line 178, in build_action_value_in_scope
     return build_action_value(model_out)
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/agents/dqn/distributional_q_model.py", line 68, in build_action_value
     "hidden_%d" % i, action_out, q_hiddens[i], sigma0)
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/agents/dqn/distributional_q_model.py", line 259, in _noisy_layer
     initializer=tf.initializers.GlorotUniform())
   File "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/util/module_wrapper.py", line 193, in __getattr__
     attr = getattr(self._tfmw_wrapped_module, name)
 AttributeError: module 'tensorflow._api.v1.compat.v1.initializers' has no attribute 'GlorotUniform'
 &lt;/denchmark-code&gt;
 
 When I tried to change line 259 in /usr/local/lib/python3.7/site-packages/ray/rllib/agents/dqn/distributional_q_model.py from tf.initializers.GlorotUniform() to tf.initializers.glorot_uniform() and now I get this
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "/usr/local/lib/python3.7/site-packages/ray/tune/trial_runner.py", line 459, in _process_trial
     result = self.trial_executor.fetch_result(trial)
   File "/usr/local/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py", line 377, in fetch_result
     result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
   File "/usr/local/lib/python3.7/site-packages/ray/worker.py", line 1504, in get
     raise value.as_instanceof_cause()
 ray.exceptions.RayTaskError(FailedPreconditionError): ray::DQN.__init__() (pid=96346, ip=192.168.120.74)
   File "python/ray/_raylet.pyx", line 437, in ray._raylet.execute_task
   File "python/ray/_raylet.pyx", line 449, in ray._raylet.execute_task
   File "python/ray/_raylet.pyx", line 450, in ray._raylet.execute_task
   File "python/ray/_raylet.pyx", line 452, in ray._raylet.execute_task
   File "python/ray/_raylet.pyx", line 430, in ray._raylet.execute_task.function_executor
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py", line 86, in __init__
     Trainer.__init__(self, config, env, logger_creator)
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 447, in __init__
     super().__init__(config, logger_creator)
   File "/usr/local/lib/python3.7/site-packages/ray/tune/trainable.py", line 172, in __init__
     self._setup(copy.deepcopy(self.config))
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 591, in _setup
     self._init(self.config, self.env_creator)
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py", line 105, in _init
     self.config["num_workers"])
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 658, in _make_workers
     logdir=self.logdir)
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/evaluation/worker_set.py", line 60, in __init__
     RolloutWorker, env_creator, policy, 0, self._local_config)
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/evaluation/worker_set.py", line 262, in _make_worker
     _fake_sampler=config.get("_fake_sampler", False))
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py", line 355, in __init__
     self._build_policy_map(policy_dict, policy_config)
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py", line 820, in _build_policy_map
     policy_map[name] = cls(obs_space, act_space, merged_conf)
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/policy/tf_policy_template.py", line 138, in __init__
     obs_include_prev_action_reward=obs_include_prev_action_reward)
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/policy/dynamic_tf_policy.py", line 137, in __init__
     self.model = make_model(self, obs_space, action_space, config)
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/agents/dqn/dqn_policy.py", line 183, in build_q_model
     parameter_noise=config["parameter_noise"])
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/models/catalog.py", line 349, in get_model_v2
     name, **model_kwargs)
   File "/usr/local/lib/python3.7/site-packages/ray/rllib/agents/dqn/distributional_q_model.py", line 186, in __init__
     self.q_value_head = tf.keras.Model(self.model_out, q_out)
   File "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py", line 147, in __init__
     super(Model, self).__init__(*args, **kwargs)
   File "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py", line 164, in __init__
     self._init_graph_network(*args, **kwargs)
   File "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py", line 457, in _method_wrapper
     result = method(self, *args, **kwargs)
   File "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py", line 267, in _init_graph_network
     base_layer_utils.create_keras_history(self._nested_outputs)
   File "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py", line 184, in create_keras_history
     _, created_layers = _create_keras_history_helper(tensors, set(), [])
   File "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py", line 231, in _create_keras_history_helper
     layer_inputs, processed_ops, created_layers)
   File "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py", line 231, in _create_keras_history_helper
     layer_inputs, processed_ops, created_layers)
   File "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py", line 231, in _create_keras_history_helper
     layer_inputs, processed_ops, created_layers)
   [Previous line repeated 1 more time]
   File "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py", line 229, in _create_keras_history_helper
     constants[i] = backend.function([], op_input)([])
   File "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py", line 3476, in __call__
     run_metadata=self.run_metadata)
   File "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py", line 1472, in __call__
     run_metadata_ptr)
 tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value default_policy/q_func/action_value/output_fc_b
 	 [[{{node default_policy/q_func/action_value/output_fc_b/read}}]]
 &lt;/denchmark-code&gt;
 
 Ray version and other system information (Python version, TensorFlow version, OS):
 Ray version: 0.8.2
 Python version: 3.7
 TensorFlow version: 1.15.0
 OS: macOS 10.15.3
 &lt;denchmark-h:h3&gt;Reproduction&lt;/denchmark-h&gt;
 
 rllib train --run=DQN --env=CartPole-v0 --config '{"n_step": 5, "v_min": -10.0, "v_max": 10.0, "noisy": true, "num_atoms": 10, "dueling": true, "double_q": true}'
 
  I have verified my script runs in a clean environment and reproduces the issue.
  I have verified the issue also occurs with the latest wheels.
 
 	</description>
 	<comments>
 		<comment id='1' author='fominskayage' date='2020-03-17T17:42:39Z'>
 		I see this with TF2.0 as well.
 &lt;denchmark-link:https://github.com/sven1977&gt;@sven1977&lt;/denchmark-link&gt;
  will this be fixed by the param noise refactor?
 		</comment>
 		<comment id='2' author='fominskayage' date='2020-03-23T16:20:40Z'>
 		It's the noisy layers, not the param noise. Should be an easy fix. It's on my current sprint. ...
 		</comment>
 		<comment id='3' author='fominskayage' date='2020-03-26T09:32:59Z'>
 		I'm thinking it's the non-"keras.layers" that goes into constructing the Keras model that's causing this.
 If you replace (in distributional_q_model.py):
 &lt;denchmark-code&gt;action_activation = tf.nn.xw_plus_b(action_in, w + sigma_w * epsilon_w, b + epsilon_b)
 &lt;/denchmark-code&gt;
 
 with:
 &lt;denchmark-code&gt;action_activation = tf.keras.layers.Lambda(lambda x: tf.matmul(x, w + sigma_w * epsilon_w) + b + sigma_b * epsilon_b)(action_in)
 &lt;/denchmark-code&gt;
 
 it should work. I'll create a PR right now...
 		</comment>
 		<comment id='4' author='fominskayage' date='2020-03-26T11:00:49Z'>
 		Here is the PR that fixes this issue. There is also a new test case for a complete rainbow-compiled DQN agent in .
 &lt;denchmark-link:https://github.com/ray-project/ray/pull/7750&gt;#7750&lt;/denchmark-link&gt;
 
 Closing this issue. Please feel free to re-open in case it still does not work for you.
 Thanks!
 		</comment>
 		<comment id='5' author='fominskayage' date='2020-05-09T03:14:33Z'>
 		&lt;denchmark-link:https://github.com/sven1977&gt;@sven1977&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
  Somehow this fix hasn't made it into the nightly build, and this bug is causing a problem for me. Can you please fix this?
 		</comment>
 	</comments>
 </bug>
<commit id='93b5c38b7dc75b64a4812c1b300f48f9f1bf5d2b' author='Sven Mika' date='2020-03-26 22:08:34-07:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='rllib\agents\dqn\distributional_q_model.py' new_name='rllib\agents\dqn\distributional_q_model.py'>
 		<file_info nloc='210' complexity='23' token_count='1284'></file_info>
 		<method name='build_action_value._layer' parameters='x'>
 				<method_info nloc='10' complexity='1' token_count='77' nesting_level='4' start_line='104' end_line='113'></method_info>
 			<added_lines>104,105,106,107,108,109,110,111,112,113</added_lines>
 			<deleted_lines>104,105,106,107,108,109,110,111,112,113</deleted_lines>
 		</method>
 		<method name='build_state_score' parameters='model_out'>
 				<method_info nloc='26' complexity='5' token_count='158' nesting_level='2' start_line='121' end_line='146'></method_info>
 			<added_lines>132,135,145</added_lines>
 			<deleted_lines>132,133,136,137</deleted_lines>
 		</method>
 		<method name='build_action_value' parameters='model_out'>
 				<method_info nloc='42' complexity='8' token_count='283' nesting_level='2' start_line='62' end_line='119'></method_info>
 			<added_lines>73,74,79,94,97,103,104,105,106,107,108,109,110,111,112,113,114,115</added_lines>
 			<deleted_lines>73,74,79,80,95,96,104,105,106,107,108,109,110,111,112,113,114,115</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>187,255,262,263,264,265</added_lines>
 			<deleted_lines>147,148,190,191,259,266,267</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\agents\dqn\tests\test_dqn.py' new_name='rllib\agents\dqn\tests\test_dqn.py'>
 		<file_info nloc='89' complexity='15' token_count='712'></file_info>
 		<method name='test_dqn_compilation' parameters='self'>
 				<method_info nloc='32' complexity='4' token_count='218' nesting_level='1' start_line='13' end_line='51'></method_info>
 			<added_lines>18,19,20,21,22,23,24,25,26,27,28,29,30,31,33,34,35,41,42,43,44,45,46,51</added_lines>
 			<deleted_lines>18,19,25,26</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>2</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
