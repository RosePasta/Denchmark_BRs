<bug_data>
<bug id='12967' author='kbc8894' open_date='2020-12-18T06:47:19Z' closed_time='2021-01-20T18:16:53Z'>
 	<summary>[k8s][autoscaler] file_mounts option apply to head and worker node differently.</summary>
 	<description>
 &lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;
 
 Ray version and other system information (Python version, TensorFlow version, OS):
 python 3.8.5
 ray 1.0.1
 &lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;
 
 Please provide a short code snippet (less than 50 lines if possible) that can be copy-pasted to reproduce the issue. The snippet should have no external library dependencies (i.e., use fake or mock data / environments):
 When I run ray up cluster.yaml -y, /example folder is mounted in head node.
 However, /example/example folder is mounted in worker node.
 &lt;denchmark-code&gt;# An unique identifier for the head node and workers of this cluster.
 cluster_name: default
 
 # The minimum number of workers nodes to launch in addition to the head
 # node. This number should be &gt;= 0.
 min_workers: 0
 
 # The maximum number of workers nodes to launch in addition to the head
 # node. This takes precedence over min_workers.
 max_workers: 2
 
 # The autoscaler will scale up the cluster faster with higher upscaling speed.
 # E.g., if the task requires adding more nodes then autoscaler will gradually
 # scale up the cluster in chunks of upscaling_speed*currently_running_nodes.
 # This number should be &gt; 0.
 upscaling_speed: 1.0
 
 # If a node is idle for this many minutes, it will be removed.
 idle_timeout_minutes: 5
 
 # Kubernetes resources that need to be configured for the autoscaler to be
 # able to manage the Ray cluster. If any of the provided resources don't
 # exist, the autoscaler will attempt to create them. If this fails, you may
 # not have the required permissions and will have to request them to be
 # created by your cluster administrator.
 provider:
     type: kubernetes
 
     # Exposing external IP addresses for ray pods isn't currently supported.
     use_internal_ips: true
 
     # Namespace to use for all resources created.
     namespace: ray
 
     # ServiceAccount created by the autoscaler for the head node pod that it
     # runs in. If this field isn't provided, the head pod config below must
     # contain a user-created service account with the proper permissions.
     autoscaler_service_account:
         apiVersion: v1
         kind: ServiceAccount
         metadata:
             name: autoscaler
 
     # Role created by the autoscaler for the head node pod that it runs in.
     # If this field isn't provided, the role referenced in
     # autoscaler_role_binding must exist and have at least these permissions.
     autoscaler_role:
         kind: Role
         apiVersion: rbac.authorization.k8s.io/v1
         metadata:
             name: autoscaler
         rules:
         - apiGroups: [""]
           resources: ["pods", "pods/status", "pods/exec"]
           verbs: ["get", "watch", "list", "create", "delete", "patch"]
 
     # RoleBinding created by the autoscaler for the head node pod that it runs
     # in. If this field isn't provided, the head pod config below must contain
     # a user-created service account with the proper permissions.
     autoscaler_role_binding:
         apiVersion: rbac.authorization.k8s.io/v1
         kind: RoleBinding
         metadata:
             name: autoscaler
         subjects:
         - kind: ServiceAccount
           name: autoscaler
         roleRef:
             kind: Role
             name: autoscaler
             apiGroup: rbac.authorization.k8s.io
 
     services:
       # Service that maps to the head node of the Ray cluster.
       - apiVersion: v1
         kind: Service
         metadata:
             # NOTE: If you're running multiple Ray clusters with services
             # on one Kubernetes cluster, they must have unique service
             # names.
             name: ray-head
         spec:
             # This selector must match the head node pod's selector below.
             selector:
                 component: ray-head
             ports:
                 - protocol: TCP
                   port: 8000
                   targetPort: 8000
 
       # Service that maps to the worker nodes of the Ray cluster.
       - apiVersion: v1
         kind: Service
         metadata:
             # NOTE: If you're running multiple Ray clusters with services
             # on one Kubernetes cluster, they must have unique service
             # names.
             name: ray-workers
         spec:
             # This selector must match the worker node pods' selector below.
             selector:
                 component: ray-worker
             ports:
                 - protocol: TCP
                   port: 8000
                   targetPort: 8000
 
 # Kubernetes pod config for the head node pod.
 head_node:
     apiVersion: v1
     kind: Pod
     metadata:
         # Automatically generates a name for the pod with this prefix.
         generateName: ray-head-
 
         # Must match the head node service selector above if a head node
         # service is required.
         labels:
             component: ray-head
     spec:
         # Change this if you altered the autoscaler_service_account above
         # or want to provide your own.
         serviceAccountName: autoscaler
 
         # Restarting the head node automatically is not currently supported.
         # If the head node goes down, `ray up` must be run again.
         restartPolicy: Never
 
         # This volume allocates shared memory for Ray to use for its plasma
         # object store. If you do not provide this, Ray will fall back to
         # /tmp which cause slowdowns if is not a shared memory volume.
         volumes:
         - name: dshm
           emptyDir:
               medium: Memory
 
         containers:
         - name: ray-node
           imagePullPolicy: Always
           # You are free (and encouraged) to use your own container image,
           # but it should have the following installed:
           #   - rsync (used for `ray rsync` commands and file mounts)
           #   - screen (used for `ray attach`)
           #   - kubectl (used by the autoscaler to manage worker pods)
           image: rayproject/ray:nightly
           # Do not change this command - it keeps the pod alive until it is
           # explicitly killed.
           command: ["/bin/bash", "-c", "--"]
           args: ["trap : TERM INT; sleep infinity &amp; wait;"]
           ports:
               - containerPort: 6379 # Redis port.
               - containerPort: 6380 # Redis port.
               - containerPort: 6381 # Redis port.
               - containerPort: 12345 # Ray internal communication.
               - containerPort: 12346 # Ray internal communication.
 
           # This volume allocates shared memory for Ray to use for its plasma
           # object store. If you do not provide this, Ray will fall back to
           # /tmp which cause slowdowns if is not a shared memory volume.
           volumeMounts:
               - mountPath: /dev/shm
                 name: dshm
           resources:
               requests:
                   cpu: 1000m
                   memory: 512Mi
               limits:
                   # The maximum memory that this pod is allowed to use. The
                   # limit will be detected by ray and split to use 10% for
                   # redis, 30% for the shared memory object store, and the
                   # rest for application memory. If this limit is not set and
                   # the object store size is not set manually, ray will
                   # allocate a very large object store in each pod that may
                   # cause problems for other pods.
                   memory: 2Gi
           env:
               # This is used in the head_start_ray_commands below so that
               # Ray can spawn the correct number of processes. Omitting this
               # may lead to degraded performance.
               - name: MY_CPU_REQUEST
                 valueFrom:
                     resourceFieldRef:
                         resource: requests.cpu
 
 # Kubernetes pod config for worker node pods.
 worker_nodes:
     apiVersion: v1
     kind: Pod
     metadata:
         # Automatically generates a name for the pod with this prefix.
         generateName: ray-worker-
 
         # Must match the worker node service selector above if a worker node
         # service is required.
         labels:
             component: ray-worker
     spec:
         serviceAccountName: default
 
         # Worker nodes will be managed automatically by the head node, so
         # do not change the restart policy.
         restartPolicy: Never
 
         # This volume allocates shared memory for Ray to use for its plasma
         # object store. If you do not provide this, Ray will fall back to
         # /tmp which cause slowdowns if is not a shared memory volume.
         volumes:
         - name: dshm
           emptyDir:
               medium: Memory
 
         containers:
         - name: ray-node
           imagePullPolicy: Always
           # You are free (and encouraged) to use your own container image,
           # but it should have the following installed:
           #   - rsync (used for `ray rsync` commands and file mounts)
           image: rayproject/ray:nightly
           # Do not change this command - it keeps the pod alive until it is
           # explicitly killed.
           command: ["/bin/bash", "-c", "--"]
           args: ["trap : TERM INT; sleep infinity &amp; wait;"]
           ports:
               - containerPort: 12345 # Ray internal communication.
               - containerPort: 12346 # Ray internal communication.
 
           # This volume allocates shared memory for Ray to use for its plasma
           # object store. If you do not provide this, Ray will fall back to
           # /tmp which cause slowdowns if is not a shared memory volume.
           volumeMounts:
               - mountPath: /dev/shm
                 name: dshm
           resources:
               requests:
                   cpu: 1000m
                   memory: 512Mi
               limits:
                   # This memory limit will be detected by ray and split into
                   # 30% for plasma, and 70% for workers.
                   memory: 2Gi
           env:
               # This is used in the head_start_ray_commands below so that
               # Ray can spawn the correct number of processes. Omitting this
               # may lead to degraded performance.
               - name: MY_CPU_REQUEST
                 valueFrom:
                     resourceFieldRef:
                         resource: requests.cpu
 
 # Files or directories to copy to the head and worker nodes. The format is a
 # dictionary from REMOTE_PATH: LOCAL_PATH, e.g.
 file_mounts: {
       "/example": "/path/to/example"
 #    "/path1/on/remote/machine": "/path1/on/local/machine",
 #    "/path2/on/remote/machine": "/path2/on/local/machine",
 }
 
 # Files or directories to copy from the head node to the worker nodes. The format is a
 # list of paths. The same path on the head node will be copied to the worker node.
 # This behavior is a subset of the file_mounts behavior. In the vast majority of cases
 # you should just use file_mounts. Only use this if you know what you're doing!
 cluster_synced_files: []
 
 # Whether changes to directories in file_mounts or cluster_synced_files in the head node
 # should sync to the worker node continuously
 file_mounts_sync_continuously: False
 
 # Patterns for files to exclude when running rsync up or rsync down.
 # This is not supported on kubernetes.
 # rsync_exclude: []
 
 # Pattern files to use for filtering out files when running rsync up or rsync down. The file is searched for
 # in the source directory and recursively through all subdirectories. For example, if .gitignore is provided
 # as a value, the behavior will match git's behavior for finding and using .gitignore files.
 # This is not supported on kubernetes.
 # rsync_filter: []
 
 # List of commands that will be run before `setup_commands`. If docker is
 # enabled, these commands will run outside the container and before docker
 # is setup.
 initialization_commands: []
 
 # List of shell commands to run to set up nodes.
 setup_commands: []
 
 # Custom commands that will be run on the head node after common setup.
 head_setup_commands: []
 
 # Custom commands that will be run on worker nodes after common setup.
 worker_setup_commands: []
 
 # Command to start ray on the head node. You don't need to change this.
 # Note webui-host is set to 0.0.0.0 so that kubernetes can port forward.
 head_start_ray_commands:
     - ray stop
     - ulimit -n 65536; ray start --head --num-cpus=$MY_CPU_REQUEST --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml --dashboard-host 0.0.0.0
 
 # Command to start ray on worker nodes. You don't need to change this.
 worker_start_ray_commands:
     - ray stop
     - ulimit -n 65536; ray start --num-cpus=$MY_CPU_REQUEST --address=$RAY_HEAD_IP:6379 --object-manager-port=8076
 &lt;/denchmark-code&gt;
 
 If the code snippet cannot be run by itself, the issue will be closed with "needs-repro-script".
 
  I have verified my script runs in a clean environment and reproduces the issue.
  I have verified the issue also occurs with the latest wheels.
 
 	</description>
 	<comments>
 		<comment id='1' author='kbc8894' date='2020-12-18T08:50:49Z'>
 		I think this is similar to &lt;denchmark-link:https://github.com/ray-project/ray/issues/12909&gt;#12909&lt;/denchmark-link&gt;
  -- can you install  via  on your container?
 cc @Gekho457 this is because kubectl cp doesn't support directories properly.
 		</comment>
 		<comment id='2' author='kbc8894' date='2020-12-18T09:21:59Z'>
 		
 I think this is similar to #12909 -- can you install rsync via apt-get install rsync on your container?
 cc @Gekho457 this is because kubectl cp doesn't support directories properly.
 
 &lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
  I use &lt;denchmark-link:https://hub.docker.com/r/rayproject/ray-ml&gt;rayproject/ray-ml&lt;/denchmark-link&gt;
  image.
 I think &lt;denchmark-link:https://github.com/ray-project/ray/blob/master/docker/base-deps/Dockerfile&gt;base-dep&lt;/denchmark-link&gt;
  image already installed rsync.
 		</comment>
 		<comment id='3' author='kbc8894' date='2020-12-18T16:44:04Z'>
 		&lt;denchmark-link:https://github.com/kbc8894&gt;@kbc8894&lt;/denchmark-link&gt;
 
 Could you try using a newer version of Ray, for example  or the latest master?
 I think the problem should have been fixed by this PR: &lt;denchmark-link:https://github.com/ray-project/ray/pull/12356&gt;#12356&lt;/denchmark-link&gt;
 .
 Probably what you're experiencing is that rsync is being used in one place and (as a result of the bug fixed in that PR) kubectl cp is being used in another place.
 		</comment>
 		<comment id='4' author='kbc8894' date='2020-12-18T17:12:25Z'>
 		
 @richardliaw I use rayproject/ray-ml image.
 
 Ah, if you're currently running ray up out of rayproject/ray-ml, one approach is to switch to a more recent image: rayproject/ray-ml:nightly.
 		</comment>
 		<comment id='5' author='kbc8894' date='2020-12-19T05:40:23Z'>
 		
 @kbc8894
 Could you try using a newer version of Ray, for example ray1.1.0 or the latest master?
 I think the problem should have been fixed by this PR: #12356.
 Probably what you're experiencing is that rsync is being used in one place and (as a result of the bug fixed in that PR) kubectl cp is being used in another place.
 
 &lt;denchmark-link:https://github.com/DmitriGekhtman&gt;@DmitriGekhtman&lt;/denchmark-link&gt;
 
 rayproject/ray-ml:1.1.0 is not working
 error message is
 &lt;denchmark-code&gt;2020-12-19 14:09:05,413 INFO command_runner.py:165 -- NodeUpdater: ray-head-bc-7724m: Running kubectl -n ray exec -it ray-head-bc-7724m -- bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; (mkdir -p /example)'
 mkdir: cannot create directory ‘/example’: Permission denied
 &lt;/denchmark-code&gt;
 
 rayproject/ray-ml:latest is also not working
 &lt;denchmark-code&gt;(base) root@ray-worker-bc-hpd9n:/example# pwd
 /example
 (base) root@ray-worker-bc-hpd9n:/example# cd example/
 (base) root@ray-worker-bc-hpd9n:/example/example# pwd
 /example/example
 &lt;/denchmark-code&gt;
 
 rayproject/ray-ml:nightly is not working
 error message is
 &lt;denchmark-code&gt;2020-12-19 14:39:06,092 INFO command_runner.py:165 -- NodeUpdater: ray-head-bc-xsrxv: Running kubectl -n ray exec -it ray-head-bc-xsrxv -- bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; (mkdir -p /example)'
 mkdir: cannot create directory ‘/example’: Permission denie
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='6' author='kbc8894' date='2020-12-19T06:36:22Z'>
 		I see -- that's happening because, as of this PR &lt;denchmark-link:https://github.com/ray-project/ray/issues/10786&gt;#10786&lt;/denchmark-link&gt;
 , the Ray images have a non-root user.
 For now, the way to get around this is to mount into a subdirectory of the remote container's home folder:
 "~/example"
 Is that sufficient for your use case?
 We'll update the examples to
 (a) suggest mounting into a subdirectory of the container's home
 (b) explain that the ray images use a non-root user
 		</comment>
 	</comments>
 </bug>
<commit id='4832b3906611a92b027747c74bec6051dfb3fe72' author='Dmitri Gekhtman' date='2020-12-19 16:09:24-08:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\kubernetes\defaults.yaml' new_name='python\ray\autoscaler\kubernetes\defaults.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>253,254,256,257</added_lines>
 			<deleted_lines>253,254</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\kubernetes\example-full.yaml' new_name='python\ray\autoscaler\kubernetes\example-full.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>253,254,256,257</added_lines>
 			<deleted_lines>253,254</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\kubernetes\example-ingress.yaml' new_name='python\ray\autoscaler\kubernetes\example-ingress.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>289,290,292,293</added_lines>
 			<deleted_lines>289,290</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
