<bug_data>
<bug id='11551' author='PidgeyBE' open_date='2020-10-22T09:37:54Z' closed_time='2020-11-23T22:55:13Z'>
 	<summary>[autoscaler] Too many workers are scaled in kubernetes</summary>
 	<description>
 &lt;denchmark-h:h2&gt;What is the problem?&lt;/denchmark-h&gt;
 
 
 ray = 1.0.0
 autoscaling on k8s
 
 When autoscaling in kubernetes, it happens sometimes (almost 50% in my case) that instead of 1 worker, 2 workers are scaled up.
 &lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;
 
 
 Have a ray autoscale cluster on kubernetes
 Deploy an actor that requires new worker resources
 Ray autoscale will start an extra worker
 When the first worker is (almost) ready, for some reason, the autoscaler triggers another worker to start
 I see this happens because for some reason NumNodesConnected becomes equal to NumNodesUsed.
 
 &lt;denchmark-code&gt; - NumNodesConnected: 3
  - NumNodesUsed: 3
 &lt;/denchmark-code&gt;
 
 A few seconds later (while the extra node is already being started and the actor is deployed) I see:
 &lt;denchmark-code&gt; - NumNodesConnected: 3
  - NumNodesUsed: 1.2
 &lt;/denchmark-code&gt;
 
 I've cut the important part out of the monitor.err logs &lt;denchmark-link:https://github.com/ray-project/ray/files/5421744/monitor.err.txt&gt;monitor.err.txt&lt;/denchmark-link&gt;
 
 It seems like there is some race condition that causes the autoscaler to trigger a worker to start, while the actor is deploying...
 
  I have verified my script runs in a clean environment and reproduces the issue.
  I have verified the issue also occurs with the latest wheels.
 
 BR, Pieterjan
 cc &lt;denchmark-link:https://github.com/edoakes&gt;@edoakes&lt;/denchmark-link&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='PidgeyBE' date='2020-10-22T16:49:53Z'>
 		Cc &lt;denchmark-link:https://github.com/wuisawesome&gt;@wuisawesome&lt;/denchmark-link&gt;
  probably should handle asap
 		</comment>
 		<comment id='2' author='PidgeyBE' date='2020-10-22T17:02:47Z'>
 		Cc &lt;denchmark-link:https://github.com/AmeerHajAli&gt;@AmeerHajAli&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 
 I'll look into this, but feel free to steal it if you have the bandwidth
 		</comment>
 		<comment id='3' author='PidgeyBE' date='2020-10-22T18:09:44Z'>
 		&lt;denchmark-link:https://github.com/wuisawesome&gt;@wuisawesome&lt;/denchmark-link&gt;
  I vaguely remember seeing this in the past. IIRC the issue is that the autoscaler has no notion of "pending" nodes, so while the node is pending in k8s (which can take awhile), it continues to add more nodes because resource utilization is still high.
 		</comment>
 		<comment id='4' author='PidgeyBE' date='2020-10-22T18:22:20Z'>
 		What's weird is that the autoscaler is reporting it has 1/1 target nodes, but 2 nodes connected. There seems to be a connected node that is not managed by the autoscaler, which leads to an off-by-one error.
 &lt;denchmark-link:https://github.com/wuisawesome&gt;@wuisawesome&lt;/denchmark-link&gt;
  is this related to that issue you had with an unmanaged node being counted as a node?
 &lt;denchmark-link:https://github.com/edoakes&gt;@edoakes&lt;/denchmark-link&gt;
  I do see  so it seems we do have pending nodes. What's odd is the number of "used" nodes is 3 when the load is low, which seems wrong.
 		</comment>
 		<comment id='5' author='PidgeyBE' date='2020-10-22T18:25:56Z'>
 		huh, ok, that's actually sort of promising because it indicates that there's just some off-by-one or similar bug somewhere instead of a more intrinsic problem. &lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
  does this mean you were able to easily reproduce the issue?
 		</comment>
 		<comment id='6' author='PidgeyBE' date='2020-10-22T18:37:04Z'>
 		Indeed, I think there is notion of pending nodes in k8s autoscaler.
 e.g. If I do
 &lt;denchmark-code&gt;setup_commands:
     - sleep 600
 &lt;/denchmark-code&gt;
 
 It does not influence the problem (doesnt make it better or worse) ...
 I'm not sure about the off-by-one thing...
 It starts with 0/0, at this time the unmanaged node is already present and not counted.
 Then, when deploying the Actor, it goes to 0/1.
 After some time, the status becomes 1/1 but the added GPU is not yet visible in the Resource list.
 At the point the GPU becomes visible in the ResourceUsage list, the target nodes are 1/2 and an extra node is spawned.
 Then it goes to 2/2 (while actor is not yet deployed (0.0/1.0 GPU)) and quickly to 2/1 as soon as the Actor is actually deployed (1.0/1.0 GPU)
 		</comment>
 		<comment id='7' author='PidgeyBE' date='2020-10-22T19:10:26Z'>
 		&lt;denchmark-link:https://github.com/edoakes&gt;@edoakes&lt;/denchmark-link&gt;
  I was just reading the logs above.
 It's possible this PR fixes the issue: &lt;denchmark-link:https://github.com/ray-project/ray/pull/11458&gt;#11458&lt;/denchmark-link&gt;
  (it should be available in nightly wheels by now)
 		</comment>
 		<comment id='8' author='PidgeyBE' date='2020-10-26T15:59:17Z'>
 		&lt;denchmark-link:https://github.com/PidgeyBE&gt;@PidgeyBE&lt;/denchmark-link&gt;
  have you had a chance to verify that the above PR fixes your issue?
 		</comment>
 		<comment id='9' author='PidgeyBE' date='2020-10-26T15:59:18Z'>
 		&lt;denchmark-link:https://github.com/PidgeyBE&gt;@PidgeyBE&lt;/denchmark-link&gt;
  have you had a chance to verify that the above PR fixes your issue?
 		</comment>
 		<comment id='10' author='PidgeyBE' date='2020-10-27T08:35:47Z'>
 		&lt;denchmark-link:https://github.com/edoakes&gt;@edoakes&lt;/denchmark-link&gt;
  Not yet. I'll try to do it today! Our build chain is not so flexible for changing ray versions but if the API is stable between 1.0 and the nightly it might go faster. I'll try
 		</comment>
 		<comment id='11' author='PidgeyBE' date='2020-10-27T16:55:24Z'>
 		&lt;denchmark-link:https://github.com/edoakes&gt;@edoakes&lt;/denchmark-link&gt;
  I tried with latest nightly wheel, but the autoscaling monitor doesn't seem to start so I can't test...
 &lt;denchmark-code&gt;2020-10-27 16:49:26,179	INFO log_timer.py:27 -- NodeUpdater: ray-head-qmcx9: Ray start commands succeeded [LogTimer=5822ms]
 2020-10-27 16:49:26,179	INFO log_timer.py:27 -- NodeUpdater: ray-head-qmcx9: Applied config 47da8d49fe813c3c58828168d32f7bc8b397761c  [LogTimer=49794ms]
 2020-10-27 16:49:26,269	INFO commands.py:716 -- get_or_create_head_node: Head node up-to-date, IP address is: 10.208.0.6
 2020-10-27 16:48:33,431	INFO commands.py:204 -- Cluster: dynamic-ray
 To monitor autoscaling activity, you can run:
   ray exec /etc/ray/config.yaml 'tail -n 100 -f /tmp/ray/session_latest/logs/monitor*'
 To open a console on the cluster:
   ray attach /etc/ray/config.yaml
 To get a remote shell to the cluster manually, run:
   kubectl -n dynamic-ray exec -it ray-head-qmcx9 bash
 &lt;/denchmark-code&gt;
 
 This looks ok (as in ray 1.0.0), but when I look into the ray head, it seems like the monitor doesnt start:
 &lt;denchmark-code&gt;(regulator) root@ray-head-qmcx9:/app/core# tail -n 100 -f /tmp/ray/session_latest/logs/monitor*
 ==&gt; /tmp/ray/session_latest/logs/monitor.err &lt;==
                                                                 '"conda '
                                                                 'activate '
                                                                 'regulator" '
                                                                 '&gt;&gt; '
                                                                 '~/.bashrc; '
                                                                 'trap : '
                                                                 'TERM INT; '
                                                                 'sleep '
                                                                 'infinity '
                                                                 '&amp; wait;'],
                                                        'command': ['/bin/bash',
                                                                    '-c',
                                                                    '--'],
                                                        'env': ....}},
                       'service': {'apiVersion': 'v1',
                                   'kind': 'Service',
                                   'spec': {'p.....
      'worker_setup_commands': ['./deploy/install-pipelines.sh',
                                'python ./main/run_on_worker_start.py'],
      'worker_start_ray_commands': ['ray stop',
                                    'ulimit -n 65536; ray start '
                                    '--num-cpus=$MY_CPU_REQUEST '
                                    '--address=$RAY_HEAD_IP:6379 '
                                    '--object-manager-port=8076 '
                                    '--resources=\'{"READY":9999}\'']}
 
 ==&gt; /tmp/ray/session_latest/logs/monitor.out &lt;==
 &lt;/denchmark-code&gt;
 
 Also when I start actors, no workers are spawned...
 		</comment>
 		<comment id='12' author='PidgeyBE' date='2020-11-05T07:35:47Z'>
 		I will try to reproduce this on a local k8s instance.
 		</comment>
 		<comment id='13' author='PidgeyBE' date='2020-11-09T06:35:32Z'>
 		Ok so I tried running this
 import ray
 
 ray.init(address="auto")
 
 @ray.remote(num_cpus=1)
 def spin():
     import time
     time.sleep(10000)
 
 ray.get(spin.remote())
 On nightly wheel+docker image, using minikube on a pretty beefy machine (32 cores). Running:
 &lt;denchmark-code&gt;$ ray up cluster.yaml
 Cluster: default
 
 Loaded cached provider configuration
 If you experience issues with the cloud provider, try re-running the command with --no-config-cache.
 No head node found. Launching a new cluster. Confirm [y/N]: y
 
 Acquiring an up-to-date head node
 2020-11-09 06:30:43,901	INFO node_provider.py:112 -- KubernetesNodeProvider: calling create_namespaced_pod (count=1).
   Launched a new head node
   Fetching the new head node
 
 &lt;1/1&gt; Setting up head node
   Prepared bootstrap config
 2020-11-09 06:30:43,924	INFO node_provider.py:85 -- KubernetesNodeProvider: Caught a 409 error while setting node tags. Retrying...
   New status: waiting-for-ssh
   [1/6] Waiting for SSH to become available
     Running `uptime` as a test.
 2020-11-09 06:30:44,442	INFO command_runner.py:165 -- NodeUpdater: ray-head-mp2vs: Running kubectl -n ray exec -it ray-head-mp2vs -- bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; (uptime)'
 error: unable to upgrade connection: container not found ("ray-node")
     SSH still not available (Exit Status 1): k u b e c t l   - n   r a y   e x e c   - i t   r a y - h e a d - m p 2 v s   - -   b a s h   - - l o g i n   - c   - i   ' t r u e   &amp; &amp;   s o u r c e   ~ / . b a s h r c   &amp; &amp;   e x p o r t   O M P _ N U M _ T H R E A D S = 1   P Y T H O N W A R N I N G S = i g n o r e   &amp; &amp;   ( u p t i m e ) ', retrying in 5 seconds.
 2020-11-09 06:30:49,569	INFO command_runner.py:165 -- NodeUpdater: ray-head-mp2vs: Running kubectl -n ray exec -it ray-head-mp2vs -- bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; (uptime)'
  06:30:50 up 3 days, 22:46,  0 users,  load average: 0.69, 0.47, 0.35
     Success.
   Updating cluster configuration. [hash=a9714d6ad81e442dc995ee12c944110d9ddddcde]
   New status: syncing-files
   [3/6] Processing file mounts
 2020-11-09 06:30:50,132	INFO command_runner.py:165 -- NodeUpdater: ray-head-mp2vs: Running kubectl -n ray exec -it ray-head-mp2vs -- bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; (mkdir -p ~)'
 /home/ubuntu/.local/lib/python3.8/site-packages/ray/autoscaler/_private/command_runner.py:204: UserWarning: NodeUpdater: ray-head-mp2vs: rsync failed: '[Errno 2] No such file or directory: '/home/ubuntu/.local/lib/python3.8/site-packages/ray/autoscaler/_private/kubernetes/kubectl-rsync.sh''. Falling back to 'kubectl cp'
   warnings.warn(
   [4/6] No worker file mounts to sync
   New status: setting-up
   [3/6] No initialization commands to run.
   [4/6] No setup commands to run.
   [6/6] Starting the Ray runtime
 2020-11-09 06:30:50,937	INFO command_runner.py:165 -- NodeUpdater: ray-head-mp2vs: Running kubectl -n ray exec -it ray-head-mp2vs -- bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; (ray stop)'
 Did not find any active Ray processes.
 2020-11-09 06:30:51,989	INFO command_runner.py:165 -- NodeUpdater: ray-head-mp2vs: Running kubectl -n ray exec -it ray-head-mp2vs -- bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; (ulimit -n 65536; ray start --head --num-cpus=$MY_CPU_REQUEST --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml --dashboard-host 0.0.0.0)'
 Local node IP: 172.17.0.3
 2020-11-09 06:30:53,338	INFO services.py:1110 -- View the Ray dashboard at http://172.17.0.3:8265
 
 --------------------
 Ray runtime started.
 --------------------
 
 Next steps
   To connect to this Ray runtime from another node, run
     ray start --address='172.17.0.3:6379' --redis-password='5241590000000000'
 
   Alternatively, use the following Python code:
     import ray
     ray.init(address='auto', _redis_password='5241590000000000')
 
   If connection fails, check your firewall settings and network configuration.
 
   To terminate the Ray runtime, run
     ray stop
   New status: up-to-date
 
 Useful commands
   Monitor autoscaling with
     ray exec /home/ubuntu/robovision-repro/cluster.yaml 'tail -n 100 -f /tmp/ray/session_latest/logs/monitor*'
   Connect to a terminal on the cluster head:
     ray attach /home/ubuntu/robovision-repro/cluster.yaml
   Get a remote shell to the cluster manually:
     kubectl -n ray exec -it ray-head-mp2vs bash
 
 
 $ ray submit cluster.yaml request_resource.py
 Loaded cached provider configuration
 If you experience issues with the cloud provider, try re-running the command with --no-config-cache.
 /home/ubuntu/.local/lib/python3.8/site-packages/ray/autoscaler/_private/command_runner.py:204: UserWarning: NodeUpdater: ray-head-mp2vs: rsync failed: '[Errno 2] No such file or directory: '/home/ubuntu/.local/lib/python3.8/site-packages/ray/autoscaler/_private/kubernetes/kubectl-rsync.sh''. Falling back to 'kubectl cp'
   warnings.warn(
 2020-11-09 06:31:01,227	INFO command_runner.py:165 -- NodeUpdater: ray-head-mp2vs: Running kubectl -n ray exec -it ray-head-mp2vs -- bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; (python ~/request_resource.py)'
 2020-11-09 06:31:01,989	INFO worker.py:672 -- Connecting to existing Ray cluster at address: 172.17.0.3:6379
 2020-11-09 06:31:45,315	WARNING worker.py:1111 -- The monitor failed with the following error:
 Traceback (most recent call last):
   File "/root/anaconda3/lib/python3.7/site-packages/ray/monitor.py", line 356, in &lt;module&gt;
     monitor.run()
   File "/root/anaconda3/lib/python3.7/site-packages/ray/monitor.py", line 301, in run
     self._run()
   File "/root/anaconda3/lib/python3.7/site-packages/ray/monitor.py", line 257, in _run
     self.autoscaler.update()
   File "/root/anaconda3/lib/python3.7/site-packages/ray/autoscaler/_private/autoscaler.py", line 139, in update
     raise e
   File "/root/anaconda3/lib/python3.7/site-packages/ray/autoscaler/_private/autoscaler.py", line 128, in update
     self._update()
   File "/root/anaconda3/lib/python3.7/site-packages/ray/autoscaler/_private/autoscaler.py", line 222, in _update
     self.load_metrics.get_static_node_resources_by_ip())
   File "/root/anaconda3/lib/python3.7/site-packages/ray/autoscaler/_private/resource_demand_scheduler.py", line 84, in get_nodes_to_launch
     self._infer_legacy_node_resources_if_needed(static_node_resources)
   File "/root/anaconda3/lib/python3.7/site-packages/ray/autoscaler/_private/resource_demand_scheduler.py", line 182, in _infer_legacy_node_resources_if_needed
     assert len(static_node_resources) == 1  # Only the head node.
 AssertionError
 
 ^CTraceback (most recent call last):
   File "/root/request_resource.py", line 10, in &lt;module&gt;
     ray.get(spin.remote())
   File "/root/anaconda3/lib/python3.7/site-packages/ray/worker.py", line 1465, in get
     values = worker.get_objects(object_refs, timeout=timeout)
   File "/root/anaconda3/lib/python3.7/site-packages/ray/worker.py", line 315, in get_objects
     object_refs, self.current_task_id, timeout_ms)
   File "python/ray/_raylet.pyx", line 817, in ray._raylet.CoreWorker.get_objects
   File "python/ray/_raylet.pyx", line 142, in ray._raylet.check_status
 KeyboardInterrupt
 command terminated with exit code 1
 2020-11-09 06:32:04,619	ERROR command_runner.py:175 -- NodeUpdater: ray-head-mp2vs: Command failed:
 
   kubectl -n ray exec -it ray-head-mp2vs --'bash --login -c -i '"'"'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; (python ~/request_resource.py)'"'"''
 &lt;/denchmark-code&gt;
 
 I saw what &lt;denchmark-link:https://github.com/PidgeyBE&gt;@PidgeyBE&lt;/denchmark-link&gt;
  observed, even though the head node has enough resource, an extra worker is started, and after 10+ seconds, that worker not crashed somehow. And monitor failed with
 &lt;denchmark-code&gt;  File "/root/anaconda3/lib/python3.7/site-packages/ray/autoscaler/_private/resource_demand_scheduler.py", line 84, in get_nodes_to_launch
     self._infer_legacy_node_resources_if_needed(static_node_resources)
   File "/root/anaconda3/lib/python3.7/site-packages/ray/autoscaler/_private/resource_demand_scheduler.py", line 182, in _infer_legacy_node_resources_if_needed
     assert len(static_node_resources) == 1  # Only the head node.
 AssertionError
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='14' author='PidgeyBE' date='2020-11-09T08:06:56Z'>
 		&lt;denchmark-link:https://github.com/simon-mo&gt;@simon-mo&lt;/denchmark-link&gt;
  , thanks for reporting this.
 Will make a quick fix.
 		</comment>
 		<comment id='15' author='PidgeyBE' date='2020-11-09T09:09:19Z'>
 		I haven't seen the exact issue of &lt;denchmark-link:https://github.com/simon-mo&gt;@simon-mo&lt;/denchmark-link&gt;
 , but have faced several others. I was wondering if k8s autoscaling is tested in CI/CD?
 		</comment>
 		<comment id='16' author='PidgeyBE' date='2020-11-23T22:55:13Z'>
 		This is fixed in the new autoscaler (nightly), please re-open if it still happens there.
 		</comment>
 	</comments>
 </bug>
<commit id='85197deece837a1f4ffdbbffb72af6028a9b6e32' author='Ameer Haj Ali' date='2020-11-11 13:36:48-08:00'>
 	<dmm_unit complexity='1.0' interfacing='0.3333333333333333' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='doc\source\cluster\autoscaling.rst' new_name='doc\source\cluster\autoscaling.rst'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>11,12,13,17,18,32,33,34,35,36,45,66</added_lines>
 			<deleted_lines>11,15,16,30,31,32,33,34,35,44,65,69,70</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\_private\autoscaler.py' new_name='python\ray\autoscaler\_private\autoscaler.py'>
 		<file_info nloc='500' complexity='87' token_count='3042'></file_info>
 		<method name='target_num_workers' parameters='self'>
 				<method_info nloc='23' complexity='6' token_count='166' nesting_level='1' start_line='369' end_line='398'></method_info>
 			<added_lines>369,370,371,372,373,374,375,377,379,380,381</added_lines>
 			<deleted_lines>369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398</deleted_lines>
 		</method>
 		<method name='_sort_based_on_last_used' parameters='self,str'>
 				<method_info nloc='2' complexity='1' token_count='26' nesting_level='1' start_line='257' end_line='258'></method_info>
 			<added_lines>257,258</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='last_time_used' parameters='NodeID'>
 				<method_info nloc='3' complexity='1' token_count='22' nesting_level='2' start_line='270' end_line='272'></method_info>
 			<added_lines>270,271,272</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_update' parameters='self'>
 				<method_info nloc='86' complexity='25' token_count='603' nesting_level='1' start_line='138' end_line='255'></method_info>
 			<added_lines>148,153,161,162,163,164,167,168,169,173,185,200,201,202,203,204,205,206,207,208,209,210,211,212,213,231</added_lines>
 			<deleted_lines>151,152,153,154,159,160,161,162,163,164,165,173,176,180,181,182,194,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,226,227,228,229,230,231,232,233,234,235,236,237,238</deleted_lines>
 		</method>
 		<method name='log_info_string' parameters='self,nodes,target'>
 				<method_info nloc='13' complexity='3' token_count='89' nesting_level='1' start_line='580' end_line='592'></method_info>
 			<added_lines>581,583,591</added_lines>
 			<deleted_lines>580,582,586,587,588,589,592</deleted_lines>
 		</method>
 		<method name='reset' parameters='self,errors_fatal'>
 				<method_info nloc='70' complexity='11' token_count='338' nesting_level='1' start_line='312' end_line='388'></method_info>
 			<added_lines>347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,377,379,380,381</added_lines>
 			<deleted_lines>352,353,354,355,357,359,360,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388</deleted_lines>
 		</method>
 		<method name='request_resources' parameters='self'>
 				<method_info nloc='6' complexity='2' token_count='41' nesting_level='1' start_line='593' end_line='603'></method_info>
 			<added_lines>593,594,597,602,603</added_lines>
 			<deleted_lines>594,596,597,603</deleted_lines>
 		</method>
 		<method name='info_string' parameters='self,nodes,target'>
 				<method_info nloc='12' complexity='5' token_count='87' nesting_level='1' start_line='594' end_line='606'></method_info>
 			<added_lines>594,597,602,603</added_lines>
 			<deleted_lines>594,596,597,603,604,606</deleted_lines>
 		</method>
 		<method name='log_info_string' parameters='self,nodes'>
 				<method_info nloc='12' complexity='2' token_count='80' nesting_level='1' start_line='570' end_line='581'></method_info>
 			<added_lines>570,572,576,577,578,581</added_lines>
 			<deleted_lines>580</deleted_lines>
 		</method>
 		<method name='request_resources' parameters='self,resources'>
 				<method_info nloc='10' complexity='4' token_count='66' nesting_level='1' start_line='608' end_line='623'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>608,609,612,613,618,619,620,621,622,623</deleted_lines>
 		</method>
 		<method name='info_string' parameters='self,nodes'>
 				<method_info nloc='8' complexity='3' token_count='58' nesting_level='1' start_line='583' end_line='591'></method_info>
 			<added_lines>583,591</added_lines>
 			<deleted_lines>586,587,588,589</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>2,58,73,259,260,261,262,263,264,265,266,267,268,269,273,274,275,290,291,292,293,294,295,296,297,298,299</added_lines>
 			<deleted_lines>2,6,59,89,118,119,256,296,297,298,299,300,301,302,303,304,399</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\_private\aws\node_provider.py' new_name='python\ray\autoscaler\_private\aws\node_provider.py'>
 		<file_info nloc='403' complexity='73' token_count='2403'></file_info>
 		<method name='_create_node' parameters='self,node_config,tags,count'>
 				<method_info nloc='70' complexity='14' token_count='355' nesting_level='1' start_line='301' end_line='395'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>305,306,307,308,309,310</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>512,513</added_lines>
 			<deleted_lines>518,519</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\_private\load_metrics.py' new_name='python\ray\autoscaler\_private\load_metrics.py'>
 		<file_info nloc='169' complexity='39' token_count='1113'></file_info>
 		<method name='approx_workers_used' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='14' nesting_level='1' start_line='102' end_line='103'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>102,103</deleted_lines>
 		</method>
 		<method name='_info' parameters='self'>
 				<method_info nloc='33' complexity='12' token_count='262' nesting_level='1' start_line='166' end_line='206'></method_info>
 			<added_lines>167</added_lines>
 			<deleted_lines>166,167,168,169,170,183,184</deleted_lines>
 		</method>
 		<method name='_get_resource_usage' parameters='self'>
 				<method_info nloc='29' complexity='10' token_count='181' nesting_level='1' start_line='123' end_line='154'></method_info>
 			<added_lines>154</added_lines>
 			<deleted_lines>131,133,146</deleted_lines>
 		</method>
 		<method name='num_workers_connected' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='14' nesting_level='1' start_line='105' end_line='106'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>105,106</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>104,107,160,164,165,214,215</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\_private\resource_demand_scheduler.py' new_name='python\ray\autoscaler\_private\resource_demand_scheduler.py'>
 		<file_info nloc='561' complexity='30' token_count='2864'></file_info>
 		<method name='reset_config' parameters='self,NodeProvider,NodeType,int,float'>
 				<method_info nloc='5' complexity='1' token_count='30' nesting_level='1' start_line='56' end_line='60'></method_info>
 			<added_lines>56,57,58,59,60</added_lines>
 			<deleted_lines>57</deleted_lines>
 		</method>
 		<method name='_legacy_worker_node_to_launch' parameters='self,NodeType'>
 				<method_info nloc='4' complexity='1' token_count='42' nesting_level='1' start_line='223' end_line='226'></method_info>
 			<added_lines>224</added_lines>
 			<deleted_lines>223,224,225,226</deleted_lines>
 		</method>
 		<method name='get_nodes_to_launch' parameters='self,NodeType,NodeIP,NodeIP,None'>
 				<method_info nloc='9' complexity='1' token_count='61' nesting_level='1' start_line='105' end_line='113'></method_info>
 			<added_lines>108</added_lines>
 			<deleted_lines>105,113</deleted_lines>
 		</method>
 		<method name='get_nodes_to_launch' parameters='self,NodeType,NodeIP,NodeIP,None'>
 				<method_info nloc='9' complexity='1' token_count='61' nesting_level='1' start_line='54' end_line='62'></method_info>
 			<added_lines>54,55,56,57,58,59,60,61,62</added_lines>
 			<deleted_lines>57</deleted_lines>
 		</method>
 		<method name='_legacy_worker_node_to_launch' parameters='self,NodeType'>
 				<method_info nloc='4' complexity='1' token_count='42' nesting_level='1' start_line='172' end_line='175'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>173</deleted_lines>
 		</method>
 		<method name='is_legacy_yaml' parameters='self,NodeType,None'>
 				<method_info nloc='3' complexity='1' token_count='18' nesting_level='1' start_line='94' end_line='96'></method_info>
 			<added_lines>94,95,96</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,NodeProvider,NodeType,int,float'>
 				<method_info nloc='5' complexity='1' token_count='30' nesting_level='1' start_line='46' end_line='50'></method_info>
 			<added_lines>46,47,49,50</added_lines>
 			<deleted_lines>49,50</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,NodeProvider,NodeType,int'>
 				<method_info nloc='3' complexity='1' token_count='22' nesting_level='1' start_line='43' end_line='45'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>43,45</deleted_lines>
 		</method>
 		<method name='_update_based_on_node_config' parameters='NodeType'>
 				<method_info nloc='10' complexity='3' token_count='74' nesting_level='4' start_line='71' end_line='83'></method_info>
 			<added_lines>71,72,73,74,75,76,77,78,79,80,81,82,83</added_lines>
 			<deleted_lines>78</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>20,21,22,26,27,28,63,64,65,66,67,68,69,70,84,85,86,87,88,89,90,91,92,93,97,98,99,100,101,102,103,129,156,164,179,184,218,233,234,237,238,239,240,241,246,247,248,249,250,266,267,268,269,270,271,272,273,274,277,278,279,280,281,282,283,284,285,286,287,288,307,308,332,333,562,563</added_lines>
 			<deleted_lines>20,21,22,51,52,128,133,167,184,185,186,191,192,193,194,195,196,197,213,214,215,218,219,220,221,222,227,228,229,230,231,250,265,266,276,505</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\_private\util.py' new_name='python\ray\autoscaler\_private\util.py'>
 		<file_info nloc='187' complexity='36' token_count='1229'></file_info>
 		<modified_lines>
 			<added_lines>104,105,108,112,118,119,120,125,126</added_lines>
 			<deleted_lines>104,105,106,112,118,119,120,125</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\aws\defaults.yaml' new_name='python\ray\autoscaler\aws\defaults.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>12,13,14,15,16</added_lines>
 			<deleted_lines>12,13,14,15,16,17,18,19,20,27,28,29,30,31,32,33</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\aws\development-example.yaml' new_name='python\ray\autoscaler\aws\development-example.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>12,13,14,15,16</added_lines>
 			<deleted_lines>12,13,14,15,16,17</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\aws\example-full.yaml' new_name='python\ray\autoscaler\aws\example-full.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>12,13,14,15,16</added_lines>
 			<deleted_lines>12,13,14,15,16,17,18,19,20,40,41,42,43,44,45,46</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\aws\example-gpu-docker.yaml' new_name='python\ray\autoscaler\aws\example-gpu-docker.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>12,13,14,15,16</added_lines>
 			<deleted_lines>12,13,14,15,16,17,18,19,20,34,35,36,37,38,39,40</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\aws\example-ml.yaml' new_name='python\ray\autoscaler\aws\example-ml.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>17,18,19,20,21</added_lines>
 			<deleted_lines>17,18,19,20,21,22,23,24,25,43,44,45,46,47,48,49</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\aws\example-multi-node-type.yaml' new_name='python\ray\autoscaler\aws\example-multi-node-type.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>6,7,8,9,10,11</added_lines>
 			<deleted_lines>65,66</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\azure\defaults.yaml' new_name='python\ray\autoscaler\azure\defaults.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>12,13,14,15,16</added_lines>
 			<deleted_lines>12,13,14,15,16,17,18,19,20,27,28,29,30,31,32,33</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\azure\example-full.yaml' new_name='python\ray\autoscaler\azure\example-full.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>12,13,14,15,16</added_lines>
 			<deleted_lines>12,13,14,15,16,17,18,19,20,40,41,42,43,44,45,46</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\azure\example-gpu-docker.yaml' new_name='python\ray\autoscaler\azure\example-gpu-docker.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>12,13,14,15,16</added_lines>
 			<deleted_lines>12,13,14,15,16,17,18,19,20,34,35,36,37,38,39,40</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\azure\example-gpu.yaml' new_name='python\ray\autoscaler\azure\example-gpu.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>12,13,14,15,16</added_lines>
 			<deleted_lines>12,13,14,15,16,17,18,19,20,38,39,40,41,42,43,44</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\gcp\defaults.yaml' new_name='python\ray\autoscaler\gcp\defaults.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>12,13,14,15,16</added_lines>
 			<deleted_lines>12,13,14,15,16,17,18,19,20,27,28,29,30,31,32,33,34</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\gcp\example-full.yaml' new_name='python\ray\autoscaler\gcp\example-full.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>12,13,14,15,16</added_lines>
 			<deleted_lines>12,13,14,15,16,17,18,19,20,40,41,42,43,44,45,46,47</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\gcp\example-gpu-docker.yaml' new_name='python\ray\autoscaler\gcp\example-gpu-docker.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>12,13,14,15,16</added_lines>
 			<deleted_lines>12,13,14,15,16,17,18,19,20,35,36,37,38,39,40,41,42</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\kubernetes\defaults.yaml' new_name='python\ray\autoscaler\kubernetes\defaults.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>12,13,14,15,16</added_lines>
 			<deleted_lines>12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\kubernetes\example-full.yaml' new_name='python\ray\autoscaler\kubernetes\example-full.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>12,13,14,15,16</added_lines>
 			<deleted_lines>12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\kubernetes\example-ingress.yaml' new_name='python\ray\autoscaler\kubernetes\example-ingress.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>12,13,14,15,16</added_lines>
 			<deleted_lines>12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\local\defaults.yaml' new_name='python\ray\autoscaler\local\defaults.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>4,8,13,16,17,18,19,20,21</added_lines>
 			<deleted_lines>4,8,10,11,12,16,19,20,21,22</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\local\development-example.yaml' new_name='python\ray\autoscaler\local\development-example.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>7</added_lines>
 			<deleted_lines>7</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\local\example-full.yaml' new_name='python\ray\autoscaler\local\example-full.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>4,8,13,16,17,18,19,20,21</added_lines>
 			<deleted_lines>4,8,10,11,12,16,19,20,21,22</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\ray-schema.json' new_name='python\ray\autoscaler\ray-schema.json'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>52,53,54,55,56</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\staroid\defaults.yaml' new_name='python\ray\autoscaler\staroid\defaults.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>13,14,15,16,17</added_lines>
 			<deleted_lines>13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\staroid\example-full.yaml' new_name='python\ray\autoscaler\staroid\example-full.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>13,14,15,16,17</added_lines>
 			<deleted_lines>13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\staroid\example-gpu.yaml' new_name='python\ray\autoscaler\staroid\example-gpu.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>13,14,15,16,17</added_lines>
 			<deleted_lines>13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\staroid\example-multi-node-type.yaml' new_name='python\ray\autoscaler\staroid\example-multi-node-type.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>6,7,8,9,10,11</added_lines>
 			<deleted_lines>111,112</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\tests\test_autoscaler.py' new_name='python\ray\tests\test_autoscaler.py'>
 		<file_info nloc='1321' complexity='128' token_count='8311'></file_info>
 		<method name='testInitialWorkers' parameters='self'>
 				<method_info nloc='19' complexity='1' token_count='98' nesting_level='1' start_line='594' end_line='613'></method_info>
 			<added_lines>595,613</added_lines>
 			<deleted_lines>594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612</deleted_lines>
 		</method>
 		<method name='testDelayedLaunchWithMinWorkers' parameters='self'>
 				<method_info nloc='33' complexity='1' token_count='227' nesting_level='1' start_line='813' end_line='849'></method_info>
 			<added_lines>813,837,838,841,844,845</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='testUpdate' parameters='self'>
 				<method_info nloc='8' complexity='1' token_count='103' nesting_level='1' start_line='257' end_line='264'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>257,258,259,260,261,262,263,264</deleted_lines>
 		</method>
 		<method name='testManualAutoscaling' parameters='self'>
 				<method_info nloc='30' complexity='3' token_count='195' nesting_level='1' start_line='582' end_line='611'></method_info>
 			<added_lines>585,586,587,588,589,590,595</added_lines>
 			<deleted_lines>582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611</deleted_lines>
 		</method>
 		<method name='testExternalNodeScalerWrongModuleFormat' parameters='self'>
 				<method_info nloc='13' complexity='1' token_count='79' nesting_level='1' start_line='1209' end_line='1221'></method_info>
 			<added_lines>1215,1216,1217,1218</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='testIgnoresCorruptedConfig' parameters='self'>
 				<method_info nloc='30' complexity='2' token_count='200' nesting_level='1' start_line='895' end_line='930'></method_info>
 			<added_lines>899,902,912,924,925,926,927,928</added_lines>
 			<deleted_lines>895,896,898,899,900,901,902,903,904,906,907,908,909,910,913,914,915,916,917,918,920,921</deleted_lines>
 		</method>
 		<method name='write_config' parameters='self,config'>
 				<method_info nloc='5' complexity='1' token_count='44' nesting_level='1' start_line='378' end_line='382'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>378,381</deleted_lines>
 		</method>
 		<method name='testDynamicScaling' parameters='self'>
 				<method_info nloc='31' complexity='1' token_count='186' nesting_level='1' start_line='556' end_line='592'></method_info>
 			<added_lines>560,563,585,586,587,588,589,590</added_lines>
 			<deleted_lines>582,583,584,585,586,587,588,589,590,591,592</deleted_lines>
 		</method>
 		<method name='testDebugString' parameters='self'>
 				<method_info nloc='17' complexity='1' token_count='112' nesting_level='1' start_line='319' end_line='335'></method_info>
 			<added_lines>325,326,327,328,331</added_lines>
 			<deleted_lines>334,335</deleted_lines>
 		</method>
 		<method name='testLoadMessages' parameters='self'>
 				<method_info nloc='25' complexity='1' token_count='436' nesting_level='1' start_line='266' end_line='296'></method_info>
 			<added_lines>288</added_lines>
 			<deleted_lines>266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296</deleted_lines>
 		</method>
 		<method name='write_config' parameters='self,config,call_prepare_config'>
 				<method_info nloc='8' complexity='2' token_count='65' nesting_level='1' start_line='325' end_line='332'></method_info>
 			<added_lines>325,326,327,328,331</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='setUp' parameters='self'>
 				<method_info nloc='6' complexity='1' token_count='40' nesting_level='1' start_line='285' end_line='290'></method_info>
 			<added_lines>288</added_lines>
 			<deleted_lines>285,286,287,288,289,290</deleted_lines>
 		</method>
 		<method name='testTargetUtilizationFraction' parameters='self'>
 				<method_info nloc='56' complexity='2' token_count='374' nesting_level='1' start_line='1095' end_line='1156'></method_info>
 			<added_lines>1095,1098,1099,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155</added_lines>
 			<deleted_lines>1123,1124,1127,1150,1153,1154</deleted_lines>
 		</method>
 		<method name='testAutoscalerConfigValidationFailNotFatal' parameters='self'>
 				<method_info nloc='18' complexity='1' token_count='106' nesting_level='1' start_line='384' end_line='403'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>389</deleted_lines>
 		</method>
 		<method name='testPruneByNodeIp' parameters='self'>
 				<method_info nloc='6' complexity='1' token_count='72' nesting_level='1' start_line='298' end_line='303'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>298,299,300,301,302,303</deleted_lines>
 		</method>
 		<method name='testDelayedLaunchWithFailure' parameters='self'>
 				<method_info nloc='40' complexity='1' token_count='273' nesting_level='1' start_line='871' end_line='924'></method_info>
 			<added_lines>899,902,912,924</added_lines>
 			<deleted_lines>871,895,896,898,899,900,901,902,903,904,906,907,908,909,910,913,914,915,916,917,918,920,921</deleted_lines>
 		</method>
 		<method name='testUnmanagedNodes' parameters='self'>
 				<method_info nloc='40' complexity='1' token_count='288' nesting_level='1' start_line='686' end_line='733'></method_info>
 			<added_lines>727,728,729,730,731</added_lines>
 			<deleted_lines>688,689,690,691,699,703,705,721,722,723,724,725,726,727,728,729,730,731,732,733</deleted_lines>
 		</method>
 		<method name='testScaleUpBasedOnLoad' parameters='self'>
 				<method_info nloc='57' complexity='1' token_count='422' nesting_level='1' start_line='1030' end_line='1093'></method_info>
 			<added_lines>1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1067,1068,1069,1070,1071,1086</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='testBottleneckResource' parameters='self'>
 				<method_info nloc='5' complexity='1' token_count='70' nesting_level='1' start_line='305' end_line='309'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>305,306,307,308,309</deleted_lines>
 		</method>
 		<method name='testAggressiveAutoscaling' parameters='self'>
 				<method_info nloc='64' complexity='2' token_count='373' nesting_level='1' start_line='615' end_line='684'></method_info>
 			<added_lines>621,625,626,627,628,630,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684</added_lines>
 			<deleted_lines>644,666</deleted_lines>
 		</method>
 		<method name='testDontScaleBelowTarget' parameters='self'>
 				<method_info nloc='30' complexity='1' token_count='249' nesting_level='1' start_line='1150' end_line='1187'></method_info>
 			<added_lines>1150,1151,1152,1153,1154,1155</added_lines>
 			<deleted_lines>1150,1153,1154,1169,1170,1171,1172,1173,1177,1178,1179,1180,1181,1184,1185,1186,1187</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>21,22,24,25,26</added_lines>
 			<deleted_lines>21,22,24,265,297,304,310,734,735,736,737,738,739,740,741,742,743,744,789,976,986,1188,1246</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\tests\test_cli_patterns\test_ray_attach.txt' new_name='python\ray\tests\test_cli_patterns\test_ray_attach.txt'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>1,2</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\tests\test_cli_patterns\test_ray_exec.txt' new_name='python\ray\tests\test_cli_patterns\test_ray_exec.txt'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>1,2</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\tests\test_cli_patterns\test_ray_submit.txt' new_name='python\ray\tests\test_cli_patterns\test_ray_submit.txt'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>1,2,5,6</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\tests\test_cli_patterns\test_ray_up.txt' new_name='python\ray\tests\test_cli_patterns\test_ray_up.txt'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>3,4</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\tests\test_cli_patterns\test_ray_up_record.txt' new_name='python\ray\tests\test_cli_patterns\test_ray_up_record.txt'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>2,3</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\tests\test_multi_node_2.py' new_name='python\ray\tests\test_multi_node_2.py'>
 		<file_info nloc='138' complexity='34' token_count='996'></file_info>
 		<method name='verify_load_metrics' parameters='monitor,expected_resource_usage,timeout'>
 				<method_info nloc='32' complexity='16' token_count='231' nesting_level='0' start_line='75' end_line='110'></method_info>
 			<added_lines>81,82,83,84,87,89,90,91,97</added_lines>
 			<deleted_lines>83,85,86,87,88,92,93,94,97</deleted_lines>
 		</method>
 		<method name='test_heartbeats_single' parameters='ray_start_cluster_head'>
 				<method_info nloc='21' complexity='1' token_count='179' nesting_level='0' start_line='120' end_line='165'></method_info>
 			<added_lines>128,142,162</added_lines>
 			<deleted_lines>128,142,143,144,145,146</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>166,167,168,169,170</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\tests\test_resource_demand_scheduler.py' new_name='python\ray\tests\test_resource_demand_scheduler.py'>
 		<file_info nloc='1274' complexity='71' token_count='8989'></file_info>
 		<method name='testScaleUpMinWorkers' parameters='self'>
 				<method_info nloc='46' complexity='5' token_count='322' nesting_level='1' start_line='1113' end_line='1161'></method_info>
 			<added_lines>1117</added_lines>
 			<deleted_lines>1115,1116</deleted_lines>
 		</method>
 		<method name='testScaleUpMinSanity' parameters='self'>
 				<method_info nloc='18' complexity='1' token_count='108' nesting_level='1' start_line='1027' end_line='1044'></method_info>
 			<added_lines>1028,1029,1030,1031</added_lines>
 			<deleted_lines>1028</deleted_lines>
 		</method>
 		<method name='testUpdateConfig' parameters='self'>
 				<method_info nloc='22' complexity='1' token_count='143' nesting_level='1' start_line='1470' end_line='1491'></method_info>
 			<added_lines>1471,1472,1473,1486</added_lines>
 			<deleted_lines>1470,1483</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>1112</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
