<bug_data>
<bug id='12441' author='PidgeyBE' open_date='2020-11-26T10:24:22Z' closed_time='2020-11-30T04:55:51Z'>
 	<summary>[autoscaler] Actor resource demands are not cleared after actor is scheduled</summary>
 	<description>
 &lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;
 
 
 ray nightly
 
 &lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;
 
 Take the following steps:
 
 start a k8s autoscaling cluster
 Execute: (I did in in ipython in ray head container)
 
 &lt;denchmark-code&gt;import os
 import ray
 
 ray.init(address="auto")
 
 @ray.remote(num_cpus=0.2)
 class ActorA:
     def __init__(self):
         pass
 
 a = ActorA.remote()
 b = ActorA.remote()
 
 # wait untill deployed, then kill actors or exit ipython
 ray.kill(a)
 ray.kill(b)
 &lt;/denchmark-code&gt;
 
 -&gt; output of autoscaling monitor:
 &lt;denchmark-code&gt;2020-11-26 10:23:43,710 INFO resource_demand_scheduler.py:193 -- Resource demands: [{'CPU': 0.2}, {'CPU': 0.2}]
 &lt;/denchmark-code&gt;
 
 -&gt; the actor resources stick, instead of getting cleaned up
 I would expect the resources to be cleaned up...
 
  I have verified my script runs in a clean environment and reproduces the issue.
  I have verified the issue also occurs with the latest wheels.
 
 	</description>
 	<comments>
 		<comment id='1' author='PidgeyBE' date='2020-11-27T18:06:20Z'>
 		I think this may be the same issue as &lt;denchmark-link:https://github.com/ray-project/ray/issues/12381&gt;#12381&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='PidgeyBE' date='2020-11-27T23:48:52Z'>
 		&lt;denchmark-link:https://github.com/wuisawesome&gt;@wuisawesome&lt;/denchmark-link&gt;
  it looks like the actor demands aren't cleared after the actor is scheduled. The following unit test reproduces:
 &lt;denchmark-code&gt;def test_actor_resource_demand(shutdown_only):
     cluster = ray.init(num_cpus=3)
     global_state_accessor = GlobalStateAccessor(
         cluster["redis_address"], ray.ray_constants.REDIS_DEFAULT_PASSWORD)
     global_state_accessor.connect()
 
     @ray.remote(num_cpus=2)
     class Actor:
         def foo(self):
             return "ok"
 
     a = Actor.remote()
     ray.get(a.foo.remote())
     time.sleep(2)
 
     message = global_state_accessor.get_all_heartbeat()
     heartbeat = ray.gcs_utils.HeartbeatBatchTableData.FromString(message)
 
     # The actor is scheduled so there should be no more demands left.
     print(heartbeat)
     assert len(heartbeat.resource_load_by_shape.resource_demands) == 0
 
     global_state_accessor.disconnect()
 &lt;/denchmark-code&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='f1cc33a6a6086122548b02bd8ff7edda9e277148' author='Alex Wu' date='2020-11-29 20:55:50-08:00'>
 	<dmm_unit complexity='0.9523809523809523' interfacing='0.9761904761904762' size='0.047619047619047616'></dmm_unit>
 	<modification change_type='MODIFY' old_name='python\ray\tests\test_actor_advanced.py' new_name='python\ray\tests\test_actor_advanced.py'>
 		<file_info nloc='770' complexity='165' token_count='5002'></file_info>
 		<method name='test_actor_resource_demand' parameters='shutdown_only'>
 				<method_info nloc='37' complexity='1' token_count='274' nesting_level='0' start_line='1042' end_line='1092'></method_info>
 			<added_lines>1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_actor_resource_demand.foo' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='7' nesting_level='2' start_line='1051' end_line='1052'></method_info>
 			<added_lines>1051,1052</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>17,1093,1094</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='src\ray\gcs\gcs_server\gcs_actor_scheduler.cc' new_name='src\ray\gcs\gcs_server\gcs_actor_scheduler.cc'>
 		<file_info nloc='323' complexity='53' token_count='2713'></file_info>
 		<method name='ray::gcs::GcsActorScheduler::GcsActorScheduler' parameters='io_context,gcs_actor_table,gcs_node_manager,gcs_pub_sub,schedule_failure_handler,schedule_success_handler,lease_client_factory,client_factory'>
 				<method_info nloc='18' complexity='2' token_count='161' nesting_level='2' start_line='25' end_line='42'></method_info>
 			<added_lines>38</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='ray::gcs::GcsActorScheduler::LeaseWorkerFromNode' parameters='actor,node'>
 				<method_info nloc='48' complexity='8' token_count='368' nesting_level='2' start_line='192' end_line='256'></method_info>
 			<added_lines>212,213,214,254,255</added_lines>
 			<deleted_lines>250</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='src\ray\gcs\gcs_server\gcs_actor_scheduler.h' new_name='src\ray\gcs\gcs_server\gcs_actor_scheduler.h'>
 		<file_info nloc='108' complexity='9' token_count='876'></file_info>
 		<modified_lines>
 			<added_lines>289,290</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
