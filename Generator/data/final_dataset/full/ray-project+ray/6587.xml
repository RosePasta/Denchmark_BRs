<bug_data>
<bug id='6587' author='flying-mojo' open_date='2019-12-23T10:24:29Z' closed_time='2019-12-28T17:51:10Z'>
 	<summary>Eager execution with Tuple actions space failing</summary>
 	<description>
 &lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;
 
 When executing in eager mode with Tuple action space the execution fails (tried on IMPALA, APPO):
 File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 568, in _make_workers                                          │
 logdir=self.logdir)                                                                                                                                        │
 File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py", line 64, in init                                         │
 RolloutWorker, env_creator, policy, 0, self._local_config)                                                                                                 │
 File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py", line 220, in _make_worker                                    │
 _fake_sampler=config.get("_fake_sampler", False))                                                                                                          │
 File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 353, in init                                    │
 policy_dict, policy_config)                                                                                                                                │
 File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 766, in _build_policy_map                           │
 policy_map[name] = cls(obs_space, act_space, merged_conf)                                                                                                  │
 File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/policy/eager_tf_policy.py", line 95, in init                                        │
 super(TracedEagerPolicy, self).init(*args, **kwargs)                                                                                                   │
 File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/policy/eager_tf_policy.py", line 247, in init                                       │
 self._initialize_loss_with_dummy_batch()                                                                                                                   │
 File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/policy/eager_tf_policy.py", line 522, in _initialize_loss_with_dummy_batch              │
 loss_fn(self, self.model, self.dist_class, postprocessed_batch)                                                                                            │
 File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/agents/ppo/appo_policy.py", line 337, in build_appo_surrogate_loss                      │
 mean_kl = make_time_major(prev_action_dist.multi_kl(action_dist))                                                                                          │
 File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/agents/ppo/appo_policy.py", line 250, in make_time_major                                │
 **kw)                                                                                                                                                      │
 File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/agents/impala/vtrace_policy.py", line 144, in _make_time_major                          │
 B = tf.shape(tensor)[0] // T                                                                                                                               │
 File "/data/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py", line 813, in _slice_helper                              │
 name=name)                                                                                                                                                 │
 File "/data/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py", line 979, in strided_slice                              │
 shrink_axis_mask=shrink_axis_mask)                                                                                                                         │
 File "/data/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py", line 10372, in strided_slice                        │
 _six.raise_from(_core._status_to_exception(e.code, message), None)                                                                                         │
 File "", line 3, in raise_from                                                                                                                       │
 tensorflow.python.framework.errors_impl.InvalidArgumentError: slice index 0 of dimension 0 out of bounds. [Op:StridedSlice] name: default_policy/strided_slice/
 Ray version and other system information (Python version, TensorFlow version, OS):
 ray 0.8.0
 Does the problem occur on the latest wheels?
 &lt;denchmark-h:h3&gt;Reproduction&lt;/denchmark-h&gt;
 
 Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
 run ReversedAddition-v0 in eager mode:
 rllib train --run APPO --env ReversedAddition-v0│--eager --trace
 If we cannot run your script, we cannot fix your issue.
 	</description>
 	<comments>
 		<comment id='1' author='flying-mojo' date='2019-12-26T21:42:16Z'>
 		Thanks for reporting. Could you try out &lt;denchmark-link:https://github.com/ray-project/ray/pull/6615&gt;#6615&lt;/denchmark-link&gt;
  ?
 		</comment>
 		<comment id='2' author='flying-mojo' date='2019-12-28T13:04:15Z'>
 		Thanks Eric! I will test it when i run in eager mode again. Looking at the &lt;denchmark-link:https://github.com/ray-project/ray/pull/6615&gt;#6615&lt;/denchmark-link&gt;
  changelist it should fix it.
 		</comment>
 	</comments>
 </bug>
<commit id='022954ac094d8af1ca54204a8f61585fe749095f' author='Eric Liang' date='2019-12-28 09:51:09-08:00'>
 	<dmm_unit complexity='0.0' interfacing='0.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='rllib\models\tf\tf_action_dist.py' new_name='rllib\models\tf\tf_action_dist.py'>
 		<file_info nloc='237' complexity='54' token_count='1879'></file_info>
 		<method name='kl' parameters='self,other'>
 				<method_info nloc='7' complexity='2' token_count='48' nesting_level='1' start_line='242' end_line='248'></method_info>
 			<added_lines>243,247,248</added_lines>
 			<deleted_lines>242,246,247</deleted_lines>
 		</method>
 		<method name='logp' parameters='self,x'>
 				<method_info nloc='17' complexity='6' token_count='153' nesting_level='1' start_line='222' end_line='239'></method_info>
 			<added_lines>235,238,239</added_lines>
 			<deleted_lines>234,237,238</deleted_lines>
 		</method>
 		<method name='entropy' parameters='self'>
 				<method_info nloc='3' complexity='2' token_count='36' nesting_level='1' start_line='251' end_line='253'></method_info>
 			<added_lines>252,253</added_lines>
 			<deleted_lines>251,252,253</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>6</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
