<bug_data>
<bug id='8254' author='rkooo567' open_date='2020-04-30T17:41:10Z' closed_time='2020-06-26T01:09:42Z'>
 	<summary>[Core] Node Failure Long Running Test Failed at `grpc::ServerInterface::RegisteredAsyncRequest::IssueRequest()`</summary>
 	<description>
 &lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;
 
 Node Failure Long Running test fails with the error log below.
 &lt;denchmark-code&gt;�[2m�[33m(pid=raylet)�[0m E0429 02:32:06.263886 22036 process.cc:274] Failed to wait for process 22047 with error system:10: No child processes
 E0429 02:32:12.346844 23272 task_manager.cc:288] 3 retries left for task b48f33dc1265b526ffffffff0100, attempting to resubmit.
 E0429 02:32:12.346899 23272 core_worker.cc:373] Will resubmit task after a 5000ms delay: Type=NORMAL_TASK, Language=PYTHON, function_descriptor={type=PythonFunctionDescriptor, module_name=__main__, class_name=, function_name=f, function_hash=7d2c6c88e5e801d48a350076f2117e717fe12224}, task_id=b48f33dc1265b526ffffffff0100, job_id=0100, num_args=2, num_returns=1
 �[2m�[33m(pid=raylet)�[0m E0429 02:32:12.347446 22089 process.cc:274] Failed to wait for process 22100 with error system:10: No child processes
 2020-04-29 02:32:12,653	INFO resource_spec.py:212 -- Starting Ray with 27.88 GiB memory available for workers and up to 0.15 GiB for objects. You can adjust these settings with ray.init(memory=&lt;bytes&gt;, object_store_memory=&lt;bytes&gt;).
 �[2m�[33m(pid=raylet)�[0m E0429 02:32:12.732946757   22142 server_chttp2.cc:40]        {"created":"@1588127532.732848116","description":"No address added out of total 1 resolved","file":"external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc","file_line":394,"referenced_errors":[{"created":"@1588127532.732846227","description":"Failed to add any wildcard listeners","file":"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_posix.cc","file_line":341,"referenced_errors":[{"created":"@1588127532.732832876","description":"Unable to configure socket","fd":44,"file":"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc","file_line":208,"referenced_errors":[{"created":"@1588127532.732823689","description":"Address already in use","errno":98,"file":"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc","file_line":181,"os_error":"Address already in use","syscall":"bind"}]},{"created":"@1588127532.732845812","description":"Unable to configure socket","fd":44,"file":"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc","file_line":208,"referenced_errors":[{"created":"@1588127532.732843382","description":"Address already in use","errno":98,"file":"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc","file_line":181,"os_error":"Address already in use","syscall":"bind"}]}]}]}
 �[2m�[33m(pid=raylet)�[0m *** Aborted at 1588127532 (unix time) try "date -d @1588127532" if you are using GNU date ***
 �[2m�[33m(pid=raylet)�[0m PC: @                0x0 (unknown)
 �[2m�[33m(pid=raylet)�[0m *** SIGSEGV (@0x58) received by PID 22142 (TID 0x7fc3a66d37c0) from PID 88; stack trace: ***
 �[2m�[33m(pid=raylet)�[0m     @     0x7fc3a5c32390 (unknown)
 �[2m�[33m(pid=raylet)�[0m     @     0x5596e3957692 grpc::ServerInterface::RegisteredAsyncRequest::IssueRequest()
 �[2m�[33m(pid=raylet)�[0m     @     0x5596e35b2149 ray::rpc::NodeManagerService::WithAsyncMethod_RequestWorkerLease&lt;&gt;::RequestRequestWorkerLease()
 �[2m�[33m(pid=raylet)�[0m     @     0x5596e35c7b1b ray::rpc::ServerCallFactoryImpl&lt;&gt;::CreateCall()
 �[2m�[33m(pid=raylet)�[0m     @     0x5596e380bfe1 ray::rpc::GrpcServer::Run()
 �[2m�[33m(pid=raylet)�[0m     @     0x5596e3629acc ray::raylet::NodeManager::NodeManager()
 �[2m�[33m(pid=raylet)�[0m     @     0x5596e35cbc07 ray::raylet::Raylet::Raylet()
 �[2m�[33m(pid=raylet)�[0m     @     0x5596e359848d main
 �[2m�[33m(pid=raylet)�[0m     @     0x7fc3a5459830 __libc_start_main
 �[2m�[33m(pid=raylet)�[0m     @     0x5596e35a9391 (unknown)
 �[2m�[36m(pid=22153)�[0m E0429 02:32:13.864451 22153 raylet_client.cc:69] Retrying to connect to socket for pathname /tmp/ray/session_2020-04-28_20-19-44_770473_22870/sockets/raylet.8 (num_attempts = 1, num_retries = 5)
 �[2m�[36m(pid=22153)�[0m E0429 02:32:14.364712 22153 raylet_client.cc:69] Retrying to connect to socket for pathname /tmp/ray/session_2020-04-28_20-19-44_770473_22870/sockets/raylet.8 (num_attempts = 2, num_retries = 5)
 �[2m�[36m(pid=22153)�[0m E0429 02:32:14.864863 22153 raylet_client.cc:69] Retrying to connect to socket for pathname /tmp/ray/session_2020-04-28_20-19-44_770473_22870/sockets/raylet.8 (num_attempts = 3, num_retries = 5)
 �[2m�[36m(pid=22153)�[0m E0429 02:32:15.365000 22153 raylet_client.cc:69] Retrying to connect to socket for pathname /tmp/ray/session_2020-04-28_20-19-44_770473_22870/sockets/raylet.8 (num_attempts = 4, num_retries = 5)
 �[2m�[36m(pid=22153)�[0m F0429 02:32:15.865115 22153 raylet_client.cc:78] Could not connect to socket /tmp/ray/session_2020-04-28_20-19-44_770473_22870/sockets/raylet.8
 �[2m�[36m(pid=22153)�[0m *** Check failure stack trace: ***
 �[2m�[36m(pid=22153)�[0m     @     0x7f5d3b2b40ed  google::LogMessage::Fail()
 �[2m�[36m(pid=22153)�[0m     @     0x7f5d3b2b555c  google::LogMessage::SendToLog()
 �[2m�[36m(pid=22153)�[0m     @     0x7f5d3b2b3dc9  google::LogMessage::Flush()
 �[2m�[36m(pid=22153)�[0m     @     0x7f5d3b2b3fe1  google::LogMessage::~LogMessage()
 �[2m�[36m(pid=22153)�[0m     @     0x7f5d3b03bb39  ray::RayLog::~RayLog()
 �[2m�[36m(pid=22153)�[0m     @     0x7f5d3ae55133  ray::raylet::RayletConnection::RayletConnection()
 �[2m�[36m(pid=22153)�[0m     @     0x7f5d3ae55abf  ray::raylet::RayletClient::RayletClient()
 �[2m�[36m(pid=22153)�[0m     @     0x7f5d3adf513b  ray::CoreWorker::CoreWorker()
 �[2m�[36m(pid=22153)�[0m     @     0x7f5d3adf8984  ray::CoreWorkerProcess::CreateWorker()
 �[2m�[36m(pid=22153)�[0m     @     0x7f5d3adf8efb  ray::CoreWorkerProcess::CoreWorkerProcess()
 �[2m�[36m(pid=22153)�[0m     @     0x7f5d3adf93fb  ray::CoreWorkerProcess::Initialize()
 �[2m�[36m(pid=22153)�[0m     @     0x7f5d3ad6c06c  __pyx_pw_3ray_7_raylet_10CoreWorker_1__cinit__()
 �[2m�[36m(pid=22153)�[0m     @     0x7f5d3ad6d155  __pyx_tp_new_3ray_7_raylet_CoreWorker()
 �[2m�[36m(pid=22153)�[0m     @     0x55db24e47965  type_call
 �[2m�[36m(pid=22153)�[0m     @     0x55db24db7d7b  _PyObject_FastCallDict
 �[2m�[36m(pid=22153)�[0m     @     0x55db24e477ce  call_function
 �[2m�[36m(pid=22153)�[0m     @     0x55db24e69cba  _PyEval_EvalFrameDefault
 �[2m�[36m(pid=22153)�[0m     @     0x55db24e40dae  _PyEval_EvalCodeWithName
 �[2m�[36m(pid=22153)�[0m     @     0x55db24e41941  fast_function
 �[2m�[36m(pid=22153)�[0m     @     0x55db24e47755  call_function
 �[2m�[36m(pid=22153)�[0m     @     0x55db24e6aa7a  _PyEval_EvalFrameDefault
 �[2m�[36m(pid=22153)�[0m     @     0x55db24e42459  PyEval_EvalCodeEx
 �[2m�[36m(pid=22153)�[0m     @     0x55db24e431ec  PyEval_EvalCode
 �[2m�[36m(pid=22153)�[0m     @     0x55db24ebd9a4  run_mod
 �[2m�[36m(pid=22153)�[0m     @     0x55db24ebdda1  PyRun_FileExFlags
 �[2m�[36m(pid=22153)�[0m     @     0x55db24ebdfa4  PyRun_SimpleFileExFlags
 �[2m�[36m(pid=22153)�[0m     @     0x55db24ec1a9e  Py_Main
 �[2m�[36m(pid=22153)�[0m     @     0x55db24d894be  main
 �[2m�[36m(pid=22153)�[0m     @     0x7f5d3cd85830  __libc_start_main
 �[2m�[36m(pid=22153)�[0m     @     0x55db24e70773  (unknown)
 Traceback (most recent call last):
   File "workloads/node_failures.py", line 57, in &lt;module&gt;
     cluster.add_node()
   File "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/cluster_utils.py", line 115, in add_node
     self._wait_for_node(node)
   File "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/cluster_utils.py", line 165, in _wait_for_node
     raise TimeoutError("Timed out while waiting for nodes to join.")
 TimeoutError: Timed out while waiting for nodes to join.
 �[2m�[33m(pid=raylet)�[0m E0429 02:32:42.965368 13125 process.cc:274] Failed to wait for process 13136 with error system:10: No child processes
 �[2m�[33m(pid=raylet)�[0m E0429 02:32:43.045863  1167 process.cc:274] Failed to wait for process 1178 with error system:10: No child processes
 2020-04-29 02:32:43,942	ERROR import_thread.py:93 -- ImportThread: Connection closed by server.
 2020-04-29 02:32:43,942	ERROR worker.py:996 -- print_logs: Connection closed by server.
 2020-04-29 02:32:43,942	ERROR worker.py:1096 -- listen_error_messages_raylet: Connection closed by server.
 E0429 02:32:45.999132 22870 raylet_client.cc:90] IOError: [RayletClient] Connection closed unexpectedly. [RayletClient] Failed to disconnect from raylet.
 &lt;/denchmark-code&gt;
 
 Ray version and other system information (Python version, TensorFlow version, OS):
 &lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;
 
 This is hard to reproduce, but it happens consistently in multiple long running tests. For the logs above, it happens after 2909 iterations.
 
  I have verified my script runs in a clean environment and reproduces the issue.
  I have verified the issue also occurs with the latest wheels.
 
 	</description>
 	<comments>
 		<comment id='1' author='rkooo567' date='2020-05-15T18:43:09Z'>
 		&lt;denchmark-link:https://github.com/stephanie-wang&gt;@stephanie-wang&lt;/denchmark-link&gt;
  Are you working on this issue?
 		</comment>
 		<comment id='2' author='rkooo567' date='2020-05-26T20:54:29Z'>
 		I don't think this is a P0. It's just a side effect of rapidly killing and adding ray nodes on the same machine. Raylet gRPC server cannot bind to a port that's just released by killing ray node.
 This is uncommon in production scenarios. Moving it to P1.
 However, we do need to fix the long running test by adding some sleep buffer between killing and adding a new node, maybe inside test_reconstruction as well.
 		</comment>
 		<comment id='3' author='rkooo567' date='2020-05-26T23:05:55Z'>
 		It looks like the ultimate source of the problem is that the port for GPRC is selected before GRPC actually uses that port:
 
 
 
 ray/python/ray/node.py
 
 
          Line 167
       in
       137519e
 
 
 
 
 
 
  # NOTE: There is a possible but unlikely race condition where 
 
 
 
 
 
 &lt;denchmark-link:https://github.com/simon-mo&gt;@simon-mo&lt;/denchmark-link&gt;
  What are your thoughts on this?
 		</comment>
 		<comment id='4' author='rkooo567' date='2020-05-26T23:59:30Z'>
 		One possible fix is to randomize the port instead of using kernel to get unused port. This can avoid possible race condition. The reason is that current _get_unused_port procedure returns unused port in sequence and is likely to duplicate port numbers, since we close the socket immediately and release the ownership of that port.
     def _get_unused_port(self):
         s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
         s.bind(("", 0))
         port = s.getsockname()[1]
         # fix: add some random integer to the port number, returns if available, else retry
         s.close()
         return port
 		</comment>
 		<comment id='5' author='rkooo567' date='2020-05-27T00:16:38Z'>
 		&lt;denchmark-link:https://github.com/simon-mo&gt;@simon-mo&lt;/denchmark-link&gt;
   I'll try that!
 For future use--this is the error seen:
 &lt;denchmark-link:https://github.com/grpc/grpc/issues/17075&gt;grpc/grpc#17075&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='6' author='rkooo567' date='2020-06-03T19:19:37Z'>
 		&lt;denchmark-link:https://github.com/simon-mo&gt;@simon-mo&lt;/denchmark-link&gt;
  Can we close this after running release tests? (I know it will be resolved 99.99%, but just to make it sure)
 		</comment>
 		<comment id='7' author='rkooo567' date='2020-06-03T19:20:36Z'>
 		Sure. This issue was automatically closed by github after merge. Feel free to open it.
 		</comment>
 		<comment id='8' author='rkooo567' date='2020-06-19T05:59:19Z'>
 		&lt;denchmark-link:https://github.com/ijrsvt&gt;@ijrsvt&lt;/denchmark-link&gt;
  Seems like it is still not fixed. I will keep the issue open.
 		</comment>
 		<comment id='9' author='rkooo567' date='2020-06-26T01:10:05Z'>
 		Let's open a new issue for any followups.
 		</comment>
 		<comment id='10' author='rkooo567' date='2020-06-26T01:13:37Z'>
 		This issue is still happening actually. &lt;denchmark-link:https://github.com/ijrsvt&gt;@ijrsvt&lt;/denchmark-link&gt;
  Would you mind creating a new one for your next sprint task?
 		</comment>
 	</comments>
 </bug>
<commit id='7a2c9524d15aa9edfd1aef76d81f6b2aae643cfc' author='Ian Rodney' date='2020-06-03 12:19:03-07:00'>
 	<dmm_unit complexity='1.0' interfacing='0.9047619047619048' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='python\ray\node.py' new_name='python\ray\node.py'>
 		<file_info nloc='607' complexity='97' token_count='3756'></file_info>
 		<method name='_get_unused_port' parameters='self,close_on_exit'>
 				<method_info nloc='20' complexity='5' token_count='131' nesting_level='1' start_line='409' end_line='432'></method_info>
 			<added_lines>409,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432</added_lines>
 			<deleted_lines>420</deleted_lines>
 		</method>
 		<method name='_prepare_socket_file' parameters='self,socket_path,default_prefix'>
 				<method_info nloc='21' complexity='7' token_count='160' nesting_level='1' start_line='434' end_line='465'></method_info>
 			<added_lines>449</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='socket' parameters='self'>
 				<method_info nloc='5' complexity='2' token_count='17' nesting_level='1' start_line='307' end_line='312'></method_info>
 			<added_lines>307,308,309,310,311,312</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_get_unused_port' parameters='self'>
 				<method_info nloc='6' complexity='1' token_count='46' nesting_level='1' start_line='398' end_line='403'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>398,402,403</deleted_lines>
 		</method>
 		<method name='start_raylet' parameters='self,use_valgrind,use_profiler'>
 				<method_info nloc='28' complexity='1' token_count='177' nesting_level='1' start_line='598' end_line='633'></method_info>
 			<added_lines>630,631</added_lines>
 			<deleted_lines>601</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>7,28,172,173,306,313</added_lines>
 			<deleted_lines>170</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\services.py' new_name='python\ray\services.py'>
 		<file_info nloc='1037' complexity='74' token_count='5641'></file_info>
 		<method name='start_raylet' parameters='redis_address,node_ip_address,node_manager_port,raylet_name,plasma_store_name,worker_path,temp_dir,session_dir,resource_spec,min_worker_port,max_worker_port,object_manager_port,redis_password,use_valgrind,use_profiler,stdout_file,stderr_file,config,include_java,java_worker_options,load_code_from_local,fate_share'>
 				<method_info nloc='22' complexity='1' token_count='73' nesting_level='0' start_line='1212' end_line='1233'></method_info>
 			<added_lines>1233</added_lines>
 			<deleted_lines>1233</deleted_lines>
 		</method>
 		<method name='start_raylet' parameters='redis_address,node_ip_address,node_manager_port,raylet_name,plasma_store_name,worker_path,temp_dir,session_dir,resource_spec,min_worker_port,max_worker_port,object_manager_port,redis_password,use_valgrind,use_profiler,stdout_file,stderr_file,config,include_java,java_worker_options,load_code_from_local,fate_share,socket_to_use'>
 				<method_info nloc='23' complexity='1' token_count='77' nesting_level='0' start_line='1212' end_line='1234'></method_info>
 			<added_lines>1233,1234</added_lines>
 			<deleted_lines>1233</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1365,1366</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
