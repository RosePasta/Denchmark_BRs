<bug_data>
<bug id='3684' author='marlonjan' open_date='2019-01-03T13:50:45Z' closed_time='2019-01-07T03:37:36Z'>
 	<summary>[RLlib] PGAgent learns with lr=0.0</summary>
 	<description>
 &lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;
 
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra
 Ray installed from (source or binary): installed with pip
 Ray version: 0.6.0
 Python version: 3.6.7
 Exact command to reproduce: See below
 
 &lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;
 
 PGAgent successfully trains with learning rate 0.0, while all model weights should remain constant.
 &lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;import ray
 import ray.tune as tune
 from ray.rllib.agents.pg import PGAgent
 from ray.tune import Experiment
 
 if __name__ == "__main__":
     ray.init()
     tune.run_experiments(
         Experiment(
             name="zero_lr_cart_pole",
             run=PGAgent,
             stop={"episode_reward_mean": 400.0},
             config={
                 "env": "CartPole-v0", 
                 "num_gpus": 0, 
                 "num_workers": 1, 
                 "lr": 0.0  # &lt;---------- zero
             },
         )
     )
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='marlonjan' date='2019-01-03T15:57:56Z'>
 		My impression is that the learning rate parameter gets ignored also for non-zero values. This is CartPole-v0 for radically different learning rates:
 &lt;denchmark-link:https://user-images.githubusercontent.com/9771046/50647354-a4fc4880-0f78-11e9-8b5d-c2ece330d386.png&gt;&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='marlonjan' date='2019-01-06T07:24:21Z'>
 		Thanks for reporting -- should be fixed in &lt;denchmark-link:https://github.com/ray-project/ray/pull/3697&gt;#3697&lt;/denchmark-link&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='e78562b2e8d2affc8b0f7fabde37aa192b4385fc' author='Eric Liang' date='2019-01-06 19:37:35-08:00'>
 	<dmm_unit complexity='0.92' interfacing='1.0' size='0.2'></dmm_unit>
 	<modification change_type='MODIFY' old_name='doc\source\rllib-training.rst' new_name='doc\source\rllib-training.rst'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>182,186,187,190,193,196</added_lines>
 			<deleted_lines>182,186,187,190,193,194,197,198</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\rllib\agents\agent.py' new_name='python\ray\rllib\agents\agent.py'>
 		<file_info nloc='366' complexity='58' token_count='2229'></file_info>
 		<method name='compute_action' parameters='self,observation,state,policy_id'>
 				<method_info nloc='14' complexity='3' token_count='109' nesting_level='1' start_line='344' end_line='369'></method_info>
 			<added_lines>344,345,346,347,348,349,350,353,354,355,363,364,365</added_lines>
 			<deleted_lines>344,364,365,366,367,368,369</deleted_lines>
 		</method>
 		<method name='get_policy' parameters='self,policy_id'>
 				<method_info nloc='2' complexity='1' token_count='19' nesting_level='1' start_line='399' end_line='406'></method_info>
 			<added_lines>399,400,401,402,403,404,405,406</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='compute_action' parameters='self,observation,state,prev_action,prev_reward,info,policy_id'>
 				<method_info nloc='7' complexity='1' token_count='27' nesting_level='1' start_line='344' end_line='350'></method_info>
 			<added_lines>344,345,346,347,348,349,350</added_lines>
 			<deleted_lines>344</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>376,377,378,379,407</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\rllib\agents\ddpg\apex.py' new_name='python\ray\rllib\agents\ddpg\apex.py'>
 		<file_info nloc='47' complexity='2' token_count='210'></file_info>
 		<method name='update_target_if_needed' parameters='self'>
 				<method_info nloc='7' complexity='2' token_count='55' nesting_level='1' start_line='47' end_line='54'></method_info>
 			<added_lines>51,52</added_lines>
 			<deleted_lines>51</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\rllib\agents\dqn\apex.py' new_name='python\ray\rllib\agents\dqn\apex.py'>
 		<file_info nloc='46' complexity='2' token_count='206'></file_info>
 		<method name='update_target_if_needed' parameters='self'>
 				<method_info nloc='7' complexity='2' token_count='55' nesting_level='1' start_line='50' end_line='57'></method_info>
 			<added_lines>54,55</added_lines>
 			<deleted_lines>54</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\rllib\agents\pg\pg_policy_graph.py' new_name='python\ray\rllib\agents\pg\pg_policy_graph.py'>
 		<file_info nloc='71' complexity='5' token_count='526'></file_info>
 		<method name='optimizer' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='21' nesting_level='1' start_line='94' end_line='95'></method_info>
 			<added_lines>94,95</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>92,93</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\rllib\agents\qmix\apex.py' new_name='python\ray\rllib\agents\qmix\apex.py'>
 		<file_info nloc='48' complexity='2' token_count='207'></file_info>
 		<method name='update_target_if_needed' parameters='self'>
 				<method_info nloc='7' complexity='2' token_count='55' nesting_level='1' start_line='49' end_line='56'></method_info>
 			<added_lines>53,54</added_lines>
 			<deleted_lines>53</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\rllib\evaluation\policy_evaluator.py' new_name='python\ray\rllib\evaluation\policy_evaluator.py'>
 		<file_info nloc='553' complexity='94' token_count='2937'></file_info>
 		<method name='get_policy' parameters='self,policy_id'>
 				<method_info nloc='2' complexity='1' token_count='19' nesting_level='1' start_line='492' end_line='499'></method_info>
 			<added_lines>492,493,494,495,496,497,498,499</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>500</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\rllib\evaluation\policy_graph.py' new_name='python\ray\rllib\evaluation\policy_graph.py'>
 		<file_info nloc='89' complexity='15' token_count='397'></file_info>
 		<method name='compute_single_action' parameters='self,obs,state,prev_action,prev_reward,info,episode,kwargs'>
 				<method_info nloc='8' complexity='1' token_count='28' nesting_level='1' start_line='71' end_line='78'></method_info>
 			<added_lines>74,75,76</added_lines>
 			<deleted_lines>74,75,76</deleted_lines>
 		</method>
 		<method name='compute_single_action' parameters='self,obs,state,prev_action_batch,prev_reward_batch,info_batch,episode,kwargs'>
 				<method_info nloc='8' complexity='1' token_count='28' nesting_level='1' start_line='71' end_line='78'></method_info>
 			<added_lines>74,75,76</added_lines>
 			<deleted_lines>74,75,76</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>84,85,86,98,99,100,101,102,103,104,105,106,107,108,109,111,112,113,114,115</added_lines>
 			<deleted_lines>84,85,86,99</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\rllib\evaluation\tf_policy_graph.py' new_name='python\ray\rllib\evaluation\tf_policy_graph.py'>
 		<file_info nloc='326' complexity='52' token_count='2200'></file_info>
 		<method name='_build_signature_def' parameters='self'>
 				<method_info nloc='32' complexity='6' token_count='246' nesting_level='1' start_line='269' end_line='307'></method_info>
 			<added_lines>304,305</added_lines>
 			<deleted_lines>304,305</deleted_lines>
 		</method>
 		<method name='_build_apply_gradients' parameters='self,builder,gradients'>
 				<method_info nloc='11' complexity='2' token_count='97' nesting_level='1' start_line='343' end_line='353'></method_info>
 			<added_lines>344,345,346,347</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>317,318,319,320</added_lines>
 			<deleted_lines>317,318,342</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\rllib\optimizers\async_samples_optimizer.py' new_name='python\ray\rllib\optimizers\async_samples_optimizer.py'>
 		<file_info nloc='344' complexity='45' token_count='2297'></file_info>
 		<modified_lines>
 			<added_lines>295,296,297</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\rllib\optimizers\multi_gpu_impl.py' new_name='python\ray\rllib\optimizers\multi_gpu_impl.py'>
 		<file_info nloc='205' complexity='28' token_count='1284'></file_info>
 		<method name='load_data' parameters='self,sess,inputs,state_inputs'>
 				<method_info nloc='43' complexity='10' token_count='348' nesting_level='1' start_line='117' end_line='194'></method_info>
 			<added_lines>164,165,166,167</added_lines>
 			<deleted_lines>164,165</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\rllib\test\test_supported_spaces.py' new_name='python\ray\rllib\test\test_supported_spaces.py'>
 		<file_info nloc='199' complexity='25' token_count='1246'></file_info>
 		<method name='testMultiAgent' parameters='self'>
 				<method_info nloc='37' complexity='1' token_count='157' nesting_level='1' start_line='171' end_line='207'></method_info>
 			<added_lines>172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='check_support_multiagent' parameters='alg,config'>
 				<method_info nloc='11' complexity='3' token_count='82' nesting_level='0' start_line='92' end_line='102'></method_info>
 			<added_lines>95</added_lines>
 			<deleted_lines>95</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
