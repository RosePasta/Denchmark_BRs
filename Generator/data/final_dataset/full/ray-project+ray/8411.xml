<bug_data>
<bug id='8411' author='jsuarez5341' open_date='2020-05-12T06:27:19Z' closed_time='2020-05-20T20:29:08Z'>
 	<summary>[rllib] Apparently broken compute_actio function for multiagent environments</summary>
 	<description>
 Problem: Internal error thrown when calling compute_action in a multiagent policy
 Details: Python 3.7 with the latest Ray on Ubuntu using a PyTorch policy
 Solution: Remove the cast to list (e.g. [action] -&gt; action) on line 161 of ray-project/ray/blob/master/rllib/policy/policy.py
 I assume the cast is required for single agent policies, so possibly just return action and cast to a list if the result is not iterable.
 	</description>
 	<comments>
 		<comment id='1' author='jsuarez5341' date='2020-05-12T08:12:17Z'>
 		Hey &lt;denchmark-link:https://github.com/jsuarez5341&gt;@jsuarez5341&lt;/denchmark-link&gt;
  not sure I understand the issue.
 Which method call actually fails?
 
 trainer.compute_action
 policy.compute_single_action
 policy.compute_actions
 Could you post a small repro script?
 
 		</comment>
 		<comment id='2' author='jsuarez5341' date='2020-05-12T08:24:56Z'>
 		We can't remove the [] around the action return, because it would break returning a single action (when passing in a batch-of-1 to the policy.compute_actions call).
 I think the reason is somewhere else. Maybe the new nested action space support broke something.
 		</comment>
 		<comment id='3' author='jsuarez5341' date='2020-05-12T23:00:02Z'>
 		Hmm is it that we can't desugar an array into a list? So we should be calling return_value[0] instead?
 		</comment>
 		<comment id='4' author='jsuarez5341' date='2020-05-20T09:58:05Z'>
 		Got it. Policy.compute_single_action() Has to be:
 &lt;denchmark-code&gt;        batched_action, state_out, info = self.compute_actions(
             [obs],
             state_batch,
             prev_action_batch=prev_action_batch,
             prev_reward_batch=prev_reward_batch,
             info_batch=info_batch,
             episodes=episodes,
             explore=explore,
             timestep=timestep)
 
         single_action = unbatch_actions(batched_action)
         assert len(single_action) == 1
         single_action = single_action[0]
 
         if clip_actions:
             single_action = clip_action(
                 single_action, self.action_space_struct)
 
         # Return action, internal state(s), infos.
         return single_action, [s[0] for s in state_out], \
             {k: v[0] for k, v in info.items()}
 &lt;/denchmark-code&gt;
 
 In case of a complex nested action, the action comes back as batched (nested struct of different batches) and hence has to be converted into a single nested action.
 Will PR now. ...
 		</comment>
 		<comment id='5' author='jsuarez5341' date='2020-05-20T10:31:14Z'>
 		&lt;denchmark-link:https://github.com/jsuarez5341&gt;@jsuarez5341&lt;/denchmark-link&gt;
  Could you check this PR and let me know, whether this fixes the issue?
 &lt;denchmark-link:https://github.com/ray-project/ray/pull/8514&gt;#8514&lt;/denchmark-link&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='d76578700dc57016fa01bd35dfea36afa9dda3c8' author='Sven Mika' date='2020-05-20 22:29:08+02:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='rllib\agents\ars\ars_tf_policy.py' new_name='rllib\agents\ars\ars_tf_policy.py'>
 		<file_info nloc='53' complexity='7' token_count='478'></file_info>
 		<method name='compute_actions' parameters='self,observation,add_noise,update'>
 				<method_info nloc='9' complexity='3' token_count='103' nesting_level='1' start_line='57' end_line='65'></method_info>
 			<added_lines>62</added_lines>
 			<deleted_lines>62</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>14</added_lines>
 			<deleted_lines>10</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\agents\es\es_tf_policy.py' new_name='rllib\agents\es\es_tf_policy.py'>
 		<file_info nloc='88' complexity='18' token_count='722'></file_info>
 		<method name='compute_actions' parameters='self,observation,add_noise,update'>
 				<method_info nloc='10' complexity='2' token_count='89' nesting_level='1' start_line='102' end_line='114'></method_info>
 			<added_lines>113</added_lines>
 			<deleted_lines>114</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>14</added_lines>
 			<deleted_lines>9,15</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\agents\es\es_torch_policy.py' new_name='rllib\agents\es\es_torch_policy.py'>
 		<file_info nloc='83' complexity='12' token_count='654'></file_info>
 		<method name='before_init._compute_actions' parameters='policy,obs_batch,add_noise,update'>
 				<method_info nloc='14' complexity='3' token_count='135' nesting_level='1' start_line='53' end_line='67'></method_info>
 			<added_lines>64</added_lines>
 			<deleted_lines>64</deleted_lines>
 		</method>
 		<method name='before_init' parameters='policy,observation_space,action_space,config'>
 				<method_info nloc='13' complexity='1' token_count='88' nesting_level='0' start_line='19' end_line='69'></method_info>
 			<added_lines>64</added_lines>
 			<deleted_lines>64</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>13</added_lines>
 			<deleted_lines>8</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\evaluation\sampler.py' new_name='rllib\evaluation\sampler.py'>
 		<file_info nloc='556' complexity='68' token_count='3530'></file_info>
 		<method name='unbatch_actions' parameters='action_batches'>
 				<method_info nloc='10' complexity='3' token_count='65' nesting_level='0' start_line='729' end_line='763'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>21,691</added_lines>
 			<deleted_lines>22,691,764,765</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\policy\policy.py' new_name='rllib\policy\policy.py'>
 		<file_info nloc='180' complexity='25' token_count='848'></file_info>
 		<modified_lines>
 			<added_lines>10,11,162,172,173,174,175,177,178,181</added_lines>
 			<deleted_lines>10,161,172,175</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\utils\space_utils.py' new_name='rllib\utils\space_utils.py'>
 		<file_info nloc='43' complexity='18' token_count='306'></file_info>
 		<method name='unbatch' parameters='batches_struct'>
 				<method_info nloc='10' complexity='3' token_count='65' nesting_level='0' start_line='97' end_line='130'></method_info>
 			<added_lines>97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>95,96</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
