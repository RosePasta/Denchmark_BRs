<bug_data>
<bug id='7843' author='pengzhenghao' open_date='2020-04-01T03:14:32Z' closed_time='2020-04-03T17:44:59Z'>
 	<summary>[rllib] PPO policy return all zeros for action_logp</summary>
 	<description>
 &lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;
 
 Ray version and other system information (Python version, TensorFlow version, OS):
 PPO policy return all zeros when computing the log probability of actions.
 ray=0.8.2 (latest)
 tensorflow=2.1.0
 &lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;
 
 Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
 import ray
 import numpy as np
 from ray.rllib.agents.ppo import PPOTrainer
 
 ray.init()
 
 ppo_agent = PPOTrainer(env="BipedalWalker-v2")
 
 ppo_agent.get_policy().get_session().run(
     ppo_agent.get_policy()._action_logp,
     feed_dict={
         ppo_agent.get_policy()._input_dict["obs"]: np.random.random((500, 24))
     }
 )
 Result in
 &lt;denchmark-code&gt;array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.], dtype=float32)
 &lt;/denchmark-code&gt;
 
 If we cannot run your script, we cannot fix your issue.
 
  I have verified my script runs in a clean environment and reproduces the issue.
  I have verified the issue also occurs with the latest wheels.
 
 	</description>
 	<comments>
 		<comment id='1' author='pengzhenghao' date='2020-04-01T03:36:18Z'>
 		The issue still happen in Ray=0.8.3
 import ray
 import numpy as np
 from ray.rllib.agents.ppo import PPOTrainer
 
 print(ray.__version__)
 
 ray.init(ignore_reinit_error=True)
 
 ppo_agent = PPOTrainer(env="BipedalWalker-v2")
 
 ppo_agent.get_policy().get_session().run(
     ppo_agent.get_policy()._sampled_action_logp,
     feed_dict={
         ppo_agent.get_policy()._input_dict["obs"]: np.random.random((500, 24))
     }
 )
 		</comment>
 		<comment id='2' author='pengzhenghao' date='2020-04-01T04:45:26Z'>
 		Ray 0.8.1 works well! Something wrong with the updates introduced by 0.8.2
 		</comment>
 		<comment id='3' author='pengzhenghao' date='2020-04-01T05:38:11Z'>
 		I did a bit of digging and it seems related to the is_exploring flag somehow being False for PPO in multi-GPU mode (works fine with simple_optimizer: True). &lt;denchmark-link:https://github.com/sven1977&gt;@sven1977&lt;/denchmark-link&gt;
 
 I think this is since the explore placeholder isn't passed to copies in dynamic_tf_policy.copy().
 		</comment>
 		<comment id='4' author='pengzhenghao' date='2020-04-01T06:11:53Z'>
 		Yeah, that's possible. I'll take a look.
 		</comment>
 		<comment id='5' author='pengzhenghao' date='2020-04-01T06:32:07Z'>
 		Yeah, it's not copied along. Also default placeholder is True for TFPolicy, but False for DynamicTFPolicy (I made them both True now).
 We are still not copying the is_training placeholder either? Shouldn't we do that as well?
 		</comment>
 		<comment id='6' author='pengzhenghao' date='2020-04-01T06:33:27Z'>
 		I'm assuming the copy()-method is used for multi-GPU tower generation?
 		</comment>
 		<comment id='7' author='pengzhenghao' date='2020-04-01T06:37:09Z'>
 		Also, you should probably rather do:
 &lt;denchmark-code&gt;a, state_out, extras = trainer.get_policy().compute_actions([your_obs])
 
 # Then:
 action_probs = extras["action_prob"]
 action_logp = extras["action_logp"]
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='8' author='pengzhenghao' date='2020-04-01T07:44:08Z'>
 		Yep, copy is only used to generate towers.
 &lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;
 
 
 On Tue, Mar 31, 2020, 11:37 PM Sven Mika ***@***.***&gt; wrote:
  Also, you should probably rather do:
 
  a, state_out, extras = trainer.get_policy().compute_actions([your_obs])
 
  # Then:
  action_probs = extras["action_prob"]
  action_logp = extras["action_logp"]
 
  —
  You are receiving this because you commented.
  Reply to this email directly, view it on GitHub
  &lt;#7843 (comment)&gt;,
  or unsubscribe
  &lt;https://github.com/notifications/unsubscribe-auth/AAADUSWBDXCPIHZOF2WQW7LRKLOKFANCNFSM4LYK5SMQ&gt;
  .
 
 
 
 		</comment>
 		<comment id='9' author='pengzhenghao' date='2020-04-01T10:12:18Z'>
 		&lt;denchmark-link:https://github.com/pengzhenghao&gt;@pengzhenghao&lt;/denchmark-link&gt;
  Could you try this PR and see whether it fixes your issue?
 &lt;denchmark-link:https://github.com/ray-project/ray/pull/7846&gt;#7846&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='10' author='pengzhenghao' date='2020-04-02T01:32:07Z'>
 		&lt;denchmark-link:https://user-images.githubusercontent.com/22206995/78201694-ca9e3780-74c4-11ea-950c-209674ae29a5.png&gt;&lt;/denchmark-link&gt;
 
 Is this related the issue and PR?
 		</comment>
 	</comments>
 </bug>
<commit id='bb6c67523171f807c631461b97c00475b21300ef' author='Sven Mika' date='2020-04-03 10:44:58-07:00'>
 	<dmm_unit complexity='0.96' interfacing='0.96' size='0.04'></dmm_unit>
 	<modification change_type='MODIFY' old_name='rllib\BUILD' new_name='rllib\BUILD'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>113</added_lines>
 			<deleted_lines>113</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\agents\ppo\ppo.py' new_name='rllib\agents\ppo\ppo.py'>
 		<file_info nloc='132' complexity='27' token_count='752'></file_info>
 		<method name='choose_policy_optimizer' parameters='workers,config'>
 				<method_info nloc='19' complexity='2' token_count='112' nesting_level='0' start_line='80' end_line='99'></method_info>
 			<added_lines>98,99</added_lines>
 			<deleted_lines>95</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>70,71,72</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\agents\ppo\tests\test_ppo.py' new_name='rllib\agents\ppo\tests\test_ppo.py'>
 		<file_info nloc='184' complexity='22' token_count='1688'></file_info>
 		<method name='test_ppo_fake_multi_gpu_learning' parameters='self'>
 				<method_info nloc='22' complexity='3' token_count='142' nesting_level='1' start_line='49' end_line='74'></method_info>
 			<added_lines>49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>75</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\optimizers\multi_gpu_optimizer.py' new_name='rllib\optimizers\multi_gpu_optimizer.py'>
 		<file_info nloc='187' complexity='23' token_count='1276'></file_info>
 		<method name='__init__' parameters='self,workers,sgd_batch_size,num_sgd_iter,rollout_fragment_length,num_envs_per_worker,train_batch_size,num_gpus,standardize_fields,shuffle_sequences'>
 				<method_info nloc='10' complexity='1' token_count='40' nesting_level='1' start_line='40' end_line='49'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>49</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,workers,sgd_batch_size,num_sgd_iter,rollout_fragment_length,num_envs_per_worker,train_batch_size,num_gpus,standardize_fields,shuffle_sequences,_fake_gpus'>
 				<method_info nloc='11' complexity='1' token_count='44' nesting_level='1' start_line='41' end_line='51'></method_info>
 			<added_lines>50,51</added_lines>
 			<deleted_lines>49</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>32,33,34,67,68,69,79,80,82,83,84,85,86,87,88</added_lines>
 			<deleted_lines>32,33,75,76,77,78,79</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\policy\dynamic_tf_policy.py' new_name='rllib\policy\dynamic_tf_policy.py'>
 		<file_info nloc='314' complexity='36' token_count='2047'></file_info>
 		<method name='copy' parameters='self,existing_inputs'>
 				<method_info nloc='36' complexity='10' token_count='327' nesting_level='1' start_line='262' end_line='304'></method_info>
 			<added_lines>285,286,287</added_lines>
 			<deleted_lines>284,285</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>115,116,128,129,130</added_lines>
 			<deleted_lines>126,127,178,179</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='rllib\tuned_examples\regression_tests\cartpole-ppo-tf-multi-gpu.yaml'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 	</modification>
 </commit>
</bug_data>
