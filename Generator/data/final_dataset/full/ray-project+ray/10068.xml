<bug_data>
<bug id='10068' author='mvindiola1' open_date='2020-08-12T16:11:13Z' closed_time='2020-11-04T08:33:56Z'>
 	<summary>[rllib] MARWIL does not convert SampleBatch state_in_* values to torch tensors</summary>
 	<description>
 Ray [0.6.8] and Ray nightly wheel: &lt;denchmark-link:https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp36-cp36m-manylinux1_x86_64.wh&gt;https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp36-cp36m-manylinux1_x86_64.wh&lt;/denchmark-link&gt;
 
 When training marwil with an LSTM layer using pytorch I get the following error:
 
 File ".../lib/python3.6/site-packages/ray/rllib/models/torch/recurrent_net.py", line 155, in forward_rnn:
 [torch.unsqueeze(state[0], 0),
 TypeError: unsqueeze(): argument 'input' (position 1) must be Tensor, not numpy.ndarray
 
 I do not get any errors in TF.
 This is the script I used to test and generate the attached error log.
 &lt;denchmark-link:https://github.com/ray-project/ray/files/5064082/error.txt&gt;torch_error.txt&lt;/denchmark-link&gt;
 
 &lt;denchmark-code&gt;import ray
 import ray.rllib.agents.marwil as marwil
 from ray import tune
 
 ray.init(redis_port=6379)
 config = marwil.DEFAULT_CONFIG.copy()
 config["env"] = "CartPole-v0"
 config["num_workers"] = 1
 config["model"]["use_lstm"] = True
 
 for framework in ["tf", "torch"]:
     print("Testing MARWIL with framework: {}".format(framework))
     input_file = "/tmp/marwil/cartpole_{}".format(framework)
     config["framework"] = framework
 
     # Create input file
     config["input"] = "sampler"
     config["output"] = input_file
     config["output_max_file_size"] = 50000000
     trainer = marwil.MARWILTrainer(config=config, env="CartPole-v0")
     tune.run("MARWIL", stop={"training_iteration": 10},
              config=config, local_dir="/tmp/marwil/logs/create_{}".format(framework), verbose=False)
 
     # Train with input file
     config["input"] = input_file
     config["output"] = None
     trainer = marwil.MARWILTrainer(config=config, env="CartPole-v0")
     tune.run("MARWIL", stop={"training_iteration": 10},
              config=config, local_dir="/tmp/marwil/logs/train_{}".format(framework), verbose=False)
 ray.shutdown()
 
 
 
 - [X ] I have verified my script runs in a clean environment and reproduces the issue.
 - [X ] I have verified the issue also occurs with the [latest wheels](https://docs.ray.io/en/latest/installation.html).
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='mvindiola1' date='2020-08-14T20:45:38Z'>
 		&lt;denchmark-link:https://github.com/sven1977&gt;@sven1977&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='mvindiola1' date='2020-10-28T21:16:40Z'>
 		&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
  I just re-tested this with the nightly wheel. The result is the same as before. It works with tf but not with pytorch. The error seems to be in evaluation sampling. If I add this to the config  then pytorch trains without issue. I tried it with  and that failed with a different type of error.
 I am willing to hunt the pytorch issue down and prepare a PR if I am given a nudge in the direction to look.
 		</comment>
 		<comment id='3' author='mvindiola1' date='2020-10-29T14:10:59Z'>
 		This change in torch_policy::compute_log_likelihoods fixed the issue for me:
 @@ -306,6 +306,10 @@ class TorchPolicy(Policy):
              if prev_reward_batch is not None:
                  input_dict[SampleBatch.PREV_REWARDS] = prev_reward_batch
              seq_lens = torch.ones(len(obs_batch), dtype=torch.int32)
 +            state_batches = [
 +                convert_to_torch_tensor(s, self.device)
 +                for s in (state_batches or [])
 +            ]
 
              # Exploration hook before each forward pass.
              self.exploration.before_compute_actions(explore=False)
 It is the same thing you did in torch_policy::compute_actions here:
 
 
 
 ray/rllib/policy/torch_policy.py
 
 
         Lines 163 to 170
       in
       91fa7e0
 
 
 
 
 
 
  state_batches = [ 
 
 
 
  convert_to_torch_tensor(s, self.device) 
 
 
 
  for s in (state_batches or []) 
 
 
 
  ] 
 
 
 
  actions, state_out, extra_fetches, logp = \ 
 
 
 
  self._compute_action_helper( 
 
 
 
  input_dict, state_batches, seq_lens, explore, timestep) 
 
 
 
  
 
 
 
 
 
 		</comment>
 	</comments>
 </bug>
<commit id='4518fe790fd4d5a25ed44080b65ac23ebb1eadcc' author='mvindiola1' date='2020-11-04 09:33:56+01:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='rllib\policy\torch_policy.py' new_name='rllib\policy\torch_policy.py'>
 		<file_info nloc='487' complexity='42' token_count='3113'></file_info>
 		<modified_lines>
 			<added_lines>309,310,311,312</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
