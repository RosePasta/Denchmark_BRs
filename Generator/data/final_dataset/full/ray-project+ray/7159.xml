<bug_data>
<bug id='7159' author='justinglibert' open_date='2020-02-13T22:22:55Z' closed_time='2020-02-17T18:26:59Z'>
 	<summary>RLLib: PyTorch Recurrent Model: Hidden_state is not a Tensor, it's a Numpy Array</summary>
 	<description>
 &lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;
 
 Latest ray installed from this wheel: &lt;denchmark-link:https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp36-cp36m-manylinux1_x86_64.whl&gt;https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp36-cp36m-manylinux1_x86_64.whl&lt;/denchmark-link&gt;
 
 &lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;
 
 PyTorch models get back a hidden state that has been casted to a Numpy array.
 That breaks torch.nn and will break backprop too.
 I took this file as an example of a RNN in PyTorch:
 &lt;denchmark-link:https://github.com/ray-project/ray/blob/master/rllib/agents/qmix/model.py&gt;https://github.com/ray-project/ray/blob/master/rllib/agents/qmix/model.py&lt;/denchmark-link&gt;
 
 import ray
 from ray import tune
 from ray.rllib.agents import ppo
 from ray.rllib.models import ModelCatalog
 from ray.rllib.models.torch.torch_modelv2 import TorchModelV2
 from ray.rllib.utils.annotations import override
 from ray.tune.registry import register_env
 import gym
 from gym.spaces import Discrete, Box
 import numpy as np
 
 import torch
 from torch import nn
 
 
 class SimpleCorridor(gym.Env):
     """Example of a custom env in which you have to walk down a corridor.
     You can configure the length of the corridor via the env config."""
 
     def __init__(self, config):
         self.end_pos = config["corridor_length"]
         self.cur_pos = 0
         self.action_space = Discrete(2)
         self.observation_space = Box(
             0.0, self.end_pos, shape=(1, ), dtype=np.float32)
 
     def reset(self):
         self.cur_pos = 0
         return [self.cur_pos]
 
     def step(self, action):
         assert action in [0, 1], action
         if action == 0 and self.cur_pos &gt; 0:
             self.cur_pos -= 1
         elif action == 1:
             self.cur_pos += 1
         done = self.cur_pos &gt;= self.end_pos
         return [self.cur_pos], 1 if done else 0, done, {}
 
 
 
 class TestNet(TorchModelV2, nn.Module):
     def init_hidden(self, hidden_size):
         h0 = self.policy_fc.weight.new(1, hidden_size).zero_()
         c0 = self.policy_fc.weight.new(1, hidden_size).zero_()
         return (h0, c0)
 
     def __init__(self, obs_space, action_space, num_outputs, config, name):
         TorchModelV2.__init__(
             self, obs_space, action_space, num_outputs, config, name
         )
         nn.Module.__init__(self)
         # Value function
         self._value_branch = nn.Sequential(
             nn.Linear(1, 4),
             nn.LeakyReLU(),
             nn.Linear(4, 1)
         )
         # Policy
         self.policy_fc = nn.Linear(64, 2)
         self.lstm = nn.LSTM(1, 64, batch_first=True)        
         self._cur_value = None
 
     @override(TorchModelV2)
     def get_initial_state(self):
         # make hidden states on same device as model
         lstm_h, lstm_c = self.init_hidden(64)
     
         initial_state = [
             lstm_h,
             lstm_c,
         ]
         return initial_state
 
     @override(TorchModelV2)
     def value_function(self):
         assert self._cur_value is not None, "must call forward() first"
         return self._cur_value
 
     def forward(self, input_dict, hidden_state, seq_lens):
         obs = input_dict['obs_flat']
         batch_size, features = obs.size()
 
         # Unpack the hidden_state
         lstm_h = hidden_state[0]
         # THIS IS NOT A TENSOR
         print(type(lstm_h))
         lstm_c = hidden_state[1]
 
         # Build the tuples
         lstm_hidden = (lstm_h.reshape(-1, 1, 64), lstm_c.reshape(-1, 1, 64))
 
         out, lstm_hidden = self.lstm(
             obs.view(-1, 1, 1), lstm_hidden
         )
         new_hidden_state = [
             lstm_hidden[0],
             lstm_hidden[1],
         ]
         print(new_hidden_state)
         # Value function
         self._cur_value = self._value_branch(obs)
         self.logits = self.policy_fc(out)
         return logits, new_hidden_state
 
 
 
 
 ModelCatalog.register_custom_model("test", TestNet)
 register_env("test", lambda config: SimpleCorridor(config))
 
 ray.init()
 tune.run(
     ppo.PPOTrainer,
     config={
         "env": "test",
         "use_pytorch": True,
         "env_config": {
             "corridor_length": 10
         },
         "model": {
             "custom_model": "test",
             "custom_options": {},
         },
     },
 )
 
  I have verified my script runs in a clean environment and reproduces the issue.
  I have verified the issue also occurs with the latest wheels.
 
 	</description>
 	<comments>
 		<comment id='1' author='justinglibert' date='2020-02-13T23:57:55Z'>
 		Hm so I think it's expected it will be numpy during inference (since we never backprop during that). However during learning, it should be a Tensor.
 		</comment>
 		<comment id='2' author='justinglibert' date='2020-02-14T00:06:59Z'>
 		Try out &lt;denchmark-link:https://github.com/ray-project/ray/pull/7162&gt;#7162&lt;/denchmark-link&gt;
  ? I couldn't get your example to run completely but it seems to have gotten further (had to also add ).
 		</comment>
 		<comment id='3' author='justinglibert' date='2020-02-14T00:49:58Z'>
 		Thanks for the fast reply!
 "I think it's expected it will be numpy during inference (since we never backprop during that). However during learning, it should be a Tensor."
 Do you mean that it is incorrectly doing some eval? Or is the default behaviour of the trainer to first eval before doing any training?
 I'll try the PR tomorrow!
 Also: If the hidden state is a numpy array during eval, what's the recommended way of making sure it's always a Tensor? Should I check the type of the variable and cast it to a Tensor when it's not (during eval I guess)?
 		</comment>
 	</comments>
 </bug>
<commit id='42aea966ff376fff015cabdcf5d059adcc967ad3' author='Eric Liang' date='2020-02-17 10:26:58-08:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='rllib\agents\trainer.py' new_name='rllib\agents\trainer.py'>
 		<file_info nloc='543' complexity='101' token_count='3192'></file_info>
 		<method name='_setup.normalize' parameters='env'>
 				<method_info nloc='7' complexity='2' token_count='34' nesting_level='3' start_line='544' end_line='550'></method_info>
 			<added_lines>544,545,546,547,548,549,550</added_lines>
 			<deleted_lines>544</deleted_lines>
 		</method>
 		<method name='_setup' parameters='self,config'>
 				<method_info nloc='46' complexity='7' token_count='309' nesting_level='1' start_line='521' end_line='588'></method_info>
 			<added_lines>543,544,545,546,547,548,549,550,551,552</added_lines>
 			<deleted_lines>543,544</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\policy\torch_policy.py' new_name='rllib\policy\torch_policy.py'>
 		<file_info nloc='206' complexity='43' token_count='1442'></file_info>
 		<method name='_lazy_tensor_dict' parameters='self,postprocessed_batch'>
 				<method_info nloc='4' complexity='1' token_count='23' nesting_level='1' start_line='215' end_line='218'></method_info>
 			<added_lines>217</added_lines>
 			<deleted_lines>215,216,217,218</deleted_lines>
 		</method>
 		<method name='_convert_to_tensor' parameters='self,arr'>
 				<method_info nloc='7' complexity='3' token_count='62' nesting_level='1' start_line='220' end_line='226'></method_info>
 			<added_lines>220,221,222,223,224,225,226</added_lines>
 			<deleted_lines>220,221,222,223,224</deleted_lines>
 		</method>
 		<method name='_lazy_tensor_dict.convert' parameters='arr'>
 				<method_info nloc='7' complexity='3' token_count='60' nesting_level='2' start_line='216' end_line='222'></method_info>
 			<added_lines>217,220,221,222</added_lines>
 			<deleted_lines>216,217,218,219,220,221,222</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>77,78,79,227</added_lines>
 			<deleted_lines>77</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
