<bug_data>
<bug id='10928' author='mkoh-asapp' open_date='2020-09-21T17:31:43Z' closed_time='2020-10-10T00:22:21Z'>
 	<summary>[autoscaler] Unable to launch Kubernetes cluster if I don't have permission to list namespaces</summary>
 	<description>
 &lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;
 
 When trying to create a cluster, Ray is trying to check if the namespace that I provide in my cluster config exists or not. And if it does not exist, it will try to create it.  (&lt;denchmark-link:https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/_private/kubernetes/config.py#L61&gt;https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/_private/kubernetes/config.py#L61&lt;/denchmark-link&gt;
 )
 This causes a problem if I do not have permission to list namespaces or create namespaces on the kube cluster I'm using.
 Ray version and other system information (Python version, TensorFlow version, OS):
 Ray 0.8.7
 Python 3.7
 &lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;
 
 Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
 Here is a ray config to reproduce
 &lt;denchmark-code&gt;# Created: 2020-09-21T10:39:01-04:00
 cluster_name: mkoh-test
 min_workers: 1
 max_workers: 2
 initial_workers: 1
 autoscaling_mode: default
 target_utilization_fraction: 0.8
 idle_timeout_minutes: 60
 provider:
   type: kubernetes
   use_internal_ips: true
   namespace: nlp
 head_node:
   apiVersion: v1
   kind: Pod
   metadata:
     generateName: ray-head-
     labels:
       component: ray-head
   spec:
     serviceAccountName: autoscaler
     restartPolicy: Never
     volumes:
     - name: dshm
       emptyDir:
         medium: Memory
     containers:
     - name: ray-node
       image:
 worker_nodes:
   apiVersion: v1
   kind: Pod
   metadata:
     generateName: ray-worker-
     labels:
       component: ray-worker
   spec:
     serviceAccountName: default
     restartPolicy: Never
     volumes:
     - name: dshm
       emptyDir:
         medium: Memory
     containers:
     - name: ray-node
       imagePullPolicy:
       image:
 file_mounts: {}
 cluster_synced_files: []
 file_mounts_sync_continuously: false
 initialization_commands: []
 setup_commands: []
 head_setup_commands: []
 worker_setup_commands: []
 head_start_ray_commands:
 - ray stop
 - ulimit -n 65536; ray start --head --num-cpus=$MY_CPU_REQUEST --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml --webui-host 0.0.0.0
 worker_start_ray_commands:
 - ray stop
 - ulimit -n 65536; ray start --num-cpus=$MY_CPU_REQUEST --address=$RAY_HEAD_IP:6379 --object-manager-port=8076
 &lt;/denchmark-code&gt;
 
 If we cannot run your script, we cannot fix your issue.
 
  I have verified my script runs in a clean environment and reproduces the issue.
  I have verified the issue also occurs with the latest wheels.
 
 	</description>
 	<comments>
 		<comment id='1' author='mkoh-asapp' date='2020-10-06T01:47:12Z'>
 		Looks like we need to use the Kubernetes  API to look at permissions for the namespace â€” I'm looking into it.
 &lt;denchmark-link:https://kubernetes.io/docs/reference/access-authn-authz/authorization/&gt;https://kubernetes.io/docs/reference/access-authn-authz/authorization/&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='mkoh-asapp' date='2020-10-06T13:20:33Z'>
 		I believe that SelfSubjectAccessReview can also be something that a user might not be permitted to do, depending on the cluster configuration. Can I suggest that it would be better to not do any sort of check at all and just let the operation fail on its own if the requested namespace does not exist? I feel like Ray should not be trying to create any namespaces and leave that up to the cluster admin.
 (By "let the operation fail on its own" I mean let the Kube api will fail when you try to create the resources in a namespace that cannot be accessed)
 		</comment>
 	</comments>
 </bug>
<commit id='48db6f8858b4c971563a41f8751ad012a7a55446' author='Gekho457' date='2020-10-09 19:22:20-05:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='0.25'></dmm_unit>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\_private\kubernetes\config.py' new_name='python\ray\autoscaler\_private\kubernetes\config.py'>
 		<file_info nloc='159' complexity='39' token_count='1165'></file_info>
 		<method name='_configure_namespace' parameters='provider_config'>
 				<method_info nloc='24' complexity='4' token_count='150' nesting_level='0' start_line='59' end_line='85'></method_info>
 			<added_lines>66,67,68,69,70,71,72,73</added_lines>
 			<deleted_lines>61</deleted_lines>
 		</method>
 		<method name='not_checking_msg' parameters='resource_type,name'>
 				<method_info nloc='2' complexity='1' token_count='16' nesting_level='0' start_line='34' end_line='35'></method_info>
 			<added_lines>34,35</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>4,36,37</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
