<bug_data>
<bug id='7073' author='matter-funds' open_date='2020-02-06T16:49:01Z' closed_time='2020-11-25T22:09:54Z'>
 	<summary>Using max_calls=1 with high memory usage functions causes OOM issues</summary>
 	<description>
 &lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;
 
 Invoking ray.remote with max_calls=1 and functions which require large amounts of memory causes workers to crash with OOM issues. Using max_calls=None causes no issues.
 Ray throws the following error:
 &lt;denchmark-code&gt;020-02-06 16:40:08,621  ERROR worker.py:1003 -- Possible unhandled error from worker: ray::IDLE (pid=4978, ip=172.31.38.182)
   File "python/ray/_raylet.pyx", line 631, in ray._raylet.execute_task
   File "/home/ubuntu/project/env/lib/python3.6/site-packages/ray/memory_monitor.py", line 126, in raise_if_low_memory
     self.error_threshold))
 ray.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node ip-172-31-38-182 is used (3.56 / 3.62 GB). The top 10 memory consumers are:
 
 PID     MEM     COMMAND
 4977    0.33GiB ray::__main__.foo()
 4125    0.13GiB ray::IDLE
 4062    0.13GiB ray::IDLE
 4335    0.13GiB ray::IDLE
 4315    0.13GiB ray::IDLE
 4229    0.13GiB ray::IDLE
 4206    0.13GiB ray::IDLE
 4175    0.13GiB ray::IDLE
 4280    0.13GiB ray::IDLE
 4436    0.13GiB ray::IDLE
 
 In addition, up to 0.0 GiB of shared memory is currently being used by the Ray object store. You can set the object store size with the `object_store_memory` parameter when starting Ray, and the max Redis size with `redis_max_memory`. Note that Ray assumes all system memory is available for use by workers. If your system has other applications running, you should manually set these memory limits to a lower value.
 &lt;/denchmark-code&gt;
 
 It seems that all those ray::IDLE processes are hanging around eating up memory. The worker machine sometimes recovers, but sometimes freezes due to lack of memory and eventually crashes.
 Ray version and other system information (Python version, TensorFlow version, OS):
 ray version 0.8.1
 Python 3.6
 Running ray cluster on AWS. Head node is r5.large (with 16G RAM), worker nodes are c5.large (with 4G RAM).
 Ubuntu 18.04 LTS on head, worker and driver nodes.
 &lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;
 
 Reproducing the issue involves setting up a ray cluster. The relevant setting from my .yaml are:
 &lt;denchmark-code&gt;cluster_name: ray_test
 min_workers: 2
 max_workers: 2
 head_node:
     InstanceType: r5.large
     ImageId: ami-0ac3869e129c60af6 # Ubuntu 18.04 LTS
     BlockDeviceMappings:
         - DeviceName: /dev/sda1
           Ebs:
               VolumeSize: 100
 worker_nodes:
     InstanceType: c5.large
     ImageId: ami-0ac3869e129c60af6 # Ubuntu 18.04 LTS
 &lt;/denchmark-code&gt;
 
 The python script looks as follows:
 ray_test.py
 &lt;denchmark-code&gt;import ray
 import time
 
 ray.init('auto')
 
 def foo():
   #This functions does nothing, but takes up around 400M of RAM
   a = [1] * 30000000
 
 jobs = []
 for _ in range(4000):
   jobs.append(ray.remote(max_calls=1)(foo).remote())
 
 objids = jobs
 while True:
   objids_ready, objids_pending = ray.wait(objids, len(objids), 0)
   time.sleep(0.5)
   if 0==len(objids_pending):
     break
 &lt;/denchmark-code&gt;
 
 The steps to repro are as follows:
 
 Launch the ray cluster with ray up autoscaler/ray_test.yaml
 Launch local ray node with 0 resources with ray start --address=$INTERNAL_HEAD_IP:6379 --redis-password='5241590000000000' --num-cpus=0 --num-gpus=0. I don't think this is mandatory, but I like to have my driver not on the head.
 Run python ray_test.py.
 
 At this point, ray will start complaining, first with:
 &lt;denchmark-code&gt;{CPU: 2.000000}, {memory: 2.197266 GiB}, {object_store_memory: 0.634766 GiB}. In total there are 1 pending tasks and 0 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
 &lt;/denchmark-code&gt;
 
 And eventually with the message I've pasted at the top of this post, the one talking about ray::IDLE.
 Interestingly, if I remove the max_calls=1 argument in ray_test.py, I can no longer reproduce the issue.
 I speculate the issue has to do with how ray spawns a new process for each function run, but there's no way for me to be sure.
 Running with memory=500*1024*1024 delays the crash, but doesn't prevent it.
 	</description>
 	<comments>
 		<comment id='1' author='matter-funds' date='2020-07-08T01:14:19Z'>
 		&lt;denchmark-link:https://github.com/matter-funds&gt;@matter-funds&lt;/denchmark-link&gt;
  we found and fixed a bug based on your script, but we're not sure it's the bug you originally detected. Can you  still reproduce this bug on the latest master (preferably on one machine?)
 		</comment>
 		<comment id='2' author='matter-funds' date='2020-07-08T01:18:11Z'>
 		Just to add on... one possible issue is the use of ray.remote(foo) in a loop... which is re-registering the function many times. If you lift it to @ray.remote once then it seems to run stably.
 		</comment>
 		<comment id='3' author='matter-funds' date='2020-11-11T21:24:46Z'>
 		Hi, I'm a bot from the Ray team :)
 To help human contributors to focus on more relevant issues, I will automatically add the stale label to issues that have had no activity for more than 4 months.
 If there is no further activity in the 14 days, the issue will be closed!
 
 If you'd like to keep the issue open, just leave any comment, and the stale label will be removed!
 If you'd like to get more attention to the issue, please tag one of Ray's contributors.
 
 You can always ask for help on our &lt;denchmark-link:https://discuss.ray.io/&gt;discussion forum&lt;/denchmark-link&gt;
  or &lt;denchmark-link:https://github.com/ray-project/ray#getting-involved&gt;Ray's public slack channel&lt;/denchmark-link&gt;
 .
 		</comment>
 		<comment id='4' author='matter-funds' date='2020-11-25T22:09:51Z'>
 		Hi again! The issue will be closed because there has been no more activity in the 14 days since the last message.
 Please feel free to reopen or open a new issue if you'd still like it to be addressed.
 Again, you can always ask for help on our &lt;denchmark-link:https://discuss.ray.io&gt;discussion forum&lt;/denchmark-link&gt;
  or &lt;denchmark-link:https://github.com/ray-project/ray#getting-involved&gt;Ray's public slack channel&lt;/denchmark-link&gt;
 .
 Thanks again for opening the issue!
 		</comment>
 	</comments>
 </bug>
<commit id='21af0ceb0c916bdafc1e8f82d13cf7c80b6d2b28' author='Alex Wu' date='2020-07-28 13:51:34-07:00'>
 	<dmm_unit complexity='1.0' interfacing='0.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='python\ray\import_thread.py' new_name='python\ray\import_thread.py'>
 		<file_info nloc='130' complexity='28' token_count='714'></file_info>
 		<method name='_process_key' parameters='self,key'>
 				<method_info nloc='30' complexity='8' token_count='165' nesting_level='1' start_line='110' end_line='157'></method_info>
 			<added_lines>139,140,141,142,143,144,145</added_lines>
 			<deleted_lines>139,140,141</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\remote_function.py' new_name='python\ray\remote_function.py'>
 		<file_info nloc='165' complexity='15' token_count='772'></file_info>
 		<modified_lines>
 			<added_lines>44</added_lines>
 			<deleted_lines>44</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\tests\test_advanced.py' new_name='python\ray\tests\test_advanced.py'>
 		<file_info nloc='361' complexity='90' token_count='2628'></file_info>
 		<method name='test_profiling_api' parameters='ray_start_2_cpus'>
 				<method_info nloc='34' complexity='6' token_count='147' nesting_level='0' start_line='176' end_line='221'></method_info>
 			<added_lines>205,206</added_lines>
 			<deleted_lines>205</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='src\ray\core_worker\core_worker.cc' new_name='src\ray\core_worker\core_worker.cc'>
 		<file_info nloc='1671' complexity='289' token_count='13222'></file_info>
 		<method name='ray::CoreWorkerProcess::GetCoreWorker' parameters=''>
 				<method_info nloc='11' complexity='2' token_count='62' nesting_level='1' start_line='149' end_line='163'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>152,153,154,155</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
