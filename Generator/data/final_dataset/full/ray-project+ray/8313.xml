<bug_data>
<bug id='8313' author='yutaizhou' open_date='2020-05-04T19:18:13Z' closed_time='2020-08-11T19:10:29Z'>
 	<summary>Ray cannot identify non-base 10 GPU numbering on SLURM</summary>
 	<description>
 Hey all,
 I am using Ray on SLURM equipped with 2 GPUs per node. I get the following error when I submit a job that acquired GPUs:
 &lt;denchmark-code&gt;                                                                                                                                                                   
   1   /state/partition1/user/yutai/raytmp
   2   Traceback (most recent call last):
   3   File "/home/gridsan/yutai/.conda/envs/football2/bin/ray", line 5, in &lt;module&gt;
   4     from ray.scripts.scripts import main
   5   File "/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/__init__.py", line 65, in &lt;module&gt;
   6     from ray.worker import (
   7   File "/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/worker.py", line 488, in &lt;module&gt;
   8     global_worker = Worker()
   9   File "/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/worker.py", line 118, in __init__
  10     self.original_gpu_ids = ray.utils.get_cuda_visible_devices()
  11   File "/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/utils.py", line 288, in get_cuda_visible_devices
  12     return [int(i) for i in gpu_ids_str.split(",")]
  13   File "/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/utils.py", line 288, in &lt;listcomp&gt;
  14     return [int(i) for i in gpu_ids_str.split(",")]
  15 srun: error: d-10-14-2: task 0: Exited with exit code 1
  16 ValueError: invalid literal for int() with base 10: 'GPU-77e3a4cd-22fb-5d31-b8a6-409df6133eeb'
  17 adding workers
  18 GPUS: GPU-77e3a4cd-22fb-5d31-b8a6-409df6133eeb,GPU-3de6a983-6da7-e1cf-ffc6-a285c71ff5d3
  19 Traceback (most recent call last):
  20   File "ray_test.py", line 5, in &lt;module&gt;
  21     import ray
  22   File "/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/__init__.py", line 65, in &lt;module&gt;
  23     from ray.worker import (
  24   File "/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/worker.py", line 488, in &lt;module&gt;
  25     global_worker = Worker()
  26   File "/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/worker.py", line 118, in __init__
  27     self.original_gpu_ids = ray.utils.get_cuda_visible_devices()
  28   File "/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/utils.py", line 288, in get_cuda_visible_devices
  29     return [int(i) for i in gpu_ids_str.split(",")]
  30   File "/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/utils.py", line 288, in &lt;listcomp&gt;
  31     return [int(i) for i in gpu_ids_str.split(",")]
  32 ValueError: invalid literal for int() with base 10: 'GPU-77e3a4cd-22fb-5d31-b8a6-409df6133eeb'
 &lt;/denchmark-code&gt;
 
 echo "GPUS: $CUDA_VISIBLE_DEVICES" gets me the following:
 GPUS: GPU-77e3a4cd-22fb-5d31-b8a6-409df6133eeb,GPU-3de6a983-6da7-e1cf-ffc6-a285c71ff5d3
 I am using Ray 0.8.4, and Python 3.6.10, on Ubuntu.
 &lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;
 
 Here are my sbatch shell script and python script. Note there are small deviations from this SLURM example in the docs as our SLURM system team initially got the example to work and I am simply using their modifications. The conda environment shouldn't be the bottleneck here.
 &lt;denchmark-code&gt;#!/bin/bash
 
 #SBATCH -o %j.log
 #SBATCH --job-name=ray_test
 #SBATCH -n 4
 #SBATCH -N 2
 #SBATCH --gres=gpu:volta:2
 
 export LC_ALL=C.UTF-8
 export LANG=C.UTF-8
 
 ((worker_num=$SLURM_NNODES-1)) # Must be one less that the total number of nodes
 echo $worker_num
 
 # Set up environment
 eval "$(conda shell.bash hook)"
 conda activate football2
 # or
 # conda activate /home/gridsan/groups/ERGO_GRF/football_env/football
 
 nodes=$(scontrol show hostnames $SLURM_JOB_NODELIST) # Getting the node names
 nodes_array=( $nodes )
 
 node1=${nodes_array[0]}
 
 # Set IP address/port of head node
 ip_prefix=$(srun --nodes=1 --ntasks=1 -w $node1 hostname --ip-address) # Making address
 port='6379'
 ip_head=$ip_prefix':'$port
 export ip_head # Exporting for latter access by trainer.py
 
 # Temporary directory for logging
 tmpdir='/state/partition1/user/'$USER'/raytmp'
 echo $tmpdir
 mkdircmd='mkdir -p '$tmpdir
 
 # Start the head
 srun --nodes=1 --ntasks=1 -w $node1 $mkdircmd
 srun --nodes=1 --ntasks=1 -w $node1 ray start --temp-dir=$tmpdir --block --head --redis-port=$port &amp;
 sleep 5
 
 # Start workers
 echo "adding workers"
 for ((  i=1; i&lt;=$worker_num; i++ ))
 do
     node2=${nodes_array[$i]}
     srun --nodes=1 --ntasks=1 -w $node2 mkdir -p $tmpdir
     srun --nodes=1 --ntasks=1 -w $node2 ray start --temp-dir=$tmpdir --block --address=$ip_head &amp; # Starting the workers
 done
 echo "GPUS: $CUDA_VISIBLE_DEVICES"
 python ray_test.py $SLURM_NTASKS # Pass the total number of allocated CPUs
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-code&gt;# trainer.py
 import os
 import sys
 import time
 import ray
 import torch 
 
 
 ray.init(address=os.environ["ip_head"])
 
 @ray.remote
 def f():
   time.sleep(1)
 
 # The following takes one second (assuming that ray was able to access all of the allocated nodes).
 start = time.time()
 num_cpus = int(sys.argv[1])
 ray.get([f.remote() for _ in range(num_cpus)])
 end = time.time()
 print(end - start)
 
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='yutaizhou' date='2020-05-04T19:54:30Z'>
 		Thanks for opening this issue &lt;denchmark-link:https://github.com/yutaizhou&gt;@yutaizhou&lt;/denchmark-link&gt;
  ! Is it possible to map the string back to the original device index (integers)?
 		</comment>
 		<comment id='2' author='yutaizhou' date='2020-05-05T00:35:44Z'>
 		&lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
  thanks for checking in! I think I could, since every node in our HPC system only has 2 GPUs anyway. I could just map the 2 GPUs as 0 and 1? Not sure if that would work across the entire system though. I will try it
 		</comment>
 		<comment id='3' author='yutaizhou' date='2020-05-07T17:12:09Z'>
 		&lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
  So I have tried remapping the GPU IDs but didn't get very far..
 
 After talking to the HPC team, here is my understanding of the status and some direct quotes from them:
 
 
 "We’re using NVIDIA’s UUID API to set the device names to the UUID, rather than the default. The problem with the default naming scheme (0,1,etc) is that it is not consistent. What’s listed as GPU 0 might change even within a job, which you can imagine would cause major problems if you have two people on a node, each allocated one GPU." -MITSC
 
 
 In a shared environment like a HPC system, UUID seems to be the best (only?) way to to make sure people only have access to the GPUs that is allocated to them. Apparently TF and PyTorch don't have a problem with that. The HPC team essentially recommends Ray to accept non-default GPU naming scheme.
 
 
 What do you think?
 		</comment>
 		<comment id='4' author='yutaizhou' date='2020-05-07T20:06:06Z'>
 		OK got it. Let me see if we can prio this. cc &lt;denchmark-link:https://github.com/anabranch&gt;@anabranch&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='5' author='yutaizhou' date='2020-05-08T15:04:20Z'>
 		Great, thank you Richard!
 		</comment>
 		<comment id='6' author='yutaizhou' date='2020-07-06T22:36:25Z'>
 		&lt;denchmark-link:https://github.com/yutaizhou&gt;@yutaizhou&lt;/denchmark-link&gt;
  Did you find a fix? I'm having the same issue on PBS.
 		</comment>
 		<comment id='7' author='yutaizhou' date='2020-07-09T13:47:25Z'>
 		&lt;denchmark-link:https://github.com/Graeme22&gt;@Graeme22&lt;/denchmark-link&gt;
  yes but it is very hacky, and I still believe this should be addressed upstream in Ray.
 When you do srun to spin up nodes, add an extra unset.sh file:
 &lt;denchmark-code&gt;srun --nodes=1 --ntasks=1 -w $node1 unset.sh &amp;&amp; ray start --temp-dir=$tmpdir --block --head --redis-port=$port &amp;
 &lt;/denchmark-code&gt;
 
 unset.sh
 &lt;denchmark-code&gt;unset CUDA_VISIBLE_DEVICES
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='8' author='yutaizhou' date='2020-07-10T21:23:20Z'>
 		For anyone having this issue, my quick fix was the following: Change line 288 of utils.py to this:
 &lt;denchmark-code&gt;return gpu_ids_str.split(",")
 &lt;/denchmark-code&gt;
 
 The string based IDs still work as expected, and avoid casting error when they're not numerical.
 		</comment>
 		<comment id='9' author='yutaizhou' date='2020-08-11T19:10:29Z'>
 		This should be fixed in &lt;denchmark-link:https://github.com/ray-project/ray/pull/9744&gt;#9744&lt;/denchmark-link&gt;
 ! Please ping (or open a new issue if not!)
 		</comment>
 	</comments>
 </bug>
<commit id='32cd94b7505bf163db83d86f06b86fec698361b1' author='yncxcw' date='2020-08-11 12:09:46-07:00'>
 	<dmm_unit complexity='0.08333333333333333' interfacing='1.0' size='0.08333333333333333'></dmm_unit>
 	<modification change_type='MODIFY' old_name='doc\source\actors.rst' new_name='doc\source\actors.rst'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>75,76</added_lines>
 			<deleted_lines>75,76</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='doc\source\using-ray-with-gpus.rst' new_name='doc\source\using-ray-with-gpus.rst'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>36,37</added_lines>
 			<deleted_lines>36,37</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\_raylet.pyx' new_name='python\ray\_raylet.pyx'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>356</added_lines>
 			<deleted_lines>356</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\tests\test_actor_resources.py' new_name='python\ray\tests\test_actor_resources.py'>
 		<file_info nloc='456' complexity='126' token_count='3541'></file_info>
 		<method name='test_actor_gpus.get_location_and_ids' parameters='self'>
 				<method_info nloc='4' complexity='1' token_count='37' nesting_level='2' start_line='100' end_line='103'></method_info>
 			<added_lines>101</added_lines>
 			<deleted_lines>101</deleted_lines>
 		</method>
 		<method name='test_actor_gpus' parameters='ray_start_cluster'>
 				<method_info nloc='25' complexity='7' token_count='194' nesting_level='0' start_line='86' end_line='122'></method_info>
 			<added_lines>98,101</added_lines>
 			<deleted_lines>98,101</deleted_lines>
 		</method>
 		<method name='test_actor_gpus.__init__' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='17' nesting_level='2' start_line='97' end_line='98'></method_info>
 			<added_lines>98</added_lines>
 			<deleted_lines>98</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\tests\test_advanced_2.py' new_name='python\ray\tests\test_advanced_2.py'>
 		<file_info nloc='542' complexity='148' token_count='4442'></file_info>
 		<method name='test_local_mode_gpus.f' parameters=''>
 				<method_info nloc='5' complexity='2' token_count='30' nesting_level='1' start_line='689' end_line='693'></method_info>
 			<added_lines>693</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_specific_gpus.g' parameters=''>
 				<method_info nloc='5' complexity='1' token_count='38' nesting_level='1' start_line='668' end_line='672'></method_info>
 			<added_lines>671,672</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_specific_gpus.f' parameters=''>
 				<method_info nloc='4' complexity='1' token_count='28' nesting_level='1' start_line='662' end_line='665'></method_info>
 			<added_lines>665</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_gpu_ids_as_str' parameters='save_gpu_ids_shutdown_only,as_str'>
 				<method_info nloc='8' complexity='3' token_count='67' nesting_level='0' start_line='637' end_line='652'></method_info>
 			<added_lines>637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652</added_lines>
 			<deleted_lines>646,652</deleted_lines>
 		</method>
 		<method name='test_gpu_ids_as_str.get_gpu_ids' parameters='as_str'>
 				<method_info nloc='7' complexity='3' token_count='37' nesting_level='1' start_line='644' end_line='650'></method_info>
 			<added_lines>644,645,646,647,648,649,650</added_lines>
 			<deleted_lines>646</deleted_lines>
 		</method>
 		<method name='test_local_mode_gpus' parameters='save_gpu_ids_shutdown_only'>
 				<method_info nloc='10' complexity='3' token_count='87' nesting_level='0' start_line='678' end_line='695'></method_info>
 			<added_lines>693</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_specific_gpus' parameters='save_gpu_ids_shutdown_only'>
 				<method_info nloc='11' complexity='4' token_count='104' nesting_level='0' start_line='655' end_line='675'></method_info>
 			<added_lines>665,671,672</added_lines>
 			<deleted_lines>674</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>636,653,654</added_lines>
 			<deleted_lines>653</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\utils.py' new_name='python\ray\utils.py'>
 		<file_info nloc='401' complexity='106' token_count='2504'></file_info>
 		<method name='get_cuda_visible_devices' parameters=''>
 				<method_info nloc='9' complexity='4' token_count='50' nesting_level='0' start_line='270' end_line='290'></method_info>
 			<added_lines>274,275,276,289,290</added_lines>
 			<deleted_lines>274,275,276,289</deleted_lines>
 		</method>
 		<method name='set_cuda_visible_devices' parameters='gpu_ids'>
 				<method_info nloc='6' complexity='3' token_count='39' nesting_level='0' start_line='296' end_line='308'></method_info>
 			<added_lines>300</added_lines>
 			<deleted_lines>299</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\worker.py' new_name='python\ray\worker.py'>
 		<file_info nloc='999' complexity='160' token_count='5322'></file_info>
 		<method name='get_gpu_ids' parameters=''>
 				<method_info nloc='13' complexity='5' token_count='83' nesting_level='0' start_line='376' end_line='405'></method_info>
 			<added_lines>376,384,385,386,387</added_lines>
 			<deleted_lines>376,403</deleted_lines>
 		</method>
 		<method name='get_gpu_ids' parameters='as_str'>
 				<method_info nloc='22' complexity='8' token_count='126' nesting_level='0' start_line='376' end_line='419'></method_info>
 			<added_lines>376,384,385,386,387,407,408,409,410,411,412,413,414,415,416,417</added_lines>
 			<deleted_lines>376,403</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\evaluation\rollout_worker.py' new_name='rllib\evaluation\rollout_worker.py'>
 		<file_info nloc='976' complexity='93' token_count='5019'></file_info>
 		<modified_lines>
 			<added_lines>416</added_lines>
 			<deleted_lines>416</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\policy\torch_policy.py' new_name='rllib\policy\torch_policy.py'>
 		<file_info nloc='483' complexity='43' token_count='3112'></file_info>
 		<modified_lines>
 			<added_lines>101</added_lines>
 			<deleted_lines>101</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
