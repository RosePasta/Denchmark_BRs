<bug_data>
<bug id='12233' author='tibogissel' open_date='2020-11-21T11:22:20Z' closed_time='2020-12-01T09:48:04Z'>
 	<summary>[rllib] Weight sharing example does not seem to actually share weights</summary>
 	<description>
 &lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;
 
 Versions: Python 3.6  | tf2.1 | rllib1.0.0
 Weight sharing in tensorflow as implemented in rllib/examples/models/shared_weights_model.py creates two different sets of weights when it is supposed to reuse part of them.
 &lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;import gym
 from ray.rllib.examples.models.shared_weights_model import SharedWeightsModel1, SharedWeightsModel2
 
 observation_space = gym.spaces.Box(low=-1, high=1, shape=(10,),)
 acition_space = gym.spaces.Discrete(3)
 
 m1 = SharedWeightsModel1(
     observation_space=observation_space,
     action_space=acition_space, 
     num_outputs=3,
     model_config={},
     name="m1"
 )
 m2 = SharedWeightsModel2(
     observation_space=observation_space,
     action_space=acition_space, 
     num_outputs=3,
     model_config={},
     name="m2"
 )
 
 v1 = m1.trainable_variables()
 v2 = m2.trainable_variables()
 for i in range(len(v1)):
     print(v1[i].name,v2[i].name)
 &lt;/denchmark-code&gt;
 
 Output
 &lt;denchmark-code&gt;fc1/kernel:0 fc1_1/kernel:0
 fc1/bias:0 fc1_1/bias:0
 fc_out/kernel:0 fc_out_1/kernel:0
 fc_out/bias:0 fc_out_1/bias:0
 value_out/kernel:0 value_out_1/kernel:0
 value_out/bias:0 value_out_1/bias:0
 &lt;/denchmark-code&gt;
 
 Weight for fc_1 should eb the same but apparently they are not. It seems that tf.keras.layer is not affected by the scope and the solution could be to create the layer outside and use it in both classes. This script seems to work:
 &lt;denchmark-code&gt;import gym
 import numpy as np
 
 from ray.rllib.models.modelv2 import ModelV2
 from ray.rllib.models.tf.tf_modelv2 import TFModelV2
 from ray.rllib.utils.annotations import override
 from ray.rllib.utils.framework import try_import_tf
 
 tf1, tf, tfv = try_import_tf()
 
 shared_last_layer = tf.keras.layers.Dense(units=64, activation=tf.nn.relu, name="fc1")
 
 class SharedWeightsModel1(TFModelV2):
     """Example of weight sharing between two different TFModelV2s.
     Here, we share the variables defined in the 'shared' variable scope
     by entering it explicitly with tf1.AUTO_REUSE. This creates the
     variables for the 'fc1' layer in a global scope called 'shared'
     (outside of the Policy's normal variable scope).
     """
 
     def __init__(self, observation_space, action_space, num_outputs,
                  model_config, name):
         super().__init__(observation_space, action_space, num_outputs,
                          model_config, name)
 
         inputs = tf.keras.layers.Input(observation_space.shape)
         last_layer = shared_last_layer(inputs)
         output = tf.keras.layers.Dense(
             units=num_outputs, activation=None, name="fc_out")(last_layer)
         vf = tf.keras.layers.Dense(
             units=1, activation=None, name="value_out")(last_layer)
         self.base_model = tf.keras.models.Model(inputs, [output, vf])
         self.register_variables(self.base_model.variables)
 
     @override(ModelV2)
     def forward(self, input_dict, state, seq_lens):
         out, self._value_out = self.base_model(input_dict["obs"])
         return out, []
 
     @override(ModelV2)
     def value_function(self):
         return tf.reshape(self._value_out, [-1])
 
 
 class SharedWeightsModel2(TFModelV2):
     """The "other" TFModelV2 using the same shared space as the one above."""
 
     def __init__(self, observation_space, action_space, num_outputs,
                  model_config, name):
         super().__init__(observation_space, action_space, num_outputs,
                          model_config, name)
 
         inputs = tf.keras.layers.Input(observation_space.shape)
 
         last_layer = shared_last_layer(inputs)
         output = tf.keras.layers.Dense(
             units=num_outputs, activation=None, name="fc_out")(last_layer)
         vf = tf.keras.layers.Dense(
             units=1, activation=None, name="value_out")(last_layer)
         self.base_model = tf.keras.models.Model(inputs, [output, vf])
         self.register_variables(self.base_model.variables)
 
     @override(ModelV2)
     def forward(self, input_dict, state, seq_lens):
         out, self._value_out = self.base_model(input_dict["obs"])
         return out, []
 
     @override(ModelV2)
     def value_function(self):
         return tf.reshape(self._value_out, [-1])
     
 observation_space = gym.spaces.Box(low=-1, high=1, shape=(10,),)
 acition_space = gym.spaces.Discrete(3)
 
 m1 = SharedWeightsModel1(
     observation_space=observation_space,
     action_space=acition_space, 
     num_outputs=3,
     model_config={},
     name="m1"
 )
 m2 = SharedWeightsModel2(
     observation_space=observation_space,
     action_space=acition_space, 
     num_outputs=3,
     model_config={},
     name="m2"
 )
 
 v1 = m1.trainable_variables()
 v2 = m2.trainable_variables()
 for i in range(len(v1)):
     print(v1[i].name,v2[i].name)
 &lt;/denchmark-code&gt;
 
 Which output is:
 &lt;denchmark-code&gt;fc1/kernel:0 fc1/kernel:0
 fc1/bias:0 fc1/bias:0
 fc_out/kernel:0 fc_out_1/kernel:0
 fc_out/bias:0 fc_out_1/bias:0
 value_out/kernel:0 value_out_1/kernel:0
 value_out/bias:0 value_out_1/bias:0
 &lt;/denchmark-code&gt;
 
 And we see that fc1 is well shared.
 
 [yes ] I have verified my script runs in a clean environment and reproduces the issue.
 [ yes] I have verified the issue also occurs with the latest wheels.
 
 	</description>
 	<comments>
 		<comment id='1' author='tibogissel' date='2020-11-24T01:19:23Z'>
 		cc &lt;denchmark-link:https://github.com/sven1977&gt;@sven1977&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='tibogissel' date='2020-11-25T11:39:36Z'>
 		Thanks for filing this issue &lt;denchmark-link:https://github.com/tibogissel&gt;@tibogissel&lt;/denchmark-link&gt;
  , and for providing a solution already! Will take a look now ...
 Yeah, I think this is also how we did this in torch (create a "global" layer outside and use that in the models).
 		</comment>
 		<comment id='3' author='tibogissel' date='2020-11-25T12:24:38Z'>
 		Yeah, this is a typical tf1 vs tf2 problem. In tf1, we need the scope (global sharing will complain about different graphs being used for the shared layer). tf2 does not respect these scipes anymore, hence the non-sharing that you observed.
 Solution: I'll provide a third model for tf2 to make this work and then - in the example - disseminate between the different frameworks used.
 		</comment>
 		<comment id='4' author='tibogissel' date='2020-11-25T12:26:11Z'>
 		Great! Thanks for taking the time to look at this!
 		</comment>
 		<comment id='5' author='tibogissel' date='2020-11-25T12:27:47Z'>
 		PR: &lt;denchmark-link:https://github.com/ray-project/ray/pull/12399&gt;#12399&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='6' author='tibogissel' date='2020-11-25T12:28:56Z'>
 		Np, :)
 		</comment>
 	</comments>
 </bug>
<commit id='841d93d366b1465959dba03512d64513c5ff568f' author='Sven Mika' date='2020-11-25 11:27:19-08:00'>
 	<dmm_unit complexity='1.0' interfacing='0.2857142857142857' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='rllib\BUILD' new_name='rllib\BUILD'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>1902</added_lines>
 			<deleted_lines>1902</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\examples\models\shared_weights_model.py' new_name='rllib\examples\models\shared_weights_model.py'>
 		<file_info nloc='149' complexity='12' token_count='1094'></file_info>
 		<method name='forward' parameters='self,input_dict,state,seq_lens'>
 				<method_info nloc='3' complexity='1' token_count='31' nesting_level='1' start_line='45' end_line='47'></method_info>
 			<added_lines>45,46,47</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,observation_space,action_space,num_outputs,model_config,name'>
 				<method_info nloc='2' complexity='1' token_count='15' nesting_level='1' start_line='30' end_line='31'></method_info>
 			<added_lines>30,31</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='value_function' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='19' nesting_level='1' start_line='50' end_line='51'></method_info>
 			<added_lines>50,51</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,32,33,34,35,36,37,38,39,40,41,42,43,44,48,49,52,57,58,59,131</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='rllib\examples\multi_agent_cartpole.py' new_name='rllib\examples\multi_agent_cartpole.py'>
 		<file_info nloc='86' complexity='1' token_count='534'></file_info>
 		<modified_lines>
 			<added_lines>21,22,39,40,48,49,50,51,52,53,54,93</added_lines>
 			<deleted_lines>21,38,46,47,86</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
