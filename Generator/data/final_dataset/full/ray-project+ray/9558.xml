<bug_data>
<bug id='9558' author='vish0910' open_date='2020-07-17T22:20:14Z' closed_time='2020-08-16T21:09:29Z'>
 	<summary>[tune] Ray Cluster deployed in Kubernetes unable to sync checkpoint directories to the driver node</summary>
 	<description>
 &lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;
 
 Getting the following error (Stacktrace attached):
 &lt;denchmark-code&gt;2020-07-17 21:53:48,101	ERROR trial_runner.py:550 -- Trial TrainExample_fd24b_00001: Error handling checkpoint /root/ray_results/TrainExample/TrainExample_1_randomforestclassifier__n_estimators=5_2020-07-17_21-53-462l3hkjfs/checkpoint_1/
 Traceback (most recent call last):
   File "/opt/conda/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 546, in _process_trial_save
     trial.on_checkpoint(trial.saving_to)
   File "/opt/conda/lib/python3.6/site-packages/ray/tune/trial.py", line 448, in on_checkpoint
     self, checkpoint.value))
 ray.tune.error.TuneError: Trial TrainExample_fd24b_00001: Checkpoint path /root/ray_results/TrainExample/TrainExample_1_randomforestclassifier__n_estimators=5_2020-07-17_21-53-462l3hkjfs/checkpoint_1/ not found after successful sync down.
 &lt;/denchmark-code&gt;
 
 This is output of ray_random_forest.py. From investigation it looks like the checkpoint directories ie. checkpoint_1, checkpoint_2 that were created in other nodes (workers) of the cluster do not get synced back to the driver node (from where I am running the python script).
 Rest of the files in the Trial directories seem to be in sync.
 This problem is not reproducible by running it on a single-machine with a cluster started up using bare init(). My guess is because all the workers point to the same file-system, and thus, the os.path.exist() returns True (This is reference to the line of code which checks and throws the above error)
 Taking a look at validate_save_restore() to validate a Trainable, I have written a bare-bone script which proves that checkpoint directories are not synced across workers.
 The stacktrace from checkpoint_validation_failed.py
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "code/checkpoint_validation_failed.py", line 115, in &lt;module&gt;
     _main()
   File "code/checkpoint_validation_failed.py", line 94, in _main
     print(f"VALIDATE TRAINABLE CLASS: {validate_save_restore(TrainExample)}")
   File "code/checkpoint_validation_failed.py", line 67, in validate_save_restore
     restore_check = ray.get(trainable_2.restore.remote(old_trainable))
   File "/opt/conda/lib/python3.6/site-packages/ray/worker.py", line 1526, in get
     raise value.as_instanceof_cause()
 ray.exceptions.RayTaskError(FileNotFoundError): ray::TrainExample.restore() (pid=2704, ip=172.17.0.8)
   File "python/ray/_raylet.pyx", line 471, in ray._raylet.execute_task
   File "python/ray/_raylet.pyx", line 424, in ray._raylet.execute_task.function_executor
   File "/opt/conda/lib/python3.6/site-packages/ray/tune/trainable.py", line 444, in restore
     with open(checkpoint_path + ".tune_metadata", "rb") as f:
 FileNotFoundError: [Errno 2] No such file or directory: '/root/ray_results/2020-07-17_21-43-17dbvo01tp/checkpoint_3/.tune_metadata'
 command terminated with exit code 1
 &lt;/denchmark-code&gt;
 
 Here, Worker X is trying to restore checkpoint created by Worker Y.
 Ray version and other system information (Python version, TensorFlow version, OS):
 Setup:
 
 Ray cluster running in K8s.
 Ray version
 pip list | grep ray ray 0.9.0.dev0
 Docker image:
 REPOSITORY                                                       TAG                 IMAGE ID            CREATED             SIZE rayproject/autoscaler                                            latest              3f8d747cb8af        7 days ago          5.25GB
 
 Playing with sync_to_driver, sync_on_checkpoint parameters did not help.
 Thanks!
 &lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;
 
 Step 1. Spin-up a Ray cluster using ray-cluster.yaml in a namespace called ray. ie
 
 kubectl create -f ray-namespace.yaml
 kubectl apply -f ray-cluster.yaml
 
 Step 2. Copy the python script to any worker node.
 
 kubectl -n ray get pods.
 Copy WORKER_1_POD_NAME
 Exec into pod kubectl -n ray exec -it &lt;WORKER_1_POD_NAME&gt; -- bin/bash
 mkdir code
 Copy the python script kubectl -n ray cp checkpoint_validation_failed.py  &lt;WORKER_1_POD_NAME&gt;:/code/checkpoint_validation_failed.py
 Exit pod
 Execute script kubectl -n ray exec &lt;WORKER_1_POD_NAME&gt; -- bin/bash -c "python code/checkpoint_validation_failed.py
 
 You can do the same steps to run .
 &lt;denchmark-link:https://github.com/ray-project/ray/files/4940566/ray_issue_related_files.zip&gt;ray_issue_related_files.zip&lt;/denchmark-link&gt;
 
 The above zip contains:
 
 ray-cluster.yaml
 ray-namespace.yaml
 checkpoint_validation_failed.py
 ray_random_forest.py
 
 If we cannot run your script, we cannot fix your issue.
 
  I have verified my script runs in a clean environment and reproduces the issue.
  I have verified the issue also occurs with the latest wheels.
 
 	</description>
 	<comments>
 		<comment id='1' author='vish0910' date='2020-07-17T22:55:35Z'>
 		Do you have an NFS for your kubernetes cluster? That might save you a lot of trouble.
 		</comment>
 		<comment id='2' author='vish0910' date='2020-07-17T23:00:44Z'>
 		Hello &lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
 ,
 Thank you for a quick reply.
 No, I do not have NFS for the cluster. Does it have to be a NFS? How does ray sync files like
 
 events.out.tfevents.1595022826.ray-worker-6d9f6b9b8d-g4zvk
 params.json
 params.pkl
 progress.csv
 result.json
 
 But not checkpoint_1, checkpoint_2 etc.?
 		</comment>
 		<comment id='3' author='vish0910' date='2020-07-17T23:03:00Z'>
 		The files are actually generated on the driver, not on the node. Do you have access to S3? S3 could work too.
 		</comment>
 		<comment id='4' author='vish0910' date='2020-07-17T23:38:23Z'>
 		Okay. So what gets synced on sync_on_checkpoint?
 sync_on_checkpoint (bool) â€“ Force sync-down of trial checkpoint to driver. If set to
 False, checkpoint syncing from worker to driver is asynchronous and best-effort. This does
 not affect persistent storage syncing. Defaults to True.
 In the above documentation snippet I am interpreting checkpoint as the checkpoint_N directories described above.
 Would the suggested S3 solution involve implementing DurableTrainable rather than Trainable?
 Also I wanted to clarify, is the above issue a bug in Ray Tune and a network/cloud storage a work-around for it?
 		</comment>
 		<comment id='5' author='vish0910' date='2020-07-19T01:39:27Z'>
 		Checkpoints (directories) are synced on sync_on_checkpoint, yep.
 The above is just a limitation of our file-transfer mechanism; we use rsync to move files between VMs, but I think that mechanism breaks down on kubernetes.
 The two approaches to workaround this limitation are to use 1. a networked storage or 2. to implement your own file-transfer protocol (check out sync_to_driver).
 Does that make sense? Let me know if you have any further questions!
 		</comment>
 		<comment id='6' author='vish0910' date='2020-07-29T16:10:38Z'>
 		Thank you Richard for this clarification.
 Upon searching more, I found this: &lt;denchmark-link:https://serverfault.com/questions/741670/rsync-files-to-a-kubernetes-pod&gt;https://serverfault.com/questions/741670/rsync-files-to-a-kubernetes-pod&lt;/denchmark-link&gt;
 
 I see similar solution applied to  in Ray. Maybe we can do the same for sync-checkpoint mechanism to resolve this issue. Or, we just implement it as a custom file-transfer protocol and make it part of the pallet.
 On a side note, I have taken your suggestion to use a NFS. It has unblocked me from aggregating all my checkpoints.
 Thanks once again.
 		</comment>
 		<comment id='7' author='vish0910' date='2020-07-29T20:45:16Z'>
 		Great! yeah, KubernetesCommandRunner is what we will want to use. More than happy to accept a PR if you want to take a quick pass!
 Otherwise, we'll try to get to this in the next couple of weeks.
 		</comment>
 		<comment id='8' author='vish0910' date='2020-07-30T15:08:08Z'>
 		I spent sometime last-week to understand if I can fix it, but for me it was not a going to be a quick pass. So I will defer it to the team. Thanks once again Richard. :)
 		</comment>
 	</comments>
 </bug>
<commit id='8f0f7371a055257b4c704bad49edccbfbfc01831' author='krfricke' date='2020-08-16 14:09:28-07:00'>
 	<dmm_unit complexity='0.9819277108433735' interfacing='0.6686746987951807' size='0.9156626506024096'></dmm_unit>
 	<modification change_type='MODIFY' old_name='doc\source\cluster\cloud.rst' new_name='doc\source\cluster\cloud.rst'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>181,182</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='doc\source\tune\user-guide.rst' new_name='doc\source\tune\user-guide.rst'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\command_runner.py' new_name='python\ray\autoscaler\command_runner.py'>
 		<file_info nloc='498' complexity='66' token_count='2911'></file_info>
 		<method name='run_rsync_down' parameters='self,source,target'>
 				<method_info nloc='15' complexity='3' token_count='86' nesting_level='1' start_line='229' end_line='244'></method_info>
 			<added_lines>244</added_lines>
 			<deleted_lines>238,239,240,241</deleted_lines>
 		</method>
 		<method name='run_rsync_up' parameters='self,source,target'>
 				<method_info nloc='15' complexity='3' token_count='86' nesting_level='1' start_line='203' end_line='218'></method_info>
 			<added_lines>218</added_lines>
 			<deleted_lines>218</deleted_lines>
 		</method>
 		<method name='run_cp_up' parameters='self,source,target'>
 				<method_info nloc='7' complexity='2' token_count='57' nesting_level='1' start_line='220' end_line='227'></method_info>
 			<added_lines>220,221,222,223,224,225,226,227</added_lines>
 			<deleted_lines>220,221</deleted_lines>
 		</method>
 		<method name='run_cp_down' parameters='self,source,target'>
 				<method_info nloc='7' complexity='2' token_count='57' nesting_level='1' start_line='246' end_line='253'></method_info>
 			<added_lines>246,247,248,249,250,251,252,253</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>219,245</added_lines>
 			<deleted_lines>219</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\kubernetes\kubectl-rsync.sh' new_name='python\ray\autoscaler\kubernetes\kubectl-rsync.sh'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>21,22</added_lines>
 			<deleted_lines>21</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\tune\BUILD' new_name='python\ray\tune\BUILD'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>96,97,98,99,100,101,102,103</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='python\ray\tune\integration\kubernetes.py'>
 		<file_info nloc='108' complexity='22' token_count='560'></file_info>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\tune\syncer.py' new_name='python\ray\tune\syncer.py'>
 		<file_info nloc='188' complexity='61' token_count='1202'></file_info>
 		<method name='get_node_syncer' parameters='local_dir,remote_dir,sync_function'>
 				<method_info nloc='20' complexity='9' token_count='134' nesting_level='0' start_line='276' end_line='307'></method_info>
 			<added_lines>291,292,293</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>6</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='python\ray\tune\tests\test_integration_kubernetes.py'>
 		<file_info nloc='109' complexity='13' token_count='849'></file_info>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\requirements_tune.txt' new_name='python\requirements_tune.txt'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>12</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
