<bug_data>
<bug id='12104' author='richardliaw' open_date='2020-11-18T09:03:20Z' closed_time='2020-12-29T02:56:29Z'>
 	<summary>[autoscaler] [docker] ray up on stopped node fails?</summary>
 	<description>
 &lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;
 
 I ran ray up on a cluster that I had previously stopped.
 The ray up was unable to finish properly due to a tmp call.
 cc &lt;denchmark-link:https://github.com/ijrsvt&gt;@ijrsvt&lt;/denchmark-link&gt;
 
 &lt;denchmark-code&gt;(base) ➜  aws git:(docker-fix) ✗ ray up example-full.yaml -y
 Cluster: default
 
 Loaded cached provider configuration
 If you experience issues with the cloud provider, try re-running the command with --no-config-cache.
 AWS config
   IAM Profile: ray-autoscaler-v1 [default]
   EC2 Key pair (head &amp; workers): ray-autoscaler_2_us-west-2 [default]
   VPC Subnets (head &amp; workers): subnet-d26e76b6, subnet-503c0e26 [default]
   EC2 Security groups (head &amp; workers): sg-0e2ad6debfc8e0814 [default]
   EC2 AMI (head &amp; workers): ami-0a2363a9cff180a64
 
 No head node found. Launching a new cluster. Confirm [y/N]: y [automatic, due to --yes]
 
 Acquiring an up-to-date head node
   Reusing nodes i-0c90dd53359e25223. To disable reuse, set `cache_stopped_nodes: False` under `provider` in the cluster configuration.
   Stopping instances to reuse
   Launched a new head node
   Fetching the new head node
 
 &lt;1/1&gt; Setting up head node
   Prepared bootstrap config
   New status: waiting-for-ssh
   [1/6] Waiting for SSH to become available
     Running `uptime` as a test.
     Waiting for IP
       Not yet available, retrying in 10 seconds
       Received: 35.165.135.138
 ssh: connect to host 35.165.135.138 port 22: Connection refused
     SSH still not available SSH command failed., retrying in 5 seconds.
 Warning: Permanently added '35.165.135.138' (ECDSA) to the list of known hosts.
  09:00:19 up 0 min,  1 user,  load average: 0.35, 0.08, 0.03
 Shared connection to 35.165.135.138 closed.
     Success.
 Shared connection to 35.165.135.138 closed.
 latest-gpu: Pulling from rayproject/ray
 Digest: sha256:9e330168fbeface86427d29b4a1a996bdefca42fff38ff155a29c8d1d1020b74
 Status: Image is up to date for rayproject/ray:latest-gpu
 docker.io/rayproject/ray:latest-gpu
 Shared connection to 35.165.135.138 closed.
 Shared connection to 35.165.135.138 closed.
 Shared connection to 35.165.135.138 closed.
 Shared connection to 35.165.135.138 closed.
 NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.
 
 Shared connection to 35.165.135.138 closed.
 2020-11-18 01:00:24,949	WARNING command_runner.py:780 -- Nvidia Container Runtime is present, but no GPUs found.
 Shared connection to 35.165.135.138 closed.
 9191520da0ddb9d6259795e5a758a298934bdab25ee153e60ddcd350318ed789
 Shared connection to 35.165.135.138 closed.
 Shared connection to 35.165.135.138 closed.
 lstat /tmp/ray_tmp_mount: no such file or directory
 Shared connection to 35.165.135.138 closed.
   New status: update-failed
   !!!
   SSH command failed.
   !!!
 
   Failed to setup head node.
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='richardliaw' date='2020-11-18T09:21:01Z'>
 		&lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
  can you rerun with something like 
 		</comment>
 		<comment id='2' author='richardliaw' date='2020-11-18T11:25:25Z'>
 		I get this error too &lt;denchmark-link:https://github.com/ijrsvt&gt;@ijrsvt&lt;/denchmark-link&gt;
  .
 		</comment>
 		<comment id='3' author='richardliaw' date='2020-11-20T23:58:09Z'>
 		&lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
  Follow up question, did you happen to edit your YAML between shutting down and restarting?
 		</comment>
 		<comment id='4' author='richardliaw' date='2020-11-21T00:11:39Z'>
 		hmmm maybe i modified the filemounts
 		</comment>
 		<comment id='5' author='richardliaw' date='2020-11-23T18:37:18Z'>
 		Same thing is happening for me. I did modify the setup_commands on the yaml between shutting down and restarting.
 		</comment>
 		<comment id='6' author='richardliaw' date='2020-12-07T19:09:03Z'>
 		+1
 		</comment>
 		<comment id='7' author='richardliaw' date='2020-12-14T19:30:51Z'>
 		&lt;denchmark-link:https://github.com/AmeerHajAli&gt;@AmeerHajAli&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/ijrsvt&gt;@ijrsvt&lt;/denchmark-link&gt;
  this sounds like a potential P0 issue, should we raise the priority?
 		</comment>
 		<comment id='8' author='richardliaw' date='2020-12-14T19:41:45Z'>
 		I'm having a related issue with DockerSyncer on sync_down:
 
 invalid output path: directory "/tmp/ray_tmp_mount/...
 
 Interestingly earlier in the logs I see the directory being created:
 
 VINFO command_runner.py:474 -- Running `^[[1mmkdir -p /tmp/ray_tmp_mount/...
 
 Unfortunately I cannot create an issue as my code is proprietary and I likely won't have time to build a minimal example, but the issues are related at least with respect to the directory in question. I'm looking over the framework code now.
 		</comment>
 		<comment id='9' author='richardliaw' date='2020-12-14T19:53:56Z'>
 		&lt;denchmark-link:https://github.com/AmeerHajAli&gt;@AmeerHajAli&lt;/denchmark-link&gt;
  can you please take a look at this ASAP? Not sure if it should be release blocking.
 		</comment>
 		<comment id='10' author='richardliaw' date='2020-12-15T12:22:16Z'>
 		I think this is a release blocker.
 Basically, it is straightforward to reproduce:
 
 ray up
 ray down
 ray up -&gt; fails because it can't locate /tmp/ray_tmp_mount
 @ijrsvt , I think the command runner is looking for the /tmp/ray_tmp_mount and fails because it assumes it is there in the cached node. But it seems like AWS cleans the /tmp directory on terminated nodes.
 
 		</comment>
 		<comment id='11' author='richardliaw' date='2020-12-15T23:19:35Z'>
 		&lt;denchmark-link:https://github.com/AmeerHajAli&gt;@AmeerHajAli&lt;/denchmark-link&gt;
  This is not a new regression and I think that rushing this fix as a cherry pick is more risky than waiting for this fix to go through nightlies.
 		</comment>
 		<comment id='12' author='richardliaw' date='2020-12-18T18:25:33Z'>
 		&lt;denchmark-link:https://github.com/ijrsvt&gt;@ijrsvt&lt;/denchmark-link&gt;
  , I think it is very common for someone to run  after a . I understand it is not a regression. But I am not sure what is the workaround here? The user will have to change the cluster name or work very hard to make it ray up again. If this is not a good reason for blocking a release then I am fine with it, but at least we should have some documentation somewhere telling the user what to do when he faces this issue.
 		</comment>
 		<comment id='13' author='richardliaw' date='2020-12-18T20:19:11Z'>
 		Hey, just to help clarify, I think there's two conversations happening -- a conversation around deadline and conversation around importance.
 &lt;denchmark-link:https://github.com/AmeerHajAli&gt;@AmeerHajAli&lt;/denchmark-link&gt;
  I think Ian and me and you absolutely agree that it is important to provide a workaround.
 I think Ian was just pushing back on the "prioritization/deadline" for this task. In terms of prioritization, I think it's ok to do it within the time frame of this sprint or even the sprint after (since everyone is going on holidays anyways).
 		</comment>
 		<comment id='14' author='richardliaw' date='2020-12-18T20:55:29Z'>
 		I definitely agree that this is important, I think a workaround should be to set the following in provider:
 &lt;denchmark-code&gt;cache_stopped_nodes: False
 &lt;/denchmark-code&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='7ad56826dbab8b718de47fbf2752ba0c81cbd843' author='Ian Rodney' date='2020-12-28 18:56:28-08:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\_private\command_runner.py' new_name='python\ray\autoscaler\_private\command_runner.py'>
 		<file_info nloc='682' complexity='109' token_count='4172'></file_info>
 		<method name='run_init' parameters='self,as_head,file_mounts'>
 				<method_info nloc='71' complexity='13' token_count='448' nesting_level='1' start_line='721' end_line='803'></method_info>
 			<added_lines>721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,785,786,787,788,789,790,791,792,793,794,795</added_lines>
 			<deleted_lines>721,743,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791</deleted_lines>
 		</method>
 		<method name='_check_if_container_restart_is_needed' parameters='self,str,str'>
 				<method_info nloc='2' complexity='1' token_count='20' nesting_level='1' start_line='721' end_line='722'></method_info>
 			<added_lines>721,722</added_lines>
 			<deleted_lines>721</deleted_lines>
 		</method>
 		<method name='run_init' parameters='self,as_head,file_mounts,sync_run_yet'>
 				<method_info nloc='55' complexity='12' token_count='363' nesting_level='1' start_line='763' end_line='833'></method_info>
 			<added_lines>763,785,786,787,788,789,790,791,792,793,794,795,815,820,821,822,823,824,833</added_lines>
 			<deleted_lines>763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\_private\updater.py' new_name='python\ray\autoscaler\_private\updater.py'>
 		<file_info nloc='381' complexity='54' token_count='2172'></file_info>
 		<method name='do_update' parameters='self'>
 				<method_info nloc='140' complexity='21' token_count='788' nesting_level='1' start_line='279' end_line='452'></method_info>
 			<added_lines>295,296,297,298,299,300,378,379</added_lines>
 			<deleted_lines>295,296,374</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\autoscaler\command_runner.py' new_name='python\ray\autoscaler\command_runner.py'>
 		<file_info nloc='83' complexity='5' token_count='204'></file_info>
 		<method name='run_init' parameters='self,bool,str,bool'>
 				<method_info nloc='2' complexity='1' token_count='29' nesting_level='1' start_line='78' end_line='79'></method_info>
 			<added_lines>78,79</added_lines>
 			<deleted_lines>78</deleted_lines>
 		</method>
 		<method name='run_init' parameters='self,bool,str'>
 				<method_info nloc='8' complexity='1' token_count='24' nesting_level='1' start_line='78' end_line='85'></method_info>
 			<added_lines>78,79,85</added_lines>
 			<deleted_lines>78</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>86,87,88</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\ray\tests\test_autoscaler.py' new_name='python\ray\tests\test_autoscaler.py'>
 		<file_info nloc='1694' complexity='173' token_count='10843'></file_info>
 		<method name='testSetupCommandsWithStoppedNodeCachingNoDocker' parameters='self'>
 				<method_info nloc='59' complexity='2' token_count='393' nesting_level='1' start_line='1577' end_line='1643'></method_info>
 			<added_lines>1577,1580,1637,1638,1639,1640,1641,1642,1643</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='testContinuousFileMounts' parameters='self'>
 				<method_info nloc='52' complexity='7' token_count='350' nesting_level='1' start_line='1766' end_line='1823'></method_info>
 			<added_lines>1778,1779,1808,1809</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='testGetOrCreateHeadNodeFromStopped' parameters='self'>
 				<method_info nloc='56' complexity='14' token_count='369' nesting_level='1' start_line='433' end_line='497'></method_info>
 			<added_lines>433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='testDockerFileMountsRemoved' parameters='self'>
 				<method_info nloc='45' complexity='1' token_count='259' nesting_level='1' start_line='548' end_line='595'></method_info>
 			<added_lines>548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='testGetOrCreateHeadNode' parameters='self'>
 				<method_info nloc='34' complexity='1' token_count='200' nesting_level='1' start_line='395' end_line='430'></method_info>
 			<added_lines>400,402</added_lines>
 			<deleted_lines>399,401</deleted_lines>
 		</method>
 		<method name='testMultiNodeReuse' parameters='self'>
 				<method_info nloc='42' complexity='3' token_count='290' nesting_level='1' start_line='1718' end_line='1763'></method_info>
 			<added_lines>1720,1721</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='testDockerFileMountsAdded' parameters='self'>
 				<method_info nloc='45' complexity='1' token_count='262' nesting_level='1' start_line='500' end_line='546'></method_info>
 			<added_lines>500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='testSetupCommandsWithStoppedNodeCaching' parameters='self'>
 				<method_info nloc='61' complexity='2' token_count='412' nesting_level='1' start_line='1411' end_line='1479'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>1411,1439,1454</deleted_lines>
 		</method>
 		<method name='testSetupCommandsWithStoppedNodeCachingDocker' parameters='self'>
 				<method_info nloc='61' complexity='2' token_count='412' nesting_level='1' start_line='1645' end_line='1716'></method_info>
 			<added_lines>1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1,432,498,499,547,596,1644</added_lines>
 			<deleted_lines>1488</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
