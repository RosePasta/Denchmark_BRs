<bug_data>
<bug id='4693' author='jacooba' open_date='2019-04-24T15:35:43Z' closed_time='2019-05-08T21:07:30Z'>
 	<summary>Qmix Bug with Truncated Episodes or when max_seq_len is set</summary>
 	<description>
 &lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;
 
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04
 Ray installed from (source or binary): source
 Ray version:  0.6.2
 Python version:  3.5.5
 Exact command to reproduce: (running on custom env)
 
 &lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;
 
 Qmix is set up to add padded q-values of -9999999 to the end of every sequence. These q-values are not being correctly masked out. They are ignored in terminal states, due to line 108 in qmix_policy_graph.py, however they are being used in non-terminal states that happen to be at the end of a sequence. This is causing astronomical TD-errors on my custom env with 1 agent.
 The issue is also a bit more complicated than just masking them out, as the code tries to do: There does need to be a next state (observation) to complete the bootstrapped return. If it is simply masked out, then certain states will never receive a back propagated loss.
 I would suggest also passing in next_states to the loss function (new_obs in samples dictionary I believe). If terminal states are not recorded with a next observation, then one can be added as padding, since it will be 0-ed out by (1 - terminated) in line 108 anyway.
 This issue can be verified by truncating episodes and noting that reward + self.gamma*-9999999 is contained in the variable masked_td_error.
 &lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='jacooba' date='2019-04-25T00:31:18Z'>
 		To clarify, this only happens if batch mode is set to truncate episodes?
 		</comment>
 		<comment id='2' author='jacooba' date='2019-04-25T09:22:14Z'>
 		I assume it would also be an issue with complete episodes but with a max_seq_len less than the length of the episode. It seems to be an issue if there is a non-terminal state at the end of a sequence.
 		</comment>
 		<comment id='3' author='jacooba' date='2019-04-25T09:41:29Z'>
 		I have attached code that I believe reproduces the same bug. The commands are:
 python3 twostep_game_no_bug.py --run QMIX
 python3 twostep_game_bug.py --run QMIX
 the variable masked_td_error seems to still contain the dummy constant in the bug version.
 &lt;denchmark-link:https://github.com/ray-project/ray/files/3116258/Qmix.bug.zip&gt;Qmix bug.zip&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='4' author='jacooba' date='2019-04-25T20:58:31Z'>
 		Hm I see. The proposed fix makes sense to me, and it also seems ok to potentially ignore the last state when truncating. Do you want to make a patch?
 		</comment>
 		<comment id='5' author='jacooba' date='2019-04-26T08:46:20Z'>
 		Sure. I can't do it this second, but should have some time next week.
 		</comment>
 	</comments>
 </bug>
<commit id='28496c8b50a576dc853e74574bc12ed2b4d9a56d' author='Jacob Beck' date='2019-05-08 14:07:29-07:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='python\ray\rllib\agents\qmix\qmix_policy_graph.py' new_name='python\ray\rllib\agents\qmix\qmix_policy_graph.py'>
 		<file_info nloc='334' complexity='44' token_count='2793'></file_info>
 		<method name='forward' parameters='self,rewards,actions,terminated,mask,obs,action_mask'>
 				<method_info nloc='37' complexity='7' token_count='419' nesting_level='1' start_line='49' end_line='122'></method_info>
 			<added_lines>49,50,54,55,56,57,59,61,74,76,85,86,88,91,92,97,98,99,100,106,107,108,109,114,115</added_lines>
 			<deleted_lines>49,53,54,55,56,71,73,82,84,85,86,87,90,95,96,106,107</deleted_lines>
 		</method>
 		<method name='learn_on_batch' parameters='self,samples'>
 				<method_info nloc='54' complexity='3' token_count='532' nesting_level='1' start_line='247' end_line='315'></method_info>
 			<added_lines>250,251,255,256,261,262,263,268,269,275,276,280,281,282,286,287,288,291,295,296</added_lines>
 			<deleted_lines>250,251,256,257,258,259,260,266,267,274,277,278,279,283</deleted_lines>
 		</method>
 		<method name='forward' parameters='self,rewards,actions,terminated,mask,obs,next_obs,action_mask,next_action_mask'>
 				<method_info nloc='2' complexity='1' token_count='21' nesting_level='1' start_line='49' end_line='50'></method_info>
 			<added_lines>49,50</added_lines>
 			<deleted_lines>49</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>245</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
