<bug_data>
<bug id='9429' author='typhoonzero' open_date='2018-03-28T05:58:31Z' closed_time='2018-03-30T04:00:19Z'>
 	<summary>using learning rate decay in optimizer in dist train cause run sub program error enforce  framework::product(lr_dims) == 1 failed, 0 != 1</summary>
 	<description>
 &lt;denchmark-code&gt;E0328 03:54:46.540556  3693 listen_and_serv_op.cc:152] run sub program error enforce framework::product(lr_dims) == 1 failed, 0 != 1
 Learning rate should have 1 element at [/paddle/paddle/fluid/operators/sgd_op.cc:36]
 PaddlePaddle Call Stacks:
 0       0x7f6a2a6ddf1cp paddle::platform::EnforceNotMet::EnforceNotMet(std::__exception_ptr::exception_ptr, char const*, int) + 572
 1       0x7f6a2ae144eap paddle::operators::SGDOp::InferShape(paddle::framework::InferShapeContext*) const + 970
 2       0x7f6a2aed7358p paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&amp;, boost::variant&lt;paddle::platform::CUDAPlace, paddle::platform::CPUPlace, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_&gt; const&amp;) const + 104
 3       0x7f6a2aed4cd8p paddle::framework::OperatorBase::Run(paddle::framework::Scope const&amp;, boost::variant&lt;paddle::platform::CUDAPlace, paddle::platform::CPUPlace, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_&gt; const&amp;) + 72
 4       0x7f6a2a78ff66p paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool) + 1750
 5       0x7f6a2a790e77p paddle::framework::Executor::Run(paddle::framework::ProgramDesc const&amp;, paddle::framework::Scope*, int, bool, bool) + 103
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='typhoonzero' date='2018-03-28T12:04:58Z'>
 		I'm checking it!
 &lt;denchmark-code&gt;15 E0328 09:39:58.410671 10820 listen_and_serv_op.cc:152] run sub program error enforce param_dims == ctx-&gt;GetInputDim("Grad") failed, 64 != 0
     16 Param and Grad input of AdamOp should have same dimension at [/paddle/paddle/fluid/operators/adam_op.cc:60]
     17 PaddlePaddle Call Stacks:
     18 0       0x7fc658bf56dcp paddle::platform::EnforceNotMet::EnforceNotMet(std::__exception_ptr::exception_ptr, char const*, int) + 572
     19 1       0x7fc65945af8ep paddle::operators::AdamOp::InferShape(paddle::framework::InferShapeContext*) const + 3214
     20 2       0x7fc65995e385p paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&amp;, boost::variant&lt;paddle::platform::CUDAPlace, paddle::platform::       CPUPlace, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, b       oost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detai       l::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant:       :void_, boost::detail::variant::void_, boost::detail::variant::void_&gt; const&amp;) const + 2213
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='2' author='typhoonzero' date='2018-03-28T12:35:18Z'>
 		Thank you &lt;denchmark-link:https://github.com/gongweibao&gt;@gongweibao&lt;/denchmark-link&gt;
  ,I'm on this work : )
 		</comment>
 	</comments>
 </bug>
<commit id='374f1ca3b76f5ed6d6f5a7e5367840663913014c' author='Yancey' date='2018-03-30 12:00:18+08:00'>
 	<dmm_unit complexity='0.4358974358974359' interfacing='0.5641025641025641' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='paddle\fluid\operators\listen_and_serv_op.cc' new_name='paddle\fluid\operators\listen_and_serv_op.cc'>
 		<file_info nloc='175' complexity='24' token_count='1155'></file_info>
 		<method name='paddle::operators::ListenAndServOp::RunImpl' parameters='scope,dev_place'>
 				<method_info nloc='74' complexity='12' token_count='554' nesting_level='3' start_line='95' end_line='192'></method_info>
 			<added_lines>159,160,161,162,163,165,166,167,168,169,170,171,173,175,176</added_lines>
 			<deleted_lines>138,139,143,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,181,182,183,184</deleted_lines>
 		</method>
 		<method name='paddle::operators::ParallelExecuteBlocks' parameters='parallel_blkids,executor,program,scope'>
 				<method_info nloc='17' complexity='4' token_count='157' nesting_level='2' start_line='57' end_line='73'></method_info>
 			<added_lines>57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>74</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\paddle\fluid\distribute_transpiler.py' new_name='python\paddle\fluid\distribute_transpiler.py'>
 		<file_info nloc='579' complexity='113' token_count='3925'></file_info>
 		<method name='_get_lr_ops' parameters='self'>
 				<method_info nloc='22' complexity='14' token_count='170' nesting_level='1' start_line='799' end_line='827'></method_info>
 			<added_lines>799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='get_pserver_program' parameters='self,endpoint'>
 				<method_info nloc='63' complexity='20' token_count='441' nesting_level='1' start_line='258' end_line='383'></method_info>
 			<added_lines>341,342,343,344,345,346,347,348,349,351,358</added_lines>
 			<deleted_lines>342,349</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>798</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
