<bug_data>
<bug id='3259' author='chris922' open_date='2019-05-14T15:58:47Z' closed_time='2019-05-17T18:28:25Z'>
 	<summary>ambassador and vizier-core not starting</summary>
 	<description>
 I am using current master (commit &lt;denchmark-link:https://github.com/kubeflow/kubeflow/commit/58022fbe26d9fc70a271519c8ae0fbe0f860eb57&gt;58022fb&lt;/denchmark-link&gt;
 ) and installed kubeflow on a previously created AWS EKS cluster (thus not using  provided by kubeflow). (3x t3.large nodes, tested in regions  and )
 IAM roles for ALB are in place as well.
 Nevertheless I am not able to get the ambassador and vizier-core pods running.
 Just a bunch of responses for various commands:
 Running pods (ns: kubeflow)
 &lt;denchmark-code&gt;$ kubectl -nkubeflow get po
 NAME                                                        READY     STATUS             RESTARTS   AGE
 alb-ingress-controller-596688f5bd-9ls7c                     2/2       Running            1          46m
 ambassador-796bcd74b5-4p949                                 1/2       CrashLoopBackOff   13         46m
 ambassador-796bcd74b5-gb4pl                                 1/2       CrashLoopBackOff   13         46m
 ambassador-796bcd74b5-rxvvb                                 1/2       CrashLoopBackOff   9          46m
 app-controller-c6d849967-rmc6q                              2/2       Running            0          46m
 argo-ui-bd754894c-twrdk                                     2/2       Running            0          46m
 centraldashboard-799849c495-qtxzk                           2/2       Running            0          46m
 jupyter-0                                                   2/2       Running            0          45m
 jupyter-web-app-56b6bf5bdf-mrt9x                            2/2       Running            0          46m
 katib-ui-7f49d9d8c6-4qdkc                                   2/2       Running            0          46m
 metacontroller-0                                            1/2       Running            0          45m
 minio-5955bbd58c-w6kmk                                      2/2       Running            0          46m
 ml-pipeline-7c697c9ffd-mcbzj                                2/2       Running            6          46m
 ml-pipeline-persistenceagent-7bffd45959-7wwfr               2/2       Running            0          46m
 ml-pipeline-scheduledworkflow-68f4b77777-gdjxx              2/2       Running            0          46m
 ml-pipeline-ui-5c947b8c57-x9gxk                             2/2       Running            0          46m
 ml-pipeline-viewer-controller-deployment-6b77d4c8cf-bmwgv   2/2       Running            1          46m
 mysql-854c579944-7z9qp                                      2/2       Running            0          46m
 notebooks-controller-68f9578d4c-js8jq                       2/2       Running            1          46m
 profiles-7db5484769-trbg4                                   2/2       Running            1          46m
 pytorch-operator-dc7b79d5c-gdk2h                            2/2       Running            0          46m
 spartakus-volunteer-95694bbf-r7q6b                          2/2       Running            0          46m
 studyjob-controller-54cdf77cc9-j7bb6                        2/2       Running            1          46m
 tf-job-dashboard-76558cbb57-6sgdd                           2/2       Running            0          45m
 tf-job-operator-757658b5cd-9h9qj                            2/2       Running            0          45m
 vizier-core-744646bc44-77hhq                                1/2       CrashLoopBackOff   7          13m
 vizier-core-rest-5d86bbf7bd-7hqts                           2/2       Running            0          45m
 vizier-db-6586c85f4d-m4krn                                  2/2       Running            0          45m
 vizier-suggestion-bayesianoptimization-5c4dc96968-vk28b     2/2       Running            0          45m
 vizier-suggestion-grid-6ddf58868-9vbfw                      2/2       Running            0          45m
 vizier-suggestion-hyperband-56f9689bbb-xthhj                2/2       Running            0          45m
 vizier-suggestion-random-674d55dc56-t5mmn                   2/2       Running            0          45m
 workflow-controller-694c8b98c4-mltrs                        2/2       Running            1          45m
 &lt;/denchmark-code&gt;
 
 Running pods (ns: istio-system)
 &lt;denchmark-code&gt;$ kubectl -nistio-system get po
 NAME                                                           READY     STATUS                       RESTARTS   AGE
 grafana-5c45779547-nhl4x                                       1/1       Running                      0          36m
 istio-citadel-6d7f8c5d-wzb9c                                   1/1       Running                      0          36m
 istio-cleanup-secrets-release-1.1-20190111-09-15-hh4ps         0/1       Completed                    0          36m
 istio-egressgateway-6f8b444984-p5mt5                           1/1       Running                      0          36m
 istio-galley-685ffb6bf4-4pmcc                                  1/1       Running                      0          36m
 istio-grafana-post-install-release-1.1-20190111-09-15-tnqs5    0/1       Completed                    0          36m
 istio-ingressgateway-7f7878c979-hv256                          1/1       Running                      0          36m
 istio-pilot-5d69c5d77c-lwcgr                                   2/2       Running                      0          36m
 istio-policy-59467fccc9-jdqkr                                  2/2       Running                      2          36m
 istio-security-post-install-release-1.1-20190111-09-15-l85hl   0/1       Completed                    0          36m
 istio-sidecar-injector-57d76787db-rqblh                        1/1       Running                      0          36m
 istio-telemetry-7c84b58bb5-ts24d                               2/2       Running                      2          36m
 istio-tracing-5445d89986-r5l95                                 1/1       Running                      0          36m
 kiali-69d7f779c8-rtmdg                                         0/1       CreateContainerConfigError   0          36m
 prometheus-5467b4b887-bbwlh                                    1/1       Running                      0          36m
 servicegraph-6ddb7d7d7-pccpp                                   1/1       Running                      0          36m
 &lt;/denchmark-code&gt;
 
 (kiali not starting due to missing kiali-secret, but I doubt this is a problem)
 Describe vizier-core pod
 &lt;denchmark-code&gt;$ kubectl -nkubeflow describe po vizier-core-744646bc44-77hhq
 Name:               vizier-core-744646bc44-77hhq
 Namespace:          kubeflow
 Priority:           0
 PriorityClassName:  &lt;none&gt;
 Node:               ip-10-0-0-82.eu-west-1.compute.internal/10.0.0.82
 Start Time:         Tue, 14 May 2019 16:44:45 +0200
 Labels:             app=vizier
                     app.kubernetes.io/name=app
                     component=core
                     pod-template-hash=744646bc44
 Annotations:        sidecar.istio.io/status={"version":"13b1f7694262f17b8ed7fc57080d5da3c9280d53d717f3290ef10d460f335d2b","initContainers":["istio-init"],"containers":["istio-proxy"],"volumes":["istio-envoy","istio-certs...
 Status:             Running
 IP:                 10.0.0.178
 Controlled By:      ReplicaSet/vizier-core-744646bc44
 Init Containers:
   istio-init:
     Container ID:  docker://7c145dfcd34aeb2a0822b0893b48cc8871cad1556cdc2256e75e39f92551b260
     Image:         gcr.io/istio-release/proxy_init:release-1.1-20190111-09-15
     Image ID:      docker-pullable://gcr.io/istio-release/proxy_init@sha256:e661adbfb76a29a8b41ebe166eb5d4fd927d3fe5bbc7b62907ab4ae56407c208
     Port:          &lt;none&gt;
     Host Port:     &lt;none&gt;
     Args:
       -p
       15001
       -u
       1337
       -m
       REDIRECT
       -i
       *
       -x
 
       -b
       6789
       -d
       15020
     State:          Terminated
       Reason:       Completed
       Exit Code:    0
       Started:      Tue, 14 May 2019 16:44:47 +0200
       Finished:     Tue, 14 May 2019 16:44:54 +0200
     Ready:          True
     Restart Count:  0
     Limits:
       cpu:     10m
       memory:  10Mi
     Requests:
       cpu:        10m
       memory:     10Mi
     Environment:  &lt;none&gt;
     Mounts:       &lt;none&gt;
 Containers:
   vizier-core:
     Container ID:  docker://1e7f08118a6e07b929254c86e0f5cba8c47cb42c8bf06923662d5e9b168ef6d5
     Image:         gcr.io/kubeflow-images-public/katib/vizier-core:v0.1.2-alpha-156-g4ab3dbd
     Image ID:      docker-pullable://gcr.io/kubeflow-images-public/katib/vizier-core@sha256:12d0a1196f49f4d492f01ec8028ee400d0be4f3b083787b04c2232dc2e28af2c
     Port:          6789/TCP
     Host Port:     0/TCP
     Command:
       ./vizier-manager
     State:          Waiting
       Reason:       CrashLoopBackOff
     Last State:     Terminated
       Reason:       Error
       Exit Code:    2
       Started:      Tue, 14 May 2019 16:53:48 +0200
       Finished:     Tue, 14 May 2019 16:54:23 +0200
     Ready:          False
     Restart Count:  7
     Liveness:       exec [/bin/grpc_health_probe -addr=:6789] delay=10s timeout=1s period=10s #success=1 #failure=3
     Readiness:      exec [/bin/grpc_health_probe -addr=:6789] delay=5s timeout=1s period=10s #success=1 #failure=3
     Environment:
       MYSQL_ROOT_PASSWORD:  &lt;set to the key 'MYSQL_ROOT_PASSWORD' in secret 'vizier-db-secrets'&gt;  Optional: false
     Mounts:
       /var/run/secrets/kubernetes.io/serviceaccount from default-token-tjmzq (ro)
   istio-proxy:
     Container ID:  docker://5f0ecac6df0a8f75e096ba29d3f8de325f5e98138aad00d6dd8cbc843214e17e
     Image:         gcr.io/istio-release/proxyv2:release-1.1-20190111-09-15
     Image ID:      docker-pullable://gcr.io/istio-release/proxyv2@sha256:325ee8ba87ce70b13dfbc9cdd0358989911f7e517ae37ba391b2c237489c973d
     Port:          15090/TCP
     Host Port:     0/TCP
     Args:
       proxy
       sidecar
       --configPath
       /etc/istio/proxy
       --binaryPath
       /usr/local/bin/envoy
       --serviceCluster
       vizier.kubeflow
       --drainDuration
       45s
       --parentShutdownDuration
       1m0s
       --discoveryAddress
       istio-pilot.istio-system:15010
       --zipkinAddress
       zipkin.istio-system:9411
       --connectTimeout
       10s
       --proxyAdminPort
       15000
       --controlPlaneAuthPolicy
       NONE
       --statusPort
       15020
       --applicationPorts
       6789
     State:          Running
       Started:      Tue, 14 May 2019 16:44:55 +0200
     Ready:          True
     Restart Count:  0
     Requests:
       cpu:      10m
     Readiness:  http-get http://:15020/healthz/ready delay=1s timeout=1s period=2s #success=1 #failure=30
     Environment:
       POD_NAME:                      vizier-core-744646bc44-77hhq (v1:metadata.name)
       POD_NAMESPACE:                 kubeflow (v1:metadata.namespace)
       INSTANCE_IP:                    (v1:status.podIP)
       ISTIO_META_POD_NAME:           vizier-core-744646bc44-77hhq (v1:metadata.name)
       ISTIO_META_CONFIG_NAMESPACE:   kubeflow (v1:metadata.namespace)
       ISTIO_META_INTERCEPTION_MODE:  REDIRECT
       ISTIO_METAJSON_LABELS:         {"app":"vizier","app.kubernetes.io/name":"app","component":"core","pod-template-hash":"744646bc44"}
 
     Mounts:
       /etc/certs/ from istio-certs (ro)
       /etc/istio/proxy from istio-envoy (rw)
       /var/run/secrets/kubernetes.io/serviceaccount from default-token-tjmzq (ro)
 Conditions:
   Type              Status
   Initialized       True
   Ready             False
   ContainersReady   False
   PodScheduled      True
 Volumes:
   default-token-tjmzq:
     Type:        Secret (a volume populated by a Secret)
     SecretName:  default-token-tjmzq
     Optional:    false
   istio-envoy:
     Type:    EmptyDir (a temporary directory that shares a pod's lifetime)
     Medium:  Memory
   istio-certs:
     Type:        Secret (a volume populated by a Secret)
     SecretName:  istio.default
     Optional:    true
 QoS Class:       Burstable
 Node-Selectors:  &lt;none&gt;
 Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                  node.kubernetes.io/unreachable:NoExecute for 300s
 Events:
   Type     Reason     Age                From                                              Message
   ----     ------     ----               ----                                              -------
   Normal   Scheduled  13m                default-scheduler                                 Successfully assigned kubeflow/vizier-core-744646bc44-77hhq to ip-10-0-0-82.eu-west-1.compute.internal
   Normal   Pulled     13m                kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Container image "gcr.io/istio-release/proxy_init:release-1.1-20190111-09-15" already present on machine
   Normal   Created    13m                kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Created container
   Normal   Started    13m                kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Started container
   Normal   Created    13m                kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Created container
   Normal   Pulled     13m                kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Container image "gcr.io/istio-release/proxyv2:release-1.1-20190111-09-15" already present on machine
   Normal   Started    13m                kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Started container
   Normal   Killing    12m                kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Killing container with id docker://vizier-core:Container failed liveness probe.. Container will be killed and recreated.
   Normal   Created    12m (x2 over 13m)  kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Created container
   Normal   Pulled     12m (x2 over 13m)  kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Container image "gcr.io/kubeflow-images-public/katib/vizier-core:v0.1.2-alpha-156-g4ab3dbd" already present on machine
   Normal   Started    12m (x2 over 13m)  kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Started container
   Warning  Unhealthy  12m (x7 over 13m)  kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Readiness probe failed: timeout: failed to connect service ":6789" within 1s
   Warning  Unhealthy  12m (x5 over 13m)  kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Liveness probe failed: timeout: failed to connect service ":6789" within 1s
   Warning  BackOff    3m (x26 over 9m)   kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Back-off restarting failed container
 &lt;/denchmark-code&gt;
 
 Logs vizier-core
 &lt;denchmark-code&gt;$ kubectl -nkubeflow logs vizier-core-744646bc44-77hhq vizier-core                                                                                                                                                 
 [mysql] 2019/05/14 14:45:54 packets.go:36: unexpected EOF
 &lt;/denchmark-code&gt;
 
 Describe ambassador pod
 &lt;denchmark-code&gt;$ kubectl -nkubeflow describe po ambassador-796bcd74b5-4p949
 Name:               ambassador-796bcd74b5-4p949
 Namespace:          kubeflow
 Priority:           0
 PriorityClassName:  &lt;none&gt;
 Node:               ip-10-0-0-82.eu-west-1.compute.internal/10.0.0.82
 Start Time:         Tue, 14 May 2019 16:11:53 +0200
 Labels:             app.kubernetes.io/name=app
                     pod-template-hash=796bcd74b5
                     service=ambassador
 Annotations:        sidecar.istio.io/status={"version":"13b1f7694262f17b8ed7fc57080d5da3c9280d53d717f3290ef10d460f335d2b","initContainers":["istio-init"],"containers":["istio-proxy"],"volumes":["istio-envoy","istio-certs...
 Status:             Running
 IP:                 10.0.0.206
 Controlled By:      ReplicaSet/ambassador-796bcd74b5
 Init Containers:
   istio-init:
     Container ID:  docker://00d470ba3f099417a21a1660045f4a7b083c0389c76e0a9c87bd22e7475b6170
     Image:         gcr.io/istio-release/proxy_init:release-1.1-20190111-09-15
     Image ID:      docker-pullable://gcr.io/istio-release/proxy_init@sha256:e661adbfb76a29a8b41ebe166eb5d4fd927d3fe5bbc7b62907ab4ae56407c208
     Port:          &lt;none&gt;
     Host Port:     &lt;none&gt;
     Args:
       -p
       15001
       -u
       1337
       -m
       REDIRECT
       -i
       *
       -x
 
       -b
 
       -d
       15020
     State:          Terminated
       Reason:       Completed
       Exit Code:    0
       Started:      Tue, 14 May 2019 16:14:51 +0200
       Finished:     Tue, 14 May 2019 16:14:57 +0200
     Ready:          True
     Restart Count:  5
     Limits:
       cpu:     10m
       memory:  10Mi
     Requests:
       cpu:        10m
       memory:     10Mi
     Environment:  &lt;none&gt;
     Mounts:       &lt;none&gt;
 Containers:
   ambassador:
     Container ID:   docker://cbe4cd544bb744f2a4a465d28bfdced88c97da4d9a5d20e7a3faec30a1716a8c
     Image:          quay.io/datawire/ambassador:0.37.0
     Image ID:       docker-pullable://quay.io/datawire/ambassador@sha256:54b8cf9d1294b91e255f275efe9ddd33544edd29325e02e606c00cf4e5304154
     Port:           &lt;none&gt;
     Host Port:      &lt;none&gt;
     State:          Waiting
       Reason:       CrashLoopBackOff
     Last State:     Terminated
       Reason:       Error
       Exit Code:    1
       Started:      Tue, 14 May 2019 16:57:38 +0200
       Finished:     Tue, 14 May 2019 16:57:42 +0200
     Ready:          False
     Restart Count:  13
     Limits:
       cpu:     1
       memory:  400Mi
     Requests:
       cpu:      200m
       memory:   100Mi
     Liveness:   http-get http://:8877/ambassador/v0/check_alive delay=30s timeout=1s period=30s #success=1 #failure=3
     Readiness:  http-get http://:8877/ambassador/v0/check_ready delay=30s timeout=1s period=30s #success=1 #failure=3
     Environment:
       AMBASSADOR_NAMESPACE:  kubeflow (v1:metadata.namespace)
     Mounts:
       /var/run/secrets/kubernetes.io/serviceaccount from ambassador-token-smjq7 (ro)
   istio-proxy:
     Container ID:  docker://b296e41760bc00b12b90b05e1223c110b44f99939362ec10a30ab8bc7929ddcf
     Image:         gcr.io/istio-release/proxyv2:release-1.1-20190111-09-15
     Image ID:      docker-pullable://gcr.io/istio-release/proxyv2@sha256:325ee8ba87ce70b13dfbc9cdd0358989911f7e517ae37ba391b2c237489c973d
     Port:          15090/TCP
     Host Port:     0/TCP
     Args:
       proxy
       sidecar
       --configPath
       /etc/istio/proxy
       --binaryPath
       /usr/local/bin/envoy
       --serviceCluster
       istio-proxy.kubeflow
       --drainDuration
       45s
       --parentShutdownDuration
       1m0s
       --discoveryAddress
       istio-pilot.istio-system:15010
       --zipkinAddress
       zipkin.istio-system:9411
       --connectTimeout
       10s
       --proxyAdminPort
       15000
       --controlPlaneAuthPolicy
       NONE
       --statusPort
       15020
       --applicationPorts
 
     State:          Running
       Started:      Tue, 14 May 2019 16:15:16 +0200
     Ready:          True
     Restart Count:  0
     Requests:
       cpu:      10m
     Readiness:  http-get http://:15020/healthz/ready delay=1s timeout=1s period=2s #success=1 #failure=30
     Environment:
       POD_NAME:                      ambassador-796bcd74b5-4p949 (v1:metadata.name)
       POD_NAMESPACE:                 kubeflow (v1:metadata.namespace)
       INSTANCE_IP:                    (v1:status.podIP)
       ISTIO_META_POD_NAME:           ambassador-796bcd74b5-4p949 (v1:metadata.name)
       ISTIO_META_CONFIG_NAMESPACE:   kubeflow (v1:metadata.namespace)
       ISTIO_META_INTERCEPTION_MODE:  REDIRECT
       ISTIO_METAJSON_LABELS:         {"app.kubernetes.io/name":"app","pod-template-hash":"796bcd74b5","service":"ambassador"}
 
     Mounts:
       /etc/certs/ from istio-certs (ro)
       /etc/istio/proxy from istio-envoy (rw)
       /var/run/secrets/kubernetes.io/serviceaccount from ambassador-token-smjq7 (ro)
 Conditions:
   Type              Status
   Initialized       True
   Ready             False
   ContainersReady   False
   PodScheduled      True
 Volumes:
   ambassador-token-smjq7:
     Type:        Secret (a volume populated by a Secret)
     SecretName:  ambassador-token-smjq7
     Optional:    false
   istio-envoy:
     Type:    EmptyDir (a temporary directory that shares a pod's lifetime)
     Medium:  Memory
   istio-certs:
     Type:        Secret (a volume populated by a Secret)
     SecretName:  istio.ambassador
     Optional:    true
 QoS Class:       Burstable
 Node-Selectors:  &lt;none&gt;
 Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                  node.kubernetes.io/unreachable:NoExecute for 300s
 Events:
   Type     Reason     Age                 From                                              Message
   ----     ------     ----                ----                                              -------
   Normal   Scheduled  48m                 default-scheduler                                 Successfully assigned kubeflow/ambassador-796bcd74b5-4p949 to ip-10-0-0-82.eu-west-1.compute.internal
   Normal   Pulling    48m                 kubelet, ip-10-0-0-82.eu-west-1.compute.internal  pulling image "gcr.io/istio-release/proxy_init:release-1.1-20190111-09-15"
   Normal   Pulled     47m                 kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Successfully pulled image "gcr.io/istio-release/proxy_init:release-1.1-20190111-09-15"
   Normal   Created    46m (x5 over 47m)   kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Created container
   Warning  Failed     46m (x5 over 47m)   kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Error: failed to start container "istio-init": Error response from daemon: OCI runtime create failed: container_linux.go:348: starting container process caused "process_linux.go:402: container init caused \"process_linux.go:367: setting cgroup config for procHooks process caused \\\"failed to write 10485760 to memory.limit_in_bytes: write /sys/fs/cgroup/memory/kubepods/burstable/pod39aa57ba-7652-11e9-84fe-027424f7c2d4/istio-init/memory.limit_in_bytes: device or resource busy\\\"\"": unknown
   Normal   Pulled     46m (x4 over 47m)   kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Container image "gcr.io/istio-release/proxy_init:release-1.1-20190111-09-15" already present on machine
   Warning  BackOff    45m (x9 over 47m)   kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Back-off restarting failed container
   Normal   Started    23m (x10 over 44m)  kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Started container
   Normal   Pulled     17m (x10 over 44m)  kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Container image "quay.io/datawire/ambassador:0.37.0" already present on machine
   Warning  BackOff    3m (x196 over 44m)  kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Back-off restarting failed container
 &lt;/denchmark-code&gt;
 
 Logs ambassador
 &lt;denchmark-code&gt;$ kubectl -nkubeflow logs ambassador-796bcd74b5-4p949 ambassador
 2019-05-14 14:47:17 kubewatch 0.37.0 INFO: generating config with gencount 1 (8 changes)
 /usr/lib/python3.6/site-packages/pkg_resources/__init__.py:1298: UserWarning: /ambassador is writable by group/others and vulnerable to attack when used with get_resource_filename. Consider a more secure location (set with .set_extraction_path or the PYTHON_EGG_CACHE environment variable).
   warnings.warn(msg, UserWarning)
 2019-05-14 14:47:18 kubewatch 0.37.0 INFO: Scout reports {"latest_version": "0.61.0", "application": "ambassador", "notices": [], "cached": false, "timestamp": 1557845237.926627}
 [2019-05-14 14:47:18.564][13][info][config] source/server/configuration_impl.cc:50] loading 0 static secret(s)
 [2019-05-14 14:47:18.566][13][info][upstream] source/common/upstream/cluster_manager_impl.cc:132] cm init: all clusters initialized
 [2019-05-14 14:47:18.566][13][info][config] source/server/configuration_impl.cc:60] loading 1 listener(s)
 [2019-05-14 14:47:18.586][13][info][config] source/server/configuration_impl.cc:94] loading tracing configuration
 [2019-05-14 14:47:18.586][13][info][config] source/server/configuration_impl.cc:116] loading stats sink configuration
 AMBASSADOR: starting diagd
 AMBASSADOR: starting Envoy
 AMBASSADOR: waiting
 PIDS: 17:diagd 18:envoy 19:kubewatch
 starting hot-restarter with target: /ambassador/start-envoy.sh
 forking and execing new child process at epoch 0
 forked new child process with PID=20
 got SIGCHLD
 PID=20 exited with code=1
 Due to abnormal exit, force killing all child processes and exiting
 exiting due to lack of child processes
 AMBASSADOR: envoy exited with status 1
 Here's the envoy.json we were trying to run with:
 {
   "listeners": [
 
     {
       "address": "tcp://0.0.0.0:80",
 
       "filters": [
         {
           "type": "read",
           "name": "http_connection_manager",
           "config": {"codec_type": "auto",
             "stat_prefix": "ingress_http",
             "access_log": [
               {
                 "format": "ACCESS [%START_TIME%] \"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\" %RESPONSE_CODE% %RESPONSE_FLAGS% %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \"%REQ(X-FORWARDED-FOR)%\" \"%REQ(USER-AGENT)%\" \"%REQ(X-REQUEST-ID)%\" \"%REQ(:AUTHORITY)%\" \"%UPSTREAM_HOST%\"\n",
                 "path": "/dev/fd/1"
               }
             ],
 
             "route_config": {
               "virtual_hosts": [
                 {
                   "name": "backend",
                   "domains": ["*"],"routes": [
 
                     {
                       "timeout_ms": 3000,"prefix": "/ambassador/v0/check_ready","prefix_rewrite": "/ambassador/v0/check_ready","weighted_clusters": {
                               "clusters": [
 
                                     { "name": "cluster_127_0_0_1_8877", "weight": 100.0 }
 
                               ]
                           }
 
 
                     }
                     ,
 
                     {
                       "timeout_ms": 3000,"prefix": "/ambassador/v0/check_alive","prefix_rewrite": "/ambassador/v0/check_alive","weighted_clusters": {
                               "clusters": [
 
                                     { "name": "cluster_127_0_0_1_8877", "weight": 100.0 }
 
                               ]
                           }
 
 
                     }
                     ,
 
                     {
                       "timeout_ms": 3000,"prefix": "/ambassador/v0/","prefix_rewrite": "/ambassador/v0/","weighted_clusters": {
                               "clusters": [
 
                                     { "name": "cluster_127_0_0_1_8877", "weight": 100.0 }
 
                               ]
                           }
 
 
                     }
                     ,
 
                     {
                       "timeout_ms": 300000,"prefix": "/pipeline","use_websocket": true,"prefix_rewrite": "/pipeline","cluster": "cluster_ml_pipeline_ui_kubeflow"
 
 
                     }
                     ,
 
                     {
                       "timeout_ms": 3000,"prefix": "/jupyter/","prefix_rewrite": "/","request_headers_to_add": [{"key": "x-forwarded-prefix", "value": "/jupyter"}],"weighted_clusters": {
                               "clusters": [
 
                                     { "name": "cluster_jupyter_web_app_kubeflow", "weight": 100.0 }
 
                               ]
                           }
 
 
                     }
                     ,
 
                     {
                       "timeout_ms": 3000,"prefix": "/tfjobs/","prefix_rewrite": "/tfjobs/","weighted_clusters": {
                               "clusters": [
 
                                     { "name": "cluster_tf_job_dashboard_kubeflow", "weight": 100.0 }
 
                               ]
                           }
 
 
                     }
                     ,
 
                     {
                       "timeout_ms": 3000,"prefix": "/katib/","prefix_rewrite": "/katib/","weighted_clusters": {
                               "clusters": [
 
                                     { "name": "cluster_katib_ui_kubeflow", "weight": 100.0 }
 
                               ]
                           }
 
 
                     }
                     ,
 
                     {
                       "timeout_ms": 300000,"prefix": "/user/","use_websocket": true,"prefix_rewrite": "/user/","cluster": "cluster_jupyter_lb_kubeflow"
 
 
                     }
                     ,
 
                     {
                       "timeout_ms": 3000,"prefix": "/argo/","prefix_rewrite": "/","weighted_clusters": {
                               "clusters": [
 
                                     { "name": "cluster_argo_ui_kubeflow", "weight": 100.0 }
 
                               ]
                           }
 
 
                     }
                     ,
 
                     {
                       "timeout_ms": 300000,"prefix": "/hub/","use_websocket": true,"prefix_rewrite": "/hub/","cluster": "cluster_jupyter_lb_kubeflow"
 
 
                     }
                     ,
 
                     {
                       "timeout_ms": 300000,"prefix": "/data","use_websocket": true,"prefix_rewrite": "/data","cluster": "cluster_ml_pipeline_ui_kubeflow"
 
 
                     }
                     ,
 
                     {
                       "timeout_ms": 3000,"prefix": "/","prefix_rewrite": "/","weighted_clusters": {
                               "clusters": [
 
                                     { "name": "cluster_centraldashboard_kubeflow", "weight": 100.0 }
 
                               ]
                           }
 
 
                     }
 
 
                   ]
                 }
               ]
             },
             "filters": [
               {
                 "name": "cors",
                 "config": {}
               },{"type": "decoder",
                 "name": "router",
                 "config": {}
               }
             ]
           }
         }
       ]
     }
   ],
   "admin": {
     "address": "tcp://127.0.0.1:8001",
     "access_log_path": "/tmp/admin_access_log"
   },
 
   "cluster_manager": {
     "clusters": [
       {
         "name": "cluster_127_0_0_1_8877",
         "connect_timeout_ms": 3000,
         "type": "strict_dns",
         "lb_type": "round_robin",
         "hosts": [
           {
             "url": "tcp://127.0.0.1:8877"
           }
 
         ]},
       {
         "name": "cluster_argo_ui_kubeflow",
         "connect_timeout_ms": 3000,
         "type": "strict_dns",
         "lb_type": "round_robin",
         "hosts": [
           {
             "url": "tcp://argo-ui.kubeflow:80"
           }
 
         ]},
       {
         "name": "cluster_centraldashboard_kubeflow",
         "connect_timeout_ms": 3000,
         "type": "strict_dns",
         "lb_type": "round_robin",
         "hosts": [
           {
             "url": "tcp://centraldashboard.kubeflow:80"
           }
 
         ]},
       {
         "name": "cluster_jupyter_lb_kubeflow",
         "connect_timeout_ms": 3000,
         "type": "strict_dns",
         "lb_type": "round_robin",
         "hosts": [
           {
             "url": "tcp://jupyter-lb.kubeflow:80"
           }
 
         ]},
       {
         "name": "cluster_jupyter_web_app_kubeflow",
         "connect_timeout_ms": 3000,
         "type": "strict_dns",
         "lb_type": "round_robin",
         "hosts": [
           {
             "url": "tcp://jupyter-web-app.kubeflow:80"
           }
 
         ]},
       {
         "name": "cluster_katib_ui_kubeflow",
         "connect_timeout_ms": 3000,
         "type": "strict_dns",
         "lb_type": "round_robin",
         "hosts": [
           {
             "url": "tcp://katib-ui.kubeflow:80"
           }
 
         ]},
       {
         "name": "cluster_ml_pipeline_ui_kubeflow",
         "connect_timeout_ms": 3000,
         "type": "strict_dns",
         "lb_type": "round_robin",
         "hosts": [
           {
             "url": "tcp://ml-pipeline-ui.kubeflow:80"
           }
 
         ]},
       {
         "name": "cluster_tf_job_dashboard_kubeflow",
         "connect_timeout_ms": 3000,
         "type": "strict_dns",
         "lb_type": "round_robin",
         "hosts": [
           {
             "url": "tcp://tf-job-dashboard.kubeflow:80"
           }
 
         ]}
 
     ]
   },
 
   "statsd_udp_ip_address": "127.0.0.1:8125",
   "stats_flush_interval_ms": 1000
 }AMBASSADOR: shutting down (1)
 &lt;/denchmark-code&gt;
 
 Could this here be the error? I saw the same error also on other pods, but it looks like they were able to start successfully after a few retries.. just the ambassador one keeps logging this event.
 &lt;denchmark-code&gt;Error: failed to start container "istio-init": Error response from daemon: OCI runtime create failed: container_linux.go:348: starting container process caused "process_linux.go:402: container init caused \"process_linux.go:367: setting cgroup config for procHooks process caused \\\"failed to write 10485760 to memory.limit_in_bytes: write /sys/fs/cgroup/memory/kubepods/burstable/pod39aa57ba-7652-11e9-84fe-027424f7c2d4/istio-init/memory.limit_in_bytes: device or resource busy\\\"\"": unknown
 &lt;/denchmark-code&gt;
 
 There is enough memory available on all nodes, there is also only kubeflow installed on the nodes, no other applications/pods running.
 Security group nodes:
 Outbound to *: All traffic allowed
 Inbound from node: All traffic allowed
 Inbound from EKS control: 443, 1025 - 65535
 Security group EKS control:
 Outbound to *: All traffic allowed
 Inbound from node: 443
 Do you need some other information?
 EDIT: Just used v0.5.1 and the same issue occurs.
 	</description>
 	<comments>
 		<comment id='1' author='chris922' date='2019-05-14T15:58:51Z'>
 		Issue-Label Bot is automatically applying the label kind/bug to this issue, with a confidence of 0.71. Please mark this comment with 👍 or 👎 to give our bot feedback!
 Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://mlbot.net/data/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
  and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
  for this bot.
 		</comment>
 		<comment id='2' author='chris922' date='2019-05-14T18:00:45Z'>
 		I confirm that I encountered the same error when trying to deploy kubeflow with Istio enabled, using platform=none.
 		</comment>
 		<comment id='3' author='chris922' date='2019-05-14T19:13:55Z'>
 		It seems that you have istio injector on for kubeflow namespace.
 We haven't done that yet, do you manually change it?
 Istio sidecar injector cannot be apply to ambassador (because they are both envoy). So you should exclude ambassador (you can add annotation to the pod).
 Not sure about vizier though.
 		</comment>
 		<comment id='4' author='chris922' date='2019-05-14T19:39:08Z'>
 		Thanks for the fast answers!
 
 It seems that you have istio injector on for kubeflow namespace.
 We haven't done that yet, do you manually change it?
 
 I haven't done this manually, I will try to disable this tomorrow.. just had a quick look and it seems that the kfctl.sh script activated it:
 
 
 
 kubeflow/scripts/aws/util.sh
 
 
         Lines 154 to 157
       in
       4b1ae00
 
 
 
 
 
 
  kubectl apply -f ${KUBEFLOW_K8S_MANIFESTS_DIR}/istio-crds.yaml 
 
 
 
  kubectl apply -f ${KUBEFLOW_K8S_MANIFESTS_DIR}/istio-noauth.yaml 
 
 
 
  
 
 
 
  kubectl label namespace ${K8S_NAMESPACE} istio-injection=enabled --overwrite 
 
 
 
 
 
 Is the istio injection required to get the istio-ingress working?
 		</comment>
 		<comment id='5' author='chris922' date='2019-05-15T13:20:47Z'>
 		I removed the label and all pods started successfully!
 For me personally the ticket can be closed, but maybe you want to remove the istio injection until it is available? &lt;denchmark-link:https://github.com/Jeffwan&gt;@Jeffwan&lt;/denchmark-link&gt;
  you added the injection according to git-blame, WDYT?
 If you like I can provide a PR that just removes the line that sets the istio-injection label.
 		</comment>
 		<comment id='6' author='chris922' date='2019-05-15T15:59:59Z'>
 		&lt;denchmark-link:https://github.com/chris922&gt;@chris922&lt;/denchmark-link&gt;
  I make the change in 0.5 and recently istio changes comes in master branch. I will have a check and come back to you soon.
 		</comment>
 		<comment id='7' author='chris922' date='2019-05-15T20:38:44Z'>
 		ConfigMap  policy has been changed from  to   in this commit.
 &lt;denchmark-link:https://github.com/kubeflow/kubeflow/commit/2455ba6134dff4556471a3cafd9bcdef3e0fb241#diff-1ac92c9005d2565157922d5a08fe8166&gt;2455ba6#diff-1ac92c9005d2565157922d5a08fe8166&lt;/denchmark-link&gt;
 
 That's the reason enabled namespace  + enabled configmap will automatically inject sidecars.
 In 0.5, istio is not default installed in kubeflow and AWS solutions install it by default to integrate with services. enabled namespace + disabled configmap will make users determine if they'd like to use istio or not by adding pod annotation sidecar.istio.io/inject.
 &lt;denchmark-link:https://github.com/lluunn&gt;@lluunn&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/kunmingg&gt;@kunmingg&lt;/denchmark-link&gt;
   Any reason to turn on  ? I need to do corresponding changes for AWS.
 		</comment>
 		<comment id='8' author='chris922' date='2019-05-15T20:48:05Z'>
 		That's part of 0.6's multi-user support
 		</comment>
 		<comment id='9' author='chris922' date='2019-05-15T20:57:07Z'>
 		
 That's part of 0.6's multi-user support
 
 &lt;denchmark-link:https://github.com/lluunn&gt;@lluunn&lt;/denchmark-link&gt;
 
 Does it mean users won't deploy any workloads needs istio in kubeflow namespace? I notice code base still have some references for . If istio inject is disabled at kubeflow namespace level, this will definitely not work unless it's deployed in other namespaces.
 Another concern is all the resources deployed in namespace enabled injection will have sidecar?
 		</comment>
 		<comment id='10' author='chris922' date='2019-05-15T21:00:40Z'>
 		I don't quite understand the question and concern.
 The plan is to turn on sidecar whenever makes sense and use istio features.
 		</comment>
 		<comment id='11' author='chris922' date='2019-05-15T21:36:08Z'>
 		
 I don't quite understand the question and concern.
 The plan is to turn on sidecar whenever makes sense and use istio features.
 
 True. There're many ways to turn on sidecar injection.
 My question is if using disable for ConfigMap and adjust namespace and pod policy is better?
 Or use enable for ConfigMap is better? This major difference is default behavior.
 		</comment>
 		<comment id='12' author='chris922' date='2019-05-16T00:16:31Z'>
 		I think we want the default is on so that everything is in istio mesh.
 Those cannot be in istio mesh we can use pod annotation to exclude them.
 		</comment>
 		<comment id='13' author='chris922' date='2019-05-16T01:15:44Z'>
 		
 I think we want the default is on so that everything is in istio mesh.
 Those cannot be in istio mesh we can use pod annotation to exclude them.
 
 OK. Then we will make some changes on AWS side. &lt;denchmark-link:https://github.com/chris922&gt;@chris922&lt;/denchmark-link&gt;
  could you file a PR to update kubeflow namespace label?  Another improvement is use same version istio as kfctl version.
 
 
 Thanks a lot! I can help review.
 		</comment>
 		<comment id='14' author='chris922' date='2019-09-14T03:08:38Z'>
 		Hello,
 I got the same error after installing kubernetes and kuberflow on my server machine.
 Three ambassador pods are stuck in CrashLoopBackOff status, vizier-core is also same.
 Ambassador log says envoy exited with code 1.
 The problem is I'm very new to kubernetes &amp; kubeflow; I don't know how to "manually disable" istio-sidecar-injector.
 For installation, I followed the steps in &lt;denchmark-link:https://github.com/NVIDIA/deepops&gt;https://github.com/NVIDIA/deepops&lt;/denchmark-link&gt;
 , particularly the instructions on &lt;denchmark-link:https://github.com/NVIDIA/deepops/blob/master/docs/kubernetes-cluster.md&gt;https://github.com/NVIDIA/deepops/blob/master/docs/kubernetes-cluster.md&lt;/denchmark-link&gt;
 .
 The instruction is to use ansible to remotely install kubernetes and kubeflow to the server machine.
 Please, anyone let me know which file(s) to correct and how.
 If pointing to a specific file is difficult, please give me some clues what to look for.
 Thank you very much for helping me.
 		</comment>
 	</comments>
 </bug>
<commit id='e1d26f6f03db221a85b44f7bc3e0fd8c4ef1df95' author='Christian Bandowski' date='2019-05-17 11:28:24-07:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='scripts\aws\util.sh' new_name='scripts\aws\util.sh'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>149,156,157</added_lines>
 			<deleted_lines>149,150,157</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
