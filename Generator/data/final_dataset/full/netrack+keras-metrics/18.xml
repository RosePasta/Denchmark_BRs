<bug_data>
<bug id='18' author='jyhong836' open_date='2018-11-15T20:42:19Z' closed_time='2018-11-20T20:00:25Z'>
 	<summary>What does the `reset_states` do?</summary>
 	<description>
 It seems that the method reset_state in metrics resets the stored values. However, I am not sure when it should be used. Is it for resetting states at the end of each epoch?
 According to my understanding, the keras-metrics is designed to avoid the incorrect approximation of recall on each batch. Thus, a practical solution is computing the metrics on the end of each epoch independently.
 But in the &lt;denchmark-link:https://github.com/netrack/keras-metrics/blob/master/README.md&gt;README.md&lt;/denchmark-link&gt;
 , the given example is
 import keras
 import keras_metrics
 
 model = models.Sequential()
 model.add(keras.layers.Dense(1, activation="sigmoid", input_dim=2))
 model.add(keras.layers.Dense(1, activation="softmax"))
 
 model.compile(optimizer="sgd",
               loss="binary_crossentropy",
               metrics=[keras_metrics.precision(), keras_metrics.recall()])
 which directly pass the keras_metrics.recall() as metrics for batch-based usage. The problem in the demo is that the states may* not be resetted. Therefore, the recall value of each epoch will be dependent on previous epochs.
 * I am not sure if the reset_states method is called at the end of each epoch.
 	</description>
 	<comments>
 		<comment id='1' author='jyhong836' date='2018-11-16T08:24:18Z'>
 		Hi &lt;denchmark-link:https://github.com/jyhong836&gt;@jyhong836&lt;/denchmark-link&gt;
 . The  indeed is called on each epoch, see &lt;denchmark-link:https://github.com/keras-team/keras/blob/2.2.4/keras/engine/training_arrays.py#L145&gt;https://github.com/keras-team/keras/blob/2.2.4/keras/engine/training_arrays.py#L145&lt;/denchmark-link&gt;
 .
 It seems the only thing with that: metrics should be marked as stateful, while they are not. Thank you for noticing that, I'll prepare appropriate changes.
 		</comment>
 		<comment id='2' author='jyhong836' date='2018-11-16T13:42:31Z'>
 		Thank you for your reply. You are right. I didn't notice the calling in training_arrays.py.
 BTW, the unit test case is actually unconvincing. The correctness of the true positive, false negative and etc. values are not tested.
 There has been a stateful metric test, i.e., &lt;denchmark-link:https://github.com/keras-team/keras/blob/75a35032e194a2d065b0071a9e786adf6cee83ea/tests/keras/metrics_test.py#L124&gt;test_stateful_metrics&lt;/denchmark-link&gt;
 , in the official package but only for binary true positive test. You may refer to that.
 Your package is really useful. Thank you for your contribution.
 		</comment>
 		<comment id='3' author='jyhong836' date='2018-11-17T02:14:56Z'>
 		I try to run the unit test in the pull request &lt;denchmark-link:https://github.com/netrack/keras-metrics/pull/19&gt;#19&lt;/denchmark-link&gt;
  . I compare the false_positive value aganist below function:
 def ref_false_pos(y_true, y_pred):
     return np.sum(np.logical_and(np.round(y_pred)==1, y_true == 0))
 y_pred = model.predict(x)
 expected_fp = ref_false_pos(y, y_pred)
 The values will not be equal occassionally. Even if I fixed the random seed by  np.random.seed(2334), the inequality still happens occasionally.
 Is there any explanation for this stochastics?
 		</comment>
 		<comment id='4' author='jyhong836' date='2018-11-17T08:09:55Z'>
 		&lt;denchmark-link:https://github.com/jyhong836&gt;@jyhong836&lt;/denchmark-link&gt;
 , could you, please post an example of run with failing test (output or input data). Unfortunately, I can't reproduce this issue after merging pull request &lt;denchmark-link:https://github.com/netrack/keras-metrics/pull/19&gt;#19&lt;/denchmark-link&gt;
 .
 		</comment>
 		<comment id='5' author='jyhong836' date='2018-11-17T14:49:22Z'>
 		&lt;denchmark-link:https://github.com/ybubnov&gt;@ybubnov&lt;/denchmark-link&gt;
  I post my test at &lt;denchmark-link:https://github.com/jyhong836/keras-metrics&gt;jyhong836/keras-metrics&lt;/denchmark-link&gt;
 . But I am not sure if you can reproduce the result. I also include a  file. Please put it under your working directory.
 Currently, I can reproduce the error on my Macbook, macos 10.14, tensorflow 1.5.0 (cpu version), keras 2.2.4 &amp; 2.1.6. However, I cannot reproduce it on another linux computer, with tensorflow 1.4.0 (GPU version), keras 2.1.6. I am not sure if it is the version issue or the computer issue.
 		</comment>
 		<comment id='6' author='jyhong836' date='2018-11-19T13:45:57Z'>
 		&lt;denchmark-link:https://github.com/jyhong836&gt;@jyhong836&lt;/denchmark-link&gt;
 , I've tried to run your tests on my Linux machine and I've managed to reproduce an issue with the tensorflow  version. There is no issue with tensoflow  though.
 		</comment>
 		<comment id='7' author='jyhong836' date='2018-11-19T14:23:38Z'>
 		Tests are failing both when model is loaded from temp_model.hdf5 and after model fitting.
 		</comment>
 		<comment id='8' author='jyhong836' date='2018-11-19T15:29:04Z'>
 		So I guess there is some bug in tensorflow &lt;=1.5.0.
 Which tensorflow do you use? GPU or CPU version?
 		</comment>
 		<comment id='9' author='jyhong836' date='2018-11-20T06:13:02Z'>
 		I'm able to reproduce an issue with tensorflow 1.6.0 and 1.7.0 as well. I'm using CPU version (from pip repository, so not optimized for AVX2 and FMA instructions).
 		</comment>
 		<comment id='10' author='jyhong836' date='2018-11-20T20:00:25Z'>
 		I think there is no better solution than upgrading. I will close the issue.
 		</comment>
 	</comments>
 </bug>
<commit id='f12f3358a458357d5abf3eeb2d05aae46aa554e2' author='Yasha Bubnov' date='2018-11-16 17:37:54+03:00'>
 	<dmm_unit complexity='1.0' interfacing='0.5789473684210527' size='0.42105263157894735'></dmm_unit>
 	<modification change_type='MODIFY' old_name='keras_metrics\__init__.py' new_name='keras_metrics\__init__.py'>
 		<file_info nloc='2' complexity='0' token_count='9'></file_info>
 		<modified_lines>
 			<added_lines>1</added_lines>
 			<deleted_lines>1</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='keras_metrics\metrics.py' new_name='keras_metrics\metrics.py'>
 		<file_info nloc='165' complexity='28' token_count='1307'></file_info>
 		<method name='__init__' parameters='self,name,kwargs'>
 				<method_info nloc='5' complexity='1' token_count='56' nesting_level='1' start_line='173' end_line='178'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>178</deleted_lines>
 		</method>
 		<method name='__call__' parameters='self,y_true,y_pred'>
 				<method_info nloc='8' complexity='1' token_count='90' nesting_level='1' start_line='186' end_line='196'></method_info>
 			<added_lines>190,191,192,193,194,195,196</added_lines>
 			<deleted_lines>189,190</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,name,kwargs'>
 				<method_info nloc='5' complexity='1' token_count='56' nesting_level='1' start_line='200' end_line='205'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>205</deleted_lines>
 		</method>
 		<method name='_categorical' parameters='self,y_true,y_pred,dtype'>
 				<method_info nloc='10' complexity='3' token_count='83' nesting_level='1' start_line='36' end_line='53'></method_info>
 			<added_lines>41,51</added_lines>
 			<deleted_lines>39,49</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,label,kwargs'>
 				<method_info nloc='8' complexity='2' token_count='73' nesting_level='1' start_line='9' end_line='20'></method_info>
 			<added_lines>11,12,16</added_lines>
 			<deleted_lines>14</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>172,221,222,223,224,225,226,227,251,252,253</added_lines>
 			<deleted_lines>170,216,217</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\test_metrics.py' new_name='tests\test_metrics.py'>
 		<file_info nloc='52' complexity='1' token_count='432'></file_info>
 		<method name='test_metrics' parameters='self'>
 				<method_info nloc='43' complexity='1' token_count='399' nesting_level='1' start_line='11' end_line='67'></method_info>
 			<added_lines>13,22,23,27,29,30,31,32,33,36,37,38,39,42,43,44,46,47,48,57,58,59,60,61,62,63,64,65,66,67</added_lines>
 			<deleted_lines>19,20,24,26,27,30,31,34,35,37,38,39,48,49,50</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>2,4</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
