<bug_data>
<bug id='1377' author='terrykong' open_date='2021-01-04T19:50:39Z' closed_time='2021-01-11T23:11:55Z'>
 	<summary>`request_id` in response header seems broken</summary>
 	<description>
 Describe the bug
 It appears that there is something wrong with the request_id value that is returned from a prediction request's response headers
 To Reproduce
 What I’m observing is if I make multiple requests:
 &lt;denchmark-code&gt;curl -L -i \
 -H "Content-Type: application/json" \
 -d "{\"query\":[[\"hello world\"]]}" \
 -X POST $URL
 &lt;/denchmark-code&gt;
 
 I will observe the same “request_id” a2f075c1-91a1-4a9c-9b25-6b634cf33f95 even if i make different requests. see
 &lt;denchmark-code&gt;HTTP/1.1 200 OK
 X-DNS-Prefetch-Control: off
 X-Frame-Options: SAMEORIGIN
 Strict-Transport-Security: max-age=15552000; includeSubDomains
 X-Download-Options: noopen
 X-Content-Type-Options: nosniff
 X-XSS-Protection: 1; mode=block
 X-Request-Id: 6d7afbf0-c72c-48d5-9818-0b87bcf873bc
 content-length: 100
 content-type: application/json
 date: Mon, 04 Jan 2021 17:42:28 GMT
 request_id: a2f075c1-91a1-4a9c-9b25-6b634cf33f95
 server: gunicorn/20.0.4
 connection: close
 Vary: Accept-Encoding
 &lt;/denchmark-code&gt;
 
 Also, aside from the “request_id” not changing, it also appears that the request_id that’s logged in predictions.log is just different, see this line
 &lt;denchmark-code&gt;{"service_name": "FooService", "service_version": "20201216204449_CC2735", "api": "is_it_correct", "request_id": "2ae81393-30b4-4a04-8029-d8c14530f43d", "task": {"data": "{\"query\":[[\"hello world\"]]}", "task_id": "2ae81393-30b4-4a04-8029-d8c14530f43d", "http_headers": [["Host", "--redacted--"], ["User-Agent", "curl/7.71.1"], ["Content-Length", "49"], ["Accept", "*/*"], ["Content-Type", "application/json"], ["X-Forwarded-For", "192.168.192.1"], ["X-Forwarded-Host", "--redacted--"], ["X-Forwarded-Port", "8093"], ["X-Forwarded-Prefix", "/predict"], ["X-Forwarded-Proto", "http"], ["X-Forwarded-Server", "6e1efdeb5044"], ["X-Real-Ip", "192.168.192.1"], ["Accept-Encoding", "gzip"]]}, "result": {"data": "{\"top_tags\": [[\"\", \"\", \"\", \"\", \"\", \"?\"]], \"result\": [\"correct\"]}", "http_status": 200, "http_headers": [["Content-Type", "application/json"]]}, "asctime": "2021-01-04 17:42:28,634"}
 &lt;/denchmark-code&gt;
 
 Interestingly, a2f075c1-91a1-4a9c-9b25-6b634cf33f95 doesn’t appear anywhere in predictions.log
 Expected behavior
 Screenshots/Logs
 Environment:
 
 OS: [e.g. MacOS 10.14.3]
 Python Version [e.g. Python 3.7.1]
 BentoML Version [e.g. BentoML-0.8.6]
 
 Additional context
 	</description>
 	<comments>
 		<comment id='1' author='terrykong' date='2021-01-11T23:12:27Z'>
 		This issue has been fixed in &lt;denchmark-link:https://github.com/bentoml/BentoML/pull/1390&gt;#1390&lt;/denchmark-link&gt;
  and will be available in the upcoming release 0.11.0
 		</comment>
 	</comments>
 </bug>
<commit id='568c44877027cf1e6032a7f0e0aaee7d0756cdad' author='bojiang' date='2021-01-11 15:11:54-08:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='bentoml\adapters\json_output.py' new_name='bentoml\adapters\json_output.py'>
 		<file_info nloc='88' complexity='10' token_count='421'></file_info>
 		<modified_lines>
 			<added_lines>19</added_lines>
 			<deleted_lines>19,20,21,22</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='bentoml\server\api_server.py' new_name='bentoml\server\api_server.py'>
 		<file_info nloc='217' complexity='32' token_count='1130'></file_info>
 		<method name='bento_service_api_func_wrapper.api_func' parameters=''>
 				<method_info nloc='30' complexity='6' token_count='159' nesting_level='2' start_line='301' end_line='337'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>335,336</deleted_lines>
 		</method>
 		<method name='bento_service_api_func_wrapper' parameters='self,InferenceAPI'>
 				<method_info nloc='5' complexity='1' token_count='26' nesting_level='1' start_line='294' end_line='345'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>299,335,336</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>18</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='bentoml\service\inference_api.py' new_name='bentoml\service\inference_api.py'>
 		<file_info nloc='235' complexity='51' token_count='1301'></file_info>
 		<method name='handle_batch_request' parameters='self'>
 				<method_info nloc='11' complexity='2' token_count='89' nesting_level='1' start_line='294' end_line='304'></method_info>
 			<added_lines>301,302,303,304</added_lines>
 			<deleted_lines>300</deleted_lines>
 		</method>
 		<method name='handle_request' parameters='self,Request'>
 				<method_info nloc='8' complexity='1' token_count='75' nesting_level='1' start_line='285' end_line='292'></method_info>
 			<added_lines>291</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
