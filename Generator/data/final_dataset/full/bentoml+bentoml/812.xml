<bug_data>
<bug id='812' author='dcferreira' open_date='2020-06-17T16:41:47Z' closed_time='2020-06-18T07:07:04Z'>
 	<summary>--enable-microbatch breaks in text files with "," in it</summary>
 	<description>
 Describe the bug
 When microbatch is enabled, a JSON input is (badly) converted to CSV, and a dataframe is generated from this CSV (I couldn't find any option to avoid this conversion, please let me know if such an option exists).
 Therefore, when a text field has the separator character (,) in it, it breaks. I suspect other characters might also break it (", \n).
 I'm not 100% sure I'm using this as intended from the client side, so please advise if that is the case.
 To Reproduce
 Steps to reproduce the behavior:
 
 Make a model, using the following code:
 
 import bentoml
 from bentoml.adapters import DataframeInput, DataframeOutput
 
 
 @bentoml.env(auto_pip_dependencies=True)
 class TestSizer(bentoml.BentoService):
     @bentoml.api(input=DataframeInput(input_dtypes={"text": "str"}),
                  output=DataframeOutput())
     def size(self, df):
         if 'text' not in df:
             raise ValueError('The Dataframe needs a column named "text".')
         return df.text.apply(len).to_frame()
 
 
 if __name__ == '__main__':
     sizer = TestSizer()
     sizer.save()
 
 
 Launch the server with bentoml serve TestSizer:latest --enable-microbatch.
 
 
 Use the following script to send some requests:
 
 
 import pandas as pd
 import requests
 
 
 class TextClient:
     def __init__(self, server_url: str):
         self.server_url = server_url
 
     def size(self, df: pd.DataFrame) -&gt; pd.DataFrame:
         if 'text' not in df:
             raise ValueError('The given Dataframe requires a column named "html".')
         resp = requests.post(self.server_url, headers={'Content-Type': 'application/json'},
                              data=df.to_json())
         if resp.status_code != 200:
             raise ValueError('Bad reply!')
         return pd.DataFrame(data=resp.json(), index=df.index)
 
 
 if __name__ == '__main__':
     client = TextClient('http://127.0.0.1:5000/size')
     ex1 = pd.DataFrame({"text": ["this is a text", "and another one"]}, index=["one", "two"])
     ex2 = pd.DataFrame({"text": ["this, is, a, text", "and yet another one"]}, index=["one", "two"])
 
     print(client.size(ex1))
     print(client.size(ex2))
 As described, ex1 is sent fine to the server, and a correct answer comes back; but for ex2 the server breaks with IndexError: list index out of range while trying to parse CSV into a DataFrame.
 If the server is running without --enable-microbatch, everything works fine.
 Full error message:
 &lt;denchmark-code&gt;[2020-06-17 18:29:02,374] ERROR - Exception on /size [POST]
 Traceback (most recent call last):
   File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/bentoml/server/bento_api_server.py", line 265, in api_func
     response_body = api.handle_batch_request(request)
   File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/bentoml/service.py", line 132, in handle_batch_request
     responses = self.handler.handle_batch_request(requests, self.func)
   File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/bentoml/adapters/dataframe_input.py", line 298, in handle_batch_request
     datas, content_types
   File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/bentoml/adapters/dataframe_input.py", line 148, in read_dataframes_from_json_n_csv
     df_merged = pd.read_csv(StringIO(df_str_csv), index_col=0)
   File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/pandas/io/parsers.py", line 676, in parser_f
     return _read(filepath_or_buffer, kwds)
   File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/pandas/io/parsers.py", line 454, in _read
     data = parser.read(nrows)
   File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/pandas/io/parsers.py", line 1133, in read
     ret = self._engine.read(nrows)
   File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/pandas/io/parsers.py", line 2078, in read
     values = data.pop(self.index_col[i])
 IndexError: list index out of range
 127.0.0.1 - - [17/Jun/2020 18:29:02] "POST /size HTTP/1.1" 500 -
 [2020-06-17 18:29:02,377] ERROR - Traceback (most recent call last):
   File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/bentoml/marshal/marshal.py", line 207, in request_dispatcher
     resp = await self.batch_handlers[api_name](req)
   File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/bentoml/marshal/dispatcher.py", line 144, in _func
     raise r
   File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/bentoml/marshal/dispatcher.py", line 204, in outbound_call
     outputs = await self.callback(tuple(d for _, d, _ in inputs_info))
   File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/bentoml/marshal/marshal.py", line 110, in _batch_handler_template
     return await func(requests, api_name)
   File "/home/dferreira/.miniconda3/envs/scam_env/lib/python3.7/site-packages/bentoml/marshal/marshal.py", line 269, in _batch_handler_template
     payload=SimpleResponse(resp.status, resp.headers, raw),
 bentoml.exceptions.RemoteException: Bad response status from model server:
 500
 b'An error has occurred in BentoML user code when handling this request, find the error details in server logs'
 &lt;/denchmark-code&gt;
 
 Environment:
 
 OS: Linux
 Python Version: 3.7
 BentoML Version: tried with 0.8.1 (stable) and also with current master branch
 
 	</description>
 	<comments>
 		<comment id='1' author='dcferreira' date='2020-06-17T20:08:56Z'>
 		&lt;denchmark-link:https://github.com/bojiang&gt;@bojiang&lt;/denchmark-link&gt;
  it probably makes more sense to separate batching JSON and CSV requests?
 I think it is ok to add a parameter to DataframeInput to specify the content type to only JSON or CSV if that makes more sense for micro batching implementation, e.g.:
 &lt;denchmark-code&gt;@api(input=DataframeInput(http_content_type='CSV'))
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='2' author='dcferreira' date='2020-06-18T08:43:28Z'>
 		&lt;denchmark-link:https://github.com/dcferreira&gt;@dcferreira&lt;/denchmark-link&gt;
  Thanks for your feedback!
 
 Fixed in PR #814
 More cases added to the test of DataframeInput https://github.com/bentoml/BentoML/blob/master/tests/handlers/test_dataframe_handler.py#L153
 As I mentioned in doc strings, the conversion before pandas.read_csv/read_json is necessary because it is N(batch size) times faster than an iteration of a batch of JSON inputs with pandas.read_csv. It helps a lot on throughput.
 
 		</comment>
 	</comments>
 </bug>
<commit id='f893fc4e439b89ced0a8ae4b9be970af447fd966' author='bojiang' date='2020-06-18 00:07:03-07:00'>
 	<dmm_unit complexity='0.2830188679245283' interfacing='0.9622641509433962' size='0.7358490566037735'></dmm_unit>
 	<modification change_type='MODIFY' old_name='bentoml\utils\dataframe_util.py' new_name='bentoml\utils\dataframe_util.py'>
 		<file_info nloc='256' complexity='109' token_count='1699'></file_info>
 		<method name='_from_json_split' parameters='DataFrameState,dict'>
 				<method_info nloc='14' complexity='8' token_count='104' nesting_level='0' start_line='96' end_line='110'></method_info>
 			<added_lines>99,110</added_lines>
 			<deleted_lines>99,105</deleted_lines>
 		</method>
 		<method name='_from_csv_without_index' parameters='DataFrameState'>
 				<method_info nloc='20' complexity='9' token_count='127' nesting_level='0' start_line='154' end_line='173'></method_info>
 			<added_lines>154,155,157,158,159,160,161,162,163,164,165,166,167,170,171,172,173</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_from_json_columns' parameters='DataFrameState,dict'>
 				<method_info nloc='11' complexity='6' token_count='95' nesting_level='0' start_line='68' end_line='79'></method_info>
 			<added_lines>71,79</added_lines>
 			<deleted_lines>71,77</deleted_lines>
 		</method>
 		<method name='_from_csv_without_index' parameters='DataFrameState'>
 				<method_info nloc='9' complexity='4' token_count='66' nesting_level='0' start_line='130' end_line='138'></method_info>
 			<added_lines>130,131,132,133,134,135,136,137,138</added_lines>
 			<deleted_lines>130,132,133,134,135,138</deleted_lines>
 		</method>
 		<method name='_to_str' parameters='v'>
 				<method_info nloc='4' complexity='2' token_count='17' nesting_level='0' start_line='48' end_line='51'></method_info>
 			<added_lines>51</added_lines>
 			<deleted_lines>48,49,50,51</deleted_lines>
 		</method>
 		<method name='_from_csv_with_index' parameters='DataFrameState'>
 				<method_info nloc='9' complexity='4' token_count='68' nesting_level='0' start_line='119' end_line='127'></method_info>
 			<added_lines>119,120,121,122,123,124,125,126,127</added_lines>
 			<deleted_lines>119,120,121,122,123,124,125,126,127</deleted_lines>
 		</method>
 		<method name='_from_json_index' parameters='DataFrameState,dict'>
 				<method_info nloc='11' complexity='6' token_count='96' nesting_level='0' start_line='82' end_line='93'></method_info>
 			<added_lines>85,93</added_lines>
 			<deleted_lines>85,91</deleted_lines>
 		</method>
 		<method name='_from_json_records' parameters='DataFrameState,list'>
 				<method_info nloc='8' complexity='5' token_count='69' nesting_level='0' start_line='48' end_line='56'></method_info>
 			<added_lines>51,56</added_lines>
 			<deleted_lines>48,49,50,51,52,53</deleted_lines>
 		</method>
 		<method name='_from_json_values' parameters='DataFrameState,list'>
 				<method_info nloc='6' complexity='3' token_count='40' nesting_level='0' start_line='59' end_line='65'></method_info>
 			<added_lines>61,65</added_lines>
 			<deleted_lines>62</deleted_lines>
 		</method>
 		<method name='_csv_split' parameters='string,delimiter,maxsplit'>
 				<method_info nloc='6' complexity='3' token_count='50' nesting_level='0' start_line='113' end_line='133'></method_info>
 			<added_lines>113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133</added_lines>
 			<deleted_lines>116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,132,133</deleted_lines>
 		</method>
 		<method name='_dataframe_csv_from_input' parameters='tables,content_types,orients'>
 				<method_info nloc='48' complexity='18' token_count='265' nesting_level='0' start_line='225' end_line='279'></method_info>
 			<added_lines>240,278,279</added_lines>
 			<deleted_lines>243,244,245,246,247,248</deleted_lines>
 		</method>
 		<method name='_csv_unquote' parameters='string'>
 				<method_info nloc='6' complexity='3' token_count='49' nesting_level='0' start_line='136' end_line='141'></method_info>
 			<added_lines>136,137,138,139,140,141</added_lines>
 			<deleted_lines>138</deleted_lines>
 		</method>
 		<method name='_csv_quote' parameters='td'>
 				<method_info nloc='8' complexity='7' token_count='64' nesting_level='0' start_line='144' end_line='151'></method_info>
 			<added_lines>144,145,146,147,148,149,150,151</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_csv_split._iter_line' parameters='line'>
 				<method_info nloc='14' complexity='7' token_count='80' nesting_level='2' start_line='117' end_line='130'></method_info>
 			<added_lines>117,118,119,120,121,122,123,124,125,126,127,128,129,130</added_lines>
 			<deleted_lines>117,118,119,120,121,122,123,124,125,126,127,128,129,130</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>15,111,112,142,143,152,153,311,321</added_lines>
 			<deleted_lines>15,57,67,205,280,284,291</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\handlers\test_dataframe_handler.py' new_name='tests\handlers\test_dataframe_handler.py'>
 		<file_info nloc='202' complexity='31' token_count='1543'></file_info>
 		<method name='test_benchmark_load_dataframes' parameters=''>
 				<method_info nloc='15' complexity='4' token_count='134' nesting_level='0' start_line='257' end_line='278'></method_info>
 			<added_lines>257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_batch_read_dataframes_from_csv_other_CRLF' parameters='df'>
 				<method_info nloc='8' complexity='2' token_count='76' nesting_level='0' start_line='203' end_line='210'></method_info>
 			<added_lines>203,204,205,206,207,208,209,210</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_batch_read_dataframes_from_mixed_json_n_csv' parameters='df'>
 				<method_info nloc='20' complexity='4' token_count='174' nesting_level='0' start_line='172' end_line='200'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>190,191,192,193,194</deleted_lines>
 		</method>
 		<method name='test_guess_orient' parameters='df,orient'>
 				<method_info nloc='4' complexity='2' token_count='36' nesting_level='0' start_line='251' end_line='254'></method_info>
 			<added_lines>251,252,253,254</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>2,3,8,10,160,161,211,212,249,250,255,256</added_lines>
 			<deleted_lines>156</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
