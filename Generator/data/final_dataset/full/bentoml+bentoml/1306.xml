<bug_data>
<bug id='1306' author='xylophone21' open_date='2020-12-05T09:16:39Z' closed_time='2020-12-24T03:06:41Z'>
 	<summary>--enable-microbatch got pickle error</summary>
 	<description>
 Describe the bug
 Run demo with --enable-microbatch
 serve PyTorchFashionClassifier:latest --enable-microbatch
 To Reproduce
 serve PyTorchFashionClassifier:latest --enable-microbatch
 Expected behavior
 run with microbatch
 Screenshots/Logs
 [2020-12-05 17:18:06,106] DEBUG - Creating local YataiService instance
 [2020-12-05 17:18:06,401] DEBUG - Upgrading tables to the latest revision
 [2020-12-05 17:18:06,427] INFO - Getting latest version PyTorchFashionClassifier:20201204180721_6A5E98
 [2020-12-05 17:18:06,427] INFO - Starting BentoML API server in development mode..
 [2020-12-05 17:18:07,866] DEBUG - HTTPSConnectionPool(host='api.amplitude.com', port=443): Max retries exceeded with url: /httpapi (Caused by ConnectTimeoutError(&lt;urllib3.connection.HTTPSConnection object at 0x7fc91aac7100&gt;, 'Connection to api.amplitude.com timed out. (connect timeout=1)'))
 [2020-12-05 17:18:09,371] DEBUG - Using BentoML default docker base image 'bentoml/model-server:0.9.2-py38'
 [2020-12-05 17:18:09,640] WARNING - BentoML by default does not include spacy and torchvision package when using PytorchModelArtifact. To make sure BentoML bundle those packages if they are required for your model, either import those packages in BentoService definition file or manually add them via @env(pip_packages=['torchvision']) when defining a BentoService
 [2020-12-05 17:18:09,642] WARNING - pip package requirement torch already exist
 [2020-12-05 17:18:10,654] DEBUG - HTTPSConnectionPool(host='api.amplitude.com', port=443): Max retries exceeded with url: /httpapi (Caused by ConnectTimeoutError(&lt;urllib3.connection.HTTPSConnection object at 0x7fc91cf88fd0&gt;, 'Connection to api.amplitude.com timed out. (connect timeout=1)'))
 [2020-12-05 17:18:10,726] INFO - Micro batch enabled for API predict
 [2020-12-05 17:18:10,726] INFO - Your system nofile limit is 10240, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.
 [2020-12-05 17:18:11,765] DEBUG - HTTPSConnectionPool(host='api.amplitude.com', port=443): Max retries exceeded with url: /httpapi (Caused by ConnectTimeoutError(&lt;urllib3.connection.HTTPSConnection object at 0x7fc91e8d87f0&gt;, 'Connection to api.amplitude.com timed out. (connect timeout=1)'))
 Traceback (most recent call last):
 File "/Users/lihui/Code/dubhe/bentomlTest/server.py", line 7, in 
 bentoml.cli.cli()
 File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/click/core.py", line 829, in call
 return self.main(*args, **kwargs)
 File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/click/core.py", line 782, in main
 rv = self.invoke(ctx)
 File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/click/core.py", line 1259, in invoke
 return _process_result(sub_ctx.command.invoke(sub_ctx))
 File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/click/core.py", line 1066, in invoke
 return ctx.invoke(self.callback, **ctx.params)
 File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/click/core.py", line 610, in invoke
 return callback(*args, **kwargs)
 File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/bentoml/cli/click_utils.py", line 138, in wrapper
 return func(*args, **kwargs)
 File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/bentoml/cli/click_utils.py", line 115, in wrapper
 return_value = func(*args, **kwargs)
 File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/bentoml/cli/click_utils.py", line 99, in wrapper
 return func(*args, **kwargs)
 File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/bentoml/cli/bento_service.py", line 243, in serve
 start_dev_server(saved_bundle_path, port, enable_microbatch, run_with_ngrok)
 File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/bentoml/server/init.py", line 76, in start_dev_server
 marshal_server.async_start(port=port)
 File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/bentoml/marshal/marshal.py", line 298, in async_start
 marshal_proc.start()
 File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/multiprocessing/process.py", line 121, in start
 self._popen = self._Popen(self)
 File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/multiprocessing/context.py", line 224, in _Popen
 return _default_context.get_context().Process._Popen(process_obj)
 File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/multiprocessing/context.py", line 284, in _Popen
 return Popen(process_obj)
 File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/multiprocessing/popen_spawn_posix.py", line 32, in init
 super().init(process_obj)
 File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/multiprocessing/popen_fork.py", line 19, in init
 self._launch(process_obj)
 File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/multiprocessing/popen_spawn_posix.py", line 47, in _launch
 reduction.dump(process_obj, fp)
 File "/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/multiprocessing/reduction.py", line 60, in dump
 ForkingPickler(file, protocol).dump(obj)
 AttributeError: Can't pickle local object 'metrics_patch.._MarshalService'
 Process finished with exit code 1
 Environment:
 
 OS: MacOS 10.15.7
 Python Version Python 3.8
 
 &lt;denchmark-code&gt;{
  "date": "2020-10-15T09:08:27-0700",
  "dirty": false,
  "error": null,
  "full-revisionid": "25c319d8161629d695eaecf6418e50fea6535d6e",
  "version": "0.9.2"
 }
 &lt;/denchmark-code&gt;
 
 Additional context
 No
 	</description>
 	<comments>
 		<comment id='1' author='xylophone21' date='2020-12-08T08:16:14Z'>
 		Maybe related: &lt;denchmark-link:https://github.com/bentoml/BentoML/pull/995&gt;#995&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='xylophone21' date='2020-12-08T09:09:42Z'>
 		I reproduced this on macOS with Python3.8. I'll draft a new PR like &lt;denchmark-link:https://github.com/bentoml/BentoML/pull/995&gt;#995&lt;/denchmark-link&gt;
 . Before the new release, you can use
 &lt;denchmark-code&gt;bentoml serve-gunicorn PyTorchFashionClassifier:latest --enable-microbatch --workers 1
 &lt;/denchmark-code&gt;
 
 or Python 3.7 instead.
 		</comment>
 	</comments>
 </bug>
<commit id='1d885119ae8e560b4fbd60f0de80f08383ab0a05' author='bojiang' date='2020-12-23 19:06:40-08:00'>
 	<dmm_unit complexity='1.0' interfacing='0.08333333333333333' size='0.5833333333333334'></dmm_unit>
 	<modification change_type='MODIFY' old_name='bentoml\marshal\dispatcher.py' new_name='bentoml\marshal\dispatcher.py'>
 		<file_info nloc='175' complexity='47' token_count='1147'></file_info>
 		<method name='__call__' parameters='self,callback'>
 				<method_info nloc='5' complexity='1' token_count='24' nesting_level='1' start_line='131' end_line='146'></method_info>
 			<added_lines>134</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>2,3,6,8</added_lines>
 			<deleted_lines>3,5,12</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='bentoml\marshal\marshal.py' new_name='bentoml\marshal\marshal.py'>
 		<file_info nloc='273' complexity='34' token_count='1629'></file_info>
 		<method name='metrics_patch.__init__' parameters='self,args,kwargs'>
 				<method_info nloc='42' complexity='3' token_count='227' nesting_level='2' start_line='39' end_line='83'></method_info>
 			<added_lines>40,41,42,43,44,45</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='add_batch_handler' parameters='self,api_name,max_latency,max_batch_size'>
 				<method_info nloc='9' complexity='2' token_count='67' nesting_level='1' start_line='178' end_line='194'></method_info>
 			<added_lines>193</added_lines>
 			<deleted_lines>187</deleted_lines>
 		</method>
 		<method name='metrics_patch' parameters='cls'>
 				<method_info nloc='6' complexity='1' token_count='21' nesting_level='0' start_line='37' end_line='117'></method_info>
 			<added_lines>40,41,42,43,44,45</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>16</added_lines>
 			<deleted_lines>20</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='bentoml\server\__init__.py' new_name='bentoml\server\__init__.py'>
 		<file_info nloc='134' complexity='7' token_count='598'></file_info>
 		<method name='start_dev_batching_server' parameters='str,int,int,int,int'>
 				<method_info nloc='6' complexity='1' token_count='22' nesting_level='0' start_line='101' end_line='106'></method_info>
 			<added_lines>101,102,103,104,105,106</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>55,56,64,65,74,75,76,77,78,79,80,81,82,83,84,86,87,88,89,90,91,94,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,138,139</added_lines>
 			<deleted_lines>20,58,61,62,64,75,76,77,78,79,80,81,82,83,84,86,108</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
