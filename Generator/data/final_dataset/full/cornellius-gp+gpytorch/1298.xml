<bug_data>
<bug id='1298' author='samuelstanton' open_date='2020-10-01T17:58:48Z' closed_time='2020-10-05T21:28:22Z'>
 	<summary>[Bug]</summary>
 	<description>
 &lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 Batched fixed noise GPs fail when the preconditioning threshold is reached. The concatenation in this line fails with a shape error.
 &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/blob/master/gpytorch/lazy/added_diag_lazy_tensor.py#L126&gt;https://github.com/cornellius-gp/gpytorch/blob/master/gpytorch/lazy/added_diag_lazy_tensor.py#L126&lt;/denchmark-link&gt;
 
 &lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;
 
 ** Code snippet to reproduce **
 import torch
 import gpytorch
 
 class BatchFixedNoiseGP(gpytorch.models.GP):
     def __init__(self, init_x, init_y, noise, batch_shape):
         super().__init__()
         self.mean_module = gpytorch.means.ZeroMean()
         self.covar_module = gpytorch.kernels.RBFKernel(batch_shape=batch_shape)
         self.likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(noise)
         
     def forward(self, inputs):
         mean = self.mean_module(inputs)
         covar = self.covar_module(inputs)
         return gpytorch.distributions.MultivariateNormal(mean, covar)
 
 batch_shape = [2]
 train_x = torch.randn(*batch_shape, 101, 3)
 train_y = torch.randn(*batch_shape, 101)
 train_noise = torch.rand(*batch_shape, 101)
 
 gp = BatchFixedNoiseGP(train_x, train_y, train_noise, batch_shape)
 mll = gpytorch.mlls.ExactMarginalLogLikelihood(gp.likelihood, gp)
 
 with gpytorch.settings.max_cholesky_size(100), gpytorch.settings.min_preconditioning_size(100):
     train_dist = gp(train_x)
     loss = -mll(train_dist, train_y).sum()
 ** Stack trace/error message **
 &lt;denchmark-code&gt;RuntimeError                              Traceback (most recent call last)
 &lt;ipython-input-4-9e151e2de37a&gt; in &lt;module&gt;
      24 with gpytorch.settings.max_cholesky_size(100), gpytorch.settings.min_preconditioning_size(100):
      25     train_dist = gp(train_x)
 ---&gt; 26     loss = -mll(train_dist, train_y).sum()
 
 ~/Code/gpytorch/gpytorch/module.py in __call__(self, *inputs, **kwargs)
      26 
      27     def __call__(self, *inputs, **kwargs):
 ---&gt; 28         outputs = self.forward(*inputs, **kwargs)
      29         if isinstance(outputs, list):
      30             return [_validate_module_outputs(output) for output in outputs]
 
 ~/Code/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py in forward(self, function_dist, target, *params)
      49         # Get the log prob of the marginal distribution
      50         output = self.likelihood(function_dist, *params)
 ---&gt; 51         res = output.log_prob(target)
      52 
      53         # Add additional terms (SGPR / learned inducing points, heteroskedastic likelihood models)
 
 ~/Code/gpytorch/gpytorch/distributions/multivariate_normal.py in log_prob(self, value)
     138 
     139         # Get log determininat and first part of quadratic form
 --&gt; 140         inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)
     141 
     142         res = -0.5 * sum([inv_quad, logdet, diff.size(-1) * math.log(2 * math.pi)])
 
 ~/Code/gpytorch/gpytorch/lazy/lazy_tensor.py in inv_quad_logdet(self, inv_quad_rhs, logdet, reduce_inv_quad)
    1069             probe_vectors,
    1070             probe_vector_norms,
 -&gt; 1071             *args,
    1072         )
    1073 
 
 ~/Code/gpytorch/gpytorch/functions/_inv_quad_log_det.py in forward(ctx, representation_tree, dtype, device, matrix_shape, batch_shape, inv_quad, logdet, probe_vectors, probe_vector_norms, *args)
      65         lazy_tsr = ctx.representation_tree(*matrix_args)
      66         with torch.no_grad():
 ---&gt; 67             preconditioner, precond_lt, logdet_correction = lazy_tsr._preconditioner()
      68 
      69         ctx.preconditioner = preconditioner
 
 ~/Code/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py in _preconditioner(self)
      84                 )
      85                 return None, None, None
 ---&gt; 86             self._init_cache()
      87 
      88         # NOTE: We cannot memoize this precondition closure as it causes a memory leak
 
 ~/Code/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py in _init_cache(self)
     107             self._init_cache_for_constant_diag(eye, batch_shape, n, k)
     108         else:
 --&gt; 109             self._init_cache_for_non_constant_diag(eye, batch_shape, n)
     110 
     111         self._precond_lt = PsdSumLazyTensor(RootLazyTensor(self._piv_chol_self), self._diag_tensor)
 
 ~/Code/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py in _init_cache_for_non_constant_diag(self, eye, batch_shape, n)
     125         # With non-constant diagonals, we cant factor out the noise as easily
     126         # eye = eye.expand(*batch_shape, -1, -1)
 --&gt; 127         self._q_cache, self._r_cache = torch.qr(torch.cat((self._piv_chol_self / self._noise.sqrt(), eye)))
     128         self._q_cache = self._q_cache[..., :n, :] / self._noise.sqrt()
     129 
 
 RuntimeError: Tensors must have same number of dimensions: got 3 and 2
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;
 
 Everything works fine until the preconditioning threshold is reached. Obviously one would hope that it would continue to work.
 &lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;
 
 Please complete the following information:
 GPyTorch Version: 1.2.0
 PyTorch Version:  '1.6.0.dev20200522'
 OS: Ubuntu 16.04 LTS
 &lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;
 
 This appears to fix the problem
 &lt;denchmark-code&gt;    def _init_cache_for_non_constant_diag(self, eye, batch_shape, n):
         # With non-constant diagonals, we cant factor out the noise as easily
         eye = eye.expand(*batch_shape, -1, -1)
         self._q_cache, self._r_cache = torch.qr(torch.cat((self._piv_chol_self / self._noise.sqrt(), eye), dim=-2))
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='samuelstanton' date='2020-10-05T12:33:42Z'>
 		Thanks for the catch. I'll take a look
 		</comment>
 	</comments>
 </bug>
<commit id='192200f98d841661160f2bd83354866a853239e0' author='Geoff Pleiss' date='2020-10-05 13:20:43+00:00'>
 	<dmm_unit complexity='0.0' interfacing='0.0' size='0.75'></dmm_unit>
 	<modification change_type='MODIFY' old_name='gpytorch\lazy\added_diag_lazy_tensor.py' new_name='gpytorch\lazy\added_diag_lazy_tensor.py'>
 		<file_info nloc='126' complexity='28' token_count='1203'></file_info>
 		<method name='_init_cache_for_non_constant_diag' parameters='self,eye,batch_shape,n'>
 				<method_info nloc='6' complexity='2' token_count='157' nesting_level='1' start_line='147' end_line='155'></method_info>
 			<added_lines>149,152</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_preconditioner' parameters='self'>
 				<method_info nloc='32' complexity='6' token_count='138' nesting_level='1' start_line='70' end_line='117'></method_info>
 			<added_lines>71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,93,94,95,96,97,111</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_init_cache' parameters='self'>
 				<method_info nloc='12' complexity='2' token_count='153' nesting_level='1' start_line='119' end_line='134'></method_info>
 			<added_lines>127</added_lines>
 			<deleted_lines>126</deleted_lines>
 		</method>
 		<method name='_preconditioner.precondition_closure' parameters='tensor'>
 				<method_info nloc='5' complexity='2' token_count='60' nesting_level='2' start_line='110' end_line='115'></method_info>
 			<added_lines>111</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='test\examples\test_white_noise_regression.py' new_name='test\examples\test_white_noise_regression.py'>
 		<file_info nloc='122' complexity='24' token_count='1103'></file_info>
 		<method name='test_posterior_latent_gp_and_likelihood_without_optimization' parameters='self,cuda'>
 				<method_info nloc='18' complexity='2' token_count='214' nesting_level='1' start_line='56' end_line='85'></method_info>
 			<added_lines>56,78</added_lines>
 			<deleted_lines>56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85</deleted_lines>
 		</method>
 		<method name='test_posterior_latent_gp_and_likelihood_without_optimization_cuda' parameters='self'>
 				<method_info nloc='4' complexity='2' token_count='27' nesting_level='1' start_line='87' end_line='90'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>87,88,89,90</deleted_lines>
 		</method>
 		<method name='test_posterior_latent_gp_and_likelihood_with_optimization' parameters='self,cuda'>
 				<method_info nloc='32' complexity='4' token_count='287' nesting_level='1' start_line='92' end_line='134'></method_info>
 			<added_lines>100,101,102</added_lines>
 			<deleted_lines>92,114</deleted_lines>
 		</method>
 		<method name='test_posterior_latent_gp_and_likelihood_with_optimization_with_cg' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='14' nesting_level='1' start_line='100' end_line='101'></method_info>
 			<added_lines>100,101</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_posterior_latent_gp_and_likelihood_with_optimization' parameters='self,cuda,cg'>
 				<method_info nloc='32' complexity='5' token_count='304' nesting_level='1' start_line='56' end_line='98'></method_info>
 			<added_lines>56,78</added_lines>
 			<deleted_lines>56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
