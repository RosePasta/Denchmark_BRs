<bug_data>
<bug id='822' author='Balandat' open_date='2019-08-01T20:39:09Z' closed_time='2019-08-02T18:25:19Z'>
 	<summary>[Bug] Upstream changes to tensor comparisons breaks things</summary>
 	<description>
 &lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 After &lt;denchmark-link:https://github.com/pytorch/pytorch/pull/21113&gt;pytorch/pytorch#21113&lt;/denchmark-link&gt;
  a bunch of tests are failing b/c of the change in tensor comparison behavior (return type from uint8 to bool). Creating this issue to track the fix.
 	</description>
 	<comments>
 		<comment id='1' author='Balandat' date='2019-08-01T20:48:27Z'>
 		It appears most of this happens in batch_symeig where we do 1 - mask - will put up a fix shortly.
 		</comment>
 		<comment id='2' author='Balandat' date='2019-08-01T21:07:08Z'>
 		&lt;denchmark-link:https://github.com/Balandat&gt;@Balandat&lt;/denchmark-link&gt;
 , we have introduced '~' operator for masks to fix this issue. You should be able to just replace '1-mask' with '~mask'
 		</comment>
 		<comment id='3' author='Balandat' date='2019-08-01T21:13:46Z'>
 		Sure that works. There are a few other places in the code where this causes failures, but they should be easy enough to fix.
 		</comment>
 		<comment id='4' author='Balandat' date='2019-08-01T22:04:57Z'>
 		Fixed by &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/pull/823&gt;#823&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='5' author='Balandat' date='2019-08-01T22:47:19Z'>
 		Ok of course there are now a bunch more warnings emitted when using uint8 on pytorch master, so all the tests that count the warnings fail there ...
 		</comment>
 		<comment id='6' author='Balandat' date='2019-08-02T00:30:50Z'>
 		It's worse than that, the change actually breaks the jit script code for _jit_linear_cg_updates. So the correct thing is to use bool everywhere, and fix the tests that count the warnings...
 		</comment>
 		<comment id='7' author='Balandat' date='2019-08-02T00:38:09Z'>
 		Won't be able to fix all this properly right now, deferring to the above issue.
 		</comment>
 		<comment id='8' author='Balandat' date='2019-08-02T00:42:49Z'>
 		&lt;denchmark-link:https://github.com/Balandat&gt;@Balandat&lt;/denchmark-link&gt;
  I'll take a look once the nightly build comes out and I can repro the errors without having to build pytorch from source.
 		</comment>
 		<comment id='9' author='Balandat' date='2019-08-02T01:02:20Z'>
 		&lt;denchmark-link:https://github.com/jacobrgardner&gt;@jacobrgardner&lt;/denchmark-link&gt;
  sounds good, if you base this on &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/pull/825&gt;#825&lt;/denchmark-link&gt;
  at least you won't have to deal with the warnings.
 		</comment>
 		<comment id='10' author='Balandat' date='2019-08-02T01:29:27Z'>
 		Fwiw, here's a notebook that shows the failure (commented out the jit decorators locally).
 It only happens when broadcasting. &lt;denchmark-link:https://github.com/izdeby&gt;@izdeby&lt;/denchmark-link&gt;
 , maybe broadcasting semantics for bool tensors are different?
 &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/files/3459249/test_broadcasting_issues.ipynb.txt&gt;test_broadcasting_issues.ipynb.txt&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='11' author='Balandat' date='2019-08-02T16:51:09Z'>
 		&lt;denchmark-link:https://github.com/Balandat&gt;@Balandat&lt;/denchmark-link&gt;
 , what exactly failed in broadcasting for you? there shouldn't be any difference in broadcasting semantics for bool tensors
 		</comment>
 		<comment id='12' author='Balandat' date='2019-08-02T17:10:27Z'>
 		&lt;denchmark-link:https://github.com/izdeby&gt;@izdeby&lt;/denchmark-link&gt;
  looks like this is a different issue related to changing sizes in , see &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/pull/827&gt;#827&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='13' author='Balandat' date='2019-08-02T17:20:02Z'>
 		For now do we think we could define like a gpytorch.bool type to be torch.bool if that will work (i.e. we are on master), and torch.uint8 otherwise?
 		</comment>
 		<comment id='14' author='Balandat' date='2019-08-02T17:21:00Z'>
 		We could similarly define gpytorch.not to do ~ and 1 - ... respectively
 		</comment>
 		<comment id='15' author='Balandat' date='2019-08-02T17:29:07Z'>
 		So ~ will work even with uint8, so there is no need for that. But we can do the gpytorch.bool thing to avoid the mass of warnings
 		</comment>
 		<comment id='16' author='Balandat' date='2019-08-02T17:46:59Z'>
 		made this change in the latest update to &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/pull/827&gt;#827&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='17' author='Balandat' date='2019-08-02T18:25:19Z'>
 		Reasonably fixed for now via &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/pull/827&gt;#827&lt;/denchmark-link&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='b5ca4235cef1eff64bb063945a422c1ebf178a1b' author='Max Balandat' date='2019-08-01 15:00:45-07:00'>
 	<dmm_unit complexity='0.0' interfacing='1.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='gpytorch\functions\_log_normal_cdf.py' new_name='gpytorch\functions\_log_normal_cdf.py'>
 		<file_info nloc='88' complexity='8' token_count='694'></file_info>
 		<method name='backward' parameters='ctx,grad_output'>
 				<method_info nloc='10' complexity='2' token_count='147' nesting_level='1' start_line='100' end_line='114'></method_info>
 			<added_lines>105</added_lines>
 			<deleted_lines>105</deleted_lines>
 		</method>
 		<method name='forward' parameters='ctx,z'>
 				<method_info nloc='71' complexity='6' token_count='519' nesting_level='1' start_line='11' end_line='97'></method_info>
 			<added_lines>63</added_lines>
 			<deleted_lines>63</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='gpytorch\kernels\grid_interpolation_kernel.py' new_name='gpytorch\kernels\grid_interpolation_kernel.py'>
 		<file_info nloc='160' complexity='29' token_count='1016'></file_info>
 		<method name='__init__' parameters='self,base_kernel,grid_size,num_dims,grid_bounds,active_dims'>
 				<method_info nloc='27' complexity='6' token_count='180' nesting_level='1' start_line='65' end_line='97'></method_info>
 			<added_lines>97</added_lines>
 			<deleted_lines>97</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='gpytorch\lazy\cat_lazy_tensor.py' new_name='gpytorch\lazy\cat_lazy_tensor.py'>
 		<file_info nloc='294' complexity='99' token_count='2817'></file_info>
 		<method name='_getitem' parameters='self,row_index,col_index,batch_indices'>
 				<method_info nloc='45' complexity='14' token_count='504' nesting_level='1' start_line='171' end_line='231'></method_info>
 			<added_lines>193</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_get_indices' parameters='self,row_index,col_index,batch_indices'>
 				<method_info nloc='27' complexity='10' token_count='358' nesting_level='1' start_line='132' end_line='169'></method_info>
 			<added_lines>140</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='gpytorch\utils\eig.py' new_name='gpytorch\utils\eig.py'>
 		<file_info nloc='19' complexity='3' token_count='237'></file_info>
 		<method name='batch_symeig' parameters='mat'>
 				<method_info nloc='18' complexity='3' token_count='234' nesting_level='0' start_line='6' end_line='30'></method_info>
 			<added_lines>26</added_lines>
 			<deleted_lines>26</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='gpytorch\utils\linear_cg.py' new_name='gpytorch\utils\linear_cg.py'>
 		<file_info nloc='189' complexity='4' token_count='1434'></file_info>
 		<modified_lines>
 			<added_lines>182,190</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='gpytorch\utils\sparse.py' new_name='gpytorch\utils\sparse.py'>
 		<file_info nloc='175' complexity='38' token_count='1906'></file_info>
 		<method name='sparse_getitem' parameters='sparse,idxs'>
 				<method_info nloc='49' complexity='15' token_count='507' nesting_level='0' start_line='137' end_line='195'></method_info>
 			<added_lines>157,158,159,160,180,181,182</added_lines>
 			<deleted_lines>157,158,178,179,180</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
