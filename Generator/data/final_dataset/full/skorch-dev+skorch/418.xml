<bug_data>
<bug id='418' author='thomasjpfan' open_date='2019-01-04T19:34:57Z' closed_time='2019-05-01T12:52:09Z'>
 	<summary>LRScheduler's batch_idx_ includes validation batches</summary>
 	<description>
 LRScheduler's batch_idx_ counts both validation and training batches. This means the learning rate is updated CyclicLR during validation batch steps.
 	</description>
 	<comments>
 		<comment id='1' author='thomasjpfan' date='2019-01-08T17:24:27Z'>
 		I would like to use the train_loss or train_batch_size in the history as a proxy for how many training iterations have occurred. The train_loss and train_batch_size are recorded by default in the fit_loop.
 This can be done by adding a parameter to LRScheduler called train_batch_indicator with default value: train_loss.
 I think this would cover 99% of the use cases.
 		</comment>
 		<comment id='2' author='thomasjpfan' date='2019-01-09T22:31:53Z'>
 		
 I would like to use the train_loss or train_batch_size in the history as a proxy
 
 Let's try to prevent using proxies, since this creates unnecessary coupling. Would it be enough to look at the  argument passed to  (&lt;denchmark-link:https://github.com/dnouri/skorch/blob/5542209d7d3d1b30bc6bfb87e11b0278fb27f89e/skorch/callbacks/lr_scheduler.py#L148&gt;here&lt;/denchmark-link&gt;
 ) and only increment if ?
 		</comment>
 		<comment id='3' author='thomasjpfan' date='2019-01-09T23:40:23Z'>
 		Using training will work as long as the learning rate scheduler records the number of training batches in the history. This way the total number of training batches can be calculated when training resumes.
 		</comment>
 		<comment id='4' author='thomasjpfan' date='2019-01-10T16:06:06Z'>
 		
 Using training will work as long as the learning rate scheduler records the number of training batches in the history
 
 You mean it must be in the history in case that after loading the model only, you would like to continue training or are there other cases that I overlook? In that case, should we rather have the net itself count the number of training batches?
 		</comment>
 		<comment id='5' author='thomasjpfan' date='2019-01-10T16:26:53Z'>
 		
 in case that after loading the model only, you would like to continue training
 
 This is the use case I am considering.
 
 should we rather have the net itself count the number of training batches?
 
 That works too. We can place the counting logic  in NeuralNet.on_batch_end.
 		</comment>
 		<comment id='6' author='thomasjpfan' date='2019-03-15T15:09:23Z'>
 		If I understand correctly, the current proposal is to have two attributes in NeuralNet:
 
 batch_count_train_
 batch_count_valid_
 both holding the current count of processed batches. The upper bound of this index is not known in advance (since there might be a dynamic amount of batches per epoch) and we can't guarantee that this number is evenly divisible by a fixed batch size (same argument). The last points might get in the way when users expect this to hold (i.e. num_epochs = (batch_count_train + batch_count_valid)/batch_size or something similar).
 
 The purpose of such a counter is to have a global step count. This does not solve the problem of having no local step count, i.e. there is no way to know how many training batches currently have passed without coupling to train_loss in the history (batch_count = len(history[-1, 'batches', :, 'train_loss'])).
 We could, of course, introduce 4 variables, one for global and one for local count or track all local counts in a list but this is just reinventing the wheel (history), I guess.
 Maybe we provide something like this:
 global_train_steps = sum(history[:, 'batches', :, 'is_training'])
 local_train_steps = sum(history[-1, 'batches', :, 'is_training'])
 for which we would need to introduce a new history key (is_training). What I dislike about this is that it feels incredibly wasteful (num_epochs * num_batches * m byte to count 2 numbers).
 So in the end I think that introducing storage to NeuralNet is preferable. We could think about adding these counts to history instead (e.g. as attributes, history.current_train_steps).
 		</comment>
 		<comment id='7' author='thomasjpfan' date='2019-03-16T22:47:51Z'>
 		I would prefer this information to be in the history. This would keep track of state when the history gets loaded from a file.
 What do you think of adding history[:, 'batch_count_train'] and history[:, 'batch_count_valid'] to keep track of the count?
 		</comment>
 		<comment id='8' author='thomasjpfan' date='2019-03-17T10:38:19Z'>
 		
 2. The last points might get in the way when users expect this to hold (i.e. `num_epochs = (batch_count_train + batch_count_valid)/batch_size` or something similar).
 
 
 Do you have an example in mind where someone would make that assumption? Would it be enough to document the behavior somewhere ("If you want to know the number of training batches, use this key from history...")
 
 I would prefer this information to be in the history.
 
 History seems to be the right place for this kind of information.
 
 What do you think of adding history[:, 'batch_count_train'] and history[:, 'batch_count_valid'] to keep track of the count?
 
 This looks reasonable. That number would be the non-cumulative number of batches and if you need the total, you should sum them. In theory, the number of batches can be misleading when not taking account of 'train_batch_size' and 'valid_batch_size', but for most practical purposes, that might not be too important.
 		</comment>
 	</comments>
 </bug>
<commit id='e1dc86d5f6a17a63e52964bf12ab9a55dbe17590' author='Thomas J Fan' date='2019-05-01 14:52:09+02:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGES.md' new_name='CHANGES.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>15</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='skorch\callbacks\logging.py' new_name='skorch\callbacks\logging.py'>
 		<file_info nloc='261' complexity='61' token_count='1203'></file_info>
 		<method name='_sorted_keys' parameters='self,keys'>
 				<method_info nloc='19' complexity='14' token_count='146' nesting_level='1' start_line='132' end_line='161'></method_info>
 			<added_lines>151</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='skorch\callbacks\lr_scheduler.py' new_name='skorch\callbacks\lr_scheduler.py'>
 		<file_info nloc='347' complexity='51' token_count='1502'></file_info>
 		<method name='on_batch_end' parameters='self,net,kwargs'>
 				<method_info nloc='2' complexity='1' token_count='15' nesting_level='1' start_line='148' end_line='149'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>148,149</deleted_lines>
 		</method>
 		<method name='on_batch_begin' parameters='self,net,training,kwargs'>
 				<method_info nloc='7' complexity='4' token_count='45' nesting_level='1' start_line='144' end_line='150'></method_info>
 			<added_lines>144,146</added_lines>
 			<deleted_lines>148,149</deleted_lines>
 		</method>
 		<method name='on_batch_end' parameters='self,net,training,kwargs'>
 				<method_info nloc='3' complexity='2' token_count='20' nesting_level='1' start_line='152' end_line='154'></method_info>
 			<added_lines>152,153,154</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='on_batch_begin' parameters='self,net,kwargs'>
 				<method_info nloc='6' complexity='3' token_count='41' nesting_level='1' start_line='141' end_line='146'></method_info>
 			<added_lines>144,146</added_lines>
 			<deleted_lines>141</deleted_lines>
 		</method>
 		<method name='on_train_begin' parameters='self,net,kwargs'>
 				<method_info nloc='9' complexity='4' token_count='76' nesting_level='1' start_line='117' end_line='125'></method_info>
 			<added_lines>119,120,121,122</added_lines>
 			<deleted_lines>119</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='skorch\net.py' new_name='skorch\net.py'>
 		<file_info nloc='839' complexity='182' token_count='4837'></file_info>
 		<method name='fit_loop' parameters='self,X,y,epochs,fit_params'>
 				<method_info nloc='40' complexity='8' token_count='377' nesting_level='1' start_line='686' end_line='767'></method_info>
 			<added_lines>738,747,748,754,763,764</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='skorch\tests\callbacks\test_lr_scheduler.py' new_name='skorch\tests\callbacks\test_lr_scheduler.py'>
 		<file_info nloc='409' complexity='53' token_count='3179'></file_info>
 		<method name='test_lr_callback_batch_steps_correctly_fallback' parameters='self,classifier_module,classifier_data,policy,kwargs'>
 				<method_info nloc='6' complexity='1' token_count='12' nesting_level='1' start_line='129' end_line='134'></method_info>
 			<added_lines>129,130,131,132,133,134</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>111,112,117,118,119,120,121,122,123,124,125,126,127,128,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,170</added_lines>
 			<deleted_lines>107,116,118</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='skorch\tests\test_net.py' new_name='skorch\tests\test_net.py'>
 		<file_info nloc='1676' complexity='205' token_count='13287'></file_info>
 		<method name='test_batch_count' parameters='self,net_cls,module_cls,data,batch_size'>
 				<method_info nloc='8' complexity='1' token_count='96' nesting_level='1' start_line='2078' end_line='2088'></method_info>
 			<added_lines>2078,2079,2080,2081,2082,2083,2084,2085,2086,2087,2088</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>2077,2089</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
