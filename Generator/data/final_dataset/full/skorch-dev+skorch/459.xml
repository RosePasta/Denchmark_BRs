<bug_data>
<bug id='459' author='gillesdami' open_date='2019-04-18T09:53:22Z' closed_time='2019-05-02T17:36:54Z'>
 	<summary>incompatibility of skorch.callbacks.TrainEndCheckpoint with sklearn.compose.TransformedTargetRegressor</summary>
 	<description>
 skorch version: 0.5.0.post0
 Minimal example inspired from the docs:
 import numpy as np
 from sklearn.compose import TransformedTargetRegressor
 from sklearn.datasets import make_classification
 from sklearn.preprocessing import StandardScaler
 from skorch.callbacks import TrainEndCheckpoint
 from skorch import NeuralNetClassifier
 from torch import nn
 
 X, y = make_classification(1000, 10, n_informative=5, random_state=0)
 X = X.astype(np.float32)
 y = y.astype(np.int64)
 
 class MyModule(nn.Sequential):
     def __init__(self, num_units=10):
         super().__init__(
             nn.Linear(10, num_units),
             nn.ReLU(inplace=True),
             nn.Dropout(0.2),
             nn.Linear(num_units, 10),
             nn.Linear(10, 2),
             nn.Softmax(dim=-1)
         )
 
 train_end_cp = TrainEndCheckpoint(dirname='exp1', fn_prefix='train_end_')
 net = NeuralNetClassifier(
     MyModule, lr=0.5, callbacks=[train_end_cp]
 )
 
 net = TransformedTargetRegressor(net, func=lambda x: x, inverse_func=lambda x: x)
 
 _ = net.fit(X, y)
 error returned:
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "test.py", line 31, in &lt;module&gt;
     _ = net.fit(X, y)
   File "/home/damien/.local/lib/python3.6/site-packages/sklearn/compose/_target.py", line 198, in fit
     self.regressor_ = clone(self.regressor)
   File "/home/damien/.local/lib/python3.6/site-packages/sklearn/base.py", line 64, in clone
     new_object_params[name] = clone(param, safe=False)
   File "/home/damien/.local/lib/python3.6/site-packages/sklearn/base.py", line 52, in clone
     return estimator_type([clone(e, safe=safe) for e in estimator])
   File "/home/damien/.local/lib/python3.6/site-packages/sklearn/base.py", line 52, in &lt;listcomp&gt;
     return estimator_type([clone(e, safe=safe) for e in estimator])
   File "/home/damien/.local/lib/python3.6/site-packages/sklearn/base.py", line 65, in clone
     new_object = klass(**new_object_params)
 TypeError: __init__() got an unexpected keyword argument 'monitor
 &lt;/denchmark-code&gt;
 
 explanation:
 For some reason, sklearn will make a copy of the callbacks when using TransformedTargetRegressor. However they does not respect the BaseEstimator specs.
 fix:
 The callbacks classes inherited from Callback should have **args in their __init__ function to ensure that they accept all Callback.__init__ parameters.
 May I open a PR ?
 	</description>
 	<comments>
 		<comment id='1' author='gillesdami' date='2019-04-19T14:42:31Z'>
 		Thanks for reporting the bug. I believe the solution would not be optimal, it is better to be explicit about parameters in this case.
 There are 3 parameters missing from TrainEndCheckpoint.__init__, target, monitor, and event_name. They are not there because they are not needed but at the same time, they should be there because TrainEndCheckpoint inherits from Checkpoint.
 The solution would be either to add these parameters with the new defaults -- but it would not really make sense to have them on TrainEndCheckpoint -- or to move the common functionality out of Checkpoint, so that it can be recycled without inheritance -- but this would require the functions to have a lot of parameters.
 I don't have a strong preference, what do others think? &lt;denchmark-link:https://github.com/gillesdami&gt;@gillesdami&lt;/denchmark-link&gt;
  If you would like to work on one of the solutions, or a better one, go ahead.
 		</comment>
 		<comment id='2' author='gillesdami' date='2019-04-23T18:12:36Z'>
 		I think adding a **kwargs to TrainEndCheckpoint (with a comment) would be the cleanest fix.
 		</comment>
 		<comment id='3' author='gillesdami' date='2019-04-24T07:59:54Z'>
 		Adding explicitly target, monitor and event_name would confuse everyone. In my opinion, we should add **kwargs or refactor the code to avoid inheritance.
 I'd be happy to work on a solution, I'll open a PR once we've chosen the way to go.
 		</comment>
 		<comment id='4' author='gillesdami' date='2019-04-25T22:08:07Z'>
 		The reason why I would avoid **kwargs if possible is that it can be confusing to the user as well (what happens with those kwargs?) and because they silently swallow typos of correct arguments. Furthermore, sklearn discourages the use of **kwargs during init.
 Another solution would be to add the three missing parameters but make a check that their values are what is needed (this check would ideally also work after a set_params). Otherwise, I prefer the refactoring approach, though that is probably the most difficult one to get right.
 		</comment>
 	</comments>
 </bug>
<commit id='17256a1325ea463003a0e927a09a11065dc0b21a' author='Thomas J Fan' date='2019-05-02 19:36:53+02:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGES.md' new_name='CHANGES.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>31</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='skorch\callbacks\training.py' new_name='skorch\callbacks\training.py'>
 		<file_info nloc='629' complexity='81' token_count='1999'></file_info>
 		<method name='initialize' parameters='self'>
 				<method_info nloc='12' complexity='1' token_count='68' nesting_level='1' start_line='699' end_line='710'></method_info>
 			<added_lines>699,700,702,703,704,705,706,707,709,710</added_lines>
 			<deleted_lines>699,701,702,703,704,705,708,709</deleted_lines>
 		</method>
 		<method name='on_train_end' parameters='self,net,kwargs'>
 				<method_info nloc='3' complexity='1' token_count='30' nesting_level='1' start_line='712' end_line='714'></method_info>
 			<added_lines>713,714</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='on_epoch_end' parameters='self,net,kwargs'>
 				<method_info nloc='2' complexity='1' token_count='11' nesting_level='1' start_line='704' end_line='705'></method_info>
 			<added_lines>704,705</added_lines>
 			<deleted_lines>704,705</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>607,691,692,693,694,695,696,697,698</added_lines>
 			<deleted_lines>607,684,692,694,695,696,697,698</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='skorch\tests\callbacks\test_training.py' new_name='skorch\tests\callbacks\test_training.py'>
 		<file_info nloc='718' complexity='60' token_count='4756'></file_info>
 		<method name='test_cloneable' parameters='self,finalcheckpoint_cls'>
 				<method_info nloc='3' complexity='1' token_count='16' nesting_level='1' start_line='896' end_line='899'></method_info>
 			<added_lines>896,897,898,899</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>10,895</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
