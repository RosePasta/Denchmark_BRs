<bug_data>
<bug id='3278' author='s-rog' open_date='2020-08-31T02:42:47Z' closed_time='2020-09-23T08:38:34Z'>
 	<summary>on_fit_start not triggering (master)</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 on_fit_start is not being triggered on master as of &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/f46318ebfeb785a659c49091a6871584ccde3ee1&gt;f46318e&lt;/denchmark-link&gt;
 
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 Steps to reproduce the behavior:
 
 Install master
 Run template with on_fit_start added (included below)
 (identical behavior for single gpu, ddp_spawn and ddp)
 
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;"""
 Runs a model on a single node across multiple gpus.
 """
 import os
 from argparse import ArgumentParser
 
 from pl_examples.models.lightning_template import LightningTemplateModel
 from pytorch_lightning import Trainer, seed_everything
 
 seed_everything(234)
 
 class custom_template(LightningTemplateModel):
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
     
     def on_epoch_start(self):
         print("on_epoch_start")
 
     def on_fit_start(self):
         print("on_fit_start")
 
 def main(args):
     """ Main training routine specific for this project. """
     # ------------------------
     # 1 INIT LIGHTNING MODEL
     # ------------------------
     model = custom_template(**vars(args))
 
     # ------------------------
     # 2 INIT TRAINER
     # ------------------------
     trainer = Trainer.from_argparse_args(args)
 
     # ------------------------
     # 3 START TRAINING
     # ------------------------
     trainer.fit(model)
 
 
 def run_cli():
     # ------------------------
     # TRAINING ARGUMENTS
     # ------------------------
     # these are project-wide arguments
     root_dir = os.path.dirname(os.path.realpath(__file__))
     parent_parser = ArgumentParser(add_help=False)
 
     # each LightningModule defines arguments relevant to it
     parser = LightningTemplateModel.add_model_specific_args(parent_parser, root_dir)
     parser = Trainer.add_argparse_args(parser)
     parser.set_defaults(gpus=2, distributed_backend='ddp')
     args = parser.parse_args()
 
     # ---------------------
     # RUN TRAINING
     # ---------------------
     main(args)
 
 
 if __name__ == '__main__':
     run_cli()
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 "on_fit_start" should be printed along with "on_epoch_start" but only "on_epoch_start" is printed
 	</description>
 	<comments>
 		<comment id='1' author='s-rog' date='2020-08-31T20:26:06Z'>
 		It's a bug I think since at this point neither model nor accelerator_backend is assigned to the trainer.
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/trainer.py
 
 
          Line 984
       in
       caf7893
 
 
 
 
 
 
  self.call_hook('on_fit_start', model) 
 
 
 
 
 
 Should be called after
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/trainer.py
 
 
          Line 1008
       in
       caf7893
 
 
 
 
 
 
  self.accelerator_backend.setup(model) 
 
 
 
 
 
 		</comment>
 		<comment id='2' author='s-rog' date='2020-09-02T15:46:11Z'>
 		I'll move the hook and test it out, if it works I'll do a PR
 		</comment>
 		<comment id='3' author='s-rog' date='2020-09-03T03:56:49Z'>
 		&lt;denchmark-link:https://github.com/rohitgr7&gt;@rohitgr7&lt;/denchmark-link&gt;
  moving the hook can work but causes another peculiar issue
 &lt;denchmark-code&gt;  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/states.py", line 48, in wrapped_fn
     result = fn(self, *args, **kwargs)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 1029, in fit
     self.call_hook('on_fit_start', model)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 1384, in call_hook
     output = hook_fx(*args, **kwargs)
 TypeError: on_fit_start() takes 1 positional argument but 2 were given
 &lt;/denchmark-code&gt;
 
 more specifically on_fit_start() gets called with 2 modules, still locating the source of this extra arg/module
 Edit:
 So call_hook has the following signature def call_hook(self, hook_name, *args, **kwargs) and within call_hook there is trainer_hook(*args, **kwargs) and hook_fx(*args, **kwargs) (in this case: on_fit_start) both are called with the same args but trainer_hook needs a model arg while hook_fx/on_fit_start can't take any args. Thoughts on how to move forward?
 Edit:
 bug still present as of &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/c64520e658806a87f282c74299b2ea4b7ea493ea&gt;c64520e&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='4' author='s-rog' date='2020-09-03T19:51:02Z'>
 		let's just wait for refactors and keep this issue open. After exploring a bit more I saw a few more bugs üòÖ
 Although for now, the only place where the model is required is in TrainerCallbackHookMixin.on_fit_start for the callbacks but there also we can avoid the model parameter and just use self.get_model() just like in on_fit_end.
 		</comment>
 		<comment id='5' author='s-rog' date='2020-09-21T06:20:14Z'>
 		&lt;denchmark-link:https://github.com/rohitgr7&gt;@rohitgr7&lt;/denchmark-link&gt;
  I implemented the following changes and now on_fit_start is working as expected:
 
 move hook
 change on_fit_start signature (like on_fit_end)
 change callback.on_fit_start args (like on_fit_end)
 
 
 After exploring a bit more I saw a few more bugs üòÖ
 
 What else did you find? I'll take a look before I submit a PR
 		</comment>
 		<comment id='6' author='s-rog' date='2020-09-22T18:52:50Z'>
 		&lt;denchmark-link:https://github.com/s-rog&gt;@s-rog&lt;/denchmark-link&gt;
  want to send the PR out?
 		</comment>
 	</comments>
 </bug>
<commit id='49767e424f2a2ba2893d27ed0c757815e355c86e' author='s-rog' date='2020-09-23 10:38:33+02:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\callback_hook.py' new_name='pytorch_lightning\trainer\callback_hook.py'>
 		<file_info nloc='121' complexity='72' token_count='1063'></file_info>
 		<method name='on_fit_start' parameters='self'>
 				<method_info nloc='3' complexity='2' token_count='25' nesting_level='1' start_line='49' end_line='52'></method_info>
 			<added_lines>49,52</added_lines>
 			<deleted_lines>49,52</deleted_lines>
 		</method>
 		<method name='on_fit_start' parameters='self,model'>
 				<method_info nloc='3' complexity='2' token_count='23' nesting_level='1' start_line='49' end_line='52'></method_info>
 			<added_lines>49,52</added_lines>
 			<deleted_lines>49,52</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\trainer.py' new_name='pytorch_lightning\trainer\trainer.py'>
 		<file_info nloc='424' complexity='58' token_count='2920'></file_info>
 		<modified_lines>
 			<added_lines>298,299,300,301</added_lines>
 			<deleted_lines>287,288,289</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
