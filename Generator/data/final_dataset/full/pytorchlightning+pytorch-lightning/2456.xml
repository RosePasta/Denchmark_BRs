<bug_data>
<bug id='2456' author='griff4692' open_date='2020-07-01T21:45:01Z' closed_time='2020-07-02T11:04:19Z'>
 	<summary>multi-gpu training triggers CUDA out of memory error</summary>
 	<description>
 Hi -
 I am running into issues when going from single to multi-gpu training.  Specifically, if I switch the line
 pl.Trainer(gpus=1, precision=16, distributed_backend='ddp')
 to
 pl.Trainer(gpus=4, precision=16, distributed_backend='ddp')
 I get the dreaded CUDA out of memory error.  Is there any reason why the parallelism causes the GPU to receive more data?
 	</description>
 	<comments>
 		<comment id='1' author='griff4692' date='2020-07-01T21:46:18Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 		<comment id='2' author='griff4692' date='2020-07-02T06:34:56Z'>
 		Hi, what are your outputs of the validation_step? If there are any large tensors, it's likely they get synced back to root GPU by &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2434&gt;#2434&lt;/denchmark-link&gt;
  . We're working on that.
 cc &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
   ^^
 		</comment>
 		<comment id='3' author='griff4692' date='2020-07-02T12:27:05Z'>
 		Hi - I actually haven't implemented the validation step yet.  this just occurs on the training side
 		</comment>
 		<comment id='4' author='griff4692' date='2020-07-02T13:04:39Z'>
 		what is your gpu consumption on a single gpu (used/available)?
 		</comment>
 		<comment id='5' author='griff4692' date='2020-07-02T13:34:42Z'>
 		On single gpu, I am using 5/11 GB.  The problem seems to be that when I switch over to multiple GPUs, there is an explosion of processes created on the first GPU.  Any ideas what could be causing this?
 &lt;denchmark-link:https://user-images.githubusercontent.com/12277915/86365204-32405c00-bc47-11ea-9211-0e6ccfb57f8d.png&gt;&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='6' author='griff4692' date='2020-07-02T13:39:04Z'>
 		Fixed it!.  I was calling .to('cuda') on my input tensors in my Dataset __get__item function which caused all the data to be uploaded to the first GPU.  Removed that and solved the problem.
 		</comment>
 		<comment id='7' author='griff4692' date='2020-07-02T13:45:20Z'>
 		&lt;denchmark-link:https://github.com/justusschock&gt;@justusschock&lt;/denchmark-link&gt;
  does that mean we should add back the all reduce for val?
 		</comment>
 		<comment id='8' author='griff4692' date='2020-07-02T15:17:56Z'>
 		No, there were other issues with that as well :D Let's just keep it out for now.
 		</comment>
 	</comments>
 </bug>
<commit id='afdfba1dc6061c5e1ee6eaf215500d6a56e95482' author='William Falcon' date='2020-07-02 07:04:18-04:00'>
 	<dmm_unit complexity='1.0' interfacing='0.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\evaluation_loop.py' new_name='pytorch_lightning\trainer\evaluation_loop.py'>
 		<file_info nloc='336' complexity='39' token_count='1239'></file_info>
 		<method name='reduce_eval_ddp' parameters='self,eval_results'>
 				<method_info nloc='10' complexity='6' token_count='82' nesting_level='1' start_line='357' end_line='368'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>357,358,359,360,361,362,363,364,365,366,367,368</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>344,345,346,347,348,369</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
