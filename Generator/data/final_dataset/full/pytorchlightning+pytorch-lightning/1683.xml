<bug_data>
<bug id='1683' author='hirune924' open_date='2020-05-01T03:39:30Z' closed_time='2020-05-10T17:19:19Z'>
 	<summary>NeptuneLogger doesn't work with distributed_backend='ddp'</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 When using NeptuneLogger with distributed_backend='ddp' and running it on a single node with two GPUs, I find an error like this.
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "pl.py", line 146, in &lt;module&gt;
     main()
   File "pl.py", line 103, in main
     trainer.fit(model)
   File "/home/hirune/anaconda3/envs/PANDA/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 751, in fit
     mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))
   File "/home/hirune/anaconda3/envs/PANDA/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
     return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
   File "/home/hirune/anaconda3/envs/PANDA/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 158, in start_processes
     while not context.join():
   File "/home/hirune/anaconda3/envs/PANDA/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 119, in join
     raise Exception(msg)
 Exception:
 
 -- Process 1 terminated with the following error:
 Traceback (most recent call last):
   File "/home/hirune/anaconda3/envs/PANDA/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 20, in _wrap
     fn(i, *args)
   File "/home/hirune/anaconda3/envs/PANDA/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py", line 370, in ddp_train
     self.run_pretrain_routine(model)
   File "/home/hirune/anaconda3/envs/PANDA/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 871, in run_pretrain_routine
     self.configure_checkpoint_callback()
   File "/home/hirune/anaconda3/envs/PANDA/lib/python3.7/site-packages/pytorch_lightning/trainer/callback_config.py", line 54, in configure_checkpoint_callback
     self.logger.name,
   File "/home/hirune/anaconda3/envs/PANDA/lib/python3.7/site-packages/pytorch_lightning/loggers/neptune.py", line 267, in name
     return self.experiment.name
   File "/home/hirune/anaconda3/envs/PANDA/lib/python3.7/site-packages/pytorch_lightning/loggers/neptune.py", line 230, in experiment
     **self._kwargs)
   File "/home/hirune/anaconda3/envs/PANDA/lib/python3.7/site-packages/neptune/__init__.py", line 222, in create_experiment
     raise Uninitialized()
 neptune.exceptions.Uninitialized: You must initialize neptune-client first. For more information, please visit: https://github.com/neptune-ai/neptune-client#initialize-neptune
 &lt;/denchmark-code&gt;
 
 And I found a similar error with CommetLogger
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 Steps to reproduce the behavior:
 Run the following code on a machine with two GPUs.
 This code is a slightly modified version of what was on this page.
 &lt;denchmark-link:https://docs.neptune.ai/integrations/pytorch_lightning.html&gt;https://docs.neptune.ai/integrations/pytorch_lightning.html&lt;/denchmark-link&gt;
 
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;import os
 
 import torch
 from torch.nn import functional as F
 from torch.utils.data import DataLoader
 from torchvision.datasets import MNIST
 from torchvision import transforms
 
 import pytorch_lightning as pl
 
 MAX_EPOCHS=20
 LR=0.01
 BATCHSIZE=32
 CHECKPOINTS_DIR = 'my_models/checkpoints/7'
 
 class CoolSystem(pl.LightningModule):
 
     def __init__(self):
         super(CoolSystem, self).__init__()
         # not the best model...
         self.l1 = torch.nn.Linear(28 * 28, 10)
 
     def forward(self, x):
         return torch.relu(self.l1(x.view(x.size(0), -1)))
 
     def training_step(self, batch, batch_idx):
         # REQUIRED
         x, y = batch
         y_hat = self.forward(x)
         loss = F.cross_entropy(y_hat, y)
         tensorboard_logs = {'train_loss': loss}
         return {'loss': loss, 'log': tensorboard_logs}
 
     def validation_step(self, batch, batch_idx):
         # OPTIONAL
         x, y = batch
         y_hat = self.forward(x)
         return {'val_loss': F.cross_entropy(y_hat, y)}
 
     def validation_end(self, outputs):
         # OPTIONAL
         avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
         tensorboard_logs = {'val_loss': avg_loss}
         return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}
 
     def test_step(self, batch, batch_idx):
         # OPTIONAL
         x, y = batch
         y_hat = self.forward(x)
         return {'test_loss': F.cross_entropy(y_hat, y)}
 
     def test_end(self, outputs):
         # OPTIONAL
         avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()
         tensorboard_logs = {'test_loss': avg_loss}
         return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}
 
     def configure_optimizers(self):
         # REQUIRED
         # can return multiple optimizers and learning_rate schedulers
         # (LBFGS it is automatically supported, no need for closure function)
         return torch.optim.Adam(self.parameters(), lr=LR)
 
     @pl.data_loader
     def train_dataloader(self):
         # REQUIRED
         return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=BATCHSIZE)
 
     @pl.data_loader
     def val_dataloader(self):
         # OPTIONAL
         return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=BATCHSIZE)
 
     @pl.data_loader
     def test_dataloader(self):
         # OPTIONAL
         return DataLoader(MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor()), batch_size=BATCHSIZE)
 
 
 from pytorch_lightning.loggers.neptune import NeptuneLogger
 def main():
     neptune_logger = NeptuneLogger(
         api_key="ANONYMOUS",
         project_name="shared/pytorch-lightning-integration",
         close_after_fit=False,
         experiment_name="default",  # Optional,
         params={"max_epochs": MAX_EPOCHS,
                 "batch_size": BATCHSIZE,
                 "lr": LR}, # Optional,
         tags=["pytorch-lightning", "mlp"]  # Optional,
     )
     model_checkpoint = pl.callbacks.ModelCheckpoint(filepath=CHECKPOINTS_DIR)
     
     from pytorch_lightning import Trainer
     
     model = CoolSystem()
     trainer = Trainer(max_epochs=MAX_EPOCHS,
                       logger=neptune_logger,
                       checkpoint_callback=model_checkpoint,
                       gpus=-1,
                       distributed_backend='ddp',
                       )
     trainer.fit(model)
     trainer.test(model)
     
     # Get predictions on external test
     import numpy as np
     
     model.freeze()
     test_loader = DataLoader(MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor()), batch_size=256)
     
     y_true, y_pred = [],[]
     for i, (x, y) in enumerate(test_loader):
         y_hat = model.forward(x).argmax(axis=1).cpu().detach().numpy()
         y = y.cpu().detach().numpy()
     
         y_true.append(y)
         y_pred.append(y_hat)
     
         if i == len(test_loader):
             break
     y_true = np.hstack(y_true)
     y_pred = np.hstack(y_pred)
     
     # Log additional metrics
     from sklearn.metrics import accuracy_score
     
     accuracy = accuracy_score(y_true, y_pred)
     neptune_logger.experiment.log_metric('test_accuracy', accuracy)
     
     # Log charts
     from scikitplot.metrics import plot_confusion_matrix
     import matplotlib.pyplot as plt
     
     fig, ax = plt.subplots(figsize=(16, 12))
     plot_confusion_matrix(y_true, y_pred, ax=ax)
     neptune_logger.experiment.log_image('confusion_matrix', fig)
     
     # Save checkpoints folder
     neptune_logger.experiment.log_artifact(CHECKPOINTS_DIR)
     
     # You can stop the experiment
     neptune_logger.experiment.stop()
 
 if __name__ == "__main__":
         main()
 
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 
 CUDA:
 - GPU:
 - GeForce GTX TITAN X
 - GeForce GTX TITAN X
 - available:         True
 - version:           10.1
 Packages:
 - numpy:             1.18.1
 - pyTorch_debug:     False
 - pyTorch_version:   1.5.0
 - pytorch-lightning: 0.7.5
 - tensorboard:       2.2.1
 - tqdm:              4.42.1
 System:
 - OS:                Linux
 - architecture:
 - 64bit
 -
 - processor:         x86_64
 - python:            3.7.6
 - version:           #86-Ubuntu SMP Fri Jan 17 17:24:28 UTC 2020
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='hirune924' date='2020-05-01T03:40:09Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 		<comment id='2' author='hirune924' date='2020-05-01T07:38:15Z'>
 		&lt;denchmark-link:https://github.com/jakubczakon&gt;@jakubczakon&lt;/denchmark-link&gt;
  pls ^^
 		</comment>
 		<comment id='3' author='hirune924' date='2020-05-01T07:54:03Z'>
 		Hi &lt;denchmark-link:https://github.com/hirune924&gt;@hirune924&lt;/denchmark-link&gt;
  and thanks for raising it!
 I've already notified the dev team and &lt;denchmark-link:https://github.com/pitercl&gt;@pitercl&lt;/denchmark-link&gt;
  will get back to you once we have a solution.
 		</comment>
 		<comment id='4' author='hirune924' date='2020-05-03T22:14:44Z'>
 		&lt;denchmark-link:https://github.com/jakubczakon&gt;@jakubczakon&lt;/denchmark-link&gt;
  I assume that this is also Neptune issue only, not related to PL, right?
 		</comment>
 		<comment id='5' author='hirune924' date='2020-05-04T06:02:40Z'>
 		&lt;denchmark-link:https://github.com/hirune924&gt;@hirune924&lt;/denchmark-link&gt;
  mentioned:
 
 And I found a similar error with CommetLogger
 
 So I think it may be a more general problem &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 ,
 		</comment>
 		<comment id='6' author='hirune924' date='2020-05-04T16:15:43Z'>
 		Hi &lt;denchmark-link:https://github.com/hirune924&gt;@hirune924&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 , just a quick update on this: from my initial tests, our Neptune logger and multiprocessing don't mix well. Tomorrow I'll dig a bit deeper into that and see if/how it can be remedied.
 &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 , I don't know yet if this is more on Neptune side or PL side.
 		</comment>
 		<comment id='7' author='hirune924' date='2020-05-06T13:15:46Z'>
 		Another quick update: I have a solution to this and will create a PR with a fix today or tomorrow.
 		</comment>
 	</comments>
 </bug>
<commit id='0cb676746568b6ca3c1ef9d9d2879b913f183179' author='Piotr ≈Åusakowski' date='2020-05-10 13:19:18-04:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\loggers\neptune.py' new_name='pytorch_lightning\loggers\neptune.py'>
 		<file_info nloc='348' complexity='24' token_count='963'></file_info>
 		<method name='__getstate__' parameters='self'>
 				<method_info nloc='6' complexity='2' token_count='33' nesting_level='1' start_line='198' end_line='206'></method_info>
 			<added_lines>200,201,203,204,205</added_lines>
 			<deleted_lines>198,199,200,201,202,203</deleted_lines>
 		</method>
 		<method name='name' parameters='self'>
 				<method_info nloc='5' complexity='2' token_count='22' nesting_level='1' start_line='257' end_line='261'></method_info>
 			<added_lines>258</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_create_or_get_experiment' parameters='self'>
 				<method_info nloc='18' complexity='3' token_count='129' nesting_level='1' start_line='361' end_line='380'></method_info>
 			<added_lines>361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='version' parameters='self'>
 				<method_info nloc='5' complexity='2' token_count='22' nesting_level='1' start_line='264' end_line='268'></method_info>
 			<added_lines>265</added_lines>
 			<deleted_lines>264</deleted_lines>
 		</method>
 		<method name='experiment' parameters='self'>
 				<method_info nloc='13' complexity='2' token_count='29' nesting_level='1' start_line='209' end_line='225'></method_info>
 			<added_lines>220,221,223,224</added_lines>
 			<deleted_lines>224,225</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>35,86,138,170,193,194,196,360</added_lines>
 			<deleted_lines>35,86,138,191,194,195,196,197,207,226,227,228,229,230,271</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\loggers\test_neptune.py' new_name='tests\loggers\test_neptune.py'>
 		<file_info nloc='65' complexity='5' token_count='551'></file_info>
 		<method name='test_neptune_additional_methods' parameters='neptune'>
 				<method_info nloc='30' complexity='1' token_count='297' nesting_level='0' start_line='34' end_line='74'></method_info>
 			<added_lines>34,35,36,37,38,40,41,44,45,48,49,52,53,56,57,60,61,64,67,70,71,74</added_lines>
 			<deleted_lines>34,37,38,41,42,45,46,49,52,55,56,59</deleted_lines>
 		</method>
 		<method name='test_neptune_offline' parameters='neptune'>
 				<method_info nloc='4' complexity='1' token_count='45' nesting_level='0' start_line='26' end_line='30'></method_info>
 			<added_lines>26,29,30</added_lines>
 			<deleted_lines>26,29,30</deleted_lines>
 		</method>
 		<method name='test_neptune_online' parameters='neptune'>
 				<method_info nloc='6' complexity='1' token_count='56' nesting_level='0' start_line='12' end_line='22'></method_info>
 			<added_lines>13,15,16,17,18,19,20,21,22</added_lines>
 			<deleted_lines>13,14,16,17,21</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>31,32,33</added_lines>
 			<deleted_lines>25,33</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
