<bug_data>
<bug id='1322' author='VitorGuizilini-TRI' open_date='2020-03-31T17:35:57Z' closed_time='2020-04-05T15:07:17Z'>
 	<summary>Training loop temporarily hangs after every 4 steps</summary>
 	<description>
 I am porting some of my code to pytorch lightning, and everything seems to work fine. However, for some reason after every 4 training steps I see some temporary hanging (~1 second), which is severely slowing down my overall training time. Am I missing some obvious configuration?  This is my Trainer configuration:
 &lt;denchmark-code&gt;    trainer = pl.Trainer(
         gpus=8
         num_nodes=1,
         distributed_backend='ddp',
         checkpoint_callback=False,
         max_epochs=50,
         max_steps=None,
         progress_bar_refresh_rate=1,
         check_val_every_n_epoch=1,
         val_check_interval=1.0,
         gradient_clip_val=0.0,
         log_save_interval=0,
         num_sanity_val_steps=0,
         amp_level='O0',
     )
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='VitorGuizilini-TRI' date='2020-03-31T17:36:45Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 		<comment id='2' author='VitorGuizilini-TRI' date='2020-04-04T12:34:00Z'>
 		&lt;denchmark-link:https://github.com/orgs/PyTorchLightning/teams/core-contributors&gt;@PyTorchLightning/core-contributors&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='3' author='VitorGuizilini-TRI' date='2020-04-04T12:39:03Z'>
 		Thanks for the issue! Would it be possible to post the code that reproduces this error? I've only seen this sort of behaviour before when the number of data loading workers is low - are you working with large data here (e.g. big images)?
 		</comment>
 		<comment id='4' author='VitorGuizilini-TRI' date='2020-04-04T16:24:16Z'>
 		I increased the number of workers and it works perfectly now, thank you very much! You can close this issue.
 		</comment>
 		<comment id='5' author='VitorGuizilini-TRI' date='2020-04-04T16:27:14Z'>
 		should we throw a warning when users use few workers?
 		</comment>
 		<comment id='6' author='VitorGuizilini-TRI' date='2020-04-04T16:34:04Z'>
 		If possible, sure! Seems like an obvious solution now, but it could save a couple of hours for other people. :)
 		</comment>
 	</comments>
 </bug>
<commit id='b18accc64ccd24095c11fdbd64cc924456134592' author='Ethan Harris' date='2020-04-05 11:07:16-04:00'>
 	<dmm_unit complexity='0.9705882352941176' interfacing='0.8529411764705882' size='0.14705882352941177'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>29</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\data_loading.py' new_name='pytorch_lightning\trainer\data_loading.py'>
 		<file_info nloc='202' complexity='37' token_count='1060'></file_info>
 		<method name='_worker_check' parameters='self,DataLoader,str'>
 				<method_info nloc='5' complexity='3' token_count='38' nesting_level='1' start_line='77' end_line='81'></method_info>
 			<added_lines>77,78,79,80,81</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='reset_train_dataloader' parameters='self,LightningModule'>
 				<method_info nloc='38' complexity='6' token_count='211' nesting_level='1' start_line='114' end_line='162'></method_info>
 			<added_lines>122,128</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1,6,82,188,189</added_lines>
 			<deleted_lines>5,179,182</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\trainer\test_dataloaders.py' new_name='tests\trainer\test_dataloaders.py'>
 		<file_info nloc='385' complexity='17' token_count='1948'></file_info>
 		<method name='test_warning_with_few_workers' parameters='tmpdir'>
 				<method_info nloc='28' complexity='1' token_count='167' nesting_level='0' start_line='489' end_line='527'></method_info>
 			<added_lines>489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>18,528,529</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
