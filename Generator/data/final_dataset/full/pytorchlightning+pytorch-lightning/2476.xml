<bug_data>
<bug id='2476' author='aeryen' open_date='2020-07-02T23:51:29Z' closed_time='2020-09-23T04:19:47Z'>
 	<summary>Model and Input not on same GPU when training with native AMP and DP</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 Training with distributed_backend='dp' and precision=16 result in the error that some input/output of the model is not on the same GPU.
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;from pytorch_lightning import Trainer, seed_everything
 from pl_examples.models.lightning_template import LightningTemplateModel
 seed_everything(234)
 
 def main():
     # model = LightningTemplateModel(**vars(args))
     model = LightningTemplateModel()
 
     trainer = Trainer(
         gpus=2,
         num_nodes=1,
         distributed_backend='dp',
         precision=16
         )
 
     trainer.fit(model)
 
 if __name__ == '__main__':
     main()
 &lt;/denchmark-code&gt;
 
 Run the above example code will result in following error:
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "./_debug_pytorch_lightning_16gpu.py", line 28, in &lt;module&gt;
     main()
   File "./_debug_pytorch_lightning_16gpu.py", line 24, in main
     trainer.fit(model)
   File "/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 973, in fit
     self.dp_train(model)
   File "/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py", line 267, in dp_train
     self.run_pretrain_routine(model)
   File "/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1139, in run_pretrain_routine
     False)
   File "/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 291, in _evaluate
     output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)
   File "/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 441, in evaluation_forward
     output = model(*args)
   File "/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
     result = self.forward(*input, **kwargs)
   File "/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py", line 65, in forward
     outputs = self.parallel_apply(replicas, inputs, kwargs)
   File "/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py", line 69, in parallel_apply
     return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
   File "/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py", line 209, in parallel_apply
     raise output
   File "/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py", line 172, in _worker
     output = module.validation_step(*input, **kwargs)
   File "/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pl_examples/models/lightning_template.py", line 102, in validation_step
     val_loss = F.cross_entropy(y_hat, y)
   File "/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/torch/nn/functional.py", line 2422, in cross_entropy
     return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
   File "/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/torch/nn/functional.py", line 2218, in nll_loss
     ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
 RuntimeError: Assertion `THCTensor_(checkGPU)(state, 4, input, target, output, total_weight)' failed. Some of weight/gradient/input tensors are located on different GPUs. Please move them to a single one. at /opt/conda/conda-bld/pytorch_1593673617738/work/aten/src/THCUNN/generic/ClassNLLCriterion.cu:31
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 The model should train successfully on 2 GPUs with native PyTorch AMP.
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 
 CUDA:
 - GPU:
 - Tesla V100-SXM2-16GB
 - Tesla V100-SXM2-16GB
 - Tesla V100-SXM2-16GB
 - Tesla V100-SXM2-16GB
 - available:         True
 - version:           10.2
 Packages:
 - numpy:             1.18.5
 - pyTorch_debug:     False
 - pyTorch_version:   1.7.0.dev20200702
 - pytorch-lightning: 0.8.5-dev
 - tensorboard:       2.2.2
 - tqdm:              4.47.0
 System:
 - OS:                Linux
 - architecture:
 - 64bit
 -
 - processor:         x86_64
 - python:            3.7.7
 - version:           #30~18.04.1-Ubuntu SMP Mon Jun 22 15:48:21 UTC 2020
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 This only seems to be an issue with native AMP, not Apex AMP.
 	</description>
 	<comments>
 		<comment id='1' author='aeryen' date='2020-07-02T23:52:26Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 		<comment id='2' author='aeryen' date='2020-07-03T00:22:17Z'>
 		I just realized that without precision=16 the script would still fail with a different error
 TypeError: zip argument #1 must support iteration
 Am I doing something wrong? This seems like a separate bug.
 		</comment>
 		<comment id='3' author='aeryen' date='2020-07-03T00:28:53Z'>
 		I'm not using native amp yet but apex only works on ddp, and I suspect the native implementation might be as well.
 		</comment>
 		<comment id='4' author='aeryen' date='2020-07-03T00:35:58Z'>
 		I see, so it's not supported yet. Thanks for the information. Unfortunately, I have to stick with DP, because PyTorch does not allow gradient checkpointing with DDP. Let me see if I can get this to work.
 		</comment>
 		<comment id='5' author='aeryen' date='2020-07-03T00:41:51Z'>
 		&lt;denchmark-link:https://github.com/s-rog&gt;@s-rog&lt;/denchmark-link&gt;
  regarding the issue that DP also doesn't work even without precision=16, can you kindly help me confirm that? should I open a separate issue?
 		</comment>
 		<comment id='6' author='aeryen' date='2020-07-03T00:50:28Z'>
 		I've never really used dp so I really can't tell you
 		</comment>
 		<comment id='7' author='aeryen' date='2020-07-05T02:22:31Z'>
 		yes that's correct... for some reason, dp and apex do not mix well.
 Maybe &lt;denchmark-link:https://github.com/mcarilli&gt;@mcarilli&lt;/denchmark-link&gt;
  might know why when forward is called with dp the casting isn't automated?
 		</comment>
 		<comment id='8' author='aeryen' date='2020-07-05T03:28:35Z'>
 		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
  Hi, sorry, the issue is actually with native amp, not apex. Also, honestly, currently the DP is completely broken, not just when using precision=16. It does feel like people have mostly abandoned DP and moved to DDP in general?
 		</comment>
 		<comment id='9' author='aeryen' date='2020-07-05T03:40:49Z'>
 		yeah, dp should not really be used but we can‚Äôt remove since some people‚Äôs research depends on it.
 		</comment>
 		<comment id='10' author='aeryen' date='2020-07-05T03:56:14Z'>
 		So here is my question (apologies if this is not a good place to discuss this): Isn't the PyTorch gradient checkpointing currently can only work with DP? My current conundrum, I'm trying to work with HuggingFace longformer, and without gradient checkpointing, I can only fit batch size 1 in a single GPU with 16g vram. So the best combo for me right now is Native AMP + DP + gradient checkpointing.
 I guess maybe the long term direction is either just move to TPU and not bother or wait to see if PyTorch can manage to get DDP to work with gradient checkpointing.
 edit: Sorry I'm slow so just realized there was an effort trying to make DDP work better with grad checkpoint, &lt;denchmark-link:https://github.com/pytorch/pytorch/pull/24800&gt;pytorch/pytorch#24800&lt;/denchmark-link&gt;
 . Guess I'll stop bothering you guys and wait for that to work...
 		</comment>
 		<comment id='11' author='aeryen' date='2020-07-05T04:55:04Z'>
 		torch.cuda.amp should support DP, and torch.cuda.amp + DP is tested in pytorch CI, so the error is unexpected.  Do you have a minimal repro that uses raw Pytorch (not lighting)?  This would help us narrow down if the problem lies with DP, torch.cuda.amp, or how lightning combines them.  Also, can you post the complete backtrace?
 		</comment>
 		<comment id='12' author='aeryen' date='2020-07-05T05:05:33Z'>
 		&lt;denchmark-link:https://github.com/mcarilli&gt;@mcarilli&lt;/denchmark-link&gt;
  My feeling is the issue is within lightning, I've done a rough test with raw Pytorch and haven't been able to reproduce the bug (but could be because I'm bad at this). I've added the trace to the top.
 		</comment>
 		<comment id='13' author='aeryen' date='2020-08-03T19:48:23Z'>
 		&lt;denchmark-link:https://github.com/mcarilli&gt;@mcarilli&lt;/denchmark-link&gt;
  can you take a look?
 		</comment>
 		<comment id='14' author='aeryen' date='2020-08-03T20:38:42Z'>
 		I can look into this but probably not in the next few days, reping if I don't respond by then.  Did you ever get a repro with raw pytorch (not lightning)?
 		</comment>
 		<comment id='15' author='aeryen' date='2020-08-03T21:53:28Z'>
 		&lt;denchmark-link:https://github.com/aeryen&gt;@aeryen&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='16' author='aeryen' date='2020-08-03T21:59:35Z'>
 		No I still don't have repro for raw pytorch. So I still think it's within lighting.
 I tried to debug it, I even tried manually move the model to each GPU according to the location of the data within my outer forward(). The issue still occur. Looks to me the model's device jump to something else as soon as my sub modules' forward() are being called. I had the impression that threads are not using the model that belongs to them. I'm probably not explaining this as clear as I should...
 		</comment>
 		<comment id='17' author='aeryen' date='2020-08-23T16:30:34Z'>
 		I had (maybe) the same issue with native AMP and DP backend after upgrading from 0.7.6 to 0.9.0.
 It turned out in my case the issue was in DataParallelBackend.setup. Native AMP wrapped the original model's forward function instead of DataParallel.forward.
 When code runs into &lt;denchmark-link:https://github.com/pytorch/pytorch/blob/a97ca93c0e698b81599f7a0ca5cdbda947799431/torch/cuda/amp/autocast_mode.py#L135&gt;this line&lt;/denchmark-link&gt;
 , it can be seen that  is always the original model. Then the error occurs when the model and data mismatch.
 I changed the order of wrapping in DataParallelBackend.setup and the problem was solved for me.
 @@ -48,13 +48,13 @@ class DataParallelBackend(object):
          # hack forward to do autocast for the user
          self.model_autocast_original_forward = model.forward
  
 +        # init torch data parallel
 +        model = self.__init_torch_data_parallel(model)
 +
          # init half precision
          if self.trainer.amp_backend:
              model = self.__init_half_precision(model)
  
 -        # init torch data parallel
 -        model = self.__init_torch_data_parallel(model)
 -
          self.trainer.model = model
  
      def __init_torch_data_parallel(self, model):
 		</comment>
 		<comment id='18' author='aeryen' date='2020-08-23T16:40:27Z'>
 		want to submit a PR?
 &lt;denchmark-link:https://github.com/mcarilli&gt;@mcarilli&lt;/denchmark-link&gt;
 , does this make sense?
 		</comment>
 		<comment id='19' author='aeryen' date='2020-08-23T16:46:46Z'>
 		Sorry I‚Äôm a bit hectic and not up for making a PR.
 		</comment>
 	</comments>
 </bug>
<commit id='031274c25dedc92e383d2715e283a55a2b102d29' author='William Falcon' date='2020-09-23 00:19:46-04:00'>
 	<dmm_unit complexity='0.7045454545454546' interfacing='0.18181818181818182' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pl_examples\README.md' new_name='pl_examples\README.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>2,3,7,8,10,11,12,14,16,17,18,19</added_lines>
 			<deleted_lines>2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,35,36,37,38,39,40,41,42,44,46,47,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,66,67</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pl_examples\__init__.py' new_name='pl_examples\__init__.py'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pl_examples\basic_examples\README.md' new_name='pl_examples\basic_examples\README.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>4,5,7,8,10,11,13,14,15,17,18,19,21,22,24,25,27,28,31,32,33,35,36,38,39,41,42,43,44,57,64</added_lines>
 			<deleted_lines>4,6,7,9,10,11,12,13,15,16,17,20,21,23,24,26,27,28,31,32,33,34,35,36,38,39,54,61</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='pl_examples\basic_examples\autoencoder.py'>
 		<file_info nloc='53' complexity='5' token_count='429'></file_info>
 	</modification>
 	<modification change_type='DELETE' old_name='pl_examples\basic_examples\cpu_template.py' new_name='None'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 	</modification>
 	<modification change_type='DELETE' old_name='pl_examples\basic_examples\gpu_template.py' new_name='None'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='pl_examples\basic_examples\image_classifier.py'>
 		<file_info nloc='70' complexity='10' token_count='610'></file_info>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='pl_examples\basic_examples\mnist.py'>
 		<file_info nloc='63' complexity='8' token_count='559'></file_info>
 	</modification>
 	<modification change_type='DELETE' old_name='pl_examples\basic_examples\multi_node_ddp2_demo.py' new_name='None'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 	</modification>
 	<modification change_type='DELETE' old_name='pl_examples\basic_examples\multi_node_ddp_demo.py' new_name='None'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 	</modification>
 	<modification change_type='MODIFY' old_name='pl_examples\basic_examples\submit_ddp2_job.sh' new_name='pl_examples\basic_examples\submit_ddp2_job.sh'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>27</added_lines>
 			<deleted_lines>27</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pl_examples\basic_examples\submit_ddp_job.sh' new_name='pl_examples\basic_examples\submit_ddp_job.sh'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>27</added_lines>
 			<deleted_lines>27</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pl_examples\domain_templates\semantic_segmentation.py' new_name='pl_examples\domain_templates\semantic_segmentation.py'>
 		<file_info nloc='189' complexity='21' token_count='1444'></file_info>
 		<modified_lines>
 			<added_lines>13</added_lines>
 			<deleted_lines>13</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='RENAME' old_name='pl_examples\models\unet.py' new_name='pl_examples\domain_templates\unet.py'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 	</modification>
 	<modification change_type='DELETE' old_name='pl_examples\models\lightning_template.py' new_name='None'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 	</modification>
 	<modification change_type='DELETE' old_name='pl_examples\test_examples.py' new_name='None'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\accelerators\dp_backend.py' new_name='pytorch_lightning\accelerators\dp_backend.py'>
 		<file_info nloc='101' complexity='33' token_count='713'></file_info>
 		<method name='training_step_end' parameters='self,output'>
 				<method_info nloc='6' complexity='3' token_count='39' nesting_level='1' start_line='121' end_line='126'></method_info>
 			<added_lines>124,125</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='validation_step_end' parameters='self,output'>
 				<method_info nloc='6' complexity='3' token_count='39' nesting_level='1' start_line='128' end_line='133'></method_info>
 			<added_lines>131,132</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_step_end' parameters='self,output'>
 				<method_info nloc='6' complexity='3' token_count='39' nesting_level='1' start_line='135' end_line='140'></method_info>
 			<added_lines>138,139</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\overrides\data_parallel.py' new_name='pytorch_lightning\overrides\data_parallel.py'>
 		<file_info nloc='244' complexity='76' token_count='1699'></file_info>
 		<method name='parallel_apply' parameters='modules,inputs,kwargs_tup,devices'>
 				<method_info nloc='51' complexity='10' token_count='298' nesting_level='0' start_line='231' end_line='325'></method_info>
 			<added_lines>262,277,280,283,285,286,287,288</added_lines>
 			<deleted_lines>246,249,253</deleted_lines>
 		</method>
 		<method name='warn_missing_output' parameters='fx_called'>
 				<method_info nloc='14' complexity='3' token_count='32' nesting_level='0' start_line='215' end_line='228'></method_info>
 			<added_lines>215,216,217,218,219,220,221,222,223,224,225,226,227,228</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='auto_squeeze_dim_zeros' parameters='output'>
 				<method_info nloc='10' complexity='5' token_count='75' nesting_level='0' start_line='328' end_line='344'></method_info>
 			<added_lines>334,335,336,337</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='parallel_apply._worker' parameters='i,module,input,kwargs,device'>
 				<method_info nloc='28' complexity='10' token_count='189' nesting_level='1' start_line='260' end_line='296'></method_info>
 			<added_lines>262,277,280,283,285,286,287,288</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='forward' parameters='self,inputs,kwargs'>
 				<method_info nloc='34' complexity='10' token_count='284' nesting_level='1' start_line='162' end_line='212'></method_info>
 			<added_lines>164,165,177,180,183,207,208,209,210,211</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>27,58,59,60,229,230</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='pytorch_lightning\utilities\warning_utils.py'>
 		<file_info nloc='8' complexity='3' token_count='52'></file_info>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\base\develop_utils.py' new_name='tests\base\develop_utils.py'>
 		<file_info nloc='70' complexity='19' token_count='517'></file_info>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>6</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='RENAME' old_name='pl_examples\models\__init__.py' new_name='tests\examples\__init__.py'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='tests\examples\test_examples.py'>
 		<file_info nloc='34' complexity='2' token_count='102'></file_info>
 	</modification>
 </commit>
</bug_data>
