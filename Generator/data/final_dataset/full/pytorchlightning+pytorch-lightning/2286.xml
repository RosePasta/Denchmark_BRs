<bug_data>
<bug id='2286' author='hjalmarlucius' open_date='2020-06-19T21:38:11Z' closed_time='2020-07-05T11:17:23Z'>
 	<summary>example_input_array dtype</summary>
 	<description>
 Currently assumed that example_input_array dtype to be equal to model dtype. This is not necessarily correct - e.g. if input is a vector of INT.
 
 
 
 pytorch-lightning/pytorch_lightning/core/memory.py
 
 
          Line 192
       in
       7dc58bd
 
 
 
 
 
 
  input_ = apply_to_collection(input_, torch.Tensor, lambda x: x.type(model.dtype)) 
 
 
 
 
 
 	</description>
 	<comments>
 		<comment id='1' author='hjalmarlucius' date='2020-06-23T17:10:20Z'>
 		Hi, I don't understand. Does it throw an error or does it display nothing? Could you clarify?
 I don't think we can very accurately define the "input shape" for anything other than tensors.
 For this reason we exclude things like dicts from the overview, because it is not very practical to visualize this in a table.
 		</comment>
 		<comment id='2' author='hjalmarlucius' date='2020-06-24T02:11:09Z'>
 		Hi, currently the model is run with input_ as an input. If the model expects a tensor of INTs then it will crash if floats come. I encountered this issue when pretraining an ALBERT-like model. This receives word embeddings as inputs, which have to be integers as they're going into a nn.Embedding
 		</comment>
 		<comment id='3' author='hjalmarlucius' date='2020-06-24T13:38:31Z'>
 		okay I see, so we should not change the dtype as given by example_input_array.
 I can't recall why I added this conversion, maybe  it was because of amp and the half conversions. I'll have a closer look, thanks for bringing it up.
 		</comment>
 		<comment id='4' author='hjalmarlucius' date='2020-06-24T13:39:20Z'>
 		as a workaround until it is fixed, cast your input to int before feeding to the embedding layer, or don't use the example_input_array.
 		</comment>
 	</comments>
 </bug>
<commit id='6bfcfa8671c4bf54b34290171f191db65fa27d8c' author='Adrian WÃ¤lchli' date='2020-07-05 07:17:22-04:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>26</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\core\memory.py' new_name='pytorch_lightning\core\memory.py'>
 		<file_info nloc='307' complexity='58' token_count='1421'></file_info>
 		<method name='_forward_example_input' parameters='self'>
 				<method_info nloc='20' complexity='6' token_count='149' nesting_level='1' start_line='204' end_line='227'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>211</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\core\test_memory.py' new_name='tests\core\test_memory.py'>
 		<file_info nloc='176' complexity='18' token_count='1495'></file_info>
 		<method name='test_mixed_dtype_model_summary' parameters=''>
 				<method_info nloc='11' complexity='1' token_count='61' nesting_level='0' start_line='104' end_line='115'></method_info>
 			<added_lines>104,105,106,107,108,109,110,111,112,113,114,115</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_linear_model_summary_shapes' parameters='device,mode'>
 				<method_info nloc='20' complexity='1' token_count='109' nesting_level='0' start_line='81' end_line='101'></method_info>
 			<added_lines>81,83</added_lines>
 			<deleted_lines>88</deleted_lines>
 		</method>
 		<method name='forward' parameters='self,x'>
 				<method_info nloc='2' complexity='1' token_count='19' nesting_level='1' start_line='54' end_line='55'></method_info>
 			<added_lines>54,55</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_linear_model_summary_shapes' parameters='device,dtype,mode'>
 				<method_info nloc='21' complexity='1' token_count='122' nesting_level='0' start_line='68' end_line='89'></method_info>
 			<added_lines>75,76,77,78,81,83</added_lines>
 			<deleted_lines>68,70,88</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self'>
 				<method_info nloc='5' complexity='1' token_count='62' nesting_level='1' start_line='48' end_line='52'></method_info>
 			<added_lines>48,49,50,51,52</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>45,46,47,53,56,57,116,117</added_lines>
 			<deleted_lines>62,63,64,65</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
