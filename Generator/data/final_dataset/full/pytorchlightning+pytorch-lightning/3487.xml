<bug_data>
<bug id='3487' author='Tim-Chard' open_date='2020-09-13T11:27:42Z' closed_time='2020-09-15T16:41:28Z'>
 	<summary>Gradient norms are not logged unless row_log_interval==1</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 In version 0.9 the guards to calculate the gradient norms and then log the metrics can't be satisfied in the same batch unless the row_log_interval  is 1. In most places the guard seems to be (batch_idx + 1) % self.row_log_interval == 0 such as here:
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_loop.py
 
 
         Lines 749 to 757
       in
       b40de54
 
 
 
 
 
 
  def save_train_loop_metrics_to_loggers(self, batch_idx, batch_output): 
 
 
 
  # when metrics should be logged 
 
 
 
  should_log_metrics = (batch_idx + 1) % self.row_log_interval == 0 or self.should_stop 
 
 
 
  if should_log_metrics or self.fast_dev_run: 
 
 
 
  # logs user requested information to logger 
 
 
 
  metrics = batch_output.batch_log_metrics 
 
 
 
  grad_norm_dic = batch_output.grad_norm_dic 
 
 
 
  if len(metrics) &gt; 0 or len(grad_norm_dic) &gt; 0: 
 
 
 
  self.log_metrics(metrics, grad_norm_dic) 
 
 
 
 
 
 However in run_batch_backward_pass it is batch_idx % self.row_log_interval == 0
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_loop.py
 
 
         Lines 929 to 939
       in
       b40de54
 
 
 
 
 
 
  def run_batch_backward_pass(self, split_batch, batch_idx, opt_idx, optimizer): 
 
 
 
  # ------------------ 
 
 
 
  # GRAD NORMS 
 
 
 
  # ------------------ 
 
 
 
  # track gradient norms when requested 
 
 
 
  grad_norm_dic = {} 
 
 
 
  if batch_idx % self.row_log_interval == 0: 
 
 
 
  if float(self.track_grad_norm) &gt; 0: 
 
 
 
  model = self.get_model() 
 
 
 
  grad_norm_dic = model.grad_norm( 
 
 
 
  self.track_grad_norm) 
 
 
 
 
 
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 Steps to reproduce the behavior:
 
 Run the code sample below (taken from #1527 ).
 Confirm that gradients are not being logged in tensorboard.
 Change row_log_interval to 1 and rerun the code.
 4 Confirm that gradients are now being logged.
 
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;import pytorch_lightning as pl
 from torch.utils.data import TensorDataset, DataLoader
 from pytorch_lightning.loggers.tensorboard import TensorBoardLogger
 from torch.optim import SGD
 import torch.nn as nn
 import torch
 
 
 class MWENet(pl.LightningModule):
     def __init__(self):
         super(MWENet, self).__init__()
 
         self.first = nn.Conv2d(1, 1, 3)
         self.second = nn.Conv2d(1, 1, 3)
         self.loss = nn.L1Loss()
 
     def train_dataloader(self):
         xs, ys = torch.zeros(16, 1, 10, 10), torch.ones(16, 1, 6, 6)
         ds = TensorDataset(xs, ys)
         return DataLoader(ds)
 
     def forward(self, xs):
         out = self.first(xs)
         out = self.second(out)
         return out
 
     def configure_optimizers(self):
         first = SGD(self.first.parameters(), lr=0.01)
         second = SGD(self.second.parameters(), lr=0.01)
         return [second, first]
 
     def training_step(self, batch, batch_idx, optimizer_idx):
         xs, ys = batch
         out = self.forward(xs)
         return {'loss': self.loss(out, ys)}
 
 
 net = MWENet()
 logger = TensorBoardLogger('tb_logs', name='testing')
 trainer = pl.Trainer(
     track_grad_norm=2,
     row_log_interval=2,
     max_epochs=50,
     logger=logger)
 trainer.fit(net)
 
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 Gradients should be logged if track_grad_norm is True
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 
 CUDA:
 - GPU:
 - GeForce GTX 1080 Ti
 - available:         True
 - version:           10.2
 Packages:
 - numpy:             1.19.1
 - pyTorch_debug:     False
 - pyTorch_version:   1.6.0
 - pytorch-lightning: 0.9.0
 - tensorboard:       2.2.1
 - tqdm:              4.48.2
 System:
 - OS:                Windows
 - architecture:
 - 64bit
 - WindowsPE
 - processor:         AMD64 Family 23 Model 113 Stepping 0, AuthenticAMD
 - python:            3.7.9
 - version:           10.0.19041
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='Tim-Chard' date='2020-09-13T11:28:24Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 		<comment id='2' author='Tim-Chard' date='2020-09-13T11:44:15Z'>
 		yeah, it's a bug, mind send a PR?
 		</comment>
 	</comments>
 </bug>
<commit id='4ed96b2eb471124184144f96d259055e49ac97e7' author='Adrian W√§lchli' date='2020-09-15 18:41:27+02:00'>
 	<dmm_unit complexity='0.0' interfacing='1.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>61,62</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\training_loop.py' new_name='pytorch_lightning\trainer\training_loop.py'>
 		<file_info nloc='410' complexity='136' token_count='3393'></file_info>
 		<method name='_track_gradient_norm' parameters='self,batch_idx'>
 				<method_info nloc='7' complexity='3' token_count='61' nesting_level='1' start_line='378' end_line='384'></method_info>
 			<added_lines>379,380,383,384</added_lines>
 			<deleted_lines>379,380,383,384</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>385</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\models\test_grad_norm.py' new_name='tests\models\test_grad_norm.py'>
 		<file_info nloc='69' complexity='15' token_count='598'></file_info>
 		<method name='test_grad_tracking_interval' parameters='tmpdir,row_log_interval'>
 				<method_info nloc='19' complexity='6' token_count='146' nesting_level='0' start_line='80' end_line='101'></method_info>
 			<added_lines>80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>2,77,78,79</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
