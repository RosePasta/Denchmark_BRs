<bug_data>
<bug id='4974' author='yikuanli' open_date='2020-12-04T15:02:39Z' closed_time='2020-12-11T13:51:46Z'>
 	<summary>AttributeError: 'LightningOptimizer' object has no attribute 'state'</summary>
 	<description>
 I am using pytorch lightning with Adam optmiser to train a BYOL model, the model and pipeline works fine while training, when I stop and resume from checkpoint, it raise this error. I didn't do anything just resume from previous checkpoint, have no idea why there is an error like this, I upgrade my bolts and pytorch lightning to the mast (I think its the latest version)
 &lt;denchmark-link:https://user-images.githubusercontent.com/40010984/101178978-8b227f00-3641-11eb-8d72-a3b3ca8c1173.png&gt;&lt;/denchmark-link&gt;
 
 my opmizer is like this, and Adam is the original Adam provided by pytorch
 &lt;denchmark-link:https://user-images.githubusercontent.com/40010984/101179109-be650e00-3641-11eb-951c-0717e217b532.png&gt;&lt;/denchmark-link&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='yikuanli' date='2020-12-04T16:04:57Z'>
 		Thanks for the report, it would be really really useful (and get this fixed faster) if you could reproduce this via the bug report model:
 &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py&lt;/denchmark-link&gt;
 
 cc &lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='yikuanli' date='2020-12-04T16:14:19Z'>
 		Hey &lt;denchmark-link:https://github.com/yikuanli&gt;@yikuanli&lt;/denchmark-link&gt;
 ,
 Thanks a lot for testing the latest version of Lightning. I am going to work on this asap.
 If you could reproduce the bug with BoringModel, it will help a lot.
 While waiting for fix, you can use Trainer(enable_pl_optimizer=False).
 Best regards,
 T.C
 		</comment>
 		<comment id='3' author='yikuanli' date='2020-12-04T16:27:52Z'>
 		Thanks for the comments, I used BoringModel to test, and seems it doesn't have this problem, I am wondering if the wrapper and LWCA LR is the problem.
 		</comment>
 		<comment id='4' author='yikuanli' date='2020-12-04T16:38:46Z'>
 		&lt;denchmark-code&gt;import os
 import torch
 from torch.utils.data import Dataset
 from pytorch_lightning import Trainer, LightningModule
 from pytorch_lightning.callbacks import ModelCheckpoint
 import pytorch_lightning as pl
 from pl_bolts.optimizers.lars_scheduling import LARSWrapper
 from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR
 
 class CheckpointEveryNSteps(pl.Callback):
     """
     Save a checkpoint every N steps, instead of Lightning's default that checkpoints
     based on validation loss.
     """
 
     def __init__(
         self,
         save_step_frequency,
         prefix="latest-Checkpoint",
         use_modelcheckpoint_filename=False,
     ):
         """
         Args:
             save_step_frequency: how often to save in steps
             prefix: add a prefix to the name, only used if
                 use_modelcheckpoint_filename=False
             use_modelcheckpoint_filename: just use the ModelCheckpoint callback's
                 default filename, don't use ours.
         """
         self.save_step_frequency = save_step_frequency
         self.prefix = prefix
         self.use_modelcheckpoint_filename = use_modelcheckpoint_filename
 
     def on_batch_end(self, trainer: pl.Trainer, _):
         """ Check if we should save a checkpoint after every train batch """
         global_step = trainer.global_step
         if global_step % self.save_step_frequency == 0:
             if self.use_modelcheckpoint_filename:
                 filename = trainer.checkpoint_callback.filename
             else:
                 filename = "{}.ckpt".format(self.prefix)
             ckpt_path = os.path.join(trainer.checkpoint_callback.dirpath, filename)
             trainer.save_checkpoint(ckpt_path)
 
 
 class RandomDataset(Dataset):
     def __init__(self, size, length):
         self.len = length
         self.data = torch.randn(length, size)
 
     def __getitem__(self, index):
         return self.data[index]
 
     def __len__(self):
         return self.len
 
 
 class BoringModel(LightningModule):
 
     def __init__(self):
         """
         Testing PL Module
         Use as follows:
         - subclass
         - modify the behavior for what you want
         class TestModel(BaseTestModel):
             def training_step(...):
                 # do your own thing
         or:
         model = BaseTestModel()
         model.training_epoch_end = None
         """
         super().__init__()
         self.layer = torch.nn.Linear(32, 2)
 
     def forward(self, x):
         return self.layer(x)
 
     def loss(self, batch, prediction):
         return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))
 
     def step(self, x):
         x = self.layer(x)
         out = torch.nn.functional.mse_loss(x, torch.ones_like(x))
         return out
 
     def training_step(self, batch, batch_idx):
         output = self.layer(batch)
         loss = self.loss(batch, output)
         return {"loss": loss}
 
     def training_step_end(self, training_step_outputs):
         return training_step_outputs
 
     def training_epoch_end(self, outputs) -&gt; None:
         torch.stack([x["loss"] for x in outputs]).mean()
 
     def validation_step(self, batch, batch_idx):
         output = self.layer(batch)
         loss = self.loss(batch, output)
         return {"x": loss}
 
     def validation_epoch_end(self, outputs) -&gt; None:
         torch.stack([x['x'] for x in outputs]).mean()
 
     def test_step(self, batch, batch_idx):
         output = self.layer(batch)
         loss = self.loss(batch, output)
         return {"y": loss}
 
     def test_epoch_end(self, outputs) -&gt; None:
         torch.stack([x["y"] for x in outputs]).mean()
 
     def configure_optimizers(self):
         optimizer = torch.optim.Adam(self.parameters(), lr=0.1)
 
         optimizer = LARSWrapper(optimizer)
         scheduler = LinearWarmupCosineAnnealingLR(
             optimizer,
             warmup_epochs= 1, 
             max_epochs= 20
         )
         return [optimizer], [scheduler]
 
 
 def run_test():
 
     class TestModel(BoringModel):
 
         def on_train_epoch_start(self) -&gt; None:
             print('override any method to prove your bug')
 
 
     train_data = torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=1)
     val_data = torch.utils.data.DataLoader(RandomDataset(32, 64),batch_size=1)
     test_data = torch.utils.data.DataLoader(RandomDataset(32, 64),batch_size=1)
 
 
     checkpoint_callback = ModelCheckpoint(monitor='loss', mode= 'min', filepath='./checkpoint')
     model = TestModel()
     trainer = Trainer(
         default_root_dir=os.getcwd(),
         resume_from_checkpoint='./latest-Checkpoint.ckpt',
         max_epochs=10,
         weights_summary=None,
         accelerator= 'ddp',
         log_every_n_steps=1,
         gpus=1,
         checkpoint_callback= checkpoint_callback,
         callbacks=[CheckpointEveryNSteps(1)]
     )
     trainer.fit(model, train_data, val_data)
 
 
 if __name__ == '__main__':
     run_test()
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='5' author='yikuanli' date='2020-12-04T16:39:22Z'>
 		I reproduce the error, it is because of the ddp, if I train with ddp, this error occurs
 		</comment>
 		<comment id='6' author='yikuanli' date='2020-12-04T16:57:47Z'>
 		Hey &lt;denchmark-link:https://github.com/yikuanli&gt;@yikuanli&lt;/denchmark-link&gt;
 ,
 Let me prioritise this :)
 Best,
 T.C
 		</comment>
 		<comment id='7' author='yikuanli' date='2020-12-04T20:08:57Z'>
 		Hey &lt;denchmark-link:https://github.com/yikuanli&gt;@yikuanli&lt;/denchmark-link&gt;
 ,
 Would you mind trying this branch: &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4981&gt;#4981&lt;/denchmark-link&gt;
 .
 I wasn't able to reproduce your bug, but another one. When resolved, it seems to train fine.
 I would be interested to see what you get your bug. Please, have a look at my test if I missed something.
 Best regards,
 T.C
 		</comment>
 		<comment id='8' author='yikuanli' date='2020-12-07T10:13:46Z'>
 		Hi, &lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;
 , I am sorry, need to ask how to pull this branch you mentioned in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4981&gt;#4981&lt;/denchmark-link&gt;
 . I am not very familiar with those management in GitHub,
 BTW, the way I got that problem is firstly run the boring model train with for example 5 epoch with ddp, and not resume from check point.
 after ran it, I got a checkpoint and resume from that checkpoint, but change to 10 epoch for example, because check point already save the first 5, and I need to increase the number to let it load the model and kept training.
 thanks for helping out, plz let me know how to test that branch ,and I will do it.
 		</comment>
 		<comment id='9' author='yikuanli' date='2020-12-07T11:54:23Z'>
 		&lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/yikuanli&gt;@yikuanli&lt;/denchmark-link&gt;
   LARSWrapper should not be passed to a Scheduler, since it is not an optimizer object. We have updated out SimCLR and SwAV code accordingly. I will update the BYOL with this soon.
 To see how LR schedule is set for LARS, refer to:
 &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/master/pl_bolts/models/self_supervised/swav/swav_module.py#L330&gt;https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/master/pl_bolts/models/self_supervised/swav/swav_module.py#L330&lt;/denchmark-link&gt;
 
 &lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;
  LARSWrapper's step method takes in a closure and the class itself sets the param_group and state property. The reason we would prefer to keep LARSWrapper as a wrapper class for an Optimizer, instead of an Optimizer itself, is because any Optimizer passed to this wrapper can get the layer-wise LR scaling property of LARS.
 		</comment>
 		<comment id='10' author='yikuanli' date='2020-12-09T11:39:50Z'>
 		Hey &lt;denchmark-link:https://github.com/ananyahjha93&gt;@ananyahjha93&lt;/denchmark-link&gt;
 ,
 Not sure to understand why Lars can't be an optimizer. I found several implementations making this cleaner: &lt;denchmark-link:https://github.com/kakaobrain/torchlars/blob/3b3d7e9c7bd35a31b2c2fa6f213beb0bf6881892/torchlars/lars.py#L11&gt;https://github.com/kakaobrain/torchlars/blob/3b3d7e9c7bd35a31b2c2fa6f213beb0bf6881892/torchlars/lars.py#L11&lt;/denchmark-link&gt;
 
 I will look into this deeper.
 Best,
 T.C
 		</comment>
 	</comments>
 </bug>
<commit id='7755572b4f37b811b83f6a933329b01af4735e66' author='chaton' date='2020-12-11 14:51:45+01:00'>
 	<dmm_unit complexity='0.9038461538461539' interfacing='0.8461538461538461' size='0.5'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\core\optimizer.py' new_name='pytorch_lightning\core\optimizer.py'>
 		<file_info nloc='131' complexity='36' token_count='893'></file_info>
 		<method name='_on_trainer_init' parameters='self,trainer'>
 				<method_info nloc='7' complexity='3' token_count='49' nesting_level='1' start_line='74' end_line='80'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>76</deleted_lines>
 		</method>
 		<method name='__optimizer_step' parameters='self,args,None,str,kwargs'>
 				<method_info nloc='32' complexity='7' token_count='213' nesting_level='1' start_line='100' end_line='138'></method_info>
 			<added_lines>114,115,116,117,118</added_lines>
 			<deleted_lines>114</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>14,64</added_lines>
 			<deleted_lines>63</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\utilities\__init__.py' new_name='pytorch_lightning\utilities\__init__.py'>
 		<file_info nloc='96' complexity='11' token_count='460'></file_info>
 		<modified_lines>
 			<added_lines>57</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='requirements\extra.txt' new_name='requirements\extra.txt'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>10</added_lines>
 			<deleted_lines>10</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\core\test_lightning_module.py' new_name='tests\core\test_lightning_module.py'>
 		<file_info nloc='69' complexity='13' token_count='438'></file_info>
 		<method name='test_automatic_optimization_num_calls.training_step' parameters='self,batch,batch_idx,optimizer_idx'>
 				<method_info nloc='4' complexity='1' token_count='35' nesting_level='3' start_line='58' end_line='61'></method_info>
 			<added_lines>58,59,60,61</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_automatic_optimization_num_calls' parameters='enable_pl_optimizer,tmpdir'>
 				<method_info nloc='36' complexity='7' token_count='193' nesting_level='0' start_line='49' end_line='105'></method_info>
 			<added_lines>58,59,60,61,62</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\core\test_lightning_optimizer.py' new_name='tests\core\test_lightning_optimizer.py'>
 		<file_info nloc='334' complexity='62' token_count='2565'></file_info>
 		<method name='test_lightning_optimizer_manual_optimization_and_accumulated_gradients.training_step' parameters='self,batch,batch_idx,optimizer_idx'>
 				<method_info nloc='10' complexity='1' token_count='79' nesting_level='2' start_line='146' end_line='160'></method_info>
 			<added_lines>154,160</added_lines>
 			<deleted_lines>152,158</deleted_lines>
 		</method>
 		<method name='test_state' parameters='tmpdir'>
 				<method_info nloc='16' complexity='3' token_count='126' nesting_level='0' start_line='192' end_line='207'></method_info>
 			<added_lines>200,201</added_lines>
 			<deleted_lines>198,199,200</deleted_lines>
 		</method>
 		<method name='test_lightning_optimizer_with_wrong_optimizer_interface.state' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='11' nesting_level='2' start_line='227' end_line='228'></method_info>
 			<added_lines>227,228</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_lightning_optimizer_with_wrong_optimizer_interface.step' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='15' nesting_level='2' start_line='238' end_line='240'></method_info>
 			<added_lines>238,239,240</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_lightning_optimizer_manual_optimization.training_step' parameters='self,batch,batch_idx,optimizer_idx'>
 				<method_info nloc='10' complexity='1' token_count='79' nesting_level='2' start_line='93' end_line='107'></method_info>
 			<added_lines>101,107</added_lines>
 			<deleted_lines>99,105</deleted_lines>
 		</method>
 		<method name='test_lightning_optimizer_with_wrong_optimizer_interface.configure_optimizers' parameters='self'>
 				<method_info nloc='4' complexity='1' token_count='35' nesting_level='2' start_line='244' end_line='247'></method_info>
 			<added_lines>244,245,246,247</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_lightning_optimizer_with_wrong_optimizer_interface.param_groups' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='11' nesting_level='2' start_line='231' end_line='232'></method_info>
 			<added_lines>231,232</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_lightning_optimizer_manual_optimization' parameters='mock_sgd_step,mock_adam_step,tmpdir'>
 				<method_info nloc='20' complexity='1' token_count='96' nesting_level='0' start_line='87' end_line='135'></method_info>
 			<added_lines>101,107</added_lines>
 			<deleted_lines>99,105</deleted_lines>
 		</method>
 		<method name='test_lightning_optimizer_with_wrong_optimizer_interface' parameters='tmpdir'>
 				<method_info nloc='22' complexity='1' token_count='73' nesting_level='0' start_line='210' end_line='256'></method_info>
 			<added_lines>210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_lightning_optimizer_with_wrong_optimizer_interface.param_groups' parameters='self,value'>
 				<method_info nloc='2' complexity='1' token_count='14' nesting_level='2' start_line='235' end_line='236'></method_info>
 			<added_lines>235,236</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_lightning_optimizer_with_wrong_optimizer_interface.__init__' parameters='self,optimizer'>
 				<method_info nloc='9' complexity='1' token_count='75' nesting_level='2' start_line='212' end_line='220'></method_info>
 			<added_lines>212,213,214,215,216,217,218,219,220</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_lightning_optimizer_manual_optimization_and_accumulated_gradients' parameters='mock_sgd_step,mock_adam_step,tmpdir'>
 				<method_info nloc='21' complexity='1' token_count='100' nesting_level='0' start_line='140' end_line='189'></method_info>
 			<added_lines>154,160</added_lines>
 			<deleted_lines>152,158</deleted_lines>
 		</method>
 		<method name='test_lightning_optimizer_with_wrong_optimizer_interface.__class__' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='7' nesting_level='2' start_line='223' end_line='224'></method_info>
 			<added_lines>223,224</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>22,24,27,85,86,138,139,257,258</added_lines>
 			<deleted_lines>25,83,84,136,137</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\trainer\optimization\test_manual_optimization.py' new_name='tests\trainer\optimization\test_manual_optimization.py'>
 		<file_info nloc='728' complexity='120' token_count='5450'></file_info>
 		<method name='test_step_with_optimizer_closure_with_different_frequencies' parameters='mock_sgd_step,mock_adam_step,tmpdir'>
 				<method_info nloc='26' complexity='3' token_count='134' nesting_level='0' start_line='864' end_line='939'></method_info>
 			<added_lines>905,936,938</added_lines>
 			<deleted_lines>905,936,938,939</deleted_lines>
 		</method>
 		<method name='test_step_with_optimizer_closure_with_different_frequencies.training_step' parameters='self,batch,batch_idx,optimizer_idx'>
 				<method_info nloc='8' complexity='2' token_count='67' nesting_level='2' start_line='871' end_line='905'></method_info>
 			<added_lines>905</added_lines>
 			<deleted_lines>905</deleted_lines>
 		</method>
 		<method name='test_step_with_optimizer_closure_and_extra_arguments.training_step' parameters='self,batch,batch_idx'>
 				<method_info nloc='7' complexity='1' token_count='48' nesting_level='2' start_line='813' end_line='828'></method_info>
 			<added_lines>828</added_lines>
 			<deleted_lines>828</deleted_lines>
 		</method>
 		<method name='test_step_with_optimizer_closure_and_extra_arguments' parameters='step_mock,tmpdir'>
 				<method_info nloc='24' complexity='2' token_count='109' nesting_level='0' start_line='806' end_line='859'></method_info>
 			<added_lines>828,858</added_lines>
 			<deleted_lines>828,858</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
