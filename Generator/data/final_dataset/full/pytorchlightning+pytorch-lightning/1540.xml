<bug_data>
<bug id='1540' author='kevinc13' open_date='2020-04-20T22:25:22Z' closed_time='2020-04-21T18:29:16Z'>
 	<summary>DDP on GPUs invalid ordinal</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 On latest version (master), training with DDP backend on GPUs (let's say 6,7) results in a CUDA error "Invalid device ordinal."
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 Steps to reproduce the behavior:
 
 Run any Lightning Module with DDP backend on more than 1 GPU with GPU indexes that do not start from 0
 See error
 
 &lt;denchmark-code&gt;INFO:lightning:GPU available: True, used: True
 INFO:lightning:VISIBLE GPUS: 6,7
 WARNING:lightning:SLURM_NODEID or NODE_RANK environment variable is not defined. Set as 0.
 WARNING:lightning:MASTER_ADDR environment variable is not defined. Set as localhost
 WARNING:lightning:SLURM_NODEID or NODE_RANK environment variable is not defined. Set as 0.
 WARNING:lightning:MASTER_ADDR environment variable is not defined. Set as localhost
 THCudaCheck FAIL file=/pytorch/torch/csrc/cuda/Module.cpp line=59 error=101 : invalid device ordinal
 THCudaCheck FAIL file=/pytorch/torch/csrc/cuda/Module.cpp line=59 error=101 : invalid device ordinal
 Traceback (most recent call last):
   File "bin/run.py", line 110, in &lt;module&gt;
     run(config, args.mode)
   File "bin/run.py", line 86, in run
     trainer.fit(system)
   File "/home/kevin/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 744, in fit
     mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))
   File "/home/kevin/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 171, in spawn
     while not spawn_context.join():
   File "/home/kevin/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 118, in join
     raise Exception(msg)
 Exception:
 
 -- Process 1 terminated with the following error:
 Traceback (most recent call last):
   File "/home/kevin/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 19, in _wrap
     fn(i, *args)
   File "/home/kevin/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py", line 332, in ddp_train
     torch.cuda.set_device(self.root_gpu)
   File "/home/kevin/miniconda3/lib/python3.7/site-packages/torch/cuda/__init__.py", line 292, in set_device
     torch._C._cuda_setDevice(device)
 RuntimeError: cuda runtime error (101) : invalid device ordinal at /pytorch/torch/csrc/cuda/Module.cpp:59
 
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 I expect this error to not occur.
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 
 CUDA:
 - GPU:
 - TITAN RTX
 - TITAN RTX
 - TITAN RTX
 - TITAN RTX
 - TITAN RTX
 - TITAN RTX
 - TITAN RTX
 - TITAN RTX
 - available:         True
 - version:           10.1
 Packages:
 - numpy:             1.17.4
 - pyTorch_debug:     False
 - pyTorch_version:   1.4.0
 - pytorch-lightning: 0.7.4rc1
 - tensorboard:       2.0.0
 - tqdm:              4.45.0
 System:
 - OS:                Linux
 - architecture:
 - 64bit
 -
 - processor:         x86_64
 - python:            3.7.4
 - version:           #97-Ubuntu SMP Wed Apr 1 03:25:46 UTC 2020
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 My best guess is that due to the new refactoring for ddp cpu backend, setting the CUDA device to root_gpu instead of using the provided gpu_idx parameter in ddp_train() is causing the error. Specifically, for 2 gpu, it's using index 6 instead of index 0 and index 7 instead of index 1 since we're setting CUDA_VISIBLE_DEVICES=6,7.
 	</description>
 	<comments>
 		<comment id='1' author='kevinc13' date='2020-04-20T22:25:56Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 		<comment id='2' author='kevinc13' date='2020-04-20T22:51:53Z'>
 		good catch. mind submitting a PR?
 		</comment>
 	</comments>
 </bug>
<commit id='bafdeca42f746aac59b4f0c1103264d7bff556db' author='Kevin Chen' date='2020-04-21 14:29:15-04:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\distrib_data_parallel.py' new_name='pytorch_lightning\trainer\distrib_data_parallel.py'>
 		<file_info nloc='294' complexity='60' token_count='1122'></file_info>
 		<method name='ddp_train' parameters='self,process_idx,model'>
 				<method_info nloc='39' complexity='12' token_count='299' nesting_level='1' start_line='280' end_line='359'></method_info>
 			<added_lines>330</added_lines>
 			<deleted_lines>330</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
