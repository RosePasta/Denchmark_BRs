<bug_data>
<bug id='316' author='ceyzaguirre4' open_date='2019-10-06T03:39:34Z' closed_time='2019-10-08T21:33:34Z'>
 	<summary>Custom logger (LightningLoggerBase) doesn't call log_hyperparams and log_metrics receives empty metrics dict</summary>
 	<description>
 Describe the bug
 Custom logger (LightningLoggerBase) doesn't call log_hyperparams and log_metrics receives empty metrics dict.
 To Reproduce
 from pytorch_lightning.logging import LightningLoggerBase, rank_zero_only
 
 class CustomLogger(LightningLoggerBase):
     def __init__(self, *args, **kwargs):
         super(CustomLogger, self).__init__()
         print("--"*30)
         print("it does at least initialize")
 
     @rank_zero_only
     def log_hyperparams(self, params):
         print("--"*30)
         print("this will never be printed")
 
     @rank_zero_only
     def log_metrics(self, metrics, step_num):
         print("--"*30)
         print("the following will be an empty dict {}")
         print(metrics)
 
 custom_logger = CustomLogger()
 trainer = Trainer(
         logger=custom_logger,
         max_nb_epochs=10
     )
 
 trainer.fit(model)
 Desktop (please complete the following information):
 
 OS: MacOS 10.13.6
 pytorch-lightning version 0.5.1
 
 I should add that model is defined correctly
     
 def validation_step(self, batch, batch_nb):
     """some processing"""
     return {
         'val_corrects': corrects(y, y_pred).item(),
         'val_ponder_costs': torch.mean(ponder_cost).item(),
         'val_steps': torch.mean(steps).item(),
     }
 
 def validation_end(self, outputs):
     n = 0
     all_corrects = 0
     ponder_costs = 0
     acc_steps = 0
 
     for output in outputs:
         all_corrects += output['val_corrects']
         ponder_costs += output['val_ponder_costs']
         acc_steps += output['val_steps']
 
     avgs = (elem / len(outputs) for elem in (all_corrects, ponder_costs, acc_steps))
     
     return {
             'val_corrects': all_corrects / len(outputs),
             'val_ponder_costs': ponder_costs / len(outputs),
             'val_steps': acc_steps / len(outputs),
         }
 	</description>
 	<comments>
 		<comment id='1' author='ceyzaguirre4' date='2019-10-06T16:21:15Z'>
 		&lt;denchmark-link:https://github.com/neggert&gt;@neggert&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='ceyzaguirre4' date='2019-10-07T00:30:49Z'>
 		&lt;denchmark-link:https://github.com/ceyzaguirre4&gt;@ceyzaguirre4&lt;/denchmark-link&gt;
  install the latest version and try again? i pushed a fix
 		</comment>
 		<comment id='3' author='ceyzaguirre4' date='2019-10-08T02:30:19Z'>
 		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
  The last update fixed the arguments passed to , however, the loggers and  methods are still never called.
 (on 0.5.13)
 		</comment>
 		<comment id='4' author='ceyzaguirre4' date='2019-10-08T11:43:06Z'>
 		Got it, &lt;denchmark-link:https://github.com/neggert&gt;@neggert&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='5' author='ceyzaguirre4' date='2019-10-08T17:22:35Z'>
 		Taking a look
 		</comment>
 		<comment id='6' author='ceyzaguirre4' date='2019-10-08T17:43:37Z'>
 		&lt;denchmark-link:https://github.com/ceyzaguirre4&gt;@ceyzaguirre4&lt;/denchmark-link&gt;
  I can reproduce  not being called, but  seems fine.
 Are your hyperparameters stored as an  in ?
 Here's the test I wrote. It makes it through up to the assert on logger.finalized.
 def test_custom_logger():
 
     class CustomLogger(LightningLoggerBase):
         def __init__(self):
             super().__init__()
             self.hparams_logged = None
             self.metrics_logged = None
             self.finalized = False
 
         @rank_zero_only
         def log_hyperparams(self, params):
             self.hparams_logged = params
 
         @rank_zero_only
         def log_metrics(self, metrics, step_num):
             self.metrics_logged = metrics
         
         @rank_zero_only
         def finalize(self):
             self.finalized = True
 
     hparams = get_hparams()
     model = LightningTestModel(hparams)
 
     logger = CustomLogger()
 
     trainer_options = dict(
         max_nb_epochs=1,
         train_percent_check=0.01,
         logger=logger
     )
 
     trainer = Trainer(**trainer_options)
     result = trainer.fit(model)
     assert result == 1, "Training failed"
     assert logger.hparams_logged == hparams
     assert logger.metrics_logged != {}
     assert logger.finalized
 		</comment>
 	</comments>
 </bug>
<commit id='8088052825276b72b650b67fd169b8f92b91401f' author='Nic Eggert' date='2019-10-08 17:33:33-04:00'>
 	<dmm_unit complexity='0.9473684210526315' interfacing='0.9473684210526315' size='0.34210526315789475'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\logging\mlflow_logger.py' new_name='pytorch_lightning\logging\mlflow_logger.py'>
 		<file_info nloc='46' complexity='12' token_count='307'></file_info>
 		<method name='finalize' parameters='self,status'>
 				<method_info nloc='4' complexity='2' token_count='29' nesting_level='1' start_line='55' end_line='58'></method_info>
 			<added_lines>56,57</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\trainer.py' new_name='pytorch_lightning\trainer\trainer.py'>
 		<file_info nloc='890' complexity='270' token_count='5624'></file_info>
 		<method name='__train' parameters='self'>
 				<method_info nloc='25' complexity='12' token_count='192' nesting_level='1' start_line='1032' end_line='1076'></method_info>
 			<added_lines>1075,1076</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1077</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\test_logging.py' new_name='tests\test_logging.py'>
 		<file_info nloc='120' complexity='12' token_count='694'></file_info>
 		<method name='test_custom_logger.__init__' parameters='self'>
 				<method_info nloc='5' complexity='1' token_count='27' nesting_level='2' start_line='141' end_line='145'></method_info>
 			<added_lines>141,142,143,144,145</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_custom_logger' parameters=''>
 				<method_info nloc='23' complexity='1' token_count='98' nesting_level='0' start_line='138' end_line='175'></method_info>
 			<added_lines>138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_custom_logger.finalize' parameters='self,status'>
 				<method_info nloc='2' complexity='1' token_count='12' nesting_level='2' start_line='156' end_line='157'></method_info>
 			<added_lines>156,157</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_custom_logger.log_metrics' parameters='self,metrics,step_num'>
 				<method_info nloc='2' complexity='1' token_count='14' nesting_level='2' start_line='152' end_line='153'></method_info>
 			<added_lines>152,153</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_custom_logger.log_hyperparams' parameters='self,params'>
 				<method_info nloc='2' complexity='1' token_count='12' nesting_level='2' start_line='148' end_line='149'></method_info>
 			<added_lines>148,149</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>10,176,177</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
