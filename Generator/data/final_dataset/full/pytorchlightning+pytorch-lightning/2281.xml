<bug_data>
<bug id='2281' author='Kshitij09' open_date='2020-06-19T20:22:20Z' closed_time='2020-06-20T11:38:48Z'>
 	<summary>RuntimeError: OrderedDict mutated during iteration</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 I was getting RuntimeError: OrderedDict mutated during iteration.
 It seems like using the same LightningModule object with ModelSummary and Trainer causes this error.
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 from pytorch_lightning.core.memory import ModelSummary
 model = CifarNet() # any pl module would work here
 ModelSummary(model,mode='full')
 trainer = Trainer(fast_dev_run=True,gpus=1)
 trainer.fit(model)
 Steps to reproduce the behavior:
 
 View model summary using ModelSummary class
 Call trainer.fit with same object.
 
 &lt;denchmark-h:h3&gt;Stacktrace&lt;/denchmark-h&gt;
 
 RuntimeError                              Traceback (most recent call last)
 &lt;ipython-input-20-8badc092c0ba&gt; in &lt;module&gt;()
       1 # Checking for errors
       2 trainer = Trainer(fast_dev_run=True,gpus=1)
 ----&gt; 3 trainer.fit(model)
 11 frames
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders)
     916 
     917         elif self.single_gpu:
 --&gt; 918             self.single_gpu_train(model)
     919 
     920         elif self.use_tpu:  # pragma: no-cover
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py in single_gpu_train(self, model)
     174             self.reinit_scheduler_properties(self.optimizers, self.lr_schedulers)
     175 
 --&gt; 176         self.run_pretrain_routine(model)
     177 
     178     def tpu_train(self, tpu_core_idx, model):
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in run_pretrain_routine(self, model)
    1091 
    1092         # CORE TRAINING LOOP
 -&gt; 1093         self.train()
    1094 
    1095     def test(
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py in train(self)
     373                 # RUN TNG EPOCH
     374                 # -----------------
 --&gt; 375                 self.run_training_epoch()
     376 
     377                 if self.max_steps and self.max_steps == self.global_step:
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py in run_training_epoch(self)
     456             # RUN TRAIN STEP
     457             # ---------------
 --&gt; 458             _outputs = self.run_training_batch(batch, batch_idx)
     459             batch_result, grad_norm_dic, batch_step_metrics, batch_output = _outputs
     460 
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py in run_training_batch(self, batch, batch_idx)
     632 
     633                 # calculate loss
 --&gt; 634                 loss, batch_output = optimizer_closure()
     635 
     636                 # check if loss or model weights are nan
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py in optimizer_closure()
     596                                                                     opt_idx, self.hiddens)
     597                         else:
 --&gt; 598                             output_dict = self.training_forward(split_batch, batch_idx, opt_idx, self.hiddens)
     599 
     600                         # format and reduce outputs accordingly
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py in training_forward(self, batch, batch_idx, opt_idx, hiddens)
     771             batch = self.transfer_batch_to_gpu(batch, gpu_id)
     772             args[0] = batch
 --&gt; 773             output = self.model.training_step(*args)
     774 
     775         # TPU support
 &lt;ipython-input-11-2482ebcf9d12&gt; in training_step(self, batch, batch_idx)
      55   def training_step(self,batch,batch_idx):
      56     x, y = batch
 ---&gt; 57     y_hat = self(x)
      58 
      59     return {'loss': F.cross_entropy(y_hat, y)}
 /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
     548             result = self._slow_forward(*input, **kwargs)
     549         else:
 --&gt; 550             result = self.forward(*input, **kwargs)
     551         for hook in self._forward_hooks.values():
     552             hook_result = hook(self, input, result)
 &lt;ipython-input-11-2482ebcf9d12&gt; in forward(self, x)
      11 
      12   def forward(self,x):
 ---&gt; 13     return self.model(x)
      14 
      15   def prepare_data(self):
 /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
     549         else:
     550             result = self.forward(*input, **kwargs)
 --&gt; 551         for hook in self._forward_hooks.values():
     552             hook_result = hook(self, input, result)
     553             if hook_result is not None:
 RuntimeError: OrderedDict mutated during iteration
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 We should be able to use same object with both the classes
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;* CUDA:
 	- GPU:
 		- Tesla T4
 	- available:         True
 	- version:           10.1
 * Packages:
 	- numpy:             1.18.5
 	- pyTorch_debug:     False
 	- pyTorch_version:   1.5.0+cu101
 	- pytorch-lightning: 0.8.1
 	- tensorboard:       2.2.2
 	- tqdm:              4.41.1
 * System:
 	- OS:                Linux
 	- architecture:
 		- 64bit
 		- 
 	- processor:         x86_64
 	- python:            3.6.9
 	- version:           #1 SMP Wed Feb 19 05:26:34 PST 2020
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='Kshitij09' date='2020-06-19T20:23:01Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 		<comment id='2' author='Kshitij09' date='2020-06-19T21:31:22Z'>
 		
 model = CifarNet() # any pl module would work here
 
 Could you paste the minimal code for CifarNet? I cannot reproduce it with PL examples, sorry.
 		</comment>
 		<comment id='3' author='Kshitij09' date='2020-06-19T22:59:56Z'>
 		Okay ! I'm not sure which part is pertaining to this issue, so here is the link to my &lt;denchmark-link:https://colab.research.google.com/drive/13ER3opHF3IacEfAyEWojuplBUHn2MBKU?usp=sharing&gt;colab notebook&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='4' author='Kshitij09' date='2020-06-20T08:48:54Z'>
 		Thanks, your notebook was very helpful. I fixed the bug here &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2298&gt;#2298&lt;/denchmark-link&gt;
 
 You can verify that it works by installing from
 !pip install --upgrade git+https://github.com/awaelchli/pytorch-lightning@bugfix/summary_hook_handles timm wandb
 in the first cell of your notebook.
 		</comment>
 	</comments>
 </bug>
<commit id='f972ab3a828eae1847a793da0b2c25c6074647a4' author='Adrian W√§lchli' date='2020-06-20 07:38:47-04:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>21,22</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\core\memory.py' new_name='pytorch_lightning\core\memory.py'>
 		<file_info nloc='307' complexity='58' token_count='1445'></file_info>
 		<method name='summarize' parameters='self'>
 				<method_info nloc='7' complexity='4' token_count='64' nesting_level='1' start_line='194' end_line='200'></method_info>
 			<added_lines>198,199</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_register_hook' parameters='self'>
 				<method_info nloc='11' complexity='1' token_count='19' nesting_level='1' start_line='61' end_line='78'></method_info>
 			<added_lines>61,63,64,66,67,68,76</added_lines>
 			<deleted_lines>69,74,78</deleted_lines>
 		</method>
 		<method name='out_size' parameters='self'>
 				<method_info nloc='2' complexity='2' token_count='18' nesting_level='1' start_line='93' end_line='94'></method_info>
 			<added_lines>93</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_register_hook.hook' parameters='module,inp,out'>
 				<method_info nloc='6' complexity='2' token_count='46' nesting_level='2' start_line='71' end_line='76'></method_info>
 			<added_lines>76</added_lines>
 			<deleted_lines>74</deleted_lines>
 		</method>
 		<method name='detach_hook' parameters='self'>
 				<method_info nloc='3' complexity='2' token_count='21' nesting_level='1' start_line='80' end_line='86'></method_info>
 			<added_lines>80,81,82,83,84,85,86</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__del__' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='10' nesting_level='1' start_line='58' end_line='59'></method_info>
 			<added_lines>58,59</added_lines>
 			<deleted_lines>59</deleted_lines>
 		</method>
 		<method name='in_size' parameters='self'>
 				<method_info nloc='2' complexity='2' token_count='18' nesting_level='1' start_line='89' end_line='90'></method_info>
 			<added_lines>89</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>10,60,87</added_lines>
 			<deleted_lines>57,60</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\core\test_memory.py' new_name='tests\core\test_memory.py'>
 		<file_info nloc='157' complexity='15' token_count='1370'></file_info>
 		<method name='test_hooks_removed_after_summarize' parameters='mode'>
 				<method_info nloc='6' complexity='2' token_count='52' nesting_level='0' start_line='96' end_line='103'></method_info>
 			<added_lines>96,97,98,99,100,101,102,103</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>92,93,94,95,104,105</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
