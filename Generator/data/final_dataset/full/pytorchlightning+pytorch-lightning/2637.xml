<bug_data>
<bug id='2637' author='JiangYize' open_date='2020-07-18T09:57:26Z' closed_time='2020-08-11T23:28:39Z'>
 	<summary>to() got an unexpected keyword argument 'non_blocking' for DGLGraph</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 I use dgl library to make a gnn and batch the DGLGraph.
 No problem during training, but in test, I got a TypeError: to() got an unexpected keyword argument 'non_blocking'
 &lt;class 'dgl.graph.DGLGraph'&gt; .to() function has no keyword argument 'non_blocking'
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 
 OS: Linux
 CUDA: 10.1
 Python Version: 3.7
 PyTorch Version: 1.5.1
 DGL Version: 0.4.3post2
 PyTorch-Lightning Version: 0.8.5
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;   File "../src/main.py", line 131, in &lt;module&gt;
     run(params)
   File "../src/main.py", line 92, in run
     trainer.test(model)
   File "/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1279, in test
     results = self.__test_given_model(model, test_dataloaders)
   File "/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1346, in __test_given_model
     results = self.fit(model)
   File "/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1003, in fit
     results = self.single_gpu_train(model)
   File "/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py", line 186, in single_gpu_train
     results = self.run_pretrain_routine(model)
   File "/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1166, in run_pretrain_routine
     results = self.run_evaluation(test_mode=True)
   File "/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 391, in run_evaluation
     eval_results = self._evaluate(self.model, dataloaders, max_batches, test_mode)
   File "/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 293, in _evaluate
     output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)
   File "/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 458, in evaluation_forward
     batch = self.transfer_batch_to_gpu(batch, root_gpu)
   File "/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py", line 159, in transfer_batch_to_gpu
     return self.__transfer_batch_to_device(batch, device)
   File "/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py", line 164, in __transfer_batch_to_device
     return model.transfer_batch_to_device(batch, device)
   File "/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/core/hooks.py", line 242, in transfer_batch_to_device
     return move_data_to_device(batch, device)
   File "/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/utilities/apply_func.py", line 109, in move_data_to_device
     return apply_to_collection(batch, dtype=(TransferableDataType, Batch), function=batch_to)
   File "/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/utilities/apply_func.py", line 40, in apply_to_collection
     for k, v in data.items()})
   File "/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/utilities/apply_func.py", line 40, in &lt;dictcomp&gt;
     for k, v in data.items()})
   File "/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/utilities/apply_func.py", line 35, in apply_to_collection
     return function(data, *args, **kwargs)
   File "/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/utilities/apply_func.py", line 107, in batch_to
     return data.to(device, non_blocking=True)
 TypeError: to() got an unexpected keyword argument 'non_blocking'
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='JiangYize' date='2020-07-18T09:58:17Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 		<comment id='2' author='JiangYize' date='2020-07-22T14:16:42Z'>
 		Having the same problem; it's because  (&lt;denchmark-link:https://docs.dgl.ai/en/0.4.x/generated/dgl.DGLGraph.to.html#dgl.DGLGraph.to&gt;docs&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://docs.dgl.ai/en/0.4.x/_modules/dgl/graph.html#DGLGraph.to&gt;source&lt;/denchmark-link&gt;
 ) doesn't take the non_blocking argument. Example:
 dgl.DGLGraph().to('cuda', non_blocking=True)
 Here's my temporary solution:
 class LightningDGLGraph(DGLGraph):
     def to(self, ctx, *args, **kwargs):
         return super().to(torch.device(ctx))
 
 g = LightningDGLGraph()
 g.to('cuda', non_blocking=True)
 Works, but probably not ideal.
 		</comment>
 		<comment id='3' author='JiangYize' date='2020-07-22T20:23:12Z'>
 		
 Having the same problem; it's because DGLGraph.to (docs, source) doesn't take the non_blocking argument. Example:
 dgl.DGLGraph().to('cuda', non_blocking=True)
 Here's my temporary solution:
 class LightningDGLGraph(DGLGraph):
     def to(self, ctx, *args, **kwargs):
         return super().to(torch.device(ctx))
 
 g = LightningDGLGraph()
 g.to('cuda', non_blocking=True)
 Works, but probably not ideal.
 
 Hi, I wonder how this will work if using dgl.batch? the class type they return to you is a DGLGraph.
 Ok, they also have this quick fix here: &lt;denchmark-link:https://github.com/dmlc/dgl/pull/1600&gt;dmlc/dgl#1600&lt;/denchmark-link&gt;
 .
 so uninstall the stable version and install the latest version from main solves my problem:
 &lt;denchmark-code&gt;pip install --pre dgl           # For CPU Build
 pip install --pre dgl-cu90      # For CUDA 9.0 Build
 pip install --pre dgl-cu92      # For CUDA 9.2 Build
 pip install --pre dgl-cu100     # For CUDA 10.0 Build
 pip install --pre dgl-cu101     # For CUDA 10.1 Build
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='4' author='JiangYize' date='2020-07-22T20:54:33Z'>
 		It seems to work for me for now.
 		</comment>
 		<comment id='5' author='JiangYize' date='2020-07-26T16:16:30Z'>
 		
 Ok, they also have this quick fix here: dmlc/dgl#1600.
 so uninstall the stable version and install the latest version from main solves my problem:
 
 Just saw your edit. This seems to work if I don't specify the number of gpus; when I do, same error. E: It's the distributed backend; it never calls graph.to. You can throw a 0/0 in there and it'll never break with distributed_backend ddp.
 		</comment>
 		<comment id='6' author='JiangYize' date='2020-08-01T17:12:01Z'>
 		For a clean solution in Lightning, override &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.hooks.html?highlight=transfer#pytorch_lightning.core.hooks.ModelHooks.transfer_batch_to_device&gt;this&lt;/denchmark-link&gt;
  model hook and call .to() yourself on the graph object.
 		</comment>
 		<comment id='7' author='JiangYize' date='2020-08-01T17:14:14Z'>
 		Regarding ddp, is DGLGraph supposed to work with that (I mean in plain pytorch)? I don't think it can work with scatter and gather.
 		</comment>
 		<comment id='8' author='JiangYize' date='2020-08-11T00:05:50Z'>
 		
 For a clean solution in Lightning, override this model hook and call .to() yourself on the graph object.
 
 &lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
  Is this supposed to be overridden in the model? It doesn't seem to get called for me in a distributed setting.
 		</comment>
 		<comment id='9' author='JiangYize' date='2020-08-11T02:57:55Z'>
 		&lt;denchmark-link:https://github.com/jacobdanovitch&gt;@jacobdanovitch&lt;/denchmark-link&gt;
  Yes, this hook only works for single gpu, because in distributed we need to scatter and gather a batch, and if it is a custom object we don't know how to do that. For this, you would have to define your own DistributedDataParallel module and configure it in the &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.html#pytorch_lightning.core.LightningModule.configure_ddp&gt;configure_ddp &lt;/denchmark-link&gt;
 model hook. We should probably update the docs regarding that.
 		</comment>
 	</comments>
 </bug>
<commit id='69d241c82e10cf40e5787fb39bb808687d693b57' author='Adrian W√§lchli' date='2020-08-11 19:28:37-04:00'>
 	<dmm_unit complexity='1.0' interfacing='0.65' size='0.75'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>126,127</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\core\hooks.py' new_name='pytorch_lightning\core\hooks.py'>
 		<file_info nloc='232' complexity='28' token_count='401'></file_info>
 		<method name='transfer_batch_to_device' parameters='self,Any,device'>
 				<method_info nloc='49' complexity='1' token_count='25' nesting_level='1' start_line='276' end_line='324'></method_info>
 			<added_lines>312,313,314,315,316,317,318</added_lines>
 			<deleted_lines>312,313</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\utilities\apply_func.py' new_name='pytorch_lightning\utilities\apply_func.py'>
 		<file_info nloc='73' complexity='18' token_count='388'></file_info>
 		<method name='move_data_to_device.batch_to' parameters='data'>
 				<method_info nloc='9' complexity='5' token_count='81' nesting_level='1' start_line='96' end_line='108'></method_info>
 			<added_lines>107,108</added_lines>
 			<deleted_lines>107</deleted_lines>
 		</method>
 		<method name='move_data_to_device' parameters='Any,device'>
 				<method_info nloc='3' complexity='1' token_count='33' nesting_level='0' start_line='78' end_line='110'></method_info>
 			<added_lines>107,108</added_lines>
 			<deleted_lines>107</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\models\test_gpu.py' new_name='tests\models\test_gpu.py'>
 		<file_info nloc='304' complexity='39' token_count='2834'></file_info>
 		<method name='test_non_blocking.to' parameters='self,args,kwargs'>
 				<method_info nloc='2' complexity='1' token_count='12' nesting_level='2' start_line='402' end_line='403'></method_info>
 			<added_lines>402,403</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_non_blocking' parameters=''>
 				<method_info nloc='12' complexity='1' token_count='115' nesting_level='0' start_line='391' end_line='408'></method_info>
 			<added_lines>391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>2,388,389,390</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
