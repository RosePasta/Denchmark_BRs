<bug_data>
<bug id='2479' author='jgbos' open_date='2020-07-03T00:34:47Z' closed_time='2020-10-05T01:27:52Z'>
 	<summary>init_slurm_connection causing hostname errors</summary>
 	<description>
 &lt;denchmark-h:h1&gt;Problem&lt;/denchmark-h&gt;
 
 Can you update this function to support checking if MASTER_ADDR and MASTER_PORT are already in os.environ?  Running into some weird errors where this code adds host to MASTER_ADDR and crashes the code.
 
 
 
 pytorch-lightning/pytorch_lightning/core/lightning.py
 
 
          Line 903
       in
       0697dd3
 
 
 
 
 
 
  def _init_slurm_connection(self) -&gt; None: 
 
 
 
 
 
 &lt;denchmark-h:h1&gt;Solution&lt;/denchmark-h&gt;
 
 Do you prefer a pull request?  Here's the check I put it
 def _init_slurm_connection(self) -&gt; None:
         """
         Sets up environment variables necessary for pytorch distributed communications
         based on slurm environment.
         """
 
         if 'MASTER_PORT' not in os.environ:
             # use slurm job id for the port number
             # guarantees unique ports across jobs from same grid search
             try:
                 # use the last 4 numbers in the job id as the id
                 default_port = os.environ['SLURM_JOB_ID']
                 default_port = default_port[-4:]
 
                 # all ports should be in the 10k+ range
                 default_port = int(default_port) + 15000
 
             except Exception:
                 default_port = 12910
 
             # if user gave a port number, use that one instead
             try:
                 default_port = os.environ['MASTER_PORT']
             except Exception:
                 os.environ['MASTER_PORT'] = str(default_port)
 
         # figure out the root node addr
         if 'MASTER_ADDR' not in os.environ:
             try:
                 root_node = os.environ['SLURM_NODELIST'].split(' ')[0]
             except Exception:
                 root_node = '127.0.0.1'
 
             root_node = self.trainer.resolve_root_node_address(root_node)
             os.environ['MASTER_ADDR'] = root_node
 	</description>
 	<comments>
 		<comment id='1' author='jgbos' date='2020-07-13T15:05:21Z'>
 		Ok, back on this and tracked down the bug.  The problem is in this line:
   root_node = os.environ['SLURM_NODELIST'].split(' ')[0]
 On my system, SLURM_NODELIST is a comma delineated list, so split(',') instead of split(' ').
 		</comment>
 		<comment id='2' author='jgbos' date='2020-07-13T15:10:02Z'>
 		Additionally, our system will use this for two systems:
  SLURM_NODE_LIST=d-10-11-[1-2]
 		</comment>
 		<comment id='3' author='jgbos' date='2020-08-04T20:52:07Z'>
 		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
  is this fixed?
 		</comment>
 		<comment id='4' author='jgbos' date='2020-08-26T14:02:47Z'>
 		fyi, this is not fixed in general.  I think the problem is fixed for the example above
 SLURM_NODE_LIST=d-10-11-[1-2]
 but not the case for
 SLURM_NODE_LIST=d-10-11-1,d-11-12-1
 My current hack for PL to run on slurm is to set MASTER_ADDR in the sbatch script and then do the following:
     # fix slurm node list
     if 'MASTER_ADDR' in os.environ and 'SLURM_NODELIST' in os.environ:
         slurm_node_list = os.environ['SLURM_NODELIST'].split(',')
         master_addr = os.environ['MASTER_ADDR']
         os.environ['SLURM_NODELIST'] = ' '.join([master_addr] + slurm_node_list)
 		</comment>
 		<comment id='5' author='jgbos' date='2020-09-22T21:27:25Z'>
 		&lt;denchmark-link:https://github.com/jgbos&gt;@jgbos&lt;/denchmark-link&gt;
  mind sending a PR for this fix?
 		</comment>
 		<comment id='6' author='jgbos' date='2020-09-23T03:28:37Z'>
 		&lt;denchmark-link:https://github.com/edenlightning&gt;@edenlightning&lt;/denchmark-link&gt;
  I currently do this before I run lightning so I  feel my solution is just a  and that there's probably a better solution.  It currently requires the user to set  first.  If I find some time I'll take a look at the code, maybe come upon a real solution once I see how lightning works here.
 		</comment>
 		<comment id='7' author='jgbos' date='2020-10-05T01:27:52Z'>
 		this is fixed now! no longer needed to hack anything. You can pass your own ClusterEnvironment and do whatever logic you need.
 &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/trainer.html#cluster-environment&gt;https://pytorch-lightning.readthedocs.io/en/latest/trainer.html#cluster-environment&lt;/denchmark-link&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='c6df63a58817b6414f8a3ae28edd9e6552be3914' author='William Falcon' date='2020-10-04 21:30:33-04:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\__init__.py' new_name='pytorch_lightning\trainer\__init__.py'>
 		<file_info nloc='1125' complexity='0' token_count='24'></file_info>
 		<modified_lines>
 			<added_lines>361,369,382,383</added_lines>
 			<deleted_lines>368</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
