<bug_data>
<bug id='3303' author='NumesSanguis' open_date='2020-09-01T09:37:48Z' closed_time='2020-09-21T09:46:50Z'>
 	<summary>AUROC metric should throw an error when used for multi-class problems</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 AUROC accepts multi-class input without throwing an error. Instead, it gives a random value, which gives the illusion that it is working.
 Background: &lt;denchmark-link:https://forums.pytorchlightning.ai/t/pytorch-lightning-auroc-value-for-multi-class-seems-to-be-completely-off-compared-to-sklearn-using-it-wrong/61/7&gt;https://forums.pytorchlightning.ai/t/pytorch-lightning-auroc-value-for-multi-class-seems-to-be-completely-off-compared-to-sklearn-using-it-wrong/61/7&lt;/denchmark-link&gt;
 
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 Steps to reproduce the behavior:
 
 Manually create some multi-class arrays
 Use PyTorch Lightning's AUROC() metric
 Use sklearn's AUROC metric
 Observe values not matching
 
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 import torch
 import sklearn.metrics
 import pytorch_lightning as pl
 from pytorch_lightning.metrics.classification import AUROC
 
 pl.seed_everything(0)
 auroc = AUROC()
 
 def test_auroc_sk_multiclass():
     for i in range(100):
         target = torch.randint(0, 3, size=(10,))  # 2 --&gt; 3
         pred = torch.rand(10, 3).softmax(dim=1)  # torch.randint(0, 2, size=(10, ))
         score_sk = sklearn.metrics.roc_auc_score(target.numpy(), pred.numpy(), multi_class='ovo', labels=[0, 1, 2])
         score_pl = auroc(pred, target)
         print(score_sk, score_pl)
         assert torch.allclose(torch.tensor(score_pl).float(), torch.tensor(score_sk).float())
 
 test_auroc_sk_multiclass()
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 
 Throw error that multi-class AUROC has not been implemented (yet).
 Note in documentation that the AUROC metric does not support multi-class yet ()
 
 &lt;denchmark-h:h3&gt;Actual behavior&lt;/denchmark-h&gt;
 
 Giving a random value, giving a false sense that it is working.
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;* CUDA:
 	- GPU:
 		- GeForce GTX 1080 Ti
 	- available:         True
 	- version:           10.2
 * Packages:
 	- numpy:             1.19.1
 	- pyTorch_debug:     False
 	- pyTorch_version:   1.6.0
 	- pytorch-lightning: 0.9.0
 	- tensorboard:       2.2.0
 	- tqdm:              4.48.2
 * System:
 	- OS:                Linux
 	- architecture:
 		- 64bit
 		- 
 	- processor:         x86_64
 	- python:            3.7.7
 	- version:           #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 The output value is nonsense in the multi-class classification, instead of an error. That's why I thought it was appropriate to file it as a bug instead of a feature request / documentation improvement. I'm aware that the AUROC implementation is not intended to be for multi-class after the discussion on the PyTorch Lightning forum.
 I used the AUROC value and noticed it was wrong after training a few models, but it will take many people off-guard in it's current form.
 Feature request for MulticlassAUROC: &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/3304&gt;#3304&lt;/denchmark-link&gt;
 
 	</description>
 	<comments>
 	</comments>
 </bug>
<commit id='b1347c956af4752560b53b891d352c48c6050305' author='Nicki Skafte' date='2020-09-21 11:46:48+02:00'>
 	<dmm_unit complexity='0.0' interfacing='0.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>20</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\metrics\classification.py' new_name='pytorch_lightning\metrics\classification.py'>
 		<file_info nloc='631' complexity='35' token_count='1891'></file_info>
 		<modified_lines>
 			<added_lines>377,380</added_lines>
 			<deleted_lines>377,380</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\metrics\functional\classification.py' new_name='pytorch_lightning\metrics\functional\classification.py'>
 		<file_info nloc='900' complexity='30' token_count='3579'></file_info>
 		<modified_lines>
 			<added_lines>587,590,592,685,688,690,861,863,865,866,867,868</added_lines>
 			<deleted_lines>587,590,592,685,688,690,861,863</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\metrics\functional\test_classification.py' new_name='tests\metrics\functional\test_classification.py'>
 		<file_info nloc='358' complexity='33' token_count='6228'></file_info>
 		<method name='test_against_sklearn' parameters='sklearn_metric,torch_metric'>
 				<method_info nloc='10' complexity='3' token_count='147' nesting_level='0' start_line='48' end_line='61'></method_info>
 			<added_lines>48,49,50,51,52,53,55,56,59,60,61</added_lines>
 			<deleted_lines>48,49,52,53,59,61</deleted_lines>
 		</method>
 		<method name='test_error_on_multiclass_input' parameters='metric'>
 				<method_info nloc='5' complexity='1' token_count='56' nesting_level='0' start_line='406' end_line='411'></method_info>
 			<added_lines>406,407,408,409,410,411</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_against_sklearn' parameters='sklearn_metric,torch_metric,only_binary'>
 				<method_info nloc='16' complexity='7' token_count='231' nesting_level='0' start_line='55' end_line='79'></method_info>
 			<added_lines>55,56,59,60,61,62,63,64,71,72,73,74,75,76,77,78,79</added_lines>
 			<deleted_lines>59,61</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>13,14,15,42,43,44,45,46,47,405,412,413,458,459</added_lines>
 			<deleted_lines>39,40,41,42,43,44,45,46,453,454,455,456</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\metrics\test_classification.py' new_name='tests\metrics\test_classification.py'>
 		<file_info nloc='172' complexity='24' token_count='1852'></file_info>
 		<method name='test_auroc' parameters='pos_label'>
 				<method_info nloc='6' complexity='1' token_count='93' nesting_level='0' start_line='117' end_line='123'></method_info>
 			<added_lines>121</added_lines>
 			<deleted_lines>121</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>116</added_lines>
 			<deleted_lines>116</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
