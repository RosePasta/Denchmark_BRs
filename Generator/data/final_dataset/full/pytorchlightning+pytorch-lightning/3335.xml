<bug_data>
<bug id='3335' author='junwen-austin' open_date='2020-09-02T21:51:08Z' closed_time='2020-10-05T17:52:47Z'>
 	<summary>Cannot replicate training results with seed_everything and deterministic flag = True with DDP</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 I noticed this when I was adding more metrics calculation to the LightningModule, for example, adding the confusion matrix at the end of validation/test epoch.  Before and after I added these functions (which do not appear to be dependent on any random seed), I noticed the training results are not the exactly the same.
 However, once I added these function and re-ran again, yes I got the same training results.
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 The training results should be identical even if some deterministic functions are added
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 Please copy and paste the output from our
 &lt;denchmark-link:https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py&gt;environment collection script&lt;/denchmark-link&gt;
 
 (or fill out the checklist below manually).
 You can get the script and run it with:
 &lt;denchmark-code&gt;wget https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py
 # For security purposes, please check the contents of collect_env_details.py before running it.
 python collect_env_details.py
 &lt;/denchmark-code&gt;
 
 
 PyTorch Version (e.g., 1.0):  1.4
 OS (e.g., Linux):  Linux
 How you installed PyTorch (conda, pip, source): pip
 Build command you used (if compiling from source):
 Python version: 3.7
 CUDA/cuDNN version: 10.1
 GPU models and configuration: 4 GPUs DDP
 Any other relevant information:
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='junwen-austin' date='2020-09-04T04:12:37Z'>
 		Thanks for the report. We're also seeing something similar in our CI, unfortunately we don't have a lead yet.
 I did not exactly understand how you came to the conclusion that it is related to metrics. Did you try several runs with and without having metric? I don't think it is metrics, but rather just a ddp issue.
 		</comment>
 		<comment id='2' author='junwen-austin' date='2020-09-04T16:20:52Z'>
 		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
  Thanks for your follow-up. No, I do not think this issue has anything to do with the metrics that I was adding but more likely to DDP.
 		</comment>
 		<comment id='3' author='junwen-austin' date='2020-09-20T21:37:48Z'>
 		&lt;denchmark-link:https://github.com/junwen-austin&gt;@junwen-austin&lt;/denchmark-link&gt;
  I may have fixed this problem with the linked PR. But to be sure it closes this issue, could you let me know the trainer flags you used? Did you use  (is also the default) together with
 ?
 		</comment>
 		<comment id='4' author='junwen-austin' date='2020-09-20T21:46:17Z'>
 		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
  Thanks for letting me know the progress. I used the ddp as the backend, not the ddp_spawn. Thanks
 		</comment>
 		<comment id='5' author='junwen-austin' date='2020-09-20T21:54:56Z'>
 		hmm, that's unfortunate because my fix only applies to ddp_spawn.
 for ddp, I ran several scripts (including my research) and always got deterministic behavior, so I am not sure what what the cause is in your case :(
 		</comment>
 		<comment id='6' author='junwen-austin' date='2020-09-20T22:25:38Z'>
 		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
  it is rather a tricky one. I have two versions of the Lightning models v1 and v2. The only difference between them is that I added additional metric (confusion matrix in this case) in v2, and I noticed the training/validation/test results are slightly off, with both case having ddp as backend, same seed for seed_everything and deterministic flag = True.
 Since the additional code about the confusion matrix has nothing to do with randomization, I expect I should get the exactly the same results.
 S
 		</comment>
 		<comment id='7' author='junwen-austin' date='2020-09-20T23:41:29Z'>
 		but the confusion matrix is not used for training, right? It is just there for visualization?
 So, just to clarify to be 100% sure we're talking about the same
 
 you get the same training loss if you rerun v1 multiple times
 you get the same training loss if you rerun v2 multiple times
 the training loss between v1 and v2 are however different
 
 sorry if I misunderstood
 		</comment>
 		<comment id='8' author='junwen-austin' date='2020-09-21T00:56:38Z'>
 		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 
 
 
 Confusion matrix is used at the end of validation/test loop but not during the training loop
 
 
 you get the same training loss if you rerun v1 multiple times: Yes, also same validation/test loss
 
 
 you get the same training loss if you rerun v2 multiple times: Yes, also same validation/test loss
 
 
 the training loss between v1 and v2 are however different: Yes, also different validation/test loss
 
 
 Sorry about any miscommunication on my part. Thanks
 		</comment>
 		<comment id='9' author='junwen-austin' date='2020-09-22T20:41:58Z'>
 		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
  able to repro? any clue?
 		</comment>
 		<comment id='10' author='junwen-austin' date='2020-09-22T21:14:16Z'>
 		Unfortunately not. I tried to reproduce by adding the confusion matrix to the validation epoch end as described, but this gives me same val and train loss as it is expected with setting the seed. &lt;denchmark-link:https://github.com/junwen-austin&gt;@junwen-austin&lt;/denchmark-link&gt;
  I think I need an example code from you or I don't know where to look for the problem.
 		</comment>
 		<comment id='11' author='junwen-austin' date='2020-09-22T21:38:18Z'>
 		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
  Thanks for looking into it. Sure, the pseudo code looks like:
 In v1, there is no confusion matrix at all or any calculation related to it.
 in v2, I added the following in the __init__ function:
 self.confusion_matrix = ConfusionMatrix()
 then at the end of validation/test epoch, I calculated confusion matrix based on sync'ed y and y_pred and then log each component of confusion matrix. Note: in both v1 and v2, y and y_pred are sync'ed before calculating any metric
 &lt;denchmark-code&gt;           def validation_epoch_end(self, result):
                y = result['y']
                y_pred = result['y_pred']
                
                if use_ddp:
                    sync y
                    sync y_pred
             
               # calculate other metrics
                    ...
              
               # start the code only in v2:
               cm = self.confusion_matrix(y_pred, y)
               
              # log each component of cm:
              self.logger.add_scalar('TP', cm[1,1], global_step=self.global_step)
              self.logger.add_scalar('FP', cm[0,1], global_step=self.global_step)
              self.logger.add_scalar('FN', cm[1,0], global_step=self.global_step)
              self.logger.add_scalar('TN', cm[0,0], global_step=self.global_step)
              
              # end the code only in v2
 
 
 
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='12' author='junwen-austin' date='2020-09-23T09:11:35Z'>
 		I had it almost like this, but without sync ddp. Can you share the whole runnable script? At this point I can only make guesses.
 		</comment>
 		<comment id='13' author='junwen-austin' date='2020-09-23T14:13:21Z'>
 		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 
 Unfortunately I cannot share the entire script but I will provide more information in hoping of getting what you need.
 I have the following helper function with the Lightning Module:
 &lt;denchmark-code&gt;def calculate(self, batch):
     data, y = batch
     logit = self.model(data).     # y_hat is probability
     loss = torch.nn.functional.CrossEntropy(logit, y)
     y_hat = torch.nn.Softmax(dim=1)(logit)     
     _, y_pred = torch.max(y_hat, dim=1)    # y_pred is the predicted label
    
    return loss, y, y_hat, y_pred
 
 def compute_metrics(self, y, y_hat, y_pred):
       # compute metrics given y, y_hat, y_pred
       acc = self.accuracy(y_pred, y)/self.trainer.world_size
       f1 = self.f1(y_pred,y) / self.trainer.world_size
       cm = self.confusion_matrix(y_pred, y)    # added in v2
      
       return acc, f1, cm
  
 @staticmethod
 def log_metrics(result, kind, scalars=('loss','accuracy','f1'):
         for scalar in scalars:
                   result.log(scalar + '/' + kind, scalar, on_step=False, on_epoch=True, sync_dist=True, sync_dist_op='mean')
 
 
 def train_valid_test_step(self, batch, kind):
           loss, y, y_hat, y_pred = self.calculate(batch)
           if kind == 'train':
                 result = pl.TrainResult(loss)
           else:
                 result = pl.EvalResult(checkpoint_on=loss)
                 result.val_loss = loss
 
           result.y =y
           result.y_hat = y_hat
           result.y_pred = y_pred
  
 def sync_across_gpus(self, t):   # t is a tensor
        
         gather_t_tensor = [torch.ones_like(t) for _ in range(self.trainer.world_size)]
         torch.distributed.all_gather(gather_t_tensor, t)
         return torch.cat(gather_t_tensor)          
 
 def valid_test_epoch_end(self, result, kind):
     loss = result.val_loss
     y = result.y
     y_hat = result.y_hat
     y_pred = result.y_pred
 
    if self.trainer.use_ddp:
           loss = self.sync_across_gpus(loss)
           y = self.sync_across_gpus(y)
           y_pred = self.sync_across_gpus(y_pred)
           y_hat = self.sync_across_gpus(y_hat)
          
    acc, f1, cm = self.compute_metrics(y, y_hat, y_pred)
    result.loss = loss.mean()
    result.accurancy = acc
    result.f1 = f1
    result.cm = cm # added in v2
    
    log_metrics(result, kind, scalars=('loss','accuracy','f1')
 
    # added in v2
    self.logger.add_scalar('TP', cm[1,1], global_step=self.global_step)
    self.logger.add_scalar('FP', cm[0,1], global_step=self.global_step)
    self.logger.add_scalar('FN', cm[1,0], global_step=self.global_step)
    self.logger.add_scalar('TN', cm[0,0], global_step=self.global_step)
 
 
 def training_step(self, batch, batch_idx):
      return self.train_valid_test_step(batch, 'train')
 
 def train_step_end(self, result):
      loss = result.minimize
      result.log('loss/train', loss, on_step=True, sync_dist=True, sync_dist_op='mean')
      return result
 
 
 def validation_step(self, batch, batch_idx):
      return self.train_valid_test_step(batch, 'valid')
 
 def validation_epoch_end(self, result):
      
      return self.valid_test_epoch_end(result,'valid')
 
 def test_step(self, batch, batch_idx):
      return self.train_valid_test_step(batch, 'test')
 
 def test_epoch_end(self, result):
      
      return self.valid_test_epoch_end(result,'test')
     
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='14' author='junwen-austin' date='2020-09-28T05:42:10Z'>
 		still no luck reproducing this. since you cannot share all the code, I had tried to combine what you show here with an existing example code, but no luck.
 maybe if you tried yourself to adapt our &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/basic_examples/mnist.py&gt;mnist example&lt;/denchmark-link&gt;
  with your confusion matrix use case we get a reproducible script. I tried but I failed.
 		</comment>
 		<comment id='15' author='junwen-austin' date='2020-09-28T16:58:43Z'>
 		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 
 Thanks I will do that and let you know.
 		</comment>
 		<comment id='16' author='junwen-austin' date='2020-10-05T17:52:47Z'>
 		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
  My apology I cannot replicate the issue seen from my work project. Let's close the issue for now and if I spot a similar one, I will have better documentation of it instead of trying to getting it from top of my head. Thanks.
 		</comment>
 	</comments>
 </bug>
<commit id='a71d62d8409f4960a4b438b8d19c924d3636c73f' author='Adrian W√§lchli' date='2020-09-20 19:42:58-04:00'>
 	<dmm_unit complexity='0.7272727272727273' interfacing='0.7272727272727273' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>71</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\accelerators\ddp_base_backend.py' new_name='pytorch_lightning\accelerators\ddp_base_backend.py'>
 		<file_info nloc='102' complexity='26' token_count='788'></file_info>
 		<method name='ddp_train_tmp' parameters='self,process_idx,mp_queue,model,is_master,proc_offset'>
 				<method_info nloc='39' complexity='8' token_count='327' nesting_level='1' start_line='89' end_line='182'></method_info>
 			<added_lines>101,102,103,104,105</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>14,25</added_lines>
 			<deleted_lines>14</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\utilities\seed.py' new_name='pytorch_lightning\utilities\seed.py'>
 		<file_info nloc='44' complexity='6' token_count='223'></file_info>
 		<method name='seed_everything' parameters='None'>
 				<method_info nloc='33' complexity='5' token_count='164' nesting_level='0' start_line='27' end_line='62'></method_info>
 			<added_lines>28,29,30,31,32,33,34,35,36,37,44,45,57</added_lines>
 			<deleted_lines>28,29,36,37,38</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
