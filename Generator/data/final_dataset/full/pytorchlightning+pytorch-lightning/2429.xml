<bug_data>
<bug id='2429' author='Uroc327' open_date='2020-06-30T13:58:35Z' closed_time='2020-07-01T11:53:20Z'>
 	<summary>Batched iterative dataloading disables validation</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 Setting the batch_size parameter for torch.utils.data.DataLoader to a number greater than 1, prevents validation_step and validation_epoch_end from being called.
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 Steps to reproduce the behavior:
 
 Run python main.py with bs = 1
 Observe exception raised in validation_step
 Run python main.py after changing to bs = 2
 Observe the model train successfully
 
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 import pytorch_lightning as pl
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from torch.utils.data import DataLoader, IterableDataset
 
 class Dataset(IterableDataset):
     def __init__(self):
         super().__init__()
 
     def __iter__(self):
         for _ in range(1024):
             yield torch.randn(20)
 
     def __len__(self):
         return 1024
 
 class Model(pl.LightningModule):
     def __init__(self):
         super().__init__()
         self.fst = nn.Linear(20, 1)
         self.snd = nn.Linear(1, 20)
 
     def forward(self, x):
         x = self.fst(x)
         x = F.relu(x)
         x = self.snd(x)
         return x
 
     def training_step(self, batch, batchIdx):
         x = self.forward(batch)
         return {'loss': F.mse_loss(x, batch)}
 
     def validation_step(self, batch, batchIdx):
         raise NotImplementedError()
         x = self.forward(batch)
         return {'val_loss': F.mse_loss(x, batch)}
 
     def validation_epoch_end(self, outputs):
         return {'val_loss': torch.mean(torch.stack([x['val_loss'] for x in outputs]))}
 
     def configure_optimizers(self):
         return torch.optim.AdamW(self.parameters())
 
 if __name__ == '__main__':
     trainer = pl.Trainer(num_sanity_val_steps=0)
     net = Model()
     dataset = Dataset()
 
     bs = 2
     trainer.fit(net, train_dataloader=DataLoader(dataset, batch_size=bs), val_dataloaders=DataLoader(dataset, batch_size=bs))
 &lt;denchmark-code&gt;&gt; python main.py # with bs=2, should fail
 GPU available: True, used: False
 TPU available: False, using: 0 TPU cores
 
   | Name | Type   | Params
 --------------------------------
 0 | fst  | Linear | 21    
 1 | snd  | Linear | 40    
 /home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
   warnings.warn(*args, **kwargs)
 /home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
   warnings.warn(*args, **kwargs)
 Epoch 1000:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                                                                                                     | 512/2048 [00:00&lt;00:01, 1396.66it/s, loss=0.890, v_num=20]
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;&gt; python main.py # with bs=1
 GPU available: True, used: False
 TPU available: False, using: 0 TPU cores
 
   | Name | Type   | Params
 --------------------------------
 0 | fst  | Linear | 21    
 1 | snd  | Linear | 40    
 /home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
   warnings.warn(*args, **kwargs)
 /home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
   warnings.warn(*args, **kwargs)
 Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñàTraceback (most recent call last):‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                                                                          | 1024/2048 [00:00&lt;00:00, 1337.66it/s, loss=1.032, v_num=19]
   File "main.py", line 57, in &lt;module&gt;
     trainer.fit(net, train_dataloader=DataLoader(dataset, batch_size=bs), val_dataloaders=DataLoader(dataset, batch_size=bs))
   File "/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 954, in fit
     self.run_pretrain_routine(model)
   File "/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 1093, in run_pretrain_routine
     self.train()
   File "/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 375, in train
     self.run_training_epoch()
   File "/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 490, in run_training_epoch
     self.run_evaluation(test_mode=self.testing)
   File "/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 379, in run_evaluation
     eval_results = self._evaluate(self.model, dataloaders, max_batches, test_mode)
   File "/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 281, in _evaluate
     output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)
   File "/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 452, in evaluation_forward
     output = model.validation_step(*args)
   File "main.py", line 39, in validation_step
     raise NotImplementedError()
 NotImplementedError
 Exception ignored in: &lt;object repr() failed&gt;
 Traceback (most recent call last):
   File "/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/tqdm/std.py", line 1086, in __del__
   File "/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/tqdm/std.py", line 1293, in close
   File "/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/tqdm/std.py", line 1471, in display
   File "/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/tqdm/std.py", line 1089, in __repr__
   File "/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/tqdm/std.py", line 1433, in format_dict
 TypeError: 'NoneType' object is not iterable
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;Collecting environment information...
 PyTorch version: 1.5.1+cu101
 Is debug build: No
 CUDA used to build PyTorch: 10.1
 
 OS: Gentoo Base System release 2.7
 GCC version: (Gentoo 9.3.0 p1) 9.3.0
 CMake version: version 3.17.3
 
 Python version: 3.6
 Is CUDA available: Yes
 CUDA runtime version: 10.1.243
 GPU models and configuration: GPU 0: GeForce GT 730
 Nvidia driver version: 440.82
 cuDNN version: /opt/cuda/targets/x86_64-linux/lib/libcudnn.so.7.6.5
 
 Versions of relevant libraries:
 [pip3] numpy==1.19.0
 [pip3] pytorch-lightning==0.8.1
 [pip3] torch==1.5.1+cu101
 [pip3] torchvision==0.6.1+cu101
 [conda] Could not collect
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 Basically a reopen of &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2351&gt;#2351&lt;/denchmark-link&gt;
 , as this issue is not fixed by changing batch size and dataset size.
 	</description>
 	<comments>
 		<comment id='1' author='Uroc327' date='2020-06-30T14:06:39Z'>
 		Same behavior with pytorch-lightning==0.8.3.
 		</comment>
 		<comment id='2' author='Uroc327' date='2020-06-30T22:18:57Z'>
 		I can reproduce. It is caused by pl.Trainer(num_sanity_val_steps=0)
 For num_sanity_val_steps&gt;0 it works fine
 		</comment>
 		<comment id='3' author='Uroc327' date='2020-06-30T22:49:11Z'>
 		trying to fix his right now. another observation: only happens with iterable dataset.
 EDIT: iterable dataset that has also length defined (see comment below)
 		</comment>
 		<comment id='4' author='Uroc327' date='2020-07-01T00:42:43Z'>
 		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
   thanks for looking into this! If I remember correctly, then for  it only raised the exception during sanity checks on my machine. When I try to raise the exception only in actual training (for example by raising the exception on the fourth time  is called), then this completes sucessfully (i.e. does not call validation) as well.
 		</comment>
 		<comment id='5' author='Uroc327' date='2020-07-01T01:09:28Z'>
 		I think the issue is that your dataset is of type Iterable but has  defined. PL interprets that wrongly. In fact, there is a weird issue with dataloaders from iterable datasets that have len defined, check out this colab that I made:
 &lt;denchmark-link:https://colab.research.google.com/drive/1RQyNLGORe4vOL_RS6khcFNObcxyFxtEm?usp=sharing&gt;https://colab.research.google.com/drive/1RQyNLGORe4vOL_RS6khcFNObcxyFxtEm?usp=sharing&lt;/denchmark-link&gt;
 
 isn't it strange?
 if I remove the len from the dataset definition, your code sample works.
 		</comment>
 		<comment id='6' author='Uroc327' date='2020-07-01T13:00:04Z'>
 		&lt;denchmark-link:https://github.com/Uroc327&gt;@Uroc327&lt;/denchmark-link&gt;
  fyi, it turned out there is no bug, but rather a technical thing with iterable datasets. We deciced to add a warning message when IterableDataset defines also length.
 In your case you have the following options:
 
 remove __len__ from your IterableDataset (this is the preferred option you want 99.9%)
 convert it to a regular torch.utils.data.Dataset
 keep the batch size 1 (not great)
 write a custom BatchSampler (I guess, I have not tried it)
 
 		</comment>
 		<comment id='7' author='Uroc327' date='2020-07-01T20:50:21Z'>
 		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
  Ok, thanks! I'll remove the  implementation.
 Btw, the pytorch docs for &lt;denchmark-link:https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader&gt;DataLoader&lt;/denchmark-link&gt;
  explicitly allow an  with a .
 		</comment>
 		<comment id='8' author='Uroc327' date='2020-07-01T20:59:26Z'>
 		Yep! However as the docs say in the note at the bottom of your link the user has to to their own batching to avoid duplicate data. That's why I added the warning to PL.
 Note that for example in your case, leaving it as is len(dataloader) would always return 1024 as the length, regardless of the batch size. That would be incorrect for batch size &gt; 1.
 		</comment>
 		<comment id='9' author='Uroc327' date='2020-07-01T21:02:16Z'>
 		Makes sense, thank you üòÑ
 		</comment>
 	</comments>
 </bug>
<commit id='927f305f7e556828b5cdd45e3977c67f3c54b8fc' author='Adrian W√§lchli' date='2020-07-01 07:53:19-04:00'>
 	<dmm_unit complexity='0.41379310344827586' interfacing='1.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>13,14</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\data_loading.py' new_name='pytorch_lightning\trainer\data_loading.py'>
 		<file_info nloc='295' complexity='66' token_count='1660'></file_info>
 		<method name='auto_add_sampler' parameters='self,DataLoader,bool'>
 				<method_info nloc='16' complexity='9' token_count='106' nesting_level='1' start_line='144' end_line='167'></method_info>
 			<added_lines>148,149</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_has_iterable_dataset' parameters='DataLoader'>
 				<method_info nloc='3' complexity='3' token_count='26' nesting_level='0' start_line='46' end_line='48'></method_info>
 			<added_lines>46,47,48</added_lines>
 			<deleted_lines>46</deleted_lines>
 		</method>
 		<method name='_has_len' parameters='DataLoader'>
 				<method_info nloc='19' complexity='7' token_count='69' nesting_level='0' start_line='51' end_line='72'></method_info>
 			<added_lines>53,54,60,62,64,65,66,67,68,69,70,71,72</added_lines>
 			<deleted_lines>52,54,56</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1,4,7,49,50,301</added_lines>
 			<deleted_lines>4,131,133,134,135,136,288,289,290,291,292</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='requirements\base.txt' new_name='requirements\base.txt'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>9</added_lines>
 			<deleted_lines>9</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\trainer\test_dataloaders.py' new_name='tests\trainer\test_dataloaders.py'>
 		<file_info nloc='476' complexity='45' token_count='3415'></file_info>
 		<method name='test_warning_with_iterable_dataset_and_len.__len__' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='10' nesting_level='2' start_line='506' end_line='507'></method_info>
 			<added_lines>506,507</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_warning_with_iterable_dataset_and_len.__iter__' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='10' nesting_level='2' start_line='503' end_line='504'></method_info>
 			<added_lines>503,504</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_warning_with_iterable_dataset_and_len' parameters='tmpdir'>
 				<method_info nloc='17' complexity='1' token_count='117' nesting_level='0' start_line='496' end_line='519'></method_info>
 			<added_lines>496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>5,7,11,492,493,494,495,520,521</added_lines>
 			<deleted_lines>6</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
