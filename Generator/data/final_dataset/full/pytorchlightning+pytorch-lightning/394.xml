<bug_data>
<bug id='394' author='neggert' open_date='2019-10-18T23:21:11Z' closed_time='2019-11-05T15:42:01Z'>
 	<summary>ModelCheckpoint wipes out current directory</summary>
 	<description>
 &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
  I think this is what you were seeing in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/389&gt;#389&lt;/denchmark-link&gt;
 . If we let the  create the default  callback and don't use , the prefix ends up being set to the current directory. Then, when  tries to clean up previous checkpoints, it wipes out everything in the current directory.
 Relevant bits of code:
 
 default_save_path set to os.getcwd(): https://github.com/williamFalcon/pytorch-lightning/blob/master/pytorch_lightning/trainer/trainer.py#L151
 ModelCheckpoint falls back to default_save_path: https://github.com/williamFalcon/pytorch-lightning/blob/master/pytorch_lightning/trainer/trainer.py#L280
 ( ModelCheckpoint blows away pre-existing files in checkpoint directory: https://github.com/williamFalcon/pytorch-lightning/blob/master/pytorch_lightning/callbacks/pt_callbacks.py#L216
 
 The most obvious fix is to provide a better default checkpoint prefix, but there would still be a lurking footgun for a user who sets default_save_path incorrectly. Should we maybe insist that the checkpoint directory not exist before training starts, or that it be empty?
 	</description>
 	<comments>
 		<comment id='1' author='neggert' date='2019-10-19T05:09:20Z'>
 		most of the time it makes sense to have the default checkpoint inside the experiment file. why don't we do that with mlflowLogger? mlflow_exp/checkpoints
 		</comment>
 		<comment id='2' author='neggert' date='2019-10-21T15:19:38Z'>
 		MLFlow doesn't always have an experiment file locally. You can set it up to do all logging to a centralized server. Presumably other loggers will have similar issues. I'm not sure it makes sense to tightly couple checkpoint saving to the logger as a general principle.
 How about this proposal:
 
 Provide a default value of default_save_path="./checkpoints"
 Add a new (required) name property to LightningLoggerBase and make the existing version property required
 Have the default ModelCheckpoint save to os.path.join(default_save_path, logger.name, logger.version) if a logger is defined, otherwise default_save_path.
 Add a warning in ModelCheckpoint if filepath already exists and has files in it.
 
 I think this should prevent accidentally deleting files unless the user manually sets default_save_path=os.getcwd() and ignores the warning. I'm happy to do a PR if this sounds good.
 		</comment>
 		<comment id='3' author='neggert' date='2019-10-22T17:40:58Z'>
 		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
  Thoughts on the above proposal? If this is what we want to do, you can assign to me.
 		</comment>
 		<comment id='4' author='neggert' date='2019-10-22T18:12:30Z'>
 		yeah, i like that. But default logging should be os.path.join(default_save_path, logger.name, logger.version, '/checkpoints')
 Isn't default_save_path also used for the logger path? and also SLURM checkpointing, etc...?  it shouldn't go to ./checkpoints specifically
 		</comment>
 		<comment id='5' author='neggert' date='2019-10-22T18:15:29Z'>
 		Yeah, I think with the other changes, leaving default_save_path=os.getcwd() is fine. I was thinking about the case where there's no logger, but the default checkpoint, but I think we can just fall back to checkpoints in that case.
 		</comment>
 		<comment id='6' author='neggert' date='2019-10-22T18:18:03Z'>
 		I notice that for test tube, we're pre-pending version_ before the version. Is that needed? If so, I maybe we can do it in the loggers version property.
 		</comment>
 		<comment id='7' author='neggert' date='2019-10-23T10:20:12Z'>
 		&lt;denchmark-link:https://github.com/neggert&gt;@neggert&lt;/denchmark-link&gt;
  i made the fix to the PR on version_. See comments there
 		</comment>
 		<comment id='8' author='neggert' date='2019-11-05T16:26:18Z'>
 		Thanks for this fix &lt;denchmark-link:https://github.com/neggert&gt;@neggert&lt;/denchmark-link&gt;
 . I just tested this with a remote databricks workspace, e.g.
 &lt;denchmark-code&gt;mlf_logger = MLFlowLogger(
             experiment_name="/Users/xxxx/xxx",
             tracking_uri="databricks"
         )
 &lt;/denchmark-code&gt;
 
 Now the experiment name is exposed as the name of the logger which in turn is used to create the checkpoint directory. Long story short it tries to create a directory  under the root partition. Now this a loud error and still better than cleaning out the current directory  but I'm still looking for a fix. Any pointers on how to address this and I can take a stab at it (also &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
  ). I'll create an issue.
 		</comment>
 		<comment id='9' author='neggert' date='2019-11-05T16:54:05Z'>
 		Why are you setting the experiment name to a file path? Is this something that's required with databricks?
 You might want to take a look at the default_save_path argument to Trainer. The checkpoints will be saved in the result of
                 ckpt_path = os.path.join(
                     self.default_save_path,
                     self.logger.name,
                     f'version_{self.logger.version}',
                     "checkpoints"
                 )
 All of those parameters except the final "checkpoints" are adjustable, so you should be able to get the checkpoints to save wherever you want.
 		</comment>
 		<comment id='10' author='neggert' date='2019-11-05T16:55:44Z'>
 		letâ€™s resolve this today to make the release for tomorrow
 		</comment>
 		<comment id='11' author='neggert' date='2019-11-05T17:03:55Z'>
 		Oh, I see. The databricks tracking URI is "special". &lt;denchmark-link:https://mlflow.org/docs/latest/quickstart.html#quickstart-logging-to-remote-server&gt;https://mlflow.org/docs/latest/quickstart.html#quickstart-logging-to-remote-server&lt;/denchmark-link&gt;
 
 In this case, I'd suggest just creating your ModelCheckpoint callback manually rather than relying on the default.
 I'm not sure any of the core contributors use Databricks, so we'd probably need some help from a Databricks user to get this working smoothly.
 		</comment>
 		<comment id='12' author='neggert' date='2019-11-05T17:30:50Z'>
 		&lt;denchmark-link:https://github.com/neggert&gt;@neggert&lt;/denchmark-link&gt;
  thanks for looking into this and the suggestion - fair enough, I'll take that route.
 		</comment>
 	</comments>
 </bug>
<commit id='9fa28066059c3bda0b022c33921796c7425cd41e' author='Nic Eggert' date='2019-11-05 10:41:59-05:00'>
 	<dmm_unit complexity='0.9895833333333334' interfacing='0.9791666666666666' size='0.2708333333333333'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\callbacks\pt_callbacks.py' new_name='pytorch_lightning\callbacks\pt_callbacks.py'>
 		<file_info nloc='261' complexity='45' token_count='1284'></file_info>
 		<modified_lines>
 			<added_lines>182,183,184,185,186,187,188,189,190,191</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\logging\base.py' new_name='pytorch_lightning\logging\base.py'>
 		<file_info nloc='33' complexity='13' token_count='178'></file_info>
 		<method name='version' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='11' nesting_level='1' start_line='74' end_line='76'></method_info>
 			<added_lines>76</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='name' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='11' nesting_level='1' start_line='69' end_line='71'></method_info>
 			<added_lines>69,70,71</added_lines>
 			<deleted_lines>71</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>68,72</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\logging\mlflow_logger.py' new_name='pytorch_lightning\logging\mlflow_logger.py'>
 		<file_info nloc='56' complexity='14' token_count='356'></file_info>
 		<method name='version' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='9' nesting_level='1' start_line='69' end_line='70'></method_info>
 			<added_lines>69,70</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='name' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='9' nesting_level='1' start_line='65' end_line='66'></method_info>
 			<added_lines>65,66</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>63,64,67,68</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\logging\test_tube_logger.py' new_name='pytorch_lightning\logging\test_tube_logger.py'>
 		<file_info nloc='83' complexity='17' token_count='475'></file_info>
 		<method name='experiment' parameters='self'>
 				<method_info nloc='13' complexity='2' token_count='70' nesting_level='1' start_line='26' end_line='39'></method_info>
 			<added_lines>32</added_lines>
 			<deleted_lines>32</deleted_lines>
 		</method>
 		<method name='name' parameters='self'>
 				<method_info nloc='5' complexity='2' token_count='24' nesting_level='1' start_line='84' end_line='88'></method_info>
 			<added_lines>84,85,86,87,88</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>18,83,89</added_lines>
 			<deleted_lines>18</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\callback_config_mixin.py' new_name='pytorch_lightning\trainer\callback_config_mixin.py'>
 		<file_info nloc='52' complexity='11' token_count='268'></file_info>
 		<method name='configure_checkpoint_callback' parameters='self'>
 				<method_info nloc='21' complexity='6' token_count='119' nesting_level='1' start_line='8' end_line='42'></method_info>
 			<added_lines>17,18,20,21,22,23,25</added_lines>
 			<deleted_lines>15,16,18,19,20,22</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1,2</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\test_y_logging.py' new_name='tests\test_y_logging.py'>
 		<file_info nloc='128' complexity='14' token_count='731'></file_info>
 		<method name='test_mlflow_pickle' parameters=''>
 				<method_info nloc='20' complexity='2' token_count='125' nesting_level='0' start_line='108' end_line='137'></method_info>
 			<added_lines>108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137</added_lines>
 			<deleted_lines>108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137</deleted_lines>
 		</method>
 		<method name='test_custom_logger.name' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='7' nesting_level='2' start_line='162' end_line='163'></method_info>
 			<added_lines>162,163</added_lines>
 			<deleted_lines>162,163</deleted_lines>
 		</method>
 		<method name='test_mlflow_logger' parameters=''>
 				<method_info nloc='21' complexity='2' token_count='119' nesting_level='0' start_line='74' end_line='105'></method_info>
 			<added_lines>74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105</added_lines>
 			<deleted_lines>74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105</deleted_lines>
 		</method>
 		<method name='test_custom_logger' parameters='tmpdir'>
 				<method_info nloc='28' complexity='1' token_count='113' nesting_level='0' start_line='140' end_line='186'></method_info>
 			<added_lines>140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186</added_lines>
 			<deleted_lines>140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182</deleted_lines>
 		</method>
 		<method name='test_custom_logger.version' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='7' nesting_level='2' start_line='166' end_line='167'></method_info>
 			<added_lines>166,167</added_lines>
 			<deleted_lines>166,167</deleted_lines>
 		</method>
 		<method name='test_custom_logger.log_metrics' parameters='self,metrics,step_num'>
 				<method_info nloc='2' complexity='1' token_count='14' nesting_level='2' start_line='154' end_line='155'></method_info>
 			<added_lines>154,155</added_lines>
 			<deleted_lines>154,155</deleted_lines>
 		</method>
 		<method name='test_custom_logger.log_hyperparams' parameters='self,params'>
 				<method_info nloc='2' complexity='1' token_count='12' nesting_level='2' start_line='150' end_line='151'></method_info>
 			<added_lines>150,151</added_lines>
 			<deleted_lines>150,151</deleted_lines>
 		</method>
 		<method name='test_custom_logger.__init__' parameters='self'>
 				<method_info nloc='5' complexity='1' token_count='27' nesting_level='2' start_line='143' end_line='147'></method_info>
 			<added_lines>143,144,145,146,147</added_lines>
 			<deleted_lines>143,144,145,146,147</deleted_lines>
 		</method>
 		<method name='test_custom_logger.finalize' parameters='self,status'>
 				<method_info nloc='2' complexity='1' token_count='12' nesting_level='2' start_line='158' end_line='159'></method_info>
 			<added_lines>158,159</added_lines>
 			<deleted_lines>158,159</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1,9,106,107,138,139</added_lines>
 			<deleted_lines>72,73,106,107,138,139</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
