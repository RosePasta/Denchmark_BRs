<bug_data>
<bug id='1506' author='s-rog' open_date='2020-04-16T05:13:36Z' closed_time='2020-04-19T20:58:59Z'>
 	<summary>0.7.3 breaks reusable dataloaders in DDP</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 0.7.3 breaks reusable dataloaders in DDP
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 19, in _wrap
     fn(i, *args)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py", line 345, in ddp_train
     self.run_pretrain_routine(model)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 864, in run_pretrain_routine
     self.train()
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 296, in train
     self.reset_train_dataloader(model)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py", line 128, in reset_train_dataloader
     self.train_dataloader = self.auto_add_sampler(self.train_dataloader, train=True)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py", line 112, in auto_add_sampler
     dataloader = type(dataloader)(**dl_args)
   File "../main/dataset.py", line 15, in __init__
     super().__init__(*args, **kwargs)
 TypeError: __init__() got an unexpected keyword argument 'iterator'
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;class _RepeatSampler(object):
     def __init__(self, sampler):
         self.sampler = sampler
 
     def __iter__(self):
         while True:
             yield from iter(self.sampler)
 
 class FastDataLoader(torch.utils.data.dataloader.DataLoader):
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         object.__setattr__(self, 'batch_sampler', _RepeatSampler(self.batch_sampler))
         self.iterator = super().__iter__()
 
     def __len__(self):
         return len(self.batch_sampler.sampler)
 
     def __iter__(self):
         for i in range(len(self)):
             yield next(self.iterator)
 &lt;/denchmark-code&gt;
 
 replace Dataloader with FastDataLoader in lightning
 (this snippet is from &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/15849&gt;pytorch/pytorch#15849&lt;/denchmark-link&gt;
 )
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 Dataloaders initialize correctly and are reused between train/val/epochs (works as expected in 0.7.1)
 &lt;denchmark-h:h3&gt;Probable Cause&lt;/denchmark-h&gt;
 
 &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1425&gt;#1425&lt;/denchmark-link&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='s-rog' date='2020-04-16T12:22:08Z'>
 		ummm yeah. we should change the dataloader swap with swapping a dataloader init from the class or not swipe the dataloder at all but set the correct sampler.
 &lt;denchmark-link:https://github.com/justusschock&gt;@justusschock&lt;/denchmark-link&gt;
  any ideas?
 		</comment>
 		<comment id='2' author='s-rog' date='2020-04-17T07:43:44Z'>
 		This is a mixture of  &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1425&gt;#1425&lt;/denchmark-link&gt;
  and &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1346&gt;#1346&lt;/denchmark-link&gt;
 
 And I don't think we can prevent this when we want to set correct samplers also in subclasses of DataLoader. We use all public attributes for reinitialization.
 The probably easiest fix for you, would be to change self.iterator to self._iterator to avoid passing this argument in reinit.
 If we just change the sampler, this might yield unexpected behaviour.
 		</comment>
 	</comments>
 </bug>
<commit id='c71bd73acb5a89bb2a8ff44beab37fd2ceba352b' author='Justus Schock' date='2020-04-19 16:58:57-04:00'>
 	<dmm_unit complexity='1.0' interfacing='0.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>11</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\data_loading.py' new_name='pytorch_lightning\trainer\data_loading.py'>
 		<file_info nloc='204' complexity='38' token_count='1076'></file_info>
 		<method name='auto_add_sampler' parameters='self,DataLoader,bool'>
 				<method_info nloc='20' complexity='10' token_count='146' nesting_level='1' start_line='87' end_line='113'></method_info>
 			<added_lines>92,93</added_lines>
 			<deleted_lines>91,92,93,94</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>64</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\trainer.py' new_name='pytorch_lightning\trainer\trainer.py'>
 		<file_info nloc='812' complexity='86' token_count='3344'></file_info>
 		<method name='__init__' parameters='self,LightningLoggerBase,True,ModelCheckpoint,True,EarlyStopping,False,None,float,int,int,int,str,None,bool,None,None,int,float,int,int,bool,int,int,1,int,int,None,None,float,float,float,float,int,int,add_row_log_interval,None,int,bool,None,str,int,None,None,None,bool,bool,bool,False,bool,default_save_path,gradient_clip,nb_gpu_nodes,max_nb_epochs,min_nb_epochs,use_amp,show_progress_bar,nb_sanity_val_steps,bool,kwargs'>
 				<method_info nloc='56' complexity='1' token_count='410' nesting_level='1' start_line='85' end_line='140'></method_info>
 			<added_lines>130</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>286,287,288,369</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
