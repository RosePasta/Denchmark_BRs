<bug_data>
<bug id='1306' author='rzepinskip' open_date='2020-03-30T17:08:20Z' closed_time='2020-04-07T00:29:56Z'>
 	<summary>RuntimeError: Unimplemented backend XLA on TPU</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
  raised for  line in  file when running MNIST on TPU. I think it was introduced in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/31b71483c47fa4aa688912b432726cdac0025a9b&gt;31b7148&lt;/denchmark-link&gt;
 .
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 Steps to reproduce the behavior:
 
 Go to MNIST on TPUs
 Run all
 Scroll down to trainer
 See error
 
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py", line 119, in _start_fn
     fn(gindex, *args)
   File "/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py", line 119, in _start_fn
     fn(gindex, *args)
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py", line 499, in tpu_train
     self.run_pretrain_routine(model)
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py", line 499, in tpu_train
     self.run_pretrain_routine(model)
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py", line 499, in tpu_train
     self.run_pretrain_routine(model)
   File "/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py", line 119, in _start_fn
     fn(gindex, *args)
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py", line 499, in tpu_train
     self.run_pretrain_routine(model)
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py", line 499, in tpu_train
     self.run_pretrain_routine(model)
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py", line 499, in tpu_train
     self.run_pretrain_routine(model)
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py", line 920, in run_pretrain_routine
     self.train()
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py", line 920, in run_pretrain_routine
     self.train()
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py", line 920, in run_pretrain_routine
     self.train()
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py", line 920, in run_pretrain_routine
     self.train()
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py", line 920, in run_pretrain_routine
     self.train()
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py", line 920, in run_pretrain_routine
     self.train()
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py", line 356, in train
     self.run_training_epoch()
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py", line 356, in train
     self.run_training_epoch()
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py", line 499, in tpu_train
     self.run_pretrain_routine(model)
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py", line 356, in train
     self.run_training_epoch()
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py", line 356, in train
     self.run_training_epoch()
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py", line 356, in train
     self.run_training_epoch()
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py", line 356, in train
     self.run_training_epoch()
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py", line 425, in run_training_epoch
     output = self.run_training_batch(batch, batch_idx)
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py", line 425, in run_training_epoch
     output = self.run_training_batch(batch, batch_idx)
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py", line 920, in run_pretrain_routine
     self.train()
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py", line 425, in run_training_epoch
     output = self.run_training_batch(batch, batch_idx)
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py", line 425, in run_training_epoch
     output = self.run_training_batch(batch, batch_idx)
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py", line 425, in run_training_epoch
     output = self.run_training_batch(batch, batch_idx)
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py", line 425, in run_training_epoch
     output = self.run_training_batch(batch, batch_idx)
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py", line 582, in run_training_batch
     self.batch_loss_value.append(loss)
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py", line 582, in run_training_batch
     self.batch_loss_value.append(loss)
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/supporting_classes.py", line 23, in append
     if self.memory.type() != x.type():
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py", line 356, in train
     self.run_training_epoch()
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py", line 582, in run_training_batch
     self.batch_loss_value.append(loss)
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py", line 582, in run_training_batch
     self.batch_loss_value.append(loss)
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py", line 582, in run_training_batch
     self.batch_loss_value.append(loss)
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/supporting_classes.py", line 23, in append
     if self.memory.type() != x.type():
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py", line 582, in run_training_batch
     self.batch_loss_value.append(loss)
 Exception in device=TPU:5: Unimplemented backend XLA
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/supporting_classes.py", line 23, in append
     if self.memory.type() != x.type():
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py", line 425, in run_training_epoch
     output = self.run_training_batch(batch, batch_idx)
 RuntimeError: Unimplemented backend XLA
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/supporting_classes.py", line 23, in append
     if self.memory.type() != x.type():
 RuntimeError: Unimplemented backend XLA
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/supporting_classes.py", line 23, in append
     if self.memory.type() != x.type():
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/supporting_classes.py", line 23, in append
     if self.memory.type() != x.type():
 RuntimeError: Unimplemented backend XLA
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py", line 582, in run_training_batch
     self.batch_loss_value.append(loss)
 RuntimeError: Unimplemented backend XLA
 Traceback (most recent call last):
 RuntimeError: Unimplemented backend XLA
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/supporting_classes.py", line 23, in append
     if self.memory.type() != x.type():
 RuntimeError: Unimplemented backend XLA
   File "/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py", line 119, in _start_fn
     fn(gindex, *args)
 RuntimeError: Unimplemented backend XLA
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py", line 499, in tpu_train
     self.run_pretrain_routine(model)
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py", line 920, in run_pretrain_routine
     self.train()
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py", line 356, in train
     self.run_training_epoch()
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py", line 425, in run_training_epoch
     output = self.run_training_batch(batch, batch_idx)
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py", line 582, in run_training_batch
     self.batch_loss_value.append(loss)
   File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/supporting_classes.py", line 23, in append
     if self.memory.type() != x.type():
 RuntimeError: Unimplemented backend XLA
 ---------------------------------------------------------------------------
 Exception                                 Traceback (most recent call last)
 &lt;ipython-input-2-12f6e300d51d&gt; in &lt;module&gt;()
       6 # most basic trainer, uses good defaults
       7 trainer = Trainer(num_tpu_cores=8)
 ----&gt; 8 trainer.fit(model)
 
 3 frames
 /usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py in join(self, timeout)
     111                 raise Exception(
     112                     "process %d terminated with exit code %d" %
 --&gt; 113                     (error_index, exitcode)
     114                 )
     115 
 
 Exception: process 4 terminated with exit code 17
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 
 PyTorch Version (e.g., 1.0): 1.6
 OS (e.g., Linux): Linux
 How you installed PyTorch (conda, pip, source): pip
 Build command you used (if compiling from source): -
 Python version: 3.6
 CUDA/cuDNN version: -
 GPU models and configuration: -
 Any other relevant information: TPU backend
 
 	</description>
 	<comments>
 		<comment id='1' author='rzepinskip' date='2020-04-05T14:37:40Z'>
 		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 
 The issue is caused by &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/f1e11d8b3874067016693c50ae253ec79eecda09/pytorch_lightning/trainer/supporters.py#L40&gt;this line&lt;/denchmark-link&gt;
 :
 &lt;denchmark-code&gt;        if self.memory.type() != x.type():
             self.memory.type_as(x)
 &lt;/denchmark-code&gt;
 
 For TPU x is a XLA tensor and x.type() results in Unimplemented backend XLA (for GPU type is torch.cuda.FloatTensor).
 Something like x = torch.Tensor([x]) before condition checking fixes the problem. Or we can just send one of the tensors to common device.
 Notebook for debugging on &lt;denchmark-link:https://colab.research.google.com/drive/16Ug8IYPkqCu_NhK1FV7W-vszDsdgXfPD&gt;Google Colab&lt;/denchmark-link&gt;
 .
 		</comment>
 	</comments>
 </bug>
<commit id='b8ff9bc1d242a18f5e7147f34d63f43fcdd0e50a' author='Pawe≈Ç Rzepi≈Ñski' date='2020-04-06 20:29:55-04:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>24,73,74,81,86</added_lines>
 			<deleted_lines>24,73,74,81</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\supporters.py' new_name='pytorch_lightning\trainer\supporters.py'>
 		<file_info nloc='45' complexity='11' token_count='231'></file_info>
 		<method name='append' parameters='self,x'>
 				<method_info nloc='10' complexity='4' token_count='94' nesting_level='1' start_line='38' end_line='54'></method_info>
 			<added_lines>39,40,41</added_lines>
 			<deleted_lines>39,40,41</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
