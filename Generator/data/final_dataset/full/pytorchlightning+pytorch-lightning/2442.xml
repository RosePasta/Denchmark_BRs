<bug_data>
<bug id='2442' author='Anjum48' open_date='2020-07-01T06:56:52Z' closed_time='2020-07-21T19:18:58Z'>
 	<summary>validation_epoch_end needs to return CUDA tensors</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 I'm not sure if this is expected behaviour or not, but upgrading to the latest version (from 0.8.1) caused my validation_epoch_end to break. It appears that a CUDA tensor is expected for the metric where before the tensor was device agnostic.
 This was using sklearn's roc_auc_score. I haven't yet got around to testing PL's new metrics.
 Feel free to close if this is expected behaviour.
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 This is my validation_epoch_end. Uncommenting .to(avg_loss.device) allows this to run with the dev version of PL.
 This was run with ddp, precision=16 using Apex AMP.
 &lt;denchmark-code&gt;def validation_epoch_end(self, outputs):
         avg_loss = torch.stack([x["val_loss"] for x in outputs]).mean()
         y_pred = torch.cat([x["y_pred"].squeeze() for x in outputs]).cpu().numpy()
         y_true = torch.cat([x["y_true"].squeeze() for x in outputs]).cpu().numpy()
 
         metric = torch.tensor(roc_auc_score(y_true, y_pred))  # .to(avg_loss.device)
 
         tensorboard_logs = {
             "loss/validation": avg_loss,
             "auc": metric,
         }
         return {"val_loss": avg_loss, "log": tensorboard_logs, "auc": metric}
 &lt;/denchmark-code&gt;
 
 The error message can be seen here: &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2411#issuecomment-652171874&gt;#2411 (comment)&lt;/denchmark-link&gt;
 
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 Please copy and paste the output from our
 &lt;denchmark-link:https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py&gt;environment collection script&lt;/denchmark-link&gt;
 
 (or fill out the checklist below manually).
 You can get the script and run it with:
 &lt;denchmark-code&gt;wget https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py
 # For security purposes, please check the contents of collect_env_details.py before running it.
 python collect_env_details.py
 &lt;/denchmark-code&gt;
 
 
 PyTorch Version (e.g., 1.0): 1.5
 OS (e.g., Linux): Ubuntu 20.04
 How you installed PyTorch (conda, pip, source):
 Build command you used (if compiling from source):
 Python version: 3.8.2
 CUDA/cuDNN version: 10.2
 GPU models and configuration: Dual GPU (ddp)
 Any other relevant information: Apex AMP
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='Anjum48' date='2020-07-01T07:11:15Z'>
 		I also have a sklearn generated metric that is only logged, with no business being on gpu. Hopefully not intended behavior?
 		</comment>
 		<comment id='2' author='Anjum48' date='2020-07-01T09:57:44Z'>
 		&lt;denchmark-link:https://github.com/justusschock&gt;@justusschock&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
  mind have look? 
 		</comment>
 		<comment id='3' author='Anjum48' date='2020-07-02T06:41:19Z'>
 		&lt;denchmark-link:https://github.com/Anjum48&gt;@Anjum48&lt;/denchmark-link&gt;
  this is probably caused by &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2434&gt;#2434&lt;/denchmark-link&gt;
  in combination with your dip backend (from here: &lt;denchmark-link:https://pytorch.org/docs/stable/distributed.html#backends&gt;https://pytorch.org/docs/stable/distributed.html#backends&lt;/denchmark-link&gt;
 ) which does not support CPU tensors (are you using NCCL backend?)
 &lt;denchmark-link:https://github.com/s-rog&gt;@s-rog&lt;/denchmark-link&gt;
  can you elaborate on your problem? Maybe in a separate issue since I'm not sure, if this is the same problem here.
 cc &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
  ^^
 		</comment>
 		<comment id='4' author='Anjum48' date='2020-07-02T07:52:20Z'>
 		Hi &lt;denchmark-link:https://github.com/justusschock&gt;@justusschock&lt;/denchmark-link&gt;
 , I'm using the default PyTorch install from conda, so I believe NCCL is being used (both GPUs are in the same machine). The weird thing is that this only started happening when I upgraded from 0.8.1 to 0.8.4 so I guess something has changed in ddp?
 		</comment>
 		<comment id='5' author='Anjum48' date='2020-07-02T07:56:58Z'>
 		This is not on ddp itself, but we added a often requested feature and synced the outputs across ddp nodes. And since you probably used nccl backend (which only supports gpu), this probably causes the drawback. Can you try to switch the backend of DDP? Maybe to Gloo or MPI?
 		</comment>
 		<comment id='6' author='Anjum48' date='2020-07-02T08:22:09Z'>
 		I added torch.distributed.Backend('gloo') to the top of my training script and am seeing the same error
 		</comment>
 		<comment id='7' author='Anjum48' date='2020-07-02T08:24:14Z'>
 		&lt;denchmark-link:https://github.com/justusschock&gt;@justusschock&lt;/denchmark-link&gt;
  I have the same issue as OP where my  returns a cpu tensor like:
 
 Edit:
 does this mean that the previous implementation was only returning/logging the rank 0 output?
 		</comment>
 		<comment id='8' author='Anjum48' date='2020-07-02T08:28:45Z'>
 		
 Hi @justusschock, I'm using the default PyTorch install from conda, so I believe NCCL is being used (both GPUs are in the same machine). The weird thing is that this only started happening when I upgraded from 0.8.1 to 0.8.4 so I guess something has changed in ddp?
 
 FYI, We are testing also against Conda PyTorch distributions
 		</comment>
 		<comment id='9' author='Anjum48' date='2020-07-02T08:29:20Z'>
 		
 does this mean that the previous implementation was only returning/logging the rank 0 output?
 
 I was also wondering this. I noticed my validation loss double after upgrading to 0.8.4
 		</comment>
 		<comment id='10' author='Anjum48' date='2020-07-02T10:20:56Z'>
 		It is synced/reduced back to process 0 and only logged there :) DO you think it would be better to log from each process with the respective rank as a prefix?
 		</comment>
 		<comment id='11' author='Anjum48' date='2020-07-02T10:23:15Z'>
 		&lt;denchmark-link:https://github.com/Anjum48&gt;@Anjum48&lt;/denchmark-link&gt;
  regarding your persisting error:
 adding it at the top of your script doesn't help, since lightning trainer overwrites this. You have to set it in the trainer: &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/trainer.py#L166&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/trainer.py#L166&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='12' author='Anjum48' date='2020-07-02T11:11:06Z'>
 		Ok I tried it in trainer, and got this error (after disabling AMP):
 &lt;denchmark-code&gt;Loaded pretrained weights for efficientnet-b0
 train.py:141: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.
   train_single_fold(args)
 GPU available: True, used: False
 TPU available: False, using: 0 TPU cores
 
   | Name      | Type             | Params
 -----------------------------------------------
 0 | critereon | CrossEntropyLoss | 0     
 1 | metric    | AUROC            | 0     
 2 | net       | EfficientNet     | 4 M   
 Traceback (most recent call last):
   File "train.py", line 141, in &lt;module&gt;
     train_single_fold(args)
   File "train.py", line 65, in train_single_fold
     trainer.fit(model)
   File "/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1020, in fit
     self.run_pretrain_routine(model)
   File "/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1128, in run_pretrain_routine
     self.reset_val_dataloader(ref_model)
   File "/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py", line 342, in reset_val_dataloader
     self._reset_eval_dataloader(model, 'val')
   File "/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py", line 268, in _reset_eval_dataloader
     dataloaders = self.request_dataloader(getattr(model, f'{mode}_dataloader'))
   File "/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py", line 363, in request_dataloader
     dataloader = dataloader_fx()
   File "/home/anjum/PycharmProjects/kaggle/siim_isic_melanoma_classification/engine.py", line 194, in val_dataloader
     sampler = DistributedSampler(dataset)
   File "/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/torch/utils/data/distributed.py", line 43, in __init__
     num_replicas = dist.get_world_size()
   File "/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 582, in get_world_size
     return _get_group_size(group)
   File "/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 196, in _get_group_size
     _check_default_pg()
   File "/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 186, in _check_default_pg
     assert _default_pg is not None, \
 AssertionError: Default process group is not initialized
 &lt;/denchmark-code&gt;
 
 I'm not familiar with gloo so I don't know what I'm looking at here. Isn't gloo for CPU only training?
 		</comment>
 		<comment id='13' author='Anjum48' date='2020-07-02T11:19:45Z'>
 		fixed on master for now.
 Yes, currently only rank 0 is used for val calculation. The problem is that reducing blows up memory and causes these kinds of issues.
 however, this will be enabled again in an upcoming PR with an object that can do this kind of syncing automatically. however we‚Äôll need to solve this problem again, so we still need to identify what the issue is.
 		</comment>
 		<comment id='14' author='Anjum48' date='2020-07-03T00:24:45Z'>
 		
 It is synced/reduced back to process 0 and only logged there :)
 
 
 Yes, currently only rank 0 is used for val calculation.
 
 &lt;denchmark-link:https://github.com/justusschock&gt;@justusschock&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
  ming clarifying? these two statements seem to be contradicting, unless I'm misunderstanding something...
 		</comment>
 		<comment id='15' author='Anjum48' date='2020-07-03T05:08:49Z'>
 		Originally we didn't reduce on val - it was on our todo. Then we added it, but it had weird issues such as this one. So we removed it and will test it more before adding back in.
 		</comment>
 		<comment id='16' author='Anjum48' date='2020-07-07T08:21:47Z'>
 		I suppose the metric package fixes/bypasses this issue?
 I just converted my vanilla sk metrics to the pl sk interface, will test soon.
 Edit:
 &lt;denchmark-code&gt;  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/metrics/metric.py", line 145, in __call__
     return apply_to_collection(self._orig_call(*args, **kwargs), torch.Tensor,
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/metrics/converters.py", line 67, in new_func
     return func_to_apply(result, *dec_args, **dec_kwargs)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/utilities/apply_func.py", line 35, in apply_to_collection
     return function(data, *args, **kwargs)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/metrics/converters.py", line 252, in _sync_ddp_if_available
     async_op=False)
   File "/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 898, in all_reduce
     work = _default_pg.allreduce([tensor], opts)
 RuntimeError: Tensors must be CUDA and dense
 &lt;/denchmark-code&gt;
 
 same issue using the built in metric interface it seems... I'm guessing the metrics need to include cuda conversion (for now) to be able to be reduced?
 		</comment>
 		<comment id='17' author='Anjum48' date='2020-07-07T09:58:56Z'>
 		are you on master?
 We won't include a cuda conversion here (we already have a push to self.device), since this also depends on your distributed backend (and you don't have access tp/want the computation to run on a gpu all the times.
 		</comment>
 		<comment id='18' author='Anjum48' date='2020-07-08T00:23:48Z'>
 		&lt;denchmark-link:https://github.com/justusschock&gt;@justusschock&lt;/denchmark-link&gt;
  I just updated and tried again with the same error
 		</comment>
 		<comment id='19' author='Anjum48' date='2020-07-08T09:05:06Z'>
 		do you have any CPU tensors there?
 		</comment>
 		<comment id='20' author='Anjum48' date='2020-07-09T00:26:35Z'>
 		nope both inputs to the lightning sklearn metric are cuda tensors
 		</comment>
 		<comment id='21' author='Anjum48' date='2020-07-09T07:55:11Z'>
 		I think I know the issue. Currently there is a problem with device propagation in utilities.device_dtype_mixin which causes the device property to be set only for the most outer instance. E.g. when you assign the metric to your LightningModule, the device of your metric isn't updated. So far I don't know why though.
 Any Idea &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 ?
 		</comment>
 		<comment id='22' author='Anjum48' date='2020-07-15T12:34:55Z'>
 		not sure at this moment, will check it...
 		</comment>
 		<comment id='23' author='Anjum48' date='2020-07-16T02:21:34Z'>
 		Should this be a separate issue? as it's an issue with metrics and not validation_epoch_end specifically
 		</comment>
 		<comment id='24' author='Anjum48' date='2020-07-21T13:39:16Z'>
 		Just some findings tho
 Currently, classification metrics return cpu tensor while regression metrics return cuda tensor, haven't tested with sk-metrics.
 &lt;denchmark-link:https://colab.research.google.com/drive/102qrhFRbH3Jkh28-icSkuVkgelKmxlAt?usp=sharing&gt;COLAB EXAMPLE&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='25' author='Anjum48' date='2020-07-21T13:39:29Z'>
 		&lt;denchmark-link:https://github.com/justusschock&gt;@justusschock&lt;/denchmark-link&gt;
  The problem is simply that when you call .to on the nn.Module, it is not calling .to on the submodules. What the pytorch nn module does instead is only move the tensors (buffers, parameters) of these submodules, since pytorch has no such thing as a device property, the device is simply defined by the device the tensors are on.
 We can come up with a creative solution that involves going over all submodules and checking if they are instance of dtypedevicemixin and calling .to on these
 But this is not a real solution because if you nest multiple levels of dtype-mixin-subclass and nn.Module in a particular way, it can still lead to the same bug
 EDIT: I think I have a fix that involves using the apply function on the module, and can submit a PR.
 		</comment>
 		<comment id='26' author='Anjum48' date='2020-07-21T19:41:23Z'>
 		&lt;denchmark-link:https://github.com/Anjum48&gt;@Anjum48&lt;/denchmark-link&gt;
  the bug should be fixed now, although I was not able to check your use case with precision=16. Let me know if it works or not.
 		</comment>
 		<comment id='27' author='Anjum48' date='2020-07-22T03:28:36Z'>
 		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
  pulled from master and still experiencing the same issue with FP16
 		</comment>
 		<comment id='28' author='Anjum48' date='2020-07-22T07:54:00Z'>
 		Yes, but this is probably something different. If you're using AMP, they don't convert inputs and outputs once, but they monkey patch every function to convert it's inputs and convert it's outputs back...
 Are you on slack? Maybe it's best to write me a message there, so that we can easily discuss your issue without spamming all the others :)
 		</comment>
 	</comments>
 </bug>
<commit id='a5538af3558cf544dffd92b1b8bab3a5793f0ba0' author='Adrian W√§lchli' date='2020-07-21 15:18:57-04:00'>
 	<dmm_unit complexity='1.0' interfacing='0.8333333333333334' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>24</added_lines>
 			<deleted_lines>24</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\core\lightning.py' new_name='pytorch_lightning\core\lightning.py'>
 		<file_info nloc='1610' complexity='86' token_count='2253'></file_info>
 		<method name='__init__' parameters='self,args,kwargs'>
 				<method_info nloc='16' complexity='1' token_count='101' nesting_level='1' start_line='37' end_line='78'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>71,72,73,74,75,76</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\metrics\metric.py' new_name='pytorch_lightning\metrics\metric.py'>
 		<file_info nloc='91' complexity='11' token_count='515'></file_info>
 		<method name='__init__' parameters='self,str'>
 				<method_info nloc='5' complexity='1' token_count='41' nesting_level='1' start_line='22' end_line='31'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>30,31</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\utilities\device_dtype_mixin.py' new_name='pytorch_lightning\utilities\device_dtype_mixin.py'>
 		<file_info nloc='147' complexity='17' token_count='476'></file_info>
 		<method name='float' parameters='self'>
 				<method_info nloc='8' complexity='1' token_count='26' nesting_level='1' start_line='131' end_line='138'></method_info>
 			<added_lines>137</added_lines>
 			<deleted_lines>138</deleted_lines>
 		</method>
 		<method name='__update_properties' parameters='self,None,None'>
 				<method_info nloc='3' complexity='1' token_count='35' nesting_level='1' start_line='158' end_line='168'></method_info>
 			<added_lines>158,159,160,161,162,163,164,165,166,167,168</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='cuda' parameters='self,None'>
 				<method_info nloc='15' complexity='1' token_count='45' nesting_level='1' start_line='95' end_line='109'></method_info>
 			<added_lines>108</added_lines>
 			<deleted_lines>108,109</deleted_lines>
 		</method>
 		<method name='__update_properties.apply_fn' parameters='module'>
 				<method_info nloc='7' complexity='4' token_count='37' nesting_level='2' start_line='160' end_line='166'></method_info>
 			<added_lines>160,161,162,163,164,165,166</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='type' parameters='self,str'>
 				<method_info nloc='11' complexity='1' token_count='38' nesting_level='1' start_line='119' end_line='129'></method_info>
 			<added_lines>128</added_lines>
 			<deleted_lines>129</deleted_lines>
 		</method>
 		<method name='to' parameters='self,args,kwargs'>
 				<method_info nloc='61' complexity='1' token_count='61' nesting_level='1' start_line='32' end_line='93'></method_info>
 			<added_lines>85,86,87,88,92</added_lines>
 			<deleted_lines>85,86,87,88,89,90,91,92</deleted_lines>
 		</method>
 		<method name='double' parameters='self'>
 				<method_info nloc='8' complexity='1' token_count='26' nesting_level='1' start_line='140' end_line='147'></method_info>
 			<added_lines>146</added_lines>
 			<deleted_lines>147</deleted_lines>
 		</method>
 		<method name='cpu' parameters='self'>
 				<method_info nloc='7' complexity='1' token_count='29' nesting_level='1' start_line='111' end_line='117'></method_info>
 			<added_lines>116</added_lines>
 			<deleted_lines>117</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self'>
 				<method_info nloc='4' complexity='1' token_count='31' nesting_level='1' start_line='9' end_line='12'></method_info>
 			<added_lines>9,10,11,12</added_lines>
 			<deleted_lines>9</deleted_lines>
 		</method>
 		<method name='half' parameters='self'>
 				<method_info nloc='8' complexity='1' token_count='26' nesting_level='1' start_line='149' end_line='156'></method_info>
 			<added_lines>155</added_lines>
 			<deleted_lines>156</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>8,157</added_lines>
 			<deleted_lines>8</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='tests\utilities\test_dtype_device_mixin.py'>
 		<file_info nloc='63' complexity='8' token_count='498'></file_info>
 	</modification>
 </commit>
</bug_data>
