<bug_data>
<bug id='1119' author='xingzhaolee' open_date='2020-03-11T13:02:06Z' closed_time='2020-03-12T14:50:01Z'>
 	<summary>Checkpoint fails in single node multi-GPU mode using  DDP</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 Checkpoint fails in single node multi-GPU mode using  DDP.
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 python pl_examples/basic_examples/gpu_template.py --distributed_backend ddp --gpus 2
 Epoch 2: : 700it [00:28, 42.69it/s, l/home/xz/anaconda3/envs/x/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown                                                                                                                                                                                                                 
   len(cache))
 Traceback (most recent call last):
   File "gpu_template.py", line 79, in &lt;module&gt;
     main(hyperparams)
   File "gpu_template.py", line 40, in main
     trainer.fit(model)
   File "/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 590, in fit
     mp.spawn(self.ddp_train, nprocs=self.num_gpus, args=(model,))
   File "/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 171, in spawn
     while not spawn_context.join():
   File "/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 118, in join
     raise Exception(msg)
 Exception: 
 
 -- Process 1 terminated with the following error:
 Traceback (most recent call last):
   File "/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 19, in _wrap
     fn(i, *args)
   File "/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py", line 342, in ddp_train
     self.run_pretrain_routine(model)
   File "/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 830, in run_pretrain_routine
     self.train()
   File "/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 343, in train
     self.run_training_epoch()
   File "/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 452, in run_training_epoch
     self.call_checkpoint_callback()
   File "/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 737, in call_checkpoint_callback
     self.checkpoint_callback.on_validation_end(self, self.get_model())
   File "/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 204, in on_validation_end
     self._do_check_save(filepath, current, epoch)
   File "/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 221, in _do_check_save
     self._del_model(delpath)
   File "/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 121, in _del_model
     os.remove(filepath)
 FileNotFoundError: [Errno 2] No such file or directory: '/home/xz/pytorch-lightning/pl_examples/basic_examples/lightning_logs/version_1/checkpoints/epoch=0.ckpt'
 	</description>
 	<comments>
 		<comment id='1' author='xingzhaolee' date='2020-03-11T13:43:26Z'>
 		yeah we shall run all examples in CI too
 		</comment>
 		<comment id='2' author='xingzhaolee' date='2020-03-11T20:39:10Z'>
 		I believe this happens with multiple gpus as well. And it only seems to happen if ModelCheckpoint(save_top_k) is set greater than 1. Still converting models to 0.7.1 but wanted to share this ...
 		</comment>
 		<comment id='3' author='xingzhaolee' date='2020-03-12T02:19:25Z'>
 		&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
  fixed. part of the code that caused the bug was removed a few commits back.
 		</comment>
 		<comment id='4' author='xingzhaolee' date='2020-03-12T03:28:12Z'>
 		fix is in master?
 		</comment>
 		<comment id='5' author='xingzhaolee' date='2020-03-12T03:32:04Z'>
 		fix for DDP checkpoint is in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1125&gt;#1125&lt;/denchmark-link&gt;
 , still waiting for it to be reviewed and merged.
 
 I believe this happens with multiple gpus as well. And it only seems to happen if ModelChckepoint(save_top_k) is set greater than 1. Still converting models to 0.7.1 but wanted to share this ...
 
 as for this issue, on my side it seems to work fine. can you double check?
 		</comment>
 	</comments>
 </bug>
<commit id='b4d4e489bf413ebf3288d29c5905d2292ce18d58' author='Xing Zhao LEE' date='2020-03-12 15:50:00+01:00'>
 	<dmm_unit complexity='0.0' interfacing='0.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\callbacks\model_checkpoint.py' new_name='pytorch_lightning\callbacks\model_checkpoint.py'>
 		<file_info nloc='187' complexity='27' token_count='918'></file_info>
 		<method name='on_validation_end' parameters='self,trainer,pl_module'>
 				<method_info nloc='33' complexity='10' token_count='187' nesting_level='1' start_line='177' end_line='218'></method_info>
 			<added_lines>178,179,180,181</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
