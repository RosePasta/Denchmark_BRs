<bug_data>
<bug id='1588' author='nathanbreitsch' open_date='2020-04-24T03:46:31Z' closed_time='2020-04-30T12:04:51Z'>
 	<summary>Named converted to regular tuples when sent to the gpu.</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 Named tuples returned from Dataset get converted to regular tuples when sent to the gpu.
 This happens because isinstance(instance_of_a_named_tuple, tuple) evaluates to True in distrib_parts.py
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/distrib_parts.py
 
 
          Line 463
       in
       67d5f4d
 
 
 
 
 
 
  if isinstance(batch, tuple): 
 
 
 
 
 
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 import pytorch_lightning as pl
 from collections import namedtuple
 import torch
 import numpy
 
 NamedTupleDemoInput = namedtuple('DemoInput', ['x1', 'x2', 'y'])
 
 class NamedTupleDemoDataset:
     def __len__(self):
         return 30000
 
     def __getitem__(self, index):
         x1 = numpy.random.uniform(0, 100)
         x2 = numpy.random.uniform(0, 100)
         y = 2*x1 + 3*x2 + numpy.random.normal(0, 0.05)
         return NamedTupleDemoInput(x1, x2, y)
 
 class WeightedSum(torch.nn.Module):
     def __init__(self):
         super(WeightedSum, self).__init__()
         self.a = torch.nn.Parameter(torch.zeros(1))
         self.b = torch.nn.Parameter(torch.zeros(1))
 
     def forward(self, x1, x2):
         return self.a * x1 + self.b * x2
 
 class NamedTupleDemo(pl.LightningModule):
 
     def __init__(self):
         super(NamedTupleDemo, self).__init__()
         self.model = WeightedSum()
 
     def forward(self, x1, x2):
         return self.model(x1, x2)
 
     def train_dataloader(self):
         return torch.utils.data.DataLoader(NamedTupleDemoDataset(), batch_size=128)
 
     def training_step(self, batch, batch_index):
         yhat = self.forward(batch.x1, batch.x2)
         return {'loss': torch.nn.functional.mse_loss(batch.y, yhat)}
 
     def configure_optimizers(self):
         return torch.optim.Adam(self.parameters(), lr=1e-2)
 
 if __name__ == '__main__':
     module = NamedTupleDemo()
     pl.Trainer(max_epochs=20, gpus=1).fit(module)
     print(f'a={float(module.model.a)} b={float(module.model.b)}')
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "demo.py", line 48, in &lt;module&gt;
     pl.Trainer(max_epochs=20, gpus=1).fit(module)
   File "/home/n/repos/pytorch-lightning/pytorch_lightning/trainer/trainer.py", line 749, in fit
     self.single_gpu_train(model)
   File "/home/n/repos/pytorch-lightning/pytorch_lightning/trainer/distrib_parts.py", line 491, in single_gpu_train
     self.run_pretrain_routine(model)
   File "/home/n/repos/pytorch-lightning/pytorch_lightning/trainer/trainer.py", line 910, in run_pretrain_routine
     self.train()
   File "/home/n/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py", line 384, in train
     self.run_training_epoch()
   File "/home/n/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py", line 456, in run_training_epoch
     _outputs = self.run_training_batch(batch, batch_idx)
   File "/home/n/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py", line 633, in run_training_batch
     loss, batch_output = optimizer_closure()
   File "/home/n/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py", line 597, in optimizer_closure
     output_dict = self.training_forward(split_batch, batch_idx, opt_idx, self.hiddens)
   File "/home/n/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py", line 770, in training_forward
     output = self.model.training_step(*args)
   File "demo.py", line 40, in training_step
     yhat = self.forward(batch.x1, batch.x2)
 AttributeError: 'tuple' object has no attribute 'x1'
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 Namedtuples returned from the dataset should be keep their original fields.
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 
 CUDA:
 - GPU:
 - GeForce RTX 2080 Ti
 - available:         True
 - version:           10.2
 Packages:
 - numpy:             1.18.3
 - pyTorch_debug:     False
 - pyTorch_version:   1.5.0
 - pytorch-lightning: 0.7.4rc5
 - tensorboard:       2.2.1
 - tqdm:              4.45.0
 System:
 - OS:                Linux
 - architecture:
 - 64bit
 - ELF
 - processor:
 - python:            3.8.2
 - version:           #1 SMP PREEMPT Sun, 05 Apr 2020 05:13:14 +0000
 
 	</description>
 	<comments>
 		<comment id='1' author='nathanbreitsch' date='2020-04-24T03:47:10Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 	</comments>
 </bug>
<commit id='3eac6cfd4fbbc4d13f4e93f6d90f8ee5302c421e' author='Nathan Breitsch' date='2020-04-30 08:04:50-04:00'>
 	<dmm_unit complexity='0.0' interfacing='0.5555555555555556' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\distrib_parts.py' new_name='pytorch_lightning\trainer\distrib_parts.py'>
 		<file_info nloc='601' complexity='89' token_count='1789'></file_info>
 		<method name='__transfer_data_to_device' parameters='self,batch,device,gpu_id'>
 				<method_info nloc='27' complexity='15' token_count='250' nesting_level='1' start_line='442' end_line='482'></method_info>
 			<added_lines>464,465,466,467,468,469,470,471,472</added_lines>
 			<deleted_lines>464,465,466,467</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\models\test_cpu.py' new_name='tests\models\test_cpu.py'>
 		<file_info nloc='262' complexity='28' token_count='1895'></file_info>
 		<method name='test_single_gpu_batch_parse' parameters=''>
 				<method_info nloc='31' complexity='12' token_count='550' nesting_level='0' start_line='185' end_line='230'></method_info>
 			<added_lines>225,226,227,228,229,230</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1,231</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
