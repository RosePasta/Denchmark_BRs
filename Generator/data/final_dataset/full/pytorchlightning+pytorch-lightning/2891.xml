<bug_data>
<bug id='2891' author='manipopopo' open_date='2020-08-09T04:52:31Z' closed_time='2020-10-04T12:32:19Z'>
 	<summary>The total number of batches shows by the progress bar of the sanity check is wrong</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 The total of the sanity check progress bar is set by
 
 
 
 pytorch-lightning/pytorch_lightning/callbacks/progress.py
 
 
          Line 296
       in
       4d0406e
 
 
 
 
 
 
  self.val_progress_bar.total = convert_inf(trainer.num_sanity_val_steps * len(trainer.val_dataloaders)) 
 
 
 
 
 
 The progress bar will always show trainer.num_sanity_val_steps even if  the length of the validation DataLoader is less than trainer.num_sanity_val_steps.
 Maybe the total could be computed by
 from pytorch_lightning.trainer import data_loading
 
 num_full_val_dataloader_batches = [
     len(dataloader) if data_loading._has_len(dataloader) else float('inf')
     for dataloader in trainer.val_dataloaders
 ]
 self.val_progress_bar.total = convert_inf(
     sum(min(num_batches, trainer.num_sanity_val_steps)
             for num_batches in num_full_val_dataloader_batches))
 We use the private function data_loading._has_len to check if dataloader has __len__, maybe we could make data_loading._has_len public.
 Or we could make num_full_val_dataloader_batches (and num_full_train_dataloader_batches) a member variable of Trainer and update the value in pytorch_lightning.trainer.data_loading.TrainerDataLoadingMixin.
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 The progress bar of the sanity check in the following code (num_sanity_val_steps == 999 and len(val_data_loader) == 10) shows
 &lt;denchmark-code&gt;Validation sanity check:   1%|          | 9/999 [00:09&lt;16:31,  1.00s/it]`
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 import time
 
 import pytorch_lightning as pl
 from torch.utils import data
 
 
 class Dataset(data.Dataset):
 
   def __init__(self, length):
     self._elements = list(range(length))
 
   def __getitem__(self, item):
     return self._elements[item]
 
   def __len__(self):
     return len(self._elements)
 
 
 class Model(pl.LightningModule):
 
   def forward(self, *args, **kwargs):
     pass
 
   def training_step(self, *args, **kwargs):
     pass
 
   def train_dataloader(self):
     pass
 
   def configure_optimizers(self):
     pass
 
   def validation_step(self, *args, **kwargs):
     time.sleep(1)
     return pl.EvalResult()
 
 
 if __name__ == '__main__':
   model = Model()
 
   val_dataset_length = 10
   val_dataset = Dataset(val_dataset_length)
   val_data_loader = data.DataLoader(val_dataset)
 
   trainer = pl.Trainer(num_sanity_val_steps=999, limit_val_batches=999,
                        max_epochs=0)
   trainer.fit(model, val_dataloaders=val_data_loader)
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 The program above should be
 &lt;denchmark-code&gt;Validation sanity check: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:10&lt;00:00,  1.00s/it]
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 
 CUDA:
 
 GPU:
 available:
 version:
 
 
 Packages:
 
 numpy:             1.18.5
 pyTorch_debug:     False
 pyTorch_version:   1.6.0+cpu
 pytorch-lightning: 0.9.0rc11
 tensorboard:       1.15.0
 tqdm:              4.48.2
 
 
 System:
 
 OS:                Windows
 architecture:
 
 64bit
 WindowsPE
 
 
 processor:
 python:            3.7.3
 version:           10.0.18362
 
 
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='manipopopo' date='2020-08-09T07:33:03Z'>
 		&lt;denchmark-link:https://github.com/manipopopo&gt;@manipopopo&lt;/denchmark-link&gt;
  I guess the first solution gets things done without changing anything else. Mind submitting a PR?
 		</comment>
 		<comment id='2' author='manipopopo' date='2020-08-09T09:30:11Z'>
 		I think once &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2882&gt;#2882&lt;/denchmark-link&gt;
  gets resolved this issue will be resolved automatically.
 		</comment>
 		<comment id='3' author='manipopopo' date='2020-08-09T10:58:00Z'>
 		It seems that resolve &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2882&gt;#2882&lt;/denchmark-link&gt;
  will make the sanity check of  run exactly  steps (or  ) if the validation  has at least 5 batches.
 Resolve this issue will make the total of the progress bar for the sanity check be the size of the validation DataLoader if it is less than num_sanity_val_steps (except num_sanity_val_steps == -1).
 		</comment>
 		<comment id='4' author='manipopopo' date='2020-08-09T12:02:11Z'>
 		then I would suggest fixing &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2882&gt;#2882&lt;/denchmark-link&gt;
  first in which we can make self.num_sanity_val_steps to be a list of correct num_steps for each val_dataloader and then we can just do  here to assign it to progress_bar.
 		</comment>
 		<comment id='5' author='manipopopo' date='2020-08-09T13:53:40Z'>
 		It seems that we have several ways to tackle the problem.
 
 
 Make self.num_sanity_val_steps a list of correct numbers of steps as @rohitgr7 suggests. ProgressBar can update self.val_progress_bar.total using self.num_sanity_val_steps. Accessing the member num_sanity_val_steps may get values different from the one passed into Trainer.__init__.
 
 
 Add a new member to save a list of correct numbers of steps or save a list of the sizes of validation DataLoaders. ProgressBar can update self.val_progress_bar.total using the new member.  Users of Trainer can still get num_sanity_val_steps passed into Trainer.__init__ by accessing the member num_sanity_val_steps. (Supposing #2882 is resolved and the member num_sanity_val_steps is independent of limit_val_batches )
 
 
 Compute the correct number of steps in the ProgressBar with the help of pytorch_lightning.trainer.data_loading._has_len
 
 
 It seems that the values of the public member Trainer.num_sanity_val_steps in stable 0.8.5 and master are different.
 
 
 In 0.8.5, Trainer.num_sanity_val_steps is num_sanity_val_steps passed in __init__.
 
 
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/trainer.py
 
 
          Line 491
       in
       1e68968
 
 
 
 
 
 
  self.num_sanity_val_steps = float("inf") if num_sanity_val_steps == -1 else num_sanity_val_steps 
 
 
 
 
 
 sets Trainer.num_sanity_val_steps to float('inf') when the one passed in __init__ is -1.
 
 
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/trainer.py
 
 
          Line 461
       in
       a59e140
 
 
 
 
 
 
  self.num_sanity_val_steps = min(num_sanity_val_steps, limit_val_batches) 
 
 
 
 
 
 decides Trainer.num_sanity_val_steps according to limit_val_batches  and num_sanity_val_steps passed in __init__.
 
 
 		</comment>
 		<comment id='6' author='manipopopo' date='2020-08-22T04:07:27Z'>
 		Fixed by &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2917&gt;#2917&lt;/denchmark-link&gt;
 .
 		</comment>
 		<comment id='7' author='manipopopo' date='2020-08-27T21:53:34Z'>
 		It's broken again due to refactors, opening this so that we don't forget to fix this. If required, let's fix this after a week once refactors are done.
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/trainer.py
 
 
          Line 1256
       in
       85cd558
 
 
 
 
 
 
  _, eval_results = self.run_evaluation(test_mode=False, max_batches=self.num_sanity_val_batches) 
 
 
 
 
 
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/evaluation_loop.py
 
 
          Line 246
       in
       85cd558
 
 
 
 
 
 
  self.evaluation_loop.on_evaluation_start() 
 
 
 
 
 
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/evaluate_loop.py
 
 
         Lines 53 to 57
       in
       85cd558
 
 
 
 
 
 
  def on_evaluation_start(self, *args, **kwargs): 
 
 
 
  if self.testing: 
 
 
 
  self.trainer.call_hook('on_test_start', *args, **kwargs) 
 
 
 
  else: 
 
 
 
  self.trainer.call_hook('on_validation_start', *args, **kwargs) 
 
 
 
 
 
 
 
 
 pytorch-lightning/pytorch_lightning/callbacks/progress.py
 
 
         Lines 341 to 344
       in
       85cd558
 
 
 
 
 
 
  def on_validation_start(self, trainer, pl_module): 
 
 
 
  super().on_validation_start(trainer, pl_module) 
 
 
 
  self.val_progress_bar = self.init_validation_tqdm() 
 
 
 
  self.val_progress_bar.total = convert_inf(self.total_val_batches) 
 
 
 
 
 
 		</comment>
 		<comment id='8' author='manipopopo' date='2020-09-30T18:26:14Z'>
 		this issue is annoying. will fix this.
 		</comment>
 	</comments>
 </bug>
<commit id='a628d181ee662a77b708a12c51477f912ce02f63' author='Rohit Gupta' date='2020-10-04 08:32:18-04:00'>
 	<dmm_unit complexity='1.0' interfacing='0.043478260869565216' size='0.17391304347826086'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>96,98,99,100</added_lines>
 			<deleted_lines>96,98</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\callbacks\progress.py' new_name='pytorch_lightning\callbacks\progress.py'>
 		<file_info nloc='301' complexity='57' token_count='1363'></file_info>
 		<method name='on_validation_start' parameters='self,trainer,pl_module'>
 				<method_info nloc='5' complexity='2' token_count='46' nesting_level='1' start_line='341' end_line='345'></method_info>
 			<added_lines>343,344,345</added_lines>
 			<deleted_lines>343,344</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\callbacks\test_progress_bar.py' new_name='tests\callbacks\test_progress_bar.py'>
 		<file_info nloc='159' complexity='25' token_count='1159'></file_info>
 		<method name='test_num_sanity_val_steps_progress_bar.__init__' parameters='self'>
 				<method_info nloc='3' complexity='1' token_count='17' nesting_level='2' start_line='207' end_line='209'></method_info>
 			<added_lines>207,208,209</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_num_sanity_val_steps_progress_bar.on_validation_epoch_end' parameters='self,trainer,pl_module'>
 				<method_info nloc='2' complexity='1' token_count='20' nesting_level='2' start_line='211' end_line='212'></method_info>
 			<added_lines>211,212</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_num_sanity_val_steps_progress_bar' parameters='tmpdir,limit_val_batches,expected'>
 				<method_info nloc='19' complexity='1' token_count='87' nesting_level='0' start_line='202' end_line='229'></method_info>
 			<added_lines>202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>196,197,198,199,200,201</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\trainer\test_trainer.py' new_name='tests\trainer\test_trainer.py'>
 		<file_info nloc='817' complexity='98' token_count='6538'></file_info>
 		<method name='test_num_sanity_val_steps' parameters='tmpdir,limit_val_batches'>
 				<method_info nloc='13' complexity='1' token_count='64' nesting_level='0' start_line='946' end_line='960'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>960</deleted_lines>
 		</method>
 		<method name='test_num_sanity_val_steps_neg_one' parameters='tmpdir,limit_val_batches'>
 				<method_info nloc='12' complexity='1' token_count='65' nesting_level='0' start_line='969' end_line='984'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>984</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
