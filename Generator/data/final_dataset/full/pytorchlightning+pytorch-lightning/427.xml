<bug_data>
<bug id='427' author='ssaru' open_date='2019-10-24T17:39:22Z' closed_time='2020-05-17T13:24:18Z'>
 	<summary>save_weights_only parameter in ModelCheckpoint class look like doesn't work</summary>
 	<description>
 &lt;denchmark-h:h3&gt;Common bugs:&lt;/denchmark-h&gt;
 
 
 Tensorboard not showing in Jupyter-notebook see issue 79.
 PyTorch 1.1.0 vs 1.2.0 support see FAQ
 
 Describe the bug
 save_weights_only parameter in ModelCheckpoint class look like doesn't work
 document describe save_weight_only like that
 save_weights_only: if True, then only the model's weights will be saved (model.save_weights(filepath)), else the full model is saved (model.save(filepath)).
 but save_weight_only parameter doesn't save model differently each different options
 To Reproduce
 Steps to reproduce the behavior:
 
 I used sample script in official document
 
 &lt;denchmark-code&gt;import os
 import torch
 from torch.nn import functional as F
 from torch.utils.data import DataLoader
 from torchvision.datasets import MNIST
 import torchvision.transforms as transforms
 
 import pytorch_lightning as pl
 
 class CoolSystem(pl.LightningModule):
 
     def __init__(self):
         super(CoolSystem, self).__init__()
         # not the best model...
         self.l1 = torch.nn.Linear(28 * 28, 10)
 
     def forward(self, x):
         return torch.relu(self.l1(x.view(x.size(0), -1)))
 
     def training_step(self, batch, batch_nb):
         # REQUIRED
         x, y = batch
         y_hat = self.forward(x)
         loss = F.cross_entropy(y_hat, y)
         tensorboard_logs = {'train_loss': loss}
         return {'loss': loss, 'log': tensorboard_logs}
 
     def validation_step(self, batch, batch_nb):
         # OPTIONAL
         x, y = batch
         y_hat = self.forward(x)
         return {'val_loss': F.cross_entropy(y_hat, y)}
 
     def validation_end(self, outputs):
         # OPTIONAL
         avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
         tensorboard_logs = {'val_loss': avg_loss}
         return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}
 
     def configure_optimizers(self):
         # REQUIRED
         # can return multiple optimizers and learning_rate schedulers
         # (LBFGS it is automatically supported, no need for closure function)
         return torch.optim.Adam(self.parameters(), lr=0.02)
 
     @pl.data_loader
     def train_dataloader(self):
         # REQUIRED
         return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)
 
     @pl.data_loader
     def val_dataloader(self):
         # OPTIONAL
         return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)
 
     @pl.data_loader
     def test_dataloader(self):
         # OPTIONAL
         return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)
 
 if __name__ == "__main__":
     from pytorch_lightning import Trainer
     from pytorch_lightning.callbacks import ModelCheckpoint
 
     weight_path = os.path.join(os.getcwd(), 'checkpoint')
     if not os.path.exists(weight_path):
         os.mkdir(weight_path)
 
     checkpoint_callback = ModelCheckpoint(
         filepath=weight_path,
         save_best_only=False,
         verbose=True,
         monitor='val_loss',
         mode='min',
         prefix='',
         save_weights_only=False
     )
 
     gpus = torch.cuda.device_count()
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
     model = CoolSystem()
     model.to(device)
     trainer = Trainer(checkpoint_callback=checkpoint_callback,
                       max_nb_epochs=1, train_percent_check=0.1)
     trainer.fit(model)
 &lt;/denchmark-code&gt;
 
 
 i just changed save_weights_only parameter &amp; weight directory for saving. because if i don't model try restore weight
 
 for example i save model in "checkpoint" directory.
 after trained i move ckpt file other directory like called test
 so, test directory have two model file save_weight_only_True, save_weight_only_False
 
 
 i was checking what they have some different using torch.load
 (PATH). but not different...
 
 
 two file has same parameter like this
 save_weights_only_False
 
 
 &lt;denchmark-code&gt;{'epoch': 0, 'global_step': 187, 'checkpoint_callback_best': inf, 'optimizer_states': [{'state': {4830682784: {'step': 187, 'exp_avg': tensor([[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]), 'exp_avg_sq': tensor([[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]])}, 4830682928: {'step': 187, 'exp_avg': tensor([ 2.8793e-12, -2.0038e-03,  1.6457e-03,  9.9219e-12, -1.4364e-02,
          7.6137e-03,  9.4487e-12,  1.0773e-11,  9.9046e-03, -3.7999e-03]), 'exp_avg_sq': tensor([7.2368e-08, 7.3899e-05, 1.2118e-04, 8.5934e-07, 1.5810e-04, 9.1419e-05,
         7.7932e-07, 1.0131e-06, 1.5449e-04, 2.0831e-04])}}, 'param_groups': [{'lr': 0.02, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [4830682784, 4830682928]}]}], 'lr_schedulers': [], 'state_dict': OrderedDict([('l1.weight', tensor([[ 0.0198, -0.0171, -0.0024,  ..., -0.0154,  0.0243, -0.0241],
         [ 0.0349, -0.0040, -0.0051,  ...,  0.0159,  0.0295,  0.0129],
         [ 0.0302, -0.0218, -0.0171,  ...,  0.0104,  0.0179, -0.0167],
         ...,
         [-0.0284,  0.0056,  0.0178,  ...,  0.0211, -0.0075,  0.0163],
         [ 0.0145,  0.0204,  0.0121,  ..., -0.0013, -0.0103,  0.0043],
         [ 0.0059, -0.0060,  0.0017,  ..., -0.0214,  0.0273, -0.0288]])), ('l1.bias', tensor([-0.1228,  0.3341, -0.0163, -0.1302,  0.1738,  0.3422, -0.1155, -0.1122,
         -0.5156, -0.1902]))])}
 &lt;/denchmark-code&gt;
 
 save_weights_only_True
 &lt;denchmark-code&gt;{'epoch': 0, 'global_step': 187, 'checkpoint_callback_best': inf, 'optimizer_states': [{'state': {4877819552: {'step': 187, 'exp_avg': tensor([[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]), 'exp_avg_sq': tensor([[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]])}, 4877819696: {'step': 187, 'exp_avg': tensor([ 1.4936e-11, -1.6439e-03, -6.1600e-04, -4.5436e-03, -8.8385e-03,
          4.9613e-12,  1.6526e-11,  4.4405e-03,  1.8937e-12, -6.5171e-04]), 'exp_avg_sq': tensor([1.9473e-06, 8.8430e-05, 1.4157e-04, 1.1038e-04, 1.3500e-04, 2.1486e-07,
         2.3841e-06, 1.1693e-04, 3.1302e-08, 2.3050e-04])}}, 'param_groups': [{'lr': 0.02, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [4877819552, 4877819696]}]}], 'lr_schedulers': [], 'state_dict': OrderedDict([('l1.weight', tensor([[-0.0299,  0.0281, -0.0246,  ...,  0.0143,  0.0314,  0.0167],
         [ 0.0066, -0.0160,  0.0200,  ..., -0.0266,  0.0097,  0.0138],
         [ 0.0257,  0.0134, -0.0111,  ..., -0.0201,  0.0199, -0.0146],
         ...,
         [ 0.0260, -0.0082, -0.0049,  ...,  0.0277, -0.0070,  0.0275],
         [ 0.0089, -0.0003, -0.0051,  ..., -0.0086,  0.0285, -0.0252],
         [-0.0202,  0.0252,  0.0083,  ..., -0.0144, -0.0181,  0.0105]])), ('l1.bias', tensor([-0.1031,  0.2837,  0.0276, -0.1406,  0.2007, -0.1086, -0.1267,  0.2557,
         -0.1239, -0.0374]))])}
 &lt;/denchmark-code&gt;
 
 Expected behavior
 if save_weights_only: if True
 expected value like this
 &lt;denchmark-code&gt;{'state_dict': OrderedDict([('l1.weight', tensor([[-0.0299,  0.0281, -0.0246,  ...,  0.0143,  0.0314,  0.0167],
         [ 0.0066, -0.0160,  0.0200,  ..., -0.0266,  0.0097,  0.0138],
         [ 0.0257,  0.0134, -0.0111,  ..., -0.0201,  0.0199, -0.0146],
         ...,
         [ 0.0260, -0.0082, -0.0049,  ...,  0.0277, -0.0070,  0.0275],
         [ 0.0089, -0.0003, -0.0051,  ..., -0.0086,  0.0285, -0.0252],
         [-0.0202,  0.0252,  0.0083,  ..., -0.0144, -0.0181,  0.0105]])), ('l1.bias', tensor([-0.1031,  0.2837,  0.0276, -0.1406,  0.2007, -0.1086, -0.1267,  0.2557,
         -0.1239, -0.0374]))])}
 &lt;/denchmark-code&gt;
 
 Screenshots
 If applicable, add screenshots to help explain your problem.
 Desktop (please complete the following information):
 
 OS: MacBook Pro (15-inch, 2018); Mojave
 Browser: chrome
 Version:  pytorch-lightning==0.5.2.1, torch==1.3.0.post2, torchvision==0.4.1.post2, test-tube==0.7.3
 
 Additional context
 I try to find some reason, why save_weights_only parameter doesn't work
 i found &lt;denchmark-link:https://github.com/williamFalcon/pytorch-lightning/blob/master/pytorch_lightning/trainer/trainer_io.py&gt;TrainerIOMixin class&lt;/denchmark-link&gt;
  inside PyTorch lightning. and I feel save_weights_only parameter not was implemented in PyTorch lightning
 	</description>
 	<comments>
 		<comment id='1' author='ssaru' date='2020-02-22T02:06:33Z'>
 		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
 		</comment>
 		<comment id='2' author='ssaru' date='2020-03-01T10:10:33Z'>
 		&lt;denchmark-link:https://github.com/ssaru&gt;@ssaru&lt;/denchmark-link&gt;
  could you check it with the latest version on master?
 		</comment>
 		<comment id='3' author='ssaru' date='2020-03-03T15:13:22Z'>
 		Appreciate your reply.
 I will check it
 		</comment>
 		<comment id='4' author='ssaru' date='2020-03-26T13:58:08Z'>
 		&lt;denchmark-link:https://github.com/ssaru&gt;@ssaru&lt;/denchmark-link&gt;
  I assume that it was fixed, if not pls feel free to reopen... 
 		</comment>
 		<comment id='5' author='ssaru' date='2020-04-29T19:41:49Z'>
 		&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
  It's not fixed yet.
 &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/42d5cfc3b056b4c82a77a7cdcb8eafc63a812b67/pytorch_lightning/trainer/training_io.py#L291&gt;dump_checkpoint&lt;/denchmark-link&gt;
  still returns everything regardless of the  parameter.
 		</comment>
 		<comment id='6' author='ssaru' date='2020-05-11T07:14:25Z'>
 		I just ran into this bug myself. I can work on it, but probably not for another week or so. I took a quick look through the code and it doesn't seem too difficult to fix, but as I'm not familiar with the entire code base there might be some distant issue I haven't seen, but I think dump_checkpoint would need an argument and then it could save nothing but the state_dict; plus there'd have to be corresponding checks for the extra checkpoint keys added to restore and restore_training_state.
 		</comment>
 		<comment id='7' author='ssaru' date='2020-05-11T08:14:22Z'>
 		&lt;denchmark-link:https://github.com/rightaditya&gt;@rightaditya&lt;/denchmark-link&gt;
  mind draft a PR and we can help to finish it fast... :]
 		</comment>
 		<comment id='8' author='ssaru' date='2020-05-11T14:04:53Z'>
 		I also just run into this and went ahead and created a draft PR. Saving only the weights is working. However, I haven't changed any logic regarding loading.
 		</comment>
 	</comments>
 </bug>
<commit id='8c4c7b105e16fbe255e4715f54af2fa5d2a12fad' author='Fabio Natanael Kepler' date='2020-05-17 09:24:17-04:00'>
 	<dmm_unit complexity='0.7692307692307693' interfacing='0.5769230769230769' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>74,75</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\callbacks\model_checkpoint.py' new_name='pytorch_lightning\callbacks\model_checkpoint.py'>
 		<file_info nloc='217' complexity='34' token_count='1053'></file_info>
 		<method name='_save_model' parameters='self,filepath'>
 				<method_info nloc='6' complexity='2' token_count='49' nesting_level='1' start_line='136' end_line='144'></method_info>
 			<added_lines>142</added_lines>
 			<deleted_lines>142</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\training_io.py' new_name='pytorch_lightning\trainer\training_io.py'>
 		<file_info nloc='333' complexity='78' token_count='1655'></file_info>
 		<method name='dump_checkpoint' parameters='self'>
 				<method_info nloc='41' complexity='13' token_count='283' nesting_level='1' start_line='309' end_line='366'></method_info>
 			<added_lines>309,315,316,317,318,319,320,321,323,324,325,326,328,330,331,332,333,335,337,338,339</added_lines>
 			<deleted_lines>309,315,316,318,319,320,322,323,324,325,327,329,330,331,332,334,341,342,343,344</deleted_lines>
 		</method>
 		<method name='restore_training_state' parameters='self,checkpoint'>
 				<method_info nloc='32' complexity='16' token_count='250' nesting_level='1' start_line='388' end_line='438'></method_info>
 			<added_lines>395,396,397,398,399,400</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='save_checkpoint' parameters='self,filepath,bool'>
 				<method_info nloc='11' complexity='4' token_count='67' nesting_level='1' start_line='259' end_line='272'></method_info>
 			<added_lines>259,260</added_lines>
 			<deleted_lines>259,260</deleted_lines>
 		</method>
 		<method name='dump_checkpoint' parameters='self,bool'>
 				<method_info nloc='42' complexity='12' token_count='273' nesting_level='1' start_line='309' end_line='367'></method_info>
 			<added_lines>309,315,316,317,318,319,320,321,323,324,325,326,328,330,331,332,333,335,337,338,339</added_lines>
 			<deleted_lines>309,315,316,318,319,320,322,323,324,325,327,329,330,331,332,334,341,342,343,344</deleted_lines>
 		</method>
 		<method name='save_checkpoint' parameters='self,filepath'>
 				<method_info nloc='11' complexity='4' token_count='60' nesting_level='1' start_line='259' end_line='272'></method_info>
 			<added_lines>259,260</added_lines>
 			<deleted_lines>259,260</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\trainer\test_trainer.py' new_name='tests\trainer\test_trainer.py'>
 		<file_info nloc='558' complexity='59' token_count='4445'></file_info>
 		<method name='test_model_checkpoint_options.mock_save_function' parameters='filepath'>
 				<method_info nloc='2' complexity='1' token_count='15' nesting_level='1' start_line='273' end_line='274'></method_info>
 			<added_lines>273</added_lines>
 			<deleted_lines>273</deleted_lines>
 		</method>
 		<method name='test_model_checkpoint_options.mock_save_function' parameters='filepath,args'>
 				<method_info nloc='2' complexity='1' token_count='18' nesting_level='1' start_line='273' end_line='274'></method_info>
 			<added_lines>273</added_lines>
 			<deleted_lines>273</deleted_lines>
 		</method>
 		<method name='test_model_checkpoint_only_weights' parameters='tmpdir'>
 				<method_info nloc='20' complexity='1' token_count='154' nesting_level='0' start_line='299' end_line='334'></method_info>
 			<added_lines>299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_model_checkpoint_options' parameters='tmpdir,save_top_k,file_prefix,expected_files'>
 				<method_info nloc='15' complexity='3' token_count='137' nesting_level='0' start_line='270' end_line='296'></method_info>
 			<added_lines>273</added_lines>
 			<deleted_lines>273</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>335,336</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
