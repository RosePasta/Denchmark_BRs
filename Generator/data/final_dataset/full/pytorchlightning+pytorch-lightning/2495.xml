<bug_data>
<bug id='2495' author='tadejsv' open_date='2020-07-04T12:30:29Z' closed_time='2020-07-05T02:52:50Z'>
 	<summary>`precision=16` displaying wrong loss in progress bar</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 When training on a GPU (using native AMP) and setting precision=16, the loss displayed by the progress bar is some crazy large number.
 Stopping the example bellow in the middle of Epoch 1 gives a loss of ~15 000. If I train with precision=32, this loss is the true value of ~0.23.
 The loss tensor is OK, if I add a print statement in the training loop it displays normal values.
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 import torch
 from torch.nn import functional as F
 from torch import nn
 
 from pytorch_lightning.core.lightning import LightningModule
 from pytorch_lightning import Trainer
 
 from torch.utils.data import DataLoader, random_split
 from torchvision.datasets import MNIST
 
 import os
 from torchvision import datasets, transforms
 
 class LitMNIST(LightningModule):
     def __init__(self):
         super().__init__()
         self.layer_1 = torch.nn.Linear(28 * 28, 128)
         self.layer_2 = torch.nn.Linear(128, 256)
         self.layer_3 = torch.nn.Linear(256, 10)
 
     def forward(self, x):
         batch_size, channels, width, height = x.size()
         x = x.view(batch_size, -1)
         x = self.layer_1(x)
         x = torch.relu(x)
         x = self.layer_2(x)
         x = torch.relu(x)
         x = self.layer_3(x)
         x = torch.log_softmax(x, dim=1)
         return x
 
     def train_dataloader(self):
         transform=transforms.Compose([transforms.ToTensor(),
                                       transforms.Normalize((0.1307,), (0.3081,))])
         mnist_train = MNIST(os.getcwd(), train=True, download=False, transform=transform)
         return DataLoader(mnist_train, batch_size=64)
 
     def configure_optimizers(self):
         return torch.optim.Adam(self.parameters(), lr=1e-3)
 
     def training_step(self, batch, batch_idx):
         x, y = batch
         logits = self(x)
         loss = F.nll_loss(logits, y)
 
         # add logging
         logs = {'loss': loss}
         return {'loss': loss, 'log': logs}
     
 
 model = LitMNIST()
 trainer = Trainer(gpus=1, precision=16)
 trainer.fit(model)
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 
 PyTorch Version:  v1.7.0.dev20200704 (nightly)
 OS (e.g., Linux): Ubuntu 18.04
 How you installed PyTorch (conda, pip, source): conda
 Python version: 3.8
 CUDA/cuDNN version: 10.2 (installed with conda from pytorch chanel)
 GPU models and configuration: RTX 2070 SUPER
 
 	</description>
 	<comments>
 		<comment id='1' author='tadejsv' date='2020-07-04T12:31:18Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 		<comment id='2' author='tadejsv' date='2020-10-30T10:18:01Z'>
 		I have the same issue, but not on the sample code provided. The only difference in my sample code is that using a custom dataset and L1 loss the numbers are (initially) over 200.0 (as opposed to MNIST where it starts very low).
 All is fine until after ~10 epochs where the loss in the pbar becomes 'inf'; whereas the logs shows the correct value being around 20.
 		</comment>
 	</comments>
 </bug>
<commit id='9924c76faa7789294811a27c392ba6b33e07f3f1' author='William Falcon' date='2020-07-04 22:52:49-04:00'>
 	<dmm_unit complexity='0.8888888888888888' interfacing='0.4444444444444444' size='0.5555555555555556'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pl_examples\models\lightning_template.py' new_name='pl_examples\models\lightning_template.py'>
 		<file_info nloc='131' complexity='20' token_count='1121'></file_info>
 		<method name='test_dataloader' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='24' nesting_level='1' start_line='162' end_line='163'></method_info>
 			<added_lines>163</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='val_dataloader' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='24' nesting_level='1' start_line='159' end_line='160'></method_info>
 			<added_lines>160</added_lines>
 			<deleted_lines>159</deleted_lines>
 		</method>
 		<method name='add_model_specific_args' parameters='parent_parser,root_dir'>
 				<method_info nloc='13' complexity='1' token_count='175' nesting_level='1' start_line='166' end_line='191'></method_info>
 			<added_lines>182</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,float,int,int,float,str,str,int,int,int,kwargs'>
 				<method_info nloc='12' complexity='1' token_count='70' nesting_level='1' start_line='40' end_line='51'></method_info>
 			<added_lines>48</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='train_dataloader' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='24' nesting_level='1' start_line='156' end_line='157'></method_info>
 			<added_lines>157</added_lines>
 			<deleted_lines>156</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>34,54,55</added_lines>
 			<deleted_lines>153</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\core\hooks.py' new_name='pytorch_lightning\core\hooks.py'>
 		<file_info nloc='162' complexity='19' token_count='291'></file_info>
 		<method name='amp_scale_loss' parameters='self,unscaled_loss,optimizer,optimizer_idx'>
 				<method_info nloc='6' complexity='2' token_count='40' nesting_level='1' start_line='191' end_line='198'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>194</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\distrib_data_parallel.py' new_name='pytorch_lightning\trainer\distrib_data_parallel.py'>
 		<file_info nloc='440' complexity='100' token_count='2071'></file_info>
 		<method name='spawn_ddp_children' parameters='self,model'>
 				<method_info nloc='35' complexity='8' token_count='290' nesting_level='1' start_line='394' end_line='450'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>439</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\training_loop.py' new_name='pytorch_lightning\trainer\training_loop.py'>
 		<file_info nloc='647' complexity='158' token_count='3325'></file_info>
 		<method name='run_training_batch' parameters='self,batch,batch_idx'>
 				<method_info nloc='58' complexity='18' token_count='442' nesting_level='1' start_line='580' end_line='687'></method_info>
 			<added_lines>639</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='optimizer_closure' parameters='self,split_batch,batch_idx,opt_idx,optimizer,hiddens'>
 				<method_info nloc='50' complexity='11' token_count='354' nesting_level='1' start_line='762' end_line='847'></method_info>
 			<added_lines>801,802,803,842</added_lines>
 			<deleted_lines>837</deleted_lines>
 		</method>
 		<method name='call_optimizer_step' parameters='self,optimizer,opt_idx,batch_idx,split_batch'>
 				<method_info nloc='28' complexity='9' token_count='187' nesting_level='1' start_line='715' end_line='760'></method_info>
 			<added_lines>745</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\base\deterministic_model.py' new_name='tests\base\deterministic_model.py'>
 		<file_info nloc='103' complexity='27' token_count='1024'></file_info>
 		<method name='backward' parameters='self,trainer,loss,optimizer,optimizer_idx'>
 				<method_info nloc='6' complexity='2' token_count='41' nesting_level='1' start_line='136' end_line='141'></method_info>
 			<added_lines>137,138,139,140</added_lines>
 			<deleted_lines>137</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\trainer\test_trainer_steps.py' new_name='tests\trainer\test_trainer_steps.py'>
 		<file_info nloc='107' complexity='8' token_count='741'></file_info>
 		<method name='test_training_step_dict' parameters='tmpdir'>
 				<method_info nloc='30' complexity='2' token_count='209' nesting_level='0' start_line='9' end_line='50'></method_info>
 			<added_lines>20,21,48,49,50</added_lines>
 			<deleted_lines>36</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>3,4,7,8,51</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
