<bug_data>
<bug id='939' author='helldragger' open_date='2020-02-25T11:51:32Z' closed_time='2020-02-27T20:54:07Z'>
 	<summary>logger is NoneType hence doesn't have any experiment or other functionality in a lightning module</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 When trying to use the logging abilities of lightning, I hit a wall, the default and tensorboard loggers both seem to stay uninitialized when calling trainer.fit(model), resulting in crashes everytime I try to log something.
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 Create a lightning module as such
 &lt;denchmark-code&gt;class SimpleRegressor(pl.LightningModule):
     ...
 &lt;/denchmark-code&gt;
 
 Use the logger anywhere to get this kind of stacktrace:
 &lt;denchmark-code&gt;d:\Documents\projects\MetaWatch\MetaWatch\notebooks\audio-video-interest\simple_regressor.py in configure_optimizers(self)
     105         #see https://pytorch-lightning.readthedocs.io/en/latest/pytorch_lightning.core.lightning.html#pytorch_lightning.core.lightning.LightningModule.configure_optimizers
     106         # REQUIRED
 --&gt; 107         self.logger.experiment.add_hparams({'hidden_layer_size':self.hidden_layer_size,
     108                                             'linear_layer_size':self.linear_layer_size,
     109                                             'lstm_layers':self.lstm_layers})
 
 AttributeError: 'NoneType' object has no attribute 'experiment'
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;import pytorch_lightning as pl
 
 class SimpleRegressor(pl.LightningModule):
     def __init__(self, cuda=False):
         super(SimpleRegressor, self).__init__()
         self.logger.experiment.add_hparams({'hidden_layer_size':1})
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 To log as described in the documentation.
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;PyTorch version: 1.4.0
 Is debug build: No
 CUDA used to build PyTorch: 10.1
 
 OS: Microsoft Windows 10 Pro    
 GCC version: Could not collect  
 CMake version: Could not collect
 
 Python version: 3.7
 Is CUDA available: Yes
 CUDA runtime version: Could not collect
 GPU models and configuration: GPU 0: GeForce GTX 970
 Nvidia driver version: 441.12
 cuDNN version: Could not collect
 
 Versions of relevant libraries:
 [pip3] numpy==1.18.1
 [pip3] pytorch-lightning==0.6.0
 [pip3] tinynumpy==1.2.1
 [pip3] torch==1.4.0
 [pip3] torchvision==0.4.1
 [conda] Could not collect
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='helldragger' date='2020-02-25T11:52:17Z'>
 		Hey, thanks for your contribution! Great first issue!
 		</comment>
 		<comment id='2' author='helldragger' date='2020-02-25T19:04:09Z'>
 		Thanks for the issue! The intended way to acheive this is through a hook. When __init__ is called on the LightningModule, the loggers won't have been created yet. I don't think there's any way to change that so we should update the docs to use a hook instead of __init__.
 &lt;denchmark-link:https://github.com/orgs/PyTorchLightning/teams/core-contributors&gt;@PyTorchLightning/core-contributors&lt;/denchmark-link&gt;
  any other thoughts on this?
 		</comment>
 		<comment id='3' author='helldragger' date='2020-02-26T21:00:09Z'>
 		it also doesn't work in other functions, I tried in the training step, in the configure_optimizers too
 		</comment>
 		<comment id='4' author='helldragger' date='2020-02-27T09:09:05Z'>
 		Ok, most of these work on master (i.e. if you install from github) for me - except configure_optimizers.
 I've opened a PR (&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/964&gt;#964&lt;/denchmark-link&gt;
 ) which fixes that and cleans up the docs a bit.
 		</comment>
 	</comments>
 </bug>
<commit id='f5e0df390c6e1eaf11ad488e297aa2d383daa177' author='Ethan Harris' date='2020-02-27 15:54:06-05:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='docs\source\experiment_logging.rst' new_name='docs\source\experiment_logging.rst'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>25,31,55,61,79,85,105,111,130,136,154,160,162,163,165,166,167</added_lines>
 			<deleted_lines>25,31,55,61,79,85,105,111,130,136,154,160</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\loggers\__init__.py' new_name='pytorch_lightning\loggers\__init__.py'>
 		<file_info nloc='112' complexity='0' token_count='119'></file_info>
 		<modified_lines>
 			<added_lines>3,4,18,19,20,21,22,23,24,30,61,67,78,79,88</added_lines>
 			<deleted_lines>3,17,18,24,55,61,80</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\loggers\base.py' new_name='pytorch_lightning\loggers\base.py'>
 		<file_info nloc='86' complexity='36' token_count='562'></file_info>
 		<method name='__getitem__' parameters='self,int'>
 				<method_info nloc='2' complexity='2' token_count='24' nesting_level='1' start_line='103' end_line='104'></method_info>
 			<added_lines>103,104</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>105</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\trainer.py' new_name='pytorch_lightning\trainer\trainer.py'>
 		<file_info nloc='1021' complexity='69' token_count='2761'></file_info>
 		<method name='run_pretrain_routine' parameters='self,LightningModule'>
 				<method_info nloc='55' complexity='17' token_count='404' nesting_level='1' start_line='1055' end_line='1159'></method_info>
 			<added_lines>1071</added_lines>
 			<deleted_lines>1068,1070,1071</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>940,941,942</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
