<bug_data>
<bug id='2438' author='Llannelongue' open_date='2020-07-01T00:46:26Z' closed_time='2020-07-01T11:38:01Z'>
 	<summary>`validation_epoch_end` and `test_epoch_end` can't return nothing</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 If validation_epoch_end or test_epoch_end returns nothing (as presented as an option in the documentation), an error occurs.
 (Happy to work on a PR to fix this)
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 Steps to reproduce the behavior:
 Overwrite test_epoch_end and remove return (same for validation_epoch_end
 &lt;denchmark-code&gt;File "/.conda/envs/PPI-env/lib/python3.7/site-packages/pytorch_lightning/trainer/logging.py", line 106, in process_output
     for k, v in output.items():
 AttributeError: 'NoneType' object has no attribute 'items'
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 import os
 
 import torch
 from torch.nn import functional as F
 from torch.utils.data import DataLoader
 from torchvision.datasets import MNIST
 from torchvision import transforms
 from pytorch_lightning.core.lightning import LightningModule
 from pytorch_lightning import Trainer, seed_everything
 
 class LitModel(LightningModule):
 
     def __init__(self):
         super().__init__()
         self.l1 = torch.nn.Linear(28 * 28, 10)
 
     def forward(self, x):
         return torch.relu(self.l1(x.view(x.size(0), -1)))
 
     def training_step(self, batch, batch_idx):
         x, y = batch
         y_hat = self(x)
         loss = F.cross_entropy(y_hat, y)
         tensorboard_logs = {'train_loss': loss}
         return {'loss': loss, 'log': tensorboard_logs}
 
     def configure_optimizers(self):
         return torch.optim.Adam(self.parameters(), lr=0.001)
 
     def train_dataloader(self):
         dataset = MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())
         loader = DataLoader(dataset, batch_size=32, num_workers=4, shuffle=True)
         return loader
 
     def validation_step(self, batch, batch_idx):
         x, y = batch
         y_hat = self(x)
         return {'val_loss': F.cross_entropy(y_hat, y)}
 
     def validation_epoch_end(self, outputs):
         avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
         tensorboard_logs = {'val_loss': avg_loss}
         # return {'val_loss': avg_loss, 'log': tensorboard_logs}
 
     def val_dataloader(self):
         # TODO: do a real train/val split
         dataset = MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor())
         loader = DataLoader(dataset, batch_size=32, num_workers=4)
         return loader
 
 
 def main():
     seed_everything(42)
 
     model = LitModel()
     # most basic trainer, uses good defaults
     trainer = Trainer(fast_dev_run=True)
     trainer.fit(model)
 
 if __name__ == '__main__':
     main()
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 It should check if there is nothing returned and carry on.
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 
 CUDA:
 
 GPU:
 available:         False
 version:           None
 
 
 Packages:
 
 numpy:             1.17.4
 pyTorch_debug:     False
 pyTorch_version:   1.5.1
 pytorch-lightning: 0.8.3
 tensorboard:       2.2.2
 tqdm:              4.41.1
 
 
 System:
 
 OS:                Darwin
 architecture:
 
 64bit
 
 
 
 processor:         i386
 python:            3.7.6
 version:           Darwin Kernel Version 19.4.0: Wed Mar  4 22:28:40 PST 2020; root:xnu-6153.101.6~15/RELEASE_X86_64
 
 
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='Llannelongue' date='2020-07-01T03:05:55Z'>
 		yes, please submit a PR? you should be allowed to return nothing
 		</comment>
 		<comment id='2' author='Llannelongue' date='2020-07-01T07:15:11Z'>
 		Yes, working on it üëç
 		</comment>
 	</comments>
 </bug>
<commit id='325852c6df93f749bb843bff1a3cdba41698722c' author='William Falcon' date='2020-07-01 07:38:00-04:00'>
 	<dmm_unit complexity='0.8947368421052632' interfacing='1.0' size='0.0'></dmm_unit>
 	<modification change_type='ADD' old_name='None' new_name='docs\source\bolts.rst'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 	</modification>
 	<modification change_type='MODIFY' old_name='docs\source\callbacks.rst' new_name='docs\source\callbacks.rst'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>52,53,54,55,56,57,58,59</added_lines>
 			<deleted_lines>52,53,54,55,56,57,58,59</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='docs\source\conf.py' new_name='docs\source\conf.py'>
 		<file_info nloc='194' complexity='18' token_count='1068'></file_info>
 		<modified_lines>
 			<added_lines>142</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='docs\source\hooks.rst' new_name='docs\source\hooks.rst'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>23,24,35,36</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='docs\source\index.rst' new_name='docs\source\index.rst'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>30,31,32,33,34,35,36</added_lines>
 			<deleted_lines>38,103</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\core\lightning.py' new_name='pytorch_lightning\core\lightning.py'>
 		<file_info nloc='1612' complexity='86' token_count='2270'></file_info>
 		<method name='test_dataloader' parameters='self'>
 				<method_info nloc='53' complexity='1' token_count='17' nesting_level='1' start_line='1387' end_line='1439'></method_info>
 			<added_lines>1394,1395,1396,1397,1398,1399,1400,1401,1402,1407</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='train_dataloader' parameters='self'>
 				<method_info nloc='46' complexity='1' token_count='12' nesting_level='1' start_line='1330' end_line='1375'></method_info>
 			<added_lines>1340,1341,1342,1343,1344,1345,1346,1347,1352</added_lines>
 			<deleted_lines>1340</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>1386</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\evaluation_loop.py' new_name='pytorch_lightning\trainer\evaluation_loop.py'>
 		<file_info nloc='349' complexity='45' token_count='1354'></file_info>
 		<method name='run_evaluation' parameters='self,bool'>
 				<method_info nloc='45' complexity='15' token_count='252' nesting_level='1' start_line='370' end_line='447'></method_info>
 			<added_lines>411,412,413,415,416,418,419,420,421,422,423,425,426,427,428,429</added_lines>
 			<deleted_lines>409,411,412,414,415,416,417,418,419,421,422,424,425</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>344,345,346,347</added_lines>
 			<deleted_lines>344,345,346</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\trainer.py' new_name='pytorch_lightning\trainer\trainer.py'>
 		<file_info nloc='1064' complexity='105' token_count='4328'></file_info>
 		<method name='run_pretrain_routine' parameters='self,LightningModule'>
 				<method_info nloc='50' complexity='23' token_count='364' nesting_level='1' start_line='1056' end_line='1156'></method_info>
 			<added_lines>1140,1141,1142,1143,1144</added_lines>
 			<deleted_lines>1145,1146</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>132,133,134,135,136</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
