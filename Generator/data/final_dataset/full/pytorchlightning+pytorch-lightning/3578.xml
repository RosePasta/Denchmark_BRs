<bug_data>
<bug id='3578' author='carmocca' open_date='2020-09-21T00:54:24Z' closed_time='2020-09-25T12:18:07Z'>
 	<summary>Incorrect "Saving latest checkpoint" warning</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 "Saving latest checkpoint..." warning appears regardless of whether a ModelCheckpoint exists or save_last is set to True
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_loop.py
 
 
         Lines 167 to 169
       in
       a71d62d
 
 
 
 
 
 
  # Save latest checkpoint 
 
 
 
  rank_zero_warn('Saving latest checkpoint..') 
 
 
 
  self.check_checkpoint_callback(should_check_val=False, force_save=True) 
 
 
 
 
 
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_loop.py
 
 
         Lines 196 to 204
       in
       a71d62d
 
 
 
 
 
 
  def check_checkpoint_callback(self, should_check_val, force_save=False): 
 
 
 
  model = self.trainer.get_model() 
 
 
 
  
 
 
 
  # when no val loop is present or fast-dev-run still need to call checkpoints 
 
 
 
  # TODO bake this logic into the checkpoint callback 
 
 
 
  should_activate = not is_overridden('validation_step', model) and not should_check_val 
 
 
 
  if should_activate or force_save: 
 
 
 
  checkpoint_callbacks = [c for c in self.trainer.callbacks if isinstance(c, ModelCheckpoint)] 
 
 
 
          [c.on_validation_end(self.trainer, model) for c in checkpoint_callbacks] 
 
 
 
 
 
 This might confuse an user to think the last checkpoint got saved when it did not.
 &lt;denchmark-h:h2&gt;Proposed change:&lt;/denchmark-h&gt;
 
 def check_checkpoint_callback(self, should_check_val, force_save=False):
     model = self.trainer.get_model()
 
     # when no val loop is present or fast-dev-run still need to call checkpoints
     # TODO bake this logic into the checkpoint callback
     should_activate = not is_overridden('validation_step', model) and not should_check_val
     if should_activate or force_save:
         checkpoint_callbacks = [c for c in self.trainer.callbacks if isinstance(c, ModelCheckpoint)]
         if any(c.save_last for c in checkpoint_callbacks):
             rank_zero_warn('Saving latest checkpoint..')
         [c.on_validation_end(self.trainer, model) for c in checkpoint_callbacks]
 	</description>
 	<comments>
 		<comment id='1' author='carmocca' date='2020-09-21T01:15:09Z'>
 		why not just remove the log line from training_loop and defer logging about saving the latest checkpoint to be within the checkpoint callback? that seems simpler to me
 		</comment>
 		<comment id='2' author='carmocca' date='2020-09-21T01:33:25Z'>
 		Because the logic to save last is inside of on_validation_end so it would appear after the first validation run
 		</comment>
 		<comment id='3' author='carmocca' date='2020-09-21T12:25:59Z'>
 		
 save_last is set to True
 
 it was meant to save the checkpoint if someone interrupts the training.
 
 regardless of whether a ModelCheckpoint exists
 
 yea, should not log if no ModelCheckpoint is used.
 		</comment>
 		<comment id='4' author='carmocca' date='2020-09-21T15:21:39Z'>
 		Thanks for the issue &lt;denchmark-link:https://github.com/carmocca&gt;@carmocca&lt;/denchmark-link&gt;
  . Mind sending a PR?
 		</comment>
 		<comment id='5' author='carmocca' date='2020-09-21T17:35:42Z'>
 		Done!
 		</comment>
 	</comments>
 </bug>
<commit id='ed12e422a42472af1acb88f870dba3d43710b31d' author='Carlos Mochol√≠' date='2020-09-25 14:18:06+02:00'>
 	<dmm_unit complexity='0.9' interfacing='0.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\training_loop.py' new_name='pytorch_lightning\trainer\training_loop.py'>
 		<file_info nloc='408' complexity='138' token_count='3386'></file_info>
 		<method name='check_checkpoint_callback' parameters='self,should_save,is_last'>
 				<method_info nloc='7' complexity='8' token_count='77' nesting_level='1' start_line='195' end_line='202'></method_info>
 			<added_lines>195,197,199,200,201</added_lines>
 			<deleted_lines>196,197,198,199,201,202</deleted_lines>
 		</method>
 		<method name='should_check_val_fx' parameters='self,batch_idx,is_last_batch'>
 				<method_info nloc='8' complexity='6' token_count='86' nesting_level='1' start_line='667' end_line='676'></method_info>
 			<added_lines>670,671,673</added_lines>
 			<deleted_lines>674,675</deleted_lines>
 		</method>
 		<method name='check_checkpoint_callback' parameters='self,should_check_val,force_save'>
 				<method_info nloc='6' complexity='7' token_count='73' nesting_level='1' start_line='196' end_line='204'></method_info>
 			<added_lines>197,199,200,201</added_lines>
 			<deleted_lines>196,197,198,199,201,202</deleted_lines>
 		</method>
 		<method name='run_training_epoch' parameters='self'>
 				<method_info nloc='41' complexity='9' token_count='304' nesting_level='1' start_line='416' end_line='505'></method_info>
 			<added_lines>430,501,502</added_lines>
 			<deleted_lines>505</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,trainer'>
 				<method_info nloc='8' complexity='1' token_count='47' nesting_level='1' start_line='37' end_line='44'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>39</deleted_lines>
 		</method>
 		<method name='on_train_epoch_start' parameters='self,epoch'>
 				<method_info nloc='17' complexity='2' token_count='115' nesting_level='1' start_line='206' end_line='236'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>227,228,229</deleted_lines>
 		</method>
 		<method name='on_train_end' parameters='self'>
 				<method_info nloc='19' complexity='9' token_count='146' nesting_level='1' start_line='161' end_line='193'></method_info>
 			<added_lines>167,168</added_lines>
 			<deleted_lines>167,168,169</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>16,17,21,23,24,27,29,30,31,32,33</added_lines>
 			<deleted_lines>19,20,22,23,24,26,27,30,31,32,506,677</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\callbacks\test_model_checkpoint.py' new_name='tests\callbacks\test_model_checkpoint.py'>
 		<file_info nloc='212' complexity='25' token_count='1567'></file_info>
 		<method name='test_model_checkpoint_save_last_warning' parameters='tmpdir,caplog,max_epochs,should_validate,save_last'>
 				<method_info nloc='11' complexity='2' token_count='75' nesting_level='0' start_line='266' end_line='277'></method_info>
 			<added_lines>266,267,268,269,270,271,272,273,274,275,276,277</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>10,11,261,262,263,264,265</added_lines>
 			<deleted_lines>9</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
