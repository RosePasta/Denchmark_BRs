<bug_data>
<bug id='850' author='hjalmarlucius' open_date='2020-02-15T01:26:14Z' closed_time='2020-02-22T01:27:20Z'>
 	<summary>Epoch end checkpoint restarts previous epoch</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 If restarting the training and reloading the model, the epoch that the checkpoint had just completed is restarted rather than beginning the next.
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 When a checkpoint upon epoch end is saved, restarting it should resume its state and start the next epoch.
 	</description>
 	<comments>
 		<comment id='1' author='hjalmarlucius' date='2020-02-16T11:25:27Z'>
 		
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_io.py
 
 
          Line 389
       in
       edd4a87
 
 
 
 
 
 
  self.current_epoch = checkpoint['epoch'] 
 
 
 
 
 
 This seems as simple as replacing the line above with self.current_epoch = checkpoint['epoch'] + 1 since the checkpointers save at the end of validation and the main loop runs from the current epoch.
 We should probably also increase the global step by 1 since this happens after saving the checkpoint.
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_io.py
 
 
          Line 388
       in
       edd4a87
 
 
 
 
 
 
  self.global_step = checkpoint['global_step'] 
 
 
 
 
 
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_loop.py
 
 
          Line 411
       in
       edd4a87
 
 
 
 
 
 
  self.run_evaluation(test=self.testing) 
 
 
 
 
 
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_loop.py
 
 
          Line 430
       in
       edd4a87
 
 
 
 
 
 
  self.global_step += 1 
 
 
 
 
 
 I'll add a test in whilst I do it.
 &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
  Any other thoughts if I put a PR in with this? The test should presumably go in ?
 		</comment>
 		<comment id='2' author='hjalmarlucius' date='2020-02-16T15:47:36Z'>
 		That works when saving at epoch end but there's many cases of saving during an epoch as well (e.g. for a very large dataset). Both the epoch number and the global step are technically correct upon saving mid or end epoch but when resuming, the loop starts at the beginning. The best solution is to make the resume reliably restart precisely where in the loop it left off. I'm not that familiar with this code but guess one should then also save the batch_idx.
 		</comment>
 		<comment id='3' author='hjalmarlucius' date='2020-02-16T17:03:20Z'>
 		I can think of at least one way to do it, although it's not ideal:
 
 Use the global step as stored and when resuming, load this into a (hidden?) trainer variable which is checked every training batch.
 Skip each batch until the batch number is above this saved variable, at which point we can set it to None or something.
 Run batches as normal
 
 Two Problems I see:
 
 When the data set is shuffled there would be no guarantee of seeing only new samples after the reload without somehow 'resuming' the dataloaders.
 Any calls that are usually made during the batch wouldn't happen. For example, the tqdm update calls would need to be faked so it didn't appear to end the epoch early in the progress bars.
 
 		</comment>
 		<comment id='4' author='hjalmarlucius' date='2020-02-17T13:11:56Z'>
 		Currently I've put in a PR (&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/866&gt;#866&lt;/denchmark-link&gt;
 )  that deals with the off by one when loading a checkpoint from epoch end. I've added a warning when loading a mid-epoch checkpoint that says resuming training is not reliable and to consider loading an end of epoch checkpoint.
 If it's preferable I can also rerun the previous epoch when we detect mid-epoch checkpoints, but this technically means you run for more epochs than you would expect/report, so I'm not sure if this is a good idea.
 I'd suggest a new issue and discussion on how to resume mid epoch checkpoints, since we have no way of ensure data set states are preserved, and close this issue with the PR I have up.
 		</comment>
 		<comment id='5' author='hjalmarlucius' date='2020-02-18T17:20:24Z'>
 		Great, a good compromise for now
 		</comment>
 	</comments>
 </bug>
<commit id='6e7dc9c2363779a12e8122ee1e2d470a0b0f013e' author='Matt Painter' date='2020-02-21 20:27:19-05:00'>
 	<dmm_unit complexity='0.8571428571428571' interfacing='1.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\training_io.py' new_name='pytorch_lightning\trainer\training_io.py'>
 		<file_info nloc='314' complexity='72' token_count='1558'></file_info>
 		<method name='dump_checkpoint' parameters='self'>
 				<method_info nloc='29' complexity='8' token_count='202' nesting_level='1' start_line='309' end_line='350'></method_info>
 			<added_lines>311,312</added_lines>
 			<deleted_lines>310,311</deleted_lines>
 		</method>
 		<method name='restore_training_state' parameters='self,checkpoint'>
 				<method_info nloc='27' complexity='14' token_count='232' nesting_level='1' start_line='375' end_line='419'></method_info>
 			<added_lines>392,393,394,395,396,397,398,399,400,401,402</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self'>
 				<method_info nloc='16' complexity='1' token_count='80' nesting_level='1' start_line='112' end_line='129'></method_info>
 			<added_lines>128,129</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>308</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\test_restore_models.py' new_name='tests\test_restore_models.py'>
 		<file_info nloc='218' complexity='19' token_count='1394'></file_info>
 		<method name='test_cpu_restore_training' parameters='tmpdir'>
 				<method_info nloc='32' complexity='1' token_count='189' nesting_level='0' start_line='262' end_line='321'></method_info>
 			<added_lines>285,286</added_lines>
 			<deleted_lines>285</deleted_lines>
 		</method>
 		<method name='test_dp_resume' parameters='tmpdir'>
 				<method_info nloc='35' complexity='2' token_count='214' nesting_level='0' start_line='184' end_line='259'></method_info>
 			<added_lines>217,218</added_lines>
 			<deleted_lines>217,218</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\test_trainer.py' new_name='tests\test_trainer.py'>
 		<file_info nloc='471' complexity='43' token_count='3090'></file_info>
 		<method name='test_resume_from_checkpoint_epoch_restored.new_model' parameters=''>
 				<method_info nloc='9' complexity='1' token_count='50' nesting_level='1' start_line='424' end_line='440'></method_info>
 			<added_lines>424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_resume_from_checkpoint_epoch_restored.test_resume_from_checkpoint_epoch_restored.new_model.increment_batch' parameters='self,_'>
 				<method_info nloc='2' complexity='1' token_count='12' nesting_level='2' start_line='433' end_line='434'></method_info>
 			<added_lines>433,434</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_resume_from_checkpoint_epoch_restored' parameters='tmpdir'>
 				<method_info nloc='33' complexity='2' token_count='200' nesting_level='0' start_line='416' end_line='481'></method_info>
 			<added_lines>416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_resume_from_checkpoint_epoch_restored.test_resume_from_checkpoint_epoch_restored.new_model.increment_epoch' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='10' nesting_level='2' start_line='430' end_line='431'></method_info>
 			<added_lines>430,431</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>482,483</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
