<bug_data>
<bug id='2600' author='p-wein' open_date='2020-07-13T13:26:48Z' closed_time='2020-09-15T09:07:28Z'>
 	<summary>Trainer flag overfit_batches does not overwrite train dataloaders shuffle flag</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 Setting the trainer flag overfit_batches (e.g. =10) does not overwrite the shuffle flag set in the training dataloader, even though the warning reads:
 UserWarning: You requested to overfit but enabled training dataloader shuffling. We are turning it off for you.
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 Steps to reproduce the behavior:
 
 Create lightning module with method train_dataloader with flag shuffle=True:
 
 &lt;denchmark-code&gt;   def train_dataloader(self) -&gt; loading.DataLoader:
         dataset = ProstateX(train=True)
         batch_transforms, gpu_transforms, sample_transforms = self.get_transformations()
         dataloader = loading.DataLoader(dataset,
                                         batch_size=self.hparams.tr_batch_size,
                                         batch_transforms=batch_transforms,
                                         shuffle=True,
                                         sample_transforms= sample_transforms,
                                         gpu_transforms=gpu_transforms,
                                         pseudo_batch_dim=True,
                                         num_workers=self.hparams.num_workers)
         return dataloader 
 &lt;/denchmark-code&gt;
 
 ( I use a rising dataloader, bug should also occur with pytorch dataloaders though)
 
 Create main.py with:
 
 &lt;denchmark-code&gt;mymodel = model.Model3D(cfg)
 trainer = pl.Trainer(gpus=1, precision=16, overfit_batches=10)
 trainer.fit(mymodel)`
 &lt;/denchmark-code&gt;
 
 
 Run main.py
 Find out that your model does not converge.
 set shuffle=False when creating Dataloader in train_dataloader
 See that your model converges after some epochs.
 
 (Or log the samples loaded by the dataloader and check if they are the same each epoch.)
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 Either model also converges with shuffle=True, since warning says that it got overwritten (assuming model converges with shuffle=False) or at least warning should read that user has to change shuffle to False.
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 
 CUDA:
 - GPU:
 - GeForce GTX 1080 Ti
 - available:         True
 - version:           10.1
 Packages:
 - numpy:             1.19.0
 - pyTorch_debug:     False
 - pyTorch_version:   1.7.0.dev20200705+cu101
 - pytorch-lightning: 0.8.5
 - tensorboard:       2.2.2
 - tqdm:              4.47.0
 System:
 - OS:                Linux
 - architecture:
 - 64bit
 -
 - processor:         x86_64
 - python:            3.7.7
 - version:           #109-Ubuntu SMP Fri Jun 19 11:33:10 UTC 2020
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='p-wein' date='2020-07-13T13:27:53Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 		<comment id='2' author='p-wein' date='2020-09-12T11:39:18Z'>
 		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
 		</comment>
 		<comment id='3' author='p-wein' date='2020-09-28T13:45:33Z'>
 		I am seeing the same issue when using . &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/859ec92da51ff56b49e3e1f1beefa75767edc175/pytorch_lightning/trainer/trainer.py#L132&gt; From a comment in the code, &lt;/denchmark-link&gt;
 I believe that option is to be removed in 1.0.0, but is it worth it to fix it anyways? The same code will do fix the issue just checking  instead.
 		</comment>
 	</comments>
 </bug>
<commit id='b5dc6998ae80b026bb6adc4040980a153390307a' author='Phil' date='2020-09-15 05:07:27-04:00'>
 	<dmm_unit complexity='0.14285714285714285' interfacing='1.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>54,55</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\data_loading.py' new_name='pytorch_lightning\trainer\data_loading.py'>
 		<file_info nloc='259' complexity='52' token_count='1570'></file_info>
 		<method name='reset_train_dataloader' parameters='self,LightningModule'>
 				<method_info nloc='47' complexity='13' token_count='324' nesting_level='1' start_line='162' end_line='219'></method_info>
 			<added_lines>170,171,172,173,174,175</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>256</added_lines>
 			<deleted_lines>250</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\trainer\test_trainer_tricks.py' new_name='tests\trainer\test_trainer_tricks.py'>
 		<file_info nloc='202' complexity='19' token_count='1621'></file_info>
 		<method name='test_overfit_batch_limits' parameters='tmpdir'>
 				<method_info nloc='51' complexity='3' token_count='566' nesting_level='0' start_line='41' end_line='142'></method_info>
 			<added_lines>59,61,62,92,93</added_lines>
 			<deleted_lines>63,90</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
