<bug_data>
<bug id='2680' author='shtoshni92' open_date='2020-07-23T19:39:33Z' closed_time='2020-08-08T10:02:44Z'>
 	<summary>Checkpoint saving order</summary>
 	<description>
 The last model save action, &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/1369012bc71f257dcf7423ec65146d055ddc1cc7/pytorch_lightning/callbacks/model_checkpoint.py#L295&gt;here&lt;/denchmark-link&gt;
 , should be after saving the top k model because the best model and best score could have changed. Swapping the order allows resuming the training from the last checkpoint, with the last checkpoint having the latest information about the best model path/score.
 	</description>
 	<comments>
 		<comment id='1' author='shtoshni92' date='2020-07-23T19:40:28Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 		<comment id='2' author='shtoshni92' date='2020-08-08T05:11:41Z'>
 		&lt;denchmark-link:https://github.com/shtoshni92&gt;@shtoshni92&lt;/denchmark-link&gt;
  Thanks for the bug report and sorry for the long wait, it was actually an easy fix (see linked PR)
 		</comment>
 	</comments>
 </bug>
<commit id='f798cffd02a0b6cbdc3033c981501c1a0c4677bd' author='Adrian WÃ¤lchli' date='2020-08-08 06:02:43-04:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>109,110</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\callbacks\model_checkpoint.py' new_name='pytorch_lightning\callbacks\model_checkpoint.py'>
 		<file_info nloc='282' complexity='46' token_count='1403'></file_info>
 		<method name='on_validation_end' parameters='self,trainer,pl_module'>
 				<method_info nloc='44' complexity='16' token_count='302' nesting_level='1' start_line='284' end_line='345'></method_info>
 			<added_lines>343,344,345</added_lines>
 			<deleted_lines>305,306,307,308</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>99,100,101,102,346</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\training_io.py' new_name='pytorch_lightning\trainer\training_io.py'>
 		<file_info nloc='369' complexity='90' token_count='1883'></file_info>
 		<method name='restore_training_state' parameters='self,checkpoint'>
 				<method_info nloc='42' complexity='19' token_count='302' nesting_level='1' start_line='421' end_line='484'></method_info>
 			<added_lines>439,440,448</added_lines>
 			<deleted_lines>439,440,448</deleted_lines>
 		</method>
 		<method name='dump_checkpoint' parameters='self,bool'>
 				<method_info nloc='45' complexity='18' token_count='317' nesting_level='1' start_line='333' end_line='400'></method_info>
 			<added_lines>357,358</added_lines>
 			<deleted_lines>357,358</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\callbacks\test_model_checkpoint.py' new_name='tests\callbacks\test_model_checkpoint.py'>
 		<file_info nloc='97' complexity='13' token_count='719'></file_info>
 		<method name='test_model_checkpoint_save_last_checkpoint_contents' parameters='tmpdir'>
 				<method_info nloc='29' complexity='3' token_count='182' nesting_level='0' start_line='99' end_line='130'></method_info>
 			<added_lines>99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>8,11,97,98</added_lines>
 			<deleted_lines>10</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
