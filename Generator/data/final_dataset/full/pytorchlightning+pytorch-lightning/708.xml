<bug_data>
<bug id='708' author='felixkreuk' open_date='2020-01-18T12:57:14Z' closed_time='2020-02-19T11:37:36Z'>
 	<summary>LR Schedulers shouldn't get `epoch` argument in `step` function</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 PyTorch LR schedulers now shouldn't get any arguments in  function, see &lt;denchmark-link:https://github.com/pytorch/pytorch/blob/9e9bfbfd8d893cdf8205db67f220551214787d7f/torch/optim/lr_scheduler.py#L13-L20&gt;here&lt;/denchmark-link&gt;
  and &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/31828&gt;here&lt;/denchmark-link&gt;
 .
 Looks like the calls in PytorchLightning are not in line with the new interface, see &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/dac59bb8d354f28b919d08aa369a7db6ab31bfa6/pytorch_lightning/trainer/training_loop.py#L334-L337&gt;here&lt;/denchmark-link&gt;
 .
 This results in unexpected LR changes. Removing the epoch argument from step call solves the issue for me.
 &lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;
 
 PyTorch 1.4
 PyTorchLightning 0.5.3.2
 	</description>
 	<comments>
 		<comment id='1' author='felixkreuk' date='2020-01-18T12:58:31Z'>
 		good catch. we need to keep it backward compatible though
 		</comment>
 		<comment id='2' author='felixkreuk' date='2020-01-18T15:40:33Z'>
 		We shall catch it with a test...
 		</comment>
 		<comment id='3' author='felixkreuk' date='2020-01-21T12:21:23Z'>
 		&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
  want to submit PR for this?
 		</comment>
 		<comment id='4' author='felixkreuk' date='2020-02-11T12:59:59Z'>
 		&lt;denchmark-link:https://github.com/felixkreuk&gt;@felixkreuk&lt;/denchmark-link&gt;
  want  to submit a PR?
 		</comment>
 	</comments>
 </bug>
<commit id='c58aab0b0024c36a9bd4d5a0a472c84b80edc61d' author='Nicki Skafte' date='2020-02-19 06:37:35-05:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\training_loop.py' new_name='pytorch_lightning\trainer\training_loop.py'>
 		<file_info nloc='481' complexity='100' token_count='2147'></file_info>
 		<method name='train' parameters='self'>
 				<method_info nloc='63' complexity='27' token_count='454' nesting_level='1' start_line='307' end_line='401'></method_info>
 			<added_lines>365,373</added_lines>
 			<deleted_lines>365,373</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
