<bug_data>
<bug id='1566' author='karlinjf' open_date='2020-04-22T21:32:23Z' closed_time='2020-04-23T18:28:21Z'>
 	<summary>Batch being moved to gpu repeatedly with multiple optimizers and single gpu training</summary>
 	<description>
 If you have multiple optimizers, then transfer_batch_to_gpu winds up getting called once per opt_idx, and the batch is copied each time via copy.copy(batch) in training_forward. Why copy the batch when there is only a single gpu? By removing the copy.copy() my GAN model moves from 8.53it/s to 9.25it/s. Pretty significant speedup.
 	</description>
 	<comments>
 		<comment id='1' author='karlinjf' date='2020-04-22T22:15:11Z'>
 		amazing find! mind submitting a PR?
 		</comment>
 		<comment id='2' author='karlinjf' date='2020-04-22T22:44:28Z'>
 		Sure, will do.
 		</comment>
 	</comments>
 </bug>
<commit id='41b6cbb3ca8a3a43e091d7f0de4d0184a8870d19' author='karlinjf' date='2020-04-23 14:28:20-04:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\training_loop.py' new_name='pytorch_lightning\trainer\training_loop.py'>
 		<file_info nloc='561' complexity='131' token_count='2792'></file_info>
 		<method name='training_forward' parameters='self,batch,batch_idx,opt_idx,hiddens'>
 				<method_info nloc='43' complexity='14' token_count='322' nesting_level='1' start_line='715' end_line='792'></method_info>
 			<added_lines>757,758,759,760,761</added_lines>
 			<deleted_lines>757</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
