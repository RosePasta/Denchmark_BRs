<bug_data>
<bug id='2188' author='xiadingZ' open_date='2020-06-15T03:10:15Z' closed_time='2020-08-03T18:44:12Z'>
 	<summary>[hparams] save_hyperparameters doesn't save kwargs</summary>
 	<description>
 &lt;denchmark-h:h2&gt;‚ùì Questions and Help&lt;/denchmark-h&gt;
 
 when I use hyperparemeters like docs:
 &lt;denchmark-code&gt;class LitMNIST(LightningModule):
 
     def __init__(self, layer_1_dim=128, learning_rate=1e-2, **kwargs):
         super().__init__()
         # call this to save (layer_1_dim=128, learning_rate=1e-4) to the checkpoint
         self.save_hyperparameters()
 
 &lt;/denchmark-code&gt;
 
 model checkpoint doesn't save args in kwargs. But kwargs is important. Args such as num_frames, img_size, img_std ... must be used in creating dataloader, but it will be tedious if writes them in __init__ explicitly .  it can make code clean if hides them in kwargs.
 Before I use hparams, it's ok. But now it's not recommended to use hparams,  is there any good idea to  deal with this problem?
 	</description>
 	<comments>
 		<comment id='1' author='xiadingZ' date='2020-06-15T03:24:08Z'>
 		if don't use hparams, it will put all args of model, dataset, dataloader... in a LightnModule' s __init__ method, and save_hyperparameters  doesn't save args in kwargs. It this really a good idea?
 		</comment>
 		<comment id='2' author='xiadingZ' date='2020-06-15T17:21:33Z'>
 		Could you please share your model example?
 		</comment>
 		<comment id='3' author='xiadingZ' date='2020-06-16T02:50:51Z'>
 		
 Could you please share your model example?
 
 &lt;denchmark-code&gt;class LitMNIST(LightningModule):
 
     def __init__(self, layer_1_dim=128, learning_rate=1e-2, **kwargs):
         super().__init__()
         # call this to save (layer_1_dim=128, learning_rate=1e-4) to the checkpoint
         self.save_hyperparameters()
         self.kwargs = kwargs
         ...
     
     def train_dataloader(self):
         img_size = self.kwargs['img_size']
         ...
 &lt;/denchmark-code&gt;
 
 I can train this model, but when I load from checkpoint, it says kwargs hasn't img_size
 		</comment>
 		<comment id='4' author='xiadingZ' date='2020-06-16T14:34:35Z'>
 		
 I can train this model, but when I load from checkpoint, it says kwargs hasn't img_size
 
 I see, we need to ignore  from the model hparams saving...
 &lt;denchmark-link:https://github.com/xiadingZ&gt;@xiadingZ&lt;/denchmark-link&gt;
  mind adding PR with a test for this case and I ll finish it with a patch?
 		</comment>
 		<comment id='5' author='xiadingZ' date='2020-06-19T00:48:13Z'>
 		&lt;denchmark-code&gt;Traceback (most recent call last):
   File "./training.py", line 101, in &lt;module&gt;
     main(hparam_trial)
   File "./training.py", line 86, in main
     model = module(hparams, fold_train, fold_val, data_dir+img_dir)
   File "../main/module.py", line 18, in __init__
     self.hparams = hparams
   File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 638, in __setattr__
     object.__setattr__(self, name, value)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py", line 1695, in hparams
     self.save_hyperparameters(hp, frame=inspect.currentframe().f_back.f_back)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py", line 1662, in save_hyperparameters
     cand_names = [k for k, v in init_args.items() if v == hp]
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py", line 1662, in &lt;listcomp&gt;
     cand_names = [k for k, v in init_args.items() if v == hp]
   File "/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py", line 1479, in __nonzero__
     f"The truth value of a {type(self).__name__} is ambiguous. "
 ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
 &lt;/denchmark-code&gt;
 
 I'm guessing this is why 0.8.0 causes this error, it's trying to save all args (including dataframes in my case) outside of hparams?
 Edit: &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2250&gt;#2250&lt;/denchmark-link&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='6ae9a97b09fd8e3239219c2882c6f3cc31a2ccf8' author='William Falcon' date='2020-06-18 23:08:25-04:00'>
 	<dmm_unit complexity='1.0' interfacing='0.8' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\core\lightning.py' new_name='pytorch_lightning\core\lightning.py'>
 		<file_info nloc='1569' complexity='84' token_count='2303'></file_info>
 		<method name='hparams' parameters='self,dict,Namespace'>
 				<method_info nloc='4' complexity='1' token_count='34' nesting_level='1' start_line='1695' end_line='1698'></method_info>
 			<added_lines>1696,1697,1698</added_lines>
 			<deleted_lines>1695</deleted_lines>
 		</method>
 		<method name='__get_hparams_assignment_variable' parameters='self'>
 				<method_info nloc='8' complexity='3' token_count='65' nesting_level='1' start_line='1700' end_line='1712'></method_info>
 			<added_lines>1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>4,1699</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\core\saving.py' new_name='pytorch_lightning\core\saving.py'>
 		<file_info nloc='304' complexity='37' token_count='1168'></file_info>
 		<method name='_load_model_state' parameters='cls,str,args,kwargs'>
 				<method_info nloc='22' complexity='10' token_count='187' nesting_level='1' start_line='175' end_line='207'></method_info>
 			<added_lines>179,184,187,190</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\models\test_hparams.py' new_name='tests\models\test_hparams.py'>
 		<file_info nloc='234' complexity='39' token_count='1870'></file_info>
 		<method name='_run_standard_hparams_test' parameters='tmpdir,model,cls,try_overwrite'>
 				<method_info nloc='16' complexity='2' token_count='134' nesting_level='0' start_line='33' end_line='62'></method_info>
 			<added_lines>42</added_lines>
 			<deleted_lines>42</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
