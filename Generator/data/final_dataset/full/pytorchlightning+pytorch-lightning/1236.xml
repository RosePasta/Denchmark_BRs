<bug_data>
<bug id='1236' author='dumitrescustefan' open_date='2020-03-25T19:02:05Z' closed_time='2020-03-29T18:56:37Z'>
 	<summary>AdvancedProfiler error</summary>
 	<description>
 Hi, as others have pointed out, the Profiler doesn't seem to work (it prints nothing), and trying out the AdvancedProfiler as in &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/profiler.html&gt;https://pytorch-lightning.readthedocs.io/en/latest/profiler.html&lt;/denchmark-link&gt;
  like:
 &lt;denchmark-code&gt;from pytorch_lightning.profiler import AdvancedProfiler
     profiler = AdvancedProfiler(output_filename="prof.txt")
     trainer = Trainer(profiler=profiler, (other params here)
 &lt;/denchmark-code&gt;
 
 gives me the following error:
 &lt;denchmark-code&gt;Validation sanity check: 50it [00:00, 212.11it/s]             Traceback (most recent call last):
   File "/Users/sdumitre/work/style/training.py", line 177, in &lt;module&gt;
     main(hparams)
   File "/Users/sdumitre/work/style/training.py", line 77, in main
     trainer.fit(model)
   File "/Users/sdumitre/virtual/p3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 630, in fit
     self.run_pretrain_routine(model)
   File "/Users/sdumitre/virtual/p3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 810, in run_pretrain_routine
     _, _, _, callback_metrics, _ = self.process_output(eval_results)
   File "/Users/sdumitre/virtual/p3/lib/python3.7/site-packages/pytorch_lightning/trainer/logging.py", line 117, in process_output
     callback_metrics[k] = v.item()
 ValueError: only one element tensors can be converted to Python scalars
                                                  
 Process finished with exit code 1
 &lt;/denchmark-code&gt;
 
 Any pointers?
 My env: torch 1.4 installed with pip, Python 3.7, no GPU, on, MacOS.
 Thanks for the great lib you're developing!
 	</description>
 	<comments>
 		<comment id='1' author='dumitrescustefan' date='2020-03-25T19:46:36Z'>
 		&lt;denchmark-link:https://github.com/jeremyjordan&gt;@jeremyjordan&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='dumitrescustefan' date='2020-03-26T00:17:17Z'>
 		Hi &lt;denchmark-link:https://github.com/dumitrescustefan&gt;@dumitrescustefan&lt;/denchmark-link&gt;
  thanks for submitting this issue.
 
 as others have pointed out, the Profiler doesn't seem to work
 
 If there are other Github Issues please reference them here
 
 the Profiler doesn't seem to work (it prints nothing)
 
 Did you configure logging? I usually do this in the root __init__.py of my projects.
 eg.
 &lt;denchmark-code&gt;import logging
 logging.basicConfig(level=logging.INFO)
 &lt;/denchmark-code&gt;
 
 Finally, this seems unrelated to the AdvancedProfiler.
 &lt;denchmark-code&gt; File "/Users/sdumitre/virtual/p3/lib/python3.7/site-packages/pytorch_lightning/trainer/logging.py", line 117, in process_output
     callback_metrics[k] = v.item()
 ValueError: only one element tensors can be converted to Python scalars
 &lt;/denchmark-code&gt;
 
 Can you show what your training_step and validation_step code looks like?
 		</comment>
 		<comment id='3' author='dumitrescustefan' date='2020-03-26T04:48:18Z'>
 		btw I have value error but not in profiler. I was using loguru instead of logging.
 		</comment>
 		<comment id='4' author='dumitrescustefan' date='2020-03-26T08:32:45Z'>
 		&lt;denchmark-link:https://github.com/jeremyjordan&gt;@jeremyjordan&lt;/denchmark-link&gt;
  thanks for the tips! I put logging into .py, and tried with the "basic" profiler again, now I get the same error.
 Here are the train/val_steps:
 &lt;denchmark-code&gt;    def training_step(self, batch: tuple, batch_nb: int, *args, **kwargs) -&gt; dict:
         x_tensor, x_lengths, y_tensor = batch
 
         model_out = self.forward(x_tensor, x_lengths)
         loss_val = self.loss(model_out, y_tensor)
 
         tqdm_dict = {"train_loss": loss_val}
         output = OrderedDict(
             {"loss": loss_val, "progress_bar": tqdm_dict, "log": tqdm_dict}
         )
         return output
 
     def validation_step(self, batch: tuple, batch_nb: int, *args, **kwargs) -&gt; dict:
         x_tensor, x_lengths, y_tensor = batch
 
         model_out = self.forward(x_tensor, x_lengths)
         loss_val = self.loss(model_out, y_tensor)
 
         output = OrderedDict({"val_loss": loss_val})
 
         return output
 &lt;/denchmark-code&gt;
 
 All the code (except the forward and the model params) is copy-pasted from a lightning tutorial. Without the profiler everything seems to work okay. The trainer is initialised with:
 &lt;denchmark-code&gt;trainer = Trainer(
         logger=setup_testube_logger(),
         checkpoint_callback=True,
         early_stop_callback=early_stop_callback,
         default_save_path="experiments/",
         gpus=hparams.gpus,
         distributed_backend=hparams.distributed_backend,
         use_amp=hparams.use_16bit,
         max_epochs=hparams.max_epochs,
         min_epochs=hparams.min_epochs,
         accumulate_grad_batches=hparams.accumulate_grad_batches,
         log_gpu_memory=hparams.log_gpu_memory,
         val_percent_check=hparams.val_percent_check,
         profiler=True &lt;-- this is what I added
     )
 &lt;/denchmark-code&gt;
 
 IMHO, shouldn't the profiler be agnostic to what I do in the code? Actually the in-built profiler is one of the main features that made me try out Lightning. I would be most grateful to have it work :) Please tell me what piece of code I could provide. The model itself aims at predicting a set of n values (floats) based on a number of sentences (embedded with BPE as ints). There is a sentence-level RNN that encodes each sentence, and then a "document" level RNN that runs over each sentence. This gets into a hidden-&gt;n linear layer and the error is MSELoss(). This is a baseline I created and I'd like to build from here, but I need to get past these initial errors. I don't know if this info is useful for you, I can provide all the code if required.
 Thanks!
 		</comment>
 		<comment id='5' author='dumitrescustefan' date='2020-03-26T09:58:08Z'>
 		My bad. I did 2 things, I added the profiler and then I also added reduction='none' (in MSELoss) and in the loss function I forgot to do the reduction myself; thus the loss function returned a 2D tensor instead of a scalar, which broke the pytorch_lightning/trainer/logging.py, which led me to believe it was a logging error. I'm usually careful with changing more than a single item per run, but hey, blame the new lib I'm learning to use :)
 I'm not closing this issue because even though now the AdvancedProfiler works (dumps to a file), the basic one still doesn't want to print anything onscreen, even after adding level=DEBUG.
 If logging is required, maybe the docs could be updated ( &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/profiler.html&gt;https://pytorch-lightning.readthedocs.io/en/latest/profiler.html&lt;/denchmark-link&gt;
  ).
 Should I do anything more besides adding logging to init?
 &lt;denchmark-code&gt;import logging
 logging.basicConfig(level=logging.DEBUG)
 &lt;/denchmark-code&gt;
 
 and in the Trainer object:
 &lt;denchmark-code&gt;profiler=True
 &lt;/denchmark-code&gt;
 
 Thanks!
 Thanks and sorry for the time taken on my mistake!
 		</comment>
 		<comment id='6' author='dumitrescustefan' date='2020-03-27T03:14:05Z'>
 		
 IMHO, shouldn't the profiler be agnostic to what I do in the code?
 
 yes! that's why i was confused about your error :)
 
 then I also added reduction='none' (in MSELoss) and in the loss function I forgot to do the reduction myself; thus the loss function returned a 2D tensor instead of a scalar, which broke the pytorch_lightning/trainer/logging.py
 
 but this makes perfect sense
 &lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;
 
 
 Actually the in-built profiler is one of the main features that made me try out Lightning. I would be most grateful to have it work :)
 
 that's great to hear! definitely want to help you get this figured out.
 i tried reproducing your error but it's working for me - check out this colab notebook
 &lt;denchmark-link:https://colab.research.google.com/drive/1wSwMd5xGb36zdNS-5yk6ptHNEXqa1IMi&gt;https://colab.research.google.com/drive/1wSwMd5xGb36zdNS-5yk6ptHNEXqa1IMi&lt;/denchmark-link&gt;
 
 could you perhaps share a colab notebook where this is failing?
 btw i agree we should add a note to the documentation about enabling the logger, i believe we used to configure logging within the library but that was removed at one point
 		</comment>
 		<comment id='7' author='dumitrescustefan' date='2020-03-27T09:36:22Z'>
 		similar question about missing logging table was raised also by &lt;denchmark-link:https://github.com/dumitrescustefan&gt;@dumitrescustefan&lt;/denchmark-link&gt;
 
 I would suggest adding also a method  returning string of the stats
 so we avoid this confusion with logging init and a user can simply use:
 &lt;denchmark-code&gt;print(trainer.profiler.summary())
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/jeremyjordan&gt;@jeremyjordan&lt;/denchmark-link&gt;
  ^^
 		</comment>
 		<comment id='8' author='dumitrescustefan' date='2020-03-27T11:11:34Z'>
 		
 
 if logging is required, let’s just auto configure it for the user? not doing so goes against our principles.
 
 
 i think a problem with the logger is that i think it prints when training completes. if you stop training early (early stopping), or just ctrl c, i suspect you won’t see it?
 
 
 adding that print suggestion i think is also yet another thing the user has to remember which goes against our principles haha.
 
 
 If (2) is true, then let’s just make a limit of 2 epochs when profiler is enabled?
 Another option is to always run the basic profiler for the sanity check. Then profiler=True would run it for training as well. but i think the sanity check profiler won’t reflect the true speed bc it doesn’t backprop?
 &lt;denchmark-link:https://github.com/jeremyjordan&gt;@jeremyjordan&lt;/denchmark-link&gt;
  can we prioritize making these fixes as this is a key feature?
 		</comment>
 		<comment id='9' author='dumitrescustefan' date='2020-03-28T00:55:48Z'>
 		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 
 
 if logging is required, let’s just auto configure it for the user? not doing so goes against our principles.
 
 at the time when the feature was merged, we were configuring logging. i went back to &lt;denchmark-link:https://github.com/jeremyjordan/pytorch-lightning/blob/feature/profiling/pytorch_lightning/__init__.py#L29&gt;the branch&lt;/denchmark-link&gt;
  and verified that it was working out of the box. there was a later PR (&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/767&gt;#767&lt;/denchmark-link&gt;
 ) which removed this.
 
 i think a problem with the logger is that i think it prints when training completes. if you stop training early (early stopping), or just ctrl c, i suspect you won’t see it?
 
 actually, we still do show it :) in both cases (early stopping + keyboard interrupt)
 
 can we prioritize making these fixes as this is a key feature?
 
 yeah for sure. as i understand, this should just involve adding the logging config back in
 		</comment>
 	</comments>
 </bug>
<commit id='54507f417eaf3317a798b3303c398152a4e35a18' author='Jeremy Jordan' date='2020-03-29 14:56:36-04:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\__init__.py' new_name='pytorch_lightning\__init__.py'>
 		<file_info nloc='30' complexity='0' token_count='108'></file_info>
 		<modified_lines>
 			<added_lines>13,15,16</added_lines>
 			<deleted_lines>13,15</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\test_profiler.py' new_name='tests\test_profiler.py'>
 		<file_info nloc='100' complexity='22' token_count='748'></file_info>
 		<method name='test_simple_profiler_describe' parameters='caplog,simple_profiler'>
 				<method_info nloc='3' complexity='1' token_count='19' nesting_level='0' start_line='77' end_line='81'></method_info>
 			<added_lines>77,81</added_lines>
 			<deleted_lines>77</deleted_lines>
 		</method>
 		<method name='test_simple_profiler_describe' parameters='simple_profiler'>
 				<method_info nloc='2' complexity='1' token_count='11' nesting_level='0' start_line='77' end_line='79'></method_info>
 			<added_lines>77</added_lines>
 			<deleted_lines>77</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>82</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
