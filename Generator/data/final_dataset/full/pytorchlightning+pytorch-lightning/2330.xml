<bug_data>
<bug id='2330' author='stllfe' open_date='2020-06-23T14:44:48Z' closed_time='2020-06-25T13:21:43Z'>
 	<summary>`use_amp` and multiple optimizers bug</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 Faced this issue when tried to use mixed precision with my two-head model which has two pairs of optimizer &amp; scheduler. Without use_amp everything works fine.
 With it enabled, I get:
 &lt;denchmark-code&gt;TypeError: 'CosineAnnealingLR' object is not subscriptable
 &lt;/denchmark-code&gt;
 
 My investigation has ended up here:
     def reinit_scheduler_properties(self, optimizers: list, schedulers: list):
         # Reinitialize optimizer.step properties added by schedulers
         for scheduler in schedulers:
             for optimizer in optimizers:
                 scheduler = scheduler['scheduler']  # &lt;===== this place
                 # ...
                 if scheduler.optimizer == optimizer:
                     # ...
 Obviously, next optimizer will get scheduler as an actual non-dict object as it was reassigned on the first iteration...
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 Steps to reproduce the behavior:
 
 Build any LightningModule, which configure_optimizers() method outputs lists of two optimizers and two schedulers. In my case it's something like:
 
 def configure_optimizers(self):
     opt_1 = Adam(params=self.head_1.params(), ...)
     opt_2 = Adam(params=self.head_2.params(), ...)
     
     sch_1 = CosineAnnealingLR(optimizer=opt_1, ...)
     sch_2 = CosineAnnealingLR(optimizer=opt_2, ...)
     return [opt_1, opt_2], [sch_1, sch_2]
 
 Build a Trainer object with  use_amp=True
 Call trainer.fit(model)
 See error
 
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;* CUDA:
         - GPU:
                 - GeForce GTX 1080 Ti
         - available:         True
         - version:           10.2
 * Packages:
         - numpy:             1.18.5
         - pyTorch_debug:     False
         - pyTorch_version:   1.5.0
         - pytorch-lightning: 0.8.1
         - tensorboard:       1.15.0
         - tqdm:              4.45.0
 * System:
         - OS:                Linux
         - architecture:
                 - 64bit
                 -
         - processor:         x86_64
         - python:            3.7.5
         - version:           #107-Ubuntu SMP Thu Jun 4 11:27:52 UTC 2020
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='stllfe' date='2020-06-23T14:45:28Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 		<comment id='2' author='stllfe' date='2020-06-24T10:09:24Z'>
 		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
  this seems easy to fix (move the  part to the outer loop), and we could also fix issue &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2078&gt;#2078&lt;/denchmark-link&gt;
  while fixing this function
 		</comment>
 	</comments>
 </bug>
<commit id='c275e1fc91df4d351799b633e9df08e010094bfe' author='William Falcon' date='2020-06-25 09:21:41-04:00'>
 	<dmm_unit complexity='0.0' interfacing='0.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\optimizers.py' new_name='pytorch_lightning\trainer\optimizers.py'>
 		<file_info nloc='117' complexity='23' token_count='714'></file_info>
 		<method name='reinit_scheduler_properties' parameters='self,list,list'>
 				<method_info nloc='17' complexity='8' token_count='111' nesting_level='1' start_line='111' end_line='132'></method_info>
 			<added_lines>114,115,121,122,123,124,126,127,128,129,130,131,132</added_lines>
 			<deleted_lines>115,120,122</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
