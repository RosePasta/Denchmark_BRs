<bug_data>
<bug id='1366' author='belskikh' open_date='2020-04-03T16:01:47Z' closed_time='2020-04-24T21:21:01Z'>
 	<summary>ModelCheckpoint tries to remove already removed checkpoint in DDP mode</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 When training in DDP mode with ModelCheckpoint callback, the train process fails, when ModelCheckpoint callback tries to remove previous checkpoint. I assume that it was already deleted by another process.
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 Steps to reproduce the behavior:
 Run training with "ddp" backend and ModelCheckpoint callback with save_top_k={some_number}
 &lt;denchmark-code&gt;  File "/home/myuser/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 19, in _wrap                                                                                                 
     fn(i, *args)                                                                                
   File "/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py", line 342, in ddp_train
     self.run_pretrain_routine(model)                                                                                                        
   File "/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 830, in run_pretrain_routine
     self.train()                                                                                                                                                                                                   
   File "/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 343, in train                                                      
     self.run_training_epoch()                            
   File "/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 452, in run_training_epoch                                                                       
     self.call_checkpoint_callback()                                                                                                                                                   
   File "/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 737, in call_checkpoint_callback
     self.checkpoint_callback.on_validation_end(self, self.get_model())                                                                                                                                             
   File "/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 204, in on_validation_end                                      
     self._do_check_save(filepath, current, epoch)    
   File "/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 221, in _do_check_save                                                                      
     self._del_model(delpath)                                                                                                                                                           
   File "/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 121, in _del_model
     os.remove(filepath)                                                                                                                                                                                            
 FileNotFoundError: [Errno 2] No such file or directory: {PREVIOUS_CHECKPOINT_NAME}
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 I expect that ModelCheckpoint callbacks from different DDP processes will not concurrent with each other in saving/deleteng files.
 I fixed by rewriting _del_model method of ModelCheckpoint callback:
 &lt;denchmark-code&gt;class DDPModelCheckpoint(ModelCheckpoint):
     def _del_model(self, filepath):
         try:
             os.remove(filepath)
         except Exception:
             pass
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 
 PyTorch Version: 1.4
 OS: Ubuntu 18.04
 How you installed PyTorch - conda
 Python version: 3.7
 CUDA/cuDNN version: 10.2
 GPU models and configuration: 2x2080Ti
 pytorch-lightning version: 0.7.1
 
 	</description>
 	<comments>
 		<comment id='1' author='belskikh' date='2020-04-03T16:02:40Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 		<comment id='2' author='belskikh' date='2020-04-04T08:16:38Z'>
 		Your suggestion to pass on an Exception is not the best, at least you should make it the specific error, i.e., FileNotFoundError. But in this case, I suggest to do simply
 &lt;denchmark-code&gt;if os.path.isfile(filepath):
     os.remove(filepath)
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='3' author='belskikh' date='2020-04-04T08:18:29Z'>
 		Is there also an issue with saving? Does it save/overwrite the file in multiple processes?
 		</comment>
 		<comment id='4' author='belskikh' date='2020-04-06T07:34:57Z'>
 		I encountered that one too. From my perspective, model updates should be happening within main worker only (master worker). However, I guess each workers lightning created is trying to delete their own checkpoints. However, slave worker never created one (and it shouldn't be). I solved the problem in a similar way with &lt;denchmark-link:https://github.com/belskikh&gt;@belskikh&lt;/denchmark-link&gt;
  's workaround but it did not feel right and downgraded to 0.6.0.
 		</comment>
 		<comment id='5' author='belskikh' date='2020-04-06T09:47:20Z'>
 		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
  of course it is not the best (it may be the worse, actually) solution
 It is just a fast workaround, waiting for solid fix
 I agree with logic, when only one ModelCheckpoint callback should save/delete weights, because all weights are the same on all nodes at the end of the training step.
 It can be done somehow like this:
 &lt;denchmark-code&gt;class ModelCheckpoint(..., main_worker_rank: int = 0):
 ....
     def _del_model(self, filepath):
         if self.main_worker_rank == dist.get_rank():
              # do delete
 &lt;/denchmark-code&gt;
 
 And the same for the saving code.
 		</comment>
 		<comment id='6' author='belskikh' date='2020-04-06T10:00:02Z'>
 		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
  should Lightning do checkpoints only on rank 0? It could be a problem if writing to a shared filesystem between nodes. AFAIK the loggers already do that by only logging in process 0.
 		</comment>
 		<comment id='7' author='belskikh' date='2020-04-06T11:20:09Z'>
 		I do not think, that it should do it only on specific rank, I think user should have ability to specify rank (node), where checkpoints will be saved
 		</comment>
 		<comment id='8' author='belskikh' date='2020-04-07T19:23:25Z'>
 		
 Your suggestion to pass on an Exception is not the best, at least you should make it the specific error, i.e., FileNotFoundError. But in this case, I suggest to do simply
 if os.path.isfile(filepath):
     os.remove(filepath)
 
 
 this does not work in async, I have observed many times that the if pass for both but then when you try really delete it, it is missing for one of them...
 		</comment>
 		<comment id='9' author='belskikh' date='2020-04-07T19:28:48Z'>
 		should checkpointing be done in only one process then, like loggers?
 		</comment>
 		<comment id='10' author='belskikh' date='2020-04-07T20:11:15Z'>
 		yup. checkpoint should only happen from world_rank = 0 gpu 0
 		</comment>
 		<comment id='11' author='belskikh' date='2020-04-07T21:01:26Z'>
 		well, ModelCeckpoint doesn't have a rank...
 		</comment>
 	</comments>
 </bug>
<commit id='58a467dd68b157fdba8824a437dbaf698ad88569' author='Jirka Borovec' date='2020-04-24 17:21:00-04:00'>
 	<dmm_unit complexity='0.20689655172413793' interfacing='0.1724137931034483' size='0.7586206896551724'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>25,26,27,28,40,45,46,84,85,102</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\callbacks\model_checkpoint.py' new_name='pytorch_lightning\callbacks\model_checkpoint.py'>
 		<file_info nloc='204' complexity='33' token_count='1016'></file_info>
 		<method name='_del_model' parameters='self,filepath'>
 				<method_info nloc='3' complexity='2' token_count='23' nesting_level='1' start_line='132' end_line='134'></method_info>
 			<added_lines>133,134</added_lines>
 			<deleted_lines>132</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>15,18,94,194</added_lines>
 			<deleted_lines>17,18</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\loggers\__init__.py' new_name='pytorch_lightning\loggers\__init__.py'>
 		<file_info nloc='126' complexity='0' token_count='175'></file_info>
 		<modified_lines>
 			<added_lines>33,34,84</added_lines>
 			<deleted_lines>33,83</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\loggers\base.py' new_name='pytorch_lightning\loggers\base.py'>
 		<file_info nloc='257' complexity='55' token_count='1338'></file_info>
 		<method name='rank' parameters='self'>
 				<method_info nloc='3' complexity='1' token_count='12' nesting_level='1' start_line='255' end_line='257'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>255,256,257</deleted_lines>
 		</method>
 		<method name='rank_zero_only' parameters='Callable'>
 				<method_info nloc='4' complexity='1' token_count='17' nesting_level='0' start_line='13' end_line='25'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>13,14,15,16,17,18,19,20,21,22,23,24,25</deleted_lines>
 		</method>
 		<method name='rank_zero_only.wrapped_fn' parameters='self,args,kwargs'>
 				<method_info nloc='3' complexity='2' token_count='28' nesting_level='1' start_line='21' end_line='23'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>21,22,23</deleted_lines>
 		</method>
 		<method name='rank' parameters='self,int'>
 				<method_info nloc='3' complexity='1' token_count='17' nesting_level='1' start_line='260' end_line='262'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>260,261,262</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>11</added_lines>
 			<deleted_lines>6,12,254,258,259,263,310,311,312,313,314</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\loggers\comet.py' new_name='pytorch_lightning\loggers\comet.py'>
 		<file_info nloc='190' complexity='12' token_count='692'></file_info>
 		<modified_lines>
 			<added_lines>27,29</added_lines>
 			<deleted_lines>27</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\loggers\mlflow.py' new_name='pytorch_lightning\loggers\mlflow.py'>
 		<file_info nloc='115' complexity='14' token_count='487'></file_info>
 		<modified_lines>
 			<added_lines>18,19</added_lines>
 			<deleted_lines>18</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\loggers\neptune.py' new_name='pytorch_lightning\loggers\neptune.py'>
 		<file_info nloc='341' complexity='20' token_count='898'></file_info>
 		<modified_lines>
 			<added_lines>21,22</added_lines>
 			<deleted_lines>21</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\loggers\tensorboard.py' new_name='pytorch_lightning\loggers\tensorboard.py'>
 		<file_info nloc='154' complexity='27' token_count='816'></file_info>
 		<modified_lines>
 			<added_lines>17,18</added_lines>
 			<deleted_lines>16</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\loggers\test_tube.py' new_name='pytorch_lightning\loggers\test_tube.py'>
 		<file_info nloc='138' complexity='15' token_count='582'></file_info>
 		<method name='rank' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='11' nesting_level='1' start_line='138' end_line='139'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>138,139</deleted_lines>
 		</method>
 		<method name='rank' parameters='self,int'>
 				<method_info nloc='4' complexity='2' token_count='31' nesting_level='1' start_line='142' end_line='145'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>142,143,144,145</deleted_lines>
 		</method>
 		<method name='experiment' parameters='self'>
 				<method_info nloc='23' complexity='2' token_count='74' nesting_level='1' start_line='75' end_line='98'></method_info>
 			<added_lines>96</added_lines>
 			<deleted_lines>95</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>14,15</added_lines>
 			<deleted_lines>14,137,140,141,146</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\loggers\trains.py' new_name='pytorch_lightning\loggers\trains.py'>
 		<file_info nloc='348' complexity='43' token_count='1232'></file_info>
 		<modified_lines>
 			<added_lines>22,23</added_lines>
 			<deleted_lines>22</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\loggers\wandb.py' new_name='pytorch_lightning\loggers\wandb.py'>
 		<file_info nloc='116' complexity='15' token_count='550'></file_info>
 		<modified_lines>
 			<added_lines>18,19</added_lines>
 			<deleted_lines>18</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\distrib_data_parallel.py' new_name='pytorch_lightning\trainer\distrib_data_parallel.py'>
 		<file_info nloc='322' complexity='69' token_count='1249'></file_info>
 		<method name='ddp_train' parameters='self,process_idx,model'>
 				<method_info nloc='36' complexity='13' token_count='294' nesting_level='1' start_line='298' end_line='373'></method_info>
 			<added_lines>328,329</added_lines>
 			<deleted_lines>325,326,328,329,330</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>123,126,150</added_lines>
 			<deleted_lines>125</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\distrib_parts.py' new_name='pytorch_lightning\trainer\distrib_parts.py'>
 		<file_info nloc='597' complexity='87' token_count='1752'></file_info>
 		<method name='tpu_train' parameters='self,tpu_core_idx,model'>
 				<method_info nloc='16' complexity='4' token_count='134' nesting_level='1' start_line='494' end_line='525'></method_info>
 			<added_lines>509</added_lines>
 			<deleted_lines>509</deleted_lines>
 		</method>
 		<method name='horovod_train' parameters='self,model'>
 				<method_info nloc='27' complexity='10' token_count='243' nesting_level='1' start_line='567' end_line='619'></method_info>
 			<added_lines>612</added_lines>
 			<deleted_lines>612,613,614,615,616</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>355</added_lines>
 			<deleted_lines>355</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\logging.py' new_name='pytorch_lightning\trainer\logging.py'>
 		<file_info nloc='117' complexity='46' token_count='775'></file_info>
 		<method name='configure_logger' parameters='self,logger'>
 				<method_info nloc='16' complexity='4' token_count='83' nesting_level='1' start_line='28' end_line='44'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>36,44</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\utilities\__init__.py' new_name='pytorch_lightning\utilities\__init__.py'>
 		<file_info nloc='2' complexity='0' token_count='11'></file_info>
 		<modified_lines>
 			<added_lines>3</added_lines>
 			<deleted_lines>3</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='pytorch_lightning\utilities\distributed.py'>
 		<file_info nloc='15' complexity='4' token_count='87'></file_info>
 	</modification>
 	<modification change_type='DELETE' old_name='pytorch_lightning\utilities\warnings.py' new_name='None'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\loggers\test_base.py' new_name='tests\loggers\test_base.py'>
 		<file_info nloc='136' complexity='22' token_count='858'></file_info>
 		<modified_lines>
 			<added_lines>8,9</added_lines>
 			<deleted_lines>8</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
