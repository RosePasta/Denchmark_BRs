<bug_data>
<bug id='3143' author='24hours' open_date='2020-08-25T03:25:50Z' closed_time='2020-09-10T21:01:21Z'>
 	<summary>Trainer crashed when optimizer frequency is defined.</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 Steps to reproduce the behavior:
 Run the following code:
 &lt;denchmark-link:https://gist.github.com/24hours/ec67de5384bb05e28544d580ae424639&gt;https://gist.github.com/24hours/ec67de5384bb05e28544d580ae424639&lt;/denchmark-link&gt;
 
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "pl_bug.py", line 40, in &lt;module&gt;
     trainer.fit(mnist_model, train_loader) 
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/states.py", line 48, in wrapped_fn
     result = fn(self, *args, **kwargs)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 1073, in fit
     results = self.accelerator_backend.train(model)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/accelerators/gpu_backend.py", line 51, in train
     results = self.trainer.run_pretrain_routine(model)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 1239, in run_pretrain_routine
     self.train()
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 394, in train
     self.run_training_epoch()
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 491, in run_training_epoch
     batch_output = self.run_training_batch(batch, batch_idx)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 883, in run_training_batch
     batch_outputs[opt_idx].append(opt_closure_result.training_step_output_for_epoch_end)
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
     def configure_optimizers(self):
         optimizer_G = torch.optim.Adam(self.parameters(), lr=0.1, weight_decay=1e-5)
         optimizer_D = torch.optim.Adam(self.parameters(), lr=0.1, weight_decay=1e-5)
 
         return [
                 {'optimizer': optimizer_D, 'frequency': 5},
                 {'optimizer': optimizer_G, 'frequency': 1}
             ]
 the culprit is 'frequency' : 5, removing the line will allow trainer to run smoothly.
 &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/lightning-module.html?highlight=optimizers#configure-optimizers&gt;https://pytorch-lightning.readthedocs.io/en/latest/lightning-module.html?highlight=optimizers#configure-optimizers&lt;/denchmark-link&gt;
 
 The definition is correct according to this documentation.
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 Model should train without crash.
 The code work in 0.8.5 environment.
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 
 CUDA:
 
 GPU:
 
 GeForce GTX TITAN X
 
 
 available:         True
 version:           11.0
 
 
 Packages:
 
 numpy:             1.18.5
 pyTorch_debug:     False
 pyTorch_version:   1.6.0a0+9907a3e
 pytorch-lightning: 0.9.0
 tensorboard:       2.2.0
 tqdm:              4.48.2
 
 
 System:
 
 OS:                Linux
 architecture:
 
 64bit
 
 
 
 processor:         x86_64
 python:            3.6.10
 version:           #110-Ubuntu SMP Tue Jun 23 02:39:32 UTC 2020
 
 
 
 	</description>
 	<comments>
 		<comment id='1' author='24hours' date='2020-08-25T03:26:32Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 		<comment id='2' author='24hours' date='2020-08-25T07:08:55Z'>
 		this is the case with multiple optimizers, you need to spec them.. so you would prefer having default 1 if freq is not specified?
 		</comment>
 		<comment id='3' author='24hours' date='2020-08-25T10:24:40Z'>
 		the exception occur because trainer_loop.py incorrect count number of optimizer if frequency is defined. Since this configuration work in version 0.8.5, this look like regression error.
 Unless of course if the configuration is unsupported in version 0.9.0
 		</comment>
 		<comment id='4' author='24hours' date='2020-08-25T22:23:14Z'>
 		
 so you would prefer having default 1 if freq is not specified?
 
 That is a totally different case I think. If the frequency is not specified we run train_step for both optimizers but if it is specified to 1 for both then in such case it will run 1st batch for opt_1, 2nd for opt_2, 3rd for opt_1, 4th for opt_2...
 A simple fix here can be:
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_loop.py
 
 
          Line 872
       in
       a7705c8
 
 
 
 
 
 
  batch_outputs[opt_idx].append(opt_closure_result.training_step_output_for_epoch_end) 
 
 
 
 
 
 if len(batch_outputs) == 1:  # when frequencies are defined
     batch_outputs[0].append(opt_closure_result.training_step_output_for_epoch_end)
 else:  # no frequencies
     batch_outputs[opt_idx].append(opt_closure_result.training_step_output_for_epoch_end)
 		</comment>
 		<comment id='5' author='24hours' date='2020-08-27T14:12:39Z'>
 		ok, can someone write a test and submit a PR for this? show the test failing on master first.
 &lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
   or &lt;denchmark-link:https://github.com/rohitgr7&gt;@rohitgr7&lt;/denchmark-link&gt;
  ?
 		</comment>
 	</comments>
 </bug>
<commit id='a1ea681c47004599ee5a47a05ddd1b4ea12e60d4' author='Rohit Gupta' date='2020-09-10 23:01:20+02:00'>
 	<dmm_unit complexity='0.9411764705882353' interfacing='0.9411764705882353' size='0.9411764705882353'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>40,41</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='docs\source\converting.rst' new_name='docs\source\converting.rst'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>19,21,39,41,51,53,81,83</added_lines>
 			<deleted_lines>19,21,39,41,51,53,81,83</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\training_loop.py' new_name='pytorch_lightning\trainer\training_loop.py'>
 		<file_info nloc='374' complexity='121' token_count='3014'></file_info>
 		<method name='run_training_batch' parameters='self,batch,batch_idx,dataloader_idx'>
 				<method_info nloc='65' complexity='20' token_count='519' nesting_level='1' start_line='431' end_line='553'></method_info>
 			<added_lines>503,504</added_lines>
 			<deleted_lines>503</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\base\model_optimizers.py' new_name='tests\base\model_optimizers.py'>
 		<file_info nloc='54' complexity='11' token_count='589'></file_info>
 		<method name='configure_optimizers__multiple_optimizers_frequency' parameters='self'>
 				<method_info nloc='7' complexity='1' token_count='63' nesting_level='1' start_line='37' end_line='43'></method_info>
 			<added_lines>37,38,39,40,41,42,43</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>44</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\trainer\test_optimizers.py' new_name='tests\trainer\test_optimizers.py'>
 		<file_info nloc='175' complexity='35' token_count='1426'></file_info>
 		<method name='test_configure_optimizers_with_frequency' parameters='tmpdir'>
 				<method_info nloc='9' complexity='1' token_count='40' nesting_level='0' start_line='245' end_line='257'></method_info>
 			<added_lines>245,246,247,248,249,250,251,252,253,254,255,256,257</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>243,244</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
